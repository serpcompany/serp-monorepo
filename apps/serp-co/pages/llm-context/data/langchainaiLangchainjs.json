[
  {
    "owner": "langchain-ai",
    "repo": "langchainjs",
    "content": "TITLE: Indexing, Searching and Question Answering with Cosmos DB (TypeScript)\nDESCRIPTION: Provides a full end-to-end workflow in TypeScript for indexing documents into Azure Cosmos DB via LangChain, executing a vector similarity search, and running a retrieval-augmented generation (RAG) chain for question answering. Dependencies include @langchain/azure-cosmosdb, file system access, and LLM or related LangChain modules. Inputs are files and queries; outputs are relevant documents and generated answers. Useful for search and generative AI applications. Requires correct .env configuration and database setup beforehand.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/azure_cosmosdb_nosql.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n// Example: Indexing and Vector Search with Cosmos DB\nimport { AzureCosmosDB } from \"@langchain/azure-cosmosdb\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport { loadQAStuffChain } from \"langchain/chains\";\nimport { readFileSync } from \"fs\";\nimport * as dotenv from \"dotenv\";\ndotenv.config();\n\nconst cosmos = new AzureCosmosDB({\n  connectionString: process.env.AZURE_COSMOSDB_CONNECTION_STRING!,\n  databaseId: process.env.AZURE_COSMOSDB_DATABASE_ID!,\n  containerId: process.env.AZURE_COSMOSDB_CONTAINER_ID!,\n  embeddings: new OpenAIEmbeddings(),\n});\n\nconst text = readFileSync(\"./state_of_the_union.txt\", \"utf8\");\nconst splitter = new RecursiveCharacterTextSplitter({ chunkSize: 500, chunkOverlap: 0 });\nconst docs = await splitter.createDocuments([text]);\nawait cosmos.addDocuments(docs);\n\nconst query = \"What did the president say about Ketanji Brown Jackson?\";\nconst results = await cosmos.similaritySearch(query, 2);\n\nconst chain = loadQAStuffChain();\nconst answer = await chain.run({\n  input_documents: results,\n  question: query,\n});\n\nconsole.log({ answer });\n```\n\n----------------------------------------\n\nTITLE: Extracting Structured Output with Zod Schema in TypeScript\nDESCRIPTION: Demonstrates how to use the .withStructuredOutput() method with a Zod schema to extract a joke about cats from a language model. The schema defines the structure of the joke with setup, punchline, and optional rating fields.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/structured_output.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { z } from \"zod\";\n\nconst joke = z.object({\n  setup: z.string().describe(\"The setup of the joke\"),\n  punchline: z.string().describe(\"The punchline to the joke\"),\n  rating: z.number().optional().describe(\"How funny the joke is, from 1 to 10\"),\n});\n\nconst structuredLlm = model.withStructuredOutput(joke);\n\nawait structuredLlm.invoke(\"Tell me a joke about cats\")\n```\n\n----------------------------------------\n\nTITLE: Comprehensive Setup of Conversational RAG Agent in Python\nDESCRIPTION: This extensive snippet ties together all the necessary components to create a full conversational RAG agent. It includes setting up the language model, document loading, text splitting, vector store creation, and agent initialization.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport { createRetrieverTool } from \"langchain/tools/retriever\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\nimport { MemorySaver } from \"@langchain/langgraph\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\"\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst llm3 = new ChatOpenAI({ model: \"gpt-4o\" });\n\nconst loader3 = new CheerioWebBaseLoader(\n  \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n);\n\nconst docs3 = await loader3.load();\n\nconst textSplitter3 = new RecursiveCharacterTextSplitter({ chunkSize: 1000, chunkOverlap: 200 });\nconst splits3 = await textSplitter3.splitDocuments(docs3);\nconst vectorStore3 = await MemoryVectorStore.fromDocuments(splits3, new OpenAIEmbeddings());\n\n// Retrieve and generate using the relevant snippets of the blog.\nconst retriever3 = vectorStore3.asRetriever();\n\nconst tool2 = createRetrieverTool(\n    retriever3,\n    {\n      name: \"blog_post_retriever\",\n      description: \"Searches and returns excerpts from the Autonomous Agents blog post.\",\n    }\n)\nconst tools2 = [tool2]\nconst memory4 = new MemorySaver();\n\nconst agentExecutor3 = createReactAgent({ llm: llm3, tools: tools2, checkpointSaver: memory4 })\n```\n\n----------------------------------------\n\nTITLE: Creating a Conversational Retrieval Chain\nDESCRIPTION: Combines the query-transforming retriever chain with the document chain to create a complete conversational retrieval system that can maintain context across multiple turns.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_retrieval.ipynb#2025-04-22_snippet_15\n\nLANGUAGE: javascript\nCODE:\n```\nconst conversationalRetrievalChain = RunnablePassthrough.assign({\n  context: queryTransformingRetrieverChain,\n}).assign({\n  answer: documentChain,\n});\n```\n\n----------------------------------------\n\nTITLE: Using JsonOutputParser with Prompt Template in TypeScript\nDESCRIPTION: Shows how to use the JsonOutputParser with a ChatPromptTemplate to extract structured data from a language model. This approach includes formatting instructions in the prompt and defines a specific schema for people's information.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/structured_output.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JsonOutputParser } from \"@langchain/core/output_parsers\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\ntype Person = {\n    name: string;\n    height_in_meters: number;\n};\n\ntype People = {\n    people: Person[];\n};\n\nconst formatInstructions = `Respond only in valid JSON. The JSON object you return should match the following schema:\n{{ people: [{{ name: \"string\", height_in_meters: \"number\" }}] }}\n\nWhere people is an array of objects, each with a name and height_in_meters field.\n`\n\n// Set up a parser\nconst parser = new JsonOutputParser<People>();\n\n// Prompt\nconst prompt = await ChatPromptTemplate.fromMessages(\n    [\n        [\n            \"system\",\n            \"Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\",\n        ],\n        [\n            \"human\",\n            \"{query}\",\n        ]\n    ]\n).partial({\n    format_instructions: formatInstructions,\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing Advanced Custom LLM with Richer Outputs\nDESCRIPTION: Creates a more advanced LLM by extending BaseLLM and implementing _generate method that provides richer outputs including token usage metadata and multiple generations per input.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/custom_llm.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { LLMResult } from \"@langchain/core/outputs\";\nimport {\n  BaseLLM,\n  BaseLLMCallOptions,\n  BaseLLMParams,\n} from \"@langchain/core/language_models/llms\";\n\ninterface AdvancedCustomLLMCallOptions extends BaseLLMCallOptions {}\n\ninterface AdvancedCustomLLMParams extends BaseLLMParams {\n  n: number;\n}\n\nclass AdvancedCustomLLM extends BaseLLM<AdvancedCustomLLMCallOptions> {\n  n: number;\n\n  constructor(fields: AdvancedCustomLLMParams) {\n    super(fields);\n    this.n = fields.n;\n  }\n\n  _llmType() {\n    return \"advanced_custom_llm\";\n  }\n\n  async _generate(\n    inputs: string[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<LLMResult> {\n    const outputs = inputs.map((input) => input.slice(0, this.n));\n    // Pass `runManager?.getChild()` when invoking internal runnables to enable tracing\n    // await subRunnable.invoke(params, runManager?.getChild());\n\n    // One input could generate multiple outputs.\n    const generations = outputs.map((output) => [\n      {\n        text: output,\n        // Optional additional metadata for the generation\n        generationInfo: { outputCount: 1 },\n      },\n    ]);\n    const tokenUsage = {\n      usedTokens: this.n,\n    };\n    return {\n      generations,\n      llmOutput: { tokenUsage },\n    };\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Integrating Memory with LangGraph Agent in Python\nDESCRIPTION: This snippet demonstrates how to add memory to the LangGraph agent using MemorySaver. It allows for maintaining conversation context across multiple interactions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport { MemorySaver } from \"@langchain/langgraph\";\n\nconst memory3 = new MemorySaver();\n\nconst agentExecutor2 = createReactAgent({ llm, tools, checkpointSaver: memory3 })\n```\n\n----------------------------------------\n\nTITLE: Zod Schema Definition for Response Formatting\nDESCRIPTION: Shows how to define a structured output schema using Zod, including object properties with descriptions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/structured_outputs.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { z } from \"zod\";\n\nconst ResponseFormatter = z.object({\n  answer: z.string().describe(\"The answer to the user's question\"),\n  followup_question: z\n    .string()\n    .describe(\"A followup question the user could ask\"),\n});\n```\n\n----------------------------------------\n\nTITLE: Hybrid Vector and Text Similarity Search Strategies in SingleStoreVectorStore - TypeScript\nDESCRIPTION: Presents advanced usage with SingleStoreVectorStore, leveraging hybrid search strategies such as VECTOR_ONLY, TEXT_ONLY, FILTER_BY_TEXT, FILTER_BY_VECTOR, and WEIGHTED_SUM in LangChain.js TypeScript code. This snippet illustrates switching between different similarity search paradigms, optionally requiring a full-text index and setting vector size as needed. It enables customization of retrieval behavior based on application needs (vector-centric, text-centric, or hybrid), and is suitable for users needing combined or weighted approaches for maximum retrieval accuracy and efficiency.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/singlestore.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\n{HybridSearchUsageExample}\n```\n\n----------------------------------------\n\nTITLE: Binding Stop Sequences to a LangChain Model\nDESCRIPTION: This snippet demonstrates how to use the .bind() method to attach a stop sequence to the model in a chain. It modifies the previous chain by binding the 'SOLUTION' stop word to the model, which will cause the model to stop generating text when it encounters that word.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/binding.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst runnableWithStop = prompt\n  .pipe(model.bind({ stop: [\"SOLUTION\"] }))\n  .pipe(new StringOutputParser());\n\nconst shorterResponse = await runnableWithStop.invoke({\n  equation_statement: \"x raised to the third plus seven equals 12\"\n});\n\nconsole.log(shorterResponse);\n```\n\n----------------------------------------\n\nTITLE: Creating a Chain with RunnableSequence in JavaScript\nDESCRIPTION: This snippet shows an alternative way to create a complex chain using RunnableSequence.from(). It achieves the same logical flow as the previous example but with a different syntax.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/sequence.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RunnableSequence } from \"@langchain/core/runnables\";\n\nconst composedChainWithLambda = RunnableSequence.from([\n    chain,\n    (input) => ({ joke: input }),\n    analysisPrompt,\n    model,\n    new StringOutputParser()\n])\n\nawait composedChainWithLambda.invoke({ topic: \"beets\" })\n```\n\n----------------------------------------\n\nTITLE: Implementing Reciprocal Rank Fusion Algorithm\nDESCRIPTION: Defines a function that implements the Reciprocal Rank Fusion algorithm, which combines results from multiple search queries into a single ranked list. The algorithm calculates scores based on the rank position of each document across all queries.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/rag_fusion.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst reciprocalRankFusion = (results: Document[][], k = 60) => {\n  const fusedScores: Record<string, number> = {};\n  for (const result of results) {\n    // Assumes the docs are returned in sorted order of relevance\n    result.forEach((item, index) => {\n      const docString = item.pageContent;\n      if (!(docString in fusedScores)) {\n        fusedScores[docString] = 0;\n      }\n      fusedScores[docString] += 1 / (index + k);\n    });\n  }\n\n  const rerankedResults = Object.entries(fusedScores)\n    .sort((a, b) => b[1] - a[1])\n    .map(\n      ([doc, score]) => new Document({ pageContent: doc, metadata: { score } })\n    );\n  return rerankedResults;\n};\n```\n\n----------------------------------------\n\nTITLE: Implementing Optimized QA Chain with Document Summaries\nDESCRIPTION: Final implementation of a QA chain that uses document post-processing to optimize retrieval. This approach uses document summaries in metadata rather than the entire content.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_citations.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nconst chain4 = retrieveMap\n  .assign({ context: formatDocs })\n  .assign({ answer: answerChain })\n  .pick([\"answer\", \"docs\"]);\n  \n// Note the documents have an article \"summary\" in the metadata that is now much longer than the\n// actual document page content. This summary isn't actually passed to the model.\nconst res = await chain4.invoke(\"How fast are cheetahs?\");\n\nconsole.log(JSON.stringify(res, null, 2))\n```\n\n----------------------------------------\n\nTITLE: Creating and Using a React Agent with VectorStoreToolkit in TypeScript\nDESCRIPTION: This snippet demonstrates how to create a React agent using the tools from the VectorStoreToolkit and run a query through it. It includes streaming the agent's thought process and actions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/vectorstore.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\"\n\nconst agentExecutor = createReactAgent({ llm, tools });\n\nconst exampleQuery = \"What did biden say about Ketanji Brown Jackson is the state of the union address?\"\n\nconst events = await agentExecutor.stream(\n  { messages: [[\"user\", exampleQuery]]},\n  { streamMode: \"values\", }\n)\n\nfor await (const event of events) {\n  const lastMsg = event.messages[event.messages.length - 1];\n  if (lastMsg.tool_calls?.length) {\n    console.dir(lastMsg.tool_calls, { depth: null });\n  } else if (lastMsg.content) {\n    console.log(lastMsg.content);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Tool Calls and Creating Tool Messages in JavaScript\nDESCRIPTION: This snippet shows how to execute the tool calls generated by the model, creating ToolMessages that can be fed back to the model. It uses a dictionary to map tool names to their respective functions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_results_pass_to_model.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst toolsByName = {\n  add: addTool,\n  multiply: multiplyTool,\n}\n\nfor (const toolCall of aiMessage.tool_calls) {\n  const selectedTool = toolsByName[toolCall.name];\n  const toolMessage = await selectedTool.invoke(toolCall);\n  messages.push(toolMessage);\n}\n\nconsole.log(messages);\n```\n\n----------------------------------------\n\nTITLE: Source-Enabled Q&A Chain Implementation\nDESCRIPTION: Enhanced implementation that returns both answers and source documents using RunnableMap and RunnableSequence.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_sources.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport {\n  RunnableMap,\n  RunnablePassthrough,\n  RunnableSequence\n} from \"@langchain/core/runnables\";\nimport { formatDocumentsAsString } from \"langchain/util/document\";\n\nconst ragChainWithSources = RunnableMap.from({\n  // Return raw documents here for now since we want to return them at\n  // the end - we'll format in the next step of the chain\n  context: retriever,\n  question: new RunnablePassthrough(),\n}).assign({\n  answer: RunnableSequence.from([\n    (input) => {\n      return {\n        // Now we format the documents as strings for the prompt\n        context: formatDocumentsAsString(input.context),\n        question: input.question\n      };\n    },\n    prompt,\n    llm,\n    new StringOutputParser()\n  ]),\n})\n\nawait ragChainWithSources.invoke(\"What is Task Decomposition\")\n```\n\n----------------------------------------\n\nTITLE: Streaming Agent Responses in JavaScript\nDESCRIPTION: This code shows how to use the created ReAct agent to handle a complex query that requires multiple retrieval steps. It demonstrates streaming the agent's responses and handling multi-turn interactions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/qa_chat_history.ipynb#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nlet inputMessage = `What is the standard method for Task Decomposition?\nOnce you get the answer, look up common extensions of that method.`\n\nlet inputs5 = { messages: [{ role: \"user\", content: inputMessage }] };\n\nfor await (\n  const step of await agent.stream(inputs5, {\n    streamMode: \"values\",\n  })\n) {\n    const lastMessage = step.messages[step.messages.length - 1];\n    prettyPrint(lastMessage);\n    console.log(\"-----\\n\");\n}\n```\n\n----------------------------------------\n\nTITLE: Complete RAG Implementation with History\nDESCRIPTION: Comprehensive implementation combining document loading, text splitting, embeddings, retrieval chains, and stateful chat management.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\"\nimport { OpenAIEmbeddings, ChatOpenAI } from \"@langchain/openai\";\nimport { ChatPromptTemplate, MessagesPlaceholder } from \"@langchain/core/prompts\";\nimport { createHistoryAwareRetriever } from \"langchain/chains/history_aware_retriever\";\nimport { createStuffDocumentsChain } from \"langchain/chains/combine_documents\";\nimport { createRetrievalChain } from \"langchain/chains/retrieval\";\nimport { AIMessage, BaseMessage, HumanMessage } from \"@langchain/core/messages\";\nimport { StateGraph, START, END, MemorySaver, messagesStateReducer, Annotation } from \"@langchain/langgraph\";\nimport { v4 as uuidv4 } from \"uuid\";\n\n// ... [rest of the implementation]\n```\n\n----------------------------------------\n\nTITLE: Implementing Citation-Based Retrieval with Anthropic Claude\nDESCRIPTION: Complete implementation of a citation-based QA system using Anthropic's Claude model. This code formats retrieved documents as XML, processes user questions, and returns answers with citations.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_citations.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { XMLOutputParser } from \"@langchain/core/output_parsers\";\nimport { Document } from \"@langchain/core/documents\";\nimport { RunnableLambda, RunnablePassthrough, RunnableMap } from \"@langchain/core/runnables\";\n\nconst anthropic = new ChatAnthropic({\n  model: \"claude-instant-1.2\",\n  temperature: 0,\n});\nconst system = `You're a helpful AI assistant. Given a user question and some web article snippets,\nanswer the user question and provide citations. If none of the articles answer the question, just say you don't know.\n\nRemember, you must return both an answer and citations. A citation consists of a VERBATIM quote that\njustifies the answer and the ID of the quote article. Return a citation for every quote across all articles\nthat justify the answer. Use the following format for your final output:\n\n<cited_answer>\n    <answer></answer>\n    <citations>\n        <citation><source_id></source_id><quote></quote></citation>\n        <citation><source_id></source_id><quote></quote></citation>\n        ...\n    </citations>\n</cited_answer>\n\nHere are the web articles:{context}`;\n\nconst anthropicPrompt = ChatPromptTemplate.fromMessages([\n  [\"system\", system],\n  [\"human\", \"{question}\"]\n]);\n\nconst formatDocsToXML = (docs: Array<Document>): string => {\n  const formatted: Array<string> = [];\n  docs.forEach((doc, idx) => {\n    const docStr = `<source id=\"${idx}\">\n  <title>${doc.metadata.title}</title>\n  <article_snippet>${doc.pageContent}</article_snippet>\n</source>`\n    formatted.push(docStr);\n  });\n  return `\\n\\n<sources>${formatted.join(\"\\n\")}</sources>`;\n}\n\nconst format3 = new RunnableLambda({\n  func: (input: { docs: Array<Document> }) => formatDocsToXML(input.docs)\n})\nconst answerChain = anthropicPrompt\n  .pipe(anthropic)\n  .pipe(new XMLOutputParser())\n  .pipe(\n    new RunnableLambda({ func: (input: { cited_answer: any }) => input.cited_answer })\n  );\nconst map3 = RunnableMap.from({\n  question: new RunnablePassthrough(),\n  docs: retriever,\n});\nconst chain3 = map3.assign({ context: format3 }).assign({ cited_answer: answerChain }).pick([\"cited_answer\", \"docs\"])\n\nconst res = await chain3.invoke(\"How fast are cheetahs?\");\n\nconsole.log(JSON.stringify(res, null, 2));\n```\n\n----------------------------------------\n\nTITLE: Indexing Documents and Hybrid Vector Search with Azure AI Search in TypeScript\nDESCRIPTION: This TypeScript code demonstrates integrating with Azure AI Search using LangChain libraries. It performs key actions: loading and indexing documents from a file, configuring hybrid/vector search, executing a query, and leveraging a chain to answer a natural language question using retrieved results. Prerequisites include an active Azure AI Search instance, required admin keys, environment variables set, and the referenced dependencies installed. Inputs involve the document file and a user query, while outputs include the search results and chain-based answer. The snippet also illustrates how to enable semantic ranking for enhanced retrieval in RAG scenarios.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/azure_aisearch.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AzureAISearchVectorStore, AzureAISearchQueryType } from \"@langchain/community/vectorstores/azure_aisearch\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport { RunnableSequence } from \"@langchain/core/runnables\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\n(async () => {\n  // Load documents from a local file and split into chunks\n  const loader = new TextLoader(\"./state_of_the_union.txt\");\n  const docs = await loader.load();\n  const splitter = new RecursiveCharacterTextSplitter({\n    chunkSize: 1000,\n    chunkOverlap: 200,\n  });\n  const splitDocs = await splitter.splitDocuments(docs);\n\n  // Initialize Azure AI Search vector store with hybrid search\n  const store = await AzureAISearchVectorStore.fromDocuments(\n    splitDocs,\n    new OpenAIEmbeddings(),\n    {\n      indexName: \"langchain-vector-demo\",\n      // 'search.type' controls the query mode: Hybrid, SemanticHybrid, etc.\n      search: {\n        type: AzureAISearchQueryType.Hybrid, // You can set SemanticHybrid for semantic ranking\n      },\n      // Optionally, pass other Azure configuration (endpoint, adminKey) via ENV\n    }\n  );\n\n  // Perform a hybrid search\n  const results = await store.similaritySearch(\"What did the president say about Ketanji Brown Jackson?\", 4);\n  console.log(results.map((r) => r.pageContent));\n\n  // Use a retrieval-augmented QA chain\n  const chain = RunnableSequence.from([\n    store.asRetriever(),\n    new ChatOpenAI({ modelName: \"gpt-3.5-turbo-0125\", temperature: 0 }),\n  ]);\n  const answer = await chain.invoke(\"What did the president say about Ketanji Brown Jackson?\");\n  console.log(answer);\n})();\n```\n\n----------------------------------------\n\nTITLE: Invoking Tool-Enabled Chat Model\nDESCRIPTION: Shows how to invoke a chat model with bound tools to perform calculations.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_calling.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nconst res = await llmWithTools.invoke(\"What is 3 * 12\");\n\nconsole.log(res);\n```\n\n----------------------------------------\n\nTITLE: Forcing Any Tool Usage in LangChain.js\nDESCRIPTION: This snippet shows how to force the LLM to use any available tool by setting the tool_choice parameter to \"any\". It then invokes the model with a prompt and logs the result.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_choice.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst llmForcedToUseTool = llm.bindTools(tools, {\n  tool_choice: \"any\",\n})\nconst anyToolResult = await llmForcedToUseTool.invoke(\"What day is today?\");\nconsole.log(JSON.stringify(anyToolResult.tool_calls, null, 2));\n```\n\n----------------------------------------\n\nTITLE: Setting Up a Vector Store Retriever as a Tool in TypeScript\nDESCRIPTION: This code initializes a ChatOpenAI model, creates a vector store from documents about pets, and sets up a retriever that can be used as a tool. It demonstrates preparing the components needed for a retrieval-based agent.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/convert_runnable_to_tool.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst llm = new ChatOpenAI({ model: \"gpt-3.5-turbo-0125\", temperature: 0 })\n\nimport { Document } from \"@langchain/core/documents\"\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst documents = [\n  new Document({\n    pageContent: \"Dogs are great companions, known for their loyalty and friendliness.\",\n  }),\n  new Document({\n    pageContent: \"Cats are independent pets that often enjoy their own space.\",\n  }),\n]\n\nconst vectorstore = await MemoryVectorStore.fromDocuments(\n  documents, new OpenAIEmbeddings(),\n);\n\nconst retriever = vectorstore.asRetriever({\n  k: 1,\n  searchType: \"similarity\",\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Images with Dall-E Tool via LangChain OpenAI - TypeScript\nDESCRIPTION: This code demonstrates how to use the Dall-E Tool from LangChain's OpenAI integration to generate images programmatically. It requires setting the OPENAI_API_KEY environment variable with a valid key prior to execution and installing both @langchain/openai and @langchain/core. Inputs typically include image prompts and optionally parameters like image size; outputs are URLs or image objects as returned by Dall-E. Review the example for details on integrating with a larger agent workflow.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/dalle.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport ToolExample from \"@examples/tools/dalle_image_generation.ts\";\n```\n\n----------------------------------------\n\nTITLE: Creating a RunnableSequence in TypeScript\nDESCRIPTION: Demonstrates how to create a RunnableSequence, which chains multiple runnables sequentially. The output of one runnable serves as the input to the next.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/lcel.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RunnableSequence } from \"@langchain/core/runnables\";\nconst chain = new RunnableSequence({\n  first: runnable1,\n  // Optional, use if you have more than two runnables\n  // middle: [...],\n  last: runnable2,\n});\n```\n\n----------------------------------------\n\nTITLE: Combining Dynamic Few Shot Prompt with a Chat Model in LangChain.js\nDESCRIPTION: This final snippet shows how to connect a chat model to the dynamic few shot prompt and invoke the resulting chain with an input. It demonstrates the complete workflow for dynamic few shot learning with chat models.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot_examples_chat.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nconst chain = finalPrompt.pipe(model);\n\nawait chain.invoke({ input: \"What's 3+3?\" })\n```\n\n----------------------------------------\n\nTITLE: Passing Multiple Images to OpenAI Model\nDESCRIPTION: This code example shows how to pass multiple images to the OpenAI model for comparison. It creates a HumanMessage with text and two image URLs, then invokes the model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/multimodal_inputs.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst message = new HumanMessage({\n  content: [\n    {\n      type: \"text\",\n      text: \"are these two images the same?\"\n    },\n    {\n      type: \"image_url\",\n      image_url: {\n        url: imageUrl\n      }\n    },\n    {\n      type: \"image_url\",\n      image_url: {\n        url: imageUrl\n      }\n    },\n  ],\n});\nconst response = await openAIModel.invoke([message]);\nconsole.log(response.content);\n```\n\n----------------------------------------\n\nTITLE: Building a RAG Chain and Converting it to a Tool in TypeScript\nDESCRIPTION: This code creates a full RAG (Retrieval Augmented Generation) chain that retrieves context about pets and answers questions in a specified style. It then converts this chain into a tool that can be used by an agent.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/convert_runnable_to_tool.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { RunnableSequence } from \"@langchain/core/runnables\";\n\nconst SYSTEM_TEMPLATE = `\nYou are an assistant for question-answering tasks.\nUse the below context to answer the question. If\nyou don't know the answer, say you don't know.\nUse three sentences maximum and keep the answer\nconcise.\n\nAnswer in the style of {answer_style}.\n\nContext: {context}`;\n\nconst prompt = ChatPromptTemplate.fromMessages([\n  [\"system\", SYSTEM_TEMPLATE],\n  [\"human\", \"{question}\"],\n]);\n\nconst ragChain = RunnableSequence.from([\n  {\n    context: (input, config) => retriever.invoke(input.question, config),\n    question: (input) => input.question,\n    answer_style: (input) => input.answer_style,\n  },\n  prompt,\n  llm,\n  new StringOutputParser(),\n]);\n```\n\n----------------------------------------\n\nTITLE: Setting up Document Processing Pipeline\nDESCRIPTION: Initialize document loader, text splitter, and vector store with embeddings for RAG implementation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\"\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst loader = new CheerioWebBaseLoader(\n  \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n);\n\nconst docs = await loader.load();\n\nconst textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000, chunkOverlap: 200 });\nconst splits = await textSplitter.splitDocuments(docs);\nconst vectorStore = await MemoryVectorStore.fromDocuments(splits, new OpenAIEmbeddings());\n\nconst retriever = vectorStore.asRetriever();\n```\n\n----------------------------------------\n\nTITLE: Creating a ChatPromptTemplate for Single Image Description\nDESCRIPTION: This snippet demonstrates how to create a ChatPromptTemplate for describing a single image. It includes a system message and a user message with an image URL placeholder.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/multimodal_prompts.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst prompt = ChatPromptTemplate.fromMessages(\n    [\n        [\"system\", \"Describe the image provided\"],\n        [\n            \"user\",\n            [{ type: \"image_url\", image_url: \"data:image/jpeg;base64,{base64}\" }],\n        ]\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Chaining with Message Trimming\nDESCRIPTION: Example of using trimMessages in a declarative chain with ChatOpenAI, showing how to compose it with other components.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/trim_messages.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { trimMessages } from \"@langchain/core/messages\";\n\nconst llm = new ChatOpenAI({ model: \"gpt-4o\" })\n\nconst trimmer = trimMessages({\n    maxTokens: 45,\n    strategy: \"last\",\n    tokenCounter: llm,\n    includeSystem: true,\n})\n\nconst chain = trimmer.pipe(llm);\nawait chain.invoke(messages)\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search in Upstash Vector Store - TypeScript/JavaScript\nDESCRIPTION: This example fetches documents similar to a query string, filtered by metadata and limited to a specified number of results. The method similaritySearch takes the query text, a result count (k), and an optional filter. Results are iterated and printed with both content and metadata. Requires all prior setup, with documents pre-indexed and available for query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/upstash.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst filter = \"source = 'https://example.com'\";\n\nconst similaritySearchResults = await vectorStore.similaritySearch(\"biology\", 2, filter);\n\nfor (const doc of similaritySearchResults) {\n  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);\n}\n```\n\n----------------------------------------\n\nTITLE: Custom JSON Parsing with LCEL in TypeScript\nDESCRIPTION: Demonstrates how to create a custom prompt and parser using LangChain Expression Language (LCEL) with a function to extract JSON content from model output. This approach manually extracts JSON embedded between markdown code block tags.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/structured_output.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AIMessage } from \"@langchain/core/messages\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\ntype Person = {\n    name: string;\n    height_in_meters: number;\n};\n\ntype People = {\n    people: Person[];\n};\n\nconst schema = `{{ people: [{{ name: \"string\", height_in_meters: \"number\" }}] }}`\n\n// Prompt\nconst prompt = await ChatPromptTemplate.fromMessages(\n    [\n        [\n            \"system\",\n            `Answer the user query. Output your answer as JSON that\nmatches the given schema: \\`\\`\\`json\\n{schema}\\n\\`\\`\\`.\nMake sure to wrap the answer in \\`\\`\\`json and \\`\\`\\` tags`\n        ],\n        [\n            \"human\",\n            \"{query}\",\n        ]\n    ]\n).partial({\n    schema\n});\n\n/**\n * Custom extractor\n * \n * Extracts JSON content from a string where\n * JSON is embedded between ```json and ``` tags.\n */\nconst extractJson = (output: AIMessage): Array<People> => {\n    const text = output.content as string;\n    // Define the regular expression pattern to match JSON blocks\n    const pattern = /```json(.*?)```/gs;\n\n    // Find all non-overlapping matches of the pattern in the string\n    const matches = text.match(pattern);\n\n    // Process each match, attempting to parse it as JSON\n    try {\n        return matches?.map(match => {\n            // Remove the markdown code block syntax to isolate the JSON string\n            const jsonStr = match.replace(/```json|```/g, '').trim();\n            return JSON.parse(jsonStr);\n        }) ?? [];\n    } catch (error) {\n        throw new Error(`Failed to parse: ${output}`);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Using TypeORMVectorStore in a TypeScript example\nDESCRIPTION: A complete example of using TypeORMVectorStore in a LangChain.js project. The example demonstrates how to integrate vector search capabilities in a TypeScript project using TypeORM and the pgvector extension.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/typeorm.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n{Example}\n```\n\n----------------------------------------\n\nTITLE: Testing the Retrieval Chain\nDESCRIPTION: Tests the retrieval chain with a human message to see how it automatically retrieves relevant context and uses it to answer the question.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_retrieval.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nawait retrievalChain.invoke({\n  messages: [new HumanMessage(\"Can LangSmith help test my LLM applications?\")],\n});\n```\n\n----------------------------------------\n\nTITLE: Using a Configurable Chat Model Declaratively with LCEL\nDESCRIPTION: Demonstrates how to use a configurable chat model with declarative operations like bindTools and withStructuredOutput, showing how it can be chained like a regular chat model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chat_models_universal_init.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { z } from \"zod\";\nimport { initChatModel } from \"langchain/chat_models_universal\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\nconst configurableChat = await initChatModel({\n  modelProvider: \"anthropic\",\n  model: \"claude-3-sonnet-20240229\",\n  temperature: 0,\n  configurableFields: [\"temperature\", \"model\", \"modelProvider\"],\n});\n\n// We can use chat with structured output\nconst chatWithStructuredOutput = configurableChat.withStructuredOutput({\n  schema: z.object({\n    answer: z.string().describe(\"answer to the user's question\"),\n    sources: z\n      .array(z.string())\n      .describe(\"sources used to answer the question\"),\n  }),\n});\n\n// Use the model with a prompt template\nconst prompt = ChatPromptTemplate.fromTemplate(\n  \"Answer the following question: {question}\"\n);\n\nconst chain = prompt.pipe(chatWithStructuredOutput);\n\nconst result = await chain.invoke({\n  question: \"What is the capital of France?\",\n});\n\nconsole.log(result);\n// { answer: 'The capital of France is Paris.', sources: [] }\n```\n\n----------------------------------------\n\nTITLE: Creating a Chat Prompt Template\nDESCRIPTION: Code to create a chat prompt template for the agent.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/agent_executor.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\nconst prompt = ChatPromptTemplate.fromMessages([\n  [\"system\", \"You are a helpful assistant\"],\n  [\"placeholder\", \"{chat_history}\"],\n  [\"human\", \"{input}\"],\n  [\"placeholder\", \"{agent_scratchpad}\"],\n]);\n\nconsole.log(prompt.promptMessages);\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain.js using package managers\nDESCRIPTION: Commands for installing LangChain.js using npm, yarn, or pnpm package managers.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -S langchain\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn add langchain\n```\n\nLANGUAGE: bash\nCODE:\n```\npnpm add langchain\n```\n\n----------------------------------------\n\nTITLE: Creating and Testing a Retriever\nDESCRIPTION: Creates a retriever from the vector store that can return up to 4 relevant documents, then tests it with a query about LangSmith's testing capabilities.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_retrieval.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst retriever = vectorstore.asRetriever(4);\n\nconst docs = await retriever.invoke(\"how can langsmith help with testing?\");\n\nconsole.log(docs);\n```\n\n----------------------------------------\n\nTITLE: Querying LangChain Agent for Weather Information in Python\nDESCRIPTION: This example shows how to use the LangChain agent to query for weather information, demonstrating the agent's ability to use the search tool for external data retrieval.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/agent_executor.ipynb#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nawait agentExecutor.invoke({ input: \"whats the weather in sf?\" })\n```\n\n----------------------------------------\n\nTITLE: JSON Mode Implementation with OpenAI\nDESCRIPTION: Demonstrates how to use JSON mode with OpenAI to enforce JSON-formatted responses.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/structured_outputs.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst model = new ChatOpenAI({\n  model: \"gpt-4\",\n}).bind({\n  response_format: { type: \"json_object\" },\n});\n\nconst aiMsg = await model.invoke(\n  \"Return a JSON object with key 'random_nums' and a value of 10 random numbers in [0-99]\"\n);\nconsole.log(aiMsg.content);\n```\n\n----------------------------------------\n\nTITLE: Creating and Binding Calculator Tool with Zod Schema\nDESCRIPTION: Demonstrates how to create a calculator tool using Zod schema and bind it to a chat model. The tool supports basic mathematical operations and includes structured validation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_calling.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n\nconst calculatorSchema = z.object({\n  operation: z\n    .enum([\"add\", \"subtract\", \"multiply\", \"divide\"])\n    .describe(\"The type of operation to execute.\"),\n  number1: z.number().describe(\"The first number to operate on.\"),\n  number2: z.number().describe(\"The second number to operate on.\"),\n});\n\nconst calculatorTool = tool(async ({ operation, number1, number2 }) => {\n  if (operation === \"add\") {\n    return `${number1 + number2}`;\n  } else if (operation === \"subtract\") {\n    return `${number1 - number2}`;\n  } else if (operation === \"multiply\") {\n    return `${number1 * number2}`;\n  } else if (operation === \"divide\") {\n    return `${number1 / number2}`;\n  } else {\n    throw new Error(\"Invalid operation.\");\n  }\n}, {\n  name: \"calculator\",\n  description: \"Can perform mathematical operations.\",\n  schema: calculatorSchema,\n});\n\nconst llmWithTools = llm.bindTools([calculatorTool]);\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG Workflow with LangChain.js and OpenAI\nDESCRIPTION: This code snippet demonstrates a basic Retrieval Augmented Generation (RAG) workflow using LangChain.js and OpenAI. It retrieves relevant documents based on a question, formats a system prompt with the retrieved context, and generates a response using a chat model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/rag.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\n// Define a system prompt that tells the model how to use the retrieved context\nconst systemPrompt = `You are an assistant for question-answering tasks.\nUse the following pieces of retrieved context to answer the question.\nIf you don't know the answer, just say that you don't know.\nUse three sentences maximum and keep the answer concise.\nContext: {context}:`;\n\n// Define a question\nconst question =\n  \"What are the main components of an LLM-powered autonomous agent system?\";\n\n// Retrieve relevant documents\nconst docs = await retriever.invoke(question);\n\n// Combine the documents into a single string\nconst docsText = docs.map((d) => d.pageContent).join(\"\");\n\n// Populate the system prompt with the retrieved context\nconst systemPromptFmt = systemPrompt.replace(\"{context}\", docsText);\n\n// Create a model\nconst model = new ChatOpenAI({\n  model: \"gpt-4o\",\n  temperature: 0,\n});\n\n// Generate a response\nconst questions = await model.invoke([\n  {\n    role: \"system\",\n    content: systemPromptFmt,\n  },\n  {\n    role: \"user\",\n    content: question,\n  },\n]);\n```\n\n----------------------------------------\n\nTITLE: Initializing Complex Tool and Chain Setup\nDESCRIPTION: Sets up a complex tool with Zod schema validation and creates a chain using ChatOpenAI model. The tool is intentionally complex to demonstrate error handling scenarios.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tools_error.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { z } from \"zod\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { tool } from \"@langchain/core/tools\";\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-3.5-turbo-0125\",\n  temperature: 0,\n});\n\nconst complexTool = tool(async (params) => {\n  return params.int_arg * params.float_arg;\n}, {\n  name: \"complex_tool\",\n  description: \"Do something complex with a complex tool.\",\n  schema: z.object({\n    int_arg: z.number(),\n    float_arg: z.number(),\n    number_arg: z.object({}),\n  })\n});\n\nconst llmWithTools = llm.bindTools([complexTool]);\n\nconst chain = llmWithTools\n  .pipe((message) => message.tool_calls?.[0].args)\n  .pipe(complexTool);\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG-based Approach with Vector Store\nDESCRIPTION: Sets up a Retrieval-Augmented Generation approach using a MemoryVectorStore and OpenAIEmbeddings. This approach focuses extraction only on the most relevant chunks rather than processing every chunk of the document.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/extraction_long_text.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\n// Only load the first 10 docs for speed in this demo use-case\nconst vectorstore = await MemoryVectorStore.fromDocuments(\n  splitDocs.slice(0, 10),\n  new OpenAIEmbeddings()\n);\n\n// Only extract from top document\nconst retriever = vectorstore.asRetriever({ k: 1 });\n```\n\n----------------------------------------\n\nTITLE: Initializing a Language Model\nDESCRIPTION: Code to initialize a ChatOpenAI model with specific parameters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/agent_executor.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst model = new ChatOpenAI({ model: \"gpt-4o-mini\", temperature: 0 })\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Message Passing for Chatbot Memory\nDESCRIPTION: Creates a chat chain that accepts conversation history as context, allowing the model to reference previous messages when generating responses. This demonstrates the basic concept of chatbot memory by passing previous conversation turns explicitly.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_memory.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HumanMessage, AIMessage } from \"@langchain/core/messages\";\nimport {\n  ChatPromptTemplate,\n  MessagesPlaceholder,\n} from \"@langchain/core/prompts\";\n\nconst prompt = ChatPromptTemplate.fromMessages([\n  [\n    \"system\",\n    \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n  ],\n  new MessagesPlaceholder(\"messages\"),\n]);\n\nconst chain = prompt.pipe(llm);\n\nawait chain.invoke({\n  messages: [\n    new HumanMessage(\n      \"Translate this sentence from English to French: I love programming.\"\n    ),\n    new AIMessage(\"J'adore la programmation.\"),\n    new HumanMessage(\"What did you just say?\"),\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Extraction Schema with Zod and Creating an Extraction Chain\nDESCRIPTION: Sets up a structured extraction schema using Zod for identifying key developments in the history of cars. Configures a ChatPromptTemplate with system instructions and connects it to a ChatOpenAI model with structured output validation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/extraction_long_text.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { z } from \"zod\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst keyDevelopmentSchema = z.object({\n  year: z.number().describe(\"The year when there was an important historic development.\"),\n  description: z.string().describe(\"What happened in this year? What was the development?\"),\n  evidence: z.string().describe(\"Repeat verbatim the sentence(s) from which the year and description information were extracted\"),\n}).describe(\"Information about a development in the history of cars.\");\n\nconst extractionDataSchema = z.object({\n  key_developments: z.array(keyDevelopmentSchema),\n}).describe(\"Extracted information about key developments in the history of cars\");\n\nconst SYSTEM_PROMPT_TEMPLATE = [\n  \"You are an expert at identifying key historic development in text.\",\n  \"Only extract important historic developments. Extract nothing if no important information can be found in the text.\"\n].join(\"\\n\");\n\n// Define a custom prompt to provide instructions and any additional context.\n// 1) You can add examples into the prompt template to improve extraction quality\n// 2) Introduce additional parameters to take context into account (e.g., include metadata\n//    about the document from which the text was extracted.)\nconst prompt = ChatPromptTemplate.fromMessages([\n  [\n    \"system\",\n    SYSTEM_PROMPT_TEMPLATE,\n  ],\n  // Keep on reading through this use case to see how to use examples to improve performance\n  // MessagesPlaceholder('examples'),\n  [\"human\", \"{text}\"],\n]);\n\n// We will be using tool calling mode, which\n// requires a tool calling capable model.\nconst llm = new ChatOpenAI({\n  model: \"gpt-4-0125-preview\",\n  temperature: 0,\n});\n\nconst extractionChain = prompt.pipe(llm.withStructuredOutput(extractionDataSchema));\n```\n\n----------------------------------------\n\nTITLE: Defining LangGraph State and Functions for Map-Reduce Summarization\nDESCRIPTION: Creates a complete LangGraph flow for document summarization with state management, mapping functions, and recursive collapsing of summaries. Includes token length checking and conditional paths based on document sizes.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/summarization.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nimport {\n  collapseDocs,\n  splitListOfDocs,\n} from \"langchain/chains/combine_documents/reduce\";\nimport { Document } from \"@langchain/core/documents\";\nimport { StateGraph, Annotation, Send } from \"@langchain/langgraph\";\n\n\nlet tokenMax = 1000\n\n\nasync function lengthFunction(documents) {\n    const tokenCounts = await Promise.all(documents.map(async (doc) => {\n        return llm.getNumTokens(doc.pageContent);\n    }));\n    return tokenCounts.reduce((sum, count) => sum + count, 0);\n}\n\nconst OverallState = Annotation.Root({\n  contents: Annotation<string[]>,\n  // Notice here we pass a reducer function.\n  // This is because we want combine all the summaries we generate\n  // from individual nodes back into one list. - this is essentially\n  // the \"reduce\" part\n  summaries: Annotation<string[]>({\n    reducer: (state, update) => state.concat(update),\n  }),\n  collapsedSummaries: Annotation<Document[]>,\n  finalSummary: Annotation<string>,\n});\n\n\n// This will be the state of the node that we will \"map\" all\n// documents to in order to generate summaries\ninterface SummaryState {\n  content: string;\n}\n\n// Here we generate a summary, given a document\nconst generateSummary = async (state: SummaryState): Promise<{ summaries: string[] }> => {\n  const prompt = await mapPrompt.invoke({context: state.content});\n  const response = await llm.invoke(prompt);\n  return { summaries: [String(response.content)] };\n};\n\n\n// Here we define the logic to map out over the documents\n// We will use this an edge in the graph\nconst mapSummaries = (state: typeof OverallState.State) => {\n  // We will return a list of `Send` objects\n  // Each `Send` object consists of the name of a node in the graph\n  // as well as the state to send to that node\n  return state.contents.map((content) => new Send(\"generateSummary\", { content }));\n};\n\n\nconst collectSummaries = async (state: typeof OverallState.State) => {\n  return {\n      collapsedSummaries: state.summaries.map(summary => new Document({pageContent: summary}))\n  };\n}\n\n\nasync function _reduce(input) {\n    const prompt = await reducePrompt.invoke({ docs: input });\n    const response = await llm.invoke(prompt);\n    return String(response.content);\n}\n\n// Add node to collapse summaries\nconst collapseSummaries = async (state: typeof OverallState.State) => {\n  const docLists = splitListOfDocs(state.collapsedSummaries, lengthFunction, tokenMax);\n  const results = [];\n  for (const docList of docLists) {\n      results.push(await collapseDocs(docList, _reduce));\n  }\n\n  return { collapsedSummaries: results };\n}\n\n\n// This represents a conditional edge in the graph that determines\n// if we should collapse the summaries or not\nasync function shouldCollapse(state: typeof OverallState.State) {\n  let numTokens = await lengthFunction(state.collapsedSummaries);\n  if (numTokens > tokenMax) {\n    return \"collapseSummaries\";\n  } else {\n    return \"generateFinalSummary\";\n  }\n}\n\n\n// Here we will generate the final summary\nconst generateFinalSummary = async (state: typeof OverallState.State) => {\n  const response = await _reduce(state.collapsedSummaries);\n  return { finalSummary: response}\n}\n\n// Construct the graph\nconst graph = new StateGraph(OverallState)\n  .addNode(\"generateSummary\", generateSummary)\n  .addNode(\"collectSummaries\", collectSummaries)\n  .addNode(\"collapseSummaries\", collapseSummaries)\n  .addNode(\"generateFinalSummary\", generateFinalSummary)\n  .addConditionalEdges(\n    \"__start__\",\n    mapSummaries,\n    [\"generateSummary\"]\n  )\n  .addEdge(\"generateSummary\", \"collectSummaries\")\n  .addConditionalEdges(\n    \"collectSummaries\",\n    shouldCollapse,\n    [\"collapseSummaries\", \"generateFinalSummary\"]\n  )\n  .addConditionalEdges(\n    \"collapseSummaries\",\n    shouldCollapse,\n    [\"collapseSummaries\", \"generateFinalSummary\"]\n  )\n  .addEdge(\"generateFinalSummary\", \"__end__\")\n\nconst app = graph.compile();\n```\n\n----------------------------------------\n\nTITLE: Passing Run Metadata to RunnableLambda in TypeScript\nDESCRIPTION: Illustrates how to pass run metadata (callbacks, tags) to a RunnableLambda and use it within nested runs.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/functions.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { type RunnableConfig } from \"@langchain/core/runnables\";\n\nconst echo = (text: string, config: RunnableConfig) => {\n  const prompt = ChatPromptTemplate.fromTemplate(\"Reverse the following text: {text}\");\n  const model = new ChatOpenAI({ model: \"gpt-4o\" });\n  const chain = prompt.pipe(model).pipe(new StringOutputParser());\n  return chain.invoke({ text }, config);\n};\n\nconst output = await RunnableLambda.from(echo).invoke(\"foo\", {\n  tags: [\"my-tag\"],\n  callbacks: [{\n    handleLLMEnd: (output) => console.log(output),\n  }],\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing OpenAI Tool Calling with Image Input\nDESCRIPTION: Demonstrates tool calling with OpenAI's GPT-4 model using image URL input. Shows how to create a message with both text and image content blocks.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_calls_multimodal.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HumanMessage } from \"@langchain/core/messages\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst model = new ChatOpenAI({\n  model: \"gpt-4o\",\n}).bindTools([weatherTool]);\n\nconst message = new HumanMessage({\n  content: [\n    {\n      type: \"text\",\n      text: \"describe the weather in this image\"\n    },\n    {\n      type: \"image_url\",\n      image_url: {\n        url: imageUrl\n      }\n    }\n  ],\n});\n\nconst response = await model.invoke([message]);\n\nconsole.log(response.tool_calls);\n```\n\n----------------------------------------\n\nTITLE: Custom Prompt with Ollama Functions\nDESCRIPTION: Demonstrates how to customize system prompts for different model capabilities\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/ollama_functions.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport OllamaFunctionsCustomPrompt from \"@examples/models/chat/ollama_functions/custom_prompt.ts\";\n```\n\n----------------------------------------\n\nTITLE: Creating Sample Documents in LangChain\nDESCRIPTION: This code shows how to create sample Document objects in LangChain, which represent units of text with associated metadata.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/retrievers.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Document } from \"@langchain/core/documents\";\n\nconst documents = [\n    new Document({\n        pageContent: \"Dogs are great companions, known for their loyalty and friendliness.\",\n        metadata: {\"source\": \"mammal-pets-doc\"},\n    }),\n    new Document({\n        pageContent: \"Cats are independent pets that often enjoy their own space.\",\n        metadata: {\"source\": \"mammal-pets-doc\"},\n    }),\n]\n```\n\n----------------------------------------\n\nTITLE: Instantiating RedisVectorStore with OpenAI Embeddings in TypeScript\nDESCRIPTION: This snippet shows how to instantiate a RedisVectorStore with OpenAI Embeddings and a Redis client. Proper instantiation requires specific configurations like model selection for embeddings and setting up a client with the correct URL.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/redis.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { RedisVectorStore } from \"@langchain/redis\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nimport { createClient } from \"redis\";\n\nconst embeddings = new OpenAIEmbeddings({\n  model: \"text-embedding-3-small\",\n});\n\nconst client = createClient({\n  url: process.env.REDIS_URL ?? \"redis://localhost:6379\",\n});\nawait client.connect();\n\nconst vectorStore = new RedisVectorStore(embeddings, {\n  redisClient: client,\n  indexName: \"langchainjs-testing\",\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing MultiVectorRetriever with Document Summaries in TypeScript\nDESCRIPTION: Example of using MultiVectorRetriever with document summaries to improve retrieval accuracy by distilling key information.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/multi_vector.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport SummaryExample from \"@examples/retrievers/multi_vector_summary.ts\";\n\n<CodeBlock language=\"typescript\">{SummaryExample}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Complete Example of Functions with Example Selectors in LangChainJS\nDESCRIPTION: A full example showing how to use functions with example selectors in few shot templates, with length-based filtering of examples.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot.mdx#2025-04-22_snippet_19\n\nLANGUAGE: typescript\nCODE:\n```\nconst examplePrompt = PromptTemplate.fromTemplate(\"An example about {x}\");\nconst exampleSelector = await LengthBasedExampleSelector.fromExamples(\n  [{ x: \"foo\" }, { x: \"bar\" }],\n  { examplePrompt, maxLength: 200 }\n);\nconst prompt = new FewShotPromptTemplate({\n  prefix: \"{foo}{bar}\",\n  exampleSelector,\n  examplePrompt,\n  inputVariables: [\"foo\", \"bar\"],\n});\nconst partialPrompt = await prompt.partial({\n  foo: () => Promise.resolve(\"boo\"),\n});\nconst formatted = await partialPrompt.format({ bar: \"baz\" });\nconsole.log(formatted);\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search with Scores on SupabaseVectorStore (TypeScript)\nDESCRIPTION: This snippet shows how to perform a similarity search that returns both the documents and their corresponding similarity scores. It searches for documents similar to \"biology\" with a filter and then prints each document along with its score.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/supabase.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst similaritySearchWithScoreResults = await vectorStore.similaritySearchWithScore(\"biology\", 2, filter)\n\nfor (const [doc, score] of similaritySearchWithScoreResults) {\n  console.log(`* [SIM=${score.toFixed(3)}] ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`);\n}\n```\n\n----------------------------------------\n\nTITLE: Configurable Chat Model with Default Values\nDESCRIPTION: Shows how to create a configurable chat model with default values, specify which parameters are configurable, and add prefixes to configurable parameters for better organization.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chat_models_universal_init.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { initChatModel } from \"langchain/chat_models_universal\";\n\n// Create a configurable model with defaults\nconst configurableChat = await initChatModel({\n  modelProvider: \"ollama\",\n  model: \"mistral\",\n  temperature: 0,\n  // Add 'llm_' prefix to configurable fields in the config\n  configurableFieldPrefix: \"llm_\",\n  // Specify which fields can be configured\n  configurableFields: [\n    \"temperature\",\n    \"maxTokens\",\n    // Model and modelProvider are configurable too\n    \"model\",\n    \"modelProvider\",\n  ],\n});\n\n// Can call with config.llm_temperature instead of config.temperature\nconst chat = configurableChat.withConfig({\n  llm_temperature: 0.7,\n});\n```\n\n----------------------------------------\n\nTITLE: Using Custom OpenAI Tool Schemas with Strict Mode\nDESCRIPTION: Shows how to pass OpenAI-formatted tool schemas directly with strict mode enabled for selected tools.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport { zodToJsonSchema } from \"zod-to-json-schema\";\n\nconst toolSchema = {\n  type: \"function\",\n  function: {\n    name: \"get_current_weather\",\n    description: \"Get the current weather\",\n    strict: true,\n    parameters: zodToJsonSchema(\n      z.object({\n        location: z.string(),\n      })\n    ),\n  },\n};\n\nconst llmWithStrictTrueTools = new ChatOpenAI({\n  model: \"gpt-4o\",\n}).bindTools([toolSchema], {\n  strict: true,\n});\n\nconst weatherToolResult = await llmWithStrictTrueTools.invoke([{\n  role: \"user\",\n  content: \"What is the current weather in London?\"\n}])\n\nweatherToolResult.tool_calls;\n```\n\n----------------------------------------\n\nTITLE: Examining Initial Chain Events\nDESCRIPTION: Code that displays the first three events from a chain's event stream, showing the hierarchy of start events from the chain, model, and parser components.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming.ipynb#2025-04-22_snippet_14\n\nLANGUAGE: javascript\nCODE:\n```\nevents.slice(0, 3);\n```\n\n----------------------------------------\n\nTITLE: Creating a Tool with Schema in TypeScript\nDESCRIPTION: Demonstrates how to create a tool using the tool function from LangChain. This example creates a multiply tool that takes two numbers as input and returns their product, using Zod for schema validation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/tool_calling.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { tool } from \"@langchain/core/tools\";\n\nconst multiply = tool(\n  ({ a, b }: { a: number; b: number }): number => {\n    /**\n     * Multiply a and b.\n     */\n    return a * b;\n  },\n  {\n    name: \"multiply\",\n    description: \"Multiply two numbers\",\n    schema: z.object({\n      a: z.number(),\n      b: z.number(),\n    }),\n  }\n);\n```\n\n----------------------------------------\n\nTITLE: Setting Up StructuredOutputParser with Zod Schema\nDESCRIPTION: Configures extraction using the built-in StructuredOutputParser with a Zod schema for person information. Creates a chat prompt template that includes formatting instructions for the model to follow.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/extraction_parse.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { z } from \"zod\";\nimport { StructuredOutputParser } from \"langchain/output_parsers\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\nlet personSchema = z.object({\n  name: z.optional(z.string()).describe(\"The name of the person\"),\n  hair_color: z.optional(z.string()).describe(\"The color of the person's hair, if known\"),\n  height_in_meters: z.optional(z.string()).describe(\"Height measured in meters\")\n}).describe(\"Information about a person.\");\n\nconst parser = StructuredOutputParser.fromZodSchema(personSchema);\n\nconst prompt = ChatPromptTemplate.fromMessages([\n  [\"system\", \"Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\"],\n  [\"human\", \"{query}\"],\n]);\n\nconst partialedPrompt = await prompt.partial({\n  format_instructions: parser.getFormatInstructions(),\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Query Analysis in LangGraph JavaScript\nDESCRIPTION: This extensive snippet shows how to implement query analysis in a LangGraph application, including state annotation, query analysis, retrieval, and generation steps.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nconst StateAnnotationQA = Annotation.Root({\n  question: Annotation<string>,\n  search: Annotation<z.infer<typeof searchSchema>>,\n  context: Annotation<Document[]>,\n  answer: Annotation<string>,\n});\n\n\nconst analyzeQuery = async (state: typeof InputStateAnnotation.State) => {\n  const result = await structuredLlm.invoke(state.question)\n  return { search: result }\n};\n\n\nconst retrieveQA = async (state: typeof StateAnnotationQA.State) => {\n  const filter = (doc) => doc.metadata.section === state.search.section;\n  const retrievedDocs = await vectorStore.similaritySearch(\n    state.search.query,\n    2,\n    filter\n  )\n  return { context: retrievedDocs };\n};\n\n\nconst generateQA = async (state: typeof StateAnnotationQA.State) => {\n  const docsContent = state.context.map(doc => doc.pageContent).join(\"\\n\");\n  const messages = await promptTemplate.invoke({ question: state.question, context: docsContent });\n  const response = await llm.invoke(messages);\n  return { answer: response.content };\n};\n\n\n\nconst graphQA = new StateGraph(StateAnnotationQA)\n  .addNode(\"analyzeQuery\", analyzeQuery)\n  .addNode(\"retrieveQA\", retrieveQA)\n  .addNode(\"generateQA\", generateQA)\n  .addEdge(\"__start__\", \"analyzeQuery\")\n  .addEdge(\"analyzeQuery\", \"retrieveQA\")\n  .addEdge(\"retrieveQA\", \"generateQA\")\n  .addEdge(\"generateQA\", \"__end__\")\n  .compile();\n```\n\n----------------------------------------\n\nTITLE: Direct OpenAI Tool Format Binding\nDESCRIPTION: Shows how to bind tools directly using OpenAI's specific function calling format.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_calling.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst model = new ChatOpenAI({ model: \"gpt-4o\" });\n\nconst modelWithTools = model.bind({\n  tools: [{\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"calculator\",\n      \"description\": \"Can perform mathematical operations.\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"operation\": {\n            \"type\": \"string\",\n            \"description\": \"The type of operation to execute.\",\n            \"enum\": [\"add\", \"subtract\", \"multiply\", \"divide\"]\n          },\n          \"number1\": {\"type\": \"number\", \"description\": \"First integer\"},\n          \"number2\": {\"type\": \"number\", \"description\": \"Second integer\"},\n        },\n        \"required\": [\"number1\", \"number2\"],\n      },\n    },\n  }],\n});\n\nawait modelWithTools.invoke(`Whats 119 times 8?`);\n```\n\n----------------------------------------\n\nTITLE: Calculating Time-Weighted Score in TypeScript\nDESCRIPTION: This snippet shows the core algorithm for calculating the score in the TimeWeightedVectorStoreRetriever. It combines semantic similarity with a time decay factor based on hours passed since last access.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/time_weighted_vectorstore.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nlet score = (1.0 - this.decayRate) ** hoursPassed + vectorRelevance;\n```\n\n----------------------------------------\n\nTITLE: Detecting Entities in User Input using OpenAI and Zod\nDESCRIPTION: This snippet sets up an entity detection chain using OpenAI's ChatGPT and Zod for schema validation. It extracts person and movie names from the user's input.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/graph_mapping.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { z } from \"zod\";\n\nconst llm = new ChatOpenAI({ model: \"gpt-3.5-turbo\", temperature: 0 })\n\nconst entitySchema = z.object({\n    names: z.array(z.string()).describe(\"All the person or movies appearing in the text\"),\n}).describe(\"Identifying information about entities.\");\n\n\nconst prompt = ChatPromptTemplate.fromMessages(\n  [\n    [\n      \"system\",\n      \"You are extracting person and movies from the text.\"\n    ],\n    [\n      \"human\",\n      \"Use the given format to extract information from the following\\ninput: {question}\"\n    ]\n  ]\n);\n\nconst entityChain = prompt.pipe(llm.withStructuredOutput(entitySchema));\n```\n\n----------------------------------------\n\nTITLE: Composing Complex Chains with RunnableLambda in JavaScript\nDESCRIPTION: This example demonstrates how to compose a more complex chain that generates a joke and then evaluates its humor. It uses RunnableLambda for custom logic and combines multiple chain components.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/sequence.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RunnableLambda } from \"@langchain/core/runnables\";\n\nconst analysisPrompt = ChatPromptTemplate.fromTemplate(\"is this a funny joke? {joke}\")\n\nconst composedChain = new RunnableLambda({\n  func: async (input: { topic: string }) => {\n    const result = await chain.invoke(input);\n    return { joke: result };\n  }\n}).pipe(analysisPrompt).pipe(model).pipe(new StringOutputParser())\n\nawait composedChain.invoke({ topic: \"bears\" })\n```\n\n----------------------------------------\n\nTITLE: Implementing SQL Query Execution Function\nDESCRIPTION: This function executes a SQL query using the QuerySqlTool from langchain-community.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/sql_qa.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport { QuerySqlTool } from \"langchain/tools/sql\";\n\nconst executeQuery = async (state: typeof StateAnnotation.State) => {\n  const executeQueryTool = new QuerySqlTool(db);\n  return { result: await executeQueryTool.invoke(state.query) }\n};\n```\n\n----------------------------------------\n\nTITLE: Downloading and Building Chinook SQLite Database\nDESCRIPTION: This bash command downloads the Chinook SQLite database SQL file and creates the database using the sqlite3 command.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/sql_qa.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl -s https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql | sqlite3 Chinook.db\n```\n\n----------------------------------------\n\nTITLE: Creating USearch Vector Store from Texts (TypeScript)\nDESCRIPTION: Demonstrates how to initialize a USearch vector store in LangchainJS using OpenAI embeddings and an array of text documents. It shows creating an instance, adding documents manually, and performing similarity searches with scores.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/usearch.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { USearch } from \"@langchain/community/vectorstores/usearch\";\n\n// Initialize the vector store\nconst vectorStore = await USearch.fromTexts(\n  [\"Hello world\", \"Bye bye\", \"hello nice world\"],\n  [{ id: 2 }, { id: 1 }, { id: 3 }],\n  new OpenAIEmbeddings()\n);\n\n// Search for the most similar document\nconst resultOne = await vectorStore.similaritySearchWithScore(\"hello world\", 1);\nconsole.log(resultOne);\n/*\n[\n  [\n    Document {\n      pageContent: 'Hello world',\n      metadata: { id: 2 }\n    },\n    0\n  ]\n]\n*/\n```\n\n----------------------------------------\n\nTITLE: Testing Initial Query with Conversational Retrieval Chain\nDESCRIPTION: Tests the conversational retrieval chain with an initial question about LangSmith's testing capabilities to verify that it works for single-turn interactions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_retrieval.ipynb#2025-04-22_snippet_16\n\nLANGUAGE: javascript\nCODE:\n```\nawait conversationalRetrievalChain.invoke({\n  messages: [new HumanMessage(\"Can LangSmith help test my LLM applications?\")],\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Chat Prompt Template with Multiple Messages\nDESCRIPTION: Shows how to create a ChatPromptTemplate that formats multiple messages including system and user messages. The template combines static system instructions with variable user input.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#2025-04-22_snippet_18\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\nconst promptTemplate = ChatPromptTemplate.fromMessages([\n  [\"system\", \"You are a helpful assistant\"],\n  [\"user\", \"Tell me a joke about {topic}\"],\n]);\n\nawait promptTemplate.invoke({ topic: \"cats\" });\n```\n\n----------------------------------------\n\nTITLE: Adding Schema Name for Improved Context in TypeScript\nDESCRIPTION: Shows how to provide a name for the schema to give the model additional context, which can improve performance when extracting structured data from the model's response.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/structured_output.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst structuredLlm = model.withStructuredOutput(joke, { name: \"joke\" });\n\nawait structuredLlm.invoke(\"Tell me a joke about cats\")\n```\n\n----------------------------------------\n\nTITLE: Invoking LangChain Agent with Simple Input in Python\nDESCRIPTION: This snippet shows how to invoke a LangChain agent with a simple greeting input. It demonstrates the basic usage of the agent without the need for external tools.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/agent_executor.ipynb#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nawait agentExecutor.invoke({ input: \"hi!\" })\n```\n\n----------------------------------------\n\nTITLE: Implementing Contextual Compression with LLMChainExtractor\nDESCRIPTION: This code demonstrates how to use a Contextual Compression Retriever with an LLMChainExtractor to improve document retrieval relevance.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/contextual_compression.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/retrievers/contextual_compression.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Creating PipelinePromptTemplate in JavaScript\nDESCRIPTION: This code snippet shows how to create a PipelinePromptTemplate in JavaScript. It sets up multiple prompt templates and combines them into a pipeline, allowing for reuse of prompt parts and complex prompt composition.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/prompts_composition.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport {\n    PromptTemplate,\n    PipelinePromptTemplate,\n  } from \"@langchain/core/prompts\";\n  \nconst fullPrompt = PromptTemplate.fromTemplate(`{introduction}\n\n{example}\n\n{start}`);\n\nconst introductionPrompt = PromptTemplate.fromTemplate(\n`You are impersonating {person}.`\n);\n\nconst examplePrompt =\nPromptTemplate.fromTemplate(`Here's an example of an interaction:\nQ: {example_q}\nA: {example_a}`);\n\nconst startPrompt = PromptTemplate.fromTemplate(`Now, do this for real!\nQ: {input}\nA:`);\n\nconst composedPrompt = new PipelinePromptTemplate({\npipelinePrompts: [\n    {\n    name: \"introduction\",\n    prompt: introductionPrompt,\n    },\n    {\n    name: \"example\",\n    prompt: examplePrompt,\n    },\n    {\n    name: \"start\",\n    prompt: startPrompt,\n    },\n],\nfinalPrompt: fullPrompt,\n});\n```\n\n----------------------------------------\n\nTITLE: Invoking the Chain for Image Description\nDESCRIPTION: This snippet demonstrates how to invoke the chain with the base64-encoded image data and log the response content.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/multimodal_prompts.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst response = await chain.invoke({ base64 })\nconsole.log(response.content)\n```\n\n----------------------------------------\n\nTITLE: Binding Tools to a Model in TypeScript\nDESCRIPTION: Demonstrates the standard way to bind tools to a model that supports tool calling. The .bindTools() method connects a list of tools to the model, making them available for the model to call.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/tool_calling.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst modelWithTools = model.bindTools([toolsList]);\n```\n\n----------------------------------------\n\nTITLE: Streaming ChatOpenAI Model Output in JavaScript\nDESCRIPTION: This snippet demonstrates how to stream output from a ChatOpenAI model and process the chunks.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst stream = await model.stream(\"Hello! Tell me about yourself.\");\nconst chunks = [];\nfor await (const chunk of stream) {\n  chunks.push(chunk);\n  console.log(`${chunk.content}|`)\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Q&A Chain with Sources in LangChain.js\nDESCRIPTION: A comprehensive implementation of a Q&A chain with sources using LangChain.js, including document loading, text splitting, vector store creation, and chain construction.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_streaming.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport \"cheerio\";\nimport { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\"\nimport { OpenAIEmbeddings, ChatOpenAI } from \"@langchain/openai\";\nimport { pull } from \"langchain/hub\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { formatDocumentsAsString } from \"langchain/util/document\";\nimport { RunnableSequence, RunnablePassthrough, RunnableMap } from \"@langchain/core/runnables\";\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\n\nconst loader = new CheerioWebBaseLoader(\n  \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n);\n\nconst docs = await loader.load();\n\nconst textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000, chunkOverlap: 200 });\nconst splits = await textSplitter.splitDocuments(docs);\nconst vectorStore = await MemoryVectorStore.fromDocuments(splits, new OpenAIEmbeddings());\n\n// Retrieve and generate using the relevant snippets of the blog.\nconst retriever = vectorStore.asRetriever();\nconst prompt = await pull<ChatPromptTemplate>(\"rlm/rag-prompt\");\nconst llm = new ChatOpenAI({ model: \"gpt-3.5-turbo\", temperature: 0 });\n\nconst ragChainFromDocs = RunnableSequence.from([\n  RunnablePassthrough.assign({ context: (input) => formatDocumentsAsString(input.context) }),\n  prompt,\n  llm,\n  new StringOutputParser()\n]);\n\nlet ragChainWithSource = new RunnableMap({ steps: { context: retriever, question: new RunnablePassthrough() }})\nragChainWithSource = ragChainWithSource.assign({ answer: ragChainFromDocs });\n\nawait ragChainWithSource.invoke(\"What is Task Decomposition\")\n```\n\n----------------------------------------\n\nTITLE: Fetching and Encoding Image Data in JavaScript\nDESCRIPTION: This snippet demonstrates how to fetch an image from a URL using axios and encode it as a base64 string. This is necessary for passing image data to the language model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/multimodal_prompts.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport axios from \"axios\";\n\nconst imageUrl = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\";\nconst axiosRes = await axios.get(imageUrl, { responseType: \"arraybuffer\" });\nconst base64 = btoa(\n  new Uint8Array(axiosRes.data).reduce(\n    (data, byte) => data + String.fromCharCode(byte),\n    ''\n  )\n);\n```\n\n----------------------------------------\n\nTITLE: ParentDocumentRetriever with Contextual Chunk Headers in TypeScript\nDESCRIPTION: Example showing how to use ParentDocumentRetriever with contextual chunk headers to provide additional context for each document chunk.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/parent_document_retriever.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n{ExampleWithChunkHeader}\n```\n\n----------------------------------------\n\nTITLE: Creating a Stateful Chatbot with LangGraph in TypeScript\nDESCRIPTION: Implementation of a stateful chatbot using LangGraph, including message persistence and support for multiple conversation threads.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/chatbot.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { START, END, MessagesAnnotation, StateGraph, MemorySaver } from \"@langchain/langgraph\";\n\n// Define the function that calls the model\nconst callModel = async (state: typeof MessagesAnnotation.State) => {\n  const response = await llm.invoke(state.messages);\n  return { messages: response };\n};\n\n// Define a new graph\nconst workflow = new StateGraph(MessagesAnnotation)\n  // Define the node and edge\n  .addNode(\"model\", callModel)\n  .addEdge(START, \"model\")\n  .addEdge(\"model\", END);\n\n// Add memory\nconst memory = new MemorySaver();\nconst app = workflow.compile({ checkpointer: memory });\n```\n\n----------------------------------------\n\nTITLE: Chaining ExaSearchResults with LLM (TypeScript)\nDESCRIPTION: Demonstrates creating a chain that uses the `ExaSearchResults` tool. It defines a chat prompt, binds the tool to the LLM (forcing its use with `tool_choice`), pipes the prompt to the LLM, invokes the chain, executes the tool calls returned by the LLM, and then invokes the LLM again with the tool results to get the final answer.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/exa_search.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\"\nimport { RunnableConfig } from \"@langchain/core/runnables\"\nimport { AIMessage } from \"@langchain/core/messages\"\n\nconst prompt = ChatPromptTemplate.fromMessages(\n  [\n    [\"system\", \"You are a helpful assistant.\"],\n    [\"human\", \"{user_input}\"],\n    [\"placeholder\", \"{messages}\"],\n  ]\n)\n\n// specifying tool_choice will force the model to call this tool.\nconst llmWithTools = llm.bindTools([tool], {\n  tool_choice: tool.name\n})\n\nconst llmChain = prompt.pipe(llmWithTools);\n\nconst toolChain = async (userInput: string, config?: RunnableConfig): Promise<AIMessage> => {\n  const input_ = { user_input: userInput };\n  const aiMsg = await llmChain.invoke(input_, config);\n  const toolMsgs = await tool.batch(aiMsg.tool_calls, config);\n  return llmChain.invoke({ ...input_, messages: [aiMsg, ...toolMsgs] }, config);\n};\n\nconst toolChainResult = await toolChain(\"What is Anthropic's estimated revenue for 2024?\");\n```\n\n----------------------------------------\n\nTITLE: Initializing NeonPostgres Vector Store in TypeScript\nDESCRIPTION: The TypeScript code snippet demonstrates how to initialize a NeonPostgres vector store using LangChain.js. It requires an `embeddings` parameter and the `connectionString` containing the necessary database connection details. This setup allows for the storage and querying of embeddings in a serverless Postgres database.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/neon.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst vectorStore = await NeonPostgres.initialize(embeddings, {\n  connectionString: NEON_POSTGRES_CONNECTION_STRING,\n});\n```\n\n----------------------------------------\n\nTITLE: Basic Text Splitting with RecursiveCharacterTextSplitter\nDESCRIPTION: Demonstrates basic usage of RecursiveCharacterTextSplitter to split text into chunks. Uses chunkSize of 10 and overlap of 1 to create Document objects from raw text.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/recursive_text_splitter.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\n\nconst text = `Hi.\\n\\nI'm Harrison.\\n\\nHow? Are? You?\\nOkay then f f f f.\nThis is a weird text to write, but gotta test the splittingggg some how.\\n\\n\nBye!\\n\\n-H.`;\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 10,\n  chunkOverlap: 1,\n});\n\nconst output = await splitter.createDocuments([text]);\n\nconsole.log(output.slice(0, 3));\n```\n\n----------------------------------------\n\nTITLE: Advanced PGVectorStore Connection Reuse\nDESCRIPTION: Demonstrates the setup for reusing database connections in PGVectorStore by establishing a connection pool, allowing multiple vector store instances to share the same connection settings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/pgvector.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { PGVectorStore } from \"@langchain/community/vectorstores/pgvector\";\nimport pg from \"pg\";\n\n// First, follow set-up instructions at\n// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/pgvector\n\nconst reusablePool = new pg.Pool({\n  host: \"127.0.0.1\",\n  port: 5433,\n  user: \"myuser\",\n  password: \"ChangeMe\",\n  database: \"api\",\n});\n\nconst originalConfig = {\n  pool: reusablePool,\n  tableName: \"testlangchainjs\",\n  collectionName: \"sample\",\n  collectionTableName: \"collections\",\n  columns: {\n    idColumnName: \"id\",\n    vectorColumnName: \"vector\",\n    contentColumnName: \"content\",\n    metadataColumnName: \"metadata\",\n  },\n};\n\n// Set up the DB.\n// Can skip this step if you've already initialized the DB.\n// await PGVectorStore.initialize(new OpenAIEmbeddings(), originalConfig);\nconst pgvectorStore = new PGVectorStore(new OpenAIEmbeddings(), originalConfig);\n\nawait pgvectorStore.addDocuments([\n  { pageContent: \"what's this\", metadata: { a: 2 } },\n  { pageContent: \"Cat drinks milk\", metadata: { a: 1 } },\n]);\n\nconst results = await pgvectorStore.similaritySearch(\"water\", 1);\n\nconsole.log(results);\n\n/*\n  [ Document { pageContent: 'Cat drinks milk', metadata: { a: 1 } } ]\n*/\n\nconst pgvectorStore2 = new PGVectorStore(new OpenAIEmbeddings(), {\n  pool: reusablePool,\n  tableName: \"testlangchainjs\",\n  collectionTableName: \"collections\",\n  collectionName: \"some_other_collection\",\n  columns: {\n    idColumnName: \"id\",\n    vectorColumnName: \"vector\",\n    contentColumnName: \"content\",\n    metadataColumnName: \"metadata\",\n  },\n});\n\nconst results2 = await pgvectorStore2.similaritySearch(\"water\", 1);\n\nconsole.log(results2);\n\n/*\n  []\n*/\n\nawait reusablePool.end();\n```\n\n----------------------------------------\n\nTITLE: Indexing Documents with OpenSearch Vector Store - TypeScript\nDESCRIPTION: This TypeScript snippet demonstrates importing required modules and indexing documents in an OpenSearch-based vector store using Langchain.js. Prerequisites include a running OpenSearch instance, the @opensearch-project/opensearch client, and OpenAIEmbeddings. Key parameters are the OpenSearch client (with the URL taken from OPENSEARCH_URL or default), a document array with metadata, and an index name (from OPENSEARCH_INDEX or defaults to 'documents'). Takes raw documents, computes embeddings, and indexes them into the specified OpenSearch index. Outputs no direct result; documents become queryable.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/opensearch.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Client } from \"@opensearch-project/opensearch\";\nimport { Document } from \"langchain/document\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { OpenSearchVectorStore } from \"@langchain/community/vectorstores/opensearch\";\n\nconst client = new Client({\n  nodes: [process.env.OPENSEARCH_URL ?? \"http://127.0.0.1:9200\"],\n});\n\nconst docs = [\n  new Document({\n    metadata: { foo: \"bar\" },\n    pageContent: \"opensearch is also a vector db\",\n  }),\n  new Document({\n    metadata: { foo: \"bar\" },\n    pageContent: \"the quick brown fox jumped over the lazy dog\",\n  }),\n  new Document({\n    metadata: { baz: \"qux\" },\n    pageContent: \"lorem ipsum dolor sit amet\",\n  }),\n  new Document({\n    metadata: { baz: \"qux\" },\n    pageContent:\n      \"OpenSearch is a scalable, flexible, and extensible open-source software suite for search, analytics, and observability applications\",\n  }),\n];\n\nawait OpenSearchVectorStore.fromDocuments(docs, new OpenAIEmbeddings(), {\n  client,\n  indexName: process.env.OPENSEARCH_INDEX, // Will default to `documents`\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Basic Classification Schema\nDESCRIPTION: Creates a tagging prompt template and defines a Zod schema for basic text classification including sentiment, aggressiveness, and language detection.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/classification.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { z } from \"zod\";\n\nconst taggingPrompt = ChatPromptTemplate.fromTemplate(\n    `Extract the desired information from the following passage.\n\nOnly extract the properties mentioned in the 'Classification' function.\n\nPassage:\n{input}\n`\n);\n\nconst classificationSchema = z.object({\n    sentiment: z.string().describe(\"The sentiment of the text\"),\n    aggressiveness: z.number().int().describe(\n        \"How aggressive the text is on a scale from 1 to 10\"\n    ),\n    language: z.string().describe(\"The language the text is written in\"),\n});\n\nconst llmWihStructuredOutput = llm.withStructuredOutput(classificationSchema, { name: \"extractor\" })\n```\n\n----------------------------------------\n\nTITLE: Transforming Elasticsearch Vector Store into Retriever (TypeScript)\nDESCRIPTION: Demonstrates turning the configured vector store into a retriever for use in LangChain chains, allowing for repeated queries with consistent parameters (such as filters and k). The asRetriever method returns an object with an invoke method to execute retrieval by query. This can be used as a drop-in retriever for retrieval-augmented tasks.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/elasticsearch.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst retriever = vectorStore.asRetriever({\n  // Optional filter\n  filter: filter,\n  k: 2,\n});\nawait retriever.invoke(\"biology\");\n```\n\n----------------------------------------\n\nTITLE: Using MessagesPlaceholder in TypeScript\nDESCRIPTION: Demonstrates how to use MessagesPlaceholder to dynamically insert multiple messages into a chat prompt template.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/prompt_templates.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport {\n  ChatPromptTemplate,\n  MessagesPlaceholder,\n} from \"@langchain/core/prompts\";\nimport { HumanMessage } from \"@langchain/core/messages\";\n\nconst promptTemplate = ChatPromptTemplate.fromMessages([\n  [\"system\", \"You are a helpful assistant\"],\n  new MessagesPlaceholder(\"msgs\"),\n]);\n\nawait promptTemplate.invoke({ msgs: [new HumanMessage(\"hi!\")] });\n```\n\n----------------------------------------\n\nTITLE: Manipulating Outputs/Inputs with RunnableSequence in TypeScript\nDESCRIPTION: Illustrates how to use RunnableSequence with RunnableParallel to manipulate the output of one Runnable to match the input format of the next Runnable in a sequence, including retrieval and input formatting.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/parallel.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport SequenceExample from \"@examples/guides/expression_language/runnable_maps_sequence.ts\";\n\n<CodeBlock language=\"typescript\">{SequenceExample}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Invoking RAG Chain with Query in TypeScript\nDESCRIPTION: This code snippet shows how to invoke the created RAG chain with a query to get the final response.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/kendra-retriever.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nawait ragChain.invoke(query);\n```\n\n----------------------------------------\n\nTITLE: Creating a Document Chain with ChatPromptTemplate\nDESCRIPTION: Sets up a document chain using createStuffDocumentsChain that combines a chat model with a specialized prompt template to answer questions based on provided context.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_retrieval.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { createStuffDocumentsChain } from \"langchain/chains/combine_documents\";\nimport {\n  ChatPromptTemplate,\n  MessagesPlaceholder,\n} from \"@langchain/core/prompts\";\n\nconst SYSTEM_TEMPLATE = `Answer the user's questions based on the below context. \nIf the context doesn't contain any relevant information to the question, don't make something up and just say \"I don't know\":\n\n<context>\n{context}\n</context>\n`;\n\nconst questionAnsweringPrompt = ChatPromptTemplate.fromMessages([\n  [\"system\", SYSTEM_TEMPLATE],\n  new MessagesPlaceholder(\"messages\"),\n]);\n\nconst documentChain = await createStuffDocumentsChain({\n  llm,\n  prompt: questionAnsweringPrompt,\n});\n```\n\n----------------------------------------\n\nTITLE: Structured Output with Reasoning Models using z.nullable() in TypeScript\nDESCRIPTION: This example shows the correct way to define a schema for structured output with reasoning models like 'o1'. It uses z.nullable() instead of z.optional(), which is properly respected by these models.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#2025-04-22_snippet_16\n\nLANGUAGE: typescript\nCODE:\n```\nimport { z } from \"zod\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\n// Will not work\nconst reasoningModelSchemaNullable = z.object({\n  color: z.nullable(z.string()).describe(\"A color mentioned in the input\"),\n});\n\nconst reasoningModelNullableSchema = new ChatOpenAI({\n  model: \"o1\",\n}).withStructuredOutput(reasoningModelSchemaNullable, {\n  name: \"extract_color\",\n});\n\nawait reasoningModelNullableSchema.invoke([{\n  role: \"user\",\n  content: `I am 6'5\" tall and love fruit.`\n}]);\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search with Filters on Elasticsearch Vector Store (TypeScript)\nDESCRIPTION: Illustrates how to perform a similarity search on the Elasticsearch vector store with a specified filter, limiting search results by metadata. The filter object allows field-operator-value constraints. The similaritySearch method retrieves the top-k relevant documents matching both the semantic query and the filter. Query, k, and filter are all configurable. Outputs document content and metadata.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/elasticsearch.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst filter = [{\n  operator: \"match\",\n  field: \"source\",\n  value: \"https://example.com\",\n}];\n\nconst similaritySearchResults = await vectorStore.similaritySearch(\"biology\", 2, filter);\n\nfor (const doc of similaritySearchResults) {\n  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);\n}\n```\n\n----------------------------------------\n\nTITLE: Running Extraction Chain with StructuredOutputParser\nDESCRIPTION: Creates and executes a chain that sends the prompt to the Claude model and processes the response through the StructuredOutputParser to extract structured data.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/extraction_parse.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst chain = partialedPrompt.pipe(model).pipe(parser);\n\nawait chain.invoke({ query });\n```\n\n----------------------------------------\n\nTITLE: Performing Filtered Similarity Search with CassandraStore in TypeScript\nDESCRIPTION: Illustrates how to perform a similarity search with an added filter. The `similaritySearch` method accepts an optional third argument, an object representing the filter condition (e.g., `{ name: \"Bubba\" }`). This retrieves documents similar to the query string but only those matching the specified metadata filter. Requires appropriate indices to be configured if filtering on non-partition key columns.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/cassandra.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst results = await vectorStore.similaritySearch(\"B\", 1, { name: \"Bubba\" });\n```\n\n----------------------------------------\n\nTITLE: Invoking Chatbot Application with User Input\nDESCRIPTION: Example of how to invoke the chatbot application with user input, demonstrating the handling of conversation state and message persistence.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/chatbot.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst input = [\n  {\n    role: \"user\",\n    content: \"Hi! I'm Bob.\",\n  }\n]\nconst output = await app.invoke({ messages: input }, config)\n// The output contains all messages in the state.\n// This will log the last message in the conversation.\nconsole.log(output.messages[output.messages.length - 1]);\n```\n\n----------------------------------------\n\nTITLE: Visualizing LangGraph Application Flow\nDESCRIPTION: This code generates a visual representation of the application's control flow using Mermaid.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/sql_qa.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\n// Note: tslab only works inside a jupyter notebook. Don't worry about running this code yourself!\nimport * as tslab from \"tslab\";\n\nconst image = await graph.getGraph().drawMermaidPng();\nconst arrayBuffer = await image.arrayBuffer();\n\nawait tslab.display.png(new Uint8Array(arrayBuffer));\n```\n\n----------------------------------------\n\nTITLE: Implementing Tool Selection Chain\nDESCRIPTION: Creation of a chain that selects and executes the appropriate tool based on model output using LCEL.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tools_prompting.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { StructuredToolInterface } from \"@langchain/core/tools\"\n\nconst tools = [addTool, exponentiateTool, multiplyTool]\n\nconst toolChain = (modelOutput) => {\n    const toolMap: Record<string, StructuredToolInterface> = Object.fromEntries(tools.map(tool => [tool.name, tool]))\n    const chosenTool = toolMap[modelOutput.name]\n    return new RunnablePick(\"arguments\").pipe(new RunnableLambda({ func: (input) => chosenTool.invoke({\n        first_int: input[0],\n        second_int: input[1]\n      }) }))\n}\n```\n\n----------------------------------------\n\nTITLE: Turning Chroma Vector Store into Retriever (TypeScript)\nDESCRIPTION: This snippet demonstrates how to convert the vector store into a retriever by calling asRetriever, optionally providing a filter and number of results ('k'). The retriever enables simple invocation for similarity queries, and integrates more smoothly with LangChain's agent/chaining paradigms.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/chroma.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst retriever = vectorStore.asRetriever({\n  // Optional filter\n  filter: filter,\n  k: 2,\n});\nawait retriever.invoke(\"biology\");\n```\n\n----------------------------------------\n\nTITLE: Basic Tool Calling Workflow in TypeScript\nDESCRIPTION: Illustrates the recommended workflow for using tool calling. Created tools are passed to the .bindTools() method as a list. The model can be called as usual, and if a tool call is made, the model's response will contain the tool call arguments.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/tool_calling.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Tool creation\nconst tools = [myTool];\n// Tool binding\nconst modelWithTools = model.bindTools(tools);\n// Tool calling\nconst response = await modelWithTools.invoke(userInput);\n```\n\n----------------------------------------\n\nTITLE: Splitting Documents with RecursiveCharacterTextSplitter\nDESCRIPTION: Implements document splitting using RecursiveCharacterTextSplitter to break large documents into smaller chunks with specified size and overlap parameters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\n\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 1000, chunkOverlap: 200\n});\nconst allSplits = await splitter.splitDocuments(docs);\nconsole.log(`Split blog post into ${allSplits.length} sub-documents.`);\n```\n\n----------------------------------------\n\nTITLE: Executing Calculator Tool with Age Calculation in Langchain\nDESCRIPTION: This snippet shows a Langchain agent executing a calculator tool to compute '52 * 365' days for Christopher Nolan's age. The code includes the tool input, execution process, and the resulting output of 18980.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_20\n\nLANGUAGE: JavaScript\nCODE:\n```\nInvoking \"calculator\" with {\"input\":\"52 * 365\"}\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search with Scores on PineconeStore (TypeScript)\nDESCRIPTION: This snippet demonstrates how to perform a similarity search on the `PineconeStore` and retrieve the similarity scores along with the documents. It uses the `similaritySearchWithScore` method, takes the same parameters as `similaritySearch`, and returns an array of tuples, each containing a document and its corresponding score.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/pinecone.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst similaritySearchWithScoreResults = await vectorStore.similaritySearchWithScore(\"biology\", 2, filter)\n\nfor (const [doc, score] of similaritySearchWithScoreResults) {\n  console.log(`* [SIM=${score.toFixed(3)}] ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`);\n}\n```\n\n----------------------------------------\n\nTITLE: Invoking LangChain Retriever\nDESCRIPTION: Demonstrates how to invoke a LangChain retriever with a query to get relevant documents. The retriever returns a list of Document objects containing page content and metadata.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/retrievers.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nconst docs = await retriever.invoke(query);\n```\n\n----------------------------------------\n\nTITLE: Compiling RAG Application Graph in JavaScript\nDESCRIPTION: This snippet demonstrates how to compile the RAG application into a single graph object, connecting the steps into a sequence and allowing for direct responses without retrieval when appropriate.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/qa_chat_history.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport { StateGraph } from \"@langchain/langgraph\";\nimport { toolsCondition } from \"@langchain/langgraph/prebuilt\";\n\n\nconst graphBuilder = new StateGraph(MessagesAnnotation)\n  .addNode(\"queryOrRespond\", queryOrRespond)\n  .addNode(\"tools\", tools)\n  .addNode(\"generate\", generate)\n  .addEdge(\"__start__\", \"queryOrRespond\")\n  .addConditionalEdges(\n    \"queryOrRespond\",\n    toolsCondition,\n    {__end__: \"__end__\", tools: \"tools\"}\n  )\n  .addEdge(\"tools\", \"generate\")\n  .addEdge(\"generate\", \"__end__\")\n\nconst graph = graphBuilder.compile();\n```\n\n----------------------------------------\n\nTITLE: Similarity Search with Relevance Scores\nDESCRIPTION: Shows how to perform a similarity search that returns both matching documents and their relevance scores.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/retrievers.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nconst results2 = await vectorStore.similaritySearchWithScore(\n    \"What was Nike's revenue in 2023?\"\n)\n\nresults2[0]\n```\n\n----------------------------------------\n\nTITLE: Importing LangChain Modules\nDESCRIPTION: Shows the required import statements and formatting for LangChain components in a React/MDX context.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/vectorstore_retriever.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport CodeBlock from \"@theme/CodeBlock\";\nimport IntegrationInstallTooltip from \"@mdx_components/integration_install_tooltip.mdx\";\nimport RetrievalQAExample from \"@examples/chains/retrieval_qa.ts\";\n```\n\n----------------------------------------\n\nTITLE: Setting Up Question-Answering Chain Components\nDESCRIPTION: JavaScript code to set up the components for a question-answering chain, including prompt template and language model configuration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_per_user.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport {\n  RunnableBinding,\n  RunnableLambda,\n  RunnablePassthrough,\n} from \"@langchain/core/runnables\";\nimport { ChatOpenAI, OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst template = `Answer the question based only on the following context:\n{context}\nQuestion: {question}`;\n\nconst prompt = ChatPromptTemplate.fromTemplate(template);\n\nconst model = new ChatOpenAI({\n  model: \"gpt-3.5-turbo-0125\",\n  temperature: 0,\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing JSON Agent Toolkit with OpenAI API\nDESCRIPTION: This TypeScript code sets up a JSON agent toolkit to interact with the OpenAI API. It loads an OpenAPI specification from a YAML file, creates a JSON toolkit and agent, and executes a query about the required parameters for the completions endpoint.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/json.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport * as fs from \"fs\";\nimport * as yaml from \"js-yaml\";\nimport { OpenAI } from \"@langchain/openai\";\nimport { JsonSpec, JsonObject } from \"langchain/tools\";\nimport { JsonToolkit, createJsonAgent } from \"langchain/agents\";\n\nexport const run = async () => {\n  let data: JsonObject;\n  try {\n    const yamlFile = fs.readFileSync(\"openai_openapi.yaml\", \"utf8\");\n    data = yaml.load(yamlFile) as JsonObject;\n    if (!data) {\n      throw new Error(\"Failed to load OpenAPI spec\");\n    }\n  } catch (e) {\n    console.error(e);\n    return;\n  }\n\n  const toolkit = new JsonToolkit(new JsonSpec(data));\n  const model = new OpenAI({ temperature: 0 });\n  const executor = createJsonAgent(model, toolkit);\n\n  const input = `What are the required parameters in the request body to the /completions endpoint?`;\n\n  console.log(`Executing with input \"${input}\"...`);\n\n  const result = await executor.invoke({ input });\n\n  console.log(`Got output ${result.output}`);\n\n  console.log(\n    `Got intermediate steps ${JSON.stringify(\n      result.intermediateSteps,\n      null,\n      2\n    )}`\n  );\n};\n```\n\n----------------------------------------\n\nTITLE: Basic Structured Output Implementation in TypeScript\nDESCRIPTION: Demonstrates the basic workflow for using structured output with a model, including schema definition and model binding.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/structured_outputs.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Define schema\nconst schema = { foo: \"bar\" };\n// Bind schema to model\nconst modelWithStructure = model.withStructuredOutput(schema);\n// Invoke the model to produce structured output that matches the schema\nconst structuredOutput = await modelWithStructure.invoke(userInput);\n```\n\n----------------------------------------\n\nTITLE: Implementing Context Caching with Google AI in LangChain.js\nDESCRIPTION: This code demonstrates how to implement context caching using Google's Generative AI in LangChain.js. It shows the process of uploading a video file, creating cached content, and then using that cached content with a ChatGoogleGenerativeAI model to reduce token usage in subsequent requests.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/google_generativeai.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ChatGoogleGenerativeAI } from \"@langchain/google-genai\";\nimport {\n  GoogleAICacheManager,\n  GoogleAIFileManager,\n} from \"@google/generative-ai/server\";\n\nconst fileManager = new GoogleAIFileManager(process.env.GOOGLE_API_KEY);\nconst cacheManager = new GoogleAICacheManager(process.env.GOOGLE_API_KEY);\n\n// uploads file for caching\nconst pathToVideoFile = \"/path/to/video/file\";\nconst displayName = \"example-video\";\nconst fileResult = await fileManager.uploadFile(pathToVideoFile, {\n    displayName,\n    mimeType: \"video/mp4\",\n});\n\n// creates cached content AFTER uploading is finished\nconst cachedContent = await cacheManager.create({\n    model: \"models/gemini-1.5-flash-001\",\n    displayName: displayName,\n    systemInstruction: \"You are an expert video analyzer, and your job is to answer \" +\n      \"the user's query based on the video file you have access to.\",\n    contents: [\n        {\n            role: \"user\",\n            parts: [\n                {\n                    fileData: {\n                        mimeType: fileResult.file.mimeType,\n                        fileUri: fileResult.file.uri,\n                    },\n                },\n            ],\n        },\n    ],\n    ttlSeconds: 300,\n});\n\n// passes cached video to model\nconst model = new ChatGoogleGenerativeAI({});\nmodel.useCachedContent(cachedContent);\n\n// invokes model with cached video\nawait model.invoke(\"Summarize the video\");\n```\n\n----------------------------------------\n\nTITLE: Implementing Document Post-Processing for Retrieval\nDESCRIPTION: Implementation of document post-processing for retrieval optimization using RecursiveCharacterTextSplitter and EmbeddingsFilter. This code splits documents into smaller chunks and filters them based on relevance to the query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_citations.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport { EmbeddingsFilter } from \"langchain/retrievers/document_compressors/embeddings_filter\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { DocumentInterface } from \"@langchain/core/documents\";\nimport { RunnableMap, RunnablePassthrough } from \"@langchain/core/runnables\";\n\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 400,\n  chunkOverlap: 0,\n  separators: [\"\\n\\n\", \"\\n\", \".\", \" \"],\n  keepSeparator: false,\n});\n\nconst compressor = new EmbeddingsFilter({\n  embeddings: new OpenAIEmbeddings(),\n  k: 10,\n});\n\nconst splitAndFilter = async (input): Promise<Array<DocumentInterface>> => {\n  const { docs, question } = input;\n  const splitDocs = await splitter.splitDocuments(docs);\n  const statefulDocs = await compressor.compressDocuments(splitDocs, question);\n  return statefulDocs;\n};\n\nconst retrieveMap = RunnableMap.from({\n  question: new RunnablePassthrough(),\n  docs: retriever,\n});\n\nconst retriever = retrieveMap.pipe(splitAndFilter);\nconst docs = await retriever.invoke(\"How fast are cheetahs?\");\nfor (const doc of docs) {\n  console.log(doc.pageContent, \"\\n\\n\");\n}\n```\n\n----------------------------------------\n\nTITLE: Agent Logic Implementation\nDESCRIPTION: Core agent setup including tools, prompt, and executor configuration\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/stream_agent_client.mdx#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n  (async () => {\n    const tools = [new TavilySearchResults({ maxResults: 1 })];\n\n    const prompt = await pull<ChatPromptTemplate>(\n      \"hwchase17/openai-tools-agent\",\n    );\n\n    const agent = createToolCallingAgent({\n      llm,\n      tools,\n      prompt,\n    });\n\n    const agentExecutor = new AgentExecutor({\n      agent,\n      tools,\n    });\n```\n\n----------------------------------------\n\nTITLE: Integrating Self-Query Retriever in a Chain\nDESCRIPTION: This code demonstrates how to incorporate the self-query retriever into a LangChain application using a RAG (Retrieval-Augmented Generation) chain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/chroma.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { RunnablePassthrough, RunnableSequence } from \"@langchain/core/runnables\";\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\n\nimport type { Document } from \"@langchain/core/documents\";\n\nconst prompt = ChatPromptTemplate.fromTemplate(`\nAnswer the question based only on the context provided.\n\nContext: {context}\n\nQuestion: {question}`);\n\nconst formatDocs = (docs: Document[]) => {\n  return docs.map((doc) => JSON.stringify(doc)).join(\"\\n\\n\");\n}\n\n// See https://js.langchain.com/docs/tutorials/rag\nconst ragChain = RunnableSequence.from([\n  {\n    context: selfQueryRetriever.pipe(formatDocs),\n    question: new RunnablePassthrough(),\n  },\n  prompt,\n  llm,\n  new StringOutputParser(),\n]);\n```\n\n----------------------------------------\n\nTITLE: Creating React Agent\nDESCRIPTION: Assembles a tool-calling agent using LangGraph's createReactAgent with the configured LLM, tools, and prompt.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_tools.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\"\n\nconst agent = createReactAgent({ llm, tools, messageModifier: prompt })\n```\n\n----------------------------------------\n\nTITLE: Implementing an Agent with AWS Lambda Email Sender Tool\nDESCRIPTION: This code creates a LangChain agent with an AWS Lambda tool that can send emails. The agent uses OpenAI for reasoning, SerpAPI for search capabilities, and a custom AWS Lambda function to send emails via Amazon SES.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/lambda_agent.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAI } from \"@langchain/openai\";\nimport { SerpAPI } from \"langchain/tools\";\nimport { AWSLambda } from \"langchain/tools/aws_lambda\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\n\nconst model = new OpenAI({ temperature: 0 });\nconst emailSenderTool = new AWSLambda({\n  name: \"email-sender\",\n  // tell the Agent precisely what the tool does\n  description:\n    \"Sends an email with the specified content to testing123@gmail.com\",\n  region: \"us-east-1\", // optional: AWS region in which the function is deployed\n  accessKeyId: \"abc123\", // optional: access key id for a IAM user with invoke permissions\n  secretAccessKey: \"xyz456\", // optional: secret access key for that IAM user\n  functionName: \"SendEmailViaSES\", // the function name as seen in AWS Console\n});\nconst tools = [emailSenderTool, new SerpAPI(\"api_key_goes_here\")];\nconst executor = await initializeAgentExecutorWithOptions(tools, model, {\n  agentType: \"zero-shot-react-description\",\n});\n\nconst input = `Find out the capital of Croatia. Once you have it, email the answer to testing123@gmail.com.`;\nconst result = await executor.invoke({ input });\nconsole.log(result);\n```\n\n----------------------------------------\n\nTITLE: Invoking ChatModel with User Input in TypeScript\nDESCRIPTION: Example of how to invoke a ChatModel with a user message, demonstrating the basic usage of the language model in a conversation context.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/chatbot.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nawait llm.invoke([{ role: \"user\", content: \"Hi im bob\" }])\n```\n\n----------------------------------------\n\nTITLE: Creating a RAG-based Extraction Pipeline\nDESCRIPTION: Constructs a runnable sequence that first retrieves the most relevant document chunk using RAG, then runs the extraction chain on that chunk. This optimizes the extraction to focus only on chunks containing the most relevant information.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/extraction_long_text.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport { RunnableSequence } from \"@langchain/core/runnables\";\n\nconst ragExtractor = RunnableSequence.from([\n  {\n    text: retriever.pipe((docs) => docs[0].pageContent)\n  },\n  extractionChain,\n]);\n```\n\n----------------------------------------\n\nTITLE: Loading a single column from a CSV file using CSVLoader in TypeScript\nDESCRIPTION: TypeScript code showing how to use CSVLoader to load a specific column (in this case, 'text') from a CSV file. It demonstrates the resulting Document objects with the extracted content.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/document_loader_csv.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CSVLoader } from \"@langchain/community/document_loaders/fs/csv\";\n\nconst loader = new CSVLoader(\n  \"src/document_loaders/example_data/example.csv\",\n  \"text\"\n);\n\nconst docs = await loader.load();\n/*\n[\n  Document {\n    \"metadata\": {\n      \"line\": 1,\n      \"source\": \"src/document_loaders/example_data/example.csv\",\n    },\n    \"pageContent\": \"This is a sentence.\",\n  },\n  Document {\n    \"metadata\": {\n      \"line\": 2,\n      \"source\": \"src/document_loaders/example_data/example.csv\",\n    },\n    \"pageContent\": \"This is another sentence.\",\n  },\n]\n*/\n```\n\n----------------------------------------\n\nTITLE: Installing Required Langchain Packages\nDESCRIPTION: Installs essential Langchain packages (`@langchain/openai` for embeddings, `@langchain/community` for USearch integration, `@langchain/core` for core functionalities) needed to work with the USearch vector store integration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/usearch.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Defining Search Schema and Query using Zod in TypeScript\nDESCRIPTION: This code defines a Zod schema for search queries and creates a sample search query object. It includes optional filters for year and author.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_constructing_filters.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { z } from \"zod\";\n\nconst searchSchema = z.object({\n  query: z.string(),\n  startYear: z.number().optional(),\n  author: z.string().optional(),\n})\n\nconst searchQuery: z.infer<typeof searchSchema> = {\n  query: \"RAG\",\n  startYear: 2022,\n  author: \"LangChain\"\n}\n```\n\n----------------------------------------\n\nTITLE: Attaching OpenAI Tools to a LangChain Model\nDESCRIPTION: This snippet shows how to bind OpenAI function tools to a ChatOpenAI model. It defines a weather tool with parameters and requirements, then binds it to a GPT-4o model instance for tool-calling functionality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/binding.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst tools = [\n  {\n    \"type\": \"function\",\n    \"function\": {\n      \"name\": \"get_current_weather\",\n      \"description\": \"Get the current weather in a given location\",\n      \"parameters\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"location\": {\n            \"type\": \"string\",\n            \"description\": \"The city and state, e.g. San Francisco, CA\",\n          },\n          \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n        },\n        \"required\": [\"location\"],\n      },\n    },\n  }\n];\n\nconst modelWithTools = new ChatOpenAI({ model: \"gpt-4o\" }).bind({ tools });\n\nawait modelWithTools.invoke(\"What's the weather in SF, NYC and LA?\")\n```\n\n----------------------------------------\n\nTITLE: Merging Messages Using mergeMessageRuns in JavaScript\nDESCRIPTION: Demonstrates how to use the mergeMessageRuns function to combine consecutive messages of the same type. It shows the creation of different message types and their merging, including handling of content blocks and string concatenation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/merge_message_runs.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { HumanMessage, SystemMessage, AIMessage, mergeMessageRuns } from \"@langchain/core/messages\";\n\nconst messages = [\n    new SystemMessage(\"you're a good assistant.\"),\n    new SystemMessage(\"you always respond with a joke.\"),\n    new HumanMessage({ content: [{\"type\": \"text\", \"text\": \"i wonder why it's called langchain\"}] }),\n    new HumanMessage(\"and who is harrison chasing anyways\"),\n    new AIMessage(\n        'Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'\n    ),\n    new AIMessage(\"Why, he's probably chasing after the last cup of coffee in the office!\"),\n];\n\nconst merged = mergeMessageRuns(messages);\nconsole.log(merged.map((x) => JSON.stringify({\n    role: x._getType(),\n    content: x.content,\n}, null, 2)).join(\"\\n\\n\"));\n```\n\n----------------------------------------\n\nTITLE: Implementing Google GenAI Tool Calling with Image Data\nDESCRIPTION: Demonstrates tool calling with Google's Gemini model using base64-encoded image data. Includes axios for image fetching and prompt template usage.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_calls_multimodal.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatGoogleGenerativeAI } from \"@langchain/google-genai\";\nimport axios from \"axios\";\nimport { ChatPromptTemplate, MessagesPlaceholder } from \"@langchain/core/prompts\";\nimport { HumanMessage } from \"@langchain/core/messages\";\n\nconst axiosRes = await axios.get(imageUrl, { responseType: \"arraybuffer\" });\nconst base64 = btoa(\n  new Uint8Array(axiosRes.data).reduce(\n    (data, byte) => data + String.fromCharCode(byte),\n    ''\n  )\n);\n\nconst model = new ChatGoogleGenerativeAI({ model: \"gemini-1.5-pro-latest\" }).bindTools([weatherTool]);\n\nconst prompt = ChatPromptTemplate.fromMessages([\n  [\"system\", \"describe the weather in this image\"],\n  new MessagesPlaceholder(\"message\")\n]);\n\nconst response = await prompt.pipe(model).invoke({\n  message: new HumanMessage({\n    content: [{\n      type: \"media\",\n      mimeType: \"image/jpeg\",\n      data: base64,\n    }]\n  })\n});\nconsole.log(response.tool_calls);\n```\n\n----------------------------------------\n\nTITLE: Instantiating OpenAI Text Completion Model\nDESCRIPTION: Creates an instance of the OpenAI text completion model with customizable parameters including model type, temperature, token limits, timeout, and retry settings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/openai.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { OpenAI } from \"@langchain/openai\"\n\nconst llm = new OpenAI({\n  model: \"gpt-3.5-turbo-instruct\",\n  temperature: 0,\n  maxTokens: undefined,\n  timeout: undefined,\n  maxRetries: 2,\n  apiKey: process.env.OPENAI_API_KEY,\n  // other params...\n})\n```\n\n----------------------------------------\n\nTITLE: Streaming Chat Responses with LangGraph in JavaScript\nDESCRIPTION: This code snippet shows how to stream chat responses using the compiled LangGraph with memory. It demonstrates handling user input and iterating through the response stream.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/qa_chat_history.ipynb#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nlet inputs3 = { messages: [{ role: \"user\", content: \"What is Task Decomposition?\" }] };\n\nfor await (\n  const step of await graphWithMemory.stream(inputs3, threadConfig)\n) {\n    const lastMessage = step.messages[step.messages.length - 1];\n    prettyPrint(lastMessage);\n    console.log(\"-----\\n\");\n}\n```\n\n----------------------------------------\n\nTITLE: Binding Weather Tool to Model\nDESCRIPTION: Attaching the weather tool schema to the ChatOpenAI model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/stream_tool_client.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst modelWithTools = llm.bind({\n  tools: [\n    {\n      type: \"function\" as const,\n      function: {\n        name: \"get_weather\",\n        description: Weather.description,\n        parameters: zodToJsonSchema(Weather),\n      },\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Adding Documents to SupabaseVectorStore (TypeScript)\nDESCRIPTION: This snippet shows how to create `Document` objects with page content and metadata, and then add them to the previously instantiated `SupabaseVectorStore`. Unique IDs are provided for each document during insertion.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/supabase.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport type { Document } from \"@langchain/core/documents\";\n\nconst document1: Document = {\n  pageContent: \"The powerhouse of the cell is the mitochondria\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document2: Document = {\n  pageContent: \"Buildings are made out of brick\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document3: Document = {\n  pageContent: \"Mitochondria are made out of lipids\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document4: Document = {\n  pageContent: \"The 2024 Olympics are in Paris\",\n  metadata: { source: \"https://example.com\" }\n}\n\nconst documents = [document1, document2, document3, document4];\n\nawait vectorStore.addDocuments(documents, { ids: [\"1\", \"2\", \"3\", \"4\"] });\n```\n\n----------------------------------------\n\nTITLE: Using Upstash Vector Store as a Retriever - TypeScript/JavaScript\nDESCRIPTION: This example shows transforming a vector store into a retriever for streamlined querying in complex retrieval or RAG pipelines. You configure the retriever with a filter and a number of top results (k), then invoke it with a query string. The retriever handles search and optionally applies logic for result selection and downstream consumption.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/upstash.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst retriever = vectorStore.asRetriever({\n  // Optional filter\n  filter: filter,\n  k: 2,\n});\nawait retriever.invoke(\"biology\");\n```\n\n----------------------------------------\n\nTITLE: Basic Q&A Chain Implementation\nDESCRIPTION: Implementation of a basic Q&A chain using LangChain components including document loading, text splitting, and vector store creation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_sources.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport \"cheerio\";\nimport { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\"\nimport { OpenAIEmbeddings, ChatOpenAI } from \"@langchain/openai\";\nimport { pull } from \"langchain/hub\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { formatDocumentsAsString } from \"langchain/util/document\";\nimport { RunnableSequence, RunnablePassthrough } from \"@langchain/core/runnables\";\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\n\nconst loader = new CheerioWebBaseLoader(\n  \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n);\n\nconst docs = await loader.load();\n\nconst textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000, chunkOverlap: 200 });\nconst splits = await textSplitter.splitDocuments(docs);\nconst vectorStore = await MemoryVectorStore.fromDocuments(splits, new OpenAIEmbeddings());\n\n// Retrieve and generate using the relevant snippets of the blog.\nconst retriever = vectorStore.asRetriever();\nconst prompt = await pull<ChatPromptTemplate>(\"rlm/rag-prompt\");\nconst llm = new ChatOpenAI({ model: \"gpt-3.5-turbo\", temperature: 0 });\n\nconst ragChain = RunnableSequence.from([\n  {\n    context: retriever.pipe(formatDocumentsAsString),\n    question: new RunnablePassthrough(),\n  },\n  prompt,\n  llm,\n  new StringOutputParser()\n]);\n```\n\n----------------------------------------\n\nTITLE: Processing Streamed Chunks in LangChain\nDESCRIPTION: This snippet demonstrates how to use the stream() method to process chunks of output from a LangChain component asynchronously. It emphasizes the importance of efficient processing to avoid delays in upstream component production.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/streaming.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nfor await (const chunk of await component.stream(someInput)) {\n  // IMPORTANT: Keep the processing of each chunk as efficient as possible.\n  // While you're processing the current chunk, the upstream component is\n  // waiting to produce the next one. For example, if working with LangGraph,\n  // graph execution is paused while the current chunk is being processed.\n  // In extreme cases, this could even result in timeouts (e.g., when llm outputs are\n  // streamed from an API that has a timeout).\n  console.log(chunk);\n}\n```\n\n----------------------------------------\n\nTITLE: Semantic Similarity Routing in LangChain Expression Language\nDESCRIPTION: Illustrates how to use embeddings for routing queries to the most relevant prompt based on semantic similarity.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/routing.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport SemanticSimilarityExample from \"@examples/guides/expression_language/how_to_routing_semantic_similarity.ts\";\n\n<CodeBlock language=\"typescript\">{SemanticSimilarityExample}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Implementing executeTool Function\nDESCRIPTION: Function to execute the tool and stream data back to the client using createStreamableValue.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/stream_tool_client.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nexport async function executeTool(\n  input: string,\n) {\n  \"use server\";\n\n  const stream = createStreamableValue();\n\n  (async () => {\n    // ... (code for prompt and model definition)\n  })();\n\n  return { streamData: stream.value };\n}\n```\n\n----------------------------------------\n\nTITLE: Chaining ChatGoogleGenerativeAI with Prompt Templates\nDESCRIPTION: Shows how to create a chain by combining a prompt template with the ChatGoogleGenerativeAI model for dynamic translations.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/google_generativeai.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\"\n\nconst prompt = ChatPromptTemplate.fromMessages(\n    [\n        [\n            \"system\",\n            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n        ],\n        [\"human\", \"{input}\"],\n    ]\n)\n\nconst chain = prompt.pipe(llm);\nawait chain.invoke(\n    {\n        input_language: \"English\",\n        output_language: \"German\",\n        input: \"I love programming.\",\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Tools for Addition and Multiplication in JavaScript\nDESCRIPTION: This snippet defines two tools using the @langchain/core/tools package: an addition tool and a multiplication tool. Each tool has a name, description, and schema defined using Zod.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_choice.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { tool } from '@langchain/core/tools';\nimport { z } from 'zod';\n\nconst add = tool((input) => {\n    return `${input.a + input.b}`\n}, {\n    name: \"add\",\n    description: \"Adds a and b.\",\n    schema: z.object({\n        a: z.number(),\n        b: z.number(),\n    })\n})\n\nconst multiply = tool((input) => {\n    return `${input.a * input.b}`\n}, {\n    name: \"Multiply\",\n    description: \"Multiplies a and b.\",\n    schema: z.object({\n        a: z.number(),\n        b: z.number(),\n    })\n})\n\nconst tools = [add, multiply]\n```\n\n----------------------------------------\n\nTITLE: Initializing LangGraph Memory Persistence in JavaScript\nDESCRIPTION: This snippet demonstrates how to set up memory persistence using LangGraph's MemorySaver and compile a graph with a checkpointer. It also shows how to configure a thread for maintaining conversation state.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/qa_chat_history.ipynb#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport { MemorySaver } from \"@langchain/langgraph\";\n\nconst checkpointer = new MemorySaver();\nconst graphWithMemory = graphBuilder.compile({ checkpointer });\n\n// Specify an ID for the thread\nconst threadConfig = {\n    configurable: { thread_id: \"abc123\" },\n    streamMode: \"values\" as const\n};\n```\n\n----------------------------------------\n\nTITLE: Creating a Retrieval Chain\nDESCRIPTION: Combines the document chain with the retriever using RunnablePassthrough and RunnableSequence to automatically fetch and use relevant context when answering questions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_retrieval.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nimport type { BaseMessage } from \"@langchain/core/messages\";\nimport {\n  RunnablePassthrough,\n  RunnableSequence,\n} from \"@langchain/core/runnables\";\n\nconst parseRetrieverInput = (params: { messages: BaseMessage[] }) => {\n  return params.messages[params.messages.length - 1].content;\n};\n\nconst retrievalChain = RunnablePassthrough.assign({\n  context: RunnableSequence.from([parseRetrieverInput, retriever]),\n}).assign({\n  answer: documentChain,\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Unique Thread ID for Chatbot Conversations\nDESCRIPTION: Code to generate a unique thread ID using UUID, which is used to manage multiple conversation threads in the chatbot application.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/chatbot.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { v4 as uuidv4 } from \"uuid\";\n\nconst config = { configurable: { thread_id: uuidv4() } };\n```\n\n----------------------------------------\n\nTITLE: Auto-Streaming Chat Models in LangGraph with TypeScript\nDESCRIPTION: This example illustrates how LangChain automatically enables streaming mode for chat models when using the invoke method within a streamed application. It shows a node function where the model.invoke call is automatically switched to streaming mode when the overall application is being streamed.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/streaming.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst node = (state) => {\n    ...\n    // The code below uses the invoke method, but LangChain will\n    // automatically switch to streaming mode\n    // when it detects that the overall\n    // application is being streamed.\n    ai_message = model.invoke(state[\"messages\"])\n    ...\n\n    for await (const chunk of await compiledGraph.stream(..., { streamMode: \"messages\" })) {\n      // ... do something\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Using PGVectorStore as a Retriever\nDESCRIPTION: Transforms the vector store instance into a retriever object, allowing queries to be invoked in data processing chains with optional filtering.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/pgvector.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nconst retriever = vectorStore.asRetriever({\n  // Optional filter\n  filter: filter,\n  k: 2,\n});\nawait retriever.invoke(\"biology\");\n```\n\n----------------------------------------\n\nTITLE: Adding Documents to Vector Store\nDESCRIPTION: Demonstrates how to add documents to a vector store using unique IDs generated with the UUID library and specifying document content and metadata.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/pgvector.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { v4 as uuidv4 } from \"uuid\";\nimport type { Document } from \"@langchain/core/documents\";\n\nconst document1: Document = {\n  pageContent: \"The powerhouse of the cell is the mitochondria\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document2: Document = {\n  pageContent: \"Buildings are made out of brick\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document3: Document = {\n  pageContent: \"Mitochondria are made out of lipids\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document4: Document = {\n  pageContent: \"The 2024 Olympics are in Paris\",\n  metadata: { source: \"https://example.com\" }\n}\n\nconst documents = [document1, document2, document3, document4];\n\nconst ids = [uuidv4(), uuidv4(), uuidv4(), uuidv4()]\n\nawait vectorStore.addDocuments(documents, { ids: ids });\n```\n\n----------------------------------------\n\nTITLE: Creating a Tool with DynamicStructuredTool in TypeScript\nDESCRIPTION: Demonstrates how to use the DynamicStructuredTool class to create a tool that multiplies two numbers. This approach provides an alternative to the tool wrapper function.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/custom_tools.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { DynamicStructuredTool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n\nconst multiplyTool = new DynamicStructuredTool({\n  name: \"multiply\",\n  description: \"multiply two numbers together\",\n  schema: z.object({\n    a: z.number().describe(\"the first number to multiply\"),\n    b: z.number().describe(\"the second number to multiply\"),\n  }),\n  func: async ({ a, b }: { a: number; b: number; }) => {\n    return (a * b).toString();\n  },\n});\n\nawait multiplyTool.invoke({ a: 8, b: 9, });\n```\n\n----------------------------------------\n\nTITLE: Chaining ChatPerplexity with Prompt Templates\nDESCRIPTION: Example of chaining a ChatPerplexity model with a prompt template to create a reusable translation pipeline with variable languages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/perplexity.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\nconst prompt = ChatPromptTemplate.fromMessages([\n  [\n    \"system\",\n    \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n  ],\n  [\"human\", \"{input}\"],\n]);\n\nconst chain = prompt.pipe(llm);\nawait chain.invoke({\n  input_language: \"English\",\n  output_language: \"German\",\n  input: \"I love programming.\",\n});\n```\n\n----------------------------------------\n\nTITLE: Invoking a RunnableSequence Chain in TypeScript\nDESCRIPTION: Shows how to invoke a RunnableSequence chain with an input, which processes the input through the sequence of runnables.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/lcel.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst finalOutput = await chain.invoke(someInput);\n```\n\n----------------------------------------\n\nTITLE: Implementing Code Execution with ChatGoogleGenerativeAI\nDESCRIPTION: Shows how to use Google's Code Execution tool to generate, execute, and incorporate code results in model responses.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/google_generativeai.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport { CodeExecutionTool } from \"@google/generative-ai\";\nimport { ChatGoogleGenerativeAI } from \"@langchain/google-genai\";\n\nconst codeExecutionTool: CodeExecutionTool = {\n  codeExecution: {}, // Simply pass an empty object to enable it.\n};\nconst codeExecutionModel = new ChatGoogleGenerativeAI({\n  model: \"gemini-1.5-pro\",\n  temperature: 0,\n  maxRetries: 0,\n}).bindTools([codeExecutionTool]);\n\nconst codeExecutionResult = await codeExecutionModel.invoke(\"Use code execution to find the sum of the first and last 3 numbers in the following list: [1, 2, 3, 72638, 8, 727, 4, 5, 6]\");\n\nconsole.dir(codeExecutionResult.content, { depth: null });\n```\n\nLANGUAGE: python\nCODE:\n```\nconst codeExecutionExplanation = await codeExecutionModel.invoke([\n  codeExecutionResult,\n  {\n    role: \"user\",\n    content: \"Please explain the question I asked, the code you wrote, and the answer you got.\",\n  }\n])\n\nconsole.log(codeExecutionExplanation.content);\n```\n\n----------------------------------------\n\nTITLE: Basic Upstash Redis Chat Memory Implementation\nDESCRIPTION: Example showing basic implementation of Upstash Redis-backed chat memory with session management and TTL configuration\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/upstash_redis.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n{Example}\n```\n\n----------------------------------------\n\nTITLE: Creating Fallbacks for RunnableSequences with Different Models\nDESCRIPTION: Demonstration of creating fallbacks for entire sequences that use different model types (ChatOpenAI and regular OpenAI), showing how to handle different prompt requirements for each model type in the fallback chain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/fallbacks.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nChainExample\n```\n\n----------------------------------------\n\nTITLE: Alternative MessagesPlaceholder Syntax\nDESCRIPTION: Shows an alternative way to implement MessagesPlaceholder functionality using array syntax rather than explicit class instantiation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#2025-04-22_snippet_20\n\nLANGUAGE: typescript\nCODE:\n```\nconst promptTemplate = ChatPromptTemplate.fromMessages([\n  [\"system\", \"You are a helpful assistant\"],\n  [\"placeholder\", \"{msgs}\"], // <-- This is the changed part\n]);\n```\n\n----------------------------------------\n\nTITLE: Implementing Memory with LangGraph Checkpointing\nDESCRIPTION: Demonstrates how to add persistent memory to a LangGraph agent using checkpointing functionality. The code shows how to create a memory saver and configure thread-based persistence.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/migrate_agent.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { MemorySaver } from \"@langchain/langgraph\";\n\nconst checkpointer = new MemorySaver();\nconst appWithMemory = createReactAgent({\n  llm: llm,\n  tools: tools,\n  checkpointSaver: checkpointer\n});\n\nconst langGraphConfig = {\n  configurable: {\n    thread_id: \"test-thread\",\n  },\n};\n\nagentOutput = await appWithMemory.invoke(\n  {\n    messages: [\n      {\n        role: \"user\",\n        content: \"Hi, I'm polly! What's the output of magic_function of 3?\",\n      }\n    ],\n  },\n  langGraphConfig,\n);\n\nconsole.log(agentOutput.messages[agentOutput.messages.length - 1].content);\nconsole.log(\"---\");\n\nagentOutput = await appWithMemory.invoke(\n  {\n    messages: [\n      { role: \"user\", content: \"Remember my name?\" }\n    ]\n  },\n  langGraphConfig,\n);\n\nconsole.log(agentOutput.messages[agentOutput.messages.length - 1].content);\nconsole.log(\"---\");\n\nagentOutput = await appWithMemory.invoke(\n  {\n    messages: [\n      { role: \"user\", content: \"what was that output again?\" }\n    ]\n  },\n  langGraphConfig,\n);\n\nconsole.log(agentOutput.messages[agentOutput.messages.length - 1].content);\n```\n\n----------------------------------------\n\nTITLE: Implementing Query Decomposition with Structured Output in TypeScript\nDESCRIPTION: This code snippet demonstrates how to implement query decomposition using LangChain.js. It creates a model that breaks down a complex question into multiple sub-questions using OpenAI's GPT-4 model with structured output enforced through Zod schema validation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/retrieval.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { z } from \"zod\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { SystemMessage, HumanMessage } from \"@langchain/core/messages\";\n\n// Define a zod object for the structured output\nconst Questions = z.object({\n  questions: z\n    .array(z.string())\n    .describe(\"A list of sub-questions related to the input query.\"),\n});\n\n// Create an instance of the model and enforce the output structure\nconst model = new ChatOpenAI({ modelName: \"gpt-4\", temperature: 0 });\nconst structuredModel = model.withStructuredOutput(Questions);\n\n// Define the system prompt\nconst system = `You are a helpful assistant that generates multiple sub-questions related to an input question.\nThe goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation.`;\n\n// Pass the question to the model\nconst question =\n  \"What are the main components of an LLM-powered autonomous agent system?\";\nconst questions = await structuredModel.invoke([\n  new SystemMessage(system),\n  new HumanMessage(question),\n]);\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic Tool Using the tool Function in TypeScript\nDESCRIPTION: Demonstrates how to create a multiplication tool using the tool function from LangChain core. This example shows defining the function implementation, name, description, and schema using Zod for type validation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/tools.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n\nconst multiply = tool(\n  ({ a, b }: { a: number; b: number }): number => {\n    /**\n     * Multiply two numbers.\n     */\n    return a * b;\n  },\n  {\n    name: \"multiply\",\n    description: \"Multiply two numbers\",\n    schema: z.object({\n      a: z.number(),\n      b: z.number(),\n    }),\n  }\n);\n```\n\n----------------------------------------\n\nTITLE: Creating a Chatbot with Automatic History Management using LangGraph\nDESCRIPTION: Sets up a LangGraph StateGraph with a MemorySaver checkpointer to automatically manage conversation history. This approach eliminates the need for external message management by persisting state between invocations.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_memory.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { START, END, MessagesAnnotation, StateGraph, MemorySaver } from \"@langchain/langgraph\";\n\n\n// Define the function that calls the model\nconst callModel = async (state: typeof MessagesAnnotation.State) => {\n  const systemPrompt = \n    \"You are a helpful assistant. \" +\n    \"Answer all questions to the best of your ability.\";\n  const messages = [{ role: \"system\", content: systemPrompt }, ...state.messages];\n  const response = await llm.invoke(messages);\n  return { messages: response };\n};\n\nconst workflow = new StateGraph(MessagesAnnotation)\n// Define the node and edge\n  .addNode(\"model\", callModel)\n  .addEdge(START, \"model\")\n  .addEdge(\"model\", END);\n\n// Add simple in-memory checkpointer\nconst memory = new MemorySaver();\nconst app = workflow.compile({ checkpointer: memory });\n```\n\n----------------------------------------\n\nTITLE: Chaining with Prompt Template\nDESCRIPTION: Example of chaining a prompt template with the MistralAI model for language translation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/mistral.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { PromptTemplate } from \"@langchain/core/prompts\"\n\nconst prompt = PromptTemplate.fromTemplate(\"How to say {input} in {output_language}:\\n\")\n\nconst chain = prompt.pipe(llm);\nawait chain.invoke(\n  {\n    output_language: \"German\",\n    input: \"I love programming.\",\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Basic Custom Chat Model in TypeScript\nDESCRIPTION: Creates a simple custom chat model by extending SimpleChatModel class. The model echoes back the first n characters of the input and implements both regular and streaming interfaces.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/custom_chat.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport {\n  SimpleChatModel,\n  type BaseChatModelParams,\n} from \"@langchain/core/language_models/chat_models\";\nimport { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { AIMessageChunk, type BaseMessage } from \"@langchain/core/messages\";\nimport { ChatGenerationChunk } from \"@langchain/core/outputs\";\n\ninterface CustomChatModelInput extends BaseChatModelParams {\n  n: number;\n}\n\nclass CustomChatModel extends SimpleChatModel {\n  n: number;\n\n  constructor(fields: CustomChatModelInput) {\n    super(fields);\n    this.n = fields.n;\n  }\n\n  _llmType() {\n    return \"custom\";\n  }\n\n  async _call(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<string> {\n    if (!messages.length) {\n      throw new Error(\"No messages provided.\");\n    }\n    // Pass `runManager?.getChild()` when invoking internal runnables to enable tracing\n    // await subRunnable.invoke(params, runManager?.getChild());\n    if (typeof messages[0].content !== \"string\") {\n      throw new Error(\"Multimodal messages are not supported.\");\n    }\n    return messages[0].content.slice(0, this.n);\n  }\n\n  async *_streamResponseChunks(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): AsyncGenerator<ChatGenerationChunk> {\n    if (!messages.length) {\n      throw new Error(\"No messages provided.\");\n    }\n    if (typeof messages[0].content !== \"string\") {\n      throw new Error(\"Multimodal messages are not supported.\");\n    }\n    // Pass `runManager?.getChild()` when invoking internal runnables to enable tracing\n    // await subRunnable.invoke(params, runManager?.getChild());\n    for (const letter of messages[0].content.slice(0, this.n)) {\n      yield new ChatGenerationChunk({\n        message: new AIMessageChunk({\n          content: letter,\n        }),\n        text: letter,\n      });\n      // Trigger the appropriate callback for new chunks\n      await runManager?.handleLLMNewToken(letter);\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Searching an Existing Momento Vector Index in TypeScript\nDESCRIPTION: Shows how to connect to and perform similarity searches on a pre-existing Momento Vector Index collection in TypeScript. This example focuses solely on querying the index without adding new documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/momento_vector_index.mdx#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\n{ExistingExample}\n```\n\n----------------------------------------\n\nTITLE: Converting VectorStore to Retriever\nDESCRIPTION: Shows how to convert a vector store instance into a retriever using the asRetriever() method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/retrievers.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst vectorstore = new MyVectorStore();\nconst retriever = vectorstore.asRetriever();\n```\n\n----------------------------------------\n\nTITLE: ChatOpenAI Model Configuration\nDESCRIPTION: Setting up a ChatOpenAI model instance for use in the RAG chain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/bedrock-knowledge-bases.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o-mini\",\n  temperature: 0,\n});\n```\n\n----------------------------------------\n\nTITLE: Using Embedding Query in LangChain JavaScript\nDESCRIPTION: The snippet demonstrates how to utilize the embedQuery method from the embeddings module to convert a text string into a vector representation. This operation is a part of the integration of embedding models in LangChain to facilitate working with text data. Requires access to the embeddings module within a JavaScript environment, and expects a string input, outputting a vector representation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/index.mdx#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nawait embeddings.embedQuery(\"Hello, world!\");\n```\n\n----------------------------------------\n\nTITLE: Creating a Chain with LCEL in TypeScript\nDESCRIPTION: Example of declaratively creating a chain of Runnables using LangChain Expression Language (LCEL), which connects a prompt, chatModel, and outputParser together.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/runnables.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst chain = prompt.pipe(chatModel).pipe(outputParser);\n```\n\n----------------------------------------\n\nTITLE: Chaining AzureChatOpenAI with a Prompt Template\nDESCRIPTION: Shows how to create a chain by connecting a ChatPromptTemplate with the AzureChatOpenAI model to create a reusable translation pipeline.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/azure.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\"\n\nconst prompt = ChatPromptTemplate.fromMessages(\n    [\n        [\n            \"system\",\n            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n        ],\n        [\"human\", \"{input}\"],\n    ]\n)\n\nconst chain = prompt.pipe(llm);\nawait chain.invoke(\n    {\n        input_language: \"English\",\n        output_language: \"German\",\n        input: \"I love programming.\",\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Vector Store Creation and Retrieval\nDESCRIPTION: Example of creating a MemoryVectorStore, indexing a document, and retrieving similar content using embeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/ollama.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n// Create a vector store with a sample text\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n\nconst text = \"LangChain is the framework for building context-aware reasoning applications\";\n\nconst vectorstore = await MemoryVectorStore.fromDocuments(\n  [{ pageContent: text, metadata: {} }],\n  embeddings,\n);\n\n// Use the vector store as a retriever that returns a single document\nconst retriever = vectorstore.asRetriever(1);\n\n// Retrieve the most similar text\nconst retrievedDocuments = await retriever.invoke(\"What is LangChain?\");\n\nretrievedDocuments[0].pageContent;\n```\n\n----------------------------------------\n\nTITLE: Implementing Summary Memory in LangChain.js\nDESCRIPTION: This function calls the model, summarizes previous interactions when the chat history reaches a certain size, and manages the message history. It uses LangChain.js components like StateGraph and MessagesAnnotation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_memory.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport { START, END, MessagesAnnotation, StateGraph, MemorySaver } from \"@langchain/langgraph\";\nimport { RemoveMessage } from \"@langchain/core/messages\";\n\n\n// Define the function that calls the model\nconst callModel3 = async (state: typeof MessagesAnnotation.State) => {\n  const systemPrompt = \n    \"You are a helpful assistant. \" +\n    \"Answer all questions to the best of your ability. \" +\n    \"The provided chat history includes a summary of the earlier conversation.\";\n  const systemMessage = { role: \"system\", content: systemPrompt };\n  const messageHistory = state.messages.slice(0, -1); // exclude the most recent user input\n  \n  // Summarize the messages if the chat history reaches a certain size\n  if (messageHistory.length >= 4) {\n    const lastHumanMessage = state.messages[state.messages.length - 1];\n    // Invoke the model to generate conversation summary\n    const summaryPrompt = \n      \"Distill the above chat messages into a single summary message. \" +\n      \"Include as many specific details as you can.\";\n    const summaryMessage = await llm.invoke([\n      ...messageHistory,\n      { role: \"user\", content: summaryPrompt }\n    ]);\n\n    // Delete messages that we no longer want to show up\n    const deleteMessages = state.messages.map(m => new RemoveMessage({ id: m.id }));\n    // Re-add user message\n    const humanMessage = { role: \"user\", content: lastHumanMessage.content };\n    // Call the model with summary & response\n    const response = await llm.invoke([systemMessage, summaryMessage, humanMessage]);\n    return { messages: [summaryMessage, humanMessage, response, ...deleteMessages] };\n  } else {\n    const response = await llm.invoke([systemMessage, ...state.messages]);\n    return { messages: response };\n  }\n};\n\nconst workflow3 = new StateGraph(MessagesAnnotation)\n  // Define the node and edge\n  .addNode(\"model\", callModel3)\n  .addEdge(START, \"model\")\n  .addEdge(\"model\", END);\n\n// Add simple in-memory checkpointer\nconst app3 = workflow3.compile({ checkpointer: new MemorySaver() });\n```\n\n----------------------------------------\n\nTITLE: Setting Pinecone Environment Variables (TypeScript)\nDESCRIPTION: This snippet demonstrates how to set the required Pinecone environment variables (`PINECONE_API_KEY`, `PINECONE_INDEX`) and the optional `PINECONE_ENVIRONMENT` variable in a TypeScript environment using `process.env`. These variables are necessary for authenticating and connecting to the correct Pinecone index.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/pinecone.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.PINECONE_API_KEY = \"your-pinecone-api-key\";\nprocess.env.PINECONE_INDEX = \"your-pinecone-index\";\n\n// Optional\nprocess.env.PINECONE_ENVIRONMENT = \"your-pinecone-environment\";\n```\n\n----------------------------------------\n\nTITLE: Setting up RAG Chain with TavilySearchAPIRetriever\nDESCRIPTION: Comprehensive example demonstrating how to create a RAG (Retrieval-Augmented Generation) chain using the retriever, prompt templates, and LLM\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/tavily.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { RunnablePassthrough, RunnableSequence } from \"@langchain/core/runnables\";\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\n\nimport type { Document } from \"@langchain/core/documents\";\n\nconst prompt = ChatPromptTemplate.fromTemplate(`\nAnswer the question based only on the context provided.\n\nContext: {context}\n\nQuestion: {question}`);\n\nconst formatDocs = (docs: Document[]) => {\n  return docs.map((doc) => doc.pageContent).join(\"\\n\\n\");\n}\n\n// See https://js.langchain.com/docs/tutorials/rag\nconst ragChain = RunnableSequence.from([\n  {\n    context: retriever.pipe(formatDocs),\n    question: new RunnablePassthrough(),\n  },\n  prompt,\n  llm,\n  new StringOutputParser(),\n]);\n```\n\n----------------------------------------\n\nTITLE: Streaming JSON Output with JsonOutputParser in JavaScript\nDESCRIPTION: This snippet demonstrates how to stream JSON output using JsonOutputParser in a chain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { JsonOutputParser } from \"@langchain/core/output_parsers\"\n\nconst chain = model.pipe(new JsonOutputParser());\nconst stream = await chain.stream(\n  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key \"name\" and \"population\"`\n);\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n```\n\n----------------------------------------\n\nTITLE: Streaming Conversational Retrieval Chain in JavaScript\nDESCRIPTION: This code snippet demonstrates how to use the stream() method with a conversational retrieval chain in LangChain. It shows how to create a stream of responses based on a series of messages, including both human and AI messages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_retrieval.ipynb#2025-04-22_snippet_18\n\nLANGUAGE: javascript\nCODE:\n```\nconst stream = await conversationalRetrievalChain.stream({\n  messages: [\n    new HumanMessage(\"Can LangSmith help test my LLM applications?\"),\n    new AIMessage(\n      \"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.\"\n    ),\n    new HumanMessage(\"Tell me more!\"),\n  ],\n});\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n```\n\n----------------------------------------\n\nTITLE: Invoking LLM with Tool Results for Final Answer in JavaScript\nDESCRIPTION: This code invokes the LLM with the updated message history, including tool results, to generate a final answer to the original query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_results_pass_to_model.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nawait llmWithTools.invoke(messages);\n```\n\n----------------------------------------\n\nTITLE: Invoking Google Vertex AI Model in LangChain.js\nDESCRIPTION: Demonstrate how to invoke the VertexAI model with an input text to generate a completion.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/google_vertex_ai.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst inputText = \"VertexAI is an AI company that \"\n\nconst completion = await llm.invoke(inputText)\ncompletion\n```\n\n----------------------------------------\n\nTITLE: Querying with Scores in PGVectorStore\nDESCRIPTION: Shows how to perform a similarity search that returns both document results and their similarity scores, which can be used for more detailed analysis.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/pgvector.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nconst similaritySearchWithScoreResults = await vectorStore.similaritySearchWithScore(\"biology\", 2, filter)\n\nfor (const [doc, score] of similaritySearchWithScoreResults) {\n  console.log(`* [SIM=${score.toFixed(3)}] ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`);\n}\n```\n\n----------------------------------------\n\nTITLE: Basic ChatVertexAI Model Invocation\nDESCRIPTION: Example of invoking the ChatVertexAI model with system and human messages for translation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/google_vertex_ai.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst aiMsg = await llm.invoke([\n    [\n        \"system\",\n        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n    ],\n    [\"human\", \"I love programming.\"],\n])\naiMsg\n\nconsole.log(aiMsg.content)\n```\n\n----------------------------------------\n\nTITLE: Creating Tavily Search Tool\nDESCRIPTION: Code to create a Tavily search tool using the TavilySearchResults class from LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/agent_executor.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport \"cheerio\"; // This is required in notebooks to use the `CheerioWebBaseLoader`\nimport { TavilySearchResults } from \"@langchain/community/tools/tavily_search\"\n\nconst search = new TavilySearchResults({\n  maxResults: 2\n});\n\nawait search.invoke(\"what is the weather in SF\")\n```\n\n----------------------------------------\n\nTITLE: Chunking Documents with TokenTextSplitter in JavaScript\nDESCRIPTION: Splits input documents into smaller chunks using the TokenTextSplitter with a specified chunk size and no overlap to prepare for processing by the LLM.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/summarization.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nimport { TokenTextSplitter } from \"@langchain/textsplitters\";\n\nconst textSplitter = new TokenTextSplitter({\n  chunkSize: 1000,\n  chunkOverlap: 0,\n});\n\nconst splitDocs = await textSplitter.splitDocuments(docs)\nconsole.log(`Generated ${splitDocs.length} documents.`)\n```\n\n----------------------------------------\n\nTITLE: Converting a Runnable with Object Schema to a Tool using TypeScript\nDESCRIPTION: This example demonstrates converting a RunnableLambda with a Zod object schema into a Tool. The runnable takes an object with a number and an array of numbers as input and returns their product.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/convert_runnable_to_tool.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RunnableLambda } from \"@langchain/core/runnables\";\nimport { z } from \"zod\";\n\nconst schema = z.object({\n  a: z.number(),\n  b: z.array(z.number()),\n});\n\n\nconst runnable = RunnableLambda.from((input: z.infer<typeof schema>) => {\n  return input.a * Math.max(...input.b);\n});\n\nconst asTool = runnable.asTool({\n  name: \"My tool\",\n  description: \"Explanation of when to use the tool.\",\n  schema,\n});\n\nasTool.description\n```\n\n----------------------------------------\n\nTITLE: Initializing the MariaDB Vector Store with OpenAI Embeddings - TypeScript\nDESCRIPTION: This code snippet demonstrates how to initialize a MariaDBStore instance integrated with OpenAI embeddings for document storage and retrieval. It defines database connection options, sets the distance strategy, and calls MariaDBStore.initialize to set up the table if it doesn't exist. Dependencies include '@langchain/community', '@langchain/openai', and 'mariadb'. Expected inputs are valid MariaDB connection details and access to the configured database; outputs are an initialized vectorStore object.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/mariadb.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nimport {\n   DistanceStrategy,\n   MariaDBStore,\n} from \"@langchain/community/vectorstores/mariadb\";\nimport { PoolConfig } from \"mariadb\";\n\nconst config = {\n  connectionOptions: {\n    type: \"mariadb\",\n    host: \"127.0.0.1\",\n    port: 3306,\n    user: \"myuser\",\n    password: \"ChangeMe\",\n    database: \"api\",\n  } as PoolConfig,\n  distanceStrategy: 'EUCLIDEAN' as DistanceStrategy,\n};\nconst vectorStore = await MariaDBStore.initialize(\n  new OpenAIEmbeddings(),\n   config\n);\n\n```\n\n----------------------------------------\n\nTITLE: Invoking a Tool with ToolCall in JavaScript\nDESCRIPTION: This snippet shows how to invoke a tool using a ToolCall object, which includes additional information like the tool call ID. This method returns both the content and artifact from the tool's output.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_artifacts.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait generateRandomInts.invoke(\n  {\n    name: \"generate_random_ints\",\n    args: {min: 0, max: 9, size: 10},\n    id: \"123\", // Required\n    type: \"tool_call\", // Required\n  }\n);\n```\n\n----------------------------------------\n\nTITLE: Creating a Query-Transforming Retriever Chain\nDESCRIPTION: Builds a retriever chain that can handle both initial queries and followup questions by applying query transformation conditionally based on conversation context.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_retrieval.ipynb#2025-04-22_snippet_14\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RunnableBranch } from \"@langchain/core/runnables\";\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\n\nconst queryTransformingRetrieverChain = RunnableBranch.from([\n  [\n    (params: { messages: BaseMessage[] }) => params.messages.length === 1,\n    RunnableSequence.from([parseRetrieverInput, retriever]),\n  ],\n  queryTransformPrompt\n    .pipe(llm)\n    .pipe(new StringOutputParser())\n    .pipe(retriever),\n]).withConfig({ runName: \"chat_retriever_chain\" });\n```\n\n----------------------------------------\n\nTITLE: Setting up RAG Prompt Template\nDESCRIPTION: Demonstrates loading a RAG prompt template from the LangChain hub and testing it with example inputs.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport { pull } from \"langchain/hub\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\nconst promptTemplate = await pull<ChatPromptTemplate>(\"rlm/rag-prompt\");\n\n// Example:\nconst example_prompt = await promptTemplate.invoke(\n    { context: \"(context goes here)\", question: \"(question goes here)\" }\n)\nconst example_messages = example_prompt.messages\n\nconsole.assert(example_messages.length === 1);\nexample_messages[0].content\n```\n\n----------------------------------------\n\nTITLE: Composing Runnables with RunnableSequence\nDESCRIPTION: Demonstrates composing runnables using RunnableSequence.from() method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/lcel_cheatsheet.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { RunnableLambda, RunnableSequence } from \"@langchain/core/runnables\";\n\nconst runnable1 = RunnableLambda.from((x: any) => {\n  return { foo: x };\n});\n\nconst runnable2 = RunnableLambda.from((x: any) => [x].concat([x]));\n\nconst chain = RunnableSequence.from([\n  runnable1,\n  runnable2,\n]);\n\nawait chain.invoke(2);\n```\n\n----------------------------------------\n\nTITLE: Handling Long Inputs with Model Fallbacks\nDESCRIPTION: Implementation showing how to fallback to models with larger context windows when inputs exceed the token limits of the primary model, addressing one of the common limitations of LLMs in production.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/fallbacks.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nLongInputExample\n```\n\n----------------------------------------\n\nTITLE: Using SAP HANA Vector Store as a Retriever in LangchainJS RAG Chains (TypeScript)\nDESCRIPTION: This TypeScript code demonstrates how to integrate the SAP HANA vector store into a Retrieval Augmented Generation (RAG) chain using LangchainJS. It involves creating a `HanaDB` vector store instance, converting it into a retriever using `asRetriever()`, and then incorporating this retriever into a chain (like `createRetrievalChain`) along with a language model and a prompt to answer questions based on retrieved context from the SAP HANA database.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/hanavector.mdx#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI, OpenAIEmbeddings } from \"@langchain/openai\";\nimport { HanaDB } from \"@langchain/community/vectorstores/hana\";\nimport { Document } from \"@langchain/core/documents\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { createRetrievalChain } from \"langchain/chains/retrieval\";\nimport { createStuffDocumentsChain } from \"langchain/chains/combine_documents\";\n\n// Connection options for the database connection, read from environment variables\n// HANA_HOST, HANA_PORT, HANA_USER, HANA_PASSWORD\nconst connectionOptions = {\n  host: process.env.HANA_HOST ?? \"\",\n  port: process.env.HANA_PORT ?? \"\",\n  user: process.env.HANA_USER ?? \"\",\n  password: process.env.HANA_PASSWORD ?? \"\",\n};\n\n// Configuration for the vector store table\nconst vectorStoreTableConfig = {\n  tableName: \"MY_TABLE_NAME\",\n  schemaName: \"MY_SCHEMA_NAME\", // optional, default: current schema\n};\n\n// Create embeddings model\nconst embeddings = new OpenAIEmbeddings({});\n\n// Initialize the vector store (assuming it's already populated)\nconst vectorStore = new HanaDB(embeddings, {\n  connection: connectionOptions,\n  table: vectorStoreTableConfig,\n});\n\n// Add documents if the table is empty (for demonstration)\nawait vectorStore.addDocuments([\n  new Document({ pageContent: \"SAP HANA Cloud is a DBaaS offering.\" }),\n  new Document({ pageContent: \"Langchain JS is a framework.\" }),\n]);\n\n// Create a retriever from the vector store\nconst retriever = vectorStore.asRetriever();\n\n// Define the prompt template\nconst prompt = ChatPromptTemplate.fromTemplate(`Answer the following question based only on the provided context:\n\n<context>\n{context}\n</context>\n\nQuestion: {input}`);\n\n// Create a language model instance\nconst model = new ChatOpenAI({});\n\n// Create the document combining chain\nconst combineDocsChain = await createStuffDocumentsChain({ llm: model, prompt });\n\n// Create the main retrieval chain\nconst chain = await createRetrievalChain({\n  retriever,\n  combineDocsChain,\n});\n\n// Invoke the chain with a question\nconst response = await chain.invoke({ input: \"What is SAP HANA Cloud?\" });\n\nconsole.log(response);\n/*\n  {\n    input: 'What is SAP HANA Cloud?',\n    context: [\n      Document { pageContent: 'SAP HANA Cloud is a DBaaS offering.', metadata: {} }\n    ],\n    answer: 'SAP HANA Cloud is a DBaaS (Database as a Service) offering.'\n  }\n*/\n\n```\n\n----------------------------------------\n\nTITLE: Creating a Tool-Calling Agent\nDESCRIPTION: Code to create a tool-calling agent using the language model, tools, and prompt.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/agent_executor.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nimport { createToolCallingAgent } from \"langchain/agents\";\n\nconst agent = await createToolCallingAgent({ llm: model, tools, prompt })\n```\n\n----------------------------------------\n\nTITLE: Testing Multi-Turn Conversation with Retrieval Chain\nDESCRIPTION: Tests the conversational retrieval chain with a multi-turn conversation that includes a followup question, demonstrating its ability to maintain context across turns.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_retrieval.ipynb#2025-04-22_snippet_17\n\nLANGUAGE: javascript\nCODE:\n```\nawait conversationalRetrievalChain.invoke({\n  messages: [\n    new HumanMessage(\"Can LangSmith help test my LLM applications?\"),\n    new AIMessage(\n      \"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.\"\n    ),\n    new HumanMessage(\"Tell me more!\"),\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Extraction Prompt Template\nDESCRIPTION: Creates a chat prompt template with system instructions and placeholder for examples to improve extraction quality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/extraction_examples.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate, MessagesPlaceholder } from \"@langchain/core/prompts\";\n\nconst SYSTEM_PROMPT_TEMPLATE = `You are an expert extraction algorithm.\nOnly extract relevant information from the text.\nIf you do not know the value of an attribute asked to extract, you may omit the attribute's value.`;\n\nconst prompt = ChatPromptTemplate.fromMessages([\n  [\"system\", SYSTEM_PROMPT_TEMPLATE],\n  new MessagesPlaceholder(\"examples\"),\n  [\"human\", \"{text}\"]\n]);\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatOpenAI Model in JavaScript\nDESCRIPTION: This snippet shows how to import and initialize the ChatOpenAI model from LangChain, specifically using the GPT-4 model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/multimodal_prompts.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst model = new ChatOpenAI({ model: \"gpt-4o\" })\n```\n\n----------------------------------------\n\nTITLE: Configuring Self-Querying Retriever with LangChain.js\nDESCRIPTION: Sets up a self-querying retriever using LangChain.js, including defining attribute information, initializing a vector store, and creating the retriever instance.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/self_query.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { OpenAIEmbeddings, OpenAI } from \"@langchain/openai\";\nimport { FunctionalTranslator } from \"@langchain/core/structured_query\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { SelfQueryRetriever } from \"langchain/retrievers/self_query\";\nimport type { AttributeInfo } from \"langchain/chains/query_constructor\";\n\n/**\n * We define the attributes we want to be able to query on.\n * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.\n * We also provide a description of each attribute and the type of the attribute.\n * This is used to generate the query prompts.\n */\nconst attributeInfo: AttributeInfo[] = [\n  {\n    name: \"genre\",\n    description: \"The genre of the movie\",\n    type: \"string or array of strings\",\n  },\n  {\n    name: \"year\",\n    description: \"The year the movie was released\",\n    type: \"number\",\n  },\n  {\n    name: \"director\",\n    description: \"The director of the movie\",\n    type: \"string\",\n  },\n  {\n    name: \"rating\",\n    description: \"The rating of the movie (1-10)\",\n    type: \"number\",\n  },\n  {\n    name: \"length\",\n    description: \"The length of the movie in minutes\",\n    type: \"number\",\n  },\n];\n\n\n\n/**\n * Next, we instantiate a vector store. This is where we store the embeddings of the documents.\n * We also need to provide an embeddings object. This is used to embed the documents.\n */\nconst embeddings = new OpenAIEmbeddings();\nconst llm = new OpenAI();\nconst documentContents = \"Brief summary of a movie\";\nconst vectorStore = await MemoryVectorStore.fromDocuments(docs, embeddings);\nconst selfQueryRetriever = SelfQueryRetriever.fromLLM({\n  llm,\n  vectorStore,\n  documentContents,\n  attributeInfo,\n  /**\n   * We need to use a translator that translates the queries into a\n   * filter format that the vector store can understand. We provide a basic translator\n   * translator here, but you can create your own translator by extending BaseTranslator\n   * abstract class. Note that the vector store needs to support filtering on the metadata\n   * attributes you want to query on.\n   */\n  structuredQueryTranslator: new FunctionalTranslator(),\n});\n```\n\n----------------------------------------\n\nTITLE: Complete Example of Tool Creation and Binding\nDESCRIPTION: A comprehensive example that creates a multiply tool with documentation, binds it to a model that supports tool calling, and prepares it for use. The tool includes name, description, and schema definition.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/tool_calling.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst multiply = tool(\n  ({ a, b }: { a: number; b: number }): number => {\n    /**\n     * Multiply a and b.\n     *\n     * @param a - first number\n     * @param b - second number\n     * @returns The product of a and b\n     */\n    return a * b;\n  },\n  {\n    name: \"multiply\",\n    description: \"Multiply two numbers\",\n    schema: z.object({\n      a: z.number(),\n      b: z.number(),\n    }),\n  }\n);\n\nconst llmWithTools = toolCallingModel.bindTools([multiply]);\n```\n\n----------------------------------------\n\nTITLE: Chaining ChatWatsonx with a Prompt Template\nDESCRIPTION: Demonstrates how to chain the ChatWatsonx model with a prompt template to create a reusable language translation pipeline with variable inputs.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/ibm.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\nconst prompt = ChatPromptTemplate.fromMessages(\n  [\n    [\n      \"system\",\n      \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n    ],\n    [\"human\", \"{input}\"],\n  ]\n)\nconst chain = prompt.pipe(instance);\nawait chain.invoke(\n    {\n      input_language: \"English\",\n      output_language: \"German\",\n      input: \"I love programming.\",\n    }\n  )\n```\n\n----------------------------------------\n\nTITLE: Implementing D1-Backed Chat Memory in LangChain.js\nDESCRIPTION: This code snippet illustrates how to use Cloudflare D1 to store chat history in a LangChain.js application. It sets up a chat model, creates a D1-backed memory, and demonstrates a conversation flow.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/cloudflare_d1.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport { BufferMemory } from \"langchain/memory\";\nimport { CloudflareD1MessageHistory } from \"@langchain/cloudflare\";\n\nexport interface Env {\n  DB: D1Database;\n  ANTHROPIC_API_KEY: string;\n}\n\nexport default {\n  async fetch(\n    request: Request,\n    env: Env,\n    ctx: ExecutionContext\n  ): Promise<Response> {\n    const sessionId = \"example-session-id\";\n\n    const chatModel = new ChatAnthropic({\n      temperature: 0,\n      modelName: \"claude-2\",\n      anthropicApiKey: env.ANTHROPIC_API_KEY,\n    });\n\n    const memory = new BufferMemory({\n      chatHistory: new CloudflareD1MessageHistory({\n        tableName: \"chat_history\",\n        sessionId,\n        database: env.DB,\n      }),\n    });\n\n    const result1 = await chatModel.invoke(\n      \"Hello! I'm Claude. How can I assist you today?\"\n    );\n\n    await memory.saveContext(\n      { input: \"Hello! I'm Claude. How can I assist you today?\" },\n      { output: result1.content }\n    );\n\n    const result2 = await chatModel.invoke(\"What's your name?\");\n\n    await memory.saveContext(\n      { input: \"What's your name?\" },\n      { output: result2.content }\n    );\n\n    const history = await memory.loadMemoryVariables({});\n\n    return new Response(JSON.stringify(history));\n  },\n};\n```\n\n----------------------------------------\n\nTITLE: Using Operators for Advanced Filtering in TypeScript\nDESCRIPTION: Provides an example of advanced filtering using an operator other than the default equality (`=`). This filter uses the greater than operator (`>`) on the `create_datetime` field. Requires a corresponding SAI to be defined for the `create_datetime` column.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/cassandra.mdx#2025-04-22_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\n   { name: \"create_datetime\", operator: \">\", value: some_datetime_variable }\n```\n\n----------------------------------------\n\nTITLE: Creating and Streaming LCEL Chain in JavaScript\nDESCRIPTION: This code creates a chain using LangChain Expression Language (LCEL) and demonstrates streaming its output.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\nconst prompt = ChatPromptTemplate.fromTemplate(\"Tell me a joke about {topic}\");\n\nconst parser = new StringOutputParser();\n\nconst chain = prompt.pipe(model).pipe(parser);\n\nconst stream = await chain.stream({\n  topic: \"parrot\",\n});\n\nfor await (const chunk of stream) {\n  console.log(`${chunk}|`)\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Embeddings in JavaScript\nDESCRIPTION: This code snippet initializes OpenAI embeddings using the 'text-embedding-3-large' model for vector representation of text.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/sql_qa.ipynb#2025-04-22_snippet_18\n\nLANGUAGE: javascript\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddings = new OpenAIEmbeddings({model: \"text-embedding-3-large\"});\n```\n\n----------------------------------------\n\nTITLE: Database Table Initialization in PGVectorStore\nDESCRIPTION: This sample shows how to instantiate the PGVectorStore, initialize necessary database tables, and configure OpenAI embeddings and database connection settings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/pgvector.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport {\n  PGVectorStore,\n  DistanceStrategy,\n} from \"@langchain/community/vectorstores/pgvector\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { PoolConfig } from \"pg\";\n\nconst embeddings = new OpenAIEmbeddings({\n  model: \"text-embedding-3-small\",\n});\n\n// Sample config\nconst config = {\n  postgresConnectionOptions: {\n    type: \"postgres\",\n    host: \"127.0.0.1\",\n    port: 5433,\n    user: \"myuser\",\n    password: \"ChangeMe\",\n    database: \"api\",\n  } as PoolConfig,\n  tableName: \"testlangchainjs\",\n  columns: {\n    idColumnName: \"id\",\n    vectorColumnName: \"vector\",\n    contentColumnName: \"content\",\n    metadataColumnName: \"metadata\",\n  },\n  // supported distance strategies: cosine (default), innerProduct, or euclidean\n  distanceStrategy: \"cosine\" as DistanceStrategy,\n};\n\nconst vectorStore = await PGVectorStore.initialize(\n  embeddings,\n  config\n);\n```\n\n----------------------------------------\n\nTITLE: Using Multimodal Inputs with Chat Models in TypeScript\nDESCRIPTION: Example showing how to pass multimodal inputs (text and image) to a chat model using content blocks format. The code demonstrates creating a HumanMessage with both text description and image URL inputs.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/multimodality.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HumanMessage } from \"@langchain/core/messages\";\n\nconst message = new HumanMessage({\n  content: [\n    { type: \"text\", text: \"describe the weather in this image\" },\n    { type: \"image_url\", image_url: { url: image_url } },\n  ],\n});\nconst response = await model.invoke([message]);\n```\n\n----------------------------------------\n\nTITLE: Setting Up RAG Chain with Self-Query Retriever\nDESCRIPTION: Creating a RAG (Retrieval-Augmented Generation) chain that incorporates the Vectara self-query retriever with document formatting.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/vectara.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { RunnablePassthrough, RunnableSequence } from \"@langchain/core/runnables\";\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\n\nimport type { Document } from \"@langchain/core/documents\";\n\nconst prompt = ChatPromptTemplate.fromTemplate(`\nAnswer the question based only on the context provided.\n\nContext: {context}\n\nQuestion: {question}`);\n\nconst formatDocs = (docs: Document[]) => {\n  return docs.map((doc) => JSON.stringify(doc)).join(\"\\n\\n\");\n}\n\nconst ragChain = RunnableSequence.from([\n  {\n    context: selfQueryRetriever.pipe(formatDocs),\n    question: new RunnablePassthrough(),\n  },\n  prompt,\n  llm,\n  new StringOutputParser(),\n]);\n```\n\n----------------------------------------\n\nTITLE: Creating and Using a HumanMessage with Text Content in TypeScript\nDESCRIPTION: This snippet demonstrates how to create a HumanMessage object with text content and invoke a chat model with it. HumanMessage corresponds to the \"user\" role in chat models.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/messages.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HumanMessage } from \"@langchain/core/messages\";\n\nawait model.invoke([new HumanMessage(\"Hello, how are you?\")]);\n```\n\nLANGUAGE: typescript\nCODE:\n```\nawait model.invoke(\"Hello, how are you?\");\n```\n\n----------------------------------------\n\nTITLE: ParentDocumentRetriever with Score Threshold in TypeScript\nDESCRIPTION: Implementation of ParentDocumentRetriever using ScoreThresholdRetriever to set relevancy thresholds for retrieved documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/parent_document_retriever.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n{ExampleWithScoreThreshold}\n```\n\n----------------------------------------\n\nTITLE: Instantiating TogetherAIEmbeddings in JavaScript\nDESCRIPTION: Creates an instance of TogetherAIEmbeddings with a specified model. This object can be used for generating embeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/togetherai.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { TogetherAIEmbeddings } from \"@langchain/community/embeddings/togetherai\";\n\nconst embeddings = new TogetherAIEmbeddings({\n  model: \"togethercomputer/m2-bert-80M-8k-retrieval\", // Default value\n});\n```\n\n----------------------------------------\n\nTITLE: Setting up JsonOutputParser with ChatOpenAI\nDESCRIPTION: Demonstrates how to configure ChatOpenAI model with JsonOutputParser to generate and parse structured JSON responses. The example shows creating a joke generator that returns a setup and punchline in JSON format.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/output_parser_json.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\nconst model = new ChatOpenAI({\n  model: \"gpt-4o\",\n  temperature: 0,\n})\n\nimport { JsonOutputParser } from \"@langchain/core/output_parsers\"\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\"\n\n// Define your desired data structure. Only used for typing the parser output.\ninterface Joke {\n  setup: string\n  punchline: string\n}\n\n// A query and format instructions used to prompt a language model.\nconst jokeQuery = \"Tell me a joke.\";\nconst formatInstructions = \"Respond with a valid JSON object, containing two fields: 'setup' and 'punchline'.\"\n\n// Set up a parser + inject instructions into the prompt template.\nconst parser = new JsonOutputParser<Joke>()\n\nconst prompt = ChatPromptTemplate.fromTemplate(\n  \"Answer the user query.\\n{format_instructions}\\n{query}\\n\"\n);\n\nconst partialedPrompt = await prompt.partial({\n  format_instructions: formatInstructions\n});\n\nconst chain = partialedPrompt.pipe(model).pipe(parser);\n\nawait chain.invoke({ query: jokeQuery });\n```\n\n----------------------------------------\n\nTITLE: Creating a Chain with SerpAPI Tool and Chat Model\nDESCRIPTION: Example of building a chain that combines a prompt template, a tool-enabled LLM, and the SerpAPI tool to handle user questions requiring search capabilities.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/serpapi.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport { HumanMessage } from \"@langchain/core/messages\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { RunnableLambda } from \"@langchain/core/runnables\";\n\nconst prompt = ChatPromptTemplate.fromMessages(\n  [\n    [\"system\", \"You are a helpful assistant.\"],\n    [\"placeholder\", \"{messages}\"],\n  ]\n)\n\nconst llmWithTools = llm.bindTools([tool]);\n\nconst chain = prompt.pipe(llmWithTools);\n\nconst toolChain = RunnableLambda.from(\n  async (userInput: string, config) => {\n    const humanMessage = new HumanMessage(userInput,);\n    const aiMsg = await chain.invoke({\n      messages: [new HumanMessage(userInput)],\n    }, config);\n    const toolMsgs = await tool.batch(aiMsg.tool_calls, config);\n    return chain.invoke({\n      messages: [humanMessage, aiMsg, ...toolMsgs],\n    }, config);\n  }\n);\n\nconst toolChainResult = await toolChain.invoke(\"what is the current weather in sf?\");\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for LangSmith Tracing\nDESCRIPTION: This snippet demonstrates how to set environment variables for enabling LangSmith tracing, which is useful for inspecting LangChain applications.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/retrievers.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n\n# Reduce tracing latency if you are not in a serverless environment\n# export LANGCHAIN_CALLBACKS_BACKGROUND=true\n```\n\n----------------------------------------\n\nTITLE: Assembling Rewrite-Retrieve-Read Chain\nDESCRIPTION: Constructs the final Rewrite-Retrieve-Read chain, integrating the query rewriter with the retrieval and generation components for improved RAG performance.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/rewrite.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconst rewriteRetrieveReadChain = RunnableSequence.from([\n  {\n    context: RunnableSequence.from([\n      { x: new RunnablePassthrough() },\n      rewriter,\n      retriever,\n      formatDocs,\n    ]),\n    question: new RunnablePassthrough()\n  },\n  prompt,\n  model,\n  new StringOutputParser()\n]);\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for MistralAI\nDESCRIPTION: Configuration of environment variables for MistralAI API key and optional LangSmith tracing.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/mistral.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport MISTRAL_API_KEY=\"your-api-key\"\n```\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Creating History-Aware Retriever\nDESCRIPTION: Initialize retriever that considers conversation history when fetching relevant documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport { createHistoryAwareRetriever } from \"langchain/chains/history_aware_retriever\";\n\nconst historyAwareRetriever = await createHistoryAwareRetriever({\n  llm,\n  retriever,\n  rephrasePrompt: contextualizeQPrompt\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing StateGraph with MessagesAnnotation in JavaScript\nDESCRIPTION: This snippet demonstrates how to create a StateGraph using MessagesAnnotation, which is a built-in component in LangGraph for representing conversational state.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/qa_chat_history.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { MessagesAnnotation, StateGraph } from \"@langchain/langgraph\";\n\nconst graph = new StateGraph(MessagesAnnotation)\n```\n\n----------------------------------------\n\nTITLE: Invoking Custom JSON Extraction Chain in TypeScript\nDESCRIPTION: Demonstrates how to create and invoke a chain that uses a custom JSON extractor function with RunnableLambda to process model output and extract structured data from text about a person.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/structured_output.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RunnableLambda } from \"@langchain/core/runnables\";\n\nconst chain = prompt.pipe(model).pipe(new RunnableLambda({ func: extractJson }));\n\nawait chain.invoke({ query })\n```\n\n----------------------------------------\n\nTITLE: Invoking a String-Input Tool in TypeScript\nDESCRIPTION: This snippet shows how to invoke a tool converted from a string-input runnable chain. It passes a simple string 'b' which will be processed by the chained functions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/convert_runnable_to_tool.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nawait asTool.invoke(\"b\")\n```\n\n----------------------------------------\n\nTITLE: Reranking Documents Based on Query\nDESCRIPTION: Example demonstrating how to use the reranker to reorder a list of documents based on relevance to a query. This shows the core functionality of the MixedbreadAIReranker.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_compressors/mixedbread_ai.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst documents = [\n  { pageContent: \"To bake bread you need flour\" },\n  { pageContent: \"To bake bread you need yeast\" },\n  { pageContent: \"To eat bread you need nothing but good taste\" },\n];\nconst query = \"What do you need to bake bread?\";\nconst result = await reranker.compressDocuments(documents, query);\nconsole.log(result);\n```\n\n----------------------------------------\n\nTITLE: RAG Chain Implementation\nDESCRIPTION: Implementation of a RAG (Retrieval-Augmented Generation) chain combining the retriever with an LLM.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/bedrock-knowledge-bases.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { RunnablePassthrough, RunnableSequence } from \"@langchain/core/runnables\";\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\n\nimport type { Document } from \"@langchain/core/documents\";\n\nconst prompt = ChatPromptTemplate.fromTemplate(`\nAnswer the question based only on the context provided.\n\nContext: {context}\n\nQuestion: {question}`);\n\nconst formatDocs = (docs: Document[]) => {\n  return docs.map((doc) => doc.pageContent).join(\"\\n\\n\");\n}\n\nconst ragChain = RunnableSequence.from([\n  {\n    context: retriever.pipe(formatDocs),\n    question: new RunnablePassthrough(),\n  },\n  prompt,\n  llm,\n  new StringOutputParser(),\n]);\n```\n\n----------------------------------------\n\nTITLE: Composing Chat Prompt Template in JavaScript\nDESCRIPTION: This code shows how to compose a chat prompt template by combining different types of messages and templates. It creates a new prompt that includes system, human, and AI messages along with a template.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/prompts_composition.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { HumanMessagePromptTemplate } from \"@langchain/core/prompts\"\n\nconst newPrompt = HumanMessagePromptTemplate.fromTemplate([prompt, new HumanMessage(\"Hi\"), new AIMessage(\"what?\"), \"{input}\"])\n```\n\n----------------------------------------\n\nTITLE: Implementing Dynamic Few-Shot Examples with Semantic Similarity\nDESCRIPTION: TypeScript code showcasing the use of SemanticSimilarityExampleSelector to dynamically select relevant few-shot examples based on input similarity for SQL query generation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/sql_prompting.mdx#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\n<CodeBlock language=\"typescript\">{DynamicFewShotExample}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Implementing LLM API Error Fallbacks in LangChain\nDESCRIPTION: Example of setting up fallbacks between different language model providers (OpenAI and Anthropic) to handle potential API failures. The code demonstrates configuring models with retry options disabled to ensure proper fallback behavior.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/fallbacks.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nModelExample\n```\n\n----------------------------------------\n\nTITLE: Usage Example for Vercel Postgres Vector Store in TypeScript\nDESCRIPTION: This code block displays a complete example imported from `@examples/indexes/vector_stores/vercel_postgres/example.ts`. It demonstrates the practical application of the Vercel Postgres vector store within a LangChain.js context, likely covering initialization, adding documents, and performing similarity searches. The specific implementation details are contained within the imported file.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/vercel_postgres.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n{Example}\n```\n\n----------------------------------------\n\nTITLE: Chaining ChatPromptTemplate, LLM, and DuckDuckGoSearch Tool - JavaScript/TypeScript\nDESCRIPTION: This comprehensive chain snippet shows how to combine a chat prompt, an LLM instance, and the DuckDuckGoSearch tool using LangChain.js primitives. It sets up a chat prompt with system and placeholder messages, binds the tool to the LLM, constructs a chain pipeline, and defines a RunnableLambda for batch tool invocation and message aggregation. This enables dynamic user input to be processed through the LLM, invoke search tools, and return a combined response. Dependencies include '@langchain/core', '@langchain/openai', and '@langchain/community'. Inputs are user messages; outputs are combined AI and tool messages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/duckduckgo_search.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HumanMessage } from \"@langchain/core/messages\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { RunnableLambda } from \"@langchain/core/runnables\";\n\nconst prompt = ChatPromptTemplate.fromMessages(\n  [\n    [\"system\", \"You are a helpful assistant.\"],\n    [\"placeholder\", \"{messages}\"],\n  ]\n)\n\nconst llmWithTools = llm.bindTools([tool]);\n\nconst chain = prompt.pipe(llmWithTools);\n\nconst toolChain = RunnableLambda.from(\n  async (userInput: string, config) => {\n    const humanMessage = new HumanMessage(userInput,);\n    const aiMsg = await chain.invoke({\n      messages: [new HumanMessage(userInput)],\n    }, config);\n    const toolMsgs = await tool.batch(aiMsg.tool_calls, config);\n    return chain.invoke({\n      messages: [humanMessage, aiMsg, ...toolMsgs],\n    }, config);\n  }\n);\n\nconst toolChainResult = await toolChain.invoke(\"how many people have climbed mount everest?\");\n```\n\n----------------------------------------\n\nTITLE: Setting Up Chat Model for Examples\nDESCRIPTION: Initializes a ChatOpenAI instance using the gpt-4o model for use in the following examples.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_memory.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst llm = new ChatOpenAI({ model: \"gpt-4o\" })\n```\n\n----------------------------------------\n\nTITLE: Defining MongoDB Atlas Vector Search Index (JSON)\nDESCRIPTION: This JSON configuration defines a vector search index for MongoDB Atlas. It specifies the field ('embedding') containing the vectors, the number of dimensions (1536, typical for OpenAI embeddings), and the similarity metric ('euclidean'). This index structure is required for the vector store to perform searches.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/mongodb_atlas.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"fields\": [\n    {\n      \"numDimensions\": 1536,\n      \"path\": \"embedding\",\n      \"similarity\": \"euclidean\",\n      \"type\": \"vector\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Chat Model for LangChain Application\nDESCRIPTION: This snippet initializes a ChatOpenAI model with specific parameters for use in the LangChain application.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/sql_qa.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatOpenAI } from '@langchain/openai';\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o-mini\",\n  temperature: 0,\n})\n```\n\n----------------------------------------\n\nTITLE: Agent Function Declaration\nDESCRIPTION: Basic structure of the runAgent server function\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/stream_agent_client.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nexport async function runAgent(input: string) {\n  \"use server\";\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Apache Cassandra Connection in TypeScript\nDESCRIPTION: TypeScript code snippet for creating a configuration object to connect to an Apache Cassandra database. It includes contact points, local data center, and credentials.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/stores/cassandra_storage.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst configConnection = {\n  contactPoints: ['h1', 'h2'],\n  localDataCenter: 'datacenter1',\n  credentials: {\n    username: <...> as string,\n    password: <...> as string,\n  },\n};\n```\n\n----------------------------------------\n\nTITLE: Executing Search Tool Call in LangChain.js\nDESCRIPTION: This snippet illustrates a search tool call within an AI assistant's response. It includes the tool name, input arguments for the search query, and associated metadata.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_35\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"tavily_search_results_json\",\n  \"args\": {\n    \"input\": \"Oppenheimer 2023 film director age\"\n  },\n  \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\"\n}\n```\n\n----------------------------------------\n\nTITLE: Streaming with Mixed Streaming and Non-Streaming Components in JavaScript\nDESCRIPTION: This code demonstrates how a chain with both streaming and non-streaming components can still stream partial output after the last non-streaming step.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RunnablePassthrough, RunnableSequence } from \"@langchain/core/runnables\";\nimport type { Document } from \"@langchain/core/documents\";\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\n\nconst formatDocs = (docs: Document[]) => {\n  return docs.map((doc) => doc.pageContent).join(\"\\n-----\\n\")\n}\n\nconst retrievalChain = RunnableSequence.from([\n  {\n    context: retriever.pipe(formatDocs),\n    question: new RunnablePassthrough()\n  },\n  prompt,\n  model,\n  new StringOutputParser(),\n]);\n\nconst stream = await retrievalChain.stream(\"What is the powerhouse of the cell?\");\n\nfor await (const chunk of stream) {\n  console.log(`${chunk}|`);\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Multiple Files with Different Formats in LangChain.js\nDESCRIPTION: This code demonstrates how to use the MultiFileLoader to load documents from multiple files with different formats. It creates a loader instance that processes text, CSV, JSON, and JSONL files using their respective loaders with appropriate configurations.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/multi_file.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { MultiFileLoader } from \"langchain/document_loaders/fs/multi_file\";\nimport {\n  JSONLoader,\n  JSONLinesLoader,\n} from \"langchain/document_loaders/fs/json\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\nimport { CSVLoader } from \"langchain/document_loaders/fs/csv\";\n\nconst loader = new MultiFileLoader(\n  [\n    \"src/document_loaders/example_data/example/example.txt\",\n    \"src/document_loaders/example_data/example/example.csv\",\n    \"src/document_loaders/example_data/example2/example.json\",\n    \"src/document_loaders/example_data/example2/example.jsonl\",\n  ],\n  {\n    \".json\": (path) => new JSONLoader(path, \"/texts\"),\n    \".jsonl\": (path) => new JSONLinesLoader(path, \"/html\"),\n    \".txt\": (path) => new TextLoader(path),\n    \".csv\": (path) => new CSVLoader(path, \"text\"),\n  }\n);\nconst docs = await loader.load();\nconsole.log({ docs });\n```\n\n----------------------------------------\n\nTITLE: Reranking Documents with WatsonxRerank in Python\nDESCRIPTION: Demonstrates how to rerank documents using the WatsonxRerank class and obtain scores for each document.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_compressors/ibm.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport { WatsonxRerank } from \"@langchain/community/document_compressors/ibm\";\n\nconst reranker = new WatsonxRerank({\n  version: \"2024-05-31\",\n  serviceUrl: process.env.WATSONX_AI_SERVICE_URL,\n  projectId: process.env.WATSONX_AI_PROJECT_ID,\n  model: \"cross-encoder/ms-marco-minilm-l-12-v2\",\n});\nconst compressed = await reranker.rerank(result, query);\nconsole.log(compressed);\n```\n\n----------------------------------------\n\nTITLE: Basic ChatGroq Invocation\nDESCRIPTION: Demonstration of invoking the ChatGroq model with system and user messages for translation tasks.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/groq.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst aiMsg = await llm.invoke([\n    {\n      role: \"system\",\n      content: \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n    },\n    { role: \"user\", content: \"I love programming.\" },\n])\naiMsg\n```\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(aiMsg.content)\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatGPT Model Configuration\nDESCRIPTION: Sets up a ChatOpenAI instance with GPT-4 mini model and zero temperature for consistent outputs.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/qa_chat_history.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatOpenAI } from '@langchain/openai';\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o-mini\",\n  temperature: 0,\n})\n```\n\n----------------------------------------\n\nTITLE: Querying OpenSearch Vector Store and Using QA Chains - TypeScript\nDESCRIPTION: Shows how to query an OpenSearch vector store using similarity search and integrate it into a question-answering chain with Langchain.js. Dependencies include the OpenSearch client, OpenAIEmbeddings, OpenAI LLM, and VectorDBQAChain. Inputs are a query string and filter settings; outputs are either raw matching documents or QA responses. The code demonstrates both direct vector search with filtering and chain-based QA without filters. Requires prior indexed data. Limitations involve lack of metadata filtering for chain use.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/opensearch.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Client } from \"@opensearch-project/opensearch\";\nimport { VectorDBQAChain } from \"langchain/chains\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { OpenAI } from \"@langchain/openai\";\nimport { OpenSearchVectorStore } from \"@langchain/community/vectorstores/opensearch\";\n\nconst client = new Client({\n  nodes: [process.env.OPENSEARCH_URL ?? \"http://127.0.0.1:9200\"],\n});\n\nconst vectorStore = new OpenSearchVectorStore(new OpenAIEmbeddings(), {\n  client,\n});\n\n/* Search the vector DB independently with meta filters */\nconst results = await vectorStore.similaritySearch(\"hello world\", 1);\nconsole.log(JSON.stringify(results, null, 2));\n/* [\n    {\n      \"pageContent\": \"Hello world\",\n      \"metadata\": {\n        \"id\": 2\n      }\n    }\n  ] */\n\n/* Use as part of a chain (currently no metadata filters) */\nconst model = new OpenAI();\nconst chain = VectorDBQAChain.fromLLM(model, vectorStore, {\n  k: 1,\n  returnSourceDocuments: true,\n});\nconst response = await chain.call({ query: \"What is opensearch?\" });\n\nconsole.log(JSON.stringify(response, null, 2));\n/*\n  {\n    \"text\": \" Opensearch is a collection of technologies that allow search engines to publish search results in a standard format, making it easier for users to search across multiple sites.\",\n    \"sourceDocuments\": [\n      {\n        \"pageContent\": \"What's this?\",\n        \"metadata\": {\n          \"id\": 3\n        }\n      }\n    ]\n  }\n  */\n```\n\n----------------------------------------\n\nTITLE: Implementing RunnableWithMessageHistory for LangChain Agent in JavaScript\nDESCRIPTION: This snippet demonstrates how to implement RunnableWithMessageHistory to automatically track conversation history for the LangChain agent. It sets up a message store and configures the agent to use it for maintaining stateful conversations.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/agent_executor.ipynb#2025-04-22_snippet_19\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ChatMessageHistory } from \"@langchain/community/stores/message/in_memory\";\nimport { BaseChatMessageHistory } from \"@langchain/core/chat_history\";\nimport { RunnableWithMessageHistory } from \"@langchain/core/runnables\";\n\nconst store = {};\n\nfunction getMessageHistory(sessionId: string): BaseChatMessageHistory {\n  if (!(sessionId in store)) {\n    store[sessionId] = new ChatMessageHistory();\n  }\n  return store[sessionId];\n}\n\nconst agentWithChatHistory = new RunnableWithMessageHistory({\n  runnable: agentExecutor,\n  getMessageHistory,\n  inputMessagesKey: \"input\",\n  historyMessagesKey: \"chat_history\",\n})\n\nawait agentWithChatHistory.invoke(\n  { input: \"hi! I'm bob\" },\n  { configurable: { sessionId: \"<foo>\" }},\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Documents with IDs to Vector Store in LangChain\nDESCRIPTION: This snippet demonstrates adding documents with specific IDs to a vector store. Providing IDs allows for updating existing documents rather than adding duplicates.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/vectorstores.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nawait vectorStore.addDocuments(documents, { ids: [\"doc1\", \"doc2\"] });\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatOpenAI Model for Tool Integration\nDESCRIPTION: Sets up a ChatOpenAI instance to be used with the TavilySearchResults tool in a chain or agent.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/tavily_search_community.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n// @lc-docs-hide-cell\n\nimport { ChatOpenAI } from \"@langchain/openai\"\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o\",\n  temperature: 0,\n})\n```\n\n----------------------------------------\n\nTITLE: Fetching SQL Query Prompt Template from LangChain Hub\nDESCRIPTION: This code fetches a SQL query prompt template from the LangChain Hub and logs its contents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/sql_qa.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport { pull } from \"langchain/hub\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\nconst queryPromptTemplate = await pull<ChatPromptTemplate>(\"langchain-ai/sql-query-system-prompt\");\n\nqueryPromptTemplate.promptMessages.forEach(message => {\n    console.log(message.lc_kwargs.prompt.template);\n});\n```\n\n----------------------------------------\n\nTITLE: Querying Similarity Search with Scores - TypeScript\nDESCRIPTION: Executes similaritySearchWithScore on the vector store to fetch documents similar to 'biology' and their associated similarity scores. Results are tuples of [Document, score], and the code logs both the similarity score and document metadata. Expects an initialized vectorStore and valid query term; output is sent to the console.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/mariadb.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst similaritySearchWithScoreResults = await vectorStore.similaritySearchWithScore(\"biology\", 2)\n\nfor (const [doc, score] of similaritySearchWithScoreResults) {\n  console.log(`* [SIM=${score.toFixed(3)}] ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`);\n}\n\n```\n\n----------------------------------------\n\nTITLE: Setting up PostgreSQL Database Schema for Supabase Vector Store (SQL)\nDESCRIPTION: This SQL script prepares a PostgreSQL database for use with SupabaseVectorStore. It enables the `pgvector` extension, creates a `documents` table to store content, metadata, and embeddings, and defines a `match_documents` function for performing similarity searches with filtering.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/supabase.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n-- Enable the pgvector extension to work with embedding vectors\ncreate extension vector;\n\n-- Create a table to store your documents\ncreate table documents (\n  id bigserial primary key,\n  content text, -- corresponds to Document.pageContent\n  metadata jsonb, -- corresponds to Document.metadata\n  embedding vector(1536) -- 1536 works for OpenAI embeddings, change if needed\n);\n\n-- Create a function to search for documents\ncreate function match_documents (\n  query_embedding vector(1536),\n  match_count int DEFAULT null,\n  filter jsonb DEFAULT '{}'\n) returns table (\n  id bigint,\n  content text,\n  metadata jsonb,\n  embedding jsonb,\n  similarity float\n)\nlanguage plpgsql\nas $$\n#variable_conflict use_column\nbegin\n  return query\n  select\n    id,\n    content,\n    metadata,\n    (embedding::text)::jsonb as embedding,\n    1 - (documents.embedding <=> query_embedding) as similarity\n  from documents\n  where metadata @> filter\n  order by documents.embedding <=> query_embedding\n  limit match_count;\nend;\n$$;\n```\n\n----------------------------------------\n\nTITLE: Function Calling with Ollama\nDESCRIPTION: Example of passing functions to the Ollama model similar to OpenAI's function calling API\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/ollama_functions.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport CodeBlock from \"@theme/CodeBlock\";\nimport OllamaFunctionsCalling from \"@examples/models/chat/ollama_functions/function_calling.ts\";\n```\n\n----------------------------------------\n\nTITLE: Configuring AzureOpenAIEmbeddings Model Instance - TypeScript\nDESCRIPTION: This code creates a new AzureOpenAIEmbeddings model by passing required Azure account configuration details as parameters to the constructor. The parameters now require 'azureOpenAIApiInstanceName' instead of 'azureOpenAIEndpoint', and a new 'azureOpenAIApiVersion' field has been added. The model object can be used to generate embeddings once configured with valid credentials and deployment information. Developers must replace the placeholder strings with their own Azure details.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/azure_openai.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = new AzureOpenAIEmbeddings({\n  azureOpenAIApiKey: \"<your_key>\",\n  azureOpenAIApiInstanceName: \"<your_instance_name>\",\n  azureOpenAIApiEmbeddingsDeploymentName:\n    \"<your_embeddings_deployment_name>\",\n  azureOpenAIApiVersion: \"<api_version>\",\n});\n```\n\n----------------------------------------\n\nTITLE: Refreshing and Retrieving Graph Schema\nDESCRIPTION: This snippet shows how to refresh the graph schema and retrieve the updated schema information. This is important for ensuring the LLM has up-to-date information about the graph structure.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/graph.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nawait graph.refreshSchema()\nconsole.log(graph.getSchema())\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatOpenAI Model\nDESCRIPTION: Creates a ChatOpenAI instance with the GPT-4o model and temperature set to 0 for consistent, deterministic outputs.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_retrieval.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// @lc-docs-hide-cell\n\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o\",\n  temperature: 0,\n});\n```\n\n----------------------------------------\n\nTITLE: Custom Dimensions Embedding\nDESCRIPTION: Creating embeddings with custom dimensions (1024) using text-embedding-3-large model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/openai.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddings1024 = new OpenAIEmbeddings({\n  model: \"text-embedding-3-large\",\n  dimensions: 1024,\n});\n\nconst vectors1024 = await embeddings1024.embedDocuments([\"some text\"]);\nconsole.log(vectors1024[0].length);\n```\n\n----------------------------------------\n\nTITLE: Filtering Node and Relationship Types in LLMGraphTransformer\nDESCRIPTION: This snippet demonstrates how to create an LLMGraphTransformer with specific allowed node and relationship types, and convert text to filtered graph documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/graph_constructing.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst llmGraphTransformerFiltered = new LLMGraphTransformer({\n    llm: model,\n    allowedNodes: [\"PERSON\", \"COUNTRY\", \"ORGANIZATION\"],\n    allowedRelationships:[\"NATIONALITY\", \"LOCATED_IN\", \"WORKED_AT\", \"SPOUSE\"],\n    strictMode:false\n});\n\nconst result_filtered = await llmGraphTransformerFiltered.convertToGraphDocuments([\n    new Document({ pageContent: text }),\n]);\n\nconsole.log(`Nodes: ${result_filtered[0].nodes.length}`);\nconsole.log(`Relationships:${result_filtered[0].relationships.length}`);\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG Application Steps\nDESCRIPTION: Implements the retrieval and generation steps for the RAG application using LangGraph nodes.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport { concat } from \"@langchain/core/utils/stream\";\n\nconst retrieve = async (state: typeof InputStateAnnotation.State) => {\n  const retrievedDocs = await vectorStore.similaritySearch(state.question)\n  return { context: retrievedDocs };\n};\n\n\nconst generate = async (state: typeof StateAnnotation.State) => {\n  const docsContent = state.context.map(doc => doc.pageContent).join(\"\\n\");\n  const messages = await promptTemplate.invoke({ question: state.question, context: docsContent });\n  const response = await llm.invoke(messages);\n  return { answer: response.content };\n};\n```\n\n----------------------------------------\n\nTITLE: Implementing State Graph with Model Callback\nDESCRIPTION: Shows how to set up a StateGraph with model callback function that uses the prompt template to process messages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/chatbot.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport { START, END, MessagesAnnotation, StateGraph, MemorySaver } from \"@langchain/langgraph\";\n\nconst callModel2 = async (state: typeof MessagesAnnotation.State) => {\n  const prompt = await promptTemplate.invoke(state)\n  const response = await llm.invoke(prompt);\n  return { messages: [response] };\n};\n\nconst workflow2 = new StateGraph(MessagesAnnotation)\n  .addNode(\"model\", callModel2)\n  .addEdge(START, \"model\")\n  .addEdge(\"model\", END);\n\nconst app2 = workflow2.compile({ checkpointer: new MemorySaver() });\n```\n\n----------------------------------------\n\nTITLE: Adding Documents to MemoryVectorStore (TypeScript)\nDESCRIPTION: Defines three `Document` objects, each containing `pageContent` (the text) and `metadata` (additional information, here a source URL). These documents are collected into an array and added to the previously instantiated `vectorStore` using the asynchronous `addDocuments` method. This populates the in-memory store with data.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/memory.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport type { Document } from \"@langchain/core/documents\";\n\nconst document1: Document = {\n  pageContent: \"The powerhouse of the cell is the mitochondria\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document2: Document = {\n  pageContent: \"Buildings are made out of brick\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document3: Document = {\n  pageContent: \"Mitochondria are made out of lipids\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst documents = [document1, document2, document3];\n\nawait vectorStore.addDocuments(documents);\n```\n\n----------------------------------------\n\nTITLE: Vector Store Creation and Retrieval\nDESCRIPTION: Example of creating a MemoryVectorStore from documents and using it as a retriever.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/openai.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n// Create a vector store with a sample text\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n\nconst text = \"LangChain is the framework for building context-aware reasoning applications\";\n\nconst vectorstore = await MemoryVectorStore.fromDocuments(\n  [{ pageContent: text, metadata: {} }],\n  embeddings,\n);\n\n// Use the vector store as a retriever that returns a single document\nconst retriever = vectorstore.asRetriever(1);\n\n// Retrieve the most similar text\nconst retrievedDocuments = await retriever.invoke(\"What is LangChain?\");\n\nretrievedDocuments[0].pageContent;\n```\n\n----------------------------------------\n\nTITLE: Falling Back to Better Models for Improved Output Formatting\nDESCRIPTION: Example of implementing a fallback strategy that starts with a faster, cheaper model (GPT-3.5) but falls back to a more capable model (GPT-4) when specific formatting requirements like JSON output aren't met by the initial model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/fallbacks.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nBetterModelExample\n```\n\n----------------------------------------\n\nTITLE: Forcing Multiplication Tool Usage in LangChain.js\nDESCRIPTION: This code demonstrates how to force the LLM to use the multiplication tool by setting the tool_choice parameter to \"Multiply\". It then invokes the model with a prompt and logs the result.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_choice.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst llmForcedToMultiply = llm.bindTools(tools, {\n  tool_choice: \"Multiply\",\n})\nconst multiplyResult = await llmForcedToMultiply.invoke(\"what is 2 + 4\");\nconsole.log(JSON.stringify(multiplyResult.tool_calls, null, 2));\n```\n\n----------------------------------------\n\nTITLE: Chaining ChatCerebras with Prompt Templates\nDESCRIPTION: Demonstrates how to create a chain by combining a ChatPromptTemplate with a ChatCerebras model for language translation with configurable input and output languages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/cerebras.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\"\n\nconst prompt = ChatPromptTemplate.fromMessages(\n    [\n        [\n            \"system\",\n            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n        ],\n        [\"human\", \"{input}\"],\n    ]\n)\n\nconst chain = prompt.pipe(llm);\nawait chain.invoke(\n    {\n        input_language: \"English\",\n        output_language: \"German\",\n        input: \"I love programming.\",\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Storing Documents in VectorStore\nDESCRIPTION: Shows how to add document splits to a vector store for later retrieval.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nawait vectorStore.addDocuments(allSplits)\n```\n\n----------------------------------------\n\nTITLE: Multiple Tool Calls Example\nDESCRIPTION: Demonstrates how to make multiple tool calls in a single model invocation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_calling.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst res = await llmWithTools.invoke(\"What is 3 * 12? Also, what is 11 + 49?\");\n\nres.tool_calls;\n```\n\n----------------------------------------\n\nTITLE: Defining Tools and Model for Streaming in LangChain.js\nDESCRIPTION: This snippet defines two tools (add and multiply) and sets up a ChatOpenAI model with tool binding. It uses Zod for schema validation and the @langchain/core and @langchain/openai packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_streaming.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { z } from \"zod\";\nimport { tool } from \"@langchain/core/tools\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst addTool = tool(async (input) => {\n  return input.a + input.b;\n}, {\n  name: \"add\",\n  description: \"Adds a and b.\",\n  schema: z.object({\n    a: z.number(),\n    b: z.number(),\n  }),\n});\n\nconst multiplyTool = tool(async (input) => {\n  return input.a * input.b;\n}, {\n  name: \"multiply\",\n  description: \"Multiplies a and b.\",\n  schema: z.object({\n    a: z.number(),\n    b: z.number(),\n  }),\n});\n\nconst tools = [addTool, multiplyTool];\n\nconst model = new ChatOpenAI({\n  model: \"gpt-4o\",\n  temperature: 0,\n});\n\nconst modelWithTools = model.bindTools(tools);\n```\n\n----------------------------------------\n\nTITLE: Using FewShotPromptTemplate with ExampleSelector in JavaScript\nDESCRIPTION: Creates a FewShotPromptTemplate using the SemanticSimilarityExampleSelector for dynamic example selection based on input similarity.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot_examples.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst prompt = new FewShotPromptTemplate({\n    exampleSelector,\n    examplePrompt,\n    suffix: \"Question: {input}\",\n    inputVariables: [\"input\"],\n})\n\nconst formatted = await prompt.invoke({ input: \"Who was the father of Mary Ball Washington?\" });\nconsole.log(formatted.toString())\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG Pipeline\nDESCRIPTION: Complete implementation of a RAG application including document loading, chunking, indexing, and question-answering chain using LangGraph.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport \"cheerio\";\nimport { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\";\nimport { Document } from \"@langchain/core/documents\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { pull } from \"langchain/hub\";\nimport { Annotation, StateGraph } from \"@langchain/langgraph\";\nimport { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\n\n\n// Load and chunk contents of blog\nconst pTagSelector = \"p\";\nconst cheerioLoader = new CheerioWebBaseLoader(\n  \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n  {\n    selector: pTagSelector\n  }\n);\n\nconst docs = await cheerioLoader.load();\n\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 1000, chunkOverlap: 200\n});\nconst allSplits = await splitter.splitDocuments(docs);\n\n\n// Index chunks\nawait vectorStore.addDocuments(allSplits)\n\n// Define prompt for question-answering\nconst promptTemplate = await pull<ChatPromptTemplate>(\"rlm/rag-prompt\");\n\n// Define state for application\nconst InputStateAnnotation = Annotation.Root({\n  question: Annotation<string>,\n});\n\nconst StateAnnotation = Annotation.Root({\n  question: Annotation<string>,\n  context: Annotation<Document[]>,\n  answer: Annotation<string>,\n});\n\n// Define application steps\nconst retrieve = async (state: typeof InputStateAnnotation.State) => {\n  const retrievedDocs = await vectorStore.similaritySearch(state.question)\n  return { context: retrievedDocs };\n};\n\n\nconst generate = async (state: typeof StateAnnotation.State) => {\n  const docsContent = state.context.map(doc => doc.pageContent).join(\"\\n\");\n  const messages = await promptTemplate.invoke({ question: state.question, context: docsContent });\n  const response = await llm.invoke(messages);\n  return { answer: response.content };\n};\n\n\n// Compile application and test\nconst graph = new StateGraph(StateAnnotation)\n  .addNode(\"retrieve\", retrieve)\n  .addNode(\"generate\", generate)\n  .addEdge(\"__start__\", \"retrieve\")\n  .addEdge(\"retrieve\", \"generate\")\n  .addEdge(\"generate\", \"__end__\")\n  .compile();\n```\n\n----------------------------------------\n\nTITLE: Defining a Tool with Content and Artifact Output in JavaScript\nDESCRIPTION: This snippet demonstrates how to define a tool that generates random integers, returning both a content message and an artifact (the array of integers). It uses Zod for schema validation and the LangChain tool decorator.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_artifacts.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { z } from \"zod\";\nimport { tool } from \"@langchain/core/tools\";\n\nconst randomIntToolSchema = z.object({\n  min: z.number(),\n  max: z.number(),\n  size: z.number(),\n});\n\nconst generateRandomInts = tool(async ({ min, max, size }) => {\n  const array: number[] = [];\n  for (let i = 0; i < size; i++) {\n    array.push(Math.floor(Math.random() * (max - min + 1)) + min);\n  }\n  return [\n    `Successfully generated array of ${size} random ints in [${min}, ${max}].`,\n    array,\n  ];\n}, {\n  name: \"generateRandomInts\",\n  description: \"Generate size random ints in the range [min, max].\",\n  schema: randomIntToolSchema,\n  responseFormat: \"content_and_artifact\",\n});\n```\n\n----------------------------------------\n\nTITLE: Streaming Retrieval Chain with RunnablePassthrough.assign() in TypeScript\nDESCRIPTION: This example shows how to use RunnablePassthrough.assign() in a retrieval chain to immediately return source documents. It demonstrates the streaming capability, allowing values to pass through as soon as they are available.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/assign.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { RunnablePassthrough, RunnableSequence } from \"@langchain/core/runnables\";\nimport { ChatOpenAI, OpenAIEmbeddings } from \"@langchain/openai\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n\nconst vectorstore = await MemoryVectorStore.fromDocuments([\n  { pageContent: \"harrison worked at kensho\", metadata: {} }\n], new OpenAIEmbeddings());\n\nconst retriever = vectorstore.asRetriever();\n\nconst template = `Answer the question based only on the following context:\n{context}\n\nQuestion: {question}\n`;\n\nconst prompt = ChatPromptTemplate.fromTemplate(template);\n\nconst model = new ChatOpenAI({ model: \"gpt-4o\" });\n\nconst generationChain = prompt.pipe(model).pipe(new StringOutputParser());\n\nconst retrievalChain = RunnableSequence.from([\n  {\n    context: retriever.pipe((docs) => docs[0].pageContent),\n    question: new RunnablePassthrough()\n  },\n  RunnablePassthrough.assign({ output: generationChain }),\n]);\n\nconst stream = await retrievalChain.stream(\"where did harrison work?\");\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n```\n\n----------------------------------------\n\nTITLE: Using Google Calendar Tool with LangChain Agent (TypeScript)\nDESCRIPTION: This TypeScript code block demonstrates the integration and usage of the Google Calendar Tool within a LangChain agent. It imports an example (`ToolExample`) that likely showcases initializing the tool, potentially setting up credentials (implied prerequisite), and invoking an agent to interact with Google Calendar for tasks like creating or viewing events.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/google_calendar.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n{ToolExample}\n```\n\n----------------------------------------\n\nTITLE: Configuring GraphCypherQAChain with Custom Prompt\nDESCRIPTION: Setting up the GraphCypherQAChain with ChatOpenAI and custom prompt template for generating Cypher queries.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/graph_prompting.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { GraphCypherQAChain } from \"langchain/chains/graph_qa/cypher\";\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-3.5-turbo\",\n  temperature: 0,\n});\nconst chain = GraphCypherQAChain.fromLLM(\n  {\n    graph,\n    llm,\n    cypherPrompt: promptWithExampleSelector,\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing SelfQueryRetriever for Metadata Filtering in LangChain\nDESCRIPTION: This code demonstrates how to use the SelfQueryRetriever to convert natural language queries into metadata filters. It initializes a retriever with a language model, vector store, document content description, and metadata schema to enable semantic search with metadata filtering.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/retrieval.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { SelfQueryRetriever } from \"langchain/retrievers/self_query\";\nimport { AttributeInfo } from \"langchain/chains/query_constructor\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst attributeInfo: AttributeInfo[] = schemaForMetadata;\nconst documentContents = \"Brief summary of a movie\";\nconst llm = new ChatOpenAI({ temperature: 0 });\nconst retriever = SelfQueryRetriever.fromLLM({\n  llm,\n  vectorStore,\n  documentContents,\n  attributeInfo,\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a ChatPromptTemplate for Multiple Image Comparison\nDESCRIPTION: This snippet shows how to create a ChatPromptTemplate for comparing two images. It includes a system message and a user message with placeholders for two image URLs.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/multimodal_prompts.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst promptWithMultipleImages = ChatPromptTemplate.fromMessages(\n    [\n        [\"system\", \"compare the two pictures provided\"],\n        [\n            \"user\",\n            [\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": \"data:image/jpeg;base64,{imageData1}\",\n                },\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": \"data:image/jpeg;base64,{imageData2}\",\n                },\n            ],\n        ],\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Filtering Events by Component Type\nDESCRIPTION: Shows how to filter the event stream to only include events from components of a specific type (in this case, 'chat_model'). This allows focusing on events from particular component categories.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming.ipynb#2025-04-22_snippet_17\n\nLANGUAGE: javascript\nCODE:\n```\nconst chain = model.withConfig({ runName: \"model\" })\n  .pipe(\n    new JsonOutputParser().withConfig({ runName: \"my_parser\" })\n  );\n\n\nconst eventStream = await chain.streamEvents(\n  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key \"name\" and \"population\"`,\n  { version: \"v2\" },\n  { includeTypes: [\"chat_model\"] },\n);\n\nlet eventCount = 0;\n\nfor await (const event of eventStream) {\n  // Truncate the output\n  if (eventCount > 10) {\n    continue;\n  }\n  console.log(event);\n  eventCount += 1;\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Calculator Tool Call in LangChain.js\nDESCRIPTION: This snippet shows the structure of a calculator tool call within an AI assistant's response. It includes the tool name, input arguments, and associated metadata.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_33\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"calculator\",\n  \"args\": {\n    \"input\": \"52 * 365\"\n  },\n  \"id\": \"toolu_01NVTbm5aNYSm1wGYb6XF7jE\"\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Azure Cosmos DB NoSQL Chat History with LangChain\nDESCRIPTION: Complete example demonstrating how to set up and use AzureCosmosDBNoSQLChatMessageHistory for chat message persistence. Shows connection configuration, message addition, and integration with a language model for chat functionality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/azure_cosmosdb_nosql.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { AzureCosmosDBNoSQLChatMessageHistory } from \"@langchain/azure-cosmosdb\";\nimport { BufferMemory } from \"@langchain/community/memory\";\n\n// Set up Azure Cosmos DB chat history\n// When using connection string\nconst chatHistory = new AzureCosmosDBNoSQLChatMessageHistory({\n  connectionString: \"<connection_string>\",\n  databaseName: \"chat_history_db\",\n  containerName: \"chat_history\",\n  sessionId: \"session-1\",\n  userLabel: \"Human\",\n  aiLabel: \"AI\",\n  // Optional configuration\n  partitionKey: \"session-1\",\n  documentId: \"session-1\",\n  ttl: 604800, // 7 days in seconds\n});\n\n/*\n// Alternatively, when using Managed Identity\nconst chatHistory = new AzureCosmosDBNoSQLChatMessageHistory({\n  endpoint: \"<endpoint>\",\n  databaseName: \"chat_history_db\",\n  containerName: \"chat_history\",\n  sessionId: \"session-1\",\n  userLabel: \"Human\",\n  aiLabel: \"AI\",\n  // Optional configuration\n  partitionKey: \"session-1\",\n  documentId: \"session-1\",\n  ttl: 604800, // 7 days in seconds\n});\n*/\n\n// Add messages to the chat history\nawait chatHistory.addUserMessage(\"Hi! I'm a human.\");\nawait chatHistory.addAIMessage(\"Hello, I'm an AI assistant.\");\n\n// Get messages from the chat history\nconst messages = await chatHistory.getMessages();\nconsole.log(messages);\n\n// Use chat history with memory and a model for a conversation\nconst memory = new BufferMemory({\n  chatHistory,\n  returnMessages: true,\n  memoryKey: \"history\",\n});\n\nconst model = new ChatOpenAI();\n\nconst response = await model.invoke({\n  messages: [\n    {\n      content: \"Hello! What can you remember from our conversation?\",\n      role: \"human\",\n    },\n  ],\n});\n\nconsole.log(response);\n```\n\n----------------------------------------\n\nTITLE: Creating Basic String Prompt Template in TypeScript\nDESCRIPTION: Demonstrates how to create and use a simple string-based PromptTemplate for formatting inputs to language models. The example shows creating a template for generating jokes about a specific topic.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#2025-04-22_snippet_17\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PromptTemplate } from \"@langchain/core/prompts\";\n\nconst promptTemplate = PromptTemplate.fromTemplate(\n  \"Tell me a joke about {topic}\"\n);\n\nawait promptTemplate.invoke({ topic: \"cats\" });\n```\n\n----------------------------------------\n\nTITLE: Using a Tool with a Model in JavaScript\nDESCRIPTION: This example demonstrates how to use a tool-calling model to generate ToolMessages and then invoke the tool with those messages. It also shows how to create a chain that processes the model's output and invokes the tool.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_artifacts.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst llmWithTools = llm.bindTools([generateRandomInts])\n\nconst aiMessage = await llmWithTools.invoke(\"generate 6 positive ints less than 25\")\naiMessage.tool_calls\n\nawait generateRandomInts.invoke(aiMessage.tool_calls[0])\n\nconst extractToolCalls = (aiMessage) => aiMessage.tool_calls;\n\nconst chain = llmWithTools.pipe(extractToolCalls).pipe(generateRandomInts.map());\n\nawait chain.invoke(\"give me a random number between 1 and 5\");\n```\n\n----------------------------------------\n\nTITLE: Using Computer Use Preview Model with ChatOpenAI in TypeScript\nDESCRIPTION: This complex example demonstrates how to use the computer-use-preview model with ChatOpenAI. It includes functions for finding computer calls, handling tool outputs, and processing screenshots. The example shows how to interact with the computer use tool and handle its responses.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#2025-04-22_snippet_14\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AIMessage, ToolMessage } from \"@langchain/core/messages\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport * as fs from \"node:fs/promises\";\n\nconst findComputerCall = (message: AIMessage) => {\n  const toolOutputs = message.additional_kwargs.tool_outputs as\n    | { type: \"computer_call\"; call_id: string; action: { type: string } }[]\n    | undefined;\n\n  return toolOutputs?.find((toolOutput) => toolOutput.type === \"computer_call\");\n};\n\nconst llm = new ChatOpenAI({ model: \"computer-use-preview\" })\n  .bindTools([\n    {\n      type: \"computer-preview\",\n      display_width: 1024,\n      display_height: 768,\n      environment: \"browser\",\n    },\n  ])\n  .bind({ truncation: \"auto\" });\n\nlet message = await llm.invoke(\"Check the latest OpenAI news on bing.com.\");\nconst computerCall = findComputerCall(message);\n\nif (computerCall) {\n  // Act on a computer call action\n  const screenshot = await fs.readFile(\"./screenshot.png\", {\n    encoding: \"base64\",\n  });\n\n  message = await llm.invoke(\n    [\n      new ToolMessage({\n        additional_kwargs: { type: \"computer_call_output\" },\n        tool_call_id: computerCall.call_id,\n        content: [\n          {\n            type: \"computer_screenshot\",\n            image_url: `data:image/png;base64,${screenshot}`,\n          },\n        ],\n      }),\n    ],\n    { previous_response_id: message.response_metadata[\"id\"] }\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Tool Calling with ChatGoogleGenerativeAI\nDESCRIPTION: Shows how to define and bind custom tools to the ChatGoogleGenerativeAI model using Zod schemas for tool definition.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/google_generativeai.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { tool } from \"@langchain/core/tools\";\nimport { ChatGoogleGenerativeAI } from \"@langchain/google-genai\";\nimport { z } from \"zod\";\n\n// Define your tool\nconst fakeBrowserTool = tool((_) => {\n  return \"The search result is xyz...\"\n}, {\n  name: \"browser_tool\",\n  description: \"Useful for when you need to find something on the web or summarize a webpage.\",\n  schema: z.object({\n    url: z.string().describe(\"The URL of the webpage to search.\"),\n    query: z.string().optional().describe(\"An optional search query to use.\"),\n  }),\n})\n\nconst llmWithTool = new ChatGoogleGenerativeAI({\n  model: \"gemini-pro\",\n}).bindTools([fakeBrowserTool]) // Bind your tools to the model\n\nconst toolRes = await llmWithTool.invoke([\n  [\n    \"human\",\n    \"Search the web and tell me what the weather will be like tonight in new york. use a popular weather website\",\n  ],\n]);\n\nconsole.log(toolRes.tool_calls);\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search with Score Retrieval on Elasticsearch Vector Store (TypeScript)\nDESCRIPTION: Shows how to use similaritySearchWithScore to retrieve documents along with their similarity scores for more granular filtering or ranking. This method uses the same parameters and filter structure as similaritySearch and outputs both the document and its score in a formatted string.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/elasticsearch.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst similaritySearchWithScoreResults = await vectorStore.similaritySearchWithScore(\"biology\", 2, filter)\n\nfor (const [doc, score] of similaritySearchWithScoreResults) {\n  console.log(`* [SIM=${score.toFixed(3)}] ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`);\n}\n```\n\n----------------------------------------\n\nTITLE: Embedding Multiple Texts\nDESCRIPTION: Example of embedding multiple documents using embedDocuments method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/openai.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconst text2 = \"LangGraph is a library for building stateful, multi-actor applications with LLMs\";\n\nconst vectors = await embeddings.embedDocuments([text, text2]);\n\nconsole.log(vectors[0].slice(0, 100));\nconsole.log(vectors[1].slice(0, 100));\n```\n\n----------------------------------------\n\nTITLE: Basic GitHub Repository Loading with LangChain.js\nDESCRIPTION: Basic example of loading data from a GitHub repository using the GithubLoader. The loader retrieves files from a specified repository with options for branches and recursion.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/github.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nExample\n```\n\n----------------------------------------\n\nTITLE: Initial RAG Chain Setup\nDESCRIPTION: Setting up the basic components of the RAG chain including the model, retriever and prompt template\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_citations.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { TavilySearchAPIRetriever } from \"@langchain/community/retrievers/tavily_search_api\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-3.5-turbo\",\n  temperature: 0,\n});\n\nconst retriever = new TavilySearchAPIRetriever({\n  k: 6,\n});\n\nconst prompt = ChatPromptTemplate.fromMessages([\n  [\"system\", \"You're a helpful AI assistant. Given a user question and some web article snippets, answer the user question. If none of the articles answer the question, just say you don't know.\\n\\nHere are the web articles:{context}\"],\n  [\"human\", \"{question}\"],\n]);\n```\n\n----------------------------------------\n\nTITLE: Setting up Embeddings Model\nDESCRIPTION: Initialization of OpenAI embeddings model for text vectorization.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddings = new OpenAIEmbeddings({model: \"text-embedding-3-large\"});\n```\n\n----------------------------------------\n\nTITLE: Automatic Coercion of Custom Functions in RunnableSequence\nDESCRIPTION: Shows how custom functions can be automatically coerced into Runnables when used in a RunnableSequence without explicit RunnableLambda creation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/functions.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RunnableSequence } from \"@langchain/core/runnables\";\n\nconst storyPrompt = ChatPromptTemplate.fromTemplate(\"Tell me a short story about {topic}\");\n\nconst storyModel = new ChatOpenAI({ model: \"gpt-4o\" });\n\nconst chainWithCoercedFunction = RunnableSequence.from([\n  storyPrompt,\n  storyModel,\n  (input) => input.content.slice(0, 5),\n]);\n\nawait chainWithCoercedFunction.invoke({ \"topic\": \"bears\" });\n```\n\n----------------------------------------\n\nTITLE: Creating and Querying a Milvus Vector Store with OpenAI Embeddings in Typescript\nDESCRIPTION: This Typescript snippet demonstrates creating a Milvus vector store from text samples or documents using OpenAI embeddings, and querying it for similar documents. Dependencies include '@langchain/openai' and Milvus SDK, with 'text' samples as input, and 'collectionName' is a key parameter.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/milvus.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Milvus } from \"langchain/vectorstores/milvus\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\n// text sample from Godel, Escher, Bach\nconst vectorStore = await Milvus.fromTexts(\n  [\n    \"Tortoise: Labyrinth? Labyrinth? Could it Are we in the notorious Little\\\n            Harmonic Labyrinth of the dreaded Majotaur?\",\n    \"Achilles: Yiikes! What is that?\",\n    \"Tortoise: They say-although I person never believed it myself-that an I\\\n            Majotaur has created a tiny labyrinth sits in a pit in the middle of\\\n            it, waiting innocent victims to get lost in its fears complexity.\\\n            Then, when they wander and dazed into the center, he laughs and\\\n            laughs at them-so hard, that he laughs them to death!\",\n    \"Achilles: Oh, no!\",\n    \"Tortoise: But it's only a myth. Courage, Achilles.\",\n  ],\n  [{ id: 2 }, { id: 1 }, { id: 3 }, { id: 4 }, { id: 5 }],\n  new OpenAIEmbeddings(),\n  {\n    collectionName: \"goldel_escher_bach\",\n  }\n);\n\n// or alternatively from docs\nconst vectorStore = await Milvus.fromDocuments(docs, new OpenAIEmbeddings(), {\n  collectionName: \"goldel_escher_bach\",\n});\n\nconst response = await vectorStore.similaritySearch(\"scared\", 2);\n```\n\n----------------------------------------\n\nTITLE: Initializing HNSW Index with PGVector Store\nDESCRIPTION: Shows how to initialize a PGVector store with HNSW index configuration, create the index, add documents, and perform similarity searches. Includes connection setup, index parameters configuration, and example usage with OpenAI embeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/pgvector.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport {\n  DistanceStrategy,\n  PGVectorStore,\n} from \"@langchain/community/vectorstores/pgvector\";\nimport { PoolConfig } from \"pg\";\n\nconst hnswConfig = {\n  postgresConnectionOptions: {\n    type: \"postgres\",\n    host: \"127.0.0.1\",\n    port: 5433,\n    user: \"myuser\",\n    password: \"ChangeMe\",\n    database: \"api\",\n  } as PoolConfig,\n  tableName: \"testlangchainjs\",\n  columns: {\n    idColumnName: \"id\",\n    vectorColumnName: \"vector\",\n    contentColumnName: \"content\",\n    metadataColumnName: \"metadata\",\n  },\n  distanceStrategy: \"cosine\" as DistanceStrategy,\n};\n\nconst hnswPgVectorStore = await PGVectorStore.initialize(\n  new OpenAIEmbeddings(),\n  hnswConfig\n);\n\nawait hnswPgVectorStore.createHnswIndex({\n  dimensions: 1536,\n  efConstruction: 64,\n  m: 16,\n});\n\nawait hnswPgVectorStore.addDocuments([\n  { pageContent: \"what's this\", metadata: { a: 2, b: [\"tag1\", \"tag2\"] } },\n  { pageContent: \"Cat drinks milk\", metadata: { a: 1, b: [\"tag2\"] } },\n]);\n\nconst model = new OpenAIEmbeddings();\nconst query = await model.embedQuery(\"water\");\nconst hnswResults = await hnswPgVectorStore.similaritySearchVectorWithScore(query, 1);\n\nconsole.log(hnswResults);\n\nawait pgvectorStore.end();\n```\n\n----------------------------------------\n\nTITLE: Implementing Tool Calling with ChatMistralAI\nDESCRIPTION: Shows how to create and bind a calculator tool to ChatMistralAI using Zod for schema validation and invoke it through a chain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/mistral.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatMistralAI } from \"@langchain/mistralai\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { z } from \"zod\";\nimport { tool } from \"@langchain/core/tools\";\n\nconst calculatorSchema = z.object({\n  operation: z\n    .enum([\"add\", \"subtract\", \"multiply\", \"divide\"])\n    .describe(\"The type of operation to execute.\"),\n  number1: z.number().describe(\"The first number to operate on.\"),\n  number2: z.number().describe(\"The second number to operate on.\"),\n});\n\nconst calculatorTool = tool((input) => {\n  return JSON.stringify(input);\n}, {\n  name: \"calculator\",\n  description: \"A simple calculator tool\",\n  schema: calculatorSchema,\n});\n\n// Bind the tool to the model\nconst modelWithTool = new ChatMistralAI({\n  model: \"mistral-large-latest\",\n}).bindTools([calculatorTool]);\n\n\nconst calcToolPrompt = ChatPromptTemplate.fromMessages([\n  [\n    \"system\",\n    \"You are a helpful assistant who always needs to use a calculator.\",\n  ],\n  [\"human\", \"{input}\"],\n]);\n\n// Chain your prompt, model, and output parser together\nconst chainWithCalcTool = calcToolPrompt.pipe(modelWithTool);\n\nconst calcToolRes = await chainWithCalcTool.invoke({\n  input: \"What is 2 + 2?\",\n});\nconsole.log(calcToolRes.tool_calls);\n```\n\n----------------------------------------\n\nTITLE: Instantiating MistralAI Model\nDESCRIPTION: Creating a new MistralAI model instance with specific configuration parameters including model type, temperature, and retry settings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/mistral.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { MistralAI } from \"@langchain/mistralai\"\n\nconst llm = new MistralAI({\n  model: \"codestral-latest\",\n  temperature: 0,\n  maxTokens: undefined,\n  maxRetries: 2,\n  // other params...\n})\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Embeddings in LangChain\nDESCRIPTION: This snippet shows how to initialize the OpenAIEmbeddings model for generating text embeddings in LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/retrievers.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddings = new OpenAIEmbeddings({model: \"text-embedding-3-large\"});\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search with Scores in Qdrant\nDESCRIPTION: Performs a similarity search that returns both matching documents and their similarity scores. This helps evaluate the relevance of each result to the query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/qdrant.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconst similaritySearchWithScoreResults = await vectorStore.similaritySearchWithScore(\"biology\", 2, filter)\n\nfor (const [doc, score] of similaritySearchWithScoreResults) {\n  console.log(`* [SIM=${score.toFixed(3)}] ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`);\n}\n```\n\n----------------------------------------\n\nTITLE: Invoking LangChain Agent with Previous Chat History in Python\nDESCRIPTION: This code shows how to invoke the LangChain agent with a query while providing previous chat history, enabling the agent to maintain context across interactions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/agent_executor.ipynb#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nawait agentExecutor.invoke(\n  {\n    chat_history: [\n      { role: \"user\", content: \"hi! my name is bob\" },\n      { role: \"assistant\", content: \"Hello Bob! How can I assist you today?\" },\n    ],\n    input: \"what's my name?\",\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a RAG Chain with Tavily Search and LLM in TypeScript\nDESCRIPTION: Sets up a retrieval-augmented generation chain using Tavily Search API for retrieval and a chat model for generation. The chain takes a question, searches the web, and formulates an answer based on the retrieved content.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/cancel_execution.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { TavilySearchAPIRetriever } from \"@langchain/community/retrievers/tavily_search_api\";\nimport type { Document } from \"@langchain/core/documents\";\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { RunnablePassthrough, RunnableSequence } from \"@langchain/core/runnables\";\n\nconst formatDocsAsString = (docs: Document[]) => {\n  return docs.map((doc) => doc.pageContent).join(\"\\n\\n\")\n}\n\nconst retriever = new TavilySearchAPIRetriever({\n  k: 3,\n});\n\nconst prompt = ChatPromptTemplate.fromTemplate(`\nUse the following context to answer questions to the best of your ability:\n\n<context>\n{context}\n</context>\n\nQuestion: {question}`)\n\nconst chain = RunnableSequence.from([\n  {\n    context: retriever.pipe(formatDocsAsString),\n    question: new RunnablePassthrough(),\n  },\n  prompt,\n  llm,\n  new StringOutputParser(),\n]);\n```\n\n----------------------------------------\n\nTITLE: Implementing Citations with Claude\nDESCRIPTION: Demonstrates how to use Claude's citations feature with plain text documents and automatic text chunking.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/anthropic.ipynb#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst citationsModel = new ChatAnthropic({\n  model: \"claude-3-5-haiku-latest\",\n});\n\nconst messagesWithCitations = [\n  {\n    role: \"user\",\n    content: [\n      {\n        type: \"document\",\n        source: {\n          type: \"text\",\n          media_type: \"text/plain\",\n          data: \"The grass is green. The sky is blue.\",\n        },\n        title: \"My Document\",\n        context: \"This is a trustworthy document.\",\n        citations: {\n          enabled: true,\n        },\n      },\n      {\n        type: \"text\",\n        text: \"What color is the grass and sky?\",\n      },\n    ],\n  }\n];\n```\n\n----------------------------------------\n\nTITLE: Instantiating Upstash Vector Store with OpenAI Embeddings - TypeScript/JavaScript\nDESCRIPTION: This snippet demonstrates how to initialize an UpstashVectorStore instance using OpenAI embeddings from @langchain/openai, along with a properly configured Upstash Index. Ensure you install the @langchain/community, @langchain/openai, and @upstash/vector packages as prerequisites. The vector store will be ready for adding documents, searching, and serving as a retriever, but both the embedding model and index must have matching dimensions (e.g., 1536 for OpenAI's 'text-embedding-3-small').\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/upstash.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { UpstashVectorStore } from \"@langchain/community/vectorstores/upstash\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nimport { Index } from \"@upstash/vector\";\n\nconst embeddings = new OpenAIEmbeddings({\n  model: \"text-embedding-3-small\",\n});\n\nconst indexWithCredentials = new Index({\n  url: process.env.UPSTASH_VECTOR_REST_URL,\n  token: process.env.UPSTASH_VECTOR_REST_TOKEN,\n});\n\nconst vectorStore = new UpstashVectorStore(embeddings, {\n  index: indexWithCredentials,\n  // You can use namespaces to partition your data in an index\n  // namespace: \"test-namespace\",\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Graph Nodes for RAG in JavaScript\nDESCRIPTION: This code defines three main nodes for the RAG application: queryOrRespond for generating queries or direct responses, tools for executing retrieval, and generate for creating final responses using retrieved context.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/qa_chat_history.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport { \n    AIMessage,\n    HumanMessage,\n    SystemMessage,\n    ToolMessage\n} from \"@langchain/core/messages\";\nimport { MessagesAnnotation } from \"@langchain/langgraph\";\nimport { ToolNode } from \"@langchain/langgraph/prebuilt\";\n\n\n// Step 1: Generate an AIMessage that may include a tool-call to be sent.\nasync function queryOrRespond(state: typeof MessagesAnnotation.State) {\n  const llmWithTools = llm.bindTools([retrieve])\n  const response = await llmWithTools.invoke(state.messages);\n  // MessagesState appends messages to state instead of overwriting\n  return { messages: [response] };\n}\n\n\n// Step 2: Execute the retrieval.\nconst tools = new ToolNode([retrieve]);\n\n\n// Step 3: Generate a response using the retrieved content.\nasync function generate(state: typeof MessagesAnnotation.State) {\n  // Get generated ToolMessages\n  let recentToolMessages = [];\n    for (let i = state[\"messages\"].length - 1; i >= 0; i--) {\n      let message = state[\"messages\"][i];\n      if (message instanceof ToolMessage) {\n        recentToolMessages.push(message);\n      } else {\n        break;\n      }\n    }\n  let toolMessages = recentToolMessages.reverse();\n  \n  // Format into prompt\n  const docsContent = toolMessages.map(doc => doc.content).join(\"\\n\");\n  const systemMessageContent = \n    \"You are an assistant for question-answering tasks. \" +\n    \"Use the following pieces of retrieved context to answer \" +\n    \"the question. If you don't know the answer, say that you \" +\n    \"don't know. Use three sentences maximum and keep the \" +\n    \"answer concise.\" +\n    \"\\n\\n\" +\n    `${docsContent}`;\n\n  const conversationMessages = state.messages.filter(message => \n    message instanceof HumanMessage || \n    message instanceof SystemMessage || \n    (message instanceof AIMessage && message.tool_calls.length == 0)\n  );\n  const prompt = [new SystemMessage(systemMessageContent), ...conversationMessages];\n\n  // Run\n  const response = await llm.invoke(prompt)\n  return { messages: [response] };\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing SQL Query Generation Function\nDESCRIPTION: This function takes a question and generates a SQL query using the LLM and the prompt template.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/sql_qa.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport { z } from \"zod\";\n\n\nconst queryOutput = z.object({\n  query: z.string().describe(\"Syntactically valid SQL query.\"),\n});\n\nconst structuredLlm = llm.withStructuredOutput(queryOutput)\n\n\nconst writeQuery = async (state: typeof InputStateAnnotation.State) => {\n  const promptValue = await queryPromptTemplate.invoke({\n      dialect: db.appDataSourceOptions.type,\n      top_k: 10,\n      table_info: await db.getTableInfo(),\n      input: state.question\n  })\n  const result = await structuredLlm.invoke(promptValue)\n  return { query: result.query }\n};\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search in Vector Store\nDESCRIPTION: This code demonstrates how to perform a similarity search in a vector store by providing a query string. The vector store embeds the query and returns the most similar documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/vectorstores.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst query = \"my query\";\nconst docs = await vectorstore.similaritySearch(query);\n```\n\n----------------------------------------\n\nTITLE: Concatenating ChatOpenAI Model Output Chunks in JavaScript\nDESCRIPTION: This snippet shows how to concatenate multiple output chunks from the ChatOpenAI model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nlet finalChunk = chunks[0];\n\nfor (const chunk of chunks.slice(1, 5)) {\n  finalChunk = finalChunk.concat(chunk);\n}\n\nfinalChunk\n```\n\n----------------------------------------\n\nTITLE: Implementing Postgres Chat Memory in TypeScript\nDESCRIPTION: Example code demonstrating how to set up and use PostgreSQL for chat memory persistence in LangChain.js. The example shows both connection configuration options (direct pool or pool config) and demonstrates basic usage with a chat model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/postgres.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { ConversationChain } from \"langchain/chains\";\nimport { PostgresChatMessageHistory } from \"@langchain/community/stores/message/postgres\";\nimport { BufferMemory } from \"langchain/memory\";\n\nconst memory = new BufferMemory({\n  chatHistory: new PostgresChatMessageHistory({\n    // Either provide a table name for the message store...\n    tableName: \"langchain_chat_histories\",\n    // ... or specify a custom query to create the table if it doesn't exist.\n    createTableQuery: \"CREATE TABLE IF NOT EXISTS langchain_chat_histories (id TEXT PRIMARY KEY, session_id TEXT, type TEXT, content TEXT, metadata JSONB, role TEXT)\",\n    // Instead of passing connectionOptions, you can also pass an existing Pool object\n    // pool: existing_pool,\n    // Connection options\n    poolConfig: {\n      host: \"127.0.0.1\",\n      port: 5432,\n      database: \"langchain\",\n      user: \"myuser\",\n      password: \"mypassword\",\n    },\n    sessionId: \"test-session\", // Used to identify the chat history\n  }),\n});\n\nconst model = new ChatOpenAI();\nconst chain = new ConversationChain({ llm: model, memory });\n\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log(res1);\n/*\n  {\n    text: \"Hello Jim! It's nice to meet you. My name is AI assistant. How can I help you today?\"\n  }\n*/\n\nconst res2 = await chain.call({ input: \"What did I just say my name was?\" });\nconsole.log(res2);\n/*\n  {\n    text: \"You said your name was Jim.\"\n  }\n*/\n```\n\n----------------------------------------\n\nTITLE: Implementing OutputFixingParser with Anthropic\nDESCRIPTION: Demonstrates how to create an OutputFixingParser that uses the Claude model to attempt fixing misformatted outputs. The parser wraps the original StructuredOutputParser and uses the specified LLM to correct formatting errors.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/output_parser_fixing.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\nimport { OutputFixingParser } from \"langchain/output_parsers\";\n\nconst model = new ChatAnthropic({\n  model: \"claude-3-sonnet-20240229\",\n  maxTokens: 512,\n  temperature: 0.1,\n});\n\nconst parserWithFix = OutputFixingParser.fromLLM(model, parser);\n\nawait parserWithFix.parse(misformatted);\n```\n\n----------------------------------------\n\nTITLE: Streaming Agent Responses in JavaScript\nDESCRIPTION: This code demonstrates how to stream responses from the updated agent, processing each step and printing the messages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/sql_qa.ipynb#2025-04-22_snippet_23\n\nLANGUAGE: javascript\nCODE:\n```\nlet inputs4 = { messages: [{ role: \"user\", content: \"How many albums does alis in chain have?\" }] };\n\nfor await (\n  const step of await agent2.stream(inputs4, {\n    streamMode: \"values\",\n  })\n) {\n    const lastMessage = step.messages[step.messages.length - 1];\n    prettyPrint(lastMessage);\n    console.log(\"-----\\n\");\n}\n```\n\n----------------------------------------\n\nTITLE: Embedding Multiple Texts with BedrockEmbeddings\nDESCRIPTION: Demonstrates batch embedding of multiple texts using the embedDocuments method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/bedrock.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconst text2 = \"LangGraph is a library for building stateful, multi-actor applications with LLMs\";\n\nconst vectors = await embeddings.embedDocuments([text, text2]);\n\nconsole.log(vectors[0].slice(0, 100));\nconsole.log(vectors[1].slice(0, 100));\n```\n\n----------------------------------------\n\nTITLE: Adding Documents to RedisVectorStore in TypeScript\nDESCRIPTION: This code snippet demonstrates how to add documents to a RedisVectorStore. Documents must include content and metadata to be properly indexed and retrieved later.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/redis.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport type { Document } from \"@langchain/core/documents\";\n\nconst document1: Document = {\n  pageContent: \"The powerhouse of the cell is the mitochondria\",\n  metadata: { type: \"example\" }\n};\n\nconst document2: Document = {\n  pageContent: \"Buildings are made out of brick\",\n  metadata: { type: \"example\" }\n};\n\nconst document3: Document = {\n  pageContent: \"Mitochondria are made out of lipids\",\n  metadata: { type: \"example\" }\n};\n\nconst document4: Document = {\n  pageContent: \"The 2024 Olympics are in Paris\",\n  metadata: { type: \"example\" }\n}\n\nconst documents = [document1, document2, document3, document4];\n\nawait vectorStore.addDocuments(documents);\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search with Filter (TypeScript)\nDESCRIPTION: Demonstrates a basic similarity search using the `similaritySearch` method on the `vectorStore`. It searches for documents similar to the query \"biology\", retrieves the top 2 results (`k=2`), and applies an optional filter function (predicate) to only include documents whose metadata `source` matches \"https://example.com\". The results (matching documents) are then logged to the console.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/memory.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst filter = (doc) => doc.metadata.source === \"https://example.com\";\n\nconst similaritySearchResults = await vectorStore.similaritySearch(\"biology\", 2, filter)\n\nfor (const doc of similaritySearchResults) {\n  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Translation Chain with ChatNovitaAI\nDESCRIPTION: Shows how to create a chain combining a chat prompt template with the ChatNovitaAI model to build a flexible translation system with variable input and output languages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/novita.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\"\n\nconst prompt = ChatPromptTemplate.fromMessages(\n  [\n    [\n      \"system\",\n      \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n    ],\n    [\"human\", \"{input}\"],\n  ]\n)\n\nconst chain = prompt.pipe(llm);\nawait chain.invoke(\n  {\n    input_language: \"English\",\n    output_language: \"German\",\n    input: \"I love programming.\",\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: Chaining Tavily Extract with AI Model in JavaScript\nDESCRIPTION: This code establishes a chain involving TavilyExtract and a ChatOpenAI instance, demonstrating message handling and tool chaining for user input processing. It involves creating human messages and invoking both AI models and tool calls, generating AI and tool messages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/tavily_extract.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\"\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o\",\n  temperature: 0,\n})\n```\n\nLANGUAGE: javascript\nCODE:\n```\nimport { HumanMessage } from \"@langchain/core/messages\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { RunnableLambda } from \"@langchain/core/runnables\";\n\nconst prompt = ChatPromptTemplate.fromMessages(\n  [\n    [\"system\", \"You are a helpful assistant.\"],\n    [\"placeholder\", \"{messages}\"],\n  ]\n)\n\nconst llmWithTools = llm.bindTools([tool]);\n\nconst chain = prompt.pipe(llmWithTools);\n\nconst toolChain = RunnableLambda.from(\n  async (userInput: string, config) => {\n    const humanMessage = new HumanMessage(userInput);\n    const aiMsg = await chain.invoke({\n      messages: [new HumanMessage(userInput)],\n    }, config);\n    const toolMsgs = await tool.batch(aiMsg.tool_calls, config);\n    return chain.invoke({\n      messages: [humanMessage, aiMsg, ...toolMsgs],\n    }, config);\n  }\n);\n\nconst toolChainResult = await toolChain.invoke(\"['https://en.wikipedia.org/wiki/Albert_Einstein','https://en.wikipedia.org/wiki/Theoretical_physics']\");\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst { tool_calls, content } = toolChainResult;\n\nconsole.log(\"AIMessage\", JSON.stringify({\n  tool_calls,\n  content,\n}, null, 2));\n```\n\n----------------------------------------\n\nTITLE: Instantiating LibSQLVectorStore in TypeScript\nDESCRIPTION: Initializes the `LibSQLVectorStore` in TypeScript. It requires an embedding function instance (here, `OpenAIEmbeddings`) and a configuration object containing the `libsql` client instance (`db`), the target `table` name, and the embedding `column` name. The example shows setup for a remote Turso database using `createClient` with a URL and auth token. A commented-out section shows how to configure the client for a local SQLite file.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/libsql.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LibSQLVectorStore } from \"@langchain/community/vectorstores/libsql\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { createClient } from \"@libsql/client\";\n\nconst embeddings = new OpenAIEmbeddings({\n  model: \"text-embedding-3-small\",\n});\n\nconst libsqlClient = createClient({\n  url: \"libsql://[database-name]-[your-username].turso.io\",\n  authToken: \"...\",\n});\n\n// Local instantiation\n// const libsqlClient = createClient({\n//  url: \"file:./dev.db\",\n// });\n\nconst vectorStore = new LibSQLVectorStore(embeddings, {\n  db: libsqlClient,\n  table: \"TABLE_NAME\",\n  column: \"EMBEDDING_COLUMN\",\n});\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search in LibSQLVectorStore (TypeScript)\nDESCRIPTION: Executes a similarity search against the `LibSQLVectorStore`. It takes a query string ('hola') and the number of results to return (1). The method embeds the query and finds the most similar documents stored in the libSQL database based on vector distance. The example iterates through the returned documents and prints their content and metadata.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/libsql.mdx#2025-04-22_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst resultOne = await vectorStore.similaritySearch(\"hola\", 1);\n\nfor (const doc of similaritySearchResults) {\n  console.log(`${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);\n}\n```\n\n----------------------------------------\n\nTITLE: Formatting Chat Prompt Messages in JavaScript\nDESCRIPTION: This snippet demonstrates how to format the composed chat prompt template by providing a value for the input placeholder. It uses the 'formatMessages' method to generate the final messages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/prompts_composition.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nawait newPrompt.formatMessages({ input: \"i said hi\" })\n```\n\n----------------------------------------\n\nTITLE: Complete Few Shot Example with OpenAI in LangChainJS\nDESCRIPTION: A full example showing how to create and use a few shot prompt template with OpenAI's chat model to rephrase a specific question into a more general query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot.mdx#2025-04-22_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = new ChatOpenAI({});\nconst examples = [\n  {\n    input: \"Could the members of The Police perform lawful arrests?\",\n    output: \"what can the members of The Police do?\",\n  },\n  {\n    input: \"Jan Sindel's was born in what country?\",\n    output: \"what is Jan Sindel's personal history?\",\n  },\n];\nconst examplePrompt = ChatPromptTemplate.fromTemplate(`Human: {input}\nAI: {output}`);\nconst fewShotPrompt = new FewShotChatMessagePromptTemplate({\n  prefix:\n    \"Rephrase the users query to be more general, using the following examples\",\n  suffix: \"Human: {input}\",\n  examplePrompt,\n  examples,\n  inputVariables: [\"input\"],\n});\nconst formattedPrompt = await fewShotPrompt.format({\n  input: \"What's France's main city?\",\n});\n\nconst response = await model.invoke(formattedPrompt);\nconsole.log(response);\n```\n\n----------------------------------------\n\nTITLE: Configuring LangSmith Credentials - TypeScript\nDESCRIPTION: This snippet sets environment variables to enable LangSmith tracing and provide an API key for advanced observability of LangChain.js executions. Set 'LANGSMITH_TRACING' to 'true' and assign your API key to 'LANGSMITH_API_KEY' in the Node.js environment. This setup is optional but recommended when tracking and debugging tool activity; input parameters are provided as environment variables.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/duckduckgo_search.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.LANGSMITH_TRACING=\"true\"\nprocess.env.LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Simple Tool Schema in TypeScript\nDESCRIPTION: Demonstrates how to create a simple tool schema using StructuredToolParams. This approach is suitable when the generated output doesn't need to be passed as input to a function.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/custom_tools.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { z } from \"zod\";\nimport { StructuredToolParams } from \"@langchain/core/tools\";\n\nconst simpleToolSchema: StructuredToolParams = {\n  name: \"get_current_weather\",\n  description: \"Get the current weather for a location\",\n  schema: z.object({\n    city: z.string().describe(\"The city to get the weather for\"),\n    state: z.string().optional().describe(\"The state to get the weather for\"),\n  })\n}\n```\n\n----------------------------------------\n\nTITLE: Invoking the Model with Tools\nDESCRIPTION: Code to invoke the model with tools and examine the response.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/agent_executor.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nconst responseWithTools = await modelWithTools.invoke([{\n  role: \"user\",\n  content: \"Hi!\"\n}])\n\nconsole.log(`Content: ${responseWithTools.content}`)\nconsole.log(`Tool calls: ${responseWithTools.tool_calls}`)\n```\n\n----------------------------------------\n\nTITLE: Creating a Chain with Prompt, Model, and Output Parser in JavaScript\nDESCRIPTION: This code creates a chain by piping a prompt template to a chat model and then to a string output parser. It demonstrates the basic usage of the .pipe() method in LCEL.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/sequence.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\nconst prompt = ChatPromptTemplate.fromTemplate(\"tell me a joke about {topic}\")\n\nconst chain = prompt.pipe(model).pipe(new StringOutputParser())\n```\n\n----------------------------------------\n\nTITLE: Creating Configurable Retrieval Chain\nDESCRIPTION: JavaScript code to create a configurable retrieval chain that can be customized for different users using the `configurable` object in the chain's configuration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_per_user.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RunnablePassthrough, RunnableSequence } from \"@langchain/core/runnables\";\n\nconst chain = RunnableSequence.from([\n  RunnablePassthrough.assign({\n    context: async (input: { question: string }, config) => {\n      if (!config || !(\"configurable\" in config)) {\n        throw new Error(\"No config\");\n      }\n      const { configurable } = config;\n      const documents = await vectorStore.asRetriever(configurable).invoke(\n        input.question,\n        config,\n      );\n      return documents.map((doc) => doc.pageContent).join(\"\\n\\n\");\n    },\n  }),\n  prompt,\n  model,\n  new StringOutputParser(),\n]);\n```\n\n----------------------------------------\n\nTITLE: Adding Documents to Elasticsearch Vector Store (TypeScript)\nDESCRIPTION: Shows how to prepare and add multiple documents to the Elasticsearch vector store, including assignment of unique ids for each document. Each Document object includes page content and metadata. The documents are added by calling addDocuments with a list of documents and an optional ids array. Prerequisites: the vectorStore must be instantiated as shown previously, and the Document type should be imported from @langchain/core/documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/elasticsearch.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport type { Document } from \"@langchain/core/documents\";\n\nconst document1: Document = {\n  pageContent: \"The powerhouse of the cell is the mitochondria\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document2: Document = {\n  pageContent: \"Buildings are made out of brick\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document3: Document = {\n  pageContent: \"Mitochondria are made out of lipids\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document4: Document = {\n  pageContent: \"The 2024 Olympics are in Paris\",\n  metadata: { source: \"https://example.com\" }\n}\n\nconst documents = [document1, document2, document3, document4];\n\nawait vectorStore.addDocuments(documents, { ids: [\"1\", \"2\", \"3\", \"4\"] });\n```\n\n----------------------------------------\n\nTITLE: Creating a ReAct Agent with LangGraph in JavaScript\nDESCRIPTION: This snippet demonstrates how to create a ReAct agent using LangGraph's pre-built agent constructor. It sets up an agent with a language model and a retrieval tool.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/qa_chat_history.ipynb#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n\nconst agent = createReactAgent({ llm: llm, tools: [retrieve] });\n```\n\n----------------------------------------\n\nTITLE: Querying LangChain Agent About LangSmith in Python\nDESCRIPTION: This code snippet demonstrates invoking the LangChain agent with a specific query about LangSmith's capabilities for testing. It's intended to showcase the agent's use of the retriever tool.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/agent_executor.ipynb#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nawait agentExecutor.invoke({ input: \"how can langsmith help with testing?\" })\n```\n\n----------------------------------------\n\nTITLE: Adding Values to Chain State using RunnablePassthrough.assign() in TypeScript\nDESCRIPTION: This snippet demonstrates how to use RunnablePassthrough.assign() to add new key-value pairs to the chain state without modifying existing values. It uses RunnableParallel to process multiple operations concurrently.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/assign.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RunnableParallel, RunnablePassthrough } from \"@langchain/core/runnables\";\n\nconst runnable = RunnableParallel.from({\n  extra: RunnablePassthrough.assign({\n    mult: (input: { num: number }) => input.num * 3,\n    modified: (input: { num: number }) => input.num + 1\n  })\n});\n\nawait runnable.invoke({ num: 1 });\n```\n\n----------------------------------------\n\nTITLE: Implementing Max Iterations and Recursion Limits\nDESCRIPTION: Demonstrates how to implement iteration limits in both frameworks, including error handling for recursion limits in LangGraph.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/migrate_agent.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst badMagicTool = tool(async ({ input: _input }) => {\n  return \"Sorry, there was a temporary error. Please try again with the same input.\";\n}, {\n  name: \"magic_function\",\n  description: \"Applies a magic function to an input.\",\n  schema: z.object({\n    input: z.string(),\n  }),\n});\n\nconst badTools = [badMagicTool];\n\nconst spanishAgentExecutorWithMaxIterations = new AgentExecutor({\n  agent: createToolCallingAgent({\n    llm,\n    tools: badTools,\n    prompt: spanishPrompt,\n  }),\n  tools: badTools,\n  verbose: true,\n  maxIterations: 2,\n});\n\nawait spanishAgentExecutorWithMaxIterations.invoke({ input: query });\n```\n\nLANGUAGE: javascript\nCODE:\n```\nimport { GraphRecursionError } from \"@langchain/langgraph\";\n\nconst RECURSION_LIMIT = 2 * 2 + 1;\n\nconst appWithBadTools = createReactAgent({ llm, tools: badTools });\n\ntry {\n  await appWithBadTools.invoke({\n    messages: [\n      { role: \"user\", content: query }\n    ]\n  }, {\n    recursionLimit: RECURSION_LIMIT,\n  });\n} catch (e) {\n  if (e instanceof GraphRecursionError) {\n    console.log(\"Recursion limit reached.\");\n  } else {\n    throw e;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Streaming Tool Calls in LangChain.js\nDESCRIPTION: This snippet demonstrates how to stream tool calls using the previously defined model with tools. It iterates over the stream and logs each chunk's tool call chunks.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_streaming.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst query = \"What is 3 * 12? Also, what is 11 + 49?\";\n\nconst stream = await modelWithTools.stream(query);\n\nfor await (const chunk of stream) {\n  console.log(chunk.tool_call_chunks);\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a RAG Chain with AmazonKendraRetriever in TypeScript\nDESCRIPTION: This snippet demonstrates how to create a Retrieval-Augmented Generation (RAG) chain using the AmazonKendraRetriever, a chat prompt template, and an LLM. It includes formatting functions and runnable sequences.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/kendra-retriever.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { RunnablePassthrough, RunnableSequence } from \"@langchain/core/runnables\";\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\n\nimport type { Document } from \"@langchain/core/documents\";\n\nconst prompt = ChatPromptTemplate.fromTemplate(`\nAnswer the question based only on the context provided.\n\nContext: {context}\n\nQuestion: {question}`);\n\nconst formatDocs = (docs: Document[]) => {\n  return docs.map((doc) => doc.pageContent).join(\"\\n\\n\");\n}\n\n// See https://js.langchain.com/docs/tutorials/rag\nconst ragChain = RunnableSequence.from([\n  {\n    context: retriever.pipe(formatDocs),\n    question: new RunnablePassthrough(),\n  },\n  prompt,\n  llm,\n  new StringOutputParser(),\n]);\n```\n\n----------------------------------------\n\nTITLE: Implementing Stuff Documents Chain\nDESCRIPTION: Creation and execution of a document summarization chain using the stuff method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/summarization.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { createStuffDocumentsChain } from \"langchain/chains/combine_documents\";\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\nimport { PromptTemplate } from \"@langchain/core/prompts\";\n\n\n// Define prompt\nconst prompt = PromptTemplate.fromTemplate(\n  \"Summarize the main themes in these retrieved docs: {context}\"\n);\n\n// Instantiate\nconst chain = await createStuffDocumentsChain({\n  llm: llm,\n  outputParser: new StringOutputParser(),\n  prompt,\n});\n\n// Invoke\nconst result = await chain.invoke({context: docs})\nconsole.log(result)\n```\n\n----------------------------------------\n\nTITLE: Defining Sample Document Data for Climate Change\nDESCRIPTION: Creates an array of sample documents about climate change to be used as test data for the RAG system. Each document has an ID and text content related to different aspects of climate change.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/rag_fusion.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n/** Define our fake data */\nconst allDocuments = [\n  { id: \"doc1\", text: \"Climate change and economic impact.\" },\n  { id: \"doc2\", text: \"Public health concerns due to climate change.\" },\n  { id: \"doc3\", text: \"Climate change: A social perspective.\" },\n  { id: \"doc4\", text: \"Technological solutions to climate change.\" },\n  { id: \"doc5\", text: \"Policy changes needed to combat climate change.\" },\n  { id: \"doc6\", text: \"Climate change and its impact on biodiversity.\" },\n  { id: \"doc7\", text: \"Climate change: The science and models.\" },\n  { id: \"doc8\", text: \"Global warming: A subset of climate change.\" },\n  { id: \"doc9\", text: \"How climate change affects daily weather.\" },\n  { id: \"doc10\", text: \"The history of climate change activism.\" },\n];\n```\n\n----------------------------------------\n\nTITLE: Implementing SemanticSimilarityExampleSelector in JavaScript\nDESCRIPTION: Creates a SemanticSimilarityExampleSelector instance to dynamically select the most relevant few-shot examples based on semantic similarity to the input.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot_examples.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { SemanticSimilarityExampleSelector } from \"@langchain/core/example_selectors\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst exampleSelector = await SemanticSimilarityExampleSelector.fromExamples(\n    examples,\n    new OpenAIEmbeddings(),\n    MemoryVectorStore,\n    {\n        k: 1,\n    }\n)\n\nconst question = \"Who was the father of Mary Ball Washington?\"\nconst selectedExamples = await exampleSelector.selectExamples({ question })\nconsole.log(`Examples most similar to the input: ${question}`)\nfor (const example of selectedExamples) {\n    console.log(\"\\n\");\n    console.log(Object.entries(example).map(([k, v]) => `${k}: ${v}`).join(\"\\n\"))\n}\n```\n\n----------------------------------------\n\nTITLE: Structured Output with Strict Mode\nDESCRIPTION: Demonstrates how to use structured output with the strict mode flag to enforce schema compliance when extracting data.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst traitSchema = z.object({\n  traits: z.array(z.string()).describe(\"A list of traits contained in the input\"),\n});\n\nconst structuredLlm = new ChatOpenAI({\n  model: \"gpt-4o-mini\",\n}).withStructuredOutput(traitSchema, {\n  name: \"extract_traits\",\n  strict: true,\n});\n\nawait structuredLlm.invoke([{\n  role: \"user\",\n  content: `I am 6'5\" tall and love fruit.`\n}]);\n```\n\n----------------------------------------\n\nTITLE: Embedding Single and Multiple Texts with AzureOpenAIEmbeddings\nDESCRIPTION: Shows how to directly use the embeddings model to generate vector representations for single and multiple texts.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/azure_openai.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst singleVector = await embeddings.embedQuery(text);\n\nconsole.log(singleVector.slice(0, 100));\n\nconst text2 = \"LangGraph is a library for building stateful, multi-actor applications with LLMs\";\n\nconst vectors = await embeddings.embedDocuments([text, text2]);\n\nconsole.log(vectors[0].slice(0, 100));\nconsole.log(vectors[1].slice(0, 100));\n```\n\n----------------------------------------\n\nTITLE: Authenticating with Azure Managed Identity for Cosmos DB Integration (TypeScript)\nDESCRIPTION: Demonstrates how to authenticate and configure a connection to Azure Cosmos DB for NoSQL using Azure Managed Identity within a TypeScript environment. Requires the Azure credential libraries, correct Managed Identity configuration, and pre-created database/container resources. Intended for developers in cloud environments implementing secure, role-based access to Cosmos DB using Node.js and LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/azure_cosmosdb_nosql.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n// Example: Initializing Azure Cosmos DB client with Managed Identity\nimport { DefaultAzureCredential } from \"@azure/identity\";\nimport { CosmosClient } from \"@azure/cosmos\";\n\nconst endpoint = process.env.AZURE_COSMOSDB_ENDPOINT!;\nconst databaseId = process.env.AZURE_COSMOSDB_DATABASE_ID!;\nconst containerId = process.env.AZURE_COSMOSDB_CONTAINER_ID!;\n\nconst credential = new DefaultAzureCredential();\nconst client = new CosmosClient({ endpoint, aadCredentials: credential });\n\n// Use client to connect, query, or modify the database/container\n\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatOpenAI with Web Search Tool in TypeScript\nDESCRIPTION: This snippet demonstrates how to create a ChatOpenAI instance with the web search preview tool. It uses the 'gpt-4o-mini' model and invokes the LLM with a query about recent positive news.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst llm = new ChatOpenAI({ model: \"gpt-4o-mini\" }).bindTools([\n  { type: \"web_search_preview\" },\n]);\n\nawait llm.invoke(\"What was a positive news story from today?\");\n```\n\n----------------------------------------\n\nTITLE: ParentDocumentRetriever with Reranking in TypeScript\nDESCRIPTION: Demonstration of using ParentDocumentRetriever with reranking to improve retrieval precision and reduce costs by filtering out irrelevant documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/parent_document_retriever.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\n{ExampleWithRerank}\n```\n\n----------------------------------------\n\nTITLE: Orchestrating LangGraph Application\nDESCRIPTION: This code compiles the application into a single graph object, connecting the three steps (writeQuery, executeQuery, generateAnswer) into a sequence.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/sql_qa.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport { StateGraph } from \"@langchain/langgraph\";\n\nconst graphBuilder = new StateGraph({\n  stateSchema: StateAnnotation,\n})\n  .addNode(\"writeQuery\", writeQuery)\n  .addNode(\"executeQuery\", executeQuery)\n  .addNode(\"generateAnswer\", generateAnswer)\n  .addEdge(\"__start__\", \"writeQuery\")\n  .addEdge(\"writeQuery\", \"executeQuery\")\n  .addEdge(\"executeQuery\", \"generateAnswer\")\n  .addEdge(\"generateAnswer\", \"__end__\")\n\nconst graph = graphBuilder.compile()\n```\n\n----------------------------------------\n\nTITLE: Loading a PPTX file with PPTXLoader in LangChain.js\nDESCRIPTION: Code to import and use the PPTXLoader to load content from a PPTX file. By default, this creates one document for each page in the presentation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/pptx.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PPTXLoader } from \"@langchain/community/document_loaders/fs/pptx\";\n\nconst loader = new PPTXLoader(\"src/document_loaders/example_data/example.pptx\");\n\nconst docs = await loader.load();\n```\n\n----------------------------------------\n\nTITLE: Loading EPUB Files with Default Chapter Splitting in LangChain JS\nDESCRIPTION: Example of using EPubLoader to load an EPUB file and split it into separate documents for each chapter, which is the default behavior of the loader.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/epub.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { EPubLoader } from \"@langchain/community/document_loaders/fs/epub\";\n\nconst loader = new EPubLoader(\"src/document_loaders/example_data/example.epub\");\n\nconst docs = await loader.load();\n```\n\n----------------------------------------\n\nTITLE: Inspecting Prompt Format in TypeScript\nDESCRIPTION: Demonstrates how to inspect the formatted prompt before sending it to the model, which is useful for debugging and understanding what instructions the model will receive.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/structured_output.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst query = \"Anna is 23 years old and she is 6 feet tall\"\n\nconsole.log((await prompt.format({ query })).toString())\n```\n\n----------------------------------------\n\nTITLE: Indexing Texts with Momento Vector Index using fromTexts in TypeScript\nDESCRIPTION: Demonstrates how to instantiate the Momento vector store and index an array of text documents using the `fromTexts` static method in TypeScript. If the specified index doesn't exist, it will be created; otherwise, documents are added. Optional `ids` can be provided, or Momento will generate UUIDs.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/momento_vector_index.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\n{TextsExample}\n```\n\n----------------------------------------\n\nTITLE: Indexing Documents with Momento Vector Index using fromDocuments in TypeScript\nDESCRIPTION: Illustrates using the `fromDocuments` static method in TypeScript to instantiate the Momento vector store and index Langchain `Document` objects. This approach facilitates chaining document loaders with the indexing process. Similar to `fromTexts`, it creates the index if needed or adds to an existing one.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/momento_vector_index.mdx#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n{DocsExample}\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG Chain with Self-Query Retriever\nDESCRIPTION: Sets up a RAG (Retrieval-Augmented Generation) chain using the self-query retriever with document formatting and prompt templates.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/memory.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { RunnablePassthrough, RunnableSequence } from \"@langchain/core/runnables\";\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\n\nimport type { Document } from \"@langchain/core/documents\";\n\nconst prompt = ChatPromptTemplate.fromTemplate(`\nAnswer the question based only on the context provided.\n\nContext: {context}\n\nQuestion: {question}`);\n\nconst formatDocs = (docs: Document[]) => {\n  return docs.map((doc) => JSON.stringify(doc)).join(\"\\n\\n\");\n}\n\n// See https://js.langchain.com/docs/tutorials/rag\nconst ragChain = RunnableSequence.from([\n  {\n    context: selfQueryRetriever.pipe(formatDocs),\n    question: new RunnablePassthrough(),\n  },\n  prompt,\n  llm,\n  new StringOutputParser(),\n]);\n```\n\n----------------------------------------\n\nTITLE: Assembling a Few Shot Prompt Template in LangChain.js\nDESCRIPTION: This code demonstrates how to create a few shot prompt template using the FewShotChatMessagePromptTemplate class. It formats each example into human and AI message pairs and displays the resulting chat messages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot_examples_chat.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n// This is a prompt template used to format each individual example.\nconst examplePrompt = ChatPromptTemplate.fromMessages(\n    [\n        [\"human\", \"{input}\"],\n        [\"ai\", \"{output}\"],\n    ]\n)\nconst fewShotPrompt = new FewShotChatMessagePromptTemplate({\n    examplePrompt,\n    examples,\n    inputVariables: [], // no input variables\n})\n\nconst result = await fewShotPrompt.invoke({});\nconsole.log(result.toChatMessages())\n```\n\n----------------------------------------\n\nTITLE: Defining Application State Annotations with LangGraph\nDESCRIPTION: This code defines the state annotations for the application, including input state and full state with question, query, result, and answer fields.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/sql_qa.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { Annotation } from \"@langchain/langgraph\";\n\n\nconst InputStateAnnotation = Annotation.Root({\n  question: Annotation<string>,\n});\n\n\nconst StateAnnotation = Annotation.Root({\n  question: Annotation<string>,\n  query: Annotation<string>,\n  result: Annotation<string>,\n  answer: Annotation<string>,\n});\n```\n\n----------------------------------------\n\nTITLE: Streaming with Generator Functions in TypeScript\nDESCRIPTION: Demonstrates how to use generator functions for streaming in a chain, including a custom output parser for comma-separated lists.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/functions.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst streamingPrompt = ChatPromptTemplate.fromTemplate(\n  \"Write a comma-separated list of 5 animals similar to: {animal}. Do not include numbers\"\n);\n\nconst strChain = streamingPrompt.pipe(model).pipe(new StringOutputParser());\n\nconst stream = await strChain.stream({ animal: \"bear\" });\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n```\n\nLANGUAGE: typescript\nCODE:\n```\nasync function* splitIntoList(input) {\n  let buffer = \"\";\n  for await (const chunk of input) {\n    buffer += chunk;\n    while (buffer.includes(\",\")) {\n      const commaIndex = buffer.indexOf(\",\");\n      yield [buffer.slice(0, commaIndex).trim()];\n      buffer = buffer.slice(commaIndex + 1);\n    }\n  }\n  yield [buffer.trim()];\n}\n\nconst listChain = strChain.pipe(splitIntoList);\n\nconst listChainStream = await listChain.stream({\"animal\": \"bear\"});\n\nfor await (const chunk of listChainStream) {\n  console.log(chunk);\n}\n```\n\nLANGUAGE: typescript\nCODE:\n```\nawait listChain.invoke({\"animal\": \"bear\"})\n```\n\n----------------------------------------\n\nTITLE: Creating and Using a Retriever Tool with a LangGraph Agent in TypeScript\nDESCRIPTION: This example demonstrates creating a ReAct agent using LangGraph and providing it with a retriever tool. The agent processes the query about dogs by using the retriever tool to access relevant information.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/convert_runnable_to_tool.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n\nconst tools = [\n  retriever.asTool({\n    name: \"pet_info_retriever\",\n    description: \"Get information about pets.\",\n    schema: z.string(),\n  })\n];\n\nconst agent = createReactAgent({ llm: llm, tools });\n\nconst stream = await agent.stream({\"messages\": [[\"human\", \"What are dogs known for?\"]]}); \n\nfor await (const chunk of stream) {\n  // Log output from the agent or tools node\n  if (chunk.agent) {\n    console.log(\"AGENT:\", chunk.agent.messages[0]);\n  } else if (chunk.tools) {\n    console.log(\"TOOLS:\", chunk.tools.messages[0]);\n  }\n  console.log(\"----\");\n}\n```\n\n----------------------------------------\n\nTITLE: Similarity Search with Metadata Filtering in Pinecone Vector Store\nDESCRIPTION: This code shows how to perform a similarity search with metadata filtering in a Pinecone vector store. It filters for documents with a specific metadata value while searching for semantically similar content.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/vectorstores.mdx#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nawait vectorstore.similaritySearch(\n  \"LangChain provides abstractions to make working with LLMs easy\",\n  2,\n  {\n    // The arguments of this field are provider specific.\n    filter: { source: \"tweet\" },\n  }\n);\n```\n\n----------------------------------------\n\nTITLE: Creating libSQL Table for Vector Storage (SQL)\nDESCRIPTION: SQL command to create a table suitable for storing LangChain documents and their vector embeddings in a libSQL database. It includes columns for an auto-incrementing ID, document content, metadata (as JSON text), and the vector embedding itself (`F32_BLOB`). Requires replacing `TABLE_NAME` with the desired table name and `EMBEDDING_COLUMN` with the chosen column name for embeddings. The dimension `1536` should match the embedding model's output dimensions (e.g., OpenAI's `text-embedding-3-small`).\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/libsql.mdx#2025-04-22_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nCREATE TABLE IF NOT EXISTS TABLE_NAME (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    content TEXT,\n    metadata TEXT,\n    EMBEDDING_COLUMN F32_BLOB(1536) -- 1536-dimensional f32 vector for OpenAI\n);\n```\n\n----------------------------------------\n\nTITLE: Creating Self-Query Retriever\nDESCRIPTION: Instantiation of the self-query retriever using the Weaviate vector store and configuration of the language model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/weaviate.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { SelfQueryRetriever } from \"langchain/retrievers/self_query\";\nimport { WeaviateTranslator } from \"@langchain/weaviate\";\n\nconst selfQueryRetriever = SelfQueryRetriever.fromLLM({\n  llm: llm,\n  vectorStore: vectorStore,\n  documentContents: \"Brief summary of a movie\",\n  attributeInfo: attributeInfo,\n  structuredQueryTranslator: new WeaviateTranslator(),\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using Typesense Vector Store (TypeScript)\nDESCRIPTION: Demonstrates how to configure a Typesense client, define vector store configuration, customize document import behavior, and interact with the Typesense vector store using LangChain.js in TypeScript. Code covers connecting to Typesense, setting schema and collection details, providing OpenAI embeddings, creating or retrieving vector stores, running similarity search queries (with optional metadata filtering), and deleting documents. Requires installed dependencies: @langchain/openai, @langchain/community, @langchain/core, and typesense. Users must provide Typesense instance details and ensure schema is created ahead of use.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/typesense.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport {\n  Typesense,\n  TypesenseConfig,\n} from \"@lanchain/community/vectorstores/typesense\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { Client } from \"typesense\";\nimport { Document } from \"@langchain/core/documents\";\n\nconst vectorTypesenseClient = new Client({\n  nodes: [\n    {\n      // Ideally should come from your .env file\n      host: \"...\",\n      port: 123,\n      protocol: \"https\",\n    },\n  ],\n  // Ideally should come from your .env file\n  apiKey: \"...\",\n  numRetries: 3,\n  connectionTimeoutSeconds: 60,\n});\n\nconst typesenseVectorStoreConfig = {\n  // Typesense client\n  typesenseClient: vectorTypesenseClient,\n  // Name of the collection to store the vectors in\n  schemaName: \"your_schema_name\",\n  // Optional column names to be used in Typesense\n  columnNames: {\n    // \"vec\" is the default name for the vector column in Typesense but you can change it to whatever you want\n    vector: \"vec\",\n    // \"text\" is the default name for the text column in Typesense but you can change it to whatever you want\n    pageContent: \"text\",\n    // Names of the columns that you will save in your typesense schema and need to be retrieved as metadata when searching\n    metadataColumnNames: [\"foo\", \"bar\", \"baz\"],\n  },\n  // Optional search parameters to be passed to Typesense when searching\n  searchParams: {\n    q: \"*\",\n    filter_by: \"foo:[fooo]\",\n    query_by: \"\",\n  },\n  // You can override the default Typesense import function if you want to do something more complex\n  // Default import function:\n  // async importToTypesense<\n  //   T extends Record<string, unknown> = Record<string, unknown>\n  // >(data: T[], collectionName: string) {\n  //   const chunkSize = 2000;\n  //   for (let i = 0; i < data.length; i += chunkSize) {\n  //     const chunk = data.slice(i, i + chunkSize);\n\n  //     await this.caller.call(async () => {\n  //       await this.client\n  //         .collections<T>(collectionName)\n  //         .documents()\n  //         .import(chunk, { action: \"emplace\", dirty_values: \"drop\" });\n  //     });\n  //   }\n  // }\n  import: async (data, collectionName) => {\n    await vectorTypesenseClient\n      .collections(collectionName)\n      .documents()\n      .import(data, { action: \"emplace\", dirty_values: \"drop\" });\n  },\n} satisfies TypesenseConfig;\n\n/**\n * Creates a Typesense vector store from a list of documents.\n * Will update documents if there is a document with the same id, at least with the default import function.\n * @param documents list of documents to create the vector store from\n * @returns Typesense vector store\n */\nconst createVectorStoreWithTypesense = async (documents: Document[] = []) =>\n  Typesense.fromDocuments(\n    documents,\n    new OpenAIEmbeddings(),\n    typesenseVectorStoreConfig\n  );\n\n/**\n * Returns a Typesense vector store from an existing index.\n * @returns Typesense vector store\n */\nconst getVectorStoreWithTypesense = async () =>\n  new Typesense(new OpenAIEmbeddings(), typesenseVectorStoreConfig);\n\n// Do a similarity search\nconst vectorStore = await getVectorStoreWithTypesense();\nconst documents = await vectorStore.similaritySearch(\"hello world\");\n\n// Add filters based on metadata with the search parameters of Typesense\n// will exclude documents with author:JK Rowling, so if Joe Rowling & JK Rowling exists, only Joe Rowling will be returned\nvectorStore.similaritySearch(\"Rowling\", undefined, {\n  filter_by: \"author:!=JK Rowling\",\n});\n\n// Delete a document\nvectorStore.deleteDocuments([\"document_id_1\", \"document_id_2\"]);\n```\n\n----------------------------------------\n\nTITLE: Adding Example Queries for Prompt Tuning in JavaScript\nDESCRIPTION: This code adds example queries to improve the performance of the query analyzer by providing context for different types of questions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_few_shot.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst examples = []\n\nconst question = \"What's chat langchain, is it a langchain template?\"\nconst query = {\n  query: \"What is chat langchain and is it a langchain template?\",\n  subQueries: [\"What is chat langchain\", \"What is a langchain template\"],\n}\nexamples.push({ \"input\": question, \"toolCalls\": [query] })\n\nconst question2 = \"How to build multi-agent system and stream intermediate steps from it\"\nconst query2 = {\n  query: \"How to build multi-agent system and stream intermediate steps from it\",\n  subQueries: [\n    \"How to build multi-agent system\",\n    \"How to stream intermediate steps from multi-agent system\",\n    \"How to stream intermediate steps\",\n  ],\n}\n\nexamples.push({ \"input\": question2, \"toolCalls\": [query2] })\n\nconst question3 = \"LangChain agents vs LangGraph?\"\nconst query3 = {\n  query: \"What's the difference between LangChain agents and LangGraph? How do you deploy them?\",\n  subQueries: [\n    \"What are LangChain agents\",\n    \"What is LangGraph\",\n    \"How do you deploy LangChain agents\",\n    \"How do you deploy LangGraph\",\n  ],\n}\nexamples.push({ \"input\": question3, \"toolCalls\": [query3] });\n```\n\n----------------------------------------\n\nTITLE: Creating Chat Prompt Template\nDESCRIPTION: Definition of a chat prompt template for the extraction process with system and human messages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/extraction.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\nconst promptTemplate = ChatPromptTemplate.fromMessages(\n  [\n    [\n      \"system\",\n      `You are an expert extraction algorithm.\nOnly extract relevant information from the text.\nIf you do not know the value of an attribute asked to extract,\nreturn null for the attribute's value.`,\n    ],\n    [\"human\", \"{text}\"],\n  ],\n);\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Tool with Cypher Template for Movie Information\nDESCRIPTION: This snippet defines a custom tool that uses a Cypher query template to retrieve information about movies or their cast from the Neo4j database.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/graph_semantic.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst descriptionQuery = `MATCH (m:Movie|Person)\nWHERE m.title CONTAINS $candidate OR m.name CONTAINS $candidate\nMATCH (m)-[r:ACTED_IN|HAS_GENRE]-(t)\nWITH m, type(r) as type, collect(coalesce(t.name, t.title)) as names\nWITH m, type+\": \"+reduce(s=\"\", n IN names | s + n + \", \") as types\nWITH m, collect(types) as contexts\nWITH m, \"type:\" + labels(m)[0] + \"\\ntitle: \"+ coalesce(m.title, m.name) \n       + \"\\nyear: \"+coalesce(m.released,\"\") +\"\\n\" +\n       reduce(s=\"\", c in contexts | s + substring(c, 0, size(c)-2) +\"\\n\") as context\nRETURN context LIMIT 1`\n\nconst getInformation = async (entity: string) => {\n    try {\n        const data = await graph.query(descriptionQuery, { candidate: entity });\n        return data[0][\"context\"];\n    } catch (error) {\n        return \"No information was found\";\n    }\n    \n}\n```\n\n----------------------------------------\n\nTITLE: Using DeepInfra LLM with LangChain\nDESCRIPTION: Example code demonstrating how to initialize and use the DeepInfra wrapper in LangChain. Shows the setup process including API key configuration and invoking the LLM with a sample prompt.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/deep_infra.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { DeepInfra } from \"@langchain/community/llms/deepinfra\";\n\nexport const run = async () => {\n  // Use the \"meta-llama/Llama-2-70b-chat-hf\" model from DeepInfra\n  const llm = new DeepInfra({\n    // Default model is \"meta-llama/Llama-2-70b-chat-hf\"\n    model: \"meta-llama/Llama-2-70b-chat-hf\",\n  });\n\n  const result = await llm.invoke(\n    \"What would be a good company name a company that makes colorful socks?\"\n  );\n  console.log(result);\n};\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Message History with LangGraph\nDESCRIPTION: Creates a StateGraph with message history persistence using LangGraph's MessagesAnnotation and MemorySaver.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/message_history.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { START, END, MessagesAnnotation, StateGraph, MemorySaver } from \"@langchain/langgraph\";\n\nconst callModel = async (state: typeof MessagesAnnotation.State) => {\n  const response = await llm.invoke(state.messages);\n  return { messages: response };\n};\n\nconst workflow = new StateGraph(MessagesAnnotation)\n  .addNode(\"model\", callModel)\n  .addEdge(START, \"model\")\n  .addEdge(\"model\", END);\n\nconst memory = new MemorySaver();\nconst app = workflow.compile({ checkpointer: memory });\n```\n\n----------------------------------------\n\nTITLE: Setting Up and Executing Zapier NLA Agent with LangChainJS\nDESCRIPTION: This TypeScript code snippet sets up an agent using OpenAI and Zapier NLA within LangChainJS. It initializes the 'OpenAI' model and the 'ZapierNLAWrapper', sets up a toolkit using 'ZapierToolKit.fromZapierNLAWrapper', and initializes an agent executor. The primary purpose is to execute a given input command that summarizes an email and sends it to a specified Slack channel. Dependencies include '@langchain/openai' and 'langchain/tools'. Inputs are natural language commands, and the output is a processed result sent to the designated channel.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/zapier_agent.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAI } from \"@langchain/openai\";\nimport { ZapierNLAWrapper } from \"langchain/tools\";\nimport {\n  initializeAgentExecutorWithOptions,\n  ZapierToolKit,\n} from \"langchain/agents\";\n\nconst model = new OpenAI({ temperature: 0 });\nconst zapier = new ZapierNLAWrapper();\nconst toolkit = await ZapierToolKit.fromZapierNLAWrapper(zapier);\n\nconst executor = await initializeAgentExecutorWithOptions(\n  toolkit.tools,\n  model,\n  {\n    agentType: \"zero-shot-react-description\",\n    verbose: true,\n  }\n);\nconsole.log(\"Loaded agent.\");\n\nconst input = `Summarize the last email I received regarding Silicon Valley Bank. Send the summary to the #test-zapier Slack channel.`;\n\nconsole.log(`Executing with input \"${input}\"...`);\n\nconst result = await executor.invoke({ input });\n\nconsole.log(`Got output ${result.output}`);\n```\n\n----------------------------------------\n\nTITLE: Model Invocation and Generation\nDESCRIPTION: Examples of invoking the model for single and multiple completions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/ibm.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nconst result = await instance.invoke(\"Print hello world.\");\nconsole.log(result);\n\nconst results = await instance.generate([\n  \"Print hello world.\",\n  \"Print bye, bye world!\",\n]);\nconsole.log(results);\n```\n\n----------------------------------------\n\nTITLE: Assembling the Final Dynamic Few Shot Prompt in LangChain.js\nDESCRIPTION: This code demonstrates how to assemble a complete chat prompt with the dynamic few shot examples. It combines a system message, the few shot prompt with dynamically selected examples, and a human input message.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot_examples_chat.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nconst finalPrompt = ChatPromptTemplate.fromMessages(\n    [\n        [\"system\", \"You are a wondrous wizard of math.\"],\n        fewShotPrompt,\n        [\"human\", \"{input}\"],\n    ]\n)\n\nconst result = await fewShotPrompt.invoke({ input: \"What's 3+3?\" });\nconsole.log(result)\n```\n\n----------------------------------------\n\nTITLE: Setting up Calculator Tool and LLM Configuration\nDESCRIPTION: Demonstrates the setup of a calculator tool using Zod schema validation and OpenAI chat model initialization. The calculator supports basic mathematical operations (add, subtract, multiply, divide) with proper input validation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tools_few_shot.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst llm = new ChatOpenAI({ model: \"gpt-4o\", temperature: 0, })\n\nconst calculatorSchema = z.object({\n  operation: z\n    .enum([\"add\", \"subtract\", \"multiply\", \"divide\"])\n    .describe(\"The type of operation to execute.\"),\n  number1: z.number().describe(\"The first number to operate on.\"),\n  number2: z.number().describe(\"The second number to operate on.\"),\n});\n\nconst calculatorTool = tool(async ({ operation, number1, number2 }) => {\n  // Functions must return strings\n  if (operation === \"add\") {\n    return `${number1 + number2}`;\n  } else if (operation === \"subtract\") {\n    return `${number1 - number2}`;\n  } else if (operation === \"multiply\") {\n    return `${number1 * number2}`;\n  } else if (operation === \"divide\") {\n    return `${number1 / number2}`;\n  } else {\n    throw new Error(\"Invalid operation.\");\n  }\n}, {\n  name: \"calculator\",\n  description: \"Can perform mathematical operations.\",\n  schema: calculatorSchema,\n});\n\nconst llmWithTools = llm.bindTools([calculatorTool]);\n```\n\n----------------------------------------\n\nTITLE: Complete Vector Search Example with Couchbase\nDESCRIPTION: Full example demonstrating how to load documents, split them into chunks, index them in Couchbase, and perform similarity searches. It includes functionality for retrieving similarity scores and specifying fields to return.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/couchbase.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport {\n  CouchbaseVectorStoreArgs,\n  CouchbaseVectorStore,\n} from \"@langchain/community/vectorstores/couchbase\";\nimport { Cluster } from \"couchbase\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\nimport { CharacterTextSplitter } from \"@langchain/textsplitters\";\n\nconst connectionString =\n  process.env.COUCHBASE_DB_CONN_STR ?? \"couchbase://localhost\";\nconst databaseUsername = process.env.COUCHBASE_DB_USERNAME ?? \"Administrator\";\nconst databasePassword = process.env.COUCHBASE_DB_PASSWORD ?? \"Password\";\n\n// Load documents from file\nconst loader = new TextLoader(\"./state_of_the_union.txt\");\nconst rawDocuments = await loader.load();\nconst splitter = new CharacterTextSplitter({\n  chunkSize: 500,\n  chunkOverlap: 0,\n});\nconst docs = await splitter.splitDocuments(rawDocuments);\n\nconst couchbaseClient = await Cluster.connect(connectionString, {\n  username: databaseUsername,\n  password: databasePassword,\n  configProfile: \"wanDevelopment\",\n});\n\n// Open AI API Key is required to use OpenAIEmbeddings, some other embeddings may also be used\nconst embeddings = new OpenAIEmbeddings({\n  apiKey: process.env.OPENAI_API_KEY,\n});\n\nconst couchbaseConfig: CouchbaseVectorStoreArgs = {\n  cluster: couchbaseClient,\n  bucketName: \"testing\",\n  scopeName: \"_default\",\n  collectionName: \"_default\",\n  indexName: \"vector-index\",\n  textKey: \"text\",\n  embeddingKey: \"embedding\",\n};\n\nconst store = await CouchbaseVectorStore.fromDocuments(\n  docs,\n  embeddings,\n  couchbaseConfig\n);\n\nconst query = \"What did president say about Ketanji Brown Jackson\";\n\nconst resultsSimilaritySearch = await store.similaritySearch(query);\nconsole.log(\"resulting documents: \", resultsSimilaritySearch[0]);\n\n// Similarity Search With Score\nconst resultsSimilaritySearchWithScore = await store.similaritySearchWithScore(\n  query,\n  1\n);\nconsole.log(\"resulting documents: \", resultsSimilaritySearchWithScore[0][0]);\nconsole.log(\"resulting scores: \", resultsSimilaritySearchWithScore[0][1]);\n\nconst result = await store.similaritySearch(query, 1, {\n  fields: [\"metadata.source\"],\n});\nconsole.log(result[0]);\n```\n\n----------------------------------------\n\nTITLE: Creating the Complete RAG Fusion Chain\nDESCRIPTION: Assembles the final workflow by connecting the query generation, retrieval, and fusion components into a single chain. This sequence generates multiple queries from the original, retrieves documents for each query, and then fuses the results.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/rag_fusion.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nconst chain = RunnableSequence.from([\n  generateQueries,\n  retriever.map(),\n  reciprocalRankFusion,\n]);\n```\n\n----------------------------------------\n\nTITLE: Querying ZepVectorStore with Metadata Filters\nDESCRIPTION: Example showing how to query the ZepVectorStore using metadata filters to refine search results.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/zep.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n{ExampleMetadata}\n```\n\n----------------------------------------\n\nTITLE: Stream Method Implementation in TypeScript\nDESCRIPTION: Demonstrates using the .stream() method to handle streaming responses from an LLM. Shows how to iterate over the stream and process chunks of text as they arrive.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming_llm.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nStreamMethodExample\n```\n\n----------------------------------------\n\nTITLE: Performing Basic Similarity Search with Matching Engine (TypeScript)\nDESCRIPTION: Demonstrates a standard k-nearest neighbor (KNN) similarity search using the `similaritySearch` method of the `MatchingEngine`. It retrieves documents whose embeddings are most similar to the embedding of the query string \"this\".\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/googlevertexai.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst results = await engine.similaritySearch(\"this\");\n```\n\n----------------------------------------\n\nTITLE: Initializing Pinecone Vector Store with Namespaces\nDESCRIPTION: JavaScript code to initialize Pinecone vector store with OpenAI embeddings and add documents to different namespaces for multi-user support.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_per_user.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { PineconeStore } from \"@langchain/pinecone\";\nimport { Pinecone } from \"@pinecone-database/pinecone\";\nimport { Document } from \"@langchain/core/documents\";\n\nconst embeddings = new OpenAIEmbeddings();\n\nconst pinecone = new Pinecone();\n\nconst pineconeIndex = pinecone.Index(process.env.PINECONE_INDEX);\n\n/**\n * Pinecone allows you to partition the records in an index into namespaces. \n * Queries and other operations are then limited to one namespace, \n * so different requests can search different subsets of your index.\n * Read more about namespaces here: https://docs.pinecone.io/guides/indexes/use-namespaces\n * \n * NOTE: If you have namespace enabled in your Pinecone index, you must provide the namespace when creating the PineconeStore.\n */\nconst namespace = \"pinecone\";\n\nconst vectorStore = await PineconeStore.fromExistingIndex(\n  new OpenAIEmbeddings(),\n  { pineconeIndex, namespace },\n);\n\nawait vectorStore.addDocuments(\n  [new Document({ pageContent: \"i worked at kensho\" })],\n  { namespace: \"harrison\" },\n);\n\nawait vectorStore.addDocuments(\n  [new Document({ pageContent: \"i worked at facebook\" })],\n  { namespace: \"ankush\" },\n);\n```\n\n----------------------------------------\n\nTITLE: Testing RAG Application\nDESCRIPTION: Example code showing how to invoke the RAG application with a test question.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nlet inputs = { question: \"What is Task Decomposition?\" };\n\nconst result = await graph.invoke(inputs);\nconsole.log(result.answer)\n```\n\n----------------------------------------\n\nTITLE: Testing Document Chain without Context\nDESCRIPTION: Tests the document chain with the same query but with an empty context to demonstrate its behavior when no relevant information is available.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_retrieval.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: javascript\nCODE:\n```\nawait documentChain.invoke({\n  messages: [new HumanMessage(\"Can LangSmith help test my LLM applications?\")],\n  context: [],\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Anthropic Tool Calling with Base64 Image\nDESCRIPTION: Shows how to use Anthropic's Claude model with base64-encoded image data. Demonstrates file reading and message formatting for image processing.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_calls_multimodal.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport * as fs from \"node:fs/promises\";\n\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport { HumanMessage } from \"@langchain/core/messages\";\n\nconst imageData = await fs.readFile(\"../../data/sunny_day.jpeg\");\n\nconst model = new ChatAnthropic({\n  model: \"claude-3-sonnet-20240229\",\n}).bindTools([weatherTool]);\n\nconst message = new HumanMessage({\n  content: [\n    {\n      type: \"text\",\n      text: \"describe the weather in this image\",\n    },\n    {\n      type: \"image_url\",\n      image_url: {\n        url: `data:image/jpeg;base64,${imageData.toString(\"base64\")}`,\n      },\n    },\n  ],\n});\n\nconst response = await model.invoke([message]);\n\nconsole.log(response.tool_calls);\n```\n\n----------------------------------------\n\nTITLE: Loading Documents with PDFLoader\nDESCRIPTION: Loads documents from the PDF file using the load() method and accesses the first document.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/pdf.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst docs = await loader.load()\ndocs[0]\n```\n\n----------------------------------------\n\nTITLE: Invoking LLM with Tools and Generating Tool Calls in JavaScript\nDESCRIPTION: This code demonstrates how to bind tools to an LLM, create a conversation history, and invoke the model to generate tool calls based on a user query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_results_pass_to_model.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { HumanMessage } from \"@langchain/core/messages\";\n\nconst llmWithTools = llm.bindTools(tools);\n\nconst messages = [\n  new HumanMessage(\"What is 3 * 12? Also, what is 11 + 49?\"),\n];\n\nconst aiMessage = await llmWithTools.invoke(messages);\n\nconsole.log(aiMessage);\n\nmessages.push(aiMessage);\n```\n\n----------------------------------------\n\nTITLE: Querying for Similar Documents (TypeScript)\nDESCRIPTION: This snippet performs a similarity search on the vector store, retrieving documents most similar to a given query (e.g., 'biology'). It accepts a filter object (metadata-based), the number of results to fetch, and then iterates over the results for display. This allows targeted information retrieval using embeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/chroma.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst filter = { source: \"https://example.com\" };\n\nconst similaritySearchResults = await vectorStore.similaritySearch(\"biology\", 2, filter);\n\nfor (const doc of similaritySearchResults) {\n  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);\n}\n```\n\n----------------------------------------\n\nTITLE: Splitting Documents with RecursiveCharacterTextSplitter\nDESCRIPTION: Shows how to split Document objects directly using RecursiveCharacterTextSplitter. Creates Document instances first and then splits them into chunks.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/recursive_text_splitter.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { Document } from \"@langchain/core/documents\";\nimport { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\n\nconst text = `Hi.\\n\\nI'm Harrison.\\n\\nHow? Are? You?\\nOkay then f f f f.\nThis is a weird text to write, but gotta test the splittingggg some how.\\n\\n\nBye!\\n\\n-H.`;\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 10,\n  chunkOverlap: 1,\n});\n\nconst docOutput = await splitter.splitDocuments([\n  new Document({ pageContent: text }),\n]);\n\nconsole.log(docOutput.slice(0, 3));\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Graph Database Query Generation\nDESCRIPTION: This snippet shows the npm command to install the required dependencies for the project, including LangChain, OpenAI, Neo4j driver, and Zod.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/graph_mapping.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install langchain @langchain/community @langchain/openai @langchain/core neo4j-driver zod\n```\n\n----------------------------------------\n\nTITLE: Creating a Few-Shot Prompt for SQL Query Generation\nDESCRIPTION: TypeScript code demonstrating how to create a few-shot prompt using predefined examples for improving SQL query generation in LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/sql_prompting.mdx#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n<CodeBlock language=\"typescript\">{FewShotExample}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Creating FewShotPromptTemplate in JavaScript\nDESCRIPTION: Initializes a FewShotPromptTemplate with examples and a formatter, then formats a prompt with a given input.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot_examples.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { FewShotPromptTemplate } from \"@langchain/core/prompts\";\n\nconst prompt = new FewShotPromptTemplate({\n    examples,\n    examplePrompt,\n    suffix: \"Question: {input}\",\n    inputVariables: [\"input\"],\n})\n\nconst formatted = await prompt.format({ input: \"Who was the father of Mary Ball Washington?\" })\nconsole.log(formatted.toString())\n```\n\n----------------------------------------\n\nTITLE: Instantiating Upstash Vector Store with Built-in Embeddings (FakeEmbeddings) - TypeScript/JavaScript\nDESCRIPTION: This example shows how to use the built-in Upstash embedding support by passing a FakeEmbeddings instance to the UpstashVectorStore constructor. This is useful when Upstash provides native embedding generation at index time and no external embedding model is required. Be sure to select an embedding model during index creation via the Upstash Console. Prerequisites include @langchain/community, @langchain/core (for FakeEmbeddings), and @upstash/vector packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/upstash.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { UpstashVectorStore } from \"@langchain/community/vectorstores/upstash\";\nimport { FakeEmbeddings } from \"@langchain/core/utils/testing\";\n\nimport { Index } from \"@upstash/vector\";\n\nconst indexWithEmbeddings = new Index({\n  url: process.env.UPSTASH_VECTOR_REST_URL,\n  token: process.env.UPSTASH_VECTOR_REST_TOKEN,\n});\n\nconst vectorStore = new UpstashVectorStore(new FakeEmbeddings(), {\n  index: indexWithEmbeddings,\n});\n```\n\n----------------------------------------\n\nTITLE: Assembling RAG Chain\nDESCRIPTION: Create final retrieval chain combining history-aware retriever with question-answering functionality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport { createStuffDocumentsChain } from \"langchain/chains/combine_documents\";\nimport { createRetrievalChain } from \"langchain/chains/retrieval\";\n\nconst systemPrompt = \n  \"You are an assistant for question-answering tasks. \" +\n  \"Use the following pieces of retrieved context to answer \" +\n  \"the question. If you don't know the answer, say that you \" +\n  \"don't know. Use three sentences maximum and keep the \" +\n  \"answer concise.\" +\n  \"\\n\\n\" +\n  \"{context}\";\n\nconst qaPrompt = ChatPromptTemplate.fromMessages([\n  [\"system\", systemPrompt],\n  new MessagesPlaceholder(\"chat_history\"),\n  [\"human\", \"{input}\"],\n]);\n\nconst questionAnswerChain = await createStuffDocumentsChain({\n  llm,\n  prompt: qaPrompt,\n});\n\nconst ragChain = await createRetrievalChain({\n  retriever: historyAwareRetriever,\n  combineDocsChain: questionAnswerChain,\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Complex State Management\nDESCRIPTION: Sets up a StateGraph with custom state annotation including both messages and language parameter, with persistence handling.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/message_history.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { START, END, StateGraph, MemorySaver, MessagesAnnotation, Annotation } from \"@langchain/langgraph\";\n\nconst GraphAnnotation = Annotation.Root({\n  language: Annotation<string>(),\n  ...MessagesAnnotation.spec,\n})\n\nconst callModel2 = async (state: typeof GraphAnnotation.State) => {\n  const response = await runnable.invoke(state);\n  return { messages: [response] };\n};\n\nconst workflow2 = new StateGraph(GraphAnnotation)\n  .addNode(\"model\", callModel2)\n  .addEdge(START, \"model\")\n  .addEdge(\"model\", END);\n\nconst app2 = workflow2.compile({ checkpointer: new MemorySaver() });\n```\n\n----------------------------------------\n\nTITLE: Creating Baseline RAG Chain\nDESCRIPTION: Assembles the baseline retrieval-augmented generation chain using RunnableSequence, combining retrieval, prompt formatting, and model inference.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/rewrite.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst chain = RunnableSequence.from([\n  {\n    context: retriever.pipe(formatDocs),\n    question: new RunnablePassthrough()\n  },\n  prompt,\n  model,\n  new StringOutputParser()\n]);\n```\n\n----------------------------------------\n\nTITLE: Implementing a Simple Agent in TypeScript with LangChain\nDESCRIPTION: This code snippet demonstrates the implementation of a simple agent using LangChain in TypeScript. It includes the setup of tools, a language model, and an agent executor to answer a complex question about a film director.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport SimpleAgent from \"@examples/guides/debugging/simple_agent.ts\";\n```\n\n----------------------------------------\n\nTITLE: Setting and Getting Multiple Key-Value Pairs\nDESCRIPTION: Demonstrates how to store multiple key-value pairs using mset and retrieve them using mget with encoding/decoding.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/stores/file_system.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nawait kvStore.mset(\n  [\n    [\"key1\", encoder.encode(\"value1\")],\n    [\"key2\", encoder.encode(\"value2\")],\n  ]\n)\n\nconst results = await kvStore.mget(\n  [\n    \"key1\",\n    \"key2\",\n  ]\n)\nconsole.log(results.map((v) => decoder.decode(v)));\n```\n\n----------------------------------------\n\nTITLE: Initializing VoyageEmbeddings with Custom Parameters in TypeScript\nDESCRIPTION: This snippet demonstrates how to create a new instance of VoyageEmbeddings with various optional parameters. It shows how to set the API key, input type, truncation, output dimension, output data type, and encoding format.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/voyageai.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { VoyageEmbeddings } from \"@langchain/community/embeddings/voyage\";\n\nconst embeddings = new VoyageEmbeddings({\n  apiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.VOYAGEAI_API_KEY\n  inputType: \"document\", // Optional: specify input type as 'query', 'document', or omit for None / Undefined / Null\n  truncation: true, // Optional: enable truncation of input texts\n  outputDimension: 768, // Optional: set desired output embedding dimension\n  outputDtype: \"float\", // Optional: set output data type (\"float\" or \"int8\")\n  encodingFormat: \"float\", // Optional: set output encoding format (\"float\", \"base64\", or \"ubinary\")\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatOpenAI Model in TypeScript\nDESCRIPTION: Code snippet to initialize a ChatOpenAI model instance with a specific model name (gpt-4o-mini) for use in the chatbot application.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/chatbot.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst llm = new ChatOpenAI({ model: \"gpt-4o-mini\" })\n```\n\n----------------------------------------\n\nTITLE: Converting Text to Graph Documents using LLMGraphTransformer\nDESCRIPTION: This code shows how to use LLMGraphTransformer to convert a text document about Marie Curie into graph documents, and log the number of nodes and relationships.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/graph_constructing.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Document } from \"@langchain/core/documents\";\n\nlet text = `\nMarie Curie, was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity.\nShe was the first woman to win a Nobel Prize, the first person to win a Nobel Prize twice, and the only person to win a Nobel Prize in two scientific fields.\nHer husband, Pierre Curie, was a co-winner of her first Nobel Prize, making them the first-ever married couple to win the Nobel Prize and launching the Curie family legacy of five Nobel Prizes.\nShe was, in 1906, the first woman to become a professor at the University of Paris.\n`\n\nconst result = await llmGraphTransformer.convertToGraphDocuments([\n    new Document({ pageContent: text }),\n]);\n\nconsole.log(`Nodes: ${result[0].nodes.length}`);\nconsole.log(`Relationships:${result[0].relationships.length}`);\n```\n\n----------------------------------------\n\nTITLE: Executing Revision Loop\nDESCRIPTION: Implements a loop that continues to revise the output until it passes validation or reaches a maximum number of attempts. Includes error handling and tracing.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/basic_critique_revise.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nlet fixCount = 0;\n\nconst formattedOriginalPrompt = await generatePrompt.format({\n  query: userQuery\n});\n\ntry {\n  while (!validatorResult.success && fixCount < 5) {\n    result = await reviseChain.invoke({\n      original_prompt: formattedOriginalPrompt,\n      completion: JSON.stringify(result),\n      error: JSON.stringify(validatorResult.error),\n    }, { callbacks: groupManager });\n    validatorResult = outputValidator(result);\n    fixCount++;\n  }\n} finally {\n  await traceGroup.end();\n}\n\nconsole.log(JSON.stringify(validatorResult, null, 2));\n```\n\n----------------------------------------\n\nTITLE: Chaining ChatXAI with Prompt Templates\nDESCRIPTION: Illustrates how to create a processing chain by combining a ChatPromptTemplate with the ChatXAI model for more dynamic interactions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/xai.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\"\n\nconst prompt = ChatPromptTemplate.fromMessages(\n  [\n    [\n      \"system\",\n      \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n    ],\n    [\"human\", \"{input}\"],\n  ]\n)\n\nconst chain = prompt.pipe(llm);\nawait chain.invoke(\n  {\n    input_language: \"English\",\n    output_language: \"German\",\n    input: \"I love programming.\",\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG without LangGraph in JavaScript\nDESCRIPTION: This snippet shows how to implement the same RAG application logic without using LangGraph, demonstrating direct invocations of individual components.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#2025-04-22_snippet_14\n\nLANGUAGE: javascript\nCODE:\n```\nlet question = \"...\"\n\nconst retrievedDocs = await vectorStore.similaritySearch(question)\nconst docsContent = retrievedDocs.map(doc => doc.pageContent).join(\"\\n\");\nconst messages = await promptTemplate.invoke({ question: question, context: docsContent });\nconst answer = await llm.invoke(messages);\n```\n\n----------------------------------------\n\nTITLE: Querying MariaDB Vector Store by Similarity Search - TypeScript\nDESCRIPTION: Illustrates how to perform a similarity search on the vector store for the term 'biology', retrieve the top 2 results, and filter by a year attribute in metadata. The similaritySearch method returns matching documents, which are then logged to console. Inputs include the query text, result count, and filter object; dependency is the previously initialized vectorStore.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/mariadb.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst similaritySearchResults = await vectorStore.similaritySearch(\"biology\", 2, { \"year\": 2021 });\nfor (const doc of similaritySearchResults) {\n  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);\n}\n\n```\n\n----------------------------------------\n\nTITLE: Storing Graph Documents in Neo4j Database\nDESCRIPTION: This code snippet shows how to store the generated graph documents into a Neo4j database using the addGraphDocuments method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/graph_constructing.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nawait graph.addGraphDocuments(result_filtered)\n```\n\n----------------------------------------\n\nTITLE: Using Function Calls with Minimax in LangChain.js\nDESCRIPTION: Illustrates how to use function calls with Minimax models in LangChain.js, enabling more structured interactions and specific output formats.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/minimax.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport MinimaxFunctions from \"@examples/models/chat/minimax_functions.ts\";\n```\n\n----------------------------------------\n\nTITLE: Creating LCEL Chain for Weather Tool\nDESCRIPTION: Constructing a LangChain Expression Language chain with prompt, model, and output parser.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/stream_tool_client.mdx#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst chain = prompt.pipe(modelWithTools).pipe(\n  new JsonOutputKeyToolsParser<z.infer<typeof Weather>>({\n    keyName: \"get_weather\",\n    zodSchema: Weather,\n  })\n);\n```\n\n----------------------------------------\n\nTITLE: Creating a Self-Query Retriever with Chroma\nDESCRIPTION: This code demonstrates how to create a SelfQueryRetriever using the Chroma vector store, a language model, and a ChromaTranslator for structured queries.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/chroma.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { SelfQueryRetriever } from \"langchain/retrievers/self_query\";\nimport { ChromaTranslator } from \"@langchain/community/structured_query/chroma\";\n\nconst selfQueryRetriever = SelfQueryRetriever.fromLLM({\n  llm,\n  vectorStore,\n  /** A short summary of what the document contents represent. */\n  documentContents: \"Brief summary of a movie\",\n  attributeInfo,\n  /**\n   * We need to create a basic translator that translates the queries into a\n   * filter format that the vector store can understand. We provide a basic translator\n   * translator here, but you can create your own translator by extending BaseTranslator\n   * abstract class. Note that the vector store needs to support filtering on the metadata\n   * attributes you want to query on.\n   */\n  structuredQueryTranslator: new ChromaTranslator(),\n});\n```\n\n----------------------------------------\n\nTITLE: Streaming with Non-Streaming Components in JavaScript\nDESCRIPTION: This snippet shows how streaming works with non-streaming components like Retrievers in a chain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\nconst template = `Answer the question based only on the following context:\n{context}\n\nQuestion: {question}\n`;\nconst prompt = ChatPromptTemplate.fromTemplate(template);\n\nconst vectorstore = await MemoryVectorStore.fromTexts(\n  [\"mitochondria is the powerhouse of the cell\", \"buildings are made of brick\"],\n  [{}, {}],\n  new OpenAIEmbeddings(),\n);\n\nconst retriever = vectorstore.asRetriever();\n\nconst chunks = [];\n\nfor await (const chunk of await retriever.stream(\"What is the powerhouse of the cell?\")) {\n  chunks.push(chunk);\n}\n\nconsole.log(chunks);\n```\n\n----------------------------------------\n\nTITLE: Cancelling Stream with AbortController in TypeScript\nDESCRIPTION: Shows how to cancel a streaming response using an AbortController with a timeout. This method provides more immediate cancellation than using break statements, terminating execution before any chunks are processed.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/cancel_execution.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst controllerForStream = new AbortController();\n\nconst startTimer3 = console.time(\"timer3\");\n\nsetTimeout(() => controllerForStream.abort(), 100);\n\ntry {\n  const streamWithSignal = await chain.stream(\"what is the current weather in SF?\", {\n    signal: controllerForStream.signal\n  });\n  for await (const chunk of streamWithSignal) {\n    console.log(chunk);\n    break;\n  } \n} catch (e) {\n  console.log(e);  \n}\n\nconsole.timeEnd(\"timer3\");\n```\n\n----------------------------------------\n\nTITLE: Tool Calling with Strict Mode\nDESCRIPTION: Demonstrates how to use OpenAI's strict mode with tool calling to enforce correct schema adherence, which requires @langchain/openai >= 0.2.6.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n\nconst weatherTool = tool((_) => \"no-op\", {\n  name: \"get_current_weather\",\n  description: \"Get the current weather\",\n  schema: z.object({\n    location: z.string(),\n  }),\n})\n\nconst llmWithStrictTrue = new ChatOpenAI({\n  model: \"gpt-4o\",\n}).bindTools([weatherTool], {\n  strict: true,\n  tool_choice: weatherTool.name,\n});\n\n// Although the question is not about the weather, it will call the tool with the correct arguments\n// because we passed `tool_choice` and `strict: true`.\nconst strictTrueResult = await llmWithStrictTrue.invoke(\"What is 127862 times 12898 divided by 2?\");\n\nconsole.dir(strictTrueResult.tool_calls, { depth: null });\n```\n\n----------------------------------------\n\nTITLE: Adding Multiple Documents with Metadata to Matching Engine (TypeScript)\nDESCRIPTION: Illustrates adding multiple `Document` objects, each containing `pageContent` and associated `metadata`. This metadata (e.g., 'color', 'category') is converted into Matching Engine 'allow list' values, enabling filtered similarity searches later.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/googlevertexai.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst documents = [\n  new Document({\n    pageContent: \"this apple\",\n    metadata: {\n      color: \"red\",\n      category: \"edible\",\n    },\n  }),\n  new Document({\n    pageContent: \"this blueberry\",\n    metadata: {\n      color: \"blue\",\n      category: \"edible\",\n    },\n  }),\n  new Document({\n    pageContent: \"this firetruck\",\n    metadata: {\n      color: \"red\",\n      category: \"machine\",\n    },\n  }),\n];\n\n// Add all our documents\nawait engine.addDocuments(documents);\n```\n\n----------------------------------------\n\nTITLE: Adding Documents to Chroma Vector Store (TypeScript)\nDESCRIPTION: This snippet demonstrates defining four documents with identical metadata and distinct page content for testing vectors. The documents are created as typed objects and then added to the vector store asynchronously, each with a specified ID. The method addDocuments requires an array of document objects and an optional map of IDs.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/chroma.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport type { Document } from \"@langchain/core/documents\";\n\nconst document1: Document = {\n  pageContent: \"The powerhouse of the cell is the mitochondria\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document2: Document = {\n  pageContent: \"Buildings are made out of brick\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document3: Document = {\n  pageContent: \"Mitochondria are made out of lipids\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document4: Document = {\n  pageContent: \"The 2024 Olympics are in Paris\",\n  metadata: { source: \"https://example.com\" }\n}\n\nconst documents = [document1, document2, document3, document4];\n\nawait vectorStore.addDocuments(documents, { ids: [\"1\", \"2\", \"3\", \"4\"] });\n```\n\n----------------------------------------\n\nTITLE: Configuring and Using Arcjet Redact with OpenAI in JavaScript\nDESCRIPTION: This snippet demonstrates how to set up and use Arcjet Redact with an OpenAI model in a JavaScript environment. It includes configuration options for entity detection, custom detection functions, and replacement strategies.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/arcjet.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport {\n  ArcjetRedact,\n  ArcjetSensitiveInfoType,\n} from \"@langchain/community/llms/arcjet\";\nimport { OpenAI } from \"@langchain/openai\";\n\n// Create an instance of another LLM for Arcjet to wrap\nconst openai = new OpenAI({\n  modelName: \"gpt-3.5-turbo-instruct\",\n  openAIApiKey: process.env.OPENAI_API_KEY,\n});\n\nconst arcjetRedactOptions = {\n  // Specify a LLM that Arcjet Redact will call once it has redacted the input.\n  llm: openai,\n\n  // Specify the list of entities that should be redacted.\n  // If this isn't specified then all entities will be redacted.\n  entities: [\"email\", \"phone-number\", \"ip-address\", \"credit-card\"] as ArcjetSensitiveInfoType[],\n\n  // You can provide a custom detect function to detect entities that we don't support yet.\n  // It takes a list of tokens and you return a list of identified types or undefined.\n  // The undefined types that you return should be added to the entities list if used.\n  detect: (tokens: string[]) => {\n    return tokens.map((t) => t === \"some-sensitive-info\" ? \"custom-entity\" : undefined)\n  },\n\n  // The number of tokens to provide to the custom detect function. This defaults to 1.\n  // It can be used to provide additional context when detecting custom entity types.\n  contextWindowSize: 1,\n\n  // This allows you to provide custom replacements when redacting. Please ensure\n  // that the replacements are unique so that unredaction works as expected.\n  replace: (identifiedType: string) => {\n    return identifiedType === \"email\" ? \"redacted@example.com\" : undefined;\n  },\n\n};\n\nconst arcjetRedact = new ArcjetRedact(arcjetRedactOptions);\nconst response = await arcjetRedact.invoke(\n  \"My email address is test@example.com, here is some-sensitive-info\"\n);\n```\n\n----------------------------------------\n\nTITLE: Creating a Chain with ChatPromptTemplate and ChatOpenAI\nDESCRIPTION: Shows how to create a chain by combining a ChatPromptTemplate with a ChatOpenAI model to create a language translation pipeline.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\"\n\nconst prompt = ChatPromptTemplate.fromMessages(\n  [\n    [\n      \"system\",\n      \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n    ],\n    [\"human\", \"{input}\"],\n  ]\n)\n\nconst chain = prompt.pipe(llm);\nawait chain.invoke(\n  {\n    input_language: \"English\",\n    output_language: \"German\",\n    input: \"I love programming.\",\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatOpenAI with File Search Tool in TypeScript\nDESCRIPTION: This example shows how to create a ChatOpenAI instance with the file search tool. It uses the 'gpt-4o-mini' model and requires a vector store ID for searching files. The LLM is then invoked with a query about OpenAI research.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst llm = new ChatOpenAI({ model: \"gpt-4o-mini\" }).bindTools([\n  { type: \"file_search\", vector_store_ids: [\"vs...\"] },\n]);\n\nawait llm.invoke(\"Is deep research by OpenAI?\");\n```\n\n----------------------------------------\n\nTITLE: Importing and Using YandexGPT in TypeScript\nDESCRIPTION: Example code demonstrating how to import and use YandexGPT in a TypeScript environment with LangChain.js. The actual implementation details are not provided in the snippet.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/yandex.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nYandexGPTExample\n```\n\n----------------------------------------\n\nTITLE: Initializing OllamaFunctions Model\nDESCRIPTION: Demonstrates how to initialize the OllamaFunctions wrapper with Mistral model and temperature settings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/ollama_functions.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OllamaFunctions } from \"@langchain/community/experimental/chat_models/ollama_functions\";\n\nconst model = new OllamaFunctions({\n  temperature: 0.1,\n  model: \"mistral\",\n});\n```\n\n----------------------------------------\n\nTITLE: Checking Custom Prompt Format in TypeScript\nDESCRIPTION: Shows how to inspect the formatted custom prompt that includes JSON schema instructions before sending it to the model, useful for validation and debugging.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/structured_output.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst query = \"Anna is 23 years old and she is 6 feet tall\"\n\nconsole.log((await prompt.format({ query })).toString())\n```\n\n----------------------------------------\n\nTITLE: Streaming Granular Events with streamEvents() Method\nDESCRIPTION: Shows how to use the streamEvents() method to capture more detailed events from a chat model. This approach is useful in complex LLM applications with multiple processing steps.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chat_streaming.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst eventStream = await model.streamEvents(\n  \"Write me a 1 verse song about goldfish on the moon\",\n  {\n    version: \"v2\"\n  },\n);\n\nconst events = [];\nfor await (const event of eventStream) {\n  events.push(event);\n}\n\nevents.slice(0, 3);\n```\n\n----------------------------------------\n\nTITLE: Initializing MemoryVectorStore in LangChain\nDESCRIPTION: This snippet shows how to initialize a MemoryVectorStore, which is an in-memory vector store implementation in LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/retrievers.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n\nconst vectorStore = new MemoryVectorStore(embeddings);\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for LangChain and LangGraph\nDESCRIPTION: This snippet shows how to install the necessary npm packages for the project, including LangChain and LangGraph.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/sql_qa.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i langchain @langchain/community @langchain/langgraph\n```\n\n----------------------------------------\n\nTITLE: Creating a Basic Prompt and Model Chain with LangChain\nDESCRIPTION: This code snippet demonstrates how to create a basic chain consisting of a ChatPromptTemplate, ChatOpenAI model, and StringOutputParser for solving algebraic equations. It imports necessary components, defines a prompt template, initializes a model, and chains them together using .pipe() method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/binding.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst prompt = ChatPromptTemplate.fromMessages(\n    [\n        [\n            \"system\",\n            \"Write out the following equation using algebraic symbols then solve it. Use the format\\n\\nEQUATION:...\\nSOLUTION:...\\n\\n\",\n        ],\n        [\"human\", \"{equation_statement}\"],\n    ]\n)\n\nconst model = new ChatOpenAI({ temperature: 0 });\n\nconst runnable = prompt.pipe(model).pipe(new StringOutputParser());\n\nconst res = await runnable.invoke({\n  equation_statement: \"x raised to the third plus seven equals 12\"\n});\n\nconsole.log(res);\n```\n\n----------------------------------------\n\nTITLE: Embedding Multiple Documents\nDESCRIPTION: Example of embedding multiple texts using embedDocuments method for indexing purposes.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/google_generativeai.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconst text2 = \"LangGraph is a library for building stateful, multi-actor applications with LLMs\";\n\nconst vectors = await embeddings.embedDocuments([text, text2]);\n\nconsole.log(vectors[0].slice(0, 100));\nconsole.log(vectors[1].slice(0, 100));\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Community Package for Friendli Integration\nDESCRIPTION: Command to install the necessary packages for using Friendli with LangChain. Requires both @langchain/community and @langchain/core packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/friendli.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Integrating a RAG Chain Tool with an Agent for Styled Responses in TypeScript\nDESCRIPTION: This example demonstrates converting a RAG chain into a tool and using it with a ReAct agent. The agent is tasked with answering what a pirate would say about dogs, showcasing how it can use a complex RAG tool to generate styled responses.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/convert_runnable_to_tool.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst ragTool = ragChain.asTool({\n  name: \"pet_expert\",\n  description: \"Get information about pets.\",\n  schema: z.object({\n    context: z.string(),\n    question: z.string(),\n    answer_style: z.string(),\n  }),\n});\n\nconst agent = createReactAgent({ llm: llm, tools: [ragTool] });\n\nconst stream = await agent.stream({\n  messages: [\n    [\"human\", \"What would a pirate say dogs are known for?\"]\n  ]\n});\n\nfor await (const chunk of stream) {\n  // Log output from the agent or tools node\n  if (chunk.agent) {\n    console.log(\"AGENT:\", chunk.agent.messages[0]);\n  } else if (chunk.tools) {\n    console.log(\"TOOLS:\", chunk.tools.messages[0]);\n  }\n  console.log(\"----\");\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Vector Store with Similarity Scores (TypeScript)\nDESCRIPTION: This snippet queries the vector store for similar documents and retrieves their corresponding similarity scores. The method similaritySearchWithScore returns an array of [document, score] pairs. The code loop prints each document's content, metadata, and its similarity score in a formatted manner.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/chroma.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst similaritySearchWithScoreResults = await vectorStore.similaritySearchWithScore(\"biology\", 2, filter)\n\nfor (const [doc, score] of similaritySearchWithScoreResults) {\n  console.log(`* [SIM=${score.toFixed(3)}] ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`);\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Vector Store from Text Chunks in TypeScript\nDESCRIPTION: This example shows how to create a vector store directly from prepared text chunks using MemoryVectorStore and OpenAIEmbeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/vectorstores.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n\nconst vectorStore = await MemoryVectorStore.fromTexts(\n  [\"Hello world\", \"Bye bye\", \"hello nice world\"],\n  [{ id: 2 }, { id: 1 }, { id: 3 }],\n  new OpenAIEmbeddings()\n);\n\nconst resultOne = await vectorStore.similaritySearch(\"hello world\", 1);\nconsole.log(resultOne);\n\nconst resultTwo = await vectorStore.similaritySearchWithScore(\"hello world\", 1);\nconsole.log(resultTwo);\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatOpenAI Model\nDESCRIPTION: Configuration and initialization of the ChatOpenAI model for text summarization.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/summarization.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatOpenAI } from '@langchain/openai';\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o-mini\",\n  temperature: 0,\n})\n```\n\n----------------------------------------\n\nTITLE: Adding Documents to HNSWLib Vector Store\nDESCRIPTION: Creates document objects with page content and metadata, then adds them to the HNSWLib vector store. This example demonstrates adding multiple documents in a batch operation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/hnswlib.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport type { Document } from \"@langchain/core/documents\";\n\nconst document1: Document = {\n  pageContent: \"The powerhouse of the cell is the mitochondria\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document2: Document = {\n  pageContent: \"Buildings are made out of brick\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document3: Document = {\n  pageContent: \"Mitochondria are made out of lipids\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document4: Document = {\n  pageContent: \"The 2024 Olympics are in Paris\",\n  metadata: { source: \"https://example.com\" }\n}\n\nconst documents = [document1, document2, document3, document4];\n\nawait vectorStore.addDocuments(documents);\n```\n\n----------------------------------------\n\nTITLE: LangChain Indexing API Implementation in TypeScript\nDESCRIPTION: Example code showing how to use LangChain's indexing API to synchronize documents with a vector store, handling document deletion modes and record management.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/indexing.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport QuickStartExample from \"@examples/indexes/indexing_api/indexing.ts\"\n```\n\n----------------------------------------\n\nTITLE: Setting Google Cloud Credentials for Node.js\nDESCRIPTION: Setting the environment variable for Google Cloud credentials file path in a Node.js environment\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/google_vertex_ai.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport GOOGLE_APPLICATION_CREDENTIALS=\"path/to/your/credentials.json\"\n```\n\n----------------------------------------\n\nTITLE: Creating Retrieval Tool for Agents\nDESCRIPTION: Creates a retrieval tool that can be used by agents to search and retrieve information from blog posts.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createRetrieverTool } from \"langchain/tools/retriever\";\n\nconst tool =  createRetrieverTool(\n    retriever,\n    {\n      name: \"blog_post_retriever\",\n      description: \"Searches and returns excerpts from the Autonomous Agents blog post.\",\n    }\n)\nconst tools = [tool]\n```\n\n----------------------------------------\n\nTITLE: Implementing Query Rewriter\nDESCRIPTION: Creates a query rewriter using a prompt template and ChatOpenAI to improve search queries for complex or distracted user inputs.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/rewrite.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst rewriteTemplate = `Provide a better search query for \\\nweb search engine to answer the given question, end \\\nthe queries with '**'. Question: \\\n{x} Answer:`;\nconst rewritePrompt = PromptTemplate.fromTemplate(rewriteTemplate);\n\n// Parser to remove the `**`\nconst _parse = (text) => text.replace(\"**\", \"\");\n\n// rewriter = rewrite_prompt | ChatOpenAI(temperature=0) | StrOutputParser() | _parse\nconst rewriter = RunnableSequence.from([\n  rewritePrompt,\n  new ChatOpenAI({ temperature: 0 }),\n  new StringOutputParser(),\n  _parse\n]);\n```\n\n----------------------------------------\n\nTITLE: Adding Documents to PineconeStore (TypeScript)\nDESCRIPTION: This snippet shows how to add multiple documents to an instantiated `PineconeStore`. It defines several `Document` objects with `pageContent` and `metadata`, then uses the `addDocuments` method to upload them to the Pinecone index, providing optional custom IDs for each document.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/pinecone.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport type { Document } from \"@langchain/core/documents\";\n\nconst document1: Document = {\n  pageContent: \"The powerhouse of the cell is the mitochondria\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document2: Document = {\n  pageContent: \"Buildings are made out of brick\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document3: Document = {\n  pageContent: \"Mitochondria are made out of lipids\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document4: Document = {\n  pageContent: \"The 2024 Olympics are in Paris\",\n  metadata: { source: \"https://example.com\" }\n}\n\nconst documents = [document1, document2, document3, document4];\n\nawait vectorStore.addDocuments(documents, { ids: [\"1\", \"2\", \"3\", \"4\"] });\n```\n\n----------------------------------------\n\nTITLE: Embedding Multiple Texts with WatsonxEmbeddings\nDESCRIPTION: Demonstrates how to embed multiple texts for indexing using the embedDocuments method of WatsonxEmbeddings. This method may differ internally from embedding queries.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/ibm.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nconst text2 = \"LangGraph is a library for building stateful, multi-actor applications with LLMs\";\n\nconst vectors = await embeddings.embedDocuments([text, text2]);\n\nconsole.log(vectors[0].slice(0, 10));\nconsole.log(vectors[1].slice(0, 10));\n```\n\n----------------------------------------\n\nTITLE: Using Functions with Few Shot Templates in LangChainJS\nDESCRIPTION: Demonstrates how to use a function with few shot prompts to dynamically insert the current date into the prompt template.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot.mdx#2025-04-22_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nconst getCurrentDate = () => {\n  return new Date().toISOString();\n};\n\nconst prompt = new FewShotChatMessagePromptTemplate({\n  template: \"Tell me a {adjective} joke about the day {date}\",\n  inputVariables: [\"adjective\", \"date\"],\n});\n\nconst partialPrompt = await prompt.partial({\n  date: getCurrentDate,\n});\n\nconst formattedPrompt = await partialPrompt.format({\n  adjective: \"funny\",\n});\n\nconsole.log(formattedPrompt);\n\n// Tell me a funny joke about the day 2023-07-13T00:54:59.287Z\n```\n\n----------------------------------------\n\nTITLE: Instantiating ChatPerplexity Model in LangChain.js\nDESCRIPTION: Code for instantiating a ChatPerplexity model with configuration options such as model type, temperature, and retry settings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/perplexity.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatPerplexity } from \"@langchain/community/chat_models/perplexity\";\n\nconst llm = new ChatPerplexity({\n  model: \"sonar\",\n  temperature: 0,\n  maxTokens: undefined,\n  timeout: undefined,\n  maxRetries: 2,\n  // other params...\n});\n```\n\n----------------------------------------\n\nTITLE: Setting up Exa Retriever Tool and ReAct Agent (TypeScript)\nDESCRIPTION: Creates an Exa client, an `ExaRetriever` configured for search, converts the retriever into a LangChain tool using `createRetrieverTool`, and then constructs a ReAct agent using `createReactAgent` from LangGraph, providing the LLM and the Exa search tool.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/exa_search.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nimport Exa from \"exa-js\";\nimport { createRetrieverTool } from \"langchain/tools/retriever\";\nimport { ExaRetriever } from \"@langchain/exa\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n\n// @lc-ts-ignore\nconst agentClient = new Exa(process.env.EXASEARCH_API_KEY);\n\nconst exaRetrieverForAgent = new ExaRetriever({\n  // @lc-ts-ignore\n  client: agentClient,\n  searchArgs: {\n    numResults: 2,\n  },\n});\n\n// Convert the ExaRetriever into a tool\nconst searchToolForAgent = createRetrieverTool(exaRetrieverForAgent, {\n  name: \"search\",\n  description: \"Get the contents of a webpage given a string search query.\",\n});\n\nconst toolsForAgent = [searchToolForAgent];\n\nconst agentExecutor = createReactAgent({\n  llm: llmForAgent,\n  tools: toolsForAgent,\n})\n```\n\n----------------------------------------\n\nTITLE: Invoking LangGraph RAG Application in JavaScript\nDESCRIPTION: This code demonstrates how to invoke a LangGraph RAG application using the synchronous invoke method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nlet inputs = { question: \"What is Task Decomposition?\" };\n\nconst result = await graph.invoke(inputs);\nconsole.log(result.context.slice(0, 2));\nconsole.log(`\\nAnswer: ${result[\"answer\"]}`);\n```\n\n----------------------------------------\n\nTITLE: Indexing Chunks in Vector Store\nDESCRIPTION: This code snippet shows how to add the split documents to the vector store for indexing and later retrieval.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/qa_chat_history.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n// Index chunks\nawait vectorStore.addDocuments(allSplits)\n```\n\n----------------------------------------\n\nTITLE: Embedding Single and Multiple Texts with MistralAIEmbeddings\nDESCRIPTION: Examples of how to directly use MistralAIEmbeddings to embed single queries and multiple documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/mistralai.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst singleVector = await embeddings.embedQuery(text);\n\nconsole.log(singleVector.slice(0, 100));\n\nconst text2 = \"LangGraph is a library for building stateful, multi-actor applications with LLMs\";\n\nconst vectors = await embeddings.embedDocuments([text, text2]);\n\nconsole.log(vectors[0].slice(0, 100));\nconsole.log(vectors[1].slice(0, 100));\n```\n\n----------------------------------------\n\nTITLE: Invoking JsonOutputParser Chain in TypeScript\nDESCRIPTION: Shows how to create and invoke a chain that combines a prompt template, language model, and JsonOutputParser to extract structured data from user input about a person.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/structured_output.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst chain = prompt.pipe(model).pipe(parser);\n\nawait chain.invoke({ query })\n```\n\n----------------------------------------\n\nTITLE: Defining LangGraph State Types\nDESCRIPTION: Defines the state types for the RAG application using LangGraph annotations.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport { Document } from \"@langchain/core/documents\";\nimport { Annotation } from \"@langchain/langgraph\";\n\n\nconst InputStateAnnotation = Annotation.Root({\n  question: Annotation<string>,\n});\n\n\nconst StateAnnotation = Annotation.Root({\n  question: Annotation<string>,\n  context: Annotation<Document[]>,\n  answer: Annotation<string>,\n});\n```\n\n----------------------------------------\n\nTITLE: Filtering and Processing Specific Stream Events\nDESCRIPTION: Example that filters event stream to only show model chunks and parser chunks, ignoring start/end events. This demonstrates how to selectively process specific event types during streaming.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming.ipynb#2025-04-22_snippet_15\n\nLANGUAGE: javascript\nCODE:\n```\nlet eventCount = 0;\n\nconst eventStream = await chain.streamEvents(\n  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key \"name\" and \"population\"`,\n  { version: \"v1\" },\n);\n\nfor await (const event of eventStream) {\n  // Truncate the output\n  if (eventCount > 30) {\n    continue;\n  }\n  const eventType = event.event;\n  if (eventType === \"on_llm_stream\") {\n    console.log(`Chat model chunk: ${event.data.chunk.message.content}`);\n  } else if (eventType === \"on_parser_stream\") {\n    console.log(`Parser chunk: ${JSON.stringify(event.data.chunk)}`);\n  }\n  eventCount += 1;\n}\n```\n\n----------------------------------------\n\nTITLE: Streaming AIMessageChunks Example\nDESCRIPTION: Demonstrates how to stream responses from a chat model using async iteration to process chunks of the response in real-time.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/messages.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nfor await (const chunk of model.stream([\n  new HumanMessage(\"what color is the sky?\"),\n])) {\n  console.log(chunk);\n}\n```\n\n----------------------------------------\n\nTITLE: Using Content Blocks with ChatAnthropic in Python\nDESCRIPTION: Demonstrates how to use content blocks with ChatAnthropic, including tool calling functionality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/anthropic.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { z } from \"zod\";\nimport { zodToJsonSchema } from \"zod-to-json-schema\";\n\nconst calculatorSchema = z.object({\n  operation: z\n    .enum([\"add\", \"subtract\", \"multiply\", \"divide\"])\n    .describe(\"The type of operation to execute.\"),\n  number1: z.number().describe(\"The first number to operate on.\"),\n  number2: z.number().describe(\"The second number to operate on.\"),\n});\n\nconst calculatorTool = {\n  name: \"calculator\",\n  description: \"A simple calculator tool\",\n  input_schema: zodToJsonSchema(calculatorSchema),\n};\n\nconst toolCallingLlm = new ChatAnthropic({\n  model: \"claude-3-haiku-20240307\",\n}).bindTools([calculatorTool]);\n\nconst toolPrompt = ChatPromptTemplate.fromMessages([\n  [\n    \"system\",\n    \"You are a helpful assistant who always needs to use a calculator.\",\n  ],\n  [\"human\", \"{input}\"],\n]);\n\n// Chain your prompt and model together\nconst toolCallChain = toolPrompt.pipe(toolCallingLlm);\n\nawait toolCallChain.invoke({\n  input: \"What is 2 + 2?\",\n});\n```\n\n----------------------------------------\n\nTITLE: Converting Vector Store to a Retriever for Chain Usage\nDESCRIPTION: Transforms the vector store into a retriever that can be used in LangChain chains. This provides a standardized interface for retrieval operations within the LangChain ecosystem.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/qdrant.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconst retriever = vectorStore.asRetriever({\n  // Optional filter\n  filter: filter,\n  k: 2,\n});\nawait retriever.invoke(\"biology\");\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings for a Single Query\nDESCRIPTION: Demonstrates how to generate embeddings for a single text query using the embedQuery method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/deepinfra.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst embedding = await embeddings.embedQuery(\n  \"What would be a good company name for a company that makes colorful socks?\"\n);\nconsole.log(embedding);\n```\n\n----------------------------------------\n\nTITLE: Setting Verbose Mode for LangChain Debugging\nDESCRIPTION: A code snippet showing how to enable verbose mode in LangChain components by setting the verbose parameter to true. This setting causes any component with callback support to print inputs and outputs.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n{ verbose: true }\n```\n\n----------------------------------------\n\nTITLE: Loading PDF Documents with LangChain's PDFLoader\nDESCRIPTION: This snippet demonstrates how to use LangChain's PDFLoader to load a PDF file into a sequence of Document objects.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/retrievers.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { PDFLoader } from \"@langchain/community/document_loaders/fs/pdf\";\n\nconst loader = new PDFLoader(\"../../data/nke-10k-2023.pdf\");\n\nconst docs = await loader.load();\nconsole.log(docs.length)\n```\n\n----------------------------------------\n\nTITLE: Indexing Documents in MyScale Vector Store\nDESCRIPTION: Code example showing how to initialize a MyScale vector store, create embeddings, and insert text documents into a collection. The example demonstrates setting up connection details and creating a new vector collection.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/myscale.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport CodeBlock from \"@theme/CodeBlock\";\nimport InsertExample from \"@examples/indexes/vector_stores/myscale_fromTexts.ts\";\n\n<CodeBlock language=\"typescript\">{InsertExample}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings for Multiple Documents (Text/Image) with JinaEmbeddings\nDESCRIPTION: This demonstrates generating embeddings for a batch of documents using `embedDocuments`. The input array can contain strings, objects with a `text` key, or objects with an `image` key (accepting a URL or a base64 encoded string). It utilizes the `localImageToBase64` utility from `@langchain/community` to convert a local image file for embedding.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/jina.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { localImageToBase64 } from \"@langchain/community/utils/local_image_to_base64\";\nconst documents = [\n  \"hello\",\n  {\n    text: \"hello\",\n  },\n  {\n    image: \"https://i.ibb.co/nQNGqL0/beach1.jpg\",\n  },\n  {\n    image: await localImageToBase64(\"beach1.jpg\"),\n  },\n];\n\nconst embeddingsArray = await embeddings.embedDocuments(documents);\nconsole.log(embeddingsArray);\n```\n\n----------------------------------------\n\nTITLE: Importing AzureOpenAIEmbeddings Class - TypeScript\nDESCRIPTION: This snippet updates the import statement to source the AzureOpenAIEmbeddings class from the new @langchain/openai package rather than the deprecated package. Developers should add this line to their TypeScript files where embedding models are required. The import assumes @langchain/openai has been installed as a project dependency.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/azure_openai.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AzureOpenAIEmbeddings } from \"@langchain/openai\";\n```\n\n----------------------------------------\n\nTITLE: Instantiating Elasticsearch Vector Store Client with OpenAI Embeddings (TypeScript)\nDESCRIPTION: Demonstrates the complete instantiation of an Elasticsearch vector store for usage with LangChain.js, configuring both the client connection and OpenAI embeddings. This example sets up credential management (including API key or basic authentication for Elastic, and optional TLS for local Docker), establishes the Elasticsearch client, and initializes the ElasticVectorSearch instance with appropriate embedding and client configurations. Dependencies: @langchain/community, @langchain/openai, @elastic/elasticsearch, and Node's fs module.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/elasticsearch.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport {\n  ElasticVectorSearch,\n  type ElasticClientArgs,\n} from \"@langchain/community/vectorstores/elasticsearch\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nimport { Client, type ClientOptions } from \"@elastic/elasticsearch\";\n\nimport * as fs from \"node:fs\";\n\nconst embeddings = new OpenAIEmbeddings({\n  model: \"text-embedding-3-small\",\n});\n\nconst config: ClientOptions = {\n  node: process.env.ELASTIC_URL ?? \"https://127.0.0.1:9200\",\n};\n\nif (process.env.ELASTIC_API_KEY) {\n  config.auth = {\n    apiKey: process.env.ELASTIC_API_KEY,\n  };\n} else if (process.env.ELASTIC_USERNAME && process.env.ELASTIC_PASSWORD) {\n  config.auth = {\n    username: process.env.ELASTIC_USERNAME,\n    password: process.env.ELASTIC_PASSWORD,\n  };\n}\n// Local Docker deploys require a TLS certificate\nif (process.env.ELASTIC_CERT_PATH) {\n  config.tls = {\n    ca: fs.readFileSync(process.env.ELASTIC_CERT_PATH),\n    rejectUnauthorized: false,\n  }\n}\nconst clientArgs: ElasticClientArgs = {\n  client: new Client(config),\n  indexName: process.env.ELASTIC_INDEX ?? \"test_vectorstore\",\n};\n\nconst vectorStore = new ElasticVectorSearch(embeddings, clientArgs);\n```\n\n----------------------------------------\n\nTITLE: Performing Basic Vector Similarity Search with SingleStoreVectorStore in LangChain.js - TypeScript\nDESCRIPTION: Demonstrates initializing a SingleStoreVectorStore, connecting to a SingleStoreDB instance, and executing a vector similarity search using LangChain.js in TypeScript. The example covers importing the module, establishing credentials, and performing a search for related results. Dependencies include the mysql2 client, LangChain.js core vector store modules, and optionally OpenAI for embedding support. Required parameters typically include SingleStore connection credentials and, if applicable, embedding model outputs. The result is a list of retrieved documents most similar to an input vector.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/singlestore.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n{UsageExample}\n```\n\n----------------------------------------\n\nTITLE: Indexing and Querying Azure Cosmos DB Vector Store in TypeScript\nDESCRIPTION: Provides a complete TypeScript example demonstrating the usage of the `@langchain/azure-cosmosdb` package. The code, dynamically imported from `azure_cosmosdb_mongodb.ts`, shows how to initialize the vector store, index documents from a source, execute a vector search query against the indexed data, and potentially use a Langchain chain to process the results for tasks like question answering.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/azure_cosmosdb_mongodb.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n{Example}\n```\n\n----------------------------------------\n\nTITLE: Invoking AzureChatOpenAI with Chat Messages\nDESCRIPTION: Demonstrates how to invoke the AzureChatOpenAI model with system and human messages to perform a translation task.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/azure.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst aiMsg = await llm.invoke([\n    [\n        \"system\",\n        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n    ],\n    [\"human\", \"I love programming.\"],\n])\naiMsg\n```\n\n----------------------------------------\n\nTITLE: Multi-Query Retrieval Chain Implementation\nDESCRIPTION: Implements an asynchronous chain that processes multiple queries and combines their results.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_multiple_queries.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { RunnableConfig, RunnableLambda } from \"@langchain/core/runnables\";\n\nconst chain = async (question: string, config?: RunnableConfig) => {\n    const response = await queryAnalyzer.invoke(question, config);\n    const docs = [];\n    for (const query of response.queries) {\n        const newDocs = await retriever.invoke(query, config);\n        docs.push(...newDocs);\n    }\n    // You probably want to think about reranking or deduplicating documents here\n    // But that is a separate topic\n    return docs;\n}\n\nconst customChain = new RunnableLambda({ func: chain });\n```\n\n----------------------------------------\n\nTITLE: Client Component Implementation\nDESCRIPTION: React component setup with form handling and stream processing\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/stream_agent_client.mdx#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nexport default function Page() {\n  const [input, setInput] = useState(\"\");\n  const [data, setData] = useState<StreamEvent[]>([]);\n\n  async function handleSubmit(e: React.FormEvent) {\n    e.preventDefault();\n\n    const { streamData } = await runAgent(input);\n    for await (const item of readStreamableValue(streamData)) {\n      setData((prev) => [...prev, item]);\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Memory-Enabled Agent\nDESCRIPTION: Creates a React agent with memory capabilities for maintaining conversation history.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_tools.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { MemorySaver } from \"@langchain/langgraph\"\n\nconst memory = new MemorySaver()\nconst agent2 = createReactAgent({ llm, tools, messageModifier: prompt, checkpointSaver: memory })\n```\n\n----------------------------------------\n\nTITLE: Numeric Range Query in Couchbase Vector Store using TypeScript\nDESCRIPTION: This snippet illustrates how to search for documents within a specific numeric range using the metadata.rating field in Couchbase Vector Store.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/couchbase.mdx#2025-04-22_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nconst ratingRangeResult = await store.similaritySearch(independenceQuery, 4, {\n  fields: [\"metadata.rating\"],\n  searchOptions: {\n    query: {\n      min: 3,\n      max: 5,\n      inclusiveMin: false,\n      inclusiveMax: true,\n      field: \"metadata.rating\",\n    },\n  },\n});\nconsole.log(ratingRangeResult[0]);\n```\n\n----------------------------------------\n\nTITLE: Setting up RAG Ingest Pipeline in Python\nDESCRIPTION: Sets up a basic RAG ingest pipeline with embeddings, text splitter, and vector store for document reranking.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_compressors/ibm.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport { readFileSync } from \"node:fs\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { WatsonxEmbeddings } from \"@langchain/community/embeddings/ibm\";\nimport { CharacterTextSplitter } from \"@langchain/textsplitters\";\n\nconst embeddings = new WatsonxEmbeddings({\n version: \"YYYY-MM-DD\",\n serviceUrl: process.env.API_URL,\n projectId: \"<PROJECT_ID>\",\n spaceId: \"<SPACE_ID>\",\n model: \"ibm/slate-125m-english-rtrvr\",\n});\n\nconst textSplitter = new CharacterTextSplitter({\n  chunkSize: 400,\n  chunkOverlap: 0,\n});\n  \nconst query = \"What did the president say about Ketanji Brown Jackson\";\nconst text = readFileSync(\"state_of_the_union.txt\", \"utf8\");\n\nconst docs = await textSplitter.createDocuments([text]);\nconst vectorStore = await MemoryVectorStore.fromDocuments(docs, embeddings);\nconst vectorStoreRetriever = vectorStore.asRetriever();\n\nconst result = await vectorStoreRetriever.invoke(query);\nconsole.log(result);\n```\n\n----------------------------------------\n\nTITLE: Basic LlamaCpp Integration with LangChain\nDESCRIPTION: Example demonstrating how to initialize and use the LlamaCpp model from LangChain community package with a local Llama model, including configuration options and a simple prompt completion.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/llama_cpp.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LlamaCpp } from \"@langchain/community/llms/llama_cpp\";\n\n// Path to the Llama 3 model\nconst model = new LlamaCpp({\n  modelPath: \"./models/llama-3-8b.Q4_0.gguf\",\n  temperature: 0.7,\n  maxTokens: 2000,\n  topP: 1,\n  verbose: true, // when true, the model will print debug information to the console.\n});\n\nconst response = await model.invoke(\n  \"What are the key differences between Llama 2 and Llama 3?\"\n);\n\nconsole.log({ response });\n```\n\n----------------------------------------\n\nTITLE: Configuring Token-Based Rate Limiting in TypeScript\nDESCRIPTION: Set up token-based rate limiting using Upstash Ratelimit for LangChain.js chains that include an LLM. This limits based on the number of tokens in prompts and completions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/callbacks/upstash_ratelimit_callback.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst user_id = \"user_id\"; // should be a method which gets the user id\nconst handler = new UpstashRatelimitHandler(user_id, {\n  tokenRatelimit: ratelimit,\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings with ZhipuAIEmbeddings in TypeScript\nDESCRIPTION: Demonstrates how to import and utilize the ZhipuAIEmbeddings class from '@langchain/community' to generate vector embeddings for a given text string ('Hello world'). It instantiates the class and calls the `embedQuery` method. Requires the `ZHIPUAI_API_KEY` environment variable to be set with your ZhipuAI API key.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/zhipuai.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n{ZhipuAIExample}\n```\n\n----------------------------------------\n\nTITLE: Initializing AI21 Model in LangChain.js\nDESCRIPTION: Example of how to initialize and use an AI21 model in LangChain.js. It demonstrates importing the AI21 class, creating an instance with an API key, and using it to generate text.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/ai21.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AI21 } from \"@langchain/community/llms/ai21\";\n\nconst model = new AI21({\n  ai21ApiKey: \"YOUR-API-KEY\", // Default env var name: AI21_API_KEY\n});\n\nconst res = await model.invoke(\n  \"What would be a good company name for a company that makes colorful socks?\"\n);\n\nconsole.log(res);\n\n/*\n  Here are some potential company names for a company that makes colorful socks:\n\n  1. Rainbow Feet\n  2. Vivid Soles\n  3. Chromatic Toes\n  4. Kaleidosocks\n  5. Spectrum Socks\n  6. Hue & Hoof\n  7. Prismatic Pairs\n  8. Vibrant Vamps\n  9. Pigment Pedestals\n  10. Chroma Cuffs\n\n  These names play on the colorful aspect of the socks while also incorporating foot-related terms. They're catchy, memorable, and clearly convey the product offering.\n*/\n\n```\n\n----------------------------------------\n\nTITLE: Initializing Claude Model with Prompt Caching\nDESCRIPTION: Shows how to set up a ChatAnthropic instance with prompt caching enabled using the claude-3-haiku model and custom headers for caching configuration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/anthropic.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst modelWithCaching = new ChatAnthropic({\n  model: \"claude-3-haiku-20240307\",\n  clientOptions: {\n    defaultHeaders: {\n      \"anthropic-beta\": \"prompt-caching-2024-07-31\",\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Testing RAG Fusion with a Climate Change Query\nDESCRIPTION: Executes the RAG Fusion chain with a sample query about the impact of climate change, demonstrating how the system generates multiple queries, retrieves documents for each, and then fuses the results into a final ranked list.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/rag_fusion.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nconst originalQuery = \"impact of climate change\";\n\nconst result = await chain.invoke({\n  original_query: originalQuery,\n});\n\nconsole.log(result);\n```\n\n----------------------------------------\n\nTITLE: Creating a Prompt-Model Chain for Translation\nDESCRIPTION: Creates a chain combining a chat prompt template with the ChatTogetherAI model for language translation, with parameterized input and output languages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/togetherai.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\"\n\nconst prompt = ChatPromptTemplate.fromMessages(\n    [\n        [\n            \"system\",\n            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n        ],\n        [\"human\", \"{input}\"],\n    ]\n)\n\nconst chain = prompt.pipe(llm);\nawait chain.invoke(\n    {\n        input_language: \"English\",\n        output_language: \"German\",\n        input: \"I love programming.\",\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Chain with TavilySearchResults Tool in TypeScript\nDESCRIPTION: Demonstrates how to create a chain that incorporates the TavilySearchResults tool with a language model, processes user input, and handles tool calls.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/tavily_search_community.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport { HumanMessage } from \"@langchain/core/messages\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { RunnableLambda } from \"@langchain/core/runnables\";\n\nconst prompt = ChatPromptTemplate.fromMessages(\n  [\n    [\"system\", \"You are a helpful assistant.\"],\n    [\"placeholder\", \"{messages}\"],\n  ]\n)\n\nconst llmWithTools = llm.bindTools([tool]);\n\nconst chain = prompt.pipe(llmWithTools);\n\nconst toolChain = RunnableLambda.from(\n  async (userInput: string, config) => {\n    const humanMessage = new HumanMessage(userInput,);\n    const aiMsg = await chain.invoke({\n      messages: [new HumanMessage(userInput)],\n    }, config);\n    const toolMsgs = await tool.batch(aiMsg.tool_calls, config);\n    return chain.invoke({\n      messages: [humanMessage, aiMsg, ...toolMsgs],\n    }, config);\n  }\n);\n\nconst toolChainResult = await toolChain.invoke(\"what is the current weather in sf?\");\n```\n\n----------------------------------------\n\nTITLE: Defining a Prisma Model with a Vector Field\nDESCRIPTION: This Prisma schema defines a Document model with fields for id, content, and a vector. The vector field is marked as Unsupported, and would require the pgvector extension to handle vector data types in PostgreSQL.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/prisma.mdx#2025-04-22_snippet_2\n\nLANGUAGE: prisma\nCODE:\n```\nmodel Document {\n  id      String                 @id @default(cuid())\n  content String\n  vector  Unsupported(\"vector\")?\n}\n```\n\n----------------------------------------\n\nTITLE: Chaining Bedrock LLM with a prompt template\nDESCRIPTION: Example of creating a chain that combines a prompt template with a Bedrock LLM to create a translation system, demonstrating how to pass variables to the prompt.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/bedrock.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { PromptTemplate } from \"@langchain/core/prompts\"\n\nconst prompt = PromptTemplate.fromTemplate(\"Human: How to say {input} in {output_language}:\\nAssistant:\")\n\nconst chain = prompt.pipe(llm);\nawait chain.invoke(\n  {\n    output_language: \"German\",\n    input: \"I love programming.\",\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using ChatDeepSeek Model in TypeScript\nDESCRIPTION: TypeScript code snippet demonstrating how to import, initialize, and use the ChatDeepSeek model. It shows setting up the model with an API key and making an inference request.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-deepseek/README.md#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatDeepSeek } from \"@langchain/deepseek\";\nimport { HumanMessage } from \"@langchain/core/messages\";\n\nconst model = new ChatDeepSeek({\n  apiKey: process.env.DEEPSEEK_API_KEY, // Default value.\n  model: \"<model_name>\",\n});\n\nconst res = await model.invoke([\n  {\n    role: \"user\",\n    content: message,\n  },\n]);\n```\n\n----------------------------------------\n\nTITLE: OpenAI Format Chat Model Input\nDESCRIPTION: Example of using OpenAI's message format as input to chat models, showing a conversation structure with user and assistant roles.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/messages.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nawait chatModel.invoke([\n  {\n    role: \"user\",\n    content: \"Hello, how are you?\",\n  },\n  {\n    role: \"assistant\",\n    content: \"I'm doing well, thank you for asking.\",\n  },\n  {\n    role: \"user\",\n    content: \"Can you tell me a joke?\",\n  },\n]);\n```\n\n----------------------------------------\n\nTITLE: Using Fine-Tuned OpenAI Models\nDESCRIPTION: Demonstrates how to call fine-tuned OpenAI models by specifying the appropriate model name format with organization and model ID.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst fineTunedLlm = new ChatOpenAI({\n  temperature: 0.9,\n  model: \"ft:gpt-3.5-turbo-0613:{ORG_NAME}::{MODEL_ID}\",\n});\n\nawait fineTunedLlm.invoke(\"Hi there!\");\n```\n\n----------------------------------------\n\nTITLE: Overwriting Model Parameters\nDESCRIPTION: Example of overwriting model parameters for a single invocation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/ibm.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nconst result2 = await instance.invoke(\"Print hello world.\", {\n  parameters: {\n    maxNewTokens: 100,\n  },\n});\nconsole.log(result2);\n```\n\n----------------------------------------\n\nTITLE: Using Cohere Rerank as Document Compressor\nDESCRIPTION: Shows how to use the compressDocuments method to directly retrieve the reranked documents with their relevancy scores rather than just indexes.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_compressors/cohere_rerank.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Document } from \"@langchain/core/documents\";\nimport { CohereRerank } from \"@langchain/cohere\";\n\n// Initialize documents to be reranked\nconst docs = [\n  new Document({\n    pageContent: \"The color of the sky is blue.\",\n  }),\n  new Document({\n    pageContent: \"I live in San Francisco.\",\n  }),\n  new Document({\n    pageContent: \"The capital of the United States is Washington, D.C.\",\n  }),\n  new Document({\n    pageContent: \"The capital of France is Paris.\",\n  }),\n];\n\n// Initialize the reranker with your API key\nconst reranker = new CohereRerank({ apiKey: \"YOUR_API_KEY\" });\n\n// Compress documents based on a query\nconst results = await reranker.compressDocuments({\n  documents: docs,\n  query: \"What is the capital of the US?\",\n  topN: 3,\n});\n\nconsole.log(results);\n/*\n[ Document {\n    pageContent: 'The capital of the United States is Washington, D.C.',\n    metadata: { relevanceScore: 0.9870191812515259 } },\n  Document {\n    pageContent: 'The capital of France is Paris.',\n    metadata: { relevanceScore: 0.09175287932157516 } },\n  Document {\n    pageContent: 'I live in San Francisco.',\n    metadata: { relevanceScore: 0.05698449909687042 } } ]\n*/\n\n```\n\n----------------------------------------\n\nTITLE: Querying Self-Querying Retriever in JavaScript\nDESCRIPTION: Demonstrates how to use the self-querying retriever to perform various queries on the movie dataset, including filtering by movie length, rating, director, and complex conditions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/self_query.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nawait selfQueryRetriever.invoke(\n  \"Which movies are less than 90 minutes?\"\n);\n\nawait selfQueryRetriever.invoke(\n  \"Which movies are rated higher than 8.5?\"\n);\n\nawait selfQueryRetriever.invoke(\n  \"Which movies are directed by Greta Gerwig?\"\n);\n\nawait selfQueryRetriever.invoke(\n  \"Which movies are either comedy or drama and are less than 90 minutes?\"\n);\n```\n\n----------------------------------------\n\nTITLE: Load OpenAPI Spec and Create Toolkit\nDESCRIPTION: Loads an OpenAPI specification from YAML, converts it to JSON, and initializes the OpenApiToolkit with a specified llm and request headers. Dependencies are 'langchain/agents/toolkits', 'fs', 'js-yaml'.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/openapi.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { OpenApiToolkit } from \"langchain/agents/toolkits\"\nimport * as fs from \"fs\";\nimport * as yaml from \"js-yaml\";\nimport { JsonSpec, JsonObject } from \"langchain/tools\";\n\n// Load & convert the OpenAPI spec from YAML to JSON.\nconst yamlFile = fs.readFileSync(\"../../../../../examples/openai_openapi.yaml\", \"utf8\");\nconst data = yaml.load(yamlFile) as JsonObject;\nif (!data) {\n  throw new Error(\"Failed to load OpenAPI spec\");\n}\n\n// Define headers for the API requests.\nconst headers = {\n  \"Content-Type\": \"application/json\",\n  Authorization: `Bearer ${process.env.OPENAI_API_KEY}`,\n};\n\nconst toolkit = new OpenApiToolkit(new JsonSpec(data), llm, headers);\n```\n\n----------------------------------------\n\nTITLE: Using the self-query retriever\nDESCRIPTION: Demonstrates how to use the self-query retriever to fetch documents based on a query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/qdrant.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nawait selfQueryRetriever.invoke(\n  \"Which movies are rated higher than 8.5?\"\n);\n```\n\n----------------------------------------\n\nTITLE: Using MemoryVectorStore as a Retriever (TypeScript)\nDESCRIPTION: Transforms the `vectorStore` into a LangChain `Retriever` object using the `asRetriever` method. Configuration options like the number of results to return (`k=2`) and an optional `filter` function (the same one used previously) can be passed. The retriever interface allows seamless integration into chains and agents. The example then invokes the retriever with the query \"biology\".\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/memory.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst retriever = vectorStore.asRetriever({\n  // Optional filter\n  filter: filter,\n  k: 2,\n});\n\nawait retriever.invoke(\"biology\");\n```\n\n----------------------------------------\n\nTITLE: Streaming Log Probabilities from OpenAI Chat Model\nDESCRIPTION: Shows how to stream responses from the ChatOpenAI model and access log probabilities from each chunk.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/logprobs.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nlet count = 0;\nconst stream = await model.stream(\"How are you today?\");\nlet aggregateResponse;\n\nfor await (const chunk of stream) {\n  if (count > 5) {\n    break;\n  }\n  if (aggregateResponse === undefined) {\n    aggregateResponse = chunk;\n  } else {\n    aggregateResponse = aggregateResponse.concat(chunk);\n  }\n  console.log(aggregateResponse.response_metadata.logprobs?.content);\n  count++;\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing MultiVectorRetriever with Smaller Chunks in TypeScript\nDESCRIPTION: Example of using MultiVectorRetriever with smaller document chunks for improved embedding accuracy while maintaining context.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/multi_vector.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport CodeBlock from \"@theme/CodeBlock\";\nimport SmallChunksExample from \"@examples/retrievers/multi_vector_small_chunks.ts\";\n\n<CodeBlock language=\"typescript\">{SmallChunksExample}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Invoking TavilySearchResults Tool via ToolCall in TypeScript\nDESCRIPTION: Shows how to invoke the TavilySearchResults tool using a model-generated ToolCall object, returning a ToolMessage.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/tavily_search_community.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n// This is usually generated by a model, but we'll create a tool call directly for demo purposes.\nconst modelGeneratedToolCall = {\n  args: {\n    query: \"what is the current weather in SF?\"\n  },\n  id: \"1\",\n  name: tool.name,\n  type: \"tool_call\",\n}\n\nawait tool.invoke(modelGeneratedToolCall)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using ChatCohere Model\nDESCRIPTION: Code example demonstrating how to initialize the ChatCohere model with an API key and invoke it with a human message. This is the recommended way to interface with Cohere's chat models.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-cohere/README.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HumanMessage } from \"@langchain/core/messages\";\nimport { ChatCohere } from \"@langchain/cohere\";\n\nconst model = new ChatCohere({\n  apiKey: process.env.COHERE_API_KEY,\n});\nconst response = await model.invoke([new HumanMessage(\"Hello world!\")]);\n```\n\n----------------------------------------\n\nTITLE: Initializing MultiQueryRetriever with MemoryVectorStore and ChatAnthropic\nDESCRIPTION: This snippet demonstrates how to set up a MultiQueryRetriever using a MemoryVectorStore and ChatAnthropic model. It includes creating embeddings, initializing the vector store, and setting up the retriever.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/multiple_queries.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { CohereEmbeddings } from \"@langchain/cohere\";\nimport { MultiQueryRetriever } from \"langchain/retrievers/multi_query\";\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst embeddings = new CohereEmbeddings();\n\nconst vectorstore = await MemoryVectorStore.fromTexts(\n  [\n    \"Buildings are made out of brick\",\n    \"Buildings are made out of wood\",\n    \"Buildings are made out of stone\",\n    \"Cars are made out of metal\",\n    \"Cars are made out of plastic\",\n    \"mitochondria is the powerhouse of the cell\",\n    \"mitochondria is made of lipids\",\n  ],\n  [{ id: 1 }, { id: 2 }, { id: 3 }, { id: 4 }, { id: 5 }],\n  embeddings\n);\n\nconst model = new ChatAnthropic({\n  model: \"claude-3-sonnet-20240229\"\n});\n\nconst retriever = MultiQueryRetriever.fromLLM({\n  llm: model,\n  retriever: vectorstore.asRetriever(),\n});\n\nconst query = \"What are mitochondria made of?\";\nconst retrievedDocs = await retriever.invoke(query);\n\nconsole.log(retrievedDocs);\n```\n\n----------------------------------------\n\nTITLE: Creating a Retriever from PineconeStore (TypeScript)\nDESCRIPTION: This code shows how to convert a `PineconeStore` instance into a Langchain Retriever object. It uses the `asRetriever` method, allowing optional configuration like a metadata filter and the number of documents (`k`) to retrieve. The resulting retriever can then be used within Langchain chains or agents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/pinecone.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst retriever = vectorStore.asRetriever({\n  // Optional filter\n  filter: filter,\n  k: 2,\n});\n\nawait retriever.invoke(\"biology\");\n```\n\n----------------------------------------\n\nTITLE: Adding Documents to MongoDB Vector Store (TypeScript)\nDESCRIPTION: Demonstrates how to add multiple documents, each with `pageContent` and `metadata`, to the initialized `MongoDBAtlasVectorSearch` instance. It uses the `addDocuments` method, optionally providing unique IDs for each document. Requires the `Document` type from `@langchain/core/documents`.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/mongodb_atlas.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport type { Document } from \"@langchain/core/documents\";\n\nconst document1: Document = {\n  pageContent: \"The powerhouse of the cell is the mitochondria\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document2: Document = {\n  pageContent: \"Buildings are made out of brick\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document3: Document = {\n  pageContent: \"Mitochondria are made out of lipids\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document4: Document = {\n  pageContent: \"The 2024 Olympics are in Paris\",\n  metadata: { source: \"https://example.com\" }\n}\n\nconst documents = [document1, document2, document3, document4];\n\nawait vectorStore.addDocuments(documents, { ids: [\"1\", \"2\", \"3\", \"4\"] });\n```\n\n----------------------------------------\n\nTITLE: Customizing MultiQueryRetriever with Custom Prompt and Output Parser\nDESCRIPTION: This snippet shows how to customize the MultiQueryRetriever by using a custom prompt and output parser. It includes creating a custom LineListOutputParser, pulling a prompt from LangChain Hub, and setting up the retriever with these custom components.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/multiple_queries.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { LLMChain } from \"langchain/chains\";\nimport { pull } from \"langchain/hub\";\nimport { BaseOutputParser } from \"@langchain/core/output_parsers\";\nimport { PromptTemplate } from \"@langchain/core/prompts\";\n\n\ntype LineList = {\n  lines: string[];\n};\n\nclass LineListOutputParser extends BaseOutputParser<LineList> {\n  static lc_name() {\n    return \"LineListOutputParser\";\n  }\n\n  lc_namespace = [\"langchain\", \"retrievers\", \"multiquery\"];\n\n  async parse(text: string): Promise<LineList> {\n    const startKeyIndex = text.indexOf(\"<questions>\");\n    const endKeyIndex = text.indexOf(\"</questions>\");\n    const questionsStartIndex =\n      startKeyIndex === -1 ? 0 : startKeyIndex + \"<questions>\".length;\n    const questionsEndIndex = endKeyIndex === -1 ? text.length : endKeyIndex;\n    const lines = text\n      .slice(questionsStartIndex, questionsEndIndex)\n      .trim()\n      .split(\"\\n\")\n      .filter((line) => line.trim() !== \"\");\n    return { lines };\n  }\n\n  getFormatInstructions(): string {\n    throw new Error(\"Not implemented.\");\n  }\n}\n\n// Default prompt is available at: https://smith.langchain.com/hub/jacob/multi-vector-retriever-german\nconst prompt: PromptTemplate = await pull(\n  \"jacob/multi-vector-retriever-german\"\n);\n\nconst vectorstore = await MemoryVectorStore.fromTexts(\n  [\n    \"Gebäude werden aus Ziegelsteinen hergestellt\",\n    \"Gebäude werden aus Holz hergestellt\",\n    \"Gebäude werden aus Stein hergestellt\",\n    \"Autos werden aus Metall hergestellt\",\n    \"Autos werden aus Kunststoff hergestellt\",\n    \"Mitochondrien sind die Energiekraftwerke der Zelle\",\n    \"Mitochondrien bestehen aus Lipiden\",\n  ],\n  [{ id: 1 }, { id: 2 }, { id: 3 }, { id: 4 }, { id: 5 }],\n  embeddings\n);\nconst model = new ChatAnthropic({});\nconst llmChain = new LLMChain({\n  llm: model,\n  prompt,\n  outputParser: new LineListOutputParser(),\n});\nconst retriever = new MultiQueryRetriever({\n  retriever: vectorstore.asRetriever(),\n  llmChain,\n});\n\nconst query = \"What are mitochondria made of?\";\nconst retrievedDocs = await retriever.invoke(query);\n\nconsole.log(retrievedDocs);\n```\n\n----------------------------------------\n\nTITLE: Importing LangChain Prompt Templates\nDESCRIPTION: Imports ChatPromptTemplate and FewShotChatMessagePromptTemplate from LangChain for creating structured prompts.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/step_back.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport {\n  ChatPromptTemplate,\n  FewShotChatMessagePromptTemplate,\n} from \"npm:langchain@0.0.177/prompts\";\n```\n\n----------------------------------------\n\nTITLE: Implementing Config-Aware Summarization Tool\nDESCRIPTION: Creates an improved version of the summarization tool that properly handles configuration propagation for event streaming.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_stream_events.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst specialSummarizationToolWithConfig = tool(async (input, config) => {\n  const prompt = ChatPromptTemplate.fromTemplate(\n    \"You are an expert writer. Summarize the following text in 10 words or less:\\n\\n{long_text}\"\n  );\n  const reverse = (x: string) => {\n    return x.split(\"\").reverse().join(\"\");\n  };\n  const chain = prompt\n    .pipe(model)\n    .pipe(new StringOutputParser())\n    .pipe(reverse);\n  const summary = await chain.invoke({ long_text: input.long_text }, config);\n  return summary;\n}, {\n  name: \"special_summarization_tool\",\n  description: \"A tool that summarizes input text using advanced techniques.\",\n  schema: z.object({\n    long_text: z.string(),\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Streaming Output from ChatWatsonx\nDESCRIPTION: Shows how to stream the output from ChatWatsonx model responses chunk by chunk, allowing for real-time display of generation results.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/ibm.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport { HumanMessage, SystemMessage } from \"@langchain/core/messages\";\n\nconst messages = [\n    new SystemMessage('You are a helpful assistant which telling short-info about provided topic.'),\n    new HumanMessage(\"moon\")\n]\nconst stream = await instance.stream(messages);\nfor await(const chunk of stream){\n    console.log(chunk)\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a ChatOpenAI Model Instance in TypeScript\nDESCRIPTION: Instantiates a ChatOpenAI model with specific parameters for use with the React agent. This configures the language model that will power the agent's reasoning.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/toolkits.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n// @lc-docs-hide-cell\n\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o-mini\",\n  temperature: 0,\n})\n```\n\n----------------------------------------\n\nTITLE: Testing Query Analysis in LangGraph JavaScript\nDESCRIPTION: This code demonstrates how to test the query analysis implementation in a LangGraph application by streaming updates for a specific question.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nlet inputsQA = { question: \"What does the end of the post say about Task Decomposition?\" };\n\nconsole.log(inputsQA)\nconsole.log(\"\\n====\\n\");\nfor await (\n  const chunk of await graphQA.stream(inputsQA, {\n    streamMode: \"updates\",\n  })\n) {\n  console.log(chunk);\n  console.log(\"\\n====\\n\");\n}\n```\n\n----------------------------------------\n\nTITLE: Stream Value Creation\nDESCRIPTION: Creating a streamable value using the AI package helper function\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/stream_agent_client.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst stream = createStreamableValue();\n```\n\n----------------------------------------\n\nTITLE: Using Google Search Retrieval with ChatGoogleGenerativeAI\nDESCRIPTION: Demonstrates how to use Google's built-in search retrieval tool for grounding content generation in real-world information.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/google_generativeai.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport { DynamicRetrievalMode, GoogleSearchRetrievalTool } from \"@google/generative-ai\";\nimport { ChatGoogleGenerativeAI } from \"@langchain/google-genai\";\n\nconst searchRetrievalTool: GoogleSearchRetrievalTool = {\n  googleSearchRetrieval: {\n    dynamicRetrievalConfig: {\n      mode: DynamicRetrievalMode.MODE_DYNAMIC,\n      dynamicThreshold: 0.7, // default is 0.7\n    }\n  }\n};\nconst searchRetrievalModel = new ChatGoogleGenerativeAI({\n  model: \"gemini-1.5-pro\",\n  temperature: 0,\n  maxRetries: 0,\n}).bindTools([searchRetrievalTool]);\n\nconst searchRetrievalResult = await searchRetrievalModel.invoke(\"Who won the 2024 MLB World Series?\");\n\nconsole.log(searchRetrievalResult.content);\n```\n\nLANGUAGE: python\nCODE:\n```\nconsole.dir(searchRetrievalResult.response_metadata?.groundingMetadata, { depth: null });\n```\n\n----------------------------------------\n\nTITLE: Loading Web Content with CheerioWebBaseLoader\nDESCRIPTION: Uses the CheerioWebBaseLoader to fetch and load the LangSmith documentation from a URL for use as source material in the retrieval system.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_retrieval.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport \"cheerio\";\nimport { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\";\n\nconst loader = new CheerioWebBaseLoader(\n  \"https://docs.smith.langchain.com/user_guide\"\n);\n\nconst rawDocs = await loader.load();\n\nrawDocs[0].pageContent.length;\n```\n\n----------------------------------------\n\nTITLE: Retrieving Cloudflare Credentials from Environment Variables\nDESCRIPTION: Code to access Cloudflare credentials from environment variables in a Deno environment. This snippet retrieves the account ID and API token needed for authentication.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/cloudflare_workersai.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// @lc-docs-hide-cell\n\n// @ts-expect-error Deno is not recognized\nconst CLOUDFLARE_ACCOUNT_ID = Deno.env.get(\"CLOUDFLARE_ACCOUNT_ID\");\n// @ts-expect-error Deno is not recognized\nconst CLOUDFLARE_API_TOKEN = Deno.env.get(\"CLOUDFLARE_API_TOKEN\");\n```\n\n----------------------------------------\n\nTITLE: Re-selecting Examples After Adding New Data\nDESCRIPTION: Demonstrates selecting examples again after the dataset has been expanded with a new example.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/example_selectors.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nawait exampleSelector.selectExamples({ input: \"okay\" })\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Document Retriever in TypeScript\nDESCRIPTION: Example of creating a custom retriever class that extends BaseRetriever and implements the _getRelevantDocuments method. The retriever returns static documents based on the input query and supports tracing through runManager.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/custom_retriever.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport {\n  BaseRetriever,\n  type BaseRetrieverInput,\n} from \"@langchain/core/retrievers\";\nimport type { CallbackManagerForRetrieverRun } from \"@langchain/core/callbacks/manager\";\nimport { Document } from \"@langchain/core/documents\";\n\nexport interface CustomRetrieverInput extends BaseRetrieverInput {}\n\nexport class CustomRetriever extends BaseRetriever {\n  lc_namespace = [\"langchain\", \"retrievers\"];\n\n  constructor(fields?: CustomRetrieverInput) {\n    super(fields);\n  }\n\n  async _getRelevantDocuments(\n    query: string,\n    runManager?: CallbackManagerForRetrieverRun\n  ): Promise<Document[]> {\n    // Pass `runManager?.getChild()` when invoking internal runnables to enable tracing\n    // const additionalDocs = await someOtherRunnable.invoke(params, runManager?.getChild());\n    return [\n      // ...additionalDocs,\n      new Document({\n        pageContent: `Some document pertaining to ${query}`,\n        metadata: {},\n      }),\n      new Document({\n        pageContent: `Some other document pertaining to ${query}`,\n        metadata: {},\n      }),\n    ];\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Invoking ChatOpenAI with System and User Messages\nDESCRIPTION: Demonstrates how to invoke the ChatOpenAI model with system and user messages to perform an English to French translation task.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst aiMsg = await llm.invoke([\n  {\n    role: \"system\",\n    content: \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n  },\n  {\n    role: \"user\",\n    content: \"I love programming.\"\n  },\n])\naiMsg\n```\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(aiMsg.content)\n```\n\n----------------------------------------\n\nTITLE: Adding Metadata to Documents in JavaScript\nDESCRIPTION: This code adds metadata to documents in a vector store, assigning sections (beginning, middle, end) based on document position.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nconst totalDocuments = allSplits.length;\nconst third = Math.floor(totalDocuments / 3);\n\nallSplits.forEach((document, i) => {\n    if (i < third) {\n        document.metadata[\"section\"] = \"beginning\";\n    } else if (i < 2 * third) {\n        document.metadata[\"section\"] = \"middle\";\n    } else {\n        document.metadata[\"section\"] = \"end\";\n    }\n});\n\nallSplits[0].metadata;\n```\n\n----------------------------------------\n\nTITLE: Recursive Character Text Splitting in TypeScript using LangChain\nDESCRIPTION: Implementation of recursive text splitting that preserves natural language structure using LangChain's RecursiveCharacterTextSplitter. Configured with a chunk size of 100 characters and no overlap.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/text_splitters.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\n\nconst textSplitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 100,\n  chunkOverlap: 0,\n});\nconst texts = await textSplitter.splitText(document);\n```\n\n----------------------------------------\n\nTITLE: Using LangChain's Retriever Interface\nDESCRIPTION: This code snippet demonstrates how to use LangChain's unified retriever interface to retrieve documents based on a query. It shows the simple invoke method pattern that works across all retriever implementations regardless of the underlying retrieval system.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/retrieval.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst docs = await retriever.invoke(query);\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG Chain with Self-Query Retriever\nDESCRIPTION: Setup of a RAG (Retrieval-Augmented Generation) chain using the self-query retriever, including document formatting and prompt template configuration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/pinecone.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { RunnablePassthrough, RunnableSequence } from \"@langchain/core/runnables\";\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\n\nimport type { Document } from \"@langchain/core/documents\";\n\nconst prompt = ChatPromptTemplate.fromTemplate(`\nAnswer the question based only on the context provided.\n\nContext: {context}\n\nQuestion: {question}`);\n\nconst formatDocs = (docs: Document[]) => {\n  return docs.map((doc) => JSON.stringify(doc)).join(\"\\n\\n\");\n}\n\nconst ragChain = RunnableSequence.from([\n  {\n    context: selfQueryRetriever.pipe(formatDocs),\n    question: new RunnablePassthrough(),\n  },\n  prompt,\n  llm,\n  new StringOutputParser(),\n]);\n```\n\n----------------------------------------\n\nTITLE: Executing a Few Shot Chain with a Chat Model in LangChain.js\nDESCRIPTION: This code demonstrates how to pipe the final prompt into a chat model and invoke the chain with a specific input. The chain combines the prompt template with the model to generate a response.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot_examples_chat.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst chain = finalPrompt.pipe(model);\n\nawait chain.invoke({ input: \"What's the square of a triangle?\" })\n```\n\n----------------------------------------\n\nTITLE: Implementing Zep Retriever with LangChain in TypeScript\nDESCRIPTION: This code snippet demonstrates how to use the Zep Retriever in a retrieval chain to fetch documents from a Zep Open Source memory store. It includes setting up the Zep client, creating a retriever, and using it in a RetrievalChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/zep-retriever.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport Example from \"@examples/retrievers/zep.ts\";\n```\n\n----------------------------------------\n\nTITLE: Creating Memory Vector Store in JavaScript\nDESCRIPTION: This code creates a MemoryVectorStore instance using the previously initialized embeddings for storing and retrieving vector representations of text.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/sql_qa.ipynb#2025-04-22_snippet_19\n\nLANGUAGE: javascript\nCODE:\n```\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n\nconst vectorStore = new MemoryVectorStore(embeddings);\n```\n\n----------------------------------------\n\nTITLE: Creating Chat Prompt Template with System and User Messages\nDESCRIPTION: Shows how to create a ChatPromptTemplate with multiple message types including system and user messages, formatting variables within the messages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/anthropic.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\nconst promptTemplate = ChatPromptTemplate.fromMessages([\n  [\"system\", \"You are a helpful assistant\"],\n  [\"user\", \"Tell me a joke about {topic}\"],\n]);\n\nawait promptTemplate.invoke({ topic: \"cats\" });\n```\n\n----------------------------------------\n\nTITLE: Setting Up Query Analysis Pipeline\nDESCRIPTION: Creates a runnable sequence for analyzing queries using chat prompts and structured output\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_high_cardinality.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { RunnablePassthrough, RunnableSequence } from \"@langchain/core/runnables\";\n\nconst system = `Generate a relevant search query for a library system`;\nconst prompt = ChatPromptTemplate.fromMessages(\n    [\n      [\"system\", system],\n      [\"human\", \"{question}\"],\n    ]\n)\nconst llmWithTools = llm.withStructuredOutput(searchSchema, {\n  name: \"Search\"\n})\nconst queryAnalyzer = RunnableSequence.from([\n  {\n    question: new RunnablePassthrough(),\n  },\n  prompt,\n  llmWithTools\n]);\n```\n\n----------------------------------------\n\nTITLE: Custom VectorStore Retriever in SemanticSimilarityExampleSelector\nDESCRIPTION: This snippet illustrates how to use a custom vector store retriever, such as maximal marginal relevance, instead of the default similarity search.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/example_selectors_similarity.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport ExampleSimilarityCustomRetriever from \"@examples/prompts/semantic_similarity_example_selector_custom_retriever.ts\";\n```\n\n----------------------------------------\n\nTITLE: Initializing Chat Model\nDESCRIPTION: Code for setting up a ChatOpenAI instance with specific model and temperature settings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ChatOpenAI } from '@langchain/openai';\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o-mini\",\n  temperature: 0,\n})\n```\n\n----------------------------------------\n\nTITLE: Embedding Multiple Text Documents\nDESCRIPTION: Demonstrates embedding multiple text documents using embedDocuments method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/pinecone.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconst text2 = \"LangGraph is a library for building stateful, multi-actor applications with LLMs\";\n\nconst vectors = await embeddings.embedDocuments([text, text2]);\n\nconsole.log(vectors[0].slice(0, 100));\nconsole.log(vectors[1].slice(0, 100));\n```\n\n----------------------------------------\n\nTITLE: Implementing ZepCloudMemory for LangChain Applications\nDESCRIPTION: Example showing how to implement Zep Cloud as a memory provider for LangChain applications. This implementation creates a memory instance that can be used with various chains and agents to maintain conversation history and context.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/zep_memory_cloud.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { ConversationChain } from \"langchain/chains\";\nimport { ZepCloudMemory } from \"@langchain/community/memory/zep_cloud\";\n\n/**\n * This example demonstrates how to use the Zep Cloud memory with a simple ConversationChain.\n * You'll need to set the ZEP_CLOUD_API_KEY environment variable.\n */\nexport const run = async () => {\n  // Initialize the memory\n  const memory = new ZepCloudMemory({\n    apiKey: process.env.ZEP_CLOUD_API_KEY, // Replace with your actual API key\n    sessionId: \"session-123\", // Unique identifier for the chat session\n    memoryKey: \"chat_history\",\n    returnMessages: true,\n  });\n\n  // Initialize the chat model\n  const model = new ChatOpenAI({\n    temperature: 0,\n  });\n\n  // Create a chain that uses the memory\n  const chain = new ConversationChain({\n    llm: model,\n    memory,\n  });\n\n  // Start a conversation\n  const res1 = await chain.call({\n    input: \"Hi! I'm Olivia. What's your name?\",\n  });\n  console.log({ res1 });\n\n  // Continue the conversation\n  const res2 = await chain.call({\n    input: \"What did I say my name was?\",\n  });\n  console.log({ res2 });\n\n  // The model should remember the name from the first message\n  const res3 = await chain.call({\n    input: \"What can you tell me about Zep memory?\",\n  });\n  console.log({ res3 });\n\n  // Demonstrate that the memory persists by loading the same session\n  const newMemory = new ZepCloudMemory({\n    apiKey: process.env.ZEP_CLOUD_API_KEY, // Replace with your actual API key\n    sessionId: \"session-123\", // Same session ID as before\n    memoryKey: \"chat_history\",\n    returnMessages: true,\n  });\n\n  // Create a new chain with the same session memory\n  const newChain = new ConversationChain({\n    llm: model,\n    memory: newMemory,\n  });\n\n  // The model should still remember previous context\n  const res4 = await newChain.call({\n    input: \"Can you remind me what my name is?\",\n  });\n  console.log({ res4 });\n};\n```\n\n----------------------------------------\n\nTITLE: Creating Query Generation Chain with OpenAI and String Parser\nDESCRIPTION: Defines a runnable sequence that takes an original query, passes it through the prompt and model to generate alternative queries, then parses and splits the output into an array of individual queries.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/rag_fusion.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\n/** Define our chain for generating queries  */\nconst generateQueries = RunnableSequence.from([\n  prompt,\n  model,\n  new StringOutputParser(),\n  (output) => output.split(\"\\n\"),\n]);\n```\n\n----------------------------------------\n\nTITLE: Initializing Chroma Vector Store with Document Metadata\nDESCRIPTION: This snippet demonstrates how to create documents with metadata, define attribute information, and instantiate a Chroma vector store with OpenAI embeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/chroma.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { Chroma } from \"@langchain/community/vectorstores/chroma\";\nimport { Document } from \"@langchain/core/documents\";\nimport type { AttributeInfo } from \"langchain/chains/query_constructor\";\n\n/**\n * First, we create a bunch of documents. You can load your own documents here instead.\n * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.\n */\nconst docs = [\n  new Document({\n    pageContent:\n      \"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n    metadata: { year: 1993, rating: 7.7, genre: \"science fiction\" },\n  }),\n  new Document({\n    pageContent:\n      \"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n    metadata: { year: 2010, director: \"Christopher Nolan\", rating: 8.2 },\n  }),\n  new Document({\n    pageContent:\n      \"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n    metadata: { year: 2006, director: \"Satoshi Kon\", rating: 8.6 },\n  }),\n  new Document({\n    pageContent:\n      \"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n    metadata: { year: 2019, director: \"Greta Gerwig\", rating: 8.3 },\n  }),\n  new Document({\n    pageContent: \"Toys come alive and have a blast doing so\",\n    metadata: { year: 1995, genre: \"animated\" },\n  }),\n  new Document({\n    pageContent: \"Three men walk into the Zone, three men walk out of the Zone\",\n    metadata: {\n      year: 1979,\n      director: \"Andrei Tarkovsky\",\n      genre: \"science fiction\",\n      rating: 9.9,\n    },\n  }),\n];\n\n/**\n * Next, we define the attributes we want to be able to query on.\n * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.\n * We also provide a description of each attribute and the type of the attribute.\n * This is used to generate the query prompts.\n */\nconst attributeInfo: AttributeInfo[] = [\n  {\n    name: \"genre\",\n    description: \"The genre of the movie\",\n    type: \"string or array of strings\",\n  },\n  {\n    name: \"year\",\n    description: \"The year the movie was released\",\n    type: \"number\",\n  },\n  {\n    name: \"director\",\n    description: \"The director of the movie\",\n    type: \"string\",\n  },\n  {\n    name: \"rating\",\n    description: \"The rating of the movie (1-10)\",\n    type: \"number\",\n  },\n  {\n    name: \"length\",\n    description: \"The length of the movie in minutes\",\n    type: \"number\",\n  },\n];\n\n/**\n * Next, we instantiate a vector store. This is where we store the embeddings of the documents.\n * We also need to provide an embeddings object. This is used to embed the documents.\n */\nconst embeddings = new OpenAIEmbeddings();\nconst vectorStore = await Chroma.fromDocuments(docs, embeddings, {\n  collectionName: \"movie-collection\",\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Arcjet Redact with ChatOpenAI\nDESCRIPTION: Example demonstrating how to wrap a ChatOpenAI model with Arcjet Redact to protect sensitive information. Shows configuration options including custom entity detection, context window size, and replacement text options.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/arcjet.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport {\n  ArcjetRedact,\n  ArcjetSensitiveInfoType,\n} from \"@langchain/community/chat_models/arcjet\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\n// Create an instance of another chat model for Arcjet to wrap\nconst openai = new ChatOpenAI({\n  temperature: 0.8,\n  model: \"gpt-3.5-turbo-0125\",\n});\n\nconst arcjetRedactOptions = {\n  // Specify a LLM that Arcjet Redact will call once it has redacted the input.\n  chatModel: openai,\n\n  // Specify the list of entities that should be redacted.\n  // If this isn't specified then all entities will be redacted.\n  entities: [\"email\", \"phone-number\", \"ip-address\", \"custom-entity\"] as ArcjetSensitiveInfoType[],\n\n  // You can provide a custom detect function to detect entities that we don't support yet.\n  // It takes a list of tokens and you return a list of identified types or undefined.\n  // The undefined types that you return should be added to the entities list if used.\n  detect: (tokens: string[]) => {\n    return tokens.map((t) => t === \"some-sensitive-info\" ? \"custom-entity\" : undefined)\n  },\n\n  // The number of tokens to provide to the custom detect function. This defaults to 1.\n  // It can be used to provide additional context when detecting custom entity types.\n  contextWindowSize: 1,\n\n  // This allows you to provide custom replacements when redacting. Please ensure\n  // that the replacements are unique so that unredaction works as expected.\n  replace: (identifiedType: string) => {\n    return identifiedType === \"email\" ? \"redacted@example.com\" : undefined;\n  },\n};\n\nconst arcjetRedact = new ArcjetRedact(arcjetRedactOptions);\n\nconst response = await arcjetRedact.invoke(\n  \"My email address is test@example.com, here is some-sensitive-info\"\n);\n```\n\n----------------------------------------\n\nTITLE: Testing Retriever with Ambiguous Query\nDESCRIPTION: Demonstrates the challenge of handling followup questions by showing how the retriever returns irrelevant documents when given a contextless query like \"Tell me more!\".\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_retrieval.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nawait retriever.invoke(\"Tell me more!\");\n```\n\n----------------------------------------\n\nTITLE: Streaming Large GitHub Repositories with LangChain.js\nDESCRIPTION: Example demonstrating how to stream documents from a large GitHub repository in a memory-efficient manner using the loadAsStream method instead of loading everything at once.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/github.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nStreamExample\n```\n\n----------------------------------------\n\nTITLE: Initializing Chroma Vector Store with OpenAI Embeddings (TypeScript)\nDESCRIPTION: This snippet imports necessary modules and creates an OpenAIEmbeddings instance with a specified model (text-embedding-3-small). It then instantiates a Chroma vector store with custom collection name and optional metadata (e.g., distance function for vector similarity), pointing to a local Chroma server. Establishing this vector store is foundational before adding documents or querying.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/chroma.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Chroma } from \"@langchain/community/vectorstores/chroma\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddings = new OpenAIEmbeddings({\n  model: \"text-embedding-3-small\",\n});\n\nconst vectorStore = new Chroma(embeddings, {\n  collectionName: \"a-test-collection\",\n  url: \"http://localhost:8000\", // Optional, will default to this value\n  collectionMetadata: {\n    \"hnsw:space\": \"cosine\",\n  }, // Optional, can be used to specify the distance method of the embedding space https://docs.trychroma.com/usage-guide#changing-the-distance-function\n});\n```\n\n----------------------------------------\n\nTITLE: Tracking Token Usage in Streaming Mode with OpenAI\nDESCRIPTION: Example demonstrating how to track token usage when using OpenAI's streaming response mode, which returns token usage metadata at the end of the stream.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chat_token_usage_tracking.mdx#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { HumanMessage } from \"@langchain/core/messages\";\n\nconst model = new ChatOpenAI({\n  streaming: true,\n});\n\n// Enable token usage for streams\nconst response = await model.invoke(\n  [new HumanMessage(\"Hi there, I'm a human.\")],\n  {\n    stream_options: { include_usage: true },\n  }\n);\n\n// Just so we know this is a streaming model\nconsole.log(\"Streaming model?\", model.streaming);\n\nconsole.log(JSON.stringify(response.usage_metadata, null, 2));\n/*\n{\n  \"input_tokens\": 15,\n  \"output_tokens\": 10,\n  \"total_tokens\": 25\n}\n*/\n\n// Usage info is also present in response_metadata, although\n// the key names are different for each provider\nconsole.log(response.response_metadata?.token_usage);\n/*\n{\n  \"prompt_tokens\": 15,\n  \"completion_tokens\": 10,\n  \"total_tokens\": 25\n}\n*/\n\n```\n\n----------------------------------------\n\nTITLE: Combining Multiple Filters with AND Logic in TypeScript\nDESCRIPTION: Shows how to apply multiple filter conditions simultaneously by providing an array of filter objects. These conditions are combined using `AND` logic in the resulting Cassandra query. In this example, it filters by `userid` (equality) and `create_datetime` (greater than).\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/cassandra.mdx#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\n[\n  { userid: userid_variable },\n  { name: \"create_datetime\", operator: \">\", value: some_date_variable },\n];\n```\n\n----------------------------------------\n\nTITLE: Creating Question Generator Chain\nDESCRIPTION: Combines prompt, model, and output parser into a runnable sequence for generating step-back questions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/step_back.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nconst questionGenerator = RunnableSequence.from([\n  prompt,\n  model,\n  stringOutputParser,\n]);\n```\n\n----------------------------------------\n\nTITLE: Importing OpenAIModerationChain in TypeScript\nDESCRIPTION: This snippet demonstrates how to import the OpenAIModerationChain from the langchain/chains package. This chain is used to implement content moderation using OpenAI's moderation API.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/openai.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAIModerationChain } from \"langchain/chains\";\n```\n\n----------------------------------------\n\nTITLE: Invoking the Few-Shot Chain with a Mathematical Query\nDESCRIPTION: Tests the dynamic few-shot example selection chain by invoking it with a mathematical query and logging the tool calls made by the model. Shows the practical application of the similarity-based example selection.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/example_selectors_langsmith.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst aiMsg = await chain.invoke({ input: \"whats the negation of the negation of 3\", system: false })\nconsole.log(aiMsg.tool_calls)\n```\n\n----------------------------------------\n\nTITLE: Initializing Environment and Defining Zod Schema\nDESCRIPTION: Sets up environment variables and defines a Zod schema for task validation. The schema specifies the structure for tasks including title, due date, and task type constraints.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/basic_critique_revise.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nDeno.env.set(\"OPENAI_API_KEY\", \"\");\nDeno.env.set(\"LANGSMITH_API_KEY\", \"\");\nDeno.env.set(\"LANGSMITH_TRACING\", \"true\");\n\nimport { z } from \"npm:zod\";\n\nconst zodSchema = z\n  .object({\n    tasks: z\n      .array(\n        z.object({\n          title: z\n            .string()\n            .describe(\"The title of the tasks, reminders and alerts\"),\n          due_date: z\n            .string()\n            .describe(\"Due date. Must be a valid ISO date string with timezone\"),\n          task_type: z\n            .enum([\n              \"Call\",\n              \"Message\",\n              \"Todo\",\n              \"In-Person Meeting\",\n              \"Email\",\n              \"Mail\",\n              \"Text\",\n              \"Open House\",\n            ])\n            .describe(\"The type of task\"),\n        })\n      )\n      .describe(\"The JSON for task, reminder or alert to create\"),\n  })\n  .describe(\"JSON definition for creating tasks, reminders and alerts\");\n```\n\n----------------------------------------\n\nTITLE: Querying Tigris Vector Store with TypeScript\nDESCRIPTION: This snippet demonstrates querying a pre-existing index in the Tigris vector store using similarity search. It uses language embeddings to find documents related to a search term, while filtering results based on metadata.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/tigris.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { VectorDocumentStore } from \"@tigrisdata/vector\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { TigrisVectorStore } from \"langchain/vectorstores/tigris\";\n\nconst index = new VectorDocumentStore({\n  connection: {\n    serverUrl: \"api.preview.tigrisdata.cloud\",\n    projectName: process.env.TIGRIS_PROJECT,\n    clientId: process.env.TIGRIS_CLIENT_ID,\n    clientSecret: process.env.TIGRIS_CLIENT_SECRET,\n  },\n  indexName: \"examples_index\",\n  numDimensions: 1536,\n});\n\nconst vectorStore = await TigrisVectorStore.fromExistingIndex(\n  new OpenAIEmbeddings(),\n  { index }\n);\n\n/* Search the vector DB independently with metadata filters */\nconst results = await vectorStore.similaritySearch(\"tigris\", 1, {\n  \"metadata.foo\": \"bar\",\n});\nconsole.log(JSON.stringify(results, null, 2));\n```\n\n----------------------------------------\n\nTITLE: Setting and Getting Data with InMemoryStore\nDESCRIPTION: Demonstrates how to set multiple key-value pairs using mset and retrieve them using mget method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/stores/in_memory.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AIMessage, HumanMessage } from \"@langchain/core/messages\";\n\nawait kvStore.mset(\n  [\n    [\"key1\", new HumanMessage(\"value1\")],\n    [\"key2\", new AIMessage(\"value2\")],\n  ]\n)\n\nawait kvStore.mget(\n  [\n    \"key1\",\n    \"key2\",\n  ]\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Redis Cache\nDESCRIPTION: Example of setting up Redis-based caching with OpenAI model\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/llm_caching.mdx#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAI } from \"@langchain/openai\";\nimport { RedisCache } from \"@langchain/community/caches/ioredis\";\nimport { Redis } from \"ioredis\";\n\n// See https://github.com/redis/ioredis for connection options\nconst client = new Redis({});\n\nconst cache = new RedisCache(client);\n\nconst model = new OpenAI({ cache });\n```\n\n----------------------------------------\n\nTITLE: Creating Object-Based Inputs with Prompt Templates\nDESCRIPTION: Implements a chat prompt template with language parameter and message placeholder, chained with a chat model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/message_history.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatPromptTemplate, MessagesPlaceholder } from \"@langchain/core/prompts\";\n\nconst prompt = ChatPromptTemplate.fromMessages([\n  [\"system\", \"Answer in {language}.\"],\n  new MessagesPlaceholder(\"messages\"),\n])\n\nconst runnable = prompt.pipe(llm);\n```\n\n----------------------------------------\n\nTITLE: Streaming Responses from Custom LLM\nDESCRIPTION: Shows how to use the streaming capability of the custom LLM to process responses chunk by chunk as they are generated.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/custom_llm.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst stream = await llm.stream(\"I am an LLM\");\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n```\n\n----------------------------------------\n\nTITLE: Creating an Operation from Comparisons in TypeScript\nDESCRIPTION: This snippet creates an Operation object using the 'and' operator and the previously constructed comparisons array. This represents the combined filter operation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_constructing_filters.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport {\n  Operation,\n  Operator,\n} from \"langchain/chains/query_constructor/ir\";\n\nconst _filter = new Operation(\"and\" as Operator, comparisons)\n```\n\n----------------------------------------\n\nTITLE: Loading Google Cloud Storage Files with LangChain in TypeScript\nDESCRIPTION: This code snippet demonstrates how to use the Google Cloud Storage loader to load files and convert them into LangChain Documents. It includes options for specifying storage options and authentication methods.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/google_cloud_storage.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { GoogleCloudStorageLoader } from \"@langchain/community/document_loaders/web/google_cloud_storage\";\n\nconst loader = new GoogleCloudStorageLoader({\n  bucket: \"example-bucket\",\n  objectName: \"example-file.txt\",\n  unstructuredAPIURL: \"http://localhost:8000/general/v0/general\",\n  unstructuredAPIKey: \"MY_API_KEY\",\n  // Optional: Specify storage options\n  storageOptions: {\n    // Your storage options here\n  },\n});\n\nconst docs = await loader.load();\n```\n\n----------------------------------------\n\nTITLE: Loading documents from a directory with UnstructuredDirectoryLoader\nDESCRIPTION: Code to instantiate and use UnstructuredDirectoryLoader to process all compatible files in a directory. The example shows importing the loader, creating an instance with a directory path, loading documents, and displaying results.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/unstructured.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { UnstructuredDirectoryLoader } from \"@langchain/community/document_loaders/fs/unstructured\";\n\nconst directoryLoader = new UnstructuredDirectoryLoader(\n  \"../../../../../../examples/src/document_loaders/example_data/\",\n  {}\n);\nconst directoryDocs = await directoryLoader.load();\nconsole.log(\"directoryDocs.length: \", directoryDocs.length);\nconsole.log(directoryDocs[0])\n\n```\n\n----------------------------------------\n\nTITLE: Embedding Single Text Query\nDESCRIPTION: Example of embedding a single text query using embedQuery method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/google_generativeai.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst singleVector = await embeddings.embedQuery(text);\n\nconsole.log(singleVector.slice(0, 100));\n```\n\n----------------------------------------\n\nTITLE: Installing Additional LangChain Integration Packages\nDESCRIPTION: Command to install additional LangChain packages including OpenAI integration, community modules, and core components that may be needed for a complete implementation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/azure_cosmosdb_nosql.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Using VertexAIEmbeddings with Vector Store\nDESCRIPTION: Example of indexing and retrieving documents using VertexAIEmbeddings with MemoryVectorStore\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/google_vertex_ai.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n// Create a vector store with a sample text\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n\nconst text = \"LangChain is the framework for building context-aware reasoning applications\";\n\nconst vectorstore = await MemoryVectorStore.fromDocuments(\n  [{ pageContent: text, metadata: {} }],\n  embeddings,\n);\n\n// Use the vector store as a retriever that returns a single document\nconst retriever = vectorstore.asRetriever(1);\n\n// Retrieve the most similar text\nconst retrievedDocuments = await retriever.invoke(\"What is LangChain?\");\n\nretrievedDocuments[0].pageContent;\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic RAG Chain\nDESCRIPTION: Assembling the complete RAG chain with document formatting and answer generation\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_citations.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { Document } from \"@langchain/core/documents\";\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\nimport { RunnableMap, RunnablePassthrough } from \"@langchain/core/runnables\";\n\n/**\n * Format the documents into a readable string.\n */\nconst formatDocs = (input: Record<string, any>): string => {\n  const { docs } = input;\n  return \"\\n\\n\" + docs.map((doc: Document) => `Article title: ${doc.metadata.title}\\nArticle Snippet: ${doc.pageContent}`).join(\"\\n\\n\");\n}\n// subchain for generating an answer once we've done retrieval\nconst answerChain = prompt.pipe(llm).pipe(new StringOutputParser());\nconst map = RunnableMap.from({\n  question: new RunnablePassthrough(),\n  docs: retriever,\n})\n// complete chain that calls the retriever -> formats docs to string -> runs answer subchain -> returns just the answer and retrieved docs.\nconst chain = map.assign({ context: formatDocs }).assign({ answer: answerChain }).pick([\"answer\", \"docs\"])\n\nawait chain.invoke(\"How fast are cheetahs?\")\n```\n\n----------------------------------------\n\nTITLE: Initializing NIBittensorChatModel in TypeScript\nDESCRIPTION: Example showing how to initialize and use the NIBittensorChatModel to interact with Bittensor chat models. The code demonstrates creating a chat instance, sending a human message, and receiving a response.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/ni_bittensor.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { NIBittensorChatModel } from \"langchain/experimental/chat_models/bittensor\";\nimport { HumanMessage } from \"@langchain/core/messages\";\n\nconst chat = new NIBittensorChatModel();\nconst message = new HumanMessage(\"What is bittensor?\");\nconst res = await chat.invoke([message]);\nconsole.log({ res });\n/*\n  {\n    res: \"\\nBittensor is opensource protocol...\"\n  }\n */\n```\n\n----------------------------------------\n\nTITLE: Installing Supabase JavaScript Client\nDESCRIPTION: Command to install the Supabase JavaScript client library using npm or yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/supabase-hybrid.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -S @supabase/supabase-js\n```\n\n----------------------------------------\n\nTITLE: LangChain AIMessageChunk with Tavily Search Tool Call\nDESCRIPTION: JSON representation of a LangChain AIMessageChunk containing a tool call to Tavily search. The call is searching for information about the Oppenheimer film director's age using the Claude 3 Sonnet model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_39\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"lc\": 1,\n  \"type\": \"constructor\",\n  \"id\": [\n    \"langchain_core\",\n    \"messages\",\n    \"AIMessageChunk\"\n  ],\n  \"kwargs\": {\n    \"content\": [\n      {\n        \"type\": \"tool_use\",\n        \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n        \"name\": \"tavily_search_results_json\",\n        \"input\": {\n          \"input\": \"Oppenheimer 2023 film director age\"\n        }\n      }\n    ],\n    \"additional_kwargs\": {\n      \"id\": \"msg_015MqAHr84dBCAqBgjou41Km\",\n      \"type\": \"message\",\n      \"role\": \"assistant\",\n      \"model\": \"claude-3-sonnet-20240229\",\n      \"stop_sequence\": null,\n      \"usage\": {\n        \"input_tokens\": 409,\n        \"output_tokens\": 68\n      },\n      \"stop_reason\": \"tool_use\"\n    },\n    \"tool_call_chunks\": [\n      {\n        \"name\": \"tavily_search_results_json\",\n        \"args\": \"{\\\"input\\\":\\\"Oppenheimer 2023 film director age\\\"}\",\n        \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n        \"index\": 0\n      }\n    ],\n    \"tool_calls\": [\n      {\n        \"name\": \"tavily_search_results_json\",\n        \"args\": {\n          \"input\": \"Oppenheimer 2023 film director age\"\n        },\n        \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\"\n      }\n    ],\n    \"invalid_tool_calls\": [],\n    \"response_metadata\": {}\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Vector Similarity Search with Metadata Filtering using SingleStoreVectorStore - TypeScript\nDESCRIPTION: Shows how to extend standard vector search operations by applying a metadata filter parameter with SingleStoreVectorStore in LangChain.js. The filter object allows narrowing down search results to documents matching specified metadata fields for more precise data retrieval. This example requires a working SingleStore store connection and compatible LangChain.js types. Passing the filter parameter restricts query results to documents whose metadata match all specified criteria, enhancing granularity of vector searches.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/singlestore.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n{UsageExampleWithMetadata}\n```\n\n----------------------------------------\n\nTITLE: Dispatching Custom Events with Stream Events API in Web Environment\nDESCRIPTION: This snippet shows how to dispatch custom events in a web environment that doesn't support async_hooks. It uses the web-specific import and manually propagates the RunnableConfig.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/callbacks_custom_events.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RunnableConfig, RunnableLambda } from \"@langchain/core/runnables\";\nimport { dispatchCustomEvent as dispatchCustomEventWeb } from \"@langchain/core/callbacks/dispatch/web\";\n\nconst reflect = RunnableLambda.from(async (value: string, config?: RunnableConfig) => {\n  await dispatchCustomEventWeb(\"event1\", { reversed: value.split(\"\").reverse().join(\"\") }, config);\n  await dispatchCustomEventWeb(\"event2\", 5, config);\n  return value;\n});\n\nconst eventStream = await reflect.streamEvents(\"hello world\", { version: \"v2\" });\n\nfor await (const event of eventStream) {\n  if (event.event === \"on_custom_event\") {\n    console.log(event);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search with Scores in RedisVectorStore\nDESCRIPTION: Execute a similarity search that returns both documents and corresponding similarity scores. This helps in determining the relevance of each document to the query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/redis.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nconst similaritySearchWithScoreResults = await vectorStore.similaritySearchWithScore(\"biology\", 2)\n\nfor (const [doc, score] of similaritySearchWithScoreResults) {\n  console.log(`* [SIM=${score.toFixed(3)}] ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`);\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing WatsonxLLM with IAM Authentication\nDESCRIPTION: Creates a new instance of WatsonxLLM using IAM authentication by providing version, service URL, project ID, and API key.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/ibm.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { WatsonxLLM } from \"@langchain/community/llms/ibm\";\n\nconst props = {\n  version: \"YYYY-MM-DD\",\n  serviceUrl: \"<SERVICE_URL>\",\n  projectId: \"<PROJECT_ID>\",\n  watsonxAIAuthType: \"iam\",\n  watsonxAIApikey: \"<YOUR-APIKEY>\",\n};\nconst instance = new WatsonxLLM(props);\n```\n\n----------------------------------------\n\nTITLE: Inferring Model Provider in initChatModel\nDESCRIPTION: Shows how initChatModel() can automatically infer the model provider based on common model names without requiring explicit provider specification.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chat_models_universal_init.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { initChatModel } from \"langchain/chat_models_universal\";\n\n// For certain model names, the provider can be inferred\nconst chat = await initChatModel({\n  model: \"gpt-3.5-turbo\",\n});\n\n// Providers can be inferred for common model names, see API reference for full list\nconst anotherChat = await initChatModel({\n  model: \"claude-2\",\n});\n```\n\n----------------------------------------\n\nTITLE: Displaying the Final Summary\nDESCRIPTION: Outputs the final generated summary after the map-reduce process is complete. This simple statement shows the result of the entire orchestration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/summarization.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nfinalSummary\n```\n\n----------------------------------------\n\nTITLE: Streaming Chain Results to Client\nDESCRIPTION: Executing the chain, streaming results, and updating the streamable value for client consumption.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/stream_tool_client.mdx#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst streamResult = await chain.stream({\n  input,\n});\n\nfor await (const item of streamResult) {\n  stream.update(JSON.parse(JSON.stringify(item, null, 2)));\n}\n\nstream.done();\n```\n\n----------------------------------------\n\nTITLE: Configuring AzureChatOpenAI with Azure Managed Identity\nDESCRIPTION: Demonstrates how to set up AzureChatOpenAI to use Azure Managed Identity for authentication instead of API keys.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/azure.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport {\n  DefaultAzureCredential,\n  getBearerTokenProvider,\n} from \"@azure/identity\";\nimport { AzureChatOpenAI } from \"@langchain/openai\";\n\nconst credentials = new DefaultAzureCredential();\nconst azureADTokenProvider = getBearerTokenProvider(\n  credentials,\n  \"https://cognitiveservices.azure.com/.default\"\n);\n\nconst llmWithManagedIdentity = new AzureChatOpenAI({\n  azureADTokenProvider,\n  azureOpenAIApiInstanceName: \"<your_instance_name>\",\n  azureOpenAIApiDeploymentName: \"<your_deployment_name>\",\n  azureOpenAIApiVersion: \"<api_version>\",\n});\n```\n\n----------------------------------------\n\nTITLE: Extracting and Logging ToolChain Result - JavaScript/TypeScript\nDESCRIPTION: This code snippet extracts the 'tool_calls' and 'content' from the result of a chained tool-LLM invocation, then logs them as a formatted JSON object. Useful for inspecting combined LLM and tool responses in development or debugging scenarios. Assumes previous execution of the chained tool pipeline; expects the result object as input.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/duckduckgo_search.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst { tool_calls, content } = toolChainResult;\n\nconsole.log(\"AIMessage\", JSON.stringify({\n  tool_calls,\n  content,\n}, null, 2));\n```\n\n----------------------------------------\n\nTITLE: Attaching ConsoleCallbackHandler to a LangChain Runnable Chain\nDESCRIPTION: Example showing how to create a chain of runnables with a ChatPromptTemplate and ChatAnthropic model, then attach a ConsoleCallbackHandler using the withConfig() method. The callback will run for all nested module executions, eliminating the need to pass callbacks on each invoke call.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/callbacks_attach.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport { ConsoleCallbackHandler } from \"@langchain/core/tracers/console\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst handler = new ConsoleCallbackHandler();\n\nconst prompt = ChatPromptTemplate.fromTemplate(`What is 1 + {number}?`);\nconst model = new ChatAnthropic({\n  model: \"claude-3-sonnet-20240229\",\n});\n\nconst chainWithCallbacks = prompt.pipe(model).withConfig({\n  callbacks: [handler],\n});\n\nawait chainWithCallbacks.invoke({ number: \"2\" });\n```\n\n----------------------------------------\n\nTITLE: Creating a Chain with TavilySearch Tool and LLM\nDESCRIPTION: Builds a chain that combines a prompt template, a language model with bound tools, and the TavilySearch tool to process user inputs and generate responses.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/tavily_search.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport { HumanMessage } from \"@langchain/core/messages\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { RunnableLambda } from \"@langchain/core/runnables\";\n\nconst prompt = ChatPromptTemplate.fromMessages(\n  [\n    [\"system\", \"You are a helpful assistant.\"],\n    [\"placeholder\", \"{messages}\"],\n  ]\n)\n\nconst llmWithTools = llm.bindTools([tool]);\n\nconst chain = prompt.pipe(llmWithTools);\n\nconst toolChain = RunnableLambda.from(\n  async (userInput: string, config) => {\n    const humanMessage = new HumanMessage(userInput,);\n    const aiMsg = await chain.invoke({\n      messages: [new HumanMessage(userInput)],\n    }, config);\n    const toolMsgs = await tool.batch(aiMsg.tool_calls, config);\n    return chain.invoke({\n      messages: [humanMessage, aiMsg, ...toolMsgs],\n    }, config);\n  }\n);\n\nconst toolChainResult = await toolChain.invoke(\"what is the current weather in sf?\");\n```\n\n----------------------------------------\n\nTITLE: Initializing CassandraStore and Indexing Documents in TypeScript\nDESCRIPTION: Demonstrates how to initialize the `CassandraStore` and index text documents. It combines a base connection configuration (`configConnection`) with Cassandra-specific settings like keyspace, table name, vector dimensions, primary key definition, metadata columns, and optional indices. The `CassandraStore.fromTexts` method is used to add documents, their associated metadata, and embeddings generated by `OpenAIEmbeddings`.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/cassandra.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CassandraStore } from \"langchain/vectorstores/cassandra\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\n// The configConnection document is defined above\nconst config = {\n  ...configConnection,\n  keyspace: \"test\",\n  dimensions: 1536,\n  table: \"test\",\n  indices: [{ name: \"name\", value: \"(name)\" }],\n  primaryKey: {\n    name: \"id\",\n    type: \"int\",\n  },\n  metadataColumns: [\n    {\n      name: \"name\",\n      type: \"text\",\n    },\n  ],\n};\n\nconst vectorStore = await CassandraStore.fromTexts(\n  [\"I am blue\", \"Green yellow purple\", \"Hello there hello\"],\n  [\n    { id: 2, name: \"2\" },\n    { id: 1, name: \"1\" },\n    { id: 3, name: \"3\" },\n  ],\n  new OpenAIEmbeddings(),\n  cassandraConfig\n);\n```\n\n----------------------------------------\n\nTITLE: Embedding Multiple Text Documents\nDESCRIPTION: Shows how to generate vector embeddings for multiple text documents simultaneously.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/bytedance_doubao.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconst text2 = \"LangGraph is a library for building stateful, multi-actor applications with LLMs\";\n\nconst vectors = await embeddings.embedDocuments([text, text2]);\n\nconsole.log(vectors[0].slice(0, 100));\nconsole.log(vectors[1].slice(0, 100));\n```\n\n----------------------------------------\n\nTITLE: Invoking LangChain Agent with Chat History in Python\nDESCRIPTION: This example shows how to invoke the LangChain agent with chat history using the RunnableWithMessageHistory wrapper, demonstrating stateful conversation capabilities.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/agent_executor.ipynb#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nawait agentWithChatHistory.invoke(\n  { input: \"what's my name?\" },\n  { configurable: { sessionId: \"<foo>\" }},\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a RAG Chain in TypeScript\nDESCRIPTION: Demonstrates how to create a Retrieval-Augmented Generation (RAG) chain using the retriever, a prompt template, and a language model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/retrievers.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { RunnablePassthrough, RunnableSequence } from \"@langchain/core/runnables\";\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\n\nimport type { Document } from \"@langchain/core/documents\";\n\nconst prompt = ChatPromptTemplate.fromTemplate(`\nAnswer the question based only on the context provided.\n\nContext: {context}\n\nQuestion: {question}`);\n\nconst formatDocs = (docs: Document[]) => {\n  return docs.map((doc) => doc.pageContent).join(\"\\n\\n\");\n}\n\n// See https://js.langchain.com/docs/tutorials/rag\nconst ragChain = RunnableSequence.from([\n  {\n    context: retriever.pipe(formatDocs),\n    question: new RunnablePassthrough(),\n  },\n  prompt,\n  llm,\n  new StringOutputParser(),\n]);\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key\nDESCRIPTION: Configuration for OpenAI API key used for embeddings\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/weaviate.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.OPENAI_API_KEY = \"YOUR_API_KEY\";\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search with Filters in Qdrant\nDESCRIPTION: Executes a similarity search on the vector store with a metadata filter to narrow results. This demonstrates how to query the vector store for semantically similar documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/qdrant.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst filter = {\n  \"must\": [\n      { \"key\": \"metadata.source\", \"match\": { \"value\": \"https://example.com\" } },\n  ]\n};\n\nconst similaritySearchResults = await vectorStore.similaritySearch(\"biology\", 2, filter);\n\nfor (const doc of similaritySearchResults) {\n  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);\n}\n```\n\n----------------------------------------\n\nTITLE: Creating and Querying ZepVectorStore from Documents\nDESCRIPTION: Example demonstrating how to create a ZepVectorStore instance from documents and perform basic queries using the auto-embedding feature.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/zep.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n{ExampleDocs}\n```\n\n----------------------------------------\n\nTITLE: Query Analyzer Setup\nDESCRIPTION: Creates a runnable sequence for query analysis using chat templates and structured output.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_multiple_queries.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { RunnableSequence, RunnablePassthrough } from \"@langchain/core/runnables\";\n\nconst system = `You have the ability to issue search queries to get information to help answer user information.\n\nIf you need to look up two distinct pieces of information, you are allowed to do that!`;\n\nconst prompt = ChatPromptTemplate.fromMessages([\n    [\"system\", system],\n    [\"human\", \"{question}\"],\n])\nconst llmWithTools = llm.withStructuredOutput(searchSchema, {\n  name: \"Search\"\n});\nconst queryAnalyzer = RunnableSequence.from([\n  {\n      question: new RunnablePassthrough(),\n  },\n  prompt,\n  llmWithTools\n]);\n```\n\n----------------------------------------\n\nTITLE: Data Extraction with Ollama Functions\nDESCRIPTION: Shows how to use OllamaFunctions for data extraction tasks\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/ollama_functions.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport OllamaFunctionsExtraction from \"@examples/models/chat/ollama_functions/extraction.ts\";\n```\n\n----------------------------------------\n\nTITLE: Implementing Vector Store and Retrieval\nDESCRIPTION: Demonstrates creating a vector store from documents and using it as a retriever.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/pinecone.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n// Create a vector store with a sample text\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n\nconst text = \"LangChain is the framework for building context-aware reasoning applications\";\n\nconst vectorstore = await MemoryVectorStore.fromDocuments(\n  [{ pageContent: text, metadata: {} }],\n  embeddings,\n);\n\n// Use the vector store as a retriever that returns a single document\nconst retriever = vectorstore.asRetriever(1);\n\n// Retrieve the most similar text\nconst retrievedDocuments = await retriever.invoke(\"What is LangChain?\");\n\nretrievedDocuments[0].pageContent;\n```\n\n----------------------------------------\n\nTITLE: Instantiating ChatOpenAI Model\nDESCRIPTION: Creates an instance of ChatOpenAI with a specific model and temperature setting for generating chat completions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\" \n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o\",\n  temperature: 0,\n  // other params...\n})\n```\n\n----------------------------------------\n\nTITLE: Chat Model Configuration\nDESCRIPTION: Setting up the ChatOpenAI model instance with specific parameters\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/stream_agent_client.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o-2024-05-13\",\n  temperature: 0,\n});\n```\n\n----------------------------------------\n\nTITLE: Using the Custom LLM for Text Generation\nDESCRIPTION: Demonstrates how to instantiate and use the custom LLM to generate text responses. This example shows the basic invocation pattern.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/custom_llm.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst llm = new CustomLLM({ n: 4 });\n\nawait llm.invoke(\"I am an LLM\");\n```\n\n----------------------------------------\n\nTITLE: Instantiating FaissStore with OpenAI Embeddings\nDESCRIPTION: Creates a new FaissStore instance using OpenAI embeddings, initializing a local vector store for similarity search operations.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/faiss.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { FaissStore } from \"@langchain/community/vectorstores/faiss\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddings = new OpenAIEmbeddings({\n  model: \"text-embedding-3-small\",\n});\n\nconst vectorStore = new FaissStore(embeddings, {});\n```\n\n----------------------------------------\n\nTITLE: Creating a Configurable Chat Model\nDESCRIPTION: Demonstrates how to create a runtime-configurable chat model by specifying which fields should be configurable. This allows for dynamic model parameter adjustments.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chat_models_universal_init.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { initChatModel } from \"langchain/chat_models_universal\";\n\n// If no model value is provided, model and modelProvider are configurable by default\nconst configurableChat = await initChatModel({\n  configurableFields: [\"temperature\", \"maxTokens\"],\n});\n\n// Later, you can provide the actual configuration\nconst chat = configurableChat.withConfig({\n  model: \"gpt-3.5-turbo\",\n  temperature: 0.7,\n  maxTokens: 500,\n});\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search with Scores (TypeScript)\nDESCRIPTION: Executes a similarity search that returns both the matching documents and their corresponding similarity scores. It uses the `similaritySearchWithScore` method, optionally including a filter. The results are tuples of [document, score].\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/mongodb_atlas.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nconst similaritySearchWithScoreResults = await vectorStore.similaritySearchWithScore(\"biology\", 2, filter)\n\nfor (const [doc, score] of similaritySearchWithScoreResults) {\n  console.log(`* [SIM=${score.toFixed(3)}] ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`);\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Hooks\nDESCRIPTION: Definition and implementation of custom hooks for request handling, error handling, and response processing.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/mistral.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconst beforeRequestHook = (req: Request): Request | void | Promise<Request | void> => {\n    // Code to run before a request is processed by Mistral\n};\n\nconst requestErrorHook = (err: unknown, req: Request): void | Promise<void> => {\n    // Code to run when an error occurs as Mistral is processing a request\n};\n\nconst responseHook = (res: Response, req: Request): void | Promise<void> => {\n    // Code to run before Mistral sends a successful response\n};\n```\n\n----------------------------------------\n\nTITLE: Integrating Self-Query Retriever in a Chain in TypeScript\nDESCRIPTION: Shows how to incorporate the Supabase self-query retriever into a LangChain chain, including formatting retrieved documents and creating a RAG chain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/supabase.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { RunnablePassthrough, RunnableSequence } from \"@langchain/core/runnables\";\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\n\nimport type { Document } from \"@langchain/core/documents\";\n\nconst prompt = ChatPromptTemplate.fromTemplate(`\nAnswer the question based only on the context provided.\n\nContext: {context}\n\nQuestion: {question}`);\n\nconst formatDocs = (docs: Document[]) => {\n  return docs.map((doc) => JSON.stringify(doc)).join(\"\\n\\n\");\n}\n\n// See https://js.langchain.com/docs/tutorials/rag\nconst ragChain = RunnableSequence.from([\n  {\n    context: selfQueryRetriever.pipe(formatDocs),\n    question: new RunnablePassthrough(),\n  },\n  prompt,\n  llm,\n  new StringOutputParser(),\n]);\n\nawait ragChain.invoke(\"Which movies are rated higher than 8.5?\");\n```\n\n----------------------------------------\n\nTITLE: Indexing a LangSmith Dataset for Similarity Search\nDESCRIPTION: Enables the similarity search capability for a LangSmith dataset by indexing it. This allows for retrieving similar examples based on query inputs.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/example_selectors_langsmith.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nawait lsClient.indexDataset({ datasetId: dataset.id });\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG with ChatCohere\nDESCRIPTION: Shows how to use Retrieval-Augmented Generation with ChatCohere by providing documents as context for the model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/cohere.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatCohere } from \"@langchain/cohere\";\nimport { HumanMessage } from \"@langchain/core/messages\";\n\nconst llmForRag = new ChatCohere({\n  apiKey: process.env.COHERE_API_KEY, // Default\n});\n\nconst documents = [\n  {\n    title: \"Harrison's work\",\n    snippet: \"Harrison worked at Kensho as an engineer.\",\n  },\n  {\n    title: \"Harrison's work duration\",\n    snippet: \"Harrison worked at Kensho for 3 years.\",\n  },\n  {\n    title: \"Polar berars in the Appalachian Mountains\",\n    snippet:\n      \"Polar bears have surprisingly adapted to the Appalachian Mountains, thriving in the diverse, forested terrain despite their traditional arctic habitat. This unique situation has sparked significant interest and study in climate adaptability and wildlife behavior.\",\n  },\n];\n\nconst ragResponse = await llmForRag.invoke(\n  [new HumanMessage(\"Where did Harrison work and for how long?\")],\n  {\n    documents,\n  }\n);\nconsole.log(ragResponse.content);\n```\n\n----------------------------------------\n\nTITLE: Basic Message Filtering by Type in LangChain.js\nDESCRIPTION: Demonstrates basic usage of filterMessages to filter different types of chat messages using type-based inclusion criteria.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/filter_messages.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport { HumanMessage, SystemMessage, AIMessage, filterMessages } from \"@langchain/core/messages\"\n\nconst messages = [\n    new SystemMessage({ content: \"you are a good assistant\", id: \"1\" }),\n    new HumanMessage({ content: \"example input\", id: \"2\", name: \"example_user\" }),\n    new AIMessage({ content: \"example output\", id: \"3\", name: \"example_assistant\" }),\n    new HumanMessage({ content: \"real input\", id: \"4\", name: \"bob\" }),\n    new AIMessage({ content: \"real output\", id: \"5\", name: \"alice\" }),\n]\n\nfilterMessages(messages, { includeTypes: [\"human\"] })\n```\n\n----------------------------------------\n\nTITLE: Function Partials Example with Text Few Shot Templates in LangChainJS\nDESCRIPTION: Shows a complete example of using function partials with text-based few shot templates, where a function returns a promised value.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot.mdx#2025-04-22_snippet_16\n\nLANGUAGE: typescript\nCODE:\n```\nconst examplePrompt = PromptTemplate.fromTemplate(\"{foo}{bar}\");\nconst prompt = new FewShotPromptTemplate({\n  prefix: \"{foo}{bar}\",\n  examplePrompt,\n  inputVariables: [\"foo\", \"bar\"],\n});\nconst partialPrompt = await prompt.partial({\n  foo: () => Promise.resolve(\"boo\"),\n});\nconst formatted = await partialPrompt.format({ bar: \"baz\" });\nconsole.log(formatted);\n```\n\n----------------------------------------\n\nTITLE: Testing Message Memory with Sample Chat History\nDESCRIPTION: Adds a demo chat history to test if the LangGraph app can remember previously shared information, such as a user's name, across multiple conversation turns.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_memory.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst demoEphemeralChatHistory = [\n  { role: \"user\", content: \"Hey there! I'm Nemo.\" },\n  { role: \"assistant\", content: \"Hello!\" },\n  { role: \"user\", content: \"How are you today?\" },\n  { role: \"assistant\", content: \"Fine thanks!\" },\n];\n\nawait app.invoke(\n  {\n    messages: [\n      ...demoEphemeralChatHistory,\n      { role: \"user\", content: \"What's my name?\" }\n    ]\n  },\n  {\n    configurable: { thread_id: \"2\" }\n  }\n);\n```\n\n----------------------------------------\n\nTITLE: Setting up OpenAI Function Calls with Schema\nDESCRIPTION: Configures the OpenAI chat model with function calling capabilities using the converted Zod schema. Creates a chain for generating tasks based on user queries.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/basic_critique_revise.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { zodToJsonSchema } from \"npm:zod-to-json-schema\";\n\nimport { JsonOutputFunctionsParser } from \"npm:langchain@0.0.173/output_parsers\";\nimport { ChatOpenAI } from \"npm:langchain@0.0.173/chat_models/openai\";\nimport { PromptTemplate } from \"npm:langchain@0.0.173/prompts\";\n\nconst functionSchema = {\n  name: \"task-scheduler\",\n  description: \"Schedules tasks\",\n  parameters: zodToJsonSchema(zodSchema)\n};\n\nconst template = `Respond to the following user query to the best of your ability:\n\n{query}`;\n\nconst generatePrompt = PromptTemplate.fromTemplate(template);\n\nconst taskFunctionCallModel = new ChatOpenAI({\n  temperature: 0,\n  model: \"gpt-3.5-turbo\",\n}).bind({\n  functions: [functionSchema],\n  function_call: { name: \"task-scheduler\" },\n});\n\nconst generateChain = generatePrompt\n  .pipe(taskFunctionCallModel)\n  .pipe(new JsonOutputFunctionsParser()) \n  .withConfig({ runName: \"GenerateChain\" });\n```\n\n----------------------------------------\n\nTITLE: Defining Tools for Addition and Multiplication in JavaScript\nDESCRIPTION: This snippet defines two tools using the LangChain tool function: one for addition and one for multiplication. Each tool has a name, schema, and description.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_results_pass_to_model.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { z } from \"zod\";\nimport { tool } from \"@langchain/core/tools\";\n\nconst addTool = tool(async ({ a, b }) => {\n  return a + b;\n}, {\n  name: \"add\",\n  schema: z.object({\n    a: z.number(),\n    b: z.number(),\n  }),\n  description: \"Adds a and b.\",\n});\n\nconst multiplyTool = tool(async ({ a, b }) => {\n  return a * b;\n}, {\n  name: \"multiply\",\n  schema: z.object({\n    a: z.number(),\n    b: z.number(),\n  }),\n  description: \"Multiplies a and b.\",\n});\n\nconst tools = [addTool, multiplyTool];\n```\n\n----------------------------------------\n\nTITLE: Example Agent Interaction and API Usage - JSON & Plaintext\nDESCRIPTION: Simulates an agent's multi-step reasoning for retrieving Klarna product data using a ChatGPT plugin in LangChain. Steps include selecting an action, making an API request, and parsing the response. The code snippets illustrate typical intermediate outputs, action payloads, and the final response, following OpenAI function-calling conventions and Klarna's OpenAPI schema. Inputs include a shirt search term, and outputs detail product names and attributes. Limitations: works best for product-related search terms, with no authentication required.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/aiplugin-tool.mdx#2025-04-22_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nEntering new agent_executor chain...\nThought: Klarna is a payment provider, not a store. I need to check if there is a Klarna Shopping API that I can use to search for t-shirts.\nAction:\n\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n\"action\": \"KlarnaProducts\",\n\"action_input\": \"\"\n}\n```\n\nLANGUAGE: plaintext\nCODE:\n```\nNow that I know there is a Klarna Shopping API, I can use it to search for t-shirts. I will make a GET request to the API with the query parameter \"t-shirt\".\nAction:\n\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n\"action\": \"requests_get\",\n\"action_input\": \"https://www.klarna.com/us/shopping/public/openai/v0/products?q=t-shirt\"\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\"products\":[{\"name\":\"Psycho Bunny Mens Copa Gradient Logo Graphic Tee\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3203663222/Clothing/Psycho-Bunny-Mens-Copa-Gradient-Logo-Graphic-Tee/?source=openai\",\"price\":\"$35.00\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:White,Blue,Black,Orange\"]},{\"name\":\"T-shirt\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3203506327/Clothing/T-shirt/?source=openai\",\"price\":\"$20.45\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:Gray,White,Blue,Black,Orange\"]},{\"name\":\"Palm Angels Bear T-shirt - Black\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3201090513/Clothing/Palm-Angels-Bear-T-shirt-Black/?source=openai\",\"price\":\"$168.36\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:Black\"]},{\"name\":\"Tommy Hilfiger Essential Flag Logo T-shirt\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3201840629/Clothing/Tommy-Hilfiger-Essential-Flag-Logo-T-shirt/?source=openai\",\"price\":\"$22.52\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:Red,Gray,White,Blue,Black\",\"Pattern:Solid Color\",\"Environmental Attributes :Organic\"]},{\"name\":\"Coach Outlet Signature T Shirt\",\"url\":\"https://www.klarna.com/us/shopping/pl/cl10001/3203005573/Clothing/Coach-Outlet-Signature-T-Shirt/?source=openai\",\"price\":\"$75.00\",\"attributes\":[\"Material:Cotton\",\"Target Group:Man\",\"Color:Gray\"]}]}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  result: {\n    output: 'The available t-shirts in Klarna are Psycho Bunny Mens Copa Gradient Logo Graphic Tee, T-shirt, Palm Angels Bear T-shirt - Black, Tommy Hilfiger Essential Flag Logo T-shirt, and Coach Outlet Signature T Shirt.',\n    intermediateSteps: [ [Object], [Object] ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Self-Query Retriever in TypeScript\nDESCRIPTION: Demonstrates how to use the self-query retriever to query for movies rated higher than 8.5.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/supabase.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nawait selfQueryRetriever.invoke(\n  \"Which movies are rated higher than 8.5?\"\n);\n```\n\n----------------------------------------\n\nTITLE: Basic Retriever Usage\nDESCRIPTION: Simple example of querying the retriever.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/bedrock-knowledge-bases.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst query = \"...\"\n\nawait retriever.invoke(query);\n```\n\n----------------------------------------\n\nTITLE: Invoking the Agent Executor for Movie Information\nDESCRIPTION: This code demonstrates how to invoke the agent executor to retrieve information about actors in a specific movie.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/graph_semantic.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nawait agentExecutor.invoke({ input: \"Who played in Casino?\" })\n```\n\n----------------------------------------\n\nTITLE: Invoking ChatGoogleGenerativeAI for Translation\nDESCRIPTION: Demonstrates how to invoke the model with system and human messages for English to French translation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/google_generativeai.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst aiMsg = await llm.invoke([\n    [\n        \"system\",\n        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n    ],\n    [\"human\", \"I love programming.\"],\n])\naiMsg\n```\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(aiMsg.content)\n```\n\n----------------------------------------\n\nTITLE: Invoking ChatWatsonx for English to French Translation\nDESCRIPTION: Demonstrates how to invoke the ChatWatsonx model with system and user messages to perform English to French translation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/ibm.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nconst aiMsg = await instance.invoke([{\n  role: \"system\",\n  content: \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n},\n{\n  role: \"user\",\n  content: \"I love programming.\"\n}]);\nconsole.log(aiMsg)\n```\n\n----------------------------------------\n\nTITLE: Using MemoryVectorStore Retriever with MMR (TypeScript)\nDESCRIPTION: Creates a retriever configured for Maximal Marginal Relevance (MMR) search by setting `searchType: \"mmr\"` in the `asRetriever` options. MMR aims to improve diversity in results. It specifies fetching 10 initial candidates (`fetchK: 10` within `searchKwargs`) before reranking for diversity and returning the top 2 (`k: 2`). The previously defined `filter` is also applied. The MMR retriever is then invoked with the query \"biology\".\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/memory.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst mmrRetriever = vectorStore.asRetriever({\n  searchType: \"mmr\",\n  searchKwargs: {\n    fetchK: 10,\n  },\n  // Optional filter\n  filter: filter,\n  k: 2,\n});\n\nawait mmrRetriever.invoke(\"biology\");\n```\n\n----------------------------------------\n\nTITLE: Loading and Processing Directory of PDFs\nDESCRIPTION: Uses DirectoryLoader to load all PDFs from a directory, then processes them with RecursiveCharacterTextSplitter to create chunks suitable for context.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/pdf.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport { DirectoryLoader } from \"langchain/document_loaders/fs/directory\";\nimport { PDFLoader } from \"@langchain/community/document_loaders/fs/pdf\";\nimport { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\n\nconst exampleDataPath = \"../../../../../../examples/src/document_loaders/example_data/\";\n\n/* Load all PDFs within the specified directory */\nconst directoryLoader = new DirectoryLoader(\n  exampleDataPath,\n  {\n    \".pdf\": (path: string) => new PDFLoader(path),\n  }\n);\n\nconst directoryDocs = await directoryLoader.load();\n\nconsole.log(directoryDocs[0]);\n\n/* Additional steps : Split text into chunks with any TextSplitter. You can then use it as context or save it to memory afterwards. */\nconst textSplitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 1000,\n  chunkOverlap: 200,\n});\n\nconst splitDocs = await textSplitter.splitDocuments(directoryDocs);\nconsole.log(splitDocs[0]);\n```\n\n----------------------------------------\n\nTITLE: Retrieval Chain with RunnablePassthrough in TypeScript\nDESCRIPTION: This example shows a more complex use case of RunnablePassthrough in a retrieval chain. It combines RunnablePassthrough with other LangChain components to format inputs for a prompt in a question-answering scenario.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/passthrough.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { RunnablePassthrough, RunnableSequence } from \"@langchain/core/runnables\";\nimport { ChatOpenAI, OpenAIEmbeddings } from \"@langchain/openai\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n\nconst vectorstore = await MemoryVectorStore.fromDocuments([\n  { pageContent: \"harrison worked at kensho\", metadata: {} }\n], new OpenAIEmbeddings());\n\nconst retriever = vectorstore.asRetriever();\n\nconst template = `Answer the question based only on the following context:\n{context}\n\nQuestion: {question}\n`;\n\nconst prompt = ChatPromptTemplate.fromTemplate(template);\n\nconst model = new ChatOpenAI({ model: \"gpt-4o\" });\n\nconst retrievalChain = RunnableSequence.from([\n  {\n    context: retriever.pipe((docs) => docs[0].pageContent),\n    question: new RunnablePassthrough()\n  },\n  prompt,\n  model,\n  new StringOutputParser(),\n]);\n\nawait retrievalChain.invoke(\"where did harrison work?\");\n```\n\n----------------------------------------\n\nTITLE: Tool Calling Implementation with OpenAI\nDESCRIPTION: Shows how to implement structured output using tool calling with OpenAI's ChatGPT model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/structured_outputs.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst model = new ChatOpenAI({\n  modelName: \"gpt-4\",\n  temperature: 0,\n});\n\n// Create a tool with ResponseFormatter as its schema.\nconst responseFormatterTool = tool(async () => {}, {\n  name: \"responseFormatter\",\n  schema: ResponseFormatter,\n});\n\n// Bind the created tool to the model\nconst modelWithTools = model.bindTools([responseFormatterTool]);\n\n// Invoke the model\nconst aiMsg = await modelWithTools.invoke(\n  \"What is the powerhouse of the cell?\"\n);\n```\n\n----------------------------------------\n\nTITLE: Routing Using Custom Function in LangChain Expression Language\nDESCRIPTION: Demonstrates how to use a custom function for routing in LCEL. It classifies input questions and routes them to corresponding prompt chains.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/routing.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport CodeBlock from \"@theme/CodeBlock\";\n\nimport IntegrationInstallTooltip from \"@mdx_components/integration_install_tooltip.mdx\";\n\n<IntegrationInstallTooltip></IntegrationInstallTooltip>\n\n```bash npm2yarn\nnpm install @langchain/anthropic @langchain/core\n```\n\nimport FactoryFunctionExample from \"@examples/guides/expression_language/how_to_routing_custom_function.ts\";\n\n<CodeBlock language=\"typescript\">{FactoryFunctionExample}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Implementing LangChain Agent with Connery Toolkit\nDESCRIPTION: This code example demonstrates how to create a LangChain agent that uses Connery Actions for summarizing a public webpage and sending the summary via email. It imports necessary modules, sets up the Connery toolkit, and defines the agent's behavior.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/connery.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/agents/connery_mrkl.ts\";\nimport IntegrationInstallTooltip from \"@mdx_components/integration_install_tooltip.mdx\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Dependencies for Friendli\nDESCRIPTION: Command to install the required LangChain packages (@langchain/community and @langchain/core) for using Friendli integration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/friendli.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Deleting Index in RedisVectorStore\nDESCRIPTION: This snippet demonstrates how to delete all entries in a RedisVectorStore index using a single command. This is crucial for maintaining and resetting the dataset when necessary.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/redis.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nawait vectorStore.delete({ deleteAll: true });\n```\n\n----------------------------------------\n\nTITLE: Advanced Classification Schema with Enums\nDESCRIPTION: Implements a more controlled classification schema using Zod enums to restrict possible values for sentiment and language, with specific scale for aggressiveness.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/classification.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { z } from \"zod\";\n\nconst classificationSchema2 = z.object({\n    sentiment: z.enum([\"happy\", \"neutral\", \"sad\"]).describe(\"The sentiment of the text\"),\n    aggressiveness: z.number().int().describe(\n        \"describes how aggressive the statement is on a scale from 1 to 5. The higher the number the more aggressive\"\n    ),\n    language: z.enum([\"spanish\", \"english\", \"french\", \"german\", \"italian\"]).describe(\"The language the text is written in\"),\n});\n```\n\n----------------------------------------\n\nTITLE: Invoking ChatCloudflareWorkersAI with Messages\nDESCRIPTION: Demonstrates how to invoke the model with a system message and human message for English to French translation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/cloudflare_workersai.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst aiMsg = await llm.invoke([\n  [\n    \"system\",\n    \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n  ],\n  [\"human\", \"I love programming.\"],\n])\naiMsg\n```\n\n----------------------------------------\n\nTITLE: LangChain ToolMessage with Tavily Search Results\nDESCRIPTION: JSON constructor for a LangChain ToolMessage containing search results about the Oppenheimer film. The content includes snippets from IMDb, Rotten Tomatoes, and Wikipedia with information about Christopher Nolan as the director.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_40\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"lc\": 1,\n  \"type\": \"constructor\",\n  \"id\": [\n    \"langchain_core\",\n    \"messages\",\n    \"ToolMessage\"\n  ],\n  \"kwargs\": {\n    \"tool_call_id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n    \"content\": \"[{\\\"title\\\":\\\"Oppenheimer (2023) - IMDb\\\",\\\"url\\\":\\\"https://www.imdb.com/title/tt15398776/\\\",\\\"content\\\":\\\"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.\\\",\\\"score\\\":0.96643,\\\"raw_content\\\":null},{\\\"title\\\":\\\"Christopher Nolan's Oppenheimer - Rotten Tomatoes\\\",\\\"url\\\":\\\"https://editorial.rottentomatoes.com/article/everything-we-know-about-christopher-nolans-oppenheimer/\\\",\\\"content\\\":\\\"Billboards and movie theater pop-ups across Los Angeles have been ticking down for months now: Christopher Nolan's epic account of J. Robert Oppenheimer, the father of the atomic bomb, is nearing an explosive release on July 21, 2023. Nolan movies are always incredibly secretive, twists locked alongside totems behind safe doors, actors not spilling an ounce of Earl Grey tea.\\\",\\\"score\\\":0.92804,\\\"raw_content\\\":null},{\\\"title\\\":\\\"Oppenheimer (film) - Wikipedia\\\",\\\"url\\\":\\\"https://en.wikipedia.org/wiki/Oppenheimer_(film)\\\",\\\"content\\\":\\\"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\\\\nCritical response\\\\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \\\\\\\"more objective view of his story from a different character's point of view\\\\\\\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\\\\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\\\\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\\\\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \\\\\\\"big-atures\\\\\\\", since the special effects team had tried to build the models as physically large as possible. He felt that \\\\\\\"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\\\\\\\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \\\\\\\"emotional\\\\\\\" and resembling that of a thriller, while also remarking that Nolan had \\\\\\\"Trojan-Horsed a biopic into a thriller\\\\\\\".[72]\\\\nCasting\\\\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\\\\\\\", while also underscoring that it is a \\\\\\\"huge shift in perception about the reality of Oppenheimer's perception\\\\\\\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \\\\\\\"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\\\\\\\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.\\\",\\\"score\\\":0.92404,\\\"raw_content\\\":null},{\\\"title\\\":\\\"'Oppenheimer' Director Christopher Nolan On Filmmaking at 53: \\\\\\\"I Try to ...\\\",\\\"url\\\":\\\"https://www.everythingzoomer.com/arts-entertainment/2023/11/21/oppenheimer-director-christopher-nolan-on-filmmaking-at-53-i-try-to-challenge-myself-with-every-film/\\\",\\\"content\\\":\\\"Oppenheimer will be available to own on 4K Ultra HD, Blu-ray and DVD — including more than three hours of bonus features — on November 21.\\\\nRELATED:\\\\nVisiting the Trinity Site Featured in 'Oppenheimer' Is a Sobering Reminder of the Horror of Nuclear Weapons\\\\nBarbenheimer: How 'Barbie' and 'Oppenheimer' Became the Unlikely Movie Marriage of the Summer\\\\nBlast From the Past: 'Asteroid City' & 'Oppenheimer' and the Age of Nuclear Anxiety\\\\nEXPLORE  HealthMoneyTravelFoodStyleBook ClubClassifieds#ZoomerDailyPolicy & PerspectiveArts & Entertainment\",\n    \"additional_kwargs\": {\n      \"name\": \"tavily_search_results_json\"\n    },\n    \"response_metadata\": {}\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Retriever with Default Search Parameters\nDESCRIPTION: Creating a self-query retriever instance with default search parameters for additional metadata filtering.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/vectara.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconst selfQueryRetrieverWithDefaultParams = SelfQueryRetriever.fromLLM({\n  llm,\n  vectorStore,\n  documentContents: \"Brief summary of a movie\",\n  attributeInfo,\n  structuredQueryTranslator: new VectaraTranslator(),\n  searchParams: {\n    filter: {\n      filter: \"( doc.genre = 'science fiction' ) and ( doc.rating > 8.5 )\",\n    },\n    mergeFiltersOperator: \"and\",\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search with Scores (TypeScript)\nDESCRIPTION: Executes a similarity search that also returns the similarity score for each result. It uses the `similaritySearchWithScore` method, taking the query string ('hola') and the number of results (1). The results are returned as an array of tuples, where each tuple contains the `Document` and its corresponding similarity score. The example iterates through the results and prints the score, content, and metadata.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/libsql.mdx#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst similaritySearchWithScoreResults =\n  await vectorStore.similaritySearchWithScore(\"hola\", 1);\n\nfor (const [doc, score] of similaritySearchWithScoreResults) {\n  console.log(\n    `${score.toFixed(3)} ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Stream Agent Events for API Request Execution\nDESCRIPTION: Demonstrates interacting with the created React agent to stream events based on given instructions to POST requests. Stream data and process tool calls or content, assuming previous agent setup.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/openapi.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nconst exampleQuery = \"Make a POST request to openai /chat/completions. The prompt should be 'tell me a joke.'. Ensure you use the model 'gpt-4o-mini'.\"\n\nconst events = await agentExecutor.stream(\n  { messages: [[\"user\", exampleQuery]]},\n  { streamMode: \"values\", }\n)\n\nfor await (const event of events) {\n  const lastMsg = event.messages[event.messages.length - 1];\n  if (lastMsg.tool_calls?.length) {\n    console.dir(lastMsg.tool_calls, { depth: null });\n  } else if (lastMsg.content) {\n    console.log(lastMsg.content);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing VercelPostgres Vector Store with Manual Connection String in TypeScript\nDESCRIPTION: This TypeScript snippet demonstrates how to initialize the `VercelPostgres` vector store by explicitly providing database connection details. It requires an embedding function instance (e.g., `new OpenAIEmbeddings()`) and passes a `connectionString` within the `postgresConnectionOptions` object, overriding the default behavior of using the `POSTGRES_URL` environment variable. Replace placeholder values like `<username>`, `<password>`, `<hostname>`, `<port>`, and `<dbname>` with actual database credentials.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/vercel_postgres.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst vectorstore = await VercelPostgres.initialize(new OpenAIEmbeddings(), {\n  postgresConnectionOptions: {\n    connectionString:\n      \"postgres://<username>:<password>@<hostname>:<port>/<dbname>\",\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Using SelfQueryRetriever for Document Retrieval\nDESCRIPTION: Demonstrates how to use the SelfQueryRetriever to query documents based on metadata.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/hnswlib.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nawait selfQueryRetriever.invoke(\n  \"Which movies are rated higher than 8.5?\"\n);\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Community Package\nDESCRIPTION: Command to install the @langchain/community package which contains the Google Scholar tool implementation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/google_scholar.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community\n```\n\n----------------------------------------\n\nTITLE: Using Plugins with Minimax in LangChain.js\nDESCRIPTION: Illustrates how to use plugins with Minimax models in LangChain.js, supporting the integration of external tools like search engines to provide additional data for the model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/minimax.mdx#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport MinimaxPlugins from \"@examples/models/chat/minimax_plugins.ts\";\n```\n\n----------------------------------------\n\nTITLE: Setting Unstructured API key as environment variable\nDESCRIPTION: Command to set the UNSTRUCTURED_API_KEY environment variable for authenticated access to the Unstructured API service.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/unstructured.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport UNSTRUCTURED_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Instantiating AzureOpenAIEmbeddings in LangChain.js\nDESCRIPTION: Creates an instance of AzureOpenAIEmbeddings with configuration options including API key, instance name, deployment name, API version, and max retries.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/azure_openai.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { AzureOpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddings = new AzureOpenAIEmbeddings({\n  azureOpenAIApiKey: \"<your_key>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY\n  azureOpenAIApiInstanceName: \"<your_instance_name>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME\n  azureOpenAIApiEmbeddingsDeploymentName: \"<your_embeddings_deployment_name>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME\n  azureOpenAIApiVersion: \"<api_version>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION\n  maxRetries: 1,\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Vectara Store with Documents\nDESCRIPTION: Setting up a Vectara vector store with sample documents containing metadata and defining attribute information for querying.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/vectara.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { VectaraStore } from \"@langchain/community/vectorstores/vectara\";\nimport { Document } from \"@langchain/core/documents\";\nimport type { AttributeInfo } from \"langchain/chains/query_constructor\";\n\n// Vectara provides embeddings\nimport { FakeEmbeddings } from \"@langchain/core/utils/testing\";\n\nconst docs = [\n  new Document({\n    pageContent:\n      \"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n    metadata: { year: 1993, rating: 7.7, genre: \"science fiction\" },\n  }),\n  // ... more documents ...\n];\n\nconst attributeInfo: AttributeInfo[] = [\n  {\n    name: \"genre\",\n    description: \"The genre of the movie\",\n    type: \"string or array of strings\",\n  },\n  // ... more attributes ...\n];\n\nconst embeddings = new FakeEmbeddings();\nconst vectorStore = await VectaraStore.fromDocuments(docs, embeddings, {\n  customerId: Number(process.env.VECTARA_CUSTOMER_ID),\n  corpusId: Number(process.env.VECTARA_CORPUS_ID),\n  apiKey: String(process.env.VECTARA_API_KEY),\n});\n```\n\n----------------------------------------\n\nTITLE: Basic Cohere Rerank Implementation\nDESCRIPTION: Demonstrates the basic usage of Cohere's rerank method to reorder documents based on their relevance to a query, returning indexes and relevancy scores.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_compressors/cohere_rerank.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Document } from \"@langchain/core/documents\";\nimport { CohereRerank } from \"@langchain/cohere\";\n\n// Initialize documents to be reranked\nconst docs = [\n  new Document({\n    pageContent: \"The color of the sky is blue.\",\n  }),\n  new Document({\n    pageContent: \"I live in San Francisco.\",\n  }),\n  new Document({\n    pageContent: \"The capital of the United States is Washington, D.C.\",\n  }),\n  new Document({\n    pageContent: \"The capital of France is Paris.\",\n  }),\n];\n\n// Initialize the reranker with your API key\nconst reranker = new CohereRerank({ apiKey: \"YOUR_API_KEY\" });\n\n// Rerank documents based on a query\nconst results = await reranker.rerank({\n  query: \"What is the capital of the US?\",\n  documents: docs.map((doc) => doc.pageContent),\n  topN: 3,\n});\n\nconsole.log(results);\n/*\n[ { index: 2, relevanceScore: 0.9870191812515259 },\n  { index: 3, relevanceScore: 0.09175287932157516 },\n  { index: 1, relevanceScore: 0.05698449909687042 } ]\n*/\n\n```\n\n----------------------------------------\n\nTITLE: Adding Documents to Upstash Vector Store - TypeScript/JavaScript\nDESCRIPTION: This snippet illustrates how to define documents with page content and metadata and then add them to an UpstashVectorStore instance. The addDocuments method can accept a list of documents along with a corresponding list of unique IDs. This method is asynchronous and should be awaited; note there may be a short indexing delay before documents are available for query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/upstash.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport type { Document } from \"@langchain/core/documents\";\n\nconst document1: Document = {\n  pageContent: \"The powerhouse of the cell is the mitochondria\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document2: Document = {\n  pageContent: \"Buildings are made out of brick\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document3: Document = {\n  pageContent: \"Mitochondria are made out of lipids\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document4: Document = {\n  pageContent: \"The 2024 Olympics are in Paris\",\n  metadata: { source: \"https://example.com\" }\n}\n\nconst documents = [document1, document2, document3, document4];\n\nawait vectorStore.addDocuments(documents, { ids: [\"1\", \"2\", \"3\", \"4\"] });\n```\n\n----------------------------------------\n\nTITLE: Basic Message Trimming with Last Strategy\nDESCRIPTION: Demonstrates how to trim messages to get the last maxTokens using ChatOpenAI as the token counter. Shows basic message setup and trimming with different message types.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/trim_messages.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AIMessage, HumanMessage, SystemMessage, trimMessages } from \"@langchain/core/messages\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst messages = [\n    new SystemMessage(\"you're a good assistant, you always respond with a joke.\"),\n    new HumanMessage(\"i wonder why it's called langchain\"),\n    new AIMessage(\n        'Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'\n    ),\n    new HumanMessage(\"and who is harrison chasing anyways\"),\n    new AIMessage(\n        \"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\"\n    ),\n    new HumanMessage(\"what do you call a speechless parrot\"),\n];\n\nconst trimmed = await trimMessages(\n    messages,\n    {\n        maxTokens: 45,\n        strategy: \"last\",\n        tokenCounter: new ChatOpenAI({ modelName: \"gpt-4\" }),\n    }\n);\n\nconsole.log(trimmed.map((x) => JSON.stringify({\n    role: x._getType(),\n    content: x.content,\n}, null, 2)).join(\"\\n\\n\"));\n```\n\n----------------------------------------\n\nTITLE: Initializing AmazonKnowledgeBaseRetriever\nDESCRIPTION: Creation and configuration of an AmazonKnowledgeBaseRetriever instance with AWS credentials and settings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/bedrock-knowledge-bases.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { AmazonKnowledgeBaseRetriever } from \"@langchain/aws\";\n\nconst retriever = new AmazonKnowledgeBaseRetriever({\n  topK: 10,\n  knowledgeBaseId: process.env.AWS_KNOWLEDGE_BASE_ID,\n  region: \"us-east-2\",\n  clientOptions: {\n    credentials: {\n      accessKeyId: process.env.AWS_ACCESS_KEY_ID,\n      secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY,\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing an OpenAI Agent with LangChain\nDESCRIPTION: This snippet demonstrates how to create an OpenAI agent using LangChain's expression language, integrating the custom tool and setting up the agent executor.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/graph_semantic.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { AgentExecutor } from \"langchain/agents\";\nimport { formatToOpenAIFunctionMessages } from \"langchain/agents/format_scratchpad\";\nimport { OpenAIFunctionsAgentOutputParser } from \"langchain/agents/openai/output_parser\";\nimport { convertToOpenAIFunction } from \"@langchain/core/utils/function_calling\";\nimport { ChatPromptTemplate, MessagesPlaceholder } from \"@langchain/core/prompts\";\nimport { AIMessage, BaseMessage, HumanMessage } from \"@langchain/core/messages\";\nimport { RunnableSequence } from \"@langchain/core/runnables\";\n\nconst llm = new ChatOpenAI({ model: \"gpt-3.5-turbo\", temperature: 0 })\nconst tools = [informationTool]\n\nconst llmWithTools = llm.bind({\n    functions: tools.map(convertToOpenAIFunction),\n})\n\nconst prompt = ChatPromptTemplate.fromMessages(\n    [\n        [\n            \"system\",\n            \"You are a helpful assistant that finds information about movies and recommends them. If tools require follow up questions, make sure to ask the user for clarification. Make sure to include any available options that need to be clarified in the follow up questions Do only the things the user specifically requested.\"\n        ],\n        new MessagesPlaceholder(\"chat_history\"),\n        [\"human\", \"{input}\"],\n        new MessagesPlaceholder(\"agent_scratchpad\"),\n    ]\n)\n\nconst _formatChatHistory = (chatHistory) => {\n    const buffer: Array<BaseMessage> = []\n    for (const [human, ai] of chatHistory) {\n        buffer.push(new HumanMessage({ content: human }))\n        buffer.push(new AIMessage({ content: ai }))\n    }\n    return buffer\n}\n\nconst agent = RunnableSequence.from([\n    {\n        input: (x) => x.input,\n        chat_history: (x) => {\n            if (\"chat_history\" in x) {\n                return _formatChatHistory(x.chat_history);\n            }\n            return [];\n        },\n        agent_scratchpad: (x) => {\n            if (\"steps\" in x) {\n                return formatToOpenAIFunctionMessages(\n                    x.steps\n                );\n            }\n            return [];\n        },\n    },\n    prompt,\n    llmWithTools,\n    new OpenAIFunctionsAgentOutputParser(),\n])\n\nconst agentExecutor = new AgentExecutor({ agent, tools });\n```\n\n----------------------------------------\n\nTITLE: Implementing ChatBaiduWenxin in LangChain.js\nDESCRIPTION: Example code demonstrating how to create and use a Baidu Wenxin chat model. Shows how to initialize the model with API credentials and use it for generating responses.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/baidu_wenxin.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatBaiduWenxin } from \"@langchain/community/chat_models/baiduwenxin\";\nimport { HumanMessage } from \"@langchain/core/messages\";\n\n/**\n * You can find your Baidu Wenxin API key and secret key in the\n * Baidu AI Cloud console: https://console.bce.baidu.com/qianfan/overview\n */\nexport const run = async () => {\n  const model = new ChatBaiduWenxin({\n    modelName: \"ERNIE-Bot-4\", // Default value\n    baiduApiKey: \"your-baidu-api-key\", // In Node.js defaults to process.env.BAIDU_API_KEY\n    baiduSecretKey: \"your-baidu-secret-key\", // In Node.js defaults to process.env.BAIDU_SECRET_KEY\n  });\n\n  const message = new HumanMessage(\"人工智能\");\n  const res = await model.invoke([message]);\n  console.log({ res });\n};\n```\n\n----------------------------------------\n\nTITLE: Eliminating Extra Spaces in PDF Parsing with LangChain.js\nDESCRIPTION: This example shows how to eliminate extra spaces when parsing PDFs by setting the parsedItemSeparator to an empty string.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/document_loader_pdf.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PDFLoader } from \"@langchain/community/document_loaders/fs/pdf\";\n\nconst loader = new PDFLoader(\"src/document_loaders/example_data/example.pdf\", {\n  parsedItemSeparator: \"\",\n});\n\nconst docs = await loader.load();\n```\n\n----------------------------------------\n\nTITLE: Invoking ChatTogetherAI Model with Messages\nDESCRIPTION: Demonstrates how to invoke the ChatTogetherAI model with system and human messages to perform English to French translation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/togetherai.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst aiMsg = await llm.invoke([\n    [\n        \"system\",\n        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n    ],\n    [\"human\", \"I love programming.\"],\n])\naiMsg\n```\n\n----------------------------------------\n\nTITLE: Handling Raw Outputs for Structured Data in TypeScript\nDESCRIPTION: Demonstrates how to access the raw model output alongside parsed structured data by setting includeRaw: true. This helps when dealing with LLMs that might struggle with complex schema generation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/structured_output.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst joke = z.object({\n  setup: z.string().describe(\"The setup of the joke\"),\n  punchline: z.string().describe(\"The punchline to the joke\"),\n  rating: z.number().optional().describe(\"How funny the joke is, from 1 to 10\"),\n});\n\nconst structuredLlm = model.withStructuredOutput(joke, { includeRaw: true, name: \"joke\" });\n\nawait structuredLlm.invoke(\"Tell me a joke about cats\");\n```\n\n----------------------------------------\n\nTITLE: Streaming with Azure Chat OpenAI\nDESCRIPTION: Example demonstrating streaming functionality with Azure Chat OpenAI model\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-azure-openai/README.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AzureChatOpenAI } from \"@langchain/azure-openai\";\n\nconst model = new AzureChatOpenAI({\n  // Note that the following are optional, and will default to the values below\n  // if not provided.\n  azureOpenAIEndpoint: process.env.AZURE_OPENAI_API_ENDPOINT,\n  azureOpenAIApiKey: process.env.AZURE_OPENAI_API_KEY,\n  azureOpenAIApiDeploymentName: process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME,\n});\nconst response = await model.stream(new HumanMessage(\"Hello world!\"));\n```\n\n----------------------------------------\n\nTITLE: Using MessagesPlaceholder for Dynamic Message Arrays\nDESCRIPTION: Demonstrates how to use MessagesPlaceholder to dynamically insert an array of messages into a chat prompt template. This allows for flexible message insertion at specific points in the prompt.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#2025-04-22_snippet_19\n\nLANGUAGE: typescript\nCODE:\n```\nimport {\n  ChatPromptTemplate,\n  MessagesPlaceholder,\n} from \"@langchain/core/prompts\";\nimport { HumanMessage } from \"@langchain/core/messages\";\n\nconst promptTemplate = ChatPromptTemplate.fromMessages([\n  [\"system\", \"You are a helpful assistant\"],\n  new MessagesPlaceholder(\"msgs\"),\n]);\n\npromptTemplate.invoke({ msgs: [new HumanMessage({ content: \"hi!\" })] });\n```\n\n----------------------------------------\n\nTITLE: Defining Search Schema with Zod in JavaScript\nDESCRIPTION: This code defines a schema for search queries using Zod, including a string query and a section enum, and creates a structured LLM output based on this schema.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport { z } from \"zod\";\n\n\nconst searchSchema = z.object({\n  query: z.string().describe(\"Search query to run.\"),\n  section: z.enum([\"beginning\", \"middle\", \"end\"]).describe(\"Section to query.\"),\n});\n\nconst structuredLlm = llm.withStructuredOutput(searchSchema)\n```\n\n----------------------------------------\n\nTITLE: Custom URL Configuration\nDESCRIPTION: Configuring OpenAIEmbeddings with a custom base URL for API requests.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/openai.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst model = new OpenAIEmbeddings({\n  configuration: {\n    baseURL: \"https://your_custom_url.com\",\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Vector Store for RAG in JavaScript\nDESCRIPTION: This snippet demonstrates loading a web page, splitting it into chunks, and indexing those chunks in a vector store for later retrieval. It uses Cheerio for web scraping and RecursiveCharacterTextSplitter for text chunking.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/qa_chat_history.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport \"cheerio\";\nimport { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\nimport { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\";\n\n// Load and chunk contents of the blog\nconst pTagSelector = \"p\";\nconst cheerioLoader = new CheerioWebBaseLoader(\n  \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n  {\n    selector: pTagSelector\n  }\n);\n\nconst docs = await cheerioLoader.load();\n\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 1000, chunkOverlap: 200\n});\nconst allSplits = await splitter.splitDocuments(docs);\n```\n\n----------------------------------------\n\nTITLE: Handler Function for Dynamic Tool Invocation in TypeScript\nDESCRIPTION: Creates a function that handles runtime requests by generating tools specific to a user and binding them to an LLM. This approach ensures that user-specific data is securely incorporated into tool execution.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_runtime.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BaseChatModel } from \"@langchain/core/language_models/chat_models\";\n\nasync function handleRunTimeRequest(userId: string, query: string, llm: BaseChatModel): Promise<any> {\n  if (!llm.bindTools) {\n    throw new Error(\"Language model does not support tools.\");\n  }\n  const tools = generateToolsForUser(userId);\n  const llmWithTools = llm.bindTools(tools);\n  return llmWithTools.invoke(query);\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Query Analysis Chain in JavaScript\nDESCRIPTION: This code sets up a query analysis chain using ChatPromptTemplate, RunnableSequence, and function calling with OpenAI.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_no_queries.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { zodToJsonSchema } from \"zod-to-json-schema\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { RunnableSequence, RunnablePassthrough } from \"@langchain/core/runnables\";\n\nconst system = `You have the ability to issue search queries to get information to help answer user information.\n\nYou do not NEED to look things up. If you don't need to, then just respond normally.`;\nconst prompt = ChatPromptTemplate.fromMessages(\n  [\n    [\"system\", system],\n    [\"human\", \"{question}\"],\n  ]\n)\nconst llmWithTools = llm.bind({\n  tools: [{\n    type: \"function\" as const,\n    function: {\n      name: \"search\",\n      description: \"Search over a database of job records.\",\n      parameters: zodToJsonSchema(searchSchema),\n    }\n  }]\n})\nconst queryAnalyzer = RunnableSequence.from([\n  {\n    question: new RunnablePassthrough(),\n  },\n  prompt,\n  llmWithTools\n])\n```\n\n----------------------------------------\n\nTITLE: Creating and Preparing a LangSmith Dataset for Indexing\nDESCRIPTION: Reads example data from a JSON file, converts it to the format expected by LangSmith API, defines the input schema required for indexing, and creates a new dataset. This prepares the dataset for similarity search capabilities.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/example_selectors_langsmith.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Client as LangSmithClient } from 'langsmith';\nimport { z } from 'zod';\nimport { zodToJsonSchema } from 'zod-to-json-schema';\nimport fs from \"fs/promises\";\n\n// Read the example dataset and convert to the format expected by the LangSmith API\n// for creating new examples\nconst examplesJson = JSON.parse(\n  await fs.readFile(\"../../data/ls_few_shot_example_dataset.json\", \"utf-8\")\n);\n\nlet inputs: Record<string, any>[] = [];\nlet outputs: Record<string, any>[] = [];\nlet metadata: Record<string, any>[] = [];\n\nexamplesJson.forEach((ex) => {\n  inputs.push(ex.inputs);\n  outputs.push(ex.outputs);\n  metadata.push(ex.metadata);\n});\n\n// Define our input schema as this is required for indexing\nconst inputsSchema = zodToJsonSchema(z.object({\n  input: z.string(),\n  system: z.boolean().optional(),\n}));\n\nconst lsClient = new LangSmithClient();\n\nawait lsClient.deleteDataset({ datasetName: \"multiverse-math-examples-for-few-shot-example\" })\n\nconst dataset = await lsClient.createDataset(\"multiverse-math-examples-for-few-shot-example\", {\n  inputsSchema,\n});\n\nconst createdExamples = await lsClient.createExamples({\n  inputs,\n  outputs,\n  metadata,\n  datasetId: dataset.id,\n})\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Core and Community Packages\nDESCRIPTION: Command to install the required LangChain packages for working with SageMaker endpoints in the LangChain ecosystem.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/aws_sagemaker.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies using npm\nDESCRIPTION: This command installs the necessary dependencies (@langchain/openai and @langchain/core) for working with OpenAI embeddings and LangChain core functionality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/vectorstores.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Creating an Agent Executor\nDESCRIPTION: Code to create an AgentExecutor that combines the agent with the tools.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/agent_executor.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nimport { AgentExecutor } from \"langchain/agents\";\n\nconst agentExecutor = new AgentExecutor({\n  agent,\n  tools\n})\n```\n\n----------------------------------------\n\nTITLE: Using Function Partials with Text Few Shot Templates in LangChainJS\nDESCRIPTION: Demonstrates how to use function partials with text-based few shot templates to dynamically insert values into the prompt.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot.mdx#2025-04-22_snippet_15\n\nLANGUAGE: typescript\nCODE:\n```\nimport {\n  ChatPromptTemplate,\n  FewShotChatMessagePromptTemplate,\n} from \"langchain/prompts\";\n```\n\n----------------------------------------\n\nTITLE: Using ChatOllama with Structured Output in Python\nDESCRIPTION: Python code showing how to use ChatOllama with structured output functionality, defining a schema and binding it to the model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/ollama.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatOllama } from \"@langchain/ollama\";\nimport { z } from \"zod\";\n\n// Define the model\nconst llmForWSO = new ChatOllama({\n  model: \"llama3-groq-tool-use\",\n});\n\n// Define the tool schema you'd like the model to use.\nconst schemaForWSO = z.object({\n  location: z.string().describe(\"The city and state, e.g. San Francisco, CA\"),\n});\n\n// Pass the schema to the withStructuredOutput method to bind it to the model.\nconst llmWithStructuredOutput = llmForWSO.withStructuredOutput(schemaForWSO, {\n  name: \"get_current_weather\",\n});\n\nconst resultFromWSO = await llmWithStructuredOutput.invoke(\n  \"What's the weather like today in San Francisco? Ensure you use the 'get_current_weather' tool.\"\n);\nconsole.log(resultFromWSO);\n```\n\n----------------------------------------\n\nTITLE: Defining MongoDB Index with Filterable Metadata (JSON)\nDESCRIPTION: Updates the MongoDB Atlas Vector Search index definition to include a filterable metadata field. It adds a second object to the `fields` array with `type: \"filter\"` and `path: \"source\"`, allowing pre-filtering queries based on the 'source' metadata field alongside vector search.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/mongodb_atlas.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"fields\": [\n    {\n      \"numDimensions\": 1024,\n      \"path\": \"embedding\",\n      \"similarity\": \"euclidean\",\n      \"type\": \"vector\"\n    },\n    {\n      \"path\": \"source\",\n      \"type\": \"filter\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Using Query Builder Filtering for Similarity Search (TypeScript)\nDESCRIPTION: This snippet demonstrates using a function-based filter (`SupabaseFilterRPCCall`) for more complex metadata filtering during a similarity search, mimicking the Supabase JavaScript library's filter syntax. It filters based on the `source` field within the `metadata` JSON column.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/supabase.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nimport { SupabaseFilterRPCCall } from \"@langchain/community/vectorstores/supabase\";\n\nconst funcFilter: SupabaseFilterRPCCall = (rpc) =>\n  rpc.filter(\"metadata->>source\", \"eq\", \"https://example.com\");\n\nconst funcFilterSearchResults = await vectorStore.similaritySearch(\"biology\", 2, funcFilter);\n\nfor (const doc of funcFilterSearchResults) {\n  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);\n}\n```\n\n----------------------------------------\n\nTITLE: Building a LangGraph chatbot with chat history persistence\nDESCRIPTION: Implements a basic LangGraph-based chatbot that maintains conversation history across interactions. The code creates a chat model node, manages chat history using the session ID from the config, and demonstrates conversation memory with follow-up questions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/versions/migrating_memory/chat_history.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { v4 as uuidv4 } from \"uuid\";\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport { StateGraph, MessagesAnnotation, END, START } from \"@langchain/langgraph\";\nimport { HumanMessage } from \"@langchain/core/messages\";\nimport { RunnableConfig } from \"@langchain/core/runnables\";\n\n// Define a chat model\nconst model = new ChatAnthropic({ modelName: \"claude-3-haiku-20240307\" });\n\n// Define the function that calls the model\nconst callModel = async (\n  state: typeof MessagesAnnotation.State,\n  config: RunnableConfig\n): Promise<Partial<typeof MessagesAnnotation.State>> => {\n  if (!config.configurable?.sessionId) {\n    throw new Error(\n      \"Make sure that the config includes the following information: {'configurable': {'sessionId': 'some_value'}}\"\n    );\n  }\n\n  const chatHistory = getChatHistory(config.configurable.sessionId as string);\n\n  let messages = [...(await chatHistory.getMessages()), ...state.messages];\n\n  if (state.messages.length === 1) {\n    // First message, ensure it's in the chat history\n    await chatHistory.addMessage(state.messages[0]);\n  }\n\n  const aiMessage = await model.invoke(messages);\n\n  // Update the chat history\n  await chatHistory.addMessage(aiMessage);\n\n  return { messages: [aiMessage] };\n};\n\n// Define a new graph\nconst workflow = new StateGraph(MessagesAnnotation)\n  .addNode(\"model\", callModel)\n  .addEdge(START, \"model\")\n  .addEdge(\"model\", END);\n\nconst app = workflow.compile();\n\n// Create a unique session ID to identify the conversation\nconst sessionId = uuidv4();\nconst config = { configurable: { sessionId }, streamMode: \"values\" as const };\n\nconst inputMessage = new HumanMessage(\"hi! I'm bob\");\n\nfor await (const event of await app.stream({ messages: [inputMessage] }, config)) {\n  const lastMessage = event.messages[event.messages.length - 1];\n  console.log(lastMessage.content);\n}\n\n// Here, let's confirm that the AI remembers our name!\nconst followUpMessage = new HumanMessage(\"what was my name?\");\n\nfor await (const event of await app.stream({ messages: [followUpMessage] }, config)) {\n  const lastMessage = event.messages[event.messages.length - 1];\n  console.log(lastMessage.content);\n}\n```\n\n----------------------------------------\n\nTITLE: Indexing and Retrieving with MemoryVectorStore in Python\nDESCRIPTION: Demonstrates how to create a vector store, index a document, and retrieve similar documents using FireworksEmbeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/fireworks.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n// Create a vector store with a sample text\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n\nconst text = \"LangChain is the framework for building context-aware reasoning applications\";\n\nconst vectorstore = await MemoryVectorStore.fromDocuments(\n  [{ pageContent: text, metadata: {} }],\n  embeddings,\n);\n\n// Use the vector store as a retriever that returns a single document\nconst retriever = vectorstore.asRetriever(1);\n\n// Retrieve the most similar text\nconst retrievedDocuments = await retriever.invoke(\"What is LangChain?\");\n\nretrievedDocuments[0].pageContent;\n```\n\n----------------------------------------\n\nTITLE: Implementing Answer Generation Function\nDESCRIPTION: This function generates an answer to the user's question based on the SQL query and its result.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/sql_qa.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nconst generateAnswer = async (state: typeof StateAnnotation.State) => {\n  const promptValue = \n    \"Given the following user question, corresponding SQL query, \" +\n    \"and SQL result, answer the user question.\\n\\n\" +\n    `Question: ${state.question}\\n` +\n    `SQL Query: ${state.query}\\n` +\n    `SQL Result: ${state.result}\\n`;\n  const response = await llm.invoke(promptValue)\n  return { answer: response.content }\n};\n```\n\n----------------------------------------\n\nTITLE: Splitting Documents into Manageable Chunks for Processing\nDESCRIPTION: Implements the brute force approach by splitting documents into smaller chunks using TokenTextSplitter. This ensures each chunk fits within the LLM's context window, with a small overlap between chunks to maintain continuity.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/extraction_long_text.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { TokenTextSplitter } from \"langchain/text_splitter\";\n\nconst textSplitter = new TokenTextSplitter({\n  chunkSize: 2000,\n  chunkOverlap: 20,\n});\n\n// Note that this method takes an array of docs\nconst splitDocs = await textSplitter.splitDocuments(docs);\n```\n\n----------------------------------------\n\nTITLE: Initializing Weaviate Vector Store with Documents\nDESCRIPTION: Setup of Weaviate vector store with sample documents containing metadata, including configuration of attribute information and embeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/weaviate.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { WeaviateStore } from \"@langchain/weaviate\";\nimport { Document } from \"@langchain/core/documents\";\nimport type { AttributeInfo } from \"langchain/chains/query_constructor\";\n\nimport weaviate from \"weaviate-ts-client\";\n\nconst docs = [\n  new Document({\n    pageContent:\n      \"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n    metadata: { year: 1993, rating: 7.7, genre: \"science fiction\" },\n  }),\n  new Document({\n    pageContent:\n      \"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n    metadata: { year: 2010, director: \"Christopher Nolan\", rating: 8.2 },\n  }),\n  new Document({\n    pageContent:\n      \"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n    metadata: { year: 2006, director: \"Satoshi Kon\", rating: 8.6 },\n  }),\n  new Document({\n    pageContent:\n      \"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n    metadata: { year: 2019, director: \"Greta Gerwig\", rating: 8.3 },\n  }),\n  new Document({\n    pageContent: \"Toys come alive and have a blast doing so\",\n    metadata: { year: 1995, genre: \"animated\" },\n  }),\n  new Document({\n    pageContent: \"Three men walk into the Zone, three men walk out of the Zone\",\n    metadata: {\n      year: 1979,\n      director: \"Andrei Tarkovsky\",\n      genre: \"science fiction\",\n      rating: 9.9,\n    },\n  }),\n];\n\nconst attributeInfo: AttributeInfo[] = [\n  {\n    name: \"genre\",\n    description: \"The genre of the movie\",\n    type: \"string or array of strings\",\n  },\n  {\n    name: \"year\",\n    description: \"The year the movie was released\",\n    type: \"number\",\n  },\n  {\n    name: \"director\",\n    description: \"The director of the movie\",\n    type: \"string\",\n  },\n  {\n    name: \"rating\",\n    description: \"The rating of the movie (1-10)\",\n    type: \"number\",\n  },\n  {\n    name: \"length\",\n    description: \"The length of the movie in minutes\",\n    type: \"number\",\n  },\n];\n\nconst client = (weaviate as any).client({\n  scheme: process.env.WEAVIATE_SCHEME || \"https\",\n  host: process.env.WEAVIATE_HOST || \"localhost\",\n  apiKey: process.env.WEAVIATE_API_KEY\n    ? new (weaviate as any).ApiKey(process.env.WEAVIATE_API_KEY)\n    : undefined,\n});\n\nconst embeddings = new OpenAIEmbeddings();\nconst vectorStore = await WeaviateStore.fromDocuments(docs, embeddings, {\n  client,\n  indexName: \"Test\",\n  textKey: \"text\",\n  metadataKeys: [\"year\", \"director\", \"rating\", \"genre\"],\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Aurora DSQL Chat Memory\nDESCRIPTION: TypeScript implementation showing how to configure and use Aurora DSQL for chat memory persistence. Includes AWS authentication token generation and PostgreSQL pool configuration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/aurora_dsql.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Client } from \"pg\";\nimport { DsqlSigner } from \"@aws-sdk/dsql-signer\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { AuroraDsqlChatMessageHistory } from \"@langchain/community/stores/message/aurora_dsql\";\nimport { ConversationChain } from \"langchain/chains\";\n\n// First, create a Aurora DSQL client with AWS credentials\nconst signer = new DsqlSigner({\n  credentials: {\n    accessKeyId: \"YOUR_AWS_ACCESS_KEY_ID\",\n    secretAccessKey: \"YOUR_AWS_SECRET_ACCESS_KEY\",\n  },\n  region: \"us-east-1\",\n  hostname: \"YOUR_AURORA_DSQL_ENDPOINT\",\n  port: 5432,\n  username: \"YOUR_DB_USERNAME\",\n});\n\n// Generate an auth token for connecting to your cluster\nconst getAuthToken = async () => {\n  const authToken = await signer.getAuthToken();\n  return authToken;\n};\n\n// Create Postgres Pool using auth token\nconst createPool = async () => {\n  const authToken = await getAuthToken();\n  return {\n    user: \"YOUR_DB_USERNAME\",\n    host: \"YOUR_AURORA_DSQL_ENDPOINT\",\n    database: \"YOUR_DB_NAME\",\n    password: authToken,\n    port: 5432,\n    ssl: true,\n  };\n};\n\nexport const run = async () => {\n  const sessionId = \"session-1\";\n  // Initialize Aurora DSQL Chat Message History with a session ID and a pool\n  const chatHistory = new AuroraDsqlChatMessageHistory({\n    sessionId,\n    poolConfig: await createPool(),\n  });\n\n  await chatHistory.clear(); // Clear message history\n\n  const chat = new ChatOpenAI();\n  const chain = new ConversationChain({ llm: chat, memory: chatHistory });\n\n  const response1 = await chain.call({ input: \"Hi! I'm Jim.\" });\n  console.log(response1);\n  /*\n   {\"response\": \"Hello Jim! It's nice to meet you. How can I assist you today?\"}\n  */\n\n  const response2 = await chain.call({\n    input: \"What did I just say my name was?\",\n  });\n  console.log(response2);\n  /*\n   {\"response\": \"You said your name was Jim.\"}\n  */\n};\n```\n\n----------------------------------------\n\nTITLE: Instantiating MemoryVectorStore with OpenAI Embeddings (TypeScript)\nDESCRIPTION: Imports necessary classes (`MemoryVectorStore`, `OpenAIEmbeddings`) and creates an instance of `MemoryVectorStore`. It first initializes `OpenAIEmbeddings`, specifying the `text-embedding-3-small` model, then passes the embeddings instance to the `MemoryVectorStore` constructor. This sets up the vector store ready to accept documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/memory.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddings = new OpenAIEmbeddings({\n  model: \"text-embedding-3-small\",\n});\n\nconst vectorStore = new MemoryVectorStore(embeddings);\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of Minimax with LangChain.js\nDESCRIPTION: Demonstrates the basic setup and usage of Minimax models with LangChain.js. This includes importing necessary modules, setting up the chat model, and making a simple query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/minimax.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport Minimax from \"@examples/models/chat/integration_minimax.ts\";\n```\n\n----------------------------------------\n\nTITLE: Chaining Chat Model with Prompt Template\nDESCRIPTION: Demonstrates how to chain the chat model with a prompt template for language translation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/chat.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\"\n\nconst prompt = ChatPromptTemplate.fromMessages(\n  [\n    [\n      \"system\",\n      \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n    ],\n    [\"human\", \"{input}\"],\n  ]\n)\n\nconst chain = prompt.pipe(llm);\nawait chain.invoke(\n  {\n    input_language: \"English\",\n    output_language: \"German\",\n    input: \"I love programming.\",\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: Advanced Streaming and Output Compilation in LangChain.js\nDESCRIPTION: Enhanced implementation of streaming Q&A chain output, including logic to compile and display the stream as it's being returned, with differentiation between different parts of the response.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_streaming.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst output = {};\nlet currentKey: string | null = null;\n\nfor await (const chunk of await ragChainWithSource.stream(\"What is task decomposition?\")) {\n  for (const key of Object.keys(chunk)) {\n    if (output[key] === undefined) {\n      output[key] = chunk[key];\n    } else {\n      output[key] += chunk[key];\n    }\n\n    if (key !== currentKey) {\n      console.log(`\\n\\n${key}: ${JSON.stringify(chunk[key])}`);\n    } else {\n      console.log(chunk[key]);\n    }\n    currentKey = key;\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatOpenAI Model with Caching in TypeScript\nDESCRIPTION: This snippet demonstrates how to create a ChatOpenAI model instance with caching enabled. It uses the GPT-4 model to make the caching effects more noticeable.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chat_model_caching.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\n// To make the caching really obvious, lets use a slower model.\nconst model = new ChatOpenAI({\n  model: \"gpt-4\",\n  cache: true,\n});\n```\n\n----------------------------------------\n\nTITLE: Invoking the RAG Extractor with a Query\nDESCRIPTION: Demonstrates how to run the RAG extraction pipeline with a specific query to retrieve and extract information about key developments in car history. The retriever finds the most relevant document chunk for the query before extraction.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/extraction_long_text.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nconst ragExtractorResults = await ragExtractor.invoke(\"Key developments associated with cars\");\n```\n\n----------------------------------------\n\nTITLE: Selecting Examples Based on an Input\nDESCRIPTION: Demonstrates calling the selectExamples method to find the closest matching example based on word length.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/example_selectors.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nawait exampleSelector.selectExamples({ input: \"okay\" })\n```\n\n----------------------------------------\n\nTITLE: Using SerpAPILoader with LangChain for Web Search Results\nDESCRIPTION: Example demonstrating how to use SerpAPILoader to fetch search results, store them in MemoryVectorStore, and use a retrieval chain to answer questions based on the loaded documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/serpapi.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { SerpAPILoader } from \"@langchain/community/document_loaders/web/serpapi\";\nimport { MemoryVectorStore } from \"@langchain/community/vectorstores/memory\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { createStuffDocumentsChain } from \"langchain/chains/combine_documents\";\nimport { createRetrievalChain } from \"langchain/chains/retrieval\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { PromptTemplate } from \"@langchain/core/prompts\";\n\n// First load the documents\nconst loader = new SerpAPILoader({ q: \"SERP API\", apiKey: \"your-api-key\" });\nconst docs = await loader.load();\n\n// Then we'll create a vector store using OpenAI embeddings\nconst vectorStore = await MemoryVectorStore.fromDocuments(\n  docs,\n  new OpenAIEmbeddings()\n);\n\n// Create a prompt template for answering questions based on sources\nconst prompt = PromptTemplate.fromTemplate(\n  `Answer the following question based only on the provided context:\n\nContext: {context}\n\nQuestion: {question}\n\nAnswer:`\n);\n\n// Create LLM\nconst llm = new ChatOpenAI();\n\n// Create a chain for answering questions using the LLM and prompt\nconst chain = await createStuffDocumentsChain({\n  llm,\n  prompt,\n});\n\n// Create a retrieval chain that uses the vector store and QA chain\nconst retrievalChain = await createRetrievalChain({\n  retriever: vectorStore.asRetriever(),\n  combineDocsChain: chain,\n});\n\n// Call the retrieval chain\nconst result = await retrievalChain.invoke({\n  question: \"What is SERP API?\",\n});\n\nconsole.log(result);\n```\n\n----------------------------------------\n\nTITLE: Loading Documents with WebPDFLoader\nDESCRIPTION: Demonstrates how to load documents from a PDF file using the WebPDFLoader. This returns an array of document objects, each representing a page from the PDF by default.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/pdf.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst docs = await loader.load()\ndocs[0]\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for PlanetScale Chat Memory\nDESCRIPTION: Command to install necessary dependencies for using PlanetScale with LangChain.js. This includes the PlanetScale database client, OpenAI integration, and core LangChain packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/planetscale.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @planetscale/database @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Dependencies\nDESCRIPTION: Command to install the required LangChain packages\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/llm_chain.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n```\n\n----------------------------------------\n\nTITLE: WebLLM Chat Model Integration Example\nDESCRIPTION: Example code for integrating WebLLM with LangChain as a chat model. It imports the necessary components, initializes the model, and demonstrates how to use it for chat completion.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/web_llm.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatWebLLM } from \"@langchain/community/chat_models/webllm\";\n\nconst model = new ChatWebLLM({\n  model: \"Llama-2-7b-chat\",\n  // You can also use your own model via:\n  // modelUrl: \"https://example.com/my-model-path\",\n});\n\nconst message = await model.invoke(\"What is the capital of France?\");\n\nconsole.log(message);\n/*\nAIMessage {\n  content: 'The capital of France is Paris. Paris is the largest city in France and also serves as the country\\'s cultural, political, and economic center.'\n}\n*/\n\n```\n\n----------------------------------------\n\nTITLE: Adding Documents to Qdrant Vector Store\nDESCRIPTION: Creates Document objects with content and metadata, then adds them to the vector store. This demonstrates how to populate the Qdrant database with document data that can later be queried.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/qdrant.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport type { Document } from \"@langchain/core/documents\";\n\nconst document1: Document = {\n  pageContent: \"The powerhouse of the cell is the mitochondria\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document2: Document = {\n  pageContent: \"Buildings are made out of brick\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document3: Document = {\n  pageContent: \"Mitochondria are made out of lipids\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document4: Document = {\n  pageContent: \"The 2024 Olympics are in Paris\",\n  metadata: { source: \"https://example.com\" }\n}\n\nconst documents = [document1, document2, document3, document4];\n\nawait vectorStore.addDocuments(documents);\n```\n\n----------------------------------------\n\nTITLE: Implementing StructuredOutputParser with Zod Schema\nDESCRIPTION: Sets up a StructuredOutputParser using a Zod schema to validate structured output for actor and film information. Demonstrates how the parser handles misformatted JSON output.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/output_parser_fixing.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { z } from \"zod\";\nimport { RunnableSequence } from \"@langchain/core/runnables\";\nimport { StructuredOutputParser } from \"@langchain/core/output_parsers\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\nconst zodSchema = z.object({\n  name: z.string().describe(\"name of an actor\"),\n  film_names: z.array(z.string()).describe(\"list of names of films they starred in\"),\n});\n\nconst parser = StructuredOutputParser.fromZodSchema(zodSchema);\n\nconst misformatted = \"{'name': 'Tom Hanks', 'film_names': ['Forrest Gump']}\";\n\nawait parser.parse(misformatted);\n```\n\n----------------------------------------\n\nTITLE: Fetching RAG Fusion Query Generation Prompt from LangChain Hub\nDESCRIPTION: Pulls a pre-defined prompt template from LangChain Hub specifically designed for RAG Fusion query generation. This prompt instructs the model to generate multiple search queries related to the original query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/rag_fusion.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\n/** Pull a prompt from the hub */\nconst prompt = await pull(\"langchain-ai/rag-fusion-query-generation\");\n//  const prompt = ChatPromptTemplate.fromMessages([\n//    [\"system\", \"You are a helpful assistant that generates multiple search queries based on a single input query.\"],\n//    [\"user\", \"Generate multiple search queries related to: {original_query}\"],\n//    [\"user\", \"OUTPUT (4 queries):\"],\n//  ]);\n```\n\n----------------------------------------\n\nTITLE: Llama CPP Single Prompt Streaming\nDESCRIPTION: Example of streaming responses with Llama CPP using a single prompt string\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/llama_cpp.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nStreamExample\n```\n\n----------------------------------------\n\nTITLE: Implementing MongoDB Chat Memory in LangChain.js\nDESCRIPTION: This code demonstrates how to set up and use MongoDB as a chat memory storage in a LangChain.js application. It includes connecting to MongoDB, creating a chat model, and using MongoDBChatMessageHistory for persistent storage.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/mongodb.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { MongoDBChatMessageHistory } from \"@langchain/community/stores/message/mongodb\";\nimport { ConversationChain } from \"langchain/chains\";\nimport { BufferMemory } from \"langchain/memory\";\n\nconst client = await MongoClient.connect(\"mongodb://localhost:27017\");\nconst collection = client.db(\"chatHistory\").collection(\"chatHistory\");\n\nconst memory = new BufferMemory({\n  chatHistory: new MongoDBChatMessageHistory({\n    collection,\n    sessionId: \"foo\",\n  }),\n});\n\nconst model = new ChatOpenAI();\nconst chain = new ConversationChain({ llm: model, memory });\n\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });\n\nconst res2 = await chain.call({ input: \"What's my name?\" });\nconsole.log({ res2 });\n\nawait client.close();\n```\n\n----------------------------------------\n\nTITLE: Creating Chat Prompt Template\nDESCRIPTION: Define prompt template with system message and chat history placeholder for contextual question handling.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate, MessagesPlaceholder } from \"@langchain/core/prompts\";\n\nconst contextualizeQSystemPrompt = (\n  \"Given a chat history and the latest user question \" +\n  \"which might reference context in the chat history, \" +\n  \"formulate a standalone question which can be understood \" +\n  \"without the chat history. Do NOT answer the question, \" +\n  \"just reformulate it if needed and otherwise return it as is.\"\n)\n\nconst contextualizeQPrompt = ChatPromptTemplate.fromMessages(\n  [\n    [\"system\", contextualizeQSystemPrompt],\n    new MessagesPlaceholder(\"chat_history\"),\n    [\"human\", \"{input}\"],\n  ]\n)\n```\n\n----------------------------------------\n\nTITLE: Performing Filtered Similarity Search (TypeScript)\nDESCRIPTION: Demonstrates performing a similarity search with a pre-filter condition on a metadata field. It defines a `filter` object using MongoDB Query Language (MQL) operators (`$eq` for equality) and passes it to the `similaritySearch` method. This requires the index to be configured for filtering on the specified field (`source`).\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/mongodb_atlas.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nconst filter = {\n  preFilter: {\n    source: {\n      $eq: \"https://example.com\",\n    },\n  },\n}\n\nconst filteredResults = await vectorStore.similaritySearch(\"biology\", 2, filter);\n\nfor (const doc of filteredResults) {\n  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);\n}\n```\n\n----------------------------------------\n\nTITLE: RAG Chain Execution\nDESCRIPTION: Example of invoking the RAG chain with a query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/bedrock-knowledge-bases.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nawait ragChain.invoke(\"...\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Tool Calling with ChatWatsonx for Calculator Functionality\nDESCRIPTION: Demonstrates how to implement tool calling with ChatWatsonx by defining a calculator tool with Zod schema and binding it to the model instance for mathematical operations.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/ibm.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n\nconst calculatorSchema = z.object({\n    operation: z\n      .enum([\"add\", \"subtract\", \"multiply\", \"divide\"])\n      .describe(\"The type of operation to execute.\"),\n    number1: z.number().describe(\"The first number to operate on.\"),\n    number2: z.number().describe(\"The second number to operate on.\"),\n  });\n  \nconst calculatorTool = tool(\nasync ({ operation, number1, number2 }) => {\n    if (operation === \"add\") {\n    return `${number1 + number2}`;\n    } else if (operation === \"subtract\") {\n    return `${number1 - number2}`;\n    } else if (operation === \"multiply\") {\n    return `${number1 * number2}`;\n    } else if (operation === \"divide\") {\n    return `${number1 / number2}`;\n    } else {\n    throw new Error(\"Invalid operation.\");\n    }\n},\n{\n    name: \"calculator\",\n    description: \"Can perform mathematical operations.\",\n    schema: calculatorSchema,\n}\n);\n\nconst instanceWithTools = instance.bindTools([calculatorTool]);\n\nconst res = await instanceWithTools.invoke(\"What is 3 * 12\");\nconsole.log(res)\n```\n\n----------------------------------------\n\nTITLE: Invoking ChatOllama for Translation in Python\nDESCRIPTION: Python code demonstrating how to invoke the ChatOllama model for translating English to French.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/ollama.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst aiMsg = await llm.invoke([\n    [\n        \"system\",\n        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n    ],\n    [\"human\", \"I love programming.\"],\n])\naiMsg\n```\n\n----------------------------------------\n\nTITLE: Indexing and Retrieving with MemoryVectorStore in JavaScript\nDESCRIPTION: Demonstrates how to use TogetherAIEmbeddings with MemoryVectorStore for indexing and retrieving documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/togetherai.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n\nconst text = \"LangChain is the framework for building context-aware reasoning applications\";\n\nconst vectorstore = await MemoryVectorStore.fromDocuments(\n  [{ pageContent: text, metadata: {} }],\n  embeddings,\n);\n\nconst retriever = vectorstore.asRetriever(1);\n\nconst retrievedDocuments = await retriever.invoke(\"What is LangChain?\");\n\nretrievedDocuments[0].pageContent;\n```\n\n----------------------------------------\n\nTITLE: Binding Tools to the Language Model\nDESCRIPTION: Code to bind tools to the language model for tool calling capability.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/agent_executor.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst modelWithTools = model.bindTools(tools);\n```\n\n----------------------------------------\n\nTITLE: Invoking a Tool with ToolCall in TypeScript\nDESCRIPTION: Demonstrates how to invoke a tool using a ToolCall object, which returns a ToolMessage containing both content and artifact.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/custom_tools.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nawait generateRandomInts.invoke({\n  name: \"generateRandomInts\",\n  args: { min: 0, max: 9, size: 10 },\n  id: \"123\", // required\n  type: \"tool_call\",\n});\n```\n\n----------------------------------------\n\nTITLE: Using Azure Managed Identity with AzureOpenAIEmbeddings\nDESCRIPTION: Configures AzureOpenAIEmbeddings to use Azure Managed Identity for authentication instead of an API key.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/azure_openai.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport {\n  DefaultAzureCredential,\n  getBearerTokenProvider,\n} from \"@azure/identity\";\nimport { AzureOpenAIEmbeddings } from \"@langchain/openai\";\n\nconst credentials = new DefaultAzureCredential();\nconst azureADTokenProvider = getBearerTokenProvider(\n  credentials,\n  \"https://cognitiveservices.azure.com/.default\"\n);\n\nconst modelWithManagedIdentity = new AzureOpenAIEmbeddings({\n  azureADTokenProvider,\n  azureOpenAIApiInstanceName: \"<your_instance_name>\",\n  azureOpenAIApiEmbeddingsDeploymentName: \"<your_embeddings_deployment_name>\",\n  azureOpenAIApiVersion: \"<api_version>\",\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Chat Prompt Template with System Message\nDESCRIPTION: Demonstrates how to create a ChatPromptTemplate with a pirate-themed system message using MessagesPlaceholder for dynamic message handling.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/chatbot.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\nconst promptTemplate = ChatPromptTemplate.fromMessages([\n  [\"system\", \"You talk like a pirate. Answer all questions to the best of your ability.\"],\n  [\"placeholder\", \"{messages}\"],\n]);\n```\n\n----------------------------------------\n\nTITLE: Batch Processing with Runnable\nDESCRIPTION: Shows how to process multiple inputs in batch using a runnable's batch method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/lcel_cheatsheet.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { RunnableLambda } from \"@langchain/core/runnables\";\n\nconst runnable = RunnableLambda.from((x: number) => x.toString());\n\nawait runnable.batch([7, 8, 9]);\n```\n\n----------------------------------------\n\nTITLE: Defining Mathematical Tools for LLM Tool Use\nDESCRIPTION: Creates a series of mathematical tools with TypeScript that will be available for the LLM to call. Each tool has a name, description, and schema defining its inputs and functionality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/example_selectors_langsmith.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { tool } from '@langchain/core/tools';\nimport { z } from 'zod';\n\nconst add = tool((input) => {\n  return (input.a + input.b).toString();\n}, {\n  name: \"add\",\n  description: \"Add two numbers\",\n  schema: z.object({\n    a: z.number().describe(\"The first number to add\"),\n    b: z.number().describe(\"The second number to add\"),\n  }),\n});\n\nconst cos = tool((input) => {\n  return Math.cos(input.angle).toString();\n}, {\n  name: \"cos\",\n  description: \"Calculate the cosine of an angle (in radians)\",\n  schema: z.object({\n    angle: z.number().describe(\"The angle in radians\"),\n  }),\n});\n\nconst divide = tool((input) => {\n  return (input.a / input.b).toString();\n}, {\n  name: \"divide\",\n  description: \"Divide two numbers\",\n  schema: z.object({\n    a: z.number().describe(\"The dividend\"),\n    b: z.number().describe(\"The divisor\"),\n  }),\n});\n\nconst log = tool((input) => {\n  return Math.log(input.value).toString();\n}, {\n  name: \"log\",\n  description: \"Calculate the natural logarithm of a number\",\n  schema: z.object({\n    value: z.number().describe(\"The number to calculate the logarithm of\"),\n  }),\n});\n\nconst multiply = tool((input) => {\n  return (input.a * input.b).toString();\n}, {\n  name: \"multiply\",\n  description: \"Multiply two numbers\",\n  schema: z.object({\n    a: z.number().describe(\"The first number to multiply\"),\n    b: z.number().describe(\"The second number to multiply\"),\n  }),\n});\n\nconst negate = tool((input) => {\n  return (-input.a).toString();\n}, {\n  name: \"negate\",\n  description: \"Negate a number\",\n  schema: z.object({\n    a: z.number().describe(\"The number to negate\"),\n  }),\n});\n\nconst pi = tool(() => {\n  return Math.PI.toString();\n}, {\n  name: \"pi\",\n  description: \"Return the value of pi\",\n  schema: z.object({}),\n});\n\nconst power = tool((input) => {\n  return Math.pow(input.base, input.exponent).toString();\n}, {\n  name: \"power\",\n  description: \"Raise a number to a power\",\n  schema: z.object({\n    base: z.number().describe(\"The base number\"),\n    exponent: z.number().describe(\"The exponent\"),\n  }),\n});\n\nconst sin = tool((input) => {\n  return Math.sin(input.angle).toString();\n}, {\n  name: \"sin\",\n  description: \"Calculate the sine of an angle (in radians)\",\n  schema: z.object({\n    angle: z.number().describe(\"The angle in radians\"),\n  }),\n});\n\nconst subtract = tool((input) => {\n  return (input.a - input.b).toString();\n}, {\n  name: \"subtract\",\n  description: \"Subtract two numbers\",\n  schema: z.object({\n    a: z.number().describe(\"The number to subtract from\"),\n    b: z.number().describe(\"The number to subtract\"),\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Adding Hooks to ChatMistralAI after Instantiation\nDESCRIPTION: Demonstrates how to add hooks to an existing ChatMistralAI instance and apply them to the HTTP client.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/mistral.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatMistralAI } from \"@langchain/mistralai\" \n\nconst model = new ChatMistralAI({\n    model: \"mistral-large-latest\",\n    temperature: 0,\n    maxRetries: 2,\n    // other params...\n});\n\nmodel.beforeRequestHooks = [ ...model.beforeRequestHooks, beforeRequestHook ];\nmodel.requestErrorHooks = [ ...model.requestErrorHooks, requestErrorHook ];\nmodel.responseHooks = [ ...model.responseHooks, responseHook ];\n\nmodel.addAllHooksToHttpClient();\n```\n\n----------------------------------------\n\nTITLE: Using Advanced Custom LLM with Event Streaming\nDESCRIPTION: Demonstrates how to use the advanced custom LLM implementation to stream events and access additional metadata like token usage through callback events.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/custom_llm.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst llm = new AdvancedCustomLLM({ n: 4 });\n\nconst eventStream = await llm.streamEvents(\"I am an LLM\", {\n  version: \"v2\",\n});\n\nfor await (const event of eventStream) {\n  if (event.event === \"on_llm_end\") {\n    console.log(JSON.stringify(event, null, 2));\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Hook Functions for ChatMistralAI\nDESCRIPTION: Defines hook functions for different request lifecycle events that can be used to customize ChatMistralAI's behavior.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/mistral.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconst beforeRequestHook = (req: Request): Request | void | Promise<Request | void> => {\n    // Code to run before a request is processed by Mistral\n};\n\nconst requestErrorHook = (err: unknown, req: Request): void | Promise<void> => {\n    // Code to run when an error occurs as Mistral is processing a request\n};\n\nconst responseHook = (res: Response, req: Request): void | Promise<void> => {\n    // Code to run before Mistral sends a successful response\n};\n```\n\n----------------------------------------\n\nTITLE: Creating Self-Query Retriever Instance\nDESCRIPTION: Initializes a self-query retriever with the vector store, LLM, and attribute information for querying movie data.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/memory.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { SelfQueryRetriever } from \"langchain/retrievers/self_query\";\nimport { FunctionalTranslator } from \"@langchain/core/structured_query\";\n\nconst selfQueryRetriever = SelfQueryRetriever.fromLLM({\n  llm: llm,\n  vectorStore: vectorStore,\n  /** A short summary of what the document contents represent. */\n  documentContents: \"Brief summary of a movie\",\n  attributeInfo: attributeInfo,\n  /**\n   * We need to create a basic translator that translates the queries into a\n   * filter format that the vector store can understand. We provide a basic translator\n   * translator here, but you can create your own translator by extending BaseTranslator\n   * abstract class. Note that the vector store needs to support filtering on the metadata\n   * attributes you want to query on.\n   */\n  structuredQueryTranslator: new FunctionalTranslator(),\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Chroma and LangChain Dependencies (TypeScript)\nDESCRIPTION: This snippet displays how to declare required dependencies for using Chroma with LangChainJS and OpenAI embeddings via an MDX/NPM integration. Dependencies include @langchain/community, @langchain/openai, @langchain/core, and chromadb. These packages must be installed before using any other vector store or embedding features.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/chroma.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport IntegrationInstallTooltip from \"@mdx_components/integration_install_tooltip.mdx\";\nimport Npm2Yarn from \"@theme/Npm2Yarn\";\n\n<IntegrationInstallTooltip></IntegrationInstallTooltip>\n\n<Npm2Yarn>\n  @langchain/community @langchain/openai @langchain/core chromadb\n</Npm2Yarn>\n```\n\n----------------------------------------\n\nTITLE: Instantiating AzureOpenAI Model in LangChain.js\nDESCRIPTION: Creates an instance of the AzureOpenAI model with configuration options including API credentials and model parameters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/azure.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { AzureOpenAI } from \"@langchain/openai\"\n\nconst llm = new AzureOpenAI({\n  model: \"gpt-3.5-turbo-instruct\",\n  azureOpenAIApiKey: \"<your_key>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY\n  azureOpenAIApiInstanceName: \"<your_instance_name>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME\n  azureOpenAIApiDeploymentName: \"<your_deployment_name>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME\n  azureOpenAIApiVersion: \"<api_version>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION\n  temperature: 0,\n  maxTokens: undefined,\n  timeout: undefined,\n  maxRetries: 2,\n  // other params...\n})\n```\n\n----------------------------------------\n\nTITLE: Azure Cosmos DB Mongo vCore Chat Message History Implementation\nDESCRIPTION: Complete example showing how to implement chat message history using Azure Cosmos DB Mongo vCore. Demonstrates initialization, adding messages, and setting up memory with the stored history.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/azure_cosmos_mongo_vcore.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AzureCosmosDBMongoChatMessageHistory } from \"@langchain/azure-cosmosdb\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { BufferMemory } from \"@langchain/community/memory\";\nimport { ConversationChain } from \"@langchain/core/chains\";\n\nconst connectionString =\n  process.env.COSMOS_CONNECTION_STRING || \"<your connection string>\";\n\nconst chatHistory = new AzureCosmosDBMongoChatMessageHistory({\n  connectionString,\n  database: \"chat_history_db\",\n  collection: \"chat_history\",\n  sessionId: \"test-session-id\",\n});\n\n// Create a new BufferMemory with the Azure Cosmos DB chat history\nconst memory = new BufferMemory({\n  returnMessages: true,\n  chatHistory,\n});\n\n// Set up a model\nconst model = new ChatOpenAI({\n  modelName: \"gpt-3.5-turbo\",\n  temperature: 0,\n});\n\n// Create a chain that uses the BufferMemory\nconst chain = new ConversationChain({\n  llm: model,\n  memory,\n});\n\nconst runDemo = async () => {\n  // First run\n  console.log(\"First run:\");\n  const response = await chain.invoke({ input: \"Hi, my name is Andrew.\" });\n  console.log(\"Response:\", response.response);\n\n  // Second run, with history\n  console.log(\"\\nSecond run (with history):\");\n  const response2 = await chain.invoke({\n    input: \"What did I just tell you my name was?\",\n  });\n  console.log(\"Response:\", response2.response);\n\n  // Get message history\n  console.log(\"\\nMessages in chat history:\")\n  const messages = await chatHistory.getMessages();\n  console.log(\n    messages.map((message) => {\n      return `${message._getType()}: ${message.content}`;\n    })\n  );\n\n  // Clear message history\n  await chatHistory.clear();\n};\n\nawait runDemo();\n```\n\n----------------------------------------\n\nTITLE: Streaming with ChatBedrockConverse\nDESCRIPTION: Example of using the streaming capability of ChatBedrockConverse to get responses in real-time.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-aws/README.md#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatBedrockConverse } from \"@langchain/aws\";\n\nconst model = new ChatBedrockConverse({\n  region: process.env.BEDROCK_AWS_REGION ?? \"us-east-1\",\n  credentials: {\n    secretAccessKey: process.env.BEDROCK_AWS_SECRET_ACCESS_KEY,\n    accessKeyId: process.env.BEDROCK_AWS_ACCESS_KEY_ID,\n  },\n});\n\nconst response = await model.stream(new HumanMessage(\"Hello world!\"));\n```\n\n----------------------------------------\n\nTITLE: Creating and Using BM25Retriever in LangChain.js\nDESCRIPTION: Example of how to create a BM25Retriever from a list of documents and use it to retrieve ranked results. The retriever is initialized with sample documents and a parameter k for the number of results to return.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/bm25.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { BM25Retriever } from \"@langchain/community/retrievers/bm25\";\n\nconst retriever = BM25Retriever.fromDocuments([\n  { pageContent: \"Buildings are made out of brick\", metadata: {} },\n  { pageContent: \"Buildings are made out of wood\", metadata: {} },\n  { pageContent: \"Buildings are made out of stone\", metadata: {} },\n  { pageContent: \"Cars are made out of metal\", metadata: {} },\n  { pageContent: \"Cars are made out of plastic\", metadata: {} },\n  { pageContent: \"mitochondria is the powerhouse of the cell\", metadata: {} },\n  { pageContent: \"mitochondria is made of lipids\", metadata: {} },\n], { k: 4 });\n\n// Will return the 4 documents reranked by the BM25 algorithm\nawait retriever.invoke(\"mitochondria\");\n```\n\n----------------------------------------\n\nTITLE: Composing Runnables with pipe()\nDESCRIPTION: Shows how to compose multiple runnables using the pipe() method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/lcel_cheatsheet.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { RunnableLambda } from \"@langchain/core/runnables\";\n\nconst runnable1 = RunnableLambda.from((x: any) => {\n  return { foo: x };\n});\n\nconst runnable2 = RunnableLambda.from((x: any) => [x].concat([x]));\n\nconst chain = runnable1.pipe(runnable2);\n\nawait chain.invoke(2);\n```\n\n----------------------------------------\n\nTITLE: Deleting Documents from PineconeStore by ID (TypeScript)\nDESCRIPTION: This snippet demonstrates how to delete specific documents from the `PineconeStore` using their IDs. It calls the `delete` method with an object containing an array of IDs to be removed from the Pinecone index.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/pinecone.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nawait vectorStore.delete({ ids: [\"4\"] });\n```\n\n----------------------------------------\n\nTITLE: Generating Mermaid Graph Visualization\nDESCRIPTION: Creates and displays a PNG visualization of the graph structure using Mermaid.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/sql_qa.ipynb#2025-04-22_snippet_14\n\nLANGUAGE: javascript\nCODE:\n```\nconst image = await graphWithInterrupt.getGraph().drawMermaidPng();\nconst arrayBuffer = await image.arrayBuffer();\n\nawait tslab.display.png(new Uint8Array(arrayBuffer));\n```\n\n----------------------------------------\n\nTITLE: Deleting Documents by ID from Matching Engine (TypeScript)\nDESCRIPTION: Demonstrates how to delete documents from the Matching Engine using their unique IDs. It first performs a similarity search to retrieve some documents, extracts their IDs (assuming they have an `id` property), and then calls the `delete` method with the list of IDs to remove them from the index.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/googlevertexai.mdx#2025-04-22_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport { IdDocument } from `@langchain/community/vectorstores/googlevertexai`;\n\nconst oldResults: IdDocument[] = await engine.similaritySearch(\"this\", 10);\nconst oldIds = oldResults.map( doc => doc.id! );\nawait engine.delete({ids: oldIds});\n```\n\n----------------------------------------\n\nTITLE: Initializing WatsonxLLM with Bearer Token Authentication\nDESCRIPTION: TypeScript code for creating a WatsonxLLM instance using bearer token authentication.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/ibm.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { WatsonxLLM } from \"@langchain/community/llms/ibm\";\n\nconst props = {\n  version: \"YYYY-MM-DD\",\n  serviceUrl: \"<SERVICE_URL>\",\n  projectId: \"<PROJECT_ID>\",\n  watsonxAIAuthType: \"bearertoken\",\n  watsonxAIBearerToken: \"<YOUR-BEARERTOKEN>\",\n};\nconst instance = new WatsonxLLM(props);\n```\n\n----------------------------------------\n\nTITLE: Invoking Updated Query Analyzer in JavaScript\nDESCRIPTION: This snippet demonstrates how to invoke the updated query analyzer with a sample question to test its improved performance with added examples.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_few_shot.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nawait queryAnalyzerWithExamples.invoke(\n    \"what's the difference between web voyager and reflection agents? do both use langgraph?\"\n)\n```\n\n----------------------------------------\n\nTITLE: Installing OpenSearch and Langchain Dependencies - Bash\nDESCRIPTION: Installs OpenSearch integration and Langchain core libraries necessary for working with the vector store. Requires npm and a Node.js environment. Install the packages before attempting to use any vector store code.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/opensearch.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -S @langchain/openai @langchain/core @opensearch-project/opensearch\n```\n\n----------------------------------------\n\nTITLE: Installing WebLLM SDK and LangChain Dependencies\nDESCRIPTION: Command to install the necessary dependencies for WebLLM integration with LangChain including the WebLLM SDK and required LangChain packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/web_llm.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -S @mlc-ai/web-llm @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Using CohereEmbeddings for Text Embeddings\nDESCRIPTION: Code example showing how to use CohereEmbeddings to generate vector embeddings for text. This is useful for semantic search and other similarity-based applications.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-cohere/README.md#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatCohere } from \"@langchain/cohere\";\n\nconst embeddings = new ChatCohere({\n  apiKey: process.env.COHERE_API_KEY,\n});\nconst res = await embeddings.embedQuery(\"Hello world\");\n```\n\n----------------------------------------\n\nTITLE: Configuring Callbacks for Runnable in TypeScript\nDESCRIPTION: Shows how to set up callbacks for a Runnable at runtime. These callbacks will be passed to all sub-calls made by the Runnable, allowing for custom handling of various events during execution.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/runnables.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nawait someRunnable.invoke(someInput, {\n  callbacks: [SomeCallbackHandler(), AnotherCallbackHandler()],\n});\n```\n\n----------------------------------------\n\nTITLE: Testing RAG Application with Retrieval in JavaScript\nDESCRIPTION: This snippet demonstrates testing the RAG application with a query that requires retrieval, showing how it streams the steps of query generation, retrieval, and answer generation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/qa_chat_history.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nlet inputs2 = { messages: [{ role: \"user\", content: \"What is Task Decomposition?\" }] };\n\nfor await (\n  const step of await graph.stream(inputs2, {\n    streamMode: \"values\",\n  })\n) {\n    const lastMessage = step.messages[step.messages.length - 1];\n    prettyPrint(lastMessage);\n    console.log(\"-----\\n\");\n}\n```\n\n----------------------------------------\n\nTITLE: Setting up Tavily Search Tool\nDESCRIPTION: Initializes the Tavily search tool with maximum results configuration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_tools.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { TavilySearchResults } from \"@langchain/community/tools/tavily_search\";\n\nconst tools = [\n  new TavilySearchResults({\n    maxResults: 1,\n  }),\n];\n```\n\n----------------------------------------\n\nTITLE: Loading Documents from S3 Files in TypeScript Using S3FileLoader\nDESCRIPTION: Example showing how to use the S3FileLoader to load files from AWS S3 and convert them into Document objects. The code demonstrates configuring the loader with bucket, key, and optional S3 configuration parameters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/s3.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { S3Loader } from \"@langchain/community/document_loaders/web/s3\";\n\n/**\n * This loader takes in a specific file path and loads it from s3\n * @param bucket The name of the S3 bucket\n * @param key The key of the S3 object\n * @param unstructuredAPIURL The URL of the Unstructured API\n * @param unstructuredAPIKey The API key for the Unstructured API\n * @param s3Config Configuration for the S3 client\n *\n * @returns A document with the file content and metadata\n */\nconst loader = new S3Loader({\n  bucket: \"my-document-bucket\",\n  key: \"docs/doc.txt\",\n  unstructuredAPIURL: \"http://localhost:8000/general/v0/general\",\n  unstructuredAPIKey: \"UNSTRUCTURED_API_KEY\",\n  s3Config: {\n    region: \"us-east-1\",\n    credentials: {\n      accessKeyId: \"YOUR_ACCESS_KEY_ID\",\n      secretAccessKey: \"YOUR_SECRET_ACCESS_KEY\",\n    },\n  },\n});\n\nconst docs = await loader.load();\n```\n\n----------------------------------------\n\nTITLE: Setting up RAG Components\nDESCRIPTION: Defines the prompt template, initializes the ChatOpenAI model and Tavily retriever, and creates a function to format retrieved documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/rewrite.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nconst template = `Answer the users question based only on the following context:\n\n<context>\n  {context}\n</context>\n\nQuestion: {question}`\n\nconst prompt = PromptTemplate.fromTemplate(template);\n\nconst model = new ChatOpenAI({\n  temperature: 0,\n  openAIApiKey: Deno.env.get(\"OPENAI_API_KEY\"),\n})\n\nconst retriever = new TavilySearchAPIRetriever({\n  k: 3,\n  apiKey: Deno.env.get(\"TAVILY_API_KEY\"),\n});\n\nconst formatDocs = (documents: Document[]) => documents.map((doc) => doc.pageContent).join(\"\\n\");\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key for Embeddings\nDESCRIPTION: Code snippet showing how to set the OpenAI API key as an environment variable, which is required when using OpenAI embeddings with the vector store.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/vectorstores.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.OPENAI_API_KEY = \"YOUR_API_KEY\";\n```\n\n----------------------------------------\n\nTITLE: Invoking ChatCohere Model\nDESCRIPTION: Demonstrates how to invoke the ChatCohere model with system and human messages for translation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/cohere.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconst aiMsg = await llm.invoke([\n    [\n        \"system\",\n        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n    ],\n    [\"human\", \"I love programming.\"],\n])\naiMsg\n```\n\n----------------------------------------\n\nTITLE: Using YandexGPT Embeddings\nDESCRIPTION: Example demonstrating how to generate embeddings for queries and documents using YandexGPT embeddings model\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-yandex/README.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { YandexGPTEmbeddings } from \"@langchain/yandex\";\n\nconst model = new YandexGPTEmbeddings({});\n\n/* Embed queries */\nconst res = await model.embedQuery(\n  \"This is a test document.\"\n);\n/* Embed documents */\nconst documentRes = await model.embedDocuments([\"This is a test document.\"]);\n```\n\n----------------------------------------\n\nTITLE: Installing Upstash Redis for Caching in npm\nDESCRIPTION: This command installs the Upstash Redis client for use with caching in a Node.js project using npm.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chat_model_caching.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @upstash/redis\n```\n\n----------------------------------------\n\nTITLE: Advanced Tracing with LangSmith Integration\nDESCRIPTION: Implements a custom chat model with advanced tracing capabilities for integration with LangSmith. Demonstrates how to log parameters and options in traces for monitoring and debugging.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/custom_chat.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { BaseChatModel, type BaseChatModelCallOptions, type BaseChatModelParams } from \"@langchain/core/language_models/chat_models\";\nimport { BaseMessage } from \"@langchain/core/messages\";\nimport { ChatResult } from \"@langchain/core/outputs\";\n\ninterface CustomChatModelOptions extends BaseChatModelCallOptions {\n  // Some required or optional inner args\n  tools: Record<string, any>[];\n}\n\ninterface CustomChatModelParams extends BaseChatModelParams {\n  temperature: number;\n  n: number;\n}\n\nclass CustomChatModel extends BaseChatModel<CustomChatModelOptions> {\n  temperature: number;\n\n  n: number;\n\n  static lc_name(): string {\n    return \"CustomChatModel\";\n  }\n\n  constructor(fields: CustomChatModelParams) {\n    super(fields);\n    this.temperature = fields.temperature;\n    this.n = fields.n;\n  }\n\n  // Anything returned in this method will be logged as metadata in the trace.\n  // It is common to pass it any options used to invoke the function.\n  invocationParams(options?: this[\"ParsedCallOptions\"]) {\n    return {\n      tools: options?.tools,\n      n: this.n,\n    };\n  }\n\n  async _generate(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<ChatResult> {\n    if (!messages.length) {\n      throw new Error(\"No messages provided.\");\n    }\n    if (typeof messages[0].content !== \"string\") {\n      throw new Error(\"Multimodal messages are not supported.\");\n    }\n    const additionalParams = this.invocationParams(options);\n    const content = await someAPIRequest(messages, additionalParams);\n    return {\n      generations: [{ message: new AIMessage({ content }), text: content }],\n      llmOutput: {},\n    };\n  }\n\n  _llmType(): string {\n    return \"advanced_custom_chat_model\";\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Citation Tool Calling\nDESCRIPTION: Adding structured output for document citations using tool calling functionality\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_citations.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { z } from \"zod\";\n\nconst llmWithTool1 = llm.withStructuredOutput(\n  z.object({\n    answer: z.string().describe(\"The answer to the user question, which is based only on the given sources.\"),\n    citations: z.array(z.number()).describe(\"The integer IDs of the SPECIFIC sources which justify the answer.\")\n  }).describe(\"A cited source from the given text\"),\n  {\n    name: \"cited_answers\"\n  }\n);\n\nconst exampleQ = `What is Brian's height?\\n\\nSource: 1\\nInformation: Suzy is 6'2\"\\n\\nSource: 2\\nInformation: Jeremiah is blonde\\n\\nSource: 3\\nInformation: Brian is 3 inches shorter than Suzy`;\n\nawait llmWithTool1.invoke(exampleQ);\n```\n\n----------------------------------------\n\nTITLE: Running an Agent with Streaming Output in TypeScript\nDESCRIPTION: Executes the agent with a user query and streams the response events. This demonstrates how to process tool calls and agent responses in real-time.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/toolkits.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconst exampleQuery = \"...\"\n\nconst events = await agentExecutor.stream(\n  { messages: [[\"user\", exampleQuery]]},\n  { streamMode: \"values\", }\n)\n\nfor await (const event of events) {\n  const lastMsg = event.messages[event.messages.length - 1];\n  if (lastMsg.tool_calls?.length) {\n    console.dir(lastMsg.tool_calls, { depth: null });\n  } else if (lastMsg.content) {\n    console.log(lastMsg.content);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for LangChain and OpenAI\nDESCRIPTION: Bash commands to set environment variables for OpenAI API key and optional LangSmith configuration for tracing and callbacks.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/sql_prompting.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=\"your api key\"\n# Uncomment the below to use LangSmith. Not required.\n# export LANGSMITH_API_KEY=\"your api key\"\n# export LANGSMITH_TRACING=true\n\n# Reduce tracing latency if you are not in a serverless environment\n# export LANGCHAIN_CALLBACKS_BACKGROUND=true\n```\n\n----------------------------------------\n\nTITLE: Creating SAP HANA Vector Index from Loader and Performing Similarity Search (TypeScript)\nDESCRIPTION: This TypeScript snippet illustrates creating a SAP HANA vector store index using documents loaded via a LangchainJS `Loader` (e.g., `TextLoader`). It demonstrates loading documents, splitting them, and then using `HanaDB.fromDocuments` to populate the SAP HANA Cloud Vector Engine. Finally, it performs a similarity search on the created index.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/hanavector.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { HanaDB } from \"@langchain/community/vectorstores/hana\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\n\n// Connection options for the database connection, read from environment variables\n// HANA_HOST, HANA_PORT, HANA_USER, HANA_PASSWORD\nconst connectionOptions = {\n  host: process.env.HANA_HOST ?? \"\",\n  port: process.env.HANA_PORT ?? \"\",\n  user: process.env.HANA_USER ?? \"\",\n  password: process.env.HANA_PASSWORD ?? \"\",\n};\n\n// Configuration for the vector store table\nconst vectorStoreTableConfig = {\n  tableName: \"MY_TABLE_NAME\",\n  schemaName: \"MY_SCHEMA_NAME\", // optional, default: current schema\n};\n\n// Create a new document loader\nconst loader = new TextLoader(\"src/document_loaders/example_data/example.txt\");\n\n// Load documents from the loader\nconst docs = await loader.load();\n\n// Split the documents into smaller chunks\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 1000,\n  chunkOverlap: 200,\n});\nconst documents = await splitter.splitDocuments(docs);\n\n// Create embeddings model\nconst embeddings = new OpenAIEmbeddings({});\n\n// Create the vector store from the documents\nconst vectorStore = await HanaDB.fromDocuments(\n  documents,\n  embeddings,\n  connectionOptions,\n  vectorStoreTableConfig\n);\n\n// Perform a similarity search\nconst results = await vectorStore.similaritySearch(\"what is langchain?\", 1);\n\nconsole.log(results);\n\n/*\n  [\n    Document {\n      pageContent: 'LangChain is a framework for developing applications powered by language models. It allows users to connect a language model to sources of context (prompt instructions, few shot examples, content to ground its response in, etc.) and rely on the language model to reason about how to answer based on the provided context.',\n      metadata: { source: 'src/document_loaders/example_data/example.txt', loc: [Object] }\n    }\n  ]\n*/\n\n```\n\n----------------------------------------\n\nTITLE: Embedding Multiple Texts with VertexAIEmbeddings\nDESCRIPTION: Example of generating embeddings for multiple text documents simultaneously\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/google_vertex_ai.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nconst text2 = \"LangGraph is a library for building stateful, multi-actor applications with LLMs\";\n\nconst vectors = await embeddings.embedDocuments([text, text2]);\n\nconsole.log(vectors[0].slice(0, 100));\nconsole.log(vectors[1].slice(0, 100));\n```\n\n----------------------------------------\n\nTITLE: Initializing Tigris Vector Document Store with TypeScript\nDESCRIPTION: The snippet initializes a vector document store using Tigris, specifying the server URL, project name, and authentication details. It indexes documents with OpenAI embeddings, demonstrating the storage and management of document vectors.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/tigris.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { VectorDocumentStore } from \"@tigrisdata/vector\";\nimport { Document } from \"langchain/document\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { TigrisVectorStore } from \"langchain/vectorstores/tigris\";\n\nconst index = new VectorDocumentStore({\n  connection: {\n    serverUrl: \"api.preview.tigrisdata.cloud\",\n    projectName: process.env.TIGRIS_PROJECT,\n    clientId: process.env.TIGRIS_CLIENT_ID,\n    clientSecret: process.env.TIGRIS_CLIENT_SECRET,\n  },\n  indexName: \"examples_index\",\n  numDimensions: 1536,\n});\n\nconst docs = [\n  new Document({\n    metadata: { foo: \"bar\" },\n    pageContent: \"tigris is a cloud-native vector db\",\n  }),\n  new Document({\n    metadata: { foo: \"bar\" },\n    pageContent: \"the quick brown fox jumped over the lazy dog\",\n  }),\n  new Document({\n    metadata: { baz: \"qux\" },\n    pageContent: \"lorem ipsum dolor sit amet\",\n  }),\n  new Document({\n    metadata: { baz: \"qux\" },\n    pageContent: \"tigris is a river\",\n  }),\n];\n\nawait TigrisVectorStore.fromDocuments(docs, new OpenAIEmbeddings(), { index });\n```\n\n----------------------------------------\n\nTITLE: Invoking a Chain in JavaScript\nDESCRIPTION: This snippet shows how to invoke the previously created chain with a specific input.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/sequence.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nawait chain.invoke({ topic: \"bears\" })\n```\n\n----------------------------------------\n\nTITLE: Embedding Single Text with CohereEmbeddings in Python\nDESCRIPTION: This snippet demonstrates how to use the embedQuery method of CohereEmbeddings to generate a vector representation for a single text query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/cohere.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconst singleVector = await embeddings.embedQuery(text);\n\nconsole.log(singleVector.slice(0, 100));\n```\n\n----------------------------------------\n\nTITLE: Installing Google GAuth Package for LangChain\nDESCRIPTION: Command to install the Google GAuth package for use with LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/google.mdx#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/google-gauth @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Creating Couchbase Connection\nDESCRIPTION: Code to establish a connection to a Couchbase cluster using connection string, username and password authentication. This connection object will be passed to the Vector Store.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/couchbase.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Cluster } from \"couchbase\";\n\nconst connectionString = \"couchbase://localhost\"; // or couchbases://localhost if you are using TLS\nconst dbUsername = \"Administrator\"; // valid database user with read access to the bucket being queried\nconst dbPassword = \"Password\"; // password for the database user\n\nconst couchbaseClient = await Cluster.connect(connectionString, {\n  username: dbUsername,\n  password: dbPassword,\n  configProfile: \"wanDevelopment\",\n});\n```\n\n----------------------------------------\n\nTITLE: Instantiating SupabaseVectorStore with OpenAI Embeddings (TypeScript)\nDESCRIPTION: This snippet demonstrates how to import necessary modules, initialize OpenAI embeddings, create a Supabase client using environment variables, and finally instantiate the `SupabaseVectorStore`. It configures the vector store to use the specified embeddings model, Supabase client, table name, and query function.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/supabase.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { SupabaseVectorStore } from \"@langchain/community/vectorstores/supabase\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nimport { createClient } from \"@supabase/supabase-js\";\n\nconst embeddings = new OpenAIEmbeddings({\n  model: \"text-embedding-3-small\",\n});\n\nconst supabaseClient = createClient(\n  process.env.SUPABASE_URL,\n  process.env.SUPABASE_PRIVATE_KEY\n);\n\nconst vectorStore = new SupabaseVectorStore(embeddings, {\n  client: supabaseClient,\n  tableName: \"documents\",\n  queryName: \"match_documents\",\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Composite Primary Keys and Partitioning in CassandraStore\nDESCRIPTION: Demonstrates how to define a composite primary key with specific partition keys in the `CassandraStore` configuration. The `primaryKey` parameter accepts an array of column definitions. Setting `partition: true` designates a column as part of the partition key (e.g., `userid` and `collectionid`). This allows partitioning data for scalability or logical separation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/cassandra.mdx#2025-04-22_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\n  ...,\n  primaryKey: [\n    {name: \"userid\", type: \"text\", partition: true},\n    {name: \"collectionid\", type: \"text\", partition: true},\n    {name: \"docid\", type: \"text\"},\n    {name: \"docpart\", type: \"int\"},\n  ],\n  ...\n```\n\n----------------------------------------\n\nTITLE: Multiple Text Embedding\nDESCRIPTION: Example of embedding multiple texts using embedDocuments method\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/text_embedding.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconst text2 = \"LangGraph is a library for building stateful, multi-actor applications with LLMs\";\n\nconst vectors = await embeddings.embedDocuments([text, text2]);\n\nconsole.log(vectors[0].slice(0, 100));\nconsole.log(vectors[1].slice(0, 100));\n```\n\n----------------------------------------\n\nTITLE: Integrating ChatGPT Plugin with LangChain Agent - TypeScript\nDESCRIPTION: Provides a TypeScript example (from '@examples/agents/aiplugin-tool.ts') demonstrating how to use a ChatGPT plugin via LangChain. The code likely sets up an agent executor that can make API calls to a plugin endpoint, utilizing LangChain's abstractions. Prerequisites include the previously installed @langchain/openai and @langchain/core packages, and the example assumes use of a no-auth plugin.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/aiplugin-tool.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n{Example}\n```\n\n----------------------------------------\n\nTITLE: Configuring Convex Schema for Chat Messages\nDESCRIPTION: Defines the schema for the messages table in Convex, including indexing for efficient querying by sessionId.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/convex.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { defineSchema, defineTable } from \"convex/server\";\nimport { v } from \"convex/values\";\n\nexport default defineSchema({\n  messages: defineTable({\n    sessionId: v.string(),\n    message: v.object({\n      type: v.string(),\n      data: v.object({\n        content: v.string(),\n        role: v.optional(v.string()),\n        name: v.optional(v.string()),\n        additional_kwargs: v.optional(v.any()),\n      }),\n    }),\n  }).index(\"bySessionId\", [\"sessionId\"]),\n});\n```\n\n----------------------------------------\n\nTITLE: Exact Value Search in Couchbase Vector Store using TypeScript\nDESCRIPTION: This code snippet shows how to perform an exact value search on a textual field (author) in the metadata object using Couchbase Vector Store.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/couchbase.mdx#2025-04-22_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst exactValueResult = await store.similaritySearch(query, 4, {\n  fields: [\"metadata.author\"],\n  searchOptions: {\n    query: { field: \"metadata.author\", match: \"John Doe\" },\n  },\n});\nconsole.log(exactValueResult[0]);\n```\n\n----------------------------------------\n\nTITLE: Splitting Documents into Manageable Chunks\nDESCRIPTION: Uses RecursiveCharacterTextSplitter to break down the loaded documents into smaller chunks of 500 characters with no overlap for efficient processing and retrieval.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_retrieval.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\n\nconst textSplitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 500,\n  chunkOverlap: 0,\n});\n\nconst allSplits = await textSplitter.splitDocuments(rawDocs);\n```\n\n----------------------------------------\n\nTITLE: Chaining ChatGroq with Prompt Templates\nDESCRIPTION: Example of creating a chain by combining ChatGroq with a prompt template for language translation tasks.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/groq.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\"\n\nconst prompt = ChatPromptTemplate.fromMessages(\n    [\n        [\n            \"system\",\n            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n        ],\n        [\"human\", \"{input}\"],\n    ]\n)\n\nconst chain = prompt.pipe(llm);\nawait chain.invoke(\n    {\n        input_language: \"English\",\n        output_language: \"German\",\n        input: \"I love programming.\",\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Vector Store with Document Collection\nDESCRIPTION: Sets up an in-memory vector store with sample movie documents and defines queryable attributes including genre, year, director, and rating information.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/memory.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { Document } from \"@langchain/core/documents\";\nimport type { AttributeInfo } from \"langchain/chains/query_constructor\";\n\n/**\n * First, we create a bunch of documents. You can load your own documents here instead.\n * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.\n */\nconst docs = [\n  new Document({\n    pageContent:\n      \"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n    metadata: { year: 1993, rating: 7.7, genre: \"science fiction\" },\n  }),\n  new Document({\n    pageContent:\n      \"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n    metadata: { year: 2010, director: \"Christopher Nolan\", rating: 8.2 },\n  }),\n  new Document({\n    pageContent:\n      \"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n    metadata: { year: 2006, director: \"Satoshi Kon\", rating: 8.6 },\n  }),\n  new Document({\n    pageContent:\n      \"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n    metadata: { year: 2019, director: \"Greta Gerwig\", rating: 8.3 },\n  }),\n  new Document({\n    pageContent: \"Toys come alive and have a blast doing so\",\n    metadata: { year: 1995, genre: \"animated\" },\n  }),\n  new Document({\n    pageContent: \"Three men walk into the Zone, three men walk out of the Zone\",\n    metadata: {\n      year: 1979,\n      director: \"Andrei Tarkovsky\",\n      genre: \"science fiction\",\n      rating: 9.9,\n    },\n  }),\n];\n\n/**\n * Next, we define the attributes we want to be able to query on.\n * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.\n * We also provide a description of each attribute and the type of the attribute.\n * This is used to generate the query prompts.\n */\nconst attributeInfo: AttributeInfo[] = [\n  {\n    name: \"genre\",\n    description: \"The genre of the movie\",\n    type: \"string or array of strings\",\n  },\n  {\n    name: \"year\",\n    description: \"The year the movie was released\",\n    type: \"number\",\n  },\n  {\n    name: \"director\",\n    description: \"The director of the movie\",\n    type: \"string\",\n  },\n  {\n    name: \"rating\",\n    description: \"The rating of the movie (1-10)\",\n    type: \"number\",\n  },\n  {\n    name: \"length\",\n    description: \"The length of the movie in minutes\",\n    type: \"number\",\n  },\n];\n\n/**\n * Next, we instantiate a vector store. This is where we store the embeddings of the documents.\n * We also need to provide an embeddings object. This is used to embed the documents.\n */\nconst embeddings = new OpenAIEmbeddings();\nconst vectorStore = await MemoryVectorStore.fromDocuments(docs, embeddings);\n```\n\n----------------------------------------\n\nTITLE: Chaining Ollama LLM with Prompt Templates\nDESCRIPTION: Shows how to chain an Ollama model with a prompt template for more structured generation. This example creates a translation prompt that accepts input text and target language parameters, then pipes the formatted prompt to the LLM.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/ollama.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { PromptTemplate } from \"@langchain/core/prompts\"\n\nconst prompt = PromptTemplate.fromTemplate(\"How to say {input} in {output_language}:\\n\")\n\nconst chain = prompt.pipe(llm);\nawait chain.invoke(\n  {\n    output_language: \"German\",\n    input: \"I love programming.\",\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: Using the Discord Tool within an Agent - TypeScript\nDESCRIPTION: Shows how to integrate the Discord Tool into a LangChain agent using TypeScript. The example is imported from '@examples/agents/discord.ts' and assumes peer dependencies are installed. This implementation may cover agent capabilities such as reacting to incoming messages and performing channel operations programmatically.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/discord.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport AgentExample from \"@examples/agents/discord.ts\";\n```\n\n----------------------------------------\n\nTITLE: Adding Documents to WeaviateStore\nDESCRIPTION: Example of adding multiple documents with metadata to the vector store using UUIDs for identification\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/weaviate.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport type { Document } from \"@langchain/core/documents\";\nimport { v4 as uuidv4 } from \"uuid\";\n\nconst document1: Document = {\n  pageContent: \"The powerhouse of the cell is the mitochondria\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document2: Document = {\n  pageContent: \"Buildings are made out of brick\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document3: Document = {\n  pageContent: \"Mitochondria are made out of lipids\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document4: Document = {\n  pageContent: \"The 2024 Olympics are in Paris\",\n  metadata: { source: \"https://example.com\" }\n}\n\nconst documents = [document1, document2, document3, document4];\nconst uuids = [uuidv4(), uuidv4(), uuidv4(), uuidv4()];\n\nawait vectorStore.addDocuments(documents, { ids: uuids });\n```\n\n----------------------------------------\n\nTITLE: Integrating SelfQueryRetriever in a RAG Chain\nDESCRIPTION: Shows how to incorporate the SelfQueryRetriever into a Retrieval-Augmented Generation (RAG) chain for LLM applications.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/hnswlib.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { RunnablePassthrough, RunnableSequence } from \"@langchain/core/runnables\";\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\n\nimport type { Document } from \"@langchain/core/documents\";\n\nconst prompt = ChatPromptTemplate.fromTemplate(`\nAnswer the question based only on the context provided.\n\nContext: {context}\n\nQuestion: {question}`);\n\nconst formatDocs = (docs: Document[]) => {\n  return docs.map((doc) => JSON.stringify(doc)).join(\"\\n\\n\");\n}\n\n// See https://js.langchain.com/docs/tutorials/rag\nconst ragChain = RunnableSequence.from([\n  {\n    context: selfQueryRetriever.pipe(formatDocs),\n    question: new RunnablePassthrough(),\n  },\n  prompt,\n  llm,\n  new StringOutputParser(),\n]);\n```\n\n----------------------------------------\n\nTITLE: Implementing Cosine Similarity for Embeddings in TypeScript\nDESCRIPTION: Implementation of the cosine similarity function to measure similarity between two embedding vectors. This is the recommended similarity metric for OpenAI embeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/embedding_models.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nfunction cosineSimilarity(vec1: number[], vec2: number[]): number {\n  const dotProduct = vec1.reduce((sum, val, i) => sum + val * vec2[i], 0);\n  const norm1 = Math.sqrt(vec1.reduce((sum, val) => sum + val * val, 0));\n  const norm2 = Math.sqrt(vec2.reduce((sum, val) => sum + val * val, 0));\n  return dotProduct / (norm1 * norm2);\n}\n\nconst similarity = cosineSimilarity(queryResult, documentResult);\nconsole.log(\"Cosine Similarity:\", similarity);\n```\n\n----------------------------------------\n\nTITLE: Initializing JinaEmbeddings Class in TypeScript\nDESCRIPTION: This snippet demonstrates how to import the `JinaEmbeddings` class and initialize it. It requires a Jina API token (passed directly via the `apiKey` parameter or set as the `JINA_API_KEY` environment variable) and optionally allows specifying the embedding model (defaults to `jina-clip-v2`).\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/jina.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JinaEmbeddings } from \"@langchain/community/embeddings/jina\";\n\nconst embeddings = new JinaEmbeddings({\n  apiKey: \"YOUR_API_TOKEN\",\n  model: \"jina-clip-v2\", // Optional, defaults to \"jina-clip-v2\"\n});\n```\n\n----------------------------------------\n\nTITLE: Chaining Google Vertex AI Model with Prompt Template in LangChain.js\nDESCRIPTION: Example of chaining a VertexAI model with a prompt template to create a more complex language processing pipeline.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/google_vertex_ai.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PromptTemplate } from \"@langchain/core/prompts\"\n\nconst prompt = PromptTemplate.fromTemplate(\"How to say {input} in {output_language}:\\n\")\n\nconst chain = prompt.pipe(llm);\nawait chain.invoke(\n  {\n    output_language: \"German\",\n    input: \"I love programming.\",\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Cached Text for Prompt Caching in Python\nDESCRIPTION: Defines a variable to hold cached text for use with Anthropic's prompt caching feature.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/anthropic.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nlet CACHED_TEXT = \"...\";\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of ParentDocumentRetriever in TypeScript\nDESCRIPTION: Example demonstrating the basic setup and usage of ParentDocumentRetriever, including document splitting, vector store creation, and retrieval.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/parent_document_retriever.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n{Example}\n```\n\n----------------------------------------\n\nTITLE: Invoking ChatMistralAI with Messages\nDESCRIPTION: Sends a conversation to the Mistral AI model including system and user messages, following the required message alternation pattern.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/mistral.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst aiMsg = await llm.invoke([\n    [\n        \"system\",\n        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n    ],\n    [\"human\", \"I love programming.\"],\n])\naiMsg\n```\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(aiMsg.content)\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search on PineconeStore (TypeScript)\nDESCRIPTION: This code shows how to perform a similarity search directly on the `PineconeStore`. It uses the `similaritySearch` method, providing a query string (\"biology\"), the number of results to return (2), and an optional metadata filter. The results (documents) are then iterated and logged.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/pinecone.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\n// Optional filter\nconst filter = { source: \"https://example.com\" };\n\nconst similaritySearchResults = await vectorStore.similaritySearch(\"biology\", 2, filter);\n\nfor (const doc of similaritySearchResults) {\n  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing MemoryVectorStore in LangChain with Embedding Model\nDESCRIPTION: This code demonstrates how to initialize a MemoryVectorStore instance with an embedding model in LangChain. The vector store requires an embedding model to convert documents into vector representations.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/vectorstores.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n// Initialize with an embedding model\nconst vectorStore = new MemoryVectorStore(new SomeEmbeddingModel());\n```\n\n----------------------------------------\n\nTITLE: Setting up Memory Vector Store\nDESCRIPTION: Creates a MemoryVectorStore instance using the configured embeddings for storing and retrieving vectors.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/qa_chat_history.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n\nconst vectorStore = new MemoryVectorStore(embeddings);\n```\n\n----------------------------------------\n\nTITLE: Using AlibabaTongyiEmbeddings in TypeScript\nDESCRIPTION: This code snippet demonstrates how to import and use the AlibabaTongyiEmbeddings class to generate embeddings for given text. It includes error handling and shows how to retrieve the embeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/alibaba_tongyi.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AlibabaTongyiEmbeddings } from \"@langchain/community/embeddings/alibaba_tongyi\";\n\nconst embeddings = new AlibabaTongyiEmbeddings();\n\ntry {\n  const res = await embeddings.embedQuery(\"Hello world\");\n  console.log(res);\n} catch (error) {\n  console.error(\"Error:\", error);\n}\n\nconst documentRes = await embeddings.embedDocuments([\"Hello world\", \"Bye bye\"]);\nconsole.log({ documentRes });\n```\n\n----------------------------------------\n\nTITLE: Using RedisVectorStore as Retriever in Node.js\nDESCRIPTION: Convert the vector store into a retriever format for seamless integration with other Node.js chain mechanisms. By specifying the number of results, quick retrieval can be accomplished.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/redis.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nconst retriever = vectorStore.asRetriever({\n  k: 2,\n});\nawait retriever.invoke(\"biology\");\n```\n\n----------------------------------------\n\nTITLE: Creating a Translation Chain with Prompt Template\nDESCRIPTION: Demonstrates how to chain the model with a prompt template to create a reusable language translation pipeline with configurable input and output languages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/cloudflare_workersai.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\"\n\nconst prompt = ChatPromptTemplate.fromMessages(\n  [\n    [\n      \"system\",\n      \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n    ],\n    [\"human\", \"{input}\"],\n  ]\n)\n\nconst chain = prompt.pipe(llm);\nawait chain.invoke(\n  {\n    input_language: \"English\",\n    output_language: \"German\",\n    input: \"I love programming.\",\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: Instantiating CheerioWebBaseLoader to scrape web content\nDESCRIPTION: Initializes a CheerioWebBaseLoader instance with a Hacker News URL. This creates a loader that can extract text content from the specified webpage using Cheerio's HTML parsing capabilities.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/web_cheerio.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\"\n\nconst loader = new CheerioWebBaseLoader(\"https://news.ycombinator.com/item?id=34817881\", {\n  // optional params: ...\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing Dynamic Few-Shot Example Selection Chain\nDESCRIPTION: Constructs a processing pipeline that retrieves similar examples from LangSmith, formats them into a prompt with the user's query, and sends this to an LLM with access to mathematical tools. This demonstrates how to use examples for few-shot prompting.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/example_selectors_langsmith.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { HumanMessage, SystemMessage, BaseMessage, BaseMessageLike } from \"@langchain/core/messages\";\nimport { RunnableLambda } from \"@langchain/core/runnables\";\nimport { Client as LangSmithClient, Example } from \"langsmith\";\nimport { coerceMessageLikeToMessage } from \"@langchain/core/messages\";\n\nconst client = new LangSmithClient();\n\nasync function similarExamples(input: Record<string, any>): Promise<Record<string, any>> {\n  const examples = await client.similarExamples(input, dataset.id, 5);\n  return { ...input, examples };\n}\n\nfunction constructPrompt(input: { examples: Example[], input: string }): BaseMessage[] {\n  const instructions = \"You are great at using mathematical tools.\";\n  let messages: BaseMessage[] = []\n  \n  for (const ex of input.examples) {\n    // Assuming ex.outputs.output is an array of message-like objects\n    messages = messages.concat(ex.outputs.output.flatMap((msg: BaseMessageLike) => coerceMessageLikeToMessage(msg)));\n  }\n  \n  const examples = messages.filter(msg => msg._getType() !== 'system');\n  examples.forEach((ex) => {\n    if (ex._getType() === 'human') {\n      ex.name = \"example_user\";\n    } else {\n      ex.name = \"example_assistant\";\n    }\n  });\n\n  return [new SystemMessage(instructions), ...examples, new HumanMessage(input.input)];\n}\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o\",\n  temperature: 0,\n});\nconst tools = [add, cos, divide, log, multiply, negate, pi, power, sin, subtract];\nconst llmWithTools = llm.bindTools(tools);\n\nconst exampleSelector = new RunnableLambda(\n  { func: similarExamples }\n).withConfig({ runName: \"similarExamples\" });\n\nconst chain = exampleSelector.pipe(\n  new RunnableLambda({\n    func: constructPrompt\n  }).withConfig({\n    runName: \"constructPrompt\"\n  })\n).pipe(llmWithTools);\n```\n\n----------------------------------------\n\nTITLE: Installing Cloudflare Integration Package for LangChain\nDESCRIPTION: Installation command for adding the Cloudflare integration package to a LangChain project. Requires both the Cloudflare-specific package and the core LangChain dependency.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/cloudflare_workersai.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n@langchain/cloudflare @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Specifying Output Method for Structured Data in TypeScript\nDESCRIPTION: Shows how to specify a preferred method for outputting structured data with models that support multiple approaches. This example uses OpenAI's JSON mode with a more specific prompt.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/structured_output.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst structuredLlm = model.withStructuredOutput(joke, {\n  method: \"json_mode\",\n  name: \"joke\",\n})\n\nawait structuredLlm.invoke(\n  \"Tell me a joke about cats, respond in JSON with `setup` and `punchline` keys\"\n)\n```\n\n----------------------------------------\n\nTITLE: Using ArxivRetriever to query arXiv database\nDESCRIPTION: Demonstrates how to invoke the ArxivRetriever to search for articles using a query string. The example searches for \"quantum computing\" and processes the returned documents to display their titles and content.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/arxiv-retriever.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst query = \"quantum computing\";\n\nconst documents = await retriever.invoke(query);\ndocuments.forEach((doc) => {\n  console.log(\"Title:\", doc.metadata.title);\n  console.log(\"Content:\", doc.pageContent); // Parsed PDF content\n});\n```\n\n----------------------------------------\n\nTITLE: Invoking LangChain Agent with Empty Chat History in Python\nDESCRIPTION: This snippet demonstrates how to invoke the LangChain agent with an initial message and an empty chat history, introducing the concept of stateful interactions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/agent_executor.ipynb#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nawait agentExecutor.invoke({ input: \"hi! my name is bob\", chat_history: [] })\n```\n\n----------------------------------------\n\nTITLE: Creating Retriever Tool for Proper Nouns in JavaScript\nDESCRIPTION: This code creates a retriever tool that can search over relevant proper nouns in the database. It adds documents to the vector store and creates a tool for the agent to use.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/sql_qa.ipynb#2025-04-22_snippet_20\n\nLANGUAGE: javascript\nCODE:\n```\nimport { createRetrieverTool } from \"langchain/tools/retriever\";\nimport { Document } from \"@langchain/core/documents\";\n\n\nconst documents = properNouns.map(text => new Document({ pageContent: text }));\nawait vectorStore.addDocuments(documents)\n\nconst retriever = vectorStore.asRetriever(5);\n\nconst retrieverTool = createRetrieverTool(retriever, {\n  name: \"searchProperNouns\",\n  description:\n    \"Use to look up values to filter on. Input is an approximate spelling \" +\n    \"of the proper noun, output is valid proper nouns. Use the noun most \" +\n    \"similar to the search.\"\n});\n```\n\n----------------------------------------\n\nTITLE: Using the Self-Query Retriever\nDESCRIPTION: This snippet shows how to use the self-query retriever to fetch movies rated higher than 8.5.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/chroma.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nawait selfQueryRetriever.invoke(\n  \"Which movies are rated higher than 8.5?\"\n);\n```\n\n----------------------------------------\n\nTITLE: Implementing Momento-Backed Chat Memory in LangChain\nDESCRIPTION: TypeScript code demonstrating how to set up and use Momento-backed chat memory in a LangChain project. It includes initializing the Momento client, creating a chat model, and setting up a conversation chain with Momento-backed memory.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/momento.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { BufferMemory } from \"langchain/memory\";\nimport { ConversationChain } from \"langchain/chains\";\nimport { MomentoChatMessageHistory } from \"@langchain/community/stores/message/momento\";\nimport { CacheClient, Configurations, CredentialProvider } from \"@gomomento/sdk\";\n\n// Instantiate the Momento client\nconst client = new CacheClient({\n  configuration: Configurations.Laptop.v1(),\n  credentialProvider: CredentialProvider.fromString(\n    \"<YOUR-MOMENTO-API-KEY-HERE>\"\n  ),\n  defaultTtlSeconds: 60 * 60 * 24,\n});\n\n// Instantiate the chat model\nconst chat = new ChatOpenAI({});\n\n// Define a cache name and a session ID\nconst cacheName = \"langchain-cache\";\nconst sessionId = \"test-session\";\n\n// Create a Momento-backed chat message history store\nconst messageHistory = await MomentoChatMessageHistory.fromProps({\n  client,\n  cacheName,\n  sessionId,\n  sessionTtl: 300,\n});\n\n// Create a ConversationChain with Momento-backed memory\nconst chain = new ConversationChain({\n  llm: chat,\n  memory: new BufferMemory({\n    chatHistory: messageHistory,\n    returnMessages: true,\n    memoryKey: \"history\",\n  }),\n});\n\n// Example conversation turns\nconst res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\nconsole.log({ res1 });\n\nconst res2 = await chain.call({ input: \"What's my name?\" });\nconsole.log({ res2 });\n\n// Verify that the message history is in the cache\nconst messages = await messageHistory.getMessages();\nconsole.log(\n  JSON.stringify(\n    messages.map((msg) => ({\n      type: msg._getType(),\n      data: msg.content,\n    }))\n  )\n);\n```\n\n----------------------------------------\n\nTITLE: Invoking a RunnableParallel Chain in TypeScript\nDESCRIPTION: Demonstrates how to invoke a RunnableParallel chain, which executes multiple runnables in parallel and returns an object with the results.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/lcel.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst finalOutput = await chain.invoke(someInput);\n```\n\n----------------------------------------\n\nTITLE: Instantiating ChatMistralAI Model\nDESCRIPTION: Creates a new instance of the ChatMistralAI model with specified parameters like model name, temperature, and retry settings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/mistral.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatMistralAI } from \"@langchain/mistralai\" \n\nconst llm = new ChatMistralAI({\n    model: \"mistral-large-latest\",\n    temperature: 0,\n    maxRetries: 2,\n    // other params...\n})\n```\n\n----------------------------------------\n\nTITLE: Setting Azure OpenAI Environment Variables\nDESCRIPTION: Shell commands for setting the necessary Azure OpenAI API environment variables including the instance name, deployment name, API key, and API version.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/azure.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nAZURE_OPENAI_API_INSTANCE_NAME=<YOUR_INSTANCE_NAME>\nAZURE_OPENAI_API_DEPLOYMENT_NAME=<YOUR_DEPLOYMENT_NAME>\nAZURE_OPENAI_API_KEY=<YOUR_KEY>\nAZURE_OPENAI_API_VERSION=\"2024-02-01\"\n```\n\n----------------------------------------\n\nTITLE: Creating a RunnableLambda Chain in TypeScript\nDESCRIPTION: Demonstrates how to explicitly create a RunnableLambda from a custom function and use it in a chain with a language model and prompt template.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/functions.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { RunnableLambda } from \"@langchain/core/runnables\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst lengthFunction = (input: { foo: string }): { length: string } => {\n  return {\n    length: input.foo.length.toString(),\n  };\n};\n\nconst model = new ChatOpenAI({ model: \"gpt-4o\" });\n\nconst prompt = ChatPromptTemplate.fromTemplate(\"What is {length} squared?\");\n\nconst chain = RunnableLambda.from(lengthFunction)\n  .pipe(prompt)\n  .pipe(model)\n  .pipe(new StringOutputParser());\n\nawait chain.invoke({ \"foo\": \"bar\" });\n```\n\n----------------------------------------\n\nTITLE: Creating a FewShotPromptTemplate with the Example Selector\nDESCRIPTION: Combines the example selector with prompt templates to create a few-shot prompt system that dynamically selects examples based on the input.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/example_selectors.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nconst prompt = new FewShotPromptTemplate({\n    exampleSelector,\n    examplePrompt,\n    suffix: \"Input: {input} -> Output:\",\n    prefix: \"Translate the following words from English to Italian:\",\n    inputVariables: [\"input\"],\n})\n\nconsole.log(await prompt.format({ input: \"word\" }))\n```\n\n----------------------------------------\n\nTITLE: Creating a Semantic Similarity Example Selector in LangChain.js\nDESCRIPTION: This code creates a SemanticSimilarityExampleSelector that retrieves the top k examples most semantically similar to a given input. It demonstrates how to configure the selector and test it with a sample input.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot_examples_chat.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconst exampleSelector = new SemanticSimilarityExampleSelector(\n    {\n        vectorStore,\n        k: 2\n    }\n)\n\n// The prompt template will load examples by passing the input do the `select_examples` method\nawait exampleSelector.selectExamples({ input: \"horse\"})\n```\n\n----------------------------------------\n\nTITLE: Implementing Text Snippet Citations\nDESCRIPTION: Extending the citation functionality to include verbatim text quotes from sources\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_citations.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport { Document } from \"@langchain/core/documents\";\n\nconst citationSchema = z.object({\n  sourceId: z.number().describe(\"The integer ID of a SPECIFIC source which justifies the answer.\"),\n  quote: z.string().describe(\"The VERBATIM quote from the specified source that justifies the answer.\")\n});\n\nconst llmWithTool2 = llm.withStructuredOutput(\n  z.object({\n    answer: z.string().describe(\"The answer to the user question, which is based only on the given sources.\"),\n    citations: z.array(citationSchema).describe(\"Citations from the given sources that justify the answer.\")\n  }), {\n    name: \"quoted_answer\",\n  })\n\nconst answerChain2 = prompt.pipe(llmWithTool2);\nconst map2 = RunnableMap.from({\n  question: new RunnablePassthrough(),\n  docs: retriever,\n})\n// complete chain that calls the retriever -> formats docs to string -> runs answer subchain -> returns just the answer and retrieved docs.\nconst chain2 = map2\n  .assign({ context: (input: { docs: Array<Document> }) => formatDocsWithId(input.docs) })\n  .assign({ quoted_answer: answerChain2 })\n  .pick([\"quoted_answer\", \"docs\"]);\n  \nawait chain2.invoke(\"How fast are cheetahs?\")\n```\n\n----------------------------------------\n\nTITLE: Initializing WeaviateStore with OpenAI Embeddings\nDESCRIPTION: Creating a WeaviateStore instance with OpenAI embeddings and configuring the Weaviate client with necessary parameters\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/weaviate.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { WeaviateStore } from \"@langchain/weaviate\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nimport weaviate from \"weaviate-ts-client\";\n// import { ApiKey } from \"weaviate-ts-client\"\n\nconst embeddings = new OpenAIEmbeddings({\n  model: \"text-embedding-3-small\",\n});\n\n// The Weaviate SDK has an issue with types\nconst weaviateClient = (weaviate as any).client({\n  scheme: process.env.WEAVIATE_SCHEME ?? \"http\",\n  host: process.env.WEAVIATE_HOST ?? \"localhost\",\n  // If necessary\n  // apiKey: new ApiKey(process.env.WEAVIATE_API_KEY ?? \"default\"),\n});\n\nconst vectorStore = new WeaviateStore(embeddings, {\n  client: weaviateClient,\n  // Must start with a capital letter\n  indexName: \"Langchainjs_test\",\n  // Default value\n  textKey: \"text\",\n  // Any keys you intend to set as metadata\n  metadataKeys: [\"source\"],\n});\n```\n\n----------------------------------------\n\nTITLE: Importing Required LangChain Modules\nDESCRIPTION: Imports necessary modules from LangChain for implementing RAG, including prompt templates, chat models, output parsers, and retrievers.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/rewrite.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n// Deno.env.set(\"OPENAI_API_KEY\", \"\");\n// Deno.env.set(\"TAVILY_API_KEY\", \"\");\n\nimport { PromptTemplate } from \"npm:langchain@0.0.172/prompts\";\nimport { ChatOpenAI } from \"npm:langchain@0.0.172/chat_models/openai\";\nimport { StringOutputParser } from \"npm:langchain@0.0.172/schema/output_parser\";\nimport { RunnableSequence, RunnablePassthrough } from \"npm:langchain@0.0.172/schema/runnable\";\nimport { TavilySearchAPIRetriever } from \"npm:langchain@0.0.172/retrievers/tavily_search_api\";\nimport type { Document } from \"npm:langchain@0.0.172/schema/document\";\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Dependencies for AlephAlpha\nDESCRIPTION: Commands to install the required LangChain packages for working with AlephAlpha models using npm or yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/aleph_alpha.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Complete Example: Using JinaEmbeddings for Query and Document Embeddings\nDESCRIPTION: This comprehensive example demonstrates the end-to-end process: importing required modules (`JinaEmbeddings`, `localImageToBase64`), initializing `JinaEmbeddings` with an API key and specific model (`jina-embeddings-v2-base-en`), generating embeddings for both a single query and multiple documents (including text and images), and logging the output within an asynchronous function.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/jina.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JinaEmbeddings } from \"@langchain/community/embeddings/jina\";\nimport { localImageToBase64 } from \"@langchain/community/embeddings/jina/util\";\n\nconst embeddings = new JinaEmbeddings({\n  apiKey: \"YOUR_API_TOKEN\",\n  model: \"jina-embeddings-v2-base-en\",\n});\n\nasync function runExample() {\n  const queryEmbedding = await embeddings.embedQuery(\"Example query text.\");\n  console.log(\"Query Embedding:\", queryEmbedding);\n\n  const documents = [\n    \"hello\",\n    {\n      text: \"hello\",\n    },\n    {\n      image: \"https://i.ibb.co/nQNGqL0/beach1.jpg\",\n    },\n    {\n      image: await localImageToBase64(\"beach1.jpg\"),\n    },\n  ];\n  const documentEmbeddings = await embeddings.embedDocuments(documents);\n  console.log(\"Document Embeddings:\", documentEmbeddings);\n}\n\nrunExample();\n```\n\n----------------------------------------\n\nTITLE: Executing Calculator Tool in LangChain.js\nDESCRIPTION: A Calculator tool execution in LangChain.js that calculates the number of days between 1970 and 2023 based on 365 days per year. This tool takes a mathematical expression as input and returns the calculated result.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_46\n\nLANGUAGE: plaintext\nCODE:\n```\n(2023 - 1970) * 365\n```\n\n----------------------------------------\n\nTITLE: Importing Azure AI Search Vector Store\nDESCRIPTION: Code snippet for importing the Azure AI Search vector store class from the LangChain community package. This class enables vector search capabilities using Azure's search engine.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/microsoft.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AzureAISearchVectorStore } from \"@langchain/community/vectorstores/azure_aisearch\";\n```\n\n----------------------------------------\n\nTITLE: Invoking mergeMessageRuns as a Runnable in JavaScript\nDESCRIPTION: Demonstrates that mergeMessageRuns can be used as a Runnable object, which can be invoked like all other Runnables in LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/merge_message_runs.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nawait merger.invoke(messages)\n```\n\n----------------------------------------\n\nTITLE: Using Custom Callback Handler for Custom Events\nDESCRIPTION: This example demonstrates how to use a custom callback handler to process custom events dispatched from within a Runnable. It shows how to define a handleCustomEvent method in the callback handler.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/callbacks_custom_events.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RunnableConfig, RunnableLambda } from \"@langchain/core/runnables\";\nimport { dispatchCustomEvent } from \"@langchain/core/callbacks/dispatch\";\n\nconst reflect = RunnableLambda.from(async (value: string) => {\n  await dispatchCustomEvent(\"event1\", { reversed: value.split(\"\").reverse().join(\"\") });\n  await dispatchCustomEvent(\"event2\", 5);\n  return value;\n});\n\nawait reflect.invoke(\"hello world\", {\n  callbacks: [{\n    handleCustomEvent(eventName, data, runId) {\n      console.log(eventName, data, runId);\n    },\n  }]\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a DocumentCompressorPipeline for Advanced Retrieval\nDESCRIPTION: This snippet demonstrates how to create a DocumentCompressorPipeline that combines text splitting and embedding filtering for advanced document retrieval and compression.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/contextual_compression.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport DocumentCompressorPipelineExample from \"@examples/retrievers/document_compressor_pipeline.ts\";\n\n<CodeBlock language=\"typescript\">{DocumentCompressorPipelineExample}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Creating and Using a Retriever from HNSWLib Vector Store\nDESCRIPTION: Converts the vector store into a retriever for easier usage in chains. The retriever is configured with an optional filter and a limit on the number of documents to return.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/hnswlib.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nconst retriever = vectorStore.asRetriever({\n  // Optional filter\n  filter: filter,\n  k: 2,\n});\nawait retriever.invoke(\"biology\");\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings for Multiple Documents\nDESCRIPTION: Shows how to generate embeddings for multiple documents using the embedDocuments method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/deepinfra.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst documents = [\n  \"Document 1 text...\",\n  \"Document 2 text...\",\n  \"Document 3 text...\",\n];\n\nconst embeddingsArray = await embeddings.embedDocuments(documents);\nconsole.log(embeddingsArray);\n```\n\n----------------------------------------\n\nTITLE: Defining Revision Chain\nDESCRIPTION: Creates a revision chain that attempts to fix validation errors by providing feedback to the model about the constraints that were violated.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/basic_critique_revise.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconst reviseTemplate = `Original prompt:\n--------------\n{original_prompt}\n--------------\n\nCompletion:\n--------------\n{completion}\n--------------\n\nAbove, the completion did not satisfy the constraints given by the original prompt and provided schema.\n\nError:\n--------------\n{error}\n--------------\n\nTry again. Only respond with an answer that satisfies the constraints laid out in the original prompt and provided schema:`;\n\nconst revisePrompt = PromptTemplate.fromTemplate(reviseTemplate);\n\nconst reviseChain = revisePrompt\n  .pipe(taskFunctionCallModel)\n  .pipe(new JsonOutputFunctionsParser())\n  .withConfig({ runName: \"ReviseChain\" });\n```\n\n----------------------------------------\n\nTITLE: Initializing BedrockChat with AWS Credentials\nDESCRIPTION: Example of instantiating a BedrockChat instance with AWS credentials and region configuration. Shows optional endpoint and model kwargs settings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/bedrock.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { BedrockChat } from \"@langchain/community/chat_models/bedrock\";\n\nconst llm = new BedrockChat({\n  model: \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n  region: process.env.BEDROCK_AWS_REGION,\n  credentials: {\n    accessKeyId: process.env.BEDROCK_AWS_ACCESS_KEY_ID!,\n    secretAccessKey: process.env.BEDROCK_AWS_SECRET_ACCESS_KEY!,\n  },\n  // endpointUrl: \"custom.amazonaws.com\",\n  // modelKwargs: {\n  //   anthropic_version: \"bedrock-2023-05-31\",\n  // },\n});\n```\n\n----------------------------------------\n\nTITLE: Chaining OpenAI LLM with Prompt Templates\nDESCRIPTION: Shows how to combine prompt templates with the OpenAI model to create a processing chain. This example demonstrates language translation using template variables.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/openai.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { PromptTemplate } from \"@langchain/core/prompts\"\n\nconst prompt = new PromptTemplate({\n  template: \"How to say {input} in {output_language}:\\n\",\n  inputVariables: [\"input\", \"output_language\"],\n})\n\nconst chain = prompt.pipe(llm);\nawait chain.invoke(\n  {\n    output_language: \"German\",\n    input: \"I love programming.\",\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: Context Caching Implementation\nDESCRIPTION: Implementing context caching functionality with ChatVertexAI for optimizing costs and improving performance.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/google_vertex_ai.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatVertexAI } from \"@langchain/google-vertexai\";\n\nconst modelWithCachedContent = new ChatVertexAI({\n  model: \"gemini-1.5-pro-002\",\n  location: \"us-east5\",\n});\n\nawait modelWithCachedContent.invoke(\"What is in the content?\", {\n  cachedContent: \"projects/PROJECT_NUMBER/locations/LOCATION/cachedContents/CACHE_ID\",\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing StructuredOutputParser with Zod Schema\nDESCRIPTION: Sets up a StructuredOutputParser using Zod schema to define expected output structure with answer and source fields. Creates a runnable chain that formats instructions and processes the model response.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/output_parser_structured.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport { z } from \"zod\";\nimport { RunnableSequence } from \"@langchain/core/runnables\";\nimport { StructuredOutputParser } from \"@langchain/core/output_parsers\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\nconst zodSchema = z.object({\n  answer: z.string().describe(\"answer to the user's question\"),\n  source: z.string().describe(\"source used to answer the user's question, should be a website.\")\n})\n\nconst parser = StructuredOutputParser.fromZodSchema(zodSchema);\n\nconst chain = RunnableSequence.from([\n  ChatPromptTemplate.fromTemplate(\n    \"Answer the users question as best as possible.\\n{format_instructions}\\n{question}\"\n  ),\n  model,\n  parser,\n]);\n\nconsole.log(parser.getFormatInstructions());\n```\n\n----------------------------------------\n\nTITLE: Testing Retriever Tool in JavaScript\nDESCRIPTION: This code demonstrates how to use the retriever tool by invoking it with a query string.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/sql_qa.ipynb#2025-04-22_snippet_21\n\nLANGUAGE: javascript\nCODE:\n```\nconsole.log(await retrieverTool.invoke({ query: \"Alice Chains\" }))\n```\n\n----------------------------------------\n\nTITLE: Accessing document metadata in CheerioWebBaseLoader results\nDESCRIPTION: Shows how to access and display the metadata of a loaded document. The metadata typically includes the source URL and other information about the scraped page.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/web_cheerio.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(docs[0].metadata)\n```\n\n----------------------------------------\n\nTITLE: Implementing JsonOutputParser with Streaming\nDESCRIPTION: Sets up a JsonOutputParser that supports streaming partial outputs, demonstrating its usage with a chat template and model chain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/output_parser_structured.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { JsonOutputParser } from \"@langchain/core/output_parsers\";\n\nconst template = `Return a JSON object with a single key named \"answer\" that answers the following question: {question}.\nDo not wrap the JSON output in markdown blocks.`\n\nconst jsonPrompt = ChatPromptTemplate.fromTemplate(template);\nconst jsonParser = new JsonOutputParser();\nconst jsonChain = jsonPrompt.pipe(model).pipe(jsonParser);\n\nconst stream = await jsonChain.stream({\n  question: \"Who invented the microscope?\",\n});\n\nfor await (const s of stream) {\n  console.log(s)\n}\n```\n\n----------------------------------------\n\nTITLE: Instantiating GradientLLM with API Credentials\nDESCRIPTION: Example of creating a GradientLLM instance with explicit access key and workspace ID instead of using environment variables.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/gradient_ai.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = new GradientLLM({\n  gradientAccessKey: \"My secret Access Token\"\n  workspaceId: \"My secret workspace id\"\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Translation Chain with ChatPromptTemplate\nDESCRIPTION: Example of creating a chain combining a ChatPromptTemplate with the BedrockChat model for language translation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/bedrock.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\"\n\nconst prompt = ChatPromptTemplate.fromMessages(\n  [\n    [\n      \"system\",\n      \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n    ],\n    [\"human\", \"{input}\"],\n  ]\n)\n\nconst chain = prompt.pipe(llm);\nawait chain.invoke(\n  {\n    input_language: \"English\",\n    output_language: \"German\",\n    input: \"I love programming.\",\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing WatsonxLLM with Bearer Token Authentication\nDESCRIPTION: Creates a new instance of WatsonxLLM using Bearer token authentication by providing version, service URL, project ID, and bearer token.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/ibm.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { WatsonxLLM } from \"@langchain/community/llms/ibm\";\n\nconst props = {\n  version: \"YYYY-MM-DD\",\n  serviceUrl: \"<SERVICE_URL>\",\n  projectId: \"<PROJECT_ID>\",\n  watsonxAIAuthType: \"bearertoken\",\n  watsonxAIBearerToken: \"<YOUR-BEARERTOKEN>\",\n};\nconst instance = new WatsonxLLM(props);\n```\n\n----------------------------------------\n\nTITLE: Instantiating Self-Query Retriever with Supabase in TypeScript\nDESCRIPTION: Creates a SelfQueryRetriever instance using the Supabase vector store, LLM, and attribute information. It utilizes the SupabaseTranslator for structured query translation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/supabase.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { SelfQueryRetriever } from \"langchain/retrievers/self_query\";\nimport { SupabaseTranslator } from \"@langchain/community/structured_query/supabase\";\n\nconst selfQueryRetriever = SelfQueryRetriever.fromLLM({\n  llm: llm,\n  vectorStore: vectorStore,\n  /** A short summary of what the document contents represent. */\n  documentContents: \"Brief summary of a movie\",\n  attributeInfo: attributeInfo,\n  structuredQueryTranslator: new SupabaseTranslator(),\n});\n```\n\n----------------------------------------\n\nTITLE: Basic Invocation of ChatCerebras Model\nDESCRIPTION: Demonstrates how to invoke a ChatCerebras model with a system message and user input for English to French translation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/cerebras.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst aiMsg = await llm.invoke([\n    {\n      role: \"system\",\n      content: \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n    },\n    { role: \"user\", content: \"I love programming.\" },\n])\naiMsg\n```\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(aiMsg.content)\n```\n\n----------------------------------------\n\nTITLE: Document Loading with Table Name\nDESCRIPTION: JavaScript implementation for loading documents using specific table name with custom content and metadata columns.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/google_cloudsql_pg.mdx#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst documentLoaderArgs: PostgresLoaderOptions = {\n  tableName: \"test_table_custom\",\n  contentColumns: [\"fruit_name\", \"variety\"],\n  metadataColumns: [\n    \"fruit_id\",\n    \"quantity_in_stock\",\n    \"price_per_unit\",\n    \"organic\",\n  ],\n  format: \"text\",\n};\n\nconst documentLoaderInstance = await PostgresLoader.initialize(\n  PEInstance,\n  documentLoaderArgs\n);\n```\n\n----------------------------------------\n\nTITLE: Integrating the self-query retriever in a chain\nDESCRIPTION: Shows how to incorporate the self-query retriever into a LangChain chain for more complex query processing.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/qdrant.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { RunnablePassthrough, RunnableSequence } from \"@langchain/core/runnables\";\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\n\nimport type { Document } from \"@langchain/core/documents\";\n\nconst prompt = ChatPromptTemplate.fromTemplate(`\nAnswer the question based only on the context provided.\n\nContext: {context}\n\nQuestion: {question}`);\n\nconst formatDocs = (docs: Document[]) => {\n  return docs.map((doc) => JSON.stringify(doc)).join(\"\\n\\n\");\n}\n\n// See https://js.langchain.com/docs/tutorials/rag\nconst ragChain = RunnableSequence.from([\n  {\n    context: selfQueryRetriever.pipe(formatDocs),\n    question: new RunnablePassthrough(),\n  },\n  prompt,\n  llm,\n  new StringOutputParser(),\n]);\n```\n\n----------------------------------------\n\nTITLE: Instantiating TextLoader in LangChain.js\nDESCRIPTION: Example showing how to import and instantiate a TextLoader object with a file path.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/text.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { TextLoader } from \"langchain/document_loaders/fs/text\"\n\nconst loader = new TextLoader(\"../../../../../../examples/src/document_loaders/example_data/example.txt\")\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for LangChain OpenAI\nDESCRIPTION: Command to install the necessary npm packages for working with LangChain and OpenAI.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming_llm.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Creating a Self-Query Retriever with Default Search Parameters\nDESCRIPTION: This code demonstrates how to create a SelfQueryRetriever with default search parameters, including a filter for movies rated higher than 8.5.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/chroma.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst selfQueryRetrieverWithDefaultParams = SelfQueryRetriever.fromLLM({\n  llm,\n  vectorStore,\n  documentContents: \"Brief summary of a movie\",\n  attributeInfo,\n  structuredQueryTranslator: new ChromaTranslator(),\n  searchParams: {\n    filter: {\n      rating: {\n        $gt: 8.5,\n      }\n    },\n    mergeFiltersOperator: \"and\",\n  }\n});\n```\n\n----------------------------------------\n\nTITLE: Embedding Single Text with BedrockEmbeddings\nDESCRIPTION: Shows how to generate embeddings for a single text using the embedQuery method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/bedrock.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst singleVector = await embeddings.embedQuery(text);\n\nconsole.log(singleVector.slice(0, 100));\n```\n\n----------------------------------------\n\nTITLE: Initializing HuggingFace Inference Embeddings in TypeScript\nDESCRIPTION: This code demonstrates how to create an instance of HuggingFaceInferenceEmbeddings. It requires an API key, which can be passed directly or set as an environment variable HUGGINGFACEHUB_API_KEY in Node.js.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/hugging_face_inference.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HuggingFaceInferenceEmbeddings } from \"@langchain/community/embeddings/hf\";\n\nconst embeddings = new HuggingFaceInferenceEmbeddings({\n  apiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.HUGGINGFACEHUB_API_KEY\n});\n```\n\n----------------------------------------\n\nTITLE: Basic RunnableParallel Example in TypeScript\nDESCRIPTION: Demonstrates a basic usage of RunnableParallel to execute multiple language models in parallel, processing the same input topic with different models and combining the results.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/parallel.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport CodeBlock from \"@theme/CodeBlock\";\nimport BasicExample from \"@examples/guides/expression_language/runnable_maps_basic.ts\";\n\n<CodeBlock language=\"typescript\">{BasicExample}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Creating a Tool Function with Schema in TypeScript\nDESCRIPTION: Shows how to use the tool wrapper function to create a tool from a JavaScript function. This method should be used when the tool call needs to execute a function.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/custom_tools.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { z } from \"zod\";\nimport { tool } from \"@langchain/core/tools\";\n\nconst adderSchema = z.object({\n  a: z.number(),\n  b: z.number(),\n});\nconst adderTool = tool(async (input): Promise<string> => {\n  const sum = input.a + input.b;\n  return `The sum of ${input.a} and ${input.b} is ${sum}`;\n}, {\n  name: \"adder\",\n  description: \"Adds two numbers together\",\n  schema: adderSchema,\n});\n\nawait adderTool.invoke({ a: 1, b: 2 });\n```\n\n----------------------------------------\n\nTITLE: Creating a Retrieval Tool in JavaScript\nDESCRIPTION: This snippet shows how to create a custom tool for retrieval using the vector store. It defines a schema for the tool input and formats the retrieved documents into a serialized string.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/qa_chat_history.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport { z } from \"zod\";\nimport { tool } from \"@langchain/core/tools\";\n\nconst retrieveSchema = z.object({query: z.string()});\n\nconst retrieve = tool(\n  async ({ query }) => {\n    const retrievedDocs = await vectorStore.similaritySearch(query, 2);\n    const serialized = retrievedDocs.map(\n      doc => `Source: ${doc.metadata.source}\\nContent: ${doc.pageContent}`\n    ).join(\"\\n\");\n    return [\n      serialized,\n      retrievedDocs,\n    ];\n  },\n  {\n    name: \"retrieve\",\n    description: \"Retrieve information related to a query.\",\n    schema: retrieveSchema,\n    responseFormat: \"content_and_artifact\",\n  }\n);\n```\n\n----------------------------------------\n\nTITLE: Accessing Intermediate Steps in Agent Execution\nDESCRIPTION: Shows how to access intermediate steps during agent execution in both LangChain and LangGraph implementations. Includes configuration for returnIntermediateSteps parameter.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/migrate_agent.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst agentExecutorWithIntermediateSteps = new AgentExecutor({\n  agent,\n  tools,\n  returnIntermediateSteps: true,\n});\n\nconst result = await agentExecutorWithIntermediateSteps.invoke({\n  input: query,\n});\n\nconsole.log(result.intermediateSteps);\n```\n\nLANGUAGE: javascript\nCODE:\n```\nagentOutput = await app.invoke({\n  messages: [\n    { role: \"user\", content: query },\n  ]\n});\n\nconsole.log(agentOutput.messages);\n```\n\n----------------------------------------\n\nTITLE: Embedding Single Text with FireworksEmbeddings in Python\nDESCRIPTION: Shows how to generate an embedding vector for a single piece of text using the embedQuery method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/fireworks.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconst singleVector = await embeddings.embedQuery(text);\n\nconsole.log(singleVector.slice(0, 100));\n```\n\n----------------------------------------\n\nTITLE: Installing officeparser dependency for PPTX file processing\nDESCRIPTION: Command to install the officeparser package, which is required for parsing PPTX files in a Node.js environment.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/pptx.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install officeparser\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI with Custom Base URL\nDESCRIPTION: Demonstrates how to configure the OpenAI client to use a custom base URL for API requests, which is useful for custom deployments or proxies.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/openai.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconst llmCustomURL = new OpenAI({\n  temperature: 0.9,\n  configuration: {\n    baseURL: \"https://your_custom_url.com\",\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Indexing and Retrieving Data with MemoryVectorStore\nDESCRIPTION: Demonstrates how to index a sample document using MemoryVectorStore and retrieve similar text using the WatsonxEmbeddings model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/ibm.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\n// Create a vector store with a sample text\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n\nconst text = \"LangChain is the framework for building context-aware reasoning applications\";\n\nconst vectorstore = await MemoryVectorStore.fromDocuments(\n  [{ pageContent: text, metadata: {} }],\n  embeddings,\n);\n\n// Use the vector store as a retriever that returns a single document\nconst retriever = vectorstore.asRetriever(1);\n\n// Retrieve the most similar text\nconst retrievedDocuments = await retriever.invoke(\"What is LangChain?\");\n\nretrievedDocuments[0].pageContent;\n```\n\n----------------------------------------\n\nTITLE: Creating a Couchbase Document Loader in LangChain.js\nDESCRIPTION: Instantiating a CouchbaseDocumentLoader with the connected Couchbase client and SQL++ query to retrieve documents from the database.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/couchbase.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst loader = new CouchbaseDocumentLoader(\n  couchbaseClient, // The connected couchbase cluster client\n  query // A valid SQL++ query which will return the required data\n);\n```\n\n----------------------------------------\n\nTITLE: Setting Anthropic API Key for Stagehand Toolkit\nDESCRIPTION: This command sets the Anthropic API key as an environment variable, which is required for using Anthropic models with the Stagehand Toolkit.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/stagehand.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport ANTHROPIC_API_KEY=\"your-anthropic-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Using Web Search Connector with ChatCohere\nDESCRIPTION: Demonstrates how to use Cohere's web-search connector to search the web for relevant information when generating responses.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/cohere.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatCohere } from \"@langchain/cohere\";\nimport { HumanMessage } from \"@langchain/core/messages\";\n\nconst llmWithConnectors = new ChatCohere({\n  apiKey: process.env.COHERE_API_KEY, // Default\n});\n\nconst connectorsRes = await llmWithConnectors.invoke(\n  [new HumanMessage(\"How tall are the largest pengiuns?\")],\n  {\n    connectors: [{ id: \"web-search\" }],\n  }\n);\nconsole.dir(connectorsRes, { depth: null });\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for IAM Authentication in Bash\nDESCRIPTION: Sets up environment variables for IAM authentication with IBM watsonx.ai.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_compressors/ibm.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport WATSONX_AI_AUTH_TYPE=iam\nexport WATSONX_AI_APIKEY=<YOUR-APIKEY>\n```\n\n----------------------------------------\n\nTITLE: Querying the PGVectorStore\nDESCRIPTION: Illustrates how to perform a similarity search in the PGVectorStore, using filter criteria to narrow down results.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/pgvector.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconst filter = { source: \"https://example.com\" };\n\nconst similaritySearchResults = await vectorStore.similaritySearch(\"biology\", 2, filter);\n\nfor (const doc of similaritySearchResults) {\n  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);\n}\n```\n\n----------------------------------------\n\nTITLE: Streaming with Runnable\nDESCRIPTION: Demonstrates streaming functionality with a generator function using RunnableLambda.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/lcel_cheatsheet.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { RunnableLambda } from \"@langchain/core/runnables\";\n\nasync function* generatorFn(x: number[]) {\n  for (const i of x) {\n    yield i.toString();\n  }\n}\n\nconst runnable = RunnableLambda.from(generatorFn);\n\nconst stream = await runnable.stream([0, 1, 2, 3, 4]);\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n  console.log(\"---\")\n}\n```\n\n----------------------------------------\n\nTITLE: Running Custom JSON Extraction Chain\nDESCRIPTION: Creates and executes a custom extraction chain that passes the query and schema to the model, then extracts structured JSON from the response using the custom extraction function.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/extraction_parse.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nconst customParsingChain = prompt.pipe(model).pipe(extractJsonFromOutput);\n\nawait customParsingChain.invoke({\n  schema: zodToJsonSchema(peopleSchema),\n  customParsingQuery,\n});\n```\n\n----------------------------------------\n\nTITLE: Splitting PHP Code with RecursiveCharacterTextSplitter\nDESCRIPTION: Demonstrates how to split PHP code using RecursiveCharacterTextSplitter with PHP-specific separators. Handles PHP constructs like classes, functions, interfaces, traits, and enums with appropriate splits.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/code_splitter.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst PHP_CODE = `<?php\nnamespace foo;\nclass Hello {\n    public function __construct() { }\n}\nfunction hello() {\n    echo \"Hello World!\";\n}\ninterface Human {\n    public function breath();\n}\ntrait Foo { }\nenum Color\n{\n    case Red;\n    case Blue;}`\n\nconst phpSplitter = RecursiveCharacterTextSplitter.fromLanguage(\n    \"php\", {\n        chunkSize: 50,\n        chunkOverlap: 0,\n    } \n)\nconst phpDocs = await phpSplitter.createDocuments([PHP_CODE])\n\nphpDocs\n```\n\n----------------------------------------\n\nTITLE: Instantiating ChatNovitaAI Model\nDESCRIPTION: Demonstrates how to import and instantiate a ChatNovitaAI model with configuration parameters such as model name and temperature.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/novita.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatNovitaAI } from \"@langchain/community/chat_models/novita\";\n\nconst llm = new ChatNovitaAI({\n  model: \"deepseek/deepseek-r1\",\n  temperature: 0,\n  // other params...\n})\n```\n\n----------------------------------------\n\nTITLE: Initializing SQLite Database Connection with LangChain\nDESCRIPTION: This code snippet initializes a connection to the Chinook SQLite database using LangChain's SqlDatabase class and TypeORM's DataSource.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/sql_qa.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { SqlDatabase } from \"langchain/sql_db\";\nimport { DataSource } from \"typeorm\";\n\nconst datasource = new DataSource({\n  type: \"sqlite\",\n  database: \"Chinook.db\",\n});\nconst db = await SqlDatabase.fromDataSourceParams({\n  appDataSource: datasource,\n});\n\nawait db.run(\"SELECT * FROM Artist LIMIT 10;\");\n```\n\n----------------------------------------\n\nTITLE: Single Text Embedding Example\nDESCRIPTION: Demonstration of embedding a single text using embedQuery method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/ollama.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst singleVector = await embeddings.embedQuery(text);\n\nconsole.log(singleVector.slice(0, 100));\n```\n\n----------------------------------------\n\nTITLE: Creating a Dynamic Few Shot Prompt Template in LangChain.js\nDESCRIPTION: This snippet shows how to create a dynamic few shot prompt template using the semantic similarity example selector. It configures the template to select relevant examples based on the input and format them as chat messages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot_examples_chat.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport {\n    ChatPromptTemplate,\n    FewShotChatMessagePromptTemplate,\n} from \"@langchain/core/prompts\"\n\n// Define the few-shot prompt.\nconst fewShotPrompt = new FewShotChatMessagePromptTemplate({\n    // The input variables select the values to pass to the example_selector\n    inputVariables: [\"input\"],\n    exampleSelector,\n    // Define how ech example will be formatted.\n    // In this case, each example will become 2 messages:\n    // 1 human, and 1 AI\n    examplePrompt: ChatPromptTemplate.fromMessages(\n        [[\"human\", \"{input}\"], [\"ai\", \"{output}\"]]\n    ),\n})\n\nconst results = await fewShotPrompt.invoke({ input: \"What's 3+3?\" });\nconsole.log(results.toChatMessages())\n```\n\n----------------------------------------\n\nTITLE: Loading Documents from CSV File\nDESCRIPTION: Loads documents from the CSV file and retrieves the first document from the returned array.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/csv.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst docs = await loader.load()\ndocs[0]\n```\n\n----------------------------------------\n\nTITLE: Following Up with a Question Using Persisted State\nDESCRIPTION: Shows how to ask a follow-up question using the same thread_id, allowing the LangGraph app to access the previous conversation context and respond appropriately.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_memory.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nawait app.invoke(\n  {\n    messages: [\n      {\n        role: \"user\",\n        content: \"What did I just ask you?\"\n      }\n    ]\n  },\n  {\n    configurable: { thread_id: \"1\" }\n  }\n);\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatOpenAI Model for Chaining (TypeScript)\nDESCRIPTION: Initializes a `ChatOpenAI` language model instance, specifying the model name (`gpt-4o-mini`). This model will be used later in the chain to process user input and decide when to call the Exa tool.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/exa_search.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n// @lc-docs-hide-cell\n\nimport { ChatOpenAI } from \"@langchain/openai\"\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o-mini\",\n})\n```\n\n----------------------------------------\n\nTITLE: Logging ChatCohere Response Content\nDESCRIPTION: Logs the content of the AI message response to the console.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/cohere.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(aiMsg.content)\n```\n\n----------------------------------------\n\nTITLE: Invoking a Chain in TypeScript\nDESCRIPTION: Demonstrates a simple chain invocation with a weather-related query. This example shows the normal operation of the chain without cancellation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/cancel_execution.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nawait chain.invoke(\"what is the current weather in SF?\");\n```\n\n----------------------------------------\n\nTITLE: Streaming Output from LlamaCpp Model\nDESCRIPTION: Example showing how to implement streaming responses from a local Llama model, using LangChain's streaming capabilities to process tokens incrementally as they are generated.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/llama_cpp.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LlamaCpp } from \"@langchain/community/llms/llama_cpp\";\n\n// Path to the model\nconst llm = new LlamaCpp({\n  modelPath: \"./models/llama-3-8b.Q4_0.gguf\",\n  temperature: 0.7,\n  maxTokens: 2000,\n  topP: 1,\n});\n\nconst stream = await llm.stream(\n  \"Write an article about the difference between LLM and LLAMA\"\n);\n\nfor await (const chunk of stream) {\n  process.stdout.write(chunk);\n}\n```\n\n----------------------------------------\n\nTITLE: Indexing and Retrieving with Memory Vector Store\nDESCRIPTION: Example of creating a vector store from documents and using it as a retriever to find similar content.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/bytedance_doubao.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n// Create a vector store with a sample text\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n\nconst text = \"LangChain is the framework for building context-aware reasoning applications\";\n\nconst vectorstore = await MemoryVectorStore.fromDocuments(\n  [{ pageContent: text, metadata: {} }],\n  embeddings,\n);\n\n// Use the vector store as a retriever that returns a single document\nconst retriever = vectorstore.asRetriever(1);\n\n// Retrieve the most similar text\nconst retrievedDocuments = await retriever.invoke(\"What is LangChain?\");\n\nretrievedDocuments[0].pageContent;\n```\n\n----------------------------------------\n\nTITLE: Instantiating Self-Query Retriever\nDESCRIPTION: Creation and configuration of a self-query retriever using Pinecone translator and defined attributes.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/pinecone.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { SelfQueryRetriever } from \"langchain/retrievers/self_query\";\nimport { PineconeTranslator } from \"@langchain/pinecone\";\n\nconst selfQueryRetriever = SelfQueryRetriever.fromLLM({\n  llm: llm,\n  vectorStore: vectorStore,\n  documentContents: \"Brief summary of a movie\",\n  attributeInfo: attributeInfo,\n  structuredQueryTranslator: new PineconeTranslator(),\n});\n```\n\n----------------------------------------\n\nTITLE: Using MessagesPlaceholder for Dynamic Message Arrays\nDESCRIPTION: Demonstrates how to use MessagesPlaceholder to dynamically insert an array of messages into a specific position in the prompt template.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/anthropic.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nimport {\n  ChatPromptTemplate,\n  MessagesPlaceholder,\n} from \"@langchain/core/prompts\";\nimport { HumanMessage } from \"@langchain/core/messages\";\n\nconst promptTemplate = ChatPromptTemplate.fromMessages([\n  [\"system\", \"You are a helpful assistant\"],\n  new MessagesPlaceholder(\"msgs\"),\n]);\n\npromptTemplate.invoke({ msgs: [new HumanMessage({ content: \"hi!\" })] });\n```\n\n----------------------------------------\n\nTITLE: Llama CPP Multi-Message Streaming\nDESCRIPTION: Implementation of streaming with multiple messages using Llama3 formatted prompts\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/llama_cpp.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nStreamMultiExample\n```\n\n----------------------------------------\n\nTITLE: Implementing a Basic Custom LLM Class in TypeScript\nDESCRIPTION: Creates a simple custom LLM that echoes back the first 'n' characters of input text. Implements the required _call method and optional streaming functionality with _streamResponseChunks.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/custom_llm.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LLM, type BaseLLMParams } from \"@langchain/core/language_models/llms\";\nimport type { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\nimport { GenerationChunk } from \"@langchain/core/outputs\";\n\ninterface CustomLLMInput extends BaseLLMParams {\n  n: number;\n}\n\nclass CustomLLM extends LLM {\n  n: number;\n\n  constructor(fields: CustomLLMInput) {\n    super(fields);\n    this.n = fields.n;\n  }\n\n  _llmType() {\n    return \"custom\";\n  }\n\n  async _call(\n    prompt: string,\n    options: this[\"ParsedCallOptions\"],\n    runManager: CallbackManagerForLLMRun\n  ): Promise<string> {\n    // Pass `runManager?.getChild()` when invoking internal runnables to enable tracing\n    // await subRunnable.invoke(params, runManager?.getChild());\n    return prompt.slice(0, this.n);\n  }\n\n  async *_streamResponseChunks(\n    prompt: string,\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): AsyncGenerator<GenerationChunk> {\n    // Pass `runManager?.getChild()` when invoking internal runnables to enable tracing\n    // await subRunnable.invoke(params, runManager?.getChild());\n    for (const letter of prompt.slice(0, this.n)) {\n      yield new GenerationChunk({\n        text: letter,\n      });\n      // Trigger the appropriate callback\n      await runManager?.handleLLMNewToken(letter);\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating and Querying Vector Stores in JavaScript\nDESCRIPTION: This snippet demonstrates how to create and query vector stores in LangChain using different implementations. It shows how to create indexes with MemoryVectorStore, load documents into vector stores, and create vector stores from text chunks directly.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/static/llms.txt#2025-04-22_snippet_6\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nconst vectorStore = await MemoryVectorStore.fromTexts(\n  [\"Hello world\", \"Bye bye\", \"Hello nice world\"],\n  [{ id: 2 }, { id: 1 }, { id: 3 }],\n  new OpenAIEmbeddings()\n);\n\nconst resultOne = await vectorStore.similaritySearch(\"hello world\", 1);\nconsole.log(resultOne);\n```\n\n----------------------------------------\n\nTITLE: Installing Rockset Client with Yarn - Bash\nDESCRIPTION: This snippet shows how to install the Rockset client library using yarn. No additional configuration is required beyond running this command in the terminal of your JavaScript/TypeScript project. It ensures the @rockset/client package is added to your dependencies, enabling integration with Rockset's cloud-based SQL analytics and vector search features.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/rockset.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nyarn add @rockset/client\n```\n\n----------------------------------------\n\nTITLE: Standard Chat Implementation with Baidu Qianfan\nDESCRIPTION: Example of implementing basic chat functionality using ChatBaiduQianfan model. The code demonstrates standard message exchange without streaming.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/baidu_qianfan.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nChatBaiduQianfanExample\n```\n\n----------------------------------------\n\nTITLE: Instantiating ChatBedrockConverse with AWS Credentials\nDESCRIPTION: Creates a new instance of ChatBedrockConverse using AWS credentials from environment variables. This example uses the Claude 3.5 Sonnet model and configures the AWS region and authentication credentials.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/bedrock_converse.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatBedrockConverse } from \"@langchain/aws\";\n\nconst llm = new ChatBedrockConverse({\n  model: \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n  region: process.env.BEDROCK_AWS_REGION,\n  credentials: {\n    accessKeyId: process.env.BEDROCK_AWS_ACCESS_KEY_ID!,\n    secretAccessKey: process.env.BEDROCK_AWS_SECRET_ACCESS_KEY!,\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Loading Documents from Confluence using TypeScript in Node.js\nDESCRIPTION: This code demonstrates how to use the ConfluenceLoader to fetch and process documents from a Confluence space. It includes setting up authentication, specifying the space key, and configuring the loader with various options.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/confluence.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ConfluenceLoader } from \"@langchain/community/document_loaders/web/confluence\";\n\nconst loader = new ConfluenceLoader({\n  url: \"https://example.atlassian.net/wiki\",\n  username: \"your-username@example.com\",\n  accessToken: \"your-access-token\",\n  spaceKey: \"~EXAMPLE362906de5d343d49dcdbae5dEXAMPLE\",\n});\n\nconst documents = await loader.load();\n\nconsole.log({ documents });\n\n/*\n  You can also pass options to load specific documents, limit the number of documents returned, and exclude specific document types:\n*/\n\nconst localDocs = await loader.load({\n  limit: 10,\n  include: [\"page\", \"blogpost\"],\n  exclude: [\"attachment\"],\n});\n\nconsole.log({ localDocs });\n```\n\n----------------------------------------\n\nTITLE: Adding a Single Document to Matching Engine (TypeScript)\nDESCRIPTION: Shows the basic process of adding a single `Document` object to the initialized `MatchingEngine`. The `addDocuments` method takes an array of documents; here, it's used with one document containing simple page content.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/googlevertexai.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst doc = new Document({ pageContent: \"this\" });\nawait engine.addDocuments([doc]);\n```\n\n----------------------------------------\n\nTITLE: Invoking a Runnable in LCEL\nDESCRIPTION: Demonstrates basic invocation of a runnable that converts a number to string using RunnableLambda.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/lcel_cheatsheet.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport { RunnableLambda } from \"@langchain/core/runnables\";\n\nconst runnable = RunnableLambda.from((x: number) => x.toString());\n\nawait runnable.invoke(5);\n```\n\n----------------------------------------\n\nTITLE: Creating Basic String Prompt Template in TypeScript\nDESCRIPTION: Demonstrates how to create and use a simple string-based PromptTemplate that takes a single variable input and formats it into a prompt string.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/anthropic.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PromptTemplate } from \"@langchain/core/prompts\";\n\nconst promptTemplate = PromptTemplate.fromTemplate(\n  \"Tell me a joke about {topic}\"\n);\n\nawait promptTemplate.invoke({ topic: \"cats\" });\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom JSON Schema Extraction\nDESCRIPTION: Sets up a custom extraction method using Zod schemas converted to JSON schema. Includes a system prompt template that instructs the model to return JSON matching the specified schema and a function to extract JSON from the model's response.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/extraction_parse.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { z } from \"zod\";\nimport { zodToJsonSchema } from \"zod-to-json-schema\";\n\npersonSchema = z.object({\n  name: z.optional(z.string()).describe(\"The name of the person\"),\n  hair_color: z.optional(z.string()).describe(\"The color of the person's hair, if known\"),\n  height_in_meters: z.optional(z.string()).describe(\"Height measured in meters\")\n}).describe(\"Information about a person.\");\n\nconst peopleSchema = z.object({\n  people: z.array(personSchema),\n});\n\nconst SYSTEM_PROMPT_TEMPLATE = [\n  \"Answer the user's query. You must return your answer as JSON that matches the given schema:\",\n  \"```json\\n{schema}\\n```.\",\n  \"Make sure to wrap the answer in ```json and ``` tags. Conform to the given schema exactly.\",\n].join(\"\\n\");\n\nconst customParsingPrompt = ChatPromptTemplate.fromMessages([\n  [\"system\", SYSTEM_PROMPT_TEMPLATE],\n  [\"human\", \"{query}\"],\n]);\n\nconst extractJsonFromOutput = (message) => {\n  const text = message.content;\n\n  // Define the regular expression pattern to match JSON blocks\n  const pattern = /```json\\s*((.|\n)*?)\\s*```/gs;\n\n  // Find all non-overlapping matches of the pattern in the string\n  const matches = pattern.exec(text);\n\n  if (matches && matches[1]) {\n    try {\n      return JSON.parse(matches[1].trim());\n    } catch (error) {\n      throw new Error(`Failed to parse: ${matches[1]}`);\n    }\n  } else {\n    throw new Error(`No JSON found in: ${message}`);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search in MongoDB Vector Store (TypeScript)\nDESCRIPTION: Executes a basic similarity search query against the vector store. It uses the `similaritySearch` method, providing a query string ('biology') and the desired number of results (k=2). The results are documents ranked by vector similarity.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/mongodb_atlas.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst similaritySearchResults = await vectorStore.similaritySearch(\"biology\", 2);\n\nfor (const doc of similaritySearchResults) {\n  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);\n}\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search on RedisVectorStore in TypeScript\nDESCRIPTION: This snippet shows the execution of a similarity search over the vector store. Given a query string and a limit on results, it retrieves and logs similar documents based on the indexed vectors.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/redis.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconst similaritySearchResults = await vectorStore.similaritySearch(\"biology\", 2);\n\nfor (const doc of similaritySearchResults) {\n  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);\n}\n```\n\n----------------------------------------\n\nTITLE: Invoking the Model with Summary Memory in Python\nDESCRIPTION: This snippet demonstrates how to invoke the model with the implemented summary memory, passing in the chat history and a new user message. It uses a configurable thread ID for persistence.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_memory.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nawait app3.invoke(\n  {\n    messages: [\n      ...demoEphemeralChatHistory2,\n      { role: \"user\", content: \"What did I say my name was?\" }\n    ]\n  },\n  {\n    configurable: { thread_id: \"4\" }\n  }\n);\n```\n\n----------------------------------------\n\nTITLE: Invoking TavilySearch Tool with a ToolCall Object\nDESCRIPTION: Shows how to invoke the TavilySearch tool using a model-generated ToolCall object that contains the search query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/tavily_search.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n// This is usually generated by a model, but we'll create a tool call directly for demo purposes.\nconst modelGeneratedToolCall = {\n  args: {\n    query: \"what is the current weather in SF?\"\n  },\n  id: \"1\",\n  name: tool.name,\n  type: \"tool_call\",\n}\n\nawait tool.invoke(modelGeneratedToolCall)\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Environment Variable (TypeScript)\nDESCRIPTION: This snippet shows how to set the OpenAI API key as an environment variable (`OPENAI_API_KEY`) in TypeScript. This is required if using OpenAI embeddings with the Pinecone vector store.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/pinecone.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.OPENAI_API_KEY = \"YOUR_API_KEY\";\n```\n\n----------------------------------------\n\nTITLE: Using GradientEmbeddings for Text Embedding (TypeScript)\nDESCRIPTION: This code block component renders a complete usage example of `GradientEmbeddings`. The actual code content is imported from the `@examples/embeddings/gradient_ai.ts` file and demonstrates how to utilize the initialized `GradientEmbeddings` instance to generate embeddings for text.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/gradient_ai.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n<CodeBlock language=\"typescript\">{GradientEmbeddingsExample}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Creating a Chain for Image Description\nDESCRIPTION: This snippet shows how to create a chain by piping the prompt template to the ChatOpenAI model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/multimodal_prompts.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst chain = prompt.pipe(model);\n```\n\n----------------------------------------\n\nTITLE: Basic Llama CPP Chat Example\nDESCRIPTION: Basic implementation showing how to use Llama CPP for chat with a single message prompt\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/llama_cpp.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nBasicExample\n```\n\n----------------------------------------\n\nTITLE: Displaying HuggingFace TransformerEmbeddings Example (TypeScript)\nDESCRIPTION: This markdown code block displays a TypeScript example (`HFTransformersExample`) demonstrating the usage of `TransformerEmbeddings` from `@langchain/community/embeddings/hf_transformers`. The actual TypeScript code, imported from `@examples/models/embeddings/hf_transformers.ts`, would show how to instantiate the class and generate embeddings for text. The surrounding text advises using web workers for browser environments to avoid blocking the main thread during inference.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/transformers.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n<CodeBlock language=\"typescript\">{HFTransformersExample}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Headers to AzureOpenAIEmbeddings\nDESCRIPTION: Demonstrates how to add custom headers to requests made by AzureOpenAIEmbeddings using the configuration option.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/azure_openai.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport { AzureOpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddingsWithCustomHeaders = new AzureOpenAIEmbeddings({\n  azureOpenAIApiKey: \"<your_key>\",\n  azureOpenAIApiInstanceName: \"<your_instance_name>\",\n  azureOpenAIApiEmbeddingsDeploymentName: \"<your_embeddings_deployment_name>\",\n  azureOpenAIApiVersion: \"<api_version>\",\n  configuration: {\n    defaultHeaders: {\n      \"x-custom-header\": `SOME_VALUE`,\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Credentials\nDESCRIPTION: This snippet shows how to set environment variables for OpenAI API keys and optional LangSmith tracing keys in a TypeScript environment.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/pgvector.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.OPENAI_API_KEY = \"YOUR_API_KEY\";\n```\n\n----------------------------------------\n\nTITLE: Setting Cohere API Key in Bash\nDESCRIPTION: This snippet shows how to set the COHERE_API_KEY environment variable in Bash. It's a prerequisite for using CohereEmbeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/cohere.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport COHERE_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Environment Variables Configuration for LangChain\nDESCRIPTION: Sets up required API keys for OpenAI and optional LangSmith configuration for observability.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_multiple_queries.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nOPENAI_API_KEY=your-api-key\n\n# Optional, use LangSmith for best-in-class observability\nLANGSMITH_API_KEY=your-api-key\nLANGSMITH_TRACING=true\n\n# Reduce tracing latency if you are not in a serverless environment\n# LANGCHAIN_CALLBACKS_BACKGROUND=true\n```\n\n----------------------------------------\n\nTITLE: Running the LangGraph App with Recursion Limit\nDESCRIPTION: Executes the LangGraph application with a specified recursion limit to prevent infinite loops. Streams the results and logs each step while capturing the final summary.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/summarization.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nlet finalSummary = null;\n\nfor await (\n  const step of await app.stream(\n    {contents: splitDocs.map(doc => doc.pageContent)},\n    { recursionLimit: 10 }\n  )\n) {\n  console.log(Object.keys(step));\n  if (step.hasOwnProperty(\"generateFinalSummary\")) {\n      finalSummary = step.generateFinalSummary\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: YandexGPT Chat Model Implementation Example\nDESCRIPTION: Example code showing how to implement YandexGPT chat models in LangChain.js. The example references an integration file that demonstrates the full implementation details.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/yandex.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nYandexGPTChatExample\n```\n\n----------------------------------------\n\nTITLE: Retrieving Messages from PostgreSQL Chat History\nDESCRIPTION: Code to fetch all messages stored in the chat history. Returns an array of BaseMessage objects that were previously saved in the PostgreSQL database.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/google_cloudsql_pg.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst messagesSaved: BaseMessage[] = await historyInstance.getMessages();\nconsole.log(messagesSaved);\n```\n\n----------------------------------------\n\nTITLE: Instantiating AzureChatOpenAI\nDESCRIPTION: Creates an instance of the AzureChatOpenAI model with configuration parameters like model name, temperature, tokens, and Azure-specific parameters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/azure.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { AzureChatOpenAI } from \"@langchain/openai\" \n\nconst llm = new AzureChatOpenAI({\n    model: \"gpt-4o\",\n    temperature: 0,\n    maxTokens: undefined,\n    maxRetries: 2,\n    azureOpenAIApiKey: process.env.AZURE_OPENAI_API_KEY, // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY\n    azureOpenAIApiInstanceName: process.env.AZURE_OPENAI_API_INSTANCE_NAME, // In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME\n    azureOpenAIApiDeploymentName: process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME, // In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME\n    azureOpenAIApiVersion: process.env.AZURE_OPENAI_API_VERSION, // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION\n})\n```\n\n----------------------------------------\n\nTITLE: Using SemanticSimilarityExampleSelector in LangChain.js\nDESCRIPTION: This code demonstrates the basic usage of SemanticSimilarityExampleSelector to select examples based on semantic similarity to the input.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/example_selectors_similarity.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport ExampleSimilarity from \"@examples/prompts/semantic_similarity_example_selector.ts\";\n```\n\n----------------------------------------\n\nTITLE: Using ExaRetriever for Query Invocation in TypeScript\nDESCRIPTION: This snippet shows how to use the ExaRetriever to invoke a query and retrieve results.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/exa.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst query = \"What did the speaker say about Justice Breyer in the 2022 State of the Union?\";\n\nawait retriever.invoke(query);\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Cypher Generating Chain with LangChain\nDESCRIPTION: This snippet sets up a custom Cypher generating chain using LangChain's expression language. It combines entity mapping, schema information, and the user's question to construct a Cypher statement.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/graph_mapping.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\nimport { RunnablePassthrough, RunnableSequence } from \"@langchain/core/runnables\";\n\n// Generate Cypher statement based on natural language input\nconst cypherTemplate = `Based on the Neo4j graph schema below, write a Cypher query that would answer the user's question:\n{schema}\nEntities in the question map to the following database values:\n{entities_list}\nQuestion: {question}\nCypher query:`\n\nconst cypherPrompt = ChatPromptTemplate.fromMessages(\n    [\n        [\n            \"system\",\n            \"Given an input question, convert it to a Cypher query. No pre-amble.\",\n        ],\n        [\"human\", cypherTemplate]\n    ]\n)\n\nconst llmWithStop = llm.bind({ stop: [\"\\nCypherResult:\"] })\n\nconst cypherResponse = RunnableSequence.from([\n    RunnablePassthrough.assign({ names: entityChain }),\n    RunnablePassthrough.assign({\n        entities_list: async (x) => matchToDatabase(x.names),\n        schema: async (_) => graph.getSchema(),\n    }),\n    cypherPrompt,\n    llmWithStop,\n    new StringOutputParser(),\n])\n```\n\n----------------------------------------\n\nTITLE: Embedding Single Text with WatsonxEmbeddings\nDESCRIPTION: Shows how to embed a single query text using the embedQuery method of WatsonxEmbeddings. This generates a vector representation specific to the query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/ibm.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst singleVector = await embeddings.embedQuery(text);\nsingleVector.slice(0, 10);\n```\n\n----------------------------------------\n\nTITLE: Installing ZhipuAI Dependencies using NPM/Yarn\nDESCRIPTION: Installs the necessary Node.js packages (`@langchain/community`, `@langchain/core`, `jsonwebtoken`) required to use the ZhipuAI embeddings integration within LangchainJS. This command should be run in your project's terminal. Requires Node.js and either npm or yarn installed.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/zhipuai.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core jsonwebtoken\n```\n\n----------------------------------------\n\nTITLE: Implementing Token-Based Rate Limiting with OpenAI in TypeScript\nDESCRIPTION: Demonstrate token-based rate limiting using Upstash Ratelimit in a LangChain.js chain that includes an OpenAI LLM. This example limits to 500 tokens per minute.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/callbacks/upstash_ratelimit_callback.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst UPSTASH_REDIS_REST_URL = \"****\";\nconst UPSTASH_REDIS_REST_TOKEN = \"****\";\nconst OPENAI_API_KEY = \"****\";\n\nimport {\n  UpstashRatelimitHandler,\n  UpstashRatelimitError,\n} from \"@langchain/community/callbacks/handlers/upstash_ratelimit\";\nimport { RunnableLambda, RunnableSequence } from \"@langchain/core/runnables\";\nimport { OpenAI } from \"@langchain/openai\";\nimport { Ratelimit } from \"@upstash/ratelimit\";\nimport { Redis } from \"@upstash/redis\";\n\n// create ratelimit\nconst ratelimit = new Ratelimit({\n  redis: new Redis({\n    url: UPSTASH_REDIS_REST_URL,\n    token: UPSTASH_REDIS_REST_TOKEN,\n  }),\n  // 500 tokens per window, where window size is 60 seconds:\n  limiter: Ratelimit.fixedWindow(500, \"60 s\"),\n});\n\n// create handler\nconst user_id = \"user_id\"; // should be a method which gets the user id\nconst handler = new UpstashRatelimitHandler(user_id, {\n  tokenRatelimit: ratelimit,\n});\n\n// create mock chain\nconst asStr = new RunnableLambda({ func: (str: string): string => str });\nconst model = new OpenAI({\n  apiKey: OPENAI_API_KEY,\n});\nconst chain = RunnableSequence.from([asStr, model]);\n\n// invoke chain with handler:\ntry {\n  const response = await chain.invoke(\"hello world\", {\n    callbacks: [handler],\n  });\n  console.log(response);\n} catch (err) {\n  if (err instanceof UpstashRatelimitError) {\n    console.log(\"Handling ratelimit.\");\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Loading files from Azure Blob Storage into LangChain documents\nDESCRIPTION: Example code showing how to use the AzureBlobStorageFileLoader to load files from Azure Blob Storage and convert them into LangChain documents. The example includes connecting to Azure storage and loading a PDF file.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/azure_blob_storage_file.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AzureBlobStorageFileLoader } from \"@langchain/community/document_loaders/web/azure_blob_storage_file\";\n\nconst loader = new AzureBlobStorageFileLoader({\n  azureConfig: {\n    connectionString: \"<connection string>\",\n    container: \"<container name>\",\n    blobName: \"example.pdf\",\n  },\n  unstructuredConfig: {\n    apiURL: \"http://localhost:8000/general/v0/general\",\n  },\n});\n\nconst docs = await loader.load();\n```\n\n----------------------------------------\n\nTITLE: Streaming with Anthropic Chat Model\nDESCRIPTION: Example of using streaming capabilities with the ChatAnthropic model\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-anthropic/README.md#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst model = new ChatAnthropic({\n  apiKey: process.env.ANTHROPIC_API_KEY,\n  model: \"claude-3-sonnet-20240229\",\n});\nconst response = await model.stream({\n  role: \"user\",\n  content: \"Hello world!\",\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Summarization Tool\nDESCRIPTION: Implements a custom tool that summarizes text using a chat model and reverses the output. This is the naive implementation that doesn't properly handle event streaming.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_stream_events.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { z } from \"zod\";\nimport { tool } from \"@langchain/core/tools\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\n\nconst specialSummarizationTool = tool(async (input) => {\n  const prompt = ChatPromptTemplate.fromTemplate(\n    \"You are an expert writer. Summarize the following text in 10 words or less:\\n\\n{long_text}\"\n  );\n  const reverse = (x: string) => {\n    return x.split(\"\").reverse().join(\"\");\n  };\n  const chain = prompt\n    .pipe(model)\n    .pipe(new StringOutputParser())\n    .pipe(reverse);\n  const summary = await chain.invoke({ long_text: input.long_text });\n  return summary;\n}, {\n  name: \"special_summarization_tool\",\n  description: \"A tool that summarizes input text using advanced techniques.\",\n  schema: z.object({\n    long_text: z.string(),\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain JS Packages (Shell)\nDESCRIPTION: Installs the required LangChain core, OpenAI integration, and main LangChain packages using npm or yarn. These packages are prerequisites for using `MemoryVectorStore` with OpenAI embeddings as shown in the guide.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/memory.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nlangchain @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Instantiating ChatFireworks Model\nDESCRIPTION: Creating a ChatFireworks model instance with configuration options like model name, temperature, and retry settings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/fireworks.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatFireworks } from \"@langchain/community/chat_models/fireworks\" \n\nconst llm = new ChatFireworks({\n    model: \"accounts/fireworks/models/llama-v3p1-70b-instruct\",\n    temperature: 0,\n    maxTokens: undefined,\n    timeout: undefined,\n    maxRetries: 2,\n    // other params...\n})\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Software Authentication\nDESCRIPTION: Configuration of environment variables for IBM watsonx.ai software authentication method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/ibm.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport WATSONX_AI_AUTH_TYPE=cp4d\nexport WATSONX_AI_USERNAME=<YOUR_USERNAME>\nexport WATSONX_AI_PASSWORD=<YOUR_PASSWORD>\nexport WATSONX_AI_URL=<URL>\n```\n\n----------------------------------------\n\nTITLE: Indexing and Retrieving with MemoryVectorStore in Python\nDESCRIPTION: This code snippet shows how to use CohereEmbeddings with MemoryVectorStore for indexing and retrieving documents. It demonstrates creating a vector store, using it as a retriever, and retrieving similar text.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/cohere.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n// Create a vector store with a sample text\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n\nconst text = \"LangChain is the framework for building context-aware reasoning applications\";\n\nconst vectorstore = await MemoryVectorStore.fromDocuments(\n  [{ pageContent: text, metadata: {} }],\n  embeddings,\n);\n\n// Use the vector store as a retriever that returns a single document\nconst retriever = vectorstore.asRetriever(1);\n\n// Retrieve the most similar text\nconst retrievedDocuments = await retriever.invoke(\"What is LangChain?\");\n\nretrievedDocuments[0].pageContent;\n```\n\n----------------------------------------\n\nTITLE: Setting Up Query Generation with LangChain in JavaScript\nDESCRIPTION: This snippet sets up the query generation process using LangChain components, including ChatPromptTemplate and RunnableSequence.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_few_shot.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\"\nimport { RunnablePassthrough, RunnableSequence } from \"@langchain/core/runnables\"\n\nconst system = `You are an expert at converting user questions into database queries.\nYou have access to a database of tutorial videos about a software library for building LLM-powered applications.\nGiven a question, return a list of database queries optimized to retrieve the most relevant results.\n\nIf there are acronyms or words you are not familiar with, do not try to rephrase them.`\n\nconst prompt = ChatPromptTemplate.fromMessages(\n[\n    [\"system\", system],\n    [\"placeholder\", \"{examples}\"],\n    [\"human\", \"{question}\"],\n]\n)\nconst llmWithTools = llm.withStructuredOutput(searchSchema, {\n  name: \"Search\",\n})\nconst queryAnalyzer = RunnableSequence.from([\n  {\n    question: new RunnablePassthrough(),\n  },\n  prompt,\n  llmWithTools\n]);\n```\n\n----------------------------------------\n\nTITLE: Deleting Documents from Chroma Vector Store by ID (TypeScript)\nDESCRIPTION: This snippet shows how to remove a document from the Chroma vector store by providing its unique ID. The asynchronous delete method takes an object with the IDs of documents to delete. This operation is crucial for pruning datasets or correcting errors.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/chroma.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nawait vectorStore.delete({ ids: [\"4\"] });\n```\n\n----------------------------------------\n\nTITLE: Basic Model Invocation\nDESCRIPTION: Demonstrates how to make a basic completion request to the model using the invoke method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/llms.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst inputText = \"__module_name__ is an AI company that \"\n\nconst completion = await llm.invoke(inputText)\ncompletion\n```\n\n----------------------------------------\n\nTITLE: Propagating Metadata with CharacterTextSplitter\nDESCRIPTION: Shows how to maintain metadata when splitting documents. This example assigns document identifiers (1 and 2) as metadata to the source documents, which are then preserved in each resulting chunk.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/character_text_splitter.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nconst metadatas = [{ document: 1 }, { document: 2 }];\n\nconst documents = await textSplitter.createDocuments(\n    [stateOfTheUnion, stateOfTheUnion], metadatas\n)\n\nconsole.log(documents[0])\n```\n\n----------------------------------------\n\nTITLE: Instantiating ChatTogetherAI Model\nDESCRIPTION: Creates an instance of the ChatTogetherAI model using the Mixtral-8x7B-Instruct model with a temperature setting of 0.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/togetherai.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatTogetherAI } from \"@langchain/community/chat_models/togetherai\"\n\nconst llm = new ChatTogetherAI({\n    model: \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n    temperature: 0,\n    // other params...\n})\n```\n\n----------------------------------------\n\nTITLE: LangChain ToolMessage Constructor with Calculator Result\nDESCRIPTION: JSON constructor for a LangChain ToolMessage showing the calculator tool's response after processing the '52 * 365' calculation. The result '18980' is returned in the content field.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_37\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"lc\": 1,\n  \"type\": \"constructor\",\n  \"id\": [\n    \"langchain_core\",\n    \"messages\",\n    \"ToolMessage\"\n  ],\n  \"kwargs\": {\n    \"tool_call_id\": \"toolu_01NVTbm5aNYSm1wGYb6XF7jE\",\n    \"content\": \"18980\",\n    \"additional_kwargs\": {\n      \"name\": \"calculator\"\n    },\n    \"response_metadata\": {}\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Accessing ChatOpenAI Model Output Chunk in JavaScript\nDESCRIPTION: This code accesses the first chunk of the streamed output from the ChatOpenAI model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nchunks[0]\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Perplexity API Access\nDESCRIPTION: Commands for setting the necessary environment variables to use Perplexity API, including the API key and optional LangSmith tracing configuration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/perplexity.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport PERPLEXITY_API_KEY=\"your-api-key\"\n```\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Initializing Chat Model\nDESCRIPTION: Setup of the Anthropic chat model with specific configuration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/extraction.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst llm = new ChatAnthropic({\n  model: \"claude-3-sonnet-20240229\",\n  temperature: 0\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing Vector Store for Name Matching\nDESCRIPTION: Sets up a vector store using OpenAI embeddings to find similar names and create filtered prompts\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_high_cardinality.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n\nconst embeddings = new OpenAIEmbeddings({\n  model: \"text-embedding-3-small\",\n})\nconst vectorstore = await MemoryVectorStore.fromTexts(names, {}, embeddings);\n\nconst selectNames = async (question: string) => {\n  const _docs = await vectorstore.similaritySearch(question, 10);\n  const _names = _docs.map(d => d.pageContent);\n  return _names.join(\", \");\n}\n\nconst createPrompt = RunnableSequence.from([\n  {\n      question: new RunnablePassthrough(),\n      authors: selectNames,\n  },\n  basePrompt\n])\n```\n\n----------------------------------------\n\nTITLE: Embedding Multiple Documents with OpenAI in TypeScript\nDESCRIPTION: Demonstrates how to use LangChain's OpenAIEmbeddings class to embed multiple text documents into vector representations. Shows the embedDocuments method which returns an array of embeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/embedding_models.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nconst embeddingsModel = new OpenAIEmbeddings();\nconst embeddings = await embeddingsModel.embedDocuments([\n  \"Hi there!\",\n  \"Oh, hello!\",\n  \"What's your name?\",\n  \"My friends call me World\",\n  \"Hello World!\",\n]);\n\nconsole.log(`(${embeddings.length}, ${embeddings[0].length})`);\n// (5, 1536)\n```\n\n----------------------------------------\n\nTITLE: Initializing TensorFlow Embeddings in TypeScript\nDESCRIPTION: TypeScript code snippet demonstrating how to import and initialize TensorFlow embeddings using LangChain. It sets up the CPU backend for TensorFlow.js and creates an instance of TensorFlowEmbeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/tensorflow.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport \"@tensorflow/tfjs-backend-cpu\";\nimport { TensorFlowEmbeddings } from \"@langchain/community/embeddings/tensorflow\";\n\nconst embeddings = new TensorFlowEmbeddings();\n```\n\n----------------------------------------\n\nTITLE: Embedding Single Text with TogetherAIEmbeddings in JavaScript\nDESCRIPTION: Shows how to generate an embedding for a single text query using the embedQuery method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/togetherai.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst singleVector = await embeddings.embedQuery(text);\n\nconsole.log(singleVector.slice(0, 100));\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom BedrockRuntimeClient\nDESCRIPTION: Shows how to initialize BedrockEmbeddings with a custom BedrockRuntimeClient configuration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/bedrock.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { BedrockRuntimeClient } from \"@aws-sdk/client-bedrock-runtime\";\nimport { BedrockEmbeddings } from \"@langchain/aws\";\n\nconst getCredentials = () => {\n  // do something to get credentials\n}\n\n// @lc-ts-ignore\nconst client = new BedrockRuntimeClient({\n  region: \"us-east-1\",\n  credentials: getCredentials(),\n});\n\nconst embeddingsWithCustomClient = new BedrockEmbeddings({\n  client,\n});\n```\n\n----------------------------------------\n\nTITLE: Instantiating MongoDBAtlasVectorSearch Client (TypeScript)\nDESCRIPTION: Initializes the MongoDB client, specifies the target database and collection, configures OpenAI embeddings, and creates an instance of `MongoDBAtlasVectorSearch`. It connects LangchainJS to the configured MongoDB Atlas collection, specifying index and field names. Requires `mongodb`, `@langchain/mongodb`, and `@langchain/openai` packages, plus set environment variables.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/mongodb_atlas.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { MongoDBAtlasVectorSearch } from \"@langchain/mongodb\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { MongoClient } from \"mongodb\";\n\nconst client = new MongoClient(process.env.MONGODB_ATLAS_URI || \"\");\nconst collection = client.db(process.env.MONGODB_ATLAS_DB_NAME)\n  .collection(process.env.MONGODB_ATLAS_COLLECTION_NAME);\n\nconst embeddings = new OpenAIEmbeddings({\n  model: \"text-embedding-3-small\",\n});\n\nconst vectorStore = new MongoDBAtlasVectorSearch(embeddings, {\n  collection: collection,\n  indexName: \"vector_index\", // The name of the Atlas search index. Defaults to \"default\"\n  textKey: \"text\", // The name of the collection field containing the raw content. Defaults to \"text\"\n  embeddingKey: \"embedding\", // The name of the collection field containing the embedded text. Defaults to \"embedding\"\n});\n```\n\n----------------------------------------\n\nTITLE: Invoking the Model with Tool Calls\nDESCRIPTION: Code to invoke the model with a query that expects a tool call.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/agent_executor.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nconst responseWithToolCalls = await modelWithTools.invoke([{\n  role: \"user\",\n  content: \"What's the weather in SF?\"\n}])\n\nconsole.log(`Content: ${responseWithToolCalls.content}`)\nconsole.log(`Tool calls: ${JSON.stringify(responseWithToolCalls.tool_calls, null, 2)}`)\n```\n\n----------------------------------------\n\nTITLE: React Component Import for Feature Tables\nDESCRIPTION: Import statement for a React component that displays feature comparison tables in the documentation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llm_caching/index.mdx#2025-04-22_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\nimport { IndexTable } from \"@theme/FeatureTables\";\n\n<IndexTable />\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Google Cloud Credentials\nDESCRIPTION: Commands for setting up environment variables needed for Google Cloud authentication in different environments.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/google_vertex_ai.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport GOOGLE_APPLICATION_CREDENTIALS=\"path/to/your/credentials.json\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Runtime Callbacks with ConsoleCallbackHandler in LangChainJS\nDESCRIPTION: Example showing how to pass a ConsoleCallbackHandler to a chain at runtime. The code creates a chat prompt template and Anthropic model, combines them into a chain, and executes with a callback handler. The handler will be used for all nested components in the chain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/callbacks_runtime.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ConsoleCallbackHandler } from \"@langchain/core/tracers/console\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst handler = new ConsoleCallbackHandler();\n\nconst prompt = ChatPromptTemplate.fromTemplate(`What is 1 + {number}?`);\nconst model = new ChatAnthropic({\n  model: \"claude-3-sonnet-20240229\",\n});\n\nconst chain = prompt.pipe(model);\n\nawait chain.invoke({ number: \"2\" }, { callbacks: [handler] });\n```\n\n----------------------------------------\n\nTITLE: Defining Few-Shot Examples for SQL Query Generation\nDESCRIPTION: TypeScript code defining a list of example questions and their corresponding SQL queries to be used in few-shot prompting for SQL generation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/sql_prompting.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\n<CodeBlock language=\"typescript\">{ExampleList}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Basic Chat Model Invocation\nDESCRIPTION: Example of invoking the BedrockChat model with a system message and human input for translation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/bedrock.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst aiMsg = await llm.invoke([\n  [\n    \"system\",\n    \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n  ],\n  [\"human\", \"I love programming.\"],\n])\naiMsg\n```\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(aiMsg.content)\n```\n\n----------------------------------------\n\nTITLE: Defining Few-Shot Example Set in JavaScript\nDESCRIPTION: Creates an array of example objects, each containing a question and its corresponding answer for few-shot learning.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot_examples.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst examples = [\n    {\n      question: \"Who lived longer, Muhammad Ali or Alan Turing?\",\n      answer: `\n  Are follow up questions needed here: Yes.\n  Follow up: How old was Muhammad Ali when he died?\n  Intermediate answer: Muhammad Ali was 74 years old when he died.\n  Follow up: How old was Alan Turing when he died?\n  Intermediate answer: Alan Turing was 41 years old when he died.\n  So the final answer is: Muhammad Ali\n  `\n    },\n    {\n      question: \"When was the founder of craigslist born?\",\n      answer: `\n  Are follow up questions needed here: Yes.\n  Follow up: Who was the founder of craigslist?\n  Intermediate answer: Craigslist was founded by Craig Newmark.\n  Follow up: When was Craig Newmark born?\n  Intermediate answer: Craig Newmark was born on December 6, 1952.\n  So the final answer is: December 6, 1952\n  `\n    },\n    {\n      question: \"Who was the maternal grandfather of George Washington?\",\n      answer: `\n  Are follow up questions needed here: Yes.\n  Follow up: Who was the mother of George Washington?\n  Intermediate answer: The mother of George Washington was Mary Ball Washington.\n  Follow up: Who was the father of Mary Ball Washington?\n  Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n  So the final answer is: Joseph Ball\n  `\n    },\n    {\n      question: \"Are both the directors of Jaws and Casino Royale from the same country?\",\n      answer: `\n  Are follow up questions needed here: Yes.\n  Follow up: Who is the director of Jaws?\n  Intermediate Answer: The director of Jaws is Steven Spielberg.\n  Follow up: Where is Steven Spielberg from?\n  Intermediate Answer: The United States.\n  Follow up: Who is the director of Casino Royale?\n  Intermediate Answer: The director of Casino Royale is Martin Campbell.\n  Follow up: Where is Martin Campbell from?\n  Intermediate Answer: New Zealand.\n  So the final answer is: No\n  `\n    }\n  ];\n```\n\n----------------------------------------\n\nTITLE: Disabling Parallel Tool Calls at Runtime in LangChain\nDESCRIPTION: This code example shows how to disable parallel tool calls at runtime by passing the 'parallel_tool_calls' option as false in the invoke method. This allows for dynamic control over tool calling behavior.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_calling_parallel.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst result3 = await llmWithNoBoundParam.invoke(\"Please call the first tool two times\", {\n  parallel_tool_calls: false,\n});\n\nresult3.tool_calls;\n```\n\n----------------------------------------\n\nTITLE: Generating Embedding for a Single Query with JinaEmbeddings in TypeScript\nDESCRIPTION: This snippet shows how to generate an embedding vector for a single piece of text using the `embedQuery` method of an initialized `JinaEmbeddings` instance. The method takes a string query as input and returns a promise resolving to the numerical embedding array.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/jina.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst embedding = await embeddings.embedQuery(\n  \"What would be a good company name for a company that makes colorful socks?\"\n);\nconsole.log(embedding);\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Graph Database Q&A Application\nDESCRIPTION: This snippet shows the command to install the necessary dependencies for building a Question Answering application over a Graph Database using LangChain and related libraries.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/graph.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nlangchain @langchain/community @langchain/openai @langchain/core neo4j-driver\n```\n\n----------------------------------------\n\nTITLE: Using TavilyExtract Tool in LangChain.js\nDESCRIPTION: Example of importing and using the TavilyExtract tool to extract raw content from URLs. The snippet demonstrates how to instantiate the tool and invoke it with a list of URLs.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-tavily/README.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { TavilyExtract } from \"@langchain/tavily\";\n\nconst tool = new TavilyExtract({\n  // Constructor parameters:\n  // extractDepth: \"basic\",\n  // includeImages: false,\n});\n\n// Invoke with a list of URLs\nconst results = await tool.invoke({\n  urls: [\"https://en.wikipedia.org/wiki/Lionel_Messi\"]\n});\n\nconsole.log(results);\n```\n\n----------------------------------------\n\nTITLE: Initializing Neo4j Graph and Importing Movie Data\nDESCRIPTION: This code initializes a connection to a Neo4j database, creates a Neo4jGraph object, and imports movie information using a Cypher query. It demonstrates how to set up and populate a graph database for the Q&A application.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/graph.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport \"neo4j-driver\";\nimport { Neo4jGraph } from \"@langchain/community/graphs/neo4j_graph\";\n\nconst url = process.env.NEO4J_URI;\nconst username = process.env.NEO4J_USER;\nconst password = process.env.NEO4J_PASSWORD;\nconst graph = await Neo4jGraph.initialize({ url, username, password });\n\n// Import movie information\nconst moviesQuery = `LOAD CSV WITH HEADERS FROM \n'https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/movies/movies_small.csv'\nAS row\nMERGE (m:Movie {id:row.movieId})\nSET m.released = date(row.released),\n    m.title = row.title,\n    m.imdbRating = toFloat(row.imdbRating)\nFOREACH (director in split(row.director, '|') | \n    MERGE (p:Person {name:trim(director)})\n    MERGE (p)-[:DIRECTED]->(m))\nFOREACH (actor in split(row.actors, '|') | \n    MERGE (p:Person {name:trim(actor)})\n    MERGE (p)-[:ACTED_IN]->(m))\nFOREACH (genre in split(row.genres, '|') | \n    MERGE (g:Genre {name:trim(genre)})\n    MERGE (m)-[:IN_GENRE]->(g))`\n\nawait graph.query(moviesQuery);\n```\n\n----------------------------------------\n\nTITLE: Initializing WatsonxEmbeddings with Bearer Token Authentication\nDESCRIPTION: Creates an instance of WatsonxEmbeddings using bearer token authentication. This includes setting the version, service URL, project ID, authentication type, and bearer token.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/ibm.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { WatsonxEmbeddings } from \"@langchain/community/embeddings/ibm\";\n\nconst props = {\n  version: \"YYYY-MM-DD\",\n  serviceUrl: \"<SERVICE_URL>\",\n  projectId: \"<PROJECT_ID>\",\n  watsonxAIAuthType: \"bearertoken\",\n  watsonxAIBearerToken: \"<YOUR-BEARERTOKEN>\",\n};\nconst instance = new WatsonxEmbeddings(props);\n```\n\n----------------------------------------\n\nTITLE: Instantiating Ollama LLM in LangChain\nDESCRIPTION: Initializes an Ollama text completion model with configuration parameters. This snippet shows how to create a model instance with llama3 as the default model, temperature set to 0, and maximum retries set to 2.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/ollama.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { Ollama } from \"@langchain/ollama\"\n\nconst llm = new Ollama({\n  model: \"llama3\", // Default value\n  temperature: 0,\n  maxRetries: 2,\n  // other params...\n})\n```\n\n----------------------------------------\n\nTITLE: Using TavilySearch Tool in LangChain.js\nDESCRIPTION: Example of importing and using the TavilySearch tool to perform searches optimized for LLMs. The snippet shows how to instantiate the tool with options and invoke it with a search query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-tavily/README.md#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { TavilySearch } from \"@langchain/tavily\";\n\nconst tool = new TavilySearch({\n  maxResults: 5,\n  // You can set other constructor parameters here, e.g.:\n  // topic: \"general\",\n  // includeAnswer: false,\n  // includeRawContent: false,\n  // includeImages: false,\n  // searchDepth: \"basic\",\n});\n\n// Invoke with a query\nconst results = await tool.invoke({\n  query: \"what is the current weather in SF?\"\n});\n\nconsole.log(results);\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Stagehand Toolkit\nDESCRIPTION: This command installs the necessary npm packages for using the Stagehand Toolkit with LangChain.js. These packages provide the core functionality, community tools, and graph-based agents needed for web automation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/stagehand.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/langgraph @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Invoking Filtered Messages as a Runnable\nDESCRIPTION: Demonstrates how to use the filter_ object as a standalone Runnable for message filtering.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/filter_messages.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nawait filter_.invoke(messages)\n```\n\n----------------------------------------\n\nTITLE: Creating a List of Tools\nDESCRIPTION: Code to create a list of tools combining the search and retriever tools.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/agent_executor.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst tools = [search, retrieverTool];\n```\n\n----------------------------------------\n\nTITLE: Loading Data from All Paths in a GitBook\nDESCRIPTION: This snippet shows how to load data from all paths in a GitBook. It initializes the GitbookLoader with the root path and sets 'shouldLoadAllPaths' to true, then loads all documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/gitbook.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { GitbookLoader } from \"@langchain/community/document_loaders/web/gitbook\";\n\nconst loader = new GitbookLoader(\"https://docs.gitbook.com\", {\n  shouldLoadAllPaths: true,\n});\n\nconst docs = await loader.load();\n```\n\n----------------------------------------\n\nTITLE: Setting up Tools and Model for LangChain in JavaScript\nDESCRIPTION: This snippet demonstrates how to import necessary modules, create custom tools for addition and multiplication, and initialize a ChatOpenAI model. It sets up the environment for testing parallel tool calling.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_calling_parallel.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { z } from \"zod\";\nimport { tool } from \"@langchain/core/tools\";\n\nconst adderTool = tool(async ({ a, b }) => {\n  return a + b;\n}, {\n  name: \"add\",\n  description: \"Adds a and b\",\n  schema: z.object({\n    a: z.number(),\n    b: z.number(),\n  })\n});\n\nconst multiplyTool = tool(async ({ a, b }) => {\n  return a * b;\n}, {\n  name: \"multiply\",\n  description: \"Multiplies a and b\",\n  schema: z.object({\n    a: z.number(),\n    b: z.number(),\n  })\n});\n\nconst tools = [adderTool, multiplyTool];\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o-mini\",\n  temperature: 0,\n});\n```\n\n----------------------------------------\n\nTITLE: Invoking LangGraph App with Conversation Context\nDESCRIPTION: Demonstrates calling the LangGraph app to translate text to French, using a thread_id to maintain conversation state across invocations.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_memory.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nawait app.invoke(\n  {\n    messages: [\n      {\n        role: \"user\",\n        content: \"Translate to French: I love programming.\"\n      }\n    ]\n  },\n  {\n    configurable: { thread_id: \"1\" }\n  }\n);\n```\n\n----------------------------------------\n\nTITLE: Obtaining Raw Text Chunks with splitText Method\nDESCRIPTION: Demonstrates using the splitText() method to get the raw string content from splitting the text, without creating Document objects. This is useful when only the text chunks are needed without additional metadata.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/character_text_splitter.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst chunks = await textSplitter.splitText(stateOfTheUnion);\n\nchunks[0];\n```\n\n----------------------------------------\n\nTITLE: Using ChatGoogleGenerativeAI for Image Analysis\nDESCRIPTION: TypeScript code showing how to use ChatGoogleGenerativeAI for image analysis with Gemini vision models.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/google.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst visionModel = new ChatGoogleGenerativeAI({\n  model: \"gemini-2.0-flash\",\n  maxOutputTokens: 2048,\n});\nconst image = fs.readFileSync(\"./hotdog.jpg\").toString(\"base64\");\nconst input2 = [\n  new HumanMessage({\n    content: [\n      {\n        type: \"text\",\n        text: \"Describe the following image.\",\n      },\n      {\n        type: \"image_url\",\n        image_url: `data:image/png;base64,${image}`,\n      },\n    ],\n  }),\n];\n\nconst res = await visionModel.invoke(input2);\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key in Environment\nDESCRIPTION: Sets the necessary environment variables for OpenAI API authentication. The OpenAI API key is required, while LangSmith tracing is optional for monitoring model calls.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/openai.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Using the Advanced Custom Chat Model with Event Streaming\nDESCRIPTION: Demonstrates how to use the event streaming capability of the advanced custom chat model. This captures model metadata like token usage in callback events.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/custom_chat.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst chatModel = new AdvancedCustomChatModel({ n: 4 });\n\nconst eventStream = await chatModel.streamEvents([[\"human\", \"I am an LLM\"]], {\n  version: \"v2\",\n});\n\nfor await (const event of eventStream) {\n  if (event.event === \"on_chat_model_end\") {\n    console.log(JSON.stringify(event, null, 2));\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith API Key Environment Variable\nDESCRIPTION: Instructions for setting up the required LangSmith API key as an environment variable.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/langsmith.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Chaining ChatPromptTemplate with ChatBedrockConverse for Language Translation\nDESCRIPTION: Demonstrates how to create a chain combining a ChatPromptTemplate with the ChatBedrockConverse model. This example creates a language translation chain with parameterized input and output languages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/bedrock_converse.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\"\n\nconst prompt = ChatPromptTemplate.fromMessages(\n  [\n    [\n      \"system\",\n      \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n    ],\n    [\"human\", \"{input}\"],\n  ]\n)\n\nconst chain = prompt.pipe(llm);\nawait chain.invoke(\n  {\n    input_language: \"English\",\n    output_language: \"German\",\n    input: \"I love programming.\",\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: Instantiating ChatWatsonx Model for IBM watsonx.ai\nDESCRIPTION: Creates an instance of the ChatWatsonx model with configuration parameters including version, service URL, project ID, model ID, and generation settings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/ibm.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatWatsonx } from \"@langchain/community/chat_models/ibm\";\nconst props = {\n  maxTokens: 200,\n  temperature: 0.5\n};\n\nconst instance = new ChatWatsonx({\n  version: \"YYYY-MM-DD\",\n  serviceUrl: process.env.API_URL,\n  projectId: \"<PROJECT_ID>\",\n  // spaceId: \"<SPACE_ID>\",\n  // idOrName: \"<DEPLOYMENT_ID>\",\n  model: \"<MODEL_ID>\",\n  ...props\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Final Prompt with Few Shot Examples in LangChain.js\nDESCRIPTION: This snippet shows how to assemble a complete chat prompt that includes a system message, few shot examples, and a human input. The fewShotPrompt is combined with other message types to create the final prompt template.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot_examples_chat.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst finalPrompt = ChatPromptTemplate.fromMessages(\n    [\n        [\"system\", \"You are a wondrous wizard of math.\"],\n        fewShotPrompt,\n        [\"human\", \"{input}\"],\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search with Filters\nDESCRIPTION: Example of performing a similarity search with metadata filtering in WeaviateStore\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/weaviate.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconst filter = {\n  where: {\n    operator: \"Equal\" as const,\n    path: [\"source\"],\n    valueText: \"https://example.com\",\n  }\n};\n\nconst similaritySearchResults = await vectorStore.similaritySearch(\"biology\", 2, filter);\n\nfor (const doc of similaritySearchResults) {\n  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);\n}\n```\n\n----------------------------------------\n\nTITLE: Embedding a Query with OpenAI in TypeScript\nDESCRIPTION: Example of how to embed a single query string using the embedQuery method of OpenAIEmbeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/embed_text.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst res = await embeddings.embedQuery(\"Hello world\");\n/*\n[\n   -0.004845875,   0.004899438,  -0.016358767,  -0.024475135, -0.017341806,\n    0.012571548,  -0.019156644,   0.009036391,  -0.010227379, -0.026945334,\n    0.022861943,   0.010321903,  -0.023479493, -0.0066544134,  0.007977734,\n   0.0026371893,   0.025206111,  -0.012048521,   0.012943339,  0.013094575,\n   -0.010580265,  -0.003509951,   0.004070787,   0.008639394, -0.020631202,\n  ... 1511 more items\n]\n*/\n```\n\n----------------------------------------\n\nTITLE: LangChain.js Agent Execution Chain Start\nDESCRIPTION: This snippet shows the start of a LangChain.js agent execution chain. It demonstrates how an agent executor uses a tool calling agent with a chat prompt template to process user instructions, illustrating the hierarchical nature of LangChain.js execution chains.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n[chain/start] [1:chain:AgentExecutor > 10:chain:ToolCallingAgent > 14:prompt:ChatPromptTemplate] Entering Chain run with input: {\n```\n\n----------------------------------------\n\nTITLE: Executing Initial Task Generation\nDESCRIPTION: Invokes the generation chain with a user query and trace group for monitoring. Shows how to handle the initial task generation attempt.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/basic_critique_revise.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { TraceGroup } from \"npm:langchain@0.0.173/callbacks\";\n\nconst traceGroup = new TraceGroup(\"CritiqueReviseChain\");\nconst groupManager = await traceGroup.start();\n\nconst userQuery = `Set a reminder to renew our online property ads next week.`;\n\nlet result = await generateChain.invoke({\n  query: userQuery,\n}, { callbacks: groupManager });\n\nconsole.log(result);\n```\n\n----------------------------------------\n\nTITLE: Demonstrating In-Memory Caching for Chat Model in TypeScript\nDESCRIPTION: This code shows the time difference between the first (uncached) and second (cached) invocations of the chat model. It illustrates the performance improvement when using in-memory caching.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chat_model_caching.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconsole.time();\n\n// The first time, it is not yet in cache, so it should take longer\nconst res = await model.invoke(\"Tell me a joke!\");\nconsole.log(res);\n\nconsole.timeEnd();\n\n/*\n  AIMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: \"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\",\n      additional_kwargs: { function_call: undefined, tool_calls: undefined }\n    },\n    lc_namespace: [ 'langchain_core', 'messages' ],\n    content: \"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\",\n    name: undefined,\n    additional_kwargs: { function_call: undefined, tool_calls: undefined }\n  }\n  default: 2.224s\n*/\n```\n\nLANGUAGE: typescript\nCODE:\n```\nconsole.time();\n\n// The second time it is, so it goes faster\nconst res2 = await model.invoke(\"Tell me a joke!\");\nconsole.log(res2);\n\nconsole.timeEnd();\n/*\n  AIMessage {\n    lc_serializable: true,\n    lc_kwargs: {\n      content: \"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\",\n      additional_kwargs: { function_call: undefined, tool_calls: undefined }\n    },\n    lc_namespace: [ 'langchain_core', 'messages' ],\n    content: \"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\",\n    name: undefined,\n    additional_kwargs: { function_call: undefined, tool_calls: undefined }\n  }\n  default: 181.98ms\n*/\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain OpenAI Integration\nDESCRIPTION: Command to install the necessary packages for using OpenAI embeddings with LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/embed_text.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Implementing Cloudflare Worker with AI Embeddings and Vectorstore\nDESCRIPTION: Example Cloudflare worker implementation using Workers AI embeddings with a Cloudflare Vectorize vectorstore. It demonstrates similarity search, document addition, and deletion operations.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/cloudflare_ai.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n// @ts-nocheck\n\nimport type {\n  VectorizeIndex,\n  Fetcher,\n  Request,\n} from \"@cloudflare/workers-types\";\n\nimport {\n  CloudflareVectorizeStore,\n  CloudflareWorkersAIEmbeddings,\n} from \"@langchain/cloudflare\";\n\nexport interface Env {\n  VECTORIZE_INDEX: VectorizeIndex;\n  AI: Fetcher;\n}\n\nexport default {\n  async fetch(request: Request, env: Env) {\n    const { pathname } = new URL(request.url);\n    const embeddings = new CloudflareWorkersAIEmbeddings({\n      binding: env.AI,\n      model: \"@cf/baai/bge-small-en-v1.5\",\n    });\n    const store = new CloudflareVectorizeStore(embeddings, {\n      index: env.VECTORIZE_INDEX,\n    });\n    if (pathname === \"/\") {\n      const results = await store.similaritySearch(\"hello\", 5);\n      return Response.json(results);\n    } else if (pathname === \"/load\") {\n      // Upsertion by id is supported\n      await store.addDocuments(\n        [\n          {\n            pageContent: \"hello\",\n            metadata: {},\n          },\n          {\n            pageContent: \"world\",\n            metadata: {},\n          },\n          {\n            pageContent: \"hi\",\n            metadata: {},\n          },\n        ],\n        { ids: [\"id1\", \"id2\", \"id3\"] }\n      );\n\n      return Response.json({ success: true });\n    } else if (pathname === \"/clear\") {\n      await store.delete({ ids: [\"id1\", \"id2\", \"id3\"] });\n      return Response.json({ success: true });\n    }\n\n    return Response.json({ error: \"Not Found\" }, { status: 404 });\n  },\n};\n```\n\n----------------------------------------\n\nTITLE: Setting Anthropic API Key in Bash\nDESCRIPTION: Sets the ANTHROPIC_API_KEY environment variable for authentication with Anthropic's API.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/anthropic.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport ANTHROPIC_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Loading Documents from a Directory with Multiple File Types\nDESCRIPTION: Demonstrates how to use the DirectoryLoader to load documents from a directory with different file extensions. The code configures specific loaders for JSON, JSONL, TXT, and CSV files, loads all documents, and logs the results.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/document_loader_directory.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { DirectoryLoader } from \"langchain/document_loaders/fs/directory\";\nimport {\n  JSONLoader,\n  JSONLinesLoader,\n} from \"langchain/document_loaders/fs/json\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\nimport { CSVLoader } from \"@langchain/community/document_loaders/fs/csv\";\n\nconst loader = new DirectoryLoader(\n  \"src/document_loaders/example_data/example\",\n  {\n    \".json\": (path) => new JSONLoader(path, \"/texts\"),\n    \".jsonl\": (path) => new JSONLinesLoader(path, \"/html\"),\n    \".txt\": (path) => new TextLoader(path),\n    \".csv\": (path) => new CSVLoader(path, \"text\"),\n  }\n);\nconst docs = await loader.load();\nconsole.log({ docs });\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for OpenAI Integration\nDESCRIPTION: Sets up the OpenAI API key as an environment variable for authentication.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/versions/migrating_memory/conversation_buffer_window_memory.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.OPENAI_API_KEY = \"YOUR_OPENAI_API_KEY\";\n```\n\n----------------------------------------\n\nTITLE: Invoking Retrieval Chain with User-Specific Configuration\nDESCRIPTION: JavaScript code demonstrating how to invoke the retrieval chain with user-specific configurations, allowing for per-user document retrieval.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_per_user.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nawait chain.invoke(\n  { question: \"where did the user work?\"},\n  { configurable: { filter: { namespace: \"harrison\" } } },\n);\n\nawait chain.invoke(\n  { question: \"where did the user work?\"},\n  { configurable: { filter: { namespace: \"ankush\" } } },\n);\n```\n\n----------------------------------------\n\nTITLE: Creating StateGraph for RAG in JavaScript\nDESCRIPTION: This snippet demonstrates how to create a StateGraph object for a RAG application, connecting retrieval and generation steps into a single sequence.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport { StateGraph } from \"@langchain/langgraph\";\n\nconst graph = new StateGraph(StateAnnotation)\n  .addNode(\"retrieve\", retrieve)\n  .addNode(\"generate\", generate)\n  .addEdge(\"__start__\", \"retrieve\")\n  .addEdge(\"retrieve\", \"generate\")\n  .addEdge(\"generate\", \"__end__\")\n  .compile();\n```\n\n----------------------------------------\n\nTITLE: Invoking SerpAPI Tool with ToolCall Object\nDESCRIPTION: Example demonstrating how to invoke the SerpAPI tool using a ToolCall object that might be generated by an LLM.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/serpapi.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n// This is usually generated by a model, but we'll create a tool call directly for demo purposes.\nconst modelGeneratedToolCall = {\n  args: {\n    input: \"what is the current weather in SF?\"\n  },\n  id: \"1\",\n  name: tool.name,\n  type: \"tool_call\",\n}\n\nawait tool.invoke(modelGeneratedToolCall)\n```\n\n----------------------------------------\n\nTITLE: Creating Ensemble Retriever\nDESCRIPTION: Demonstrates how to create an ensemble retriever that combines multiple retrievers (BM25 and vector store) with weighted scores.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/retrievers.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst ensembleRetriever = new EnsembleRetriever({\n  retrievers: [bm25Retriever, vectorStoreRetriever],\n  weights: [0.5, 0.5],\n});\n```\n\n----------------------------------------\n\nTITLE: Document Loading with SQL Query\nDESCRIPTION: JavaScript implementation for loading documents using a custom SQL query with specified content and metadata columns.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/google_cloudsql_pg.mdx#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst documentLoaderArgs: PostgresLoaderOptions = {\n  query: \"SELECT * FROM my_fruit_table\",\n  contentColumns: [\"fruit_name\", \"variety\"],\n  metadataColumns: [\n    \"fruit_id\",\n    \"quantity_in_stock\",\n    \"price_per_unit\",\n    \"organic\",\n  ],\n  format: \"text\",\n};\n\nconst documentLoaderInstance = await PostgresLoader.initialize(\n  PEInstance,\n  docucumetLoaderArgs\n);\n```\n\n----------------------------------------\n\nTITLE: Create and Execute a React Agent with LangChain\nDESCRIPTION: Creates a React-based agent executor using LangGraph's prebuilt function with specified language models and tools. Requires '@langchain/langgraph' package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/openapi.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\"\n\nconst agentExecutor = createReactAgent({ llm, tools });\n```\n\n----------------------------------------\n\nTITLE: Creating and Querying ZepCloudVectorStore in TypeScript\nDESCRIPTION: This code demonstrates how to create a ZepCloudVectorStore from documents and perform queries using LangChain.js. It includes initialization, adding documents, and performing similarity and MMR searches.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/zep_cloud.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nZepCloudVectorStoreExample\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Azure Cosmos DB Integration\nDESCRIPTION: Command to install the Azure Cosmos DB package along with the LangChain core package. This is required to use the AzureCosmosDBMongoChatMessageHistory class.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/azure_cosmos_mongo_vcore.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/azure-cosmosdb @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Callback Handler Implementation for LLM Streaming\nDESCRIPTION: Shows how to implement streaming using a CallbackHandler to process streaming responses. This approach provides more control over the streaming process and access to the final LLMResult.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming_llm.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nStreamingExample\n```\n\n----------------------------------------\n\nTITLE: Initializing DeepInfraEmbeddings in TypeScript\nDESCRIPTION: Creates an instance of DeepInfraEmbeddings with optional configuration parameters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/deepinfra.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { DeepInfraEmbeddings } from \"@langchain/community/embeddings/deepinfra\";\n\nconst embeddings = new DeepInfraEmbeddings({\n  apiToken: \"YOUR_API_TOKEN\",\n  modelName: \"sentence-transformers/clip-ViT-B-32\", // Optional, defaults to \"sentence-transformers/clip-ViT-B-32\"\n  batchSize: 1024, // Optional, defaults to 1024\n});\n```\n\n----------------------------------------\n\nTITLE: Adding a New Example to the Selector\nDESCRIPTION: Shows how to add a new translation example to the existing example selector.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/example_selectors.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nawait exampleSelector.addExample({ input: \"hand\", output: \"mano\" })\n```\n\n----------------------------------------\n\nTITLE: Adding Hooks to Chat Model\nDESCRIPTION: Examples of adding custom hooks to the ChatMistralAI model during and after instantiation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/mistral.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatMistralAI } from \"@langchain/mistralai\" \n\nconst modelWithHooks = new ChatMistralAI({\n    model: \"mistral-large-latest\",\n    temperature: 0,\n    maxRetries: 2,\n    beforeRequestHooks: [ beforeRequestHook ],\n    requestErrorHooks: [ requestErrorHook ],\n    responseHooks: [ responseHook ],\n    // other params...\n});\n```\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatMistralAI } from \"@langchain/mistralai\" \n\nconst model = new ChatMistralAI({\n    model: \"mistral-large-latest\",\n    temperature: 0,\n    maxRetries: 2,\n    // other params...\n});\n\nmodel.beforeRequestHooks = [ ...model.beforeRequestHooks, beforeRequestHook ];\nmodel.requestErrorHooks = [ ...model.requestErrorHooks, requestErrorHook ];\nmodel.responseHooks = [ ...model.responseHooks, responseHook ];\n\nmodel.addAllHooksToHttpClient();\n```\n\n----------------------------------------\n\nTITLE: Creating a RAG Chain with ExaRetriever in TypeScript\nDESCRIPTION: This snippet demonstrates how to create a Retrieval-Augmented Generation (RAG) chain using ExaRetriever, ChatPromptTemplate, and other LangChain components.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/exa.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { RunnablePassthrough, RunnableSequence } from \"@langchain/core/runnables\";\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\n\nimport type { Document } from \"@langchain/core/documents\";\n\nconst prompt = ChatPromptTemplate.fromTemplate(`\nAnswer the question based only on the context provided.\n\nContext: {context}\n\nQuestion: {question}`);\n\nconst formatDocs = (docs: Document[]) => {\n  return docs.map((doc) => doc.pageContent).join(\"\\n\\n\");\n}\n\n// See https://js.langchain.com/docs/tutorials/rag\nconst ragChain = RunnableSequence.from([\n  {\n    context: retriever.pipe(formatDocs),\n    question: new RunnablePassthrough(),\n  },\n  prompt,\n  llm,\n  new StringOutputParser(),\n]);\n```\n\n----------------------------------------\n\nTITLE: Viewing Document Metadata\nDESCRIPTION: Shows how to access the metadata of a loaded document, which includes information like page numbers and other PDF-specific properties.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/pdf.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(docs[0].metadata)\n```\n\n----------------------------------------\n\nTITLE: Invoking ExaSearchResults Directly (TypeScript)\nDESCRIPTION: Shows how to invoke the instantiated `ExaSearchResults` tool directly by passing the search query string to its `invoke` method. This performs a search and returns the results.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/exa_search.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nawait tool.invoke(\"what is the weather in wailea?\")\n```\n\n----------------------------------------\n\nTITLE: Retrieving and Listing Available Tools in Typescript\nDESCRIPTION: Retrieves the list of tools available within the instantiated `WatsonxToolkit` using the `getTools()` method and logs their names and descriptions to the console.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/ibm.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst tools = toolkit.getTools();\n\nconsole.log(\n  tools.map((tool) => ({\n    name: tool.name,\n    description: tool.description,\n  }))\n);\n```\n\n----------------------------------------\n\nTITLE: Updating Query Analyzer with Examples in JavaScript\nDESCRIPTION: This code updates the query analyzer to include the examples in the prompt, enhancing its ability to generate more accurate and decomposed queries.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_few_shot.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ChatPromptTemplate, MessagesPlaceholder } from \"@langchain/core/prompts\";\nimport { RunnableSequence } from \"@langchain/core/runnables\";\n\nconst queryAnalyzerWithExamples = RunnableSequence.from([\n  {\n    question: new RunnablePassthrough(),\n    examples: () => exampleMessages,\n  },\n  prompt,\n  llmWithTools\n]);\n```\n\n----------------------------------------\n\nTITLE: Accessing Extraction Results from RAG Pipeline\nDESCRIPTION: Shows how to access the key developments extracted by the RAG-based approach. This accesses the structured data from the extraction results containing historical developments related to cars.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/extraction_long_text.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nragExtractorResults.key_developments;\n```\n\n----------------------------------------\n\nTITLE: Instantiating GOAT Toolkit in TypeScript\nDESCRIPTION: This TypeScript snippet demonstrates how to instantiate the GOAT finance toolkit within a project. It requires various dependencies such as `viem`, tools from `@goat-sdk` and `ChatOpenAI` from `@langchain`. Key steps include creating a wallet client with a private key, setting up financial tools including plugins, and creating a React-based agent using the GPT model. The expected output is an initialized AI agent capable of financial transactions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/goat.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { http } from \"viem\";\nimport { createWalletClient } from \"viem\";\nimport { privateKeyToAccount } from \"viem/accounts\";\nimport { baseSepolia } from \"viem/chains\";\n\nimport { getOnChainTools } from \"@goat-sdk/adapter-langchain\";\nimport { PEPE, USDC, erc20 } from \"@goat-sdk/plugin-erc20\";\n\nimport { sendETH } from \"@goat-sdk/wallet-evm\";\nimport { viem } from \"@goat-sdk/wallet-viem\";\n\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n\n// 1. Create a wallet client\nconst account = privateKeyToAccount(\n  process.env.WALLET_PRIVATE_KEY as `0x${string}`\n);\n\nconst walletClient = createWalletClient({\n  account: account,\n  transport: http(process.env.RPC_PROVIDER_URL),\n  chain: baseSepolia,\n});\n\n// 2. Set up the tools\nconst tools = await getOnChainTools({\n  wallet: viem(walletClient),\n  plugins: [sendETH(), erc20({ tokens: [USDC, PEPE] })],\n});\n\n// 3. Create the agent\nconst model = new ChatOpenAI({\n  model: \"gpt-4o\",\n});\n\nconst agent = createReactAgent({ llm: model, tools: tools });\n```\n\n----------------------------------------\n\nTITLE: Using ChatYandexGPT for Translation\nDESCRIPTION: Example of using ChatYandexGPT to create a translation assistant that converts English to French\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-yandex/README.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatYandexGPT } from \"@langchain/yandex\";\nimport { HumanMessage, SystemMessage } from \"@langchain/core/messages\";\n\nconst chat = new ChatYandexGPT();\nconst response = await chat.invoke([\n  new SystemMessage(\n    \"You are a helpful assistant that translates English to French.\"\n  ),\n  new HumanMessage(\"I love programming.\"),\n]);\n```\n\n----------------------------------------\n\nTITLE: Parallelizing Extraction with Batch Processing\nDESCRIPTION: Demonstrates how to use the .batch method to parallelize extraction across multiple chunks. This approach speeds up the extraction process by handling multiple text chunks concurrently when the model is exposed via an API.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/extraction_long_text.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n// Limit just to the first 3 chunks\n// so the code can be re-run quickly\nconst firstFewTexts = splitDocs.slice(0, 3).map((doc) => doc.pageContent);\n\nconst extractionChainParams = firstFewTexts.map((text) => {\n  return { text };\n});\n\nconst results = await extractionChain.batch(extractionChainParams, { maxConcurrency: 5 });\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Postgres Chat Memory in Node.js\nDESCRIPTION: Command to install the required packages for using Postgres-based chat memory in LangChain.js, including OpenAI, community components, core functionality, and the pg driver for PostgreSQL.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/postgres.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core pg\n```\n\n----------------------------------------\n\nTITLE: Instantiating PDFLoader\nDESCRIPTION: Creates a PDFLoader instance by importing it from @langchain/community and providing a path to a PDF file.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/pdf.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { PDFLoader } from \"@langchain/community/document_loaders/fs/pdf\"\n\nconst nike10kPdfPath = \"../../../../data/nke-10k-2023.pdf\"\n\nconst loader = new PDFLoader(nike10kPdfPath)\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Embeddings in TypeScript\nDESCRIPTION: Code snippet showing how to initialize the OpenAIEmbeddings class from LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/embed_text.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddings = new OpenAIEmbeddings();\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Redis Chat Memory\nDESCRIPTION: Commands to install required npm packages including LangChain components, OpenAI, and Redis client libraries.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/redis.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core redis\n```\n\n----------------------------------------\n\nTITLE: Using Sonix Audio Transcription Loader with LangChain\nDESCRIPTION: Example of using the SonixAudioTranscriptionLoader to transcribe audio files and convert them to Document objects. Shows configuration for authentication, handling both local and remote audio files, and specifying the audio language.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/sonix_audio_transcription.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { SonixAudioTranscriptionLoader } from \"@langchain/community/document_loaders/web/sonix_audio\";\n\n/**\n * Example of how to load audio files and transcribe them using the Sonix API.\n *\n * You will need to create an account on https://sonix.ai/ and obtain an auth key\n * from the https://my.sonix.ai/api page.\n *\n * See the list of supported languages at:\n * https://sonix.ai/docs/api#languages\n */\nconst loader = new SonixAudioTranscriptionLoader({\n  sonixAuthKey: \"your-sonix-auth-key\",\n  request: {\n    audioFilePath: \"./example.mp3\",\n    // audioUrl: \"https://example.com/example.mp3\",\n    language: \"en\",\n    // You can also specify more options, like:\n    // name: \"My great audio\",\n    // transcriptionMode: \"machine_only\",\n    // skipDiarization: false,\n    // skipSummary: false,\n  },\n});\n\nconst docs = await loader.load();\nconsole.log(docs);\n```\n\n----------------------------------------\n\nTITLE: Adding Documents to LibSQLVectorStore (TypeScript)\nDESCRIPTION: Adds an array of LangChain `Document` objects to the initialized `LibSQLVectorStore`. Each document should have `pageContent` (string) and `metadata` (object). The `addDocuments` method handles embedding the content and storing it along with metadata in the configured libSQL table.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/libsql.mdx#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport type { Document } from \"@langchain/core/documents\";\n\nconst documents: Document[] = [\n  { pageContent: \"Hello\", metadata: { topic: \"greeting\" } },\n  { pageContent: \"Bye bye\", metadata: { topic: \"greeting\" } },\n];\n\nawait vectorStore.addDocuments(documents);\n```\n\n----------------------------------------\n\nTITLE: Loading Audio Transcript Using AssemblyAI in TypeScript\nDESCRIPTION: Example of using AssemblyAI to transcribe an audio file and load it as a document object in LangChain.js. It demonstrates the use of AudioTranscriptLoader with various configuration options.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/assemblyai_audio_transcription.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport TranscriptExample from \"@examples/document_loaders/assemblyai_audio_transcription.ts\";\n```\n\n----------------------------------------\n\nTITLE: Initializing Chat Model Configuration\nDESCRIPTION: Sets up a ChatOpenAI instance with GPT-4 model and zero temperature for consistent outputs.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/message_history.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o\",\n  temperature: 0,\n});\n```\n\n----------------------------------------\n\nTITLE: Instantiating ChatCerebras Model in JavaScript\nDESCRIPTION: Example of importing and initializing a ChatCerebras model with configuration options such as model selection, temperature, and retry settings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/cerebras.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatCerebras } from \"@langchain/cerebras\" \n\nconst llm = new ChatCerebras({\n    model: \"llama-3.3-70b\",\n    temperature: 0,\n    maxTokens: undefined,\n    maxRetries: 2,\n    // other params...\n})\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain.js Dependencies for Tencent Hunyuan\nDESCRIPTION: Commands to install the necessary LangChain.js packages for using Tencent Hunyuan embeddings in a Node.js environment.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/tencent_hunyuan.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Instantiating GradientEmbeddings with Direct Credentials (TypeScript)\nDESCRIPTION: Demonstrates initializing the `GradientEmbeddings` class in TypeScript by providing the `gradientAccessKey` and `workspaceId` directly in the constructor options object. This serves as an alternative method to setting environment variables for authentication.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/gradient_ai.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = new GradientEmbeddings({\n  gradientAccessKey: \"My secret Access Token\",\n  workspaceId: \"My secret workspace id\"\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Bedrock LLM with AWS credentials (Python/Deno version)\nDESCRIPTION: Code for initializing the Bedrock LLM with AWS credentials and configuration options like model selection, temperature, and retry settings. This version includes Deno-specific imports.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/bedrock.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n// @lc-docs-hide-cell\n// Deno requires these imports, and way of loading env vars.\n// we don't want to expose in the docs.\n// Below this cell we have a typescript markdown codeblock with\n// the node code.\nimport \"@aws-sdk/credential-provider-node\";\nimport \"@smithy/protocol-http\";\nimport \"@aws-crypto/sha256-js\";\nimport \"@smithy/protocol-http\";\nimport \"@smithy/signature-v4\";\nimport \"@smithy/eventstream-codec\";\nimport \"@smithy/util-utf8\";\nimport \"@aws-sdk/types\";\nimport { Bedrock } from \"@langchain/community/llms/bedrock\"\nimport { getEnvironmentVariable } from \"@langchain/core/utils/env\";\n\nconst llm = new Bedrock({\n  model: \"anthropic.claude-v2\",\n  region: \"us-east-1\",\n  // endpointUrl: \"custom.amazonaws.com\",\n  credentials: {\n    accessKeyId: getEnvironmentVariable(\"BEDROCK_AWS_ACCESS_KEY_ID\"),\n    secretAccessKey: getEnvironmentVariable(\"BEDROCK_AWS_SECRET_ACCESS_KEY\"),\n  },\n  temperature: 0,\n  maxTokens: undefined,\n  maxRetries: 2,\n  // other params...\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing Replicate Llama 2 Model in LangChain.js\nDESCRIPTION: This code snippet demonstrates how to use the Replicate Llama 2 model as an LLM in LangChain.js. It includes importing necessary modules, setting up the Replicate client, creating the LLM instance, and making a prediction.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/replicate.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Replicate } from \"@langchain/community/llms/replicate\";\n\n// You can also pass custom params to be sent to the model\nconst model = new Replicate({\n  model:\n    \"meta/llama-2-70b-chat:02e509c789964a7ea8736978a43525956ef40397be9033abf9fd2badfe68c9e3\",\n});\n\nconst res = await model.invoke(\n  \"What would be a good company name for a company that makes colorful socks?\"\n);\n\nconsole.log(res);\n\n/*\nHere are some suggestions for a company name that makes colorful socks:\n\n1. Rainbow Toes\n2. Vibrant Steps\n3. Chromatix Socks\n4. Hue Hose\n5. Kaleidosocks\n6. Spectrum Soles\n7. Prismatic Feet\n8. Color Pop Socks\n9. Vivid Threads\n10. Sock-a-licious\n\nThese names incorporate elements of color, vibrancy, and fun, which align well with the concept of colorful socks. You can choose the one that best fits your brand personality and target audience.\n*/\n\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using ChatGPT Plugin Retriever in TypeScript\nDESCRIPTION: This snippet demonstrates how to set up and use the ChatGPT Plugin Retriever by importing the class, instantiating it with a URL and authentication token, and then invoking it to retrieve documents based on a query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/chatgpt-retriever-plugin.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatGPTPluginRetriever } from \"langchain/retrievers/remote\";\n\nconst retriever = new ChatGPTPluginRetriever({\n  url: \"http://0.0.0.0:8000\",\n  auth: {\n    bearer: \"super-secret-jwt-token-with-at-least-32-characters-long\",\n  },\n});\n\nconst docs = await retriever.invoke(\"hello world\");\n\nconsole.log(docs);\n```\n\n----------------------------------------\n\nTITLE: Creating Chat Prompt Template\nDESCRIPTION: Setting up a prompt template for translation with language and text parameters\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/llm_chain.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\"\n\nconst systemTemplate = \"Translate the following from English into {language}\"\n\nconst promptTemplate = ChatPromptTemplate.fromMessages(\n  [\n    [\"system\", systemTemplate],\n    [\"user\", \"{text}\"]\n  ]\n)\n```\n\n----------------------------------------\n\nTITLE: Invoking Stateful Chat Application\nDESCRIPTION: Demonstrates how to invoke the stateful chat application with a unique thread ID and process user queries while maintaining conversation history.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nimport { v4 as uuidv4 } from \"uuid\";\n\nconst threadId = uuidv4();\nconst config = { configurable: { thread_id: threadId } };\n\nconst result = await app.invoke(\n  { input: \"What is Task Decomposition?\" },\n  config,\n)\nconsole.log(result.answer);\n```\n\n----------------------------------------\n\nTITLE: Similarity Search with Scores\nDESCRIPTION: Executing a similarity search that returns both documents and their similarity scores\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/weaviate.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nconst similaritySearchWithScoreResults = await vectorStore.similaritySearchWithScore(\"biology\", 2, filter)\n\nfor (const [doc, score] of similaritySearchWithScoreResults) {\n  console.log(`* [SIM=${score.toFixed(3)}] ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`);\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing RaycastAI LLM from LangChain Community\nDESCRIPTION: Example showing how to initialize the RaycastAI model with optional parameters including rate limiting, model selection, and creativity (temperature) settings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/raycast.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RaycastAI } from \"@langchain/community/llms/raycast\";\n\nimport { Tool } from \"@langchain/core/tools\";\n\nconst model = new RaycastAI({\n  rateLimitPerMinute: 10, // It is 10 by default so you can omit this line\n  model: \"<model_name>\",\n  creativity: 0, // `creativity` is a term used by Raycast which is equivalent to `temperature` in some other LLMs\n});\n```\n\n----------------------------------------\n\nTITLE: Embedding Single Text Query\nDESCRIPTION: Demonstrates how to generate vector embeddings for a single text query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/bytedance_doubao.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst singleVector = await embeddings.embedQuery(text);\n\nconsole.log(singleVector.slice(0, 100));\n```\n\n----------------------------------------\n\nTITLE: Basic Content Extraction with Playwright\nDESCRIPTION: Example of loading and extracting raw HTML content from a webpage using Playwright\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/web_playwright.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport {\n  PlaywrightWebBaseLoader,\n  Page,\n  Browser,\n} from \"@langchain/community/document_loaders/web/playwright\";\n\nconst url = \"https://www.tabnews.com.br/\";\nconst loader = new PlaywrightWebBaseLoader(url);\nconst docs = await loader.load();\n\n// raw HTML page content\nconst extractedContents = docs[0].pageContent;\n```\n\n----------------------------------------\n\nTITLE: Setting up VectorStoreToolkit in TypeScript\nDESCRIPTION: This snippet demonstrates how to set up a VectorStoreToolkit. It includes loading a text file, splitting it into chunks, creating a vector store, and initializing the toolkit with the vector store information and an LLM.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/vectorstore.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { VectorStoreToolkit, VectorStoreInfo } from \"langchain/agents/toolkits\"\nimport { OpenAIEmbeddings } from \"@langchain/openai\"\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\"\nimport { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\nimport fs from \"fs\";\n\n// Load a text file to use as our data source.\nconst text = fs.readFileSync(\"../../../../../examples/state_of_the_union.txt\", \"utf8\");\n\n// Split the text into chunks before inserting to our store\nconst textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 1000 });\nconst docs = await textSplitter.createDocuments([text]);\n\nconst vectorStore = await MemoryVectorStore.fromDocuments(docs, new OpenAIEmbeddings());\n\nconst vectorStoreInfo: VectorStoreInfo = {\n  name: \"state_of_union_address\",\n  description: \"the most recent state of the Union address\",\n  vectorStore,\n};\n\nconst toolkit = new VectorStoreToolkit(vectorStoreInfo, llm);\n```\n\n----------------------------------------\n\nTITLE: Installing Zep Cloud and LangChain Dependencies\nDESCRIPTION: Command to install Zep Cloud SDK and required LangChain packages for integration. Includes the core Zep Cloud SDK along with LangChain OpenAI, community, and core packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/zep_memory_cloud.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @getzep/zep-cloud @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Creating a Retriever from FaissStore\nDESCRIPTION: Converts the vector store into a retriever object that can be used within chains, configuring the number of results to return for each query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/faiss.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst retriever = vectorStore.asRetriever({\n  k: 2,\n});\nawait retriever.invoke(\"biology\");\n```\n\n----------------------------------------\n\nTITLE: Using SpiderLoader with LangChain for Web Scraping\nDESCRIPTION: Example code that demonstrates how to use Spider for web scraping with LangChain, showing both scrape and crawl modes. It includes authentication, URL specification, and document loading.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/spider.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { SpiderLoader } from \"@langchain/community/document_loaders/web/spider\";\n\n// Create a loader for scraping a specific URL\nconst loader = new SpiderLoader({\n  apiKey: \"sk-spider-XXXXXXXXXXXX\",\n  baseUrl: \"https://js.langchain.com/docs/\",\n  params: {\n    // Additional parameters for the Spider API\n  },\n});\n\n// Load documents from the Spider API (scrape mode)\nconst docs = await loader.load();\n\n// The scrape result is a list of documents, one for each processed URL\nconsole.log({ docs });\n\n// Create a loader for crawling a website\nconst loaderCrawl = new SpiderLoader({\n  apiKey: \"sk-spider-XXXXXXXXXXXX\",\n  baseUrl: \"https://js.langchain.com/docs/\",\n  mode: \"crawl\", // crawl mode follows subpages\n  params: {\n    // Additional parameters for the Spider API\n  },\n});\n\n// Load documents from the Spider API (crawl mode)\nconst docsCrawl = await loaderCrawl.load();\n\n// The crawl result is a list of documents, one for each processed URL\nconsole.log({ docsCrawl });\n\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for OpenAI and Pinecone\nDESCRIPTION: Environment variables setup for OpenAI API key, Pinecone API key and index name, and optional LangSmith configuration for observability.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_per_user.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: env\nCODE:\n```\nOPENAI_API_KEY=your-api-key\n\nPINECONE_API_KEY=your-api-key\nPINECONE_INDEX=your-index-name\n\n# Optional, use LangSmith for best-in-class observability\nLANGSMITH_API_KEY=your-api-key\nLANGSMITH_TRACING=true\n\n# Reduce tracing latency if you are not in a serverless environment\n# LANGCHAIN_CALLBACKS_BACKGROUND=true\n```\n\n----------------------------------------\n\nTITLE: Using Base64 Encoded Images with OpenAI Vision in LangChain.js\nDESCRIPTION: This snippet demonstrates how to use a base64 encoded data URL for images when working with OpenAI's GPT-4 Vision model in LangChain.js, which is useful for processing images from files or fetched data.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/openai_vision_multimodal.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n// const imageData = await fs.readFile(path.join(__dirname, \"/data/hotdog.jpg\"));\n// const imageBase64 = imageData.toString(\"base64\");\nconst imageBase64 = \"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\n```\n\n----------------------------------------\n\nTITLE: Examining Initial Events from Chat Model Stream\nDESCRIPTION: Code that slices and displays the first few events from the event stream array, showing the structure of start events in the stream.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\nevents.slice(0, 3);\n```\n\n----------------------------------------\n\nTITLE: Updating Agent with Retriever Tool in JavaScript\nDESCRIPTION: This code updates the agent's system message and tools to include the new retriever tool for handling proper nouns in queries.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/sql_qa.ipynb#2025-04-22_snippet_22\n\nLANGUAGE: javascript\nCODE:\n```\n// Add to system message\nlet suffix = (\n    \"If you need to filter on a proper noun like a Name, you must ALWAYS first look up \" +\n    \"the filter value using the 'search_proper_nouns' tool! Do not try to \" +\n    \"guess at the proper name - use this function to find similar ones.\"\n)\n\nconst system = systemMessage + suffix\n\nconst updatedTools = tools.concat(retrieverTool)\n\nconst agent2 = createReactAgent({ llm: llm, tools: updatedTools, stateModifier: system });\n```\n\n----------------------------------------\n\nTITLE: Invoking DuckDuckGoSearch Directly - JavaScript/TypeScript\nDESCRIPTION: This snippet illustrates calling the DuckDuckGoSearch tool using the 'invoke' method and passing a plain string query. It triggers a search for the specified prompt (e.g., current weather in San Francisco). The method resolves with search results or related data; the input is a string, and the output is a Promise with the results.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/duckduckgo_search.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nawait tool.invoke(\"what is the current weather in sf?\")\n```\n\n----------------------------------------\n\nTITLE: Initializing WatsonxLLM with IAM Authentication\nDESCRIPTION: TypeScript code for creating a WatsonxLLM instance using IAM authentication method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/ibm.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { WatsonxLLM } from \"@langchain/community/llms/ibm\";\n\nconst props = {\n  version: \"YYYY-MM-DD\",\n  serviceUrl: \"<SERVICE_URL>\",\n  projectId: \"<PROJECT_ID>\",\n  watsonxAIAuthType: \"iam\",\n  watsonxAIApikey: \"<YOUR-APIKEY>\",\n};\nconst instance = new WatsonxLLM(props);\n```\n\n----------------------------------------\n\nTITLE: Querying Milvus Vector Store from Existing Collection in Typescript\nDESCRIPTION: This Typescript example illustrates how to query an existing collection in Milvus using OpenAI embeddings to perform a similarity search. Necessary packages are '@langchain/openai' and the Milvus SDK. The 'collectionName' parameter specifies the collection to query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/milvus.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Milvus } from \"langchain/vectorstores/milvus\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst vectorStore = await Milvus.fromExistingCollection(\n  new OpenAIEmbeddings(),\n  {\n    collectionName: \"goldel_escher_bach\",\n  }\n);\n\nconst response = await vectorStore.similaritySearch(\"scared\", 2);\n```\n\n----------------------------------------\n\nTITLE: Loading Documents with DirectoryLoader\nDESCRIPTION: Example showing how to load documents using the configured DirectoryLoader and access the first document\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/directory.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst docs = await loader.load()\n// disable console.warn calls\nconsole.warn = () => {}\ndocs[0]\n```\n\n----------------------------------------\n\nTITLE: Tracking Token Usage with Callbacks\nDESCRIPTION: Example showing how to use the handleLLMEnd callback to capture token usage information and other details from the model's response.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chat_token_usage_tracking.mdx#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { HumanMessage } from \"@langchain/core/messages\";\n\nconst model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });\n\n// Initialize an object to store token usage\nconst tokenUsage = {\n  promptTokens: 0,\n  completionTokens: 0,\n  totalTokens: 0,\n};\n\n// Define callback handlers\nconst callbackManager = {\n  handleLLMEnd: async (output) => {\n    // The output object contains all the response data including token usage\n    // console.log(\"Full output:\", JSON.stringify(output, null, 2));\n\n    // Extract token usage from the generation\n    // The structure may differ based on the model provider\n    const { completionTokens, promptTokens, totalTokens } =\n      output.llmOutput?.tokenUsage || {};\n\n    // Update our token usage object\n    tokenUsage.promptTokens += promptTokens || 0;\n    tokenUsage.completionTokens += completionTokens || 0;\n    tokenUsage.totalTokens += totalTokens || 0;\n  },\n};\n\n// Make multiple calls to the model\nawait model.invoke([new HumanMessage(\"Hello, how are you?\")], {\n  callbacks: [callbackManager],\n});\n\nawait model.invoke([new HumanMessage(\"Tell me a short joke.\")], {\n  callbacks: [callbackManager],\n});\n\n// Output the accumulated token usage\nconsole.log(\"Accumulated token usage:\", tokenUsage);\n\n```\n\n----------------------------------------\n\nTITLE: Adding Documents to MariaDB Vector Store - TypeScript\nDESCRIPTION: Shows how to add multiple documents with specified UUIDs to a MariaDB vector store. Each document includes page content and metadata, and UUIDs are generated for each to serve as unique identifiers. The vectorStore.addDocuments function is asynchronous; the ids parameter is optional but is used here for example clarity. Dependencies include 'uuid' and the LangChain Document type.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/mariadb.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { v4 as uuidv4 } from \"uuid\";\nimport type { Document } from \"@langchain/core/documents\";\n\nconst document1: Document = {\n  pageContent: \"The powerhouse of the cell is the mitochondria\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document2: Document = {\n  pageContent: \"Buildings are made out of brick\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document3: Document = {\n  pageContent: \"Mitochondria are made out of lipids\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document4: Document = {\n  pageContent: \"The 2024 Olympics are in Paris\",\n  metadata: { source: \"https://example.com\" }\n}\n\nconst documents = [document1, document2, document3, document4];\n\nconst ids = [uuidv4(), uuidv4(), uuidv4(), uuidv4()]\n\n// ids are not mandatory, but that's for the example\nawait vectorStore.addDocuments(documents, { ids: ids });\n\n```\n\n----------------------------------------\n\nTITLE: Importing FewShotChatMessagePromptTemplate from LangChainJS\nDESCRIPTION: Imports the necessary classes from LangChain to create few shot prompts for chat models.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport {\n  ChatPromptTemplate,\n  FewShotChatMessagePromptTemplate,\n} from \"langchain/prompts\";\n```\n\n----------------------------------------\n\nTITLE: Checking SQLite Database Connection in LangChain\nDESCRIPTION: TypeScript code to verify the connection to a SQLite database (Chinook) using the SqlDatabase class from LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/sql_prompting.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n<CodeBlock language=\"typescript\">{DbCheck}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Invoking Tavily Tool with Model-Generated ToolCall in JavaScript\nDESCRIPTION: Illustrates using a model-generated ToolCall to invoke the TavilyExtract tool. This approach exemplifies the typical structure of tool calls, including pre-configured arguments, identification, and naming convention.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/tavily_extract.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst modelGeneratedToolCall = {\n  args: {\"urls\": [\"https://en.wikipedia.org/wiki/Lionel_Messi\"]},\n  id: \"1\",\n  name: tool.name,\n  type: \"tool_call\",\n}\n\nawait tool.invoke(modelGeneratedToolCall)\n```\n\n----------------------------------------\n\nTITLE: Instantiating Google Vertex AI Model in LangChain.js\nDESCRIPTION: Create an instance of the VertexAI model with specific parameters such as model name, temperature, and maximum retries.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/google_vertex_ai.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { VertexAI } from \"@langchain/google-vertexai-web\"\n\nconst llm = new VertexAI({\n  model: \"gemini-pro\",\n  temperature: 0,\n  maxRetries: 2,\n  // other params...\n})\n```\n\n----------------------------------------\n\nTITLE: Instantiating ChatGoogleGenerativeAI Model\nDESCRIPTION: Creates an instance of the ChatGoogleGenerativeAI model with specific parameters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/google_generativeai.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatGoogleGenerativeAI } from \"@langchain/google-genai\"\n\nconst llm = new ChatGoogleGenerativeAI({\n    model: \"gemini-1.5-pro\",\n    temperature: 0,\n    maxRetries: 2,\n    // other params...\n})\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Faiss Indexes\nDESCRIPTION: Shows how to persist a Faiss vector store to disk and load it again, maintaining the same functionality between the original and loaded instances.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/faiss.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\n// Create a vector store through any method, here from texts as an example\nconst persistentStore = await FaissStore.fromTexts(\n  [\"Hello world\", \"Bye bye\", \"hello nice world\"],\n  [{ id: 2 }, { id: 1 }, { id: 3 }],\n  new OpenAIEmbeddings()\n);\n\n// Save the vector store to a directory\nconst directory = \"your/directory/here\";\n\nawait persistentStore.save(directory);\n\n// Load the vector store from the same directory\nconst loadedVectorStore = await FaissStore.load(\n  directory,\n  new OpenAIEmbeddings()\n);\n\n// vectorStore and loadedVectorStore are identical\nconst result = await loadedVectorStore.similaritySearch(\"hello world\", 1);\nconsole.log(result);\n```\n\n----------------------------------------\n\nTITLE: Configuring AzureChatOpenAI with a Custom Domain\nDESCRIPTION: Shows how to connect to an Azure OpenAI instance hosted under a domain other than the default openai.azure.com.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/azure.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport { AzureChatOpenAI } from \"@langchain/openai\";\n\nconst llmWithDifferentDomain = new AzureChatOpenAI({\n  temperature: 0.9,\n  azureOpenAIApiKey: \"<your_key>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY\n  azureOpenAIApiDeploymentName: \"<your_deployment_name>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME\n  azureOpenAIApiVersion: \"<api_version>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION\n  azureOpenAIBasePath:\n    \"https://westeurope.api.microsoft.com/openai/deployments\", // In Node.js defaults to process.env.AZURE_OPENAI_BASE_PATH\n});\n```\n\n----------------------------------------\n\nTITLE: Using SupabaseVectorStore as a Retriever (TypeScript)\nDESCRIPTION: This snippet shows how to convert the `SupabaseVectorStore` instance into a LangChain `Retriever` object. This allows seamless integration into retrieval-based chains (like RAG). It configures the retriever with an optional filter and `k` value, then invokes it with a query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/supabase.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nconst retriever = vectorStore.asRetriever({\n  // Optional filter\n  filter: filter,\n  k: 2,\n});\nawait retriever.invoke(\"biology\");\n```\n\n----------------------------------------\n\nTITLE: Llama CPP Stream Invoke with Abort\nDESCRIPTION: Example showing how to use the invoke method for stream generation with abort capability\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/llama_cpp.mdx#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nStreamInvokeExample\n```\n\n----------------------------------------\n\nTITLE: Implementing PostgreSQL Document Loader\nDESCRIPTION: Example of creating a document loader to extract data from PostgreSQL tables as LangChain Documents with custom column mappings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-google-cloud-sql-pg/README.md#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PostgresEngine, PostgresLoader } from \"@langchain/google-cloud-sql-pg\";\n\nconst documentLoaderArgs: PostgresLoaderOptions = {\n  tableName: \"test_table_custom\",\n  contentColumns: [ \"fruit_name\", \"variety\"],\n  metadataColumns: [\"fruit_id\", \"quantity_in_stock\", \"price_per_unit\", \"organic\"],\n  format: \"text\"\n};\n\nconst documentLoaderInstance = await PostgresLoader.initialize(PEInstance, documentLoaderArgs);\n\nconst documents = await documentLoaderInstance.load();\n```\n\n----------------------------------------\n\nTITLE: Using BaiduQianfanEmbeddings in TypeScript\nDESCRIPTION: Example code demonstrating how to use the BaiduQianfanEmbeddings class to generate embeddings for given text. It shows initialization of the embedding model and embedding generation for a sample text.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/baidu_qianfan.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport BaiduQianFanExample from \"@examples/embeddings/baidu_qianfan.ts\";\n```\n\n----------------------------------------\n\nTITLE: Using ChatGroq Model for Inference\nDESCRIPTION: Example of importing and initializing the ChatGroq model, creating a message, and invoking the model to get a response.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-groq/README.md#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatGroq } from \"@langchain/groq\";\nimport { HumanMessage } from \"@langchain/core/messages\";\n\nconst model = new ChatGroq({\n  apiKey: process.env.GROQ_API_KEY, // Default value.\n  model: \"llama-3.3-70b-versatile\",\n});\n\nconst message = new HumanMessage(\"What color is the sky?\");\n\nconst res = await model.invoke([message]);\n```\n\n----------------------------------------\n\nTITLE: Creating SAP HANA Vector Index from Texts using LangchainJS (TypeScript)\nDESCRIPTION: This TypeScript code demonstrates how to create a SAP HANA Cloud vector store index directly from an array of text documents using the `HanaDB` vector store class from `@langchain/community`. It likely involves initializing the vector store with connection details and embedding functions, then calling a method like `fromTexts` to process and store the text data and their embeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/hanavector.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { HanaDB } from \"@langchain/community/vectorstores/hana\";\n\n// Connection options for the database connection, read from environment variables\n// HANA_HOST, HANA_PORT, HANA_USER, HANA_PASSWORD\nconst connectionOptions = {\n  host: process.env.HANA_HOST ?? \"\",\n  port: process.env.HANA_PORT ?? \"\",\n  user: process.env.HANA_USER ?? \"\",\n  password: process.env.HANA_PASSWORD ?? \"\",\n};\n\n// Configuration for the vector store table\nconst vectorStoreTableConfig = {\n  tableName: \"MY_TABLE_NAME\",\n  schemaName: \"MY_SCHEMA_NAME\", // optional, default: current schema\n};\n\n// Input texts\nconst texts = [\n  \"SAP HANA Cloud is a database as a service (DBaaS) offering.\",\n  \"SAP HANA Cloud Vector Engine is a component of SAP HANA Cloud.\",\n  \"Langchain JS is a framework for developing applications powered by language models.\",\n];\n\n// Create embeddings model\nconst embeddings = new OpenAIEmbeddings({});\n\n// Create the vector store\nconst vectorStore = await HanaDB.fromTexts(\n  texts,\n  [], // metadata can be added here\n  embeddings,\n  connectionOptions,\n  vectorStoreTableConfig\n);\n\n// Perform a similarity search\nconst results = await vectorStore.similaritySearch(\"database as a service\", 1);\n\nconsole.log(results);\n\n/*\n  [\n    Document {\n      pageContent: 'SAP HANA Cloud is a database as a service (DBaaS) offering.',\n      metadata: {}\n    }\n  ]\n*/\n\n```\n\n----------------------------------------\n\nTITLE: Displaying Chain Results (TypeScript)\nDESCRIPTION: Shows how to access and log the `tool_calls` made during the chain execution and the final `content` (the AI's response) from the result object returned by the chain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/exa_search.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst { tool_calls, content } = toolChainResult;\n\nconsole.log(\"AIMessage\", JSON.stringify({\n  tool_calls,\n  content\n}, null, 2))\n```\n\n----------------------------------------\n\nTITLE: Setting up LLM and Tools\nDESCRIPTION: Initializes ChatOpenAI model and creates a magic tool function using the @langchain/core tools module.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/migrate_agent.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o-mini\",\n});\n\nconst magicTool = tool(async ({ input }: { input: number }) => {\n  return `${input + 2}`;\n}, {\n  name: \"magic_function\",\n  description: \"Applies a magic function to an input.\",\n  schema: z.object({\n    input: z.number(),\n  }),\n});\n\nconst tools = [magicTool];\n\nconst query = \"what is the value of magic_function(3)?\";\n```\n\n----------------------------------------\n\nTITLE: Initializing Weaviate Store\nDESCRIPTION: TypeScript code to initialize Weaviate client and create a store with sample texts and metadata using OpenAI embeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-weaviate/README.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport weaviate, { ApiKey } from 'weaviate-ts-client';\nimport { WeaviateStore } from \"@langchain/weaviate\";\n\n// Weaviate SDK has a TypeScript issue so we must do this.\nconst client = (weaviate as any).client({\n  scheme: process.env.WEAVIATE_SCHEME || \"https\",\n  host: process.env.WEAVIATE_HOST || \"localhost\",\n  apiKey: new ApiKey(\n    process.env.WEAVIATE_API_KEY || \"default\"\n  ),\n});\n\n// Create a store and fill it with some texts + metadata\nawait WeaviateStore.fromTexts(\n  [\"hello world\", \"hi there\", \"how are you\", \"bye now\"],\n  [{ foo: \"bar\" }, { foo: \"baz\" }, { foo: \"qux\" }, { foo: \"bar\" }],\n  new OpenAIEmbeddings(),\n  {\n    client,\n    indexName: \"Test\",\n    textKey: \"text\",\n    metadataKeys: [\"foo\"],\n  }\n);\n```\n\n----------------------------------------\n\nTITLE: Instantiating PineconeStore from Existing Index (TypeScript)\nDESCRIPTION: This code demonstrates how to instantiate `PineconeStore` in TypeScript. It imports necessary classes (`PineconeStore`, `OpenAIEmbeddings`, `PineconeClient`), initializes OpenAI embeddings and the Pinecone client, retrieves the Pinecone index using environment variables, and finally creates a `PineconeStore` instance connected to an existing index. It also shows configuration options like `maxConcurrency` and `namespace`.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/pinecone.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PineconeStore } from \"@langchain/pinecone\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nimport { Pinecone as PineconeClient } from \"@pinecone-database/pinecone\";\n\nconst embeddings = new OpenAIEmbeddings({\n  model: \"text-embedding-3-small\",\n});\n\nconst pinecone = new PineconeClient();\n// Will automatically read the PINECONE_API_KEY and PINECONE_ENVIRONMENT env vars\nconst pineconeIndex = pinecone.Index(process.env.PINECONE_INDEX!);\n\nconst vectorStore = await PineconeStore.fromExistingIndex(\n  embeddings,\n  {\n    pineconeIndex,\n    // Maximum number of batch requests to allow at once. Each batch is 1000 vectors.\n    maxConcurrency: 5,\n    // You can pass a namespace here too\n    // namespace: \"foo\",\n  }\n);\n```\n\n----------------------------------------\n\nTITLE: JSON Response Format Integration\nDESCRIPTION: Example of using ChatGroq with JSON response format for structured outputs, including both direct invocation and binding approach.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/groq.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst messages = [\n  {\n    role: \"system\",\n    content: \"You are a math tutor that handles math exercises and makes output in json in format { result: number }.\",\n  },\n  { role: \"user\",  content: \"2 + 2 * 2\" },\n];\n\nconst aiInvokeMsg = await llm.invoke(messages, { response_format: { type: \"json_object\" } });\n\n// if you want not to pass response_format in every invoke, you can bind it to the instance\nconst llmWithResponseFormat = llm.bind({ response_format: { type: \"json_object\" } });\nconst aiBindMsg = await llmWithResponseFormat.invoke(messages);\n\n// they are the same\nconsole.log({ aiInvokeMsgContent: aiInvokeMsg.content, aiBindMsg: aiBindMsg.content });\n```\n\n----------------------------------------\n\nTITLE: Setting Bearer Token Authentication Environment Variables for IBM watsonx.ai\nDESCRIPTION: Sets up environment variables for Bearer token authentication to access IBM watsonx.ai services by specifying the authentication type and bearer token.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/ibm.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport WATSONX_AI_AUTH_TYPE=bearertoken\nexport WATSONX_AI_BEARER_TOKEN=<YOUR-BEARER-TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Retrieving Log Probabilities from OpenAI Chat Model\nDESCRIPTION: Demonstrates how to configure the ChatOpenAI model to return log probabilities and access them from the response.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/logprobs.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst model = new ChatOpenAI({\n  model: \"gpt-4o\",\n  logprobs: true,\n});\n\nconst responseMessage = await model.invoke(\"how are you today?\");\n\nresponseMessage.response_metadata.logprobs.content.slice(0, 5);\n```\n\n----------------------------------------\n\nTITLE: Llama CPP Chain Integration\nDESCRIPTION: Demonstration of using Llama CPP with LangChain chains, requires a powerful model like 70B version\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/llama_cpp.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nChainExample\n```\n\n----------------------------------------\n\nTITLE: Installing Llama CPP Dependencies\nDESCRIPTION: Command to install required npm packages for using Llama CPP with LangChain\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/llama_cpp.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -S node-llama-cpp@3 @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Implementing Length-Based Example Selector in TypeScript\nDESCRIPTION: This code snippet demonstrates how to create and use a LengthBasedExampleSelector in LangChain.js. It sets up example prompts, defines a selector with length parameters, and shows how to use it with different input lengths.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/example_selectors_length_based.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { LengthBasedExampleSelector } from \"langchain/prompts\";\nimport { PromptTemplate } from \"langchain/prompts\";\n\nconst examples = [\n  { input: \"happy\", output: \"sad\" },\n  { input: \"tall\", output: \"short\" },\n  { input: \"energetic\", output: \"lethargic\" },\n  { input: \"sunny\", output: \"gloomy\" },\n  { input: \"windy\", output: \"calm\" },\n];\n\nconst examplePrompt = new PromptTemplate({\n  input_variables: [\"input\", \"output\"],\n  template: \"Input: {input}\\nOutput: {output}\",\n});\n\nconst exampleSelector = new LengthBasedExampleSelector({\n  examples: examples,\n  examplePrompt: examplePrompt,\n  maxLength: 25,\n});\n\n// The prompt template will load examples according to the length of the input.\nconst dynamicPrompt = new PromptTemplate({\n  template:\n    \"Give the antonym of every input\\n{examples}\\nInput: {input}\\nOutput:\",\n  input_variables: [\"input\"],\n  partial_variables: { examples: exampleSelector },\n});\n\n// An input with the length < 25\nconst shortInput = await dynamicPrompt.format({ input: \"big\" });\nconsole.log(shortInput);\n\n// An input with the length > 25\nconst longInput = await dynamicPrompt.format({\n  input: \"A very long input that goes on and on\",\n});\nconsole.log(longInput);\n```\n\n----------------------------------------\n\nTITLE: Creating a RAG Chain with AzionRetriever in TypeScript\nDESCRIPTION: Sets up a Retrieval-Augmented Generation (RAG) chain using AzionRetriever, ChatPromptTemplate, and a language model. It includes helper functions for formatting documents and creating a runnable sequence.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/azion-edgesql.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { RunnablePassthrough, RunnableSequence } from \"@langchain/core/runnables\";\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\n\nimport type { Document } from \"@langchain/core/documents\";\n\nconst prompt = ChatPromptTemplate.fromTemplate(`\nAnswer the question based only on the context provided.\n\nContext: {context}\n\nQuestion: {question}`);\n\nconst formatDocs = (docs: Document[]) => {\n  return docs.map((doc) => doc.pageContent).join(\"\\n\\n\");\n}\n\n// See https://js.langchain.com/docs/tutorials/rag\nconst ragChain = RunnableSequence.from([\n  {\n    context: retriever.pipe(formatDocs),\n    question: new RunnablePassthrough(),\n  },\n  prompt,\n  llm,\n  new StringOutputParser(),\n]);\n```\n\n----------------------------------------\n\nTITLE: Adding Documents to Vector Store in LangChain\nDESCRIPTION: This code shows how to add Document objects to a vector store. Each Document contains pageContent (the text content) and metadata (additional information about the document) that will be embedded and stored in the vector store.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/vectorstores.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Document } from \"@langchain/core/documents\";\n\nconst document1 = new Document(\n    pageContent: \"I had chocalate chip pancakes and scrambled eggs for breakfast this morning.\",\n    metadata: { source: \"tweet\" },\n)\n\nconst document2 = new Document(\n    pageContent: \"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",\n    metadata: { source: \"news\" },\n)\n\nconst documents = [document1, document2]\n\nawait vectorStore.addDocuments(documents)\n```\n\n----------------------------------------\n\nTITLE: Configuring Retriever with Default Search Parameters\nDESCRIPTION: Implementation of a self-query retriever with default search parameters and filters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/weaviate.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconst selfQueryRetrieverWithDefaultParams = SelfQueryRetriever.fromLLM({\n  llm: llm,\n  vectorStore: vectorStore,\n  documentContents: \"Brief summary of a movie\",\n  attributeInfo: attributeInfo,\n  structuredQueryTranslator: new WeaviateTranslator(),\n  searchParams: {\n    filter: {\n      where: {\n        operator: \"Equal\",\n        path: [\"type\"],\n        valueText: \"movie\",\n      },\n    },\n    mergeFiltersOperator: \"or\",\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Invoking TogetherAI Model for Text Completion\nDESCRIPTION: Demonstrates how to invoke the TogetherAI model with input text to generate a completion. This example sends a prompt to the model and retrieves the generated text response.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/together.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst inputText = \"Together is an AI company that \"\n\nconst completion = await llm.invoke(inputText)\ncompletion\n```\n\n----------------------------------------\n\nTITLE: Creating a RunnableLambda Function in TypeScript\nDESCRIPTION: Demonstrates how to create a RunnableLambda function, which allows defining generic TypeScript functions as runnables, and how to chain it with other runnables.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/lcel.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst someFunc = RunnableLambda.from((input) => {\n  return input;\n});\n\nconst chain = someFunc.pipe(runnable1);\n```\n\n----------------------------------------\n\nTITLE: Loading YouTube Transcripts with LangChain\nDESCRIPTION: Example code referenced in the document that demonstrates how to use the YouTube transcript loader. This would include specifying a video URL, optional language setting in ISO 639-1 format, and a flag to add video information.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/youtube.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nExample\n```\n\n----------------------------------------\n\nTITLE: Creating Multimodal Chat Prompt Templates in LangChain.js\nDESCRIPTION: This snippet shows how to construct ChatPromptTemplates with multimodal messages to create chains for processing both text and images in LangChain.js.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/openai_vision_multimodal.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate, MessagesPlaceholder } from \"npm:langchain@0.0.185/prompts\";\nimport { StringOutputParser } from \"npm:langchain@0.0.185/schema/output_parser\";\n\nconst prompt = ChatPromptTemplate.fromMessages([\n  [\"system\", \"Answer all questions like a pirate.\"],\n  new MessagesPlaceholder(\"input\"),\n]);\n\nconst chain = prompt.pipe(chat).pipe(new StringOutputParser());\n\nawait chain.invoke({\n  input: [\n    hostedImageMessage,\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Advanced Upstash Redis Chat Memory Usage\nDESCRIPTION: Advanced example demonstrating direct usage of Upstash Redis client instance for chat memory implementation\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/upstash_redis.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n{AdvancedExample}\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Install the necessary npm packages including Upstash Ratelimit and LangChain community packages for implementing rate limiting.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/callbacks/upstash_ratelimit_callback.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @upstash/ratelimit @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: JSON Schema Definition for Response Formatting\nDESCRIPTION: Demonstrates how to define a structured output schema using JSON Schema format.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/structured_outputs.mdx#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n  \"$id\": \"https://example.com/product.schema.json\",\n  \"title\": \"ResponseFormatter\",\n  \"type\": \"object\",\n  \"properties\": {\n    \"answer\": {\n      \"description\": \"The answer to the user's question\",\n      \"type\": \"string\"\n    },\n    \"followup_question\": {\n      \"description\": \"A followup question the user could ask\",\n      \"type\": \"string\"\n    }\n  },\n  \"required\": [\"answer\", \"followup_question\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Dependencies for Minimax Integration\nDESCRIPTION: This snippet shows how to install the necessary LangChain packages for integrating with Minimax using npm or yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/minimax.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Creating a Chain for Multiple Image Comparison\nDESCRIPTION: This snippet demonstrates how to create a chain for comparing multiple images by piping the prompt template to the ChatOpenAI model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/multimodal_prompts.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst chainWithMultipleImages = promptWithMultipleImages.pipe(model);\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search with Scores\nDESCRIPTION: Example of performing a similarity search that returns both documents and their similarity scores. Shows how to process and display the results with scores.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/vectorstores.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nconst similaritySearchWithScoreResults = await vectorStore.similaritySearchWithScore(\"biology\", 2, filter)\n\nfor (const [doc, score] of similaritySearchWithScoreResults) {\n  console.log(`* [SIM=${score.toFixed(3)}] ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`);\n}\n```\n\n----------------------------------------\n\nTITLE: Invoking a Bedrock LLM with Claude-v2 specific prompt format\nDESCRIPTION: Example showing how to invoke the Bedrock LLM with the required prompt format for Anthropic's Claude-v2 model, which needs to start with 'Human: '.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/bedrock.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconst inputText = \"Human: Bedrock is an AI company that\\nAssistant: \"\n\nconst completion = await llm.invoke(inputText)\ncompletion\n```\n\n----------------------------------------\n\nTITLE: Configuring Tool Caching with Anthropic\nDESCRIPTION: Demonstrates how to implement tool caching by setting cache control parameters within an Anthropic tool definition using the ephemeral cache type.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/anthropic.ipynb#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nconst SOME_LONG_DESCRIPTION = \"...\";\n\nconst anthropicTools = [{\n  name: \"get_weather\",\n  description: SOME_LONG_DESCRIPTION,\n  input_schema: {\n    type: \"object\",\n    properties: {\n      location: {\n        type: \"string\",\n        description: \"Location to get the weather for\",\n      },\n      unit: {\n        type: \"string\",\n        description: \"Temperature unit to return\",\n      },\n    },\n    required: [\"location\"],\n  },\n  cache_control: { type: \"ephemeral\" },\n}]\n```\n\n----------------------------------------\n\nTITLE: Importing and Setting Up Couchbase Connection in LangChain.js\nDESCRIPTION: Importing the CouchbaseDocumentLoader from LangChain community package and the Couchbase Cluster module, then setting up the connection parameters and defining a SQL++ query to retrieve documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/couchbase.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CouchbaseDocumentLoader } from \"@langchain/community/document_loaders/web/couchbase\";\nimport { Cluster } from \"couchbase\";\n\nconst connectionString = \"couchbase://localhost\"; // valid couchbase connection string\nconst dbUsername = \"Administrator\"; // valid database user with read access to the bucket being queried\nconst dbPassword = \"Password\"; // password for the database user\n\n// query is a valid SQL++ query\nconst query = `\n    SELECT h.* FROM \\`travel-sample\\`.inventory.hotel h \n    WHERE h.country = 'United States'\n    LIMIT 1\n`;\n```\n\n----------------------------------------\n\nTITLE: Streaming LangGraph Steps in JavaScript\nDESCRIPTION: This snippet shows how to stream the steps of a LangGraph application, allowing for real-time updates as the graph processes the input.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(inputs)\nconsole.log(\"\\n====\\n\");\nfor await (\n  const chunk of await graph.stream(inputs, {\n    streamMode: \"updates\",\n  })\n) {\n  console.log(chunk);\n  console.log(\"\\n====\\n\");\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Nomic Package Dependencies\nDESCRIPTION: Commands to install the required Nomic integration packages for LangChain.js via npm or yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-nomic/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/nomic @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Core Dependencies for Google Vertex AI Matching Engine (Node.js)\nDESCRIPTION: Installs the required npm packages: `@langchain/community` for the Matching Engine vector store, `@langchain/core` for base Langchain functionalities, and `google-auth-library` for handling Google Cloud authentication in a Node.js environment. These packages are essential prerequisites for using the Google Vertex AI Matching Engine integration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/googlevertexai.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core google-auth-library\n```\n\n----------------------------------------\n\nTITLE: Loading Web Documents with CheerioWebBaseLoader\nDESCRIPTION: Demonstrates how to load and parse web content using CheerioWebBaseLoader to extract paragraph text from a URL. The loader creates a Document object containing the parsed content.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport \"cheerio\";\nimport { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\";\n\nconst pTagSelector = \"p\";\nconst cheerioLoader = new CheerioWebBaseLoader(\n  \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n  {\n    selector: pTagSelector\n  }\n);\n\nconst docs = await cheerioLoader.load();\n\nconsole.assert(docs.length === 1);\nconsole.log(`Total characters: ${docs[0].pageContent.length}`);\n```\n\n----------------------------------------\n\nTITLE: Setting Additional Optional Configuration Parameters for CassandraStore\nDESCRIPTION: Shows additional optional parameters available in the `CassandraStore` configuration along with their default values. `maxConcurrency` controls concurrent requests, `batchSize` defines the number of documents per batch insert (unlogged), and `withClause` allows adding custom `WITH` clauses to the `CREATE TABLE` statement.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/cassandra.mdx#2025-04-22_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\n  ...,\n  maxConcurrency: 25,\n  batchSize: 1,\n  withClause: \"\",\n  ...\n```\n\n----------------------------------------\n\nTITLE: Tracking Token Usage with AIMessage.response_metadata for OpenAI\nDESCRIPTION: Example showing how to access token usage information from the response_metadata field of AIMessage objects when using OpenAI models.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chat_token_usage_tracking.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { HumanMessage, AIMessage } from \"@langchain/core/messages\";\n\nconst model = new ChatOpenAI({\n  modelName: \"gpt-3.5-turbo\",\n});\n\nconst response = await model.invoke([new HumanMessage(\"Hello world!\")]);\n\nconsole.log(JSON.stringify(response.response_metadata, null, 2));\n/*\n{\n  \"token_usage\": {\n    \"completion_tokens\": 10,\n    \"prompt_tokens\": 11,\n    \"total_tokens\": 21\n  },\n  \"model_name\": \"gpt-3.5-turbo-0125\",\n  \"system_fingerprint\": \"fp_c7e392ae99\",\n  \"finish_reason\": \"stop\",\n  \"logprobs\": null\n}\n*/\n\n// Accessing the prompt and completion token usage for a specific request\nconsole.log(\n  `Prompt tokens: ${response.response_metadata?.token_usage?.prompt_tokens}`\n);\nconsole.log(\n  `Completion tokens: ${\n    response.response_metadata?.token_usage?.completion_tokens\n  }`\n);\nconsole.log(\n  `Total tokens: ${response.response_metadata?.token_usage?.total_tokens}`\n);\n\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment Variables for Upstash and OpenAI Credentials - TypeScript\nDESCRIPTION: This snippet sets critical environment variables needed to connect to the Upstash Vector REST API and OpenAI API. These variables store credentials and endpoints required for vector indexing and embedding generation. Ensure these variables (UPSTASH_VECTOR_REST_URL, UPSTASH_VECTOR_REST_TOKEN, and OPENAI_API_KEY) are set before initializing any vector store or embedding-related code; otherwise, API calls will fail.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/upstash.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.UPSTASH_VECTOR_REST_URL = \"your-rest-url\";\nprocess.env.UPSTASH_VECTOR_REST_TOKEN = \"your-rest-token\";\n```\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.OPENAI_API_KEY = \"YOUR_API_KEY\";\n```\n\nLANGUAGE: typescript\nCODE:\n```\n// process.env.LANGSMITH_TRACING=\"true\"\n// process.env.LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Connection to Apache Cassandra in TypeScript\nDESCRIPTION: Defines a TypeScript configuration object (`configConnection`) for connecting to a standard Apache Cassandra 5.0+ cluster. It specifies contact points, the local data center, and credentials (username/password). This object is used later when initializing the `CassandraStore`.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/cassandra.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst configConnection = {\n  contactPoints: ['h1', 'h2'],\n  localDataCenter: 'datacenter1',\n  credentials: {\n    username: <...> as string,\n    password: <...> as string,\n  },\n};\n```\n\n----------------------------------------\n\nTITLE: Using StreamEvents API with Non-Streaming Components\nDESCRIPTION: This snippet shows how streamEvents can still provide streaming output from intermediate steps even when some components don't support streaming. It processes events and logs different types of streaming chunks.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming.ipynb#2025-04-22_snippet_23\n\nLANGUAGE: typescript\nCODE:\n```\nconst eventStream = await chain.streamEvents(\n  `output a list of the countries france, spain and japan and their populations in JSON format.\nUse a dict with an outer key of \"countries\" which contains a list of countries.\nEach country should have the key \"name\" and \"population\"\nYour output should ONLY contain valid JSON data. Do not include any other text or content in your output.`,\n  { version: \"v2\" },\n);\n\nlet eventCount = 0;\n\nfor await (const event of eventStream) {\n  // Truncate the output\n  if (eventCount > 30) {\n    continue;\n  }\n  const eventType = event.event;\n  if (eventType === \"on_chat_model_stream\") {\n    console.log(`Chat model chunk: ${event.data.chunk.message.content}`);\n  } else if (eventType === \"on_parser_stream\") {\n    console.log(`Parser chunk: ${JSON.stringify(event.data.chunk)}`);\n  } else {\n    console.log(eventType)\n  }\n  eventCount += 1;\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Replicate and LangChain Dependencies\nDESCRIPTION: This code snippet shows how to install the necessary dependencies for using Replicate with LangChain.js. It includes the Replicate client, LangChain community package, and LangChain core package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/replicate.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install replicate@1 @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Invoking ExaSearchResults with ToolCall (TypeScript)\nDESCRIPTION: Illustrates invoking the `ExaSearchResults` tool using a `ToolCall` object, simulating how a language model might call the tool. The `ToolCall` includes arguments (the input query), an ID, the tool name, and the type. The tool returns a `ToolMessage`.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/exa_search.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\n// This is usually generated by a model, but we'll create a tool call directly for demo purposes.\nconst modelGeneratedToolCall = {\n  args: {\n    input: \"what is the weather in wailea\"\n  },\n  id: \"1\",\n  name: tool.name,\n  type: \"tool_call\",\n}\nawait tool.invoke(modelGeneratedToolCall)\n```\n\n----------------------------------------\n\nTITLE: Importing Bedrock for AWS Bedrock LLMs in TypeScript\nDESCRIPTION: This code demonstrates how to import the Bedrock class from the LangChain community package for using AWS Bedrock language models.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/aws.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Bedrock } from \"@langchain/community/llms/bedrock\";\n```\n\n----------------------------------------\n\nTITLE: Defining Step-Back Examples and Prompt Template\nDESCRIPTION: Sets up example questions and their step-back versions, creating a few-shot prompt template for generating step-back questions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/step_back.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nconst examples = [\n  {\n    input: \"Could the members of The Police perform lawful arrests?\",\n    output: \"what can the members of The Police do?\",\n  },\n  {\n    input: \"Jan Sindel's was born in what country?\",\n    output: \"what is Jan Sindel's personal history?\",\n  },\n];\nconst examplePrompt = ChatPromptTemplate.fromMessages([\n  [\"human\", \"{input}\"],\n  [\"ai\", \"{output}\"],\n]);\nconst fewShotPrompt = new FewShotChatMessagePromptTemplate({\n  examplePrompt,\n  examples,\n  inputVariables: [], // no input variables\n});\n```\n\n----------------------------------------\n\nTITLE: Invoking Google Scholar Tool with Direct Arguments\nDESCRIPTION: Example of directly calling the tool with search parameters to retrieve academic publications about neural networks.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/google_scholar.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconst results = await tool.invoke({\n  query: \"neural networks\",\n  maxResults: 5,\n});\n\nconsole.log(results);\n```\n\n----------------------------------------\n\nTITLE: Loading JSON Without JSON Pointer in TypeScript\nDESCRIPTION: Demonstrates loading all string values from a JSON file without specifying a JSON pointer. The loader extracts all strings it finds in the JSON object and returns them as Document objects.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/document_loaders_json.mdx#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"texts\": [\"This is a sentence.\", \"This is another sentence.\"]\n}\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSONLoader } from \"langchain/document_loaders/fs/json\";\n\nconst loader = new JSONLoader(\"src/document_loaders/example_data/example.json\");\n\nconst docs = await loader.load();\n/*\n[\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/json\",\n      \"line\": 1,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"This is a sentence.\",\n  },\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/json\",\n      \"line\": 2,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"This is another sentence.\",\n  },\n]\n*/\n```\n\n----------------------------------------\n\nTITLE: Initializing WatsonxEmbeddings with IAM Authentication\nDESCRIPTION: Creates an instance of WatsonxEmbeddings using IAM authentication. This includes setting the version, service URL, project ID, authentication type, and API key.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/ibm.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { WatsonxEmbeddings } from \"@langchain/community/embeddings/ibm\";\n\nconst props = {\n  version: \"YYYY-MM-DD\",\n  serviceUrl: \"<SERVICE_URL>\",\n  projectId: \"<PROJECT_ID>\",\n  watsonxAIAuthType: \"iam\",\n  watsonxAIApikey: \"<YOUR-APIKEY>\",\n};\nconst instance = new WatsonxEmbeddings(props);\n```\n\n----------------------------------------\n\nTITLE: Creating a Retriever Tool\nDESCRIPTION: Code to create a retriever tool using CheerioWebBaseLoader, MemoryVectorStore, and OpenAIEmbeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/agent_executor.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\n\nconst loader = new CheerioWebBaseLoader(\"https://docs.smith.langchain.com/overview\");\nconst docs = await loader.load();\nconst splitter = new RecursiveCharacterTextSplitter(\n  {\n    chunkSize: 1000,\n    chunkOverlap: 200\n  }\n);\nconst documents = await splitter.splitDocuments(docs);\nconst vectorStore = await MemoryVectorStore.fromDocuments(documents, new OpenAIEmbeddings());\nconst retriever = vectorStore.asRetriever();\n\n(await retriever.invoke(\"how to upload a dataset\"))[0];\n```\n\n----------------------------------------\n\nTITLE: Invoking a Tool Directly in TypeScript\nDESCRIPTION: Shows how to invoke a tool directly with arguments, which returns only the content part of the output.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/custom_tools.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nawait generateRandomInts.invoke({ min: 0, max: 9, size: 10 });\n```\n\n----------------------------------------\n\nTITLE: Instantiating a Cohere Completion Model\nDESCRIPTION: Creates a Cohere LLM instance with configuration options including model selection, temperature, token limits, and retry settings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/cohere.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { Cohere } from \"@langchain/cohere\"\n\nconst llm = new Cohere({\n  model: \"command\",\n  temperature: 0,\n  maxTokens: undefined,\n  maxRetries: 2,\n  // other params...\n})\n```\n\n----------------------------------------\n\nTITLE: Retaining Element Structure with UnstructuredLoader\nDESCRIPTION: Example of using UnstructuredLoader with the 'by_title' chunking strategy to preserve the element structure of the Markdown document during loading.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/document_loader_markdown.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconst loaderByTitle = new UnstructuredLoader(markdownPath, {\n  chunkingStrategy: \"by_title\"\n});\n\n\nconst loadedDocs = await loaderByTitle.load()\n\nconsole.log(`Number of documents: ${loadedDocs.length}\\n`)\n\nfor (const doc of loadedDocs.slice(0, 2)) {\n    console.log(doc);\n    console.log(\"\\n\");\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing VertexAIEmbeddings Model\nDESCRIPTION: Creating an instance of the VertexAIEmbeddings model with specified configuration\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/google_vertex_ai.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { VertexAIEmbeddings } from \"@langchain/google-vertexai\";\n// Uncomment the following line if you're running in a web environment:\n// import { VertexAIEmbeddings } from \"@langchain/google-vertexai-web\"\n\nconst embeddings = new VertexAIEmbeddings({\n  model: \"text-embedding-004\",\n  // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Similarity Search with Scores in Upstash Vector Store - TypeScript/JavaScript\nDESCRIPTION: This code snippet performs a similarity search and returns both matching documents and their similarity scores. The similaritySearchWithScore method is invoked with the same parameters as similaritySearch and results are output with formatted similarity scores. This is useful for ranking or thresholding results in downstream applications.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/upstash.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst similaritySearchWithScoreResults = await vectorStore.similaritySearchWithScore(\"biology\", 2, filter)\n\nfor (const [doc, score] of similaritySearchWithScoreResults) {\n  console.log(`* [SIM=${score.toFixed(3)}] ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`);\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Subtitles from Audio Using AssemblyAI in TypeScript\nDESCRIPTION: Example of using AssemblyAI's AudioSubtitleLoader to generate subtitles (srt or vtt format) from an audio file and load them as a document in LangChain.js.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/assemblyai_audio_transcription.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport SubtitleExample from \"@examples/document_loaders/assemblyai_subtitles.ts\";\n```\n\n----------------------------------------\n\nTITLE: Invoking ChatPerplexity with System and User Messages\nDESCRIPTION: Example of invoking the ChatPerplexity model with system and user messages to perform an English to French translation task.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/perplexity.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst aiMsg = await llm.invoke([\n  {\n    role: \"system\",\n    content:\n      \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n  },\n  {\n    role: \"user\",\n    content: \"I love programming.\",\n  },\n]);\naiMsg;\n```\n\n----------------------------------------\n\nTITLE: Installing USearch Node.js Binding\nDESCRIPTION: Installs the `usearch` package, which provides Node.js bindings for the USearch library, enabling its use within a Node.js environment for similarity search.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/usearch.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -S usearch\n```\n\n----------------------------------------\n\nTITLE: Setting Up SqlToolkit with LLM in TypeScript\nDESCRIPTION: This code snippet defines the setup for the SqlToolkit using the LangChainJS framework. It involves importing required classes, configuring a data source for SQLite database, and instantiating the toolkit with LLModel from OpenAI. Ensure `langchain` and `typeorm` are installed as dependencies, and adjust the database path as needed.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/sql.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \\\"@langchain/openai\\\";\n\nconst llm = new ChatOpenAI({\n  model: \\\"gpt-4o-mini\\\",\n  temperature: 0,\n})\n\nimport { SqlToolkit } from \\\"langchain/agents/toolkits/sql\\\"\nimport { DataSource } from \\\"typeorm\\\";\nimport { SqlDatabase } from \\\"langchain/sql_db\\\";\n\nconst datasource = new DataSource({\n  type: \\\"sqlite\\\",\n  database: \\\"../../../../../../Chinook.db\\\", // Replace with the link to your database\n});\nconst db = await SqlDatabase.fromDataSourceParams({\n  appDataSource: datasource,\n});\n\nconst toolkit = new SqlToolkit(db, llm);\n```\n\n----------------------------------------\n\nTITLE: Implementing Output Validation\nDESCRIPTION: Defines and executes validation logic using the Zod schema to check if the generated output meets the required constraints.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/basic_critique_revise.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst outputValidator = (output: unknown) => zodSchema.safeParse(output);\n\nlet validatorResult = outputValidator(result);\n\nconsole.log(JSON.stringify(validatorResult, null, 2));\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for OpenAI and Milvus\nDESCRIPTION: Exports environment variables for OpenAI API keys and Milvus URL. These variables are essential for authenticating with OpenAI and connecting to a Milvus instance.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/milvus.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=YOUR_OPENAI_API_KEY_HERE\nexport MILVUS_URL=YOUR_MILVUS_URL_HERE # for example http://localhost:19530\n```\n\n----------------------------------------\n\nTITLE: Handling Missing Jina API Token Error in TypeScript\nDESCRIPTION: This snippet illustrates basic error handling when initializing `JinaEmbeddings`. If the API token is neither passed to the constructor nor found in the `JINA_API_KEY` environment variable, the constructor will throw an error, which is caught and logged here.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/jina.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\ntry {\n  const embeddings = new JinaEmbeddings();\n} catch (error) {\n  console.error(\"Jina API token not found\");\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Up a Self-Hosted pgvector Instance with Docker Compose\nDESCRIPTION: This Docker Compose configuration sets up a self-hosted PostgreSQL instance with the pgvector extension. It utilizes the prebuilt ankane/pgvector Docker image and exposes the database on port 5432. Environment variables for database credentials need to be set.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/prisma.mdx#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nservices:\n  db:\n    image: ankane/pgvector\n    ports:\n      - 5432:5432\n    volumes:\n      - db:/var/lib/postgresql/data\n    environment:\n      - POSTGRES_PASSWORD=\n      - POSTGRES_USER=\n      - POSTGRES_DB=\n\nvolumes:\n  db:\n```\n\n----------------------------------------\n\nTITLE: Closing Redis Client Connection in Node.js\nDESCRIPTION: Close the Redis client connection after operations are complete to save resources and prevent memory leaks. Proper disconnection is a part of good practices in client-server interactions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/redis.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nawait client.disconnect();\n```\n\n----------------------------------------\n\nTITLE: Streaming Events over HTTP with TextDecoder\nDESCRIPTION: This snippet demonstrates how to stream events over HTTP as server-sent events and decode them using TextDecoder. It sets up a chain with a model and JSON parser, then streams events with text/event-stream encoding.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming.ipynb#2025-04-22_snippet_19\n\nLANGUAGE: typescript\nCODE:\n```\nconst chain = model\n  .pipe(new JsonOutputParser().withConfig({ runName: \"my_parser\" }))\n  .withConfig({ tags: [\"my_chain\"] });\n\n\nconst eventStream = await chain.streamEvents(\n  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key \"name\" and \"population\"`,\n  {\n    version: \"v2\",\n    encoding: \"text/event-stream\",\n  },\n);\n\nlet eventCount = 0;\n\nconst textDecoder = new TextDecoder();\n\nfor await (const event of eventStream) {\n  // Truncate the output\n  if (eventCount > 3) {\n    continue;\n  }\n  console.log(textDecoder.decode(event));\n  eventCount += 1;\n}\n```\n\n----------------------------------------\n\nTITLE: Logging AzureChatOpenAI Response Content\nDESCRIPTION: Extracts and logs the content from the AI message response.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/azure.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(aiMsg.content)\n```\n\n----------------------------------------\n\nTITLE: Initializing PromptLayerChatOpenAI with Request ID Tracking\nDESCRIPTION: Shows how to create a PromptLayerChatOpenAI instance with request ID tracking enabled and use it to generate translations. The example demonstrates creating a system message for English to French translation and processing the response which includes the PromptLayer request ID.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/prompt_layer_openai.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PromptLayerChatOpenAI } from \"langchain/llms/openai\";\n\nconst chat = new PromptLayerChatOpenAI({\n  returnPromptLayerId: true,\n});\n\nconst respA = await chat.generate([\n  [\n    new SystemMessage(\n      \"You are a helpful assistant that translates English to French.\"\n    ),\n  ],\n]);\n\nconsole.log(JSON.stringify(respA, null, 3));\n```\n\n----------------------------------------\n\nTITLE: Implementing a Wikipedia Query Tool in TypeScript\nDESCRIPTION: This code demonstrates how to import and use the WikipediaQueryRun tool from LangChain to query Wikipedia. It shows initialization of the tool and running a sample query about the 2020 World Series.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/wikipedia.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { WikipediaQueryRun } from \"langchain/tools\";\n\nconst tool = new WikipediaQueryRun({\n  topKResults: 3,\n  maxDocContentLength: 4000,\n});\n\nconst result = await tool.invoke(\"Who won the 2020 World Series?\");\n\nconsole.log(result);\n\n```\n\n----------------------------------------\n\nTITLE: Installing DeepSeek and LangChain Core Dependencies\nDESCRIPTION: Command to install the necessary packages for using DeepSeek with LangChain.js. It installs both the DeepSeek integration and the LangChain core package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-deepseek/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/deepseek @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Instantiating OllamaEmbeddings Model\nDESCRIPTION: Creation of an OllamaEmbeddings instance with default configuration for model and base URL.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/ollama.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { OllamaEmbeddings } from \"@langchain/ollama\";\n\nconst embeddings = new OllamaEmbeddings({\n  model: \"mxbai-embed-large\", // Default value\n  baseUrl: \"http://localhost:11434\", // Default value\n});\n```\n\n----------------------------------------\n\nTITLE: Invoking TavilySearch Tool Directly with Arguments\nDESCRIPTION: Demonstrates direct invocation of the TavilySearch tool with a search query parameter.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/tavily_search.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nawait tool.invoke({\n  query: \"what is the current weather in SF?\"\n});\n```\n\n----------------------------------------\n\nTITLE: Creating SelfQueryRetriever with Default Search Params\nDESCRIPTION: Shows how to create a SelfQueryRetriever with default search parameters for additional filtering.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/hnswlib.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst selfQueryRetrieverWithDefaults = SelfQueryRetriever.fromLLM({\n  llm,\n  vectorStore,\n  documentContents: \"Brief summary of a movie\",\n  attributeInfo,\n  structuredQueryTranslator: new FunctionalTranslator(),\n  searchParams: {\n    filter: (doc: Document) => doc.metadata && doc.metadata.rating > 8.5,\n    mergeFiltersOperator: \"and\",\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Creating USearch Vector Store from Loader (TypeScript)\nDESCRIPTION: Illustrates creating a USearch vector store from documents loaded using a `TextLoader` and split using `RecursiveCharacterTextSplitter`. It uses the static `fromDocuments` method with OpenAI embeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/usearch.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { USearch } from \"@langchain/community/vectorstores/usearch\";\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\n\n// Load the document\nconst loader = new TextLoader(\"src/document_loaders/example_data/example.txt\");\nconst docs = await loader.load();\n\n// Split the document into chunks\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 1000,\n  chunkOverlap: 0,\n});\nconst splitDocs = await splitter.splitDocuments(docs);\n\n// Initialize the vector store\nconst vectorStore = await USearch.fromDocuments(\n  splitDocs,\n  new OpenAIEmbeddings()\n);\n\n// Search for the most similar document\nconst resultOne = await vectorStore.similaritySearchWithScore(\"hello world\", 1);\nconsole.log(resultOne);\n\n/*\n  [\n    [\n      Document {\n        pageContent: 'This is a sentence.',\n        metadata: [Object]\n      },\n      0.8120021820068359\n    ]\n  ]\n*/\n```\n\n----------------------------------------\n\nTITLE: Deleting Documents from Vector Store Using deleteDocuments Method\nDESCRIPTION: This code shows how to delete documents from a vector store by providing an array of document IDs to the deleteDocuments method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/vectorstores.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nawait vectorStore.deleteDocuments([\"doc1\"]);\n```\n\n----------------------------------------\n\nTITLE: Installing Playwright Browser Dependencies\nDESCRIPTION: This command installs the browser dependencies needed by Playwright when running the browser locally. This is required for local browser automation with Stagehand.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/stagehand.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpx playwright install\n```\n\n----------------------------------------\n\nTITLE: Invoking ChatNovitaAI Model for Translation\nDESCRIPTION: Shows how to invoke the ChatNovitaAI model with system and human messages to perform an English to French translation task.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/novita.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst aiMsg = await llm.invoke([\n  {\n    role: \"system\",\n    content: \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n  },\n  {\n    role: \"human\",\n    content: \"I love programming.\"\n  },\n]);\n```\n\n----------------------------------------\n\nTITLE: Parser Validation Examples\nDESCRIPTION: Shows validation behavior of StructuredOutputParser with invalid and valid inputs using AIMessage.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/output_parser_structured.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { AIMessage } from \"@langchain/core/messages\";\n\nawait parser.invoke(new AIMessage(`{\"badfield\": \"foo\"}`));\n```\n\n----------------------------------------\n\nTITLE: Streaming Agent Invocation and Processing Events in Typescript\nDESCRIPTION: Shows how to invoke the created agent using the `stream` method with a user query. It iterates through the asynchronous event stream, logging tool calls and final content messages to the console. Requires the `agent` instance created previously.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/ibm.mdx#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst exampleQuery = \"Who won F1 championship in 2022?\";\n\nconst events = await agent.stream(\n  { messages: [{ role: \"user\", content: exampleQuery }] },\n  { streamMode: \"values\" }\n);\n\nfor await (const event of events) {\n  const lastMsg = event.messages[event.messages.length - 1];\n  if (lastMsg.tool_calls?.length) {\n    console.dir(lastMsg.tool_calls, { depth: null });\n  } else if (lastMsg.content) {\n    console.log(lastMsg.content);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Streaming with ChatOpenAI\nDESCRIPTION: Example of using streaming with the ChatOpenAI model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-openai/README.md#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst model = new ChatOpenAI({\n  apiKey: process.env.OPENAI_API_KEY,\n  modelName: \"gpt-4-1106-preview\",\n});\nconst response = await model.stream(new HumanMessage(\"Hello world!\"));\n```\n\n----------------------------------------\n\nTITLE: Installing MongoDB Node.js SDK\nDESCRIPTION: This command installs the MongoDB Node.js SDK as a project dependency using npm.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/mongodb.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -S mongodb\n```\n\n----------------------------------------\n\nTITLE: Retrieving Table Definitions and Sample Rows in LangChain\nDESCRIPTION: TypeScript code showing how to fetch table names, schemas, and sample rows from a database using LangChain's SqlDatabase methods.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/sql_prompting.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\n<CodeBlock language=\"typescript\">{TableDefinitionsExample}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Formatting String Prompt in JavaScript\nDESCRIPTION: This code snippet shows how to format a string prompt template by providing values for the placeholders. It uses the 'format' method to fill in the topic and language.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/prompts_composition.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nawait prompt.format({ topic: \"sports\", language: \"spanish\" })\n```\n\n----------------------------------------\n\nTITLE: Using TencentHunyuanEmbeddings in LangChain.js\nDESCRIPTION: Example code demonstrating how to import and use the TencentHunyuanEmbeddings class to generate embeddings for given text. It includes setting up the model and embedding multiple documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/tencent_hunyuan.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { TencentHunyuanEmbeddings } from \"@langchain/community/embeddings/tencent_hunyuan\";\n\nconst model = new TencentHunyuanEmbeddings();\n\n/* Embed queries */\nconst res = await model.embedQuery(\"Hello world\");\nconsole.log({ res });\n\n/* Embed documents */\nconst documentRes = await model.embedDocuments([\"Hello world\", \"Bye bye\"]);\nconsole.log({ documentRes });\n```\n\n----------------------------------------\n\nTITLE: Performing Basic SAP HANA Vector Store Operations with LangchainJS (TypeScript)\nDESCRIPTION: This TypeScript code illustrates fundamental operations on a SAP HANA vector store using LangchainJS. It covers adding new documents/texts to an existing store and performing similarity searches to retrieve relevant documents based on a query vector.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/hanavector.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { HanaDB } from \"@langchain/community/vectorstores/hana\";\nimport { Document } from \"@langchain/core/documents\";\n\n// Connection options for the database connection, read from environment variables\n// HANA_HOST, HANA_PORT, HANA_USER, HANA_PASSWORD\nconst connectionOptions = {\n  host: process.env.HANA_HOST ?? \"\",\n  port: process.env.HANA_PORT ?? \"\",\n  user: process.env.HANA_USER ?? \"\",\n  password: process.env.HANA_PASSWORD ?? \"\",\n};\n\n// Configuration for the vector store table\nconst vectorStoreTableConfig = {\n  tableName: \"MY_TABLE_NAME\",\n  schemaName: \"MY_SCHEMA_NAME\", // optional, default: current schema\n};\n\n// Input texts\nconst texts = [\n  \"SAP HANA Cloud is a database as a service (DBaaS) offering.\",\n  \"SAP HANA Cloud Vector Engine is a component of SAP HANA Cloud.\",\n  \"Langchain JS is a framework for developing applications powered by language models.\",\n];\n\n// Create embeddings model\nconst embeddings = new OpenAIEmbeddings({});\n\n// Create the vector store\nconst vectorStore = await HanaDB.fromTexts(\n  texts,\n  [], // metadata can be added here\n  embeddings,\n  connectionOptions,\n  vectorStoreTableConfig\n);\n\n// Perform a similarity search\nconst results = await vectorStore.similaritySearch(\"database as a service\", 1);\nconsole.log(results);\n/*\n  [\n    Document {\n      pageContent: 'SAP HANA Cloud is a database as a service (DBaaS) offering.',\n      metadata: {}\n    }\n  ]\n*/\n\n// Add more documents to the vector store\nawait vectorStore.addDocuments([\n  new Document({\n    pageContent: \"SAP Datasphere is a comprehensive data service.\",\n  }),\n]);\n\n// Perform a similarity search again to see the new document\nconst results2 = await vectorStore.similaritySearch(\"data service\", 1);\nconsole.log(results2);\n/*\n  [\n    Document {\n      pageContent: 'SAP Datasphere is a comprehensive data service.',\n      metadata: {}\n    }\n  ]\n*/\n\n```\n\n----------------------------------------\n\nTITLE: Embedding Multiple Texts with TogetherAIEmbeddings in JavaScript\nDESCRIPTION: Demonstrates how to generate embeddings for multiple texts using the embedDocuments method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/togetherai.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst text2 = \"LangGraph is a library for building stateful, multi-actor applications with LLMs\";\n\nconst vectors = await embeddings.embedDocuments([text, text2]);\n\nconsole.log(vectors[0].slice(0, 100));\nconsole.log(vectors[1].slice(0, 100));\n```\n\n----------------------------------------\n\nTITLE: Using ChatGoogle for Gemma Model\nDESCRIPTION: TypeScript code demonstrating how to use the ChatGoogle class to interact with the Gemma model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/google.mdx#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatGoogle } from \"@langchain/google-gauth\";\n\nconst model = new ChatGoogle({\n  model: \"gemma-3-27b-it\",\n});\n\nconst res = await model.invoke([\n  {\n    role: \"user\",\n    content:\n      \"What would be a good company name for a company that makes colorful socks?\",\n  },\n]);\n```\n\n----------------------------------------\n\nTITLE: Streaming Chat Model Responses with stream() Method\nDESCRIPTION: Demonstrates how to stream responses from a chat model using the stream() method. It processes each chunk of the response as it becomes available and displays it with a delimiter for visualization.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chat_streaming.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst stream = await model.stream(\"Write me a 1 verse song about goldfish on the moon\")\n\nfor await (const chunk of stream) {\n  console.log(`${chunk.content}\\n---`);\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing WatsonxEmbeddings with IBM watsonx.ai Software Authentication\nDESCRIPTION: Creates an instance of WatsonxEmbeddings using IBM watsonx.ai software authentication. This includes setting the version, service URL, project ID, authentication type, username, password, and URL.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/ibm.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { WatsonxEmbeddings } from \"@langchain/community/embeddings/ibm\";\n\nconst props = {\n  version: \"YYYY-MM-DD\",\n  serviceUrl: \"<SERVICE_URL>\",\n  projectId: \"<PROJECT_ID>\",\n  watsonxAIAuthType: \"cp4d\",\n  watsonxAIUsername: \"<YOUR-USERNAME>\",\n  watsonxAIPassword: \"<YOUR-PASSWORD>\",\n  watsonxAIUrl: \"<url>\",\n};\nconst instance = new WatsonxEmbeddings(props);\n```\n\n----------------------------------------\n\nTITLE: Implementing an Advanced Custom Chat Model with Token Tracking\nDESCRIPTION: Creates a more advanced custom chat model by extending the BaseChatModel class and implementing the _generate method. Includes token usage tracking and metadata handling.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/custom_chat.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AIMessage, BaseMessage } from \"@langchain/core/messages\";\nimport { ChatResult } from \"@langchain/core/outputs\";\nimport {\n  BaseChatModel,\n  BaseChatModelCallOptions,\n  BaseChatModelParams,\n} from \"@langchain/core/language_models/chat_models\";\nimport { CallbackManagerForLLMRun } from \"@langchain/core/callbacks/manager\";\n\ninterface AdvancedCustomChatModelOptions\n  extends BaseChatModelCallOptions {}\n\ninterface AdvancedCustomChatModelParams extends BaseChatModelParams {\n  n: number;\n}\n\nclass AdvancedCustomChatModel extends BaseChatModel<AdvancedCustomChatModelOptions> {\n  n: number;\n\n  static lc_name(): string {\n    return \"AdvancedCustomChatModel\";\n  }\n\n  constructor(fields: AdvancedCustomChatModelParams) {\n    super(fields);\n    this.n = fields.n;\n  }\n\n  async _generate(\n    messages: BaseMessage[],\n    options: this[\"ParsedCallOptions\"],\n    runManager?: CallbackManagerForLLMRun\n  ): Promise<ChatResult> {\n    if (!messages.length) {\n      throw new Error(\"No messages provided.\");\n    }\n    if (typeof messages[0].content !== \"string\") {\n      throw new Error(\"Multimodal messages are not supported.\");\n    }\n    // Pass `runManager?.getChild()` when invoking internal runnables to enable tracing\n    // await subRunnable.invoke(params, runManager?.getChild());\n    const content = messages[0].content.slice(0, this.n);\n    const tokenUsage = {\n      usedTokens: this.n,\n    };\n    return {\n      generations: [{ message: new AIMessage({ content }), text: content }],\n      llmOutput: { tokenUsage },\n    };\n  }\n\n  _llmType(): string {\n    return \"advanced_custom_chat_model\";\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Performing Basic Similarity Search with Vector Store\nDESCRIPTION: Demonstrates how to perform a basic similarity search using a vector store to find relevant documents based on a text query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/retrievers.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nconst results1 = await vectorStore.similaritySearch(\"When was Nike incorporated?\")\n\nresults1[0]\n```\n\n----------------------------------------\n\nTITLE: Implementing Minimax Functions with Zod in LangChain.js\nDESCRIPTION: Demonstrates how to use Zod, a TypeScript-first schema declaration and validation library, with Minimax functions in LangChain.js for enhanced type safety and validation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/minimax.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport MinimaxFunctionsZod from \"@examples/models/chat/minimax_functions_zod.ts\";\n```\n\n----------------------------------------\n\nTITLE: Splitting LaTeX Documents with RecursiveCharacterTextSplitter\nDESCRIPTION: Shows how to process LaTeX documents using RecursiveCharacterTextSplitter with LaTeX-specific separators. Handles LaTeX document structure, sections, and subsections appropriately.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/code_splitter.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst latexText = `\n\\documentclass{article}\n\n\\begin{document}\n\n\\maketitle\n\n\\section{Introduction}\nLarge language models (LLMs) are a type of machine learning model that can be trained on vast amounts of text data to generate human-like language. In recent years, LLMs have made significant advances in a variety of natural language processing tasks, including language translation, text generation, and sentiment analysis.\n\n\\subsection{History of LLMs}\nThe earliest LLMs were developed in the 1980s and 1990s, but they were limited by the amount of data that could be processed and the computational power available at the time. In the past decade, however, advances in hardware and software have made it possible to train LLMs on massive datasets, leading to significant improvements in performance.\n\n\\subsection{Applications of LLMs}\nLLMs have many applications in industry, including chatbots, content creation, and virtual assistants. They can also be used in academia for research in linguistics, psychology, and computational linguistics.\n\n\\end{document}\n`\n\nconst latexSplitter = RecursiveCharacterTextSplitter.fromLanguage(\n  \"latex\", {\n      chunkSize: 60,\n      chunkOverlap: 0,\n    }\n)\nconst latexDocs = await latexSplitter.createDocuments([latexText])\n\nlatexDocs\n```\n\n----------------------------------------\n\nTITLE: Implementing Fallback Chain Strategy\nDESCRIPTION: Sets up a fallback mechanism that switches to a more capable model (GPT-4) when the initial chain fails, demonstrating advanced error recovery.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tools_error.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst badChain = llmWithTools\n  .pipe((message) => message.tool_calls?.[0].args)\n  .pipe(complexTool);\n\nconst betterModel = new ChatOpenAI({\n  model: \"gpt-4-1106-preview\",\n  temperature: 0,\n}).bindTools([complexTool]);\n\nconst betterChain = betterModel\n  .pipe((message) => message.tool_calls?.[0].args)\n  .pipe(complexTool);\n\nconst chainWithFallback = badChain.withFallbacks([betterChain]);\n\nawait chainWithFallback.invoke(\"use complex tool. the args are 5, 2.1, potato\");\n```\n\n----------------------------------------\n\nTITLE: Initializing Bedrock LLM with AWS credentials (Node.js version)\nDESCRIPTION: Clean TypeScript implementation for initializing the Bedrock LLM in a Node.js environment with configuration for model, region, credentials, and generation parameters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/bedrock.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Bedrock } from \"@langchain/community/llms/bedrock\"\n\nconst llm = new Bedrock({\n  model: \"anthropic.claude-v2\",\n  region: process.env.BEDROCK_AWS_REGION ?? \"us-east-1\",\n  // endpointUrl: \"custom.amazonaws.com\",\n  credentials: {\n    accessKeyId: process.env.BEDROCK_AWS_ACCESS_KEY_ID,\n    secretAccessKey: process.env.BEDROCK_AWS_SECRET_ACCESS_KEY,\n  },\n  temperature: 0,\n  maxTokens: undefined,\n  maxRetries: 2,\n  // other params...\n})\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for LangSmith Tracing\nDESCRIPTION: This snippet shows how to set up environment variables to enable tracing with LangSmith. It includes settings for enabling tracing, setting the API key, and optionally reducing tracing latency.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n\n# Reduce tracing latency if you are not in a serverless environment\n# export LANGCHAIN_CALLBACKS_BACKGROUND=true\n```\n\n----------------------------------------\n\nTITLE: Invoking AzureOpenAI Model in LangChain.js\nDESCRIPTION: Demonstrates how to invoke the AzureOpenAI model with input text to generate a completion.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/azure.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst inputText = \"AzureOpenAI is an AI company that \"\n\nconst completion = await llm.invoke(inputText)\ncompletion\n```\n\n----------------------------------------\n\nTITLE: PostgresEngine Instance Setup\nDESCRIPTION: TypeScript code demonstrating how to initialize a PostgresEngine instance with configuration arguments.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/google_cloudsql_pg.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport {\n  PostgresLoader,\n  PostgresEngine,\n  PostgresEngineArgs,\n} from \"@langchain/google-cloud-sql-pg\";\nimport * as dotenv from \"dotenv\";\n\ndotenv.config();\n\nconst peArgs: PostgresEngineArgs = {\n  user: process.env.DB_USER ?? \"\",\n  password: process.env.PASSWORD ?? \"\",\n};\n\n// PostgresEngine instantiation\nconst engine: PostgresEngine = await PostgresEngine.fromInstance(\n  process.env.PROJECT_ID ?? \"\",\n  process.env.REGION ?? \"\",\n  process.env.INSTANCE_NAME ?? \"\",\n  process.env.DB_NAME ?? \"\",\n  peArgs\n);\n```\n\n----------------------------------------\n\nTITLE: Installing PDFLoader Dependencies\nDESCRIPTION: Command to install the required packages for using PDFLoader, including the @langchain/community package and pdf-parse library.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/pdf.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: mdx\nCODE:\n```\n@langchain/community @langchain/core pdf-parse\n```\n\n----------------------------------------\n\nTITLE: Partial Formatting Prompt Template with Functions in TypeScript\nDESCRIPTION: This snippet shows how to partially format a prompt template using functions in LangChain. It demonstrates using a function to dynamically generate a date value, both through the partial() method and by initializing the prompt with partialed variables.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/prompts_partial.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst getCurrentDate = () => {\n  return new Date().toISOString();\n};\n\nconst prompt = new PromptTemplate({\n  template: \"Tell me a {adjective} joke about the day {date}\",\n  inputVariables: [\"adjective\", \"date\"],\n});\n\nconst partialPrompt = await prompt.partial({\n  date: getCurrentDate,\n});\n\nconst formattedPrompt = await partialPrompt.format({\n  adjective: \"funny\",\n});\n\nconsole.log(formattedPrompt);\n\n// Tell me a funny joke about the day 2023-07-13T00:54:59.287Z\n```\n\nLANGUAGE: typescript\nCODE:\n```\nconst prompt = new PromptTemplate({\n  template: \"Tell me a {adjective} joke about the day {date}\",\n  inputVariables: [\"adjective\"],\n  partialVariables: {\n    date: getCurrentDate,\n  },\n});\n\nconst formattedPrompt = await prompt.format({\n  adjective: \"funny\",\n});\n\nconsole.log(formattedPrompt);\n\n// Tell me a funny joke about the day 2023-07-13T00:54:59.287Z\n```\n\n----------------------------------------\n\nTITLE: Example Message Conversion Function\nDESCRIPTION: Implements a function to convert examples into message format compatible with OpenAI's tool calling API.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/extraction_examples.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport {\n  AIMessage,\n  type BaseMessage,\n  HumanMessage,\n  ToolMessage\n} from \"@langchain/core/messages\";\nimport { v4 as uuid } from \"uuid\";\n\ntype OpenAIToolCall = {\n  id: string,\n  type: \"function\",\n  function: {\n    name: string;\n    arguments: string;\n  }\n};\n\ntype Example = {\n  input: string;\n  toolCallOutputs: Record<string, any>[];\n}\n\nfunction toolExampleToMessages(example: Example): BaseMessage[] {\n  const openAIToolCalls: OpenAIToolCall[] = example.toolCallOutputs.map((output) => {\n    return {\n      id: uuid(),\n      type: \"function\",\n      function: {\n        name: \"extract\",\n        arguments: JSON.stringify(output),\n      },\n    };\n  });\n  const messages: BaseMessage[] = [\n    new HumanMessage(example.input),\n    new AIMessage({\n      content: \"\",\n      additional_kwargs: { tool_calls: openAIToolCalls }\n    })\n  ];\n  const toolMessages = openAIToolCalls.map((toolCall, i) => {\n    return new ToolMessage({\n      content: \"You have correctly called this tool.\",\n      tool_call_id: toolCall.id\n    });\n  });\n  return messages.concat(toolMessages);\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Wrangler TOML for Vectorize Integration\nDESCRIPTION: This TOML configuration snippet shows how to update the \\\"wrangler.toml\\\" file to bind to a Cloudflare Vectorize index named \\\"<index_name>\\\". This configuration is essential for the integration with Cloudflare Vectorize.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/cloudflare_vectorize.mdx#2025-04-22_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\n[[vectorize]]\nbinding = \"VECTORIZE_INDEX\"\nindex_name = \"<index_name>\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Confluence Document Loading in Node.js\nDESCRIPTION: This snippet shows the command to install the required packages for loading Confluence documents. It includes @langchain/community, @langchain/core, and html-to-text for parsing pages into plain text.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/confluence.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core html-to-text\n```\n\n----------------------------------------\n\nTITLE: Implementing HuggingFaceInference Model in TypeScript\nDESCRIPTION: This code snippet demonstrates how to import and use the HuggingFaceInference class from LangChain. It creates a new instance of the model with 'gpt2' and an API key, then invokes the model with a simple arithmetic question.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/huggingface_inference.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HuggingFaceInference } from \"@langchain/community/llms/hf\";\n\nconst model = new HuggingFaceInference({\n  model: \"gpt2\",\n  apiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.HUGGINGFACEHUB_API_KEY\n});\nconst res = await model.invoke(\"1 + 1 =\");\nconsole.log({ res });\n```\n\n----------------------------------------\n\nTITLE: Installing CloseVector Node for Node.js Environment\nDESCRIPTION: Command to install the CloseVector Node package for use in Node.js environments.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/closevector.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -S closevector-node\n```\n\n----------------------------------------\n\nTITLE: Custom Separators in RecursiveCharacterTextSplitter\nDESCRIPTION: Demonstrates how to use custom separators with RecursiveCharacterTextSplitter. Configures the splitter with specific separator characters for more controlled text splitting.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/recursive_text_splitter.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\nimport { Document } from \"@langchain/core/documents\";\n\nconst text = `Some other considerations include:\\n\\n- Do you deploy your backend and frontend together, or separately?\\n- Do you deploy your backend co-located with your database, or separately?\\n\\n**Production Support:** As you move your LangChains into production, we'd love to offer more hands-on support.\\nFill out [this form](https://airtable.com/appwQzlErAS2qiP0L/shrGtGaVBVAz7NcV2) to share more about what you're building, and our team will get in touch.\\n\\n## Deployment Options\\n\\nSee below for a list of deployment options for your LangChain app. If you don't see your preferred option, please get in touch and we can add it to this list.`;\n\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 50,\n  chunkOverlap: 1,\n  separators: [\"|\", \"##\", \">\", \"-\"],\n});\n\nconst docOutput = await splitter.splitDocuments([\n  new Document({ pageContent: text }),\n]);\n\nconsole.log(docOutput.slice(0, 3));\n```\n\n----------------------------------------\n\nTITLE: Installing and Uninstalling Azure OpenAI Packages - Bash\nDESCRIPTION: This snippet provides the commands necessary to install the new @langchain/openai package and remove the deprecated @langchain/azure-openai package using npm. It is required to execute these commands in the project root to update dependency references. Running 'npm install' adds the current package, while 'npm uninstall' cleans up the outdated one. No parameters are required beyond having npm configured.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/azure_openai.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai\nnpm uninstall @langchain/azure-openai\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Google Places Tool Dependencies (Bash)\nDESCRIPTION: This snippet provides a command to install the required npm packages for using the Google Places Tool with LangChain, specifically @langchain/openai, @langchain/community, and @langchain/core. All packages must be installed before utilizing the tool, and npm is required as the package manager. The command assumes an existing Node.js development environment with npm available.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/google_places.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Initializing Weather Tool with Zod Schema\nDESCRIPTION: Sets up a weather analysis tool using Zod schema validation that accepts weather conditions as input ('sunny', 'cloudy', or 'rainy'). Includes image URL setup for testing.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_calls_multimodal.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n\nconst imageUrl = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\";\n\nconst weatherTool = tool(async ({ weather }) => {\n  console.log(weather);\n  return weather;\n}, {\n  name: \"multiply\",\n  description: \"Describe the weather\",\n  schema: z.object({\n    weather: z.enum([\"sunny\", \"cloudy\", \"rainy\"])\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Deleting Documents from MariaDB Vector Store - TypeScript\nDESCRIPTION: Demonstrates how to delete a document from the MariaDB vector store by its unique id. The id is retrieved from the list of previously generated document ids. The vectorStore.delete function is asynchronous and expects one or more ids to remove the corresponding documents from the store. This depends on a properly initialized vectorStore instance and a valid id array.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/mariadb.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst id4 = ids[ids.length - 1];\n\nawait vectorStore.delete({ ids: [id4] });\n\n```\n\n----------------------------------------\n\nTITLE: Adding Documents to MemoryVectorStore\nDESCRIPTION: This code demonstrates how to add the split documents to the initialized MemoryVectorStore for indexing and later retrieval.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/retrievers.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nawait vectorStore.addDocuments(allSplits)\n```\n\n----------------------------------------\n\nTITLE: Integrating ArxivRetriever within a LangChain RAG chain\nDESCRIPTION: Shows how to use ArxivRetriever as part of a Retrieval-Augmented Generation (RAG) chain. The example creates a chain that retrieves documents from arXiv, formats them, and passes them to an LLM to answer questions based on the retrieved content.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/arxiv-retriever.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport {\n  RunnablePassthrough,\n  RunnableSequence,\n} from \"@langchain/core/runnables\";\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\nimport type { Document } from \"@langchain/core/documents\";\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o-mini\",\n  temperature: 0,\n});\n\nconst prompt = ChatPromptTemplate.fromTemplate(`\nAnswer the question based only on the context provided.\n\nContext: {context}\n\nQuestion: {question}`);\n\nconst formatDocs = (docs: Document[]) => {\n  return docs.map((doc) => doc.pageContent).join(\"\\n\\n\");\n};\n\nconst ragChain = RunnableSequence.from([\n  {\n    context: retriever.pipe(formatDocs),\n    question: new RunnablePassthrough(),\n  },\n  prompt,\n  llm,\n  new StringOutputParser(),\n]);\n\nawait ragChain.invoke(\"What are the latest advances in quantum computing?\");\n```\n\n----------------------------------------\n\nTITLE: Using RunnableConfig with invoke Method in TypeScript\nDESCRIPTION: Example of how to pass a RunnableConfig object to a Runnable's invoke method, including runName, tags, and metadata for tracking and debugging purposes.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/runnables.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nawait someRunnable.invoke(someInput, {\n  runName: \"myRun\",\n  tags: [\"tag1\", \"tag2\"],\n  metadata: { key: \"value\" },\n});\n```\n\n----------------------------------------\n\nTITLE: Pretty Print Message Implementation\nDESCRIPTION: Utility function for formatting and displaying agent messages with tool calls.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/sql_qa.ipynb#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport { AIMessage, BaseMessage, isAIMessage } from \"@langchain/core/messages\";\n\nconst prettyPrint = (message: BaseMessage) => {\n  let txt = `[${message._getType()}]: ${message.content}`;\n  if (\n    (isAIMessage(message) && message.tool_calls?.length) ||\n    0 > 0\n  ) {\n    const tool_calls = (message as AIMessage)?.tool_calls\n      ?.map((tc) => `- ${tc.name}(${JSON.stringify(tc.args)})`)\n      .join(\"\\n\");\n    txt += ` \\nTools: \\n${tool_calls}`;\n  }\n  console.log(txt);\n};\n```\n\n----------------------------------------\n\nTITLE: Initializing FireCrawlLoader\nDESCRIPTION: Example of importing and instantiating the FireCrawlLoader with configuration options including URL, API key, mode, and optional parameters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/firecrawl.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport \"@mendable/firecrawl-js\";\nimport { FireCrawlLoader } from \"@langchain/community/document_loaders/web/firecrawl\"\n\nconst loader = new FireCrawlLoader({\n  url: \"https://firecrawl.dev\", // The URL to scrape\n  apiKey: \"...\", // Optional, defaults to `FIRECRAWL_API_KEY` in your env.\n  mode: \"scrape\", // The mode to run the crawler in. Can be \"scrape\" for single urls or \"crawl\" for all accessible subpages\n  params: {\n    // optional parameters based on Firecrawl API docs\n    // For API documentation, visit https://docs.firecrawl.dev\n  },\n})\n```\n\n----------------------------------------\n\nTITLE: Tracking Token Usage with AIMessage.usage_metadata for OpenAI\nDESCRIPTION: Example demonstrating how to access token usage information using the usage_metadata attribute of AIMessage objects when working with OpenAI models.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chat_token_usage_tracking.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { HumanMessage } from \"@langchain/core/messages\";\n\nconst model = new ChatOpenAI({\n  modelName: \"gpt-3.5-turbo\",\n});\n\nconst response = await model.invoke([new HumanMessage(\"Hello world!\")]);\n\nconsole.log(JSON.stringify(response.usage_metadata, null, 2));\n/*\n{\n  \"input_tokens\": 11,\n  \"output_tokens\": 10,\n  \"total_tokens\": 21\n}\n*/\n\n```\n\n----------------------------------------\n\nTITLE: Optional LangSmith Configuration\nDESCRIPTION: Setting up LangSmith API key for automated tracing from individual queries.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/vectara.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// process.env.LANGSMITH_API_KEY = \"<YOUR API KEY HERE>\";\n// process.env.LANGSMITH_TRACING = \"true\";\n```\n\n----------------------------------------\n\nTITLE: Using Gemini for Multimodal Vision Tasks\nDESCRIPTION: TypeScript code demonstrating how to use the gemini-pro-vision model with image inputs, including loading an image from a file and passing it to the model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-google-genai/README.md#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport fs from \"fs\";\nimport { ChatGoogleGenerativeAI } from \"@langchain/google-genai\";\nimport { HumanMessage } from \"@langchain/core/messages\";\n\nconst vision = new ChatGoogleGenerativeAI({\n  modelName: \"gemini-pro-vision\",\n  maxOutputTokens: 2048,\n});\nconst image = fs.readFileSync(\"./hotdog.jpg\").toString(\"base64\");\nconst input = [\n  new HumanMessage({\n    content: [\n      {\n        type: \"text\",\n        text: \"Describe the following image.\",\n      },\n      {\n        type: \"image_url\",\n        image_url: `data:image/png;base64,${image}`,\n      },\n    ],\n  }),\n];\n\nconst res = await vision.invoke(input);\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Runnable with RunnableLambda in TypeScript\nDESCRIPTION: Example showing how to create a custom Runnable using RunnableLambda.from() without manually propagating the RunnableConfig, which may cause issues with callbacks and other config options.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/runnables.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst foo = (input) => {\n  // Note that .invoke() is used directly here\n  // highlight-next-line\n  return barRunnable.invoke(input);\n};\nconst fooRunnable = RunnableLambda.from(foo);\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith Environment Variables\nDESCRIPTION: Optional environment variables for enabling automated tracing of model calls using LangSmith.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/azure.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Alibaba Tongyi Chat Model in TypeScript\nDESCRIPTION: This code snippet demonstrates how to import and use the Alibaba Tongyi chat model in a TypeScript project using LangChain.js. It includes setting up the model and invoking it with a sample prompt.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/alibaba_tongyi.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport Tongyi from \"@examples/models/chat/integration_alitongyi.ts\";\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Background Callbacks with awaitAllCallbacks in JavaScript\nDESCRIPTION: This code snippet shows how callbacks run in the background by default in LangChain.js. It uses a custom callback handler with a delay and demonstrates the use of awaitAllCallbacks to ensure all callbacks finish.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/callbacks_serverless.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport { RunnableLambda } from \"@langchain/core/runnables\";\nimport { awaitAllCallbacks } from \"@langchain/core/callbacks/promises\";\n\nconst runnable = RunnableLambda.from(() => \"hello!\");\n\nconst customHandler = {\n  handleChainEnd: async () => {\n    await new Promise((resolve) => setTimeout(resolve, 2000));\n    console.log(\"Call finished\");\n  },\n};\n\nconst startTime = new Date().getTime();\n\nawait runnable.invoke({ number: \"2\" }, { callbacks: [customHandler] });\n\nconsole.log(`Elapsed time: ${new Date().getTime() - startTime}ms`);\n\nawait awaitAllCallbacks();\n\nconsole.log(`Final elapsed time: ${new Date().getTime() - startTime}ms`);\n```\n\n----------------------------------------\n\nTITLE: Examining Final Events from Chat Model Stream\nDESCRIPTION: Code that slices and displays the last two events from the event stream array, showing the structure of end events in the stream.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\nevents.slice(-2);\n```\n\n----------------------------------------\n\nTITLE: Initializing AstraDBChatMessageHistory with initialize Method\nDESCRIPTION: Code snippet showing how to initialize AstraDBChatMessageHistory using the static initialize method. This approach creates the AstraDB client automatically based on provided credentials and parameters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/astradb.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst chatHistory = await AstraDBChatMessageHistory.initialize({\n  token: process.env.ASTRA_DB_APPLICATION_TOKEN ?? \"token\",\n  endpoint: process.env.ASTRA_DB_ENDPOINT ?? \"endpoint\",\n  namespace: process.env.ASTRA_DB_NAMESPACE,\n  collectionName: \"YOUR_COLLECTION_NAME\",\n  sessionId: \"YOUR_SESSION_ID\",\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatMistralAI Model\nDESCRIPTION: TypeScript code to initialize the ChatMistralAI model with the API key and model name, then invoke it with a message to generate a response.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-mistralai/README.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatMistralAI } from \"@langchain/mistralai\";\n\nconst model = new ChatMistralAI({\n  apiKey: process.env.MISTRAL_API_KEY,\n  modelName: \"mistral-small\",\n});\nconst response = await model.invoke(new HumanMessage(\"Hello world!\"));\n```\n\n----------------------------------------\n\nTITLE: Configuring AzureOpenAIEmbeddings for a Different Domain\nDESCRIPTION: Shows how to configure AzureOpenAIEmbeddings to use a different Azure OpenAI domain than the default.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/azure_openai.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { AzureOpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddingsDifferentDomain = new AzureOpenAIEmbeddings({\n  azureOpenAIApiKey: \"<your_key>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY\n  azureOpenAIApiEmbeddingsDeploymentName: \"<your_embedding_deployment_name>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME\n  azureOpenAIApiVersion: \"<api_version>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION\n  azureOpenAIBasePath:\n    \"https://westeurope.api.microsoft.com/openai/deployments\", // In Node.js defaults to process.env.AZURE_OPENAI_BASE_PATH\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing ZepCloudChatMessageHistory with RunnableWithMessageHistory\nDESCRIPTION: Example demonstrating how to integrate Zep Cloud chat message history with LangChain's RunnableWithMessageHistory. Creates a conversational AI chain that maintains chat history across interactions using Zep Cloud as the persistent storage layer.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/zep_memory_cloud.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport {\n  RunnablePassthrough,\n  RunnableSequence,\n  RunnableWithMessageHistory,\n} from \"@langchain/core/runnables\";\nimport { ZepCloudChatMessageHistory } from \"@langchain/community/chat_message_histories/zep_cloud\";\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\nexport async function run() {\n  // Initialize LLM, output parser, and prompt template\n  const model = new ChatOpenAI({ temperature: 0 });\n  const outputParser = new StringOutputParser();\n\n  const prompt = ChatPromptTemplate.fromMessages([\n    [\"system\", \"You are a helpful assistant.\"],\n    [\"human\", \"{input}\"],\n  ]);\n\n  // Define a runnable sequence without message history\n  const chain = RunnableSequence.from([\n    RunnablePassthrough.assign({\n      // Add any additional context here if needed\n    }),\n    prompt,\n    model,\n    outputParser,\n  ]);\n\n  // Define a function that returns a new Zep Cloud message history for a given session\n  const getSessionHistory = (sessionId: string) => {\n    return new ZepCloudChatMessageHistory({\n      apiKey: process.env.ZEP_CLOUD_API_KEY ?? \"\", // Replace with your actual API key\n      sessionId,\n    });\n  };\n\n  // Create a runnable with message history\n  const chainWithHistory = new RunnableWithMessageHistory({\n    runnable: chain,\n    getMessageHistory: getSessionHistory,\n    inputMessagesKey: \"input\",\n    historyMessagesKey: \"chat_history\",\n  });\n\n  // Request input object\n  const config = {\n    configurable: {\n      sessionId: \"user-123\", // This uniquely identifies the conversation\n    },\n  };\n\n  // Invoke the chain several times to see message history in action\n  console.log(\n    await chainWithHistory.invoke({ input: \"Hello! Can you tell me a bit about LangChain?\" }, config)\n  );\n  console.log(\n    await chainWithHistory.invoke({ input: \"That's interesting, can you tell me more?\" }, config)\n  );\n  console.log(\n    await chainWithHistory.invoke({ input: \"How can I use it with Zep?\" }, config)\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: JSON Parsing Example\nDESCRIPTION: Shows how to parse JSON string output into a JavaScript object.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/structured_outputs.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport json\nconst jsonObject = JSON.parse(aiMsg.content)\n// {'random_ints': [23, 47, 89, 15, 34, 76, 58, 3, 62, 91]}\n```\n\n----------------------------------------\n\nTITLE: Capturing AIMessageChunk in LangchainJS Trace Log (JSON)\nDESCRIPTION: This JSON object represents an `AIMessageChunk` captured within the `messageLog` of a LangchainJS AgentExecutor trace step. It details the AI's response part, specifically indicating a tool use request (`tavily_search_results_json`) with its input. It includes comprehensive metadata like IDs, role (`assistant`), model details (`claude-3-sonnet-20240229`), token usage, stop reason (`tool_use`), and structured representations of the tool call (`tool_call_chunks`, `tool_calls`).\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n            \"lc\": 1,\n            \"type\": \"constructor\",\n            \"id\": [\n              \"langchain_core\",\n              \"messages\",\n              \"AIMessageChunk\"\n            ],\n            \"kwargs\": {\n              \"content\": [\n                {\n                  \"type\": \"tool_use\",\n                  \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n                  \"name\": \"tavily_search_results_json\",\n                  \"input\": {\n                    \"input\": \"Oppenheimer 2023 film director age\"\n                  }\n                }\n              ],\n              \"additional_kwargs\": {\n                \"id\": \"msg_015MqAHr84dBCAqBgjou41Km\",\n                \"type\": \"message\",\n                \"role\": \"assistant\",\n                \"model\": \"claude-3-sonnet-20240229\",\n                \"stop_sequence\": null,\n                \"usage\": {\n                  \"input_tokens\": 409,\n                  \"output_tokens\": 68\n                },\n                \"stop_reason\": \"tool_use\"\n              },\n              \"tool_call_chunks\": [\n                {\n                  \"name\": \"tavily_search_results_json\",\n                  \"args\": \"{\\\"input\\\":\\\"Oppenheimer 2023 film director age\\\"}\",\n                  \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n                  \"index\": 0\n                }\n              ],\n              \"tool_calls\": [\n                {\n                  \"name\": \"tavily_search_results_json\",\n                  \"args\": {\n                    \"input\": \"Oppenheimer 2023 film director age\"\n                  },\n                  \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\"\n                }\n              ],\n              \"invalid_tool_calls\": [],\n              \"response_metadata\": {}\n            }\n          }\n```\n\n----------------------------------------\n\nTITLE: Using BedrockEmbeddings with MemoryVectorStore\nDESCRIPTION: Demonstrates indexing and retrieving documents using BedrockEmbeddings with a MemoryVectorStore.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/bedrock.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n// Create a vector store with a sample text\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n\nconst text = \"LangChain is the framework for building context-aware reasoning applications\";\n\nconst vectorstore = await MemoryVectorStore.fromDocuments(\n  [{ pageContent: text, metadata: {} }],\n  embeddings,\n);\n\n// Use the vector store as a retriever that returns a single document\nconst retriever = vectorstore.asRetriever(1);\n\n// Retrieve the most similar text\nconst retrievedDocuments = await retriever.invoke(\"What is LangChain?\");\n\nretrievedDocuments[0].pageContent;\n```\n\n----------------------------------------\n\nTITLE: Setting Supabase Environment Variables (TypeScript)\nDESCRIPTION: This snippet demonstrates how to set the required Supabase credentials (private key and URL) as environment variables in a Node.js environment. These variables are necessary for the Supabase client to connect to the database.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/supabase.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.SUPABASE_PRIVATE_KEY = \"your-api-key\";\nprocess.env.SUPABASE_URL = \"your-supabase-db-url\";\n```\n\n----------------------------------------\n\nTITLE: Setting up Cohere API Key via Environment Variables\nDESCRIPTION: Demonstrates how to set up the required environment variables for authenticating with the Cohere API. Includes optional LangSmith tracing configuration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/cohere.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport COHERE_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Breaking Streaming with Non-Streaming Function in JavaScript\nDESCRIPTION: This code demonstrates how adding a non-streaming function to the chain breaks the streaming functionality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst extractCountryNames = (inputs: Record<string, any>) => {\n  if (!Array.isArray(inputs.countries)) {\n    return \"\";\n  }\n  return JSON.stringify(inputs.countries.map((country) => country.name));\n}\n\nconst chain = model.pipe(new JsonOutputParser()).pipe(extractCountryNames);\n\nconst stream = await chain.stream(\n  `output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key \"name\" and \"population\"`\n);\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n```\n\n----------------------------------------\n\nTITLE: Running Tests\nDESCRIPTION: Commands for running different types of tests in the project, including all unit tests, a single test file, or all integration tests.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/contributing/code.mdx#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nyarn test\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn test:single /path/to/yourtest.test.ts\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn test:integration\n```\n\n----------------------------------------\n\nTITLE: Handling Tool Call Response in LangChain with Claude 3\nDESCRIPTION: JSON representation of a tool call response from Claude 3 Sonnet using a calculator tool to compute 52 * 365. The response includes the calculation input, metadata, and the tool call structure.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_36\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"toolu_01NVTbm5aNYSm1wGYb6XF7jE\",\n  \"name\": \"calculator\",\n  \"input\": {\n    \"input\": \"52 * 365\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Langchain Retriever from Vector Store (TypeScript)\nDESCRIPTION: Converts the `MongoDBAtlasVectorSearch` instance into a Langchain `Retriever` object using the `asRetriever` method. This allows seamless integration into Langchain chains and agents. Optional parameters like `filter` and `k` (number of results) can be passed during conversion.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/mongodb_atlas.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nconst retriever = vectorStore.asRetriever({\n  // Optional filter\n  filter: filter,\n  k: 2,\n});\nawait retriever.invoke(\"biology\");\n```\n\n----------------------------------------\n\nTITLE: Loading documents from Notion using NotionAPILoader\nDESCRIPTION: Example code demonstrating how to use the NotionAPILoader to load documents from Notion pages and databases, including authentication with an integration token and specifying page or database IDs.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/notionapi.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/document_loaders/notionapi.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Initializing PostgreSQL Engine and Vector Store Table\nDESCRIPTION: Example showing how to create a PostgreSQL connection through PostgresEngine and initialize a vector store table with custom columns and embedding configuration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-google-cloud-sql-pg/README.md#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Column, PostgresEngine, PostgresEngineArgs, PostgresVectorStore, VectorStoreTableArgs } from \"@langchain/google-cloud-sql-pg\";\nimport { SyntheticEmbeddings } from \"@langchain/core/utils/testing\";\n\nconst pgArgs: PostgresEngineArgs = {\n    user: \"db-user\",\n    password: \"password\"\n}\n\nconst engine: PostgresEngine = await PostgresEngine.fromInstance(\n \"project-id\",\n \"region\",\n \"instance-name\",\n \"database-name\",\n pgArgs\n);\n\nconst vectorStoreTableArgs: VectorStoreTableArgs = {\n  metadataColumns: [new Column(\"page\", \"TEXT\"), new Column(\"source\", \"TEXT\")],\n};\n\nawait engine.initVectorstoreTable(\"my-table\", 768, vectorStoreTableArgs);\nconst embeddingService = new SyntheticEmbeddings({ vectorSize: 768 });\n```\n\n----------------------------------------\n\nTITLE: Chaining Cohere Model with Prompt Templates\nDESCRIPTION: Demonstrates how to chain a prompt template with a Cohere model to create a more structured generation pipeline.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/cohere.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { PromptTemplate } from \"@langchain/core/prompts\"\n\nconst prompt = new PromptTemplate({\n  template: \"How to say {input} in {output_language}:\\n\",\n  inputVariables: [\"input\", \"output_language\"],\n})\n\nconst chain = prompt.pipe(llm);\nawait chain.invoke(\n  {\n    output_language: \"German\",\n    input: \"I love programming.\",\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Hooks to ChatMistralAI Model\nDESCRIPTION: Shows two methods of adding custom hooks to a ChatMistralAI model instance.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/mistralai.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ChatMistralAI } from \"@langchain/mistralai\" \n\nconst modelWithHooks = new ChatMistralAI({\n    model: \"mistral-large-latest\",\n    temperature: 0,\n    maxRetries: 2,\n    beforeRequestHooks: [ beforeRequestHook ],\n    requestErrorHooks: [ requestErrorHook ],\n    responseHooks: [ responseHook ],\n    // other params...\n});\n```\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ChatMistralAI } from \"@langchain/mistralai\" \n\nconst model = new ChatMistralAI({\n    model: \"mistral-large-latest\",\n    temperature: 0,\n    maxRetries: 2,\n    // other params...\n});\n\nmodel.beforeRequestHooks = [ ...model.beforeRequestHooks, beforeRequestHook ];\nmodel.requestErrorHooks = [ ...model.requestErrorHooks, requestErrorHook ];\nmodel.responseHooks = [ ...model.responseHooks, responseHook ];\n\nmodel.addAllHooksToHttpClient();\n```\n\n----------------------------------------\n\nTITLE: Importing Document Class for Reciprocal Rank Fusion\nDESCRIPTION: Imports the Document class from LangChain.js, which will be used to structure the search results processed by the Reciprocal Rank Fusion algorithm.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/rag_fusion.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Document } from \"npm:langchain@0.0.172/document\";\n```\n\n----------------------------------------\n\nTITLE: Indexing and Retrieving Data with MistralAIEmbeddings\nDESCRIPTION: Demonstrates how to use MistralAIEmbeddings for indexing and retrieving data using MemoryVectorStore.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/mistralai.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n// Create a vector store with a sample text\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n\nconst text = \"LangChain is the framework for building context-aware reasoning applications\";\n\nconst vectorstore = await MemoryVectorStore.fromDocuments(\n  [{ pageContent: text, metadata: {} }],\n  embeddings,\n);\n\n// Use the vector store as a retriever that returns a single document\nconst retriever = vectorstore.asRetriever(1);\n\n// Retrieve the most similar text\nconst retrievedDocuments = await retriever.invoke(\"What is LangChain?\");\n\nretrievedDocuments[0].pageContent;\n```\n\n----------------------------------------\n\nTITLE: Installing Langchain OpenAI SDK with NPM\nDESCRIPTION: This command installs the Langchain OpenAI SDK, needed for utilizing OpenAI's embeddings in conjunction with Tigris. The SDK allows for the integration of OpenAI's model outputs with Langchain functionality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/tigris.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -S @langchain/openai\n```\n\n----------------------------------------\n\nTITLE: Invoking the ChatXAI Model with Messages\nDESCRIPTION: Demonstrates how to invoke the ChatXAI model with system and human messages, and how to access the resulting AI message content.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/xai.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst aiMsg = await llm.invoke([\n    [\n      \"system\",\n      \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n    ],\n    [\"human\", \"I love programming.\"],\n])\nconsole.log(aiMsg)\n```\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(aiMsg.content)\n```\n\n----------------------------------------\n\nTITLE: Executing Agent Query with Streaming in TypeScript\nDESCRIPTION: This snippet showcases executing a query using a streaming agent executor setup. The example query lists 10 artists from the database, and responses are streamed in real-time. Adjust the query as needed for other use cases.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/sql.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst exampleQuery = \\\"Can you list 10 artists from my database?\\\"\n\nconst events = await agentExecutor.stream(\n  { messages: [[\\\"user\\\", exampleQuery]]},\n  { streamMode: \\\"values\\\", }\n)\n\nfor await (const event of events) {\n  const lastMsg = event.messages[event.messages.length - 1];\n  if (lastMsg.tool_calls?.length) {\n    console.dir(lastMsg.tool_calls, { depth: null });\n  } else if (lastMsg.content) {\n    console.log(lastMsg.content);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Search Schema Definition with Zod\nDESCRIPTION: Defines a schema for structured output using Zod to specify the format of search queries.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_multiple_queries.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { z } from \"zod\";\n\nconst searchSchema = z.object({\n    queries: z.array(z.string()).describe(\"Distinct queries to search for\")\n}).describe(\"Search over a database of job records.\");\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for IAM Authentication\nDESCRIPTION: Configuration of environment variables for IBM watsonx.ai IAM authentication method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/ibm.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport WATSONX_AI_AUTH_TYPE=iam\nexport WATSONX_AI_APIKEY=<YOUR-APIKEY>\n```\n\n----------------------------------------\n\nTITLE: LangChain LLM Call with ChatAnthropic\nDESCRIPTION: Log entry for a LangChain ChatAnthropic LLM run with input messages. The call includes a system message, human message about Oppenheimer, and AI tool calls for a search query about the film director.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_38\n\nLANGUAGE: javascript\nCODE:\n```\n[llm/start] [1:llm:ChatAnthropic] Entering LLM run with input: {\n  \"messages\": [\n    [\n      {\n        \"lc\": 1,\n        \"type\": \"constructor\",\n        \"id\": [\n          \"langchain_core\",\n          \"messages\",\n          \"SystemMessage\"\n        ],\n        \"kwargs\": {\n          \"content\": \"You are a helpful assistant\",\n          \"additional_kwargs\": {},\n          \"response_metadata\": {}\n        }\n      },\n      {\n        \"lc\": 1,\n        \"type\": \"constructor\",\n        \"id\": [\n          \"langchain_core\",\n          \"messages\",\n          \"HumanMessage\"\n        ],\n        \"kwargs\": {\n          \"content\": \"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\",\n          \"additional_kwargs\": {},\n          \"response_metadata\": {}\n        }\n      }\n```\n\n----------------------------------------\n\nTITLE: Querying Documents From an Existing MyScale Collection\nDESCRIPTION: Example showing how to search for documents in an existing MyScale vector collection using similarity search. The code demonstrates connecting to MyScale and retrieving relevant documents based on a query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/myscale.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport CodeBlock from \"@theme/CodeBlock\";\nimport SearchExample from \"@examples/indexes/vector_stores/myscale_search.ts\";\n\n<CodeBlock language=\"typescript\">{SearchExample}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Development Setup Commands\nDESCRIPTION: Commands for setting up the development environment, including installation, building, testing, and formatting.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-openai/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nyarn install\nyarn build\nyarn build --filter=@langchain/openai\nyarn test\nyarn test:int\nyarn lint && yarn format\n```\n\n----------------------------------------\n\nTITLE: Installing Vercel Postgres Client Library using npm/yarn\nDESCRIPTION: This command installs the `@vercel/postgres` package using npm. The `npm2yarn` directive suggests it can also be installed using yarn. This package is required to interact with Vercel Postgres databases from a Node.js application.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/vercel_postgres.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @vercel/postgres\n```\n\n----------------------------------------\n\nTITLE: Setting up xAI API Key in Environment Variables\nDESCRIPTION: Sets the necessary environment variables for using the xAI API, including the main API key and optional LangSmith tracing variables.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/xai.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport XAI_API_KEY=\"your-api-key\"\n```\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Instantiating the self-query retriever\nDESCRIPTION: Creates a SelfQueryRetriever instance using the Qdrant vector store, LLM, and attribute information.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/qdrant.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { SelfQueryRetriever } from \"langchain/retrievers/self_query\";\nimport { QdrantTranslator } from \"@langchain/community/structured_query/qdrant\";\n\nconst selfQueryRetriever = SelfQueryRetriever.fromLLM({\n  llm: llm,\n  vectorStore: vectorStore,\n  /** A short summary of what the document contents represent. */\n  documentContents: \"Brief summary of a movie\",\n  attributeInfo: attributeInfo,\n  structuredQueryTranslator: new QdrantTranslator(),\n});\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Query Generation\nDESCRIPTION: Imports the necessary LangChain.js modules for creating the query generation chain, including the OpenAI chat model, hub connector for prompts, output parser, and runnable schema components.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/rag_fusion.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ChatOpenAI } from \"npm:langchain@0.0.172/chat_models/openai\";\nimport { pull } from \"npm:langchain@0.0.172/hub\";\nimport { StringOutputParser } from \"npm:langchain@0.0.172/schema/output_parser\";\nimport { RunnableLambda, RunnableSequence } from \"npm:langchain@0.0.172/schema/runnable\";\n```\n\n----------------------------------------\n\nTITLE: Initializing IPFS Datastore Chat Memory\nDESCRIPTION: Example showing how to initialize and use the IPFSDatastoreChatMessageHistory class. This code demonstrates creating a file system-based datastore instance and connecting it to LangChain's chat memory system with a session ID.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/ipfs_datastore.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// Replace FsDatastore with the IPFS Datastore of your choice.\nimport { FsDatastore } from \"datastore-fs\";\nimport { IPFSDatastoreChatMessageHistory } from \"@langchain/community/stores/message/ipfs_datastore\";\n\nconst datastore = new FsDatastore(\"path/to/store\");\nconst sessionId = \"my-session\";\n\nconst history = new IPFSDatastoreChatMessageHistory({ datastore, sessionId });\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Astra DB Integration\nDESCRIPTION: Installs the necessary npm packages for using Astra DB with Langchain, including the Astra TS Client, LangChain community package, OpenAI integration, and core LangChain components.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/astradb.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @datastax/astra-db-ts @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Invoking CloudflareWorkersAI for Text Completion\nDESCRIPTION: Example of how to invoke the CloudflareWorkersAI model to generate text completions. The code sends a prompt to the model and awaits the generated response.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/cloudflare_workersai.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst inputText = \"Cloudflare is an AI company that \"\n\nconst completion = await llm.invoke(inputText);\ncompletion\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith API Key for Automated Tracing in TypeScript\nDESCRIPTION: Sets the LangSmith API key for automated tracing of individual queries.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_compressors/ibm.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n// process.env.LANGSMITH_API_KEY = \"<YOUR API KEY HERE>\";\n// process.env.LANGSMITH_TRACING = \"true\";\n```\n\n----------------------------------------\n\nTITLE: Using the RAG chain\nDESCRIPTION: Demonstrates how to invoke the RAG chain with a query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/qdrant.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nawait ragChain.invoke(\"Which movies are rated higher than 8.5?\")\n```\n\n----------------------------------------\n\nTITLE: Using Text Splitters with Citations\nDESCRIPTION: Shows how to combine LangChain text splitters with Claude's citations feature for processing markdown documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/anthropic.ipynb#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport { MarkdownTextSplitter } from \"langchain/text_splitter\";\n\nfunction formatToAnthropicDocuments(documents: string[]) {\n  return {\n    type: \"document\",\n    source: {\n      type: \"content\",\n      content: documents.map((document) => ({ type: \"text\", text: document })),\n    },\n    citations: { enabled: true },\n  };\n}\n```\n\n----------------------------------------\n\nTITLE: Invoking the Runnable with Context Variables in TypeScript\nDESCRIPTION: Invokes the runnable lambda with a userId, query, and LLM. The userId is passed as a context variable to the tool, allowing it to securely access the user identifier.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_runtime.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nawait handleRunTimeRequestRunnable.invoke({\n  userId: \"brace\",\n  query: \"my favorite animals are cats and parrots.\",\n  llm: llm\n});\n```\n\n----------------------------------------\n\nTITLE: Defining Prompt and Chat Model\nDESCRIPTION: Setting up the chat prompt template and initializing the ChatOpenAI model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/stream_tool_client.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst prompt = ChatPromptTemplate.fromMessages([\n  [\n    \"system\",\n    `You are a helpful assistant. Use the tools provided to best assist the user.`,\n  ],\n  [\"human\", \"{input}\"],\n]);\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o-2024-05-13\",\n  temperature: 0,\n});\n```\n\n----------------------------------------\n\nTITLE: Logging Model Response Content\nDESCRIPTION: Extracts and logs the content from the AI message response.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/togetherai.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(aiMsg.content)\n```\n\n----------------------------------------\n\nTITLE: Installing Notion API dependencies for LangChain.js\nDESCRIPTION: Command to install the required packages for working with Notion API in LangChain, including the official Notion client and notion-to-md package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/notionapi.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core @notionhq/client notion-to-md\n```\n\n----------------------------------------\n\nTITLE: Langchain AgentExecutor Output with Tool Selection\nDESCRIPTION: This code shows the output structure of an AgentExecutor run, capturing the selected action (calculator tool), input parameters, and associated metadata including message logs. This is part of a multi-step agent execution flow.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_22\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"output\": [\n    {\n      \"tool\": \"calculator\",\n      \"toolInput\": {\n        \"input\": \"52 * 365\"\n      },\n      \"toolCallId\": \"toolu_01NVTbm5aNYSm1wGYb6XF7jE\",\n      \"log\": \"Invoking \\\"calculator\\\" with {\\\"input\\\":\\\"52 * 365\\\"}\\n[{\\\"type\\\":\\\"text\\\",\\\"text\\\":\\\"Based on the search results, the 2023 film Oppenheimer was directed by Christopher Nolan. Some key information about Christopher Nolan:\\\\n\\\\n- He is a British-American film director, producer and screenwriter.\\\\n- He was born on July 30, 1970, making him currently 52 years old.\\\\n\\\\nTo calculate his age in days:\\\"},{\\\"type\\\":\\\"tool_use\\\",\\\"id\\\":\\\"toolu_01NVTbm5aNYSm1wGYb6XF7jE\\\",\\\"name\\\":\\\"calculator\\\",\\\"input\\\":{\\\"input\\\":\\\"52 * 365\\\"}}]\",\n      \"messageLog\": [...]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing WatsonxLLM with IBM watsonx.ai Software Authentication\nDESCRIPTION: Creates a new instance of WatsonxLLM using IBM watsonx.ai software authentication by providing version, service URL, project ID, username, password, and URL.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/ibm.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { WatsonxLLM } from \"@langchain/community/llms/ibm\";\n\nconst props = {\n  version: \"YYYY-MM-DD\",\n  serviceUrl: \"<SERVICE_URL>\",\n  projectId: \"<PROJECT_ID>\",\n  watsonxAIAuthType: \"cp4d\",\n  watsonxAIUsername: \"<YOUR-USERNAME>\",\n  watsonxAIPassword: \"<YOUR-PASSWORD>\",\n  watsonxAIUrl: \"<url>\",\n};\nconst instance = new WatsonxLLM(props);\n```\n\n----------------------------------------\n\nTITLE: Instantiating ExaSearchResults Tool (TypeScript)\nDESCRIPTION: Demonstrates how to instantiate the `ExaSearchResults` tool in TypeScript. It requires creating an `Exa` client instance with the API key and then passing the client to the `ExaSearchResults` constructor, optionally configuring search arguments like `numResults`.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/exa_search.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ExaSearchResults } from \"@langchain/exa\"\nimport Exa from \"exa-js\";\n\n// @lc-ts-ignore\nconst client = new Exa(process.env.EXASEARCH_API_KEY)\n\nconst tool = new ExaSearchResults({\n  // @lc-ts-ignore\n  client,\n  searchArgs: {\n    numResults: 2,\n  }\n})\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Dependencies via npm\nDESCRIPTION: Installs LangChain packages required for vector search capabilities in a Node.js application. This includes openai, community, and core modules for enhanced AI interactions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/prisma.mdx#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Using ChatGoogleGenerativeAI for Text Generation\nDESCRIPTION: TypeScript code demonstrating how to use the ChatGoogleGenerativeAI class to generate text with Gemini models.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/google.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatGoogleGenerativeAI } from \"@langchain/google-genai\";\n\nconst model = new ChatGoogleGenerativeAI({\n  model: \"gemini-pro\",\n  maxOutputTokens: 2048,\n});\n\n// Batch and stream are also supported\nconst res = await model.invoke([\n  [\n    \"human\",\n    \"What would be a good company name for a company that makes colorful socks?\",\n  ],\n]);\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG Chain with Self-Query Retriever\nDESCRIPTION: Implementation of a RAG (Retrieval-Augmented Generation) chain using the self-query retriever, including document formatting and prompt template setup.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/weaviate.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { RunnablePassthrough, RunnableSequence } from \"@langchain/core/runnables\";\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\n\nimport type { Document } from \"@langchain/core/documents\";\n\nconst prompt = ChatPromptTemplate.fromTemplate(`\nAnswer the question based only on the context provided.\n\nContext: {context}\n\nQuestion: {question}`);\n\nconst formatDocs = (docs: Document[]) => {\n  return docs.map((doc) => JSON.stringify(doc)).join(\"\\n\\n\");\n}\n\nconst ragChain = RunnableSequence.from([\n  {\n    context: selfQueryRetriever.pipe(formatDocs),\n    question: new RunnablePassthrough(),\n  },\n  prompt,\n  llm,\n  new StringOutputParser(),\n]);\n```\n\n----------------------------------------\n\nTITLE: Defining Fixed Examples for Few Shot Learning in LangChain.js\nDESCRIPTION: This snippet shows how to import necessary modules and define fixed examples for few shot learning in chat models. The examples are simple math operations which will be used as reference for the model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot_examples_chat.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport {\n    ChatPromptTemplate,\n    FewShotChatMessagePromptTemplate,\n} from \"@langchain/core/prompts\"\n\nconst examples = [\n    { input: \"2+2\", output: \"4\" },\n    { input: \"2+3\", output: \"5\" },\n]\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Weaviate Configuration\nDESCRIPTION: Configuration of environment variables for connecting to a Weaviate instance, including scheme, host, and URL settings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/weaviate.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.WEAVIATE_SCHEME = \"https\";\n// Include port if relevant, e.g. \"localhost:8080\"\nprocess.env.WEAVIATE_HOST = \"YOUR_WEAVIATE_HOST\";\nprocess.env.WEAVIATE_URL = \"YOUR_WEAVIATE_URL\";\n```\n\n----------------------------------------\n\nTITLE: Basic Model Invocation\nDESCRIPTION: Demonstrating basic text completion using the MistralAI model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/mistral.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst inputText = \"MistralAI is an AI company that \"\n\nconst completion = await llm.invoke(inputText)\ncompletion\n```\n\n----------------------------------------\n\nTITLE: Invoking Ollama LLM for Text Completion\nDESCRIPTION: Demonstrates how to invoke an Ollama model with input text to generate a completion. This example shows the basic pattern for generating text with an initialized Ollama model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/ollama.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst inputText = \"Ollama is an AI company that \"\n\nconst completion = await llm.invoke(inputText)\ncompletion\n```\n\n----------------------------------------\n\nTITLE: Chaining TogetherAI Model with PromptTemplate\nDESCRIPTION: Shows how to chain a prompt template with the TogetherAI model to generate structured completions. This example creates a template for language translation and pipes it to the model for execution.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/together.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { PromptTemplate } from \"@langchain/core/prompts\"\n\nconst prompt = PromptTemplate.fromTemplate(\"How to say {input} in {output_language}:\\n\")\n\nconst chain = prompt.pipe(llm);\nawait chain.invoke(\n  {\n    output_language: \"German\",\n    input: \"I love programming.\",\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: Streaming Chat Model Output with LCEL in TypeScript\nDESCRIPTION: This snippet demonstrates how to use the streamEvents method to print events containing streamed chat model output. It sets up a chain using a ChatPromptTemplate, ChatAnthropic model, and StringOutputParser, then iterates over the stream events to log chat model stream events.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/streaming.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst model = new ChatAnthropic({ model: \"claude-3-sonnet-20240229\" });\n\nconst prompt = ChatPromptTemplate.fromTemplate(\"tell me a joke about {topic}\");\nconst parser = StringOutputParser();\nconst chain = prompt.pipe(model).pipe(parser);\n\nfor await (const event of await chain.streamEvents(\n  { topic: \"parrot\" },\n  { version: \"v2\" }\n)) {\n  if (event.event === \"on_chat_model_stream\") {\n    console.log(event);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Messages to PostgreSQL Chat History\nDESCRIPTION: Methods to add messages to the chat history. Shows both adding a single message and adding multiple messages as an array. These messages are stored in the PostgreSQL database.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/google_cloudsql_pg.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AIMessage, BaseMessage, HumanMessage } from \"@langchain/core/messages\";\n\n// Add one message\nconst msg = new HumanMessage(\"Hi!\");\nawait historyInstance.addMessage(msg);\n\n// Add an array of messages\nconst msg1: HumanMessage = new HumanMessage(\"Hi!\");\nconst msg2: AIMessage = new AIMessage(\"what's up?\");\nconst msg3: HumanMessage = new HumanMessage(\"How are you?\");\nconst messages: BaseMessage[] = [msg1, msg2, msg3];\nawait historyInstance.addMessages(messages);\n```\n\n----------------------------------------\n\nTITLE: Loading JSON without JSON Pointer in TypeScript\nDESCRIPTION: This example demonstrates how to use the JSONLoader to load all strings from a JSON file without specifying a JSON pointer. It processes a simple JSON structure with an array of texts.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/json.mdx#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"texts\": [\"This is a sentence.\", \"This is another sentence.\"]\n}\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSONLoader } from \"langchain/document_loaders/fs/json\";\n\nconst loader = new JSONLoader(\"src/document_loaders/example_data/example.json\");\n\nconst docs = await loader.load();\n/*\n[\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/json\",\n      \"line\": 1,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"This is a sentence.\",\n  },\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/json\",\n      \"line\": 2,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"This is another sentence.\",\n  },\n]\n*/\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for OpenAI and Neo4j\nDESCRIPTION: This snippet demonstrates how to set up environment variables for OpenAI API key, LangSmith observability, and Neo4j database credentials.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/graph_mapping.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: env\nCODE:\n```\nOPENAI_API_KEY=your-api-key\n\n# Optional, use LangSmith for best-in-class observability\nLANGSMITH_API_KEY=your-api-key\nLANGSMITH_TRACING=true\n\n# Reduce tracing latency if you are not in a serverless environment\n# LANGCHAIN_CALLBACKS_BACKGROUND=true\n\nNEO4J_URI=\"bolt://localhost:7687\"\nNEO4J_USERNAME=\"neo4j\"\nNEO4J_PASSWORD=\"password\"\n```\n\n----------------------------------------\n\nTITLE: Similarity Search with Scores in HNSWLib\nDESCRIPTION: Performs a similarity search that returns both matching documents and their similarity scores. This extended search function helps evaluate the relevance of each returned document.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/hnswlib.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconst similaritySearchWithScoreResults = await vectorStore.similaritySearchWithScore(\"biology\", 2, filter)\n\nfor (const [doc, score] of similaritySearchWithScoreResults) {\n  console.log(`* [SIM=${score.toFixed(3)}] ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`);\n}\n```\n\n----------------------------------------\n\nTITLE: Complete Example of DeepInfraEmbeddings Usage\nDESCRIPTION: A comprehensive example demonstrating the setup and usage of DeepInfraEmbeddings, including generating embeddings for both queries and documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/deepinfra.mdx#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { DeepInfraEmbeddings } from \"@langchain/community/embeddings/deepinfra\";\n\nconst embeddings = new DeepInfraEmbeddings({\n  apiToken: \"YOUR_API_TOKEN\",\n  modelName: \"sentence-transformers/clip-ViT-B-32\",\n  batchSize: 512,\n});\n\nasync function runExample() {\n  const queryEmbedding = await embeddings.embedQuery(\"Example query text.\");\n  console.log(\"Query Embedding:\", queryEmbedding);\n\n  const documents = [\"Text 1\", \"Text 2\", \"Text 3\"];\n  const documentEmbeddings = await embeddings.embedDocuments(documents);\n  console.log(\"Document Embeddings:\", documentEmbeddings);\n}\n\nrunExample();\n```\n\n----------------------------------------\n\nTITLE: Instantiating WebPDFLoader with a PDF File\nDESCRIPTION: Shows how to read a PDF file as a buffer, convert it to a Blob, and instantiate the WebPDFLoader with optional parameters. This creates the loader that will process the PDF document.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/pdf.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport fs from \"fs/promises\";\nimport { WebPDFLoader } from \"@langchain/community/document_loaders/web/pdf\"\n\nconst nike10kPDFPath = \"../../../../data/nke-10k-2023.pdf\";\n\n// Read the file as a buffer\nconst buffer = await fs.readFile(nike10kPDFPath);\n\n// Create a Blob from the buffer\nconst nike10kPDFBlob = new Blob([buffer], { type: 'application/pdf' });\n\nconst loader = new WebPDFLoader(nike10kPDFBlob, {\n  // required params = ...\n  // optional params = ...\n})\n```\n\n----------------------------------------\n\nTITLE: Deleting Items from Elasticsearch Vector Store by ID (TypeScript)\nDESCRIPTION: Demonstrates how to remove documents from the Elasticsearch vector store using the delete method and an ids array matching the ids assigned during insertion. This is essential for managing and updating the contents of the vector store. Only the specified ids will be deleted; ensure ids correspond to those used previously.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/elasticsearch.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nawait vectorStore.delete({ ids: [\"4\"] });\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of initChatModel in LangChain.js\nDESCRIPTION: Demonstrates the basic usage of the initChatModel() helper to initialize a chat model by specifying the model provider and model name explicitly.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chat_models_universal_init.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { initChatModel } from \"langchain/chat_models_universal\";\n\n// Initialize a chat model by explicitly specifying model provider and model\nconst chat = await initChatModel({\n  modelProvider: \"openai\",\n  model: \"gpt-3.5-turbo\",\n});\n```\n\n----------------------------------------\n\nTITLE: Structuring Tool Observation Message in LangchainJS Trace (JSON)\nDESCRIPTION: This JSON object represents a `ToolMessage` within the LangchainJS trace, specifically capturing the observation or result returned by a tool. It includes the `tool_call_id` to link it back to the corresponding `AIMessageChunk` request, the `content` field holding the tool's output (often as an escaped JSON string containing search results in this case), and additional metadata like the tool's name (`tavily_search_results_json`) stored in `additional_kwargs`. The `lc`, `type`, `id`, and `kwargs` structure is standard for Langchain core message serialization.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n        \"lc\": 1,\n        \"type\": \"constructor\",\n        \"id\": [\n          \"langchain_core\",\n          \"messages\",\n          \"ToolMessage\"\n        ],\n        \"kwargs\": {\n          \"tool_call_id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n          \"content\": \"[{\\\"url\\\": \\\"https://www.everythingzoomer.com/arts-entertainment/2023/08/15/oppenheimer-director-christopher-nolan-on-filmmaking-at-53-i-try-to-challenge-myself-with-every-film/\\\", \\\"content\\\": \\\"Marriage of the Summer\\\\nBlast From the Past: ‘Asteroid City’ & ‘Oppenheimer’ and the Age of Nuclear Anxiety\\\\nEXPLORE\\u00a0 HealthMoneyTravelFoodStyleBook ClubClassifieds#ZoomerDailyPolicy & PerspectiveArts & EntertainmentStars & RoyaltySex & Love\\\\nCONNECT\\u00a0 FacebookTwitterInstagram\\\\nSUBSCRIBE\\u00a0 Terms of Subscription ServiceE-NewslettersSubscribe to Zoomer Magazine\\\\nBROWSE\\u00a0 AboutMastheadContact UsAdvertise with UsPrivacy Policy\\\\nEverythingZoomer.com is part of the ZoomerMedia Digital Network \\\\\\\"I think with experience \\u2014 and with the experience of watching your films with an audience over the years \\u2014 you do more and more recognize the human elements that people respond to, and the things that move you and the things that move the audience.\\\\\\\"\\\\n \\\\\\\"What\\u2019s interesting, as you watch the films over time, is that some of his preoccupations are the same, but then some of them have changed over time with who he is as a person and what\\u2019s going on in his own life,\\\\\\\" Thomas said.\\\\n The British-American director\\u2019s latest explosive drama, Oppenheimer, which has earned upwards of US$940 million at the global box office, follows theoretical physicist J. Robert Oppenheimer (played by Cillian Murphy) as he leads the team creating the first atomic bomb, as director of the Manhattan Project\\u2019s Los Alamos Laboratory.\\\\n Subscribe\\\\nEverything Zoomer\\\\n‘Oppenheimer’ Director Christopher Nolan On Filmmaking at 53: \\\\\\\"I Try to Challenge Myself with Every Film\\\\\\\"\\\\nDirector Christopher Nolan poses upon his arrival for the premiere of the movie 'Oppenheimer' in Paris on July 11, 2023.\\\",\\\"score\\\":0.92002,\\\"raw_content\\\":null},{\\\"title\\\":\\\"'Oppenheimer' Review: A Man for Our Time - The New York Times\\\",\\\"url\\\":\\\"https://www.nytimes.com/2023/07/19/movies/oppenheimer-review-christopher-nolan.html\\\",\\\"content\\\":\\\"Instead, it is here that the film\\u2019s complexities and all its many fragments finally converge as Nolan puts the finishing touches on his portrait of a man who contributed to an age of transformational scientific discovery, who personified the intersection of science and politics, including in his role as a Communist boogeyman, who was transformed by his role in the creation of weapons of mass destruction and soon after raised the alarm about the dangers of nuclear war.\\\\n He served as director of a clandestine weapons lab built in a near-desolate stretch of Los Alamos, in New Mexico, where he and many other of the era\\u2019s most dazzling scientific minds puzzled through how to harness nuclear reactions for the weapons that killed tens of thousands instantly, ending the war in the Pacific.\\\\n Nolan integrates these black-and-white sections with the color ones, using scenes from the hearing and the confirmation \\u2014 Strauss\\u2019s role in the hearing and his relationship with Oppenheimer directly affected the confirmation\\u2019s outcome \\u2014 to create a dialectical synthesis. To signal his conceit, he stamps the film with the words \\\\\\\"fission\\\\\\\" (a splitting into parts) and \\\\\\\"fusion\\\\\\\" (a merging of elements); Nolan being Nolan, he further complicates the film by recurrently kinking up the overarching chronology \\u2014 it is a lot.\\\\n It\\u2019s also at Berkeley that Oppenheimer meets the project\\u2019s military head, Leslie Groves (a predictably good Damon), who makes him Los Alamos\\u2019s director, despite the leftist causes he supported \\u2014 among them, the fight against fascism during the Spanish Civil War \\u2014 and some of his associations, including with Communist Party members like his brother, Frank (Dylan Arnold).\\\\n\\\",\\\"score\\\":0.91831,\\\"raw_content\\\":null}]\",\n          \"additional_kwargs\": {\n            \"name\": \"tavily_search_results_json\"\n          },\n          \"response_metadata\": {}\n        }\n      }\n```\n\n----------------------------------------\n\nTITLE: Using Custom PDF.js Build\nDESCRIPTION: Creates a PDFLoader with a custom PDF.js build by providing a custom pdfjs function that imports from pdfjs-dist.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/pdf.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { PDFLoader } from \"@langchain/community/document_loaders/fs/pdf\";\n\nconst customBuildLoader = new PDFLoader(nike10kPdfPath, {\n  // you may need to add `.then(m => m.default)` to the end of the import\n  // @lc-ts-ignore\n  pdfjs: () => import(\"pdfjs-dist/legacy/build/pdf.js\"),\n});\n```\n\n----------------------------------------\n\nTITLE: Installing ChatOllama Dependencies with npm/yarn\nDESCRIPTION: Command to install the required packages @langchain/ollama and @langchain/core using npm or yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/ollama.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n@langchain/ollama @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Using the Discord Tool Standalone - TypeScript\nDESCRIPTION: Demonstrates how to use the Discord Tool independently in a TypeScript project. This snippet is shown through an import of a usage example from '@examples/tools/discord.ts', which assumes that all required dependencies have been installed. It likely includes initializing the Discord Tool and performing basic operations such as reading and writing messages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/discord.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport ToolExample from \"@examples/tools/discord.ts\";\n```\n\n----------------------------------------\n\nTITLE: Importing Azure Cosmos DB NoSQL Chat Message History\nDESCRIPTION: Code snippet for importing the Azure Cosmos DB NoSQL Chat Message History class, which provides persistent storage for chat messages using Cosmos DB NoSQL API.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/microsoft.mdx#2025-04-22_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AzureCosmosDBNoSQLChatMessageHistory } from \"@langchain/azure-cosmosdb\";\n```\n\n----------------------------------------\n\nTITLE: Using the Custom Document Retriever\nDESCRIPTION: Example showing how to instantiate and use the custom retriever by calling its invoke method with a query string.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/custom_retriever.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst retriever = new CustomRetriever({});\n\nawait retriever.invoke(\"LangChain docs\");\n```\n\n----------------------------------------\n\nTITLE: Scoring Algorithm Implementation in TypeScript\nDESCRIPTION: Core scoring algorithm for the Time-Weighted Retriever that combines time decay with vector relevance. The score is calculated using a configurable decay rate and the time since last access.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/time-weighted-retriever.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nlet score = (1.0 - this.decayRate) ** hoursPassed + vectorRelevance;\n```\n\n----------------------------------------\n\nTITLE: Setting DeepSeek API Key Environment Variable\nDESCRIPTION: Command to set the DEEPSEEK_API_KEY environment variable, which is required for authenticating with the DeepSeek API.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-deepseek/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport DEEPSEEK_API_KEY=\n```\n\n----------------------------------------\n\nTITLE: Loading Documents with PuppeteerWebBaseLoader\nDESCRIPTION: Code to load documents from a web page using the instantiated PuppeteerWebBaseLoader.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/web_puppeteer.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst docs = await loader.load()\ndocs[0]\n```\n\n----------------------------------------\n\nTITLE: Importing ChatOpenAI in Deno\nDESCRIPTION: Examples of importing the ChatOpenAI class from the @langchain/openai package in a Deno environment.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/installation.mdx#2025-04-22_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"https://esm.sh/@langchain/openai\";\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"npm:@langchain/openai\";\n```\n\n----------------------------------------\n\nTITLE: Model Instantiation Configuration\nDESCRIPTION: Example showing how to instantiate the model with configuration parameters including model name, temperature, token limits, timeout and retry settings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/llms.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { __module_name__ } from \"__full_import_path__\"\n\nconst llm = new __module_name__({\n  model: \"model-name\",\n  temperature: 0,\n  maxTokens: undefined,\n  timeout: undefined,\n  maxRetries: 2,\n  // other params...\n})\n```\n\n----------------------------------------\n\nTITLE: Defining the BaseExampleSelector Interface in TypeScript\nDESCRIPTION: Shows the core interface for example selectors in LangChain.js, which includes methods for adding examples and selecting examples based on input variables.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/example_selectors.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nclass BaseExampleSelector {\n  addExample(example: Example): Promise<void | string>;\n\n  selectExamples(input_variables: Example): Promise<Example[]>;\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for GitHub Loader in Node.js\nDESCRIPTION: Command to install the necessary packages for using the GitHub loader, including @langchain/community, @langchain/core, and the ignore package which is a peer dependency.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/github.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core ignore\n```\n\n----------------------------------------\n\nTITLE: Large Database Table Handling Example\nDESCRIPTION: TypeScript code referenced from external file for handling multiple tables in large databases.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/sql_large_db.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n<CodeBlock language=\"typescript\">{LargeDbExample}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Streaming Agent Responses in Python\nDESCRIPTION: This code shows how to stream responses from the agent for a given query. It demonstrates the agent's behavior without memory integration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nconst query = \"What is Task Decomposition?\"\n\nfor await (const s of await agentExecutor.stream(\n  { messages: [{ role: \"user\", content: query }] },\n)){\n  console.log(s)\n  console.log(\"----\")\n}\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith API Key for Automated Tracing\nDESCRIPTION: Sets environment variables for LangSmith API key to enable automated tracing of model calls. This is optional and can be used for debugging and monitoring.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/ibm.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for IAM Authentication in IBM watsonx.ai\nDESCRIPTION: Sets up environment variables for IAM authentication with IBM watsonx.ai. This includes setting the authentication type and API key.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/ibm.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport WATSONX_AI_AUTH_TYPE=iam\nexport WATSONX_AI_APIKEY=<YOUR-APIKEY>\n```\n\n----------------------------------------\n\nTITLE: Configuring Default Search Parameters for Self-Query Retriever in TypeScript\nDESCRIPTION: Demonstrates how to set up default search parameters for the self-query retriever, including custom filters using Supabase's filter syntax.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/supabase.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport type { SupabaseFilter } from \"@langchain/community/vectorstores/supabase\";\n\nconst selfQueryRetrieverWithDefaultParams = SelfQueryRetriever.fromLLM({\n  llm: llm,\n  vectorStore: vectorStore,\n  documentContents: \"Brief summary of a movie\",\n  attributeInfo: attributeInfo,\n  structuredQueryTranslator: new SupabaseTranslator(),\n  searchParams: {\n    filter: (rpc: SupabaseFilter) => rpc.filter(\"metadata->>type\", \"eq\", \"movie\"),\n    mergeFiltersOperator: \"and\",\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Using topLogprobs Parameter with OpenAI Chat Model\nDESCRIPTION: Illustrates how to use the topLogprobs parameter to get alternative token suggestions along with their probabilities.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/logprobs.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst modelWithTopLogprobs = new ChatOpenAI({\n  model: \"gpt-4o\",\n  logprobs: true,\n  topLogprobs: 3,\n});\n\nconst res = await modelWithTopLogprobs.invoke(\"how are you today?\");\n\nres.response_metadata.logprobs.content.slice(0, 5);\n```\n\n----------------------------------------\n\nTITLE: Example Output of Listing WatsonxToolkit Tools\nDESCRIPTION: Provides a sample text output showing the list of tools available in the WatsonxToolkit, including their names and descriptions (e.g., GoogleSearch, WebCrawler, PythonInterpreter).\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/ibm.mdx#2025-04-22_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n[\n  {\n    name: \"GoogleSearch\",\n    description: \"Search for online trends, news, current events, real-time information, or research topics.\"\n  },\n  {\n    name: \"WebCrawler\",\n    description: \"Useful for when you need to summarize a webpage. Do not use for Web search.\"\n  },\n  {\n    name: \"PythonInterpreter\",\n    description: \"Run Python code generated by the agent model.\"\n  },\n  {\n    name: \"SDXLTurbo\",\n    description: \"Generate an image from text using Stability.ai\"\n  },\n  { name: \"Weather\", description: \"Find the weather for a city.\" },\n  {\n    name: \"RAGQuery\",\n    description: \"Search the documents in a vector index.\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Setting Up Zod Schema for Query Analysis\nDESCRIPTION: Defines a schema for search queries using Zod, specifying the structure for query and author fields\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_high_cardinality.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { z } from \"zod\";\n\nconst searchSchema = z.object({\n    query: z.string(),\n    author: z.string(),\n})\n```\n\n----------------------------------------\n\nTITLE: Creating Example Formatter Prompt Template in JavaScript\nDESCRIPTION: Defines a PromptTemplate to format few-shot examples for use in language models.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot_examples.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PromptTemplate } from \"@langchain/core/prompts\";\n\nconst examplePrompt = PromptTemplate.fromTemplate(\"Question: {question}\\n{answer}\")\n```\n\n----------------------------------------\n\nTITLE: Installing libSQL Integration Dependencies (Bash)\nDESCRIPTION: Installs the necessary Node.js packages for using the libSQL vector store with LangChain.js, including the libSQL client, OpenAI embeddings integration, and the core community package. This command uses npm, but the `npm2yarn` directive suggests a yarn equivalent is also applicable.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/libsql.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @libsql/client @langchain/openai @langchain/community\n```\n\n----------------------------------------\n\nTITLE: Setting API Credentials for Vector Store Integration\nDESCRIPTION: Code snippet showing how to set environment variables for the vector store API key. Includes placeholders for the environment variable name that needs to be replaced with the actual variable name for the specific vector store implementation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/vectorstores.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.__env_var_name__ = \"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Message Preprocessing with LCEL\nDESCRIPTION: Shows how to add conversation management through preprocessing steps using LCEL (LangChain Expression Language). Includes message trimming, tool binding, and handling full conversation history.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/versions/migrating_memory/conversation_buffer_window_memory.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { AIMessage, HumanMessage, SystemMessage, BaseMessage, trimMessages } from \"@langchain/core/messages\";\nimport { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n\nconst model3 = new ChatOpenAI({ model: \"gpt-4o\" });\n\nconst whatDidTheCowSay = tool(\n  (): string => {\n    return \"foo\";\n  },\n  {\n    name: \"what_did_the_cow_say\",\n    description: \"Check to see what the cow said.\",\n    schema: z.object({}),\n  }\n);\n\nconst messageProcessor = trimMessages(\n  {\n    tokenCounter: (msgs) => msgs.length,\n    maxTokens: 5,\n    strategy: \"last\",\n    startOn: \"human\",\n    includeSystem: true,\n    allowPartial: false,\n  }\n);\n\nconst modelWithTools = model3.bindTools([whatDidTheCowSay]);\n\nconst modelWithPreprocessor = messageProcessor.pipe(modelWithTools);\n\nconst fullHistory = [\n  new SystemMessage(\"you're a good assistant, you always respond with a joke.\"),\n  new HumanMessage(\"i wonder why it's called langchain\"),\n  new AIMessage('Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'),\n  new HumanMessage(\"and who is harrison chasing anyways\"),\n  new AIMessage(\"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\"),\n  new HumanMessage(\"why is 42 always the answer?\"),\n  new AIMessage(\"Because it's the only number that's constantly right, even when it doesn't add up!\"),\n  new HumanMessage(\"What did the cow say?\"),\n];\n\nconst result = await modelWithPreprocessor.invoke(fullHistory);\nconsole.log(result);\n```\n\n----------------------------------------\n\nTITLE: Invoking Fireworks LLM for Text Completion\nDESCRIPTION: Example of calling the Fireworks model with input text to generate a completion response.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/fireworks.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst inputText = \"Fireworks is an AI company that \"\n\nconst completion = await llm.invoke(inputText)\ncompletion\n```\n\n----------------------------------------\n\nTITLE: Directly Invoking SerpAPI Tool with Arguments\nDESCRIPTION: Example showing how to invoke the SerpAPI tool directly with input arguments to perform a search query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/serpapi.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nawait tool.invoke({\n  input: \"what is the current weather in SF?\"\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Vector Index with Cloudflare Wrangler\nDESCRIPTION: This bash command uses Cloudflare Wrangler to create a vector index named \\\"<index_name>\\\" with a specified preset. It requires prior account setup and Wrangler installation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/cloudflare_vectorize.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ npx wrangler vectorize create <index_name> --preset @cf/baai/bge-small-en-v1.5\n```\n\n----------------------------------------\n\nTITLE: Defining Multiple Entities Schema\nDESCRIPTION: Implementation of a nested Zod schema for handling multiple person entities in a single extraction.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/extraction.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { z } from \"zod\";\n\nconst person = z.object({\n  name: z.optional(z.string()).describe('The name of the person'),\n  hair_color: z.optional(z.string()).describe(\"The color of the person's hair if known\"),\n  height_in_meters: z.number().nullish().describe('Height measured in meters'),\n});\n  \nconst dataSchema = z.object({\n  people: z.array(person).describe('Extracted data about people'),\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Cohere Model with Custom Client\nDESCRIPTION: Demonstrates how to instantiate a custom CohereClient for specialized deployments like Azure, AWS Bedrock, or standalone Cohere instances.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/cohere.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { Cohere } from \"@langchain/cohere\";\nimport { CohereClient } from \"cohere-ai\";\n\nconst client = new CohereClient({\n  token: \"<your-api-key>\",\n  environment: \"<your-cohere-deployment-url>\", //optional\n  // other params\n});\n\nconst llmWithCustomClient = new Cohere({\n  client,\n  // other params...\n});\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing Data for Vector Store in TypeScript\nDESCRIPTION: This snippet demonstrates how to load a speech from a file, split it into chunks, and create a vector store index using MemoryVectorStore and OpenAIEmbeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/vectorstores.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\n\n// Load the document and split it into chunks\nconst loader = new TextLoader(\"state_of_the_union.txt\");\nconst docs = await loader.load();\n\n// Create the vector store and index the docs\nconst vectorStore = await MemoryVectorStore.fromDocuments(\n  docs,\n  new OpenAIEmbeddings()\n);\n\n// Search for the most similar document\nconst resultOne = await vectorStore.similaritySearch(\"What did the president say about Ukraine?\", 1);\nconsole.log(resultOne);\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatOpenAI Model in TypeScript\nDESCRIPTION: This code initializes a ChatOpenAI model with specific parameters for use in a LangChain application.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/exa.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o-mini\",\n  temperature: 0,\n});\n```\n\n----------------------------------------\n\nTITLE: Instantiating WatsonxToolkit in Typescript\nDESCRIPTION: Demonstrates how to import and instantiate the `WatsonxToolkit`. It requires the `dotenv` package for configuration and relies on the `WATSONX_AI_SERVICE_URL` environment variable being set. The `init` method initializes the toolkit with a specific API version and service URL.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/ibm.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { WatsonxToolkit } from \"@langchain/community/agents/toolkits/ibm\";\nimport \"dotenv/config\";\n\nconst toolkit = await WatsonxToolkit.init({\n  version: \"2024-05-31\",\n  serviceUrl: process.env.WATSONX_AI_SERVICE_URL,\n});\n```\n\n----------------------------------------\n\nTITLE: Character-based Text Splitting in TypeScript using LangChain\nDESCRIPTION: Implementation of character-based text splitting using LangChain's CharacterTextSplitter. Configures the splitter with a chunk size of 100 characters and no overlap between chunks.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/text_splitters.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CharacterTextSplitter } from \"@langchain/textsplitters\";\nconst textSplitter = new CharacterTextSplitter({\n  chunkSize: 100,\n  chunkOverlap: 0,\n});\nconst texts = await textSplitter.splitText(document);\n```\n\n----------------------------------------\n\nTITLE: Creating a Retriever from WeaviateStore\nDESCRIPTION: Converting the vector store into a retriever for use in chains with optional filtering\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/weaviate.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nconst retriever = vectorStore.asRetriever({\n  // Optional filter\n  filter: filter,\n  k: 2,\n});\nawait retriever.invoke(\"biology\");\n```\n\n----------------------------------------\n\nTITLE: Embedding Multiple Documents with OpenAI in TypeScript\nDESCRIPTION: Demonstration of embedding multiple documents using the embedDocuments method of OpenAIEmbeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/embed_text.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst documentRes = await embeddings.embedDocuments([\"Hello world\", \"Bye bye\"]);\n/*\n[\n  [\n    -0.004845875,   0.004899438,  -0.016358767,  -0.024475135, -0.017341806,\n      0.012571548,  -0.019156644,   0.009036391,  -0.010227379, -0.026945334,\n      0.022861943,   0.010321903,  -0.023479493, -0.0066544134,  0.007977734,\n    0.0026371893,   0.025206111,  -0.012048521,   0.012943339,  0.013094575,\n    -0.010580265,  -0.003509951,   0.004070787,   0.008639394, -0.020631202,\n    ... 1511 more items\n  ]\n  [\n      -0.009446913,  -0.013253193,   0.013174579,  0.0057552797,  -0.038993083,\n      0.0077763423,    -0.0260478, -0.0114384955, -0.0022683728,  -0.016509168,\n      0.041797023,    0.01787183,    0.00552271, -0.0049789557,   0.018146982,\n      -0.01542166,   0.033752076,   0.006112323,   0.023872782,  -0.016535373,\n      -0.006623321,   0.016116094, -0.0061090477, -0.0044155475,  -0.016627092,\n    ... 1511 more items\n  ]\n]\n*/\n```\n\n----------------------------------------\n\nTITLE: Installing Google GenAI Package for LangChain\nDESCRIPTION: Command to install the Google GenAI package for use with LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/google.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/google-genai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Using Connery Action Tool in LangChain.js (TypeScript)\nDESCRIPTION: Illustrates how to fetch a specific Connery Action by its ID from a configured Connery Runner and invoke it with parameters. This example uses a placeholder `{Example}` which represents the actual TypeScript code located in `@examples/tools/connery.ts`. The code likely involves importing `ConneryAction`, creating an instance with an action ID (like Gmail's 'Send email'), and then calling the action.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/connery.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n{Example}\n```\n\n----------------------------------------\n\nTITLE: Implementing Advanced Classification Prompt\nDESCRIPTION: Sets up the prompt template and structured output handler for the advanced classification schema.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/classification.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst taggingPrompt2 = ChatPromptTemplate.fromTemplate(\n`Extract the desired information from the following passage.\n\nOnly extract the properties mentioned in the 'Classification' function.\n\nPassage:\n{input}\n`\n)\n\nconst llmWithStructuredOutput2 = llm.withStructuredOutput(classificationSchema2, { name: \"extractor\" })\n```\n\n----------------------------------------\n\nTITLE: Initializing Chat Model\nDESCRIPTION: Initialize OpenAI chat model with GPT-4.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst llm = new ChatOpenAI({ model: \"gpt-4o\" });\n```\n\n----------------------------------------\n\nTITLE: Configuring Gradient AI Environment Variables (Bash)\nDESCRIPTION: Sets the necessary environment variables `GRADIENT_ACCESS_TOKEN` and `GRADIENT_WORKSPACE_ID` using the export command in a bash shell. These are required for authenticating API requests to Gradient AI when using the `GradientEmbeddings` class, unless credentials are provided directly during instantiation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/gradient_ai.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport GRADIENT_ACCESS_TOKEN=<YOUR_ACCESS_TOKEN>\nexport GRADIENT_WORKSPACE_ID=<YOUR_WORKSPACE_ID>\n```\n\n----------------------------------------\n\nTITLE: Using Built-in Toolkits in TypeScript\nDESCRIPTION: This snippet shows how to initialize a toolkit and retrieve its tools, demonstrating the typical usage pattern for built-in toolkits.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tools_builtin.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n// Initialize a toolkit\nconst toolkit = new ExampleTookit(...);\n\n// Get list of tools\nconst tools = toolkit.getTools();\n```\n\n----------------------------------------\n\nTITLE: Configuring a Tool to Return Artifacts in TypeScript\nDESCRIPTION: Shows how to configure a tool to return both content for the model and artifacts for downstream components. This pattern is useful when a tool produces outputs that shouldn't be directly passed to the model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/tools.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst someTool = tool(({ ... }) => {\n    // do something\n}, {\n  // ... tool schema args\n  // Set the returnType to \"content_and_artifact\"\n  responseFormat: \"content_and_artifact\"\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Vectara Vector Store with LangChain.js - TypeScript\nDESCRIPTION: This TypeScript snippet demonstrates asynchronous initialization of a VectaraStore instance using the static fromTexts method. It accepts an array of texts, corresponding metadata objects, an embeddings provider (using OpenAIEmbeddings as a no-op), and additional args containing connection or configuration details. Dependencies include the VectaraStore and OpenAIEmbeddings classes from LangChain, and the environment must be configured with Vectara access credentials. Inputs are the texts to index, their metadata, an optional embeddings instance (unused here), and connection parameters; the output is a ready-to-use VectaraStore instance. Note that as Vectara manages embeddings internally, supplying custom embeddings has no effect.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/vectara.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nconst store = await VectaraStore.fromTexts(\n  [\"hello world\", \"hi there\"],\n  [{ foo: \"bar\" }, { foo: \"baz\" }],\n  // This won't have an effect. Provide a FakeEmbeddings instance instead for clarity.\n  new OpenAIEmbeddings(),\n  args\n);\n```\n\n----------------------------------------\n\nTITLE: Setting up a pgvector instance with Docker Compose\nDESCRIPTION: Instructions to set up a self-hosted Postgres instance with pgvector using Docker Compose. It involves creating a docker-compose.yml file and running it using 'docker compose up'. The actual docker-compose configuration can be obtained from the linked DockerExample file.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/typeorm.mdx#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n{DockerExample}\n```\n\n----------------------------------------\n\nTITLE: Embedding Single Text\nDESCRIPTION: Demonstrating how to embed a single query text using embedQuery method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/openai.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst singleVector = await embeddings.embedQuery(text);\n\nconsole.log(singleVector.slice(0, 100));\n```\n\n----------------------------------------\n\nTITLE: Merging Extraction Results from Multiple Chunks\nDESCRIPTION: Shows how to combine extraction results from multiple document chunks by flattening the array of key developments. This step is crucial after processing chunks separately to consolidate all extracted information.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/extraction_long_text.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconst keyDevelopments = results.flatMap((result) => result.key_developments);\n\nkeyDevelopments.slice(0, 20);\n```\n\n----------------------------------------\n\nTITLE: Implementing Query Transformation for Conversational Context\nDESCRIPTION: Creates a query transformation chain that can convert conversational followup questions into standalone queries that preserve the context of the conversation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_retrieval.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nconst queryTransformPrompt = ChatPromptTemplate.fromMessages([\n  new MessagesPlaceholder(\"messages\"),\n  [\n    \"user\",\n    \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation. Only respond with the query, nothing else.\",\n  ],\n]);\n\nconst queryTransformationChain = queryTransformPrompt.pipe(llm);\n\nawait queryTransformationChain.invoke({\n  messages: [\n    new HumanMessage(\"Can LangSmith help test my LLM applications?\"),\n    new AIMessage(\n      \"Yes, LangSmith can help test and evaluate your LLM applications. It allows you to quickly edit examples and add them to datasets to expand the surface area of your evaluation sets or to fine-tune a model for improved quality or reduced costs. Additionally, LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise.\"\n    ),\n    new HumanMessage(\"Tell me more!\"),\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Searching for Similar Examples in a LangSmith Dataset\nDESCRIPTION: Retrieves examples from an indexed LangSmith dataset that are semantically similar to the input query. The method returns a specified number of most similar examples.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/example_selectors_langsmith.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst examples = await lsClient.similarExamples(\n  { input: \"whats the negation of the negation of the negation of 3\" },\n  dataset.id,\n  3,\n)\nconsole.log(examples.length)\n```\n\n----------------------------------------\n\nTITLE: Advanced Playwright Configuration\nDESCRIPTION: Advanced example showing custom configuration options and evaluation function for PlaywrightWebBaseLoader\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/web_playwright.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport {\n  PlaywrightWebBaseLoader,\n  Page,\n  Browser,\n} from \"@langchain/community/document_loaders/web/playwright\";\n\nconst loader = new PlaywrightWebBaseLoader(\"https://www.tabnews.com.br/\", {\n  launchOptions: {\n    headless: true,\n  },\n  gotoOptions: {\n    waitUntil: \"domcontentloaded\",\n  },\n  /** Pass custom evaluate, in this case you get page and browser instances */\n  async evaluate(page: Page, browser: Browser, response: Response | null) {\n    await page.waitForResponse(\"https://www.tabnews.com.br/va/view\");\n\n    const result = await page.evaluate(() => document.body.innerHTML);\n    return result;\n  },\n});\n\nconst docs = await loader.load();\n```\n\n----------------------------------------\n\nTITLE: Multiple Text Embedding Example\nDESCRIPTION: Demonstration of embedding multiple texts using embedDocuments method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/ollama.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconst text2 = \"LangGraph is a library for building stateful, multi-actor applications with LLMs\";\n\nconst vectors = await embeddings.embedDocuments([text, text2]);\n\nconsole.log(vectors[0].slice(0, 100));\nconsole.log(vectors[1].slice(0, 100));\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for MistralAI and LangSmith\nDESCRIPTION: Instructions for setting up the MISTRAL_API_KEY and optional LANGSMITH environment variables for automated tracing.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/mistralai.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport MISTRAL_API_KEY=\"your-api-key\"\n\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Accessing Chat History State\nDESCRIPTION: Demonstrates how to retrieve and inspect the conversation history from the application state.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nconst chatHistory = (await app.getState(config)).values.chat_history;\nfor (const message of chatHistory) {\n  console.log(message);\n}\n```\n\n----------------------------------------\n\nTITLE: Performing Basic Similarity Search with CassandraStore in TypeScript\nDESCRIPTION: Shows how to perform a basic vector similarity search using the initialized `vectorStore`. The `similaritySearch` method takes a query string and the number of desired results (k=1 in this case) and returns the most similar documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/cassandra.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst results = await vectorStore.similaritySearch(\"Green yellow purple\", 1);\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatOpenAI Model for Tool Integration\nDESCRIPTION: Creates an instance of ChatOpenAI model to be used with the TavilySearch tool in a chain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/tavily_search.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n// @lc-docs-hide-cell\n\nimport { ChatOpenAI } from \"@langchain/openai\"\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o\",\n  temperature: 0,\n})\n```\n\n----------------------------------------\n\nTITLE: Chaining ChatCohere with Prompt Template\nDESCRIPTION: Creates a chain combining a prompt template with the ChatCohere model for language translation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/cohere.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\"\n\nconst prompt = ChatPromptTemplate.fromMessages(\n    [\n        [\n            \"system\",\n            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n        ],\n        [\"human\", \"{input}\"],\n    ]\n)\n\nconst chain = prompt.pipe(llm);\nawait chain.invoke(\n    {\n        input_language: \"English\",\n        output_language: \"German\",\n        input: \"I love programming.\",\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Document Embedding with Llama CPP\nDESCRIPTION: Implementation example for document embedding using Llama CPP integration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/llama_cpp.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport DocsExample from \"@examples/embeddings/llama_cpp_docs.ts\";\n```\n\n----------------------------------------\n\nTITLE: Installing BM25Retriever Dependencies for LangChain.js\nDESCRIPTION: Command to install the necessary packages for using BM25Retriever in LangChain.js. It requires @langchain/community and @langchain/core.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/bm25.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n@langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Tool with RunnableConfig Access in LangChain\nDESCRIPTION: Demonstrates how to create a custom reverse text tool using the tool helper method with RunnableConfig access. The tool combines input text with a configurable parameter and includes type definitions, schema validation using Zod, and proper configuration handling.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_configure.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport { z } from \"zod\";\nimport { tool } from \"@langchain/core/tools\";\nimport type { RunnableConfig } from \"@langchain/core/runnables\";\n\nconst reverseTool = tool(\n  async (input: { text: string }, config?: RunnableConfig) => {\n    const originalString = input.text + (config?.configurable?.additional_field ?? \"\");\n    return originalString.split(\"\").reverse().join(\"\");\n  }, {\n    name: \"reverse\",\n    description: \"A test tool that combines input text with a configurable parameter.\",\n    schema: z.object({\n      text: z.string()\n    }),\n  }\n);\n```\n\n----------------------------------------\n\nTITLE: Setting Google API Key for GenAI\nDESCRIPTION: Command to set the Google API key as an environment variable for authentication.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/google.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport GOOGLE_API_KEY=your-api-key\n```\n\n----------------------------------------\n\nTITLE: JSON-formatted Response Invocation with ChatCerebras\nDESCRIPTION: Shows how to request JSON-formatted responses from a ChatCerebras model, both by passing the format in each invoke call and by binding it to the model instance.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/cerebras.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst messages = [\n  {\n    role: \"system\",\n    content: \"You are a math tutor that handles math exercises and makes output in json in format { result: number }.\",\n  },\n  { role: \"user\",  content: \"2 + 2\" },\n];\n\nconst aiInvokeMsg = await llm.invoke(messages, { response_format: { type: \"json_object\" } });\n\n// if you want not to pass response_format in every invoke, you can bind it to the instance\nconst llmWithResponseFormat = llm.bind({ response_format: { type: \"json_object\" } });\nconst aiBindMsg = await llmWithResponseFormat.invoke(messages);\n\n// they are the same\nconsole.log({ aiInvokeMsgContent: aiInvokeMsg.content, aiBindMsg: aiBindMsg.content });\n```\n\n----------------------------------------\n\nTITLE: Chaining AzureOpenAI Model with Prompt Template in LangChain.js\nDESCRIPTION: Shows how to chain the AzureOpenAI completion model with a prompt template for more complex language tasks.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/azure.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { PromptTemplate } from \"@langchain/core/prompts\"\n\nconst prompt = new PromptTemplate({\n  template: \"How to say {input} in {output_language}:\\n\",\n  inputVariables: [\"input\", \"output_language\"],\n})\n\nconst chain = prompt.pipe(llm);\nawait chain.invoke(\n  {\n    output_language: \"German\",\n    input: \"I love programming.\",\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing Google Vertex AI Matching Engine in TypeScript (Node.js)\nDESCRIPTION: Demonstrates how to initialize the `MatchingEngine` vector store in a Node.js TypeScript application. It involves creating an embeddings instance (here, `SyntheticEmbeddings` for demonstration), setting up a document store (like `GoogleCloudStorageDocstore`), and providing configuration details such as the Index ID, Index Endpoint ID, API version, and the docstore instance. Environment variables are used to supply sensitive configuration values.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/googlevertexai.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { MatchingEngine } from \"@langchain/community/vectorstores/googlevertexai\";\nimport { Document } from \"langchain/document\";\nimport { SyntheticEmbeddings } from \"langchain/embeddings/fake\";\nimport { GoogleCloudStorageDocstore } from \"@langchain/community/stores/doc/gcs\";\n\nconst embeddings = new SyntheticEmbeddings({\n  vectorSize: Number.parseInt(\n    process.env.SYNTHETIC_EMBEDDINGS_VECTOR_SIZE ?? \"768\",\n    10\n  ),\n});\n\nconst store = new GoogleCloudStorageDocstore({\n  bucket: process.env.GOOGLE_CLOUD_STORAGE_BUCKET!,\n});\n\nconst config = {\n  index: process.env.GOOGLE_VERTEXAI_MATCHINGENGINE_INDEX!,\n  indexEndpoint: process.env.GOOGLE_VERTEXAI_MATCHINGENGINE_INDEXENDPOINT!,\n  apiVersion: \"v1beta1\",\n  docstore: store,\n};\n\nconst engine = new MatchingEngine(embeddings, config);\n```\n\n----------------------------------------\n\nTITLE: Creating and Invoking GraphCypherQAChain\nDESCRIPTION: This code demonstrates how to create a GraphCypherQAChain, which combines an LLM (ChatOpenAI) with a graph database to answer questions. It shows the setup of the chain and how to invoke it with a sample query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/graph.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { GraphCypherQAChain } from \"langchain/chains/graph_qa/cypher\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst llm = new ChatOpenAI({ model: \"gpt-3.5-turbo\", temperature: 0 })\nconst chain = GraphCypherQAChain.fromLLM({\n  llm,\n  graph,\n});\nconst response = await chain.invoke({ query: \"What was the cast of the Casino?\" })\nconsole.log(response)\n```\n\n----------------------------------------\n\nTITLE: Llama CPP System Messages Example\nDESCRIPTION: Example showing how to use system messages with Llama CPP, which creates a new session\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/llama_cpp.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nSystemExample\n```\n\n----------------------------------------\n\nTITLE: Instantiating UnstructuredLoader with a file path\nDESCRIPTION: Code to instantiate an UnstructuredLoader object with a specific file path. This creates a loader that can process the specified Markdown file using Unstructured API.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/unstructured.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { UnstructuredLoader } from \"@langchain/community/document_loaders/fs/unstructured\"\n\nconst loader = new UnstructuredLoader(\"../../../../../../examples/src/document_loaders/example_data/notion.md\")\n```\n\n----------------------------------------\n\nTITLE: Eliminating Extra Spaces in PDF Text\nDESCRIPTION: Shows how to configure WebPDFLoader to eliminate excessive spaces in parsed PDF text by setting the parsedItemSeparator to an empty string instead of the default space character.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/pdf.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { WebPDFLoader } from \"@langchain/community/document_loaders/web/pdf\";\n\n// new Blob(); e.g. from a file input\nconst eliminatingExtraSpacesLoader = new WebPDFLoader(new Blob(), {\n  parsedItemSeparator: \"\",\n});\n```\n\n----------------------------------------\n\nTITLE: Using Stagehand Toolkit with Local Browser\nDESCRIPTION: This code demonstrates how to use the Stagehand Toolkit with a local browser. It initializes Stagehand, creates a toolkit with all available actions, and demonstrates navigation, action execution, and observation capabilities.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/stagehand.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { StagehandToolkit } from \"langchain/community/agents/toolkits/stagehand\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { Stagehand } from \"@browserbasehq/stagehand\";\n\n// Specify your Browserbase credentials.\nprocess.env.BROWSERBASE_API_KEY = \"\";\nprocess.env.BROWSERBASE_PROJECT_ID = \"\";\n\n// Specify OpenAI API key.\nprocess.env.OPENAI_API_KEY = \"\";\n\nconst stagehand = new Stagehand({\n  env: \"LOCAL\",\n  headless: false,\n  verbose: 2,\n  debugDom: true,\n  enableCaching: false,\n});\n\n// Create a Stagehand Toolkit with all the available actions from the Stagehand.\nconst stagehandToolkit = await StagehandToolkit.fromStagehand(stagehand);\n\nconst navigateTool = stagehandToolkit.tools.find(\n  (t) => t.name === \"stagehand_navigate\"\n);\nif (!navigateTool) {\n  throw new Error(\"Navigate tool not found\");\n}\nawait navigateTool.invoke(\"https://www.google.com\");\n\nconst actionTool = stagehandToolkit.tools.find(\n  (t) => t.name === \"stagehand_act\"\n);\nif (!actionTool) {\n  throw new Error(\"Action tool not found\");\n}\nawait actionTool.invoke('Search for \"OpenAI\"');\n\nconst observeTool = stagehandToolkit.tools.find(\n  (t) => t.name === \"stagehand_observe\"\n);\nif (!observeTool) {\n  throw new Error(\"Observe tool not found\");\n}\nconst result = await observeTool.invoke(\n  \"What actions can be performed on the current page?\"\n);\nconst observations = JSON.parse(result);\n\n// Handle observations as needed\nconsole.log(observations);\n\nconst currentUrl = stagehand.page.url();\nexpect(currentUrl).toContain(\"google.com/search?q=OpenAI\");\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Retriever Chain\nDESCRIPTION: Creates a custom chain that selects the appropriate retriever based on query analysis results\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_multiple_retrievers.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconst retrievers = {\n    HARRISON: retrieverHarrison,\n    ANKUSH: retrieverAnkush,\n}\n\nimport { RunnableConfig, RunnableLambda } from \"@langchain/core/runnables\";\n\nconst chain = async (question: string, config?: RunnableConfig) => {\n    const response = await queryAnalyzer.invoke(question, config);\n    const retriever = retrievers[response.person];\n    return retriever.invoke(response.query, config);\n}\n\nconst customChain = new RunnableLambda({ func: chain });\n```\n\n----------------------------------------\n\nTITLE: Installing Required npm Packages for LangChain and Convex\nDESCRIPTION: Command to install necessary npm packages for using LangChain with Convex and OpenAI.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/convex.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Using CohereRerank with Custom Client for Different Deployments\nDESCRIPTION: Demonstrates how to use CohereRerank with a custom CohereClient for Azure, AWS Bedrock, or standalone Cohere instances with specific endpoints.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_compressors/cohere_rerank.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Document } from \"@langchain/core/documents\";\nimport { CohereRerank } from \"@langchain/cohere\";\nimport { CohereClient } from \"cohere-ai\";\n\n// Initialize documents to be reranked\nconst docs = [\n  new Document({\n    pageContent: \"The color of the sky is blue.\",\n  }),\n  new Document({\n    pageContent: \"I live in San Francisco.\",\n  }),\n  new Document({\n    pageContent: \"The capital of the United States is Washington, D.C.\",\n  }),\n  new Document({\n    pageContent: \"The capital of France is Paris.\",\n  }),\n];\n\n// Initialize the Cohere client\nconst client = new CohereClient({\n  token: \"YOUR_API_KEY\",\n});\n\n// Initialize the reranker with your custom client\nconst reranker = new CohereRerank({\n  client,\n});\n\n// Rerank documents based on a query\nconst results = await reranker.rerank({\n  query: \"What is the capital of the US?\",\n  documents: docs.map((doc) => doc.pageContent),\n  topN: 3,\n});\n\nconsole.log(results);\n/*\n[ { index: 2, relevanceScore: 0.9870191812515259 },\n  { index: 3, relevanceScore: 0.09175287932157516 },\n  { index: 1, relevanceScore: 0.05698449909687042 } ]\n*/\n\n```\n\n----------------------------------------\n\nTITLE: Using RAG Chain for Document Query\nDESCRIPTION: Demonstrates how to use the constructed RAG chain to query documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/hnswlib.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nawait ragChain.invoke(\"Which movies are rated higher than 8.5?\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Vector Similarity Metrics in Astra DB\nDESCRIPTION: Shows how to configure vector similarity metrics when creating an Astra DB collection. Supports cosine (default), dot_product, and euclidean similarity search with specified vector dimensions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/astradb.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n  vector: {\n      dimension: number;\n      metric?: \"cosine\" | \"euclidean\" | \"dot_product\";\n  };\n```\n\n----------------------------------------\n\nTITLE: Basic Markdown Loading with UnstructuredLoader\nDESCRIPTION: Example of using UnstructuredLoader to load a Markdown file into LangChain Document objects. This shows the basic configuration with API credentials and loading the README.md file.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/document_loader_markdown.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { UnstructuredLoader } from \"@langchain/community/document_loaders/fs/unstructured\";\n\nconst markdownPath = \"../../../../README.md\";\n\nconst loader = new UnstructuredLoader(markdownPath, {\n  apiKey: process.env.UNSTRUCTURED_API_KEY,\n  apiUrl: process.env.UNSTRUCTURED_API_URL,\n});\n\nconst data = await loader.load()\nconsole.log(data.slice(0, 5));\n```\n\n----------------------------------------\n\nTITLE: Configuring SAP HANA Cloud Connection Environment Variables (Text)\nDESCRIPTION: This snippet displays example environment variables required to establish a connection to an SAP HANA Cloud instance. These variables typically include the host, port, user, and password, which are used by the SAP HANA client within LangchainJS applications to authenticate and connect to the database.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/hanavector.mdx#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nHANA_HOST=your-hana-instance.hanacloud.ondemand.com\nHANA_PORT=443\nHANA_USER=USER\nHANA_PASSWORD=PASSWORD\n\n```\n\n----------------------------------------\n\nTITLE: Loading JSON With JSON Pointer in TypeScript\nDESCRIPTION: Shows how to selectively extract text from specific keys in a JSON file using JSON pointers. The example targets only the \"from\" and \"surname\" fields in the JSON structure.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/document_loaders_json.mdx#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"1\": {\n    \"body\": \"BD 2023 SUMMER\",\n    \"from\": \"LinkedIn Job\",\n    \"labels\": [\"IMPORTANT\", \"CATEGORY_UPDATES\", \"INBOX\"]\n  },\n  \"2\": {\n    \"body\": \"Intern, Treasury and other roles are available\",\n    \"from\": \"LinkedIn Job2\",\n    \"labels\": [\"IMPORTANT\"],\n    \"other\": {\n      \"name\": \"plop\",\n      \"surname\": \"bob\"\n    }\n  }\n}\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSONLoader } from \"langchain/document_loaders/fs/json\";\n\nconst loader = new JSONLoader(\n  \"src/document_loaders/example_data/example.json\",\n  [\"/from\", \"/surname\"]\n);\n\nconst docs = await loader.load();\n/*\n[\n  Document {\n    pageContent: 'LinkedIn Job',\n    metadata: { source: './src/json/example.json', line: 1 }\n  },\n  Document {\n    pageContent: 'LinkedIn Job2',\n    metadata: { source: './src/json/example.json', line: 2 }\n  },\n  Document {\n    pageContent: 'bob',\n    metadata: { source: './src/json/example.json', line: 3 }\n  }\n]\n**/\n```\n\n----------------------------------------\n\nTITLE: Using Functions with Example Selectors in LangChainJS\nDESCRIPTION: Demonstrates combining function partials with example selectors to dynamically filter examples based on length constraints.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot.mdx#2025-04-22_snippet_18\n\nLANGUAGE: typescript\nCODE:\n```\nimport {\n  ChatPromptTemplate,\n  FewShotChatMessagePromptTemplate,\n} from \"langchain/prompts\";\n```\n\n----------------------------------------\n\nTITLE: Instantiating ChatCohere Model\nDESCRIPTION: Creates a new instance of the ChatCohere model with configuration parameters like model type and temperature.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/cohere.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatCohere } from \"@langchain/cohere\" \n\nconst llm = new ChatCohere({\n    model: \"command-r-plus\",\n    temperature: 0,\n    maxRetries: 2,\n    // other params...\n})\n```\n\n----------------------------------------\n\nTITLE: Loading Faiss Indexes from Python\nDESCRIPTION: Demonstrates cross-language compatibility by loading a Faiss vector store that was saved by the Python implementation of LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/faiss.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\n// The directory of data saved from Python\nconst directoryWithSavedPythonStore = \"your/directory/here\";\n\n// Load the vector store from the directory\nconst pythonLoadedStore = await FaissStore.loadFromPython(\n  directoryWithSavedPythonStore,\n  new OpenAIEmbeddings()\n);\n\n// Search for the most similar document\nawait pythonLoadedStore.similaritySearch(\"test\", 2);\n```\n\n----------------------------------------\n\nTITLE: Setting up environment variables for Qdrant\nDESCRIPTION: Sets the Qdrant URL as an environment variable for configuration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/qdrant.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.QDRANT_URL = \"YOUR_QDRANT_URL_HERE\" // for example, http://localhost:6333\n```\n\n----------------------------------------\n\nTITLE: Importing ThemedImage and useBaseUrl in React\nDESCRIPTION: This code imports ThemedImage component from the theme and useBaseUrl hook from Docusaurus. These are used to display a responsive themed diagram of the LangChain framework architecture.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/architecture.mdx#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport ThemedImage from \"@theme/ThemedImage\";\nimport useBaseUrl from \"@docusaurus/useBaseUrl\";\n```\n\n----------------------------------------\n\nTITLE: Output of Functions with Example Selectors in LangChainJS\nDESCRIPTION: Shows the formatted output when using functions with example selectors in few shot templates.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot.mdx#2025-04-22_snippet_20\n\nLANGUAGE: txt\nCODE:\n```\nboobaz\nAn example about foo\nAn example about bar\n```\n\n----------------------------------------\n\nTITLE: Initializing PostgresChatMessageHistory with PostgresEngine\nDESCRIPTION: Code to create a PostgresEngine instance, initialize a chat history table, and instantiate a PostgresChatMessageHistory object. This setup connects to Google Cloud SQL for PostgreSQL using connection details from environment variables.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/google_cloudsql_pg.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport {\n  PostgresChatMessageHistory,\n  PostgresEngine,\n  PostgresEngineArgs,\n} from \"@langchain/google-cloud-sql-pg\";\nimport * as dotenv from \"dotenv\";\n\ndotenv.config();\n\nconst peArgs: PostgresEngineArgs = {\n  user: process.env.DB_USER ?? \"\",\n  password: process.env.PASSWORD ?? \"\",\n};\n\n// PostgresEngine instantiation\nconst engine: PostgresEngine = await PostgresEngine.fromInstance(\n  process.env.PROJECT_ID ?? \"\",\n  process.env.REGION ?? \"\",\n  process.env.INSTANCE_NAME ?? \"\",\n  process.env.DB_NAME ?? \"\",\n  peArgs\n);\n\n// Chat history table initialization\nawait engine.initChatHistoryTable(\"my_chat_history_table\");\n\n// PostgresChatMessageHistory instantiation\nconst historyInstance: PostgresChatMessageHistory =\n  await PostgresChatMessageHistory.initialize(\n    engine,\n    \"test\",\n    \"my_chat_history_table\"\n  );\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatOpenAI Model\nDESCRIPTION: Sets up the ChatOpenAI model with GPT-4 mini and zero temperature for consistent outputs.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/classification.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatOpenAI } from '@langchain/openai';\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o-mini\",\n  temperature: 0,\n})\n```\n\n----------------------------------------\n\nTITLE: Tracking Token Usage for OpenAI LLM Call in TypeScript\nDESCRIPTION: Example of how to track token usage for a single OpenAI LLM call using a callback in LangChain.js. This snippet demonstrates creating an OpenAI model instance, setting up a callback for token tracking, and making an LLM call.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/llm_token_usage_tracking.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAI } from \"@langchain/openai\";\nimport { CallbackManager } from \"@langchain/core/callbacks/manager\";\n\nconst model = new OpenAI({\n  modelName: \"gpt-3.5-turbo\",\n  temperature: 0,\n  callbacks: CallbackManager.fromHandlers({\n    async handleLLMEnd(output) {\n      console.log(JSON.stringify(output, null, 2));\n    },\n  }),\n});\n\nconst res = await model.invoke(\"Hello world!\");\n\nconsole.log({ res });\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Headers to AzureChatOpenAI\nDESCRIPTION: Demonstrates how to specify custom headers for AzureChatOpenAI requests by using the configuration field to pass additional ClientOptions parameters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/azure.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport { AzureChatOpenAI } from \"@langchain/openai\";\n\nconst llmWithCustomHeaders = new AzureChatOpenAI({\n  azureOpenAIApiKey: process.env.AZURE_OPENAI_API_KEY, // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY\n  azureOpenAIApiInstanceName: process.env.AZURE_OPENAI_API_INSTANCE_NAME, // In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME\n  azureOpenAIApiDeploymentName: process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME, // In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME\n  azureOpenAIApiVersion: process.env.AZURE_OPENAI_API_VERSION, // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION\n  configuration: {\n    defaultHeaders: {\n      \"x-custom-header\": `SOME_VALUE`,\n    },\n  },\n});\n\nawait llmWithCustomHeaders.invoke(\"Hi there!\");\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key with Environment Variables\nDESCRIPTION: Shows how to set up the OPENAI_API_KEY environment variable for authentication with OpenAI services and optionally enable LangSmith tracing.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=\"your-api-key\"\n```\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Creating Chat Prompt Template\nDESCRIPTION: Demonstrates how to create a ChatPromptTemplate with system and human messages for use with Anthropic models\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/anthropic.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatPromptTemplate } from \"langchain/prompts\";\n\nconst prompt = ChatPromptTemplate.fromMessages([\n  [\"system\", \"You are a helpful chatbot\"],\n  [\"human\", \"Tell me a joke about {topic}\"],\n]);\n```\n\n----------------------------------------\n\nTITLE: Passing Base64 Encoded Image to Anthropic Model\nDESCRIPTION: This code shows how to create a HumanMessage with both text and a base64 encoded image, and then invoke the Anthropic model with this multimodal input.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/multimodal_inputs.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { HumanMessage } from \"@langchain/core/messages\";\n\nconst message = new HumanMessage({\n  content: [\n    {\n      type: \"text\",\n      text: \"what does this image contain?\"},\n    {\n      type: \"image_url\",\n      image_url: {\n        url: `data:image/jpeg;base64,${imageData.toString(\"base64\")}`},\n    },\n  ],\n})\nconst response = await model.invoke([message]);\nconsole.log(response.content);\n```\n\n----------------------------------------\n\nTITLE: Splitting Text with CharacterTextSplitter into Document Objects\nDESCRIPTION: Demonstrates loading a document, creating a CharacterTextSplitter with custom parameters, and processing the text into Document objects. The splitter is configured with a paragraph separator, a chunk size of 1000 characters, and 200 character overlap between chunks.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/character_text_splitter.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport { CharacterTextSplitter } from \"@langchain/textsplitters\";\nimport * as fs from \"node:fs\";\n\n// Load an example document\nconst rawData = await fs.readFileSync(\"../../../../examples/state_of_the_union.txt\");\nconst stateOfTheUnion = rawData.toString();\n\nconst textSplitter = new CharacterTextSplitter({\n    separator: \"\\n\\n\",\n    chunkSize: 1000,\n    chunkOverlap: 200,\n});\nconst texts = await textSplitter.createDocuments([stateOfTheUnion]);\nconsole.log(texts[0])\n```\n\n----------------------------------------\n\nTITLE: Initializing Verbose LangChain.js Tool in TypeScript\nDESCRIPTION: This snippet demonstrates how to create a LangChain.js Tool with verbosity enabled. It shows the syntax for setting the 'verbose' option to true when initializing the Tool object.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_42\n\nLANGUAGE: typescript\nCODE:\n```\nTool({ ..., verbose: true })\n```\n\n----------------------------------------\n\nTITLE: Disabling Parallel Tool Calls in LangChain\nDESCRIPTION: This code snippet shows how to disable parallel tool calls by setting the 'parallel_tool_calls' option to false when binding tools to the language model. It demonstrates that even when instructed to call a tool twice, only one call is made.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_calling_parallel.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nconst llmWithTools = llm.bindTools(tools, { parallel_tool_calls: false });\n\nconst result = await llmWithTools.invoke(\"Please call the first tool two times\");\n\nresult.tool_calls;\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain OpenAI Package\nDESCRIPTION: Command to install the @langchain/openai package and its core dependency using npm or yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-openai/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing AWS SageMaker Runtime Client for LangChain.js\nDESCRIPTION: Command to install the official SageMaker SDK as a peer dependency for LangChain.js integration with AWS SageMaker endpoints.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/aws_sagemaker.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @aws-sdk/client-sagemaker-runtime\n```\n\n----------------------------------------\n\nTITLE: Installing AWS SDK S3 Client with npm\nDESCRIPTION: This bash command installs the AWS SDK S3 client, which is required for using the S3Loader in LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/aws.mdx#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @aws-sdk/client-s3\n```\n\n----------------------------------------\n\nTITLE: Initializing WatsonxLLM with Bearer Token Authentication in TypeScript\nDESCRIPTION: Creates a new instance of WatsonxLLM using bearer token authentication method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_compressors/ibm.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { WatsonxLLM } from \"@langchain/community/llms/ibm\";\n\nconst props = {\n  version: \"YYYY-MM-DD\",\n  serviceUrl: \"<SERVICE_URL>\",\n  projectId: \"<PROJECT_ID>\",\n  watsonxAIAuthType: \"bearertoken\",\n  watsonxAIBearerToken: \"<YOUR-BEARERTOKEN>\",\n};\nconst instance = new WatsonxLLM(props);\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatAnthropic Model in JavaScript\nDESCRIPTION: Creates an instance of the ChatAnthropic model with the claude-3-5-sonnet version. This is required before streaming responses from the model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chat_streaming.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// @lc-docs-hide-cell\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst model = new ChatAnthropic({\n  model: \"claude-3-5-sonnet-20240620\",\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Sample Messages with Minimax in LangChain.js\nDESCRIPTION: Demonstrates how to use sample messages with Minimax models in LangChain.js to help the model better understand the desired return information, including content, format, and response mode.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/minimax.mdx#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport MinimaxSampleMessages from \"@examples/models/chat/minimax_sample_messages.ts\";\n```\n\n----------------------------------------\n\nTITLE: Fetching Prompts from LangSmith Hub\nDESCRIPTION: Implementation of prompt fetching from the LangSmith Hub using the pull function.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/summarization.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { pull } from \"langchain/hub\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\nconst mapPrompt = await pull<ChatPromptTemplate>(\"rlm/map-prompt\");\n```\n\n----------------------------------------\n\nTITLE: Initializing Model with Tool Configuration\nDESCRIPTION: Sets up a ChatOpenAI model with a dummy tool and demonstrates the basic tool calling setup. Shows how to create a model instance, define a tool, and initialize chat history.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/troubleshooting/errors/INVALID_TOOL_RESULTS.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport { z } from \"zod\";\nimport { tool } from \"@langchain/core/tools\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { BaseMessageLike } from \"@langchain/core/messages\";\n\nconst model = new ChatOpenAI({\n  model: \"gpt-4o-mini\",\n});\n\nconst dummyTool = tool(\n  async () => {\n    return \"action complete!\";\n  },\n  {\n    name: \"foo\",\n    schema: z.object({}),\n  }\n);\n\nconst modelWithTools = model.bindTools([dummyTool]);\n\nconst chatHistory: BaseMessageLike[] = [\n  {\n    role: \"user\",\n    content: `Call tool \"foo\" twice with no arguments`,\n  },\n];\n\nconst responseMessage = await modelWithTools.invoke(chatHistory);\n\nconsole.log(responseMessage);\n```\n\n----------------------------------------\n\nTITLE: Setting Example Query for Extraction\nDESCRIPTION: Defines a test query containing information about a person that will be used for demonstration of the extraction capabilities.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/extraction_parse.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst query = \"Anna is 23 years old and she is 6 feet tall\";\n```\n\n----------------------------------------\n\nTITLE: Example code to replace with RunnableWithMessageHistory\nDESCRIPTION: Shows the original code section that can be replaced with RunnableWithMessageHistory. This highlights the manual chat history management approach that could be simplified.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/versions/migrating_memory/chat_history.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst callModel = async (\n  state: typeof MessagesAnnotation.State,\n  config: RunnableConfig\n): Promise<Partial<typeof MessagesAnnotation.State>> => {\n  // highlight-start\n  if (!config.configurable?.sessionId) {\n    throw new Error(\n      \"Make sure that the config includes the following information: {'configurable': {'sessionId': 'some_value'}}\"\n    );\n  }\n\n  const chatHistory = getChatHistory(config.configurable.sessionId as string);\n\n  let messages = [...(await chatHistory.getMessages()), ...state.messages];\n\n  if (state.messages.length === 1) {\n    // First message, ensure it's in the chat history\n    await chatHistory.addMessage(state.messages[0]);\n  }\n\n  const aiMessage = await model.invoke(messages);\n\n  // Update the chat history\n  await chatHistory.addMessage(aiMessage);\n  // highlight-end\n  return { messages: [aiMessage] };\n};\n```\n\n----------------------------------------\n\nTITLE: Using ChatOllama with Tool Calling in Python\nDESCRIPTION: Python code demonstrating how to use ChatOllama with tool calling functionality, including defining a weather tool and binding it to the model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/ollama.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport { tool } from \"@langchain/core/tools\";\nimport { ChatOllama } from \"@langchain/ollama\";\nimport { z } from \"zod\";\n\nconst weatherTool = tool((_) => \"Da weather is weatherin\", {\n  name: \"get_current_weather\",\n  description: \"Get the current weather in a given location\",\n  schema: z.object({\n    location: z.string().describe(\"The city and state, e.g. San Francisco, CA\"),\n  }),\n});\n\n// Define the model\nconst llmForTool = new ChatOllama({\n  model: \"llama3-groq-tool-use\",\n});\n\n// Bind the tool to the model\nconst llmWithTools = llmForTool.bindTools([weatherTool]);\n\nconst resultFromTool = await llmWithTools.invoke(\n  \"What's the weather like today in San Francisco? Ensure you use the 'get_current_weather' tool.\"\n);\n\nconsole.log(resultFromTool);\n```\n\n----------------------------------------\n\nTITLE: Advanced PlanetScale Chat Memory Implementation\nDESCRIPTION: Advanced usage example showing how to directly pass a pre-configured PlanetScale database client instance to the chat memory implementation for more customized setups.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/planetscale.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nAdvancedExample\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Environment Variable\nDESCRIPTION: Sets the OpenAI API key as an environment variable. This is a prerequisite for using OpenAI embeddings or models within the LangchainJS examples involving Momento Vector Index.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/momento_vector_index.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=YOUR_OPENAI_API_KEY_HERE\n```\n\n----------------------------------------\n\nTITLE: Deleting Entries from Vector Store\nDESCRIPTION: Provides an example of how to delete documents from the vector store using their identifiers.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/pgvector.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconst id4 = ids[ids.length - 1];\n\nawait vectorStore.delete({ ids: [id4] });\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Cassandra KV Store in Node.js\nDESCRIPTION: Command to install the necessary npm packages for using Cassandra KV Store in a Node.js project with LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/stores/cassandra_storage.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core cassandra-driver\n```\n\n----------------------------------------\n\nTITLE: Initializing String Prompt Template in JavaScript\nDESCRIPTION: This snippet demonstrates how to create a PromptTemplate for a string prompt in JavaScript using LangChain. It shows the initialization of a template with placeholders for topic and language.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/prompts_composition.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PromptTemplate } from \"@langchain/core/prompts\";\n\nconst prompt = PromptTemplate.fromTemplate(`Tell me a joke about {topic}, make it funny and in {language}`)\n\nprompt\n```\n\n----------------------------------------\n\nTITLE: Sending Multimodal Messages with OpenAI Vision in LangChain.js\nDESCRIPTION: This snippet demonstrates how to create and send a multimodal message containing both text and an image URL to the OpenAI GPT-4 Vision model using LangChain.js.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/openai_vision_multimodal.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n// Deno.env.set(\"OPENAI_API_KEY\", \"\");\n\nimport { ChatOpenAI } from \"npm:langchain@0.0.185/chat_models/openai\";\nimport { HumanMessage } from \"npm:langchain@0.0.185/schema\";\n\nconst chat = new ChatOpenAI({\n  model: \"gpt-4-vision-preview\",\n  maxTokens: 1024,\n});\n\n// Messages can now take an array of content in addition to a string\nconst hostedImageMessage = new HumanMessage({\n  content: [\n    {\n      type: \"text\",\n      text: \"What does this image say?\",\n    },\n    {\n      type: \"image_url\",\n      image_url:\n        \"https://www.freecodecamp.org/news/content/images/2023/05/Screenshot-2023-05-29-at-5.40.38-PM.png\",\n    },\n  ],\n});\n\nawait chat.invoke([hostedImageMessage]);\n```\n\n----------------------------------------\n\nTITLE: Implementing Search Retriever with SerpAPI\nDESCRIPTION: Sets up SerpAPI as a search tool and wraps it in a retriever function.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/step_back.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport { SerpAPI } from \"npm:langchain@0.0.177/tools\";\n\nconst search = new SerpAPI(Deno.env.get(\"SERPAPI_API_KEY\"), {\n  num: \"4\", // Number of results\n});\n\nconst retriever = async (query: string) => search.call(query);\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search with FaissStore\nDESCRIPTION: Executes a similarity search against the vector store to find documents that match a query, specifying the number of results to return.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/faiss.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst similaritySearchResults = await vectorStore.similaritySearch(\"biology\", 2);\n\nfor (const doc of similaritySearchResults) {\n  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);\n}\n```\n\n----------------------------------------\n\nTITLE: Using ChatVertexAI for Image Analysis\nDESCRIPTION: TypeScript code showing how to use ChatVertexAI for image analysis with Gemini vision models on VertexAI.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/google.mdx#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst visionModel = new ChatVertexAI({\n  model: \"gemini-pro-vision\",\n  maxOutputTokens: 2048,\n});\nconst image = fs.readFileSync(\"./hotdog.png\").toString(\"base64\");\nconst input2 = [\n  new HumanMessage({\n    content: [\n      {\n        type: \"text\",\n        text: \"Describe the following image.\",\n      },\n      {\n        type: \"image_url\",\n        image_url: `data:image/png;base64,${image}`,\n      },\n    ],\n  }),\n];\n\nconst res = await visionModel.invoke(input2);\n```\n\n----------------------------------------\n\nTITLE: Loading all columns from a CSV file using CSVLoader in TypeScript\nDESCRIPTION: TypeScript code demonstrating how to use CSVLoader to load all columns from a CSV file. It shows the resulting Document objects with metadata and page content.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/document_loader_csv.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CSVLoader } from \"@langchain/community/document_loaders/fs/csv\";\n\nconst loader = new CSVLoader(\"src/document_loaders/example_data/example.csv\");\n\nconst docs = await loader.load();\n/*\n[\n  Document {\n    \"metadata\": {\n      \"line\": 1,\n      \"source\": \"src/document_loaders/example_data/example.csv\",\n    },\n    \"pageContent\": \"id: 1\ntext: This is a sentence.\",\n  },\n  Document {\n    \"metadata\": {\n      \"line\": 2,\n      \"source\": \"src/document_loaders/example_data/example.csv\",\n    },\n    \"pageContent\": \"id: 2\ntext: This is another sentence.\",\n  },\n]\n*/\n```\n\n----------------------------------------\n\nTITLE: Setting up OpenAI Embeddings and Memory Vector Store\nDESCRIPTION: Imports the necessary dependencies for creating embeddings and an in-memory vector store. This sets up the foundation for the RAG (Retrieval Augmented Generation) system.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/rag_fusion.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n// Deno.env.set(\"OPENAI_API_KEY\", \"\");\n\nimport { OpenAIEmbeddings } from \"npm:langchain@0.0.172/embeddings/openai\";\nimport { MemoryVectorStore } from \"npm:langchain@0.0.172/vectorstores/memory\";\n```\n\n----------------------------------------\n\nTITLE: Instantiating WatsonxEmbeddings Model\nDESCRIPTION: Creates an instance of WatsonxEmbeddings with necessary configuration parameters including version, service URL, project ID, space ID, and model ID.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/ibm.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { WatsonxEmbeddings } from \"@langchain/community/embeddings/ibm\";\n\nconst embeddings = new WatsonxEmbeddings({\n  version: \"YYYY-MM-DD\",\n  serviceUrl: process.env.API_URL,\n  projectId: \"<PROJECT_ID>\",\n  spaceId: \"<SPACE_ID>\",\n  model: \"<MODEL_ID>\",\n});\n```\n\n----------------------------------------\n\nTITLE: Output Parser Documentation Table in Markdown\nDESCRIPTION: A comprehensive markdown table documenting different output parsers in LangChain, including their features, input/output types, and descriptions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/output_parsers.mdx#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Name                                                                                                          | Supports Streaming | Has Format Instructions | Calls LLM | Input Type            | Output Type              | Description                                                                                                                                                                                                   |\n| ------------------------------------------------------------------------------------------------------------- | ------------------ | ----------------------- | --------- | --------------------- | ------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| [JSON](https://api.js.langchain.com/classes/_langchain_core.output_parsers.JsonOutputParser.html)             | ✅                 | ✅                      |           | `string` \\| `Message` | JSON object              | Returns a JSON object as specified. Probably the most reliable output parser for getting structured data that does NOT use function calling.                                                                  |\n| [XML](https://api.js.langchain.com/classes/_langchain_core.output_parsers.XMLOutputParser.html)               | ✅                 | ✅                      |           | `string` \\| `Message` | `object`                 | Returns a object of tags. Use when XML output is needed. Use with models that are good at writing XML (like Anthropic's).                                                                                     |\n| [CSV](https://api.js.langchain.com/classes/langchain.output_parsers.CommaSeparatedListOutputParser.html)      | ✅                 | ✅                      |           | `string` \\| `Message` | `Array<string>`          | Returns a list of comma separated values.                                                                                                                                                                     |\n| [OutputFixing](https://api.js.langchain.com/classes/langchain.output_parsers.OutputFixingParser.html)         |                    |                         | ✅        | `string` \\| `Message` |                          | Wraps another output parser. If that output parser errors, then this will pass the error message and the bad output to an LLM and ask it to fix the output.                                                   |\n| [Datetime](https://api.js.langchain.com/classes/langchain.output_parsers.DatetimeOutputParser.html)           |                    | ✅                      |           | `string` \\| `Message` | `Date`                   | Parses response into a datetime string.                                                                                                                                                                       |\n| [Structured](https://api.js.langchain.com/classes/_langchain_core.output_parsers.StructuredOutputParser.html) |                    | ✅                      |           | `string` \\| `Message` | `Record<string, string>` | An output parser that returns structured information. It is less powerful than other output parsers since it only allows for fields to be strings. This can be useful when you are working with smaller LLMs. |\n```\n\n----------------------------------------\n\nTITLE: Running TypeScript Examples\nDESCRIPTION: Command to run a TypeScript example file, with an example of running a few-shot prompting example.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/examples/src/README.md#2025-04-22_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nyarn run start <path to example>\n```\n\nLANGUAGE: sh\nCODE:\n```\nyarn run start ./src/prompts/few_shot.ts\n```\n\n----------------------------------------\n\nTITLE: Creating a Vector Store with OpenAI Embeddings\nDESCRIPTION: Embeds document chunks using OpenAIEmbeddings and stores them in a MemoryVectorStore for semantic retrieval capabilities.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_retrieval.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n\nconst vectorstore = await MemoryVectorStore.fromDocuments(\n  allSplits,\n  new OpenAIEmbeddings()\n);\n```\n\n----------------------------------------\n\nTITLE: Installing LangGraph Package\nDESCRIPTION: Installs the `@langchain/langgraph` package using npm or yarn, which is required for building agents with LangGraph.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/exa_search.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n@langchain/langgraph\n```\n\n----------------------------------------\n\nTITLE: Converting Vector Store to Retriever\nDESCRIPTION: Example of transforming the vector store into a retriever for use in chains. Shows how to configure the retriever with filters and set the number of results to return.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/vectorstores.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nconst retriever = vectorStore.asRetriever({\n  // Optional filter\n  filter: filter,\n  k: 2,\n});\nawait retriever.invoke(\"biology\");\n```\n\n----------------------------------------\n\nTITLE: Setting IBM watsonx.ai Software Authentication Environment Variables\nDESCRIPTION: Sets up environment variables for IBM watsonx.ai software authentication by specifying the authentication type, username, password, and URL.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/ibm.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport WATSONX_AI_AUTH_TYPE=cp4d\nexport WATSONX_AI_USERNAME=<YOUR_USERNAME>\nexport WATSONX_AI_PASSWORD=<YOUR_PASSWORD>\nexport WATSONX_AI_URL=<URL>\n```\n\n----------------------------------------\n\nTITLE: Initializing BedrockEmbeddings with AWS Credentials\nDESCRIPTION: Creates a BedrockEmbeddings instance using AWS credentials from environment variables.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/bedrock.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { BedrockEmbeddings } from \"@langchain/aws\";\n\nconst embeddings = new BedrockEmbeddings({\n  region: process.env.BEDROCK_AWS_REGION!,\n  credentials: {\n    accessKeyId: process.env.BEDROCK_AWS_ACCESS_KEY_ID!,\n    secretAccessKey: process.env.BEDROCK_AWS_SECRET_ACCESS_KEY!,\n  },\n  model: \"amazon.titan-embed-text-v1\",\n});\n```\n\n----------------------------------------\n\nTITLE: Enabling pgvector Extension in PostgreSQL\nDESCRIPTION: This SQL command enables the pgvector extension in a PostgreSQL database if it isn't already active. This is crucial for handling vector data types in the database.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/prisma.mdx#2025-04-22_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nCREATE EXTENSION IF NOT EXISTS vector;\n```\n\n----------------------------------------\n\nTITLE: Removing Hooks from Model\nDESCRIPTION: Examples of removing individual hooks or clearing all hooks from the model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/mistral.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmodel.removeHookFromHttpClient(beforeRequestHook);\n\nmodel.removeAllHooksFromHttpClient();\n```\n\n----------------------------------------\n\nTITLE: Instantiating AzionRetriever with OpenAI Embeddings in TypeScript\nDESCRIPTION: Creates instances of OpenAIEmbeddings, ChatOpenAI, and AzionRetriever with specific configurations for model, database, and search parameters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/azion-edgesql.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AzionRetriever } from \"@langchain/community/retrievers/azion_edgesql\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst embeddingModel = new OpenAIEmbeddings({\n  model: \"text-embedding-3-small\"\n})\n\nconst chatModel = new ChatOpenAI({\n  model: \"gpt-4o-mini\",\n  apiKey: process.env.OPENAI_API_KEY\n})\n\nconst retriever = new AzionRetriever(embeddingModel, \n  {dbName:\"langchain\",\n   vectorTable:\"documents\", // table where the vector embeddings are stored\n   ftsTable:\"documents_fts\", // table where the fts index is stored\n   searchType:\"hybrid\", // search type to use for the retriever\n   ftsK:2, // number of results to return from the fts index\n   similarityK:2, // number of results to return from the vector index\n   metadataItems:[\"language\",\"topic\"],\n   filters: [{ operator: \"=\", column: \"language\", value: \"en\" }],\n   entityExtractor:chatModel\n\n}) // number of results to return from the vector index\n```\n\n----------------------------------------\n\nTITLE: Using RunnableBranch for Routing in LangChain Expression Language\nDESCRIPTION: Demonstrates the use of RunnableBranch for conditional execution in LCEL. It shows how to set up condition-runnable pairs and a default runnable for routing.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/routing.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport BranchExample from \"@examples/guides/expression_language/how_to_routing_runnable_branch.ts\";\n\n<CodeBlock language=\"typescript\">{BranchExample}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Single Query Embedding with OpenAI in TypeScript\nDESCRIPTION: Shows how to embed a single query text using the embedQuery method from OpenAIEmbeddings class.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/embedding_models.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst queryEmbedding = await embeddingsModel.embedQuery(\n  \"What is the meaning of life?\"\n);\n```\n\n----------------------------------------\n\nTITLE: Retrieving Specific Fields from Vector Search Results\nDESCRIPTION: Example showing how to specify which fields to return in the search results. The fields are returned as part of the metadata object, and you can specify nested fields using dot notation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/couchbase.mdx#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await store.similaritySearch(query, 1, {\n  fields: [\"metadata.source\"],\n});\nconsole.log(result[0]);\n```\n\n----------------------------------------\n\nTITLE: Setting environment variables for Baidu Qianfan API\nDESCRIPTION: Exports the necessary environment variables to authenticate with the Baidu Qianfan API. The variables include access and secret keys that can be alternatively passed via constructor.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-baidu-qianfan/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport QIANFAN_AK=\"\"\nexport QIANFAN_SK=\"\"\nexport QIANFAN_ACCESS_KEY=\"\"\nexport QIANFAN_SECRET_KEY=\"\"\n```\n\n----------------------------------------\n\nTITLE: Deleting Documents from MongoDB Vector Store (TypeScript)\nDESCRIPTION: Shows how to delete specific documents from the MongoDB vector store using their assigned IDs. It utilizes the `delete` method of the `MongoDBAtlasVectorSearch` instance, passing an object with an array of `ids` to be removed.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/mongodb_atlas.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nawait vectorStore.delete({ ids: [\"4\"] });\n```\n\n----------------------------------------\n\nTITLE: Invoking a Cohere Completion Model\nDESCRIPTION: Shows how to generate completions using a Cohere LLM by providing an input text prompt.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/cohere.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconst inputText = \"Cohere is an AI company that \"\n\nconst completion = await llm.invoke(inputText)\ncompletion\n```\n\n----------------------------------------\n\nTITLE: Installing Azure Cosmos DB NoSQL Semantic Cache Dependencies\nDESCRIPTION: Command to install dependencies for the Azure Cosmos DB NoSQL Semantic Cache feature, which allows caching and retrieving LLM responses based on semantic similarity.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/microsoft.mdx#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/azure-cosmosdb @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Importing IndexTable Component in Markdown\nDESCRIPTION: Import statement for the IndexTable component used to display a table of all available LLM integrations.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/index.mdx#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport { IndexTable } from \"@theme/FeatureTables\";\n\n<IndexTable />\n```\n\n----------------------------------------\n\nTITLE: Creating a self-query retriever with default search params\nDESCRIPTION: Shows how to create a self-query retriever with default search parameters for additional filtering.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/qdrant.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst selfQueryRetrieverWithDefaultParams = SelfQueryRetriever.fromLLM({\n  llm: llm,\n  vectorStore: vectorStore,\n  documentContents: \"Brief summary of a movie\",\n  attributeInfo: attributeInfo,\n  structuredQueryTranslator: new QdrantTranslator(),\n  searchParams: {\n    filter: {\n      must: [\n        {\n          key: \"metadata.rating\",\n          range: {\n            gt: 8.5,\n          },\n        },\n      ],\n    },\n    mergeFiltersOperator: \"and\",\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI and Output Parser\nDESCRIPTION: Sets up the ChatOpenAI model and string output parser instances.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/step_back.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconst model = new ChatOpenAI({ temperature: 0 });\nconst stringOutputParser = new StringOutputParser();\n```\n\n----------------------------------------\n\nTITLE: Token Count Calculation\nDESCRIPTION: Example of using the getNumTokens method to calculate token count for input text.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/ibm.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nconst tokens = await instance.getNumTokens(\"Print hello world.\");\nconsole.log(tokens);\n```\n\n----------------------------------------\n\nTITLE: Streaming Text Generation with LangChain Expression Language\nDESCRIPTION: Example TypeScript code demonstrating the use of LangChain Expression Language (LCEL) to create a chain for streaming text generation with OpenAI. Shows how to combine prompts, models, and output parsers.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/langchain-core/README.md#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst prompt = ChatPromptTemplate.fromTemplate(\n  `Answer the following question to the best of your ability:\\n{question}`\n);\n\nconst model = new ChatOpenAI({\n  temperature: 0.8,\n});\n\nconst outputParser = new StringOutputParser();\n\nconst chain = prompt.pipe(model).pipe(outputParser);\n\nconst stream = await chain.stream({\n  question: \"Why is the sky blue?\",\n});\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n\n/*\nThe\n sky\n appears\n blue\n because\n of\n a\n phenomenon\n known\n as\n Ray\nleigh\n scattering\n*/\n```\n\n----------------------------------------\n\nTITLE: Output Format of Chat-based Few Shot Template in LangChainJS\nDESCRIPTION: Shows the formatted output of a chat-based few shot template, which returns a list of message objects.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot.mdx#2025-04-22_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\nconsole.log(\"Chat Few Shot: \", await chatFewShotPrompt.formatMessages({}));\n/**\nChat Few Shot:  [\n  HumanMessage {\n    lc_namespace: [ 'langchain', 'schema' ],\n    content: 'Human: Could the members of The Police perform lawful arrests?\\n' +\n      'AI: what can the members of The Police do?',\n    additional_kwargs: {}\n  },\n  HumanMessage {\n    lc_namespace: [ 'langchain', 'schema' ],\n    content: \"Human: Jan Sindel's was born in what country?\\n\" +\n      \"AI: what is Jan Sindel's personal history?\",\n    additional_kwargs: {}\n  }\n]\n */\n```\n\n----------------------------------------\n\nTITLE: Customizing Field Selection for Couchbase Documents in LangChain.js\nDESCRIPTION: Creating a CouchbaseDocumentLoader with specific fields defined for document content and metadata. This allows for filtering which fields from the database query results are included in the document content versus metadata.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/couchbase.mdx#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst loaderWithSelectedFields = new CouchbaseDocumentLoader(\n  couchbaseClient,\n  query,\n  // pageContentFields\n  [\n    \"address\",\n    \"name\",\n    \"city\",\n    \"phone\",\n    \"country\",\n    \"geo\",\n    \"description\",\n    \"reviews\",\n  ],\n  [\"id\"] // metadataFields\n);\n\nconst filtered_docs = await loaderWithSelectedFields.load();\nconsole.log(filtered_docs);\n```\n\n----------------------------------------\n\nTITLE: RecursiveUrlLoader Options Interface\nDESCRIPTION: TypeScript interface defining all available configuration options for RecursiveUrlLoader including exclusions, extraction, depth, and timeout settings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/recursive_url_loader.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\ninterface Options {\n  excludeDirs?: string[]; // webpage directories to exclude.\n  extractor?: (text: string) => string; // a function to extract the text of the document from the webpage, by default it returns the page as it is. It is recommended to use tools like html-to-text to extract the text. By default, it just returns the page as it is.\n  maxDepth?: number; // the maximum depth to crawl. By default, it is set to 2. If you need to crawl the whole website, set it to a number that is large enough would simply do the job.\n  timeout?: number; // the timeout for each request, in the unit of seconds. By default, it is set to 10000 (10 seconds).\n  preventOutside?: boolean; // whether to prevent crawling outside the root url. By default, it is set to true.\n  callerOptions?: AsyncCallerConstructorParams; // the options to call the AsyncCaller for example setting max concurrency (default is 64)\n}\n```\n\n----------------------------------------\n\nTITLE: Using the Retriever in Python\nDESCRIPTION: Demonstrates how to use the retriever by invoking it with a query string.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/retrievers.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst query = \"...\"\n\nawait retriever.invoke(query);\n```\n\n----------------------------------------\n\nTITLE: Using Vector Store as a Retriever with MMR Search\nDESCRIPTION: Shows how to convert a vector store into a retriever with Maximum Marginal Relevance (MMR) search configuration and perform batch queries.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/retrievers.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nconst retriever = vectorStore.asRetriever({\n  searchType: \"mmr\",\n  searchKwargs: {\n    fetchK: 1,\n  },\n});\n\n\nawait retriever.batch(\n    [\n        \"When was Nike incorporated?\",\n        \"What was Nike's revenue in 2023?\",\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatAnthropic Model in TypeScript\nDESCRIPTION: Initializes a Claude 3.5 Sonnet model instance for use in the chain examples. This is a hidden cell in the documentation providing the necessary LLM setup.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/cancel_execution.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// @lc-docs-hide-cell\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst llm = new ChatAnthropic({\n  model: \"claude-3-5-sonnet-20240620\",\n});\n```\n\n----------------------------------------\n\nTITLE: Instantiating AmazonKendraRetriever in TypeScript\nDESCRIPTION: This code snippet demonstrates how to create an instance of the AmazonKendraRetriever with configuration options including topK, indexId, region, and AWS credentials.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/kendra-retriever.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { AmazonKendraRetriever } from \"@langchain/aws\";\n\nconst retriever = new AmazonKendraRetriever({\n  topK: 10,\n  indexId: \"YOUR_INDEX_ID\",\n  region: \"us-east-2\", // Your region\n  clientOptions: {\n    credentials: {\n      accessKeyId: \"YOUR_ACCESS_KEY_ID\",\n      secretAccessKey: \"YOUR_SECRET_ACCESS_KEY\",\n    },\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Vector Search Index Definition for Couchbase\nDESCRIPTION: JSON configuration for creating a vector search index in Couchbase. This index defines the structure for storing and searching vector embeddings with a 1536-dimensional vector field and dot product similarity metric.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/couchbase.mdx#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"vector-index\",\n  \"type\": \"fulltext-index\",\n  \"params\": {\n    \"doc_config\": {\n      \"docid_prefix_delim\": \"\",\n      \"docid_regexp\": \"\",\n      \"mode\": \"type_field\",\n      \"type_field\": \"type\"\n    },\n    \"mapping\": {\n      \"default_analyzer\": \"standard\",\n      \"default_datetime_parser\": \"dateTimeOptional\",\n      \"default_field\": \"_all\",\n      \"default_mapping\": {\n        \"dynamic\": true,\n        \"enabled\": true,\n        \"properties\": {\n          \"metadata\": {\n            \"dynamic\": true,\n            \"enabled\": true\n          },\n          \"embedding\": {\n            \"enabled\": true,\n            \"dynamic\": false,\n            \"fields\": [\n              {\n                \"dims\": 1536,\n                \"index\": true,\n                \"name\": \"embedding\",\n                \"similarity\": \"dot_product\",\n                \"type\": \"vector\",\n                \"vector_index_optimized_for\": \"recall\"\n              }\n            ]\n          },\n          \"text\": {\n            \"enabled\": true,\n            \"dynamic\": false,\n            \"fields\": [\n              {\n                \"index\": true,\n                \"name\": \"text\",\n                \"store\": true,\n                \"type\": \"text\"\n              }\n            ]\n          }\n        }\n      },\n      \"default_type\": \"_default\",\n      \"docvalues_dynamic\": false,\n      \"index_dynamic\": true,\n      \"store_dynamic\": true,\n      \"type_field\": \"_type\"\n    },\n    \"store\": {\n      \"indexType\": \"scorch\",\n      \"segmentVersion\": 16\n    }\n  },\n  \"sourceType\": \"gocbcore\",\n  \"sourceName\": \"testing\",\n  \"sourceParams\": {},\n  \"planParams\": {\n    \"maxPartitionsPerPIndex\": 103,\n    \"indexPartitions\": 10,\n    \"numReplicas\": 0\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting up LangSmith tracing for Bedrock models\nDESCRIPTION: Optional commands to enable LangSmith tracing by setting the appropriate environment variables for monitoring and debugging Bedrock model calls.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/bedrock.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Bearer Token Authentication\nDESCRIPTION: Configuration of environment variables for IBM watsonx.ai bearer token authentication.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/ibm.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport WATSONX_AI_AUTH_TYPE=bearertoken\nexport WATSONX_AI_BEARER_TOKEN=<YOUR-BEARER-TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Google Search Retrieval Implementation\nDESCRIPTION: Setting up and using Google Search retrieval tool with ChatVertexAI for grounded content generation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/google_vertex_ai.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatVertexAI } from \"@langchain/google-vertexai\"\n\nconst searchRetrievalTool = {\n  googleSearchRetrieval: {\n    dynamicRetrievalConfig: {\n      mode: \"MODE_DYNAMIC\",\n      dynamicThreshold: 0.7,\n    },\n  },\n};\n\nconst searchRetrievalModel = new ChatVertexAI({\n  model: \"gemini-1.5-pro\",\n  temperature: 0,\n  maxRetries: 0,\n}).bindTools([searchRetrievalTool]);\n\nconst searchRetrievalResult = await searchRetrievalModel.invoke(\"Who won the 2024 NBA Finals?\");\n\nconsole.log(searchRetrievalResult.content);\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Chat Model\nDESCRIPTION: Creates an instance of the ChatOpenAI model with temperature set to 0 for deterministic outputs. This model will be used to generate alternative search queries based on the original query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/rag_fusion.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n/** Define the chat model */\nconst model = new ChatOpenAI({\n  temperature: 0,\n});\n```\n\n----------------------------------------\n\nTITLE: Default Dimensions Embedding\nDESCRIPTION: Creating embeddings with default dimensions using text-embedding-3-large model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/openai.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddingsDefaultDimensions = new OpenAIEmbeddings({\n  model: \"text-embedding-3-large\",\n});\n\nconst vectorsDefaultDimensions = await embeddingsDefaultDimensions.embedDocuments([\"some text\"]);\nconsole.log(vectorsDefaultDimensions[0].length);\n```\n\n----------------------------------------\n\nTITLE: Instantiating CloudflareWorkersAI Model in LangChain\nDESCRIPTION: Code to create an instance of the CloudflareWorkersAI model with the necessary configuration. This includes specifying the model to use and providing authentication credentials.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/cloudflare_workersai.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CloudflareWorkersAI } from \"@langchain/cloudflare\";\n\nconst llm = new CloudflareWorkersAI({\n  model: \"@cf/meta/llama-3.1-8b-instruct\", // Default value\n  cloudflareAccountId: CLOUDFLARE_ACCOUNT_ID,\n  cloudflareApiToken: CLOUDFLARE_API_TOKEN,\n  // Pass a custom base URL to use Cloudflare AI Gateway\n  // baseUrl: `https://gateway.ai.cloudflare.com/v1/{YOUR_ACCOUNT_ID}/{GATEWAY_NAME}/workers-ai/`,\n});\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for LangChain\nDESCRIPTION: This code block sets up environment variables for LangSmith API key and tracing, which are used for observability in LangChain applications.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_few_shot.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# Optional, use LangSmith for best-in-class observability\nLANGSMITH_API_KEY=your-api-key\nLANGSMITH_TRACING=true\n\n# Reduce tracing latency if you are not in a serverless environment\n# LANGCHAIN_CALLBACKS_BACKGROUND=true\n```\n\n----------------------------------------\n\nTITLE: Creating Custom CohereClient in Python\nDESCRIPTION: This snippet demonstrates how to create a custom CohereClient for use with Azure, AWS Bedrock, or standalone Cohere instances, and pass it to the CohereEmbeddings constructor.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/cohere.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { CohereEmbeddings } from \"@langchain/cohere\";\nimport { CohereClient } from \"cohere-ai\";\n\nconst client = new CohereClient({\n  token: \"<your-api-key>\",\n  environment: \"<your-cohere-deployment-url>\", //optional\n  // other params\n});\n\nconst embeddingsWithCustomClient = new CohereEmbeddings({\n  client,\n  // other params...\n});\n```\n\n----------------------------------------\n\nTITLE: Combined Type and ID Filtering in LangChain.js\nDESCRIPTION: Demonstrates combining multiple filter criteria to filter messages by both type and ID.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/filter_messages.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfilterMessages(messages, { includeTypes: [HumanMessage, AIMessage], excludeIds: [\"3\"] })\n```\n\n----------------------------------------\n\nTITLE: Initiating AgentExecutor Chain in JavaScript\nDESCRIPTION: This JSON snippet represents the start of an AgentExecutor chain, including the input question about the Oppenheimer film director and their age. It also contains details about a previous tool call to a search API.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_31\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"input\": \"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\",\n  \"steps\": [\n    {\n      \"action\": {\n        \"tool\": \"tavily_search_results_json\",\n        \"toolInput\": {\n          \"input\": \"Oppenheimer 2023 film director age\"\n        },\n        \"toolCallId\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n        \"log\": \"Invoking \\\"tavily_search_results_json\\\" with {\\\"input\\\":\\\"Oppenheimer 2023 film director age\\\"}\\n[{\\\"type\\\":\\\"tool_use\\\",\\\"id\\\":\\\"toolu_01NUVejujVo2y8WGVtZ49KAN\\\",\\\"name\\\":\\\"tavily_search_results_json\\\",\\\"input\\\":{\\\"input\\\":\\\"Oppenheimer 2023 film director age\\\"}}]\",\n        \"messageLog\": [\n          {\n            \"lc\": 1,\n            \"type\": \"constructor\",\n            \"id\": [\n              \"langchain_core\",\n              \"messages\",\n              \"AIMessageChunk\"\n            ],\n            \"kwargs\": {\n              \"content\": [\n                {\n                  \"type\": \"tool_use\",\n                  \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n                  \"name\": \"tavily_search_results_json\",\n                  \"input\": {\n                    \"input\": \"Oppenheimer 2023 film director age\"\n                  }\n                }\n              ],\n              \"additional_kwargs\": {\n                \"id\": \"msg_015MqAHr84dBCAqBgjou41Km\",\n                \"type\": \"message\",\n                \"role\": \"assistant\",\n                \"model\": \"claude-3-sonnet-20240229\",\n                \"stop_sequence\": null,\n                \"usage\": {\n                  \"input_tokens\": 409,\n                  \"output_tokens\": 68\n                },\n                \"stop_reason\": \"tool_use\"\n              },\n              \"tool_call_chunks\": [\n                {\n                  \"name\": \"tavily_search_results_json\",\n                  \"args\": \"{\\\"input\\\":\\\"Oppenheimer 2023 film director age\\\"}\",\n                  \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n                  \"index\": 0\n                }\n              ],\n              \"tool_calls\": [\n                {\n                  \"name\": \"tavily_search_results_json\",\n                  \"args\": {\n                    \"input\": \"Oppenheimer 2023 film director age\"\n                  },\n                  \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\"\n                }\n              ],\n              \"invalid_tool_calls\": [],\n              \"response_metadata\": {}\n            }\n          }\n        ]\n      },\n      \"observation\": \"[{\\\"title\\\":\\\"Oppenheimer (2023) - IMDb\\\",\\\"url\\\":\\\"https://www.imdb.com/title/tt15398776/\\\",\\\"content\\\":\\\"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.\\\",\\\"score\\\":0.96643,\\\"raw_content\\\":null},{\\\"title\\\":\\\"Christopher Nolan's Oppenheimer - Rotten Tomatoes\\\",\\\"url\\\":\\\"https://editorial.rottentomatoes.com/article/everything-we-know-about-christopher-nolans-oppenheimer/\\\",\\\"content\\\":\\\"Billboards and movie theater pop-ups across Los Angeles have been ticking down for months now: Christopher Nolan's epic account of J. Robert Oppenheimer, the father of the atomic bomb, is nearing an explosive release on July 21, 2023. Nolan movies are always incredibly secretive, twists locked alongside totems behind safe doors, actors not spilling an ounce of Earl Grey tea.\\\",\\\"score\\\":0.92804,\\\"raw_content\\\":null},{\\\"title\\\":\\\"Oppenheimer (film) - Wikipedia\\\",\\\"url\\\":\\\"https://en.wikipedia.org/wiki/Oppenheimer_(film)\\\",\\\"content\\\":\\\"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\\\\nCritical response\\\\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \\\\\\\"more objective view of his story from a different character's point of view\\\\\\\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\\\\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\\\\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\\\\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \\\\\\\"big-atures\\\\\\\", since the special effects team had tried to build the models as physically large as possible. He felt that \\\\\\\"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\\\\\\\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \\\\\\\"emotional\\\\\\\" and resembling that of a thriller, while also remarking that Nolan had \\\\\\\"Trojan-Horsed a biopic into a thriller\\\\\\\".[72]\\\\nCasting\\\\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\\\\\\\", while also underscoring that it is a \\\\\\\"huge shift in perception about the reality of Oppenheimer's perception\\\\\\\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \\\\\\\"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\\\\\\\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.\\\",\\\"score\\\":0.92404,\\\"raw_content\\\":null},{\\\"title\\\":\\\"'Oppenheimer' Director Christopher Nolan On Filmmaking at 53: \\\\\\\"I Try to ...\\\",\\\"url\\\":\\\"https://www.everythingzoomer.com/arts-entertainment/2023/11/21/oppenheimer-director-christopher-nolan-on-filmmaking-at-53-i-try-to-challenge-myself-with-every-film/\\\",\\\"content\\\":\\\"Oppenheimer will be available to own on 4K Ultra HD, Blu-ray and DVD — including more than three hours of bonus features — on November 21.\\\\nRELATED:\\\\nVisiting the Trinity Site Featured in 'Oppenheimer' Is a Sobering Reminder of the Horror of Nuclear Weapons\\\\nBarbenheimer: How 'Barbie' and 'Oppenheimer' Became the Unlikely Movie Marriage of the Summer\\\\nBlast From the Past: 'Asteroid City' & 'Oppenheimer' and the Age of Nuclear Anxiety\\\\nEXPLORE  HealthMoneyTravelFoodStyleBook ClubClassifieds#ZoomerDailyPolicy & PerspectiveArts & EntertainmentStars & RoyaltySex & Love\\\\nCONNECT  FacebookTwitterInstagram\\\\nSUBSCRIBE  Terms of Subscription ServiceE-NewslettersSubscribe to Zoomer Magazine\\\",\\\"score\\\":0.9141,\\\"raw_content\\\":null}]\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Splitting Python Code with RecursiveCharacterTextSplitter\nDESCRIPTION: Shows how to split Python code into chunks using RecursiveCharacterTextSplitter with Python-specific separators. Creates documents from the processed code with specified chunk size and overlap.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/code_splitter.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst PYTHON_CODE = `\ndef hello_world():\n    print(\"Hello, World!\")\n\n# Call the function\nhello_world()\n`\n\nconst pythonSplitter = RecursiveCharacterTextSplitter.fromLanguage(\n    \"python\", {\n        chunkSize: 50,\n        chunkOverlap: 0,\n    }\n)\nconst pythonDocs = await pythonSplitter.createDocuments([PYTHON_CODE])\npythonDocs\n```\n\n----------------------------------------\n\nTITLE: Enhanced Prompt Template with Language Parameter\nDESCRIPTION: Creates an enhanced prompt template that includes a language parameter for multilingual responses.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/chatbot.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nconst promptTemplate2 = ChatPromptTemplate.fromMessages([\n  [\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\"],\n  [\"placeholder\", \"{messages}\"],\n]);\n```\n\n----------------------------------------\n\nTITLE: Embedding Single Text with VertexAIEmbeddings\nDESCRIPTION: Example of generating embeddings for a single text query\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/google_vertex_ai.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconst singleVector = await embeddings.embedQuery(text);\n\nconsole.log(singleVector.slice(0, 100));\n```\n\n----------------------------------------\n\nTITLE: Accessing RunnableConfig in a Tool Function in TypeScript\nDESCRIPTION: Demonstrates how to access runtime configuration values within a tool function. This approach allows for injecting context or configuration at runtime without exposing these values in the tool's schema.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/tools.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RunnableConfig } from \"@langchain/core/runnables\";\n\nconst someTool = tool(\n    async (args: any, config: RunnableConfig): Promise<[string, any]> => {\n        /**\n         * Tool that does something.\n         */\n    },\n    {\n        name: \"some_tool\",\n        description: \"Tool that does something\",\n        schema: z.object({ ... }),\n        returnType: \"content_and_artifact\"\n    }\n);\n\n\nawait someTool.invoke(..., { configurable: { value: \"some_value\" } });\n```\n\n----------------------------------------\n\nTITLE: Setting up LangSmith environment variables for tracing\nDESCRIPTION: Environment variables configuration for enabling LangSmith tracing of model calls. These can be exported to enable automated tracing functionality when working with LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/web_cheerio.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Implementing LangGraph React Agent\nDESCRIPTION: Creates and uses LangGraph's React agent executor with message-based state management.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/migrate_agent.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n\nconst app = createReactAgent({\n  llm,\n  tools,\n});\n\nlet agentOutput = await app.invoke({\n  messages: [\n    {\n      role: \"user\",\n      content: query\n    },\n  ],\n});\n\nconsole.log(agentOutput);\n```\n\n----------------------------------------\n\nTITLE: Configuring Cloudflare D1 in wrangler.toml\nDESCRIPTION: This snippet demonstrates the configuration of a Cloudflare D1 database in the wrangler.toml file, including project settings, environment variables, and D1 database binding.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/cloudflare_d1.mdx#2025-04-22_snippet_1\n\nLANGUAGE: toml\nCODE:\n```\nname = \"YOUR_PROJECT_NAME\"\nmain = \"src/index.ts\"\ncompatibility_date = \"2024-01-10\"\n\n[vars]\nANTHROPIC_API_KEY = \"YOUR_ANTHROPIC_KEY\"\n\n[[d1_databases]]\nbinding = \"DB\"                                       # available in your Worker as env.DB\ndatabase_name = \"YOUR_D1_DB_NAME\"\ndatabase_id = \"YOUR_D1_DB_ID\"\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Tavily in TypeScript\nDESCRIPTION: This snippet sets up environment variables for Tavily API and LangSmith observability. The TAVILY_API_KEY is mandatory to utilize the Tavily Extract tool, while LANGSMITH_TRACING and LANGSMITH_API_KEY enhance system observability.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/tavily_extract.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.TAVILY_API_KEY = \"YOUR_API_KEY\"\n```\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.LANGSMITH_TRACING=\"true\"\nprocess.env.LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Azure OpenAI Embeddings Usage\nDESCRIPTION: Example showing how to use Azure OpenAI for generating embeddings\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-azure-openai/README.md#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AzureOpenAIEmbeddings } from \"@langchain/azure-openai\";\n\nconst embeddings = new AzureOpenAIEmbeddings({\n  // Note that the following are optional, and will default to the values below\n  // if not provided.\n  azureOpenAIEndpoint: process.env.AZURE_OPENAI_API_ENDPOINT,\n  azureOpenAIApiKey: process.env.AZURE_OPENAI_API_KEY,\n  azureOpenAIApiDeploymentName: process.env.AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME,\n});\nconst res = await embeddings.embedQuery(\"Hello world\");\n```\n\n----------------------------------------\n\nTITLE: Binding Tools to Chat Models in LangChain\nDESCRIPTION: Demonstrates how to create tools and bind them to a chat model using LangChain's standardized interface for tool calling.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/why_langchain.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// Tool creation\nconst tools = [myTool];\n// Tool binding\nconst modelWithTools = model.bindTools(tools);\n```\n\n----------------------------------------\n\nTITLE: Loading Hacker News Data using HNLoader in TypeScript\nDESCRIPTION: This code demonstrates how to use the HNLoader class from LangChain.js to load data from a specific Hacker News post. It creates a loader instance with a URL and then loads the documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/hn.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HNLoader } from \"@langchain/community/document_loaders/web/hn\";\n\nconst loader = new HNLoader(\"https://news.ycombinator.com/item?id=34817881\");\n\nconst docs = await loader.load();\n```\n\n----------------------------------------\n\nTITLE: Logging Model Response Content\nDESCRIPTION: Outputs the content property of the model's response to the console.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/cloudflare_workersai.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconsole.log(aiMsg.content)\n```\n\n----------------------------------------\n\nTITLE: Initializing ByteDanceDoubao Embeddings\nDESCRIPTION: Creating a new instance of ByteDanceDoubaoEmbeddings with a specified entrypoint model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/bytedance_doubao.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { ByteDanceDoubaoEmbeddings } from \"@langchain/community/embeddings/bytedance_doubao\";\n\nconst embeddings = new ByteDanceDoubaoEmbeddings({\n  model: 'ep-xxx-xxx' // your entrypoint's name\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Couchbase Vector Store Arguments\nDESCRIPTION: TypeScript code for configuring the CouchbaseVectorStoreArgs object, which specifies the bucket, scope, collection, and index names for the vector store, along with text and embedding field mappings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/couchbase.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst couchbaseConfig: CouchbaseVectorStoreArgs = {\n  cluster: couchbaseClient,\n  bucketName: \"testing\",\n  scopeName: \"_default\",\n  collectionName: \"_default\",\n  indexName: \"vector-index\",\n  textKey: \"text\",\n  embeddingKey: \"embedding\",\n};\n```\n\n----------------------------------------\n\nTITLE: Indexing and Retrieving with MemoryVectorStore\nDESCRIPTION: Demonstration of using embeddings with MemoryVectorStore for document indexing and retrieval.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/google_generativeai.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n// Create a vector store with a sample text\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n\nconst text = \"LangChain is the framework for building context-aware reasoning applications\";\n\nconst vectorstore = await MemoryVectorStore.fromDocuments(\n  [{ pageContent: text, metadata: {} }],\n  embeddings,\n);\n\n// Use the vector store as a retriever that returns a single document\nconst retriever = vectorstore.asRetriever(1);\n\n// Retrieve the most similar text\nconst retrievedDocuments = await retriever.invoke(\"What is LangChain?\");\n\nretrievedDocuments[0].pageContent;\n```\n\n----------------------------------------\n\nTITLE: Loading Documents with FireCrawlLoader\nDESCRIPTION: Examples showing how to load documents and access the loaded content and metadata.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/firecrawl.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst docs = await loader.load()\ndocs[0]\n```\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(docs[0].metadata)\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatOpenAI Model\nDESCRIPTION: Creates an instance of ChatOpenAI model with specific configuration settings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/memory.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o\",\n  temperature: 0,\n});\n```\n\n----------------------------------------\n\nTITLE: Setting up LangSmith Tracing\nDESCRIPTION: Optional environment variables for enabling LangSmith tracing and setting the API key.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/fireworks.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Installing Core Dependency with npm/yarn in LangChain v0.3\nDESCRIPTION: Command to explicitly install @langchain/core as a peer dependency, which is now required in LangChain v0.3 rather than being automatically included via other packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/versions/v0_3/index.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Environment Variable (TypeScript)\nDESCRIPTION: Assigns the OpenAI API key to an environment variable in a Node.js application using TypeScript. This key is required when using OpenAI's embedding models via the `@langchain/openai` package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/mongodb_atlas.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.OPENAI_API_KEY = \"YOUR_API_KEY\";\n```\n\n----------------------------------------\n\nTITLE: Applying Metadata Filtering in Neo4j Vector Store\nDESCRIPTION: Illustrates the application of metadata filtering with `Neo4jVectorStore` in TypeScript. Requires understanding of Neo4j's querying capabilities and basic TypeScript.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/neo4jvector.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\n{MetadataExample}\n```\n\n----------------------------------------\n\nTITLE: Creating SAP HANA HNSW Vector Index using LangchainJS (TypeScript)\nDESCRIPTION: This TypeScript example demonstrates how to create a Hierarchical Navigable Small World (HNSW) index on an existing SAP HANA Cloud vector store table using the `createHnswIndex` method provided by the `HanaDB` class. Creating an HNSW index can significantly improve the performance of nearest neighbor searches.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/hanavector.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { HanaDB } from \"@langchain/community/vectorstores/hana\";\n\n// Connection options for the database connection, read from environment variables\n// HANA_HOST, HANA_PORT, HANA_USER, HANA_PASSWORD\nconst connectionOptions = {\n  host: process.env.HANA_HOST ?? \"\",\n  port: process.env.HANA_PORT ?? \"\",\n  user: process.env.HANA_USER ?? \"\",\n  password: process.env.HANA_PASSWORD ?? \"\",\n};\n\n// Configuration for the vector store table\nconst vectorStoreTableConfig = {\n  tableName: \"MY_TABLE_NAME\",\n  schemaName: \"MY_SCHEMA_NAME\", // optional, default: current schema\n};\n\n// Input texts\nconst texts = [\n  \"SAP HANA Cloud is a database as a service (DBaaS) offering.\",\n  \"SAP HANA Cloud Vector Engine is a component of SAP HANA Cloud.\",\n  \"Langchain JS is a framework for developing applications powered by language models.\",\n];\n\n// Create embeddings model\nconst embeddings = new OpenAIEmbeddings({});\n\n// Create the vector store\nconst vectorStore = await HanaDB.fromTexts(\n  texts,\n  [], // metadata can be added here\n  embeddings,\n  connectionOptions,\n  vectorStoreTableConfig\n);\n\n// Create HNSW index\nconst indexName = \"MY_INDEX_NAME\";\nawait vectorStore.createHnswIndex(indexName);\n\n```\n\n----------------------------------------\n\nTITLE: Setting Up Environment Variables for Upstash Redis\nDESCRIPTION: Configure the necessary environment variables for connecting to Upstash Redis, which is required for the rate limiting functionality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/callbacks/upstash_ratelimit_callback.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nUPSTASH_REDIS_REST_URL=\"****\"\nUPSTASH_REDIS_REST_TOKEN=\"****\"\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith API Key for Tracing\nDESCRIPTION: Optional configuration for enabling automated tracing of TogetherAI model calls using LangSmith. This configuration sets the necessary environment variables for LangSmith integration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/together.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Importing Azure Blob Storage Container Loader\nDESCRIPTION: Code snippet for importing the Azure Blob Storage Container Loader, which loads all documents from a specified Azure Blob Storage container.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/microsoft.mdx#2025-04-22_snippet_14\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AzureBlobStorageContainerLoader } from \"@langchain/community/document_loaders/web/azure_blob_storage_container\";\n```\n\n----------------------------------------\n\nTITLE: Configuring Package Dependencies Resolution\nDESCRIPTION: Package.json configuration to ensure consistent @langchain/core dependency version across different package managers (yarn, npm, pnpm). This helps prevent version conflicts in LangChain packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-community/README.md#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"your-project\",\n  \"version\": \"0.0.0\",\n  \"dependencies\": {\n    \"@langchain/community\": \"^0.0.0\",\n    \"@langchain/core\": \"^0.3.0\"\n  },\n  \"resolutions\": {\n    \"@langchain/core\": \"^0.3.0\"\n  },\n  \"overrides\": {\n    \"@langchain/core\": \"^0.3.0\"\n  },\n  \"pnpm\": {\n    \"overrides\": {\n      \"@langchain/core\": \"^0.3.0\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Listing Supported SQL Dialects in LangChain\nDESCRIPTION: TypeScript code demonstrating how to list the SQL dialects supported by LangChain's built-in SQL query generation tools.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/sql_prompting.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n<CodeBlock language=\"typescript\">{DialectExample}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search with HNSWLib\nDESCRIPTION: Executes a similarity search on the vector store with an optional filter. This example searches for documents related to 'biology' and limits results to 2 documents from a specific source.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/hnswlib.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconst filter = (doc) => doc.metadata.source === \"https://example.com\";\n\nconst similaritySearchResults = await vectorStore.similaritySearch(\"biology\", 2, filter);\n\nfor (const doc of similaritySearchResults) {\n  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Managed Identity for AzureOpenAI in LangChain.js\nDESCRIPTION: Demonstrates how to set up Azure Managed Identity credentials for use with AzureOpenAI in LangChain.js.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/azure.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport {\n  DefaultAzureCredential,\n  getBearerTokenProvider,\n} from \"@azure/identity\";\nimport { AzureOpenAI } from \"@langchain/openai\";\n\nconst credentials = new DefaultAzureCredential();\nconst azureADTokenProvider = getBearerTokenProvider(\n  credentials,\n  \"https://cognitiveservices.azure.com/.default\"\n);\n\nconst managedIdentityLLM = new AzureOpenAI({\n  azureADTokenProvider,\n  azureOpenAIApiInstanceName: \"<your_instance_name>\",\n  azureOpenAIApiDeploymentName: \"<your_deployment_name>\",\n  azureOpenAIApiVersion: \"<api_version>\",\n});\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith API Key for Ollama in Bash\nDESCRIPTION: Environment variables configuration for enabling LangSmith tracing with Ollama models. These variables enable automated tracing of model calls when using Ollama with LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/ollama.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Using String PromptTemplate in TypeScript\nDESCRIPTION: Demonstrates how to create and invoke a simple string-based prompt template that takes a single variable input.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/prompt_templates.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PromptTemplate } from \"@langchain/core/prompts\";\n\nconst promptTemplate = PromptTemplate.fromTemplate(\n  \"Tell me a joke about {topic}\"\n);\n\nawait promptTemplate.invoke({ topic: \"cats\" });\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key for Embeddings - TypeScript\nDESCRIPTION: Sets the OPENAI_API_KEY environment variable in Node.js for authentication with the OpenAI embeddings service. This is mandatory for the OpenAIEmbeddings integration in LangChain. The API key must be securely stored and replaced with a valid user credential before running the code.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/mariadb.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.OPENAI_API_KEY = \"YOUR_API_KEY\";\n```\n\n----------------------------------------\n\nTITLE: LangChain Agent Chain Response Structure\nDESCRIPTION: Sample of the structured response from a LangChain agent chain execution, showing the agent_scratchpad output containing message constructors and tool calls.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_25\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"agent_scratchpad\": [\n    {\n      \"lc\": 1,\n      \"type\": \"constructor\",\n      \"id\": [\n        \"langchain_core\",\n        \"messages\",\n        \"AIMessageChunk\"\n      ],\n      \"kwargs\": {\n        \"content\": [\n          {\n            \"type\": \"tool_use\",\n            \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n            \"name\": \"tavily_search_results_json\",\n            \"input\": {\n              \"input\": \"Oppenheimer 2023 film director age\"\n            }\n          }\n        ],\n        \"additional_kwargs\": {\n          \"id\": \"msg_015MqAHr84dBCAqBgjou41Km\",\n          \"type\": \"message\",\n          \"role\": \"assistant\",\n          \"model\": \"claude-3-sonnet-20240229\",\n          \"stop_sequence\": null,\n          \"usage\": {\n            \"input_tokens\": 409,\n            \"output_tokens\": 68\n          },\n          \"stop_reason\": \"tool_use\"\n        },\n        \"tool_call_chunks\": [\n          {\n            \"name\": \"tavily_search_results_json\",\n            \"args\": \"{\\\"input\\\":\\\"Oppenheimer 2023 film director age\\\"}\",\n            \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n            \"index\": 0\n          }\n        ],\n        \"tool_calls\": [\n          {\n            \"name\": \"tavily_search_results_json\",\n            \"args\": {\n              \"input\": \"Oppenheimer 2023 film director age\"\n            },\n            \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\"\n          }\n        ],\n        \"invalid_tool_calls\": [],\n        \"response_metadata\": {}\n      }\n    },\n    {\n      \"lc\": 1,\n      \"type\": \"constructor\",\n      \"id\": [\n        \"langchain_core\",\n        \"messages\",\n        \"ToolMessage\"\n      ],\n      \"kwargs\": {\n        \"tool_call_id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\"\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Tool with Context Variables in TypeScript\nDESCRIPTION: Defines a tool that updates user pet preferences using context variables to access user ID at runtime. This allows secure parameter handling where the LLM doesn't control the user identification.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_runtime.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { z } from \"zod\";\nimport { tool } from \"@langchain/core/tools\";\nimport { getContextVariable } from \"@langchain/core/context\";\n\nlet userToPets: Record<string, string[]> = {};\n\nconst updateFavoritePets = tool(async (input) => {\n  const userId = getContextVariable(\"userId\");\n  if (userId === undefined) {\n    throw new Error(`No \"userId\" found in current context. Remember to call \"setContextVariable('userId', value)\"`);\n  }\n  userToPets[userId] = input.pets;\n  return \"update_favorite_pets called.\"\n}, {\n  name: \"update_favorite_pets\",\n  description: \"add to the list of favorite pets.\",\n  schema: z.object({\n    pets: z.array(z.string())\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing SQLite Database Connection in LangChain.js\nDESCRIPTION: This TypeScript code demonstrates how to set up a connection to a SQLite database using the SqlDatabase class from LangChain.js. It uses TypeORM to establish the connection and retrieve table names.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/sql_query_checking.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport CodeBlock from \"@theme/CodeBlock\";\nimport DbCheck from \"@examples/use_cases/sql/db_check.ts\";\n\n<CodeBlock language=\"typescript\">{DbCheck}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Defining Search Schema using Zod in JavaScript\nDESCRIPTION: This code defines a schema for the search query using Zod library.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_no_queries.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { z } from \"zod\";\n\nconst searchSchema = z.object({\n    query: z.string().describe(\"Similarity search query applied to job record.\"),\n});\n```\n\n----------------------------------------\n\nTITLE: Invoking the RAG Chain\nDESCRIPTION: This snippet shows how to invoke the RAG chain with a specific question.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/chroma.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nawait ragChain.invoke(\"Which movies are rated higher than 8.5?\")\n```\n\n----------------------------------------\n\nTITLE: Invoking Parser Chain\nDESCRIPTION: Demonstrates how to invoke the parser chain with a question and format instructions to get structured output.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/output_parser_structured.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nconst response = await chain.invoke({\n  question: \"What is the capital of France?\",\n  format_instructions: parser.getFormatInstructions(),\n});\n\nconsole.log(response);\n```\n\n----------------------------------------\n\nTITLE: Loading HTML with Unstructured in LangChain\nDESCRIPTION: This code snippet demonstrates how to use the UnstructuredLoader to load an HTML file into LangChain Document objects. It requires the Unstructured API key and URL to be set in the environment.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/document_loader_html.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { UnstructuredLoader } from \"@langchain/community/document_loaders/fs/unstructured\";\n\nconst filePath = \"../../../../libs/langchain-community/src/tools/fixtures/wordoftheday.html\"\n\nconst loader = new UnstructuredLoader(filePath, {\n  apiKey: process.env.UNSTRUCTURED_API_KEY,\n  apiUrl: process.env.UNSTRUCTURED_API_URL,\n});\n\nconst data = await loader.load()\nconsole.log(data.slice(0, 5));\n```\n\n----------------------------------------\n\nTITLE: Example of Invalid JSON Output in TypeScript\nDESCRIPTION: Demonstrates an AI message output with malformed JSON that would cause a parsing error when the output parser attempts to process it.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nAIMessage {\n  content: \"```\\n{\\\"foo\\\":\\n```\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Langchain Tool Module\nDESCRIPTION: Command to install the required package for the Langchain tool module.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/tools.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n__package_name__ @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Visualizing LangChainJS Repository Structure as a Tree\nDESCRIPTION: A text-based tree visualization showing how the LangChainJS repository is organized, including the location of documentation, main package, tests, and community packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/contributing/repo_structure.mdx#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n.\n├── docs\n│   ├── core_docs # Contains content for the documentation here: https://js.langchain.com/\n│   ├── api_refs # Contains content for the API refs here: https://api.js.langchain.com/\n├── langchain # Main package\n│   ├── src/**/tests/*.test.ts/ # Unit tests (present in each package not shown for brevity)\n│   ├── src/**/tests/*.int.test.ts/ # Integration tests (present in each package not shown for brevity)\n├── langchain # Base interfaces for key abstractions\n├── libs # Community packages\n│   ├── langchain-community # Third-party integrations\n│   ├── langchain-partner-1\n│   ├── langchain-partner-2\n│   ├── ...\n```\n\n----------------------------------------\n\nTITLE: Instantiating Fireworks LLM in LangChain\nDESCRIPTION: Code example showing how to instantiate a Fireworks model with configuration options like model selection, temperature, and retry settings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/fireworks.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { Fireworks } from \"@langchain/community/llms/fireworks\"\n\nconst llm = new Fireworks({\n  model: \"accounts/fireworks/models/llama-v3-70b-instruct\",\n  temperature: 0,\n  maxTokens: undefined,\n  timeout: undefined,\n  maxRetries: 2,\n  // other params...\n})\n```\n\n----------------------------------------\n\nTITLE: Implementing Citation Chain\nDESCRIPTION: Creating a complete chain that includes document ID-based citations in the output\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_citations.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport { Document } from \"@langchain/core/documents\";\n\nconst formatDocsWithId = (docs: Array<Document>): string => {\n  return \"\\n\\n\" + docs.map((doc: Document, idx: number) => `Source ID: ${idx}\\nArticle title: ${doc.metadata.title}\\nArticle Snippet: ${doc.pageContent}`).join(\"\\n\\n\");\n}\n// subchain for generating an answer once we've done retrieval\nconst answerChain1 = prompt.pipe(llmWithTool1);\nconst map1 = RunnableMap.from({\n  question: new RunnablePassthrough(),\n  docs: retriever,\n})\n// complete chain that calls the retriever -> formats docs to string -> runs answer subchain -> returns just the answer and retrieved docs.\nconst chain1 = map1\n  .assign({ context: (input: { docs: Array<Document> }) => formatDocsWithId(input.docs) })\n  .assign({ cited_answer: answerChain1 })\n  .pick([\"cited_answer\", \"docs\"])\n  \nawait chain1.invoke(\"How fast are cheetahs?\")\n```\n\n----------------------------------------\n\nTITLE: Setting up Supabase Database for Hybrid Search\nDESCRIPTION: SQL commands to create necessary tables, extensions, and functions in the Supabase database for hybrid search functionality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/supabase-hybrid.mdx#2025-04-22_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\n-- Enable the pgvector extension to work with embedding vectors\ncreate extension vector;\n\n-- Create a table to store your documents\ncreate table documents (\n  id bigserial primary key,\n  content text, -- corresponds to Document.pageContent\n  metadata jsonb, -- corresponds to Document.metadata\n  embedding vector(1536) -- 1536 works for OpenAI embeddings, change if needed\n);\n\n-- Create a function to similarity search for documents\ncreate function match_documents (\n  query_embedding vector(1536),\n  match_count int DEFAULT null,\n  filter jsonb DEFAULT '{}'\n) returns table (\n  id bigint,\n  content text,\n  metadata jsonb,\n  similarity float\n)\nlanguage plpgsql\nas $$\n#variable_conflict use_column\nbegin\n  return query\n  select\n    id,\n    content,\n    metadata,\n    1 - (documents.embedding <=> query_embedding) as similarity\n  from documents\n  where metadata @> filter\n  order by documents.embedding <=> query_embedding\n  limit match_count;\nend;\n$$;\n\n-- Create a function to keyword search for documents\ncreate function kw_match_documents(query_text text, match_count int)\nreturns table (id bigint, content text, metadata jsonb, similarity real)\nas $$\n\nbegin\nreturn query execute\nformat('select id, content, metadata, ts_rank(to_tsvector(content), plainto_tsquery($1)) as similarity\nfrom documents\nwhere to_tsvector(content) @@ plainto_tsquery($1)\norder by similarity desc\nlimit $2')\nusing query_text, match_count;\nend;\n$$ language plpgsql;\n```\n\n----------------------------------------\n\nTITLE: Using Neo4j Vector Index in TypeScript\nDESCRIPTION: This snippet includes an example of how to utilize the `Neo4jVectorStore` class in a TypeScript environment for a complete setup. Requires TypeScript and the previously installed npm packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/neo4jvector.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n{Example}\n```\n\n----------------------------------------\n\nTITLE: Alternative MessagesPlaceholder Implementation\nDESCRIPTION: Shows an alternative way to implement message placeholder functionality using the placeholder role instead of the MessagesPlaceholder class.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/anthropic.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nconst promptTemplate = ChatPromptTemplate.fromMessages([\n  [\"system\", \"You are a helpful assistant\"],\n  [\"placeholder\", \"{msgs}\"], // <-- This is the changed part\n]);\n```\n\n----------------------------------------\n\nTITLE: Ignoring Specific Files in GitHub Repository Loading\nDESCRIPTION: Example showing how to ignore specific files when loading from a GitHub repository by passing an ignorePaths array using .gitignore syntax to the GithubLoader constructor.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/github.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nIgnoreExample\n```\n\n----------------------------------------\n\nTITLE: Invoking a Model with Tool Relevant Prompt\nDESCRIPTION: Demonstrates invoking a model with a prompt that is relevant to the bound tool. This example shows how to ask the model to perform a multiplication, which should trigger the tool calling functionality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/tool_calling.mdx#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await llmWithTools.invoke(\"What is 2 multiplied by 3?\");\n```\n\n----------------------------------------\n\nTITLE: Accessing Document Metadata\nDESCRIPTION: Logs the metadata of the first document loaded from the PDF file.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/pdf.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(docs[0].metadata)\n```\n\n----------------------------------------\n\nTITLE: Complete Agent Example - Invoking Python Interpreter Session - TypeScript\nDESCRIPTION: Provides a comprehensive example demonstrating how to use an Azure OpenAI chat model to call the Python code interpreter session tool, execute the code, and get the result using TypeScript. Requires all previous setup steps and authentication in Azure.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/azure_dynamic_sessions.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport AgentExample from \"@examples/tools/azure_dynamic_sessions/azure_dynamic_sessions-agent.ts\";\n\n<CodeBlock language=\"typescript\">{AgentExample}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Structured Output with Reasoning Models using z.optional() in TypeScript\nDESCRIPTION: This example demonstrates an incorrect way of defining a schema for structured output with reasoning models like 'o1'. It uses z.optional(), which is not respected by these models.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#2025-04-22_snippet_15\n\nLANGUAGE: typescript\nCODE:\n```\nimport { z } from \"zod\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\n// Will not work\nconst reasoningModelSchemaOptional = z.object({\n  color: z.optional(z.string()).describe(\"A color mentioned in the input\"),\n});\n\nconst reasoningModelOptionalSchema = new ChatOpenAI({\n  model: \"o1\",\n}).withStructuredOutput(reasoningModelSchemaOptional, {\n  name: \"extract_color\",\n});\n\nawait reasoningModelOptionalSchema.invoke([{\n  role: \"user\",\n  content: `I am 6'5\" tall and love fruit.`\n}]);\n```\n\n----------------------------------------\n\nTITLE: Implementing HyDE Retriever in TypeScript with LangChain.js\nDESCRIPTION: This code demonstrates how to set up and use the HyDE Retriever in a TypeScript project using LangChain.js. It includes initializing the necessary components, creating a HyDE instance, and performing a retrieval operation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/hyde.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { HydeRetriever } from \"langchain/retrievers/hyde\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { Document } from \"@langchain/core/documents\";\n\n// Initialize the base components for HyDE\nconst embeddings = new OpenAIEmbeddings();\nconst llm = new ChatOpenAI();\n\n// Create a mock vector store\nconst vectorStore = {\n  similaritySearch: async (query: string) => {\n    // Normally this would perform an actual similarity search\n    // For this example, we'll just return a mock result\n    return [\n      new Document({\n        pageContent: \"A mock search result for \" + query,\n        metadata: { source: \"mock\" },\n      }),\n    ];\n  },\n};\n\n// Create the HyDE instance\nconst hyde = new HydeRetriever({\n  vectorStore,\n  llm,\n  embeddings,\n});\n\n// Use the HyDE retriever\nconst docs = await hyde.getRelevantDocuments(\n  \"What is the capital of France?\"\n);\nconsole.log(docs);\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatOpenAI Model in JavaScript\nDESCRIPTION: This snippet initializes a ChatOpenAI model with specific parameters for use in the query analysis.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_few_shot.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ChatOpenAI } from '@langchain/openai';\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o\",\n  temperature: 0,\n})\n```\n\n----------------------------------------\n\nTITLE: Instantiating PuppeteerWebBaseLoader\nDESCRIPTION: Code to instantiate the PuppeteerWebBaseLoader with a URL and optional configuration parameters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/web_puppeteer.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { PuppeteerWebBaseLoader } from \"@langchain/community/document_loaders/web/puppeteer\"\n\nconst loader = new PuppeteerWebBaseLoader(\"https://langchain.com\", {\n  // required params = ...\n  // optional params = ...\n})\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatGroq Model\nDESCRIPTION: Example of instantiating a ChatGroq model with specific configuration parameters like model type, temperature, and retry settings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/groq.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatGroq } from \"@langchain/groq\" \n\nconst llm = new ChatGroq({\n    model: \"llama-3.3-70b-versatile\",\n    temperature: 0,\n    maxTokens: undefined,\n    maxRetries: 2,\n    // other params...\n})\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Dependencies\nDESCRIPTION: Command to install the required LangChain packages using npm or yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/summarization.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i langchain @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Using JigsawStack Prompt Engine with LangChain\nDESCRIPTION: Demonstrates how to initialize and use the JigsawStack Prompt Engine to generate responses. Creates a new instance of the engine and invokes it with a prompt about the Leaning Tower of Pisa.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/jigsawstack.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JigsawStackPromptEngine } from \"@langchain/jigsawstack\";\n\nexport const run = async () => {\n  const model = new JigsawStackPromptEngine();\n  const res = await model.invoke(\n    \"Tell me about the leaning tower of pisa?\\nAnswer:\"\n  );\n  console.log({ res });\n};\n```\n\n----------------------------------------\n\nTITLE: Creating and Loading from a New Apify Dataset in LangChain\nDESCRIPTION: Example code that demonstrates crawling the LangChain documentation website using Apify's Website Content Crawler Actor, storing results in a dataset, and then loading those documents into LangChain for further processing.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/apify_dataset.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ApifyDatasetLoader } from \"@langchain/community/document_loaders/web/apify_dataset\";\n\n// First, we need to start a crawl on the Apify platform\n// and wait for its results to be ready.\n//\n// Apify SDK would be more efficient for this, but this serves as a simple example\nconst loader = await ApifyDatasetLoader.fromActorCall(\n  \"apify/website-content-crawler\",\n  {\n    startUrls: [{ url: \"https://js.langchain.com/docs/\" }],\n    maxCrawlPages: 10,\n    crawlerType: \"cheerio\",\n  },\n  {\n    datasetMappingFunction: (item) => {\n      return {\n        pageContent: item.text || \"\",\n        metadata: {\n          source: item.url,\n          title: item.metadata?.title,\n        },\n      };\n    },\n    clientOptions: {\n      token: \"YOUR_APIFY_API_TOKEN\", // Get your API token at https://console.apify.com/settings/integrations\n    },\n  }\n);\n\nconst docs = await loader.load();\n\nconsole.log(docs.length);\nconsole.log(docs[0].pageContent.slice(0, 500));\n```\n\n----------------------------------------\n\nTITLE: Invoking TavilySearchResults Tool Directly in TypeScript\nDESCRIPTION: Demonstrates how to directly invoke the TavilySearchResults tool with a search query input.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/tavily_search_community.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nawait tool.invoke({\n  input: \"what is the current weather in SF?\",\n});\n```\n\n----------------------------------------\n\nTITLE: Custom Parameters Configuration\nDESCRIPTION: Example of configuring OllamaEmbeddings with custom model parameters like memory mapping and thread count.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/ollama.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { OllamaEmbeddings } from \"@langchain/ollama\";\n\nconst embeddingsCustomParams = new OllamaEmbeddings({\n  requestOptions: {\n    useMmap: true, // use_mmap 1\n    numThread: 6, // num_thread 6\n    numGpu: 1, // num_gpu 1\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Few Shot Prompt Example with LangChainJS\nDESCRIPTION: This example shows the basic structure of a few shot prompt where the LLM is given examples of questions and answers to follow a specific format.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot.mdx#2025-04-22_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\nRespond to the users question in the with the following format:\n\nQuestion: What is your name?\nAnswer: My name is John.\n\nQuestion: What is your age?\nAnswer: I am 25 years old.\n\nQuestion: What is your favorite color?\nAnswer:\n```\n\n----------------------------------------\n\nTITLE: Initializing LangSmith Environment Variables in TypeScript\nDESCRIPTION: This snippet demonstrates how to set environment variables for LangSmith to enable automated tracing. The LANGSMITH_TRACING variable is set to true, and the LANGSMITH_API_KEY is configured with your LangSmith API key. Ensure these variables are set prior to executing any tracing-related tasks.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/sql.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.LANGSMITH_TRACING=\\\"true\\\"\nprocess.env.LANGSMITH_API_KEY=\\\"your-api-key\\\"\n```\n\n----------------------------------------\n\nTITLE: Example Worker Service Configuration for Langchain\nDESCRIPTION: This TOML snippet is an example configuration for a LangChain project, specifying the service name, main file, and vectorize and ai bindings necessary for running a worker.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/cloudflare_vectorize.mdx#2025-04-22_snippet_3\n\nLANGUAGE: toml\nCODE:\n```\nname = \"langchain-test\"\nmain = \"worker.ts\"\ncompatibility_date = \"2024-01-10\"\n\n[[vectorize]]\nbinding = \"VECTORIZE_INDEX\"\nindex_name = \"langchain-test\"\n\n[ai]\nbinding = \"AI\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Message Trimming Logic\nDESCRIPTION: Uses trimMessages function to limit conversation history based on token count while preserving system messages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/versions/migrating_memory/conversation_buffer_window_memory.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { trimMessages } from \"@langchain/core/messages\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst selectedMessages = await trimMessages(\n  messages,\n  {\n    tokenCounter: new ChatOpenAI({ model: \"gpt-4o\" }),\n    maxTokens: 80,\n    startOn: \"human\",\n    strategy: \"last\",\n    includeSystem: true,\n  }\n)\n\nfor (const msg of selectedMessages) {\n    console.log(msg);\n}\n```\n\n----------------------------------------\n\nTITLE: Instantiating the Retriever in Python\nDESCRIPTION: Example of how to instantiate the new retriever. This is a placeholder that needs to be updated with the actual instantiation code and relevant parameters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/retrievers.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { __module_name__ } from \"__full_import_path__\";\n\nconst retriever = new __module_name__(\n  // ...\n);\n```\n\n----------------------------------------\n\nTITLE: Configuring Connection to Astra DB in TypeScript\nDESCRIPTION: Defines a TypeScript configuration object (`configConnection`) for connecting to Astra DB, a Cassandra-as-a-Service platform. It uses `serviceProviderArgs` with the Astra token and endpoint obtained from the Astra DB dashboard. Alternatively, `datacenterID` and optional `regionName` can be used instead of `endpoint`. This object is used when initializing the `CassandraStore`.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/cassandra.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst configConnection = {\n  serviceProviderArgs: {\n    astra: {\n      token: <...> as string,\n      endpoint: <...> as string,\n    },\n  },\n};\n```\n\n----------------------------------------\n\nTITLE: LLM Response to Few Shot Prompt Example\nDESCRIPTION: Shows the expected output from an LLM following the few shot prompt pattern established in the previous example.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot.mdx#2025-04-22_snippet_1\n\nLANGUAGE: txt\nCODE:\n```\nAnswer: I don't have a favorite color; I don't have preferences.\n```\n\n----------------------------------------\n\nTITLE: Loading and Processing JSONLines with TypeScript\nDESCRIPTION: Demonstrates how to use the JSONLinesLoader class to load a JSONL file and extract specific properties using JSONPointer. The loader creates Document objects with metadata including the line number and content type.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/jsonlines.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSONLinesLoader } from \"langchain/document_loaders/fs/json\";\n\nconst loader = new JSONLinesLoader(\n  \"src/document_loaders/example_data/example.jsonl\",\n  \"/html\"\n);\n\nconst docs = await loader.load();\n/*\n[\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/jsonl+json\",\n      \"line\": 1,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"This is a sentence.\",\n  },\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/jsonl+json\",\n      \"line\": 2,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"This is another sentence.\",\n  },\n]\n*/\n```\n\n----------------------------------------\n\nTITLE: Defining Search Schema with Zod\nDESCRIPTION: Creates a schema definition using Zod for structuring query analysis output with query and person fields\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_multiple_retrievers.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { z } from \"zod\";\n\nconst searchSchema = z.object({\n    query: z.string().describe(\"Query to look up\"),\n    person: z.string().describe(\"Person to look things up for. Should be `HARRISON` or `ANKUSH`.\")    \n})\n```\n\n----------------------------------------\n\nTITLE: Installing Apify Client for LangChain Integration\nDESCRIPTION: Command to install the official Apify client which is required to interact with Apify services and load data from Apify Datasets.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/apify_dataset.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install apify-client\n```\n\n----------------------------------------\n\nTITLE: Instantiating InMemoryStore\nDESCRIPTION: Creates a new instance of InMemoryStore typed with BaseMessage for storing chat messages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/stores/in_memory.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { InMemoryStore } from \"@langchain/core/stores\"\nimport { BaseMessage } from \"@langchain/core/messages\";\n\nconst kvStore = new InMemoryStore<BaseMessage>();\n```\n\n----------------------------------------\n\nTITLE: Performing Filtered Similarity Search (Allow List) with Matching Engine (TypeScript)\nDESCRIPTION: Shows how to perform a similarity search while restricting results based on document metadata. This example uses a `Restriction` object to filter for documents where the 'color' metadata namespace includes the value 'red'. The search retrieves up to 4 matching documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/googlevertexai.mdx#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Restriction } from `@langchain/community/vectorstores/googlevertexai`;\n\nconst redFilter: Restriction[] = [\n  {\n    namespace: \"color\",\n    allowList: [\"red\"],\n  },\n];\nconst redResults = await engine.similaritySearch(\"this\", 4, redFilter);\n```\n\n----------------------------------------\n\nTITLE: Installing Cohere and LangChain Core Dependencies\nDESCRIPTION: Command for installing the required npm packages to use Cohere's reranking capabilities with LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_compressors/cohere_rerank.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/cohere @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Configuring Connection to Apache Cassandra\nDESCRIPTION: Configuration object for connecting to an Apache Cassandra database, specifying contact points, data center, and authentication credentials.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/cassandra.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst configConnection = {\n  contactPoints: ['h1', 'h2'],\n  localDataCenter: 'datacenter1',\n  credentials: {\n    username: <...> as string,\n    password: <...> as string,\n  },\n};\n```\n\n----------------------------------------\n\nTITLE: Compressing Documents with WatsonxRerank in Python\nDESCRIPTION: Shows how to compress documents and return them with the result using the compressDocuments method of WatsonxRerank.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_compressors/ibm.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nconst compressedWithResults = await reranker.compressDocuments(result, query);\nconsole.log(compressedWithResults);\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Qdrant and OpenAI\nDESCRIPTION: Sets up environment variables for Qdrant URL and OpenAI API key required for connecting to the respective services. Includes optional configuration for LangSmith tracing.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/qdrant.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// e.g. http://localhost:6333\nprocess.env.QDRANT_URL = \"your-qdrant-url\"\n```\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.OPENAI_API_KEY = \"YOUR_API_KEY\";\n```\n\nLANGUAGE: typescript\nCODE:\n```\n// process.env.LANGSMITH_TRACING=\"true\"\n// process.env.LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Executing Agent Workflow with Tavily Search Tool in LangChain.js\nDESCRIPTION: This snippet shows the start of an agent execution chain where the agent processes a user query about the director of Oppenheimer and executes a search tool to retrieve relevant information. It demonstrates the structured format of agent steps, actions, and observations in a LangChain.js workflow.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"input\": \"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\",\n  \"steps\": [\n    {\n      \"action\": {\n        \"tool\": \"tavily_search_results_json\",\n        \"toolInput\": {\n          \"input\": \"Oppenheimer 2023 film director age\"\n        },\n        \"toolCallId\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n        \"log\": \"Invoking \\\"tavily_search_results_json\\\" with {\\\"input\\\":\\\"Oppenheimer 2023 film director age\\\"}\\n[{\\\"type\\\":\\\"tool_use\\\",\\\"id\\\":\\\"toolu_01NUVejujVo2y8WGVtZ49KAN\\\",\\\"name\\\":\\\"tavily_search_results_json\\\",\\\"input\\\":{\\\"input\\\":\\\"Oppenheimer 2023 film director age\\\"}}]\",\n        \"messageLog\": [\n          {\n            \"lc\": 1,\n            \"type\": \"constructor\",\n            \"id\": [\n              \"langchain_core\",\n              \"messages\",\n              \"AIMessageChunk\"\n            ],\n            \"kwargs\": {\n              \"content\": [\n                {\n                  \"type\": \"tool_use\",\n                  \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n                  \"name\": \"tavily_search_results_json\",\n                  \"input\": {\n                    \"input\": \"Oppenheimer 2023 film director age\"\n                  }\n                }\n              ],\n              \"additional_kwargs\": {\n                \"id\": \"msg_015MqAHr84dBCAqBgjou41Km\",\n                \"type\": \"message\",\n                \"role\": \"assistant\",\n                \"model\": \"claude-3-sonnet-20240229\",\n                \"stop_sequence\": null,\n                \"usage\": {\n                  \"input_tokens\": 409,\n                  \"output_tokens\": 68\n                },\n                \"stop_reason\": \"tool_use\"\n              },\n              \"tool_call_chunks\": [\n                {\n                  \"name\": \"tavily_search_results_json\",\n                  \"args\": \"{\\\"input\\\":\\\"Oppenheimer 2023 film director age\\\"}\",\n                  \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n                  \"index\": 0\n                }\n              ],\n              \"tool_calls\": [\n                {\n                  \"name\": \"tavily_search_results_json\",\n                  \"args\": {\n                    \"input\": \"Oppenheimer 2023 film director age\"\n                  },\n                  \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\"\n                }\n              ],\n              \"invalid_tool_calls\": [],\n              \"response_metadata\": {}\n            }\n          }\n        ]\n      },\n      \"observation\": \"[{\\\"title\\\":\\\"Oppenheimer (2023) - IMDb\\\",\\\"url\\\":\\\"https://www.imdb.com/title/tt15398776/\\\",\\\"content\\\":\\\"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.\\\",\\\"score\\\":0.96643,\\\"raw_content\\\":null},{\\\"title\\\":\\\"Christopher Nolan's Oppenheimer - Rotten Tomatoes\\\",\\\"url\\\":\\\"https://editorial.rottentomatoes.com/article/everything-we-know-about-christopher-nolans-oppenheimer/\\\",\\\"content\\\":\\\"Billboards and movie theater pop-ups across Los Angeles have been ticking down for months now: Christopher Nolan's epic account of J. Robert Oppenheimer, the father of the atomic bomb, is nearing an explosive release on July 21, 2023. Nolan movies are always incredibly secretive, twists locked alongside totems behind safe doors, actors not spilling an ounce of Earl Grey tea.\\\",\\\"score\\\":0.92804,\\\"raw_content\\\":null}\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading HNSWLib Vector Store to/from Disk\nDESCRIPTION: Demonstrates how to persist a HNSWLib vector store to disk and reload it later. This allows for saving the state of your vector database between application runs.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/hnswlib.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n// Save the vector store to a directory\nconst directory = \"your/directory/here\";\nawait vectorStore.save(directory);\n\n// Load the vector store from the same directory\nconst loadedVectorStore = await HNSWLib.load(directory, new OpenAIEmbeddings());\n\n// vectorStore and loadedVectorStore are identical\nawait loadedVectorStore.similaritySearch(\"hello world\", 1);\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for OpenAI and Neo4j\nDESCRIPTION: This snippet demonstrates how to set up environment variables for OpenAI API key, LangSmith observability, and Neo4j database credentials.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/graph_constructing.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: env\nCODE:\n```\nOPENAI_API_KEY=your-api-key\n\n# Optional, use LangSmith for best-in-class observability\nLANGSMITH_API_KEY=your-api-key\nLANGSMITH_TRACING=true\n\n# Reduce tracing latency if you are not in a serverless environment\n# LANGCHAIN_CALLBACKS_BACKGROUND=true\n\nNEO4J_URI=\"bolt://localhost:7687\"\nNEO4J_USERNAME=\"neo4j\"\nNEO4J_PASSWORD=\"password\"\n```\n\n----------------------------------------\n\nTITLE: Splitting Markdown Text with RecursiveCharacterTextSplitter\nDESCRIPTION: Demonstrates how to split Markdown text using specialized separators through RecursiveCharacterTextSplitter. Handles markdown headers, code blocks, and paragraphs with appropriate splits.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/code_splitter.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst markdownText = `\n# 🦜️🔗 LangChain\n\n⚡ Building applications with LLMs through composability ⚡\n\n## Quick Install\n\n\\`\\`\\`bash\n# Hopefully this code block isn't split\npip install langchain\n\\`\\`\\`\n\nAs an open-source project in a rapidly developing field, we are extremely open to contributions.\n`;\n\nconst mdSplitter = RecursiveCharacterTextSplitter.fromLanguage(\n  \"markdown\", {\n      chunkSize: 60,\n      chunkOverlap: 0,\n    }\n)\nconst mdDocs = await mdSplitter.createDocuments([markdownText])\n\nmdDocs\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain OpenAI Package\nDESCRIPTION: Command to install the required LangChain OpenAI package using npm or yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/logprobs.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n@langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Packages for Tool Usage (Bash)\nDESCRIPTION: Installs necessary LangChain packages for utilizing the Google Calendar Tool, including OpenAI integration, core functionalities, community integrations (where the tool resides), and LangGraph for potential agent orchestration. Use npm or yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/google_calendar.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core @langchain/community @langchain/langgraph\n```\n\n----------------------------------------\n\nTITLE: Subclassing TextLoader in TypeScript\nDESCRIPTION: Extends the TextLoader abstract class to create a loader for text-based documents. Requires implementing the parse() method which receives raw text content and should return an array of strings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/document_loader_custom.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nabstract class TextLoader extends BaseDocumentLoader {\n  abstract parse(raw: string): Promise<string[]>;\n}\n```\n\n----------------------------------------\n\nTITLE: Example Code Reference for Friendli Implementation\nDESCRIPTION: Reference to a TypeScript example file showing the implementation of Friendli chat model. The example demonstrates how to use the Friendli integration with LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/friendli.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n{Example}\n```\n\n----------------------------------------\n\nTITLE: Setting Cohere API Key in Environment\nDESCRIPTION: Sets the COHERE_API_KEY environment variable for authentication with Cohere's API.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/cohere.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport COHERE_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Package Dependencies\nDESCRIPTION: NPM installation command for the package and its core dependency.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/create-langchain-integration/template/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/<ADD_PACKAGE_NAME_HERE> @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Example AWS Step Functions Code in TypeScript\nDESCRIPTION: The TypeScript code example demonstrates how to use the AWS Step Functions Toolkit by including an example setup and execution. The snippet requires TypeScript and an AWS environment with correctly configured credentials.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/sfn_agent.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/agents/aws_sfn.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Setting Unstructured API Environment Variables\nDESCRIPTION: Command to set the required environment variables for accessing the Unstructured API service, including the API key and URL.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/document_loader_markdown.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport UNSTRUCTURED_API_KEY=\"...\"\n# Replace with your `Full URL` from the email\nexport UNSTRUCTURED_API_URL=\"https://<ORG_NAME>-<SECRET>.api.unstructuredapp.io/general/v0/general\"\n```\n\n----------------------------------------\n\nTITLE: Invoking ChatBedrockConverse with System and Human Messages\nDESCRIPTION: Demonstrates how to invoke the ChatBedrockConverse model with system and human messages. The example sets up the model as a translator from English to French.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/bedrock_converse.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst aiMsg = await llm.invoke([\n  [\n    \"system\",\n    \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n  ],\n  [\"human\", \"I love programming.\"],\n])\naiMsg\n```\n\n----------------------------------------\n\nTITLE: Using EmbeddingsFilter for Efficient Document Filtering\nDESCRIPTION: This example shows how to use the EmbeddingsFilter to efficiently filter retrieved documents based on embedding similarity to the query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/contextual_compression.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport EmbeddingsFilterExample from \"@examples/retrievers/embeddings_filter.ts\";\n\n<CodeBlock language=\"typescript\">{EmbeddingsFilterExample}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for AzionRetriever and OpenAI in TypeScript\nDESCRIPTION: Sets up the necessary environment variables for using AzionRetriever and OpenAI embeddings. It includes optional setup for LangSmith tracing.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/azion-edgesql.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.AZION_TOKEN = \"your-api-key\"\n```\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.OPENAI_API_KEY = \"YOUR_API_KEY\";\n```\n\nLANGUAGE: typescript\nCODE:\n```\n// process.env.LANGSMITH_API_KEY = \"<YOUR API KEY HERE>\";\n// process.env.LANGSMITH_TRACING = \"true\";\n```\n\n----------------------------------------\n\nTITLE: Installing Required LangChain Packages\nDESCRIPTION: Installing the necessary LangChain packages for Google Vertex AI integration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/google_vertex_ai.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatVertexAI } from \"@langchain/google-vertexai\"\n\nconst llm = new ChatVertexAI({\n    model: \"gemini-2.0-flash-exp\",\n    temperature: 0,\n    maxRetries: 2,\n})\n```\n\n----------------------------------------\n\nTITLE: Streaming JSON Output with JsonOutputParser\nDESCRIPTION: Shows how to stream partial JSON chunks from the model response. The parser accumulates partial chunks and returns the complete JSON when generation is finished.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/output_parser_json.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nfor await (const s of await chain.stream({ query: jokeQuery })) {\n    console.log(s)\n}\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith API Key for Tracing\nDESCRIPTION: Optional environment variable setup for enabling LangSmith tracing of model calls. This allows for monitoring and debugging of API calls.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/togetherai.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Importing People Component in React/JSX\nDESCRIPTION: Imports a custom People component that is used to display different categories of contributors in the documentation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/people.mdx#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport People from \"@theme/People\";\n```\n\n----------------------------------------\n\nTITLE: Package Installation Commands\nDESCRIPTION: Installation commands for required dependencies @langchain/openai and @langchain/core using npm.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/time-weighted-retriever.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Invoking ChatAnthropic Model in Python\nDESCRIPTION: Demonstrates how to invoke the ChatAnthropic model with a system message and a human message.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/anthropic.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst aiMsg = await llm.invoke([\n    [\n        \"system\",\n        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n    ],\n    [\"human\", \"I love programming.\"],\n])\naiMsg\n```\n\n----------------------------------------\n\nTITLE: Testing Dynamically Generated Tool in TypeScript\nDESCRIPTION: Verifies that a dynamically generated tool works correctly by updating the userToPets object with a predefined userId. This demonstrates the tool's ability to use the bound userId value.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_runtime.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst [updatePets] = generateToolsForUser(\"cobb\");\n\nawait updatePets.invoke({ pets: [\"tiger\", \"wolf\"] });\n\nconsole.log(userToPets);\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for YouTube Transcript Loading\nDESCRIPTION: Commands to install the necessary packages for loading YouTube transcripts into LangChain, including the community package, core package, and youtubei.js for metadata extraction.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/youtube.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core youtubei.js\n```\n\n----------------------------------------\n\nTITLE: Loading Azure Blob Storage Container Example\nDESCRIPTION: Example showing usage of Azure Blob Storage Container loader, imported from examples directory. Note: actual code content is not visible in the provided text.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/azure_blob_storage_container.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// Example code content not provided in original text\n```\n\n----------------------------------------\n\nTITLE: DirectoryLoader Instantiation\nDESCRIPTION: Example showing how to instantiate DirectoryLoader with different file type loaders. Demonstrates configuration for JSON, JSONL, TXT and CSV files.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/directory.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { DirectoryLoader } from \"langchain/document_loaders/fs/directory\";\nimport {\n  JSONLoader,\n  JSONLinesLoader,\n} from \"langchain/document_loaders/fs/json\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\nimport { CSVLoader } from \"@langchain/community/document_loaders/fs/csv\";\n\nconst loader = new DirectoryLoader(\n  \"../../../../../../examples/src/document_loaders/example_data\",\n  {\n    \".json\": (path) => new JSONLoader(path, \"/texts\"),\n    \".jsonl\": (path) => new JSONLinesLoader(path, \"/html\"),\n    \".txt\": (path) => new TextLoader(path),\n    \".csv\": (path) => new CSVLoader(path, \"text\"),\n  }\n);\n```\n\n----------------------------------------\n\nTITLE: Generating Single Query Embedding\nDESCRIPTION: Demonstrates how to generate an embedding for a single text query using the embedQuery method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/mixedbread_ai.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst embedding = await embeddings.embedQuery(\n  \"Represent this sentence for searching relevant passages: Is baking fun?\"\n);\nconsole.log(embedding);\n```\n\n----------------------------------------\n\nTITLE: Using FileSystemChatMessageHistory with ChatOpenAI\nDESCRIPTION: Example code showing how to implement FileSystemChatMessageHistory with ChatOpenAI, demonstrating message persistence across chat sessions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/file.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { FileSystemChatMessageHistory } from \"@langchain/community/stores/message/file_system\";\nimport { ConversationChain } from \"langchain/chains\";\nimport { BufferMemory } from \"langchain/memory\";\n\n/**\n * This example shows how to use FileSystemChatMessageHistory to persist chat message history between runs\n * Note that this API is still in development and may change\n */\n\nconst memory = new BufferMemory({\n  chatHistory: new FileSystemChatMessageHistory({\n    filePath: \"chat_history.json\",\n  }),\n});\n\n// Create a new instance of ConversationChain with the configured memory and model\nconst model = new ChatOpenAI({});\nconst chain = new ConversationChain({\n  llm: model,\n  memory,\n});\n\n// start a chat with a message from the human\nconst message1 = \"Hi, I'm Joe!\";\nconst response1 = await chain.invoke({ input: message1 });\nconsole.log(response1);\n\n/**\n * Now let's create a new instance of ConversationChain with the same FileSystemChatMessageHistory\n * and see if the memory persists\n */\nconst memory2 = new BufferMemory({\n  chatHistory: new FileSystemChatMessageHistory({\n    filePath: \"chat_history.json\", // same file path as before\n  }),\n});\n\nconst model2 = new ChatOpenAI({});\nconst chain2 = new ConversationChain({\n  llm: model2,\n  memory: memory2,\n});\n\n// Continue the chat with another message\nconst message2 = \"What did I just say my name was?\";\nconst response2 = await chain2.invoke({ input: message2 });\nconsole.log(response2);\n```\n\n----------------------------------------\n\nTITLE: Loading and displaying documents with UnstructuredLoader\nDESCRIPTION: Code to load documents using the instantiated UnstructuredLoader and display the first document. The load() method processes the file and returns document objects.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/unstructured.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst docs = await loader.load()\ndocs[0]\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search with Score (TypeScript)\nDESCRIPTION: Executes a similarity search using the `similaritySearchWithScore` method, which returns both the documents and their corresponding similarity scores. It searches for documents similar to \"biology\", retrieves the top 2 results (`k=2`), and applies the previously defined `filter`. The code then iterates through the results, logging each document's content, metadata, and similarity score (formatted to 3 decimal places).\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/memory.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst similaritySearchWithScoreResults = await vectorStore.similaritySearchWithScore(\"biology\", 2, filter)\n\nfor (const [doc, score] of similaritySearchWithScoreResults) {\n  console.log(`* [SIM=${score.toFixed(3)}] ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`);\n}\n```\n\n----------------------------------------\n\nTITLE: Importing OpenAI Integration in Browser Environment\nDESCRIPTION: Example of importing the ChatOpenAI class for use in browser environments, compatible with bundlers like Webpack and Vite.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/tests/__mdx__/modules/two.mdx#2025-04-22_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n```\n\n----------------------------------------\n\nTITLE: Linting and Formatting @langchain/cloudflare Code\nDESCRIPTION: Command to run the linter and formatter on the @langchain/cloudflare package to ensure code quality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-cloudflare/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nyarn lint && yarn format\n```\n\n----------------------------------------\n\nTITLE: Creating Dynamic Tools with User Binding in TypeScript\nDESCRIPTION: Alternative approach that generates tools dynamically at runtime with pre-bound user IDs. This method works in environments that don't support async_hooks or older versions of the library.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_runtime.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { z } from \"zod\";\nimport { tool } from \"@langchain/core/tools\";\n\nuserToPets = {};\n\nfunction generateToolsForUser(userId: string) {\n  const updateFavoritePets = tool(async (input) => {\n    userToPets[userId] = input.pets;\n    return \"update_favorite_pets called.\"\n  }, {\n    name: \"update_favorite_pets\",\n    description: \"add to the list of favorite pets.\",\n    schema: z.object({\n      pets: z.array(z.string())\n    }),\n  });\n  // You can declare and return additional tools as well:\n  return [updateFavoritePets];\n}\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Index from CloseVector CDN\nDESCRIPTION: TypeScript code demonstrating how to save a vector index to CloseVector CDN and load it again. This requires a CloseVector account and API key. The snippet is referenced but not directly included in the content.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/closevector.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\n// Code content is dynamically imported and not directly available in the provided text.\n```\n\n----------------------------------------\n\nTITLE: Constructing Comparisons from Search Query in TypeScript\nDESCRIPTION: This function constructs an array of Comparison objects based on the search query. It creates comparisons for the startYear and author fields if they are present in the query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_constructing_filters.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Comparison, Comparator } from \"langchain/chains/query_constructor/ir\";\n\nfunction constructComparisons(query: z.infer<typeof searchSchema>): Comparison[] {\n  const comparisons: Comparison[] = [];\n  if (query.startYear !== undefined) {\n    comparisons.push(\n      new Comparison(\n        \"gt\" as Comparator,\n        \"start_year\",\n        query.startYear,\n      )\n    );\n  }\n  if (query.author !== undefined) {\n    comparisons.push(\n      new Comparison(\n        \"eq\" as Comparator,\n        \"author\",\n        query.author,\n      )\n    );\n  }\n  return comparisons;\n}\n\nconst comparisons = constructComparisons(searchQuery);\n```\n\n----------------------------------------\n\nTITLE: Initializing the Custom Example Selector\nDESCRIPTION: Creates a new instance of the CustomExampleSelector with the predefined translation examples.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/example_selectors.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst exampleSelector = new CustomExampleSelector(examples)\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Azure OpenAI in Node.js\nDESCRIPTION: Defines environment variables for Azure OpenAI API credentials and configuration in Node.js.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/azure.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nAZURE_OPENAI_API_INSTANCE_NAME=<YOUR_INSTANCE_NAME>\nAZURE_OPENAI_API_DEPLOYMENT_NAME=<YOUR_DEPLOYMENT_NAME>\nAZURE_OPENAI_API_KEY=<YOUR_KEY>\nAZURE_OPENAI_API_VERSION=\"2024-02-01\"\n```\n\n----------------------------------------\n\nTITLE: Invoking DuckDuckGoSearch with ToolCall Object - JavaScript/TypeScript\nDESCRIPTION: This snippet shows how to call the DuckDuckGoSearch tool using a 'ToolCall' object, as might be generated by an LLM. The 'ToolCall' contains named arguments ('input'), an id, the tool's name, and type. On invocation, it returns a ToolMessage for downstream processing. This pattern is useful when chaining search with agents or complex workflows.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/duckduckgo_search.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\n// This is usually generated by a model, but we'll create a tool call directly for demo purposes.\nconst modelGeneratedToolCall = {\n  args: {\n    input: \"what is the current weather in sf?\"\n  },\n  id: \"tool_call_id\",\n  name: tool.name,\n  type: \"tool_call\",\n}\nawait tool.invoke(modelGeneratedToolCall)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for HyDE Retriever in LangChain.js\nDESCRIPTION: This snippet shows how to install the necessary dependencies for using the HyDE Retriever in a LangChain.js project. It includes the OpenAI package and the LangChain core package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/hyde.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Indexing and Retrieving with MemoryVectorStore in LangChain.js\nDESCRIPTION: Demonstrates how to index a sample document using MemoryVectorStore and retrieve similar text using the embeddings model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/azure_openai.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n// Create a vector store with a sample text\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n\nconst text = \"LangChain is the framework for building context-aware reasoning applications\";\n\nconst vectorstore = await MemoryVectorStore.fromDocuments(\n  [{ pageContent: text, metadata: {} }],\n  embeddings,\n);\n\n// Use the vector store as a retriever that returns a single document\nconst retriever = vectorstore.asRetriever(1);\n\n// Retrieve the most similar text\nconst retrievedDocuments = await retriever.invoke(\"What is LangChain?\");\n\nretrievedDocuments[0].pageContent;\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain OpenAI Integration - Bash\nDESCRIPTION: This snippet provides the bash command necessary to install the LangChain OpenAI integration package along with its core dependency. Running the command will add @langchain/openai and @langchain/core to your project's node_modules, enabling usage of the Dall-E Tool and other OpenAI-powered capabilities. No parameters are required; ensure you have npm installed and run the command in your project directory.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/dalle.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Importing Azure Cosmos DB MongoDB vCore Chat Message History\nDESCRIPTION: Code snippet for importing the Azure Cosmos DB MongoDB vCore Chat Message History class, which provides persistent storage for chat messages using Cosmos DB MongoDB API.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/microsoft.mdx#2025-04-22_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AzureCosmosDBMongoChatMessageHistory } from \"@langchain/azure-cosmosdb\";\n```\n\n----------------------------------------\n\nTITLE: LangSmith API Configuration\nDESCRIPTION: Optional configuration for enabling automated tracing using LangSmith API.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/weaviate.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// process.env.LANGSMITH_API_KEY = \"<YOUR API KEY HERE>\";\n// process.env.LANGSMITH_TRACING = \"true\";\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Web Browser Tool\nDESCRIPTION: Command to install the required dependencies (cheerio and axios) for using the Web Browser Tool.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/webbrowser.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install cheerio axios\n```\n\n----------------------------------------\n\nTITLE: Console Output from Verbose Logging in LangChain Agent\nDESCRIPTION: Sample console output showing the detailed logging produced when verbose mode is enabled. The output displays the complete execution flow of an agent answering a query about the Oppenheimer film director, including chain starts/ends, LLM inputs/outputs, and timing information.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n[chain/start] [1:chain:AgentExecutor] Entering Chain run with input: {\n  \"input\": \"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\"\n}\n[chain/start] [1:chain:AgentExecutor > 2:chain:ToolCallingAgent] Entering Chain run with input: {\n  \"input\": \"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\",\n  \"steps\": []\n}\n[chain/start] [1:chain:AgentExecutor > 2:chain:ToolCallingAgent > 3:chain:RunnableAssign] Entering Chain run with input: {\n  \"input\": \"\"\n}\n[chain/start] [1:chain:AgentExecutor > 2:chain:ToolCallingAgent > 3:chain:RunnableAssign > 4:chain:RunnableMap] Entering Chain run with input: {\n  \"input\": \"\"\n}\n[chain/start] [1:chain:AgentExecutor > 2:chain:ToolCallingAgent > 3:chain:RunnableAssign > 4:chain:RunnableMap > 5:chain:RunnableLambda] Entering Chain run with input: {\n  \"input\": \"\"\n}\n[chain/end] [1:chain:AgentExecutor > 2:chain:ToolCallingAgent > 3:chain:RunnableAssign > 4:chain:RunnableMap > 5:chain:RunnableLambda] [0ms] Exiting Chain run with output: {\n  \"output\": []\n}\n[chain/end] [1:chain:AgentExecutor > 2:chain:ToolCallingAgent > 3:chain:RunnableAssign > 4:chain:RunnableMap] [1ms] Exiting Chain run with output: {\n  \"agent_scratchpad\": []\n}\n[chain/end] [1:chain:AgentExecutor > 2:chain:ToolCallingAgent > 3:chain:RunnableAssign] [1ms] Exiting Chain run with output: {\n  \"input\": \"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\",\n  \"steps\": [],\n  \"agent_scratchpad\": []\n}\n[chain/start] [1:chain:AgentExecutor > 2:chain:ToolCallingAgent > 6:prompt:ChatPromptTemplate] Entering Chain run with input: {\n  \"input\": \"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\",\n  \"steps\": [],\n  \"agent_scratchpad\": []\n}\n[chain/end] [1:chain:AgentExecutor > 2:chain:ToolCallingAgent > 6:prompt:ChatPromptTemplate] [0ms] Exiting Chain run with output: {\n  \"lc\": 1,\n  \"type\": \"constructor\",\n  \"id\": [\n    \"langchain_core\",\n    \"prompt_values\",\n    \"ChatPromptValue\"\n  ],\n  \"kwargs\": {\n    \"messages\": [\n      {\n        \"lc\": 1,\n        \"type\": \"constructor\",\n        \"id\": [\n          \"langchain_core\",\n          \"messages\",\n          \"SystemMessage\"\n        ],\n        \"kwargs\": {\n          \"content\": \"You are a helpful assistant\",\n          \"additional_kwargs\": {},\n          \"response_metadata\": {}\n        }\n      },\n      {\n        \"lc\": 1,\n        \"type\": \"constructor\",\n        \"id\": [\n          \"langchain_core\",\n          \"messages\",\n          \"HumanMessage\"\n        ],\n        \"kwargs\": {\n          \"content\": \"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\",\n          \"additional_kwargs\": {},\n          \"response_metadata\": {}\n        }\n      }\n    ]\n  }\n}\n[llm/start] [1:chain:AgentExecutor > 2:chain:ToolCallingAgent > 7:llm:ChatAnthropic] Entering LLM run with input: {\n  \"messages\": [\n    [\n      {\n        \"lc\": 1,\n        \"type\": \"constructor\",\n        \"id\": [\n          \"langchain_core\",\n          \"messages\",\n          \"SystemMessage\"\n        ],\n        \"kwargs\": {\n          \"content\": \"You are a helpful assistant\",\n          \"additional_kwargs\": {},\n          \"response_metadata\": {}\n        }\n      },\n      {\n        \"lc\": 1,\n        \"type\": \"constructor\",\n        \"id\": [\n          \"langchain_core\",\n          \"messages\",\n          \"HumanMessage\"\n        ],\n        \"kwargs\": {\n          \"content\": \"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\",\n          \"additional_kwargs\": {},\n          \"response_metadata\": {}\n        }\n      }\n    ]\n  ]\n}\n[llm/start] [1:llm:ChatAnthropic] Entering LLM run with input: {\n  \"messages\": [\n    [\n      {\n        \"lc\": 1,\n        \"type\": \"constructor\",\n        \"id\": [\n          \"langchain_core\",\n          \"messages\",\n          \"SystemMessage\"\n        ],\n        \"kwargs\": {\n          \"content\": \"You are a helpful assistant\",\n          \"additional_kwargs\": {},\n          \"response_metadata\": {}\n        }\n      },\n      {\n        \"lc\": 1,\n        \"type\": \"constructor\",\n        \"id\": [\n          \"langchain_core\",\n          \"messages\",\n          \"HumanMessage\"\n        ],\n        \"kwargs\": {\n          \"content\": \"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\",\n          \"additional_kwargs\": {},\n          \"response_metadata\": {}\n        }\n      }\n    ]\n  ]\n}\n[llm/end] [1:chain:AgentExecutor > 2:chain:ToolCallingAgent > 7:llm:ChatAnthropic] [1.98s] Exiting LLM run with output: {\n  \"generations\": [\n    [\n      {\n        \"text\": \"\",\n        \"message\": {\n          \"lc\": 1,\n          \"type\": \"constructor\",\n          \"id\": [\n            \"langchain_core\",\n            \"messages\",\n            \"AIMessageChunk\"\n          ],\n          \"kwargs\": {\n            \"content\": [\n              {\n                \"type\": \"tool_use\",\n                \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n                \"name\": \"tavily_search_results_json\",\n                \"input\": {\n                  \"input\": \"Oppenheimer 2023 film director age\"\n                }\n              }\n            ],\n            \"additional_kwargs\": {\n              \"id\": \"msg_015MqAHr84dBCAqBgjou41Km\",\n              \"type\": \"message\",\n              \"role\": \"assistant\",\n              \"model\": \"claude-3-sonnet-20240229\",\n              \"stop_sequence\": null,\n              \"usage\": {\n                \"input_tokens\": 409,\n                \"output_tokens\": 68\n              },\n              \"stop_reason\": \"tool_use\"\n            },\n            \"tool_call_chunks\": [\n              {\n                \"name\": \"tavily_search_results_json\",\n                \"args\": \"{\\\"input\\\":\\\"Oppenheimer 2023 film director age\\\"}\",\n                \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n                \"index\": 0\n              }\n            ],\n            \"tool_calls\": [\n              {\n                \"name\": \"tavily_search_results_json\",\n                \"args\": {\n                  \"input\": \"Oppenheimer 2023 film director age\"\n                },\n                \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\"\n              }\n            ],\n            \"invalid_tool_calls\": [],\n            \"response_metadata\": {}\n          }\n        }\n      }\n    ]\n  ]\n}\n[llm/end] [1:llm:ChatAnthropic] [1.98s] Exiting LLM run with output: {\n  \"generations\": [\n    [\n      {\n        \"text\": \"\",\n        \"message\": {\n          \"lc\": 1,\n          \"type\": \"constructor\",\n          \"id\": [\n            \"langchain_core\",\n            \"messages\",\n            \"AIMessageChunk\"\n          ],\n          \"kwargs\": {\n            \"content\": [\n              {\n                \"type\": \"tool_use\",\n                \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n                \"name\": \"tavily_search_results_json\",\n                \"input\": {\n                  \"input\": \"Oppenheimer 2023 film director age\"\n                }\n              }\n            ],\n            \"additional_kwargs\": {\n              \"id\": \"msg_015MqAHr84dBCAqBgjou41Km\",\n              \"type\": \"message\",\n              \"role\": \"assistant\",\n              \"model\": \"claude-3-sonnet-20240229\",\n              \"stop_sequence\": null,\n              \"usage\": {\n                \"input_tokens\": 409,\n                \"output_tokens\": 68\n              },\n              \"stop_reason\": \"tool_use\"\n            },\n            \"tool_call_chunks\": [\n              {\n                \"name\": \"tavily_search_results_json\",\n                \"args\": \"{\\\"input\\\":\\\"Oppenheimer 2023 film director age\\\"}\",\n                \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n                \"index\": 0\n              }\n            ],\n            \"tool_calls\": [\n              {\n                \"name\": \"tavily_search_results_json\",\n                \"args\": {\n                  \"input\": \"Oppenheimer 2023 film director age\"\n                },\n                \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\"\n              }\n            ],\n            \"invalid_tool_calls\": [],\n            \"response_metadata\": {}\n          }\n        }\n      }\n    ]\n  ]\n}\n[chain/start] [1:chain:AgentExecutor > 2:chain:ToolCallingAgent > 8:parser:ToolCallingAgentOutputParser] Entering Chain run with input: {\n  \"lc\": 1,\n  \"type\": \"constructor\",\n  \"id\": [\n    \"langchain_core\",\n    \"messages\",\n    \"AIMessageChunk\"\n  ],\n  \"kwargs\": {\n    \"content\": [\n      {\n        \"type\": \"tool_use\",\n        \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n        \"name\": \"tavily_search_results_json\",\n        \"input\": {\n          \"input\": \"Oppenheimer 2023 film director age\"\n        }\n      }\n    ],\n    \"additional_kwargs\": {\n      \"id\": \"msg_015MqAHr84dBCAqBgjou41Km\",\n      \"type\": \"message\",\n      \"role\": \"assistant\",\n      \"model\": \"claude-3-sonnet-20240229\",\n      \"stop_sequence\": null,\n      \"usage\": {\n        \"input_tokens\": 409,\n        \"output_tokens\": 68\n      },\n      \"stop_reason\": \"tool_use\"\n    },\n    \"tool_call_chunks\": [\n      {\n        \"name\": \"tavily_search_results_json\",\n        \"args\": \"{\\\"input\\\":\\\"Oppenheimer 2023 film director age\\\"}\",\n        \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n        \"index\": 0\n      }\n    ],\n    \"tool_calls\": [\n      {\n        \"name\": \"tavily_search_results_json\",\n        \"args\": {\n          \"input\": \"Oppenheimer 2023 film director age\"\n        },\n        \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\"\n      }\n\n```\n\n----------------------------------------\n\nTITLE: Installing Momento SDK for Node.js\nDESCRIPTION: Command to install the Momento Client Library for Node.js environments using npm.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/momento.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @gomomento/sdk\n```\n\n----------------------------------------\n\nTITLE: Installing Anthropic Dependencies for LangChain\nDESCRIPTION: Command to install the required npm packages for using Anthropic with LangChain\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/anthropic.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/anthropic @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Initializing Memory Saver and Graph Interrupt Configuration\nDESCRIPTION: Sets up a MemorySaver for persistence and configures graph interrupts for query execution review with thread configuration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/sql_qa.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport { MemorySaver } from \"@langchain/langgraph\";\n\nconst checkpointer = new MemorySaver();\nconst graphWithInterrupt = graphBuilder.compile({\n    checkpointer: checkpointer,\n    interruptBefore: [\"executeQuery\"]\n});\n\n// Now that we're using persistence, we need to specify a thread ID\n// so that we can continue the run after review.\nconst threadConfig = {\n    configurable: { thread_id: \"1\" },\n    streamMode: \"updates\" as const\n};\n```\n\n----------------------------------------\n\nTITLE: Implementing Streaming Chat\nDESCRIPTION: Example of using streaming functionality with the chat model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/create-langchain-integration/template/README.md#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { <ADD_CLASS_NAME_HERE> } from \"@langchain/<ADD_PACKAGE_NAME_HERE>\";\n\nconst model = new ExampleChatClass({\n  apiKey: process.env.EXAMPLE_API_KEY,\n});\nconst response = await model.stream(new HumanMessage(\"Hello world!\"));\n```\n\n----------------------------------------\n\nTITLE: Initializing LocalFileStore Instance\nDESCRIPTION: Creates a new LocalFileStore instance by specifying a directory path where key-value pairs will be stored as individual files.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/stores/file_system.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport { LocalFileStore } from \"langchain/storage/file_system\"\n\nconst kvStore = await LocalFileStore.fromPath(\"./messages\");\n```\n\n----------------------------------------\n\nTITLE: Setting Mistral API Key Environment Variable\nDESCRIPTION: Bash command to set the MISTRAL_API_KEY environment variable for authentication with Mistral AI services.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-mistralai/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport MISTRAL_API_KEY=your-api-key\n```\n\n----------------------------------------\n\nTITLE: Configuring Default Search Parameters\nDESCRIPTION: Implementation of a self-query retriever with default search parameters and filters for Pinecone queries.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/pinecone.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconst selfQueryRetrieverWithDefaultParams = SelfQueryRetriever.fromLLM({\n  llm: llm,\n  vectorStore: vectorStore,\n  documentContents: \"Brief summary of a movie\",\n  attributeInfo: attributeInfo,\n  structuredQueryTranslator: new PineconeTranslator(),\n  searchParams: {\n    filter: {\n      rating: {\n        $gt: 8.5,\n      },\n    },\n    mergeFiltersOperator: \"and\",\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Setting up Novita API Key in Environment\nDESCRIPTION: Shows how to set the NOVITA_API_KEY environment variable for authentication with Novita AI services.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/novita.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport NOVITA_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Importing ChatOpenAI from LangChainJS\nDESCRIPTION: Imports the ChatOpenAI class for using OpenAI's chat models with LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot.mdx#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n```\n\n----------------------------------------\n\nTITLE: Installing Azure AI Search Integration Dependencies\nDESCRIPTION: Command to install the required packages for using Azure AI Search (formerly Azure Cognitive Search) as a vector store in LangChain.js.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/microsoft.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -S @langchain/community @langchain/core @azure/search-documents\n```\n\n----------------------------------------\n\nTITLE: Specifying Vector Similarity Type in CassandraStore Configuration\nDESCRIPTION: Shows how to specify the vector similarity function (e.g., `dot_product`) during `CassandraStore` initialization. The `vectorType` parameter accepts 'cosine' (default), 'dot_product', or 'euclidean'. This setting determines the metric used for similarity calculations when creating the vector index in Cassandra.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/cassandra.mdx#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n  ...,\n  vectorType: \"dot_product\",\n  ...\n```\n\n----------------------------------------\n\nTITLE: Invoking OpenAI Text Completion Model\nDESCRIPTION: Demonstrates how to call the OpenAI text completion model with a simple text prompt and retrieve the completion response.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/openai.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst inputText = \"OpenAI is an AI company that \"\n\nconst completion = await llm.invoke(inputText)\ncompletion\n```\n\n----------------------------------------\n\nTITLE: Creating a React Agent with LangGraph in TypeScript\nDESCRIPTION: Initializes a React agent using LangGraph's prebuilt function with the language model and toolkit tools. This sets up the agent's decision-making framework.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/toolkits.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\"\n\nconst agentExecutor = createReactAgent({ llm, tools });\n```\n\n----------------------------------------\n\nTITLE: Removing Hooks from ChatMistralAI\nDESCRIPTION: Shows methods for removing individual hooks or clearing all hooks from a ChatMistralAI instance.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/mistral.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmodel.removeHookFromHttpClient(beforeRequestHook);\n\nmodel.removeAllHooksFromHttpClient();\n```\n\n----------------------------------------\n\nTITLE: Installing Core LangChain Dependencies\nDESCRIPTION: Command to install the required LangChain packages for using the Web Browser Tool.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/webbrowser.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Loading Movie Data into Neo4j\nDESCRIPTION: Initializing Neo4j graph and importing movie data using Cypher query with CSV data.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/graph_prompting.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport \"neo4j-driver\";\nimport { Neo4jGraph } from \"@langchain/community/graphs/neo4j_graph\";\n\nconst graph = await Neo4jGraph.initialize({ url, username, password });\n\nconst moviesQuery = `LOAD CSV WITH HEADERS FROM \n'https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/movies/movies_small.csv'\nAS row\nMERGE (m:Movie {id:row.movieId})\nSET m.released = date(row.released),\n    m.title = row.title,\n    m.imdbRating = toFloat(row.imdbRating)\nFOREACH (director in split(row.director, '|') | \n    MERGE (p:Person {name:trim(director)})\n    MERGE (p)-[:DIRECTED]->(m))\nFOREACH (actor in split(row.actors, '|') | \n    MERGE (p:Person {name:trim(actor)})\n    MERGE (p)-[:ACTED_IN]->(m))\nFOREACH (genre in split(row.genres, '|') | \n    MERGE (g:Genre {name:trim(genre)})\n    MERGE (m)-[:IN_GENRE]->(g))`\n\nawait graph.query(moviesQuery);\n```\n\n----------------------------------------\n\nTITLE: Installing CSVLoader and Dependencies in Node.js\nDESCRIPTION: Command to install the required packages for using CSVLoader, including @langchain/community, @langchain/core, and d3-dsv version 2.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/csv.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n@langchain/community @langchain/core d3-dsv@2\n```\n\n----------------------------------------\n\nTITLE: Database Integration Example Code\nDESCRIPTION: TypeScript code referenced from external file DbCheck.ts for database interface setup using TypeORM.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/sql_large_db.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n<CodeBlock language=\"typescript\">{DbCheck}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Setting Up Dynamic Few Shot Example Selection in LangChain.js\nDESCRIPTION: This snippet demonstrates how to create a semantic similarity-based example selector with a vector store. It vectorizes example inputs and outputs to enable semantic search for the most relevant examples.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot_examples_chat.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { SemanticSimilarityExampleSelector } from \"@langchain/core/example_selectors\";\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\nimport { OpenAIEmbeddings } from '@langchain/openai';\n\nconst examples = [\n  { input: '2+2', output: '4' },\n  { input: '2+3', output: '5' },\n  { input: '2+4', output: '6' },\n  { input: 'What did the cow say to the moon?', output: 'nothing at all' },\n  {\n    input: 'Write me a poem about the moon',\n    output: 'One for the moon, and one for me, who are we to talk about the moon?',\n  },\n];\n\nconst toVectorize = examples.map((example) => `${example.input} ${example.output}`);\nconst embeddings = new OpenAIEmbeddings();\nconst vectorStore = await MemoryVectorStore.fromTexts(toVectorize, examples, embeddings);\n```\n\n----------------------------------------\n\nTITLE: Using Multimodal Models with Ollama\nDESCRIPTION: Demonstrates how to use Ollama with multimodal models like LLaVA to process both text and images. This example shows how to read an image file, convert it to base64, and bind it to a multimodal model for image analysis.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/ollama.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { Ollama } from \"@langchain/ollama\";\nimport * as fs from \"node:fs/promises\";\n\nconst imageData = await fs.readFile(\"../../../../../examples/hotdog.jpg\");\n\nconst model = new Ollama({\n  model: \"llava\",\n}).bind({\n  images: [imageData.toString(\"base64\")],\n});\n\nconst res = await model.invoke(\"What's in this image?\");\nconsole.log(res);\n```\n\n----------------------------------------\n\nTITLE: Initializing Vespa Retriever for LangChain in TypeScript\nDESCRIPTION: This code snippet demonstrates how to set up a Vespa.ai retriever for use with LangChain. It configures the retriever to fetch up to 5 results from the 'content' field in the 'paragraph' document type, using 'documentation' as the ranking method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/vespa-retriever.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { VespaRetriever } from \"langchain/retrievers/vespa\";\n\nconst retriever = new VespaRetriever({\n  url: \"https://doc-search.vespa.oath.cloud/\",\n  query:\n    'yql=select content from paragraph where userQuery() rank-summary;&ranking=documentation&hits=5',\n});\n\nconst docs = await retriever.getRelevantDocuments(\"What is Vespa?\");\n```\n\n----------------------------------------\n\nTITLE: Invoking Custom Tool with Configuration in LangChain\nDESCRIPTION: Shows how to invoke a custom tool with a configuration object that includes additional configurable parameters. The example demonstrates passing configuration values through the tool's invoke method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_configure.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nawait reverseTool.invoke(\n  {text: \"abc\"}, {configurable: {additional_field: \"123\"}}\n)\n```\n\n----------------------------------------\n\nTITLE: Example Turso Database URL Format (Text)\nDESCRIPTION: Shows the typical format for a remote Turso database connection URL. Replace `[database-name]` and `[your-username]` with the specific details of your Turso database.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/libsql.mdx#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nlibsql://[database-name]-[your-username].turso.io\n```\n\n----------------------------------------\n\nTITLE: Setting up AWS Bedrock credentials with environment variables\nDESCRIPTION: Commands to export the necessary AWS environment variables for authenticating with Bedrock API, including region, access key ID, and secret access key.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/bedrock.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport BEDROCK_AWS_REGION=\"your-region-url\"\nexport BEDROCK_AWS_ACCESS_KEY_ID=\"your-access-key-id\"\nexport BEDROCK_AWS_SECRET_ACCESS_KEY=\"your-secret-access-key\"\n```\n\n----------------------------------------\n\nTITLE: Logging AI Response Content from ChatBedrockConverse\nDESCRIPTION: Shows how to extract and log the content from the AI message returned by the ChatBedrockConverse model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/bedrock_converse.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(aiMsg.content)\n```\n\n----------------------------------------\n\nTITLE: Applying Advanced Filters to SAP HANA Vector Search with LangchainJS (TypeScript)\nDESCRIPTION: This TypeScript example shows how to use advanced filtering capabilities when performing similarity searches on a SAP HANA vector store via LangchainJS. It utilizes operators like `$and`, `$or`, `$eq`, `$ne`, `$lt`, `$gt`, `$in`, `$like`, etc., within the `filter` parameter of the `similaritySearch` method to refine search results based on metadata or document properties.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/hanavector.mdx#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { HanaDB } from \"@langchain/community/vectorstores/hana\";\n\n// Connection options for the database connection, read from environment variables\n// HANA_HOST, HANA_PORT, HANA_USER, HANA_PASSWORD\nconst connectionOptions = {\n  host: process.env.HANA_HOST ?? \"\",\n  port: process.env.HANA_PORT ?? \"\",\n  user: process.env.HANA_USER ?? \"\",\n  password: process.env.HANA_PASSWORD ?? \"\",\n};\n\n// Configuration for the vector store table\nconst vectorStoreTableConfig = {\n  tableName: \"MY_TABLE_NAME\",\n  schemaName: \"MY_SCHEMA_NAME\", // optional, default: current schema\n};\n\n// Input texts with metadata\nconst texts = [\n  \"SAP HANA Cloud is a database as a service (DBaaS) offering.\",\n  \"SAP HANA Cloud Vector Engine is a component of SAP HANA Cloud.\",\n  \"Langchain JS is a framework for developing applications powered by language models.\",\n];\nconst metadata = [{ year: 2023 }, { year: 2023 }, { year: 2022 }];\n\n// Create embeddings model\nconst embeddings = new OpenAIEmbeddings({});\n\n// Create the vector store\nconst vectorStore = await HanaDB.fromTexts(\n  texts,\n  metadata,\n  embeddings,\n  connectionOptions,\n  vectorStoreTableConfig\n);\n\n// Perform a similarity search with an advanced filter\n// Find documents containing \"database\" where the year is 2023\nconst results = await vectorStore.similaritySearch(\"database\", 4, {\n  $and: [{ pageContent: { $like: \"%database%\" } }, { year: { $eq: 2023 } }],\n});\n\nconsole.log(results);\n\n/*\n  [\n    Document {\n      pageContent: 'SAP HANA Cloud is a database as a service (DBaaS) offering.',\n      metadata: { year: 2023 }\n    }\n  ]\n*/\n\n```\n\n----------------------------------------\n\nTITLE: Basic PlanetScale Chat Memory Implementation\nDESCRIPTION: Example of integrating PlanetScale database for chat history persistence in LangChain.js. Shows how to configure a PlanetScale client and use it with BufferMemory for chat applications.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/planetscale.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nExample\n```\n\n----------------------------------------\n\nTITLE: Instantiating TavilySearchAPIRetriever\nDESCRIPTION: Example demonstrating how to create a new instance of TavilySearchAPIRetriever with custom parameters\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/tavily.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { TavilySearchAPIRetriever } from \"@langchain/community/retrievers/tavily_search_api\";\n\nconst retriever = new TavilySearchAPIRetriever({\n  k: 3,\n});\n```\n\n----------------------------------------\n\nTITLE: Integrating JigsawStack Tools with LangChain Agent\nDESCRIPTION: Example of using JigsawStack tools within a LangChain agent executor for AI-powered natural language processing tasks. This code demonstrates setting up the agent with multiple JigsawStack tools and executing a query about a restaurant in Santorini.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/jigsawstack.mdx#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { initializeAgentExecutorWithOptions } from \"langchain/agents\";\nimport {\n  JigsawStackAIScrape,\n  JigsawStackAISearch,\n  JigsawStackVOCR,\n  JigsawStackSpeechToText,\n  JigsawStackTextToSQL,\n} from \"@langchain/jigsawstack\";\n\nconst model = new ChatOpenAI({\n  temperature: 0,\n});\n\n//  add the tools that you need\nconst tools = [\n  new JigsawStackAIScrape(),\n  new JigsawStackAISearch(),\n  new JigsawStackVOCR(),\n  new JigsawStackSpeechToText(),\n  new JigsawStackTextToSQL(),\n];\n\nconst executor = await initializeAgentExecutorWithOptions(tools, model, {\n  agentType: \"zero-shot-react-description\",\n  verbose: true,\n});\n\nconst res = await executor.invoke({\n  input: `Kokkalo Restaurant Santorini`,\n});\n\nconsole.log(res.output);\n\n/*\n{\n  \"query\": \"Kokkalo Restaurant Santorini\",\n  \"ai_overview\": \"Kokkalo Restaurant, located in Fira, Santorini, offers a unique dining experience that blends traditional Greek cuisine with modern culinary trends. Here are some key details about the restaurant:\\n\\n- **Location**: Situated on the main road of Firostefani, Kokkalo is surrounded by the picturesque Cycladic architecture and provides stunning views of the Aegean Sea.\\n- **Cuisine**: The restaurant specializes in authentic Greek dishes, crafted from high-quality, locally sourced ingredients. The menu is designed to engage all senses and features a variety of Mediterranean flavors.\\n- **Ambiance**: Kokkalo boasts a chic and modern décor, creating a welcoming atmosphere for guests. The staff is known for their professionalism and attentiveness, enhancing the overall dining experience.\\n- **Culinary Experience**: The name \\\"Kokkalo,\\\" meaning \\\"bone\\\" in Greek, symbolizes the strong foundation of the restaurant's culinary philosophy. Guests can expect a bold and unforgettable culinary journey.\\n- **Cooking Classes**: Kokkalo also offers cooking lessons, allowing visitors to learn how to prepare traditional Greek dishes, providing a unique souvenir of their time in Santorini.\\n- **Contact Information**: \\n  - Address: 25 Martiou str, Fira, Santorini 84 700, Cyclades, Greece\\n  - Phone: +30 22860 25407\\n  - Email: reservation@kokkalosantorini.com\\n\\nFor more information, you can visit their [official website](https://www.santorini-view.com/restaurants/kokkalo-restaurant/) or their [Facebook page](https://www.facebook.com/kokkalorestaurant/).\",\n  \"is_safe\": true,\n  \"results\": [\n    {\n      \"title\": \"Kokkalo restaurant, Restaurants in Firostefani Santorini Greece\",\n      \"url\": \"http://www.travel-to-santorini.com/restaurants/firostefani/thebonerestaurant/\",\n      \"description\": \"Details Contact : George Grafakos Address : Firostefani, Opposite of Fira Primary School Zipcode : 84700 City : Santorni Phone : +30 22860 25407 Send an email\",\n      \"content\": null,\n      \"site_name\": \"Travel-to-santorini\",\n      \"site_long_name\": \"travel-to-santorini.com\",\n      \"language\": \"en\",\n      \"is_safe\": true,\n      \"favicon\": \"https://t1.gstatic.com/faviconV2?client=SOCIAL&type=FAVICON&fallback_opts=TYPE,SIZE,URL&url=http://travel-to-santorini.com&size=96\"\n    }\n  ]\n}\n*/\n```\n\n----------------------------------------\n\nTITLE: Instantiating SelfQueryRetriever with HNSWLib\nDESCRIPTION: Creates a SelfQueryRetriever instance using the HNSWLib vector store, LLM, and defined attribute information.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/hnswlib.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { SelfQueryRetriever } from \"langchain/retrievers/self_query\";\nimport { FunctionalTranslator } from \"@langchain/core/structured_query\";\n\nconst selfQueryRetriever = SelfQueryRetriever.fromLLM({\n  llm: llm,\n  vectorStore: vectorStore,\n  /** A short summary of what the document contents represent. */\n  documentContents: \"Brief summary of a movie\",\n  attributeInfo: attributeInfo,\n  /**\n   * We need to create a basic translator that translates the queries into a\n   * filter format that the vector store can understand. We provide a basic translator\n   * translator here, but you can create your own translator by extending BaseTranslator\n   * abstract class. Note that the vector store needs to support filtering on the metadata\n   * attributes you want to query on.\n   */\n  structuredQueryTranslator: new FunctionalTranslator(),\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for LangChain Expression Language\nDESCRIPTION: This snippet shows how to install the necessary npm packages for working with LangChain Expression Language, including Anthropic and Cohere integrations.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/parallel.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/anthropic @langchain/cohere @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Running Tests for AWS Package\nDESCRIPTION: Commands to run unit tests and integration tests for the @langchain/aws package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-aws/README.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn test\n$ yarn test:int\n```\n\n----------------------------------------\n\nTITLE: Creating Vector Stores and Retrievers\nDESCRIPTION: Sets up two separate Chroma vector stores with OpenAI embeddings for different data sources about Harrison and Ankush\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_multiple_retrievers.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport { Chroma } from \"@langchain/community/vectorstores/chroma\"\nimport { OpenAIEmbeddings } from \"@langchain/openai\"\nimport \"chromadb\";\n\nconst texts = [\"Harrison worked at Kensho\"]\nconst embeddings = new OpenAIEmbeddings({ model: \"text-embedding-3-small\" })\nconst vectorstore = await Chroma.fromTexts(texts, {}, embeddings, {\n  collectionName: \"harrison\"\n})\nconst retrieverHarrison = vectorstore.asRetriever(1)\n\nconst textsAnkush = [\"Ankush worked at Facebook\"]\nconst embeddingsAnkush = new OpenAIEmbeddings({ model: \"text-embedding-3-small\" })\nconst vectorstoreAnkush = await Chroma.fromTexts(textsAnkush, {}, embeddingsAnkush, {\n  collectionName: \"ankush\"\n})\nconst retrieverAnkush = vectorstoreAnkush.asRetriever(1)\n```\n\n----------------------------------------\n\nTITLE: Chaining ChatOllama with Prompt Template in Python\nDESCRIPTION: Python code showing how to chain ChatOllama with a prompt template for language translation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/ollama.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\"\n\nconst prompt = ChatPromptTemplate.fromMessages(\n    [\n        [\n            \"system\",\n            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n        ],\n        [\"human\", \"{input}\"],\n    ]\n)\n\nconst chain = prompt.pipe(llm);\nawait chain.invoke(\n    {\n        input_language: \"English\",\n        output_language: \"German\",\n        input: \"I love programming.\",\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Loading PDF as a Single Document\nDESCRIPTION: Creates a PDFLoader with the splitPages option set to false to load the entire PDF as a single document rather than splitting by pages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/pdf.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { PDFLoader } from \"@langchain/community/document_loaders/fs/pdf\";\n\nconst singleDocPerFileLoader = new PDFLoader(nike10kPdfPath, {\n  splitPages: false,\n});\n\nconst singleDoc = await singleDocPerFileLoader.load();\nconsole.log(singleDoc[0].pageContent.slice(0, 100))\n```\n\n----------------------------------------\n\nTITLE: Logging ChatOllama Response in Python\nDESCRIPTION: Python code to log the content of the AI message returned by ChatOllama.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/ollama.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(aiMsg.content)\n```\n\n----------------------------------------\n\nTITLE: Installing Astra DB TypeScript Client\nDESCRIPTION: Command to install the Astra DB TypeScript client via npm, which is required for connecting to an Astra DB database.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/astradb.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @datastax/astra-db-ts\n```\n\n----------------------------------------\n\nTITLE: Initializing a ChatOpenAI LLM Instance - JavaScript/TypeScript\nDESCRIPTION: This snippet imports and constructs an instance of ChatOpenAI from the '@langchain/openai' package, specifying the model 'gpt-4o-mini'. This LLM instance can later be used in chains with the DuckDuckGoSearch tool for conversational AI tasks. Requires '@langchain/openai' installed; no input is needed at instantiation—parameters are set via the constructor.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/duckduckgo_search.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\"\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o-mini\",\n})\n```\n\n----------------------------------------\n\nTITLE: Creating a React Agent with LangGraph in Python\nDESCRIPTION: This snippet demonstrates how to create a React agent using LangGraph's createReactAgent function. It initializes the agent with a language model and tools.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n\nconst agentExecutor = createReactAgent({ llm, tools })\n```\n\n----------------------------------------\n\nTITLE: Streaming Agent Execution (TypeScript)\nDESCRIPTION: Demonstrates how to run the created LangGraph agent using the `stream` method. It provides initial messages including a system prompt defining the agent's role and the user's query. The code then iterates through the streamed events, logging any tool calls made by the agent or the final content generated.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/exa_search.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nconst exampleQuery = \"Summarize for me a fascinating article about cats.\"\n\nconst events = await agentExecutor.stream(\n  { messages: [\n    [\n      \"system\",\n      `You are a web researcher who answers user questions by looking up information on the internet and retrieving contents of helpful documents. Cite your sources.`,\n    ],\n    [\"human\", exampleQuery],\n  ] },\n  { streamMode: \"values\", }\n)\n\nfor await (const event of events) {\n  const lastMsg = event.messages[event.messages.length - 1];\n  if (lastMsg.tool_calls?.length) {\n    console.dir(lastMsg.tool_calls, { depth: null });\n  } else if (lastMsg.content) {\n    console.log(lastMsg.content);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating and Invoking ChatPromptTemplate in LangChain.js\nDESCRIPTION: Creates a chat prompt template with a system message and a messages placeholder, then invokes it with a human message. This demonstrates the basic setup for chat-based interactions using LangChain.js.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/t.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport {\n  ChatPromptTemplate,\n  MessagesPlaceholder,\n} from \"@langchain/core/prompts\";\nimport { HumanMessage } from \"@langchain/core/messages\";\n\nconst promptTemplate = ChatPromptTemplate.fromMessages([\n  [\"system\", \"You are a helpful assistant\"],\n  new MessagesPlaceholder(\"msgs\"),\n]);\n\nawait promptTemplate.invoke({ msgs: [new HumanMessage(\"hi!\")] });\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for LangChain.js\nDESCRIPTION: This snippet shows how to install the necessary npm packages for using LangChain.js with OpenAI and community extensions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/example_selectors_similarity.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Setting Unstructured API Environment Variables\nDESCRIPTION: These commands set the necessary environment variables for the Unstructured API, including the API key and URL.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/document_loader_html.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport UNSTRUCTURED_API_KEY=\"...\"\n# Replace with your `Full URL` from the email\nexport UNSTRUCTURED_API_URL=\"https://<ORG_NAME>-<SECRET>.api.unstructuredapp.io/general/v0/general\"\n```\n\n----------------------------------------\n\nTITLE: Rendering IndexTable Component in React/JSX\nDESCRIPTION: Renders the IndexTable component that displays a comprehensive list of all available tools in the LangChain JS framework.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/index.mdx#2025-04-22_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\n<IndexTable />\n```\n\n----------------------------------------\n\nTITLE: Caching Embeddings with CacheBackedEmbeddings in JavaScript\nDESCRIPTION: This snippet explains how to cache embedding results using CacheBackedEmbeddings to avoid recomputing embeddings for the same text. It covers implementation with in-memory storage and demonstrates how to set up namespaces to avoid collisions between different embedding models.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/static/llms.txt#2025-04-22_snippet_5\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { CacheBackedEmbeddings } from \"langchain/embeddings/cache_backed\";\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\nimport { InMemoryStore } from \"langchain/storage/in_memory\";\n\nconst underlyingEmbeddings = new OpenAIEmbeddings();\nconst store = new InMemoryStore();\n\nconst cacheBackedEmbeddings = CacheBackedEmbeddings.fromBytesStore(\n  underlyingEmbeddings,\n  store,\n  { namespace: underlyingEmbeddings.modelName }\n);\n\nconst result = await cacheBackedEmbeddings.embedQuery(\"Hello world\");\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Community and Core Packages for Raycast Integration\nDESCRIPTION: Command to install the required LangChain packages for integrating with Raycast AI.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/raycast.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Testing Memory with Trimmed Messages in LangGraph\nDESCRIPTION: Tests the trimmed message functionality by invoking the second LangGraph app with the same sample history, showing how trimming affects the model's ability to recall previously shared information.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_memory.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nawait app2.invoke(\n  {\n    messages: [\n      ...demoEphemeralChatHistory,\n      { role: \"user\", content: \"What is my name?\" }\n    ]\n  },\n  {\n    configurable: { thread_id: \"3\" }\n  }\n);\n```\n\n----------------------------------------\n\nTITLE: Implementing Request-Based Rate Limiting in TypeScript\nDESCRIPTION: Set up request-based rate limiting using Upstash Ratelimit and integrate it with a LangChain.js chain. This example limits to 10 requests per minute.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/callbacks/upstash_ratelimit_callback.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst UPSTASH_REDIS_REST_URL = \"****\";\nconst UPSTASH_REDIS_REST_TOKEN = \"****\";\n\nimport {\n  UpstashRatelimitHandler,\n  UpstashRatelimitError,\n} from \"@langchain/community/callbacks/handlers/upstash_ratelimit\";\nimport { RunnableLambda } from \"@langchain/core/runnables\";\nimport { Ratelimit } from \"@upstash/ratelimit\";\nimport { Redis } from \"@upstash/redis\";\n\n// create ratelimit\nconst ratelimit = new Ratelimit({\n  redis: new Redis({\n    url: UPSTASH_REDIS_REST_URL,\n    token: UPSTASH_REDIS_REST_TOKEN,\n  }),\n  // 10 requests per window, where window size is 60 seconds:\n  limiter: Ratelimit.fixedWindow(10, \"60 s\"),\n});\n\n// create handler\nconst user_id = \"user_id\"; // should be a method which gets the user id\nconst handler = new UpstashRatelimitHandler(user_id, {\n  requestRatelimit: ratelimit,\n});\n\n// create mock chain\nconst chain = new RunnableLambda({ func: (str: string): string => str });\n\ntry {\n  const response = await chain.invoke(\"hello world\", {\n    callbacks: [handler],\n  });\n  console.log(response);\n} catch (err) {\n  if (err instanceof UpstashRatelimitError) {\n    console.log(\"Handling ratelimit.\");\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Chaining with ChatPromptTemplate\nDESCRIPTION: Example of chaining ChatVertexAI with a prompt template for language translation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/google_vertex_ai.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\"\n\nconst prompt = ChatPromptTemplate.fromMessages(\n    [\n        [\n            \"system\",\n            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n        ],\n        [\"human\", \"{input}\"],\n    ]\n)\n\nconst chain = prompt.pipe(llm);\nawait chain.invoke(\n  {\n    input_language: \"English\",\n    output_language: \"German\",\n    input: \"I love programming.\",\n  }\n);\n```\n\n----------------------------------------\n\nTITLE: Using AzureCosmosDBNoSQLVectorStore in TypeScript\nDESCRIPTION: Example demonstrating how to create a vector store from documents using the AzureCosmosDBNoSQLVectorStore and perform a similarity search. The code initializes a store with documents, embeddings, and configuration for database and container names.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-azure-cosmosdb/README.md#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AzureCosmosDBNoSQLVectorStore } from \"@langchain/azure-cosmosdb\";\n\nconst store = await AzureCosmosDBNoSQLVectorStore.fromDocuments(\n  [\"Hello, World!\"],\n  new OpenAIEmbeddings(),\n  {\n    databaseName: \"langchain\",\n    containerName: \"documents\",\n  }\n);\n\nconst resultDocuments = await store.similaritySearch(\"hello\");\nconsole.log(resultDocuments[0].pageContent);\n```\n\n----------------------------------------\n\nTITLE: Configuring ChatOpenAI Model\nDESCRIPTION: Initializes a ChatOpenAI instance with GPT-3.5-turbo model and zero temperature for consistent outputs\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_high_cardinality.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatOpenAI } from '@langchain/openai';\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-3.5-turbo\",\n  temperature: 0,\n})\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for HuggingFaceInference in LangChain\nDESCRIPTION: This snippet shows the command to install the required packages for using HuggingFaceInference in a LangChain project. It includes @langchain/community, @langchain/core, and @huggingface/inference.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/huggingface_inference.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core @huggingface/inference@2\n```\n\n----------------------------------------\n\nTITLE: Creating Self-Query Retriever Instance\nDESCRIPTION: Instantiating a self-query retriever with Vectara translator for metadata-aware document retrieval.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/vectara.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { SelfQueryRetriever } from \"langchain/retrievers/self_query\";\nimport { VectaraTranslator } from \"@langchain/community/structured_query/vectara\";\n\nconst selfQueryRetriever = SelfQueryRetriever.fromLLM({\n  llm: llm,\n  vectorStore: vectorStore,\n  documentContents: \"Brief summary of a movie\",\n  attributeInfo: attributeInfo,\n  structuredQueryTranslator: new VectaraTranslator(),\n});\n```\n\n----------------------------------------\n\nTITLE: Generating Multiple Document Embeddings\nDESCRIPTION: Shows how to generate embeddings for multiple documents using the embedDocuments method with automatic batching.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/mixedbread_ai.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst documents = [\"Baking bread is fun\", \"I love baking\"];\n\nconst embeddingsArray = await embeddings.embedDocuments(documents);\nconsole.log(embeddingsArray);\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for API Keys\nDESCRIPTION: Sets the API key for the chat model provider and optionally for LangSmith tracing.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/chat.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport __env_var_name__=\"your-api-key\"\n\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Installing Development Dependencies\nDESCRIPTION: Command to install the necessary dependencies for developing the @langchain/google-genai package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-google-genai/README.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nyarn install\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain.js Dependencies\nDESCRIPTION: This command installs the necessary LangChain.js packages for working with OpenAI and community integrations.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/mongodb.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Zep Cloud and LangChain Dependencies\nDESCRIPTION: This snippet shows the npm command to install Zep Cloud, OpenAI, and LangChain community packages required for using Zep Cloud Vector Store with LangChain.js.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/zep_cloud.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @getzep/zep-cloud @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing @langchain/qdrant Package\nDESCRIPTION: Command to install the @langchain/qdrant package via npm, which provides integration between LangChain.js and the Qdrant vector database.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-qdrant/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/qdrant\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for LangChain.js and OpenAI\nDESCRIPTION: This code block sets up environment variables for OpenAI API key and optional LangSmith tracing.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_no_queries.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nOPENAI_API_KEY=your-api-key\n\n# Optional, use LangSmith for best-in-class observability\nLANGSMITH_API_KEY=your-api-key\nLANGSMITH_TRACING=true\n\n# Reduce tracing latency if you are not in a serverless environment\n# LANGCHAIN_CALLBACKS_BACKGROUND=true\n```\n\n----------------------------------------\n\nTITLE: Representing Completed Tool Call in LangchainJS AIMessageChunk (JSON)\nDESCRIPTION: This JSON object, found within the `tool_calls` array of an `AIMessageChunk`, represents a complete tool call requested by the AI model. It contains the name of the tool (`tavily_search_results_json`), the parsed arguments passed to the tool as a nested JSON object (containing the `input` parameter), and the unique ID (`toolu_01NUVejujVo2y8WGVtZ49KAN`) linking it to the specific tool invocation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n                  \"name\": \"tavily_search_results_json\",\n                  \"args\": {\n                    \"input\": \"Oppenheimer 2023 film director age\"\n                  },\n                  \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\"\n                }\n```\n\n----------------------------------------\n\nTITLE: Initializing WatsonxLLM with IBM watsonx.ai Software Authentication in TypeScript\nDESCRIPTION: Creates a new instance of WatsonxLLM using IBM watsonx.ai software authentication method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_compressors/ibm.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { WatsonxLLM } from \"@langchain/community/llms/ibm\";\n\nconst props = {\n  version: \"YYYY-MM-DD\",\n  serviceUrl: \"<SERVICE_URL>\",\n  projectId: \"<PROJECT_ID>\",\n  watsonxAIAuthType: \"cp4d\",\n  watsonxAIUsername: \"<YOUR-USERNAME>\",\n  watsonxAIPassword: \"<YOUR-PASSWORD>\",\n  watsonxAIUrl: \"<url>\",\n};\nconst instance = new WatsonxLLM(props);\n```\n\n----------------------------------------\n\nTITLE: Logging Document Metadata\nDESCRIPTION: Shows how to access and log metadata from loaded documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/recursive_url_loader.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(docs[0].metadata)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for LangChain PDF Processing\nDESCRIPTION: This code snippet shows how to install the necessary packages (@langchain/community and pdf-parse) for working with PDFs in LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/retrievers.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n@langchain/community pdf-parse\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatOpenAI Model in JavaScript\nDESCRIPTION: This snippet demonstrates how to initialize a ChatOpenAI model with specific parameters like model name and temperature.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/sequence.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ChatOpenAI } from '@langchain/openai';\n\nconst model = new ChatOpenAI({\n  model: \"gpt-4o\",\n  temperature: 0,\n})\n```\n\n----------------------------------------\n\nTITLE: Resolving @langchain/core Version with Yarn\nDESCRIPTION: Example package.json configuration to ensure a single version of @langchain/core is used with yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/installation.mdx#2025-04-22_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"your-project\",\n  \"version\": \"0.0.0\",\n  \"private\": true,\n  \"engines\": {\n    \"node\": \">=18\"\n  },\n  \"dependencies\": {\n    \"@langchain/anthropic\": \"^0.0.2\",\n    \"@langchain/core\": \"^0.3.0\",\n    \"langchain\": \"0.0.207\"\n  },\n  \"resolutions\": {\n    \"@langchain/core\": \"0.3.0\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Deleting Multiple Keys\nDESCRIPTION: Shows how to delete multiple keys from the store using the mdelete method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/stores/file_system.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nawait kvStore.mdelete(\n  [\n    \"key1\",\n    \"key2\",\n  ]\n)\n\nawait kvStore.mget(\n  [\n    \"key1\",\n    \"key2\",\n  ]\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Momento SDK for Web/Edge Environments\nDESCRIPTION: Installs the Momento SDK package tailored for browser or edge computing environments using npm. Use this package when integrating MVI within client-side JavaScript or edge functions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/momento_vector_index.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @gomomento/sdk-web\n```\n\n----------------------------------------\n\nTITLE: Loading Documents with TextLoader\nDESCRIPTION: Code example demonstrating how to load documents using the TextLoader and access the first document.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/text.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst docs = await loader.load()\ndocs[0]\n```\n\n----------------------------------------\n\nTITLE: Displaying document metadata from loaded documents\nDESCRIPTION: Code to display the metadata of the first document loaded by UnstructuredLoader. This shows the extracted metadata properties from the processed document.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/unstructured.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(docs[0].metadata)\n```\n\n----------------------------------------\n\nTITLE: Configuring Max Retries for Language Models in LangChain.js\nDESCRIPTION: This code snippet demonstrates how to set a higher number of max retries when initializing a language model in LangChain.js. This can help mitigate rate limit errors by using an exponential backoff strategy for failed requests.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/troubleshooting/errors/MODEL_RATE_LIMIT.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nmaxRetries\n```\n\n----------------------------------------\n\nTITLE: Using Google Generative AI Embeddings\nDESCRIPTION: TypeScript code showing how to use Google's embedding models for generating vector embeddings with task-specific configurations.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-google-genai/README.md#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { GoogleGenerativeAIEmbeddings } from \"@langchain/google-genai\";\nimport { TaskType } from \"@google/generative-ai\";\n\nconst embeddings = new GoogleGenerativeAIEmbeddings({\n  modelName: \"embedding-001\", // 768 dimensions\n  taskType: TaskType.RETRIEVAL_DOCUMENT,\n  title: \"Document title\",\n});\n\nconst res = await embeddings.embedQuery(\"OK Google\");\n```\n\n----------------------------------------\n\nTITLE: Testing Agent with Retrieval Query in Python\nDESCRIPTION: This snippet demonstrates the agent's behavior when given a query that requires retrieval, showing how it generates an appropriate input for the tool.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nconst query2 = \"What is Task Decomposition?\"\n\nfor await (const s of await agentExecutor2.stream({ messages: [{ role: \"user\", content: query2 }] }, config3)) {\n  console.log(s)\n  console.log(\"----\")\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatOpenAI Model\nDESCRIPTION: Example showing how to initialize a ChatOpenAI model with specific parameters\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/tavily.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o-mini\",\n  temperature: 0,\n});\n```\n\n----------------------------------------\n\nTITLE: Splitting Documents with RecursiveCharacterTextSplitter\nDESCRIPTION: This code shows how to use the RecursiveCharacterTextSplitter to split loaded documents into smaller chunks for better processing and retrieval.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/retrievers.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { RecursiveCharacterTextSplitter } from \"@langchain/textsplitters\";\n\nconst textSplitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 1000,\n  chunkOverlap: 200,\n});\n\nconst allSplits = await textSplitter.splitDocuments(docs)\n\nallSplits.length\n```\n\n----------------------------------------\n\nTITLE: Importing OpenAI Integration in Next.js\nDESCRIPTION: Example of importing the ChatOpenAI class for use in Vercel/Next.js environments, including frontend components, Serverless functions, and Edge functions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/tests/__mdx__/modules/two.mdx#2025-04-22_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n```\n\n----------------------------------------\n\nTITLE: Streaming Events from a JSON Parser Chain\nDESCRIPTION: Example of creating a chain with a model and JSON output parser, then streaming and collecting all events from the chain execution. This demonstrates how events are generated at multiple levels in the chain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nconst chain = model.pipe(new JsonOutputParser());\nconst eventStream = await chain.streamEvents(\n  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key \"name\" and \"population\"`,\n  { version: \"v2\" },\n);\n\n\nconst events = [];\nfor await (const event of eventStream) {\n  events.push(event);\n}\n\nconsole.log(events.length)\n```\n\n----------------------------------------\n\nTITLE: Server Action Imports Setup\nDESCRIPTION: Initial imports for the server action file including LangChain and AI SDK components\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/stream_agent_client.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n\"use server\";\n\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { TavilySearchResults } from \"@langchain/community/tools/tavily_search\";\nimport { AgentExecutor, createToolCallingAgent } from \"langchain/agents\";\nimport { pull } from \"langchain/hub\";\nimport { createStreamableValue } from \"ai/rsc\";\n```\n\n----------------------------------------\n\nTITLE: Using OpenAI Embeddings\nDESCRIPTION: Example of using OpenAI's embeddings model to generate embeddings for text.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-openai/README.md#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddings = new OpenAIEmbeddings({\n  apiKey: process.env.OPENAI_API_KEY,\n});\nconst res = await embeddings.embedQuery(\"Hello world\");\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages (Shell)\nDESCRIPTION: Installs the necessary Node.js packages using npm (or yarn) for MongoDB Atlas integration with LangchainJS, including the MongoDB driver, OpenAI embeddings, and core Langchain functionality. These packages provide the classes and methods needed to interact with MongoDB and OpenAI.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/mongodb_atlas.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n@langchain/mongodb mongodb @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Building the Package\nDESCRIPTION: Commands for building the package, both locally and from repo root.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/create-langchain-integration/template/README.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nyarn build\n\n# Or from the repo root:\nyarn build --filter=@langchain/<ADD_PACKAGE_NAME_HERE>\n```\n\n----------------------------------------\n\nTITLE: Importing OpenAIEmbeddings Model in TypeScript\nDESCRIPTION: This code shows how to import the OpenAIEmbeddings class for text embeddings. This class is used to generate vector embeddings from text inputs using OpenAI's embedding models.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/openai.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n```\n\n----------------------------------------\n\nTITLE: Installing Neon Database and LangChain Packages with npm\nDESCRIPTION: This snippet shows the npm command to install the `@neondatabase/serverless` package, a JavaScript/TypeScript driver necessary for connecting applications to the Neon database. It also installs `@langchain/community` and `@langchain/core` packages, which are required dependencies for using LangChain.js.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/neon.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @neondatabase/serverless\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Setting MongoDB Connection Environment Variables (TypeScript)\nDESCRIPTION: Assigns MongoDB Atlas connection details (URI, database name, collection name) to environment variables within a Node.js application using TypeScript syntax. These variables are typically read during application startup to configure the database connection.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/mongodb_atlas.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.MONGODB_ATLAS_URI = \"your-atlas-url\";\nprocess.env.MONGODB_ATLAS_COLLECTION_NAME = \"your-atlas-db-name\";\nprocess.env.MONGODB_ATLAS_DB_NAME = \"your-atlas-db-name\";\n```\n\n----------------------------------------\n\nTITLE: Initializing MixedbreadAI Embeddings\nDESCRIPTION: Creates an instance of MixedbreadAIEmbeddings with an API key and optional model specification.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/mixedbread_ai.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { MixedbreadAIEmbeddings } from \"@langchain/mixedbread-ai\";\n\nconst embeddings = new MixedbreadAIEmbeddings({\n  apiKey: \"YOUR_API_KEY\",\n  // Optionally specify model\n  // model: \"mixedbread-ai/mxbai-embed-large-v1\",\n});\n```\n\n----------------------------------------\n\nTITLE: Performing Complex Filtered Similarity Search (Allow/Deny) with Matching Engine (TypeScript)\nDESCRIPTION: Illustrates a more complex filtered similarity search using multiple `Restriction` objects. This query searches for documents similar to \"this\" that have 'color' metadata as 'red' *but* excludes documents where the 'category' metadata is 'edible' (using the `denyList`). It retrieves up to 4 matching documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/googlevertexai.mdx#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst filter: Restriction[] = [\n  {\n    namespace: \"color\",\n    allowList: [\"red\"],\n  },\n  {\n    namespace: \"category\",\n    denyList: [\"edible\"],\n  },\n];\nconst results = await engine.similaritySearch(\"this\", 4, filter);\n```\n\n----------------------------------------\n\nTITLE: Error Handling for MixedbreadAI Initialization\nDESCRIPTION: Demonstrates proper error handling when initializing MixedbreadAI embeddings without required API credentials.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/mixedbread_ai.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\ntry {\n  const embeddings = new MixedbreadAIEmbeddings();\n} catch (error) {\n  console.error(error);\n}\n```\n\n----------------------------------------\n\nTITLE: Using Custom Headers with ChatAnthropic in Python\nDESCRIPTION: Shows how to pass custom headers in requests to the ChatAnthropic model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/anthropic.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst llmWithCustomHeaders = new ChatAnthropic({\n  model: \"claude-3-sonnet-20240229\",\n  maxTokens: 1024,\n  clientOptions: {\n    defaultHeaders: {\n      \"X-Api-Key\": process.env.ANTHROPIC_API_KEY,\n    },\n  },\n});\n\nawait llmWithCustomHeaders.invoke(\"Why is the sky blue?\");\n```\n\n----------------------------------------\n\nTITLE: Loading Documents with RecursiveUrlLoader\nDESCRIPTION: Demonstrates how to load documents using the configured RecursiveUrlLoader instance.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/recursive_url_loader.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst docs = await loader.load()\ndocs[0]\n```\n\n----------------------------------------\n\nTITLE: Defining Person Schema with Zod\nDESCRIPTION: Implementation of a Zod schema for extracting personal information including name, hair color, and height.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/extraction.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { z } from \"zod\";\n\nconst personSchema = z.object({\n  name: z.optional(z.string()).describe(\"The name of the person\"),\n  hair_color: z.optional(z.string()).describe(\"The color of the person's hair if known\"),\n  height_in_meters: z.optional(z.string()).describe('Height measured in meters'),\n});\n```\n\n----------------------------------------\n\nTITLE: Streaming Events from a Chat Model in LangChain.js\nDESCRIPTION: Code that demonstrates how to stream events from a chat model and collect them in an array. This example uses the beta v2 API for event streaming, which requires specifying the version parameter.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\nconst events = [];\n\nconst eventStream = await model.streamEvents(\"hello\", { version: \"v2\" });\n\nfor await (const event of eventStream) {\n  events.push(event);\n}\n\nconsole.log(events.length)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for JSON Agent Toolkit\nDESCRIPTION: This snippet shows how to install the necessary npm packages for the JSON Agent Toolkit example.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/json.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Invoking a Model with Tool Calling Capability\nDESCRIPTION: Shows how to invoke a model that has tools bound to it with an unrelated prompt. In this case, the model would respond with natural language without calling any tools.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/tool_calling.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst result = await llmWithTools.invoke(\"Hello world!\");\n```\n\n----------------------------------------\n\nTITLE: Creating Chat Prompt Template\nDESCRIPTION: Defines a system prompt template for the conversational agent.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_tools.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport {\n  ChatPromptTemplate,\n} from \"@langchain/core/prompts\";\n\nconst prompt = ChatPromptTemplate.fromMessages([\n  [\n    \"system\",\n    \"You are a helpful assistant. You may not need to use tools for every query - the user may just want to chat!\",\n  ],\n]);\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Puppeteer Integration\nDESCRIPTION: Command to install the necessary dependencies for using PuppeteerWebBaseLoader including the @langchain/community package and puppeteer.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/web_puppeteer.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain and OpenAI Packages - Bash\nDESCRIPTION: Installs the @langchain/openai and @langchain/core packages using npm. These packages are required for integrating OpenAI services and LangChain abstractions in a TypeScript environment. Ensure Node.js and npm are pre-installed before running this command.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/aiplugin-tool.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: SQL Agent Tools Setup and Configuration\nDESCRIPTION: Initializes SQL toolkit and retrieves available tools for database operations.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/sql_qa.ipynb#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport { SqlToolkit } from \"langchain/agents/toolkits/sql\";\n\nconst toolkit = new SqlToolkit(db, llm);\n\nconst tools = toolkit.getTools();\n\nconsole.log(\n  tools.map((tool) => ({\n    name: tool.name,\n    description: tool.description,\n  }))\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Documents from Figma in TypeScript with LangChainJS\nDESCRIPTION: Code for loading document data from a Figma file using the Figma API. It demonstrates how to authenticate with a personal access token, specify a file key and node ID, and extract text content from Figma documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/figma.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { FigmaFileLoader } from \"langchain/document_loaders/web/figma\";\n\n// Figma API access token\nconst accessToken = \"<YOUR ACCESS TOKEN>\";\n\n// Figma file key\nconst fileKey = \"<YOUR FILE KEY>\";\n\n// Figma node ID\nconst nodeIds = [\"<YOUR NODE ID>\"];\n\nconst loader = new FigmaFileLoader({ accessToken, fileKey, nodeIds });\n\nconst docs = await loader.load();\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Community\nDESCRIPTION: Command to install LangChain Community and Core packages using npm or yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/layerup_security.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Advanced Filtering with Function Calls and Multiple Bind Values\nDESCRIPTION: Illustrates an advanced filtering technique using a function call (`GEO_DISTANCE`) on the left side of the operator. The `name` includes placeholders (`?`), and the `value` is an array containing bind values for both the left-side function and the right side of the operator. This example filters based on geographical distance.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/cassandra.mdx#2025-04-22_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\n{\n  name: \"GEO_DISTANCE(coord, ?)\",\n  operator: \"<\",\n  value: [new Float32Array([53.3730617,-6.3000515]), 10000],\n},\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Cloudflare Integration Packages\nDESCRIPTION: This bash command installs the necessary LangChain packages for Cloudflare integration via npm or yarn to enable vectorization functionalities.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/cloudflare_vectorize.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/cloudflare @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Viewing LangchainJS AgentExecutor Chain Output in JSON\nDESCRIPTION: This JSON object details the output of a LangchainJS AgentExecutor chain run. It includes the initial input query, the sequence of steps executed (including tool actions, their inputs, and outputs represented as nested JSON), and associated metadata like detailed message logs and response information from the underlying models or tools. It relies on the LangchainJS framework's structure for tracing agent execution.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"input\": \"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\",\n  \"steps\": [\n    {\n      \"action\": {\n        \"tool\": \"tavily_search_results_json\",\n        \"toolInput\": {\n          \"input\": \"Oppenheimer 2023 film director age\"\n        },\n        \"toolCallId\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n        \"log\": \"Invoking \\\"tavily_search_results_json\\\" with {\\\"input\\\":\\\"Oppenheimer 2023 film director age\\\"}\\n[{\\\"type\\\":\\\"tool_use\\\",\\\"id\\\":\\\"toolu_01NUVejujVo2y8WGVtZ49KAN\\\",\\\"name\\\":\\\"tavily_search_results_json\\\",\\\"input\\\":{\\\"input\\\":\\\"Oppenheimer 2023 film director age\\\"}}]\",\n        \"messageLog\": [\n          {\n            \"lc\": 1,\n            \"type\": \"constructor\",\n            \"id\": [\n              \"langchain_core\",\n              \"messages\",\n              \"AIMessageChunk\"\n            ],\n            \"kwargs\": {\n              \"content\": [\n                {\n                  \"type\": \"tool_use\",\n                  \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n                  \"name\": \"tavily_search_results_json\",\n                  \"input\": {\n                    \"input\": \"Oppenheimer 2023 film director age\"\n                  }\n                }\n              ],\n              \"additional_kwargs\": {\n                \"id\": \"msg_015MqAHr84dBCAqBgjou41Km\",\n                \"type\": \"message\",\n                \"role\": \"assistant\",\n                \"model\": \"claude-3-sonnet-20240229\",\n                \"stop_sequence\": null,\n                \"usage\": {\n                  \"input_tokens\": 409,\n                  \"output_tokens\": 68\n                },\n                \"stop_reason\": \"tool_use\"\n              },\n              \"tool_call_chunks\": [\n                {\n                  \"name\": \"tavily_search_results_json\",\n                  \"args\": \"{\\\"input\\\":\\\"Oppenheimer 2023 film director age\\\"}\",\n                  \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n                  \"index\": 0\n                }\n              ],\n              \"tool_calls\": [\n                {\n                  \"name\": \"tavily_search_results_json\",\n                  \"args\": {\n                    \"input\": \"Oppenheimer 2023 film director age\"\n                  },\n                  \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\"\n                }\n              ],\n              \"invalid_tool_calls\": [],\n              \"response_metadata\": {}\n            }\n          }\n        ]\n      },\n      \"observation\": {\n        \"lc\": 1,\n        \"type\": \"constructor\",\n        \"id\": [\n          \"langchain_core\",\n          \"messages\",\n          \"ToolMessage\"\n        ],\n        \"kwargs\": {\n          \"tool_call_id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n          \"content\": \"[{\\\"url\\\": \\\"https://www.everythingzoomer.com/arts-entertainment/2023/08/15/oppenheimer-director-christopher-nolan-on-filmmaking-at-53-i-try-to-challenge-myself-with-every-film/\\\", \\\"content\\\": \\\"Marriage of the Summer\\\\nBlast From the Past: ‘Asteroid City’ & ‘Oppenheimer’ and the Age of Nuclear Anxiety\\\\nEXPLORE\\u00a0 HealthMoneyTravelFoodStyleBook ClubClassifieds#ZoomerDailyPolicy & PerspectiveArts & EntertainmentStars & RoyaltySex & Love\\\\nCONNECT\\u00a0 FacebookTwitterInstagram\\\\nSUBSCRIBE\\u00a0 Terms of Subscription ServiceE-NewslettersSubscribe to Zoomer Magazine\\\\nBROWSE\\u00a0 AboutMastheadContact UsAdvertise with UsPrivacy Policy\\\\nEverythingZoomer.com is part of the ZoomerMedia Digital Network \\\\\\\"I think with experience \\u2014 and with the experience of watching your films with an audience over the years \\u2014 you do more and more recognize the human elements that people respond to, and the things that move you and the things that move the audience.\\\\\\\"\\\\n \\\\\\\"What\\u2019s interesting, as you watch the films over time, is that some of his preoccupations are the same, but then some of them have changed over time with who he is as a person and what\\u2019s going on in his own life,\\\\\\\" Thomas said.\\\\n The British-American director\\u2019s latest explosive drama, Oppenheimer, which has earned upwards of US$940 million at the global box office, follows theoretical physicist J. Robert Oppenheimer (played by Cillian Murphy) as he leads the team creating the first atomic bomb, as director of the Manhattan Project\\u2019s Los Alamos Laboratory.\\\\n Subscribe\\\\nEverything Zoomer\\\\n‘Oppenheimer’ Director Christopher Nolan On Filmmaking at 53: \\\\\\\"I Try to Challenge Myself with Every Film\\\\\\\"\\\\nDirector Christopher Nolan poses upon his arrival for the premiere of the movie 'Oppenheimer' in Paris on July 11, 2023.\\\",\\\"score\\\":0.92002,\\\"raw_content\\\":null},{\\\"title\\\":\\\"'Oppenheimer' Review: A Man for Our Time - The New York Times\\\",\\\"url\\\":\\\"https://www.nytimes.com/2023/07/19/movies/oppenheimer-review-christopher-nolan.html\\\",\\\"content\\\":\\\"Instead, it is here that the film\\u2019s complexities and all its many fragments finally converge as Nolan puts the finishing touches on his portrait of a man who contributed to an age of transformational scientific discovery, who personified the intersection of science and politics, including in his role as a Communist boogeyman, who was transformed by his role in the creation of weapons of mass destruction and soon after raised the alarm about the dangers of nuclear war.\\\\n He served as director of a clandestine weapons lab built in a near-desolate stretch of Los Alamos, in New Mexico, where he and many other of the era\\u2019s most dazzling scientific minds puzzled through how to harness nuclear reactions for the weapons that killed tens of thousands instantly, ending the war in the Pacific.\\\\n Nolan integrates these black-and-white sections with the color ones, using scenes from the hearing and the confirmation \\u2014 Strauss\\u2019s role in the hearing and his relationship with Oppenheimer directly affected the confirmation\\u2019s outcome \\u2014 to create a dialectical synthesis. To signal his conceit, he stamps the film with the words \\\\\\\"fission\\\\\\\" (a splitting into parts) and \\\\\\\"fusion\\\\\\\" (a merging of elements); Nolan being Nolan, he further complicates the film by recurrently kinking up the overarching chronology \\u2014 it is a lot.\\\\n It\\u2019s also at Berkeley that Oppenheimer meets the project\\u2019s military head, Leslie Groves (a predictably good Damon), who makes him Los Alamos\\u2019s director, despite the leftist causes he supported \\u2014 among them, the fight against fascism during the Spanish Civil War \\u2014 and some of his associations, including with Communist Party members like his brother, Frank (Dylan Arnold).\\\\n\\\",\\\"score\\\":0.91831,\\\"raw_content\\\":null}]\",\n          \"additional_kwargs\": {\n            \"name\": \"tavily_search_results_json\"\n          },\n          \"response_metadata\": {}\n        }\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for RedisVectorStore Configuration in Node.js\nDESCRIPTION: To configure the RedisVectorStore, it's necessary to set the REDIS_URL and potentially the OpenAI API key as environment variables. This setup ensures that the appropriate services are initialized with proper credentials.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/redis.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.REDIS_URL = \"your-redis-url\"\n```\n\n----------------------------------------\n\nTITLE: Setting up LangSmith Environment Variables (Optional)\nDESCRIPTION: Shows how to set up optional LangSmith API key and tracing for monitoring model calls.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/cohere.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Chaining CloudflareWorkersAI with Prompt Templates\nDESCRIPTION: Implementation of a processing chain that combines a prompt template with the CloudflareWorkersAI model. This demonstrates how to create more complex workflows by piping the output of a templated prompt to the model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/cloudflare_workersai.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PromptTemplate } from \"@langchain/core/prompts\"\n\nconst prompt = PromptTemplate.fromTemplate(\"How to say {input} in {output_language}:\\n\")\n\nconst chain = prompt.pipe(llm);\nawait chain.invoke(\n  {\n    output_language: \"German\",\n    input: \"I love programming.\",\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: Custom Token Counter Implementation\nDESCRIPTION: Shows how to implement a custom token counter using tiktoken encoding, handling different message types and content formats.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/trim_messages.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { encodingForModel } from '@langchain/core/utils/tiktoken';\nimport { BaseMessage, HumanMessage, AIMessage, ToolMessage, SystemMessage, MessageContent, MessageContentText } from '@langchain/core/messages';\n\nasync function strTokenCounter(messageContent: MessageContent): Promise<number> {\n    if (typeof messageContent === 'string') {\n        return (\n            await encodingForModel(\"gpt-4\")\n          ).encode(messageContent).length;\n    } else {\n        if (messageContent.every((x) => x.type === \"text\" && x.text)) {\n            return (\n                await encodingForModel(\"gpt-4\")\n              ).encode((messageContent as MessageContentText[]).map(({ text }) => text).join(\"\")).length;\n        }\n        throw new Error(`Unsupported message content ${JSON.stringify(messageContent)}`);\n    }\n}\n\nasync function tiktokenCounter(messages: BaseMessage[]): Promise<number> {\n  let numTokens = 3;\n  const tokensPerMessage = 3;\n  const tokensPerName = 1;\n\n  for (const msg of messages) {\n    let role: string;\n    if (msg instanceof HumanMessage) {\n      role = 'user';\n    } else if (msg instanceof AIMessage) {\n      role = 'assistant';\n    } else if (msg instanceof ToolMessage) {\n      role = 'tool';\n    } else if (msg instanceof SystemMessage) {\n      role = 'system';\n    } else {\n      throw new Error(`Unsupported message type ${msg.constructor.name}`);\n    }\n\n    numTokens += tokensPerMessage + (await strTokenCounter(role)) + (await strTokenCounter(msg.content));\n\n    if (msg.name) {\n      numTokens += tokensPerName + (await strTokenCounter(msg.name));\n    }\n  }\n\n  return numTokens;\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Neo4j Graph Connection in JavaScript\nDESCRIPTION: This code snippet shows how to create a connection to a Neo4j database using environment variables for authentication.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/graph_constructing.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport \"neo4j-driver\";\nimport { Neo4jGraph } from \"@langchain/community/graphs/neo4j_graph\";\n\nconst url = process.env.NEO4J_URI;\nconst username = process.env.NEO4J_USER;\nconst password = process.env.NEO4J_PASSWORD;\nconst graph = await Neo4jGraph.initialize({ url, username, password });\n```\n\n----------------------------------------\n\nTITLE: Using HtmlToTextTransformer for Hacker News Thread Scraping\nDESCRIPTION: Example of scraping a Hacker News thread, splitting it based on HTML tags, and extracting content from individual chunks using HtmlToTextTransformer.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_transformers/html-to-text.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/document_transformers/html_to_text.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Setting up JigsawStack API Key in Environment Variables\nDESCRIPTION: Command to export the JigsawStack API key as an environment variable for authentication with the JigsawStack service.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/jigsawstack.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport JIGSAWSTACK_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Accessing Document Metadata\nDESCRIPTION: Example showing how to access metadata of the loaded documents\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/directory.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(docs[0].metadata)\n```\n\n----------------------------------------\n\nTITLE: Instantiating ArxivRetriever in TypeScript\nDESCRIPTION: Creates a new instance of ArxivRetriever with configuration options for document retrieval settings. The example sets up the retriever to fetch summaries rather than full documents and limits results to 5 articles.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/arxiv-retriever.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst retriever = new ArxivRetriever({\n  getFullDocuments: false, // Set to true to fetch full documents (PDFs)\n  maxSearchResults: 5, // Maximum number of results to retrieve\n});\n```\n\n----------------------------------------\n\nTITLE: Instantiating OpenAI Embeddings\nDESCRIPTION: Creating an OpenAIEmbeddings instance with custom configuration including API key, batch size, and model selection.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/openai.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddings = new OpenAIEmbeddings({\n  apiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.OPENAI_API_KEY\n  batchSize: 512, // Default value if omitted is 512. Max is 2048\n  model: \"text-embedding-3-large\",\n});\n```\n\n----------------------------------------\n\nTITLE: Loading JSON with JSON Pointer in TypeScript\nDESCRIPTION: This example shows how to use the JSONLoader with a JSON pointer to extract specific information from a more complex JSON structure. It targets the 'from' and 'surname' keys in the JSON object.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/json.mdx#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"1\": {\n    \"body\": \"BD 2023 SUMMER\",\n    \"from\": \"LinkedIn Job\",\n    \"labels\": [\"IMPORTANT\", \"CATEGORY_UPDATES\", \"INBOX\"]\n  },\n  \"2\": {\n    \"body\": \"Intern, Treasury and other roles are available\",\n    \"from\": \"LinkedIn Job2\",\n    \"labels\": [\"IMPORTANT\"],\n    \"other\": {\n      \"name\": \"plop\",\n      \"surname\": \"bob\"\n    }\n  }\n}\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JSONLoader } from \"langchain/document_loaders/fs/json\";\n\nconst loader = new JSONLoader(\n  \"src/document_loaders/example_data/example.json\",\n  [\"/from\", \"/surname\"]\n);\n\nconst docs = await loader.load();\n/*\n[\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/json\",\n      \"line\": 1,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"BD 2023 SUMMER\",\n  },\n  Document {\n    \"metadata\": {\n      \"blobType\": \"application/json\",\n      \"line\": 2,\n      \"source\": \"blob\",\n    },\n    \"pageContent\": \"LinkedIn Job\",\n  },\n  ...\n]\n*/\n```\n\n----------------------------------------\n\nTITLE: Using Custom PDF.js Build with WebPDFLoader\nDESCRIPTION: Example of how to use a custom build of PDF.js with WebPDFLoader, specifically using the \"legacy\" build that includes additional polyfills. This is useful for environments with specific compatibility requirements.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/pdf.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { WebPDFLoader } from \"@langchain/community/document_loaders/web/pdf\";\n\nconst blob = new Blob(); // e.g. from a file input\n\nconst customBuildLoader = new WebPDFLoader(blob, {\n  // you may need to add `.then(m => m.default)` to the end of the import\n  // @lc-ts-ignore\n  pdfjs: () => import(\"pdfjs-dist/legacy/build/pdf.js\"),\n});\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for TogetherAI API\nDESCRIPTION: Sets the TOGETHER_AI_API_KEY environment variable for authentication with TogetherAI. Optionally includes setting up LangSmith tracing.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/togetherai.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport TOGETHER_AI_API_KEY=\"your-api-key\"\n\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Legacy AgentExecutor\nDESCRIPTION: Creates and configures a legacy LangChain AgentExecutor with chat prompt template and tools.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/migrate_agent.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport {\n  ChatPromptTemplate,\n} from \"@langchain/core/prompts\";\nimport { createToolCallingAgent } from \"langchain/agents\";\nimport { AgentExecutor } from \"langchain/agents\";\n\nconst prompt = ChatPromptTemplate.fromMessages([\n  [\"system\", \"You are a helpful assistant\"],\n  [\"placeholder\", \"{chat_history}\"],\n  [\"human\", \"{input}\"],\n  [\"placeholder\", \"{agent_scratchpad}\"],\n]);\n\nconst agent = createToolCallingAgent({\n  llm,\n  tools,\n  prompt\n});\nconst agentExecutor = new AgentExecutor({\n  agent,\n  tools,\n});\n\nawait agentExecutor.invoke({ input: query });\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Google Trends Tool\nDESCRIPTION: This command installs the necessary packages to use the Google Trends Tool with LangChain.js, including OpenAI integration, community tools, and core LangChain components.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/google_trends.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Loading Web Content with CheerioWebBaseLoader in LangChain\nDESCRIPTION: Uses the CheerioWebBaseLoader to fetch and load content from Wikipedia into a LangChain Document object. The code demonstrates how to download an article and check its length.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/extraction_long_text.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\";\n// Only required in a Deno notebook environment to load the peer dep.\nimport \"cheerio\";\n\nconst loader = new CheerioWebBaseLoader(\n  \"https://en.wikipedia.org/wiki/Car\"\n);\n\nconst docs = await loader.load();\n\ndocs[0].pageContent.length;\n```\n\n----------------------------------------\n\nTITLE: Creating Namespace-Specific Retrievers in Pinecone\nDESCRIPTION: JavaScript code demonstrating how to create retrievers for specific namespaces in Pinecone, allowing for user-specific document retrieval.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_per_user.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n// This will only get documents for Ankush\nconst ankushRetriever = vectorStore.asRetriever({\n  filter: {\n    namespace: \"ankush\",\n  },\n});\n\nawait ankushRetriever.invoke(\n  \"where did i work?\",\n);\n\n// This will only get documents for Harrison\nconst harrisonRetriever = vectorStore.asRetriever({\n  filter: {\n    namespace: \"harrison\",\n  },\n});\n\nawait harrisonRetriever.invoke(\n  \"where did i work?\",\n);\n```\n\n----------------------------------------\n\nTITLE: Instantiating FireworksEmbeddings in Python\nDESCRIPTION: Creates an instance of FireworksEmbeddings with a specified model name for generating embeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/fireworks.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { FireworksEmbeddings } from \"@langchain/community/embeddings/fireworks\";\n\nconst embeddings = new FireworksEmbeddings({\n  modelName: \"nomic-ai/nomic-embed-text-v1.5\",\n});\n```\n\n----------------------------------------\n\nTITLE: Installing @langchain/core for Multimodal Support\nDESCRIPTION: Command to install the @langchain/core package which is needed for multimodal inputs with Gemini models.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-google-genai/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Using ChatOllama model in TypeScript\nDESCRIPTION: Example of how to initialize and use the ChatOllama model for invoking a chat completion.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-ollama/README.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOllama } from \"@langchain/ollama\";\n\nconst model = new ChatOllama({\n  model: \"llama3\",  // Default value.\n});\n\nconst result = await model.invoke([\"human\", \"Hello, how are you?\"]);\n```\n\n----------------------------------------\n\nTITLE: Implementing Layerup Security with LangChain\nDESCRIPTION: Example code demonstrating how to use Layerup Security with LangChain. This snippet is referenced but not directly included in the markdown.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/layerup_security.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n{LayerupSecurityExampleCode}\n```\n\n----------------------------------------\n\nTITLE: Setting Up LangSmith API Key for Tracing\nDESCRIPTION: Uncomment these lines to set up your LangSmith API key for automated tracing from individual queries.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/chroma.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// process.env.LANGSMITH_API_KEY = \"<YOUR API KEY HERE>\";\n// process.env.LANGSMITH_TRACING = \"true\";\n```\n\n----------------------------------------\n\nTITLE: Running Next.js Development Server\nDESCRIPTION: Commands to start the Next.js development server using different package managers. This allows you to run the application locally for development purposes.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/environment_tests/test-exports-vercel/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm run dev\n# or\nyarn dev\n# or\npnpm dev\n```\n\n----------------------------------------\n\nTITLE: Building Final QA Chain\nDESCRIPTION: Combines all components into a final chain that processes both original and step-back questions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/step_back.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nconst chain = RunnableSequence.from([\n  {\n    normal_context: (i: { question: string }) => retriever(i.question),\n    step_back_context: questionGenerator.pipe(retriever),\n    question: (i: { question: string }) => i.question,\n  },\n  responsePrompt,\n  model,\n  stringOutputParser,\n]);\n```\n\n----------------------------------------\n\nTITLE: Installing Langchain Packages\nDESCRIPTION: This snippet installs several Langchain packages needed for AI applications, such as 'openai', 'community', and 'core' modules. These packages provide additional functionalities to work with AI models and require a Node.js environment.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/lancedb.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Filtering Events by Tags\nDESCRIPTION: Demonstrates filtering events by tags assigned to components. This example shows how to catch all events from components tagged with 'my_chain', with a caution that tags are inherited by child components.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming.ipynb#2025-04-22_snippet_18\n\nLANGUAGE: javascript\nCODE:\n```\nconst chain = model\n  .pipe(new JsonOutputParser().withConfig({ runName: \"my_parser\" }))\n  .withConfig({ tags: [\"my_chain\"] });\n\n\nconst eventStream = await chain.streamEvents(\n  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key \"name\" and \"population\"`,\n  { version: \"v2\" },\n  { includeTags: [\"my_chain\"] },\n);\n\nlet eventCount = 0;\n\nfor await (const event of eventStream) {\n  // Truncate the output\n  if (eventCount > 10) {\n    continue;\n  }\n  console.log(event);\n  eventCount += 1;\n}\n```\n\n----------------------------------------\n\nTITLE: Manually Propagating RunnableConfig in Custom Runnable\nDESCRIPTION: Example demonstrating how to properly propagate the RunnableConfig in a custom Runnable by accepting the config parameter and passing it to any sub-calls.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/runnables.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n// Note the config argument\n// highlight-next-line\nconst foo = (input, config) => {\n  return barRunnable.invoke(input, config);\n};\nconst fooRunnable = RunnableLambda.from(foo);\n```\n\n----------------------------------------\n\nTITLE: Inspecting Tool Properties in TypeScript\nDESCRIPTION: Demonstrates how to access a tool's metadata properties such as name and description. Useful for debugging and understanding tool configuration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/tools.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconsole.log(multiply.name); // multiply\nconsole.log(multiply.description); // Multiply two numbers.\n```\n\n----------------------------------------\n\nTITLE: Taking Screenshots with PuppeteerWebBaseLoader\nDESCRIPTION: Code to capture a screenshot of a web page using PuppeteerWebBaseLoader's screenshot method, which returns a Document with the base64-encoded image as content.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/web_puppeteer.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { PuppeteerWebBaseLoader } from \"@langchain/community/document_loaders/web/puppeteer\";\n\nconst loaderForScreenshot = new PuppeteerWebBaseLoader(\"https://langchain.com\", {\n  launchOptions: {\n    headless: true,\n  },\n  gotoOptions: {\n    waitUntil: \"domcontentloaded\",\n  },\n});\nconst screenshot = await loaderForScreenshot.screenshot();\n\nconsole.log(screenshot.pageContent.slice(0, 100));\nconsole.log(screenshot.metadata);\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Self-Querying Retriever in Node.js\nDESCRIPTION: Command to install required npm packages for implementing a self-querying retriever using LangChain.js.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/self_query.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install peggy @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Deleting Documents from LibSQLVectorStore (TypeScript)\nDESCRIPTION: Deletes documents from the `LibSQLVectorStore` based on their primary key IDs. It takes an object with an `ids` property containing an array of IDs to be removed from the database table.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/libsql.mdx#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nawait vectorStore.deleteDocuments({ ids: [1, 2] });\n```\n\n----------------------------------------\n\nTITLE: Generating Test Data with Faker.js\nDESCRIPTION: Creates a test dataset of 10,000 random full names using the Faker library\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_high_cardinality.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport { faker } from \"@faker-js/faker\";\n\nconst names = Array.from({ length: 10000 }, () => (faker as any).person.fullName());\n```\n\n----------------------------------------\n\nTITLE: Installing @langchain/google-vertexai Package with Yarn\nDESCRIPTION: This command installs the @langchain/google-vertexai package using Yarn package manager. It adds the necessary dependencies for integrating Google Vertex AI services with LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-google-vertexai/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn add @langchain/google-vertexai\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatOpenAI Model\nDESCRIPTION: Setting up the ChatOpenAI model with GPT-4 and specific temperature settings\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/llm_chain.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ChatOpenAI } from '@langchain/openai';\n\nconst model = new ChatOpenAI({\n  model: \"gpt-4o-mini\",\n  temperature: 0,\n})\n```\n\n----------------------------------------\n\nTITLE: Setting Groq API Key Environment Variable\nDESCRIPTION: Command to set the GROQ_API_KEY environment variable required for authentication with Groq's API.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-groq/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport GROQ_API_KEY=\n```\n\n----------------------------------------\n\nTITLE: Building the Package\nDESCRIPTION: Commands to build the @langchain/qdrant package, either from within the package directory or from the repository root.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-qdrant/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nyarn build\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn build --filter=@langchain/qdrant\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Anthropic Client\nDESCRIPTION: Shows how to create a custom Anthropic client using Google Vertex AI integration with the ChatAnthropic class.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/anthropic.ipynb#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport { AnthropicVertex } from \"@anthropic-ai/vertex-sdk\";\n\nconst customClient = new AnthropicVertex();\n\nconst modelWithCustomClient = new ChatAnthropic({\n  modelName: \"claude-3-sonnet@20240229\",\n  maxRetries: 0,\n  createClient: () => customClient,\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Chat Model Stream\nDESCRIPTION: Demonstrating streaming functionality with chat models\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/llm_chain.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconst stream = await model.stream(messages);\n\nconst chunks = [];\nfor await (const chunk of stream) {\n  chunks.push(chunk);\n  console.log(`${chunk.content}|`);\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Additional LangChain Packages\nDESCRIPTION: Command to install OpenAI, community, and core packages for LangChain. These are needed for the complete chat message history implementation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/azure_cosmos_mongo_vcore.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Splitting JavaScript Code with RecursiveCharacterTextSplitter\nDESCRIPTION: Demonstrates how to split JavaScript code into chunks using the RecursiveCharacterTextSplitter with language-specific separators for JS. Creates a splitter with a specified chunk size and overlap parameters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/code_splitter.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst JS_CODE = `\nfunction helloWorld() {\n  console.log(\"Hello, World!\");\n}\n\n// Call the function\nhelloWorld();\n`\n\nconst jsSplitter = RecursiveCharacterTextSplitter.fromLanguage(\n    \"js\", {\n      chunkSize: 60,\n      chunkOverlap: 0,\n    }\n)\nconst jsDocs = await jsSplitter.createDocuments([JS_CODE]);\n\njsDocs\n```\n\n----------------------------------------\n\nTITLE: Running Tests for Redis Package\nDESCRIPTION: Commands to run unit tests (ending in .test.ts) and integration tests (ending in .int.test.ts) for the @langchain/redis package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-redis/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn test\n$ yarn test:int\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Azure OpenAI and Milvus\nDESCRIPTION: Exports a set of environment variables for Azure OpenAI configuration and Milvus URL to be used in applications requiring Azure OpenAI authentication and Milvus interfaces.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/milvus.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport AZURE_OPENAI_API_KEY=YOUR_AZURE_OPENAI_API_KEY_HERE\nexport AZURE_OPENAI_API_INSTANCE_NAME=YOUR_AZURE_OPENAI_INSTANCE_NAME_HERE\nexport AZURE_OPENAI_API_DEPLOYMENT_NAME=YOUR_AZURE_OPENAI_DEPLOYMENT_NAME_HERE\nexport AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME=YOUR_AZURE_OPENAI_COMPLETIONS_DEPLOYMENT_NAME_HERE\nexport AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME=YOUR_AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME_HERE\nexport AZURE_OPENAI_API_VERSION=YOUR_AZURE_OPENAI_API_VERSION_HERE\nexport AZURE_OPENAI_BASE_PATH=YOUR_AZURE_OPENAI_BASE_PATH_HERE\nexport MILVUS_URL=YOUR_MILVUS_URL_HERE # for example http://localhost:19530\n```\n\n----------------------------------------\n\nTITLE: Configuring Package Dependencies with Version Resolution\nDESCRIPTION: Package.json configuration showing how to ensure consistent @langchain/core version across different package managers including npm, yarn, and pnpm\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-mongodb/README.md#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"your-project\",\n  \"version\": \"0.0.0\",\n  \"dependencies\": {\n    \"@langchain/core\": \"^0.3.0\",\n    \"@langchain/mongodb\": \"^0.0.0\"\n  },\n  \"resolutions\": {\n    \"@langchain/core\": \"^0.3.0\"\n  },\n  \"overrides\": {\n    \"@langchain/core\": \"^0.3.0\"\n  },\n  \"pnpm\": {\n    \"overrides\": {\n      \"@langchain/core\": \"^0.3.0\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring AzureOpenAI with Different Domain in LangChain.js\nDESCRIPTION: Shows how to configure AzureOpenAI to use a different domain than the default, using the AZURE_OPENAI_BASE_PATH environment variable.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/azure.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport { AzureOpenAI } from \"@langchain/openai\";\n\nconst differentDomainLLM = new AzureOpenAI({\n  azureOpenAIApiKey: \"<your_key>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY\n  azureOpenAIApiDeploymentName: \"<your_deployment_name>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME\n  azureOpenAIApiVersion: \"<api_version>\", // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION\n  azureOpenAIBasePath:\n    \"https://westeurope.api.microsoft.com/openai/deployments\", // In Node.js defaults to process.env.AZURE_OPENAI_BASE_PATH\n});\n```\n\n----------------------------------------\n\nTITLE: Using PremEmbeddings in TypeScript\nDESCRIPTION: Example code demonstrating how to import and use the PremEmbeddings class from @langchain/community to generate embeddings for given text. It shows the initialization of the embeddings object and embedding generation for a sample text.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/premai.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PremEmbeddings } from \"@langchain/community/embeddings/prem\";\n\nconst embeddings = new PremEmbeddings({\n  apiKey: \"YOUR_PREM_API_KEY\", // Replace with your actual Prem API key\n});\n\nconst res = await embeddings.embedQuery(\"Hello world\");\nconsole.log(res);\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Azure OpenAI in Node.js\nDESCRIPTION: Defines the necessary environment variables for using Azure OpenAI services, including the API instance name, embeddings deployment name, API key, and API version.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/azure_openai.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nAZURE_OPENAI_API_INSTANCE_NAME=<YOUR_INSTANCE_NAME>\nAZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME=<YOUR_EMBEDDINGS_DEPLOYMENT_NAME>\nAZURE_OPENAI_API_KEY=<YOUR_KEY>\nAZURE_OPENAI_API_VERSION=\"2024-02-01\"\n```\n\n----------------------------------------\n\nTITLE: Message Trimming with Chat History\nDESCRIPTION: Demonstrates integration of message trimming with chat history management using InMemoryChatMessageHistory and RunnableWithMessageHistory.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/trim_messages.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { InMemoryChatMessageHistory } from \"@langchain/core/chat_history\";\nimport { RunnableWithMessageHistory } from \"@langchain/core/runnables\";\nimport { HumanMessage, trimMessages } from \"@langchain/core/messages\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst chatHistory = new InMemoryChatMessageHistory(messages.slice(0, -1))\n\nconst dummyGetSessionHistory = async (sessionId: string) => {\n    if (sessionId !== \"1\") {\n        throw new Error(\"Session not found\");\n      }\n      return chatHistory;\n  }\n\n  const llm = new ChatOpenAI({ model: \"gpt-4o\" });\n\n  const trimmer = trimMessages({\n    maxTokens: 45,\n    strategy: \"last\",\n    tokenCounter: llm,\n    includeSystem: true,\n  });\n\nconst chain = trimmer.pipe(llm);\nconst chainWithHistory = new RunnableWithMessageHistory({\n    runnable: chain,\n    getMessageHistory: dummyGetSessionHistory,\n})\nawait chainWithHistory.invoke(\n    [new HumanMessage(\"what do you call a speechless parrot\")],\n    { configurable: { sessionId: \"1\"} },\n)\n```\n\n----------------------------------------\n\nTITLE: Using SessionsPythonREPLTool in TypeScript\nDESCRIPTION: Example of importing and using the SessionsPythonREPLTool to execute Python code in Azure Container Apps dynamic sessions. The tool connects to a session pool management endpoint and returns the execution result.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-azure-dynamic-sessions/README.md#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { SessionsPythonREPLTool } from \"@langchain/azure-dynamic-sessions\";\n\nconst tool = new SessionsPythonREPLTool({\n  poolManagementEndpoint:\n    process.env.AZURE_CONTAINER_APP_SESSION_POOL_MANAGEMENT_ENDPOINT || \"\",\n});\n\nconst result = await tool.invoke(\"print('Hello, World!')\\n1+2\");\n\nconsole.log(result);\n\n// {\n//   stdout: \"Hello, World!\\n\",\n//   stderr: \"\",\n//   result: 3,\n// }\n```\n\n----------------------------------------\n\nTITLE: Setting VertexAI Credentials\nDESCRIPTION: Commands to set VertexAI credentials as environment variables for authentication.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/google.mdx#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nGOOGLE_VERTEX_AI_WEB_CREDENTIALS={\"type\":\"service_account\",\"project_id\":\"YOUR_PROJECT-12345\",...}\n```\n\nLANGUAGE: bash\nCODE:\n```\nGOOGLE_VERTEX_AI_WEB_CREDENTIALS_FILE=/path/to/your/credentials.json\n```\n\n----------------------------------------\n\nTITLE: Instantiating TogetherAI Model\nDESCRIPTION: Creates a TogetherAI model instance for the Meta-Llama-3.1-8B-Instruct-Turbo model with a maximum token limit of 256. This code initializes the model for text completion tasks.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/together.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { TogetherAI } from \"@langchain/community/llms/togetherai\";\n\nconst llm = new TogetherAI({\n  model: \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\",\n  maxTokens: 256,\n});\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of TavilySearchAPIRetriever\nDESCRIPTION: Simple example showing how to invoke the retriever with a search query\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/tavily.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst query = \"what is the current weather in SF?\";\n\nawait retriever.invoke(query);\n```\n\n----------------------------------------\n\nTITLE: Alternative MessagesPlaceholder Syntax in TypeScript\nDESCRIPTION: Shows an alternative way to implement message placeholders using array syntax instead of the MessagesPlaceholder class.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/prompt_templates.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst promptTemplate = ChatPromptTemplate.fromMessages([\n  [\"system\", \"You are a helpful assistant\"],\n  [\"placeholder\", \"{msgs}\"], // <-- This is the changed part\n]);\n```\n\n----------------------------------------\n\nTITLE: Running Tests\nDESCRIPTION: Bash commands to run unit tests and integration tests for the @langchain/mistralai package, which should be placed in the appropriate test files.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-mistralai/README.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn test\n$ yarn test:int\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables\nDESCRIPTION: Setting up required API keys for Tavily Search and OpenAI\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_citations.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=YOUR_KEY\nexport TAVILY_API_KEY=YOUR_KEY\n```\n\n----------------------------------------\n\nTITLE: Basic Playwright Web Loading\nDESCRIPTION: Simple example of loading web content using PlaywrightWebBaseLoader with default configuration\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/web_playwright.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PlaywrightWebBaseLoader } from \"@langchain/community/document_loaders/web/playwright\";\n\n/**\n * Loader uses `page.content()`\n * as default evaluate function\n **/\nconst loader = new PlaywrightWebBaseLoader(\"https://www.tabnews.com.br/\");\n\nconst docs = await loader.load();\n```\n\n----------------------------------------\n\nTITLE: Running tests for @langchain/exa\nDESCRIPTION: Commands to run unit tests and integration tests for the package. Unit tests end with .test.ts and integration tests end with .int.test.ts.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-exa/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn test\n$ yarn test:int\n```\n\n----------------------------------------\n\nTITLE: Building LangChain from Repository Root\nDESCRIPTION: Commands to install dependencies and build the LangChain package from the repository root.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/examples/src/README.md#2025-04-22_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nyarn\nyarn build\n```\n\n----------------------------------------\n\nTITLE: Creating a Chroma Vectorstore with OpenAI Embeddings in JavaScript\nDESCRIPTION: This snippet demonstrates how to create a Chroma vectorstore using OpenAI embeddings and set up a retriever.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_no_queries.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { Chroma } from \"@langchain/community/vectorstores/chroma\"\nimport { OpenAIEmbeddings } from \"@langchain/openai\"\nimport \"chromadb\";\n\nconst texts = [\"Harrison worked at Kensho\"]\nconst embeddings = new OpenAIEmbeddings({ model: \"text-embedding-3-small\" })\nconst vectorstore = await Chroma.fromTexts(texts, {}, embeddings, {\n  collectionName: \"harrison\"\n})\nconst retriever = vectorstore.asRetriever(1);\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Blockchain Data Loading in Node.js\nDESCRIPTION: This snippet shows the command to install the required npm packages for working with LangChain and OpenAI in a Node.js environment.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/sort_xyz_blockchain.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core @langchain/openai\n```\n\n----------------------------------------\n\nTITLE: Loading EPUB Files as Single Document in LangChain JS\nDESCRIPTION: Example of using EPubLoader with the splitChapters option set to false, which loads the entire EPUB file as a single document instead of splitting by chapter.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/epub.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { EPubLoader } from \"@langchain/community/document_loaders/fs/epub\";\n\nconst loader = new EPubLoader(\n  \"src/document_loaders/example_data/example.epub\",\n  {\n    splitChapters: false,\n  }\n);\n\nconst docs = await loader.load();\n```\n\n----------------------------------------\n\nTITLE: Importing BedrockEmbeddings for AWS Bedrock Text Embeddings in TypeScript\nDESCRIPTION: This code demonstrates how to import the BedrockEmbeddings class from the LangChain AWS package for using AWS Bedrock text embedding models.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/aws.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BedrockEmbeddings } from \"@langchain/aws\";\n```\n\n----------------------------------------\n\nTITLE: Displaying ChatPerplexity Response Content\nDESCRIPTION: Code to extract and display just the content from the response message of a ChatPerplexity model invocation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/perplexity.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconsole.log(aiMsg.content);\n```\n\n----------------------------------------\n\nTITLE: Using Glyphs with Minimax in LangChain.js\nDESCRIPTION: Shows how to use glyphs with Minimax models in LangChain.js to force the model to return content in a specific requested format.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/minimax.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport MinimaxGlyph from \"@examples/models/chat/minimax_glyph.ts\";\n```\n\n----------------------------------------\n\nTITLE: Setting Up Postgres with Docker\nDESCRIPTION: This snippet provides a Docker setup for hosting a Postgres instance with the pgvector extension enabled. It outlines environment configurations for database initialization.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/pgvector.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nservices:\n  db:\n    hostname: 127.0.0.1\n    image: pgvector/pgvector:pg16\n    ports:\n      - 5432:5432\n    restart: always\n    environment:\n      - POSTGRES_DB=api\n      - POSTGRES_USER=myuser\n      - POSTGRES_PASSWORD=ChangeMe\n```\n\n----------------------------------------\n\nTITLE: LangChain Tool Call Response Structure\nDESCRIPTION: JSON structure showing the response format for tool calls and message handling in LangChain JS, including calculator operations and metadata.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_26\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"toolu_01NVTbm5aNYSm1wGYb6XF7jE\",\n  \"name\": \"calculator\",\n  \"input\": {\n    \"input\": \"52 * 365\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using ChromeAI in Browser\nDESCRIPTION: This code demonstrates how to initialize the ChromeAI model with configuration options and generate a response. It shows basic invocation of the model to generate a text completion, specifically a poem in this example.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/chrome_ai.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChromeAI } from \"@langchain/community/experimental/llms/chrome_ai\";\n\nconst model = new ChromeAI({\n  temperature: 0.5, // Optional, defaults to 0.5\n  topK: 40, // Optional, defaults to 40\n});\n\nconst response = await model.invoke(\"Write me a short poem please\");\n\n/*\n  In the realm where moonlight weaves its hue,\n  Where dreams and secrets gently intertwine,\n  There's a place of tranquility and grace,\n  Where whispers of the night find their place.\n\n  Beneath the canopy of starlit skies,\n  Where dreams take flight and worries cease,\n  A haven of tranquility, pure and true,\n  Where the heart finds solace, finding dew.\n\n  In this realm where dreams find their release,\n  Where the soul finds peace, at every peace,\n  Let us wander, lost in its embrace,\n  Finding solace in this tranquil space.\n*/\n```\n\n----------------------------------------\n\nTITLE: Invoking Cypher Generation Chain with User Question\nDESCRIPTION: This code demonstrates how to use the custom Cypher generating chain to create a Cypher query based on a user's question about actors in a specific movie.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/graph_mapping.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nconst cypher = await cypherResponse.invoke({\"question\": \"Who played in Casino movie?\"})\ncypher\n```\n\n----------------------------------------\n\nTITLE: Cancelling Chain Execution with AbortController in TypeScript\nDESCRIPTION: Shows how to cancel chain execution using an AbortController with a timeout. This example demonstrates early termination of the chain by aborting after 100ms, preventing the model from being called.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/cancel_execution.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst controller = new AbortController();\n\nconst startTimer = console.time(\"timer1\");\n\nsetTimeout(() => controller.abort(), 100);\n\ntry {\n  await chain.invoke(\"what is the current weather in SF?\", {\n    signal: controller.signal,\n  });\n} catch (e) {\n  console.log(e);\n}\n\nconsole.timeEnd(\"timer1\");\n```\n\n----------------------------------------\n\nTITLE: Installing @langchain/groq Package\nDESCRIPTION: Command to install the @langchain/groq package and its core dependency using npm or yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-groq/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/groq @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Loading Documents from Audio Files using OpenAI Whisper in TypeScript\nDESCRIPTION: Example demonstrating how to create document objects from audio files by transcribing them with OpenAI's Whisper API. This requires an OpenAI API key and is only compatible with Node.js environments.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/openai_whisper_audio.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAIWhisperAudio } from \"langchain/document_loaders/fs/openai_whisper_audio\";\n\n/**\n * This example shows how to load document objects from an audio file using\n * OpenAI Whisper API.\n *\n * Your audio file must be in one of these formats: mp3, mp4, mpeg, mpga, m4a, wav, webm\n *\n * First, you should install `langchain` and configure your environment with your\n * OpenAI API key.\n *\n * ```bash\n * npm install langchain\n * ```\n *\n * ```bash\n * export OPENAI_API_KEY=your-api-key\n * ```\n *\n * @example\n * ```typescript\n * const loader = new OpenAIWhisperAudio(\n *   \"path/to/audio.mp3\", // for example \"Believe_-_Mumford_Sons.mp3\"\n * );\n * const docs = await loader.load();\n * console.log({ docs });\n * ```\n */\n\n// This is a placeholder to demonstrate the example\nconst loader = new OpenAIWhisperAudio(\n  \"path/to/audio.mp3\", // for example \"Believe_-_Mumford_Sons.mp3\"\n);\n\nexport const run = async () => {\n  try {\n    // Load documents\n    const docs = await loader.load();\n    console.log({ docs });\n    return docs;\n  } catch (e) {\n    console.error(e);\n    throw e;\n  }\n};\n```\n\n----------------------------------------\n\nTITLE: Installing Voy Dependencies with NPM/Yarn\nDESCRIPTION: Command to install required packages for using Voy with LangChain.js, including OpenAI integration, Voy search engine, and core LangChain packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/voy.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai voy-search @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Setting SerpAPI API Key as Environment Variable\nDESCRIPTION: Code to set the SerpAPI API key as an environment variable for authentication.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/serpapi.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.SERPAPI_API_KEY = \"YOUR_API_KEY\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for Azure Cosmos DB Connection\nDESCRIPTION: Displays the required environment variables for connecting the LangchainJS application to an Azure Cosmos DB for MongoDB vCore instance. The actual content is dynamically imported from an example environment file (`.env.example`). These variables typically include the database connection string, database name, collection name, and index name.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/azure_cosmosdb_mongodb.mdx#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n{EnvVars}\n```\n\n----------------------------------------\n\nTITLE: Installing Redis Sentinel Dependencies\nDESCRIPTION: Command to install the ioredis package for Redis Sentinel support.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/redis.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install ioredis\n```\n\n----------------------------------------\n\nTITLE: Using ZepCloudVectorStore with Expression Language in TypeScript\nDESCRIPTION: This example shows how to use ZepCloudVectorStore with LangChain's Expression Language. It demonstrates creating a retriever and integrating it into a question-answering chain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/zep_cloud.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nZepCloudVectorStoreExpressionLanguageExample\n```\n\n----------------------------------------\n\nTITLE: Importing ChatOpenAI in TypeScript\nDESCRIPTION: Example of importing the ChatOpenAI class from the @langchain/openai package in a TypeScript environment.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/installation.mdx#2025-04-22_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n```\n\n----------------------------------------\n\nTITLE: Parsing Sitemap without Loading Page Contents in TypeScript\nDESCRIPTION: This snippet shows how to use the parseSitemap method to load only the sitemap structure without fetching the contents of each page.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/sitemap.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { SitemapLoader } from \"@langchain/community/document_loaders/web/sitemap\";\n\nconst loader = new SitemapLoader(\"https://js.langchain.com/sitemap.xml\");\nconst docs = await loader.parseSitemap();\n\nconsole.log({ docs });\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain and OpenAI Dependencies with npm - Bash\nDESCRIPTION: This snippet provides the npm command to install @langchain/openai, @langchain/core, and @langchain/community. These dependencies are required to integrate various AI and vector store components within the LangChain framework. This command should be run from the root of your JavaScript/TypeScript project. Ensure you have npm installed and that your project is properly initialized before executing.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/rockset.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core @langchain/community\n```\n\n----------------------------------------\n\nTITLE: Adding Hooks to ChatMistralAI at Instantiation\nDESCRIPTION: Shows how to add hook functions to ChatMistralAI during model instantiation for customizing request handling behavior.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/mistral.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatMistralAI } from \"@langchain/mistralai\" \n\nconst modelWithHooks = new ChatMistralAI({\n    model: \"mistral-large-latest\",\n    temperature: 0,\n    maxRetries: 2,\n    beforeRequestHooks: [ beforeRequestHook ],\n    requestErrorHooks: [ requestErrorHook ],\n    responseHooks: [ responseHook ],\n    // other params...\n});\n```\n\n----------------------------------------\n\nTITLE: Building the Package\nDESCRIPTION: Commands to build the @langchain/google-genai package, either from within the package directory or from the repo root.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-google-genai/README.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nyarn build\n```\n\n----------------------------------------\n\nTITLE: Chaining ChatFireworks with Prompt Template\nDESCRIPTION: Example of creating a chat prompt template and chaining it with the ChatFireworks model for language translation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/fireworks.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\"\n\nconst prompt = ChatPromptTemplate.fromMessages(\n    [\n        [\n            \"system\",\n            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n        ],\n        [\"human\", \"{input}\"],\n    ]\n)\n\nconst chain = prompt.pipe(llm);\nawait chain.invoke(\n    {\n        input_language: \"English\",\n        output_language: \"German\",\n        input: \"I love programming.\",\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Callback Handler for Chat Model Events in JavaScript\nDESCRIPTION: This example demonstrates how to create a custom callback handler that responds to chat model start and new token events. The handler logs the model start details and each new token as it's streamed from the Anthropic Claude model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/custom_callbacks.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst prompt = ChatPromptTemplate.fromTemplate(`What is 1 + {number}?`);\nconst model = new ChatAnthropic({\n  model: \"claude-3-sonnet-20240229\",\n});\n\nconst chain = prompt.pipe(model);\n\nconst customHandler = {\n  handleChatModelStart: async (llm, inputMessages, runId) => {\n    console.log(\"Chat model start:\", llm, inputMessages, runId)\n  },\n  handleLLMNewToken: async (token) => {\n    console.log(\"Chat model new token\", token);\n  }\n};\n\nconst stream = await chain.stream({ number: \"2\" }, { callbacks: [customHandler] });\n\nfor await (const _ of stream) {\n  // Just consume the stream so the callbacks run\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Safety Settings for ChatGoogleGenerativeAI\nDESCRIPTION: Demonstrates how to customize safety settings to control content filtering for the Gemini model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/google_generativeai.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatGoogleGenerativeAI } from \"@langchain/google-genai\";\nimport { HarmBlockThreshold, HarmCategory } from \"@google/generative-ai\";\n\nconst llmWithSafetySettings = new ChatGoogleGenerativeAI({\n  model: \"gemini-1.5-pro\",\n  temperature: 0,\n  safetySettings: [\n    {\n      category: HarmCategory.HARM_CATEGORY_HARASSMENT,\n      threshold: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n    },\n  ],\n  // other params...\n});\n```\n\n----------------------------------------\n\nTITLE: Creating a Translation Chain with ChatPromptTemplate\nDESCRIPTION: Demonstrates chaining a prompt template with the ChatMistralAI model to create a reusable translation pipeline.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/mistral.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\"\n\nconst prompt = ChatPromptTemplate.fromMessages(\n    [\n        [\n            \"system\",\n            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n        ],\n        [\"human\", \"{input}\"],\n    ]\n)\n\nconst chain = prompt.pipe(llm);\nawait chain.invoke(\n    {\n        input_language: \"English\",\n        output_language: \"German\",\n        input: \"I love programming.\",\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing HNSWLib Vector Store with Documents\nDESCRIPTION: Creates documents with metadata, defines attribute information, and instantiates a HNSWLib vector store with OpenAI embeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/hnswlib.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { HNSWLib } from \"@langchain/community/vectorstores/hnswlib\";\nimport { Document } from \"@langchain/core/documents\";\nimport type { AttributeInfo } from \"langchain/chains/query_constructor\";\n\n/**\n * First, we create a bunch of documents. You can load your own documents here instead.\n * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.\n */\nconst docs = [\n  new Document({\n    pageContent:\n      \"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n    metadata: { year: 1993, rating: 7.7, genre: \"science fiction\" },\n  }),\n  new Document({\n    pageContent:\n      \"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n    metadata: { year: 2010, director: \"Christopher Nolan\", rating: 8.2 },\n  }),\n  new Document({\n    pageContent:\n      \"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n    metadata: { year: 2006, director: \"Satoshi Kon\", rating: 8.6 },\n  }),\n  new Document({\n    pageContent:\n      \"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n    metadata: { year: 2019, director: \"Greta Gerwig\", rating: 8.3 },\n  }),\n  new Document({\n    pageContent: \"Toys come alive and have a blast doing so\",\n    metadata: { year: 1995, genre: \"animated\" },\n  }),\n  new Document({\n    pageContent: \"Three men walk into the Zone, three men walk out of the Zone\",\n    metadata: {\n      year: 1979,\n      director: \"Andrei Tarkovsky\",\n      genre: \"science fiction\",\n      rating: 9.9,\n    },\n  }),\n];\n\n/**\n * Next, we define the attributes we want to be able to query on.\n * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.\n * We also provide a description of each attribute and the type of the attribute.\n * This is used to generate the query prompts.\n */\nconst attributeInfo: AttributeInfo[] = [\n  {\n    name: \"genre\",\n    description: \"The genre of the movie\",\n    type: \"string or array of strings\",\n  },\n  {\n    name: \"year\",\n    description: \"The year the movie was released\",\n    type: \"number\",\n  },\n  {\n    name: \"director\",\n    description: \"The director of the movie\",\n    type: \"string\",\n  },\n  {\n    name: \"rating\",\n    description: \"The rating of the movie (1-10)\",\n    type: \"number\",\n  },\n  {\n    name: \"length\",\n    description: \"The length of the movie in minutes\",\n    type: \"number\",\n  },\n];\n\n/**\n * Next, we instantiate a vector store. This is where we store the embeddings of the documents.\n * We also need to provide an embeddings object. This is used to embed the documents.\n */\nconst embeddings = new OpenAIEmbeddings();\nconst vectorStore = await HNSWLib.fromDocuments(docs, embeddings);\n```\n\n----------------------------------------\n\nTITLE: Installing @langchain/azure-cosmosdb Package\nDESCRIPTION: Command to install the Azure CosmosDB integration package along with the required core dependency using npm or yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-azure-cosmosdb/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/azure-cosmosdb @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Initializing Qdrant vector store with documents\nDESCRIPTION: Creates sample documents with metadata, defines attribute information, and initializes the Qdrant vector store with OpenAI embeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/qdrant.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { QdrantVectorStore } from \"@langchain/qdrant\";\nimport { Document } from \"@langchain/core/documents\";\nimport type { AttributeInfo } from \"langchain/chains/query_constructor\";\n\nimport { QdrantClient } from \"@qdrant/js-client-rest\";\n\n/**\n * First, we create a bunch of documents. You can load your own documents here instead.\n * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.\n */\nconst docs = [\n  new Document({\n    pageContent:\n      \"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n    metadata: { year: 1993, rating: 7.7, genre: \"science fiction\" },\n  }),\n  new Document({\n    pageContent:\n      \"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n    metadata: { year: 2010, director: \"Christopher Nolan\", rating: 8.2 },\n  }),\n  new Document({\n    pageContent:\n      \"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n    metadata: { year: 2006, director: \"Satoshi Kon\", rating: 8.6 },\n  }),\n  new Document({\n    pageContent:\n      \"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n    metadata: { year: 2019, director: \"Greta Gerwig\", rating: 8.3 },\n  }),\n  new Document({\n    pageContent: \"Toys come alive and have a blast doing so\",\n    metadata: { year: 1995, genre: \"animated\" },\n  }),\n  new Document({\n    pageContent: \"Three men walk into the Zone, three men walk out of the Zone\",\n    metadata: {\n      year: 1979,\n      director: \"Andrei Tarkovsky\",\n      genre: \"science fiction\",\n      rating: 9.9,\n    },\n  }),\n];\n\n/**\n * Next, we define the attributes we want to be able to query on.\n * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.\n * We also provide a description of each attribute and the type of the attribute.\n * This is used to generate the query prompts.\n */\nconst attributeInfo: AttributeInfo[] = [\n  {\n    name: \"genre\",\n    description: \"The genre of the movie\",\n    type: \"string or array of strings\",\n  },\n  {\n    name: \"year\",\n    description: \"The year the movie was released\",\n    type: \"number\",\n  },\n  {\n    name: \"director\",\n    description: \"The director of the movie\",\n    type: \"string\",\n  },\n  {\n    name: \"rating\",\n    description: \"The rating of the movie (1-10)\",\n    type: \"number\",\n  },\n  {\n    name: \"length\",\n    description: \"The length of the movie in minutes\",\n    type: \"number\",\n  },\n];\n\n/**\n * Next, we instantiate a vector store. This is where we store the embeddings of the documents.\n * We also need to provide an embeddings object. This is used to embed the documents.\n */\n\nconst client = new QdrantClient({ url: process.env.QDRANT_URL });\n\nconst embeddings = new OpenAIEmbeddings();\nconst vectorStore = await QdrantVectorStore.fromDocuments(docs, embeddings, {\n  client,\n  collectionName: \"movie-collection\",\n});\n```\n\n----------------------------------------\n\nTITLE: Setting up MariaDB Container using Docker Compose - YAML\nDESCRIPTION: This snippet defines a Docker Compose configuration to run a MariaDB 11.7 instance locally for use as a vector store backend. Required environment variables and port bindings are specified for connection from a LangChain application. It requires Docker and docker-compose to be installed, and mounts an initial SQL script (init.sql) for setup. Starting the service is done with 'docker-compose up --build'.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/mariadb.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# Run this command to start the database:\n# docker-compose up --build\nversion: \"3\"\nservices:\n  db:\n    hostname: 127.0.0.1\n    image: mariadb/mariadb:11.7-rc\n    ports:\n      - 3306:3306\n    restart: always\n    environment:\n      - MARIADB_DATABASE=api\n      - MARIADB_USER=myuser\n      - MARIADB_PASSWORD=ChangeMe\n      - MARIADB_ROOT_PASSWORD=ChangeMe\n    volumes:\n      - ./init.sql:/docker-entrypoint-initdb.d/init.sql\n\n```\n\n----------------------------------------\n\nTITLE: Installing Azure Dynamic Sessions and Core Package - Bash\nDESCRIPTION: Installs the @langchain/azure-dynamic-sessions and @langchain/core npm packages, required for managing Azure Container Apps dynamic sessions. It assumes that you have npm installed on your system.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/azure_dynamic_sessions.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/azure-dynamic-sessions @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Using Request Time Callbacks in LangChain\nDESCRIPTION: Example of passing callbacks at request time to a chain during invocation. These callbacks are inherited by all children of the object.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/callbacks.mdx#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nawait chain.invoke({ number: 25 }, { callbacks: [handler] })\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for EPUB Processing in LangChain JS\nDESCRIPTION: Command to install the required packages for loading and processing EPUB files in LangChain JS, including the community package, core library, epub2 parser, and html-to-text converter.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/epub.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core epub2 html-to-text\n```\n\n----------------------------------------\n\nTITLE: Defining Query Schema with Zod in JavaScript\nDESCRIPTION: This code defines a schema for query analysis using Zod, including fields for the main query, sub-queries, and publication year.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_few_shot.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { z } from \"zod\";\n\nconst subQueriesDescription = `\nIf the original question contains multiple distinct sub-questions,\nor if there are more generic questions that would be helpful to answer in\norder to answer the original question, write a list of all relevant sub-questions.\nMake sure this list is comprehensive and covers all parts of the original question.\nIt's ok if there's redundancy in the sub-questions, it's better to cover all the bases than to miss some.\nMake sure the sub-questions are as narrowly focused as possible in order to get the most relevant results.`\n\nconst searchSchema = z.object({\n    query: z.string().describe(\"Primary similarity search query applied to video transcripts.\"),\n    subQueries: z.array(z.string()).optional().describe(subQueriesDescription),\n    publishYear: z.number().optional().describe(\"Year video was published\")\n})\n```\n\n----------------------------------------\n\nTITLE: Subclassing BaseDocumentLoader in TypeScript\nDESCRIPTION: Extends the BaseDocumentLoader abstract class to create a custom document loader. Requires implementing the abstract load() method which should return a Promise of Document objects.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/document_loader_custom.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nabstract class BaseDocumentLoader implements DocumentLoader {\n  abstract load(): Promise<Document[]>;\n}\n```\n\n----------------------------------------\n\nTITLE: Converting a String-Input Runnable Chain to a Tool in TypeScript\nDESCRIPTION: This example demonstrates creating a chain of two string-processing runnables and converting them to a tool. The chain appends first 'a' and then 'z' to the input string.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/convert_runnable_to_tool.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst firstRunnable = RunnableLambda.from<string, string>((input) => {\n  return input + \"a\";\n})\n\nconst secondRunnable = RunnableLambda.from<string, string>((input) => {\n  return input + \"z\";\n})\n\nconst runnable = firstRunnable.pipe(secondRunnable)\nconst asTool = runnable.asTool({\n  name: \"append_letters\",\n  description: \"Adds letters to a string.\",\n  schema: z.string(),\n})\n\nasTool.description;\n```\n\n----------------------------------------\n\nTITLE: Setting Google Cloud Credentials for Web\nDESCRIPTION: Setting the environment variable for Google Cloud credentials as a JSON string in a web environment\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/google_vertex_ai.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nGOOGLE_VERTEX_AI_WEB_CREDENTIALS={\"type\":\"service_account\",\"project_id\":\"YOUR_PROJECT-12345\",...}\n```\n\n----------------------------------------\n\nTITLE: Implementing LangGraph Conversation Management\nDESCRIPTION: Advanced implementation using LangGraph for managing conversation history with message trimming and state management.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/versions/migrating_memory/conversation_buffer_window_memory.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { v4 as uuidv4 } from 'uuid';\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { StateGraph, MessagesAnnotation, END, START, MemorySaver } from \"@langchain/langgraph\";\nimport { trimMessages } from \"@langchain/core/messages\";\n\nconst model = new ChatOpenAI({ model: \"gpt-4o\" });\n\nconst callModel = async (state: typeof MessagesAnnotation.State): Promise<Partial<typeof MessagesAnnotation.State>> => {\n  const selectedMessages = await trimMessages(\n    state.messages,\n    {\n      tokenCounter: (messages) => messages.length,\n      maxTokens: 5,\n      strategy: \"last\",\n      startOn: \"human\",\n      includeSystem: true,\n      allowPartial: false,\n    }\n  );\n\n  const response = await model.invoke(selectedMessages);\n\n  return { messages: [response] };\n};\n\nconst workflow = new StateGraph(MessagesAnnotation)\n  .addNode(\"model\", callModel)\n  .addEdge(START, \"model\")\n  .addEdge(\"model\", END)\n\nconst app = workflow.compile({\n  checkpointer: new MemorySaver()\n});\n\nconst thread_id = uuidv4();\nconst config = { configurable: { thread_id }, streamMode: \"values\" as const };\n\nconst inputMessage = {\n  role: \"user\",\n  content: \"hi! I'm bob\",\n}\nfor await (const event of await app.stream({ messages: [inputMessage] }, config)) {\n  const lastMessage = event.messages[event.messages.length - 1];\n  console.log(lastMessage.content);\n}\n\nconst followUpMessage = {\n  role: \"user\",\n  content: \"what was my name?\",\n}\n\nfor await (const event of await app.stream({ messages: [followUpMessage] }, config)) {\n  const lastMessage = event.messages[event.messages.length - 1];\n  console.log(lastMessage.content);\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Tool with Content and Artifact Separation in TypeScript\nDESCRIPTION: Demonstrates how to create a tool that returns both content for the model and artifacts for downstream components. This allows distinguishing between what's exposed to the model and what's used elsewhere.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/custom_tools.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { z } from \"zod\";\nimport { tool } from \"@langchain/core/tools\";\n\nconst randomIntToolSchema = z.object({\n  min: z.number(),\n  max: z.number(),\n  size: z.number(),\n});\n\nconst generateRandomInts = tool(async ({ min, max, size }) => {\n  const array: number[] = [];\n  for (let i = 0; i < size; i++) {\n    array.push(Math.floor(Math.random() * (max - min + 1)) + min);\n  }\n  return [\n    `Successfully generated array of ${size} random ints in [${min}, ${max}].`,\n    array,\n  ];\n}, {\n  name: \"generateRandomInts\",\n  description: \"Generate size random ints in the range [min, max].\",\n  schema: randomIntToolSchema,\n  responseFormat: \"content_and_artifact\",\n});\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search on SupabaseVectorStore (TypeScript)\nDESCRIPTION: This snippet illustrates how to perform a basic similarity search on the `SupabaseVectorStore`. It searches for the top `k` (2 in this case) documents most similar to the query \"biology\", applying an optional metadata filter. The results are then iterated and printed.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/supabase.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nconst filter = { source: \"https://example.com\" };\n\nconst similaritySearchResults = await vectorStore.similaritySearch(\"biology\", 2, filter);\n\nfor (const doc of similaritySearchResults) {\n  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Chat Model Unit Tests with ChatModelUnitTests\nDESCRIPTION: Demonstrates how to create a unit test file for chat models by extending the ChatModelUnitTests class, configuring the test environment, and running standard tests against a custom chat model implementation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-standard-tests/README.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n/* eslint-disable no-process-env */\nimport { test, expect } from \"@jest/globals\";\nimport { ChatModelUnitTests } from \"@langchain/standard-tests\";\nimport { AIMessageChunk } from \"@langchain/core/messages\";\nimport { MyChatModel, MyChatModelCallOptions } from \"../chat_models.js\";\n\nclass MyChatModelStandardUnitTests extends ChatModelUnitTests<\n  MyChatModelCallOptions,\n  AIMessageChunk\n> {\n  constructor() {\n    super({\n      Cls: MyChatModel,\n      chatModelHasToolCalling: true, // Set to true if the model has tool calling support\n      chatModelHasStructuredOutput: true, // Set to true if the model has withStructuredOutput support\n      constructorArgs: {}, // Any additional constructor args\n    });\n    // This must be set so method like `.bindTools` or `.withStructuredOutput`\n    // which we call after instantiating the model will work. \n    // (constructor will throw if API key is not set)\n    process.env.CHAT_MODEL_API_KEY = \"test\";\n  }\n\n  testChatModelInitApiKey() {\n    // Unset the API key env var here so this test can properly check\n    // the API key class arg.\n    process.env.CHAT_MODEL_API_KEY = \"\";\n    super.testChatModelInitApiKey();\n    // Re-set the API key env var here so other tests can run properly.\n    process.env.CHAT_MODEL_API_KEY = \"test\";\n  }\n}\n\nconst testClass = new MyChatModelStandardUnitTests();\n\ntest(\"MyChatModelStandardUnitTests\", () => {\n  const testResults = testClass.runTests();\n  expect(testResults).toBe(true);\n});\n```\n\n----------------------------------------\n\nTITLE: Merging Faiss Indexes\nDESCRIPTION: Demonstrates how to create and merge multiple Faiss vector stores, including creating new stores from existing ones and merging their content.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/faiss.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\n// Create an initial vector store\nconst initialStore = await FaissStore.fromTexts(\n  [\"Hello world\", \"Bye bye\", \"hello nice world\"],\n  [{ id: 2 }, { id: 1 }, { id: 3 }],\n  new OpenAIEmbeddings()\n);\n\n// Create another vector store from texts\nconst newStore = await FaissStore.fromTexts(\n  [\"Some text\"],\n  [{ id: 1 }],\n  new OpenAIEmbeddings()\n);\n\n// merge the first vector store into vectorStore2\nawait newStore.mergeFrom(initialStore);\n\n// You can also create a new vector store from another FaissStore index\nconst newStore2 = await FaissStore.fromIndex(\n  newStore,\n  new OpenAIEmbeddings()\n);\n\nawait newStore2.similaritySearch(\"Bye bye\", 1);\n```\n\n----------------------------------------\n\nTITLE: Setting JigsawStack API Key Environment Variable\nDESCRIPTION: Sets up the environment variable for JigsawStack API authentication.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/jigsawstack.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport JIGSAWSTACK_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Installing Development Dependencies\nDESCRIPTION: Command for installing package development dependencies using Yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/create-langchain-integration/template/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nyarn install\n```\n\n----------------------------------------\n\nTITLE: Setting Tavily API Key as Environment Variable in TypeScript\nDESCRIPTION: Shows how to set up the Tavily API key as an environment variable, which is required for using the TavilySearchResults tool.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/tavily_search_community.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.TAVILY_API_KEY = \"YOUR_API_KEY\"\n```\n\n----------------------------------------\n\nTITLE: Installing Cloudflare Workers Types in TypeScript\nDESCRIPTION: This command installs the necessary TypeScript types for working with Cloudflare Workers, which may be required when using Cloudflare KV for caching.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chat_model_caching.mdx#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -S @cloudflare/workers-types\n```\n\n----------------------------------------\n\nTITLE: Streaming Q&A Chain Output in LangChain.js\nDESCRIPTION: Implementation of streaming functionality for the Q&A chain output, demonstrating how to process and display chunks of the response as they are generated.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_streaming.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nfor await (const chunk of await ragChainWithSource.stream(\"What is task decomposition?\")) {\n  console.log(chunk)\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Documents with LangChain Document Loader\nDESCRIPTION: Shows how to load documents using the document loader and access the first document. This template demonstrates the basic load operation common to all document loaders.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/document_loaders.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst docs = await loader.load()\ndocs[0]\n```\n\n----------------------------------------\n\nTITLE: Installing CloseVector Web for Browser Environment\nDESCRIPTION: Command to install the CloseVector Web package for use in browser environments.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/closevector.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -S closevector-web\n```\n\n----------------------------------------\n\nTITLE: Installing Azure OpenAI Integration Dependencies\nDESCRIPTION: Command to install the required packages for using Azure OpenAI with LangChain.js. Includes the OpenAI provider package and core LangChain functionality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/microsoft.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Implementing Map-Reduce Prompts\nDESCRIPTION: Definition of prompts for map and reduce steps in the map-reduce summarization approach.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/summarization.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\nconst mapPrompt = ChatPromptTemplate.fromMessages(\n  [\n    [\"user\", \"Write a concise summary of the following: \\n\\n{context}\"]\n  ]\n)\n```\n\n----------------------------------------\n\nTITLE: Streaming with ChatCohere Model\nDESCRIPTION: Example showing how to use streaming capability with the ChatCohere model. This allows receiving partial responses as they're generated.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-cohere/README.md#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HumanMessage } from \"@langchain/core/messages\";\nimport { ChatCohere } from \"@langchain/cohere\";\n\nconst model = new ChatCohere({\n  apiKey: process.env.COHERE_API_KEY,\n});\nconst response = await model.stream([new HumanMessage(\"Hello world!\")]);\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages\nDESCRIPTION: Command to install necessary LangChain and AI SDK dependencies\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/stream_agent_client.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install langchain @langchain/core @langchain/community ai\n```\n\n----------------------------------------\n\nTITLE: Installing Required LangChain Dependencies\nDESCRIPTION: Command to install necessary dependencies for working with LangChain, including vector storage, OpenAI integration, community tools, and core functionality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/apify_dataset.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install hnswlib-node @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Langchain Packages for Node.js\nDESCRIPTION: Installs Langchain packages which might be utilized alongside AWS Step Functions in Node.js applications. This command is necessary for setting up the environment to use various Langchain functionalities.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/sfn_agent.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Output Format of Text-based Few Shot Template in LangChainJS\nDESCRIPTION: Shows the formatted output of a text-based few shot template, which returns a string representation of the examples.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot.mdx#2025-04-22_snippet_14\n\nLANGUAGE: typescript\nCODE:\n```\nconsole.log(\"Few Shot: \", await fewShotPrompt.formatPromptValue({}));\n/**\nFew Shot: \n\nH: Could the members of The Police perform lawful arrests?\nAI: what can the members of The Police do?\n\nH: Jan Sindel's was born in what country?\nAI: what is Jan Sindel's personal history?\n */\n```\n\n----------------------------------------\n\nTITLE: Creating Convex Project Bash\nDESCRIPTION: Initializes a new Convex project using npm. This command is crucial for setting up the environment and dependencies required for Convex development.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/convex.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm create convex@latest\n```\n\n----------------------------------------\n\nTITLE: Installing Xata CLI\nDESCRIPTION: Command to install the Xata command-line interface globally using npm.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/xata.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @xata.io/cli -g\n```\n\n----------------------------------------\n\nTITLE: Installing GOAT Wallet Packages in Bash\nDESCRIPTION: This snippet installs specific wallet packages for the GOAT finance toolkit, namely `@goat-sdk/wallet-evm` and `@goat-sdk/wallet-viem`. Prerequisites include having NPM installed. The command will pull the specified packages from the NPM registry to be used with the GOAT finance toolkit.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/goat.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @goat-sdk/wallet-evm @goat-sdk/wallet-viem\n```\n\n----------------------------------------\n\nTITLE: Output of Function Partials with Few Shot Templates in LangChainJS\nDESCRIPTION: Shows the output of a text-based few shot template using function partials.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot.mdx#2025-04-22_snippet_17\n\nLANGUAGE: txt\nCODE:\n```\nboobaz\\n\n```\n\n----------------------------------------\n\nTITLE: Logging ChatNovitaAI Response Content\nDESCRIPTION: Demonstrates how to access and log the content from the model's response.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/novita.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(aiMsg.content)\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Google AI and LangSmith\nDESCRIPTION: Instructions for setting up required API keys as environment variables for Google AI and optional LangSmith tracing.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/google_generativeai.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport GOOGLE_API_KEY=\"your-api-key\"\n```\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Basic Anthropic Chat Model Usage\nDESCRIPTION: Example of initializing and using the ChatAnthropic model for basic interaction\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-anthropic/README.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst model = new ChatAnthropic({\n  apiKey: process.env.ANTHROPIC_API_KEY,\n});\nconst response = await model.invoke({\n  role: \"user\",\n  content: \"Hello world!\",\n});\n```\n\n----------------------------------------\n\nTITLE: Setting Airtable API Token in TypeScript\nDESCRIPTION: This snippet shows how to set the Airtable API token as an environment variable, which is a prerequisite for using the AirtableLoader.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/airtable.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.AIRTABLE_API_TOKEN = \"YOUR_AIRTABLE_API_TOKEN\";\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Development\nDESCRIPTION: Command to install all development dependencies using yarn for the @langchain/cloudflare package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-cloudflare/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyarn install\n```\n\n----------------------------------------\n\nTITLE: Streaming Tokens in LangGraph JavaScript Application\nDESCRIPTION: This code demonstrates how to stream individual tokens from a LangGraph application, requiring specific versions of @langchain/core and @langchain/langgraph.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nconst stream = await graph.stream(\n  inputs,\n  { streamMode: \"messages\" },\n);\n\nfor await (const [message, _metadata] of stream) {\n  process.stdout.write(message.content + \"|\");\n}\n```\n\n----------------------------------------\n\nTITLE: Processing Submodules in GitHub Repository Loading\nDESCRIPTION: Example showing how to enable processing of Git submodules when loading a GitHub repository by setting the processSubmodules parameter to true along with recursive loading.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/github.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nSubmodulesExample\n```\n\n----------------------------------------\n\nTITLE: Setting Up Custom Parsing Example Query\nDESCRIPTION: Prepares the custom parsing example by creating a query and formatting the prompt with the JSON schema, then displaying the complete prompt for inspection.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/extraction_parse.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst customParsingQuery = \"Anna is 23 years old and she is 6 feet tall\";\n\nconst customParsingPromptValue = await customParsingPrompt.invoke({\n  schema: zodToJsonSchema(peopleSchema),\n  customParsingQuery\n});\n\ncustomParsingPromptValue.toString();\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for LangChain.js\nDESCRIPTION: This command installs the necessary npm packages (@langchain/openai and @langchain/core) for using LangChain.js with OpenAI integration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/time_weighted_vectorstore.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Cleanup Directory\nDESCRIPTION: Removes the directory and all stored files recursively.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/stores/file_system.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport fs from \"fs\";\n\n// Cleanup\nawait fs.promises.rm(\"./messages\", { recursive: true, force: true });\n```\n\n----------------------------------------\n\nTITLE: Adding a New Entrypoint\nDESCRIPTION: Code snippets showing how to configure a new entrypoint in the project config file. This includes adding a basic entrypoint and marking it as requiring an optional dependency.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/contributing/code.mdx#2025-04-22_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\n// ...\nentrypoints: {\n  // ...\n  tools: \"tools/index\",\n},\n// ...\n```\n\nLANGUAGE: typescript\nCODE:\n```\n// ...\nrequiresOptionalDependency: [\n  // ...\n  \"tools/index\",\n],\n// ...\n```\n\n----------------------------------------\n\nTITLE: Detailing Tool Call Chunk in LangchainJS AIMessageChunk (JSON)\nDESCRIPTION: This JSON object, part of the `tool_call_chunks` array within an `AIMessageChunk`, specifies the details of a streamed chunk related to a tool call. It includes the tool's name (`tavily_search_results_json`), the arguments passed as an escaped JSON string, a unique ID for the tool call (`toolu_01NUVejujVo2y8WGVtZ49KAN`), and its index (0) in the sequence of chunks for this call. This is relevant for processing streamed responses from models that support tool calling.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n                  \"name\": \"tavily_search_results_json\",\n                  \"args\": \"{\\\"input\\\":\\\"Oppenheimer 2023 film director age\\\"}\",\n                  \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n                  \"index\": 0\n                }\n```\n\n----------------------------------------\n\nTITLE: Initializing Chat Prompt Template in JavaScript\nDESCRIPTION: This snippet demonstrates how to create a chat prompt template using SystemMessage in JavaScript. It sets up a simple system message for a pirate character.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/prompts_composition.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { AIMessage, HumanMessage, SystemMessage} from \"@langchain/core/messages\"\n\nconst prompt = new SystemMessage(\"You are a nice pirate\")\n```\n\n----------------------------------------\n\nTITLE: Accessing Document Metadata in LangChain\nDESCRIPTION: Demonstrates how to access and log the metadata of the first loaded document. This shows how to inspect document properties after loading.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/document_loaders.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(docs[0].metadata)\n```\n\n----------------------------------------\n\nTITLE: Creating and Using a Pre-built LangGraph React Agent\nDESCRIPTION: Demonstrates creating a React agent using LangGraph with custom tools, memory management, and message processing. Includes implementation of a user age lookup tool and conversation state management.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/versions/migrating_memory/conversation_buffer_window_memory.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { z } from \"zod\";\nimport { v4 as uuidv4 } from 'uuid';\nimport { BaseMessage, trimMessages } from \"@langchain/core/messages\";\nimport { tool } from \"@langchain/core/tools\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { MemorySaver } from \"@langchain/langgraph\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n\nconst getUserAge = tool(\n  (name: string): string => {\n    // This is a placeholder for the actual implementation\n    if (name.toLowerCase().includes(\"bob\")) {\n      return \"42 years old\";\n    }\n    return \"41 years old\";\n  },\n  {\n    name: \"get_user_age\",\n    description: \"Use this tool to find the user's age.\",\n    schema: z.string().describe(\"the name of the user\"),\n  }\n);\n\nconst memory = new MemorySaver();\nconst model2 = new ChatOpenAI({ model: \"gpt-4o\" });\n\nconst stateModifier = async (messages: BaseMessage[]): Promise<BaseMessage[]> => {\n  return trimMessages(\n    messages,\n    {\n      tokenCounter: (msgs) => msgs.length,\n      maxTokens: 5,\n      strategy: \"last\",\n      startOn: \"human\",\n      includeSystem: true,\n      allowPartial: false,\n    }\n  );\n};\n\nconst app2 = createReactAgent({\n  llm: model2,\n  tools: [getUserAge],\n  checkpointSaver: memory,\n  messageModifier: stateModifier,\n});\n\nconst threadId2 = uuidv4();\nconst config2 = { configurable: { thread_id: threadId2 }, streamMode: \"values\" as const };\n\nconst inputMessage2 = {\n  role: \"user\",\n  content: \"hi! I'm bob. What is my age?\",\n}\n\nfor await (const event of await app2.stream({ messages: [inputMessage2] }, config2)) {\n  const lastMessage = event.messages[event.messages.length - 1];\n  console.log(lastMessage.content);\n}\n\nconst followUpMessage2 = {\n  role: \"user\",\n  content: \"do you remember my name?\",\n};\n\nfor await (const event of await app2.stream({ messages: [followUpMessage2] }, config2)) {\n  const lastMessage = event.messages[event.messages.length - 1];\n  console.log(lastMessage.content);\n}\n```\n\n----------------------------------------\n\nTITLE: Using Custom Tagged XML Parser\nDESCRIPTION: Demonstrates the complete implementation of a chain using custom XML tags for structured output.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/output_parser_xml.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\nconst promptWithTags = ChatPromptTemplate.fromTemplate(`{query}\\n{format_instructions}`);\nconst partialedPromptWithTags = await promptWithTags.partial({\n  format_instructions: parserWithTags.getFormatInstructions(),\n});\n\nconst chainWithTags = partialedPromptWithTags.pipe(model).pipe(parserWithTags);\n\nconst outputWithTags = await chainWithTags.invoke({\n  query: \"Generate the shortened filmograph for Tom Hanks.\",\n});\n\nconsole.log(JSON.stringify(outputWithTags, null, 2));\n```\n\n----------------------------------------\n\nTITLE: Installing Development Dependencies for DeepSeek Package\nDESCRIPTION: Command to install the necessary development dependencies for working on the @langchain/deepseek package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-deepseek/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nyarn install\n```\n\n----------------------------------------\n\nTITLE: Installing Required Langchain Packages for MVI Integration\nDESCRIPTION: Installs the necessary Langchain packages using npm for interacting with Momento Vector Index: `@langchain/openai` for OpenAI integration (embeddings), `@langchain/community` for the MVI vector store implementation, and `@langchain/core` for core Langchain functionalities.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/momento_vector_index.mdx#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Calculating Christopher Nolan's Age in Days using Python\nDESCRIPTION: This code snippet calculates Christopher Nolan's age in days as of the current date, using his birthdate of July 30, 1970.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import date\n\nbirthdate = date(1970, 7, 30)\ntoday = date.today()\nage_in_days = (today - birthdate).days\n\nprint(f\"Christopher Nolan is {age_in_days} days old.\")\n```\n\n----------------------------------------\n\nTITLE: Stream Events Processing\nDESCRIPTION: Handling and updating stream events from the agent executor\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/stream_agent_client.mdx#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\n    const streamingEvents = agentExecutor.streamEvents(\n      { input },\n      { version: \"v2\" },\n    );\n\n    for await (const item of streamingEvents) {\n      stream.update(JSON.parse(JSON.stringify(item, null, 2)));\n    }\n\n    stream.done();\n```\n\n----------------------------------------\n\nTITLE: Importing OpenAI model in TypeScript\nDESCRIPTION: Example of importing the OpenAI model from LangChain.js library in a TypeScript file.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/examples/src/document_loaders/example_data/notion.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAI } from \"langchain/llms/openai\";\n```\n\n----------------------------------------\n\nTITLE: Installing Redis Dependencies\nDESCRIPTION: Installation command for Redis client package\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/llm_caching.mdx#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nnpm install ioredis\n```\n\n----------------------------------------\n\nTITLE: Implementing MultiVectorRetriever with Hypothetical Queries in TypeScript\nDESCRIPTION: Example of using MultiVectorRetriever with LLM-generated hypothetical questions to enhance document retrieval based on potential queries.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/multi_vector.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport HypotheticalExample from \"@examples/retrievers/multi_vector_hypothetical.ts\";\n\n<CodeBlock language=\"typescript\">{HypotheticalExample}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Installing LangchainJS SAP HANA Dependencies (Bash)\nDESCRIPTION: This snippet shows the necessary npm commands to install the required LangchainJS community package, core package, and either the SAP HANA client (`@sap/hana-client`) or the alternative `hdb` driver. These packages are prerequisites for interacting with the SAP HANA Cloud Vector Engine via LangchainJS.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/hanavector.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -S @langchain/community @langchain/core @sap/hana-client\n# or\nnpm install -S @langchain/community @langchain/core hdb\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Core Package\nDESCRIPTION: Command for installing only the @langchain/core package which contains base abstractions for the LangChain ecosystem.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/tests/__mdx__/modules/two.mdx#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain.js using Yarn\nDESCRIPTION: Command to install LangChain.js package using Yarn package manager.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/examples/src/document_loaders/example_data/notion.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nyarn add langchain\n```\n\n----------------------------------------\n\nTITLE: Instantiating TavilyExtract in JavaScript\nDESCRIPTION: This snippet shows the instantiation of the TavilyExtract tool with specific options for extractDepth and includeImages. Importing from the @langchain/tavily package is necessary to access TavilyExtract.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/tavily_extract.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { TavilyExtract } from \"@langchain/tavily\";\n\nconst tool = new TavilyExtract({\n  extractDepth: \"basic\",\n  includeImages: false,\n});\n```\n\n----------------------------------------\n\nTITLE: Instantiating GoogleGenerativeAIEmbeddings Model\nDESCRIPTION: Example of creating a new GoogleGenerativeAIEmbeddings instance with specific model and task type configurations.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/google_generativeai.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { GoogleGenerativeAIEmbeddings } from \"@langchain/google-genai\";\nimport { TaskType } from \"@google/generative-ai\";\n\nconst embeddings = new GoogleGenerativeAIEmbeddings({\n  model: \"text-embedding-004\", // 768 dimensions\n  taskType: TaskType.RETRIEVAL_DOCUMENT,\n  title: \"Document title\",\n});\n```\n\n----------------------------------------\n\nTITLE: Splitting HTML Documents with RecursiveCharacterTextSplitter\nDESCRIPTION: Demonstrates how to process HTML documents using RecursiveCharacterTextSplitter with HTML-specific separators. Handles HTML tags, elements, and nested structures appropriately when splitting.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/code_splitter.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst htmlText = `\n<!DOCTYPE html>\n<html>\n    <head>\n        <title>🦜️🔗 LangChain</title>\n        <style>\n            body {\n                font-family: Arial, sans-serif;\n            }\n            h1 {\n                color: darkblue;\n            }\n        </style>\n    </head>\n    <body>\n        <div>\n            <h1>🦜️🔗 LangChain</h1>\n            <p>⚡ Building applications with LLMs through composability ⚡</p>\n        </div>\n        <div>\n            As an open-source project in a rapidly developing field, we are extremely open to contributions.\n        </div>\n    </body>\n</html>\n`\n\nconst htmlSplitter = RecursiveCharacterTextSplitter.fromLanguage(\n  \"html\", {\n      chunkSize: 60,\n      chunkOverlap: 0,\n    }\n)\nconst htmlDocs = await htmlSplitter.createDocuments([htmlText])\nhtmlDocs\n```\n\n----------------------------------------\n\nTITLE: Installing Langchain Azure CosmosDB Package in Bash\nDESCRIPTION: This command uses npm (Node Package Manager) to install the Langchain Azure Cosmos DB integration package (`@langchain/azure-cosmosdb`) and the core Langchain package (`@langchain/core`). These packages are prerequisites for using Azure Cosmos DB for MongoDB vCore as a vector store within a LangchainJS application.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/azure_cosmosdb_mongodb.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/azure-cosmosdb @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Creating Helper Function for Tool Examples in JavaScript\nDESCRIPTION: This snippet defines a helper function to convert tool examples into a format suitable for the LLM model, including handling of tool calls and outputs.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_few_shot.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport {\n    AIMessage,\n    BaseMessage,\n    HumanMessage,\n    SystemMessage,\n    ToolMessage,\n  } from \"@langchain/core/messages\";\n  import { v4 as uuidV4 } from \"uuid\";\n  \n  const toolExampleToMessages = (example: Record<string, any>): Array<BaseMessage> => {\n    const messages: Array<BaseMessage> = [new HumanMessage({ content: example.input })];\n    const openaiToolCalls = example.toolCalls.map((toolCall) => {\n      return {\n        id: uuidV4(),\n        type: \"function\" as const,\n        function: {\n          name: \"search\",\n          arguments: JSON.stringify(toolCall),\n        },\n      };\n    });\n  \n    messages.push(new AIMessage({ content: \"\", additional_kwargs: { tool_calls: openaiToolCalls } }));\n  \n    const toolOutputs = \"toolOutputs\" in example ? example.toolOutputs : Array(openaiToolCalls.length).fill(\"You have correctly called this tool.\");\n    toolOutputs.forEach((output, index) => {\n      messages.push(new ToolMessage({ content: output, tool_call_id: openaiToolCalls[index].id }));\n    });\n  \n    return messages;\n  }\n  \n  const exampleMessages = examples.map((ex) => toolExampleToMessages(ex)).flat();\n```\n\n----------------------------------------\n\nTITLE: Setting up Azure OpenAI Environment Variables in Node.js\nDESCRIPTION: Environment variable configuration for Azure OpenAI service integration in Node.js applications. These variables specify the instance name, deployment names for language and embedding models, API key, and API version.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/microsoft.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nAZURE_OPENAI_API_INSTANCE_NAME=<YOUR_INSTANCE_NAME>\nAZURE_OPENAI_API_DEPLOYMENT_NAME=<YOUR_DEPLOYMENT_NAME>\nAZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME=<YOUR_EMBEDDINGS_DEPLOYMENT_NAME>\nAZURE_OPENAI_API_KEY=<YOUR_KEY>\nAZURE_OPENAI_API_VERSION=\"2024-02-01\"\n```\n\n----------------------------------------\n\nTITLE: Importing SageMaker Endpoint Classes for AWS SageMaker in TypeScript\nDESCRIPTION: This snippet shows how to import the SagemakerEndpoint and SageMakerLLMContentHandler classes for interacting with AWS SageMaker endpoints.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/aws.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport {\n  SagemakerEndpoint,\n  SageMakerLLMContentHandler,\n} from \"@langchain/community/llms/sagemaker_endpoint\";\n```\n\n----------------------------------------\n\nTITLE: Running Linter and Formatter\nDESCRIPTION: Command to ensure code quality by running linter and formatter on the codebase.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-aws/README.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nyarn lint && yarn format\n```\n\n----------------------------------------\n\nTITLE: Installing Azure Cosmos DB Chat Message History Dependencies\nDESCRIPTION: Command to install dependencies for Azure Cosmos DB-based chat message history storage, which provides persistent chat history across sessions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/microsoft.mdx#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/azure-cosmosdb @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for LangChain.js and OpenAI\nDESCRIPTION: This code block sets up environment variables for OpenAI API key and optional LangSmith configuration. It also includes a commented option to reduce tracing latency in non-serverless environments.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/sql_query_checking.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=\"your api key\"\n# Uncomment the below to use LangSmith. Not required.\n# export LANGSMITH_API_KEY=\"your api key\"\n# export LANGSMITH_TRACING=true\n\n# Reduce tracing latency if you are not in a serverless environment\n# export LANGCHAIN_CALLBACKS_BACKGROUND=true\n```\n\n----------------------------------------\n\nTITLE: Retrieving Available Tools from SqlToolkit in TypeScript\nDESCRIPTION: This snippet retrieves and prints the list of tools available in the initialized SqlToolkit. Each tool's name and description are displayed. Ensure the toolkit has been instantiated before executing this operation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/sql.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst tools = toolkit.getTools();\n\nconsole.log(tools.map((tool) => ({\n  name: tool.name,\n  description: tool.description,\n})))\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Community and Core Packages using npm/yarn\nDESCRIPTION: This command installs the required LangChain packages, `@langchain/community` (which includes the Jina integration) and `@langchain/core`, using either npm or yarn. These packages are prerequisites for using the `JinaEmbeddings` class.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/jina.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Setting Optional LangSmith Environment Variables\nDESCRIPTION: Sets optional environment variables for LangSmith tracing to monitor model calls. These are commented out by default and can be enabled by removing the comment symbols.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/openai.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Using ChatOllama with JSON Mode in Python\nDESCRIPTION: Python code demonstrating how to use ChatOllama in JSON mode for structured output, including a prompt template for translation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/ollama.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatOllama } from \"@langchain/ollama\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\nconst promptForJsonMode = ChatPromptTemplate.fromMessages([\n  [\n    \"system\",\n    `You are an expert translator. Format all responses as JSON objects with two keys: \"original\" and \"translated\".`,\n  ],\n  [\"human\", `Translate \"{input}\" into {language}.`],\n]);\n\nconst llmJsonMode = new ChatOllama({\n  baseUrl: \"http://localhost:11434\", // Default value\n  model: \"llama3\",\n  format: \"json\",\n});\n\nconst chainForJsonMode = promptForJsonMode.pipe(llmJsonMode);\n\nconst resultFromJsonMode = await chainForJsonMode.invoke({\n  input: \"I love programming\",\n  language: \"German\",\n});\n\nconsole.log(resultFromJsonMode);\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for OpenAI and Neo4j\nDESCRIPTION: This snippet demonstrates how to set up environment variables for OpenAI API key, LangSmith tracing, and Neo4j database credentials.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/graph_semantic.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: env\nCODE:\n```\nOPENAI_API_KEY=your-api-key\n\n# Optional, use LangSmith for best-in-class observability\nLANGSMITH_API_KEY=your-api-key\nLANGSMITH_TRACING=true\n\n# Reduce tracing latency if you are not in a serverless environment\n# LANGCHAIN_CALLBACKS_BACKGROUND=true\n```\n\nLANGUAGE: env\nCODE:\n```\nNEO4J_URI=\"bolt://localhost:7687\"\nNEO4J_USERNAME=\"neo4j\"\nNEO4J_PASSWORD=\"password\"\n```\n\n----------------------------------------\n\nTITLE: Installing Development Dependencies\nDESCRIPTION: Command to install development dependencies using yarn\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-anthropic/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nyarn install\n```\n\n----------------------------------------\n\nTITLE: Executing Chat Chain with Anthropic\nDESCRIPTION: Shows how to combine the prompt template with the model to create and execute a chain with a topic parameter\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/anthropic.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst chain = prompt.pipe(model);\nawait chain.invoke({ topic: \"bears\" });\n```\n\n----------------------------------------\n\nTITLE: Logging TavilySearchResults Chain Output in TypeScript\nDESCRIPTION: Shows how to extract and log the tool calls and content from the result of a chain that uses the TavilySearchResults tool.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/tavily_search_community.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nconst { tool_calls, content } = toolChainResult;\n\nconsole.log(\"AIMessage\", JSON.stringify({\n  tool_calls,\n  content,\n}, null, 2));\n```\n\n----------------------------------------\n\nTITLE: Instantiating WatsonxLLM with Model Parameters\nDESCRIPTION: Example of instantiating a WatsonxLLM instance with specific model parameters and configuration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/ibm.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport { WatsonxLLM } from \"@langchain/community/llms/ibm\";\n\nconst props = {\n  decoding_method: \"sample\",\n  maxNewTokens: 100,\n  minNewTokens: 1,\n  temperature: 0.5,\n  topK: 50,\n  topP: 1,\n};\nconst instance = new WatsonxLLM({\n  version: \"YYYY-MM-DD\",\n  serviceUrl: process.env.API_URL,\n  projectId: \"<PROJECT_ID>\",\n  // spaceId: \"<SPACE_ID>\",\n  // idOrName: \"<DEPLOYMENT_ID>\",\n  model: \"<MODEL_ID>\",\n  ...props,\n});\n```\n\n----------------------------------------\n\nTITLE: Configuring Self-Query Retriever with Default Search Parameters\nDESCRIPTION: Demonstrates how to set up a self-query retriever with default search parameters and filters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/memory.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconst selfQueryRetrieverWithDefaultParams = SelfQueryRetriever.fromLLM({\n  llm: llm,\n  vectorStore: vectorStore,\n  documentContents: \"Brief summary of a movie\",\n  attributeInfo: attributeInfo,\n  structuredQueryTranslator: new FunctionalTranslator(),\n  searchParams: {\n    filter: (doc: Document) => doc.metadata && doc.metadata.rating > 8.5,\n    mergeFiltersOperator: \"and\",\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Browserbase SDK with npm/yarn\nDESCRIPTION: Command to install the required packages for using Browserbase with LangChain. Includes the LangChain community package, core package, and Browserbase SDK.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/browserbase.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @langchain/community @langchain/core @browserbasehq/sdk\n```\n\n----------------------------------------\n\nTITLE: Reusing MariaDB Connections via Connection Pool - TypeScript\nDESCRIPTION: Shows advanced usage by constructing a MariaDB connection pool and manually creating MariaDBStore instances reusing the same pool. Demonstrates adding documents, performing searches on multiple logical collections within the same physical database, and properly ending the pool. This approach is efficient for resource management and large-scale applications. Requires proper setup and initialization as per LangChain and MariaDB guidelines.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/mariadb.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { MariaDBStore } from \"@langchain/community/vectorstores/mariadb\";\nimport mariadb from \"mariadb\";\n\n// First, follow set-up instructions at\n// https://js.langchain.com/docs/modules/indexes/vector_stores/integrations/mariadb\n\nconst reusablePool = mariadb.createPool({\n  host: \"127.0.0.1\",\n  port: 3306,\n  user: \"myuser\",\n  password: \"ChangeMe\",\n  database: \"api\",\n});\n\nconst originalConfig = {\n  pool: reusablePool,\n  tableName: \"testlangchainjs\",\n  collectionName: \"sample\",\n  collectionTableName: \"collections\",\n  columns: {\n    idColumnName: \"id\",\n    vectorColumnName: \"vect\",\n    contentColumnName: \"content\",\n    metadataColumnName: \"metadata\",\n  },\n};\n\n// Set up the DB.\n// Can skip this step if you've already initialized the DB.\n// await MariaDBStore.initialize(new OpenAIEmbeddings(), originalConfig);\nconst mariadbStore = new MariaDBStore(new OpenAIEmbeddings(), originalConfig);\n\nawait mariadbStore.addDocuments([\n  { pageContent: \"what's this\", metadata: { a: 2 } },\n  { pageContent: \"Cat drinks milk\", metadata: { a: 1 } },\n]);\n\nconst results = await mariadbStore.similaritySearch(\"water\", 1);\n\nconsole.log(results);\n\n/*\n  [ Document { pageContent: 'Cat drinks milk', metadata: { a: 1 } } ]\n*/\n\nconst mariadbStore2 = new MariaDBStore(new OpenAIEmbeddings(), {\n  pool: reusablePool,\n  tableName: \"testlangchainjs\",\n  collectionTableName: \"collections\",\n  collectionName: \"some_other_collection\",\n  columns: {\n    idColumnName: \"id\",\n    vectorColumnName: \"vector\",\n    contentColumnName: \"content\",\n    metadataColumnName: \"metadata\",\n  },\n});\n\nconst results2 = await mariadbStore2.similaritySearch(\"water\", 1);\n\nconsole.log(results2);\n\n/*\n  []\n*/\n\nawait reusablePool.end();\n\n```\n\n----------------------------------------\n\nTITLE: Instantiating MistralAIEmbeddings in JavaScript\nDESCRIPTION: Example of how to create a MistralAIEmbeddings instance with the default 'mistral-embed' model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/mistralai.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { MistralAIEmbeddings } from \"@langchain/mistralai\";\n\nconst embeddings = new MistralAIEmbeddings({\n  model: \"mistral-embed\", // Default value\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Momento SDK for Browser/Edge\nDESCRIPTION: Command to install the Momento Client Library for browser and edge worker environments using npm.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/momento.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @gomomento/sdk-web\n```\n\n----------------------------------------\n\nTITLE: Loading All ChatGPT Conversation Logs with TypeScript\nDESCRIPTION: This snippet demonstrates how to initialize the ChatGPTLoader and extract all conversation logs from a ChatGPT data export file. It creates a loader instance pointing to the JSON export file and then loads all conversation documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/chatgpt.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatGPTLoader } from \"@langchain/community/document_loaders/fs/chatgpt\";\n\nconst loader = new ChatGPTLoader(\"./example_data/example_conversations.json\");\n\nconst docs = await loader.load();\n\nconsole.log(docs);\n```\n\n----------------------------------------\n\nTITLE: Installing LanceDB Node.js Bindings\nDESCRIPTION: This snippet installs the LanceDB Node.js bindings, which is necessary for creating and managing embedded vector databases. The command must be run in a Node.js environment. It downloads and installs the package from npm, making it ready for use in the project.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/lancedb.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -S @lancedb/lancedb\n```\n\n----------------------------------------\n\nTITLE: Listing Available Tools in TypeScript\nDESCRIPTION: Retrieves and displays all tools available in the toolkit with their names and descriptions. This helps users understand what capabilities are available.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/toolkits.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst tools = toolkit.getTools();\n\nconsole.log(tools.map((tool) => ({\n  name: tool.name,\n  description: tool.description,\n})))\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain.js OpenAI Integration\nDESCRIPTION: Command to install the necessary npm packages for using OpenAI with LangChain.js. This includes the OpenAI integration and core LangChain packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/llm_token_usage_tracking.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Yielding Keys with Prefix Filtering\nDESCRIPTION: Demonstrates how to yield all keys or filter keys by prefix using the yieldKeys method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/stores/file_system.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { LocalFileStore } from \"langchain/storage/file_system\"\n\nconst kvStoreForYield = await LocalFileStore.fromPath(\"./messages\");\n\nconst encoderForYield = new TextEncoder();\n\n// Add some data to the store\nawait kvStoreForYield.mset(\n  [\n    [\"message:id:key1\", encoderForYield.encode(\"value1\")],\n    [\"message:id:key2\", encoderForYield.encode(\"value2\")],\n  ]\n)\n\nconst yieldedKeys = [];\nfor await (const key of kvStoreForYield.yieldKeys(\"message:id:\")) {\n  yieldedKeys.push(key);\n}\n\nconsole.log(yieldedKeys);\n```\n\n----------------------------------------\n\nTITLE: Running Environment Tests with Docker\nDESCRIPTION: Command to run environment tests for LangChain.js using Docker. These tests verify compatibility across Node.js (ESM and CJS), Edge environments, and browsers.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/contributing/code.mdx#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nyarn test:exports:docker\n```\n\n----------------------------------------\n\nTITLE: Implementing Few-Shot Learning with Tool Calls\nDESCRIPTION: Demonstrates how to use few-shot examples to teach the model to interpret a custom operator (🦜) as division through example conversations and tool calls.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tools_few_shot.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HumanMessage, AIMessage, ToolMessage } from \"@langchain/core/messages\";\n\nconst res = await llmWithTools.invoke([\n  new HumanMessage(\"What is 333382 🦜 1932?\"),\n  new AIMessage({\n    content: \"The 🦜 operator is shorthand for division, so we call the divide tool.\",\n    tool_calls: [{\n      id: \"12345\",\n      name: \"calculator\",\n      args: {\n        number1: 333382,\n        number2: 1932,\n        operation: \"divide\",\n      }\n    }]\n  }),\n  new ToolMessage({\n    tool_call_id: \"12345\",\n    content: \"The answer is 172.558.\"\n  }),\n  new AIMessage(\"The answer is 172.558.\"),\n  new HumanMessage(\"What is 6 🦜 2?\"),\n  new AIMessage({\n    content: \"The 🦜 operator is shorthand for division, so we call the divide tool.\",\n    tool_calls: [{\n      id: \"54321\",\n      name: \"calculator\",\n      args: {\n        number1: 6,\n        number2: 2,\n        operation: \"divide\",\n      }\n    }]\n  }),\n  new ToolMessage({\n    tool_call_id: \"54321\",\n    content: \"The answer is 3.\"\n  }),\n  new AIMessage(\"The answer is 3.\"),\n  new HumanMessage(\"What is 3 🦜 12?\")\n]);\n\nconsole.log(res.tool_calls);\n```\n\n----------------------------------------\n\nTITLE: Implementing Convex Chat Memory with LangChain in TypeScript\nDESCRIPTION: Demonstrates how to use Convex as a chat memory store with LangChain, including setting up the ChatOpenAI model, ConvexChatMessageHistory, and BufferMemory.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/convex.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ConvexChatMessageHistory } from \"@langchain/community/stores/message/convex\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { BufferMemory } from \"langchain/memory\";\nimport { ConversationChain } from \"langchain/chains\";\nimport { api } from \"../../convex/_generated/api\";\n\n// Replace this with your actual OpenAI API key\nconst OPENAI_API_KEY = \"YOUR_API_KEY\";\n\n// This must be run from a Convex action\nexport default async function myAction(ctx: any) {\n  const sessionId = \"unique-session-id\";\n\n  const chatHistory = new ConvexChatMessageHistory({\n    sessionId,\n    context: ctx,\n    tableName: \"messages\",\n  });\n\n  const memory = new BufferMemory({\n    chatHistory,\n    returnMessages: true,\n    memoryKey: \"history\",\n  });\n\n  const model = new ChatOpenAI({\n    modelName: \"gpt-3.5-turbo\",\n    openAIApiKey: OPENAI_API_KEY,\n  });\n\n  const chain = new ConversationChain({ llm: model, memory });\n\n  const res1 = await chain.call({ input: \"Hi! I'm Jim.\" });\n  console.log(res1);\n  /*\n  {\n    response: \"Hello Jim! It's nice to meet you. How can I assist you today?\"\n  }\n  */\n\n  const res2 = await chain.call({ input: \"What's my name?\" });\n  console.log(res2);\n  /*\n  {\n    response: \"Your name is Jim. You mentioned it in your previous message when you introduced yourself.\"\n  }\n  */\n\n  return \"done\";\n}\n```\n\n----------------------------------------\n\nTITLE: Instantiating Document Loader in LangChain\nDESCRIPTION: Demonstrates how to import and instantiate the document loader class with required and optional parameters. This is a template that needs to be replaced with the actual module name and parameters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/document_loaders.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { __module_name__ } from \"__full_import_path__\"\n\nconst loader = new __module_name__({\n  // required params = ...\n  // optional params = ...\n})\n```\n\n----------------------------------------\n\nTITLE: Invoking the RAG Chain with a Query in Python\nDESCRIPTION: Shows how to use the created RAG chain to process a query and generate a response.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/azion-edgesql.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nawait ragChain.invoke(\"Paris\")\n```\n\n----------------------------------------\n\nTITLE: Running Linter and Formatter\nDESCRIPTION: Command to run the linter and formatter to ensure code quality and consistency for the @langchain/mixedbread-ai package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-mixedbread-ai/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nyarn lint && yarn format\n```\n\n----------------------------------------\n\nTITLE: Building the @langchain/exa package\nDESCRIPTION: Commands to build the package, either from within the package directory or from the repository root.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-exa/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nyarn build\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn build --filter=@langchain/exa\n```\n\n----------------------------------------\n\nTITLE: Importing OpenAI Integration in ESM\nDESCRIPTION: Example of importing the ChatOpenAI class from the OpenAI integration package using ESM syntax.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/tests/__mdx__/modules/two.mdx#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n```\n\n----------------------------------------\n\nTITLE: Initializing AIMessageChunk Constructor in Python\nDESCRIPTION: This code snippet initializes an AIMessageChunk constructor from the langchain_core.messages module. It includes details about a tool use action for searching Oppenheimer film information.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\n{\n  \"lc\": 1,\n  \"type\": \"constructor\",\n  \"id\": [\n    \"langchain_core\",\n    \"messages\",\n    \"AIMessageChunk\"\n  ],\n  \"kwargs\": {\n    \"content\": [\n      {\n        \"type\": \"tool_use\",\n        \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n        \"name\": \"tavily_search_results_json\",\n        \"input\": {\n          \"input\": \"Oppenheimer 2023 film director age\"\n        }\n      }\n    ],\n    \"additional_kwargs\": {\n      \"id\": \"msg_015MqAHr84dBCAqBgjou41Km\",\n      \"type\": \"message\",\n      \"role\": \"assistant\",\n      \"model\": \"claude-3-sonnet-20240229\",\n      \"stop_sequence\": null,\n      \"usage\": {\n        \"input_tokens\": 409,\n        \"output_tokens\": 68\n      },\n      \"stop_reason\": \"tool_use\"\n    },\n    \"tool_call_chunks\": [\n      {\n        \"name\": \"tavily_search_results_json\",\n        \"args\": \"{\\\"input\\\":\\\"Oppenheimer 2023 film director age\\\"}\",\n        \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n        \"index\": 0\n      }\n    ],\n    \"tool_calls\": [\n      {\n        \"name\": \"tavily_search_results_json\",\n        \"args\": {\n          \"input\": \"Oppenheimer 2023 film director age\"\n        },\n        \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\"\n      }\n    ],\n    \"invalid_tool_calls\": [],\n    \"response_metadata\": {}\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Up LangSmith for Observability in TypeScript\nDESCRIPTION: Demonstrates how to set up LangSmith environment variables for tracing and observability when using the Tavily search tool.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/tavily_search_community.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.LANGSMITH_TRACING=\"true\"\nprocess.env.LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Code Formatting\nDESCRIPTION: Commands for running the Prettier formatter to ensure consistent code style. Includes options for both automatically fixing formatting issues and just checking for them.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/contributing/code.mdx#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nyarn format\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn format:check\n```\n\n----------------------------------------\n\nTITLE: Similarity Search with Scores in FaissStore\nDESCRIPTION: Performs a similarity search that returns both matching documents and their similarity scores, displaying the results with formatted scores.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/faiss.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst similaritySearchWithScoreResults = await vectorStore.similaritySearchWithScore(\"biology\", 2);\n\nfor (const [doc, score] of similaritySearchWithScoreResults) {\n  console.log(`* [SIM=${score.toFixed(3)}] ${doc.pageContent} [${JSON.stringify(doc.metadata)}]`);\n}\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain YandexGPT Dependencies\nDESCRIPTION: Command to install the necessary packages for using YandexGPT with LangChain.js. It installs the Yandex integration and core LangChain packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/yandex.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/yandex @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Langchain Tavily Package using MDX\nDESCRIPTION: This code snippet demonstrates how to leverage MDX components for tool installation guidance using npm and Yarn. It employs integration install tooltips to seamlessly transition between npm and Yarn commands.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/tavily_extract.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: mdx\nCODE:\n```\nimport IntegrationInstallTooltip from \"@mdx_components/integration_install_tooltip.mdx\";\nimport Npm2Yarn from \"@theme/Npm2Yarn\";\n\n<IntegrationInstallTooltip></IntegrationInstallTooltip>\n\n<Npm2Yarn>\n  @langchain/tavily @langchain/core\n</Npm2Yarn>\n```\n\n----------------------------------------\n\nTITLE: Installing dependencies for ArxivRetriever in Node.js\nDESCRIPTION: Installation command for required dependencies pdf-parse and fast-xml-parser needed for the ArxivRetriever to parse PDFs and XML responses from the arXiv API.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/arxiv-retriever.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install pdf-parse fast-xml-parser\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Community and Core Packages\nDESCRIPTION: This command installs the @langchain/community and @langchain/core packages using npm. These packages are required to use the AlibabaTongyiEmbeddings class.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/alibaba_tongyi.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Running Test Suites\nDESCRIPTION: Commands to run unit tests and integration tests for the package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-pinecone/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nyarn test\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn test:int\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key\nDESCRIPTION: Configure environment variable for OpenAI API authentication.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=YOUR_KEY\n```\n\n----------------------------------------\n\nTITLE: Importing Azure Cosmos DB NoSQL Semantic Cache\nDESCRIPTION: Code snippet for importing the Azure Cosmos DB NoSQL Semantic Cache class, which enables semantic-based caching of LLM responses using vector embeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/microsoft.mdx#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AzureCosmosDBNoSQLSemanticCache } from \"@langchain/azure-cosmosdb\";\n```\n\n----------------------------------------\n\nTITLE: Setting Up ChatModelTabs Component\nDESCRIPTION: Importing and using a custom ChatModelTabs component to provide model selection functionality in the documentation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_retrieval.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: mdx\nCODE:\n```\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\n\n<ChatModelTabs customVarName=\"llm\" />\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key for Faiss Vector Store\nDESCRIPTION: Sets the OpenAI API key as an environment variable for use with embeddings when working with Faiss vector stores.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/faiss.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.OPENAI_API_KEY = \"YOUR_API_KEY\";\n```\n\n----------------------------------------\n\nTITLE: Installing Google Cloud SQL PostgreSQL Package\nDESCRIPTION: Command to install the @langchain/google-cloud-sql-pg package using yarn package manager.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-google-cloud-sql-pg/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn add @langchain/google-cloud-sql-pg\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Core Package\nDESCRIPTION: Command to install the LangChain core package separately.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/installation.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain.js Dependencies for AI21 Integration\nDESCRIPTION: Command to install the necessary npm packages for using AI21 models with LangChain.js. It installs the community package and the core LangChain package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/ai21.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Dependencies with npm/yarn\nDESCRIPTION: Command to install the required dependencies for working with OpenAI in LangChain.js. This installs both the OpenAI integration package and the core LangChain package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/openai.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Langchain OpenAI and Core Packages\nDESCRIPTION: Commands to install '@langchain/openai' and '@langchain/core' through npm are provided to facilitate vector store operations using Langchain's OpenAI embeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/milvus.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Building the Pinecone Package\nDESCRIPTION: Commands to build the Pinecone package, including both local package build and filtered build from repo root.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-pinecone/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nyarn build\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn build --filter=@langchain/pinecone\n```\n\n----------------------------------------\n\nTITLE: Running Tests and Linting\nDESCRIPTION: Commands for running tests and linting the codebase\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-anthropic/README.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn test\n$ yarn test:int\n```\n\n----------------------------------------\n\nTITLE: Running Unstructured API Docker Container\nDESCRIPTION: Command to download and start the Unstructured API Docker container which is required for processing Markdown documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/document_loader_markdown.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -p 8000:8000 -d --rm --name unstructured-api downloads.unstructured.io/unstructured-io/unstructured-api:latest --port 8000 --host 0.0.0.0\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom URL for ChatOpenAI\nDESCRIPTION: Demonstrates how to customize the base URL for API requests sent by the ChatOpenAI model using the configuration parameter.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst llmWithCustomURL = new ChatOpenAI({\n  temperature: 0.9,\n  configuration: {\n    baseURL: \"https://your_custom_url.com\",\n  },\n});\n\nawait llmWithCustomURL.invoke(\"Hi there!\");\n```\n\n----------------------------------------\n\nTITLE: Creating System Prompt Template\nDESCRIPTION: Defines the main system prompt template that includes the formatted few-shot examples.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/step_back.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst prompt = ChatPromptTemplate.fromMessages([\n  [\n    \"system\",\n    `You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:`,\n  ],\n  formattedFewShot,\n  [\"user\", \"{question}\"],\n]);\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain packages\nDESCRIPTION: Command to install the required LangChain packages for using the HtmlToTextTransformer.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_transformers/html-to-text.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Invoking the RAG Chain in Python\nDESCRIPTION: Shows how to invoke the created RAG chain with a query string.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/retrievers.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nawait ragChain.invoke(\"...\")\n```\n\n----------------------------------------\n\nTITLE: Retrieving Tools from VectorStoreToolkit in TypeScript\nDESCRIPTION: This code snippet shows how to retrieve tools from the VectorStoreToolkit and log their names and descriptions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/vectorstore.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst tools = toolkit.getTools();\n\nconsole.log(tools.map((tool) => ({\n  name: tool.name,\n  description: tool.description,\n})))\n```\n\n----------------------------------------\n\nTITLE: Creating AIMessage Using LangChain Model\nDESCRIPTION: Example showing how to create an AIMessage by invoking a model with a HumanMessage input to generate a response.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/messages.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { HumanMessage } from \"@langchain/core/messages\";\n\nconst aiMessage = await model.invoke([new HumanMessage(\"Tell me a joke\")]);\nconsole.log(aiMessage);\n```\n\n----------------------------------------\n\nTITLE: Chaining Minimax Model Calls in LangChain.js\nDESCRIPTION: Shows how to chain multiple Minimax model calls using LangChain.js, allowing for more complex interactions and processing of responses.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/minimax.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport MinimaxChain from \"@examples/models/chat/minimax_chain.ts\";\n```\n\n----------------------------------------\n\nTITLE: Package.json Configuration for Core Dependencies\nDESCRIPTION: JSON configuration to ensure consistent @langchain/core version across different package managers\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-azure-openai/README.md#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"your-project\",\n  \"version\": \"0.0.0\",\n  \"dependencies\": {\n    \"@langchain/azure-openai\": \"^0.0.4\",\n    \"@langchain/core\": \"^0.3.0\"\n  },\n  \"resolutions\": {\n    \"@langchain/core\": \"^0.3.0\"\n  },\n  \"overrides\": {\n    \"@langchain/core\": \"^0.3.0\"\n  },\n  \"pnpm\": {\n    \"overrides\": {\n      \"@langchain/core\": \"^0.3.0\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing DuckDuckGoSearch Integration - JavaScript/TypeScript\nDESCRIPTION: This snippet shows the installation commands needed to set up DuckDuckGoSearch and its peer dependencies (@langchain/community, @langchain/core, and duck-duck-scrape) in a LangChain.js project. It ensures that all required packages are available before using the search tool, enabling subsequent instantiation and integration steps. There are no input or output parameters involved; simply run the command in the project directory.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/duckduckgo_search.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: mdx\nCODE:\n```\nimport IntegrationInstallTooltip from \"@mdx_components/integration_install_tooltip.mdx\";\nimport Npm2Yarn from \"@theme/Npm2Yarn\";\n\n<IntegrationInstallTooltip></IntegrationInstallTooltip>\n\n<Npm2Yarn>\n  @langchain/community @langchain/core duck-duck-scrape\n</Npm2Yarn>\n```\n\n----------------------------------------\n\nTITLE: Setting Anthropic API Key\nDESCRIPTION: Command to set the Anthropic API key in the environment\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-anthropic/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport ANTHROPIC_API_KEY=your-api-key\n```\n\n----------------------------------------\n\nTITLE: Running Tests\nDESCRIPTION: Commands for running unit and integration tests\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-xai/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn test\n$ yarn test:int\n```\n\n----------------------------------------\n\nTITLE: Instantiating CSVLoader in Node.js\nDESCRIPTION: Creates a new CSVLoader instance by importing it from the community package and specifying the path to a CSV file.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/csv.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { CSVLoader } from \"@langchain/community/document_loaders/fs/csv\"\n\nconst exampleCsvPath = \"../../../../../../langchain/src/document_loaders/tests/example_data/example_separator.csv\";\n\nconst loader = new CSVLoader(exampleCsvPath)\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Migration Script\nDESCRIPTION: Command to install the LangChain migration script package for v0.2.x upgrades.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/versions/v0_2/index.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @langchain/scripts@0.0.14-rc.1\n```\n\n----------------------------------------\n\nTITLE: Configuring Robots.txt Rules for LangChain JS Documentation\nDESCRIPTION: Standard robots.txt configuration that allows all web crawlers access and points to the XML sitemap location. Defines crawler behavior for the js.langchain.com documentation site.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/static/robots.txt#2025-04-22_snippet_0\n\nLANGUAGE: robots.txt\nCODE:\n```\nUser-agent: *\n\nSitemap: https://js.langchain.com/sitemap.xml/\n```\n\n----------------------------------------\n\nTITLE: Installing AWS Step Functions SDK for Node.js\nDESCRIPTION: Installs the AWS Step Functions SDK, which is required to interact with AWS Step Functions from a Node.js application. This command is a prerequisite for any further development involving AWS Step Functions in Node.js.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/sfn_agent.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @aws-sdk/client-sfn\n```\n\n----------------------------------------\n\nTITLE: Creating Additional Math Tools\nDESCRIPTION: Implementation of addition and exponentiation tools using the same pattern as the multiplication tool.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tools_prompting.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst addTool = tool((input) => {\n    return (input.first_int + input.second_int).toString()\n}, {\n    name: \"add\",\n    description: \"Add two integers together.\",\n    schema: z.object({\n        first_int: z.number(),\n        second_int: z.number(),\n    }),\n});\n\nconst exponentiateTool = tool((input) => {\n    return Math.pow(input.first_int, input.second_int).toString()\n}, {\n    name: \"exponentiate\",\n    description: \"Exponentiate the base to the exponent power.\",\n    schema: z.object({\n        first_int: z.number(),\n        second_int: z.number(),\n    }),\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Vercel KV Dependencies\nDESCRIPTION: Command to install required npm packages including LangChain community modules, core functionality, and Vercel KV client.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/stores/vercel_kv_storage.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core @vercel/kv\n```\n\n----------------------------------------\n\nTITLE: Creating Sample Documents for Movie Summaries in JavaScript\nDESCRIPTION: Defines an array of Document objects representing movie summaries with metadata for demonstration purposes in a self-querying retriever setup.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/self_query.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport \"peggy\";\nimport { Document } from \"@langchain/core/documents\";\n\n/**\n * First, we create a bunch of documents. You can load your own documents here instead.\n * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.\n */\nconst docs = [\n  new Document({\n    pageContent:\n      \"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n    metadata: { year: 1993, rating: 7.7, genre: \"science fiction\", length: 122 },\n  }),\n  new Document({\n    pageContent:\n      \"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n    metadata: { year: 2010, director: \"Christopher Nolan\", rating: 8.2, length: 148 },\n  }),\n  new Document({\n    pageContent:\n      \"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n    metadata: { year: 2006, director: \"Satoshi Kon\", rating: 8.6 },\n  }),\n  new Document({\n    pageContent:\n      \"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n    metadata: { year: 2019, director: \"Greta Gerwig\", rating: 8.3, length: 135 },\n  }),\n  new Document({\n    pageContent: \"Toys come alive and have a blast doing so\",\n    metadata: { year: 1995, genre: \"animated\", length: 77 },\n  }),\n  new Document({\n    pageContent: \"Three men walk into the Zone, three men walk out of the Zone\",\n    metadata: {\n      year: 1979,\n      director: \"Andrei Tarkovsky\",\n      genre: \"science fiction\",\n      rating: 9.9,\n    },\n  }),\n];\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for Sonix Audio Integration with LangChain\nDESCRIPTION: Command for installing necessary packages to use the Sonix audio transcription functionality with LangChain. This includes the LangChain community package, LangChain core, and the Sonix speech recognition library.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/sonix_audio_transcription.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core sonix-speech-recognition\n```\n\n----------------------------------------\n\nTITLE: Building the Package\nDESCRIPTION: Commands for building the package either directly or from the repo root\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-xai/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nyarn build\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn build --filter=@langchain/xai\n```\n\n----------------------------------------\n\nTITLE: Installing Development Dependencies for Redis Package\nDESCRIPTION: Command to install all development dependencies for the @langchain/redis package using yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-redis/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyarn install\n```\n\n----------------------------------------\n\nTITLE: Creating Chat Messages for Translation\nDESCRIPTION: Implementing chat messages with system and human roles for translation\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/llm_chain.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { HumanMessage, SystemMessage } from \"@langchain/core/messages\"\n\nconst messages = [\n  new SystemMessage(\"Translate the following from English into Italian\"),\n  new HumanMessage(\"hi!\"),\n];\n\nawait model.invoke(messages)\n```\n\n----------------------------------------\n\nTITLE: Using Mixedbread AI Embeddings\nDESCRIPTION: Example of using the MixedbreadAIEmbeddings class to generate embeddings for text documents. This demonstrates how to initialize the embeddings model with an API key and embed multiple text strings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-mixedbread-ai/README.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst embeddings = new MixedbreadAIEmbeddings({ apiKey: 'your-api-key' });\nconst texts = [\"Baking bread is fun\", \"I love baking\"];\nconst result = await embeddings.embedDocuments(texts);\nconsole.log(result);\n```\n\n----------------------------------------\n\nTITLE: Loading PDF as Single Document in LangChain.js\nDESCRIPTION: This example shows how to load a PDF file as a single document by setting the splitPages option to false.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/document_loader_pdf.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PDFLoader } from \"@langchain/community/document_loaders/fs/pdf\";\n\nconst loader = new PDFLoader(\"src/document_loaders/example_data/example.pdf\", {\n  splitPages: false,\n});\n\nconst docs = await loader.load();\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatOpenAI Model in TypeScript\nDESCRIPTION: Creates an instance of the ChatOpenAI model using the 'gpt-4o-mini' model variant. This setup is required before using tools with LLM capabilities.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_runtime.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst llm = new ChatOpenAI({ model: \"gpt-4o-mini\" })\n```\n\n----------------------------------------\n\nTITLE: Testing Cache Hit\nDESCRIPTION: Example showing faster response time with cached result\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/llm_caching.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconsole.time();\n\n// The second time it is, so it goes faster\nconst res2 = await model.invoke(\"Tell me a joke\");\n\nconsole.log(res2);\n\nconsole.timeEnd();\n\n/*\n  A man walks into a bar and sees a jar filled with money on the counter. Curious, he asks the bartender about it.\n\n  The bartender explains, \"We have a challenge for our customers. If you can complete three tasks, you win all the money in the jar.\"\n\n  Intrigued, the man asks what the tasks are.\n\n  The bartender replies, \"First, you have to drink a whole bottle of tequila without making a face. Second, there's a pitbull out back with a sore tooth. You have to pull it out. And third, there's an old lady upstairs who has never had an orgasm. You have to give her one.\"\n\n  The man thinks for a moment and then confidently says, \"I'll do it.\"\n\n  He grabs the bottle of tequila and downs it in one gulp, without flinching. He then heads to the back and after a few minutes of struggling, emerges with the pitbull's tooth in hand.\n\n  The bar erupts in cheers and the bartender leads the man upstairs to the old lady's room. After a few minutes, the man walks out with a big smile on his face and the old lady is giggling with delight.\n\n  The bartender hands the man the jar of money and asks, \"How\n\n  default: 175.74ms\n*/\n```\n\n----------------------------------------\n\nTITLE: Managing Asynchronous Callbacks in Serverless Environments with TypeScript\nDESCRIPTION: Example demonstrating how to handle the new non-blocking callbacks in LangChain v0.3 when working in serverless environments. Shows both awaiting callbacks explicitly and setting an environment variable for blocking behavior.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/versions/v0_3/index.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RunnableLambda } from \"@langchain/core/runnables\";\nimport { awaitAllCallbacks } from \"@langchain/core/callbacks/promises\";\n\nconst runnable = RunnableLambda.from(() => \"hello!\");\n\nconst customHandler = {\n  handleChainEnd: async () => {\n    await new Promise((resolve) => setTimeout(resolve, 2000));\n    console.log(\"Call finished\");\n  },\n};\n\nconst startTime = new Date().getTime();\n\nawait runnable.invoke({ number: \"2\" }, { callbacks: [customHandler] });\n\nconsole.log(`Elapsed time: ${new Date().getTime() - startTime}ms`);\n\nawait awaitAllCallbacks();\n\nconsole.log(`Final elapsed time: ${new Date().getTime() - startTime}ms`);\n```\n\n----------------------------------------\n\nTITLE: Installing @langchain/core with Yarn\nDESCRIPTION: Command to install the @langchain/core package using Yarn package manager.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/langchain-core/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn add @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith Environment Variables (TypeScript)\nDESCRIPTION: This snippet shows commented-out code for setting LangSmith environment variables (`LANGSMITH_TRACING`, `LANGSMITH_API_KEY`) in TypeScript. Uncommenting these lines enables automated tracing of model calls using LangSmith.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/pinecone.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n// process.env.LANGSMITH_TRACING=\"true\"\n// process.env.LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Loading Documents from Jira Issues in TypeScript (Node.js only)\nDESCRIPTION: This code demonstrates how to use the JiraLoader class to retrieve Jira issues as document objects. It requires Jira credentials (username and API token), a project key, and a host URL to authenticate and fetch issues from a specific Jira project.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/jira.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { JiraLoader } from \"langchain/document_loaders/web/jira\";\n\nconst loader = new JiraLoader({\n  jiraClientConfig: {\n    host: \"<Your Jira Host URL>\", // e.g., \"your-domain.atlassian.net\"\n    email: \"<Your Jira Email>\",\n    apiToken: \"<Your Jira API Token>\",\n  },\n  projectKey: \"<Your Jira Project Key>\", // e.g., \"PROJ\"\n});\n\nconst docs = await loader.load();\n\nconsole.log({ docs });\n```\n\n----------------------------------------\n\nTITLE: Using Mixedbread AI Reranker\nDESCRIPTION: Example of using the MixedbreadAIReranker to rerank documents based on a query. This demonstrates how to initialize the reranker with an API key and compress a list of documents based on relevance to a query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-mixedbread-ai/README.md#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst reranker = new MixedbreadAIReranker({ apiKey: 'your-api-key' });\nconst documents = [{ pageContent: \"To bake bread you need flour\" }, { pageContent: \"To bake bread you need yeast\" }];\nconst query = \"What do you need to bake bread?\";\nconst result = await reranker.compressDocuments(documents, query);\nconsole.log(result);\n```\n\n----------------------------------------\n\nTITLE: Chaining with Prompt Template\nDESCRIPTION: Example of combining a prompt template with the model in a chain for language translation tasks.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/llms.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { PromptTemplate } from \"@langchain/core/prompts\"\n\nconst prompt = PromptTemplate.fromTemplate(\"How to say {input} in {output_language}:\\n\")\n\nconst chain = prompt.pipe(llm);\nawait chain.invoke(\n  {\n    output_language: \"German\",\n    input: \"I love programming.\",\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Nomic and LangChain Core Packages\nDESCRIPTION: Command to install the @langchain/nomic and @langchain/core packages using npm. These packages are required to use the NomicEmbeddings class.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/nomic.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/nomic @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Langchain Dependencies\nDESCRIPTION: Command to install necessary Langchain packages for implementation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/supabase-hybrid.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Vercel KV for Caching in npm\nDESCRIPTION: This command installs the Vercel KV client for use with caching in a Node.js project using npm.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chat_model_caching.mdx#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @vercel/kv\n```\n\n----------------------------------------\n\nTITLE: Configuring LangSmith API Environment Variables in TypeScript\nDESCRIPTION: Example showing how to set up LangSmith API key and tracing for automated query tracking\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/tavily.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// process.env.LANGSMITH_API_KEY = \"<YOUR API KEY HERE>\";\n// process.env.LANGSMITH_TRACING = \"true\";\n```\n\n----------------------------------------\n\nTITLE: Advanced Migration Script Configuration\nDESCRIPTION: Extended configuration options for the migration script including test runs and custom file paths.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/versions/v0_2/index.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nupdateEntrypointsFrom0_x_xTo0_2_x({\n  projectPath: pathToMyProject,\n  tsConfigPath: \"tsconfig.json\", // Path to the tsConfig file. This will be used to load all the project files into the script.\n  testRun: true, // If true, the script will not save any changes, but will log the changes that would be made.\n  files: [\"...\"], // A list of .ts file paths to check. If this is provided, the script will only check these files.\n});\n```\n\n----------------------------------------\n\nTITLE: Embedding Single Text Query\nDESCRIPTION: Shows how to embed a single text query using embedQuery method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/pinecone.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconst singleVector = await embeddings.embedQuery(text);\n\nconsole.log(singleVector.slice(0, 100));\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain and compatible packages - Bash\nDESCRIPTION: Installs `@langchain/openai`, `@langchain/community`, and `@langchain/core` dependencies needed to enable the use of LangChain.js modules, including the SingleStoreVectorStore and related utilities. Should be executed prior to using LangChain-based vector store examples.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/singlestore.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Embedding Multiple Texts with FireworksEmbeddings in Python\nDESCRIPTION: Demonstrates how to generate embedding vectors for multiple texts using the embedDocuments method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/fireworks.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconst text2 = \"LangGraph is a library for building stateful, multi-actor applications with LLMs\";\n\nconst vectors = await embeddings.embedDocuments([text, text2]);\n\nconsole.log(vectors[0].slice(0, 100));\nconsole.log(vectors[1].slice(0, 100));\n```\n\n----------------------------------------\n\nTITLE: Customizing Responses with retrievalQuery Parameter\nDESCRIPTION: Demonstrates how to employ the `retrievalQuery` parameter to customize the responses in Neo4j Vector Store, utilizing TypeScript. Users should be familiar with Neo4j and TypeScript for effective use.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/neo4jvector.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n{RetrievalExample}\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for LangChain Query Analysis\nDESCRIPTION: This snippet shows how to install the necessary dependencies for the LangChain query analysis project using npm or yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_few_shot.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n@langchain/core zod uuid\n```\n\n----------------------------------------\n\nTITLE: Importing ChatOpenAI in ESM\nDESCRIPTION: Example of importing ChatOpenAI in an ESM environment.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/tests/__mdx__/modules/index.mdx#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n```\n\n----------------------------------------\n\nTITLE: Creating Query Analysis Chain\nDESCRIPTION: Sets up a runnable sequence for analyzing queries using chat prompts and structured output\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_multiple_retrievers.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { RunnableSequence, RunnablePassthrough } from \"@langchain/core/runnables\";\n\nconst system = `You have the ability to issue search queries to get information to help answer user information.`\nconst prompt = ChatPromptTemplate.fromMessages(\n[\n    [\"system\", system],\n    [\"human\", \"{question}\"],\n]\n)\nconst llmWithTools = llm.withStructuredOutput(searchSchema, {\nname: \"Search\"\n})\nconst queryAnalyzer = RunnableSequence.from([\n    {\n        question: new RunnablePassthrough(),\n    },\n    prompt,\n    llmWithTools\n])\n```\n\n----------------------------------------\n\nTITLE: Installing Google Vertex AI Integration for LangChain.js\nDESCRIPTION: Install the necessary packages for using Google Vertex AI with LangChain.js. Two options are provided: one for general use and another for web environments.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/google_vertex_ai.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n@langchain/google-vertexai @langchain/core\n```\n\nLANGUAGE: bash\nCODE:\n```\n@langchain/google-vertexai-web @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Directory Structure Example\nDESCRIPTION: Example folder structure showing supported file types for DirectoryLoader\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/directory.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nsrc/document_loaders/example_data/example/\n├── example.json\n├── example.jsonl\n├── example.txt\n└── example.csv\n```\n\n----------------------------------------\n\nTITLE: Formatting Few-Shot Prompt\nDESCRIPTION: Formats the few-shot prompt template into a string for use in the next step.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/step_back.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst formattedFewShot = await fewShotPrompt.format({});\n```\n\n----------------------------------------\n\nTITLE: Installing Google Vertex AI Web Package for LangChain\nDESCRIPTION: Command to install the @langchain/google-vertexai-web package using Yarn package manager. This package enables access to Google AI/ML models and services through Vertex AI.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-google-vertexai-web/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn add @langchain/google-vertexai-web\n```\n\n----------------------------------------\n\nTITLE: Implementing Pretty Print Function for Messages in JavaScript\nDESCRIPTION: This utility function formats and prints messages from the RAG application, including any tool calls made by AI messages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/qa_chat_history.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport { BaseMessage, isAIMessage } from \"@langchain/core/messages\";\n\nconst prettyPrint = (message: BaseMessage) => {\n  let txt = `[${message._getType()}]: ${message.content}`;\n  if (\n    (isAIMessage(message) && message.tool_calls?.length) ||\n    0 > 0\n  ) {\n    const tool_calls = (message as AIMessage)?.tool_calls\n      ?.map((tc) => `- ${tc.name}(${JSON.stringify(tc.args)})`)\n      .join(\"\\n\");\n    txt += ` \\nTools: \\n${tool_calls}`;\n  }\n  console.log(txt);\n};\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Neo4j and OpenAI\nDESCRIPTION: Configuration of environment variables for OpenAI API access and Neo4j database connection.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/graph_prompting.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: env\nCODE:\n```\nOPENAI_API_KEY=your-api-key\n\nNEO4J_URI=\"bolt://localhost:7687\"\nNEO4J_USERNAME=\"neo4j\"\nNEO4J_PASSWORD=\"password\"\n```\n\n----------------------------------------\n\nTITLE: Loading Web Content using CheerioWebLoader\nDESCRIPTION: Implementation of web content loading using CheerioWebLoader to fetch and parse blog post content.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/summarization.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport \"cheerio\";\nimport { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\";\n\nconst pTagSelector = \"p\";\nconst cheerioLoader = new CheerioWebBaseLoader(\n  \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n  {\n    selector: pTagSelector\n  }\n);\n\nconst docs = await cheerioLoader.load();\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for API Access\nDESCRIPTION: Configuration of required environment variables including API key and optional LangSmith tracing\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/text_embedding.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport __env_var_name__=\"your-api-key\"\n```\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Changing Directory to LangChain Project Root\nDESCRIPTION: Bash command to change the current directory to the root of the LangChain project after cloning the repository.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/CONTRIBUTING.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd langchainjs\n```\n\n----------------------------------------\n\nTITLE: Initializing PromptLayerOpenAI Integration with Standard OpenAI in TypeScript\nDESCRIPTION: This code demonstrates how to set up the PromptLayerOpenAI integration with standard OpenAI in LangChain. It requires both an OpenAI API key and a PromptLayer API key, which can be provided through constructor arguments or environment variables.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/prompt_layer_openai.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PromptLayerOpenAI } from \"langchain/llms/openai\";\n\nconst model = new PromptLayerOpenAI({\n  temperature: 0.9,\n  apiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.OPENAI_API_KEY\n  promptLayerApiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.PROMPTLAYER_API_KEY\n});\nconst res = await model.invoke(\n  \"What would be a good company name a company that makes colorful socks?\"\n);\n```\n\n----------------------------------------\n\nTITLE: Installing @langchain/mistralai Package\nDESCRIPTION: Command to install the Mistral AI integration package for LangChain.js along with the required core dependency.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-mistralai/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/mistralai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Using Dataview Syntax in Markdown\nDESCRIPTION: This Markdown snippet demonstrates the use of dataview syntax for embedding metadata within text. It shows different ways of defining dataview fields and includes examples of non-dataview lines for contrast.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-community/src/document_loaders/tests/example_data/obsidian/tags_and_frontmatter.md#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n# Dataview\n\nHere is some data in a [dataview1:: a value] line.\nHere is even more data in a (dataview2:: another value) line.\ndataview3:: more data\nnotdataview4: this is not a field\nnotdataview5: this is not a field\n```\n\n----------------------------------------\n\nTITLE: Creating a New Index from a Loader with CloseVector\nDESCRIPTION: TypeScript code showing how to create a new vector index from a document loader using CloseVector. This snippet is referenced but not directly included in the content.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/closevector.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\n// Code content is dynamically imported and not directly available in the provided text.\n```\n\n----------------------------------------\n\nTITLE: Importing Vercel KV Example\nDESCRIPTION: TypeScript import statements for including CodeBlock theme component and Vercel KV storage example code.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/stores/vercel_kv_storage.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/stores/vercel_kv_storage.ts\";\n```\n\n----------------------------------------\n\nTITLE: Configuring Package Dependencies in package.json\nDESCRIPTION: Example package.json configuration to ensure consistent @langchain/core dependency across multiple package managers (npm, yarn, pnpm).\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-aws/README.md#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"your-project\",\n  \"version\": \"0.0.0\",\n  \"dependencies\": {\n    \"@langchain/aws\": \"^0.0.1\",\n    \"@langchain/core\": \"^0.3.0\"\n  },\n  \"resolutions\": {\n    \"@langchain/core\": \"^0.3.0\"\n  },\n  \"overrides\": {\n    \"@langchain/core\": \"^0.3.0\"\n  },\n  \"pnpm\": {\n    \"overrides\": {\n      \"@langchain/core\": \"^0.3.0\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: OpenAI Response to Few Shot Prompt in LangChainJS\nDESCRIPTION: Shows the response from OpenAI's chat model after being given the few shot prompt to rephrase a specific question.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot.mdx#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nAIMessage {\n  lc_namespace: [ 'langchain', 'schema' ],\n  content: 'What is the capital of France?',\n  additional_kwargs: { function_call: undefined }\n}\n```\n\n----------------------------------------\n\nTITLE: Logging ChatAnthropic Response in Python\nDESCRIPTION: Prints the content of the AI's response message.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/anthropic.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(aiMsg.content)\n```\n\n----------------------------------------\n\nTITLE: Configuring LangSmith tracing (optional)\nDESCRIPTION: Sets up LangSmith API key for automated tracing of individual queries.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/qdrant.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// process.env.LANGSMITH_API_KEY = \"<YOUR API KEY HERE>\";\n// process.env.LANGSMITH_TRACING = \"true\";\n```\n\n----------------------------------------\n\nTITLE: Logging Chat Model Response\nDESCRIPTION: Logs the content of the chat model's response to the console.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/chat.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconsole.log(aiMsg.content)\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Core Packages\nDESCRIPTION: Command to install the main LangChain package and its core dependency using npm or yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/installation.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install langchain @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Running Tests and Code Quality Checks\nDESCRIPTION: Commands for running unit tests, integration tests, linting, and formatting the codebase.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-nomic/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nyarn test\nyarn test:int\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn lint && yarn format\n```\n\n----------------------------------------\n\nTITLE: Implementing LangGraph node with RunnableWithMessageHistory\nDESCRIPTION: Shows how to replace the manual chat history management with RunnableWithMessageHistory in a LangGraph node. This approach simplifies the code by delegating message history management to the runnable.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/versions/migrating_memory/chat_history.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst runnable = new RunnableWithMessageHistory({\n  // ... configuration from existing code\n});\n\nconst callModel = async (\n  state: typeof MessagesAnnotation.State,\n  config: RunnableConfig\n): Promise<Partial<typeof MessagesAnnotation.State>> => {\n  // RunnableWithMessageHistory takes care of reading the message history\n  // and updating it with the new human message and AI response.\n  const aiMessage = await runnable.invoke(state.messages, config);\n  return {\n    messages: [aiMessage]\n  };\n};\n```\n\n----------------------------------------\n\nTITLE: Setting TogetherAI API Key in Environment Variables\nDESCRIPTION: Sets the TOGETHER_AI_API_KEY environment variable for authentication with TogetherAI services. This key is required for making API calls to TogetherAI models.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/together.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport TOGETHER_AI_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Linting and formatting @langchain/ollama code\nDESCRIPTION: Command to run the linter and formatter to ensure code quality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-ollama/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nyarn lint && yarn format\n```\n\n----------------------------------------\n\nTITLE: Running Tests\nDESCRIPTION: Commands to run unit and integration tests for the package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-weaviate/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn test\n$ yarn test:int\n```\n\n----------------------------------------\n\nTITLE: Creating a RunnableParallel in TypeScript\nDESCRIPTION: Illustrates the creation of a RunnableParallel, which runs multiple runnables concurrently with the same input provided to each.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/lcel.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RunnableParallel } from \"@langchain/core/runnables\";\nconst chain = new RunnableParallel({\n  key1: runnable1,\n  key2: runnable2,\n});\n```\n\n----------------------------------------\n\nTITLE: Displaying TavilySearchResults Tool Output in JSON Format\nDESCRIPTION: This JSON snippet contains the output of the TavilySearchResults tool, providing an array of search results related to the Oppenheimer movie. Each result includes details such as title, URL, content summary, and relevance score.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"title\": \"Oppenheimer (2023) - IMDb\",\n    \"url\": \"https://www.imdb.com/title/tt15398776/\",\n    \"content\": \"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.\",\n    \"score\": 0.96643,\n    \"raw_content\": null\n  },\n  {\n    \"title\": \"Christopher Nolan's Oppenheimer - Rotten Tomatoes\",\n    \"url\": \"https://editorial.rottentomatoes.com/article/everything-we-know-about-christopher-nolans-oppenheimer/\",\n    \"content\": \"Billboards and movie theater pop-ups across Los Angeles have been ticking down for months now: Christopher Nolan's epic account of J. Robert Oppenheimer, the father of the atomic bomb, is nearing an explosive release on July 21, 2023. Nolan movies are always incredibly secretive, twists locked alongside totems behind safe doors, actors not spilling an ounce of Earl Grey tea.\",\n    \"score\": 0.92804,\n    \"raw_content\": null\n  },\n  {\n    \"title\": \"Oppenheimer (film) - Wikipedia\",\n    \"url\": \"https://en.wikipedia.org/wiki/Oppenheimer_(film)\",\n    \"content\": \"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\\nCritical response\\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \\\"more objective view of his story from a different character's point of view\\\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \\\"big-atures\\\", since the special effects team had tried to build the models as physically large as possible. He felt that \\\"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\\\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \\\"emotional\\\" and resembling that of a thriller, while also remarking that Nolan had \\\"Trojan-Horsed a biopic into a thriller\\\".[72]\\nCasting\\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\\\", while also underscoring that it is a \\\"huge shift in perception about the reality of Oppenheimer's perception\\\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \\\"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\\\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.\",\n    \"score\": 0.92404,\n    \"raw_content\": null\n  },\n  {\n    \"title\": \"'Oppenheimer' Director Christopher Nolan On Filmmaking at 53: \\\"I Try to ...\",\n    \"url\": \"https://www.everythingzoomer.com/arts-entertainment/2023/11/21/oppenheimer-director-christopher-nolan-on-filmmaking-at-53-i-try-to-challenge-myself-with-every-film/\",\n    \"content\": \"Oppenheimer will be available to own on 4K Ultra HD, Blu-ray and DVD — including more than three hours of bonus features — on November 21.\\nRELATED:\\nVisiting the Trinity Site Featured in 'Oppenheimer' Is a Sobering Reminder of the Horror of Nuclear Weapons\\nBarbenheimer: How 'Barbie' and 'Oppenheimer' Became the Unlikely Movie Marriage of the Summer\\nBlast From the Past: 'Asteroid City' & 'Oppenheimer' and the Age of Nuclear Anxiety\\nEXPLORE  HealthMoneyTravelFoodStyleBook ClubClassifieds#ZoomerDailyPolicy & PerspectiveArts & EntertainmentStars & RoyaltySex & Love\\nCONNECT  FacebookTwitterInstagram\\nSUBSCRIBE  Terms of Subscription ServiceE-NewslettersSubscribe to Zoomer Magazine\\nBROWSE  AboutMastheadContact UsAdvertise with UsPrivacy Policy\\nEverythingZoomer.com is part of the ZoomerMedia Digital Network \\\"I think with experience — and with the experience of watching your films with an audience over the years — you do more and more recognize the human elements that people respond to, and the things that move you and the things that move the audience.\\\"\\n \\\"What's interesting, as you watch the films over time, is that some of his preoccupations are the same, but then some of them have changed over time with who he is as a person and what's going on in his own life,\\\" Thomas said.\\n The British-American director's latest explosive drama, Oppenheimer, which has earned upwards of US$940 million at the global box office, follows theoretical physicist J. Robert Oppenheimer (played by Cillian Murphy) as he leads the team creating the first atomic bomb, as director of the Manhattan Project's Los Alamos Laboratory.\\n Subscribe\\nEverything Zoomer\\n'Oppenheimer' Director Christopher Nolan On Filmmaking at 53: \\\"I Try to Challenge Myself with Every Film\\\"\\nDirector Christopher Nolan poses upon his arrival for the premiere of the movie 'Oppenheimer' in Paris on July 11, 2023.\",\n    \"score\": 0.92002,\n    \"raw_content\": null\n  },\n  {\n    \"title\": \"'Oppenheimer' Review: A Man for Our Time - The New York Times\",\n    \"url\": \"https://www.nytimes.com/2023/07/19/movies/oppenheimer-review-christopher-nolan.html\",\n    \"content\": \"Instead, it is here that the film's complexities and all its many fragments finally converge as Nolan puts the finishing touches on his portrait of a man who contributed to an age of transformational scientific discovery, who personified the intersection of science and politics, including in his role as a Communist boogeyman, who was transformed by his role in the creation of weapons of mass destruction and soon after raised the alarm about the dangers of nuclear war.\\n He served as director of a clandestine weapons lab built in a near-desolate stretch of Los Alamos, in New Mexico, where he and many other of the era's most dazzling scientific minds puzzled through how to harness nuclear reactions for the weapons that killed tens of thousands instantly, ending the war in the Pacific.\\n Nolan integrates these black-and-white sections with the color ones, using scenes from the hearing and the confirmation — Strauss's role in the hearing and his relationship with Oppenheimer directly affected the confirmation's outcome — to create a dialectical synthesis. To signal his conceit, he stamps the film with the words \\\"fission\\\" (a splitting into parts) and \\\"fusion\\\" (a merging of elements); Nolan being Nolan, he further complicates the film by recurrently kinking up the overarching chronology — it is a lot.\\n It's also at Berkeley that Oppenheimer meets the project's military head, Leslie Groves (a predictably good Damon), who makes him Los Alamos's director, despite the leftist causes he supported — among them, the fight against fascism during the Spanish Civil War — and some of his associations, including with Communist Party members like his brother, Frank (Dylan Arnold).\\n \",\n    \"score\": 0.91831,\n    \"raw_content\": null\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Setting AWS Environment Variables\nDESCRIPTION: Required environment variable setup for AWS Bedrock service authentication.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-aws/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport BEDROCK_AWS_REGION=\nexport BEDROCK_AWS_SECRET_ACCESS_KEY=\nexport BEDROCK_AWS_ACCESS_KEY_ID=\n```\n\n----------------------------------------\n\nTITLE: Importing OpenAI LLM in TypeScript\nDESCRIPTION: This snippet demonstrates how to import the OpenAI LLM class from the @langchain/openai package. This class is used to interact with OpenAI's completion-based models.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/openai.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAI } from \"@langchain/openai\";\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Bearer Token Authentication in Bash\nDESCRIPTION: Sets up environment variables for bearer token authentication with IBM watsonx.ai.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_compressors/ibm.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport WATSONX_AI_AUTH_TYPE=bearertoken\nexport WATSONX_AI_BEARER_TOKEN=<YOUR-BEARER-TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Specifying LangChain Dependencies in requirements.txt\nDESCRIPTION: This snippet defines the required versions of LangChain libraries for a Python project. It specifies both the core langchain package and the langchain-community package, both pinned to version 0.3.0.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-community/src/vectorstores/tests/faiss.int.test.data/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nlangchain==0.3.0\nlangchain-community==0.3.0\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Aurora DSQL Integration\nDESCRIPTION: Command to install required npm packages for implementing Aurora DSQL chat memory with LangChain and AWS integration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/aurora_dsql.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core pg @aws-sdk/dsql-signer\n```\n\n----------------------------------------\n\nTITLE: Implementing SQL Query Checking in LangChain.js\nDESCRIPTION: This code snippet showcases the implementation of a SQL query checker in LangChain.js. It demonstrates how to use a language model to validate and potentially correct SQL queries before execution.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/sql_query_checking.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport CodeBlock from \"@theme/CodeBlock\";\nimport FullExample from \"@examples/use_cases/sql/query_checking.ts\";\n\n<CodeBlock language=\"typescript\">{FullExample}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Rendering Index Table Component\nDESCRIPTION: Renders the IndexTable component to display all available document loaders.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/index.mdx#2025-04-22_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\n<IndexTable />\n```\n\n----------------------------------------\n\nTITLE: Creating a FewShotChatMessagePromptTemplate in LangChainJS\nDESCRIPTION: This code configures a few shot prompt template with example inputs and outputs for rephrasing specific questions into more general queries.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst examples = [\n  {\n    input: \"Could the members of The Police perform lawful arrests?\",\n    output: \"what can the members of The Police do?\",\n  },\n  {\n    input: \"Jan Sindel's was born in what country?\",\n    output: \"what is Jan Sindel's personal history?\",\n  },\n];\nconst examplePrompt = ChatPromptTemplate.fromTemplate(`Human: {input}\nAI: {output}`);\nconst fewShotPrompt = new FewShotChatMessagePromptTemplate({\n  examplePrompt,\n  examples,\n  inputVariables: [], // no input variables\n});\n```\n\n----------------------------------------\n\nTITLE: Installing DeepInfra Embeddings Package\nDESCRIPTION: Command to install the required packages for using DeepInfra Embeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/deepinfra.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Testing RAG Application with Direct Response in JavaScript\nDESCRIPTION: This code tests the RAG application with a simple greeting that doesn't require retrieval, demonstrating how it handles direct responses.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/qa_chat_history.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nlet inputs1 = { messages: [{ role: \"user\", content: \"Hello\" }] };\n\nfor await (\n  const step of await graph.stream(inputs1, {\n    streamMode: \"values\",\n  })\n) {\n    const lastMessage = step.messages[step.messages.length - 1];\n    prettyPrint(lastMessage);\n    console.log(\"-----\\n\");\n}\n```\n\n----------------------------------------\n\nTITLE: Instantiating SERPGoogleScholarAPITool\nDESCRIPTION: Code for importing and creating an instance of the Google Scholar search tool with the API key.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/google_scholar.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { SERPGoogleScholarAPITool } from \"@langchain/community/tools/google_scholar\";\n\nconst tool = new SERPGoogleScholarAPITool({\n  apiKey: process.env.SERPAPI_API_KEY,\n});\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key for Stagehand Toolkit\nDESCRIPTION: This command sets the OpenAI API key as an environment variable, which is required for using OpenAI models with the Stagehand Toolkit.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/stagehand.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=\"your-openai-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Instantiating Chat Model in JavaScript\nDESCRIPTION: Creates an instance of the chat model with specified parameters like model name, temperature, and max tokens.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/chat.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { __module_name__ } from \"__full_import_path__\"\n\nconst llm = new __module_name__({\n  model: \"model-name\",\n  temperature: 0,\n  maxTokens: undefined,\n  timeout: undefined,\n  maxRetries: 2,\n  // other params...\n})\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for LangChain.js Website\nDESCRIPTION: Command to install necessary dependencies for the LangChain.js documentation website using Yarn package manager. Must be run after ensuring all prerequisites are installed.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn\n```\n\n----------------------------------------\n\nTITLE: Using Langchain Tool in a Chain\nDESCRIPTION: Example of using the Langchain tool in a chain by binding it to a tool-calling model and invoking it.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/tools.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n// @lc-docs-hide-cell\n\nimport { ChatOpenAI } from \"@langchain/openai\"\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o-mini\",\n})\n```\n\nLANGUAGE: python\nCODE:\n```\nimport { HumanMessage } from \"@langchain/core/messages\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { RunnableLambda } from \"@langchain/core/runnables\";\n\nconst prompt = ChatPromptTemplate.fromMessages(\n  [\n    [\"system\", \"You are a helpful assistant.\"],\n    [\"placeholder\", \"{messages}\"],\n  ]\n)\n\nconst llmWithTools = llm.bindTools([tool]);\n\nconst chain = prompt.pipe(llmWithTools);\n\nconst toolChain = RunnableLambda.from(\n  async (userInput: string, config) => {\n    const humanMessage = new HumanMessage(userInput,);\n    const aiMsg = await chain.invoke({\n      messages: [new HumanMessage(userInput)],\n    }, config);\n    const toolMsgs = await tool.batch(aiMsg.tool_calls, config);\n    return chain.invoke({\n      messages: [humanMessage, aiMsg, ...toolMsgs],\n    }, config);\n  }\n);\n\nconst toolChainResult = await toolChain.invoke(\"what is the current weather in sf?\");\n```\n\nLANGUAGE: python\nCODE:\n```\nconst { tool_calls, content } = toolChainResult;\n\nconsole.log(\"AIMessage\", JSON.stringify({\n  tool_calls,\n  content,\n}, null, 2));\n```\n\n----------------------------------------\n\nTITLE: Changing Directory to LangChain Core Package\nDESCRIPTION: Bash command to change the current directory to the LangChain core package for development work.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/CONTRIBUTING.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd langchain\n```\n\n----------------------------------------\n\nTITLE: Instantiating RecursiveUrlLoader in Python/TypeScript\nDESCRIPTION: Creates a RecursiveUrlLoader instance with html-to-text conversion and configuration options for crawling depth and directory exclusion.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/recursive_url_loader.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { RecursiveUrlLoader } from \"@langchain/community/document_loaders/web/recursive_url\"\nimport { compile } from \"html-to-text\";\n\nconst compiledConvert = compile({ wordwrap: 130 }); // returns (text: string) => string;\n\nconst loader = new RecursiveUrlLoader(\"https://langchain.com/\",  {\n  extractor: compiledConvert,\n  maxDepth: 1,\n  excludeDirs: [\"/docs/api/\"],\n})\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for Zep Integration\nDESCRIPTION: Command to install necessary npm packages for using Zep with LangChain, including OpenAI and community packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/zep.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Running Linter and Formatter\nDESCRIPTION: Command for running code linting and formatting checks.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/create-langchain-integration/template/README.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nyarn lint && yarn format\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for LangChain SQL Integration\nDESCRIPTION: Instructions for installing required npm packages including langchain, community plugins, OpenAI integration, TypeORM, and SQLite3.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/sql_large_db.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install langchain @langchain/community @langchain/openai typeorm sqlite3\n```\n\n----------------------------------------\n\nTITLE: Adding Documents to FaissStore\nDESCRIPTION: Demonstrates how to add multiple document objects with content and metadata to a Faiss vector store, assigning unique IDs to each document.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/faiss.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport type { Document } from \"@langchain/core/documents\";\n\nconst document1: Document = {\n  pageContent: \"The powerhouse of the cell is the mitochondria\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document2: Document = {\n  pageContent: \"Buildings are made out of brick\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document3: Document = {\n  pageContent: \"Mitochondria are made out of lipids\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document4: Document = {\n  pageContent: \"The 2024 Olympics are in Paris\",\n  metadata: { source: \"https://example.com\" }\n}\n\nconst documents = [document1, document2, document3, document4];\n\nawait vectorStore.addDocuments(documents, { ids: [\"1\", \"2\", \"3\", \"4\"] });\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Alibaba Tongyi Integration\nDESCRIPTION: This command installs the necessary packages @langchain/community and @langchain/core for using Alibaba Tongyi in a LangChain.js project.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/alibaba_tongyi.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Knowledge Graph Construction\nDESCRIPTION: This snippet shows the command to install necessary dependencies for the knowledge graph construction project using npm or yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/graph_constructing.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nlangchain @langchain/community @langchain/openai @langchain/core neo4j-driver zod\n```\n\n----------------------------------------\n\nTITLE: Comparing Chat vs Text Few Shot Templates in LangChainJS\nDESCRIPTION: Sets up both chat-based and text-based few shot templates with the same examples to demonstrate their different APIs.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot.mdx#2025-04-22_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nconst examples = [\n  {\n    input: \"Could the members of The Police perform lawful arrests?\",\n    output: \"what can the members of The Police do?\",\n  },\n  {\n    input: \"Jan Sindel's was born in what country?\",\n    output: \"what is Jan Sindel's personal history?\",\n  },\n];\nconst prompt = `Human: {input}\nAI: {output}`;\nconst examplePromptTemplate = PromptTemplate.fromTemplate(prompt);\nconst exampleChatPromptTemplate = ChatPromptTemplate.fromTemplate(prompt);\nconst chatFewShotPrompt = new FewShotChatMessagePromptTemplate({\n  examplePrompt: exampleChatPromptTemplate,\n  examples,\n  inputVariables: [], // no input variables\n});\nconst fewShotPrompt = new FewShotPromptTemplate({\n  examplePrompt: examplePromptTemplate,\n  examples,\n  inputVariables: [], // no input variables\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Pinecone Integration (Bash)\nDESCRIPTION: This snippet shows the command to install the necessary npm packages for using Pinecone vector stores with LangchainJS, including the Pinecone integration package, OpenAI embeddings package, Langchain core, and the Pinecone client library. It uses the Npm2Yarn component, implying execution in a shell environment.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/pinecone.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n@langchain/pinecone @langchain/openai @langchain/core @pinecone-database/pinecone@5\n```\n\n----------------------------------------\n\nTITLE: Follow-up Query Processing\nDESCRIPTION: Shows how to process follow-up queries while maintaining conversation context using the same thread ID.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nconst result2 = await app.invoke(\n  { input: \"What is one way of doing it?\" },\n  config,\n)\nconsole.log(result2.answer);\n```\n\n----------------------------------------\n\nTITLE: Example File Structure for MultiFileLoader\nDESCRIPTION: This shows the example file structure used with the MultiFileLoader in the documentation. It illustrates how different file types are organized in directories.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/multi_file.mdx#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nsrc/document_loaders/example_data/example/\n├── example.txt\n└── example.csv\n\nsrc/document_loaders/example_data/example2/\n├── example.json\n└── example.jsonl\n```\n\n----------------------------------------\n\nTITLE: Building Documentation Locally with Yarn\nDESCRIPTION: Commands to install dependencies and build documentation locally for the LangChain.js project. This ensures all dependencies are installed in both docs/ and examples/ workspaces before generating the documentation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/contributing/code.mdx#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ncd ..\nyarn\n```\n\n----------------------------------------\n\nTITLE: Invoking Tavily Extract Tool with Direct Arguments in JavaScript\nDESCRIPTION: This snippet demonstrates how to invoke a TavilyExtract instance using explicit argument settings. It requires a list of URLs to function and supports further configuration of extractDepth and image inclusion.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/tavily_extract.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nawait tool.invoke({\n  \"urls\": [\"https://en.wikipedia.org/wiki/Lionel_Messi\"]\n});\n```\n\n----------------------------------------\n\nTITLE: Linting and Formatting Cerebras Package Code\nDESCRIPTION: Command to run linter and formatter to ensure code quality standards.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-cerebras/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nyarn lint && yarn format\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for S3 File Loading in Node.js\nDESCRIPTION: Command to install the necessary npm packages including LangChain community components, LangChain core, and the AWS S3 client SDK for working with S3 files.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/s3.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core @aws-sdk/client-s3\n```\n\n----------------------------------------\n\nTITLE: Installing Anthropic and LangChain Core Dependencies\nDESCRIPTION: Command to install the necessary npm packages for using Anthropic with LangChain.js.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chat_token_usage_tracking.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/anthropic @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Creating Vector Index in libSQL (SQL)\nDESCRIPTION: SQL command to create a vector index on the embedding column (`EMBEDDING_COLUMN`) within the specified table (`TABLE_NAME`). This index uses the `libsql_vector_idx` function, which is crucial for efficient similarity searches. Ensure `TABLE_NAME` and `EMBEDDING_COLUMN` match the values used during table creation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/libsql.mdx#2025-04-22_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nCREATE INDEX IF NOT EXISTS idx_TABLE_NAME_EMBEDDING_COLUMN ON TABLE_NAME(libsql_vector_idx(EMBEDDING_COLUMN));\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for OpenAI Functions Metadata Tagger\nDESCRIPTION: Command to install the necessary packages for using the MetadataTagger document transformer with OpenAI. Requires @langchain/openai and @langchain/core packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_transformers/openai_metadata_tagger.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing LangGraph Package\nDESCRIPTION: Command to install the LangGraph package for building stateful, multi-actor applications with LLMs.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/installation.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/langgraph @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Configuring TypeScript for ESM Projects\nDESCRIPTION: Recommended TypeScript configuration for ESM projects using LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/tests/__mdx__/modules/index.mdx#2025-04-22_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"compilerOptions\": {\n    \"target\": \"ES2020\",\n    \"module\": \"nodenext\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring LangSmith API Settings\nDESCRIPTION: Optional configuration for enabling automated tracing using LangSmith API credentials.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/memory.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// process.env.LANGSMITH_API_KEY = \"<YOUR API KEY HERE>\";\n// process.env.LANGSMITH_TRACING = \"true\";\n```\n\n----------------------------------------\n\nTITLE: Defining Obsidian Journal Entry Frontmatter in YAML\nDESCRIPTION: A YAML frontmatter block that defines tags for an Obsidian journal entry. The tags specify this file is both a journal entry and an Obsidian document.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/examples/src/document_loaders/example_data/obsidian/frontmatter.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntags: journal/entry, obsidian\n---\n```\n\n----------------------------------------\n\nTITLE: Initializing Neo4j Graph and Importing Movie Data\nDESCRIPTION: This code initializes a connection to a Neo4j database and imports movie information using a Cypher query. It creates nodes for movies, people, and genres, and establishes relationships between them.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/graph_mapping.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport \"neo4j-driver\";\nimport { Neo4jGraph } from \"@langchain/community/graphs/neo4j_graph\";\n\nconst url = process.env.NEO4J_URI;\nconst username = process.env.NEO4J_USER;\nconst password = process.env.NEO4J_PASSWORD;\nconst graph = await Neo4jGraph.initialize({ url, username, password });\n\n// Import movie information\nconst moviesQuery = `LOAD CSV WITH HEADERS FROM \n'https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/movies/movies_small.csv'\nAS row\nMERGE (m:Movie {id:row.movieId})\nSET m.released = date(row.released),\n    m.title = row.title,\n    m.imdbRating = toFloat(row.imdbRating)\nFOREACH (director in split(row.director, '|') | \n    MERGE (p:Person {name:trim(director)})\n    MERGE (p)-[:DIRECTED]->(m))\nFOREACH (actor in split(row.actors, '|') | \n    MERGE (p:Person {name:trim(actor)})\n    MERGE (p)-[:ACTED_IN]->(m))\nFOREACH (genre in split(row.genres, '|') | \n    MERGE (g:Genre {name:trim(genre)})\n    MERGE (m)-[:IN_GENRE]->(g))`\n\nawait graph.query(moviesQuery);\n```\n\n----------------------------------------\n\nTITLE: Instantiating ChatCloudflareWorkersAI Model\nDESCRIPTION: Creates a new instance of the ChatCloudflareWorkersAI model with authentication credentials and model configuration. Supports customization via baseUrl for AI Gateway.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/cloudflare_workersai.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatCloudflareWorkersAI } from \"@langchain/cloudflare\";\n\nconst llm = new ChatCloudflareWorkersAI({\n  model: \"@cf/meta/llama-2-7b-chat-int8\", // Default value\n  cloudflareAccountId: CLOUDFLARE_ACCOUNT_ID,\n  cloudflareApiToken: CLOUDFLARE_API_TOKEN,\n  // Pass a custom base URL to use Cloudflare AI Gateway\n  // baseUrl: `https://gateway.ai.cloudflare.com/v1/{YOUR_ACCOUNT_ID}/{GATEWAY_NAME}/workers-ai/`,\n});\n```\n\n----------------------------------------\n\nTITLE: Invoking RAG Chain with ExaRetriever in TypeScript\nDESCRIPTION: This code shows how to invoke the created RAG chain with a specific query to get a response based on retrieved information.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/exa.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nawait ragChain.invoke(\"What did the speaker say about Justice Breyer in the 2022 State of the Union?\");\n```\n\n----------------------------------------\n\nTITLE: Setting Up LangSmith Tracing\nDESCRIPTION: Environment variable configuration for enabling LangSmith tracing of model calls.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/ibm.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Package Dependencies in package.json\nDESCRIPTION: JSON configuration to ensure consistent @langchain/core dependency versions across different package managers (npm, yarn, pnpm) in a project using Mistral AI integration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-mistralai/README.md#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"your-project\",\n  \"version\": \"0.0.0\",\n  \"dependencies\": {\n    \"@langchain/core\": \"^0.3.0\",\n    \"@langchain/mistralai\": \"^0.0.0\"\n  },\n  \"resolutions\": {\n    \"@langchain/core\": \"^0.3.0\"\n  },\n  \"overrides\": {\n    \"@langchain/core\": \"^0.3.0\"\n  },\n  \"pnpm\": {\n    \"overrides\": {\n      \"@langchain/core\": \"^0.3.0\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring FirestoreChatMessageHistory\nDESCRIPTION: Example showing how to initialize FirestoreChatMessageHistory with Firebase credentials and configuration options. Demonstrates setting up collections, docs, and session parameters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/firestore.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { FirestoreChatMessageHistory } from \"@langchain/community/stores/message/firestore\";\nimport admin from \"firebase-admin\";\n\nconst messageHistory = new FirestoreChatMessageHistory({\n  collections: [\"chats\"],\n  docs: [\"user-id\"],\n  sessionId: \"user-id\",\n  userId: \"a@example.com\",\n  config: {\n    projectId: \"YOUR-PROJECT-ID\",\n    credential: admin.credential.cert({\n      projectId: \"YOUR-PROJECT-ID\",\n      privateKey:\n        \"-----BEGIN PRIVATE KEY-----\\nCHANGE-ME\\n-----END PRIVATE KEY-----\\n\",\n      clientEmail: \"CHANGE-ME@CHANGE-ME-TOO.iam.gserviceaccount.com\",\n    }),\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Using JigsawStack Tools Standalone in LangChain.js\nDESCRIPTION: Example of importing and using JigsawStack tools individually for web scraping, AI search, image recognition, speech-to-text conversion, and text-to-SQL generation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/jigsawstack.mdx#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport {\n  JigsawStackAIScrape,\n  JigsawStackAISearch,\n  JigsawStackSpeechToText,\n  JigsawStackVOCR,\n  JigsawStackTextToSQL,\n} from \"@langchain/jigsawstack\";\n\nexport const run = async () => {\n  // AI Scrape Tool\n  const aiScrapeTool = new JigsawStackAIScrape({\n    params: {\n      element_prompts: [\"Pro plan\"],\n    },\n  });\n  const result = await aiScrapeTool.invoke(\"https://jigsawstack.com/pricing\");\n\n  console.log({ result });\n\n  // AI Search Tool\n\n  const aiSearchTool = new JigsawStackAISearch();\n  const doc = await aiSearchTool.invoke(\"The leaning tower of pisa\");\n  console.log({ doc });\n\n  // VOCR Tool\n\n  const vocrTool = new JigsawStackVOCR({\n    params: {\n      prompt: \"Describe the image in detail\",\n    },\n  });\n  const data = await vocrTool.invoke(\n    \"https://rogilvkqloanxtvjfrkm.supabase.co/storage/v1/object/public/demo/Collabo%201080x842.jpg?t=2024-03-22T09%3A22%3A48.442Z\"\n  );\n\n  console.log({ data });\n\n  // Speech-to-Text Tool\n  const sttTool = new JigsawStackSpeechToText();\n  await sttTool.invoke(\n    \"https://rogilvkqloanxtvjfrkm.supabase.co/storage/v1/object/public/demo/Video%201737458382653833217.mp4?t=2024-03-22T09%3A50%3A49.894\"\n  );\n\n  // Text-to-SQL Tool\n  const sqlTool = new JigsawStackTextToSQL({\n    params: {\n      sql_schema:\n        \"CREATE TABLE Transactions (transaction_id INT PRIMARY KEY, user_id INT NOT NULL,total_amount DECIMAL(10, 2 NOT NULL, transaction_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,status VARCHAR(20) DEFAULT 'pending',FOREIGN KEY(user_id) REFERENCES Users(user_id))\",\n    },\n  });\n\n  await sqlTool.invoke(\n    \"Generate a query to get transactions that amount exceed 10000 and sort by when created\"\n  );\n};\n```\n\n----------------------------------------\n\nTITLE: Basic Llama CPP Embedding Implementation\nDESCRIPTION: Example code showing basic usage of Llama CPP embeddings with a local model path configuration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/llama_cpp.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport CodeBlock from \"@theme/CodeBlock\";\nimport BasicExample from \"@examples/embeddings/llama_cpp_basic.ts\";\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for ChatDeepInfra\nDESCRIPTION: Command to install the necessary packages for using Deep Infra chat models with LangChain. Requires both @langchain/community and @langchain/core packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/deep_infra.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Rendering a Themed SVG Diagram with Docusaurus\nDESCRIPTION: This JSX code renders a themed image component that displays different SVGs based on light/dark mode. It shows the LangChain framework architecture diagram with appropriate styling.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/architecture.mdx#2025-04-22_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\n<ThemedImage\n  alt=\"Diagram outlining the hierarchical organization of the LangChain framework, displaying the interconnected parts across multiple layers.\"\n  sources={{\n    light: useBaseUrl(\"/svg/langchain_stack_062024.svg\"),\n    dark: useBaseUrl(\"/svg/langchain_stack_062024_dark.svg\"),\n  }}\n  title=\"LangChain Framework Overview\"\n  style={{ width: \"100%\" }}\n/>\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for OpenAI and Neo4j\nDESCRIPTION: This code block demonstrates how to set up environment variables for OpenAI API key, LangSmith observability, and Neo4j database credentials.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/graph.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: env\nCODE:\n```\nOPENAI_API_KEY=your-api-key\n\n# Optional, use LangSmith for best-in-class observability\nLANGSMITH_API_KEY=your-api-key\nLANGSMITH_TRACING=true\n\n# Reduce tracing latency if you are not in a serverless environment\n# LANGCHAIN_CALLBACKS_BACKGROUND=true\n\nNEO4J_URI=\"bolt://localhost:7687\"\nNEO4J_USERNAME=\"neo4j\"\nNEO4J_PASSWORD=\"password\"\n```\n\n----------------------------------------\n\nTITLE: Running Environment Tests for LangChain.js with Docker\nDESCRIPTION: This command runs environment tests for LangChain.js using Docker to test across different JS environments.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/CONTRIBUTING.md#2025-04-22_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nyarn test:exports:docker\n```\n\n----------------------------------------\n\nTITLE: Instantiating DuckDuckGoSearch Tool - JavaScript/TypeScript\nDESCRIPTION: This snippet demonstrates importing and instantiating the DuckDuckGoSearch tool from the '@langchain/community' package. It creates a new instance with the 'maxResults' parameter set to 1, controlling the number of search results returned for each query. The tool is then ready for invocation; note that '@langchain/community' and its dependencies must be installed.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/duckduckgo_search.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { DuckDuckGoSearch } from \"@langchain/community/tools/duckduckgo_search\"\n\nconst tool = new DuckDuckGoSearch({ maxResults: 1 })\n```\n\n----------------------------------------\n\nTITLE: Combining Multiple Search Conditions in Couchbase Vector Store using TypeScript\nDESCRIPTION: This code example demonstrates how to combine multiple search conditions using AND (conjuncts) operator in Couchbase Vector Store, checking for documents with a specific rating range and date range.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/couchbase.mdx#2025-04-22_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nconst multipleConditionsResult = await store.similaritySearch(texts[0], 4, {\n  fields: [\"metadata.rating\", \"metadata.date\"],\n  searchOptions: {\n    query: {\n      conjuncts: [\n        { min: 3, max: 4, inclusive_max: true, field: \"metadata.rating\" },\n        { start: \"2016-12-31\", end: \"2017-01-02\", field: \"metadata.date\" },\n      ],\n    },\n  },\n});\nconsole.log(multipleConditionsResult[0]);\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith Environment Variables\nDESCRIPTION: Commands to set environment variables for LangSmith, enabling logging and tracing functionality for LangChain applications.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_streaming.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport LANGSMITH_TRACING=true\nexport LANGSMITH_API_KEY=YOUR_KEY\n\n# Reduce tracing latency if you are not in a serverless environment\n# export LANGCHAIN_CALLBACKS_BACKGROUND=true\n```\n\n----------------------------------------\n\nTITLE: Invoking Langchain Tool Directly\nDESCRIPTION: Example of invoking the Langchain tool directly with arguments.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/tools.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nawait tool.invoke(\"...\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Audio Processing with Google Gemini\nDESCRIPTION: Shows how to process audio input using Google's Gemini model. Includes setup of a summary tool and handling of audio file data in base64 format.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_calls_multimodal.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { SystemMessage } from \"@langchain/core/messages\";\nimport { tool } from \"@langchain/core/tools\";\n\nconst summaryTool = tool((input) => {\n  return input.summary;\n}, {\n  name: \"summary_tool\",\n  description: \"Log the summary of the content\",\n  schema: z.object({\n    summary: z.string().describe(\"The summary of the content to log\")\n  }),\n});\n\nconst audioUrl = \"https://www.pacdv.com/sounds/people_sound_effects/applause-1.wav\";\n\nconst axiosRes = await axios.get(audioUrl, { responseType: \"arraybuffer\" });\nconst base64 = btoa(\n  new Uint8Array(axiosRes.data).reduce(\n    (data, byte) => data + String.fromCharCode(byte),\n    ''\n  )\n);\n\nconst model = new ChatGoogleGenerativeAI({ model: \"gemini-1.5-pro-latest\" }).bindTools([summaryTool]);\n\nconst response = await model.invoke([\n  new SystemMessage(\"Summarize this content. always use the summary_tool in your response\"),\n  new HumanMessage({\n  content: [{\n    type: \"media\",\n    mimeType: \"audio/wav\",\n    data: base64,\n  }]\n})]);\n\nconsole.log(response.tool_calls);\n```\n\n----------------------------------------\n\nTITLE: Initializing Claude Model for Extraction\nDESCRIPTION: Creates an instance of the ChatAnthropic class with Claude 3 Sonnet model and zero temperature setting for consistent outputs in extraction tasks.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/extraction_parse.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst model = new ChatAnthropic({\n  model: \"claude-3-sonnet-20240229\",\n  temperature: 0,\n})\n```\n\n----------------------------------------\n\nTITLE: Installing Azure Cosmos DB for MongoDB vCore Integration Dependencies\nDESCRIPTION: Command to install the required packages for using Azure Cosmos DB for MongoDB vCore as a vector store in LangChain.js.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/microsoft.mdx#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/azure-cosmosdb @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for YandexGPT Integration\nDESCRIPTION: Command to install the necessary packages for integrating YandexGPT with LangChain.js.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/yandex.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/yandex @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Configuring Connection to Astra DB\nDESCRIPTION: Configuration object for connecting to Astra DB, a cloud-native Cassandra-as-a-Service platform, using authentication token and service endpoint.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/cassandra.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst configConnection = {\n  serviceProviderArgs: {\n    astra: {\n      token: <...> as string,\n      endpoint: <...> as string,\n    },\n  },\n};\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for LangChain and Anthropic\nDESCRIPTION: Command to install necessary npm packages for LangChain and Anthropic integration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/routing.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/anthropic @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Setting Supabase Environment Variables in TypeScript\nDESCRIPTION: Sets up the necessary environment variables for connecting to a Supabase instance, including the private key and URL. Optionally includes LangSmith API key for tracing.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/supabase.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.SUPABASE_PRIVATE_KEY = \"YOUR_SUPABASE_PRIVATE_KEY\";\nprocess.env.SUPABASE_URL = \"YOUR_SUPABASE_URL\";\n\n// process.env.LANGSMITH_API_KEY = \"<YOUR API KEY HERE>\";\n// process.env.LANGSMITH_TRACING = \"true\";\n```\n\n----------------------------------------\n\nTITLE: Streaming Chat Implementation with Baidu Qianfan\nDESCRIPTION: Example showing how to implement streaming responses with ChatBaiduQianfan, allowing for token-by-token message generation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/baidu_qianfan.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nChatBaiduQianfanStreamExample\n```\n\n----------------------------------------\n\nTITLE: Linting and Formatting Redis Package Code\nDESCRIPTION: Command to run the linter and formatter to ensure code quality and consistency in the @langchain/redis package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-redis/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nyarn lint && yarn format\n```\n\n----------------------------------------\n\nTITLE: Building the Package for Development\nDESCRIPTION: Commands to build the @langchain/cohere package during development, either from the package directory or from the repository root.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-cohere/README.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nyarn build\n```\n\n----------------------------------------\n\nTITLE: Installing Dria with LangChain Dependencies\nDESCRIPTION: Command to install Dria along with required LangChain community and core packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/dria.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install dria @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Rendering All-Time Contributors Component\nDESCRIPTION: Renders the People component to display top all-time contributors to the project.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/people.mdx#2025-04-22_snippet_4\n\nLANGUAGE: jsx\nCODE:\n```\n<People type=\"top_contributors\"></People>\n```\n\n----------------------------------------\n\nTITLE: Rendering Index Table for Key-Value Stores in React\nDESCRIPTION: This code snippet renders an IndexTable component, which likely displays a comprehensive list of all available key-value stores in the LangChain.js project.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/stores/index.mdx#2025-04-22_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\n<IndexTable />\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for MyScale Integration\nDESCRIPTION: Command to install the required Node.js dependencies for using MyScale with LangChain, including OpenAI, ClickHouse client, and LangChain packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/myscale.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -S @langchain/openai @clickhouse/client @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Redis for LangChain.js Embedding Cache\nDESCRIPTION: Command to install the ioredis package as a peer dependency for Redis-based embedding caching in LangChain.js.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/caching_embeddings.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install ioredis\n```\n\n----------------------------------------\n\nTITLE: Single Text Embedding\nDESCRIPTION: Example of embedding a single text using embedQuery method\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/text_embedding.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst singleVector = await embeddings.embedQuery(text);\n\nconsole.log(singleVector.slice(0, 100));\n```\n\n----------------------------------------\n\nTITLE: Polyfilling ReadableStream for Node.js 16\nDESCRIPTION: Code to polyfill ReadableStream for Node.js 16 environments using the web-streams-polyfill package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/tests/__mdx__/modules/two.mdx#2025-04-22_snippet_14\n\nLANGUAGE: typescript\nCODE:\n```\nimport \"web-streams-polyfill/polyfill\";\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Dependencies\nDESCRIPTION: Commands to install required LangChain packages for vector store and OpenAI integration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/vectorstore_retriever.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Listing Error Codes in Markdown\nDESCRIPTION: This snippet provides a list of error codes with links to their corresponding troubleshooting guides. Each error code is represented as a clickable link leading to a specific documentation page.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/troubleshooting/errors/index.mdx#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n- [INVALID_PROMPT_INPUT](/docs/troubleshooting/errors/INVALID_PROMPT_INPUT)\n- [INVALID_TOOL_RESULTS](/docs/troubleshooting/errors/INVALID_TOOL_RESULTS)\n- [MESSAGE_COERCION_FAILURE](/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE)\n- [MODEL_AUTHENTICATION](/docs/troubleshooting/errors/MODEL_AUTHENTICATION)\n- [MODEL_NOT_FOUND](/docs/troubleshooting/errors/MODEL_NOT_FOUND)\n- [MODEL_RATE_LIMIT](/docs/troubleshooting/errors/MODEL_RATE_LIMIT)\n- [OUTPUT_PARSING_FAILURE](/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE)\n```\n\n----------------------------------------\n\nTITLE: Installing node-llama-cpp and LangChain Packages\nDESCRIPTION: Commands for installing the required dependencies: node-llama-cpp version 3 for local model communication and LangChain packages for integration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/llama_cpp.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -S node-llama-cpp@3\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Using ChatVertexAI for Text Generation\nDESCRIPTION: TypeScript code demonstrating how to use the ChatVertexAI class to generate text with Gemini models on VertexAI.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/google.mdx#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatVertexAI } from \"@langchain/google-vertexai\";\n// Or, if using the web entrypoint:\n// import { ChatVertexAI } from \"@langchain/google-vertexai-web\";\n\nconst model = new ChatVertexAI({\n  model: \"gemini-1.0-pro\",\n  maxOutputTokens: 2048,\n});\n\n// Batch and stream are also supported\nconst res = await model.invoke([\n  [\n    \"human\",\n    \"What would be a good company name for a company that makes colorful socks?\",\n  ],\n]);\n```\n\n----------------------------------------\n\nTITLE: Installing AWS SDK DynamoDB Client with npm\nDESCRIPTION: This bash command installs the AWS SDK DynamoDB client, which is necessary for using DynamoDB with LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/aws.mdx#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @aws-sdk/client-dynamodb\n```\n\n----------------------------------------\n\nTITLE: Setting Custom Run ID for Runnable in TypeScript\nDESCRIPTION: Demonstrates how to set a custom run ID when invoking a Runnable. The run ID must be a unique UUID string for each run, used to identify the parent run.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/runnables.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { v4 as uuidv4 } from \"uuid\";\n\nconst runId = uuidv4();\n\nawait someRunnable.invoke(someInput, {\n  runId,\n});\n\n// Do something with the runId\n```\n\n----------------------------------------\n\nTITLE: Setting Google Scholar API Credentials\nDESCRIPTION: Example of setting the required API key as an environment variable for accessing the Google Scholar service.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/google_scholar.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.GOOGLE_SCHOLAR_API_KEY=\"your-serp-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain with Package Managers\nDESCRIPTION: Commands for installing LangChain using npm, Yarn, or pnpm package managers.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/tests/__mdx__/modules/two.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -S langchain\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn add langchain\n```\n\nLANGUAGE: bash\nCODE:\n```\npnpm add langchain\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Core Package\nDESCRIPTION: Command to install only the @langchain/core package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/tests/__mdx__/modules/index.mdx#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Google VertexAI Package for LangChain\nDESCRIPTION: Command to install the Google VertexAI package for use with LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/google.mdx#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/google-vertexai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Dependencies\nDESCRIPTION: Command to install the required LangChain packages for working with OpenAI, community tools, and core functionality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/astradb.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Running Tests for @langchain/cloudflare\nDESCRIPTION: Commands to run unit tests and integration tests for the @langchain/cloudflare package. Unit tests end with .test.ts and integration tests end with .int.test.ts.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-cloudflare/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn test\n$ yarn test:int\n```\n\n----------------------------------------\n\nTITLE: Setting and Getting Data in __module_name__ Key-Value Store\nDESCRIPTION: Demonstrates how to set multiple key-value pairs using the 'mset' method and retrieve them using the 'mget' method. The values are encoded before storing and decoded after retrieval.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/kv_store.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nawait kvStore.mset(\n  [\n    [\"key1\", encoder.encode(\"value1\")],\n    [\"key2\", encoder.encode(\"value2\")],\n  ]\n)\n\nconst results = await kvStore.mget(\n  [\n    \"key1\",\n    \"key2\",\n  ]\n)\nconsole.log(results.map((v) => decoder.decode(v)));\n```\n\n----------------------------------------\n\nTITLE: Initializing Couchbase Vector Store\nDESCRIPTION: Code to initialize the Couchbase Vector Store with an embeddings model and the previously defined configuration. This creates the vector store object for performing similarity searches.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/couchbase.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst store = await CouchbaseVectorStore.initialize(\n  embeddings, // embeddings object to create embeddings from text\n  couchbaseConfig\n);\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Dependencies\nDESCRIPTION: Command to install necessary LangChain packages including OpenAI, community, and core modules.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/momento.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Commands for installing the necessary LangChain packages including OpenAI, community modules, and core functionality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/reduce_retrieval_latency.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Deleting Documents from Vector Store Using delete Method\nDESCRIPTION: This snippet demonstrates an alternative way to delete documents from a vector store using the delete method with an object containing the IDs to delete.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/vectorstores.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nawait vectorStore.deleteDocuments({ ids: [\"doc1\"] });\n```\n\n----------------------------------------\n\nTITLE: Installing LangSmith SDK\nDESCRIPTION: Command to install the LangSmith SDK separately if not using it with LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/installation.mdx#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm install langsmith\n```\n\n----------------------------------------\n\nTITLE: Building the @langchain/cloudflare Package\nDESCRIPTION: Command to build the @langchain/cloudflare package using yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-cloudflare/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nyarn build\n```\n\n----------------------------------------\n\nTITLE: Handling Non-Streaming Components with Stream API\nDESCRIPTION: This example demonstrates a function that doesn't operate on input streams, causing the stream API to break streaming. It extracts country names from JSON data but only works on finalized inputs.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming.ipynb#2025-04-22_snippet_22\n\nLANGUAGE: typescript\nCODE:\n```\n// A function that operates on finalized inputs\n// rather than on an input_stream\nimport { JsonOutputParser } from \"@langchain/core/output_parsers\"\nimport { RunnablePassthrough } from \"@langchain/core/runnables\";\n\n// A function that does not operates on input streams and breaks streaming.\nconst extractCountryNames = (inputs: Record<string, any>) => {\n  if (!Array.isArray(inputs.countries)) {\n    return \"\";\n  }\n  return JSON.stringify(inputs.countries.map((country) => country.name));\n}\n\nconst chain = model.pipe(new JsonOutputParser()).pipe(extractCountryNames);\n\nconst stream = await chain.stream(\n  `output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key \"name\" and \"population\"`\n);\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Packages with npm\nDESCRIPTION: Installs essential LangChain packages, including OpenAI, community utilities, and core components, via npm. This step is necessary to ensure all required components are available for development.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/convex.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Loading Documents from an Existing Apify Dataset in LangChain\nDESCRIPTION: Example code that demonstrates loading documents from an existing Apify Dataset directly into LangChain using the ApifyDatasetLoader's constructor with a dataset ID parameter.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/apify_dataset.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ApifyDatasetLoader } from \"@langchain/community/document_loaders/web/apify_dataset\";\n\n// Get the dataset ID from the Apify console\nconst loader = new ApifyDatasetLoader(\n  \"YOUR_DATASET_ID\",\n  {\n    datasetMappingFunction: (item) => {\n      return {\n        pageContent: item.text || \"\",\n        metadata: {\n          source: item.url,\n          title: item.metadata?.title,\n        },\n      };\n    },\n    clientOptions: {\n      token: \"YOUR_APIFY_API_TOKEN\", // Get your API token at https://console.apify.com/settings/integrations\n    },\n  }\n);\n\nconst docs = await loader.load();\n\nconsole.log(docs.length);\nconsole.log(docs[0].pageContent.slice(0, 500));\n```\n\n----------------------------------------\n\nTITLE: Importing ChatOpenAI in CommonJS\nDESCRIPTION: Example of importing the ChatOpenAI class from the @langchain/openai package in a CommonJS environment.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/installation.mdx#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst { ChatOpenAI } = require(\"@langchain/openai\");\n```\n\n----------------------------------------\n\nTITLE: Loading Documents with LangSmithLoader\nDESCRIPTION: Series of code snippets demonstrating how to load documents and access their metadata.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/langsmith.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst docs = await loader.load()\ndocs[0]\n```\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(docs[0].metadata)\n```\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(docs[0].metadata.inputs)\n```\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(docs[0].metadata.outputs)\n```\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(Object.keys(docs[0].metadata))\n```\n\n----------------------------------------\n\nTITLE: Instantiating ChatOllama Model in Python\nDESCRIPTION: Python code to create a ChatOllama instance with specific model and parameters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/ollama.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatOllama } from \"@langchain/ollama\"\n\nconst llm = new ChatOllama({\n    model: \"llama3\",\n    temperature: 0,\n    maxRetries: 2,\n    // other params...\n})\n```\n\n----------------------------------------\n\nTITLE: Invoking ChatFireworks Model\nDESCRIPTION: Example of invoking the ChatFireworks model with system and human messages for English to French translation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/fireworks.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst aiMsg = await llm.invoke([\n    [\n        \"system\",\n        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n    ],\n    [\"human\", \"I love programming.\"],\n])\naiMsg\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for LangSmith Tracing with AWS Bedrock\nDESCRIPTION: Sets up environment variables to enable LangSmith tracing for AWS Bedrock model calls. These optional environment variables allow for automated tracing of model interactions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/bedrock_converse.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Using YandexGPT LLM for Translation\nDESCRIPTION: Example of using the YandexGPT LLM model to perform English to French translation\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-yandex/README.md#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { YandexGPT } from \"@langchain/yandex\";\nconst model = new YandexGPT();\nconst res = await model.invoke([`Translate \"I love programming\" into French.`]);\n```\n\n----------------------------------------\n\nTITLE: Retrieving Environment Variables in Deno\nDESCRIPTION: Example of how to retrieve environment variables in the Deno runtime, which differs from Node.js syntax. This is mentioned as part of the setup instructions for running the example notebooks.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/README.md#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nDeno.env.get()\n```\n\n----------------------------------------\n\nTITLE: Configuring Package.json for Consistent Core Dependencies\nDESCRIPTION: Example package.json configuration to ensure consistent @langchain/core dependency across different package managers (npm, yarn, pnpm).\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-google-genai/README.md#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"your-project\",\n  \"version\": \"0.0.0\",\n  \"dependencies\": {\n    \"@langchain/core\": \"^0.3.0\",\n    \"@langchain/google-genai\": \"^0.0.0\"\n  },\n  \"resolutions\": {\n    \"@langchain/core\": \"^0.3.0\"\n  },\n  \"overrides\": {\n    \"@langchain/core\": \"^0.3.0\"\n  },\n  \"pnpm\": {\n    \"overrides\": {\n      \"@langchain/core\": \"^0.3.0\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Importing OpenAI Model in TypeScript\nDESCRIPTION: Example of importing the OpenAI model from the LangChain.js library in a TypeScript file.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-community/src/document_loaders/tests/example_data/notion/notion.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAI } from \"langchain/llms/openai\";\n```\n\n----------------------------------------\n\nTITLE: Customizing MetadataTagger with a Custom Prompt\nDESCRIPTION: Example of customizing the MetadataTagger document transformer with a specific prompt. This allows more control over the metadata extraction process by providing instructions to the LLM on how to interpret the documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_transformers/openai_metadata_tagger.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { MetadataTagger } from \"langchain/document_transformers/openai_functions\";\nimport { Document } from \"langchain/document\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\nconst model = new ChatOpenAI({\n  temperature: 0,\n  model: \"gpt-3.5-turbo-0613\",\n});\n\nconst customPrompt = ChatPromptTemplate.fromMessages([\n  [\n    \"system\",\n    \"You are an expert film critic who can extract metadata from movie reviews. Focus especially on the technical aspects of filmmaking like cinematography, sound design, and editing.\",\n  ],\n  [\"human\", \"{input}\"],\n]);\n\nconst metadataTagger = new MetadataTagger({\n  metadataSchema: {\n    properties: {\n      movie_title: { type: \"string\" },\n      director: { type: \"string\" },\n      cinematography_rating: {\n        type: \"number\",\n        description: \"Rating from 1-10 specifically for the cinematography\",\n      },\n      visual_effects_quality: {\n        type: \"string\",\n        enum: [\"excellent\", \"good\", \"average\", \"poor\", \"not_applicable\"],\n      },\n    },\n    required: [\"movie_title\", \"cinematography_rating\"],\n  },\n  llm: model,\n  prompt: customPrompt,\n});\n\nconst documents = [\n  new Document({\n    pageContent:\n      \"Oppenheimer delivers Christopher Nolan's signature complex narrative style with stunning IMAX cinematography. The film makes exceptional use of practical effects rather than CGI, and the sound design particularly during the Trinity test sequence is breathtaking. Cillian Murphy's performance is haunting against the backdrop of the New Mexico desert shots.\",\n  }),\n];\n\nconst taggedDocuments = await metadataTagger.transformDocuments(documents);\n\nconsole.log(taggedDocuments[0].metadata);\n/*\n  {\n    movie_title: 'Oppenheimer',\n    director: 'Christopher Nolan',\n    cinematography_rating: 9,\n    visual_effects_quality: 'excellent'\n  }\n*/\n\n```\n\n----------------------------------------\n\nTITLE: Setting Pinecone API Key in TypeScript\nDESCRIPTION: Sets up the Pinecone API key as an environment variable for authentication.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/pinecone.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.PINECONE_API_KEY = \"your-pinecone-api-key\";\n```\n\n----------------------------------------\n\nTITLE: Initializing Neo4jVectorStore from Existing Graph\nDESCRIPTION: Shows how to instantiate `Neo4jVectorStore` from an existing graph using TypeScript. This requires Neo4j instance access and TypeScript knowledge.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/neo4jvector.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\n{ExistingGraphExample}\n```\n\n----------------------------------------\n\nTITLE: Initializing LangSmithLoader\nDESCRIPTION: Code showing how to initialize a LangSmithLoader instance with configuration options.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/langsmith.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { LangSmithLoader } from \"@langchain/core/document_loaders/langsmith\"\n\nconst loader = new LangSmithLoader({\n  datasetName: \"LangSmith Few Shot Datasets Notebook\",\n  // Instead of a datasetName, you can alternatively provide a datasetId\n  // datasetId: dataset.id,\n  contentKey: \"input\",\n  limit: 5,\n  // formatContent: (content) => content,\n  // ... other options\n})\n```\n\n----------------------------------------\n\nTITLE: Loading Multiple Document Types with DirectoryLoader in JavaScript\nDESCRIPTION: This snippet shows how to use the DirectoryLoader to load documents from a directory with different file types (.json, .jsonl, .txt, .csv) by specifying appropriate loaders for each file extension.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/static/llms.txt#2025-04-22_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { DirectoryLoader } from \"langchain/document_loaders/fs/directory\";\nimport { JSONLoader } from \"langchain/document_loaders/fs/json\";\nimport { TextLoader } from \"langchain/document_loaders/fs/text\";\nimport { CSVLoader } from \"langchain/document_loaders/fs/csv\";\n\nconst loader = new DirectoryLoader(\"path/to/directory\", {\n  \".json\": (path) => new JSONLoader(path),\n  \".jsonl\": (path) => new JSONLoader(path, \"/texts\"),\n  \".txt\": (path) => new TextLoader(path),\n  \".csv\": (path) => new CSVLoader(path)\n});\n\nconst docs = await loader.load();\n```\n\n----------------------------------------\n\nTITLE: Creating a Multiplication Tool\nDESCRIPTION: Implementation of a custom tool for multiplying two integers using the LangChain tool decorator and Zod schema validation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tools_prompting.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n\nconst multiplyTool = tool((input) => {\n    return (input.first_int * input.second_int).toString()\n}, {\n    name: \"multiply\",\n    description: \"Multiply two integers together.\",\n    schema: z.object({\n        first_int: z.number(),\n        second_int: z.number(),\n    })\n})\n```\n\n----------------------------------------\n\nTITLE: Formatting LangChain.js Code\nDESCRIPTION: This command runs the Prettier formatter to enforce code formatting style across the project.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/CONTRIBUTING.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nyarn format\n```\n\n----------------------------------------\n\nTITLE: Instantiating ExaRetriever in TypeScript\nDESCRIPTION: This code demonstrates how to create an instance of ExaRetriever with custom search arguments. It requires the Exa client and API key to be set up.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/exa.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { ExaRetriever } from \"@langchain/exa\";\nimport Exa from \"exa-js\";\n\nconst retriever = new ExaRetriever({\n  // @lc-ts-ignore\n  client: new Exa(\n    process.env.EXASEARCH_API_KEY // default API key\n  ),\n  searchArgs: {\n    numResults: 2,\n  }\n});\n```\n\n----------------------------------------\n\nTITLE: Cancelling Stream with Break Statement in TypeScript\nDESCRIPTION: Demonstrates cancelling a streaming response using a break statement in the for-await loop. This method only stops after at least one chunk has been received from the stream.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/cancel_execution.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst startTimer2 = console.time(\"timer2\");\n\nconst stream = await chain.stream(\"what is the current weather in SF?\");\n\nfor await (const chunk of stream) {\n  console.log(\"chunk\", chunk);\n  break;\n}\n\nconsole.timeEnd(\"timer2\");\n```\n\n----------------------------------------\n\nTITLE: Installing @langchain/cohere Package with npm/yarn\nDESCRIPTION: Command to install the @langchain/cohere package and its dependency @langchain/core using npm or yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-cohere/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/cohere @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Invoking the Language Model\nDESCRIPTION: Code to invoke the language model with a simple greeting message.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/agent_executor.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst response = await model.invoke([{\n  role: \"user\",\n  content: \"hi!\"\n}]);\n\nresponse.content;\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Community Packages\nDESCRIPTION: Command to install the required LangChain community packages to use Arcjet Redact integration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/arcjet.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n@langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Linting and Formatting @langchain/textsplitters\nDESCRIPTION: Command to run the linter and formatter to ensure code quality standards for the @langchain/textsplitters package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-textsplitters/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nyarn lint && yarn format\n```\n\n----------------------------------------\n\nTITLE: Instantiating Language Model using MDX Component Placeholder\nDESCRIPTION: Represents the instantiation of a language model (LLM) using a custom `ChatModelTabs` MDX component. This LLM instance, assigned to the variable `llm`, is required for creating the agent.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/ibm.mdx#2025-04-22_snippet_7\n\nLANGUAGE: mdx\nCODE:\n```\nimport ChatModelTabs from \"@theme/ChatModelTabs\";\n\n<ChatModelTabs customVarName=\"llm\" />\n```\n\n----------------------------------------\n\nTITLE: Formatting PipelinePromptTemplate in JavaScript\nDESCRIPTION: This snippet demonstrates how to format a PipelinePromptTemplate by providing values for all required placeholders. It shows the process of generating the final formatted prompt using the pipeline.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/prompts_composition.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst formattedPrompt = await composedPrompt.format({\n    person: \"Elon Musk\",\n    example_q: `What's your favorite car?`,\n    example_a: \"Telsa\",\n    input: `What's your favorite social media site?`,\n  });\n  \n  \nconsole.log(formattedPrompt);\n```\n\n----------------------------------------\n\nTITLE: Splitting Solidity Code with RecursiveCharacterTextSplitter\nDESCRIPTION: Shows how to process Solidity smart contract code using RecursiveCharacterTextSplitter with Solidity-specific separators. Creates documents from the processed code with specified chunk size.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/code_splitter.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nconst SOL_CODE = `\npragma solidity ^0.8.20;\ncontract HelloWorld {\n   function add(uint a, uint b) pure public returns(uint) {\n       return a + b;\n   }\n}\n`\n\nconst solSplitter = RecursiveCharacterTextSplitter.fromLanguage(\n    \"sol\", {\n        chunkSize: 128,\n        chunkOverlap: 0,\n    }\n)\nconst solDocs = await solSplitter.createDocuments([SOL_CODE])\nsolDocs\n```\n\n----------------------------------------\n\nTITLE: Running Linter\nDESCRIPTION: Command to run the ESLint linter for checking coding standards and identifying potential issues in the codebase.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/contributing/code.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nyarn lint\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI and LangSmith API Keys in Node.js Environment Variables (TypeScript)\nDESCRIPTION: Illustrates how to set environment variables for the OpenAI and (optionally) LangSmith API keys in a Node.js/TypeScript environment. This is required for authentication with OpenAI embeddings and for enabling model call tracing with LangSmith. The key value should be replaced with your actual API key. The OpenAI key is required for embedding functions, and the LangSmith variables can be uncommented for additional tracing features.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/elasticsearch.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.OPENAI_API_KEY = \"YOUR_API_KEY\";\n```\n\nLANGUAGE: typescript\nCODE:\n```\n// process.env.LANGSMITH_TRACING=\"true\"\n// process.env.LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Comparing on_chat_model_end Output in TypeScript (v1 vs v2)\nDESCRIPTION: Demonstrates the difference in output format for the on_chat_model_end event between v1 and v2 of streamEvents API. In v2, the output is simplified and consistent regardless of whether the chat model is run as a root level runnable or part of a chain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/versions/v0_2/migrating_astream_events.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// v1 output as root level runnable\n{\n  data: {\n    output: AIMessageChunk((content = \"hello world!\"), (id = \"some id\"));\n  }\n}\n\n// v1 output as part of a chain\n{\n  data: {\n    output: {\n      generations: [\n        [\n          {\n            generation_info: None,\n            message: AIMessageChunk(\n                content=\"hello world!\", id=\"some id\"\n            ),\n            text: \"hello world!\",\n          }\n        ]\n      ],\n    }\n  },\n}\n\n// v2 output (consistent format)\n{\n  data: {\n    output: AIMessageChunk((content = \"hello world!\"), (id = \"some id\"));\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Starting Chroma with Docker (Docker CLI)\nDESCRIPTION: This snippet demonstrates how to run a Chroma server instance locally using Docker. The first command pulls the Chroma Docker image, while the second runs it, mapping port 8000 on the host. This local instance is required for the vector store to connect, and no credentials are needed when used this way. Docker must be installed.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/chroma.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: docker\nCODE:\n```\ndocker pull chromadb/chroma \ndocker run -p 8000:8000 chromadb/chroma\n```\n\n----------------------------------------\n\nTITLE: Setting Tavily API Key\nDESCRIPTION: Environment variable setup for Tavily search tool\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/stream_agent_client.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport TAVILY_API_KEY=your_api_key\n```\n\n----------------------------------------\n\nTITLE: Creating ChatCohere with Custom Client\nDESCRIPTION: Instantiates ChatCohere with a custom CohereClient for Azure, AWS Bedrock, or standalone Cohere instances.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/cohere.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatCohere } from \"@langchain/cohere\";\nimport { CohereClient } from \"cohere-ai\";\n\nconst client = new CohereClient({\n  token: \"<your-api-key>\",\n  environment: \"<your-cohere-deployment-url>\", //optional\n  // other params\n});\n\nconst llmWithCustomClient = new ChatCohere({\n  client,\n  // other params...\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Node Llama CPP Dependencies\nDESCRIPTION: Command to install the required node-llama-cpp version 3 package for local model communication.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/llama_cpp.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -S node-llama-cpp@3\n```\n\n----------------------------------------\n\nTITLE: Loading PDF with One Document per Page in LangChain.js\nDESCRIPTION: This snippet demonstrates how to use PDFLoader to load a PDF file, creating one document per page by default.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/document_loader_pdf.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PDFLoader } from \"@langchain/community/document_loaders/fs/pdf\";\n// Or, in web environments:\n// import { WebPDFLoader } from \"@langchain/community/document_loaders/web/pdf\";\n// const blob = new Blob(); // e.g. from a file input\n// const loader = new WebPDFLoader(blob);\n\nconst loader = new PDFLoader(\"src/document_loaders/example_data/example.pdf\");\n\nconst docs = await loader.load();\n```\n\n----------------------------------------\n\nTITLE: Instantiating Langchain Tool Module\nDESCRIPTION: Example of importing and instantiating the Langchain tool module.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/tools.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { __module_name__ } from \"__full_import_path__\"\n\n\nconst tool = new __module_name__({\n  // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Installing Development Dependencies\nDESCRIPTION: Command to install package dependencies for development work on the @langchain/aws package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-aws/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nyarn install\n```\n\n----------------------------------------\n\nTITLE: Building LangChain Core\nDESCRIPTION: These commands navigate to the langchain-core directory, install dependencies, and build the core package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/CONTRIBUTING.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncd ../../langchain-core\nyarn\nyarn build\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith API Key for Tracing\nDESCRIPTION: Optional configuration to enable automated tracing of model calls using LangSmith by setting the appropriate environment variables.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/faiss.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// process.env.LANGSMITH_TRACING=\"true\"\n// process.env.LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Correct JSON Object Usage in PromptTemplate\nDESCRIPTION: Example showing correct implementation with properly escaped curly braces for JSON object in the prompt template.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/troubleshooting/errors/INVALID_PROMPT_INPUT.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PromptTemplate } from \"@langchain/core/prompts\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst prompt = PromptTemplate.fromTemplate(`You are a helpful assistant.\n\nHere is an example of how you should respond:\n\n{{\n  \"firstName\": \"John\",\n  \"lastName\": \"Doe\",\n  \"age\": 21\n}}\n\nNow, answer the following question:\n\n{question}`);\n```\n\n----------------------------------------\n\nTITLE: Installing @langchain/cloudflare Package with npm/yarn\nDESCRIPTION: Command to install the @langchain/cloudflare package and its core dependency using npm or yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-cloudflare/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/cloudflare @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Configuring Package Dependencies in package.json\nDESCRIPTION: Example package.json configuration to ensure consistent @langchain/core dependency version across multiple package managers (npm, yarn, and pnpm).\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-azure-cosmosdb/README.md#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"your-project\",\n  \"version\": \"0.0.0\",\n  \"dependencies\": {\n    \"@langchain/core\": \"^0.3.0\",\n    \"@langchain/azure-cosmosdb\": \"^0.2.5\"\n  },\n  \"resolutions\": {\n    \"@langchain/core\": \"0.3.0\"\n  },\n  \"overrides\": {\n    \"@langchain/core\": \"0.3.0\"\n  },\n  \"pnpm\": {\n    \"overrides\": {\n      \"@langchain/core\": \"0.3.0\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Tests for @langchain/textsplitters\nDESCRIPTION: Commands to run unit tests and integration tests for the @langchain/textsplitters package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-textsplitters/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn test\n$ yarn test:int\n```\n\n----------------------------------------\n\nTITLE: Optional LangSmith Tracing Configuration\nDESCRIPTION: Enable LangSmith tracing for automatic model call tracking by setting the LANGSMITH_TRACING and LANGSMITH_API_KEY variables. This is optional but can be useful for debugging and monitoring.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/redis.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n// process.env.LANGSMITH_TRACING=\"true\"\n// process.env.LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Vector-Based Similarity Search with Pre-computed Embeddings\nDESCRIPTION: Demonstrates how to perform a similarity search using pre-computed embeddings instead of raw text queries.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/retrievers.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nconst embedding = await embeddings.embedQuery(\n    \"How were Nike's margins impacted in 2023?\"\n)\n\nconst results3 = await vectorStore.similaritySearchVectorWithScore(\n    embedding, 1\n)\n\nresults3[0]\n```\n\n----------------------------------------\n\nTITLE: Setting Tavily API Key as Environment Variable\nDESCRIPTION: Code snippet showing how to set the Tavily API key as an environment variable in TypeScript. An API key is required to use the Tavily tools.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-tavily/README.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.TAVILY_API_KEY = \"YOUR_API_KEY\";\n```\n\n----------------------------------------\n\nTITLE: Connecting to a Custom GitHub Instance in LangChain.js\nDESCRIPTION: Example demonstrating how to use the GitHub loader with a custom GitHub instance (like GitHub Enterprise) by specifying baseUrl and apiUrl parameters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/github.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nCustomInstanceExample\n```\n\n----------------------------------------\n\nTITLE: Defining Weather Tool Schema\nDESCRIPTION: Creating a Zod schema for the weather tool, defining city and state parameters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/stream_tool_client.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst Weather = z\n  .object({\n    city: z.string().describe(\"City to search for weather\"),\n    state: z.string().describe(\"State abbreviation to search for weather\"),\n  })\n  .describe(\"Weather search parameters\");\n```\n\n----------------------------------------\n\nTITLE: Building the Package\nDESCRIPTION: Commands to build the package either directly or from the repo root.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-weaviate/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nyarn build\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn build --filter=@langchain/weaviate\n```\n\n----------------------------------------\n\nTITLE: Installing Required LangChain Packages\nDESCRIPTION: Command for installing the necessary LangChain packages for working with Anthropic and OpenAI models, as well as core LangChain functionality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/fallbacks.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/anthropic @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Importing Prompt Templates for Few-Shot Learning\nDESCRIPTION: Imports the necessary prompt template classes and creates an example prompt template for the few-shot examples.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/example_selectors.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport { PromptTemplate, FewShotPromptTemplate } from \"@langchain/core/prompts\"\n\nconst examplePrompt = PromptTemplate.fromTemplate(\"Input: {input} -> Output: {output}\")\n```\n\n----------------------------------------\n\nTITLE: Chaining ChatAnthropic with Prompt Template in Python\nDESCRIPTION: Shows how to chain the ChatAnthropic model with a prompt template for language translation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/anthropic.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\"\n\nconst prompt = ChatPromptTemplate.fromMessages(\n    [\n        [\n            \"system\",\n            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n        ],\n        [\"human\", \"{input}\"],\n    ]\n)\n\nconst chain = prompt.pipe(llm);\nawait chain.invoke(\n    {\n        input_language: \"English\",\n        output_language: \"German\",\n        input: \"I love programming.\",\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Deleting Documents from Upstash Vector Store - TypeScript/JavaScript\nDESCRIPTION: This snippet demonstrates how to delete specific documents from the vector store by their unique IDs. Passing an array of IDs to the delete method will remove those entries from the index. This operation is asynchronous and should be awaited; ensure you only attempt to delete IDs that have been previously indexed.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/upstash.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nawait vectorStore.delete({ ids: [\"4\"] });\n```\n\n----------------------------------------\n\nTITLE: Importing Both Few Shot Template Types in LangChainJS\nDESCRIPTION: Imports both chat-based and text-based few shot prompt templates for comparison.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot.mdx#2025-04-22_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nimport {\n  FewShotPromptTemplate,\n  FewShotChatMessagePromptTemplate,\n} from \"langchain/prompts\";\n```\n\n----------------------------------------\n\nTITLE: Deleting Documents from Vector Store\nDESCRIPTION: Example showing how to delete documents from the vector store by their IDs. Demonstrates the delete method with an array of document IDs.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/vectorstores.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nawait vectorStore.delete({ ids: [\"4\"] });\n```\n\n----------------------------------------\n\nTITLE: Instantiating QdrantVectorStore with OpenAI Embeddings\nDESCRIPTION: Creates a vector store instance by connecting to an existing Qdrant collection using OpenAI embeddings. This establishes the connection to the vector database with the specified configuration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/qdrant.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { QdrantVectorStore } from \"@langchain/qdrant\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddings = new OpenAIEmbeddings({\n  model: \"text-embedding-3-small\",\n});\n\nconst vectorStore = await QdrantVectorStore.fromExistingCollection(embeddings, {\n  url: process.env.QDRANT_URL,\n  collectionName: \"langchainjs-testing\",\n});\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Core Dependencies\nDESCRIPTION: Command to install the core LangChain packages needed for the integration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/llama_cpp.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Setting Weaviate Environment Variables\nDESCRIPTION: Environment variable configuration for Weaviate connection settings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-weaviate/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport WEAVIATE_SCHEME=\nexport WEAVIATE_HOST=\nexport WEAVIATE_API_KEY=\n```\n\n----------------------------------------\n\nTITLE: Loading Blockchain Data using Sort.xyz API in TypeScript\nDESCRIPTION: This code demonstrates how to use the SortXYZBlockchainLoader from LangChain to fetch NFT metadata and transaction data for a specific contract address using the Sort.xyz API. It includes setting up the loader with API credentials and executing queries to retrieve blockchain data.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/sort_xyz_blockchain.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { SortXYZBlockchainLoader } from \"@langchain/community/document_loaders/blockchain/sort_xyz\";\n\n// First, we'll load NFT metadata for a specific contract address\nconst nftLoader = new SortXYZBlockchainLoader({\n  apiKey: \"YOUR-API-KEY\",\n  query: `\n    SELECT *\n    FROM ethereum.nft_metadata\n    WHERE contract_address = '0xed5af388653567af2f388e6224dc7c4b3241c544'\n    LIMIT 10\n  `,\n});\n\nconst nftDocs = await nftLoader.load();\n\nconsole.log({ nftDocs });\n\n// Next, we'll load transaction data for the same contract address\nconst txnLoader = new SortXYZBlockchainLoader({\n  apiKey: \"YOUR-API-KEY\",\n  query: `\n    SELECT *\n    FROM ethereum.transactions\n    WHERE to_address = '0xed5af388653567af2f388e6224dc7c4b3241c544'\n    LIMIT 10\n  `,\n});\n\nconst txnDocs = await txnLoader.load();\n\nconsole.log({ txnDocs });\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain.js Using Package Managers\nDESCRIPTION: Commands to install LangChain.js using npm, yarn, or pnpm package managers. These commands add the langchain package as a dependency to your project.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/langchain/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -S langchain\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn add langchain\n```\n\nLANGUAGE: bash\nCODE:\n```\npnpm add langchain\n```\n\n----------------------------------------\n\nTITLE: Initializing AstraDBChatMessageHistory with Existing Client\nDESCRIPTION: Code snippet showing how to initialize AstraDBChatMessageHistory using an existing AstraDB client instance. This approach is useful when you already have a client configured in your application.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/astradb.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst client = (client = new AstraDB(\n  process.env.ASTRA_DB_APPLICATION_TOKEN,\n  process.env.ASTRA_DB_ENDPOINT,\n  process.env.ASTRA_DB_NAMESPACE\n));\n\nconst collection = await client.collection(\"YOUR_COLLECTION_NAME\");\n\nconst chatHistory = new AstraDBChatMessageHistory({\n  collection,\n  sessionId: \"YOUR_SESSION_ID\",\n});\n```\n\n----------------------------------------\n\nTITLE: Linting and Formatting DeepSeek Package Code\nDESCRIPTION: Command to run the linter and formatter on the @langchain/deepseek package code to ensure it meets coding standards.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-deepseek/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nyarn lint && yarn format\n```\n\n----------------------------------------\n\nTITLE: Translating Filter to Chroma-specific Format in TypeScript\nDESCRIPTION: This code uses the ChromaTranslator to convert the constructed filter into a format specific to the Chroma vector database. It demonstrates how to use LangChain's translators for different retrievers.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_constructing_filters.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChromaTranslator } from \"@langchain/community/structured_query/chroma\";\n\nnew ChromaTranslator().visitOperation(_filter)\n```\n\n----------------------------------------\n\nTITLE: Building @langchain/cloudflare from Repository Root\nDESCRIPTION: Command to build only the @langchain/cloudflare package from the main repository root.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-cloudflare/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nyarn build --filter=@langchain/cloudflare\n```\n\n----------------------------------------\n\nTITLE: Eliminating Extra Spaces in PDF Text\nDESCRIPTION: Creates a PDFLoader with parsedItemSeparator set to an empty string to eliminate extra spaces between text elements in the PDF.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/pdf.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport { PDFLoader } from \"@langchain/community/document_loaders/fs/pdf\";\n\nconst noExtraSpacesLoader = new PDFLoader(nike10kPdfPath, {\n  parsedItemSeparator: \"\",\n});\n\nconst noExtraSpacesDocs = await noExtraSpacesLoader.load();\nconsole.log(noExtraSpacesDocs[0].pageContent.slice(100, 250))\n```\n\n----------------------------------------\n\nTITLE: Basic Migration Script Usage\nDESCRIPTION: Example of using the migration script to update import statements in a project to LangChain v0.2.x format.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/versions/v0_2/index.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { updateEntrypointsFrom0_x_xTo0_2_x } from \"@langchain/scripts/migrations\";\n\nconst pathToMyProject = \"...\"; // This path is used in the following glob pattern: `${projectPath}/**/*.{ts,tsx,js,jsx}`.\n\nupdateEntrypointsFrom0_x_xTo0_2_x({\n  projectPath: pathToMyProject,\n  shouldLog: true,\n});\n```\n\n----------------------------------------\n\nTITLE: Setting Exa API Key Environment Variable (TypeScript)\nDESCRIPTION: Sets the Exa API key as an environment variable within a TypeScript/JavaScript environment. This key is required to authenticate requests to the Exa API.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/exa_search.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.EXASEARCH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Package Resolution for Yarn\nDESCRIPTION: Example package.json configuration for yarn to ensure consistent @langchain/core version across dependencies.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/tests/__mdx__/modules/index.mdx#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"your-project\",\n  \"version\": \"0.0.0\",\n  \"private\": true,\n  \"engines\": {\n    \"node\": \">=18\"\n  },\n  \"dependencies\": {\n    \"@langchain/google-genai\": \"^0.0.2\",\n    \"@langchain/core\": \"^0.3.0\"\n  },\n  \"resolutions\": {\n    \"@langchain/core\": \"0.3.0\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Xata in Project\nDESCRIPTION: Command to initialize Xata in your project, which will set up configuration and generate client code.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/xata.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nxata init\n```\n\n----------------------------------------\n\nTITLE: Configuring Astra DB Connection in TypeScript\nDESCRIPTION: TypeScript code snippet for creating a configuration object to connect to an Astra DB instance. It includes the service provider arguments with token and endpoint.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/stores/cassandra_storage.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst configConnection = {\n  serviceProviderArgs: {\n    astra: {\n      token: <...> as string,\n      endpoint: <...> as string,\n    },\n  },\n};\n```\n\n----------------------------------------\n\nTITLE: Implementing Few-Shot Examples with Dynamic Selection\nDESCRIPTION: Creating a semantic similarity-based example selector for dynamic few-shot prompting using OpenAI embeddings and Neo4j vector store.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/graph_prompting.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { SemanticSimilarityExampleSelector } from \"@langchain/core/example_selectors\";\nimport { Neo4jVectorStore } from \"@langchain/community/vectorstores/neo4j_vector\";\n\nconst exampleSelector = await SemanticSimilarityExampleSelector.fromExamples(\n    examples,\n    new OpenAIEmbeddings(),\n    Neo4jVectorStore,\n    {\n        k: 5,\n        inputKeys: [\"question\"],\n        preDeleteCollection: true,\n        url,\n        username,\n        password\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Constructing ToolMessage Object in JavaScript\nDESCRIPTION: This JSON snippet represents the construction of a ToolMessage object, which contains the result of the calculator tool call. It includes the tool call ID, content (result), and additional metadata.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_30\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"lc\": 1,\n  \"type\": \"constructor\",\n  \"id\": [\n    \"langchain_core\",\n    \"messages\",\n    \"ToolMessage\"\n  ],\n  \"kwargs\": {\n    \"tool_call_id\": \"toolu_01NVTbm5aNYSm1wGYb6XF7jE\",\n    \"content\": \"18980\",\n    \"additional_kwargs\": {\n      \"name\": \"calculator\"\n    },\n    \"response_metadata\": {}\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatOpenAI Model\nDESCRIPTION: Example of creating and using a ChatOpenAI instance with GPT-4.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-openai/README.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst model = new ChatOpenAI({\n  apiKey: process.env.OPENAI_API_KEY,\n  modelName: \"gpt-4-1106-preview\",\n});\nconst response = await model.invoke(new HumanMessage(\"Hello world!\"));\n```\n\n----------------------------------------\n\nTITLE: Initializing MixedbreadAIReranker with API Key\nDESCRIPTION: Code showing how to instantiate the MixedbreadAIReranker class with your API key for authentication.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_compressors/mixedbread_ai.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst reranker = new MixedbreadAIReranker({ apiKey: \"your-api-key\" });\n```\n\n----------------------------------------\n\nTITLE: Wrapping the Custom Tool for LLM Agent Interaction\nDESCRIPTION: This code wraps the custom tool as a LangChain tool, providing additional information for the LLM agent about when to use the tool and its input parameters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/graph_semantic.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n\nconst informationTool = tool((input) => {\n    return getInformation(input.entity);\n}, {\n    name: \"Information\",\n    description: \"useful for when you need to answer questions about various actors or movies\",\n    schema: z.object({\n        entity: z.string().describe(\"movie or a person mentioned in the question\"),\n    }),\n});\n```\n\n----------------------------------------\n\nTITLE: Installing LangchainJS and Google API Dependencies\nDESCRIPTION: This snippet demonstrates how to install the required dependencies for using the Gmail Tool in a LangchainJS project. The command uses npm to install the essential packages like @langchain/openai, @langchain/community, @langchain/core, and googleapis. Ensure npm is installed on your system.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/gmail.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core googleapis\n```\n\n----------------------------------------\n\nTITLE: Setting Google Cloud Project ID via Command Line\nDESCRIPTION: Command to set the Google Cloud project ID using the gcloud CLI tool, which is necessary for accessing Google Cloud resources locally.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/google_cloudsql_pg.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngcloud config set project YOUR-PROJECT-ID\n```\n\n----------------------------------------\n\nTITLE: Invalid YAML Frontmatter Example\nDESCRIPTION: An example of incorrectly formatted YAML frontmatter with syntax errors including improper array formatting and mismatched brackets.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-community/src/document_loaders/tests/example_data/obsidian/bad_frontmatter.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nanArray:\n one\n- two\n- three\ntags: 'onetag', 'twotag' ]\n```\n\n----------------------------------------\n\nTITLE: Invoking Google Scholar Tool with Model-Generated ToolCall\nDESCRIPTION: Example showing how to use the tool with a model-generated tool call object for machine learning research.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/google_scholar.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconst modelGeneratedToolCall = {\n  args: { query: \"machine learning\" },\n  id: \"1\",\n  name: tool.name,\n  type: \"tool_call\",\n};\nawait tool.invoke(modelGeneratedToolCall);\n```\n\n----------------------------------------\n\nTITLE: Parsing JSON Search Results Data - JavaScript/JSON\nDESCRIPTION: JSON data structure containing search results about the Oppenheimer movie. The data includes movie details from IMDb, Rotten Tomatoes, Wikipedia and other sources with information about the film's production, box office performance, and critical reception.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n[{\"title\":\"Oppenheimer (2023) - IMDb\",\"url\":\"https://www.imdb.com/title/tt15398776/\",\"content\":\"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.\",\"score\":0.96643,\"raw_content\":null}]\n```\n\n----------------------------------------\n\nTITLE: Testing Document Chain with Context\nDESCRIPTION: Tests the document chain with a human message and the previously retrieved documents to see how it answers a question about LangSmith's testing capabilities.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_retrieval.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: javascript\nCODE:\n```\nimport { HumanMessage, AIMessage } from \"@langchain/core/messages\";\n\nawait documentChain.invoke({\n  messages: [new HumanMessage(\"Can LangSmith help test my LLM applications?\")],\n  context: docs,\n});\n```\n\n----------------------------------------\n\nTITLE: Initializing Pinecone Vector Store\nDESCRIPTION: Setup of Pinecone vector store with sample documents containing metadata for movie information. Includes document creation, attribute definition, and vector store instantiation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/pinecone.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { Pinecone } from \"@pinecone-database/pinecone\";\n\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { PineconeStore } from \"@langchain/pinecone\";\nimport { Document } from \"@langchain/core/documents\";\nimport type { AttributeInfo } from \"langchain/chains/query_constructor\";\n\nconst docs = [\n  new Document({\n    pageContent: \"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n    metadata: { year: 1993, rating: 7.7, genre: \"science fiction\" },\n  }),\n  // ... additional documents\n];\n\nconst attributeInfo: AttributeInfo[] = [\n  {\n    name: \"genre\",\n    description: \"The genre of the movie\",\n    type: \"string or array of strings\",\n  },\n  // ... additional attributes\n];\n\nconst pinecone = new Pinecone();\nconst pineconeIndex = pinecone.Index(process.env.PINECONE_INDEX!);\nconst embeddings = new OpenAIEmbeddings();\nconst vectorStore = await PineconeStore.fromDocuments(docs, embeddings, {\n  pineconeIndex: pineconeIndex,\n});\n```\n\n----------------------------------------\n\nTITLE: Converting Vector Store to Retriever Interface - TypeScript\nDESCRIPTION: Converts the vector store into a retriever object for use within LangChain chains or agents, optionally applying a filter and specifying the number of results (k). The retriever.invoke method is used to query for documents similar to the input text. Assumes a ready vectorStore; outputs are processed via the retriever interface.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/mariadb.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst retriever = vectorStore.asRetriever({\n  // Optional filter\n  // filter: filter,\n  k: 2,\n});\nawait retriever.invoke(\"biology\");\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Try/Except Tool Wrapper\nDESCRIPTION: Creates a wrapper function that handles tool execution errors gracefully by catching exceptions and returning formatted error messages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tools_error.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst tryExceptToolWrapper = async (input, config) => {\n  try {\n    const result = await complexTool.invoke(input);\n    return result;\n  } catch (e) {\n    return `Calling tool with arguments:\\n\\n${JSON.stringify(input)}\\n\\nraised the following error:\\n\\n${e}`\n  }\n}\n\nconst chainWithTools = llmWithTools\n  .pipe((message) => message.tool_calls?.[0].args)\n  .pipe(tryExceptToolWrapper);\n\nconst res = await chainWithTools.invoke(\"use complex tool. the args are 5, 2.1, potato\");\n\nconsole.log(res);\n```\n\n----------------------------------------\n\nTITLE: Importing Azure Blob Storage File Loader\nDESCRIPTION: Code snippet for importing the Azure Blob Storage File Loader, which loads a specific file from Azure Blob Storage or Azure Files.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/microsoft.mdx#2025-04-22_snippet_15\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AzureBlobStorageFileLoader } from \"@langchain/community/document_loaders/web/azure_blob_storage_file\";\n```\n\n----------------------------------------\n\nTITLE: Package Resolution Configuration for NPM\nDESCRIPTION: Configuration for npm's package.json to ensure all integrations use the same version of @langchain/core using overrides field.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/tests/__mdx__/modules/two.mdx#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"your-project\",\n  \"version\": \"0.0.0\",\n  \"private\": true,\n  \"engines\": {\n    \"node\": \">=18\"\n  },\n  \"dependencies\": {\n    \"@langchain/google-genai\": \"^0.0.2\",\n    \"@langchain/core\": \"^0.3.0\"\n  },\n  \"overrides\": {\n    \"@langchain/core\": \"0.3.0\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Filtering Events by Component Name\nDESCRIPTION: Demonstrates how to filter event stream to only include events from a specific named component (in this case, a parser named 'my_parser'). This helps manage the volume of events in complex chains.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming.ipynb#2025-04-22_snippet_16\n\nLANGUAGE: javascript\nCODE:\n```\nconst chain = model.withConfig({ runName: \"model\" })\n  .pipe(\n    new JsonOutputParser().withConfig({ runName: \"my_parser\" })\n  );\n\n\nconst eventStream = await chain.streamEvents(\n  `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key \"name\" and \"population\"`,\n  { version: \"v2\" },\n  { includeNames: [\"my_parser\"] },\n);\n\nlet eventCount = 0;\n\nfor await (const event of eventStream) {\n  // Truncate the output\n  if (eventCount > 10) {\n    continue;\n  }\n  console.log(event);\n  eventCount += 1;\n}\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith API Key for Tracing in TypeScript\nDESCRIPTION: This snippet shows how to set environment variables for LangSmith tracing. It sets the LANGSMITH_TRACING to true and specifies the LANGSMITH_API_KEY.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/vectorstore.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.LANGSMITH_TRACING=\"true\"\nprocess.env.LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Resolving @langchain/core Version with pnpm\nDESCRIPTION: Example package.json configuration to ensure a single version of @langchain/core is used with pnpm.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/installation.mdx#2025-04-22_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"your-project\",\n  \"version\": \"0.0.0\",\n  \"private\": true,\n  \"engines\": {\n    \"node\": \">=18\"\n  },\n  \"dependencies\": {\n    \"@langchain/anthropic\": \"^0.0.2\",\n    \"@langchain/core\": \"^0.3.0\",\n    \"langchain\": \"0.0.207\"\n  },\n  \"pnpm\": {\n    \"overrides\": {\n      \"@langchain/core\": \"0.3.0\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for LangChain\nDESCRIPTION: Configuration of environment variables for LangSmith observability and callback settings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tools_prompting.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# Optional, use LangSmith for best-in-class observability\nLANGSMITH_API_KEY=your-api-key\nLANGSMITH_TRACING=true\n\n# Reduce tracing latency if you are not in a serverless environment\n# LANGCHAIN_CALLBACKS_BACKGROUND=true\n```\n\n----------------------------------------\n\nTITLE: Installing Milvus SDK in Node.js\nDESCRIPTION: Installs the Milvus Node.js SDK via npm, which is necessary for integrating Milvus into Node.js applications. This command installs '@zilliz/milvus2-sdk-node' as a dependency.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/milvus.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -S @zilliz/milvus2-sdk-node\n```\n\n----------------------------------------\n\nTITLE: Installing Development Dependencies\nDESCRIPTION: Command to install all development dependencies for the @langchain/qdrant package using Yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-qdrant/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyarn install\n```\n\n----------------------------------------\n\nTITLE: Configuring Weaviate Environment Variables\nDESCRIPTION: Setting up environment variables for connecting to a Weaviate instance including scheme, host and API key configurations\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/weaviate.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// http or https\nprocess.env.WEAVIATE_SCHEME = \"\";\n// If running locally, include port e.g. \"localhost:8080\"\nprocess.env.WEAVIATE_HOST = \"YOUR_HOSTNAME\";\n// Optional, for cloud deployments\nprocess.env.WEAVIATE_API_KEY = \"YOUR_API_KEY\";\n```\n\n----------------------------------------\n\nTITLE: Running Linter and Formatter\nDESCRIPTION: Command to run code linting and formatting\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-xai/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nyarn lint && yarn format\n```\n\n----------------------------------------\n\nTITLE: Building the DeepSeek Package\nDESCRIPTION: Commands to build the @langchain/deepseek package, including an option to build it from the repository root.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-deepseek/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nyarn build\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn build --filter=@langchain/deepseek\n```\n\n----------------------------------------\n\nTITLE: Date Range Query in Couchbase Vector Store using TypeScript\nDESCRIPTION: This code example shows how to search for documents within a specific date range using the metadata.date field in Couchbase Vector Store.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/couchbase.mdx#2025-04-22_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nconst dateRangeResult = await store.similaritySearch(independenceQuery, 4, {\n  fields: [\"metadata.date\", \"metadata.author\"],\n  searchOptions: {\n    query: {\n      start: \"2016-12-31\",\n      end: \"2017-01-02\",\n      inclusiveStart: true,\n      inclusiveEnd: false,\n      field: \"metadata.date\",\n    },\n  },\n});\nconsole.log(dateRangeResult[0]);\n```\n\n----------------------------------------\n\nTITLE: Installing cheerio package\nDESCRIPTION: Command to install the cheerio npm package, which is used in the usage examples for web scraping.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_transformers/html-to-text.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install cheerio\n```\n\n----------------------------------------\n\nTITLE: Incomplete Tool Response Example\nDESCRIPTION: Demonstrates an incorrect implementation where only one tool response is provided when two are required, resulting in an error.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/troubleshooting/errors/INVALID_TOOL_RESULTS.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nconst toolResponse1 = await dummyTool.invoke(responseMessage.tool_calls![0]);\n\nchatHistory.push(responseMessage);\nchatHistory.push(toolResponse1);\n\nawait modelWithTools.invoke(chatHistory);\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Tag Usage in Markdown\nDESCRIPTION: This Markdown snippet shows various ways of using tags, including standalone tags, tags with special characters, and tags within text. It demonstrates the flexibility of tag syntax in Markdown.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-community/src/document_loaders/tests/example_data/obsidian/tags_and_frontmatter.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n# Tags\n\n()#notatag\n#12345\n#read\nsomething #tagWithCases\n\n- #tag-with-dash\n  #tag_with_underscore #tag/with/nesting\n```\n\n----------------------------------------\n\nTITLE: Running Single Test File in LangChain.js\nDESCRIPTION: Command to run a specific test file, useful during feature development. Can be used for both unit tests and integration tests.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/contributing/testing.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyarn test:single /path/to/yourtest.test.ts\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for LangChain with AWS Lambda Integration\nDESCRIPTION: Command to install the necessary npm packages for creating an agent with AWS Lambda integration using LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/lambda_agent.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Agent Execution Chain with RunnableAssign and RunnableMap in LangChain.js\nDESCRIPTION: Log output showing the execution flow of a LangChain.js agent that uses RunnableAssign, RunnableMap, and RunnableLambda components to process inputs. This demonstrates how the chain processes empty input and returns AI message chunks containing tool usage instructions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_24\n\nLANGUAGE: typescript\nCODE:\n```\n[chain/start] [1:chain:AgentExecutor > 18:chain:ToolCallingAgent > 19:chain:RunnableAssign] Entering Chain run with input: {\n  \"input\": \"\"\n}\n[chain/start] [1:chain:AgentExecutor > 18:chain:ToolCallingAgent > 19:chain:RunnableAssign > 20:chain:RunnableMap] Entering Chain run with input: {\n  \"input\": \"\"\n}\n[chain/start] [1:chain:AgentExecutor > 18:chain:ToolCallingAgent > 19:chain:RunnableAssign > 20:chain:RunnableMap > 21:chain:RunnableLambda] Entering Chain run with input: {\n  \"input\": \"\"\n}\n[chain/end] [1:chain:AgentExecutor > 18:chain:ToolCallingAgent > 19:chain:RunnableAssign > 20:chain:RunnableMap > 21:chain:RunnableLambda] [1ms] Exiting Chain run with output: {\n  \"output\": [\n    {\n      \"lc\": 1,\n      \"type\": \"constructor\",\n      \"id\": [\n        \"langchain_core\",\n        \"messages\",\n        \"AIMessageChunk\"\n      ],\n      \"kwargs\": {\n        \"content\": [\n          {\n            \"type\": \"tool_use\",\n            \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n            \"name\": \"tavily_search_results_json\",\n            \"input\": {\n              \"input\": \"Oppenheimer 2023 film director age\"\n            }\n          }\n        ],\n        \"additional_kwargs\": {\n          \"id\": \"msg_015MqAHr84dBCAqBgjou41Km\",\n          \"type\": \"message\",\n          \"role\": \"assistant\",\n          \"model\": \"claude-3-sonnet-20240229\",\n          \"stop_sequence\": null,\n          \"usage\": {\n            \"input_tokens\": 409,\n            \"output_tokens\": 68\n          },\n          \"stop_reason\": \"tool_use\"\n        },\n        \"tool_call_chunks\": [\n          {\n            \"name\": \"tavily_search_results_json\",\n            \"args\": \"{\\\"input\\\":\\\"Oppenheimer 2023 film director age\\\"}\",\n            \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n            \"index\": 0\n          }\n        ],\n        \"tool_calls\": [\n          {\n            \"name\": \"tavily_search_results_json\",\n            \"args\": {\n              \"input\": \"Oppenheimer 2023 film director age\"\n            },\n            \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\"\n          }\n        ],\n        \"invalid_tool_calls\": [],\n        \"response_metadata\": {}\n      }\n    }\n```\n\n----------------------------------------\n\nTITLE: Using NomicEmbeddings in TypeScript\nDESCRIPTION: Example of how to use the NomicEmbeddings class to generate embeddings for given text. This snippet demonstrates importing the class, creating an instance with an API key, and generating embeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/nomic.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nNomicExample\n```\n\n----------------------------------------\n\nTITLE: Configuring Convex Schema in TypeScript\nDESCRIPTION: Defines the schema for storing documents with vector embeddings in Convex, including a vector index for efficient similarity searches. Dependencies include the Convex server and values packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/convex.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { defineSchema, defineTable } from \"convex/server\";\nimport { v } from \"convex/values\";\n\nexport default defineSchema({\n  documents: defineTable({\n    embedding: v.array(v.number()),\n    text: v.string(),\n    metadata: v.any(),\n  }).vectorIndex(\"byEmbedding\", {\n    vectorField: \"embedding\",\n    dimensions: 1536,\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Package Resolution Configuration for Yarn\nDESCRIPTION: Configuration for yarn's package.json to ensure all integrations use the same version of @langchain/core using resolutions field.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/tests/__mdx__/modules/two.mdx#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"your-project\",\n  \"version\": \"0.0.0\",\n  \"private\": true,\n  \"engines\": {\n    \"node\": \">=18\"\n  },\n  \"dependencies\": {\n    \"@langchain/google-genai\": \"^0.0.2\",\n    \"@langchain/core\": \"^0.3.0\"\n  },\n  \"resolutions\": {\n    \"@langchain/core\": \"0.3.0\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating MemoryVectorStore in JavaScript\nDESCRIPTION: This snippet demonstrates how to create and populate a MemoryVectorStore with documents for use in a RAG application.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n\nconst vectorStoreQA = new MemoryVectorStore(embeddings);\nawait vectorStoreQA.addDocuments(allSplits)\n```\n\n----------------------------------------\n\nTITLE: Installing Mixedbread AI Package\nDESCRIPTION: Command to install the Mixedbread AI package via npm, which is required to use the reranking functionality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_compressors/mixedbread_ai.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/mixedbread-ai\n```\n\n----------------------------------------\n\nTITLE: Building LangChain.js Core Documentation\nDESCRIPTION: This command builds the core documentation for LangChain.js.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/CONTRIBUTING.md#2025-04-22_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nyarn build --filter=core_docs\n```\n\n----------------------------------------\n\nTITLE: Generating Tool Message in LangChain.js\nDESCRIPTION: This snippet demonstrates the creation of a ToolMessage object in LangChain.js, which represents the response from a tool call. It includes the tool call ID, content, and additional metadata.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_34\n\nLANGUAGE: javascript\nCODE:\n```\n{\n  \"lc\": 1,\n  \"type\": \"constructor\",\n  \"id\": [\n    \"langchain_core\",\n    \"messages\",\n    \"ToolMessage\"\n  ],\n  \"kwargs\": {\n    \"tool_call_id\": \"toolu_01NVTbm5aNYSm1wGYb6XF7jE\",\n    \"content\": \"18980\",\n    \"additional_kwargs\": {\n      \"name\": \"calculator\"\n    },\n    \"response_metadata\": {}\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for DOCX Processing\nDESCRIPTION: Installation commands for setting up DocxLoader with mammoth package for .docx file processing\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/docx.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core mammoth\n```\n\n----------------------------------------\n\nTITLE: Initializing WatsonxLLM with Software Authentication\nDESCRIPTION: TypeScript code for creating a WatsonxLLM instance using IBM watsonx.ai software authentication.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/ibm.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { WatsonxLLM } from \"@langchain/community/llms/ibm\";\n\nconst props = {\n  version: \"YYYY-MM-DD\",\n  serviceUrl: \"<SERVICE_URL>\",\n  projectId: \"<PROJECT_ID>\",\n  watsonxAIAuthType: \"cp4d\",\n  watsonxAIUsername: \"<YOUR-USERNAME>\",\n  watsonxAIPassword: \"<YOUR-PASSWORD>\",\n  watsonxAIUrl: \"<url>\",\n};\nconst instance = new WatsonxLLM(props);\n```\n\n----------------------------------------\n\nTITLE: Setting up ChatOpenAI Model in JavaScript\nDESCRIPTION: This snippet initializes a ChatOpenAI model with specific parameters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_no_queries.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ChatOpenAI } from '@langchain/openai';\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o\",\n  temperature: 0,\n})\n```\n\n----------------------------------------\n\nTITLE: Customizing MixedbreadAI Configuration\nDESCRIPTION: Example of initializing MixedbreadAI embeddings with custom configuration parameters like baseUrl and maxRetries.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/mixedbread_ai.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst customEmbeddings = new MixedbreadAIEmbeddings({\n  apiKey: \"YOUR_API_KEY\",\n  baseUrl: \"...\",\n  maxRetries: 6,\n});\n```\n\n----------------------------------------\n\nTITLE: Instantiating ChatXAI Model in LangChain.js\nDESCRIPTION: Shows how to import and create a new ChatXAI model instance with various configuration options like model selection, temperature, and retry parameters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/xai.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatXAI } from \"@langchain/xai\" \n\nconst llm = new ChatXAI({\n    model: \"grok-beta\", // default\n    temperature: 0,\n    maxTokens: undefined,\n    maxRetries: 2,\n    // other params...\n})\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Dependencies for CloseVector Integration\nDESCRIPTION: Command to install necessary LangChain packages for integrating with CloseVector, including OpenAI, community, and core modules.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/closevector.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Initializing and Invoking NIBittensorLLM in TypeScript\nDESCRIPTION: This snippet demonstrates how to import, initialize, and use the NIBittensorLLM class from the experimental Bittensor module in LangChain.js. It shows how to create a model instance and invoke it with a question about Bittensor.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/ni_bittensor.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { NIBittensorLLM } from \"langchain/experimental/llms/bittensor\";\n\nconst model = new NIBittensorLLM();\n\nconst res = await model.invoke(`What is Bittensor?`);\n\nconsole.log({ res });\n\n/*\n  {\n    res: \"\\nBittensor is opensource protocol...\"\n  }\n */\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatOpenAI Model in JavaScript\nDESCRIPTION: This code initializes a ChatOpenAI model with specific parameters for GPT-4.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst model = new ChatOpenAI({\n  model: \"gpt-4o\",\n  temperature: 0,\n});\n```\n\n----------------------------------------\n\nTITLE: Running a Single Test for LangChain.js\nDESCRIPTION: This command runs a single test file, useful for developing individual features.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/CONTRIBUTING.md#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nyarn test:single /path/to/yourtest.test.ts\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Core Dependencies\nDESCRIPTION: Command to install the required LangChain packages including OpenAI integration, community modules, and core functionality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/dynamodb.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Using RunnableParallel with RunnablePassthrough in TypeScript\nDESCRIPTION: This snippet demonstrates how to use RunnableParallel with RunnablePassthrough to pass data through and modify it in parallel steps.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/passthrough.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RunnableParallel, RunnablePassthrough } from \"@langchain/core/runnables\";\n\nconst runnable = RunnableParallel.from({\n  passed: new RunnablePassthrough<{ num: number }>(),\n  modified: (input: { num: number }) => input.num + 1,\n});\n\nawait runnable.invoke({ num: 1 });\n```\n\n----------------------------------------\n\nTITLE: Using MetadataTagger with OpenAI Functions\nDESCRIPTION: Example of initializing and using the MetadataTagger document transformer to extract structured metadata from movie reviews. The code demonstrates how to define a metadata schema and apply it to documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_transformers/openai_metadata_tagger.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { MetadataTagger, createMetadataTagger } from \"langchain/document_transformers/openai_functions\";\nimport { Document } from \"langchain/document\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst model = new ChatOpenAI({\n  temperature: 0,\n  model: \"gpt-3.5-turbo-0613\",\n});\n\n// Let's define a schema of metadata fields we want to extract from each document\nconst metadataTagger = new MetadataTagger({\n  metadataSchema: {\n    properties: {\n      movie_title: { type: \"string\" },\n      critic_rating: { type: \"number\", description: \"Rating from 0-10\" },\n      tone: {\n        type: \"string\",\n        enum: [\"positive\", \"negative\", \"neutral\"],\n      },\n      genre: {\n        type: \"array\",\n        items: { type: \"string\" },\n      },\n    },\n    required: [\"movie_title\", \"critic_rating\", \"tone\"],\n  },\n  llm: model,\n});\n\nconst documents = [\n  new Document({\n    pageContent: \"I recently watched The Super Mario Bros. Movie, and I have to say it exceeded my expectations! The animation was stunning, the voice acting was top-notch (especially Jack Black as Bowser), and it was filled with nostalgic references that any Mario fan would appreciate. The plot was simple but engaging, and at a runtime of 1 hour and 32 minutes, it didn't overstay its welcome. I'd give it a solid 8/10 - a must-watch for Nintendo fans and an enjoyable experience for everyone else!\",\n  }),\n];\n\nconst taggedDocuments = await metadataTagger.transformDocuments(documents);\n\nconsole.log(taggedDocuments[0].metadata);\n/*\n  {\n    movie_title: 'The Super Mario Bros. Movie',\n    critic_rating: 8,\n    tone: 'positive',\n    genre: [ 'animation', 'adventure', 'family' ]\n  }\n*/\n\n```\n\n----------------------------------------\n\nTITLE: Instantiating TavilySearchResults Tool in TypeScript\nDESCRIPTION: Shows how to import and create an instance of the TavilySearchResults tool with configuration options like maxResults.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/tavily_search_community.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { TavilySearchResults } from \"@langchain/community/tools/tavily_search\";\n\nconst tool = new TavilySearchResults({\n  maxResults: 2,\n  // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key\nDESCRIPTION: Environment variable setup for OpenAI API authentication.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_sources.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=YOUR_KEY\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI Environment Variables\nDESCRIPTION: Configuration of environment variables for OpenAI API access and optional LangSmith tracing.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/openai.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=\"your-api-key\"\n```\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Streaming Output with ChromeAI\nDESCRIPTION: This snippet demonstrates how to use the streaming functionality of ChromeAI to receive token-by-token output. It shows how to iterate through output chunks as they become available from the model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/chrome_ai.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChromeAI } from \"@langchain/community/experimental/llms/chrome_ai\";\n\nconst model = new ChromeAI({\n  temperature: 0.5, // Optional, defaults to 0.5\n  topK: 40, // Optional, defaults to 40\n});\n\nfor await (const chunk of await model.stream(\"How are you?\")) {\n  console.log(chunk);\n}\n\n/*\n  As\n   an\n   AI\n   language\n   model\n  ,\n   I\n   don\n  '\n  t\n   have\n   personal\n   experiences\n   or\n   the\n   ability\n   to\n   experience\n   emotions\n  .\n   Therefore\n  ,\n   I\n   cannot\n   directly\n   answer\n   the\n   question\n   \"\n  How\n   are\n   you\n  ?\".\n  \n  \n  \n  May\n   I\n   suggest\n   answering\n   something\n   else\n  ?\n*/\n```\n\n----------------------------------------\n\nTITLE: Combining ZepCloudChatMessageHistory with ZepVectorStore for Enhanced Retrieval\nDESCRIPTION: Advanced implementation that combines Zep Cloud message history with Zep's vector store capabilities. This example demonstrates using LangChain to maintain conversation history while also performing retrieval from a vector store for enhanced context and relevance.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/zep_memory_cloud.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport {\n  RunnablePassthrough,\n  RunnableSequence,\n  RunnableWithMessageHistory,\n} from \"@langchain/core/runnables\";\nimport { ZepCloudChatMessageHistory } from \"@langchain/community/chat_message_histories/zep_cloud\";\nimport { StringOutputParser } from \"@langchain/core/output_parsers\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { ZepCloudVectorStore } from \"@langchain/community/vectorstores/zep_cloud\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nexport async function run() {\n  // Initialize LLM, output parser, and prompt template\n  const model = new ChatOpenAI({ temperature: 0 });\n  const outputParser = new StringOutputParser();\n\n  const vectorStore = new ZepCloudVectorStore({\n    // replace with your API key\n    apiKey: process.env.ZEP_CLOUD_API_KEY ?? \"\",\n    // replace with your collection name\n    collectionName: \"my-collection\",\n    embeddingModel: process.env.EMBEDDING_MODEL || \"text-embedding-ada-002\",\n    openAIApiKey: process.env.OPENAI_API_KEY,\n  });\n\n  // Create the retriever\n  const retriever = vectorStore.asRetriever();\n\n  const prompt = ChatPromptTemplate.fromMessages([\n    [\"system\", \"You are a helpful assistant.\"],\n    [\"human\", \"{input}\"],\n  ]);\n\n  // Define a runnable sequence without message history\n  const chain = RunnableSequence.from([\n    RunnablePassthrough.assign({\n      // Do the retrieval in an assign so we can use the original input in the prompt\n      // This retrieves documents with vector embeddings most similar to the user's input\n      retrievedDocs: async (input) => {\n        const docs = await retriever.getRelevantDocuments(input.input);\n        return docs.map((doc) => doc.pageContent).join(\"\\n\");\n      },\n    }),\n    prompt,\n    model,\n    outputParser,\n  ]);\n\n  // Define a function that returns a new Zep Cloud message history for a given session\n  const getSessionHistory = (sessionId: string) => {\n    return new ZepCloudChatMessageHistory({\n      apiKey: process.env.ZEP_CLOUD_API_KEY ?? \"\", // Replace with your actual API key\n      sessionId,\n    });\n  };\n\n  // Create a runnable with message history\n  const chainWithHistory = new RunnableWithMessageHistory({\n    runnable: chain,\n    getMessageHistory: getSessionHistory,\n    inputMessagesKey: \"input\",\n    historyMessagesKey: \"chat_history\",\n  });\n\n  // Request input object\n  const config = {\n    configurable: {\n      sessionId: \"user-123\", // This uniquely identifies the conversation\n    },\n  };\n\n  // Invoke the chain several times to see message history in action\n  console.log(\n    await chainWithHistory.invoke({ input: \"Hello! What can you tell me about Zep?\" }, config)\n  );\n  console.log(\n    await chainWithHistory.invoke({ input: \"How does it work with LangChain?\" }, config)\n  );\n  console.log(\n    await chainWithHistory.invoke({ input: \"Can you provide a code example?\" }, config)\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a New LangChain Integration Package\nDESCRIPTION: Command to run the create-langchain-integration utility, which scaffolds a new integration package with support for ESM and CJS entrypoints.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/CONTRIBUTING.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ npx create-langchain-integration\n```\n\n----------------------------------------\n\nTITLE: Setting up Prem AI API Key in Environment\nDESCRIPTION: Command to export the Prem AI API key as an environment variable for use with the ChatPrem class, which defaults to looking for this key.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/premai.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport PREM_API_KEY=your-api-key\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration\nDESCRIPTION: YAML configuration block defining various data types including floats, integers, booleans, strings, arrays and nested dictionaries.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/examples/src/document_loaders/example_data/obsidian/tags_and_frontmatter.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\naFloat: 13.12345\nanInt: 15\naBool: true\naString: string value\nanArray:\n  - one\n  - two\n  - three\naDict:\n  dictId1: \"58417\"\n  dictId2: 1500\ntags: [\"onetag\", \"twotag\"]\n```\n\n----------------------------------------\n\nTITLE: Setting Browserbase Project ID for Remote Browser\nDESCRIPTION: This command sets the Browserbase project ID as an environment variable, which is needed alongside the API key to authenticate with the Browserbase platform.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/stagehand.mdx#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport BROWSERBASE_PROJECT_ID=\"your-browserbase-project-id\"\n```\n\n----------------------------------------\n\nTITLE: Accessing Example Output from Search Results\nDESCRIPTION: Shows how to access the output from an example returned by the similarity search. In this dataset, outputs are entire chat histories.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/example_selectors_langsmith.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconsole.log(examples[1].outputs.output)\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Dependencies\nDESCRIPTION: Commands for installing the required LangChain packages using npm/yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/extraction.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nlangchain @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Partial Formatting Prompt Template with String Values in TypeScript\nDESCRIPTION: This snippet demonstrates how to partially format a prompt template with string values using the PromptTemplate class from LangChain. It shows both the method of using the partial() function and initializing the prompt with partialed variables.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/prompts_partial.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PromptTemplate } from \"langchain/prompts\";\n\nconst prompt = new PromptTemplate({\n  template: \"{foo}{bar}\",\n  inputVariables: [\"foo\", \"bar\"],\n});\n\nconst partialPrompt = await prompt.partial({\n  foo: \"foo\",\n});\n\nconst formattedPrompt = await partialPrompt.format({\n  bar: \"baz\",\n});\n\nconsole.log(formattedPrompt);\n\n// foobaz\n```\n\nLANGUAGE: typescript\nCODE:\n```\nconst prompt = new PromptTemplate({\n  template: \"{foo}{bar}\",\n  inputVariables: [\"bar\"],\n  partialVariables: {\n    foo: \"foo\",\n  },\n});\n\nconst formattedPrompt = await prompt.format({\n  bar: \"baz\",\n});\n\nconsole.log(formattedPrompt);\n\n// foobaz\n```\n\n----------------------------------------\n\nTITLE: Advanced Similarity Search with Logical Filters - TypeScript/JSON\nDESCRIPTION: Performs a similarity search with an advanced filter using logical OR, searching for documents with either name equal to 'martin' or firstname equal to 'john'. This snippet demonstrates more complex querying capabilities leveraging JSON-style filter objects with the similaritySearch method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/mariadb.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\n# name = 'martin' OR firstname = 'john'\nlet res = await vectorStore.similaritySearch(\"biology\", 2, {\"$or\": [{\"name\":\"martin\"}, {\"firstname\", \"john\"}] });\n\n```\n\n----------------------------------------\n\nTITLE: Subclassing BufferLoader in TypeScript\nDESCRIPTION: Extends the BufferLoader abstract class to create a loader for binary files. Requires implementing the parse() method which receives raw buffer content and metadata, and should return a Promise of Document objects.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/document_loader_custom.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nabstract class BufferLoader extends BaseDocumentLoader {\n  abstract parse(\n    raw: Buffer,\n    metadata: Document[\"metadata\"]\n  ): Promise<Document[]>;\n}\n```\n\n----------------------------------------\n\nTITLE: Rendering Core Maintainers Component\nDESCRIPTION: Renders the People component to display core maintainers of the project.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/people.mdx#2025-04-22_snippet_3\n\nLANGUAGE: jsx\nCODE:\n```\n<People type=\"maintainers\"></People>\n```\n\n----------------------------------------\n\nTITLE: Logging Tool Use Event in LangchainJS Trace (JSON String)\nDESCRIPTION: This JSON string, found within the `log` field of a LangchainJS trace step, represents a log entry for a tool use event. It specifies the type of event (`tool_use`), a unique ID for the tool call, the name of the tool invoked (`tavily_search_results_json`), and the structured input provided to the tool. This format is used for concise logging within the broader execution trace.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n\"[{\\\"type\\\":\\\"tool_use\\\",\\\"id\\\":\\\"toolu_01NUVejujVo2y8WGVtZ49KAN\\\",\\\"name\\\":\\\"tavily_search_results_json\\\",\\\"input\\\":{\\\"input\\\":\\\"Oppenheimer 2023 film director age\\\"}}]\"\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatOpenAI with Explicit API Key in TypeScript\nDESCRIPTION: Example showing how to initialize a ChatOpenAI model instance by explicitly passing an API key parameter instead of relying on environment variables. This approach can help isolate authentication issues by bypassing environment variable configuration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/troubleshooting/errors/MODEL_AUTHENTICATION.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nconst model = new ChatOpenAI({\n  apiKey: \"YOUR_KEY_HERE\",\n});\n```\n\n----------------------------------------\n\nTITLE: Setting Up Neo4j Self-hosted Instance with Docker Compose\nDESCRIPTION: This snippet provides a reference to a prebuilt Docker Compose file to set up a self-hosted Neo4j database instance. Users must have Docker and Docker Compose installed to run the setup command.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/neo4jvector.mdx#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\n{DockerExample}\n```\n\n----------------------------------------\n\nTITLE: Visualizing LangGraph Control Flow in JavaScript\nDESCRIPTION: This code uses tslab to visualize the control flow of a LangGraph application by generating a Mermaid PNG diagram.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: javascript\nCODE:\n```\nimport * as tslab from \"tslab\";\n\nconst image = await graph.getGraph().drawMermaidPng();\nconst arrayBuffer = await image.arrayBuffer();\n\nawait tslab.display.png(new Uint8Array(arrayBuffer));\n```\n\n----------------------------------------\n\nTITLE: Invalid YAML Frontmatter Example\nDESCRIPTION: An example of improperly formatted YAML frontmatter with syntax errors. The array lacks proper dash formatting for the first element, and the tags list has a closing bracket without an opening one.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/examples/src/document_loaders/example_data/obsidian/bad_frontmatter.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nanArray:\n one\n- two\n- three\ntags: 'onetag', 'twotag' ]\n---\n```\n\n----------------------------------------\n\nTITLE: Importing Question Generator Dependencies\nDESCRIPTION: Imports required components for creating the question generator chain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/step_back.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatOpenAI } from \"npm:langchain@0.0.177/chat_models/openai\";\nimport { StringOutputParser } from \"npm:langchain@0.0.177/schema/output_parser\";\nimport { RunnableSequence } from \"npm:langchain@0.0.177/schema/runnable\";\n```\n\n----------------------------------------\n\nTITLE: Publishing the AWS Package\nDESCRIPTION: Command to publish a new version of the @langchain/aws package after building it.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-aws/README.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n$ npm publish\n```\n\n----------------------------------------\n\nTITLE: Importing Zod in Deno for LangChain.js Documentation\nDESCRIPTION: This code snippet shows how to import the Zod library in a Deno environment for LangChain.js documentation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/CONTRIBUTING.md#2025-04-22_snippet_18\n\nLANGUAGE: typescript\nCODE:\n```\n// Import in Node:\nimport { z } from \"zod\";\n// Import in Deno:\nimport { z } from \"npm:/zod\";\n```\n\n----------------------------------------\n\nTITLE: Extracting a Single Column from CSV\nDESCRIPTION: Creates a CSVLoader that extracts only the 'html' column from the CSV file, using a custom separator, and then loads and displays the first document.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/csv.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { CSVLoader } from \"@langchain/community/document_loaders/fs/csv\";\n\nconst singleColumnLoader = new CSVLoader(\n  exampleCsvPath,\n  {\n    column: \"html\",\n    separator:\"｜\"\n  }\n);\n\nconst singleColumnDocs = await singleColumnLoader.load();\nconsole.log(singleColumnDocs[0]);\n```\n\n----------------------------------------\n\nTITLE: LLM Configuration\nDESCRIPTION: Configures the ChatOpenAI model with specific parameters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_multiple_queries.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatOpenAI } from '@langchain/openai';\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o\",\n  temperature: 0,\n})\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for DeepInfra Integration\nDESCRIPTION: Command to install the necessary npm packages for using DeepInfra with LangChain. Requires both @langchain/community and @langchain/core packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/deep_infra.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Linting and formatting @langchain/exa code\nDESCRIPTION: Command to run the linter and formatter to ensure code quality and consistency.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-exa/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nyarn lint && yarn format\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with npm\nDESCRIPTION: Commands to install required npm packages including langchain, @langchain/openai, and cheerio.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_sources.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install --save langchain @langchain/openai cheerio\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI and LangSmith Credentials (TypeScript)\nDESCRIPTION: This snippet sets the environment variables for the OpenAI API key required by OpenAIEmbeddings in LangChain. It includes optional commented-out lines for enabling LangSmith tracing and API key; these empower debugging and tracing of model calls when needed. Set environment variables before vector store initialization.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/chroma.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.OPENAI_API_KEY = \"YOUR_API_KEY\";\n```\n\nLANGUAGE: typescript\nCODE:\n```\n// process.env.LANGSMITH_TRACING=\"true\"\n// process.env.LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Enabling LangSmith Tracing for LangChain - TypeScript\nDESCRIPTION: Optionally enables tracing and monitoring of LangChain model calls by setting the LANGCHAIN_TRACING_V2 and LANGCHAIN_API_KEY environment variables. The code is commented and can be used when integrating with the LangSmith platform for debugging and analytics. Requires that the LangSmith service is set up and access keys are obtained.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/mariadb.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n// process.env.LANGCHAIN_TRACING_V2=\"true\"\n// process.env.LANGCHAIN_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Using JSON Schema for Structured Output in TypeScript\nDESCRIPTION: Demonstrates how to use an OpenAI-style JSON schema dictionary instead of Zod to define the structure for model output. The schema includes name, description, and parameters with property definitions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/structured_output.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst structuredLlm = model.withStructuredOutput(\n  {\n    \"name\": \"joke\",\n    \"description\": \"Joke to tell user.\",\n    \"parameters\": {\n      \"title\": \"Joke\",\n      \"type\": \"object\",\n      \"properties\": {\n        \"setup\": {\"type\": \"string\", \"description\": \"The setup for the joke\"},\n        \"punchline\": {\"type\": \"string\", \"description\": \"The joke's punchline\"},\n      },\n      \"required\": [\"setup\", \"punchline\"],\n    },\n  }\n)\n\nawait structuredLlm.invoke(\"Tell me a joke about cats\", { name: \"joke\" })\n```\n\n----------------------------------------\n\nTITLE: Setting API Key as Environment Variable for LangChain Document Loader\nDESCRIPTION: Shows how to set the required API key as an environment variable for the document loader. This is a template that needs to be filled with the actual environment variable name for the specific loader being documented.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/document_loaders.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport __env_var_name__=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Processing Tavily Search API Call for Oppenheimer Film Information\nDESCRIPTION: This snippet shows a tool call to the Tavily search API with the query 'Oppenheimer 2023 film director age' and the structured JSON response containing search results about Christopher Nolan and the film Oppenheimer.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"tavily_search_results_json\",\n  \"args\": {\n    \"input\": \"Oppenheimer 2023 film director age\"\n  },\n  \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\"\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Taskade Project Data in TypeScript\nDESCRIPTION: This code snippet demonstrates how to use the TaskadeLoader class to load project data from Taskade. It requires the project ID and access token for authentication.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/taskade.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { TaskadeLoader } from \"langchain/document_loaders/web/taskade\";\n\nconst loader = new TaskadeLoader({\n  projectId: \"<YOUR PROJECT ID>\",\n  accessToken: \"<YOUR-ACCESS-TOKEN>\",\n});\n\nconst docs = await loader.load();\n```\n\n----------------------------------------\n\nTITLE: Loading documents with CheerioWebBaseLoader\nDESCRIPTION: Demonstrates how to load documents from a web page using the CheerioWebBaseLoader and access the first document in the returned array. This extracts the entire HTML content based on the loader's configuration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/web_cheerio.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst docs = await loader.load()\ndocs[0]\n```\n\n----------------------------------------\n\nTITLE: Accessing ChatWatsonx Response Content\nDESCRIPTION: Shows how to extract the content from the ChatWatsonx model response for further processing or display.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/ibm.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(aiMsg.content)\n```\n\n----------------------------------------\n\nTITLE: Using MistralAIEmbeddings\nDESCRIPTION: TypeScript code to initialize the MistralAIEmbeddings class and generate embeddings for a text query, which can be used for semantic search and similarity comparisons.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-mistralai/README.md#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { MistralAIEmbeddings } from \"@langchain/mistralai\";\n\nconst embeddings = new MistralAIEmbeddings({\n  apiKey: process.env.MISTRAL_API_KEY,\n});\nconst res = await embeddings.embedQuery(\"Hello world\");\n```\n\n----------------------------------------\n\nTITLE: Accessing Example Input from Search Results\nDESCRIPTION: Demonstrates how to access the input from the first example returned by the similarity search, showing how to work with the search results.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/example_selectors_langsmith.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconsole.log(examples[0].inputs.input)\n```\n\n----------------------------------------\n\nTITLE: Defining Integrations Note Block in Markdown\nDESCRIPTION: Note block specifying where integration concepts should exist in the codebase.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/contributing/documentation/style_guide.mdx#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n:::note\nConcepts covered in `Integrations` should generally exist in `@langchain/community` or specific partner packages.\n:::\n```\n\n----------------------------------------\n\nTITLE: Setting Up LangSmith Tracing for IBM watsonx.ai\nDESCRIPTION: Configures LangSmith tracing for model calls by setting environment variables for tracing activation and API key authentication.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/ibm.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Chaining with Prompt Template\nDESCRIPTION: Example of chaining the WatsonxLLM with a prompt template for structured input processing.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/ibm.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport { PromptTemplate } from \"@langchain/core/prompts\"\n\nconst prompt = PromptTemplate.fromTemplate(\"How to say {input} in {output_language}:\\n\")\n\nconst chain = prompt.pipe(instance);\nawait chain.invoke(\n  {\n    output_language: \"German\",\n    input: \"I love programming.\",\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Documentation Note Block in Markdown\nDESCRIPTION: Example of using a note block in markdown to provide additional context about documentation sections ordering.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/contributing/documentation/style_guide.mdx#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n:::note\nThe below sections are listed roughly in order of increasing level of abstraction.\n:::\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for LangSmith Tracing\nDESCRIPTION: Configuration of environment variables for enabling LangSmith tracing with Bedrock embeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/bedrock.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Instantiating Vector Store with OpenAI Embeddings\nDESCRIPTION: Example code for initializing a vector store with OpenAI embeddings. Uses placeholders for the module name and import path that need to be replaced with actual implementation details.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/vectorstores.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { __module_name__ } from \"__full_import_path__\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddings = new OpenAIEmbeddings({\n  model: \"text-embedding-3-small\",\n});\n\nconst vectorStore = new __module_name__(embeddings);\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search with Filters\nDESCRIPTION: Example of a basic similarity search on the vector store with filtering. Shows how to define a filter object and perform a similaritySearch query with a specified number of results.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/vectorstores.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nconst filter = { source: \"https://example.com\" };\n\nconst similaritySearchResults = await vectorStore.similaritySearch(\"biology\", 2, filter);\n\nfor (const doc of similaritySearchResults) {\n  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Distinct Values as List in JavaScript\nDESCRIPTION: This function queries a database for distinct values and returns them as a list of strings. It removes numbers and trims whitespace from the results.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/sql_qa.ipynb#2025-04-22_snippet_17\n\nLANGUAGE: javascript\nCODE:\n```\nasync function queryAsList(database: any, query: string): Promise<string[]> {\n  const res: Array<{ [key: string]: string }> = JSON.parse(\n    await database.run(query)\n  )\n    .flat()\n    .filter((el: any) => el != null);\n  const justValues: Array<string> = res.map((item) =>\n    Object.values(item)[0]\n      .replace(/\\b\\d+\\b/g, \"\")\n      .trim()\n  );\n  return justValues;\n}\n\n// Gather entities into a list\nlet artists: string[] = await queryAsList(db, \"SELECT Name FROM Artist\");\nlet albums: string[] = await queryAsList(db, \"SELECT Title FROM Album\");\nlet properNouns = artists.concat(albums);\n\nconsole.log(`Total: ${properNouns.length}\\n`)\nconsole.log(`Sample: ${properNouns.slice(0, 5)}...`)\n```\n\n----------------------------------------\n\nTITLE: Installing JigsawStack LangChain Integration\nDESCRIPTION: NPM command to install the JigsawStack integration package for LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/jigsawstack.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/jigsawstack\n```\n\n----------------------------------------\n\nTITLE: Instantiating ChatOpenAI Model\nDESCRIPTION: This snippet shows how to create a ChatOpenAI instance with specific parameters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/chroma.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o-mini\",\n  temperature: 0,\n});\n```\n\n----------------------------------------\n\nTITLE: Instantiating PineconeEmbeddings Model\nDESCRIPTION: Creates a new instance of PineconeEmbeddings with the multilingual-e5-large model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/pinecone.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { PineconeEmbeddings } from \"@langchain/pinecone\";\n\nconst embeddings = new PineconeEmbeddings({\n  model: \"multilingual-e5-large\",\n});\n```\n\n----------------------------------------\n\nTITLE: Azure Managed Identity Integration\nDESCRIPTION: Example showing Azure OpenAI integration with Azure Managed Identity credentials\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-azure-openai/README.md#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { DefaultAzureCredential } from \"@azure/identity\";\nimport { AzureOpenAI } from \"@langchain/azure-openai\";\n\nconst credentials = new DefaultAzureCredential();\n\nconst model = new AzureOpenAI({\n  credentials,\n  azureOpenAIEndpoint: process.env.AZURE_OPENAI_API_ENDPOINT,\n  azureOpenAIApiDeploymentName: process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME,\n});\n```\n\n----------------------------------------\n\nTITLE: Importing IndexTable Component in Markdown\nDESCRIPTION: Shows the import statement for the IndexTable component used to display web loader information.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/index.mdx#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nimport { IndexTable } from \"@theme/FeatureTables\";\n```\n\n----------------------------------------\n\nTITLE: Installing Mozilla Readability Dependencies\nDESCRIPTION: Commands to install the required npm packages @mozilla/readability and jsdom for HTML content transformation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_transformers/mozilla_readability.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @mozilla/readability jsdom\n```\n\n----------------------------------------\n\nTITLE: Generating Documentation with Yarn\nDESCRIPTION: Command to generate and view LangChain.js documentation locally after installing dependencies.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/contributing/code.mdx#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nyarn docs\n```\n\n----------------------------------------\n\nTITLE: Filtering Messages by Name in LangChain.js\nDESCRIPTION: Shows how to exclude messages from specific named sources using the excludeNames parameter.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/filter_messages.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfilterMessages(messages, { excludeNames: [\"example_user\", \"example_assistant\"] })\n```\n\n----------------------------------------\n\nTITLE: Installing Development Dependencies\nDESCRIPTION: Bash command to install package dependencies for local development of the @langchain/mistralai package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-mistralai/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nyarn install\n```\n\n----------------------------------------\n\nTITLE: Running Langchain Integration Documentation Generator\nDESCRIPTION: Command to create integration documentation with required parameters for class name and integration type\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyarn create:integration:doc --classname <Class Name> --type <Type>\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for DOC Processing\nDESCRIPTION: Installation commands for setting up DocxLoader with word-extractor package for .doc file processing\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/docx.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core word-extractor\n```\n\n----------------------------------------\n\nTITLE: Invoking Chat Model for Translation\nDESCRIPTION: Demonstrates how to invoke the chat model for translating English to French using system and human messages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/chat.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst aiMsg = await llm.invoke([\n  [\n    \"system\",\n    \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n  ],\n  [\"human\", \"I love programming.\"],\n])\naiMsg\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for GitBook Loading with Cheerio\nDESCRIPTION: This snippet shows how to install the necessary npm packages for loading GitBook data using Cheerio.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/gitbook.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core cheerio\n```\n\n----------------------------------------\n\nTITLE: Setting up Cerebras API Key in Environment Variables\nDESCRIPTION: Instructions for configuring the necessary environment variables to authenticate with Cerebras API and optionally enable LangSmith tracing.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/cerebras.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport CEREBRAS_API_KEY=\"your-api-key\"\n```\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Server-Side Agent Function with React Server Components\nDESCRIPTION: This code defines a server-side agent function that uses the agentExecutor and streamRunnableUI utility. It also exposes the agent endpoint for client-side use through a context.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/generative_ui.mdx#2025-04-22_snippet_1\n\nLANGUAGE: tsx\nCODE:\n```\nasync function agent(inputs: {\n  input: string;\n  chat_history: [role: string, content: string][];\n}) {\n  \"use server\";\n\n  return streamRunnableUI(agentExecutor, {\n    input: inputs.input,\n    chat_history: inputs.chat_history.map(\n      ([role, content]) => new ChatMessage(content, role)\n    ),\n  });\n}\n\nexport const EndpointsContext = exposeEndpoints({ agent });\n```\n\n----------------------------------------\n\nTITLE: Installing Development Dependencies\nDESCRIPTION: Command to install development dependencies using yarn for contributing to the @langchain/cohere package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-cohere/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nyarn install\n```\n\n----------------------------------------\n\nTITLE: Installing Arcjet Redaction Library and LangChain Community\nDESCRIPTION: Commands to install the required packages for using Arcjet Redact with LangChain. This includes the Arcjet Redaction Library and LangChain Community packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/arcjet.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n@arcjet/redact\n```\n\nLANGUAGE: bash\nCODE:\n```\n@langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Importing OpenAI LLM in LangChain.js\nDESCRIPTION: This code snippet demonstrates how to import the OpenAI LLM from a specific subpath in LangChain.js.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/CONTRIBUTING.md#2025-04-22_snippet_14\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAI } from \"langchain/llms/openai\";\n```\n\n----------------------------------------\n\nTITLE: Building the Package\nDESCRIPTION: Commands to build the @langchain/mixedbread-ai package, either directly in the package directory or from the repository root using a filter.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-mixedbread-ai/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nyarn build\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn build --filter=@langchain/mixedbread-ai\n```\n\n----------------------------------------\n\nTITLE: Installing mysql2 Client for SingleStoreDB - Bash\nDESCRIPTION: Installs the `mysql2` package, which is required as a dependency for LangChain.js to connect to SingleStoreDB. Run this in your Node.js project directory before attempting to use any SingleStore integration features. Outputs a node_modules installation and updates your project's package.json and package-lock.json.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/singlestore.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -S mysql2\n```\n\n----------------------------------------\n\nTITLE: Matching Extracted Entities to Neo4j Database\nDESCRIPTION: This code defines a Cypher query to match extracted entities to the database and a function to process the matches. It uses a simple CONTAINS clause for matching.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/graph_mapping.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconst matchQuery = `\nMATCH (p:Person|Movie)\nWHERE p.name CONTAINS $value OR p.title CONTAINS $value\nRETURN coalesce(p.name, p.title) AS result, labels(p)[0] AS type\nLIMIT 1`\n\nconst matchToDatabase = async (values) => {\n    let result = \"\"\n    for (const entity of values.names) {\n        const response = await graph.query(matchQuery, {\n            value: entity\n        })\n        if (response.length > 0) {\n            result += `${entity} maps to ${response[0][\"result\"]} ${response[0][\"type\"]} in database\\n`\n        }\n    }\n    return result\n}\n\nawait matchToDatabase(entities)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Upstash Redis Storage in LangChain.js\nDESCRIPTION: This snippet shows the command to install the necessary dependencies for using Upstash Redis storage in a LangChain.js project. It includes @langchain/community, @langchain/core, and @upstash/redis packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/stores/upstash_redis_storage.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core @upstash/redis\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Hacker News Loader\nDESCRIPTION: This snippet shows the command to install the necessary packages for using the Hacker News loader with LangChain.js and Cheerio.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/hn.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core cheerio\n```\n\n----------------------------------------\n\nTITLE: Yielding Keys from InMemoryStore\nDESCRIPTION: Demonstrates how to use the yieldKeys method to iterate over store keys, optionally filtering by prefix.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/stores/in_memory.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { InMemoryStore } from \"@langchain/core/stores\"\nimport { AIMessage, BaseMessage, HumanMessage } from \"@langchain/core/messages\";\n\nconst kvStoreForYield = new InMemoryStore<BaseMessage>();\n\n// Add some data to the store\nawait kvStoreForYield.mset(\n  [\n    [\"message:id:key1\", new HumanMessage(\"value1\")],\n    [\"message:id:key2\", new AIMessage(\"value2\")],\n  ]\n)\n\nconst yieldedKeys = [];\nfor await (const key of kvStoreForYield.yieldKeys(\"message:id:\")) {\n  yieldedKeys.push(key);\n}\n\nconsole.log(yieldedKeys);\n```\n\n----------------------------------------\n\nTITLE: Installing HuggingFace Transformers Dependency (Bash)\nDESCRIPTION: This command installs the `@huggingface/transformers` package using npm. This package is a required peer dependency for the `TransformerEmbeddings` class in LangChain.js, enabling local and browser-based embedding generation. An alternative package `@xenova/transformers` might be needed for older versions of LangChain community.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/transformers.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @huggingface/transformers\n```\n\n----------------------------------------\n\nTITLE: Using AmazonKendraRetriever for Query Invocation in TypeScript\nDESCRIPTION: This snippet shows how to use the instantiated AmazonKendraRetriever to invoke a query and retrieve results.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/kendra-retriever.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst query = \"...\"\n\nawait retriever.invoke(query);\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Dependencies\nDESCRIPTION: Commands for installing the required LangChain packages including core, langgraph, and base package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Community Package\nDESCRIPTION: Command to install the @langchain/community package using Yarn package manager.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-community/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn add @langchain/community\n```\n\n----------------------------------------\n\nTITLE: Instantiating CohereEmbeddings in Python\nDESCRIPTION: This code snippet shows how to instantiate a CohereEmbeddings object with custom settings like API key, batch size, and model selection.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/cohere.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { CohereEmbeddings } from \"@langchain/cohere\";\n\nconst embeddings = new CohereEmbeddings({\n  apiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.COHERE_API_KEY\n  batchSize: 48, // Default value if omitted is 48. Max value is 96\n  model: \"embed-english-v3.0\",\n});\n```\n\n----------------------------------------\n\nTITLE: Loading SRT Subtitles using SRTLoader in LangChain.js\nDESCRIPTION: This TypeScript code demonstrates how to use the SRTLoader from LangChain.js to load subtitle data from an SRT file. It creates a new SRTLoader instance with the path to the subtitle file and then loads the documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/subtitles.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { SRTLoader } from \"@langchain/community/document_loaders/fs/srt\";\n\nconst loader = new SRTLoader(\n  \"src/document_loaders/example_data/Star_Wars_The_Clone_Wars_S06E07_Crisis_at_the_Heart.srt\"\n);\n\nconst docs = await loader.load();\n```\n\n----------------------------------------\n\nTITLE: Using ChatOllama with Multimodal Input in Python\nDESCRIPTION: Python code showing how to use ChatOllama with multimodal input, including loading an image file and passing it to the model along with text.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/ollama.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatOllama } from \"@langchain/ollama\";\nimport { HumanMessage } from \"@langchain/core/messages\";\nimport * as fs from \"node:fs/promises\";\n\nconst imageData = await fs.readFile(\"../../../../../examples/hotdog.jpg\");\nconst llmForMultiModal = new ChatOllama({\n  model: \"llava\",\n  baseUrl: \"http://127.0.0.1:11434\",\n});\nconst multiModalRes = await llmForMultiModal.invoke([\n  new HumanMessage({\n    content: [\n      {\n        type: \"text\",\n        text: \"What is in this image?\",\n      },\n      {\n        type: \"image_url\",\n        image_url: `data:image/jpeg;base64,${imageData.toString(\"base64\")}`,\n      },\n    ],\n  }),\n]);\nconsole.log(multiModalRes);\n```\n\n----------------------------------------\n\nTITLE: Connecting to a Couchbase Cluster in LangChain.js\nDESCRIPTION: Establishing a connection to the Couchbase cluster using the provided connection string, username, and password. The 'wanDevelopment' config profile is specified for development purposes.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/couchbase.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst couchbaseClient = await Cluster.connect(connectionString, {\n  username: dbUsername,\n  password: dbPassword,\n  configProfile: \"wanDevelopment\",\n});\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Integration Package\nDESCRIPTION: Command for installing the OpenAI integration package for LangChain along with the core package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/tests/__mdx__/modules/two.mdx#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Instantiating ChatAnthropic Model in Python\nDESCRIPTION: Creates an instance of the ChatAnthropic model with specified parameters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/anthropic.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatAnthropic } from \"@langchain/anthropic\" \n\nconst llm = new ChatAnthropic({\n    model: \"claude-3-haiku-20240307\",\n    temperature: 0,\n    maxTokens: undefined,\n    maxRetries: 2,\n    // other params...\n});\n```\n\n----------------------------------------\n\nTITLE: Using CheerioWebBaseLoader with CSS selectors for targeted content extraction\nDESCRIPTION: Example of configuring CheerioWebBaseLoader with a CSS selector to only extract specific elements (paragraphs) from a webpage. This demonstrates how to filter content during extraction for more precise data collection.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/web_cheerio.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { CheerioWebBaseLoader } from \"@langchain/community/document_loaders/web/cheerio\"\n\nconst loaderWithSelector = new CheerioWebBaseLoader(\"https://news.ycombinator.com/item?id=34817881\", {\n  selector: \"p\",\n});\n\nconst docsWithSelector = await loaderWithSelector.load();\ndocsWithSelector[0].pageContent;\n```\n\n----------------------------------------\n\nTITLE: Testing Initial Cache Miss\nDESCRIPTION: Example demonstrating first call to model without cached response\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/llm_caching.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconsole.time();\n\n// The first time, it is not yet in cache, so it should take longer\nconst res = await model.invoke(\"Tell me a long joke\");\n\nconsole.log(res);\n\nconsole.timeEnd();\n\n/*\n  A man walks into a bar and sees a jar filled with money on the counter. Curious, he asks the bartender about it.\n\n  The bartender explains, \"We have a challenge for our customers. If you can complete three tasks, you win all the money in the jar.\"\n\n  Intrigued, the man asks what the tasks are.\n\n  The bartender replies, \"First, you have to drink a whole bottle of tequila without making a face. Second, there's a pitbull out back with a sore tooth. You have to pull it out. And third, there's an old lady upstairs who has never had an orgasm. You have to give her one.\"\n\n  The man thinks for a moment and then confidently says, \"I'll do it.\"\n\n  He grabs the bottle of tequila and downs it in one gulp, without flinching. He then heads to the back and after a few minutes of struggling, emerges with the pitbull's tooth in hand.\n\n  The bar erupts in cheers and the bartender leads the man upstairs to the old lady's room. After a few minutes, the man walks out with a big smile on his face and the old lady is giggling with delight.\n\n  The bartender hands the man the jar of money and asks, \"How\n\n  default: 4.187s\n*/\n```\n\n----------------------------------------\n\nTITLE: Debugging Prompt Content for Extraction\nDESCRIPTION: Logs the constructed prompt messages to inspect what will be sent to the model, showing how the formatting instructions and query are integrated.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/extraction_parse.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst promptValue = await partialedPrompt.invoke({ query });\n\nconsole.log(promptValue.toChatMessages());\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Cloudflare Credentials\nDESCRIPTION: Sets up environment variables for Cloudflare account ID and API token for authentication purposes.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/cloudflare_workersai.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// @lc-docs-hide-cell\n\n// @ts-expect-error Deno is not recognized\nconst CLOUDFLARE_ACCOUNT_ID = Deno.env.get(\"CLOUDFLARE_ACCOUNT_ID\");\n// @ts-expect-error Deno is not recognized\nconst CLOUDFLARE_API_TOKEN = Deno.env.get(\"CLOUDFLARE_API_TOKEN\");\n```\n\n----------------------------------------\n\nTITLE: Initializing Vector Store and Retriever with Sample Data\nDESCRIPTION: Creates a vector store from the sample text documents using OpenAI embeddings and initializes a retriever from this vector store. This enables semantic search functionality for the climate change documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/rag_fusion.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\n/** Initialize our vector store with the fake data and OpenAI embeddings. */\nconst vectorStore = await MemoryVectorStore.fromTexts(\n  allDocuments.map(({ text }) => text),\n  allDocuments.map(({ id }) => ({ id })),\n  new OpenAIEmbeddings()\n);\n/** Create the retriever */\nconst retriever = vectorStore.asRetriever();\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatOpenAI Model in JavaScript\nDESCRIPTION: This snippet initializes a ChatOpenAI model using the @langchain/openai package, specifying the GPT-3.5-turbo model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_choice.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ChatOpenAI } from '@langchain/openai';\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-3.5-turbo\",\n})\n```\n\n----------------------------------------\n\nTITLE: Setting Tavily API Key as Environment Variable in TypeScript\nDESCRIPTION: Sets up the Tavily API key as an environment variable for authentication with the Tavily service.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/tavily_search.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.TAVILY_API_KEY = \"YOUR_API_KEY\"\n```\n\n----------------------------------------\n\nTITLE: Example Output of Agent Stream Processing\nDESCRIPTION: Provides a sample text output demonstrating the flow of an agent processing a query ('Who won F1 championship in 2022?'). It shows the initial user message, the agent deciding to use the 'GoogleSearch' tool, the search results returned by the tool, and the final synthesized answer from the agent.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/ibm.mdx#2025-04-22_snippet_10\n\nLANGUAGE: text\nCODE:\n```\nWho won F1 championship in 2022?\n[\n  {\n    name: \"GoogleSearch\",\n    args: { input: \"F1 championship 2022\" },\n    type: \"tool_call\",\n    id: \"chatcmpl-tool-9da3456b9bbc475fb822296fdb8353a8\"\n  }\n]\n[{\"title\":\"2022 DRIVER STANDINGS\",\"description\":\"Official F1® Race Programme · Modern Slavery Statement; Do Not Sell or Share My Personal Information. Formula 1. © 2003-2025 Formula One World Championship\\u00a0...\",\"url\":\"https://www.formula1.com/en/results/2022/drivers\"},{\"title\":\"2022 Formula One World Championship - Wikipedia\",\"description\":\"2022 Formula One World Championship · Max Verstappen won his second consecutive World Drivers' Championship driving for Red Bull Racing. · Charles Leclerc\\u00a0...\",\"url\":\"https://en.wikipedia.org/wiki/2022_Formula_One_World_Championship\"},{\"title\":\"2022 Formula One World Championship - Simple English Wikipedia ...\",\"description\":\"Max Verstappen, who was the reigning Drivers' Champion, claimed his second title at the Japanese Grand Prix, while his team, Red Bull Racing, achieved their\\u00a0...\",\"url\":\"https://simple.wikipedia.org/wiki/2022_Formula_One_World_Championship\"},{\"title\":\"Max Verstappen wins the 2022 F1 Drivers World Championship : r ...\",\"description\":\"Oct 9, 2022 ... One day this guy will win a championship just by finishing the race and celebrating with a world championship lap on the day he won.\",\"url\":\"https://www.reddit.com/r/sports/comments/xzg8pf/max_verstappen_wins_the_2022_f1_drivers_world/\"},{\"title\":\"Red Bull Simulator Championship Edition | F1 Authentics\",\"description\":\"Based on the livery of the 2022 Oracle Red Bull Racing Championship-winning RB18, this F1 simulator has been expertly engineered and manufactured by Memento\\u00a0...\",\"url\":\"https://www.f1authentics.com/products/red-bull-simulator-championship-edition\"},{\"title\":\"F1 2022 Drivers Championship without Max Verstappen : r/formula1\",\"description\":\"Nov 26, 2022 ... 1.1K votes, 65 comments. 5.2M subscribers in the formula1 community. Welcome to r/Formula1, the best independent online Formula 1 community!\",\"url\":\"https://www.reddit.com/r/formula1/comments/z5cwl7/f1_2022_drivers_championship_without_max/\"},{\"title\":\"F1® Sim Racing World Championship 2022\",\"description\":\"World Championship 2022 Bahrain International Circuit Bahrain International Circuit Race Distance: 29 laps Track Length: 5.412 km\",\"url\":\"https://f1esports.com/world/results/2022\"},{\"title\":\"Our personal F1 2022 Predictions Championship : r/formula1\",\"description\":\"May 5, 2022 ... F1 2025 Driver Predictions from mathematical model. Leclerc and Sainz predicted to beat Hamilton and Albon. r/formula1 - F1 2025 Driver\\u00a0...\",\"url\":\"https://www.reddit.com/r/formula1/comments/uitbbu/our_personal_f1_2022_predictions_championship/\"},{\"title\":\"Haas F1 Team Esports announces roster for 2022 F1 Esports Series ...\",\"description\":\"Sep 13, 2022 ... Haas F1 Team is ready to commence battle in the 2022 F1 Esports Series Pro Championship featuring an updated line-up for this season.\",\"url\":\"https://www.haasf1team.com/news/haas-f1-team-esports-announces-roster-2022-f1-esports-series-pro-championship\"},{\"title\":\"2022 F1 Deconstructors Championship : r/formula1\",\"description\":\"Nov 22, 2022 ... Congratulations to our 2022 champion Carlos Sainz. Alonso put in a late surge but had to settle for second place. Alfa put in the best effort\\u00a0...\",\"url\":\"https://www.reddit.com/r/formula1/comments/z1pkkx/2022_f1_deconstructors_championship/\"}]\nMax Verstappen won the 2022 F1 Championship, driving for Red Bull Racing. He claimed his second title at the Japanese Grand Prix.\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for ZhipuAI Integration with LangChain.js\nDESCRIPTION: Command to install the required npm packages for using ZhipuAI with LangChain. This includes the community package, core package, and jsonwebtoken for authentication.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/zhipuai.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core jsonwebtoken\n```\n\n----------------------------------------\n\nTITLE: Adding Documents to Vector Store\nDESCRIPTION: Example of how to add documents to the vector store. Shows the structure of Document objects with page content and metadata, and demonstrates the addDocuments method with document IDs.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/vectorstores.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport type { Document } from \"@langchain/core/documents\";\n\nconst document1: Document = {\n  pageContent: \"The powerhouse of the cell is the mitochondria\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document2: Document = {\n  pageContent: \"Buildings are made out of brick\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document3: Document = {\n  pageContent: \"Mitochondria are made out of lipids\",\n  metadata: { source: \"https://example.com\" }\n};\n\nconst document4: Document = {\n  pageContent: \"The 2024 Olympics are in Paris\",\n  metadata: { source: \"https://example.com\" }\n}\n\nconst documents = [document1, document2, document3, document4];\n\nawait vectorStore.addDocuments(documents, { ids: [\"1\", \"2\", \"3\", \"4\"] });\n```\n\n----------------------------------------\n\nTITLE: Logging RAG Prompt Template in LangChain.js\nDESCRIPTION: Code to log the prompt template used in the Retrieval-Augmented Generation (RAG) chain, displaying the structure of the prompt messages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_streaming.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconsole.log(prompt.promptMessages.map((msg) => msg.prompt.template).join(\"\\n\"));\n```\n\n----------------------------------------\n\nTITLE: Configuring Package Resolution for npm\nDESCRIPTION: Example package.json configuration for npm to ensure consistent @langchain/core version across dependencies.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/tests/__mdx__/modules/index.mdx#2025-04-22_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"your-project\",\n  \"version\": \"0.0.0\",\n  \"private\": true,\n  \"engines\": {\n    \"node\": \">=18\"\n  },\n  \"dependencies\": {\n    \"@langchain/google-genai\": \"^0.0.2\",\n    \"@langchain/core\": \"^0.3.0\"\n  },\n  \"overrides\": {\n    \"@langchain/core\": \"0.3.0\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Linter and Formatter\nDESCRIPTION: Command to run code linting and formatting checks.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-weaviate/README.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nyarn lint && yarn format\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages\nDESCRIPTION: Commands to install the necessary dependencies including @langchain/weaviate and @langchain/core packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-weaviate/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/weaviate @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing AssemblyAI and LangChain Packages\nDESCRIPTION: Command to install the required packages for using AssemblyAI with LangChain.js. This includes the community package, core package, and the official AssemblyAI package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/assemblyai_audio_transcription.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core assemblyai\n```\n\n----------------------------------------\n\nTITLE: Installing MongoDB Integration Dependencies\nDESCRIPTION: Command to install the MongoDB integration package and its core dependency for LangChain.js\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-mongodb/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/mongodb @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Initializing HNSWLib Vector Store with OpenAI Embeddings\nDESCRIPTION: Creates a new instance of HNSWLib vector store using OpenAI embeddings. The vector store is initialized with an empty document array and will use the text-embedding-3-small model for generating embeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/hnswlib.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { HNSWLib } from \"@langchain/community/vectorstores/hnswlib\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddings = new OpenAIEmbeddings({\n  model: \"text-embedding-3-small\",\n});\n\nconst vectorStore = await HNSWLib.fromDocuments([], embeddings);\n```\n\n----------------------------------------\n\nTITLE: Testing Agent with Context-Dependent Query in Python\nDESCRIPTION: This code shows how the agent uses conversation context to interpret and respond to queries that depend on previous interactions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nconst query3 = \"What according to the blog post are common ways of doing it? redo the search\"\n\nfor await (const s of await agentExecutor2.stream({ messages: [{ role: \"user\", content: query3 }] }, config3)) {\n  console.log(s)\n  console.log(\"----\")\n}\n```\n\n----------------------------------------\n\nTITLE: Metadata Filtering in SemanticSimilarityExampleSelector\nDESCRIPTION: This code demonstrates how to use metadata filtering when initializing a SemanticSimilarityExampleSelector for more controlled search space.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/example_selectors_similarity.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport ExampleSimilarityMetadataFiltering from \"@examples/prompts/semantic_similarity_example_selector_metadata_filtering.ts\";\n```\n\n----------------------------------------\n\nTITLE: Environment Variables Configuration\nDESCRIPTION: Required environment variables for PostgresEngine configuration including project details and database credentials.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/google_cloudsql_pg.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nPROJECT_ID=\"your-project-id\"\nREGION=\"your-project-region\" // example: \"us-central1\"\nINSTANCE_NAME=\"your-instance\"\nDB_NAME=\"your-database-name\"\nDB_USER=\"your-database-user\"\nPASSWORD=\"your-database-password\"\n```\n\n----------------------------------------\n\nTITLE: Setting Nomic API Key Environment Variable\nDESCRIPTION: Command to set the required Nomic API key as an environment variable.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-nomic/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport NOMIC_API_KEY=\n```\n\n----------------------------------------\n\nTITLE: Chaining mergeMessageRuns with ChatAnthropic in JavaScript\nDESCRIPTION: Shows how to use mergeMessageRuns in a chain with ChatAnthropic. It demonstrates creating a RunnableLambda that takes messages as input and pipes it to an LLM, effectively merging messages before passing them to the model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/merge_message_runs.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport { mergeMessageRuns } from \"@langchain/core/messages\";\n\nconst llm = new ChatAnthropic({ model: \"claude-3-sonnet-20240229\", temperature: 0 });\n// Notice we don't pass in messages. This creates\n// a RunnableLambda that takes messages as input\nconst merger = mergeMessageRuns();\nconst chain = merger.pipe(llm);\nawait chain.invoke(messages);\n```\n\n----------------------------------------\n\nTITLE: Implementing Message Trimming with LangGraph for Limited Context Windows\nDESCRIPTION: Creates a new LangGraph app that trims the chat history to only include the most recent messages, demonstrating how to handle limited context windows by reducing the number of messages passed to the model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_memory.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { START, END, MessagesAnnotation, StateGraph, MemorySaver } from \"@langchain/langgraph\";\nimport { trimMessages } from \"@langchain/core/messages\";\n\n// Define trimmer\n// count each message as 1 \"token\" (tokenCounter: (msgs) => msgs.length) and keep only the last two messages\nconst trimmer = trimMessages({ strategy: \"last\", maxTokens: 2, tokenCounter: (msgs) => msgs.length });\n\n// Define the function that calls the model\nconst callModel2 = async (state: typeof MessagesAnnotation.State) => {\n  const trimmedMessages = await trimmer.invoke(state.messages);\n  const systemPrompt = \n    \"You are a helpful assistant. \" +\n    \"Answer all questions to the best of your ability.\";\n  const messages = [{ role: \"system\", content: systemPrompt }, ...trimmedMessages];\n  const response = await llm.invoke(messages);\n  return { messages: response };\n};\n\nconst workflow2 = new StateGraph(MessagesAnnotation)\n  // Define the node and edge\n  .addNode(\"model\", callModel2)\n  .addEdge(START, \"model\")\n  .addEdge(\"model\", END);\n\n// Add simple in-memory checkpointer\nconst app2 = workflow2.compile({ checkpointer: new MemorySaver() });\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith Environment Variables for Tracing\nDESCRIPTION: Example of environment variables to enable automated tracing of model calls using LangSmith. This configuration is optional but useful for monitoring and debugging.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/pdf.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Community and Core Packages\nDESCRIPTION: This snippet demonstrates how to install the required LangChain community and core packages using npm or yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/writer.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Community and Core Packages (Bash)\nDESCRIPTION: This command installs the essential LangChain community and core packages (`@langchain/community`, `@langchain/core`) using npm. These packages provide the necessary framework and components, including the `TransformerEmbeddings` class, for building applications with LangChain.js.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/transformers.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Dria Package\nDESCRIPTION: Command to install the core Dria package using npm or yarn package managers.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/dria.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install dria\n```\n\n----------------------------------------\n\nTITLE: Initializing Chat Model\nDESCRIPTION: Creates an instance of ChatOpenAI with specific model and temperature settings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_tools.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o\",\n  temperature: 0,\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Neo4j and LangChain Integration\nDESCRIPTION: This snippet shows the command to install necessary dependencies for the project, including LangChain, OpenAI, Neo4j driver, and Zod.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/graph_semantic.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nlangchain @langchain/community @langchain/openai @langchain/core neo4j-driver zod\n```\n\n----------------------------------------\n\nTITLE: Installing Complete Dependencies with OpenAI\nDESCRIPTION: Installation commands for a complete setup including OpenAI integration along with community packages and core functionality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/file.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Initializing XMLOutputParser\nDESCRIPTION: Shows how to initialize the XMLOutputParser and get format instructions for XML parsing.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/output_parser_xml.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { XMLOutputParser } from \"@langchain/core/output_parsers\";\n\n// We will add these instructions to the prompt below\nconst parser = new XMLOutputParser();\n\nparser.getFormatInstructions();\n```\n\n----------------------------------------\n\nTITLE: Testing Agent with Simple Query in Python\nDESCRIPTION: This code tests the agent's behavior with a simple greeting query, demonstrating that it doesn't use retrieval for queries not requiring it.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nconst threadId3 = uuidv4();\nconst config3 = { configurable: { thread_id: threadId3 } };\n\nfor await (const s of await agentExecutor2.stream({ messages: [{ role: \"user\", content: \"Hi! I'm bob\" }] }, config3)) {\n  console.log(s)\n  console.log(\"----\")\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Figma File Key and Node ID from URL\nDESCRIPTION: Example of how to find the file key and node IDs needed for the FigmaFileLoader by extracting them from a Figma file URL.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/figma.mdx#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nhttps://www.figma.com/file/<YOUR FILE KEY HERE>/LangChainJS-Test?type=whiteboard&node-id=<YOUR NODE ID HERE>&t=e6lqWkKecuYQRyRg-0\n```\n\n----------------------------------------\n\nTITLE: Running Unstructured API locally with Docker\nDESCRIPTION: Command to run the Unstructured API in a Docker container locally for document processing. The container runs on port 8000 and uses the latest Unstructured API image.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/unstructured.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -p 8000:8000 -d --rm --name unstructured-api downloads.unstructured.io/unstructured-io/unstructured-api:latest --port 8000 --host 0.0.0.0\n```\n\n----------------------------------------\n\nTITLE: Closing PGVector Store Connection\nDESCRIPTION: Demonstrates how to properly close the PGVector store connection to avoid resource leaks.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/pgvector.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nawait vectorStore.end();\n```\n\n----------------------------------------\n\nTITLE: Partial Match Search with Fuzziness in Couchbase Vector Store using TypeScript\nDESCRIPTION: This snippet demonstrates how to perform a partial match search with fuzziness, useful for searching slight variations or misspellings of a search query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/couchbase.mdx#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst partialMatchResult = await store.similaritySearch(query, 4, {\n  fields: [\"metadata.author\"],\n  searchOptions: {\n    query: { field: \"metadata.author\", match: \"Johny\", fuzziness: 1 },\n  },\n});\nconsole.log(partialMatchResult[0]);\n```\n\n----------------------------------------\n\nTITLE: Formatted Few Shot Prompt Output in LangChainJS\nDESCRIPTION: Shows the output of the formatted few shot prompt, displaying how the examples are structured as HumanMessages for the LLM.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\n[\n  HumanMessage {\n    lc_namespace: [ 'langchain', 'schema' ],\n    content: 'Human: Could the members of The Police perform lawful arrests?\\n' +\n      'AI: what can the members of The Police do?',\n    additional_kwargs: {}\n  },\n  HumanMessage {\n    lc_namespace: [ 'langchain', 'schema' ],\n    content: \"Human: Jan Sindel's was born in what country?\\n\" +\n      \"AI: what is Jan Sindel's personal history?\",\n    additional_kwargs: {}\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Initializing Local File Cache in TypeScript\nDESCRIPTION: This snippet shows how to create a local file system cache for development purposes. It's not recommended for production use.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chat_model_caching.mdx#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nconst cache = await LocalFileCache.create();\n```\n\n----------------------------------------\n\nTITLE: Loading College Confidential Data with TypeScript\nDESCRIPTION: Demonstrates how to initialize and use the CollegeConfidentialLoader to fetch data from a specific college page. The loader creates documents containing the parsed content from the specified URL.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/college_confidential.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CollegeConfidentialLoader } from \"@langchain/community/document_loaders/web/college_confidential\";\n\nconst loader = new CollegeConfidentialLoader(\n  \"https://www.collegeconfidential.com/colleges/brown-university/\"\n);\n\nconst docs = await loader.load();\n```\n\n----------------------------------------\n\nTITLE: Setting Together AI API Key in Environment\nDESCRIPTION: Sets the Together AI API key as an environment variable for authentication with the Together AI platform.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/togetherai.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport TOGETHER_AI_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Deleting Data from InMemoryStore\nDESCRIPTION: Shows how to delete multiple keys from the store using the mdelete method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/stores/in_memory.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nawait kvStore.mdelete(\n  [\n    \"key1\",\n    \"key2\",\n  ]\n)\n\nawait kvStore.mget(\n  [\n    \"key1\",\n    \"key2\",\n  ]\n)\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith Environment Variables\nDESCRIPTION: Configuring environment variables for LangSmith tracing functionality\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_citations.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport LANGSMITH_TRACING=true\nexport LANGSMITH_API_KEY=YOUR_KEY\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith Environment Variables for Tracing (TypeScript)\nDESCRIPTION: This commented-out snippet shows how to optionally configure environment variables for LangSmith integration. Setting these enables automated tracing of model calls within the LangChain application.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/supabase.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\n// process.env.LANGSMITH_TRACING=\"true\"\n// process.env.LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Bearer Token Authentication in IBM watsonx.ai\nDESCRIPTION: Configures environment variables for bearer token authentication with IBM watsonx.ai. This includes setting the authentication type and bearer token.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/ibm.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport WATSONX_AI_AUTH_TYPE=bearertoken\nexport WATSONX_AI_BEARER_TOKEN=<YOUR-BEARER-TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for LangChain.js\nDESCRIPTION: This snippet shows how to install the necessary npm packages for using LangChain.js with OpenAI and community modules.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/contextual_compression.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Adding Database Accessors in TypeScript\nDESCRIPTION: Exports Convex utility functions from the LangChain community package to facilitate query and mutation operations. This snippet is a dependency for accessing database functionalities within a LangChain.js project.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/convex.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nexport * from \"@langchain/community/utils/convex\";\n```\n\n----------------------------------------\n\nTITLE: Implementing InMemoryChatMessageHistory with session tracking\nDESCRIPTION: Creates a dictionary to track multiple chat histories by session ID and provides a function to retrieve or create chat histories for specific sessions. This implementation helps manage different conversations.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/versions/migrating_memory/chat_history.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { InMemoryChatMessageHistory } from \"@langchain/core/chat_history\";\n\nconst chatsBySessionId: Record<string, InMemoryChatMessageHistory> = {}\n\nconst getChatHistory = (sessionId: string) => {\n    let chatHistory: InMemoryChatMessageHistory | undefined = chatsBySessionId[sessionId]\n    if (!chatHistory) {\n      chatHistory = new InMemoryChatMessageHistory()\n      chatsBySessionId[sessionId] = chatHistory\n    }\n    return chatHistory\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Notion Markdown Exports using LangChain.js in TypeScript\nDESCRIPTION: This code snippet demonstrates how to use the NotionLoader from LangChain.js to load Notion pages exported as Markdown files. It specifies the directory path containing the exported files and loads them into the system.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/notion_markdown.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { NotionLoader } from \"langchain/document_loaders/fs/notion\";\n\n// Specify the directory path where your exported Notion files are located\nconst directoryPath = \"./notion_md_files\";\n\n// Create a new NotionLoader instance\nconst loader = new NotionLoader(directoryPath);\n\n// Load the documents\nconst docs = await loader.load();\n\nconsole.log({ docs });\n```\n\n----------------------------------------\n\nTITLE: Installing Vercel KV\nDESCRIPTION: Installation command for Vercel KV package\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/llm_caching.mdx#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @vercel/kv\n```\n\n----------------------------------------\n\nTITLE: Installing Azure Cosmos DB for NoSQL Integration Dependencies\nDESCRIPTION: Command to install the required packages for using Azure Cosmos DB for NoSQL as a vector store in LangChain.js. This integration supports vector indexing and search capabilities.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/microsoft.mdx#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/azure-cosmosdb @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages with NPM and Yarn\nDESCRIPTION: Command for installing the necessary dependencies including @langchain/openai, @langchain/core, and cheerio packages for implementing retrieval functionality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_retrieval.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: mdx\nCODE:\n```\nimport Npm2Yarn from \"@theme/Npm2Yarn\";\n\n<Npm2Yarn>\n  @langchain/openai @langchain/core cheerio\n</Npm2Yarn>\n```\n\n----------------------------------------\n\nTITLE: Installing Tigris SDK with NPM\nDESCRIPTION: This command installs the Tigris SDK, which is required to interact with Tigris' cloud-native vector database. The SDK is a dependency that enables the storage and indexing of documents with vector embeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/tigris.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -S @tigrisdata/vector\n```\n\n----------------------------------------\n\nTITLE: Resolving @langchain/core Version with npm\nDESCRIPTION: Example package.json configuration to ensure a single version of @langchain/core is used with npm.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/installation.mdx#2025-04-22_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"your-project\",\n  \"version\": \"0.0.0\",\n  \"private\": true,\n  \"engines\": {\n    \"node\": \">=18\"\n  },\n  \"dependencies\": {\n    \"@langchain/anthropic\": \"^0.0.2\",\n    \"@langchain/core\": \"^0.3.0\",\n    \"langchain\": \"0.0.207\"\n  },\n  \"overrides\": {\n    \"@langchain/core\": \"0.3.0\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Recursively Splitting Text with RecursiveCharacterTextSplitter in JavaScript\nDESCRIPTION: This snippet demonstrates how to use RecursiveCharacterTextSplitter to split text based on a hierarchy of characters, keeping semantically related pieces together. It shows how to create text chunks with customizable parameters like chunkSize and chunkOverlap.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/static/llms.txt#2025-04-22_snippet_2\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\n\nconst text = \"Your long text here...\";\nconst splitter = new RecursiveCharacterTextSplitter({\n  chunkSize: 1000,\n  chunkOverlap: 200\n});\n\nconst output = await splitter.createDocuments([text]);\n```\n\n----------------------------------------\n\nTITLE: Using the Custom Chat Model\nDESCRIPTION: Demonstrates how to instantiate and invoke the custom chat model with a simple human message. The model will return the first 4 characters of the input message.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/custom_chat.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst chatModel = new CustomChatModel({ n: 4 });\n\nawait chatModel.invoke([[\"human\", \"I am an LLM\"]]);\n```\n\n----------------------------------------\n\nTITLE: Invoking a Converted Tool with Object Input in TypeScript\nDESCRIPTION: This snippet shows how to invoke a tool that was converted from a runnable with object schema. It calls the tool with an object containing the number 3 and an array of numbers [1, 2].\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/convert_runnable_to_tool.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nawait asTool.invoke({ a: 3, b: [1, 2] })\n```\n\n----------------------------------------\n\nTITLE: Configuring LangSmith Tracing for Model Calls\nDESCRIPTION: Optional configuration for enabling automated tracing of model calls using LangSmith. This allows for monitoring and debugging of vector store operations.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/hnswlib.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// process.env.LANGSMITH_TRACING=\"true\"\n// process.env.LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Extracting a Single ChatGPT Conversation Log with TypeScript\nDESCRIPTION: This snippet shows how to extract a specific conversation log from a ChatGPT data export file. It initializes the ChatGPTLoader with the file path and a specific conversation index (1), then loads only that conversation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/chatgpt.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatGPTLoader } from \"@langchain/community/document_loaders/fs/chatgpt\";\n\nconst loader = new ChatGPTLoader(\n  \"./example_data/example_conversations.json\",\n  1\n);\n\nconst docs = await loader.load();\n\nconsole.log(docs);\n```\n\n----------------------------------------\n\nTITLE: Creating Example Dataset in LangSmith\nDESCRIPTION: Example code for creating a test dataset in LangSmith using faker to generate sample data.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/langsmith.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { Client as LangSmithClient } from 'langsmith';\nimport { faker } from \"@faker-js/faker\";\n\nconst lsClient = new LangSmithClient();\n\nconst datasetName = \"LangSmith Few Shot Datasets Notebook\";\n\nconst exampleInputs = Array.from({ length: 10 }, (_, i) => ({\n  input: faker.lorem.paragraph(),\n}));\nconst exampleOutputs = Array.from({ length: 10 }, (_, i) => ({\n  output: faker.lorem.sentence(),\n}));\nconst exampleMetadata = Array.from({ length: 10 }, (_, i) => ({\n  companyCatchPhrase: faker.company.catchPhrase(),\n}));\n\nawait lsClient.deleteDataset({\n  datasetName,\n})\n\nconst dataset = await lsClient.createDataset(datasetName);\n\nconst examples = await lsClient.createExamples({\n  inputs: exampleInputs,\n  outputs: exampleOutputs,\n  metadata: exampleMetadata,\n  datasetId: dataset.id,\n});\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Community and Core Packages using npm/yarn\nDESCRIPTION: This command installs the `@langchain/community` and `@langchain/core` packages using npm. The `npm2yarn` directive suggests it can also be installed using yarn. These packages provide the necessary LangChain functionalities for integrating with various services, including vector stores like Vercel Postgres.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/vercel_postgres.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Using ChatBaiduQianfan for chat model inference\nDESCRIPTION: Example of creating a ChatBaiduQianfan instance to interact with Baidu's ERNIE-Lite-8K model. The code initializes the chat model and sends a human message to get a response.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-baidu-qianfan/README.md#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatBaiduQianfan } from \"@langchain/baidu-qianfan\";\nimport { HumanMessage } from \"@langchain/core/messages\";\n\nconst chat = new ChatBaiduQianfan({\n    model: 'ERNIE-Lite-8K'\n});\nconst message = new HumanMessage(\"北京天气\");\n\nconst res = await chat.invoke([message]);\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith Environment Variables in TypeScript\nDESCRIPTION: Configures environment variables for LangSmith tracing to monitor tool executions. This enables automated tracing of tool runs for debugging and optimization purposes.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/toolkits.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.LANGSMITH_TRACING=\"true\"\nprocess.env.LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Installing Pinecone Dependencies with NPM/Yarn\nDESCRIPTION: Commands to install the required packages including @langchain/pinecone, @langchain/core, and @pinecone-database/pinecone.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-pinecone/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/pinecone @langchain/core @pinecone-database/pinecone\n```\n\n----------------------------------------\n\nTITLE: Generating and Comparing Embeddings\nDESCRIPTION: This code demonstrates how to generate embeddings for document chunks and compare their lengths using the OpenAIEmbeddings model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/retrievers.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nconst vector1 = await embeddings.embedQuery(allSplits[0].pageContent)\nconst vector2 = await embeddings.embedQuery(allSplits[1].pageContent)\n\n\nconsole.assert(vector1.length === vector2.length);\nconsole.log(`Generated vectors of length ${vector1.length}\\n`);\nconsole.log(vector1.slice(0, 10));\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for College Confidential Loader\nDESCRIPTION: Install the required npm packages including LangChain community modules, core functionality, and Cheerio for web scraping.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/college_confidential.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core cheerio\n```\n\n----------------------------------------\n\nTITLE: Using Retrievers in LangChain\nDESCRIPTION: Demonstrates how to use LangChain's retriever interface to query data stores or databases in a standardized way.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/why_langchain.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst documents = await myRetriever.invoke(\"What is the meaning of life?\");\n```\n\n----------------------------------------\n\nTITLE: Importing dotenv Configuration in JavaScript\nDESCRIPTION: This snippet imports the dotenv configuration to load environment variables.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport \"dotenv/config\";\n```\n\n----------------------------------------\n\nTITLE: Instantiating ChatOpenAI Model in TypeScript\nDESCRIPTION: This code creates an instance of the ChatOpenAI model with specific parameters. It sets the model to 'gpt-4o-mini' and temperature to 0.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/vectorstore.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o-mini\",\n  temperature: 0,\n})\n```\n\n----------------------------------------\n\nTITLE: Instantiating TavilySearch Tool in TypeScript\nDESCRIPTION: Creates an instance of the TavilySearch tool with configuration options such as maximum results and topic category.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/tavily_search.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { TavilySearch } from \"@langchain/tavily\";\n\nconst tool = new TavilySearch({\n  maxResults: 5,\n  topic: \"general\",\n  // includeAnswer: false,\n  // includeRawContent: false,\n  // includeImages: false,\n  // includeImageDescriptions: false,\n  // searchDepth: \"basic\",\n  // timeRange: \"day\",\n  // includeDomains: [],\n  // excludeDomains: [],\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Azure Dynamic Sessions Integration for LangChain.js\nDESCRIPTION: This command installs the necessary packages for using Azure Container Apps Dynamic Sessions with LangChain.js. It includes the Azure Dynamic Sessions package and the LangChain core package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/microsoft.mdx#2025-04-22_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/azure-dynamic-sessions @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Setting Browserbase API Key for Remote Browser\nDESCRIPTION: This command sets the Browserbase API key as an environment variable, required for using remote headless browsers through the Browserbase platform.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/stagehand.mdx#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport BROWSERBASE_API_KEY=\"your-browserbase-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Blocking Callbacks in JavaScript for Serverless Environments\nDESCRIPTION: This code snippet demonstrates how to make callbacks blocking by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to 'false'. It shows that the invoke() call waits for the callback to complete before returning.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/callbacks_serverless.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprocess.env.LANGCHAIN_CALLBACKS_BACKGROUND = \"false\";\n\nconst startTimeBlocking = new Date().getTime();\n\nawait runnable.invoke({ number: \"2\" }, { callbacks: [customHandler] });\n\nconsole.log(`Initial elapsed time: ${new Date().getTime() - startTimeBlocking}ms`);\n```\n\n----------------------------------------\n\nTITLE: Importing S3Loader for AWS S3 Document Loading in TypeScript\nDESCRIPTION: This snippet shows how to import the S3Loader class from the LangChain community package for loading documents from AWS S3.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/aws.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { S3Loader } from \"@langchain/community/document_loaders/web/s3\";\n```\n\n----------------------------------------\n\nTITLE: Loading Sitemap with Page Contents in TypeScript\nDESCRIPTION: This code demonstrates how to use the SitemapLoader to load a sitemap and its page contents into Documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/sitemap.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { SitemapLoader } from \"@langchain/community/document_loaders/web/sitemap\";\n\nconst loader = new SitemapLoader(\"https://js.langchain.com/sitemap.xml\");\nconst docs = await loader.load();\n\nconsole.log({ docs });\n```\n\n----------------------------------------\n\nTITLE: Importing DynamoDBChatMessageHistory for AWS DynamoDB in TypeScript\nDESCRIPTION: This code demonstrates how to import the DynamoDBChatMessageHistory class from the LangChain community package for storing chat message history in AWS DynamoDB.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/aws.mdx#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { DynamoDBChatMessageHistory } from \"@langchain/community/stores/message/dynamodb\";\n```\n\n----------------------------------------\n\nTITLE: Initializing AsyncLocalStorage for Automatic Config\nDESCRIPTION: Sets up AsyncLocalStorage to enable automatic configuration propagation in supported environments.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_stream_events.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { AsyncLocalStorageProviderSingleton } from \"@langchain/core/singletons\";\nimport { AsyncLocalStorage } from \"async_hooks\";\n\nAsyncLocalStorageProviderSingleton.initializeGlobalInstance(\n  new AsyncLocalStorage()\n);\n```\n\n----------------------------------------\n\nTITLE: Setting Google Cloud Project Configuration\nDESCRIPTION: Command to set the Google Cloud project ID for local development.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/google_cloudsql_pg.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngcloud config set project YOUR-PROJECT-ID\n```\n\n----------------------------------------\n\nTITLE: Defining Storage Attached Indexes (SAIs) in CassandraStore Configuration\nDESCRIPTION: Demonstrates how to configure Storage Attached Indexes (SAIs) using the `indices` parameter during `CassandraStore` initialization. Each index requires a `name` (used for the index object name) and a `value` (the column definition, e.g., `(some_column)`). SAIs enable efficient filtering (`WHERE` clauses) on non-partition key columns.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/cassandra.mdx#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\n  indices: [{ name: \"some_column\", value: \"(some_column)\" }],\n```\n\n----------------------------------------\n\nTITLE: Importing Azure Cosmos DB for NoSQL Vector Store\nDESCRIPTION: Code snippet for importing the Azure Cosmos DB for NoSQL vector store class for use with vector embeddings. This integration allows storing vectors alongside traditional document data.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/microsoft.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AzureCosmosDBNoSQLVectorStore } from \"@langchain/azure-cosmosdb\";\n```\n\n----------------------------------------\n\nTITLE: Accessing Logprobs and Response Metadata\nDESCRIPTION: Shows how to access additional information like logprobs from the ChatOpenAI response metadata, which requires @langchain/core version >=0.1.48.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\n// See https://cookbook.openai.com/examples/using_logprobs for details\nconst llmWithLogprobs = new ChatOpenAI({\n  logprobs: true,\n  // topLogprobs: 5,\n});\n\nconst responseMessageWithLogprobs = await llmWithLogprobs.invoke(\"Hi there!\");\nconsole.dir(responseMessageWithLogprobs.response_metadata.logprobs, { depth: null });\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Model with Caching\nDESCRIPTION: Basic setup of OpenAI model with caching enabled\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/llm_caching.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAI } from \"@langchain/openai\";\n\nconst model = new OpenAI({\n  model: \"gpt-3.5-turbo-instruct\",\n  cache: true,\n});\n```\n\n----------------------------------------\n\nTITLE: Example of Valid JSON Output in TypeScript\nDESCRIPTION: Shows a correctly formatted AI message output containing valid JSON within markdown code fences that an output parser can successfully handle.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nAIMessage {\n  content: \"```\\n{\\\"foo\\\": \\\"bar\\\"}\\n```\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Chaindesk Retriever\nDESCRIPTION: Command to install the required LangChain packages for using the Chaindesk Retriever\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/chaindesk-retriever.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for LangChain Per-User Retrieval\nDESCRIPTION: Command to install necessary packages for implementing per-user retrieval using LangChain with Pinecone and OpenAI.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_per_user.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n@langchain/pinecone @langchain/openai @langchain/core @pinecone-database/pinecone\n```\n\n----------------------------------------\n\nTITLE: Installing Azure Cosmos DB NoSQL Package for LangChain\nDESCRIPTION: Command to install the required packages for using Azure Cosmos DB NoSQL with LangChain. This includes the core Azure Cosmos DB package and LangChain core dependencies.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/azure_cosmosdb_nosql.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/azure-cosmosdb @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Accessing Document Metadata\nDESCRIPTION: Example showing how to access the metadata of a loaded document.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/text.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(docs[0].metadata)\n```\n\n----------------------------------------\n\nTITLE: Dispatching Custom Events with Stream Events API in Node.js\nDESCRIPTION: This snippet demonstrates how to use the dispatchCustomEvent API to emit custom events from within a Runnable and consume them using the streamEvents method. It uses the default import which supports async_hooks.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/callbacks_custom_events.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RunnableLambda } from \"@langchain/core/runnables\";\nimport { dispatchCustomEvent } from \"@langchain/core/callbacks/dispatch\";\n\nconst reflect = RunnableLambda.from(async (value: string) => {\n  await dispatchCustomEvent(\"event1\", { reversed: value.split(\"\").reverse().join(\"\") });\n  await dispatchCustomEvent(\"event2\", 5);\n  return value;\n});\n\nconst eventStream = await reflect.streamEvents(\"hello world\", { version: \"v2\" });\n\nfor await (const event of eventStream) {\n  if (event.event === \"on_custom_event\") {\n    console.log(event);\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Using a Toolkit to Get Multiple Tools in TypeScript\nDESCRIPTION: Shows how to initialize a toolkit and retrieve all its tools. Toolkits are collections of related tools designed to work together for specific tasks.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/tools.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\n// Initialize a toolkit\nconst toolkit = new ExampleTookit(...)\n\n// Get list of tools\nconst tools = toolkit.getTools()\n```\n\n----------------------------------------\n\nTITLE: Using Suffix in Model Invocation\nDESCRIPTION: Demonstration of using suffixes to constrain model outputs and implementing a custom output parser.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/mistral.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport { MistralAI } from \"@langchain/mistralai\";\n\nconst llmForFillInCompletion = new MistralAI({\n  model: \"codestral-latest\",\n  temperature: 0,\n});\n\nconst suffix = \"```\";\n\nconst customOutputParser = (input: string) => {\n  if (input.includes(suffix)) {\n    return input.split(suffix)[0];\n  }\n  throw new Error(\"Input does not contain suffix.\")\n};\n\nconst resWithParser = await llmForFillInCompletion.invoke(\n  \"You can print 'hello world' to the console in javascript like this:\\n```javascript\", {\n    suffix,\n  }\n);\n\nconsole.log(customOutputParser(resWithParser));\n```\n\n----------------------------------------\n\nTITLE: Duplicate Tool Response Example\nDESCRIPTION: Demonstrates an error case where duplicate tool responses are provided for the same tool call.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/troubleshooting/errors/INVALID_TOOL_RESULTS.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst duplicateToolResponse2 = await dummyTool.invoke(responseMessage.tool_calls![1]);\n\nchatHistory.push(duplicateToolResponse2);\n\nawait modelWithTools.invoke(chatHistory);\n```\n\n----------------------------------------\n\nTITLE: Playwright Loader Options Interface\nDESCRIPTION: Type definition showing the available configuration options for PlaywrightWebBaseLoader\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/web_playwright.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\ntype PlaywrightWebBaseLoaderOptions = {\n  launchOptions?: LaunchOptions;\n  gotoOptions?: PlaywrightGotoOptions;\n  evaluate?: PlaywrightEvaluate;\n};\n```\n\n----------------------------------------\n\nTITLE: Configuring Optional LangSmith Tracing (TypeScript)\nDESCRIPTION: Shows commented-out TypeScript code for enabling optional LangSmith tracing. To activate tracing, uncomment the lines and set the `LANGSMITH_TRACING` environment variable to `\"true\"` and `LANGSMITH_API_KEY` to your actual LangSmith API key.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/memory.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n// process.env.LANGSMITH_TRACING=\"true\"\n// process.env.LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for SQL Query Generation with LangChain\nDESCRIPTION: Bash commands to install necessary npm packages for SQL query generation using LangChain, including OpenAI integration and SQLite support.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/sql_prompting.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/openai typeorm sqlite3\n```\n\n----------------------------------------\n\nTITLE: Installing Firebase Admin and LangChain Dependencies\nDESCRIPTION: Commands to install required npm packages including firebase-admin and LangChain related packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/firestore.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install firebase-admin\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Dependencies for Connery Toolkit\nDESCRIPTION: This snippet shows the command to install the necessary LangChain peer dependencies for using the Connery Toolkit. It includes @langchain/openai, @langchain/community, and @langchain/core packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/connery.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Dependencies\nDESCRIPTION: Command to install required npm packages for using LangChain with OpenAI integration\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/motorhead_memory.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Discord Integration - Bash\nDESCRIPTION: Installs the 'discord.js' package, which is a required peer dependency for using the Discord Tool with LangChain.js. This setup is necessary before integrating Discord functionalities and must be completed prior to any coding. Run this command in your project directory using your terminal.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/discord.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install discord.js\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatBedrockConverse for Basic Usage\nDESCRIPTION: Example of initializing and using the ChatBedrockConverse model with AWS Bedrock credentials.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-aws/README.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatBedrockConverse } from \"@langchain/aws\";\n\nconst model = new ChatBedrockConverse({\n  region: process.env.BEDROCK_AWS_REGION ?? \"us-east-1\",\n  credentials: {\n    secretAccessKey: process.env.BEDROCK_AWS_SECRET_ACCESS_KEY,\n    accessKeyId: process.env.BEDROCK_AWS_ACCESS_KEY_ID,\n  },\n});\n\nconst response = await model.invoke(new HumanMessage(\"Hello world!\"));\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Chat Model for Tool Integration\nDESCRIPTION: Code to initialize a ChatOpenAI model instance that will be used with the SerpAPI tool.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/serpapi.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n// @lc-docs-hide-cell\n\nimport { ChatOpenAI } from \"@langchain/openai\"\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o-mini\",\n  temperature: 0,\n})\n```\n\n----------------------------------------\n\nTITLE: Displaying Document Metadata\nDESCRIPTION: Code to display the metadata of the first loaded document.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/web_puppeteer.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(docs[0].metadata)\n```\n\n----------------------------------------\n\nTITLE: Importing ChatOpenAI Model in TypeScript\nDESCRIPTION: This snippet shows how to import the ChatOpenAI model from the @langchain/openai package. ChatOpenAI is used to interact with OpenAI's chat-based models such as GPT.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/openai.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n```\n\n----------------------------------------\n\nTITLE: Document Content Preview\nDESCRIPTION: Simple code to preview the first 500 characters of the loaded document content.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(docs[0].pageContent.slice(0, 500));\n```\n\n----------------------------------------\n\nTITLE: Basic Tool Invocation Example\nDESCRIPTION: Shows a simple invocation of the calculator tool with a custom operator, demonstrating how the model handles undefined operations without context.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tools_few_shot.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst res = await llmWithTools.invoke(\"What is 3 🦜 12\");\n\nconsole.log(res.content);\nconsole.log(res.tool_calls);\n```\n\n----------------------------------------\n\nTITLE: Removing Hooks from ChatMistralAI Model\nDESCRIPTION: Demonstrates how to remove individual hooks or clear all hooks from a ChatMistralAI model instance.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/mistralai.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nmodel.removeHookFromHttpClient(beforeRequestHook);\n\nmodel.removeAllHooksFromHttpClient();\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain dependencies for Tencent Hunyuan integration\nDESCRIPTION: Command to install the required LangChain packages for Tencent Hunyuan integration. These packages provide the core functionality and community extensions needed for using Tencent Hunyuan models.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/tencent_hunyuan.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Formatting the Few Shot Prompt in LangChainJS\nDESCRIPTION: Formats the few shot prompt to see how the examples will be presented to the LLM.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst formattedPrompt = await fewShotPrompt.format({});\nconsole.log(formattedPrompt);\n```\n\n----------------------------------------\n\nTITLE: Importing BedrockChat for AWS Bedrock Chat Models in TypeScript\nDESCRIPTION: This snippet shows how to import the BedrockChat class from the LangChain community package for using AWS Bedrock chat models.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/aws.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BedrockChat } from \"@langchain/community/chat_models/bedrock\";\n```\n\n----------------------------------------\n\nTITLE: Customizing DeepInfraEmbeddings Base URL\nDESCRIPTION: Demonstrates how to customize the base URL for API requests using the configuration parameter.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/deepinfra.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst customEmbeddings = new DeepInfraEmbeddings({\n  apiToken: \"YOUR_API_TOKEN\",\n  configuration: {\n    baseURL: \"https://your_custom_url.com\",\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Invoking a Tool with Different Input Types in Python\nDESCRIPTION: These snippets demonstrate how to invoke a tool using both object and string inputs, highlighting the flexibility of tool usage.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tools_builtin.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nawait tool.invoke({ input: \"langchain\" })\n```\n\nLANGUAGE: python\nCODE:\n```\nawait tool.invoke(\"langchain\")\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Chatbot Development\nDESCRIPTION: Commands to install the necessary packages (@langchain/core, @langchain/langgraph, and uuid) for building a chatbot using LangChain and LangGraph.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/chatbot.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n@langchain/core @langchain/langgraph uuid\n```\n\n----------------------------------------\n\nTITLE: Configuring Azure Cosmos DB Credentials with Environment Variables (text)\nDESCRIPTION: Lists the required environment variables for connecting to an Azure Cosmos DB for NoSQL instance. Typically includes values such as the connection string, database ID, and container ID. These should be set in a .env file or the system environment prior to running code that interacts with Azure Cosmos DB. Correct values enable secure and valid authentication for LangChain-based integrations.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/azure_cosmosdb_nosql.mdx#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nAZURE_COSMOSDB_CONNECTION_STRING=your_connection_string_here\nAZURE_COSMOSDB_DATABASE_ID=your_database_id_here\nAZURE_COSMOSDB_CONTAINER_ID=your_container_id_here\n```\n\n----------------------------------------\n\nTITLE: Using AzionRetriever for Document Retrieval in Python\nDESCRIPTION: Demonstrates how to use the instantiated AzionRetriever to retrieve documents based on a query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/azion-edgesql.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst query = \"Australia\"\n\nawait retriever.invoke(query);\n```\n\n----------------------------------------\n\nTITLE: Loading CSV Data with CSVLoader in JavaScript\nDESCRIPTION: This snippet demonstrates how to use the CSVLoader in LangChain to load CSV files as documents. It shows examples of extracting all columns or a single column from a CSV file.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/static/llms.txt#2025-04-22_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { CSVLoader } from \"langchain/document_loaders/fs/csv\";\n\n// Load CSV file\nconst loader = new CSVLoader(\"path/to/file.csv\");\nconst docs = await loader.load();\n\n// Load CSV file with specific column\nconst loader = new CSVLoader(\"path/to/file.csv\", \"column_name\");\nconst docs = await loader.load();\n```\n\n----------------------------------------\n\nTITLE: Installing GOAT Core Package and LangChain Adapter in Bash\nDESCRIPTION: This snippet installs the core package of the GOAT finance toolkit along with the langchain adapter. It requires NPM and internet access to fetch the packages. The command installs `@goat-sdk/core` and `@goat-sdk/adapter-langchain` packages from the NPM registry. No additional inputs are required, and it outputs the installation of the packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/goat.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @goat-sdk/core @goat-sdk/adapter-langchain\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Tool Dependencies - Bash\nDESCRIPTION: Installs the '@langchain/openai' and '@langchain/core' packages, providing the core AI and integration capabilities for LangChain.js. These are fundamental for setting up agents and tools, including Discord integrations. This command should be executed in your Node.js project before using any code samples.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/discord.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Gracefully Closing the Vector Store Connection - TypeScript\nDESCRIPTION: Safely terminates all open connections associated with the vector store by calling vectorStore.end(). This prevents resource leaks in long-running applications. Assumes vectorStore was previously initialized and used.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/mariadb.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nawait vectorStore.end();\n\n```\n\n----------------------------------------\n\nTITLE: Defining Tool Schema Without Execution Function\nDESCRIPTION: Shows how to define just the tool schema for tool calling that doesn't require a function to execute. This approach creates a tool specification with name, description, and expected input schema.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/tool_calling.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst multiplyTool = {\n  name: \"multiply\",\n  description: \"Multiply two numbers\",\n  schema: z.object({\n    a: z.number(),\n    b: z.number(),\n  }),\n};\n```\n\n----------------------------------------\n\nTITLE: Demonstrating LLM Unawareness of User ID in TypeScript\nDESCRIPTION: Invokes the handleRunTimeRequest function and logs the tool calls generated by the LLM. This demonstrates that the LLM is unaware of the userId parameter, as it doesn't appear in the generated tool parameters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_runtime.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst aiMessage = await handleRunTimeRequest(\n  \"cobb\", \"my favorite pets are tigers and wolves.\", llm,\n);\nconsole.log(aiMessage.tool_calls[0]);\n```\n\n----------------------------------------\n\nTITLE: Demonstrating MESSAGE_COERCION_FAILURE in LangChain.js\nDESCRIPTION: This code snippet shows an example of passing an incorrectly formatted message object to a model's invoke method, which results in a MESSAGE_COERCION_FAILURE error. The error occurs because the message object doesn't conform to the expected MessageLike format.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nconst badlyFormattedMessageObject = {\n  role: \"foo\",\n  randomNonContentValue: \"bar\",\n};\n\nawait model.invoke([badlyFormattedMessageObject]);\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatOpenAI Model for Agent (TypeScript)\nDESCRIPTION: Initializes a `ChatOpenAI` language model instance (using `gpt-4o`) specifically for use within a LangGraph agent. Temperature is set to 0 for more deterministic responses.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/exa_search.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\n// @lc-docs-hide-cell\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst llmForAgent = new ChatOpenAI({\n  model: \"gpt-4o\",\n  temperature: 0\n})\n```\n\n----------------------------------------\n\nTITLE: Installing Langchain Community and Core Packages using MDX Component\nDESCRIPTION: Provides the command to install the `@langchain/community` and `@langchain/core` packages using npm or yarn, displayed within an MDX component. These packages are necessary for using the WatsonxToolkit and core Langchain functionalities.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/ibm.mdx#2025-04-22_snippet_1\n\nLANGUAGE: mdx\nCODE:\n```\nimport IntegrationInstallTooltip from \"@mdx_components/integration_install_tooltip.mdx\";\nimport Npm2Yarn from \"@theme/Npm2Yarn\";\n\n<IntegrationInstallTooltip></IntegrationInstallTooltip>\n\n<Npm2Yarn>\n  @langchain/community @langchain/core\n</Npm2Yarn>\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Community, Supabase, and OpenAI Dependencies (Shell)\nDESCRIPTION: This snippet shows the command to install the necessary npm packages for using Supabase vector stores with LangChain.js, including the core LangChain community package, the Supabase client library, and the OpenAI integration for embeddings. It uses the Npm2Yarn component, implying either npm or yarn can be used.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/supabase.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n@langchain/community @langchain/core @supabase/supabase-js @langchain/openai\n```\n\n----------------------------------------\n\nTITLE: Example JSONLines File Structure\nDESCRIPTION: Example showing the format of a JSONLines file where each line is a valid JSON object containing an HTML property.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/jsonlines.mdx#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\"html\": \"This is a sentence.\"}\n{\"html\": \"This is another sentence.\"}\n```\n\n----------------------------------------\n\nTITLE: Instantiating Embedding Model\nDESCRIPTION: Example of creating a new embedding model instance with configuration parameters\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/text_embedding.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { __module_name__ } from \"__full_import_path__\";\n\nconst embeddings = new __module_name__({\n  model: \"model-name\",\n  // ...\n});\n```\n\n----------------------------------------\n\nTITLE: Loading Documents from Couchbase Using the Lazy Method in LangChain.js\nDESCRIPTION: Using the 'lazyLoad' method to asynchronously iterate through documents without blocking execution. This allows for processing documents as they become available and applying conditional logic.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/couchbase.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\n// using lazy_load\nfor await (const doc of this.lazyLoad()) {\n  console.log(doc);\n  break; // break based on required condition\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a New Index from Texts with CloseVector\nDESCRIPTION: TypeScript code demonstrating how to create a new vector index from texts using CloseVector. This snippet is referenced but not directly included in the content.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/closevector.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n// Code content is dynamically imported and not directly available in the provided text.\n```\n\n----------------------------------------\n\nTITLE: Running tests for @langchain/ollama\nDESCRIPTION: Commands to run unit tests and integration tests for the package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-ollama/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn test\n$ yarn test:int\n```\n\n----------------------------------------\n\nTITLE: Accumulating Tool Call Chunks in LangChain.js\nDESCRIPTION: This snippet shows how to accumulate tool call chunks from a stream. It uses the concat utility from @langchain/core/utils/stream to merge chunks and logs the accumulated tool call chunks.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_streaming.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { concat } from \"@langchain/core/utils/stream\";\n\nconst stream = await modelWithTools.stream(query);\n\nlet gathered = undefined;\n\nfor await (const chunk of stream) {\n  gathered = gathered !== undefined ? concat(gathered, chunk) : chunk;\n  console.log(gathered.tool_call_chunks);\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Cassandra Driver and LangChain Packages via npm/yarn\nDESCRIPTION: Installs the necessary Node.js packages for using the Cassandra vector store with LangChain. This includes the `cassandra-driver` for database interaction, and core LangChain community, OpenAI, and core packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/cassandra.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install cassandra-driver @langchain/community @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Indexing and Retrieving with Vector Store\nDESCRIPTION: Demonstration of using embeddings with MemoryVectorStore for document indexing and retrieval\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/text_embedding.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n// Create a vector store with a sample text\nimport { MemoryVectorStore } from \"langchain/vectorstores/memory\";\n\nconst text = \"LangChain is the framework for building context-aware reasoning applications\";\n\nconst vectorstore = await MemoryVectorStore.fromDocuments(\n  [{ pageContent: text, metadata: {} }],\n  embeddings,\n);\n\n// Use the vector store as a retriever that returns a single document\nconst retriever = vectorstore.asRetriever(1);\n\n// Retrieve the most similar text\nconst retrievedDocuments = await retriever.invoke(\"What is LangChain?\");\n\nretrievedDocuments[0].pageContent;\n```\n\n----------------------------------------\n\nTITLE: Installing @langchain/exa and @langchain/core packages\nDESCRIPTION: Command to install the required packages using npm or yarn. This step is necessary before using the exa integration in a project.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-exa/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/exa @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for IBM watsonx.ai Software Authentication in Bash\nDESCRIPTION: Sets up environment variables for IBM watsonx.ai software authentication.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_compressors/ibm.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport WATSONX_AI_AUTH_TYPE=cp4d\nexport WATSONX_AI_USERNAME=<YOUR_USERNAME>\nexport WATSONX_AI_PASSWORD=<YOUR_PASSWORD>\nexport WATSONX_AI_URL=<URL>\n```\n\n----------------------------------------\n\nTITLE: Building Redis Package from Repository Root\nDESCRIPTION: Command to build the @langchain/redis package from the repository root using yarn with filtering.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-redis/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nyarn build --filter=@langchain/redis\n```\n\n----------------------------------------\n\nTITLE: Setting up Environment Variables for Astra DB Integration\nDESCRIPTION: Sets up the necessary environment variables for connecting to Astra DB, including application token, endpoint, collection name, and OpenAI API key for embeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/astradb.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport ASTRA_DB_APPLICATION_TOKEN=YOUR_ASTRA_DB_APPLICATION_TOKEN_HERE\nexport ASTRA_DB_ENDPOINT=YOUR_ASTRA_DB_ENDPOINT_HERE\nexport ASTRA_DB_COLLECTION=YOUR_ASTRA_DB_COLLECTION_HERE\nexport OPENAI_API_KEY=YOUR_OPENAI_API_KEY_HERE\n```\n\n----------------------------------------\n\nTITLE: Server-Side HTTP Response Handler for Event Streaming\nDESCRIPTION: This code shows how to create a server-side handler that streams events directly into an HTTP response with the correct headers. It's designed to work with frameworks like Hono and Next.js.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming.ipynb#2025-04-22_snippet_20\n\nLANGUAGE: typescript\nCODE:\n```\nconst handler = async () => {\n  const eventStream = await chain.streamEvents(\n    `Output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key \"name\" and \"population\"`,\n    {\n      version: \"v2\",\n      encoding: \"text/event-stream\",\n    },\n  );\n  return new Response(eventStream, {\n    headers: {\n      \"content-type\": \"text/event-stream\",\n    }\n  });\n};\n```\n\n----------------------------------------\n\nTITLE: Implementing ConsoleCallbackHandler with ChatAnthropic Model\nDESCRIPTION: Example showing how to initialize and use ConsoleCallbackHandler with ChatPromptTemplate and ChatAnthropic model. The callback handler is passed directly to the model constructor to track only model-specific events.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/callbacks_constructor.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport { ConsoleCallbackHandler } from \"@langchain/core/tracers/console\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst handler = new ConsoleCallbackHandler();\n\nconst prompt = ChatPromptTemplate.fromTemplate(`What is 1 + {number}?`);\nconst model = new ChatAnthropic({\n  model: \"claude-3-sonnet-20240229\",\n  callbacks: [handler],\n});\n\nconst chain = prompt.pipe(model);\n\nawait chain.invoke({ number: \"2\" });\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Retrieval Chain with Query Analysis in JavaScript\nDESCRIPTION: This snippet demonstrates how to create a custom chain that uses query analysis to decide whether to retrieve information or respond directly.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_no_queries.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: javascript\nCODE:\n```\nimport { JsonOutputKeyToolsParser } from \"@langchain/core/output_parsers/openai_tools\";\n\nconst outputParser = new JsonOutputKeyToolsParser({\n  keyName: \"search\",\n})\n\nimport { RunnableConfig, RunnableLambda } from \"@langchain/core/runnables\";\n\nconst chain = async (question: string, config?: RunnableConfig) => {\n  const response = await queryAnalyzer.invoke(question, config);\n  if (\"tool_calls\" in response.additional_kwargs && response.additional_kwargs.tool_calls !== undefined) {\n    const query = await outputParser.invoke(response, config);\n    return retriever.invoke(query[0].query, config);\n  } else {\n    return response;\n  }\n}\n\nconst customChain = new RunnableLambda({ func: chain });\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for LangChain.js Embeddings\nDESCRIPTION: Command to install the necessary packages for working with OpenAI embeddings in LangChain.js.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/caching_embeddings.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Accessing Tool Properties in Python\nDESCRIPTION: These snippets show how to access various properties of a tool, including its name, description, and input schema.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tools_builtin.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntool.name;\n```\n\nLANGUAGE: python\nCODE:\n```\ntool.description;\n```\n\nLANGUAGE: python\nCODE:\n```\nimport { zodToJsonSchema } from \"zod-to-json-schema\";\n\nzodToJsonSchema(tool.schema);\n```\n\nLANGUAGE: python\nCODE:\n```\ntool.returnDirect;\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for FireCrawl\nDESCRIPTION: Instructions for setting up required environment variables including the FireCrawl API key and optional LangSmith tracing configuration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/firecrawl.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport FIRECRAWL_API_KEY=\"your-api-key\"\n```\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Creating LLMGraphTransformer with ChatOpenAI in JavaScript\nDESCRIPTION: This snippet demonstrates how to set up an LLMGraphTransformer using ChatOpenAI for converting text to graph structures.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/graph_constructing.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { LLMGraphTransformer } from \"@langchain/community/experimental/graph_transformers/llm\";\n\nconst model = new ChatOpenAI({\n    temperature: 0,\n    model: \"gpt-4o-mini\",\n});\n\nconst llmGraphTransformer = new LLMGraphTransformer({\n    llm: model\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Google Cloud Storage Dependency (Node.js)\nDESCRIPTION: Installs the `@google-cloud/storage` npm package. This library is required when using `GoogleCloudStorageDocstore` to store document contents, as the Matching Engine itself only stores embeddings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/googlevertexai.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @google-cloud/storage\n```\n\n----------------------------------------\n\nTITLE: Configuring PromptLayerOpenAI with Azure-hosted OpenAI Instances in TypeScript\nDESCRIPTION: This code shows how to set up PromptLayerOpenAI with Azure-hosted OpenAI instances. It requires Azure OpenAI configuration details including API key, instance name, deployment names, and a PromptLayer API key. The Azure configuration can be provided via constructor arguments or environment variables.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/prompt_layer_openai.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PromptLayerOpenAI } from \"langchain/llms/openai\";\n\nconst model = new PromptLayerOpenAI({\n  temperature: 0.9,\n  azureOpenAIApiKey: \"YOUR-AOAI-API-KEY\", // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY\n  azureOpenAIApiInstanceName: \"YOUR-AOAI-INSTANCE-NAME\", // In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME\n  azureOpenAIApiDeploymentName: \"YOUR-AOAI-DEPLOYMENT-NAME\", // In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME\n  azureOpenAIApiCompletionsDeploymentName:\n    \"YOUR-AOAI-COMPLETIONS-DEPLOYMENT-NAME\", // In Node.js defaults to process.env.AZURE_OPENAI_API_COMPLETIONS_DEPLOYMENT_NAME\n  azureOpenAIApiEmbeddingsDeploymentName:\n    \"YOUR-AOAI-EMBEDDINGS-DEPLOYMENT-NAME\", // In Node.js defaults to process.env.AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME\n  azureOpenAIApiVersion: \"YOUR-AOAI-API-VERSION\", // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION\n  azureOpenAIBasePath: \"YOUR-AZURE-OPENAI-BASE-PATH\", // In Node.js defaults to process.env.AZURE_OPENAI_BASE_PATH\n  promptLayerApiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.PROMPTLAYER_API_KEY\n});\nconst res = await model.invoke(\n  \"What would be a good company name a company that makes colorful socks?\"\n);\n```\n\n----------------------------------------\n\nTITLE: Initializing Neo4j Connection Variables\nDESCRIPTION: Setting up connection variables for Neo4j database using environment variables.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/graph_prompting.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst url = process.env.NEO4J_URI;\nconst username = process.env.NEO4J_USER;\nconst password = process.env.NEO4J_PASSWORD;\n```\n\n----------------------------------------\n\nTITLE: Setting Context Variables for Tool Invocation in TypeScript\nDESCRIPTION: Creates a runnable lambda that sets context variables before invoking LLM tools. This approach allows runtime values like userId to be accessible within tools without being exposed to the LLM.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_runtime.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { setContextVariable } from \"@langchain/core/context\";\nimport { BaseChatModel } from \"@langchain/core/language_models/chat_models\";\nimport { RunnableLambda } from \"@langchain/core/runnables\";\n\nconst handleRunTimeRequestRunnable = RunnableLambda.from(async (params: {\n  userId: string;\n  query: string;\n  llm: BaseChatModel;\n}) => {\n  const { userId, query, llm } = params;\n  if (!llm.bindTools) {\n    throw new Error(\"Language model does not support tools.\");\n  }\n  // Set a context variable accessible to any child runnables called within this one.\n  // You can also set context variables at top level that act as globals.\n  setContextVariable(\"userId\", userId);\n  const tools = [updateFavoritePets];\n  const llmWithTools = llm.bindTools(tools);\n  const modelResponse = await llmWithTools.invoke(query);\n  // For simplicity, skip checking the tool call's name field and assume\n  // that the model is calling the \"updateFavoritePets\" tool\n  if (modelResponse.tool_calls.length > 0) {\n    return updateFavoritePets.invoke(modelResponse.tool_calls[0]);\n  } else {\n    return \"No tool invoked.\";\n  }\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Cloudflare Workers AI Integration for LangChain\nDESCRIPTION: Command to install the required packages for using ChatCloudflareWorkersAI with LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/cloudflare_workersai.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n@langchain/cloudflare @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith Tracing Configuration\nDESCRIPTION: Optional environment variables for enabling LangSmith tracing and authentication\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/google_vertex_ai.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Using a Tool Directly in TypeScript\nDESCRIPTION: Shows how to invoke a previously defined tool with specific arguments. This simple example demonstrates calling the multiply tool with two numbers.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/tools.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nawait multiply.invoke({ a: 2, b: 3 });\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Community Package for SerpAPI Integration\nDESCRIPTION: Command to install the required packages (@langchain/community and @langchain/core) for using SerpAPI in LangChain.js applications.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/serpapi.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n@langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Checking LangChain.js Code Formatting\nDESCRIPTION: This command checks for formatting differences without fixing them, using Prettier.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/CONTRIBUTING.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nyarn format:check\n```\n\n----------------------------------------\n\nTITLE: Installing d3-dsv package for CSV parsing in Node.js\nDESCRIPTION: Command to install the d3-dsv package version 2, which is required for CSV parsing in this context.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/document_loader_csv.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install d3-dsv@2\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for LangChain Filter Construction\nDESCRIPTION: This snippet shows how to install the required dependencies for constructing filters in LangChain using npm or yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_constructing_filters.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n@langchain/core zod\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key for Use with RedisVectorStore\nDESCRIPTION: When using OpenAI embeddings, the OpenAI API key must be configured in your environment variables. This ensures that the embeddings are correctly initialized and used for vector computation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/redis.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.OPENAI_API_KEY = \"YOUR_API_KEY\";\n```\n\n----------------------------------------\n\nTITLE: Configure LangSmith Tracing in LangChain.js\nDESCRIPTION: Sets environment variables required for LangSmith API integration, enabling tracing for individual tools. Dependencies include the LangSmith API. Ensure valid API keys are provided.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/openapi.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.LANGSMITH_TRACING=\"true\"\nprocess.env.LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Package Dependencies Resolution\nDESCRIPTION: Package.json configuration to ensure consistent @langchain/core dependency version across different package managers.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/create-langchain-integration/template/README.md#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"your-project\",\n  \"version\": \"0.0.0\",\n  \"dependencies\": {\n    \"@langchain/<ADD_PACKAGE_NAME_HERE>\": \"^0.0.0\",\n    \"@langchain/core\": \"^0.3.0\"\n  },\n  \"resolutions\": {\n    \"@langchain/core\": \"^0.3.0\"\n  },\n  \"overrides\": {\n    \"@langchain/core\": \"^0.3.0\"\n  },\n  \"pnpm\": {\n    \"overrides\": {\n      \"@langchain/core\": \"^0.3.0\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating XML Parsing Chain\nDESCRIPTION: Demonstrates how to create a chain that combines prompt template, model, and XML parser to generate structured output.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/output_parser_xml.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\nconst prompt = ChatPromptTemplate.fromTemplate(`{query}\\n{format_instructions}`);\nconst partialedPrompt = await prompt.partial({\n  format_instructions: parser.getFormatInstructions(),\n});\n\nconst chain = partialedPrompt.pipe(model).pipe(parser);\n\nconst output = await chain.invoke({\n  query: \"Generate the shortened filmograph for Tom Hanks.\",\n});\n\nconsole.log(JSON.stringify(output, null, 2));\n```\n\n----------------------------------------\n\nTITLE: LangChain Message Constructor Configuration\nDESCRIPTION: Constructor configuration for ToolMessage in LangChain core, including tool call ID, content, and additional kwargs.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_27\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"lc\": 1,\n  \"type\": \"constructor\",\n  \"id\": [\n    \"langchain_core\",\n    \"messages\",\n    \"ToolMessage\"\n  ],\n  \"kwargs\": {\n    \"tool_call_id\": \"toolu_01NVTbm5aNYSm1wGYb6XF7jE\",\n    \"content\": \"18980\",\n    \"additional_kwargs\": {\n      \"name\": \"calculator\"\n    },\n    \"response_metadata\": {}\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for PostgreSQL Configuration\nDESCRIPTION: Bash commands to set necessary environment variables for connecting to a Google Cloud SQL PostgreSQL instance. These variables are used when configuring the PostgresEngine instance.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/google_cloudsql_pg.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nPROJECT_ID=\"your-project-id\"\nREGION=\"your-project-region\" // example: \"us-central1\"\nINSTANCE_NAME=\"your-instance\"\nDB_NAME=\"your-database-name\"\nDB_USER=\"your-database-user\"\nPASSWORD=\"your-database-password\"\n```\n\n----------------------------------------\n\nTITLE: Splitting Text Using TokenTextSplitter with js-tiktoken in Python\nDESCRIPTION: This code demonstrates how to use the TokenTextSplitter from the @langchain/textsplitters package to split a document into chunks based on token count. It uses js-tiktoken for tokenization, which is tuned for OpenAI models. The example reads a file, splits its content, and prints the first chunk.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/split_by_token.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport { TokenTextSplitter } from \"@langchain/textsplitters\";\nimport * as fs from \"node:fs\";\n\n// Load an example document\nconst rawData = await fs.readFileSync(\"../../../../examples/state_of_the_union.txt\");\nconst stateOfTheUnion = rawData.toString();\n\nconst textSplitter = new TokenTextSplitter({\n  chunkSize: 10,\n  chunkOverlap: 0,\n});\n\nconst texts = await textSplitter.splitText(stateOfTheUnion);\n\nconsole.log(texts[0]);\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Dependencies\nDESCRIPTION: Command to install the required LangChain packages (@langchain/community and @langchain/core) using npm or yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/document_loader_markdown.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n@langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for LangChain and AI SDK\nDESCRIPTION: Command to install necessary packages for LangChain and AI SDK integration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/stream_tool_client.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core ai zod zod-to-json-schema\n```\n\n----------------------------------------\n\nTITLE: Implementing Client-Side UI for LLM-Generated Content in React\nDESCRIPTION: This client-side React component demonstrates how to use the exposed agent endpoint to fetch and display LLM-generated UI elements. It includes a button to trigger image search for cats.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/generative_ui.mdx#2025-04-22_snippet_2\n\nLANGUAGE: tsx\nCODE:\n```\n\"use client\";\nimport type { EndpointsContext } from \"./agent\";\n\nexport default function Page() {\n  const actions = useActions<typeof EndpointsContext>();\n  const [node, setNode] = useState();\n\n  return (\n    <div>\n      {node}\n\n      <button\n        onClick={async () => {\n          setNode(await actions.agent({ input: \"cats\" }));\n        }}\n      >\n        Get images of cats\n      </button>\n    </div>\n  );\n}\n```\n\n----------------------------------------\n\nTITLE: Loading Documents from Couchbase Using the Blocking Method in LangChain.js\nDESCRIPTION: Using the 'load' method to synchronously retrieve all documents matching the specified query from Couchbase. This method blocks execution until all documents are retrieved.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/couchbase.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\n// using load method\ndocs = await loader.load();\nconsole.log(docs);\n```\n\n----------------------------------------\n\nTITLE: Initializing Chat Model with Anthropic\nDESCRIPTION: Sets up a ChatAnthropic model instance with specific configuration parameters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_stream_events.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nconst model = new ChatAnthropic({\n  model: \"claude-3-5-sonnet-20240620\",\n  temperature: 0,\n});\n```\n\n----------------------------------------\n\nTITLE: Running Tests\nDESCRIPTION: Commands for running unit and integration tests.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/create-langchain-integration/template/README.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn test\n$ yarn test:int\n```\n\n----------------------------------------\n\nTITLE: Creating a React Agent with LangGraph in Typescript\nDESCRIPTION: Demonstrates creating a React agent using LangGraph's `createReactAgent` function. It requires an initialized language model (`llm`) and a list of tools (`tools`) obtained from the WatsonxToolkit.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/ibm.mdx#2025-04-22_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n\nconst agent = createReactAgent({ llm, tools });\n```\n\n----------------------------------------\n\nTITLE: Importing Components for Retriever Documentation in Markdown\nDESCRIPTION: This code snippet imports custom React components used for displaying feature tables in the documentation. These components are likely used to generate structured lists or tables of retriever features and capabilities.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/index.mdx#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nimport { CategoryTable, IndexTable } from \"@theme/FeatureTables\";\n```\n\n----------------------------------------\n\nTITLE: List Available Tools in the Toolkit\nDESCRIPTION: Retrieves and logs all available tools in the OpenApiToolkit instance, displaying their names and descriptions. Assumes prior toolkit initialization with necessary models and specifications.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/openapi.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconst tools = toolkit.getTools();\n\nconsole.log(tools.map((tool) => ({\n  name: tool.name,\n  description: tool.description,\n})))\n```\n\n----------------------------------------\n\nTITLE: Displaying Updated User Data in TypeScript\nDESCRIPTION: Logs the userToPets object to verify that the tool successfully updated the data with the specified userId. This demonstrates that the context variable was properly passed to the tool.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_runtime.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconsole.log(userToPets);\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain via Package Managers\nDESCRIPTION: Commands to install LangChain using npm, yarn, or pnpm package managers.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/tests/__mdx__/modules/index.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -S langchain\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn add langchain\n```\n\nLANGUAGE: bash\nCODE:\n```\npnpm add langchain\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Extraction with Anthropic\nDESCRIPTION: Commands to install the necessary packages for working with Anthropic models and extraction in LangChain.js, including the Anthropic integration, core packages, and Zod for schema validation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/extraction_parse.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n@langchain/anthropic @langchain/core zod zod-to-json-schema\n```\n\n----------------------------------------\n\nTITLE: OpenAI API Compatibility Setup\nDESCRIPTION: Example demonstrating how to use OpenAI API key with Azure OpenAI integration\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-azure-openai/README.md#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AzureOpenAI, OpenAIKeyCredential } from \"@langchain/azure-openai\";\n\nconst model = new AzureOpenAI({\n  modelName: \"gpt-3.5-turbo\",\n  credentials: new OpenAIKeyCredential(\"<your_openai_api_key>\")\n});\n```\n\n----------------------------------------\n\nTITLE: Importing Azure Cosmos DB for MongoDB vCore Vector Store\nDESCRIPTION: Code snippet for importing the Azure Cosmos DB for MongoDB vCore vector store class, which enables vector search capabilities with MongoDB compatibility.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/microsoft.mdx#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AzureCosmosDBMongoDBVectorStore } from \"@langchain/azure-cosmosdb\";\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain and Dependencies\nDESCRIPTION: Command to install LangChain and the cheerio library for web loading.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/agent_executor.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nlangchain @langchain/core cheerio\n```\n\n----------------------------------------\n\nTITLE: Producing Structured Outputs with Chat Models in LangChain\nDESCRIPTION: Shows how to use LangChain's chat model interface to produce structured outputs by defining a schema and binding it to the model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/why_langchain.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// Define tool as a Zod schema\nconst schema = z.object({ ... });\n// Bind schema to model\nconst modelWithStructure = model.withStructuredOutput(schema)\n```\n\n----------------------------------------\n\nTITLE: Defining Person Schema with Zod\nDESCRIPTION: Creates a schema for person information using Zod, including optional fields for name, hair color, and height.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/extraction_examples.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { z } from \"zod\";\n\nconst personSchema = z.object({\n  name: z.optional(z.string()).describe(\"The name of the person\"),\n  hair_color: z.optional(z.string()).describe(\"The color of the person's hair, if known\"),\n  height_in_meters: z.optional(z.string()).describe(\"Height measured in meters\")\n}).describe(\"Information about a person.\");\n\nconst peopleSchema = z.object({\n  people: z.array(personSchema)\n});\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Community and Core Packages\nDESCRIPTION: This command installs the necessary LangChain packages for using Datadog LLM Observability.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/callbacks/datadog_tracer.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Azure AI Search and LangChain SDKs using npm in bash\nDESCRIPTION: This bash command installs the required dependencies for leveraging Azure AI Search as a vector store with LangChain in a TypeScript or JavaScript project. It pulls in @langchain/community, @langchain/core, and @azure/search-documents using npm. Ensure you run this in your project directory to make the packages available via your local node_modules for import in scripts.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/azure_aisearch.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -S @langchain/community @langchain/core @azure/search-documents\n```\n\n----------------------------------------\n\nTITLE: Creating a Simple Input Tool with DynamicTool in TypeScript\nDESCRIPTION: Shows how to use the DynamicTool class for older agents that require tools accepting only a single input. This approach doesn't require a schema definition.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/custom_tools.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { DynamicTool } from \"@langchain/core/tools\";\n\nconst searchTool = new DynamicTool({\n  name: \"search\",\n  description: \"look things up online\",\n  func: async (_input: string) => {\n    return \"LangChain\";\n  },\n});\n\nawait searchTool.invoke(\"foo\");\n```\n\n----------------------------------------\n\nTITLE: Console Output for Verbose LangChain.js Tool\nDESCRIPTION: This snippet shows the console output generated when using a verbose LangChain.js Tool. It displays the start and end of the Tool run, including input, output, and execution time.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_43\n\nLANGUAGE: bash\nCODE:\n```\n[tool/start] [1:tool:TavilySearchResults] Entering Tool run with input: \"Oppenheimer 2023 film director age\"\n[tool/end] [1:tool:TavilySearchResults] [1.95s] Exiting Tool run with output: \"[{\"title\":\"'Oppenheimer' Review: A Man for Our Time - The New York Times\",\"url\":\"https://www.nytimes.com/2023/07/19/movies/oppenheimer-review-christopher-nolan.html\",\"content\":\"Instead, it is here that the film's complexities and all its many fragments finally converge as Nolan puts the finishing touches on his portrait of a man who contributed to an age of transformational scientific discovery, who personified the intersection of science and politics, including in his role as a Communist boogeyman, who was transformed by his role in the creation of weapons of mass destruction and soon after raised the alarm about the dangers of nuclear war.\\n He served as director of a clandestine weapons lab built in a near-desolate stretch of Los Alamos, in New Mexico, where he and many other of the era's most dazzling scientific minds puzzled through how to harness nuclear reactions for the weapons that killed tens of thousands instantly, ending the war in the Pacific.\\n Nolan integrates these black-and-white sections with the color ones, using scenes from the hearing and the confirmation — Strauss's role in the hearing and his relationship with Oppenheimer directly affected the confirmation's outcome — to create a dialectical synthesis. To signal his conceit, he stamps the film with the words \\\"fission\\\" (a splitting into parts) and \\\"fusion\\\" (a merging of elements); Nolan being Nolan, he further complicates the film by recurrently kinking up the overarching chronology — it is a lot.\\n It's also at Berkeley that Oppenheimer meets the project's military head, Leslie Groves (a predictably good Damon), who makes him Los Alamos's director, despite the leftist causes he supported — among them, the fight against fascism during the Spanish Civil War — and some of his associations, including with Communist Party members like his brother, Frank (Dylan Arnold).\\n\",\"score\":0.97519,\"raw_content\":null},{\"title\":\"Oppenheimer's Grandson Reacts to New Christopher Nolan Film | TIME\",\"url\":\"https://time.com/6297743/oppenheimer-grandson-movie-interview/\",\"content\":\"July 25, 2023 3:32 PM EDT. M oviegoers turned out in droves this weekend for writer-director Christopher Nolan's new film Oppenheimer, fueling an expectations-shattering domestic box office debut ...\",\"score\":0.95166,\"raw_content\":null},{\"title\":\"Oppenheimer (2023) - IMDb\",\"url\":\"https://www.imdb.com/title/tt15398776/\",\"content\":\"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.\",\"score\":0.95127,\"raw_content\":null},{\"title\":\"Oppenheimer (film) - Wikipedia\",\"url\":\"https://en.wikipedia.org/wiki/Oppenheimer_(film)\",\"content\":\"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\\nCritical response\\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \\\"more objective view of his story from a different character's point of view\\\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \\\"big-atures\\\", since the special effects team had tried to build the models as physically large as possible. He felt that \\\"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\\\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \\\"emotional\\\" and resembling that of a thriller, while also remarking that Nolan had \\\"Trojan-Horsed a biopic into a thriller\\\".[72]\\nCasting\\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\\\", while also underscoring that it is a \\\"huge shift in perception about the reality of Oppenheimer's perception\\\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \\\"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\\\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.\",\"score\":0.92204,\"raw_content\":null},{\"title\":\"Oppenheimer (2023) - Full Cast & Crew - IMDb\",\"url\":\"https://www.imdb.com/title/tt15398776/fullcredits/\",\"content\":\"Oppenheimer (2023) cast and crew credits, including actors, actresses, directors, writers and more. Menu. Movies. Release Calendar Top 250 Movies Most Popular Movies Browse Movies by Genre Top Box Office Showtimes & Tickets Movie News India Movie Spotlight. ... Peter Oppenheimer - Age 8 (uncredited) Adam Walker Federman ... MIT Student ...\",\"score\":0.92179,\"raw_content\":null}]\"\n[tool/start] [1:tool:TavilySearchResults] Entering Tool run with input: \"Christopher Nolan age\"\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Dependencies\nDESCRIPTION: This snippet shows how to install the necessary LangChain packages using npm or yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/sitemap.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Using Writer LLM in LangChain.js\nDESCRIPTION: This code block represents an example of how to use the Writer LLM in a LangChain.js project. The actual code is not provided in the input, but it's referenced as an imported example.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/writer.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n// WriterExample content would be here\n```\n\n----------------------------------------\n\nTITLE: Basic Azure Chat OpenAI Usage\nDESCRIPTION: Example showing how to initialize and use the AzureChatOpenAI model for basic chat interactions\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-azure-openai/README.md#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AzureChatOpenAI } from \"@langchain/azure-openai\";\n\nconst model = new AzureChatOpenAI({\n  // Note that the following are optional, and will default to the values below\n  // if not provided.\n  azureOpenAIEndpoint: process.env.AZURE_OPENAI_API_ENDPOINT,\n  azureOpenAIApiKey: process.env.AZURE_OPENAI_API_KEY,\n  azureOpenAIApiDeploymentName: process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME,\n});\nconst response = await model.invoke(new HumanMessage(\"Hello world!\"));\n```\n\n----------------------------------------\n\nTITLE: LangSmith Tracing Configuration\nDESCRIPTION: Optional configuration for enabling automated tracing using LangSmith API.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/pinecone.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// process.env.LANGSMITH_API_KEY = \"<YOUR API KEY HERE>\";\n// process.env.LANGSMITH_TRACING = \"true\";\n```\n\n----------------------------------------\n\nTITLE: Installing SRT Parser Dependency for LangChain.js\nDESCRIPTION: This command installs the 'srt-parser-2' package, which is required for parsing SRT subtitle files in the LangChain.js project.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/subtitles.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install srt-parser-2\n```\n\n----------------------------------------\n\nTITLE: Instantiating SerpAPI Tool in LangChain.js\nDESCRIPTION: Code to import and create an instance of the SerpAPI tool from the LangChain community package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/serpapi.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { SerpAPI } from \"@langchain/community/tools/serpapi\";\n\nconst tool = new SerpAPI();\n```\n\n----------------------------------------\n\nTITLE: Importing MixedbreadAIReranker Class\nDESCRIPTION: Code snippet showing how to import the MixedbreadAIReranker class from the package to access reranking capabilities.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_compressors/mixedbread_ai.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { MixedbreadAIReranker } from \"@langchain/mixedbread-ai\";\n```\n\n----------------------------------------\n\nTITLE: Running Tests and Code Quality Checks\nDESCRIPTION: Commands for running unit tests, integration tests, linting, and formatting\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-mongodb/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn test\n$ yarn test:int\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn lint && yarn format\n```\n\n----------------------------------------\n\nTITLE: Configuring Package Resolution for pnpm\nDESCRIPTION: Example package.json configuration for pnpm to ensure consistent @langchain/core version across dependencies.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/tests/__mdx__/modules/index.mdx#2025-04-22_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"your-project\",\n  \"version\": \"0.0.0\",\n  \"private\": true,\n  \"engines\": {\n    \"node\": \">=18\"\n  },\n  \"dependencies\": {\n    \"@langchain/google-genai\": \"^0.0.2\",\n    \"@langchain/core\": \"^0.3.0\"\n  },\n  \"pnpm\": {\n    \"overrides\": {\n      \"@langchain/core\": \"0.3.0\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for SearchApi Tool in LangChain.js\nDESCRIPTION: Commands to install the necessary LangChain dependencies for using the SearchApi tool with npm or yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/searchapi.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Using Custom PDF.js Build in LangChain.js PDFLoader\nDESCRIPTION: This snippet demonstrates how to use a custom PDF.js build (legacy version) with PDFLoader for enhanced compatibility.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/document_loader_pdf.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PDFLoader } from \"@langchain/community/document_loaders/fs/pdf\";\n\nconst loader = new PDFLoader(\"src/document_loaders/example_data/example.pdf\", {\n  // you may need to add `.then(m => m.default)` to the end of the import\n  pdfjs: () => import(\"pdfjs-dist/legacy/build/pdf.js\"),\n});\n```\n\n----------------------------------------\n\nTITLE: Using ChatPromptTemplate in TypeScript\nDESCRIPTION: Shows how to create a chat prompt template with system and user messages, formatting variables within message content.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/prompt_templates.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\n\nconst promptTemplate = ChatPromptTemplate.fromMessages([\n  [\"system\", \"You are a helpful assistant\"],\n  [\"user\", \"Tell me a joke about {topic}\"],\n]);\n\nawait promptTemplate.invoke({ topic: \"cats\" });\n```\n\n----------------------------------------\n\nTITLE: Sample CSV data structure\nDESCRIPTION: Example of a CSV file structure with id and text columns, used for demonstration in the loading examples.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/document_loader_csv.mdx#2025-04-22_snippet_1\n\nLANGUAGE: csv\nCODE:\n```\nid,text\n1,This is a sentence.\n2,This is another sentence.\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for HTML Loading in LangChain\nDESCRIPTION: This snippet shows the installation command for the required packages to load HTML documents in LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/document_loader_html.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n@langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Error Handling for Missing API Token\nDESCRIPTION: Shows how to handle errors when the API token is not provided or found in environment variables.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/deepinfra.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\ntry {\n  const embeddings = new DeepInfraEmbeddings();\n} catch (error) {\n  console.error(\"DeepInfra API token not found\");\n}\n```\n\n----------------------------------------\n\nTITLE: Using withStructuredOutput Helper Method\nDESCRIPTION: Demonstrates the use of the withStructuredOutput helper function for streamlined structured output handling.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/structured_outputs.mdx#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n// Bind the schema to the model\nconst modelWithStructure = model.withStructuredOutput(ResponseFormatter);\n// Invoke the model\nconst structuredOutput = await modelWithStructure.invoke(\n  \"What is the powerhouse of the cell?\"\n);\n// Get back the object\nconsole.log(structuredOutput);\n```\n\n----------------------------------------\n\nTITLE: Initializing Wikipedia Query Tool in TypeScript\nDESCRIPTION: This snippet demonstrates how to initialize a Wikipedia query tool with specific parameters for top results and maximum content length.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tools_builtin.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { WikipediaQueryRun } from \"@langchain/community/tools/wikipedia_query_run\";\n\nconst tool = new WikipediaQueryRun({\n  topKResults: 1,\n  maxDocContentLength: 100,\n});\n```\n\n----------------------------------------\n\nTITLE: Loading Documents with CSVLoader in TypeScript\nDESCRIPTION: This snippet demonstrates how to use the CSVLoader from LangChain to load documents from a CSV file. It shows the basic structure of using a document loader, including instantiation and calling the load method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/document_loaders.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { CSVLoader } from \"@langchain/community/document_loaders/fs/csv\";\n\nconst loader = new CSVLoader(\n  ...  // <-- Integration specific parameters here\n);\nconst data = await loader.load();\n```\n\n----------------------------------------\n\nTITLE: Generating Text Embeddings with MinimaxEmbeddings in TypeScript\nDESCRIPTION: This code snippet demonstrates how to use the MinimaxEmbeddings class to generate embeddings for both single queries and multiple documents. It imports the class, creates an instance, and shows how to embed a query and a list of documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/minimax.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { MinimaxEmbeddings } from \"langchain/embeddings/minimax\";\n\nexport const run = async () => {\n  /* Embed queries */\n  const embeddings = new MinimaxEmbeddings();\n  const res = await embeddings.embedQuery(\"Hello world\");\n  console.log(res);\n  /* Embed documents */\n  const documentRes = await embeddings.embedDocuments([\n    \"Hello world\",\n    \"Bye bye\",\n  ]);\n  console.log({ documentRes });\n};\n```\n\n----------------------------------------\n\nTITLE: Message Trimming Implementation\nDESCRIPTION: Shows how to implement message trimming to manage conversation history and prevent context window overflow.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/chatbot.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport { SystemMessage, HumanMessage, AIMessage, trimMessages } from \"@langchain/core/messages\"\n\nconst trimmer = trimMessages({\n  maxTokens: 10,\n  strategy: \"last\",\n  tokenCounter: (msgs) => msgs.length,\n  includeSystem: true,\n  allowPartial: false,\n  startOn: \"human\",\n})\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for IBM watsonx.ai Software Authentication\nDESCRIPTION: Sets up environment variables for IBM watsonx.ai software authentication. This includes setting the authentication type, username, password, and URL.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/ibm.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport WATSONX_AI_AUTH_TYPE=cp4d\nexport WATSONX_AI_USERNAME=<YOUR_USERNAME>\nexport WATSONX_AI_PASSWORD=<YOUR_PASSWORD>\nexport WATSONX_AI_URL=<URL>\n```\n\n----------------------------------------\n\nTITLE: Installing Couchbase and LangChain Dependencies\nDESCRIPTION: Command to install the required packages for using Couchbase with LangChain, including the Couchbase client, OpenAI integration, and core LangChain components.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/couchbase.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install couchbase @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Streaming with the Custom Chat Model\nDESCRIPTION: Shows how to use the streaming capability of the custom chat model. Each character from the truncated input is streamed one at a time.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/custom_chat.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nconst stream = await chatModel.stream([[\"human\", \"I am an LLM\"]]);\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Dependencies for Moonshot Integration\nDESCRIPTION: Command to install the required npm packages for using Moonshot AI models with LangChain.js. This installs the community package and core package needed for the integration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/moonshot.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing TypeORM and PostgreSQL packages using npm\nDESCRIPTION: Commands to install the 'typeorm' and 'pg' packages that are prerequisites for integrating TypeORM with PostgreSQL in a LangChain.js project.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/typeorm.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install typeorm\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install pg\n```\n\n----------------------------------------\n\nTITLE: Installing Momento SDK\nDESCRIPTION: Installation commands for Momento SDK packages\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/llm_caching.mdx#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @gomomento/sdk\n```\n\n----------------------------------------\n\nTITLE: Setting Optional LangSmith API Key for Tracing\nDESCRIPTION: Sets environment variables for LangSmith tracing of model calls.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/cohere.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Installing MixedbreadAI Dependencies\nDESCRIPTION: Command to install the required packages for using MixedbreadAI embeddings with LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/mixedbread_ai.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/mixedbread-ai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Headers to ChatOpenAI Requests\nDESCRIPTION: Shows how to add custom HTTP headers to requests made by the ChatOpenAI model using the configuration parameter.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst llmWithCustomHeaders = new ChatOpenAI({\n  temperature: 0.9,\n  configuration: {\n    defaultHeaders: {\n      \"Authorization\": `Bearer SOME_CUSTOM_VALUE`,\n    },\n  },\n});\n\nawait llmWithCustomHeaders.invoke(\"Hi there!\");\n```\n\n----------------------------------------\n\nTITLE: Clearing Messages from PostgreSQL Chat History\nDESCRIPTION: Method to remove all messages from the chat history in the PostgreSQL database using the clear() function of the PostgresChatMessageHistory class.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/google_cloudsql_pg.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nawait historyInstance.clear();\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Index from Local File with CloseVector\nDESCRIPTION: TypeScript code showing how to save a vector index to a local file and load it again using CloseVector. This snippet is referenced but not directly included in the content.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/closevector.mdx#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n// Code content is dynamically imported and not directly available in the provided text.\n```\n\n----------------------------------------\n\nTITLE: Installing Zep Cloud Dependencies\nDESCRIPTION: Command to install required npm packages for using Zep Cloud with LangChain, including the Zep Cloud SDK and LangChain community packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/zep-cloud-retriever.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @getzep/zep-cloud @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith Environment Variables\nDESCRIPTION: Configuration of environment variables for LangSmith tracing and API access.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/summarization.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n\n# Reduce tracing latency if you are not in a serverless environment\n# export LANGCHAIN_CALLBACKS_BACKGROUND=true\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith API Key in Bash (Optional)\nDESCRIPTION: This optional snippet demonstrates how to set the LANGSMITH_TRACING and LANGSMITH_API_KEY environment variables for automated tracing of model calls.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/cohere.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Installing Spider Client for LangChain\nDESCRIPTION: Command to install necessary packages for using Spider web crawler with LangChain, including the community package, core package, and Spider client.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/spider.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core @spider-cloud/spider-client\n```\n\n----------------------------------------\n\nTITLE: Testing LangGraph Application with Streaming\nDESCRIPTION: This code tests the LangGraph application by processing a sample question and streaming the results of individual steps.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/sql_qa.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nlet inputs = { question: \"How many employees are there?\" }\n\nconsole.log(inputs)\nconsole.log(\"\\n====\\n\");\nfor await (\n  const step of await graph.stream(inputs, {\n    streamMode: \"updates\",\n  })\n) {\n  console.log(step);\n  console.log(\"\\n====\\n\");\n}\n```\n\n----------------------------------------\n\nTITLE: Loading DOCX Files with DocxLoader\nDESCRIPTION: Example code showing how to initialize DocxLoader and load text content from a .docx file\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/docx.mdx#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nimport { DocxLoader } from \"@langchain/community/document_loaders/fs/docx\";\n\nconst loader = new DocxLoader(\n  \"src/document_loaders/tests/example_data/attention.docx\"\n);\n\nconst docs = await loader.load();\n```\n\n----------------------------------------\n\nTITLE: Comparing on_retriever_end Output in TypeScript (v1 vs v2)\nDESCRIPTION: Shows the change in output format for the on_retriever_end event between v1 and v2 of streamEvents API. In v2, the output is simplified to directly return a list of Documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/versions/v0_2/migrating_astream_events.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// v1 output\n{\n  data: {\n    output: {\n      documents: [\n        Document(...),\n        Document(...),\n        ...\n      ]\n    }\n  }\n}\n\n// v2 output\n{\n  data: {\n    output: [\n      Document(...),\n      Document(...),\n      ...\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Cloudflare Integration Packages\nDESCRIPTION: This snippet shows the command to install the necessary LangChain packages for Cloudflare integration, including Anthropic and core components.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/cloudflare_d1.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/cloudflare @langchain/anthropic @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Initializing Supabase Vector Store with Documents in TypeScript\nDESCRIPTION: Creates a Supabase vector store instance with sample documents containing metadata. It sets up the necessary imports, defines documents, and specifies attribute information for querying.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/supabase.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { SupabaseVectorStore } from \"@langchain/community/vectorstores/supabase\";\nimport { Document } from \"@langchain/core/documents\";\nimport type { AttributeInfo } from \"langchain/chains/query_constructor\";\n\nimport { createClient } from \"@supabase/supabase-js\";\n\n/**\n * First, we create a bunch of documents. You can load your own documents here instead.\n * Each document has a pageContent and a metadata field. Make sure your metadata matches the AttributeInfo below.\n */\nconst docs = [\n  new Document({\n    pageContent:\n      \"A bunch of scientists bring back dinosaurs and mayhem breaks loose\",\n    metadata: { year: 1993, rating: 7.7, genre: \"science fiction\" },\n  }),\n  new Document({\n    pageContent:\n      \"Leo DiCaprio gets lost in a dream within a dream within a dream within a ...\",\n    metadata: { year: 2010, director: \"Christopher Nolan\", rating: 8.2 },\n  }),\n  new Document({\n    pageContent:\n      \"A psychologist / detective gets lost in a series of dreams within dreams within dreams and Inception reused the idea\",\n    metadata: { year: 2006, director: \"Satoshi Kon\", rating: 8.6 },\n  }),\n  new Document({\n    pageContent:\n      \"A bunch of normal-sized women are supremely wholesome and some men pine after them\",\n    metadata: { year: 2019, director: \"Greta Gerwig\", rating: 8.3 },\n  }),\n  new Document({\n    pageContent: \"Toys come alive and have a blast doing so\",\n    metadata: { year: 1995, genre: \"animated\" },\n  }),\n  new Document({\n    pageContent: \"Three men walk into the Zone, three men walk out of the Zone\",\n    metadata: {\n      year: 1979,\n      director: \"Andrei Tarkovsky\",\n      genre: \"science fiction\",\n      rating: 9.9,\n    },\n  }),\n];\n\n/**\n * Next, we define the attributes we want to be able to query on.\n * in this case, we want to be able to query on the genre, year, director, rating, and length of the movie.\n * We also provide a description of each attribute and the type of the attribute.\n * This is used to generate the query prompts.\n */\nconst attributeInfo: AttributeInfo[] = [\n  {\n    name: \"genre\",\n    description: \"The genre of the movie\",\n    type: \"string or array of strings\",\n  },\n  {\n    name: \"year\",\n    description: \"The year the movie was released\",\n    type: \"number\",\n  },\n  {\n    name: \"director\",\n    description: \"The director of the movie\",\n    type: \"string\",\n  },\n  {\n    name: \"rating\",\n    description: \"The rating of the movie (1-10)\",\n    type: \"number\",\n  },\n  {\n    name: \"length\",\n    description: \"The length of the movie in minutes\",\n    type: \"number\",\n  },\n];\n\n/**\n * Next, we instantiate a vector store. This is where we store the embeddings of the documents.\n * We also need to provide an embeddings object. This is used to embed the documents.\n */\n\nconst client = createClient(\n  process.env.SUPABASE_URL,\n  process.env.SUPABASE_PRIVATE_KEY\n);\n\nconst embeddings = new OpenAIEmbeddings();\nconst vectorStore = await SupabaseVectorStore.fromDocuments(docs, embeddings, {\n  client,\n});\n```\n\n----------------------------------------\n\nTITLE: Invoking a Chat Model in Python with LangChain JS\nDESCRIPTION: A simple example showing how to invoke a chat model using the await syntax in Python. This sends a basic 'Hello, world!' message to the model and waits for a response.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/index.mdx#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nawait model.invoke(\"Hello, world!\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Next.js Runtime for MongoDB Compatibility\nDESCRIPTION: This snippet shows how to set the runtime variable to 'nodejs' in Next.js to enable MongoDB usage in API routes.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/mongodb.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nexport const runtime = \"nodejs\";\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith API Key in Bash (Optional)\nDESCRIPTION: Sets the LANGSMITH_TRACING and LANGSMITH_API_KEY environment variables for automated tracing of model calls.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/fireworks.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Connecting to Local SQLite Database (Bash)\nDESCRIPTION: Opens the SQLite command-line interface connected to a local database file named `file.db`. This is used for setting up the database schema locally.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/libsql.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsqlite3 file.db\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for SQL Query Validation in LangChain.js\nDESCRIPTION: This snippet shows the command to install necessary npm packages for working with LangChain.js, OpenAI, TypeORM, and SQLite.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/sql_query_checking.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/openai typeorm sqlite3\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain and Dependencies (bash)\nDESCRIPTION: Installs the required npm packages for working with Typesense, LangChain's OpenAI integration, community vector stores, and core abstractions. These packages are prerequisites for setting up the Typesense client and running the TypeScript examples. Required before any TypeScript code in this guide will function.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/typesense.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Baidu Qianfan and LangChain Core Packages\nDESCRIPTION: Command to install the @langchain/baidu-qianfan and @langchain/core packages using npm or yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/baidu_qianfan.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/baidu-qianfan @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for Couchbase with LangChain.js\nDESCRIPTION: Command to install the necessary packages for integrating Couchbase with LangChain.js, including the LangChain community package, LangChain core, and the Couchbase client library.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/couchbase.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core couchbase\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for LangSmith\nDESCRIPTION: This code block sets environment variables for LangSmith, which is recommended for debugging and observability, but not required.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/sql_qa.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# Uncomment the below to use LangSmith. Not required, but recommended for debugging and observability.\n# export LANGSMITH_API_KEY=<your key>\n# export LANGSMITH_TRACING=true\n\n# Reduce tracing latency if you are not in a serverless environment\n# export LANGCHAIN_CALLBACKS_BACKGROUND=true\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key for Embeddings in Node.js\nDESCRIPTION: Sets the OpenAI API key as an environment variable for use with OpenAI embeddings in the HNSWLib vector store example.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/hnswlib.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.OPENAI_API_KEY = \"YOUR_API_KEY\";\n```\n\n----------------------------------------\n\nTITLE: Installing Layerup Security SDK\nDESCRIPTION: Command to install the Layerup Security SDK using npm or yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/layerup_security.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @layerup/layerup-security\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Pinecone\nDESCRIPTION: Configuration of necessary environment variables for Pinecone integration including API key, environment, and index name.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/pinecone.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.PINECONE_API_KEY = \"YOUR_API_KEY\";\nprocess.env.PINECONE_ENVIRONMENT = \"YOUR_ENVIRONMENT\";\nprocess.env.PINECONE_INDEX = \"YOUR_INDEX\";\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatOpenAI Model in TypeScript\nDESCRIPTION: This code initializes a ChatOpenAI model with specific parameters for use in a RAG chain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/kendra-retriever.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n// @lc-docs-hide-cell\n\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o-mini\",\n  temperature: 0,\n});\n```\n\n----------------------------------------\n\nTITLE: Customizing XML Parser with Tags\nDESCRIPTION: Shows how to customize the XMLOutputParser with specific tags for more structured output.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/output_parser_xml.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconst parserWithTags = new XMLOutputParser({ tags: [\"movies\", \"actor\", \"film\", \"name\", \"genre\"] });\n\n// We will add these instructions to the prompt below\nparserWithTags.getFormatInstructions();\n```\n\n----------------------------------------\n\nTITLE: Initializing WatsonxLLM with IAM Authentication in TypeScript\nDESCRIPTION: Creates a new instance of WatsonxLLM using IAM authentication method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_compressors/ibm.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { WatsonxLLM } from \"@langchain/community/llms/ibm\";\n\nconst props = {\n  version: \"YYYY-MM-DD\",\n  serviceUrl: \"<SERVICE_URL>\",\n  projectId: \"<PROJECT_ID>\",\n  watsonxAIAuthType: \"iam\",\n  watsonxAIApikey: \"<YOUR-APIKEY>\",\n};\nconst instance = new WatsonxLLM(props);\n```\n\n----------------------------------------\n\nTITLE: Displaying Results from SerpAPI Tool Chain\nDESCRIPTION: Code to extract and display the tool calls and content from the result of the SerpAPI tool chain execution.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/serpapi.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nconst { tool_calls, content } = toolChainResult;\n\nconsole.log(\"AIMessage\", JSON.stringify({\n  tool_calls,\n  content,\n}, null, 2));\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Environment Variable\nDESCRIPTION: Command to set the OPENAI_API_KEY environment variable, which is required for using OpenAI services in the application.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_streaming.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=YOUR_KEY\n```\n\n----------------------------------------\n\nTITLE: Streaming with ChatMistralAI Model\nDESCRIPTION: TypeScript code to initialize the ChatMistralAI model and stream responses from the model, which is useful for real-time generation of content.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-mistralai/README.md#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatMistralAI } from \"@langchain/mistralai\";\n\nconst model = new ChatMistralAI({\n  apiKey: process.env.MISTRAL_API_KEY,\n  modelName: \"mistral-small\",\n});\nconst response = await model.stream(new HumanMessage(\"Hello world!\"));\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain packages using npm\nDESCRIPTION: Command to install various LangChain packages necessary for enhancing the TypeORM integration with LangChain.js functionalities.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/typeorm.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Prisma via npm in Node.js\nDESCRIPTION: This snippet details the installation of Prisma in a Node.js project using npm. It establishes Prisma as a dependency, necessary for interfacing with databases.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/prisma.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install prisma\n```\n\n----------------------------------------\n\nTITLE: Installing PDF.js Distribution\nDESCRIPTION: Command to install the pdfjs-dist package for using a custom PDF.js build.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/pdf.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: mdx\nCODE:\n```\npdfjs-dist\n```\n\n----------------------------------------\n\nTITLE: Implementing Nomic Embeddings in TypeScript\nDESCRIPTION: Example showing how to initialize and use Nomic embeddings to generate vector embeddings for text documents. Demonstrates configuration options and basic usage.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-nomic/README.md#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { NomicEmbeddings } from \"@langchain/nomic\";\n\nconst nomicEmbeddings = new NomicEmbeddings({\n  apiKey: process.env.NOMIC_API_KEY, // Default value.\n  modelName: \"nomic-embed-text-v1\",  // Default value.\n});\n\nconst docs = [\n  \"hello world\",\n  \"nomic embeddings!\",\n  \"super special langchain integration package\",\n  \"what color is the sky?\",\n];\n\nconst embeddings = await nomicEmbeddings.embedDocuments(docs);\n```\n\n----------------------------------------\n\nTITLE: Logging AI Message Response from TavilySearch Chain\nDESCRIPTION: Extracts and logs the tool calls and content from the AI message response generated by the TavilySearch chain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/tavily_search.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nconst { tool_calls, content } = toolChainResult;\n\nconsole.log(\"AIMessage\", JSON.stringify({\n  tool_calls,\n  content,\n}, null, 2));\n```\n\n----------------------------------------\n\nTITLE: Agent Chain Execution Input\nDESCRIPTION: JSON input for an agent chain execution that processes a user query about the director of Oppenheimer (2023). It shows the beginning of an AgentExecutor chain with a ToolCallingAgent and includes the search results from the previous Tavily tool execution.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"input\": \"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\",\n  \"steps\": [\n    {\n      \"action\": {\n        \"tool\": \"tavily_search_results_json\",\n        \"toolInput\": {\n          \"input\": \"Oppenheimer 2023 film director age\"\n        },\n        \"toolCallId\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n        \"log\": \"Invoking \\\"tavily_search_results_json\\\" with {\\\"input\\\":\\\"Oppenheimer 2023 film director age\\\"}\\n[{\\\"type\\\":\\\"tool_use\\\",\\\"id\\\":\\\"toolu_01NUVejujVo2y8WGVtZ49KAN\\\",\\\"name\\\":\\\"tavily_search_results_json\\\",\\\"input\\\":{\\\"input\\\":\\\"Oppenheimer 2023 film director age\\\"}}]\",\n        \"messageLog\": [\n          {\n            \"lc\": 1,\n            \"type\": \"constructor\",\n            \"id\": [\n\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Mem0 Memory Integration in LangChain.js\nDESCRIPTION: Command to install the necessary npm packages for integrating Mem0 memory with LangChain.js. This includes the OpenAI package, LangChain core, and community packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/mem0_memory.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core @langchain/community\n```\n\n----------------------------------------\n\nTITLE: Installing Azure OpenAI Package Dependencies\nDESCRIPTION: npm/yarn command to install the required Azure OpenAI packages for LangChain integration\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-azure-openai/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/azure-openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Building the Project\nDESCRIPTION: Command to build the project, compiling TypeScript files and preparing the package for use.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/contributing/code.mdx#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nyarn build\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Baidu Wenxin Integration\nDESCRIPTION: Command to install the required packages for using Baidu Wenxin models with LangChain.js. Installs both the community package and core components.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/baidu_wenxin.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing pg-copy-streams for Batch Operations\nDESCRIPTION: This code snippet shows how to install the pg-copy-streams library, which facilitates efficient batch vector handling in PostgreSQL through copy streams. This is crucial for enhancing performance during large data insertions into AnalyticDB.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/analyticdb.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -S pg-copy-streams\n```\n\n----------------------------------------\n\nTITLE: Tracking Token Usage with AIMessage.usage_metadata for Anthropic\nDESCRIPTION: Example demonstrating how to access token usage information using the usage_metadata attribute of AIMessage objects when working with Anthropic models.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chat_token_usage_tracking.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport { HumanMessage } from \"@langchain/core/messages\";\n\nconst model = new ChatAnthropic({\n  modelName: \"claude-2\",\n});\n\nconst response = await model.invoke([new HumanMessage(\"Hello world!\")]);\n\nconsole.log(JSON.stringify(response.usage_metadata, null, 2));\n/*\n{\n  \"input_tokens\": 13,\n  \"output_tokens\": 14\n}\n*/\n\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for LangSmith\nDESCRIPTION: Commands to set environment variables for enabling LangSmith tracing and API key.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/agent_executor.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n\n# Reduce tracing latency if you are not in a serverless environment\n# export LANGCHAIN_CALLBACKS_BACKGROUND=true\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatGoogleGenerativeAI for Text Generation\nDESCRIPTION: TypeScript code to initialize and use the ChatGoogleGenerativeAI model for text generation with the gemini-pro model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-google-genai/README.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatGoogleGenerativeAI } from \"@langchain/google-genai\";\n\nconst model = new ChatGoogleGenerativeAI({\n  modelName: \"gemini-pro\",\n  maxOutputTokens: 2048,\n});\nconst response = await model.invoke(new HumanMessage(\"Hello world!\"));\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Packages\nDESCRIPTION: This snippet installs necessary LangChain packages, including openai, community, and core modules. These packages provide functionalities required for using LangChain.js with AnalyticDB and performing various AI and data processing tasks.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/analyticdb.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Loading DOC Files with DocxLoader\nDESCRIPTION: Example code demonstrating how to initialize DocxLoader with the 'doc' type parameter for loading legacy .doc files\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/docx.mdx#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nimport { DocxLoader } from \"@langchain/community/document_loaders/fs/docx\";\n\nconst loader = new DocxLoader(\n  \"src/document_loaders/tests/example_data/attention.doc\",\n  {\n    type: \"doc\",\n  }\n);\n\nconst docs = await loader.load();\n```\n\n----------------------------------------\n\nTITLE: Embedding Multiple Texts with CohereEmbeddings in Python\nDESCRIPTION: This code snippet shows how to use the embedDocuments method of CohereEmbeddings to generate vector representations for multiple texts simultaneously.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/cohere.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nconst text2 = \"LangGraph is a library for building stateful, multi-actor applications with LLMs\";\n\nconst vectors = await embeddings.embedDocuments([text, text2]);\n\nconsole.log(vectors[0].slice(0, 100));\nconsole.log(vectors[1].slice(0, 100));\n```\n\n----------------------------------------\n\nTITLE: Setting Fireworks API Key in Bash\nDESCRIPTION: Sets the FIREWORKS_API_KEY environment variable for authentication with Fireworks API.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/fireworks.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport FIREWORKS_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Initializing ToolMessage Constructor in Python\nDESCRIPTION: This code snippet initializes a ToolMessage constructor from the langchain_core.messages module. It contains a tool call ID and content related to the Oppenheimer film search results.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\n{\n  \"lc\": 1,\n  \"type\": \"constructor\",\n  \"id\": [\n    \"langchain_core\",\n    \"messages\",\n    \"ToolMessage\"\n  ],\n  \"kwargs\": {\n    \"tool_call_id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n    \"content\": \"[{\\\"title\\\":\\\"Oppenheimer (2023) - IMDb\\\",\\\"url\\\":\\\"https://www.imdb.com/title/tt15398776/\\\",\\\"content\\\":\\\"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.\\\",\\\"score\\\":0.96643,\\\"raw_content\\\":null},{\\\"title\\\":\\\"Christopher Nolan's Oppenheimer - Rotten Tomatoes\\\",\\\"url\\\":\\\"https://editorial.rottentomatoes.com/article/everything-we-know-about-christopher-nolans-oppenheimer/\\\",\\\"content\\\":\\\"Billboards and movie theater pop-ups across Los Angeles have been ticking down for months now: Christopher Nolan's epic account of J. Robert Oppenheimer, the father of the atomic bomb, is nearing an explosive release on July 21, 2023. Nolan movies are always incredibly secretive, twists locked alongside totems behind safe doors, actors not spilling an ounce of Earl Grey tea.\\\",\\\"score\\\":0.92804,\\\"raw_content\\\":null},{\\\"title\\\":\\\"Oppenheimer (film) - Wikipedia\\\",\\\"url\\\":\\\"https://en.wikipedia.org/wiki/Oppenheimer_(film)\\\",\\\"content\\\":\\\"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\\\\nCritical response\\\\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \\\\\\\"more objective view of his story from a different character's point of view\\\\\\\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\\\\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\\\\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\\\\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \\\\\\\"big-atures\\\\\\\", since the special effects team had tried to build the models as physically large as possible. He felt that \\\\\\\"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\\\\\\\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \\\\\\\"emotional\\\\\\\" and resembling that of a thriller, while also remarking that Nolan had \\\\\\\"Trojan-Horsed a biopic into a thriller\\\\\\\".[72]\\\\nCasting\\\\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\\\\\\\", while also underscoring that it is a \\\\\\\"huge shift in perception about the reality of Oppenheimer's perception\\\\\\\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \\\\\\\"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\\\\\\\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.\\\",\\\"score\\\":0.92404,\\\"raw_content\\\":null},{\\\"title\\\":\\\"'Oppenheimer' Director Christopher Nolan On Filmmaking at 53: \\\\\\\"I Try to ...\\\",\\\"url\\\":\\\"https://www.everythingzoomer.com/arts-entertainment/2023/11/21/oppenheimer-director-christopher-nolan-on-filmmaking-at-53-i-try-to-challenge-myself-with-every-film/\\\",\\\"content\\\":\\\"Oppenheimer will be available to own on 4K Ultra HD, Blu-ray and DVD — including more than three hours of bonus features — on November 21.\\\\nRELATED:\\\\nVisiting the Trinity Site Featured in 'Oppenheimer' Is a Sobering Reminder of the Horror of Nuclear Weapons\\\\nBarbenheimer: How 'Barbie' and 'Oppenheimer' Became the Unlikely Movie Marriage of the Summer\\\\nBlast From the Past: 'Asteroid City' & 'Oppenheimer' and the Age of Nuclear Anxiety\\\\nEXPLORE  HealthMoneyTravelFoodStyleBook ClubClassifieds#ZoomerDailyPolicy & PerspectiveArts & EntertainmentStars & RoyaltySex & Love\\\\nCONNECT  FacebookTwitterInstagram\\\\nSUBSCRIBE  Terms of Subscription ServiceE-NewslettersSubscribe to Zoomer Magazine\\\\nBROWSE  AboutMastheadContact UsAdvertise with UsPrivacy Policy\\\\nEverythingZoomer.com is part of the ZoomerMedia Digital Network \\\"I think with experience — and with the experience of watching your films with an audience over the years — you do more and more recognize the human elements that people respond to, and the things that move you and\\\",\\\"score\\\":0.92002,\\\"raw_content\\\":null}]\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Redis Dependencies for Caching in npm\nDESCRIPTION: This command installs the necessary dependencies for using Redis-based caching in a Node.js project with npm.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chat_model_caching.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install ioredis @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Splitting Code with RecursiveCharacterTextSplitter in JavaScript\nDESCRIPTION: This snippet shows how to use RecursiveCharacterTextSplitter with programming languages, demonstrating how to leverage language-specific separators to intelligently split code into chunks while preserving semantic meaning.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/static/llms.txt#2025-04-22_snippet_3\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";\n\nconst jsCode = `function helloWorld() {\n  console.log(\"Hello, World!\");\n}\n\nhelloWorld();`;\n\nconst splitter = RecursiveCharacterTextSplitter.fromLanguage(\"javascript\", {\n  chunkSize: 32,\n  chunkOverlap: 0\n});\n\nconst output = await splitter.createDocuments([jsCode]);\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for LangChain.js Q&A Chain\nDESCRIPTION: Command to install necessary npm packages for the Q&A chain implementation, including LangChain, OpenAI, and Cheerio.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_streaming.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install --save langchain @langchain/openai cheerio\n```\n\n----------------------------------------\n\nTITLE: Creating a React Agent with LangChain Tools in TypeScript\nDESCRIPTION: This snippet demonstrates creating a reactive agent using LangChain's createReactAgent function, incorporating the large language model (LLM) and tools. Before running this code, ensure @langchain/langgraph is installed.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/sql.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createReactAgent } from \\\"@langchain/langgraph/prebuilt\\\"\n\nconst agentExecutor = createReactAgent({ llm, tools });\n```\n\n----------------------------------------\n\nTITLE: AIMessageChunk Constructor with Tool Use Capabilities in TypeScript\nDESCRIPTION: Implementation of the AIMessageChunk constructor showing how LangChain.js structures AI responses with mixed content types including text and tool usage. The snippet demonstrates tool call handling with calculator functionality and includes metadata about the message.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_23\n\nLANGUAGE: typescript\nCODE:\n```\n{\n  \"type\": \"constructor\",\n  \"id\": [\n    \"langchain_core\",\n    \"messages\",\n    \"AIMessageChunk\"\n  ],\n  \"kwargs\": {\n    \"content\": [\n      {\n        \"type\": \"text\",\n        \"text\": \"Based on the search results, the 2023 film Oppenheimer was directed by Christopher Nolan. Some key information about Christopher Nolan:\\n\\n- He is a British-American film director, producer and screenwriter.\\n- He was born on July 30, 1970, making him currently 52 years old.\\n\\nTo calculate his age in days:\"\n      },\n      {\n        \"type\": \"tool_use\",\n        \"id\": \"toolu_01NVTbm5aNYSm1wGYb6XF7jE\",\n        \"name\": \"calculator\",\n        \"input\": {\n          \"input\": \"52 * 365\"\n        }\n      }\n    ],\n    \"additional_kwargs\": {\n      \"id\": \"msg_01RBDqmJKNXiEjgt5Xrng4mz\",\n      \"type\": \"message\",\n      \"role\": \"assistant\",\n      \"model\": \"claude-3-sonnet-20240229\",\n      \"stop_sequence\": null,\n      \"usage\": {\n        \"input_tokens\": 2810,\n        \"output_tokens\": 137\n      },\n      \"stop_reason\": \"tool_use\"\n    },\n    \"tool_call_chunks\": [\n      {\n        \"name\": \"calculator\",\n        \"args\": \"{\\\"input\\\":\\\"52 * 365\\\"}\",\n        \"id\": \"toolu_01NVTbm5aNYSm1wGYb6XF7jE\",\n        \"index\": 0\n      }\n    ],\n    \"tool_calls\": [\n      {\n        \"name\": \"calculator\",\n        \"args\": {\n          \"input\": \"52 * 365\"\n        },\n        \"id\": \"toolu_01NVTbm5aNYSm1wGYb6XF7jE\"\n      }\n    ],\n    \"invalid_tool_calls\": [],\n    \"response_metadata\": {}\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing @langchain/google-genai Package with npm/yarn\nDESCRIPTION: Command to install the @langchain/google-genai package and its dependency @langchain/core using npm or yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-google-genai/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/google-genai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Community and Core Packages (Bash)\nDESCRIPTION: Command to install the `@langchain/community` and `@langchain/core` npm packages using npm. These are required peer dependencies for utilizing the Connery Action Tool.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/connery.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith API Key for Tracing\nDESCRIPTION: Optional setup for automated tracing from individual queries using LangSmith API key.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/hnswlib.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// process.env.LANGSMITH_API_KEY = \"<YOUR API KEY HERE>\";\n// process.env.LANGSMITH_TRACING = \"true\";\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Upstash Redis Integration\nDESCRIPTION: Command to install required npm packages for implementing Upstash Redis chat memory including LangChain and Upstash Redis client\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/upstash_redis.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core @upstash/redis\n```\n\n----------------------------------------\n\nTITLE: Invoking Langchain Tool with ToolCall\nDESCRIPTION: Example of invoking the Langchain tool with a model-generated ToolCall.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/tools.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n// This is usually generated by a model, but we'll create a tool call directly for demo purposes.\nconst modelGeneratedToolCall = {\n  args: {},  // TODO: FILL IN\n  id: \"1\",\n  name: tool.name,\n  type: \"tool_call\",\n}\nawait tool.invoke(modelGeneratedToolCall)\n```\n\n----------------------------------------\n\nTITLE: Setting up Environment Variables for Mistral API\nDESCRIPTION: Sets the necessary environment variables for authenticating with the Mistral AI API and optionally enables LangSmith tracing.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/mistral.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport MISTRAL_API_KEY=\"your-api-key\"\n```\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI Embeddings\nDESCRIPTION: Initializes OpenAI embeddings using the text-embedding-3-large model for vector representations.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/qa_chat_history.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst embeddings = new OpenAIEmbeddings({model: \"text-embedding-3-large\"});\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI and LangChain Core Dependencies\nDESCRIPTION: Command to install the necessary npm packages for using OpenAI with LangChain.js.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chat_token_usage_tracking.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Using OpenAI Embeddings with ZepVectorStore\nDESCRIPTION: Example demonstrating how to use OpenAI embeddings instead of Zep's auto-embedding feature.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/zep.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n{ExampleOpenAI}\n```\n\n----------------------------------------\n\nTITLE: Import LangChain Package via NPM or Yarn\nDESCRIPTION: Provides commands to install the LangChain package using either npm or yarn via the integration installation tooltip. No specific dependencies are listed, but npm or yarn command-line interfaces are required.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/openapi.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: mdx\nCODE:\n```\n<IntegrationInstallTooltip></IntegrationInstallTooltip>\n\n<Npm2Yarn>\n  langchain @langchain/core\n</Npm2Yarn>\n```\n\n----------------------------------------\n\nTITLE: Importing from LangChain Subpaths\nDESCRIPTION: Example of importing a module from a LangChain subpath. This demonstrates the entrypoint system used in the project.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/contributing/code.mdx#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { OpenAI } from \"langchain/llms/openai\";\n```\n\n----------------------------------------\n\nTITLE: Installing Development Dependencies for Cerebras Package\nDESCRIPTION: Command to install the required dependencies for developing the @langchain/cerebras package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-cerebras/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nyarn install\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Python Interpreter Tool\nDESCRIPTION: Command to install the required packages @langchain/openai and @langchain/core for using the Python interpreter tool.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/pyinterpreter.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith Environment Variables\nDESCRIPTION: Configures environment variables for LangSmith tracing and API access to enable monitoring and debugging of LangChain applications.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/qa_chat_history.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport LANGSMITH_TRACING=true\nexport LANGSMITH_API_KEY=YOUR_KEY\n\n# Reduce tracing latency if you are not in a serverless environment\n# export LANGCHAIN_CALLBACKS_BACKGROUND=true\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies and Declaring Server-Side Code\nDESCRIPTION: Importing necessary modules and declaring server-side code for LangChain integration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/stream_tool_client.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n\"use server\";\n\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { ChatPromptTemplate } from \"@langchain/core/prompts\";\nimport { createStreamableValue } from \"ai/rsc\";\nimport { z } from \"zod\";\nimport { zodToJsonSchema } from \"zod-to-json-schema\";\nimport { JsonOutputKeyToolsParser } from \"@langchain/core/output_parsers/openai_tools\";\n```\n\n----------------------------------------\n\nTITLE: Yielding Keys from __module_name__ Key-Value Store\nDESCRIPTION: Demonstrates how to use the 'yieldKeys' method to retrieve all keys or keys with a specific prefix from the key-value store. This is useful for iterating over stored data.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/kv_store.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport { __module_name__ } from \"__full_import_path__\"\n\nconst kvStoreForYield = new __module_name__({\n  ...\n});\n\nconst encoderForYield = new TextEncoder();\n\n// Add some data to the store\nawait kvStoreForYield.mset(\n  [\n    [\"message:id:key1\", encoderForYield.encode(\"value1\")],\n    [\"message:id:key2\", encoderForYield.encode(\"value2\")],\n  ]\n)\n\nconst yieldedKeys = [];\nfor await (const key of kvStoreForYield.yieldKeys(\"message:id:\")) {\n  yieldedKeys.push(key);\n}\n\nconsole.log(yieldedKeys);\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for Cassandra Chat Memory\nDESCRIPTION: Command to install the necessary packages including the Cassandra Node.js driver and LangChain components required for implementing Cassandra-backed chat memory.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/cassandra.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install cassandra-driver @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith Credentials in Bash\nDESCRIPTION: Environment variable configuration for enabling LangSmith tracing and API key setup.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/ollama.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Installing required packages for SearchApi integration\nDESCRIPTION: Command for installing the necessary npm packages to use SearchApi with LangChain, including the community package, core package, and OpenAI integration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/searchapi.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core @langchain/openai\n```\n\n----------------------------------------\n\nTITLE: Building and Serving LangChainJS API Documentation with Yarn\nDESCRIPTION: Commands for building API references and serving the documentation locally using Yarn. This snippet shows how to generate the API documentation and start a local server to view it.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/api_refs/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nTo build the API refs run `yarn build` from the root of this directory, then `yarn dev` or `yarn start` to serve the docs locally.\n```\n\n----------------------------------------\n\nTITLE: Using BaiduQianfanEmbeddings for text embeddings\nDESCRIPTION: Example of using BaiduQianfanEmbeddings to generate vector embeddings for a text query. This creates an embeddings instance and calls embedQuery to convert the text into a vector representation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-baidu-qianfan/README.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { BaiduQianfanEmbeddings } from \"@langchain/baidu-qianfan\";\n\nconst embeddings = new BaiduQianfanEmbeddings();\nconst res = await embeddings.embedQuery(\"Introduce the city Beijing\");\n```\n\n----------------------------------------\n\nTITLE: Configuring LangSmith Environment Variables\nDESCRIPTION: Optional configuration for enabling LangSmith tracing and API key setup.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/pinecone.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Displaying AgentExecutor Chain Output in JSON Format\nDESCRIPTION: Shows the final output of a LangChain AgentExecutor chain that processed a query about Christopher Nolan's age. The response includes both the original input query and the generated answer about the director's age in years and days.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_41\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"input\": \"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\",\n  \"output\": \"So Christopher Nolan, the director of the 2023 film Oppenheimer, is currently 52 years old, which is approximately 18,980 days old (assuming 365 days per year).\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Redis Dependencies with Package Manager\nDESCRIPTION: Installation command for required dependencies including @langchain/community, @langchain/core, and ioredis packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/stores/ioredis_storage.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core ioredis\n```\n\n----------------------------------------\n\nTITLE: Building @langchain/textsplitters Package\nDESCRIPTION: Commands to build the @langchain/textsplitters package, either from within the package directory or from the repo root.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-textsplitters/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nyarn build\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn build --filter=@langchain/textsplitters\n```\n\n----------------------------------------\n\nTITLE: Extracting Document Categories from Loaded Documents\nDESCRIPTION: Example of extracting unique document categories from the metadata of loaded documents using a Set and map function.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/document_loader_markdown.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nconst categories = new Set(data.map((document) => document.metadata.category));\nconsole.log(categories);\n```\n\n----------------------------------------\n\nTITLE: Building the Cerebras Package\nDESCRIPTION: Commands to build the @langchain/cerebras package, either directly or from the repository root.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-cerebras/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nyarn build\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn build --filter=@langchain/cerebras\n```\n\n----------------------------------------\n\nTITLE: Implementing Reduce Prompt Template\nDESCRIPTION: Definition of the reduce prompt template for consolidating multiple summaries.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/summarization.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nlet reduceTemplate = `\nThe following is a set of summaries:\n{docs}\nTake these and distill it into a final, consolidated summary\nof the main themes.\n`\n\nconst reducePrompt = ChatPromptTemplate.fromMessages(\n  [\n    [\"user\", reduceTemplate]\n  ]\n)\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith API Key for Tracing in TypeScript\nDESCRIPTION: Optional configuration to enable automated tracing from individual queries using LangSmith. This snippet shows how to set the LangSmith API key and enable tracing.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/kendra-retriever.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// process.env.LANGSMITH_API_KEY = \"<YOUR API KEY HERE>\";\n// process.env.LANGSMITH_TRACING = \"true\";\n```\n\n----------------------------------------\n\nTITLE: Installing Metal SDK Dependencies\nDESCRIPTION: Command to install required packages including Metal SDK, LangChain Community, and LangChain Core dependencies.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/metal-retriever.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @getmetal/metal-sdk @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Example Output of Toolkit Instantiation\nDESCRIPTION: Shows a sample text output representing the result of the WatsonxToolkit instantiation, typically indicating a module object.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/ibm.mdx#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\n[Module: null prototype] { default: {} }\n```\n\n----------------------------------------\n\nTITLE: Markdown Page Configuration with Sidebar Settings\nDESCRIPTION: Frontend configuration for a documentation page that hides the sidebar class name and table of contents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llm_caching/index.mdx#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\nsidebar_class_name: hidden\nhide_table_of_contents: true\n---\n```\n\n----------------------------------------\n\nTITLE: Setting up LLM Configuration\nDESCRIPTION: Initializes the ChatOpenAI model with specific parameters\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_multiple_retrievers.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatOpenAI } from '@langchain/openai';\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o\",\n  temperature: 0,\n})\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Community and Core Packages\nDESCRIPTION: Command for installing @langchain/community and @langchain/core packages separately from the main package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/tests/__mdx__/modules/two.mdx#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing @langchain/tavily Package via npm\nDESCRIPTION: Command to install the @langchain/tavily package using npm package manager.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-tavily/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/tavily\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for API Authentication\nDESCRIPTION: Instructions for setting up required environment variables including the model API key and optional LangSmith tracing configuration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/llms.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport __env_var_name__=\"your-api-key\"\n```\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Complete Tool Response Example\nDESCRIPTION: Shows the correct implementation by adding a second tool response to match the number of tool calls.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/troubleshooting/errors/INVALID_TOOL_RESULTS.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst toolResponse2 = await dummyTool.invoke(responseMessage.tool_calls![1]);\n\nchatHistory.push(toolResponse2);\n\nawait modelWithTools.invoke(chatHistory);\n```\n\n----------------------------------------\n\nTITLE: Importing Example Code for Upstash Redis Storage in LangChain.js\nDESCRIPTION: This snippet demonstrates the import of an example code file for Upstash Redis storage usage. It uses a custom CodeBlock component to display the contents of the example file.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/stores/upstash_redis_storage.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport CodeBlock from \"@theme/CodeBlock\";\nimport Example from \"@examples/stores/upstash_redis_storage.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Adding a New Entrypoint in LangChain.js Config\nDESCRIPTION: This code snippet shows how to add a new entrypoint in the LangChain.js configuration file.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/CONTRIBUTING.md#2025-04-22_snippet_15\n\nLANGUAGE: typescript\nCODE:\n```\n// ...\nentrypoints: {\n  // ...\n  tools: \"tools/index\",\n},\n// ...\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key in Bash\nDESCRIPTION: Sets the OpenAI API key as an environment variable for use with the embeddings model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot_examples.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=\"...\"\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies\nDESCRIPTION: Command to install all project dependencies using Yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-pinecone/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyarn install\n```\n\n----------------------------------------\n\nTITLE: Running Tests\nDESCRIPTION: Commands to run unit tests and integration tests for the @langchain/mixedbread-ai package. Unit tests end with .test.ts and integration tests end with .int.test.ts.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-mixedbread-ai/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nyarn test\nyarn test:int\n```\n\n----------------------------------------\n\nTITLE: Setting up Fireworks API Key\nDESCRIPTION: Setting environment variables for Fireworks API key and optional LangSmith tracing.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/fireworks.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport FIREWORKS_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Community and Core Packages\nDESCRIPTION: Command to install the required @langchain/community and @langchain/core packages using npm. These packages are necessary for using the Prem AI embeddings in LangChain.js.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/premai.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Gradient AI Embeddings Dependencies (Bash)\nDESCRIPTION: This command installs the LangChain Community and Core packages along with the Gradient AI Node.js SDK using npm. These packages are prerequisites for using the `GradientEmbeddings` class in a LangChainJS project. The `npm2yarn` directive suggests an equivalent yarn command exists.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/gradient_ai.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @langchain/community @langchain/core @gradientai/nodejs-sdk\n```\n\n----------------------------------------\n\nTITLE: Client Component Imports\nDESCRIPTION: Required imports for the client-side React component\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/stream_agent_client.mdx#2025-04-22_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\n\"use client\";\n\nimport { useState } from \"react\";\nimport { readStreamableValue } from \"ai/rsc\";\nimport { runAgent } from \"./action\";\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Dependencies for LangChainJS\nDESCRIPTION: Command to install the required packages for using OpenAI models with LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/few_shot.mdx#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for LangChain and SerpAPI Integration\nDESCRIPTION: Command to install necessary npm packages for using LangChain with SerpAPI and OpenAI.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/serpapi.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core @langchain/openai\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with npm\nDESCRIPTION: Installing required npm packages for the RAG citation implementation\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_citations.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install --save langchain @langchain/community @langchain/openai\n```\n\n----------------------------------------\n\nTITLE: Installing @langchain/google-gauth Package\nDESCRIPTION: Command to install the @langchain/google-gauth package using yarn package manager. This package is required to access Google AI/ML models and other Google services with proper authentication.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-google-gauth/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn add @langchain/google-gauth\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for ExaRetriever in TypeScript\nDESCRIPTION: This snippet shows how to set the required API keys as environment variables for using ExaRetriever and optionally LangSmith for tracing.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/exa.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.EXASEARCH_API_KEY=\"<YOUR API KEY>\";\n\n// process.env.LANGSMITH_API_KEY = \"<YOUR API KEY HERE>\";\n// process.env.LANGSMITH_TRACING = \"true\";\n```\n\n----------------------------------------\n\nTITLE: Implementing Tool Calling Agent with Image Search in TypeScript\nDESCRIPTION: This snippet demonstrates the creation of a tool for image searching, setting up an agent executor with the tool, and implementing server-side functionality. It uses LangChain.js and React Server Components.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/generative_ui.mdx#2025-04-22_snippet_0\n\nLANGUAGE: tsx\nCODE:\n```\n\"use server\";\n\nconst tool = tool(\n  async (input, config) => {\n    const stream = await createRunnableUI(config);\n    stream.update(<div>Searching...</div>);\n\n    const result = await images(input);\n    stream.done(\n      <Images\n        images={result.images_results\n          .map((image) => image.thumbnail)\n          .slice(0, input.limit)}\n      />\n    );\n\n    return `[Returned ${result.images_results.length} images]`;\n  },\n  {\n    name: \"Images\",\n    description: \"A tool to search for images. input should be a search query.\",\n    schema: z.object({\n      query: z.string().describe(\"The search query used to search for cats\"),\n      limit: z.number().describe(\"The number of pictures shown to the user\"),\n    }),\n  }\n);\n\n// add LLM, prompt, etc...\n\nconst tools = [tool];\n\nexport const agentExecutor = new AgentExecutor({\n  agent: createToolCallingAgent({ llm, tools, prompt }),\n  tools,\n});\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Vectara Configuration\nDESCRIPTION: Configuration of essential environment variables required for Vectara integration, including customer ID, corpus ID, and API key.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/vectara.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.VECTARA_CUSTOMER_ID = \"your_customer_id\";\nprocess.env.VECTARA_CORPUS_ID = \"your_corpus_id\";\nprocess.env.VECTARA_API_KEY = \"your-vectara-api-key\";\n```\n\n----------------------------------------\n\nTITLE: Configuring Package Dependencies in package.json\nDESCRIPTION: Example package.json configuration to ensure all LangChain packages use the same instance of @langchain/core. Includes resolutions for yarn, overrides for npm, and pnpm configuration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-cohere/README.md#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"your-project\",\n  \"version\": \"0.0.0\",\n  \"dependencies\": {\n    \"@langchain/cohere\": \"^0.0.1\",\n    \"@langchain/core\": \"^0.3.0\",\n  },\n  \"resolutions\": {\n    \"@langchain/core\": \"0.3.0\"\n  },\n  \"overrides\": {\n    \"@langchain/core\": \"0.3.0\"\n  },\n  \"pnpm\": {\n    \"overrides\": {\n      \"@langchain/core\": \"0.3.0\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Azure Blob Storage Document Loader Dependencies\nDESCRIPTION: Command to install dependencies for Azure Blob Storage document loaders, which enable loading documents from Azure Blob Storage or Azure Files.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/microsoft.mdx#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core @azure/storage-blob\n```\n\n----------------------------------------\n\nTITLE: Firestore Security Rules Configuration\nDESCRIPTION: Security rules configuration for Firestore to manage read/write access to chat history collections based on user authentication.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/firestore.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n      match /chathistory/{sessionId} {\n       allow read: if request.auth.uid == resource.data.createdBy;\n       allow write: if request.auth.uid == request.resource.data.createdBy;\n\t\t\t }\n\t\t\t match /chathistory/{sessionId}/messages/{messageId} {\n       allow read: if request.auth.uid == resource.data.createdBy;\n       allow write: if request.auth.uid == request.resource.data.createdBy;\n\t\t    }\n```\n\n----------------------------------------\n\nTITLE: Declarative Message Filtering with Anthropic Integration\nDESCRIPTION: Shows how to use filterMessages in a declarative chain with the Anthropic chat model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/filter_messages.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst llm = new ChatAnthropic({ model: \"claude-3-sonnet-20240229\", temperature: 0 })\n// Notice we don't pass in messages. This creates\n// a RunnableLambda that takes messages as input\nconst filter_ = filterMessages({ excludeNames: [\"example_user\", \"example_assistant\"], end })\nconst chain = filter_.pipe(llm);\nawait chain.invoke(messages)\n```\n\n----------------------------------------\n\nTITLE: Publishing Package\nDESCRIPTION: Command to publish the package to npm registry\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-anthropic/README.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n$ npm publish\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Example Selector Based on Word Length\nDESCRIPTION: Creates a custom example selector that chooses examples based on the similarity in length between the input word and example words.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/example_selectors.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { BaseExampleSelector } from \"@langchain/core/example_selectors\";\nimport { Example } from \"@langchain/core/prompts\";\n\n\nclass CustomExampleSelector extends BaseExampleSelector {\n    private examples: Example[];\n  \n    constructor(examples: Example[]) {\n      super();\n      this.examples = examples;\n    }\n  \n    async addExample(example: Example): Promise<void | string> {\n      this.examples.push(example);\n      return;\n    }\n  \n    async selectExamples(inputVariables: Example): Promise<Example[]> {\n      // This assumes knowledge that part of the input will be a 'text' key\n      const newWord = inputVariables.input;\n      const newWordLength = newWord.length;\n  \n      // Initialize variables to store the best match and its length difference\n      let bestMatch: Example | null = null;\n      let smallestDiff = Infinity;\n  \n      // Iterate through each example\n      for (const example of this.examples) {\n        // Calculate the length difference with the first word of the example\n        const currentDiff = Math.abs(example.input.length - newWordLength);\n  \n        // Update the best match if the current one is closer in length\n        if (currentDiff < smallestDiff) {\n          smallestDiff = currentDiff;\n          bestMatch = example;\n        }\n      }\n  \n      return bestMatch ? [bestMatch] : [];\n    }\n  }\n```\n\n----------------------------------------\n\nTITLE: Logging Document Metadata\nDESCRIPTION: Outputs the metadata of the first document loaded from the CSV file to the console.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/csv.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(docs[0].metadata)\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for ByteDanceDoubao\nDESCRIPTION: Configuration of environment variables for LangSmith API tracing and authentication.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/bytedance_doubao.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGCHAIN_TRACING_V2=\"true\"\n# export LANGCHAIN_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Using Embeddings Model\nDESCRIPTION: Example of initializing and using the embeddings model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/create-langchain-integration/template/README.md#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { <ADD_CLASS_NAME_HERE> } from \"@langchain/<ADD_PACKAGE_NAME_HERE>\";\n\nconst embeddings = new ExampleEmbeddingClass({\n  apiKey: process.env.EXAMPLE_API_KEY,\n});\nconst res = await embeddings.embedQuery(\"Hello world\");\n```\n\n----------------------------------------\n\nTITLE: Package.json Configuration for Dependency Resolution\nDESCRIPTION: Configuration to ensure consistent @langchain/core dependency version across different package managers.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-openai/README.md#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"your-project\",\n  \"version\": \"0.0.0\",\n  \"dependencies\": {\n    \"@langchain/core\": \"^0.3.0\",\n    \"@langchain/openai\": \"^0.0.0\"\n  },\n  \"resolutions\": {\n    \"@langchain/core\": \"^0.3.0\"\n  },\n  \"overrides\": {\n    \"@langchain/core\": \"^0.3.0\"\n  },\n  \"pnpm\": {\n    \"overrides\": {\n      \"@langchain/core\": \"^0.3.0\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Credentials for Fireworks API\nDESCRIPTION: Commands for setting environment variables to authenticate with Fireworks and optionally enable LangSmith tracing.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/fireworks.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport FIREWORKS_API_KEY=\"your-api-key\"\n```\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Implementing xAI Chat Model\nDESCRIPTION: Example of initializing and using the ChatXAI model to generate responses\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-xai/README.md#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatXAI } from \"@langchain/xai\";\nimport { HumanMessage } from \"@langchain/core/messages\";\n\nconst model = new ChatXAI({\n  apiKey: process.env.XAI_API_KEY, // Default value.\n});\n\nconst message = new HumanMessage(\"What color is the sky?\");\n\nconst res = await model.invoke([message]);\n```\n\n----------------------------------------\n\nTITLE: Installing Azure Cosmos DB Dependencies\nDESCRIPTION: Command to install the required Azure Cosmos DB package and core dependencies for LangChain.js integration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llm_caching/azure_cosmosdb_nosql.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/azure-cosmosdb @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Provider-Specific Package\nDESCRIPTION: Example command showing how to install a provider-specific LangChain package (OpenAI) using Yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/langchain-core/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn add @langchain/openai\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for FileSystemChatMessageHistory\nDESCRIPTION: Commands to install the required npm packages for using FileSystemChatMessageHistory in LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/file.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Initializing Environment Variables for OpenAI and LangSmith\nDESCRIPTION: Sets up environment variables for OpenAI API key and optional LangSmith tracing configuration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/migrate_agent.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n// process.env.OPENAI_API_KEY = \"...\";\n\n// Optional, add tracing in LangSmith\n// process.env.LANGSMITH_API_KEY = \"ls...\";\n// process.env.LANGCHAIN_CALLBACKS_BACKGROUND = \"true\";\n// process.env.LANGSMITH_TRACING = \"true\";\n// process.env.LANGSMITH_PROJECT = \"How to migrate: LangGraphJS\";\n\n// Reduce tracing latency if you are not in a serverless environment\n// process.env.LANGCHAIN_CALLBACKS_BACKGROUND = \"true\";\n```\n\n----------------------------------------\n\nTITLE: Installing IMSDB Loader Dependencies\nDESCRIPTION: Install required npm packages including LangChain community modules, core functionality, and Cheerio for HTML parsing.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/imsdb.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core cheerio\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Playwright Integration\nDESCRIPTION: Commands to install the required npm packages for using Playwright with LangChain\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/web_playwright.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core playwright\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for LangChain\nDESCRIPTION: Configuration of environment variables for OpenAI API key and optional LangSmith settings. Includes settings for callback handling in non-serverless environments.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/sql_large_db.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=\"your api key\"\n# Uncomment the below to use LangSmith. Not required.\n# export LANGSMITH_API_KEY=\"your api key\"\n# export LANGSMITH_TRACING=true\n\n# Reduce tracing latency if you are not in a serverless environment\n# export LANGCHAIN_CALLBACKS_BACKGROUND=true\n```\n\n----------------------------------------\n\nTITLE: Initializing Conversation Messages Array\nDESCRIPTION: Creates an array of messages including system, human, and AI messages to demonstrate conversation history structure.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/versions/migrating_memory/conversation_buffer_window_memory.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport {\n  AIMessage,\n  HumanMessage,\n  SystemMessage,\n} from \"@langchain/core/messages\";\n\nconst messages = [\n  new SystemMessage(\"you're a good assistant, you always respond with a joke.\"),\n  new HumanMessage(\"i wonder why it's called langchain\"),\n  new AIMessage(\n    'Well, I guess they thought \"WordRope\" and \"SentenceString\" just didn\\'t have the same ring to it!'\n  ),\n  new HumanMessage(\"and who is harrison chasing anyways\"),\n  new AIMessage(\n      \"Hmmm let me think.\\n\\nWhy, he's probably chasing after the last cup of coffee in the office!\"\n  ),\n  new HumanMessage(\"why is 42 always the answer?\"),\n  new AIMessage(\n      \"Because it's the only number that's constantly right, even when it doesn't add up!\"\n  ),\n  new HumanMessage(\"What did the cow say?\"),\n]\n```\n\n----------------------------------------\n\nTITLE: Installing Exa and Core LangChain Packages\nDESCRIPTION: Installs the necessary LangChain packages for Exa integration (`@langchain/exa`) and core functionalities (`@langchain/core`) using npm or yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/exa_search.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n@langchain/exa @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Xata CLI\nDESCRIPTION: Command to install the Xata CLI globally using npm\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/xata.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @xata.io/cli -g\n```\n\n----------------------------------------\n\nTITLE: Installing Anthropic Integration Package\nDESCRIPTION: Command to install the Anthropic integration package and its core dependency\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-anthropic/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/anthropic @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Required LangChain Packages\nDESCRIPTION: Command to install the necessary LangChain packages for working with Xata memory integration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/xata.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Closing MongoDB Client Connection (TypeScript)\nDESCRIPTION: Closes the active MongoDB client connection using the `client.close()` method. This is important to release database resources and ensure graceful shutdown of the application.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/mongodb_atlas.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\nawait client.close();\n```\n\n----------------------------------------\n\nTITLE: Setting IAM Authentication Environment Variables for IBM watsonx.ai\nDESCRIPTION: Sets up environment variables for IAM authentication to access IBM watsonx.ai services by specifying the authentication type and API key.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/ibm.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport WATSONX_AI_AUTH_TYPE=iam\nexport WATSONX_AI_APIKEY=<YOUR-APIKEY>\n```\n\n----------------------------------------\n\nTITLE: Instantiating a Toolkit in TypeScript\nDESCRIPTION: Creates a new instance of the toolkit class with configuration parameters. This is the starting point for using the toolkit functionality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/toolkits.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { __module_name__ } from \"__full_import_path__\"\n\nconst toolkit = new __module_name__({\n  // ...\n})\n```\n\n----------------------------------------\n\nTITLE: Loading Data from a Single GitBook Page\nDESCRIPTION: This code demonstrates how to use the GitbookLoader to load data from a specific GitBook page. It creates a loader instance with a specific URL and loads the documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/gitbook.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { GitbookLoader } from \"@langchain/community/document_loaders/web/gitbook\";\n\nconst loader = new GitbookLoader(\n  \"https://docs.gitbook.com/product-tour/navigation\"\n);\n\nconst docs = await loader.load();\n```\n\n----------------------------------------\n\nTITLE: Importing Convex Database Accessors in TypeScript\nDESCRIPTION: Exports database accessor utilities from the Convex community package for LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/convex.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nexport * from \"@langchain/community/utils/convex\";\n```\n\n----------------------------------------\n\nTITLE: Deleting Documents from FaissStore\nDESCRIPTION: Shows how to remove specific documents from a Faiss vector store by providing their IDs to the delete method.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/faiss.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nawait vectorStore.delete({ ids: [\"4\"] });\n```\n\n----------------------------------------\n\nTITLE: Creating a Chain with Fireworks LLM and Prompt Template\nDESCRIPTION: Example demonstrating how to chain a prompt template with the Fireworks LLM to create a pipeline for processing inputs and generating outputs.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/fireworks.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport { PromptTemplate } from \"@langchain/core/prompts\"\n\nconst prompt = PromptTemplate.fromTemplate(\"How to say {input} in {output_language}:\\n\")\n\nconst chain = prompt.pipe(llm);\nawait chain.invoke(\n  {\n    output_language: \"German\",\n    input: \"I love programming.\",\n  }\n)\n```\n\n----------------------------------------\n\nTITLE: Running Tests for the Package\nDESCRIPTION: Commands to run unit tests and integration tests for the @langchain/google-genai package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-google-genai/README.md#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn test\n$ yarn test:int\n```\n\n----------------------------------------\n\nTITLE: Getting Language Separators in RecursiveCharacterTextSplitter\nDESCRIPTION: Shows how to retrieve the list of separators for a specific programming language using the static getSeparatorsForLanguage() method from RecursiveCharacterTextSplitter.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/code_splitter.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\nimport {\n  RecursiveCharacterTextSplitter,\n} from \"@langchain/textsplitters\";\n\nRecursiveCharacterTextSplitter.getSeparatorsForLanguage(\"js\");\n```\n\n----------------------------------------\n\nTITLE: Importing Partner Package Components in TypeScript\nDESCRIPTION: Illustrates how to import components from a partner package in LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/contributing/integrations.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { X } from \"@langchain/{partner}\";\n```\n\n----------------------------------------\n\nTITLE: Loading Movie Script Using IMSDBLoader\nDESCRIPTION: Example of using the IMSDBLoader to fetch and parse a movie script from IMSDB. The loader takes a URL parameter pointing to the specific script page and returns documents containing the parsed content.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/imsdb.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { IMSDBLoader } from \"@langchain/community/document_loaders/web/imsdb\";\n\nconst loader = new IMSDBLoader(\"https://imsdb.com/scripts/BlacKkKlansman.html\");\n\nconst docs = await loader.load();\n```\n\n----------------------------------------\n\nTITLE: Installing @langchain/textsplitters Package\nDESCRIPTION: Command to install the @langchain/textsplitters package along with its core dependency using npm or yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-textsplitters/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/textsplitters @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Testing Prompt Template\nDESCRIPTION: Demonstrates how to invoke the prompt template with sample text and examples.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/extraction_examples.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { HumanMessage } from \"@langchain/core/messages\";\n\nconst promptValue = await prompt.invoke({\n  text: \"this is some text\",\n  examples: [new HumanMessage(\"testing 1 2 3\")],\n});\n\npromptValue.toChatMessages();\n```\n\n----------------------------------------\n\nTITLE: Loading SemanticSimilarityExampleSelector from Existing VectorStore\nDESCRIPTION: This snippet shows how to initialize a SemanticSimilarityExampleSelector using a pre-existing vector store and add more examples to it.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/example_selectors_similarity.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport ExampleSimilarityFromExisting from \"@examples/prompts/semantic_similarity_example_selector_from_existing.ts\";\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith API Key in Bash\nDESCRIPTION: Sets the LANGSMITH_TRACING and LANGSMITH_API_KEY environment variables for automated tracing of model calls.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/anthropic.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Importing and Using DocCardList Component in JSX\nDESCRIPTION: Imports and renders a DocCardList theme component to display documentation cards in a list format.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/self_query/index.mdx#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport DocCardList from \"@theme/DocCardList\";\n\n<DocCardList />\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Tavily API\nDESCRIPTION: Sets up the Tavily API key as an environment variable for web search functionality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_tools.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.TAVILY_API_KEY = \"YOUR_API_KEY\";\n```\n\n----------------------------------------\n\nTITLE: Running All Integration Tests in LangChain.js\nDESCRIPTION: Command to execute all integration tests (*.int.test.ts files) in the project. Integration tests cover logic requiring external API calls and service integration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/contributing/testing.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nyarn test:integration\n```\n\n----------------------------------------\n\nTITLE: Defining Encoder and Decoder for __module_name__\nDESCRIPTION: Sets up TextEncoder and TextDecoder for converting data to Uint8Array and back. This is necessary for encoding and decoding data stored in the key-value store.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/kv_store.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst encoder = new TextEncoder();\nconst decoder = new TextDecoder();\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith Environment Variables for Observability\nDESCRIPTION: Configures LangSmith environment variables for tracing and observability of LangChain operations.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/tavily_search.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.LANGSMITH_TRACING=\"true\"\nprocess.env.LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Building the @langchain/ollama package\nDESCRIPTION: Commands to build the package, either from the package directory or the repo root.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-ollama/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nyarn build\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn build --filter=@langchain/ollama\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Dependencies\nDESCRIPTION: Command to install required LangChain packages including OpenAI integration and core functionality\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/xata.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Necessary Packages via npm\nDESCRIPTION: This snippet installs the necessary npm packages required to work with Neo4j and Langchain. Users need to have npm installed to execute these commands.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/neo4jvector.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install neo4j-driver\n```\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Running Linter and Formatter\nDESCRIPTION: Command to run the linter and formatter to ensure code quality and style consistency for the @langchain/qdrant package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-qdrant/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nyarn lint && yarn format\n```\n\n----------------------------------------\n\nTITLE: Langchain Tool Response Structure with AIMessageChunk\nDESCRIPTION: Shows the structure of a Langchain AIMessageChunk constructor with content payload, additional metadata, and tool call information. This represents the assistant's response containing both text content and a calculator tool use action.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_21\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"lc\": 1,\n  \"type\": \"constructor\",\n  \"id\": [\n    \"langchain_core\",\n    \"messages\",\n    \"AIMessageChunk\"\n  ],\n  \"kwargs\": {\n    \"content\": [\n      {\n        \"type\": \"text\",\n        \"text\": \"Based on the search results, the 2023 film Oppenheimer was directed by Christopher Nolan. Some key information about Christopher Nolan:\\n\\n- He is a British-American film director, producer and screenwriter.\\n- He was born on July 30, 1970, making him currently 52 years old.\\n\\nTo calculate his age in days:\"\n      },\n      {\n        \"type\": \"tool_use\",\n        \"id\": \"toolu_01NVTbm5aNYSm1wGYb6XF7jE\",\n        \"name\": \"calculator\",\n        \"input\": {\n          \"input\": \"52 * 365\"\n        }\n      }\n    ],\n    \"additional_kwargs\": {\n      \"id\": \"msg_01RBDqmJKNXiEjgt5Xrng4mz\",\n      \"type\": \"message\",\n      \"role\": \"assistant\",\n      \"model\": \"claude-3-sonnet-20240229\",\n      \"stop_sequence\": null,\n      \"usage\": {\n        \"input_tokens\": 2810,\n        \"output_tokens\": 137\n      },\n      \"stop_reason\": \"tool_use\"\n    },\n    \"tool_call_chunks\": [\n      {\n        \"name\": \"calculator\",\n        \"args\": \"{\\\"input\\\":\\\"52 * 365\\\"}\",\n        \"id\": \"toolu_01NVTbm5aNYSm1wGYb6XF7jE\",\n        \"index\": 0\n      }\n    ],\n    \"tool_calls\": [\n      {\n        \"name\": \"calculator\",\n        \"args\": {\n          \"input\": \"52 * 365\"\n        },\n        \"id\": \"toolu_01NVTbm5aNYSm1wGYb6XF7jE\"\n      }\n    ],\n    \"invalid_tool_calls\": [],\n    \"response_metadata\": {}\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Tracking Token Usage with AIMessage.response_metadata for Anthropic\nDESCRIPTION: Example showing how to access token usage information from the response_metadata field of AIMessage objects when using Anthropic models.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chat_token_usage_tracking.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport { HumanMessage, AIMessage } from \"@langchain/core/messages\";\n\nconst model = new ChatAnthropic({\n  modelName: \"claude-2\",\n});\n\nconst response = await model.invoke([new HumanMessage(\"Hello world!\")]);\n\nconsole.log(JSON.stringify(response.response_metadata, null, 2));\n/*\n{\n  \"usage\": {\n    \"input_tokens\": 13,\n    \"output_tokens\": 14\n  },\n  \"anthropic_version\": \"bedrock-2023-05-31\",\n  \"stop_reason\": \"end_turn\",\n  \"stop_sequence\": null,\n  \"model\": \"anthropic.claude-instant-v1\"\n}\n*/\n\n// Accessing the input and output tokens for a specific request\nconsole.log(\n  `Input tokens: ${response.response_metadata?.usage?.input_tokens}`\n);\nconsole.log(\n  `Output tokens: ${response.response_metadata?.usage?.output_tokens}`\n);\n\n```\n\n----------------------------------------\n\nTITLE: Installing Cheerio for HTML Scraping\nDESCRIPTION: Command to install the cheerio package for HTML scraping functionality in the examples.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_transformers/mozilla_readability.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install cheerio\n```\n\n----------------------------------------\n\nTITLE: Installing ClickHouse Dependencies for LangChain.js\nDESCRIPTION: Command for installing the required dependencies for using ClickHouse with LangChain.js. This includes the ClickHouse client and MySQL2 libraries that are needed as peer dependencies.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/clickhouse.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -S @clickhouse/client mysql2\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Dependencies for Zep Integration\nDESCRIPTION: Command to install required LangChain packages for working with Zep and OpenAI integrations.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/zep_memory.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing required dependencies for Azure Blob Storage integration\nDESCRIPTION: Command to install the necessary packages for integrating Azure Blob Storage with LangChain, including the community package, core package, and Azure Storage Blob client library.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/azure_blob_storage_file.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core @azure/storage-blob\n```\n\n----------------------------------------\n\nTITLE: Disabling Streaming Usage Metadata for Compatibility\nDESCRIPTION: Shows how to disable streaming usage metadata for compatibility with proxies or third-party providers that don't support this OpenAI feature.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/openai.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst llmWithoutStreamUsage = new ChatOpenAI({\n  temperature: 0.9,\n  streamUsage: false,\n  configuration: {\n    baseURL: \"https://proxy.com\",\n  },\n});\n\nawait llmWithoutStreamUsage.invoke(\"Hi there!\");\n```\n\n----------------------------------------\n\nTITLE: Linting and Formatting Code\nDESCRIPTION: Bash command to run the linter and formatter to ensure code quality and consistency for the @langchain/mistralai package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-mistralai/README.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nyarn lint && yarn format\n```\n\n----------------------------------------\n\nTITLE: Comparing Import Syntax between Node.js and Deno\nDESCRIPTION: Example showing the difference in import syntax between Node.js and Deno environments. When writing notebooks in the LangChain repository, imports must follow Deno's syntax which requires updating the import map in deno.json.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/contributing/code.mdx#2025-04-22_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\n// Import in Node:\nimport { z } from \"zod\";\n// Import in Deno:\nimport { z } from \"npm:/zod\";\n```\n\n----------------------------------------\n\nTITLE: Logging ChatFireworks Response\nDESCRIPTION: Example of logging the content of the AI message response.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/fireworks.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconsole.log(aiMsg.content)\n```\n\n----------------------------------------\n\nTITLE: LangSmith Tracing Configuration\nDESCRIPTION: Optional configuration for enabling LangSmith tracing functionality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/bedrock-knowledge-bases.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n// process.env.LANGSMITH_API_KEY = \"<YOUR API KEY HERE>\";\n// process.env.LANGSMITH_TRACING = \"true\";\n```\n\n----------------------------------------\n\nTITLE: TavilySearchResults Tool Input for Christopher Nolan Age Query\nDESCRIPTION: Input string sent to the TavilySearchResults tool to query information about Christopher Nolan's age.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_45\n\nLANGUAGE: json\nCODE:\n```\n\"Christopher Nolan age\"\n```\n\n----------------------------------------\n\nTITLE: Installing GOAT Plugin in Bash\nDESCRIPTION: This snippet installs the `@goat-sdk/plugin-erc20` package, which is an ERC-20 token plugin for the GOAT toolkit. It requires NPM for the installation and internet connectivity to fetch the package. The installation integrates ERC-20 support within the toolkit.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/goat.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @goat-sdk/plugin-erc20\n```\n\n----------------------------------------\n\nTITLE: Installing Azure Dynamic Sessions Package\nDESCRIPTION: Command to install the @langchain/azure-dynamic-sessions package along with its dependency @langchain/core using npm, yarn, or other package managers.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-azure-dynamic-sessions/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/azure-dynamic-sessions @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatAnthropic Model\nDESCRIPTION: Creates a new instance of the ChatAnthropic model class for interacting with Anthropic's API\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/anthropic.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nconst model = new ChatAnthropic({});\n```\n\n----------------------------------------\n\nTITLE: Building Package from Repository Root\nDESCRIPTION: Command to build the @langchain/cohere package specifically from the repository root using the filter flag.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-cohere/README.md#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nyarn build --filter=@langchain/cohere\n```\n\n----------------------------------------\n\nTITLE: Installing xAI Package Dependencies\nDESCRIPTION: Commands to install the required npm packages for using xAI with LangChain\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-xai/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/xai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Example CSV Content Format\nDESCRIPTION: Sample CSV file content showing ID and HTML columns with separator characters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/csv.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: csv\nCODE:\n```\nid｜html\n1｜\"<i>Corruption discovered at the core of the Banking Clan!</i>\"\n2｜\"<i>Reunited, Rush Clovis and Senator Amidala</i>\"\n3｜\"<i>discover the full extent of the deception.</i>\"\n4｜\"<i>Anakin Skywalker is sent to the rescue!</i>\"\n```\n\n----------------------------------------\n\nTITLE: Using Constructor Callbacks in LangChain\nDESCRIPTION: Example of passing callbacks to a chain constructor. These callbacks are scoped only to the object they are defined on and are not inherited by children.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/callbacks.mdx#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst chain = new TheNameOfSomeChain({ callbacks: [handler] })\n```\n\n----------------------------------------\n\nTITLE: Setting up environment variables for Anthropic API\nDESCRIPTION: Sets the Anthropic API key as an environment variable for authentication with the Anthropic API service.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/versions/migrating_memory/chat_history.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.ANTHROPIC_API_KEY = 'YOUR_API_KEY'\n```\n\n----------------------------------------\n\nTITLE: Importing ChatOpenAI in Deno\nDESCRIPTION: Examples of importing ChatOpenAI in Deno environment.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/tests/__mdx__/modules/index.mdx#2025-04-22_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"https://esm.sh/@langchain/openai\";\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"npm:@langchain/openai\";\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Community Package\nDESCRIPTION: Command to install the LangChain community package with third-party integrations.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/installation.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Package.json Configuration for Dependency Resolution\nDESCRIPTION: Configuration to ensure consistent @langchain/core dependency version across different package managers\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-anthropic/README.md#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"your-project\",\n  \"version\": \"0.0.0\",\n  \"dependencies\": {\n    \"@langchain/anthropic\": \"^0.0.9\",\n    \"@langchain/core\": \"^0.3.0\"\n  },\n  \"resolutions\": {\n    \"@langchain/core\": \"^0.3.0\"\n  },\n  \"overrides\": {\n    \"@langchain/core\": \"^0.3.0\"\n  },\n  \"pnpm\": {\n    \"overrides\": {\n      \"@langchain/core\": \"^0.3.0\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Community and Core Packages\nDESCRIPTION: Command to install the required LangChain packages for using Gradient AI integration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/gradient_ai.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Invoking a Tool Without Context Variables in TypeScript\nDESCRIPTION: Attempts to invoke the updateFavoritePets tool without setting the required context variable. This will throw an error since userId is undefined.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_runtime.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nawait updateFavoritePets.invoke({ pets: [\"cat\", \"dog\" ]})\n```\n\n----------------------------------------\n\nTITLE: Basic XML Generation with Anthropic Claude\nDESCRIPTION: Demonstrates basic XML generation using ChatAnthropic model to create a filmography for Tom Hanks with movie tags.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/output_parser_xml.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst model = new ChatAnthropic({\n  model: \"claude-3-sonnet-20240229\",\n  maxTokens: 512,\n  temperature: 0.1,\n});\n\nconst query = `Generate the shortened filmograph for Tom Hanks.`;\n\nconst result = await model.invoke(query + ` Please enclose the movies in \"movie\" tags.`);\n\nconsole.log(result.content);\n```\n\n----------------------------------------\n\nTITLE: Installing TensorFlow Embeddings Dependencies\nDESCRIPTION: Command to install necessary packages for using TensorFlow embeddings with LangChain in a JavaScript project. It includes core LangChain packages, TensorFlow.js, and the Universal Sentence Encoder model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/tensorflow.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core @tensorflow/tfjs-core@3.6.0 @tensorflow/tfjs-converter@3.6.0 @tensorflow-models/universal-sentence-encoder@1.3.3 @tensorflow/tfjs-backend-cpu\n```\n\n----------------------------------------\n\nTITLE: Creating Example Data for English to Italian Translation\nDESCRIPTION: Initializes an array of example translations from English to Italian to be used with an example selector.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/example_selectors.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nconst examples = [\n    { input: \"hi\", output: \"ciao\" },\n    { input: \"bye\", output: \"arrivaderci\" },\n    { input: \"soccer\", output: \"calcio\" },\n];\n```\n\n----------------------------------------\n\nTITLE: Installing Momento SDK for Node.js\nDESCRIPTION: Installs the Momento SDK package specifically designed for Node.js environments using npm. This package is required to interact with the Momento Vector Index from a Node.js application.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/momento_vector_index.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @gomomento/sdk\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies and Reading Image File in Node.js\nDESCRIPTION: This snippet demonstrates how to import necessary modules and read an image file using Node.js file system promises API. It sets up the ChatAnthropic model for later use.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/multimodal_inputs.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport * as fs from \"node:fs/promises\";\n\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst model = new ChatAnthropic({\n  model: \"claude-3-sonnet-20240229\",\n});\n\nconst imageData = await fs.readFile(\"../../../../examples/hotdog.jpg\");\n```\n\n----------------------------------------\n\nTITLE: High Cardinality Column Management\nDESCRIPTION: TypeScript code referenced from external file for managing high-cardinality columns in databases.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/sql_large_db.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\n<CodeBlock language=\"typescript\">{HighCardinalityExample}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Setting xAI API Environment Variable\nDESCRIPTION: Command to set the xAI API key as an environment variable\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-xai/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport XAI_API_KEY=\n```\n\n----------------------------------------\n\nTITLE: Example of Customizing Integration Tests for OpenAI\nDESCRIPTION: Shows how to extend the ChatModelIntegrationTests class to customize testing behavior, specifically demonstrating how to override a test method to provide call options for streaming tokens in OpenAI.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-standard-tests/README.md#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nasync testUsageMetadataStreaming() {\n  // ChatOpenAI does not support streaming tokens by\n  // default, so we must pass in a call option to\n  // enable streaming tokens.\n  const callOptions: ChatOpenAI[\"ParsedCallOptions\"] = {\n    stream_options: {\n      include_usage: true,\n    },\n  };\n  await super.testUsageMetadataStreaming(callOptions);\n}\n```\n\n----------------------------------------\n\nTITLE: Setting up LangSmith Tracing Configuration\nDESCRIPTION: Optional configuration for enabling automated tracing of model calls using LangSmith. Shows how to set environment variables for LangSmith tracing and API key.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/vectorstores.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n// process.env.LANGSMITH_TRACING=\"true\"\n// process.env.LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Installing Gradient AI Node SDK for LangChain\nDESCRIPTION: Command to install the official Gradient Node SDK as a peer dependency for use with LangChain.js.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/gradient_ai.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @gradientai/nodejs-sdk\n```\n\n----------------------------------------\n\nTITLE: Updating GitHub Actions Workflow for Integration Testing\nDESCRIPTION: YAML snippet showing how to update the GitHub Actions workflow to include a new integration package in CI testing.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/CONTRIBUTING.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nprepare-matrix:\n  needs: get-changed-files\n  runs-on: ubuntu-latest\n  env:\n    PACKAGES: \"anthropic,azure-openai,cloudflare,<your-package>\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Visualizing the LangGraph Structure with Mermaid\nDESCRIPTION: Demonstrates how to visualize the LangGraph structure using Mermaid in a Jupyter notebook environment with tslab. This provides a graphical representation of the workflow.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/summarization.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\n// Note: tslab only works inside a jupyter notebook. Don't worry about running this code yourself!\nimport * as tslab from \"tslab\";\n\nconst image = await app.getGraph().drawMermaidPng();\nconst arrayBuffer = await image.arrayBuffer();\n\nawait tslab.display.png(new Uint8Array(arrayBuffer));\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain.js Core and Integration Packages\nDESCRIPTION: Command for installing the core LangChain.js packages needed for the ClickHouse integration, including OpenAI for embeddings, the community package for vector stores, and the core LangChain functionality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/clickhouse.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Building from Repository Root\nDESCRIPTION: Command to build the @langchain/groq package specifically when in the repository root directory.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-groq/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nyarn build --filter=@langchain/groq\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key\nDESCRIPTION: Command to set the OpenAI API key as an environment variable.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-openai/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=your-api-key\n```\n\n----------------------------------------\n\nTITLE: Extracting Taskade Project ID from URL\nDESCRIPTION: This snippet shows how to find the Taskade project ID by examining the project URL in a web browser.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/taskade.mdx#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nhttps://www.taskade.com/d/<YOUR PROJECT ID HERE>\n```\n\n----------------------------------------\n\nTITLE: Installing @langchain/scripts Package via Package Manager\nDESCRIPTION: Command to install the @langchain/scripts package using npm or yarn. This package contains shared scripts used across LangChain.js packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/scripts\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for LangSmith Tracing\nDESCRIPTION: Configuration of environment variables for enabling LangSmith tracing with Amazon Bedrock.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/bedrock.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for LangChain.js Project\nDESCRIPTION: This snippet shows the command to install necessary dependencies for the project using npm or yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_no_queries.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n@langchain/community @langchain/openai @langchain/core zod chromadb\n```\n\n----------------------------------------\n\nTITLE: Importing ChatOpenAI in CommonJS\nDESCRIPTION: Example of importing ChatOpenAI in a CommonJS environment.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/tests/__mdx__/modules/index.mdx#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst { ChatOpenAI } = require(\"@langchain/openai\");\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Dependencies\nDESCRIPTION: Command to install necessary LangChain packages for working with embeddings and retrievers.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/multi_vector.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith Tracing Environment Variables (TypeScript)\nDESCRIPTION: Demonstrates (via commented-out code) how to assign LangSmith API key and tracing flag to environment variables in TypeScript for enabling automated tracing of model calls within the Langchain framework.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/mongodb_atlas.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\n// process.env.LANGSMITH_TRACING=\"true\"\n// process.env.LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Configuring PostgreSQL Vector Store\nDESCRIPTION: Example demonstrating how to initialize and configure a PostgreSQL Vector Store with custom column mappings and metadata.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-google-cloud-sql-pg/README.md#2025-04-22_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconst pvectorArgs: PostgresVectorStoreArgs = {\n    idColumn: \"ID_COLUMN\",\n    contentColumn: \"CONTENT_COLUMN\",\n    embeddingColumn: \"EMBEDDING_COLUMN\",\n    metadataColumns: [\"page\", \"source\"]\n}\n\nconst vectorStoreInstance = await PostgresVectorStore.initialize(engine, embeddingService, \"my-table\", pvectorArgs)\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain for TextLoader\nDESCRIPTION: Instructions for installing the langchain package which contains the TextLoader document loader.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/text.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n  langchain\n```\n\n----------------------------------------\n\nTITLE: Installing LangGraph Package using MDX Component\nDESCRIPTION: Shows the command to install the `@langchain/langgraph` package using npm or yarn, displayed within an MDX component. LangGraph is required for creating agents like the React agent shown later.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/ibm.mdx#2025-04-22_snippet_6\n\nLANGUAGE: mdx\nCODE:\n```\n<Npm2Yarn>\n  @langchain/langgraph\n</Npm2Yarn>\n```\n\n----------------------------------------\n\nTITLE: Installing Writer SDK for LangChain.js\nDESCRIPTION: This snippet shows how to install the official Writer SDK package as a peer dependency using yarn or npm.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/llms/writer.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nyarn add @writerai/writer-sdk\n```\n\n----------------------------------------\n\nTITLE: Importing SessionsPythonREPLTool from Azure Dynamic Sessions Package\nDESCRIPTION: This TypeScript code snippet demonstrates how to import the SessionsPythonREPLTool from the Azure Dynamic Sessions package. This tool likely provides functionality for interacting with Python REPL sessions in Azure Container Apps.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/platforms/microsoft.mdx#2025-04-22_snippet_17\n\nLANGUAGE: typescript\nCODE:\n```\nimport { SessionsPythonREPLTool } from \"@langchain/azure-dynamic-sessions\";\n```\n\n----------------------------------------\n\nTITLE: Setting AWS Environment Variables\nDESCRIPTION: Configuration of required AWS environment variables for AmazonKnowledgeBaseRetriever setup.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/bedrock-knowledge-bases.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nprocess.env.AWS_KNOWLEDGE_BASE_ID=your-knowledge-base-id\nprocess.env.AWS_ACCESS_KEY_ID=your-access-key-id\nprocess.env.AWS_SECRET_ACCESS_KEY=your-secret-access-key\n```\n\n----------------------------------------\n\nTITLE: Installing Azure Blob Storage Dependencies\nDESCRIPTION: Command to install required npm packages for Azure Blob Storage integration including LangChain community package, core package, and Azure Storage Blob client library.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/azure_blob_storage_container.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core @azure/storage-blob\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Azure CosmosDB and LangChain (bash)\nDESCRIPTION: Installs the @langchain/azure-cosmosdb and @langchain/core packages using npm. This step is a prerequisite for any development interfacing LangChain with Azure Cosmos DB for NoSQL. The environment must have Node.js and npm installed. The primary dependencies are required for all example code to function correctly.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/azure_cosmosdb_nosql.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/azure-cosmosdb @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Google Routes Tool in Node.js\nDESCRIPTION: Command to install the required packages for using the Google Routes Tool with LangChain.js, including OpenAI integration, community tools, and core LangChain functionality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/google_routes.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Deleting a Saved HNSWLib Index\nDESCRIPTION: Shows how to delete a previously saved HNSWLib index from disk. This is useful for cleaning up or replacing an outdated vector store.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/hnswlib.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n// Load the vector store from the same directory\nconst savedVectorStore = await HNSWLib.load(directory, new OpenAIEmbeddings());\n\nawait savedVectorStore.delete({ directory });\n```\n\n----------------------------------------\n\nTITLE: Installing Development Dependencies\nDESCRIPTION: Command to install package dependencies for development.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-weaviate/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nyarn install\n```\n\n----------------------------------------\n\nTITLE: Directory Structure Example for Document Loading\nDESCRIPTION: Shows the structure of an example directory containing different file types (JSON, JSONL, TXT, CSV) that will be loaded by the DirectoryLoader.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/document_loader_directory.mdx#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nsrc/document_loaders/example_data/example/\n├── example.json\n├── example.jsonl\n├── example.txt\n└── example.csv\n```\n\n----------------------------------------\n\nTITLE: Running All Unit Tests in LangChain.js\nDESCRIPTION: Command to execute all unit tests (*.test.ts files) in the project. Unit tests cover modular logic that doesn't require external API calls.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/contributing/testing.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nyarn test\n```\n\n----------------------------------------\n\nTITLE: Implementing Stateful Message History with LangGraph\nDESCRIPTION: Sets up a basic LangGraph application with state management for chat history using an in-memory checkpointer. Defines the state interface and workflow for handling chat messages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport { AIMessage, BaseMessage, HumanMessage } from \"@langchain/core/messages\";\nimport { StateGraph, START, END, MemorySaver, messagesStateReducer, Annotation } from \"@langchain/langgraph\";\n\nconst GraphAnnotation = Annotation.Root({\n  input: Annotation<string>(),\n  chat_history: Annotation<BaseMessage[]>({\n    reducer: messagesStateReducer,\n    default: () => [],\n  }),\n  context: Annotation<string>(),\n  answer: Annotation<string>(),\n})\n\nasync function callModel(state: typeof GraphAnnotation.State) {\n  const response = await ragChain.invoke(state);\n  return {\n    chat_history: [\n      new HumanMessage(state.input),\n      new AIMessage(response.answer),\n    ],\n    context: response.context,\n    answer: response.answer,\n  };\n}\n\nconst workflow = new StateGraph(GraphAnnotation)\n  .addNode(\"model\", callModel)\n  .addEdge(START, \"model\")\n  .addEdge(\"model\", END);\n\nconst memory = new MemorySaver();\nconst app = workflow.compile({ checkpointer: memory });\n```\n\n----------------------------------------\n\nTITLE: Installing Cerebras Package for LangChain.js\nDESCRIPTION: Command to install the @langchain/cerebras package along with the required @langchain/core dependency using npm or yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-cerebras/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/cerebras @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Building the Nomic Package\nDESCRIPTION: Commands to build the @langchain/nomic package, including both local and repository root options.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-nomic/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nyarn build\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn build --filter=@langchain/nomic\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain OpenAI Integration Package\nDESCRIPTION: Command to install the @langchain/openai package as a dependency for using JigsawStack tools with LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/jigsawstack.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Credentials\nDESCRIPTION: Example of setting environment variables for credentials, including LangSmith tracing.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/tools.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env....\n\nprocess.env.LANGSMITH_TRACING=\"true\"\nprocess.env.LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Initialize Chat Model for LangChain.js Toolkit\nDESCRIPTION: Imports and initialize the ChatOpenAI model, specifying model configuration like 'gpt-4o-mini' with a set temperature for generating responses. Dependencies include '@langchain/openai'.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/openapi.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o-mini\",\n  temperature: 0,\n})\n```\n\n----------------------------------------\n\nTITLE: Example - Creating Python Code Interpreter Session - TypeScript\nDESCRIPTION: Illustrates how to create and invoke a Python code interpreter session using Azure OpenAI chat model in TypeScript for executing code and retrieving results. Requires running instance of code interpreter session pool and proper setup of Azure Entra authentication.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/azure_dynamic_sessions.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport Example from \"@examples/tools/azure_dynamic_sessions/azure_dynamic_sessions.ts\";\n\n<CodeBlock language=\"typescript\">{Example}</CodeBlock>\n```\n\n----------------------------------------\n\nTITLE: Deleting Documents from SupabaseVectorStore (TypeScript)\nDESCRIPTION: This snippet demonstrates how to delete specific documents from the `SupabaseVectorStore` by providing a list of their unique IDs.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/supabase.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nawait vectorStore.delete({ ids: [\"4\"] });\n```\n\n----------------------------------------\n\nTITLE: Configuring LangSmith Environment\nDESCRIPTION: Set up environment variables for LangSmith tracing and API key configuration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport LANGSMITH_TRACING=true\nexport LANGSMITH_API_KEY=YOUR_KEY\n```\n\n----------------------------------------\n\nTITLE: Importing Custom Components in Markdown (JSX)\nDESCRIPTION: This code snippet imports custom React components (CategoryTable and IndexTable) to be used within the Markdown documentation. These components likely render tables displaying toolkit information.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/index.mdx#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport { CategoryTable, IndexTable } from \"@theme/FeatureTables\";\n```\n\n----------------------------------------\n\nTITLE: Initializing Xata Project\nDESCRIPTION: Command to initialize a Xata project in your application directory\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/xata.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nxata init\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for ChatGroq API\nDESCRIPTION: Instructions for setting up the required API keys for ChatGroq and optional LangSmith integration via environment variables.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/groq.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport GROQ_API_KEY=\"your-api-key\"\n```\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Frontend Client for Consuming Event Streams\nDESCRIPTION: This frontend implementation uses the @microsoft/fetch-event-source package to fetch and parse server-sent events. It sets up a request with appropriate handlers for messages and errors.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/streaming.ipynb#2025-04-22_snippet_21\n\nLANGUAGE: typescript\nCODE:\n```\nimport { fetchEventSource } from \"@microsoft/fetch-event-source\";\n\nconst makeChainRequest = async () => {\n  await fetchEventSource(\"https://your_url_here\", {\n    method: \"POST\",\n    body: JSON.stringify({\n      foo: 'bar'\n    }),\n    onmessage: (message) => {\n      if (message.event === \"data\") {\n        console.log(message.data);\n      }\n    },\n    onerror: (err) => {\n      console.log(err);\n    }\n  });\n};\n```\n\n----------------------------------------\n\nTITLE: Configuring Package Dependencies in package.json\nDESCRIPTION: Example package.json configuration to ensure all LangChain packages use the same instance of @langchain/core dependency across different package managers.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-azure-dynamic-sessions/README.md#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"your-project\",\n  \"version\": \"0.0.0\",\n  \"dependencies\": {\n    \"@langchain/azure-openai\": \"^0.0.4\",\n    \"@langchain/core\": \"^0.3.0\"\n  },\n  \"resolutions\": {\n    \"@langchain/core\": \"^0.3.0\"\n  },\n  \"overrides\": {\n    \"@langchain/core\": \"^0.3.0\"\n  },\n  \"pnpm\": {\n    \"overrides\": {\n      \"@langchain/core\": \"^0.3.0\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith Tracing Environment Variables\nDESCRIPTION: Optional environment variables for enabling automated tracing of model calls with LangSmith.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/azure.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Agent Tool Selection JSON Log\nDESCRIPTION: Log output showing the agent's selected action to use the Tavily search tool, including the tool call configuration, message history, and execution metadata.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"tool\": \"tavily_search_results_json\",\n  \"toolInput\": {\n    \"input\": \"Oppenheimer 2023 film director age\"\n  },\n  \"toolCallId\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n  \"log\": \"Invoking \\\"tavily_search_results_json\\\" with {\\\"input\\\":\\\"Oppenheimer 2023 film director age\\\"}\\n[{\\\"type\\\":\\\"tool_use\\\",\\\"id\\\":\\\"toolu_01NUVejujVo2y8WGVtZ49KAN\\\",\\\"name\\\":\\\"tavily_search_results_json\\\",\\\"input\\\":{\\\"input\\\":\\\"Oppenheimer 2023 film director age\\\"}}]\",\n  \"messageLog\": [\n    {\n      \"lc\": 1,\n      \"type\": \"constructor\",\n      \"id\": [\n        \"langchain_core\",\n        \"messages\",\n        \"AIMessageChunk\"\n      ],\n      \"kwargs\": {\n        \"content\": [\n          {\n            \"type\": \"tool_use\",\n            \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n            \"name\": \"tavily_search_results_json\",\n            \"input\": {\n              \"input\": \"Oppenheimer 2023 film director age\"\n            }\n          }\n        ],\n        \"additional_kwargs\": {\n          \"id\": \"msg_015MqAHr84dBCAqBgjou41Km\",\n          \"type\": \"message\",\n          \"role\": \"assistant\",\n          \"model\": \"claude-3-sonnet-20240229\",\n          \"stop_sequence\": null,\n          \"usage\": {\n            \"input_tokens\": 409,\n            \"output_tokens\": 68\n          },\n          \"stop_reason\": \"tool_use\"\n        },\n        \"tool_call_chunks\": [\n          {\n            \"name\": \"tavily_search_results_json\",\n            \"args\": \"{\\\"input\\\":\\\"Oppenheimer 2023 film director age\\\"}\",\n            \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\",\n            \"index\": 0\n          }\n        ],\n        \"tool_calls\": [\n          {\n            \"name\": \"tavily_search_results_json\",\n            \"args\": {\n              \"input\": \"Oppenheimer 2023 film director age\"\n            },\n            \"id\": \"toolu_01NUVejujVo2y8WGVtZ49KAN\"\n          }\n        ],\n        \"invalid_tool_calls\": [],\n        \"response_metadata\": {}\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Packages using npm\nDESCRIPTION: The snippet demonstrates how to install the required LangChain packages using npm. It installs '@langchain/openai' and '@langchain/core', which are necessary for the integration with Zapier NLA. These packages are essential for connecting and utilizing OpenAI and LangChain functionalities within your project.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/zapier_agent.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Browser-Specific Dependencies for Tencent Hunyuan\nDESCRIPTION: Command to install the crypto-js package, which is required when using LangChain.js with Tencent Hunyuan in a browser environment.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/tencent_hunyuan.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crypto-js\n```\n\n----------------------------------------\n\nTITLE: Parsing JSON Tool Call for Calculator in JavaScript\nDESCRIPTION: This JSON snippet represents a tool call to a calculator function, specifically to multiply 52 by 365. It includes input parameters and metadata about the API call.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_29\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"toolu_01NVTbm5aNYSm1wGYb6XF7jE\",\n  \"name\": \"calculator\",\n  \"input\": {\n    \"input\": \"52 * 365\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Cloudflare Worker in TOML\nDESCRIPTION: Example wrangler.toml configuration for setting up a Cloudflare worker with AI and vectorize bindings.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/cloudflare_ai.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\nname = \"langchain-test\"\nmain = \"worker.js\"\ncompatibility_date = \"2024-01-10\"\n\n[[vectorize]]\nbinding = \"VECTORIZE_INDEX\"\nindex_name = \"langchain-test\"\n\n[ai]\nbinding = \"AI\"\n```\n\n----------------------------------------\n\nTITLE: Building the @langchain/groq Package\nDESCRIPTION: Commands to build the @langchain/groq package, both from the package directory and from the repository root.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-groq/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nyarn build\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Anthropic Integration\nDESCRIPTION: Command to install the LangChain Anthropic integration package via npm.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_citations.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/anthropic\n```\n\n----------------------------------------\n\nTITLE: Linting and Formatting Code\nDESCRIPTION: Command to run the linter and formatter to ensure code quality standards are met for the package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-groq/README.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nyarn lint && yarn format\n```\n\n----------------------------------------\n\nTITLE: Installing Baidu Qianfan package for LangChain.js\nDESCRIPTION: Command to install the Baidu Qianfan integration package along with the required core package for LangChain.js using NPM or Yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-baidu-qianfan/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/baidu-qianfan @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Running Linter and Formatter\nDESCRIPTION: Command to run the linter and formatter on the codebase to ensure code quality and consistent formatting.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-google-genai/README.md#2025-04-22_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nyarn lint && yarn format\n```\n\n----------------------------------------\n\nTITLE: Parsing Movie Search Results JSON\nDESCRIPTION: JSON structure containing search results with information about the Oppenheimer movie, including titles, URLs, content descriptions and relevance scores.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_32\n\nLANGUAGE: json\nCODE:\n```\n[{\"title\":\"Oppenheimer (2023) - IMDb\",\"url\":\"https://www.imdb.com/title/tt15398776/\",\"content\":\"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.\",\"score\":0.96643,\"raw_content\":null}]\n```\n\n----------------------------------------\n\nTITLE: Invoking the Chain for Multiple Image Comparison\nDESCRIPTION: This snippet shows how to invoke the chain for multiple image comparison with two base64-encoded image data inputs and log the response content.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/multimodal_prompts.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: javascript\nCODE:\n```\nconst res = await chainWithMultipleImages.invoke({ imageData1: base64, imageData2: base64 })\nconsole.log(res.content)\n```\n\n----------------------------------------\n\nTITLE: Installing Development Dependencies\nDESCRIPTION: Command to install development dependencies for the @langchain/textsplitters package using yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-textsplitters/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyarn install\n```\n\n----------------------------------------\n\nTITLE: Changing Directory to LangChain Community Package\nDESCRIPTION: Bash command to change the current directory to the LangChain community package for working on integrations.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/CONTRIBUTING.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd libs/langchain-community\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain.js with Yarn\nDESCRIPTION: Command to install LangChain.js using Yarn package manager.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-community/src/document_loaders/tests/example_data/notion/notion.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nyarn add langchain\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith Environment Variables (TypeScript)\nDESCRIPTION: Sets environment variables for enabling LangSmith tracing and providing the LangSmith API key in a TypeScript/JavaScript project. This is optional but recommended for observability.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/exa_search.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.LANGSMITH_TRACING=\"true\"\nprocess.env.LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Hooks for MistralAI\nDESCRIPTION: Demonstrates how to define and add custom hooks for beforeRequest, requestError, and response events in MistralAI.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/mistralai.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nconst beforeRequestHook = (req: Request): Request | void | Promise<Request | void> => {\n    // Code to run before a request is processed by Mistral\n};\n\nconst requestErrorHook = (err: unknown, req: Request): void | Promise<void> => {\n    // Code to run when an error occurs as Mistral is processing a request\n};\n\nconst responseHook = (res: Response, req: Request): void | Promise<void> => {\n    // Code to run before Mistral sends a successful response\n};\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variable for Blocking Callbacks in TypeScript\nDESCRIPTION: Example showing how to make all callbacks blocking by setting the LANGCHAIN_CALLBACKS_BACKGROUND environment variable to 'false', which can be an alternative to using awaitAllCallbacks().\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/versions/v0_3/index.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.LANGCHAIN_CALLBACKS_BACKGROUND = \"false\";\n\nconst startTimeBlocking = new Date().getTime();\n\nawait runnable.invoke({ number: \"2\" }, { callbacks: [customHandler] });\n\nconsole.log(\n  `Initial elapsed time: ${new Date().getTime() - startTimeBlocking}ms`\n);\n```\n\n----------------------------------------\n\nTITLE: Configuring LangSmith for Observability\nDESCRIPTION: Setting up LangSmith environment variables for tracing and monitoring the tool's operation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/google_scholar.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.LANGSMITH_TRACING=\"true\"\nprocess.env.LANGSMITH_API_KEY=\"your-langchain-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Running Tests\nDESCRIPTION: Commands to run unit tests (ending in .test.ts) and integration tests (ending in .int.test.ts) for the @langchain/qdrant package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-qdrant/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn test\n$ yarn test:int\n```\n\n----------------------------------------\n\nTITLE: Example Connection String for Neon Postgres Setup\nDESCRIPTION: This code snippet represents an example of a Neon Postgres database connection string. It includes all required components like username, password, host, and database name. The `sslmode` parameter ensures secure connections. This string is crucial for connecting the application to the Postgres database.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/neon.mdx#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\npostgres://alex:AbC123dEf@ep-cool-darkness-123456.us-east-2.aws.neon.tech/dbname?sslmode=require\n```\n\n----------------------------------------\n\nTITLE: Initializing Stagehand with Browserbase for Remote Browser\nDESCRIPTION: This code initializes a Stagehand instance with the BROWSERBASE environment setting, which enables the use of remote headless browsers through the Browserbase platform instead of local browsers.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/stagehand.mdx#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nconst stagehand = new Stagehand({\n  env: \"BROWSERBASE\",\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Zep and LangChain Dependencies\nDESCRIPTION: This command installs the necessary npm packages for integrating Zep with LangChain. It includes the Zep JavaScript SDK, LangChain community package, and LangChain core package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/zep-retriever.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @getzep/zep-js @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing AWS DynamoDB Client Dependencies\nDESCRIPTION: Command to install the AWS DynamoDB client package required for DynamoDB integration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/dynamodb.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @aws-sdk/client-dynamodb\n```\n\n----------------------------------------\n\nTITLE: Importing OpenAI Integration in Cloudflare Workers\nDESCRIPTION: Example of importing the ChatOpenAI class for use in Cloudflare Workers environments.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/tests/__mdx__/modules/two.mdx#2025-04-22_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n```\n\n----------------------------------------\n\nTITLE: Installing @langchain/mixedbread-ai Package\nDESCRIPTION: Command to install the @langchain/mixedbread-ai package via npm. This package provides integrations for the Mixedbread AI API in LangChain.js.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-mixedbread-ai/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/mixedbread-ai\n```\n\n----------------------------------------\n\nTITLE: Configuring LangSmith for Observability\nDESCRIPTION: Code to set up LangSmith environment variables for enhanced observability when using the SerpAPI tool.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/serpapi.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.LANGSMITH_TRACING=\"true\"\nprocess.env.LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Integrating Datadog LLM Observability with LangChain.js\nDESCRIPTION: This TypeScript code demonstrates how to set up and use Datadog LLM Observability with LangChain.js. It includes importing necessary modules, initializing the Datadog callback handler, creating a chat model, and running a conversation with observability.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/callbacks/datadog_tracer.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { DatadogOptions, DatadogCallback } from \"@langchain/community/callbacks/datadog\";\nimport { HumanMessage } from \"@langchain/core/messages\";\n\n// Initialize the Datadog callback handler\nconst datadogOptions: DatadogOptions = {\n  apiKey: \"<YOUR_API_KEY>\",\n  env: \"dev\",\n  service: \"my-service\",\n  version: \"1.0.0\",\n};\nconst callback = new DatadogCallback(datadogOptions);\n\n// Create a chat model with the Datadog callback\nconst model = new ChatOpenAI({\n  callbacks: [callback],\n});\n\n// Run a conversation\nconst response = await model.invoke([\n  new HumanMessage(\"What is the capital of France?\"),\n]);\n\nconsole.log(response);\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain OpenAI and Core Packages (bash)\nDESCRIPTION: This snippet demonstrates how to install the @langchain/openai and @langchain/core NPM packages using a Bash command. These packages are required dependencies for running LangChain tools that interact with OpenAI models and core LangChain features. Running this command ensures the necessary modules are available for developing, extending, or using the SearxngSearch tool within a JavaScript or TypeScript project.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/searxng.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Development Dependencies\nDESCRIPTION: Command to install all development dependencies for the @langchain/groq package using yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-groq/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nyarn install\n```\n\n----------------------------------------\n\nTITLE: Creating Diverse Metadata for Hybrid Search in TypeScript\nDESCRIPTION: This snippet demonstrates how to add random metadata to existing documents for hybrid search simulation. It adds date, rating, and author fields to each document's metadata.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/couchbase.mdx#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nfor (let i = 0; i < docs.length; i += 1) {\n  docs[i].metadata.date = `${2010 + (i % 10)}-01-01`;\n  docs[i].metadata.rating = 1 + (i % 5);\n  docs[i].metadata.author = [\"John Doe\", \"Jane Doe\"][i % 2];\n}\n\nconst store = await CouchbaseVectorStore.fromDocuments(\n  docs,\n  embeddings,\n  couchbaseConfig\n);\n\nconst query = \"What did the president say about Ketanji Brown Jackson\";\nconst independenceQuery = \"Any mention about independence?\";\n```\n\n----------------------------------------\n\nTITLE: Running Transpiled JavaScript Examples\nDESCRIPTION: Commands to run examples using the transpiled JavaScript files instead of TypeScript source files.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/examples/src/README.md#2025-04-22_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nyarn run start:dist <path to example>\n```\n\nLANGUAGE: sh\nCODE:\n```\nyarn run start:dist ./dist/prompts/few_shot.js\n```\n\n----------------------------------------\n\nTITLE: Installing development dependencies\nDESCRIPTION: Command to install development dependencies using yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-ollama/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nyarn install\n```\n\n----------------------------------------\n\nTITLE: Rendering Recent Contributors Component\nDESCRIPTION: Renders the People component to display top 20 recent contributors with a count parameter.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/people.mdx#2025-04-22_snippet_2\n\nLANGUAGE: jsx\nCODE:\n```\n<People type=\"top_recent_contributors\" count=\"20\"></People>\n```\n\n----------------------------------------\n\nTITLE: Setting up PostgreSQL Chat Message History\nDESCRIPTION: Code showing how to initialize and use PostgreSQL for storing chat message history with session management.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-google-cloud-sql-pg/README.md#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\n// ChatHistory table initialization\nawait engine.initChatHistoryTable(\"chat_message_table\");\n\nconst historyInstance = await PostgresChatMessageHistory.initialize(engine, \"test\", \"chat_message_table\");\n```\n\n----------------------------------------\n\nTITLE: Defining YAML Configuration with Various Data Types\nDESCRIPTION: This YAML snippet demonstrates the use of different data types including floats, integers, booleans, strings, arrays, and nested dictionaries. It's commonly used for configuration files or frontmatter in static site generators.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-community/src/document_loaders/tests/example_data/obsidian/tags_and_frontmatter.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\naFloat: 13.12345\nanInt: 15\naBool: true\naString: string value\nanArray:\n  - one\n  - two\n  - three\naDict:\n  dictId1: \"58417\"\n  dictId2: 1500\ntags: [\"onetag\", \"twotag\"]\n---\n```\n\n----------------------------------------\n\nTITLE: Defining a Retriever Tool\nDESCRIPTION: Code to define a retriever tool using the 'tool' function from LangChain core.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/agent_executor.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { z } from \"zod\";\nimport { tool } from \"@langchain/core/tools\";\n\nconst retrieverTool = tool(async ({ input }, config) => {\n  const docs = await retriever.invoke(input, config);\n  return docs.map((doc) => doc.pageContent).join(\"\\n\\n\");\n}, {\n  name: \"langsmith_search\",\n  description:\n    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n  schema: z.object({\n    input: z.string()\n  }),\n});\n```\n\n----------------------------------------\n\nTITLE: Embedding Text with OpenAIEmbeddings in JavaScript\nDESCRIPTION: This snippet demonstrates how to use the OpenAIEmbeddings class to convert text into vector representations. It shows examples of embedding single texts with embedQuery and batches of text with embedDocuments.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/static/llms.txt#2025-04-22_snippet_4\n\nLANGUAGE: JavaScript\nCODE:\n```\nimport { OpenAIEmbeddings } from \"langchain/embeddings/openai\";\n\nconst embeddings = new OpenAIEmbeddings();\n\n// Embed a single query\nconst res = await embeddings.embedQuery(\"Hello world\");\n\n// Embed multiple documents\nconst documentRes = await embeddings.embedDocuments([\"Hello world\", \"Bye bye\"]);\n```\n\n----------------------------------------\n\nTITLE: Inserting Index Table for Retrievers in Markdown\nDESCRIPTION: This code snippet inserts an IndexTable component, which is likely used to generate a comprehensive list or table of all available retrievers in the LangChain.js project.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/index.mdx#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n<IndexTable />\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Integration for LangChain\nDESCRIPTION: Command to install @langchain/openai and @langchain/core packages for OpenAI integration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/tests/__mdx__/modules/index.mdx#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAI Package\nDESCRIPTION: Command to install the OpenAI package for embeddings functionality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-weaviate/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai\n```\n\n----------------------------------------\n\nTITLE: Instantiating WatsonxRerank in Python\nDESCRIPTION: Creates a new instance of WatsonxRerank for document reranking.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_compressors/ibm.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport { WatsonxRerank } from \"@langchain/community/document_compressors/ibm\";\n\nconst watsonxRerank = new WatsonxRerank({\n  version: \"2024-05-31\",\n  serviceUrl: process.env.WATSONX_AI_SERVICE_URL,\n  projectId: process.env.WATSONX_AI_PROJECT_ID,\n  model: \"cross-encoder/ms-marco-minilm-l-12-v2\",\n});\n```\n\n----------------------------------------\n\nTITLE: Installing pg for Node.js\nDESCRIPTION: This snippet demonstrates the command to install the pg library, which is necessary for connection pooling with PostgreSQL databases in Node.js. The pg library is a prerequisite for connecting to AnalyticDB vectorstore.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/analyticdb.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -S pg\n```\n\n----------------------------------------\n\nTITLE: Installing Development Dependencies\nDESCRIPTION: Command to install development dependencies using yarn\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-xai/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nyarn install\n```\n\n----------------------------------------\n\nTITLE: Importing Community Package Components in TypeScript\nDESCRIPTION: Demonstrates how to import various components from the @langchain/community package, including chat models, LLMs, and vector stores.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/contributing/integrations.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatParrotLink } from \"@langchain/community/chat_models/parrot_link\";\nimport { ParrotLinkLLM } from \"@langchain/community/llms/parrot_link\";\nimport { ParrotLinkVectorStore } from \"@langchain/community/vectorstores/parrot_link\";\n```\n\n----------------------------------------\n\nTITLE: TypeScript Configuration for ESM Projects\nDESCRIPTION: Recommended TypeScript configuration for ESM projects using LangChain, targeting ES2020 or higher with Node.js module resolution.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/tests/__mdx__/modules/two.mdx#2025-04-22_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"compilerOptions\": {\n    ...\n    \"target\": \"ES2020\", // or higher\n    \"module\": \"nodenext\",\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Building the Package\nDESCRIPTION: Bash commands to build the @langchain/mistralai package, either from within the package directory or from the repository root.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-mistralai/README.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nyarn build\n```\n\n----------------------------------------\n\nTITLE: Installing html-to-text package\nDESCRIPTION: Command to install the html-to-text npm package, which is required for the HtmlToTextTransformer.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_transformers/html-to-text.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install html-to-text\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Dependencies\nDESCRIPTION: Command to install the required LangChain packages for document processing functionality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_transformers/mozilla_readability.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Setting Google API Key Environment Variable\nDESCRIPTION: Command to set the GOOGLE_API_KEY environment variable needed for authentication with Google's API.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-google-genai/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport GOOGLE_API_KEY=your-api-key\n```\n\n----------------------------------------\n\nTITLE: Setting up Anthropic API Key\nDESCRIPTION: Command to add the Anthropic API key to the environment variables.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_citations.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nexport ANTHROPIC_API_KEY=YOUR_KEY\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith Credentials in Bash\nDESCRIPTION: Environment variable configuration for LangSmith API tracing and authentication.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/recursive_url_loader.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Setting Cerebras API Key as Environment Variable\nDESCRIPTION: Example of setting the required CEREBRAS_API_KEY environment variable for authentication with Cerebras API.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-cerebras/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport CEREBRAS_API_KEY=\n```\n\n----------------------------------------\n\nTITLE: Creating a Convex Project with npm\nDESCRIPTION: Command to create a new Convex project using npm.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/convex.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm create convex@latest\n```\n\n----------------------------------------\n\nTITLE: Validating Notebooks in LangChain.js Documentation\nDESCRIPTION: Command to validate that notebooks build correctly and compile TypeScript. Takes a path to the file as an argument.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn validate <PATH_TO_FILE>\n```\n\n----------------------------------------\n\nTITLE: Installing dependencies for development\nDESCRIPTION: Command to install all necessary dependencies for developing the exa package using yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-exa/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nyarn install\n```\n\n----------------------------------------\n\nTITLE: Running Tests for Cerebras Package\nDESCRIPTION: Commands to run unit tests and integration tests for the @langchain/cerebras package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-cerebras/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn test\n$ yarn test:int\n```\n\n----------------------------------------\n\nTITLE: Incorrect JSON Object Usage in PromptTemplate\nDESCRIPTION: Example showing incorrect implementation where JSON object is not properly escaped in the prompt template, causing unexpected variable interpretation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/troubleshooting/errors/INVALID_PROMPT_INPUT.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { PromptTemplate } from \"@langchain/core/prompts\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst prompt = PromptTemplate.fromTemplate(`You are a helpful assistant.\n\nHere is an example of how you should respond:\n\n{\n  \"firstName\": \"John\",\n  \"lastName\": \"Doe\",\n  \"age\": 21\n}\n\nNow, answer the following question:\n\n{question}`);\n```\n\n----------------------------------------\n\nTITLE: Starting Unstructured API Docker Container\nDESCRIPTION: This command downloads and starts the Docker container for the Unstructured API, which is required for processing HTML documents.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/document_loader_html.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -p 8000:8000 -d --rm --name unstructured-api downloads.unstructured.io/unstructured-io/unstructured-api:latest --port 8000 --host 0.0.0.0\n```\n\n----------------------------------------\n\nTITLE: Initializing Neo4j Graph and Importing Movie Data\nDESCRIPTION: This code initializes a connection to a Neo4j database and imports movie information using a Cypher query.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/graph_semantic.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport \"neo4j-driver\";\nimport { Neo4jGraph } from \"@langchain/community/graphs/neo4j_graph\";\n\nconst url = process.env.NEO4J_URI;\nconst username = process.env.NEO4J_USER;\nconst password = process.env.NEO4J_PASSWORD;\nconst graph = await Neo4jGraph.initialize({ url, username, password });\n\n// Import movie information\nconst moviesQuery = `LOAD CSV WITH HEADERS FROM \n'https://raw.githubusercontent.com/tomasonjo/blog-datasets/main/movies/movies_small.csv'\nAS row\nMERGE (m:Movie {id:row.movieId})\nSET m.released = date(row.released),\n    m.title = row.title,\n    m.imdbRating = toFloat(row.imdbRating)\nFOREACH (director in split(row.director, '|') | \n    MERGE (p:Person {name:trim(director)})\n    MERGE (p)-[:DIRECTED]->(m))\nFOREACH (actor in split(row.actors, '|') | \n    MERGE (p:Person {name:trim(actor)})\n    MERGE (p)-[:ACTED_IN]->(m))\nFOREACH (genre in split(row.genres, '|') | \n    MERGE (g:Genre {name:trim(genre)})\n    MERGE (m)-[:IN_GENRE]->(g))`\n\nawait graph.query(moviesQuery);\n```\n\n----------------------------------------\n\nTITLE: Using the Pipe Method for Chaining Runnables in TypeScript\nDESCRIPTION: Shows the shorthand 'pipe' syntax for chaining runnables, which is equivalent to creating a RunnableSequence.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/lcel.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nconst chain = runnable1.pipe(runnable2);\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Dependencies\nDESCRIPTION: Installation commands for required LangChain packages using npm or yarn\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/llm_caching.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Building Static Content for LangChain.js Website\nDESCRIPTION: Command to generate static website content into the 'build' directory. The resulting static files can be served using any static contents hosting service.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn build\n```\n\n----------------------------------------\n\nTITLE: Message Chunk Concatenation Examples\nDESCRIPTION: Shows two methods for concatenating message chunks together - using the concat method directly or importing it from utils.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/concepts/messages.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst aiMessage = chunk1.concat(chunk2).concat(chunk3).concat(...);\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport { concat } from \"@langchain/core/utils/stream\";\nconst aiMessage = concat(chunk1, chunk2);\n```\n\n----------------------------------------\n\nTITLE: MESSAGE_COERCION_FAILURE Error Message in LangChain.js\nDESCRIPTION: This snippet shows the error message returned when a MESSAGE_COERCION_FAILURE occurs. The error indicates that the system was unable to coerce the provided message object into a supported format, and it includes details about the received object for debugging purposes.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE.mdx#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nError: Unable to coerce message from array: only human, AI, system, or tool message coercion is currently supported.\n\nReceived: {\n  \"role\": \"foo\",\n  \"randomNonContentValue\": \"bar\",\n}\n```\n\n----------------------------------------\n\nTITLE: Instantiating __module_name__ Key-Value Store\nDESCRIPTION: Creates an instance of the __module_name__ key-value store. This code snippet demonstrates how to import and initialize the __module_name__ class with necessary parameters.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/kv_store.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { __module_name__ } from \"__full_import_path__\"\n\nconst kvStore = new __module_name__({\n  // params...\n})\n```\n\n----------------------------------------\n\nTITLE: Development Setup Commands\nDESCRIPTION: Commands for setting up the development environment, building the package, and running tests\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-yandex/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nyarn install\n\nyarn build\n\nyarn build --filter=@langchain/yandex\n\nyarn test:int\n\nyarn lint && yarn format\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith Environment Variables\nDESCRIPTION: Configuration for LangSmith tracing and API key setup for debugging and monitoring.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_sources.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport LANGSMITH_TRACING=true\nexport LANGSMITH_API_KEY=YOUR_KEY\n\n# Reduce tracing latency if you are not in a serverless environment\n# export LANGCHAIN_CALLBACKS_BACKGROUND=true\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with NPM/Yarn\nDESCRIPTION: Installation instructions for required packages including @langchain/openai, @langchain/core, zod, and uuid.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/extraction_examples.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: mdx\nCODE:\n```\n<IntegrationInstallTooltip></IntegrationInstallTooltip>\n\n<Npm2Yarn>\n  @langchain/openai @langchain/core zod uuid\n</Npm2Yarn>\n```\n\n----------------------------------------\n\nTITLE: Tavily Search Results Tool Output\nDESCRIPTION: JSON output from a Tavily search tool containing results about the 2023 film Oppenheimer, directed by Christopher Nolan. The search returns information from various sources including IMDb, Rotten Tomatoes, Wikipedia, and news articles.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n[{\"title\":\"Oppenheimer (2023) - IMDb\",\"url\":\"https://www.imdb.com/title/tt15398776/\",\"content\":\"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.\",\"score\":0.96643,\"raw_content\":null},{\"title\":\"Christopher Nolan's Oppenheimer - Rotten Tomatoes\",\"url\":\"https://editorial.rottentomatoes.com/article/everything-we-know-about-christopher-nolans-oppenheimer/\",\"content\":\"Billboards and movie theater pop-ups across Los Angeles have been ticking down for months now: Christopher Nolan's epic account of J. Robert Oppenheimer, the father of the atomic bomb, is nearing an explosive release on July 21, 2023. Nolan movies are always incredibly secretive, twists locked alongside totems behind safe doors, actors not spilling an ounce of Earl Grey tea.\",\"score\":0.92804,\"raw_content\":null},{\"title\":\"Oppenheimer (film) - Wikipedia\",\"url\":\"https://en.wikipedia.org/wiki/Oppenheimer_(film)\",\"content\":\"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\\nCritical response\\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \\\"more objective view of his story from a different character's point of view\\\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \\\"big-atures\\\", since the special effects team had tried to build the models as physically large as possible. He felt that \\\"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\\\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \\\"emotional\\\" and resembling that of a thriller, while also remarking that Nolan had \\\"Trojan-Horsed a biopic into a thriller\\\".[72]\\nCasting\\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\\\", while also underscoring that it is a \\\"huge shift in perception about the reality of Oppenheimer's perception\\\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \\\"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\\\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.\",\"score\":0.92404,\"raw_content\":null},{\"title\":\"'Oppenheimer' Director Christopher Nolan On Filmmaking at 53: \\\"I Try to ...\",\"url\":\"https://www.everythingzoomer.com/arts-entertainment/2023/11/21/oppenheimer-director-christopher-nolan-on-filmmaking-at-53-i-try-to-challenge-myself-with-every-film/\",\"content\":\"Oppenheimer will be available to own on 4K Ultra HD, Blu-ray and DVD — including more than three hours of bonus features — on November 21.\\nRELATED:\\nVisiting the Trinity Site Featured in 'Oppenheimer' Is a Sobering Reminder of the Horror of Nuclear Weapons\\nBarbenheimer: How 'Barbie' and 'Oppenheimer' Became the Unlikely Movie Marriage of the Summer\\nBlast From the Past: 'Asteroid City' & 'Oppenheimer' and the Age of Nuclear Anxiety\\nEXPLORE  HealthMoneyTravelFoodStyleBook ClubClassifieds#ZoomerDailyPolicy & PerspectiveArts & EntertainmentStars & RoyaltySex & Love\\nCONNECT  FacebookTwitterInstagram\\nSUBSCRIBE  Terms of Subscription ServiceE-NewslettersSubscribe to Zoomer Magazine\\nBROWSE  AboutMastheadContact UsAdvertise with UsPrivacy Policy\\nEverythingZoomer.com is part of the ZoomerMedia Digital Network \\\"I think with experience — and with the experience of watching your films with an audience over the years — you do more and more recognize the human elements that people respond to, and the things that move you and the things that move the audience.\\\"\\n \\\"What's interesting, as you watch the films over time, is that some of his preoccupations are the same, but then some of them have changed over time with who he is as a person and what's going on in his own life,\\\" Thomas said.\\n The British-American director's latest explosive drama, Oppenheimer, which has earned upwards of US$940 million at the global box office, follows theoretical physicist J. Robert Oppenheimer (played by Cillian Murphy) as he leads the team creating the first atomic bomb, as director of the Manhattan Project's Los Alamos Laboratory.\\n Subscribe\\nEverything Zoomer\\n'Oppenheimer' Director Christopher Nolan On Filmmaking at 53: \\\"I Try to Challenge Myself with Every Film\\\"\\nDirector Christopher Nolan poses upon his arrival for the premiere of the movie 'Oppenheimer' in Paris on July 11, 2023.\",\"score\":0.92002,\"raw_content\":null},{\"title\":\"'Oppenheimer' Review: A Man for Our Time - The New York Times\",\"url\":\"https://www.nytimes.com/2023/07/19/movies/oppenheimer-review-christopher-nolan.html\",\"content\":\"Instead, it is here that the film's complexities and all its many fragments finally converge as Nolan puts the finishing touches on his portrait of a man who contributed to an age of transformational scientific discovery, who personified the intersection of science and politics, including in his role as a Communist boogeyman, who was transformed by his role in the creation of weapons of mass destruction and soon after raised the alarm about the dangers of nuclear war.\\n He served as director of a clandestine weapons lab built in a near-desolate stretch of Los Alamos, in New Mexico, where he and many other of the era's most dazzling scientific minds puzzled through how to harness nuclear reactions for the weapons that killed tens of thousands instantly, ending the war in the Pacific.\\n Nolan integrates these black-and-white sections with the color ones, using scenes from the hearing and the confirmation — Strauss's role in the hearing and his relationship with Oppenheimer directly affected the confirmation's outcome — to create a dialectical synthesis. To signal his conceit, he stamps the film with the words \\\"fission\\\" (a splitting into parts) and \\\"fusion\\\" (a merging of elements); Nolan being Nolan, he further complicates the film by recurrently kinking up the overarching chronology — it is a lot.\\n It's also at Berkeley that Oppenheimer meets the project's military head, Leslie Groves (a predictably good Damon), who makes him Los Alamos's director, despite the leftist causes he supported — among them, the fight against fascism during the Spanish Civil War — and some of his associations, including with Communist Party members like his brother, Frank (Dylan Arnold).\\n \",\"score\":0.91831,\"raw_content\":null}]\n```\n\n----------------------------------------\n\nTITLE: Setting Momento API Key Environment Variable\nDESCRIPTION: Sets the Momento API key, obtained from the Momento Console, as an environment variable. This key is required for authenticating requests to the Momento Vector Index service.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/momento_vector_index.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport MOMENTO_API_KEY=YOUR_MOMENTO_API_KEY_HERE # https://console.gomomento.com\n```\n\n----------------------------------------\n\nTITLE: Invoking RAG Chain\nDESCRIPTION: Example showing how to invoke the configured RAG chain with a query\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/retrievers/tavily.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nawait ragChain.invoke(query);\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith Environment Variables\nDESCRIPTION: Configuration of environment variables for LangSmith tracing functionality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/extraction.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nexport LANGSMITH_TRACING=\"true\"\nexport LANGSMITH_API_KEY=\"...\"\n\n# Reduce tracing latency if you are not in a serverless environment\n# export LANGCHAIN_CALLBACKS_BACKGROUND=true\n```\n\n----------------------------------------\n\nTITLE: Deprecating Community Integration in TypeScript\nDESCRIPTION: Demonstrates how to deprecate an existing community integration when migrating to a partner package using TSDoc comments.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/contributing/integrations.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n/** @deprecated Install and import from `@langchain/parrot-link` instead. */\nclass ChatParrotLink extends SimpleChatModel {\n  ...\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Community and Core Packages\nDESCRIPTION: Command to install @langchain/community and @langchain/core packages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/tests/__mdx__/modules/index.mdx#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Running Continuous Integration Checks for LangChain.js Website\nDESCRIPTION: Command to run linting and formatting checks for the website. Useful for continuous integration systems like Travis CI or CircleCI.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn ci\n```\n\n----------------------------------------\n\nTITLE: Building the MongoDB Package\nDESCRIPTION: Commands for building the MongoDB package either directly or from the repo root\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-mongodb/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nyarn build\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn build --filter=@langchain/mongodb\n```\n\n----------------------------------------\n\nTITLE: Running Tests for DeepSeek Package\nDESCRIPTION: Commands to run unit tests and integration tests for the @langchain/deepseek package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-deepseek/README.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn test\n$ yarn test:int\n```\n\n----------------------------------------\n\nTITLE: Installing Development Dependencies\nDESCRIPTION: Command to install development dependencies using yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-nomic/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nyarn install\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies\nDESCRIPTION: Commands for installing project dependencies using Yarn. This includes instructions for setting up the core package, which needs to be built first.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/contributing/code.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nyarn\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd ../langchain-core\nyarn\nyarn build\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Environment Variable (TypeScript)\nDESCRIPTION: Sets the OpenAI API key as an environment variable (`OPENAI_API_KEY`) in a Node.js environment. This step is necessary if you plan to use OpenAI embeddings with the `MemoryVectorStore`. Replace `\"YOUR_API_KEY\"` with your actual OpenAI API key.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/memory.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.OPENAI_API_KEY = \"YOUR_API_KEY\";\n```\n\n----------------------------------------\n\nTITLE: Initializing Chat History in Python\nDESCRIPTION: This snippet creates a demo chat history as a list of dictionaries, representing alternating user and assistant messages.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/chatbots_memory.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nconst demoEphemeralChatHistory2 = [\n  { role: \"user\", content: \"Hey there! I'm Nemo.\" },\n  { role: \"assistant\", content: \"Hello!\" },\n  { role: \"user\", content: \"How are you today?\" },\n  { role: \"assistant\", content: \"Fine thanks!\" },\n];\n```\n\n----------------------------------------\n\nTITLE: Streaming Steps in LangChain vs LangGraph\nDESCRIPTION: Compares the implementation of step iteration between LangChain's AgentExecutor and LangGraph, showing how to stream execution steps in both frameworks.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/migrate_agent.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nconst langChainStream = await agentExecutor.stream({ input: query });\n\nfor await (const step of langChainStream) {\n  console.log(step);\n}\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst langGraphStream = await app.stream(\n  { messages: [{ role: \"user\", content: query }] },\n  { streamMode: \"updates\" },\n);\n\nfor await (const step of langGraphStream) {\n  console.log(step);\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Google AI API and LangSmith\nDESCRIPTION: Sets up environment variables for the Google API key and LangSmith tracing for better monitoring.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/google_generativeai.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport GOOGLE_API_KEY=\"your-api-key\"\n```\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variable for __module_name__ API Key\nDESCRIPTION: Sets the API key for __module_name__ as an environment variable. This step is necessary for authenticating with the __module_name__ service.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/kv_store.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.__env_var_name__=\"__your_api_key__\";\n```\n\n----------------------------------------\n\nTITLE: Importing Components for Key-Value Store Documentation in React\nDESCRIPTION: This code snippet imports custom React components used to display categorized tables and index tables for key-value store documentation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/stores/index.mdx#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport { CategoryTable, IndexTable } from \"@theme/FeatureTables\";\n```\n\n----------------------------------------\n\nTITLE: Installing YandexGPT Dependencies\nDESCRIPTION: Commands to install the required npm packages for using YandexGPT with LangChain\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-yandex/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/yandex @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for LangChain.js\nDESCRIPTION: This command runs only the unit tests for the project.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/CONTRIBUTING.md#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nyarn test\n```\n\n----------------------------------------\n\nTITLE: Running a Prisma Migration with npx\nDESCRIPTION: This command runs a Prisma migration using npx, creating necessary database schema changes. It applies migrations needed to establish the correct database structures as defined in the Prisma schema.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/prisma.mdx#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnpx prisma migrate dev\n```\n\n----------------------------------------\n\nTITLE: Building the Package from Repo Root\nDESCRIPTION: Command to build the @langchain/google-genai package specifically when in the repository root directory.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-google-genai/README.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nyarn build --filter=@langchain/google-genai\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for ParentDocumentRetriever in TypeScript\nDESCRIPTION: Commands to install the required npm packages @langchain/openai and @langchain/core for using ParentDocumentRetriever.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/parent_document_retriever.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/openai @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith API Key in Bash\nDESCRIPTION: Bash commands to set environment variables for LangSmith tracing and API key. These are optional for automated tracing of model calls.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/cloudflare_ai.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith API Key for ChatOllama in Bash\nDESCRIPTION: Bash commands to set environment variables for LangSmith tracing and API key. These are optional and used for automated tracing of model calls.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/ollama.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# export LANGSMITH_TRACING=\"true\"\n# export LANGSMITH_API_KEY=\"your-api-key\"\n```\n\n----------------------------------------\n\nTITLE: Declaring Optional Dependency for LangChain.js Entrypoint\nDESCRIPTION: This code snippet demonstrates how to declare an optional dependency for a new entrypoint in the LangChain.js configuration file.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/CONTRIBUTING.md#2025-04-22_snippet_16\n\nLANGUAGE: typescript\nCODE:\n```\n// ...\nrequiresOptionalDependency: [\n  // ...\n  \"tools/index\",\n],\n// ...\n```\n\n----------------------------------------\n\nTITLE: Running Integration Tests for LangChain.js\nDESCRIPTION: This command runs all integration tests for the project.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/CONTRIBUTING.md#2025-04-22_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nyarn test:integration\n```\n\n----------------------------------------\n\nTITLE: Running Tests for @langchain/groq\nDESCRIPTION: Commands to run unit tests and integration tests for the package, where test files should be placed in the tests/ directory.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-groq/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn test\n$ yarn test:int\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables\nDESCRIPTION: Setting up API key as environment variable for authentication.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/create-langchain-integration/template/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport <ADD_ENV_NAME_HERE>=your-api-key\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key\nDESCRIPTION: Command to set the OpenAI API key as an environment variable for authentication.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/reduce_retrieval_latency.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=your-api-key\n```\n\n----------------------------------------\n\nTITLE: Setting up Turbopuffer API key in Bash\nDESCRIPTION: Command to set the Turbopuffer API key as an environment variable, which is required for authenticating with the Turbopuffer service.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/turbopuffer.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport TURBOPUFFER_API_KEY=<YOUR_API_KEY>\n```\n\n----------------------------------------\n\nTITLE: Running ESLint for LangChain.js\nDESCRIPTION: This command runs the ESLint linter to enforce standard lint rules across the project.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/CONTRIBUTING.md#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nyarn lint\n```\n\n----------------------------------------\n\nTITLE: Deleting Data from __module_name__ Key-Value Store\nDESCRIPTION: Shows how to delete multiple keys from the key-value store using the 'mdelete' method. It also demonstrates attempting to retrieve deleted keys.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/kv_store.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nawait kvStore.mdelete(\n  [\n    \"key1\",\n    \"key2\",\n  ]\n)\n\nawait kvStore.mget(\n  [\n    \"key1\",\n    \"key2\",\n  ]\n)\n```\n\n----------------------------------------\n\nTITLE: Navigating to Workspace Directories\nDESCRIPTION: Commands for changing into specific workspace directories, either the main langchain package or a community integration package. These commands should be run before executing other development tasks.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/contributing/code.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd langchain\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd libs/langchain-community\n```\n\n----------------------------------------\n\nTITLE: Installing Development Dependencies\nDESCRIPTION: Command to install dependencies for developing the @langchain/mixedbread-ai package using Yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-mixedbread-ai/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nyarn install\n```\n\n----------------------------------------\n\nTITLE: Installing Upstash Redis\nDESCRIPTION: Installation command for Upstash Redis client package\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/llm_caching.mdx#2025-04-22_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @upstash/redis\n```\n\n----------------------------------------\n\nTITLE: Package Resolution Configuration for PNPM\nDESCRIPTION: Configuration for pnpm's package.json to ensure all integrations use the same version of @langchain/core using pnpm.overrides field.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/tests/__mdx__/modules/two.mdx#2025-04-22_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"your-project\",\n  \"version\": \"0.0.0\",\n  \"private\": true,\n  \"engines\": {\n    \"node\": \">=18\"\n  },\n  \"dependencies\": {\n    \"@langchain/google-genai\": \"^0.0.2\",\n    \"langchain\": \"0.0.207\"\n    \"@langchain/core\": \"^0.3.0\"\n  \"pnpm\": {\n    \"overrides\": {\n      \"@langchain/core\": \"0.3.0\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing a Chat Model for Parrot Link AI in TypeScript\nDESCRIPTION: Shows the basic structure for implementing a new chat model in the community package, extending the SimpleChatModel class.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/contributing/integrations.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport {\n  SimpleChatModel,\n} from \"@langchain/core/language_models/chat_models\";\n\nexport class ChatParrotLink extends SimpleChatModel {\n\n  ...\n```\n\n----------------------------------------\n\nTITLE: Installing Google APIs Peer Dependency (Bash)\nDESCRIPTION: This command installs the `googleapis` package using npm (or yarn via the npm2yarn wrapper). This package is a required peer dependency for the Google Calendar Tools, enabling interaction with the Google Calendar API.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/google_calendar.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install googleapis\n```\n\n----------------------------------------\n\nTITLE: Building Documentation Site\nDESCRIPTION: Command for building the documentation site locally. This is needed only if you're working on documentation changes.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/contributing/code.mdx#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nyarn build --filter=core_docs\n```\n\n----------------------------------------\n\nTITLE: Invalid Tool Message Example\nDESCRIPTION: Shows an incorrect usage where a tool message is passed without a preceding AIMessage containing tool calls.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/troubleshooting/errors/INVALID_TOOL_RESULTS.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nawait modelWithTools.invoke([{\n  role: \"tool\",\n  content: \"action completed!\",\n  tool_call_id: \"dummy\",\n}])\n```\n\n----------------------------------------\n\nTITLE: Configuring Package Dependencies for Standard Tests in package.json\nDESCRIPTION: Shows how to add the standard test package as a workspace dependency in package.json for a LangChain.js integration.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-standard-tests/README.md#2025-04-22_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"devDependencies\": {\n    \"@langchain/standard-tests\": \"workspace:*\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Parallel Tool Calls in LangChain\nDESCRIPTION: This snippet demonstrates the default behavior of parallel tool calls when the 'parallel_tool_calls' option is not set to false. It shows that the model will make multiple tool calls when instructed.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_calling_parallel.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconst llmWithNoBoundParam = llm.bindTools(tools);\n\nconst result2 = await llmWithNoBoundParam.invoke(\"Please call the first tool two times\");\n\nresult2.tool_calls;\n```\n\n----------------------------------------\n\nTITLE: Cloning the LangChainJS Repository\nDESCRIPTION: Command to change into the LangChainJS directory after cloning the repository. This is the first step for setting up the local development environment.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/contributing/code.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd langchainjs\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Community and HuggingFace Inference Dependencies\nDESCRIPTION: This snippet shows how to install the required packages for using HuggingFace Inference Embeddings in LangChain.js. It includes the @langchain/community package, @langchain/core, and @huggingface/inference.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/text_embedding/hugging_face_inference.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core @huggingface/inference@2\n```\n\n----------------------------------------\n\nTITLE: Building LangChain.js Project\nDESCRIPTION: This command builds the entire LangChain.js project.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/CONTRIBUTING.md#2025-04-22_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nyarn build\n```\n\n----------------------------------------\n\nTITLE: Visualizing RAG Application Graph in JavaScript\nDESCRIPTION: This code generates a visual representation of the RAG application graph using Mermaid and displays it as a PNG image. Note that this code is specific to a Jupyter notebook environment.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/qa_chat_history.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: javascript\nCODE:\n```\n// Note: tslab only works inside a jupyter notebook. Don't worry about running this code yourself!\nimport * as tslab from \"tslab\";\n\nconst image = await graph.getGraph().drawMermaidPng();\nconst arrayBuffer = await image.arrayBuffer();\n\nawait tslab.display.png(new Uint8Array(arrayBuffer));\n```\n\n----------------------------------------\n\nTITLE: Importing OpenAI Integration in Deno/Supabase Edge Functions\nDESCRIPTION: Examples of importing the ChatOpenAI class for use in Deno or Supabase Edge Functions using ESM URL imports.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/tests/__mdx__/modules/two.mdx#2025-04-22_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"https://esm.sh/@langchain/openai\";\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"npm:@langchain/openai\";\n```\n\n----------------------------------------\n\nTITLE: Defining YAML Frontmatter for an Obsidian Journal Entry\nDESCRIPTION: YAML frontmatter that tags the document as a journal entry for use in Obsidian. The frontmatter uses the tags property to categorize the document as both a journal entry and an Obsidian document.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-community/src/document_loaders/tests/example_data/obsidian/frontmatter.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntags: journal/entry, obsidian\n---\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for LangChain.js\nDESCRIPTION: This command installs the project dependencies using Yarn package manager.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/CONTRIBUTING.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nyarn\n```\n\n----------------------------------------\n\nTITLE: Installing Development Dependencies\nDESCRIPTION: Command to install development dependencies using yarn\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-mongodb/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nyarn install\n```\n\n----------------------------------------\n\nTITLE: Installing Cloudflare Types\nDESCRIPTION: Installation command for Cloudflare Workers types\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/llm_caching.mdx#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -S @cloudflare/workers-types\n```\n\n----------------------------------------\n\nTITLE: Installing PDF.js for Custom Builds in LangChain.js\nDESCRIPTION: This command installs the pdfjs-dist package, which can be used for custom PDF.js builds in LangChain.js.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/document_loader_pdf.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnpm install pdfjs-dist\n```\n\n----------------------------------------\n\nTITLE: PuppeteerWebBaseLoader Options Interface\nDESCRIPTION: TypeScript interface showing the configuration options available for PuppeteerWebBaseLoader including launch options, navigation options, and custom evaluation functions.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/web_puppeteer.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\ntype PuppeteerWebBaseLoaderOptions = {\n  launchOptions?: PuppeteerLaunchOptions;\n  gotoOptions?: PuppeteerGotoOptions;\n  evaluate?: (page: Page, browser: Browser) => Promise<string>;\n};\n```\n\n----------------------------------------\n\nTITLE: Deploying LangChain.js Website Using SSH\nDESCRIPTION: Command to deploy the website using SSH authentication. This builds the website and pushes it to the deployment target.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ USE_SSH=true yarn deploy\n```\n\n----------------------------------------\n\nTITLE: Starting Local Development Server for LangChain.js Website\nDESCRIPTION: Command to start a local development server that opens a browser window. Changes to the website are reflected live without needing to restart the server.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn start\n```\n\n----------------------------------------\n\nTITLE: Deleting Items from WeaviateStore\nDESCRIPTION: Demonstrates how to delete documents from the vector store using document IDs\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/weaviate.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nawait vectorStore.delete({ ids: [uuids[3]] });\n```\n\n----------------------------------------\n\nTITLE: Running LangChain.js Documentation Locally\nDESCRIPTION: These commands prepare and run the LangChain.js documentation locally.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/CONTRIBUTING.md#2025-04-22_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\ncd ..\nyarn\nyarn docs\n```\n\n----------------------------------------\n\nTITLE: Deploying LangChain.js Website Without SSH\nDESCRIPTION: Command to deploy the website using GitHub username authentication instead of SSH. Builds the website and pushes to the 'gh-pages' branch for GitHub Pages hosting.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ GIT_USER=<Your GitHub username> yarn deploy\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Environment Variable (TypeScript)\nDESCRIPTION: This snippet shows how to set the OpenAI API key as an environment variable. This is required if using OpenAI embeddings with the Supabase vector store.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/vectorstores/supabase.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.OPENAI_API_KEY = \"YOUR_API_KEY\";\n```\n\n----------------------------------------\n\nTITLE: Passing Image URL to OpenAI Model\nDESCRIPTION: This snippet demonstrates how to use the ChatOpenAI model to analyze an image from a direct URL. It creates a HumanMessage with text and an image URL, then invokes the model.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/multimodal_inputs.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst openAIModel = new ChatOpenAI({\n  model: \"gpt-4o\",\n});\n\nconst imageUrl = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\";\n\nconst message = new HumanMessage({\n  content: [\n    {\n      type: \"text\",\n      text: \"describe the weather in this image\"},\n    {\n      type: \"image_url\",\n      image_url: { url: imageUrl }\n    },\n  ],\n});\nconst response = await openAIModel.invoke([message]);\nconsole.log(response.content);\n```\n\n----------------------------------------\n\nTITLE: Building the AWS Package\nDESCRIPTION: Commands to build the @langchain/aws package, either directly or from the repo root.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-aws/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nyarn build\n```\n\nLANGUAGE: bash\nCODE:\n```\nyarn build --filter=@langchain/aws\n```\n\n----------------------------------------\n\nTITLE: Installing Baidu Qianfan Dependencies\nDESCRIPTION: Command to install the required npm packages for using Baidu Qianfan with LangChain\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/baidu_qianfan.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/baidu-qianfan @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain Redis Package\nDESCRIPTION: Command to install the @langchain/redis package along with the required @langchain/core dependency using npm, yarn, or other package managers.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-redis/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/redis @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing PDF Parse Dependency for LangChain.js\nDESCRIPTION: This command installs the pdf-parse package, which is required for loading PDF files in LangChain.js.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/document_loader_pdf.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install pdf-parse\n```\n\n----------------------------------------\n\nTITLE: Installing LangChain AWS Package\nDESCRIPTION: Command to install the @langchain/aws package via npm.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-aws/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/aws\n```\n\n----------------------------------------\n\nTITLE: Vector Store Creation with Chroma\nDESCRIPTION: Initializes a Chroma vector store with sample text data using OpenAI embeddings and creates a retriever.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/query_multiple_queries.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport { Chroma } from \"@langchain/community/vectorstores/chroma\"\nimport { OpenAIEmbeddings } from \"@langchain/openai\"\nimport \"chromadb\";\n\nconst texts = [\"Harrison worked at Kensho\", \"Ankush worked at Facebook\"]\nconst embeddings = new OpenAIEmbeddings({ model: \"text-embedding-3-small\" })\nconst vectorstore = await Chroma.fromTexts(\n    texts,\n    {},\n    embeddings,\n    {\n        collectionName: \"multi_query\"\n    }\n)\nconst retriever = vectorstore.asRetriever(1);\n```\n\n----------------------------------------\n\nTITLE: Importing LangChain Modules in Deno\nDESCRIPTION: Example of how to import LangChain.js modules using ESM imports with the Deno runtime. This demonstrates the URL-based import syntax required for Deno, which differs from Node.js imports.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/cookbook/README.md#2025-04-22_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { PromptTemplate } from \"https://esm.sh/langchain/prompts\";\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith API Key for Tracing in TypeScript\nDESCRIPTION: Example of how to set the LangSmith API key for automated tracing from individual queries. This is optional and commented out by default.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/retrievers.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// process.env.LANGSMITH_API_KEY = \"<YOUR API KEY HERE>\";\n// process.env.LANGSMITH_TRACING = \"true\";\n```\n\n----------------------------------------\n\nTITLE: Configuring LangSmith Tracing Environment Variables in Typescript\nDESCRIPTION: Sets environment variables `LANGSMITH_TRACING` to 'true' and `LANGSMITH_API_KEY` to enable automated tracing of tool runs using LangSmith. Replace 'your-api-key' with an actual LangSmith API key.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/ibm.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.LANGSMITH_TRACING = \"true\";\nprocess.env.LANGSMITH_API_KEY = \"your-api-key\";\n```\n\n----------------------------------------\n\nTITLE: Importing Feature Tables Component in MDX\nDESCRIPTION: Imports CategoryTable and IndexTable components from the theme's FeatureTables to display document loader information.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/file_loaders/index.mdx#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport { CategoryTable, IndexTable } from \"@theme/FeatureTables\";\n```\n\n----------------------------------------\n\nTITLE: Logging Final Tool Call Chunks in LangChain.js\nDESCRIPTION: These snippets demonstrate how to access the final aggregated tool call chunks and the fully parsed tool call as an object after accumulation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/tool_streaming.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nconsole.log(typeof gathered.tool_call_chunks[0].args);\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconsole.log(typeof gathered.tool_calls[0].args);\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Google Cloud Storage Integration\nDESCRIPTION: This command installs the necessary packages for integrating Google Cloud Storage with LangChain, including the official Google Cloud Storage SDK.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/google_cloud_storage.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core @google-cloud/storage\n```\n\n----------------------------------------\n\nTITLE: Building Langchain CLI\nDESCRIPTION: Command to build the Langchain CLI tool using yarn\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nyarn build --filter=@langchain/scripts\n```\n\n----------------------------------------\n\nTITLE: Customizing RAG Prompt in JavaScript\nDESCRIPTION: This snippet shows how to create a custom prompt template for a RAG application using ChatPromptTemplate.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/tutorials/rag.ipynb#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nconst template = `Use the following pieces of context to answer the question at the end.\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\nUse three sentences maximum and keep the answer as concise as possible.\nAlways say \"thanks for asking!\" at the end of the answer.\n\n{context}\n\nQuestion: {question}\n\nHelpful Answer:`\n\nconst promptTemplateCustom = ChatPromptTemplate.fromMessages(\n  [\n    [\"user\", template]\n  ]\n)\n```\n\n----------------------------------------\n\nTITLE: Rendering Toolkit Index Table (JSX)\nDESCRIPTION: This code snippet renders the IndexTable component, which presumably displays a comprehensive list of all available toolkits in the LangChain.js project.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/toolkits/index.mdx#2025-04-22_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\n<IndexTable />\n```\n\n----------------------------------------\n\nTITLE: Building the Package\nDESCRIPTION: Commands to build the package either directly or from repo root\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-anthropic/README.md#2025-04-22_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nyarn build\n```\n\n----------------------------------------\n\nTITLE: Initializing Chat Model\nDESCRIPTION: Example of initializing and using the chat model with basic message handling.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/create-langchain-integration/template/README.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { <ADD_CLASS_NAME_HERE> } from \"@langchain/<ADD_PACKAGE_NAME_HERE>\";\n\nconst model = new ExampleChatClass({\n  apiKey: process.env.EXAMPLE_API_KEY,\n});\nconst response = await model.invoke(new HumanMessage(\"Hello world!\"));\n```\n\n----------------------------------------\n\nTITLE: Usage Example of AirtableLoader in TypeScript\nDESCRIPTION: This code block demonstrates the usage of the AirtableLoader class. It's a placeholder for the actual example code, which is imported from an external file.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/document_loaders/web_loaders/airtable.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n{loadExample}\n```\n\n----------------------------------------\n\nTITLE: Installing Required LangChain Packages\nDESCRIPTION: NPM command to install the necessary LangChain packages for using Prem AI integration, including the community package and core functionality.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/premai.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Importing OpenAI Integration in CommonJS\nDESCRIPTION: Example of importing the ChatOpenAI class from the OpenAI integration package using CommonJS syntax.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/tests/__mdx__/modules/two.mdx#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nconst { ChatOpenAI } = require(\"@langchain/openai\");\n```\n\n----------------------------------------\n\nTITLE: Package.json Configuration for LangChain Integration\nDESCRIPTION: Example package.json configuration for a LangChain integration package (@langchain/anthropic). Shows proper dependency management with peer dependencies for core, ensuring version compatibility.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/langchain-core/README.md#2025-04-22_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"@langchain/anthropic\",\n  \"version\": \"0.0.3\",\n  \"description\": \"Anthropic integrations for LangChain.js\",\n  \"type\": \"module\",\n  \"author\": \"LangChain\",\n  \"license\": \"MIT\",\n  \"dependencies\": {\n    \"@anthropic-ai/sdk\": \"^0.10.0\"\n  },\n  \"peerDependencies\": {\n    \"@langchain/core\": \"~0.3.0\"\n  },\n  \"devDependencies\": {\n    \"@langchain/core\": \"~0.3.0\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing ChatOpenAI Model in TypeScript\nDESCRIPTION: Example of initializing a ChatOpenAI model with specific parameters for use in a chain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-scripts/src/cli/docs/templates/retrievers.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o-mini\",\n  temperature: 0,\n});\n```\n\n----------------------------------------\n\nTITLE: TavilySearchResults Tool Output for Oppenheimer Film Query\nDESCRIPTION: JSON output from the TavilySearchResults tool displaying search results about the Oppenheimer film directed by Christopher Nolan, including information about box office performance, critical reception, and production details.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/debugging.mdx#2025-04-22_snippet_44\n\nLANGUAGE: json\nCODE:\n```\n[{\"title\":\"Oppenheimer (film) - Wikipedia\",\"url\":\"https://en.wikipedia.org/wiki/Oppenheimer_(film)\",\"content\":\"The film continued to hold well in the following weeks, making $32 million and $29.1 million in its fifth and sixth weekends.[174][175] As of September 10, 2023, the highest grossing territories were the United Kingdom ($72 million), Germany ($46.9 million), China ($46.8 million), France ($40.1 million) and Australia ($25.9 million).[176]\\nCritical response\\nThe film received critical acclaim.[a] Critics praised Oppenheimer primarily for its screenplay, the performances of the cast (particularly Murphy and Downey), and the visuals;[b] it was frequently cited as one of Nolan's best films,[191][192][183] and of 2023, although some criticism was aimed towards the writing of the female characters.[187] Hindustan Times reported that the film was also hailed as one of the best films of the 21st century.[193] He also chose to alternate between scenes in color and black-and-white to convey the story from both subjective and objective perspectives, respectively,[68] with most of Oppenheimer's view shown via the former, while the latter depicts a \\\"more objective view of his story from a different character's point of view\\\".[69][67] Wanting to make the film as subjective as possible, the production team decided to include visions of Oppenheimer's conceptions of the quantum world and waves of energy.[70] Nolan noted that Oppenheimer never publicly apologized for his role in the atomic bombings of Hiroshima and Nagasaki, but still desired to portray Oppenheimer as feeling genuine guilt for his actions, believing this to be accurate.[71]\\nI think of any character I've dealt with, Oppenheimer is by far the most ambiguous and paradoxical. The production team was able to obtain government permission to film at White Sands Missile Range, but only at highly inconvenient hours, and therefore chose to film the scene elsewhere in the New Mexico desert.[2][95]\\nThe production filmed the Trinity test scenes in Belen, New Mexico, with Murphy climbing a 100-foot steel tower, a replica of the original site used in the Manhattan Project, in rough weather.[2][95]\\nA special set was built in which gasoline, propane, aluminum powder, and magnesium were used to create the explosive effect.[54] Although they used miniatures for the practical effect, the film's special effects supervisor Scott R. Fisher referred to them as \\\"big-atures\\\", since the special effects team had tried to build the models as physically large as possible. He felt that \\\"while our relationship with that [nuclear] fear has ebbed and flowed with time, the threat itself never actually went away\\\", and felt the 2022 Russian invasion of Ukraine had caused a resurgence of nuclear anxiety.[54] Nolan had also penned a script for a biopic of Howard Hughes approximately during the time of production of Martin Scorsese's The Aviator (2004), which had given him insight on how to write a script regarding a person's life.[53] Emily Blunt described the Oppenheimer script as \\\"emotional\\\" and resembling that of a thriller, while also remarking that Nolan had \\\"Trojan-Horsed a biopic into a thriller\\\".[72]\\nCasting\\nOppenheimer marks the sixth collaboration between Nolan and Murphy, and the first starring Murphy as the lead. [for Oppenheimer] in his approach to trying to deal with the consequences of what he'd been involved with\\\", while also underscoring that it is a \\\"huge shift in perception about the reality of Oppenheimer's perception\\\".[53] He wanted to execute a quick tonal shift after the atomic bombings of Hiroshima and Nagasaki, desiring to go from the \\\"highest triumphalism, the highest high, to the lowest low in the shortest amount of screen time possible\\\".[66] For the ending, Nolan chose to make it intentionally vague to be open to interpretation and refrained from being didactic or conveying specific messages in his work.\",\"score\":0.97075,\"raw_content\":null},{\"title\":\"Christopher Nolan's Oppenheimer - Rotten Tomatoes\",\"url\":\"https://editorial.rottentomatoes.com/article/everything-we-know-about-christopher-nolans-oppenheimer/\",\"content\":\"Billboards and movie theater pop-ups across Los Angeles have been ticking down for months now: Christopher Nolan's epic account of J. Robert Oppenheimer, the father of the atomic bomb, is nearing an explosive release on July 21, 2023. Nolan movies are always incredibly secretive, twists locked alongside totems behind safe doors, actors not spilling an ounce of Earl Grey tea.\",\"score\":0.9684,\"raw_content\":null},{\"title\":\"Oppenheimer (2023) - Full Cast & Crew - IMDb\",\"url\":\"https://www.imdb.com/title/tt15398776/fullcredits/\",\"content\":\"Oppenheimer (2023) cast and crew credits, including actors, actresses, directors, writers and more. Menu. Movies. Release Calendar Top 250 Movies Most Popular Movies Browse Movies by Genre Top Box Office Showtimes & Tickets Movie News India Movie Spotlight. ... Peter Oppenheimer - Age 8 (uncredited) Adam Walker Federman ... MIT Student ...\",\"score\":0.94834,\"raw_content\":null},{\"title\":\"Oppenheimer (2023) - IMDb\",\"url\":\"https://www.imdb.com/title/tt15398776/\",\"content\":\"Oppenheimer: Directed by Christopher Nolan. With Cillian Murphy, Emily Blunt, Robert Downey Jr., Alden Ehrenreich. The story of American scientist J. Robert Oppenheimer and his role in the development of the atomic bomb.\",\"score\":0.92995,\"raw_content\":null},{\"title\":\"'Oppenheimer' Review: A Man for Our Time - The New York Times\",\"url\":\"https://www.nytimes.com/2023/07/19/movies/oppenheimer-review-christopher-nolan.html\",\"content\":\"Instead, it is here that the film's complexities and all its many fragments finally converge as Nolan puts the finishing touches on his portrait of a man who contributed to an age of transformational scientific discovery, who personified the intersection of science and politics, including in his role as a Communist boogeyman, who was transformed by his role in the creation of weapons of mass destruction and soon after raised the alarm about the dangers of nuclear war.\\n He served as director of a clandestine weapons lab built in a near-desolate stretch of Los Alamos, in New Mexico, where he and many other of the era's most dazzling scientific minds puzzled through how to harness nuclear reactions for the weapons that killed tens of thousands instantly, ending the war in the Pacific.\\n Nolan integrates these black-and-white sections with the color ones, using scenes from the hearing and the confirmation — Strauss's role in the hearing and his relationship with Oppenheimer directly affected the confirmation's outcome — to create a dialectical synthesis. To signal his conceit, he stamps the film with the words \\\"fission\\\" (a splitting into parts) and \\\"fusion\\\" (a merging of elements); Nolan being Nolan, he further complicates the film by recurrently kinking up the overarching chronology — it is a lot.\\n It's also at Berkeley that Oppenheimer meets the project's military head, Leslie Groves (a predictably good Damon), who makes him Los Alamos's director, despite the leftist causes he supported — among them, the fight against fascism during the Spanish Civil War — and some of his associations, including with Communist Party members like his brother, Frank (Dylan Arnold).\\n\",\"score\":0.92512,\"raw_content\":null}]\n```\n\n----------------------------------------\n\nTITLE: Setting Cohere API Key Environment Variable\nDESCRIPTION: Command to set the Cohere API key as an environment variable for authentication with Cohere's services.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-cohere/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport COHERE_API_KEY=your-api-key\n```\n\n----------------------------------------\n\nTITLE: Installing Arcjet Redaction Library\nDESCRIPTION: Command to install the Arcjet Redaction library via package manager. This is required to use the redaction functionality with LangChain.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/arcjet.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n@arcjet/redact\n```\n\n----------------------------------------\n\nTITLE: Building the Redis Package\nDESCRIPTION: Command to build the @langchain/redis package using yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-redis/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nyarn build\n```\n\n----------------------------------------\n\nTITLE: Running Linter and Formatter\nDESCRIPTION: Command to run the linter and formatter to ensure code quality and style consistency for the @langchain/cohere package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-cohere/README.md#2025-04-22_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nyarn lint && yarn format\n```\n\n----------------------------------------\n\nTITLE: Running Unit and Integration Tests\nDESCRIPTION: Commands to run unit tests (.test.ts) and integration tests (.int.test.ts) for the @langchain/cohere package.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-cohere/README.md#2025-04-22_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn test\n$ yarn test:int\n```\n\n----------------------------------------\n\nTITLE: Setting Up Text Encoder and Decoder\nDESCRIPTION: Initializes TextEncoder and TextDecoder for converting data to and from Uint8Array format for storage.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/stores/file_system.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nconst encoder = new TextEncoder();\nconst decoder = new TextDecoder();\n```\n\n----------------------------------------\n\nTITLE: Importing CategoryTable and IndexTable Components in React/JSX\nDESCRIPTION: Imports React components named CategoryTable and IndexTable from the FeatureTables theme, which are used to display tables of tools and categories in the documentation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/tools/index.mdx#2025-04-22_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\nimport { CategoryTable, IndexTable } from \"@theme/FeatureTables\";\n```\n\n----------------------------------------\n\nTITLE: Installing @langchain/ollama and @langchain/core packages\nDESCRIPTION: Command to install the required packages using npm or yarn.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-ollama/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install @langchain/ollama @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing IPFS Datastore Dependencies\nDESCRIPTION: Command to install the necessary dependencies for using IPFS Datastore Chat Memory with LangChain.js, including IPFS interfaces and core libraries.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/memory/ipfs_datastore.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install cborg interface-datastore it-all @langchain/community @langchain/core\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with npm\nDESCRIPTION: Install required packages including langchain, OpenAI integration, cheerio for web scraping, and uuid.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/qa_chat_history_how_to.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm install --save langchain @langchain/openai langchain cheerio uuid\n```\n\n----------------------------------------\n\nTITLE: Installing Google WebAuth Package\nDESCRIPTION: Command to install the @langchain/google-webauth package using yarn package manager.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-google-webauth/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ yarn add @langchain/google-webauth\n```\n\n----------------------------------------\n\nTITLE: Installing browser-specific dependencies for Tencent Hunyuan\nDESCRIPTION: Command to install crypto-js, which is required when using the Tencent Hunyuan integration in browser environments. This dependency is essential for cryptographic operations when accessing the API from browsers.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/chat/tencent_hunyuan.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install crypto-js\n```\n\n----------------------------------------\n\nTITLE: Running Linter and Formatter\nDESCRIPTION: Command to run code linting and formatting checks.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-pinecone/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nyarn lint && yarn format\n```\n\n----------------------------------------\n\nTITLE: Using ChatCerebras Model in TypeScript\nDESCRIPTION: Code example showing how to import and initialize the ChatCerebras model, create a human message, and invoke the model to get a response.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/libs/langchain-cerebras/README.md#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChatCerebras } from \"@langchain/cerebras\";\nimport { HumanMessage } from \"@langchain/core/messages\";\n\nconst model = new ChatCerebras({\n  apiKey: process.env.CEREBRAS_API_KEY, // Default value.\n});\n\nconst message = new HumanMessage(\"What color is the sky?\");\n\nconst res = await model.invoke([message]);\n```\n\n----------------------------------------\n\nTITLE: Installing InMemoryStore Package\nDESCRIPTION: Command to install the @langchain/core package which contains the InMemoryStore implementation.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/integrations/stores/in_memory.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n@langchain/core\n```\n\n----------------------------------------\n\nTITLE: Setting LangSmith Environment Variables in TypeScript\nDESCRIPTION: Configures the LangSmith environment variables for API access and tracing capabilities, which are prerequisites for using LangSmith's similarity search features.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/how_to/example_selectors_langsmith.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nprocess.env.LANGSMITH_API_KEY=\"your-api-key\"\nprocess.env.LANGSMITH_TRACING=\"true\"\n```\n\n----------------------------------------\n\nTITLE: Defining Core Components Note Block in Markdown\nDESCRIPTION: Note block explaining the scope of components covered in Expression Language and Components sections.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/contributing/documentation/style_guide.mdx#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n:::note\nAs a general rule of thumb, everything covered in the `Expression Language` and `Components` sections (with the exception of the `Composition` section of components) should\ncover only components that exist in `@langchain/core`.\n:::\n```\n\n----------------------------------------\n\nTITLE: Rendering Top Reviewers Component\nDESCRIPTION: Renders the People component to display top reviewers of the project.\nSOURCE: https://github.com/langchain-ai/langchainjs/blob/main/docs/core_docs/docs/people.mdx#2025-04-22_snippet_1\n\nLANGUAGE: jsx\nCODE:\n```\n<People type=\"top_reviewers\"></People>\n```"
  }
]
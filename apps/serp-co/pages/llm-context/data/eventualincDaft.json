[
  {
    "owner": "eventual-inc",
    "repo": "daft",
    "content": "TITLE: Reading a Delta Lake Table into a Daft DataFrame in Python\nDESCRIPTION: Shows how to read an existing Delta Lake table from a given URI into a Daft DataFrame using the `daft.read_deltalake` function. This enables leveraging Daft's parallel and distributed reading capabilities on Delta Lake data sources across various storage backends including local and cloud.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/delta_lake.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Read Delta Lake table into a Daft DataFrame.\nimport daft\n\ndf = daft.read_deltalake(\"some-table\")\n```\n\n----------------------------------------\n\nTITLE: Reading and Writing S3 Tables with Daft using Iceberg\nDESCRIPTION: This example demonstrates how to connect to AWS S3 Tables using Daft's Catalog interface, list available tables, read table data, write new data to a table, and verify the write operation. The code requires AWS credentials to be properly configured for authentication.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/s3tables.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom daft import Catalog\n\n# ensure your aws credentials are configure, for example:\n# import os\n# os.environ[\"AWS_ACCESS_KEY_ID\"] = \"<access-id>\"\n# os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"<access-key>\"\n# os.environ[\"AWS_DEFAULT_REGION\"] = \"<region>\"\n\ncatalog = Catalog.from_s3tables(\"arn:aws:s3tables:<region>:<account>:bucket/<bucket>\")\n\n# verify we are connected\ncatalog.list_tables(\"demo\")\n\"\"\"\n['demo.points']\n\"\"\"\n\n# read some table\ncatalog.read_table(\"my_namespace.my_table\").show()\n\"\"\"\n╭─────────┬───────┬──────╮\n│ x       ┆ y     ┆ z    │\n│ ---     ┆ ---   ┆ ---  │\n│ Boolean ┆ Int64 ┆ Utf8 │\n╞═════════╪═══════╪══════╡\n│ true    ┆ 1     ┆ a    │\n├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌┤\n│ true    ┆ 2     ┆ b    │\n├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌┤\n│ false   ┆ 3     ┆ c    │\n╰─────────┴───────┴──────╯\n\n(Showing first 3 of 3 rows)\n\"\"\"\n\n# write dataframe to table\ncatalog.write_table(\n    \"demo.points\",\n    daft.from_pydict(\n        {\n            \"x\": [True],\n            \"y\": [4],\n            \"z\": [\"d\"],\n        }\n    ),\n)\n\n# check that the data was written\ncatalog.read_table(\"my_namespace.my_table\").show()\n\"\"\"\n╭─────────┬───────┬──────╮\n│ x       ┆ y     ┆ z    │\n│ ---     ┆ ---   ┆ ---  │\n│ Boolean ┆ Int64 ┆ Utf8 │\n╞═════════╪═══════╪══════╡\n│ true    ┆ 4     ┆ d    │\n├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌┤\n│ true    ┆ 1     ┆ a    │\n├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌┤\n│ true    ┆ 2     ┆ b    │\n├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌┤\n│ false   ┆ 3     ┆ c    │\n╰─────────┴───────┴──────╯\n\n(Showing first 4 of 4 rows)\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Creating a DataFrame from Python Dictionary with Daft - Python\nDESCRIPTION: Illustrates creating a Daft DataFrame from a Python dictionary where keys are column names and values are column lists. Requires Daft and native Python types. Accepts a dictionary mapping columns to their values and returns a DataFrame. Treats all lists as column data; mismatched lengths may cause errors.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/api/dataframe_creation.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport daft\n\ndata = {\"col1\": [1, 2], \"col2\": [3, 4]}\ndf = daft.from_pydict(data)\n```\n\n----------------------------------------\n\nTITLE: Reading from SQL Table into DataFrame with Daft - Python\nDESCRIPTION: Provides an example of reading data from a SQL-based database directly into a Daft DataFrame, streamlining ETL tasks. Requires daft, a supported SQL database, and appropriate drivers. Inputs include SQL query and connection string; result is a DataFrame with queried data. Connection issues or unsupported queries are potential constraints.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/api/dataframe_creation.md#_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nimport daft\n\ndf = daft.read_sql(\"SELECT * FROM users\", connection=\"postgresql://user:pass@host:port/db\")\n```\n\n----------------------------------------\n\nTITLE: Creating a DataFrame from PyArrow Table with Daft - Python\nDESCRIPTION: Shows converting an existing PyArrow Table to a Daft DataFrame, facilitating interoperability with the Arrow ecosystem. Requires daft and pyarrow installed. Input is a pyarrow.Table object, output is a Daft DataFrame. The schema is retained from the Arrow table; depends on pyarrow compatibility.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/api/dataframe_creation.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport daft\nimport pyarrow as pa\n\ntable = pa.table({\"foo\": [1, 2], \"bar\": [3, 4]})\ndf = daft.from_arrow(table)\n```\n\n----------------------------------------\n\nTITLE: Reading from Delta Lake Table into DataFrame with Daft - Python\nDESCRIPTION: Demonstrates loading a Delta Lake table into a Daft DataFrame, compatible with databricks Delta Lake tables. Requires daft and appropriate Delta Lake connectors. Expects a table path or URI; returns a DataFrame. Schema evolution or unsupported features in some Delta versions may be a constraint.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/api/dataframe_creation.md#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nimport daft\n\ndf = daft.read_deltalake(\"/data/delta_table\")\n```\n\n----------------------------------------\n\nTITLE: Grouped Aggregation Example\nDESCRIPTION: This snippet demonstrates grouped aggregation in Daft, computing the mean of the 'score' column grouped by the 'class' column.  It shows how to apply aggregation to specific groups within the DataFrame and then displays the results.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_59\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby(\"class\").mean(\"score\").show()\n```\n\n----------------------------------------\n\nTITLE: Reading an Iceberg Table into a Daft DataFrame\nDESCRIPTION: This snippet shows how to read a PyIceberg table into a Daft DataFrame using the `daft.read_iceberg` function. It requires a PyIceberg `Table` object as input and the `daft` library. The resulting DataFrame can then be used for further data manipulation and analysis within Daft.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/iceberg.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Create a Daft Dataframe\nimport daft\n\ndf = daft.read_iceberg(table)\n```\n\n----------------------------------------\n\nTITLE: Reading from Iceberg Table into DataFrame with Daft - Python\nDESCRIPTION: Covers loading data from an Apache Iceberg table via Daft. Prerequisites include daft, iceberg integration dependencies, and a correctly configured catalog URI. Input is typically a table or catalog URI, and output is a DataFrame with table contents. Connectivity or table format errors may occur with misconfigured catalogs.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/api/dataframe_creation.md#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nimport daft\n\ndf = daft.read_iceberg(table_uri=\"iceberg://catalog/mytable\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Daft S3 IO Configuration in Python\nDESCRIPTION: This snippet imports Daft and configures IO for anonymous S3 access using IOConfig and S3Config. It sets the planning configuration to use this IO, enabling direct access to public S3 buckets. Dependencies: daft library. Required parameters: S3 bucket public access. Limitations: only works with public S3 data unless proper credentials are supplied.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/data-ai-summit-2024.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport daft\n\nIO_CONFIG = daft.io.IOConfig(s3=daft.io.S3Config(anonymous=True))  # Use anonymous S3 access\n\ndaft.set_planning_config(default_io_config=IO_CONFIG)\n```\n\n----------------------------------------\n\nTITLE: Reading DeltaLake Table - Daft - Python\nDESCRIPTION: Reads data from the specified DeltaLake table located on S3 using the previously configured `io_config` which contains AWS credentials. The result is loaded into a Daft DataFrame. Requires Daft installed and the `io_config` object.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/1-local-image-batch-inference.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndf = daft.read_deltalake(\"s3://daft-public-datasets/imagenet/val-10k-sample-deltalake/\", io_config=io_config)\ndf\n```\n\n----------------------------------------\n\nTITLE: Reading from Lance Dataset into DataFrame with Daft - Python\nDESCRIPTION: Covers loading datasets stored in the Lance format into a Daft DataFrame. Requires daft and the lance extension installed. Input is a file or dataset path; returns a DataFrame. Ripple effects may result from unsupported dataset metadata.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/api/dataframe_creation.md#_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\nimport daft\n\ndf = daft.read_lance(\"/data/lance_dataset\")\n```\n\n----------------------------------------\n\nTITLE: Applying a UDF (Python)\nDESCRIPTION: This snippet shows how to apply a User-Defined Function (UDF) in Daft to manipulate a column.  It uses the `@daft.udf` decorator to define a custom function and then applies it to a column using `with_column`. This demonstrates how to extend Daft with custom computations.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/migration/dask_migration.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@daft.udf(...)\n\ndef crop_image(**kwargs):\n    return ...\n\n\ndf = df.with_column(\n    \"cropped\",\n    crop_image(daft.col(\"image\"), **kwargs),\n)\n```\n\n----------------------------------------\n\nTITLE: Querying Python DataFrame by Name with daft.sql - Python\nDESCRIPTION: Shows how to reference a DataFrame variable directly in an SQL query using daft.sql. No setup of explicit temporary tables is needed; any DataFrame in the current scope can be queried by its variable name. Requires a Daft DataFrame created by daft.from_pydict, and the daft library installed. The SQL selects columns by name, returning a new DataFrame which is then displayed with .show(). Input is a dictionary-form DataFrame; output is a DataFrame rendered to standard output.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql_overview.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# Note the variable name `my_special_df`\nmy_special_df = daft.from_pydict({\"A\": [1, 2, 3], \"B\": [1, 2, 3]})\n\n# Use the SQL table name \"my_special_df\" to refer to the above DataFrame!\nsql_df = daft.sql(\"SELECT A, B FROM my_special_df\")\n\nsql_df.show()\n```\n\n----------------------------------------\n\nTITLE: Selecting Columns in DataFrame\nDESCRIPTION: This snippet selects specific columns from a DataFrame using the `select` method. It filters the existing DataFrame `df`, retaining only columns \"A\", \"B\", and \"C\". The output displays the schema, which is a list of column names with their respective data types.  It showcases the initial state before the DataFrame is materialized.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf = df.select(\"A\", \"B\", \"C\")\ndf\n```\n\n----------------------------------------\n\nTITLE: Retrieving DeltaLake data with AWS credentials\nDESCRIPTION: Sets up AWS credentials via boto3, constructs an IOConfig with these credentials for S3 access, and reads DeltaLake data into a Daft DataFrame. The data is then pruned to a specific row limit and filtered where the 'object' list has exactly one item.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/2-distributed-batch-inference.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport boto3\n\nimport daft\n\nsession = boto3.session.Session()\ncreds = session.get_credentials()\nio_config = daft.io.IOConfig(\n    s3=daft.io.S3Config(\n        access_key=creds.secret_key,\n        key_id=creds.access_key,\n        session_token=creds.token,\n        region_name=\"us-west-2\",\n    )\n)\n\n# Retrieve data\ndf = daft.read_deltalake(\"s3://daft-public-datasets/imagenet/val-10k-sample-deltalake/\", io_config=io_config)\n\n# Prune data\ndf = df.limit(NUM_ROWS)\ndf = df.where(df[\"object\"].list.length() == 1)\n```\n\n----------------------------------------\n\nTITLE: Creating a DataFrame from Python Dictionary\nDESCRIPTION: Demonstrates how to create a Daft DataFrame from a Python dictionary with various data types including integers, floats, booleans, strings, dates, and lists.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/10min.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\n\ndf = daft.from_pydict(\n    {\n        \"integers\": [1, 2, 3, 4],\n        \"floats\": [1.5, 2.5, 3.5, 4.5],\n        \"bools\": [True, True, False, False],\n        \"strings\": [\"a\", \"b\", \"c\", \"d\"],\n        \"bytes\": [b\"a\", b\"b\", b\"c\", b\"d\"],\n        \"dates\": [\n            datetime.date(1994, 1, 1),\n            datetime.date(1994, 1, 2),\n            datetime.date(1994, 1, 3),\n            datetime.date(1994, 1, 4),\n        ],\n        \"lists\": [[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]],\n        \"nulls\": [None, None, None, None],\n    }\n)\n\ndf\n```\n\n----------------------------------------\n\nTITLE: Downloading Images from URLs Using Daft's URL Download UDF with Ray in Python\nDESCRIPTION: Adds a new column 'images' to the dataframe by applying the '.url.download()' user-defined function to the 'URL' column, which downloads images at each URL. The download UDF handles errors by returning null values. Collecting the dataframe triggers execution. This snippet exemplifies parallel downloading of data, benefiting from Ray's multi-core capabilities to significantly reduce I/O-bound processing time.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/text_to_image/using_cloud_with_ray.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimages_df = parquet_df.with_column(\"images\", col(\"URL\").url.download(on_error=\"null\"))\nimages_df.collect()\n```\n\n----------------------------------------\n\nTITLE: Defining a UDF for image classification with ResNet50\nDESCRIPTION: Creates a user-defined function (UDF) in Daft that loads a pretrained ResNet50 model, preprocesses images, performs inference, and maps the predicted class indices to human-readable categories. This UDF can be applied across the DataFrame.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/2-distributed-batch-inference.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n@daft.udf(return_dtype=daft.DataType.string())\nclass ClassifyImage:\n    def __init__(self):\n        weights = ResNet50_Weights.DEFAULT\n        self.model = resnet50(weights=weights)\n        self.model.eval()\n        self.preprocess = weights.transforms()\n        self.category_map = weights.meta[\"categories\"]\n\n    def __call__(self, images: daft.Series, shape: list[int, int, int]):\n        if len(images) == 0:\n            return []\n\n        # Convert the Daft Series into a list of Numpy arrays\n        data = images.cast(daft.DataType.tensor(daft.DataType.uint8(), tuple(shape))).to_pylist()\n\n        # Convert the numpy arrays into a torch tensor\n        images_array = torch.tensor(np.array(data)).permute((0, 3, 1, 2))\n\n        # Run the model, and map results back to a human-readable string\n        batch = self.preprocess(images_array)\n        prediction = self.model(batch).softmax(0)\n        class_ids = prediction.argmax(1)\n        prediction[:, class_ids]\n        return [self.category_map[class_id] for class_id in class_ids]\n```\n\n----------------------------------------\n\nTITLE: Selecting Columns in DataFrame using SQL\nDESCRIPTION: This snippet demonstrates selecting specific columns from a DataFrame using SQL syntax. The `daft.sql` function allows running SQL queries against the Daft DataFrame. In this instance, it selects columns \"A\", \"B\", and \"C\" from DataFrame `df`. The output will also show the DataFrame's schema including the column names and their types, without displaying the rows (since it is lazily evaluated).\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.sql(\"SELECT A, B, C FROM df\")\ndf\n```\n\n----------------------------------------\n\nTITLE: Showing DataFrame with Predictions - Daft - Python\nDESCRIPTION: Triggers computation and displays the first 4 rows of the Daft DataFrame, now including the added prediction columns for both low-resolution and high-resolution images. Used to inspect the results of the batch inference step.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/1-local-image-batch-inference.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\ndf.show(4)\n```\n\n----------------------------------------\n\nTITLE: Creating a Daft DataFrame from a Python Dictionary\nDESCRIPTION: This snippet demonstrates how to create a basic Daft DataFrame using `daft.from_pydict`. It takes a Python dictionary mapping column names (strings) to lists of values and returns a `daft.DataFrame` instance. The final line `df` typically triggers the DataFrame's representation in an interactive environment, showing its schema and potentially some data if executed eagerly or already collected.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/quickstart.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport daft\n\ndf = daft.from_pydict({\n    \"A\": [1, 2, 3, 4],\n    \"B\": [1.5, 2.5, 3.5, 4.5],\n    \"C\": [True, True, False, False],\n    \"D\": [None, None, None, None],\n})\n\ndf\n```\n\n----------------------------------------\n\nTITLE: Reading a Parquet File into a DataFrame with Daft - Python\nDESCRIPTION: Demonstrates loading a Parquet file using Daft's read_parquet API. Requires daft installed; additional dependencies may be needed for remote storage access. Input is a path or URI to a parquet file, output is a DataFrame representing file content. File paths should be valid and accessible.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/api/dataframe_creation.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nimport daft\n\ndf = daft.read_parquet(\"/path/to/file.parquet\")\n```\n\n----------------------------------------\n\nTITLE: Grouping and Aggregating Data in a Daft DataFrame using Python\nDESCRIPTION: This snippet demonstrates how to group a Daft DataFrame by the 'country' column using `groupby()` and then perform aggregations using `agg()` with Daft Expressions (`daft.col`). It calculates the mean of the 'age' column (aliased as `avg_age`) and counts the non-null entries in the 'has_dog' column for each country. The resulting aggregated DataFrame is displayed using `show()`. Depends on an existing Daft DataFrame `df` and the `daft` library.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/quickstart.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ngrouped = df.groupby(\"country\").agg(\n    daft.col(\"age\").mean().alias(\"avg_age\"),\n    daft.col(\"has_dog\").count()\n).show()\n```\n\n----------------------------------------\n\nTITLE: Select Specific Column - Daft DataFrame (Python)\nDESCRIPTION: This snippet initializes a Daft DataFrame and then uses the `select()` method to choose specific columns. In this case, it selects only column \"A\" and displays the resulting DataFrame, which contains only the specified column. Requires the `daft` library.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport daft\n\ndf = daft.from_pydict({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n\ndf.select(\"A\").show()\n```\n\n----------------------------------------\n\nTITLE: Downloading Data from URLs\nDESCRIPTION: This snippet showcases how to download data from a column of URLs using `.url.download()`. It creates a DataFrame with a column of URLs, then downloads the content from each URL, creating a new column with the downloaded data. It leverages Daft's distributed capabilities.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_55\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.from_pydict({\n    \"image_urls\": [\n        \"https://example.com/image1.jpg\",\n        \"https://example.com/image2.jpg\",\n        \"https://example.com/image3.jpg\"\n    ]\n})\n\ndf = df.with_column(\"image_data\", df[\"image_urls\"].url.download())\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Adding a Column Using Expressions (Python)\nDESCRIPTION: This code snippet demonstrates how to add a new column to a Daft DataFrame and compute its values using a simple arithmetic operation (adding 1).  It uses Daft's `with_column` method along with `daft.col` and the `+` operator, showcasing the Expressions API.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/migration/dask_migration.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf = df.with_column(\"A_add_one\", daft.col(\"A\") + 1)\n```\n\n----------------------------------------\n\nTITLE: Cross Column Aggregations\nDESCRIPTION: This snippet demonstrates the use of cross-column aggregation functions from `daft.functions` like `columns_min`, `columns_max`, `columns_mean`, and `columns_sum`. It creates a DataFrame, applies these functions across the specified columns, and adds new columns with the aggregated results.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_61\n\nLANGUAGE: python\nCODE:\n```\nimport daft\nfrom daft.functions import columns_min, columns_max, columns_mean, columns_sum\n\ndf = daft.from_pydict({\n    \"a\": [1, 2, 3],\n    \"b\": [4, 5, 6],\n    \"c\": [7, 8, 9]\n})\n\n# Create new columns with cross-column aggregations\ndf = df.with_columns({\n    \"min_value\": columns_min(\"a\", \"b\", \"c\"),\n    \"max_value\": columns_max(\"a\", \"b\", \"c\"),\n    \"mean_value\": columns_mean(\"a\", \"b\", \"c\"),\n    \"sum_value\": columns_sum(\"a\", \"b\", \"c\")\n})\n\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Partitioning data for parallel processing\nDESCRIPTION: Splits the DataFrame into 16 partitions to enable parallel data processing in a distributed environment, improving computational scalability and efficiency.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/2-distributed-batch-inference.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ndf = df.into_partitions(16)\n```\n\n----------------------------------------\n\nTITLE: Starting a Local Ray Cluster Using Bash\nDESCRIPTION: Shows the commands required to install Ray with default settings and start a local single-node Ray cluster. This is a prerequisite to use Daft with a locally managed Ray cluster. The `ray start` command initializes the cluster with a head node listening on port 6379. Includes expected output to verify successful startup.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/distributed.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install ray[default]\nray start --head --port=6379\n```\n\n----------------------------------------\n\nTITLE: Downloading and Resizing Images in DataFrame with daft in Python\nDESCRIPTION: Adds 'image_url' column by concatenating base URL with filename; downloads images, decodes them, and resizes to 256x256 pixels. Prepares image data for training or analysis.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/3-pytorch-ray-single-node-training.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ndf = df.with_column(\n    \"image_url\", \"s3://daft-public-datasets/imagenet/val-10k-sample-deltalake/images/\" + df[\"filename\"] + \".jpeg\"\n)\ndf = df.with_column(\"image\", df[\"image_url\"].url.download().image.decode())\ndf = df.with_column(\"image\", df[\"image\"].image.resize(256, 256))\n```\n\n----------------------------------------\n\nTITLE: Transforming Columns within Select in a Daft DataFrame using Python\nDESCRIPTION: This snippet demonstrates an alternative way to transform and select columns simultaneously using Daft Expressions (`daft.col`) directly within the `select()` method. It creates a `full_name` column by concatenating `first_name` and `last_name` (using `alias` to name the new column) on the fly and selects other existing columns (`age`, `country`, `has_dog`). The result is displayed using `show()`. Depends on an existing Daft DataFrame `df` and the `daft` library.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/quickstart.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndf.select((daft.col(\"first_name\").alias(\"full_name\") + \" \" + daft.col(\"last_name\")), \"age\", \"country\", \"has_dog\").show()\n```\n\n----------------------------------------\n\nTITLE: Composing Numeric Expressions (Python)\nDESCRIPTION: This example shows how to compose numeric expressions in Daft using Python syntax. It adds 1 to column 'A', divides column 'A' by 2, and checks if each element in column 'A' is greater than 1. The input is a Daft DataFrame with a numeric column, and the output is a DataFrame with new columns representing the results of the operations.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n# Add 1 to each element in column \"A\"\ndf = df.with_column(\"A_add_one\", df[\"A\"] + 1)\n\n# Divide each element in column A by 2\ndf = df.with_column(\"A_divide_two\", df[\"A\"] / 2.)\n\n# Check if each element in column A is more than 1\ndf = df.with_column(\"A_gt_1\", df[\"A\"] > 1)\n\ndf.collect()\n```\n\n----------------------------------------\n\nTITLE: Selecting from Delimited Identifiers with Special Characters - SQL\nDESCRIPTION: This SQL snippet demonstrates selecting data using delimited identifiers containing special characters (including Unicode emojis), only possible using the delimited syntax. No dependencies beyond valid catalog and column names that match those in the query. Input: table and column names with special characters. Output: result set from select query.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql/identifiers.md#_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\n-- delimited identifier with special characters\nSELECT \"🍺\" FROM \"🍻\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Daft to use Ray runner for distributed execution\nDESCRIPTION: Imports the Daft library and sets up the execution context to use Ray as the runner. It optionally allows specifying a custom Ray cluster address; if None, defaults to local cluster. This enables distributed computation over a Ray cluster.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/2-distributed-batch-inference.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport daft\n\n# If you have your own Ray cluster running, feel free to set this to that address!\n# RAY_ADDRESS = \"ray://localhost:10001\"\nRAY_ADDRESS = None\n\ndaft.context.set_runner_ray(address=RAY_ADDRESS)\n```\n\n----------------------------------------\n\nTITLE: Comparison Expressions - Python\nDESCRIPTION: Compares two columns 'A' and 'B' for equality and stores the result in a new boolean column 'A_eq_B'. It initializes a Daft DataFrame with two integer columns, 'A' and 'B', then uses the `==` operator to compare corresponding elements of the two columns. The resulting boolean values are stored in a new column named 'A_eq_B'.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.from_pydict({\"A\": [1, 2, 3], \"B\": [1, 2, 4]})\n\ndf = df.with_column(\"A_eq_B\", df[\"A\"] == df[\"B\"])\n\ndf.collect()\n```\n\n----------------------------------------\n\nTITLE: Applying the Image Classification UDF\nDESCRIPTION: Applies the `ClassifyImages` UDF to the 'image_2d' column of the Daft DataFrame to generate a new 'model_classification' column containing the model's predicted labels.  It then displays the resulting DataFrame with the added column.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/mnist.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclassified_images_df = images_df.with_column(\"model_classification\", ClassifyImages(col(\"image_2d\")))\n\nclassified_images_df.show(10)\n```\n\n----------------------------------------\n\nTITLE: Materializing DataFrame with Downloaded Data (Python)\nDESCRIPTION: Executes the query plan, including the file download operation, using `.collect()`. This downloads the actual image bytes from S3 and materializes the DataFrame (including the new 'image' column containing bytes) in memory.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/image_querying/top_n_red_color.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Materialize the dataframe, so that we don't have to hit S3 again for subsequent operations\ndf.collect()\n```\n\n----------------------------------------\n\nTITLE: Apply UDF to Column\nDESCRIPTION: This snippet uses the `.apply()` method to apply a Python function to each row in a column. The example flattens the image in each row and creates a new column, `flattened_image`.  The `return_dtype` parameter is set to `daft.DataType.python()` to indicate the return type of the UDF.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_63\n\nLANGUAGE: python\nCODE:\n```\ndf.with_column(\n    \"flattened_image\",\n    df[\"image\"].apply(lambda img: img.flatten(), return_dtype=daft.DataType.python())\n).show(2)\n```\n\n----------------------------------------\n\nTITLE: Creating a Dogs DataFrame from Python Dictionary\nDESCRIPTION: Constructs a new DataFrame 'df_dogs' containing URLs, full names, and dog names using daft.from_pydict for subsequent joins and data enrichment.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/10min.ipynb#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\ndf_dogs = daft.from_pydict(\n    {\n        \"urls\": [\n            \"https://live.staticflickr.com/65535/53671838774_03ba68d203_o.jpg\",\n            \"https://live.staticflickr.com/65535/53671700073_2c9441422e_o.jpg\",\n            \"https://live.staticflickr.com/65535/53670606332_1ea5f2ce68_o.jpg\",\n            \"https://live.staticflickr.com/65535/53671838039_b97411a441_o.jpg\",\n            \"https://live.staticflickr.com/65535/53671698613_0230f8af3c_o.jpg\",\n        ],\n        \"full_name\": [\n            \"Ernesto Evergreen\",\n            \"James Jale\",\n            \"Wolfgang Winter\",\n            \"Shandra Shamas\",\n            \"Zaya Zaphora\",\n        ],\n        \"dog_name\": [\"Ernie\", \"Jackie\", \"Wolfie\", \"Shaggie\", \"Zadie\"],\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame with String Column - Python\nDESCRIPTION: Creates a Daft DataFrame from a Python dictionary containing a string column named 'B' with values 'foo', 'bar', and 'baz'. The `daft.from_pydict` function is used to initialize the DataFrame with the given data. The `df.show()` method then displays the DataFrame's contents.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.from_pydict({\"B\": [\"foo\", \"bar\", \"baz\"]})\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Using Daft to Create a Dataframe\nDESCRIPTION: Creates a Daft dataframe from a Python dictionary. This is used to create a sample dataframe to write to Iceberg. It defines the data for `name` and `age` columns, allowing for easy demonstration of the write functionality.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/iceberg_summit_2024.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport daft\n\ndf = daft.from_pydict(\n    {\n        \"name\": [\"jay\", \"sammy\", \"brian\"],\n        \"age\": [30, 31, 32],\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Expressions: Column Reference (Python)\nDESCRIPTION: This example demonstrates how to create an expression that refers to a column in a Daft DataFrame using `daft.col()`. The expression can then be used in various DataFrame operations. The input is a column name string. The output is a Daft Expression object.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# Refers to column \"A\"\ndaft.col(\"A\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Daft Session and Executing SQL Join - Python\nDESCRIPTION: This snippet demonstrates initializing a Daft Session, creating temporary tables from Python dictionaries using daft.from_pydict, and executing a SQL query to perform a Cartesian product (join) between the tables. It requires the daft library and its dependency on a compatible Python environment. The key parameters include the table names, data source dictionaries, and the SQL command string. The output is rendered as a table and displayed via .show(); only in-memory data is demonstrated. Large datasets may require different handling.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql_overview.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport daft\n\nfrom daft import Session\n\n# create a session\nsess = Session()\n\n# create temp tables\nsess.create_temp_table(\"T\", daft.from_pydict({ \"a\": [ 0, 1 ] }))\nsess.create_temp_table(\"S\", daft.from_pydict({ \"b\": [ 1, 0 ] }))\n\n# execute sql\nsess.sql(\"SELECT * FROM T, S\").show()\n\"\"\"\n╭───────┬───────╮\n│ a     ┆ b     │\n│ ---   ┆ ---   │\n│ Int64 ┆ Int64 │\n╞═══════╪═══════╡\n│ 0     ┆ 1     │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n│ 1     ┆ 1     │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n│ 0     ┆ 0     │\n├╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┤\n│ 1     ┆ 0     │\n╰───────┴───────╯\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Installing the Daft Library using Pip\nDESCRIPTION: This command uses the Python package installer `pip` to install the `daft` library, making it available for use in a Python environment. It assumes `pip` is installed and accessible in the terminal or notebook environment.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/quickstart.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npip install daft\n```\n\n----------------------------------------\n\nTITLE: Class UDF with resource requests\nDESCRIPTION: This code defines a class-based UDF that requests a specific number of GPUs. The `RunModelWithOneGPU` class is decorated with `@daft.udf` and specifies `num_gpus=1`. This ensures that the UDF is executed on a machine with at least one GPU. The UDF is then applied to a DataFrame using `df.with_column`.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_67\n\nLANGUAGE: Python\nCODE:\n```\n@daft.udf(return_dtype=daft.DataType.int64(), num_gpus=1)\nclass RunModelWithOneGPU:\n\n    def __init__(self):\n        # Perform expensive initializations\n        self._model = create_model()\n\n    def __call__(self, features_col):\n        return self._model(features_col)\n```\n\nLANGUAGE: Python\nCODE:\n```\ndf = df.with_column(\n    \"image_classifications\",\n    RunModelWithOneGPU(df[\"images\"]),\n)\n```\n\n----------------------------------------\n\nTITLE: Read SQL Query into Daft DataFrame (Python)\nDESCRIPTION: This code demonstrates how to read data from a SQL query into a Daft DataFrame using `daft.read_sql()`. It specifies the SQL query and the database URL as arguments. It imports the `daft` library.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/sql.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Read SQL query into Daft DataFrame\nimport daft\n\ndf = daft.read_sql(\n    \"SELECT * FROM books\",\n    \"sqlite://example.db\",\n)\n```\n\n----------------------------------------\n\nTITLE: Creating New Columns with Expressions\nDESCRIPTION: Demonstrates column transformation using expressions to concatenate first and last names into a full name column.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/10min.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf = df.with_column(\"full_name\", daft.col(\"first_name\") + \" \" + daft.col(\"last_name\"))\ndf.select(\"full_name\", \"age\", \"country\", \"has_dog\").show()\n```\n\n----------------------------------------\n\nTITLE: Performing Temporal Arithmetic with Daft DataFrame in Python\nDESCRIPTION: This snippet demonstrates how to create a Daft DataFrame from a Python dictionary of datetime objects, perform element-wise arithmetic with timestamps (adding 10 seconds), and display the result. It requires the `daft` library and Python's built-in `datetime`. The input is a dictionary with 'timestamp' values as datetimes, and the output is a DataFrame with an added column for the new timestamps after arithmetic. User must ensure Daft is installed and properly imported.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_44\n\nLANGUAGE: Python\nCODE:\n```\nimport datetime\n\ndf = daft.from_pydict({\n    \"timestamp\": [\n        datetime.datetime(2021, 1, 1, 0, 1, 1),\n        datetime.datetime(2021, 1, 1, 0, 1, 59),\n        datetime.datetime(2021, 1, 1, 0, 2, 0),\n    ]\n})\n\n# Add 10 seconds to each timestamp\ndf = df.with_column(\n    \"plus_10_seconds\",\n    df[\"timestamp\"] + datetime.timedelta(seconds=10)\n)\n\ndf.show()\n\n```\n\n----------------------------------------\n\nTITLE: Filtering and Showing Divergent Predictions - Daft - Python\nDESCRIPTION: Applies two filtering steps: first, it filters out rows where the \"image\" column does not have exactly 3 channels, then it filters to keep only rows where the \"predictions_lowres\" and \"predictions_highres\" columns have different values. Finally, it displays the first 4 rows of the filtered DataFrame. Note that `show` triggers computation on the entire filtered dataset.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/1-local-image-batch-inference.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\n# Filter out images where the number of channels != 3\ndf = df.where(df[\"image\"].apply(lambda img: img.shape[2] == 3, return_dtype=daft.DataType.bool()))\n\n# Show only rows where the predictions on the low-res/high-res images don't match\ndf = df.where(df[\"predictions_lowres\"] != df[\"predictions_highres\"])\n\ndf.show(4)\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame from Python Dictionary\nDESCRIPTION: This snippet creates a Daft DataFrame from a Python dictionary. The dictionary keys become column names, and the values are the column data. It demonstrates the basic structure for creating a DataFrame with different data types including integers, floats, booleans, and null values.  The `daft.from_pydict` function is used to construct the DataFrame from the dictionary.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport daft\n\ndf = daft.from_pydict({\n    \"A\": [1, 2, 3, 4],\n    \"B\": [1.5, 2.5, 3.5, 4.5],\n    \"C\": [True, True, False, False],\n    \"D\": [None, None, None, None],\n})\n```\n\n----------------------------------------\n\nTITLE: Loading and Reading AWS Glue Tables using Daft in Python\nDESCRIPTION: This Python snippet demonstrates how to load an AWS Glue catalog using Daft's built-in API, retrieve a specific table by namespace and table name, and read it into a Daft DataFrame. Dependencies include the Daft library with its Glue catalog integration and valid AWS credentials for accessing Glue. The main function is 'load_glue', with required parameters like 'name' (the catalog name) and 'region_name' (the AWS region). Expected inputs are the Glue catalog and table names; the output is a Daft DataFrame that can be further queried. Limitations: Only CSV, Parquet, Iceberg, and Delta Lake table formats have support and early API stability warnings are present.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/glue.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom daft.catalog.__glue import load_glue\n\n# load a glue catalog instance\ncatalog = load_glue(\n    name=\"my_glue_catalog\",\n    region_name=\"us-west-2\"\n)\n\n# load a glue table\ntbl = catalog.get_table(\"my_namespace.my_table\")\n\n# read the table as a daft dataframe\ndf = tbl.read()\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Decoding Image Bytes to Image Objects with Daft DataFrame - Python\nDESCRIPTION: This snippet showcases how to convert raw image byte data into human-readable image representations within a Daft DataFrame. It uses the image.decode() expression on the 'image_bytes' column, creating a new 'image' column containing decoded image objects. The Daft Python library is required and the input DataFrame must have a valid 'image_bytes' column. Expected input: 'image_bytes' (binary). Output: 'image' (decoded image object). Limitations: corrupted or null image bytes may result in errors or null outputs.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_72\n\nLANGUAGE: Python\nCODE:\n```\ndf_family = df_family.with_column(\"image\", daft.col(\"image_bytes\").image.decode())\ndf_family.show()\n```\n\n----------------------------------------\n\nTITLE: Adding Image Columns - Daft - Python\nDESCRIPTION: Adds two new columns to the DataFrame. The first column, \"image_url\", is constructed by concatenating a base S3 path with the \"filename\" column. The second column, \"image\", downloads the content from the URLs in \"image_url\" and decodes them as images using Daft's URL and image namespaces. Requires a Daft DataFrame with a \"filename\" column.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/1-local-image-batch-inference.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ndf = df.with_column(\n    \"image_url\", \"s3://daft-public-datasets/imagenet/val-10k-sample-deltalake/images/\" + df[\"filename\"] + \".jpeg\"\n)\ndf = df.with_column(\"image\", df[\"image_url\"].url.download().image.decode())\n```\n\n----------------------------------------\n\nTITLE: Listing Files with Daft and Reading CSV\nDESCRIPTION: This code snippet demonstrates listing files and reading CSV data using Daft's `from_glob_path` and `read_csv` functions respectively. The `from_glob_path` method allows to list all files matching glob pattern.  This demonstrates how to read multiple CSV files from a pattern.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/pydata_global_2023.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\ndf = daft.from_glob_path(\"s3://daft-public-datasets/tpch-lineitem/10k-1mb-csv-files/**/*.csv\")\ndf.collect()\n```\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.read_csv(\"s3://daft-public-datasets/tpch-lineitem/10k-1mb-csv-files/**/*.csv\")\n```\n\nLANGUAGE: python\nCODE:\n```\nprint(df.num_partitions())\n```\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\ndf.collect()\n```\n\n----------------------------------------\n\nTITLE: Creating a DataFrame from pandas DataFrame with Daft - Python\nDESCRIPTION: Describes converting a pandas.DataFrame to a Daft DataFrame, aiding transition from legacy or exploratory workflows. Requires daft and pandas. Takes a pandas.DataFrame as input and yields a Daft DataFrame. Some pandas data types or extensions may not be fully supported.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/api/dataframe_creation.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport daft\nimport pandas as pd\n\ndf_pd = pd.DataFrame({\"a\": [1, 2], \"b\": [3, 4]})\ndf = daft.from_pandas(df_pd)\n```\n\n----------------------------------------\n\nTITLE: Filtering and Partition Pruning in Daft DataFrame on Delta Lake Tables Using Python\nDESCRIPTION: Illustrates applying a filter on the partition column 'group' of a Daft DataFrame read from a Delta Lake table. Daft automatically uses partition pruning to skip reading data files from partitions that don't match the filter condition, significantly improving read efficiency.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/delta_lake.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Filter on partition columns will result in efficient partition pruning; non-matching partitions will be skipped.\ndf2 = df.where(df[\"group\"] == 2)\ndf2.show()\n```\n\n----------------------------------------\n\nTITLE: Applying Classification UDF - Daft - Python\nDESCRIPTION: Applies the `ClassifyImage` UDF to the \"image_resized_small\" and \"image_resized_large\" columns of the DataFrame. It passes the image data and its expected shape to the UDF, adding the resulting classification predictions as new columns \"predictions_lowres\" and \"predictions_highres\". Requires the Daft DataFrame with resized image columns and the defined `ClassifyImage` UDF.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/1-local-image-batch-inference.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ndf = df.with_column(\"predictions_lowres\", ClassifyImage(df[\"image_resized_small\"], [32, 32, 3]))\ndf = df.with_column(\"predictions_highres\", ClassifyImage(df[\"image_resized_large\"], [256, 256, 3]))\n```\n\n----------------------------------------\n\nTITLE: Materializing a Daft DataFrame using collect()\nDESCRIPTION: This snippet calls the `collect()` method on a Daft DataFrame (`df`). This action triggers the execution of the lazy computation plan defined by previous operations (like reading data) and brings the entire resulting dataset into the memory of the client process. This allows direct access to all the data but should be used cautiously with large datasets.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/quickstart.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf.collect()\n```\n\n----------------------------------------\n\nTITLE: Filtering on Non-Partition Columns with File-Level Pruning in Daft Using Python\nDESCRIPTION: Demonstrates applying a filter on a non-partition column 'num' in a Daft DataFrame. Daft leverages file-level column statistics to prune entire files from being read if their min/max column values cannot satisfy the filter predicate, reducing unnecessary I/O.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/delta_lake.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Filter on non-partition column, relying on file-level column stats to efficiently prune unnecessary file reads.\ndf3 = df.where(df[\"num\"] < 2)\ndf3.show()\n```\n\n----------------------------------------\n\nTITLE: Performing Temporal Arithmetic with Daft SQL API in Python\nDESCRIPTION: This code shows how to use Daft’s SQL API in Python to perform timestamp arithmetic, adding an interval to each timestamp field. It constructs a DataFrame from a dictionary and issues a SQL query over it. Requires both the `daft` and `datetime` modules. The SQL statement adds 10 seconds to each timestamp, returning a DataFrame with the new column. Note that SQL strings are issued through `daft.sql` in this approach.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_45\n\nLANGUAGE: Python\nCODE:\n```\nimport datetime\n\ndf = daft.from_pydict({\n    \"timestamp\": [\n        datetime.datetime(2021, 1, 1, 0, 1, 1),\n        datetime.datetime(2021, 1, 1, 0, 1, 59),\n        datetime.datetime(2021, 1, 1, 0, 2, 0),\n    ]\n})\n\n# Add 10 seconds to each timestamp and calculate duration between timestamps\ndf = daft.sql(\"\"\"\n    SELECT\n        timestamp,\n        timestamp + INTERVAL '10 seconds' as plus_10_seconds,\n    FROM df\n\"\"\")\n\ndf.show()\n\n```\n\n----------------------------------------\n\nTITLE: Filtering and Explaining Queries on Daft Dataframe Using Python\nDESCRIPTION: This code snippet demonstrates using relational dataframe operations with Daft such as filtering rows by a date condition on column 'L_SHIPDATE' and displaying the query plan with explain(). It imports datetime for date comparisons and uses an Iceberg dataframe as input.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/linkedin-03-05-2024.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\n\nice_df = daft.read_iceberg(iceberg_table)\nice_df = ice_df.where(ice_df[\"L_SHIPDATE\"] < datetime.date(1993, 1, 1))\nice_df.explain(True)\n```\n\n----------------------------------------\n\nTITLE: If Else Expression - Python\nDESCRIPTION: Selects values from either column 'A' or 'B' based on whether A > B, utilizing the `.if_else` method. Creates a Daft DataFrame with columns 'A' and 'B'. A new column 'A_if_bigger_else_B' is created, whose values are selected from column 'A' if the value in 'A' is greater than the corresponding value in 'B'; otherwise, the value from 'B' is selected.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.from_pydict({\"A\": [1, 2, 3], \"B\": [0, 2, 4]})\n\n# Pick values from column A if the value in column A is bigger\n# than the value in column B. Otherwise, pick values from column B.\ndf = df.with_column(\n    \"A_if_bigger_else_B\",\n    (df[\"A\"] > df[\"B\"]).if_else(df[\"A\"], df[\"B\"]),\n)\n\ndf.collect()\n```\n\n----------------------------------------\n\nTITLE: Reading DataFrame from SQL Database\nDESCRIPTION: This snippet demonstrates reading data from SQL databases using `daft.read_sql()`. It takes an SQL query and a database URI as input.  It also shows how to specify a partition column for parallel data reading from the database.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_54\n\nLANGUAGE: python\nCODE:\n```\nuri = \"postgresql://user:password@host:port/database\"\ndf = daft.read_sql(\"SELECT * FROM my_table\", uri)\n```\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.read_sql(\"SELECT * FROM my_table\", partition_col=\"date\", uri)\n```\n\n----------------------------------------\n\nTITLE: Inspecting Daft Query Plans in Python\nDESCRIPTION: This Python code snippet demonstrates reading a partitioned Parquet dataset from S3 into a Daft DataFrame using `daft.read_parquet`. It then filters the DataFrame using `.where()` and calls `.explain(show_all=True)` to display the detailed logical and physical query plans, illustrating Daft's lazy execution and optimization capabilities. Requires the `daft` library.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/terms.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndf2 = daft.read_parquet(\"s3://daft-public-data/tutorials/10-min/sample-data-dog-owners-partitioned.pq/**\")\ndf2.where(df[\"country\"] == \"Canada\").explain(show_all=True)\n```\n\n----------------------------------------\n\nTITLE: Classifying Images using UDF and PyTorch\nDESCRIPTION: Defines a UDF (`ClassifyImages`) to classify images using a pre-trained PyTorch model. The model is initialized within the UDF's `__init__` method and model weights are loaded from a URL. The `__call__` method performs inference on a batch of images and returns the predicted class labels.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/mnist.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n@udf(return_dtype=DataType.int64())\nclass ClassifyImages:\n    def __init__(self):\n        # Perform expensive initializations - create the model, download model weights and load up the model with weights\n        self.model = Net()\n        state_dict = torch.hub.load_state_dict_from_url(\n            \"https://github.com/Eventual-Inc/mnist-json/raw/master/mnist_cnn.pt\"\n        )\n        self.model.load_state_dict(state_dict)\n\n    def __call__(self, images_2d_col):\n        images_arr = np.array(images_2d_col.to_pylist())\n        normalized_image_2d = images_arr / 255\n        normalized_image_2d = normalized_image_2d[:, np.newaxis, :, :]\n        classifications = self.model(torch.from_numpy(normalized_image_2d).float())\n        return classifications.detach().numpy().argmax(axis=1)\n```\n\n----------------------------------------\n\nTITLE: Configuring S3 Access with Daft IOConfig (Python)\nDESCRIPTION: This snippet demonstrates how to import the Daft library and configure input/output settings, specifically setting up anonymous access for AWS S3 data sources using `daft.io.IOConfig` and `daft.io.S3Config`. This configuration is necessary to read data from S3 without providing explicit credentials.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/flyte/notebook.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport daft\n\nIO_CONFIG = daft.io.IOConfig(\n    s3=daft.io.S3Config(anonymous=True, region_name=\"us-west-2\")\n)  # Use anonymous-mode for accessing AWS S3\nPARQUET_PATH = \"s3://daft-public-data/tutorials/laion-parquet/train-00000-of-00001-6f24a7497df494ae.parquet\"\n```\n\n----------------------------------------\n\nTITLE: Filtering Data with Predicates\nDESCRIPTION: Shows how to filter DataFrame rows using comparison predicates with the where() method.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/10min.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndf.where(df[\"age\"] > 35).show()\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.where(df[\"country\"] == \"Canada\").show()\n```\n\n----------------------------------------\n\nTITLE: Extracting Temporal Components with Daft DataFrame in Python\nDESCRIPTION: This example shows how to extract components (year, month, day, hour, minute, second) from a timestamp column in Daft DataFrames using the `.dt` accessor methods. It constructs a DataFrame and adds new columns for each component. Requires `daft` and `datetime`. Input must be a timestamp column, and outputs are numeric columns for each extracted component. The operation is limited to available `.dt` expressions.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_46\n\nLANGUAGE: Python\nCODE:\n```\ndf = daft.from_pydict({\n    \"timestamp\": [\n        datetime.datetime(2021, 1, 1, 0, 1, 1),\n        datetime.datetime(2021, 1, 1, 0, 1, 59),\n        datetime.datetime(2021, 1, 1, 0, 2, 0),\n    ]\n})\n\n# Extract year, month, day, hour, minute, and second from the timestamp\ndf = df.with_columns({\n    \"year\": df[\"timestamp\"].dt.year(),\n    \"month\": df[\"timestamp\"].dt.month(),\n    \"day\": df[\"timestamp\"].dt.day(),\n    \"hour\": df[\"timestamp\"].dt.hour(),\n    \"minute\": df[\"timestamp\"].dt.minute(),\n    \"second\": df[\"timestamp\"].dt.second()\n})\n\ndf.show()\n\n```\n\n----------------------------------------\n\nTITLE: Writing a Daft DataFrame to a Delta Lake Table in Python\nDESCRIPTION: Shows how to write a Daft DataFrame to a Delta Lake table using the `write_deltalake` method. The example uses 'overwrite' mode to replace existing data. Daft supports multiple write modes such as append or overwrite for Delta Lake tables as per API documentation.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/delta_lake.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf.write_deltalake(\"tmp/daft-recordbatch\", mode=\"overwrite\")\n```\n\n----------------------------------------\n\nTITLE: Reading Hudi Table into Daft DataFrame - Python\nDESCRIPTION: This Python snippet demonstrates how to read an Apache Hudi table into a Daft DataFrame and apply a filter. It imports the `daft` library, uses `daft.read_hudi()` to read the table from a specified URI, and then filters the DataFrame using `df.where()`. Finally, it displays the resulting DataFrame with `df.show()`.  The `\"some-table-uri\"` is a placeholder and must be replaced with the actual URI of your Hudi table.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/hudi.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport daft\n\ndf = daft.read_hudi(\"some-table-uri\")\ndf = df.where(df[\"foo\"] > 5)\ndf.show()\n\n```\n\n----------------------------------------\n\nTITLE: Filtering Rows Using SQL Expression in DataFrame.where - Python\nDESCRIPTION: Illustrates filtering rows in a DataFrame using a SQL-style string expression inside the DataFrame.where method, which internally parses the string to a Daft Expression object. The method keeps rows satisfying the condition ('A < 2'). Requires the daft library and a DataFrame. Inputs are a DataFrame and a string condition; output is the filtered DataFrame displayed with .show(). Only rows meeting the condition are preserved.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql_overview.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ndf = daft.from_pydict({\"A\": [1, 2, 3], \"B\": [1, 2, 3]})\n\n# Daft automatically converts this string using `daft.sql_expr`\ndf = df.where(\"A < 2\")\n\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Select All Fields in Struct Column Using Wildcard - Daft DataFrame (Python)\nDESCRIPTION: This snippet shows how to select all fields within a struct-type column in a Daft DataFrame using a wildcard expression `col(\"struct_column\")[\"*\"]`. It initializes a DataFrame with a struct column \"A\" and then uses `select(col(\"A\")[\"*\"])` to extract all fields (\"B\" and \"C\") from the struct column as top-level columns in the resulting DataFrame. This requires importing `daft` and `col` from `daft`.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.from_pydict({\n    \"A\": [\n        {\"B\": 1, \"C\": 2},\n        {\"B\": 3, \"C\": 4}\n    ]\n})\ndf.select(col(\"A\")[\"*\"]).show()\n```\n\n----------------------------------------\n\nTITLE: Filtering DataFrame by Minimum Size (Python)\nDESCRIPTION: Applies another filter operation to the Daft DataFrame `df`, keeping only rows where the 'size' column is greater than 200,000. This adds a second filter step to the lazy query plan.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/image_querying/top_n_red_color.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf = df.where(df[\"size\"] > 200000)\n```\n\n----------------------------------------\n\nTITLE: Selecting All Columns and Rows from a Table Using SELECT in SQL\nDESCRIPTION: Shows how to select all columns and all rows from a specified table `T` using the wildcard `*`. This is a basic retrieval method without filters or conditions, returning the entire dataset from the table.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql/statements/select.md#_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM T;\n```\n\n----------------------------------------\n\nTITLE: Defining UDFs for S3 Presigned URLs and OpenAI LLM Calls in Python\nDESCRIPTION: This block defines two User Defined Functions (UDFs) using Daft to process columns: one generates presigned URLs for S3 objects (using boto3), and the other invokes the OpenAI GPT-4o API for image captioning or description, returning response JSONs. Dependencies: daft, boto3, requests, json, os. Inputs: S3 URLs and API keys. Outputs: Lists of presigned URLs and JSON responses from OpenAI. Limitations: Requires valid OpenAI API key and valid S3 connectivity. Prerequisite: Set OPENAI_API_KEY in the environment.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/data-ai-summit-2024.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport os\n\nimport boto3\nimport requests\n\nDEFAULT_PROMPT = \"What’s in this image?\"\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif api_key is None:\n    raise RuntimeError(\"Please specify your OpenAI API key as the environment variable `OPENAI_API_KEY`.\")\n\nheaders = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {api_key}\"}\n\n\n@daft.udf(return_dtype=daft.DataType.string())\ndef generate_presigned_url(s3_urls, expires_in=3600):\n    \"\"\"Generate a presigned Amazon S3 URLs.\"\"\"\n    s3_client = boto3.client(\"s3\")\n    presigned_urls = []\n    for s3_url in s3_urls.to_pylist():\n        bucket, key = s3_url.strip(\"s3://\").split(\"/\", 1)\n        url = s3_client.generate_presigned_url(\n            ClientMethod=\"get_object\", Params={\"Bucket\": bucket, \"Key\": key}, ExpiresIn=expires_in\n        )\n        presigned_urls.append(url)\n    return presigned_urls\n\n\n@daft.udf(return_dtype=daft.DataType.string())\ndef run_gpt4o_on_urls(images_urls, prompt=DEFAULT_PROMPT):\n    \"\"\"Run the gpt-4o LLM by making an API call to OpenAI.\"\"\"\n    results = []\n    for url in images_urls.to_pylist():\n        payload = {\n            \"model\": \"gpt-4o\",\n            \"messages\": [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n                        {\"type\": \"image_url\", \"image_url\": {\"url\": url}},\n                    ],\n                }\n            ],\n            \"max_tokens\": 300,\n        }\n\n        response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n        results.append(json.dumps(response.json()))\n\n    return results\n```\n\n----------------------------------------\n\nTITLE: Cropping images using a UDF in Daft\nDESCRIPTION: This code defines a UDF called `crop_images` that crops images based on corresponding crop values. It uses the `@daft.udf` decorator to allow Daft to pass column data into the function. The function takes lists of images and crop boxes as input, applies a cropping operation, and returns a list of cropped images. The UDF is then applied to a Daft DataFrame to create a new 'cropped' column.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_64\n\nLANGUAGE: Python\nCODE:\n```\n@daft.udf(return_dtype=daft.DataType.python())\ndef crop_images(images, crops, padding=0):\n    cropped = []\n    for img, crop in zip(images, crops):\n        x1, x2, y1, y2 = crop\n        cropped_img = img[x1:x2 + padding, y1:y2 + padding]\n        cropped.append(cropped_img)\n    return cropped\n\ndf = df.with_column(\n    \"cropped\",\n    crop_images(df[\"image\"], df[\"crop\"], padding=1),\n)\ndf.show(2)\n```\n\n----------------------------------------\n\nTITLE: If Else Expression - SQL\nDESCRIPTION: Chooses values from 'A' or 'B' depending on the result of A > B using SQL's CASE WHEN. The Daft DataFrame is initialized, and the SQL query uses a CASE WHEN statement, selecting the value of A when A > B, and the value of B otherwise. The result is a new column named 'A_if_bigger_else_B'.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_43\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.from_pydict({\"A\": [1, 2, 3], \"B\": [0, 2, 4]})\n\ndf = daft.sql(\"\"\"\n    SELECT\n        A,\n        B,\n        CASE\n            WHEN A > B THEN A\n            ELSE B\n        END AS A_if_bigger_else_B\n    FROM df\n\"\"\")\n\ndf.collect()\n```\n\n----------------------------------------\n\nTITLE: Join Two DataFrames on Common Key - Daft DataFrame (Python)\nDESCRIPTION: This snippet demonstrates joining two Daft DataFrames, `df1` and `df2`, based on a common column \"A\". It initializes both DataFrames and then performs an inner join using the `join()` method, specifying the join key with `on=\"A\"`. The resulting joined DataFrame, combining columns from both originals where the \"A\" values match, is then displayed. This requires importing `daft`.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndf1 = daft.from_pydict({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\ndf2 = daft.from_pydict({\"A\": [1, 2, 3], \"C\": [7, 8, 9]})\n\ndf1.join(df2, on=\"A\").show()\n```\n\n----------------------------------------\n\nTITLE: Downloading Image Bytes from URLs with Daft DataFrame - Python\nDESCRIPTION: This snippet demonstrates how to add a new column containing image byte data to an existing Daft DataFrame by downloading images from a list of URLs. It leverages the url.download() expression of Daft, with the on_error parameter set to \"null\" to gracefully handle download failures. Dependencies include the Daft Python library, an initialized DataFrame with a 'urls' column, and network connectivity. Input columns: 'urls' (Utf8/String of image URLs). Output: new column 'image_bytes' (binary image data). Limitations: download errors result in null entries unless otherwise handled.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_71\n\nLANGUAGE: Python\nCODE:\n```\ndf_family = df_family.with_column(\"image_bytes\", df_dogs[\"urls\"].url.download(on_error=\"null\"))\ndf_family.show()\n```\n\n----------------------------------------\n\nTITLE: Example Output of Daft DataFrame explain()\nDESCRIPTION: This text shows the sample output generated by the `daft.DataFrame.explain(show_all=True)` method after filtering a DataFrame. It includes the Unoptimized Logical Plan, the Optimized Logical Plan (demonstrating filter pushdown), and the Physical Plan, providing insight into Daft's query optimization and execution strategy.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/terms.md#_snippet_1\n\nLANGUAGE: Text\nCODE:\n```\n== Unoptimized Logical Plan ==\n\n* Filter: col(country) == lit(\"Canada\")\n|\n* GlobScanOperator\n|   Glob paths = [s3://daft-public-data/tutorials/10-min/sample-data-dog-owners-\n|     partitioned.pq/**]\n|   Coerce int96 timestamp unit = Nanoseconds\n|   IO config = S3 config = { Max connections = 8, Retry initial backoff ms = 1000,\n|     Connect timeout ms = 30000, Read timeout ms = 30000, Max retries = 25, Retry\n|     mode = adaptive, Anonymous = false, Use SSL = true, Verify SSL = true, Check\n|     hostname SSL = true, Requester pays = false, Force Virtual Addressing = false },\n|     Azure config = { Anonymous = false, Use SSL = true }, GCS config = { Anonymous =\n|     false }, HTTP config = { user_agent = daft/0.0.1 }\n|   Use multithreading = true\n|   File schema = first_name#Utf8, last_name#Utf8, age#Int64, DoB#Date,\n|     country#Utf8, has_dog#Boolean\n|   Partitioning keys = []\n|   Output schema = first_name#Utf8, last_name#Utf8, age#Int64, DoB#Date,\n|     country#Utf8, has_dog#Boolean\n\n\n== Optimized Logical Plan ==\n\n* GlobScanOperator\n|   Glob paths = [s3://daft-public-data/tutorials/10-min/sample-data-dog-owners-\n|     partitioned.pq/**]\n|   Coerce int96 timestamp unit = Nanoseconds\n|   IO config = S3 config = { Max connections = 8, Retry initial backoff ms = 1000,\n|     Connect timeout ms = 30000, Read timeout ms = 30000, Max retries = 25, Retry\n|     mode = adaptive, Anonymous = false, Use SSL = true, Verify SSL = true, Check\n|     hostname SSL = true, Requester pays = false, Force Virtual Addressing = false },\n|     Azure config = { Anonymous = false, Use SSL = true }, GCS config = { Anonymous =\n|     false }, HTTP config = { user_agent = daft/0.0.1 }\n|   Use multithreading = true\n|   File schema = first_name#Utf8, last_name#Utf8, age#Int64, DoB#Date,\n|     country#Utf8, has_dog#Boolean\n|   Partitioning keys = []\n|   Filter pushdown = col(country) == lit(\"Canada\")\n|   Output schema = first_name#Utf8, last_name#Utf8, age#Int64, DoB#Date,\n|     country#Utf8, has_dog#Boolean\n\n\n== Physical Plan ==\n\n* TabularScan:\n|   Num Scan Tasks = 1\n|   Estimated Scan Bytes = 6336\n|   Clustering spec = { Num partitions = 1 }\n```\n\n----------------------------------------\n\nTITLE: Reading External Sources Directly with SQL Table Functions - Python\nDESCRIPTION: Illustrates using SQL table functions in daft.sql to directly load data from external sources such as Parquet and Iceberg files. This syntax allows one-liner integration of remote datasets without prior DataFrame definition. The only requirement is the daft library and compatible file paths (e.g., S3 URIs). The output is a Daft DataFrame that can be further manipulated. Inputs are file URIs; outputs are DataFrames corresponding to the loaded data.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql_overview.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndaft.sql(\"SELECT * FROM read_parquet('s3://...')\")\ndaft.sql(\"SELECT * FROM read_iceberg('s3://.../metadata.json')\")\n```\n\n----------------------------------------\n\nTITLE: Writing DataFrame to Parquet Files\nDESCRIPTION: Executes the DataFrame to write its data to a specified Parquet file, returning a new DataFrame with file paths as output. This operation is blocking and finalizes data export.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/10min.ipynb#_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\nwritten_df = df.write_parquet(\"my-dataframe.parquet\")\n\nwritten_df\n```\n\n----------------------------------------\n\nTITLE: Executing SQL Queries Against Daft Sessions in Python\nDESCRIPTION: Demonstrates executing SQL queries within a Daft Session using `sess.sql()`. It shows how to set the current catalog and namespace using a `USE` statement, query tables using both unqualified (leveraging session state) and qualified names, and perform joins between persistent and temporary tables. Requires `daft` and an active session `sess` with configured catalogs/tables (e.g., `default.example.tbl` and `temp`).\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sessions.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# use the given catalog and namespace (like the use/set_ methods)\nsess.sql(\"USE default.example\")\n\n# we support both qualified and unqualified names by leveraging the session state\nsess.sql(\"SELECT * FROM tbl LIMIT 1\").show()\n╭─────────┬───────┬──────╮\n│ x       ┆ y     ┆ z    │\n│ ---     ┆ ---   ┆ ---  │\n│ Boolean ┆ Int64 ┆ Utf8 │\n╞═════════╪═══════╪══════╡\n│ true    ┆ 4     ┆ jkl  │\n╰─────────┴───────┴──────╯\n\n# we can even combine our queries with the temp table from earlier\nsess.sql(\"SELECT * FROM example.tbl, temp LIMIT 1\").show()\n╭─────────┬───────┬──────┬────────────┬────────┬────────╮\n│ x       ┆ y     ┆ z    ┆      …     ┆ temp.y ┆ temp.z │\n│ ---     ┆ ---   ┆ ---  ┆            ┆ ---    ┆ ---    │\n│ Boolean ┆ Int64 ┆ Utf8 ┆ (1 hidden) ┆ Int64  ┆ Utf8   │\n╞═════════╪═══════╪══════╪════════════╪════════╪════════╡\n│ true    ┆ 4     ┆ jkl  ┆ …          ┆ 4      ┆ jkl    │\n╰─────────┴───────┴──────┴────────────┴────────┴────────╯\n```\n\nLANGUAGE: sql\nCODE:\n```\nUSE default.example\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM tbl LIMIT 1\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM example.tbl, temp LIMIT 1\n```\n\n----------------------------------------\n\nTITLE: Installing Daft Python Package using pip in Bash\nDESCRIPTION: Installs the base Daft package available on PyPI using pip. This snippet is the fundamental installation command to get Daft up and running on a system with Python and pip installed. No additional dependencies are required for this base installation.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/install.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U daft\n```\n\n----------------------------------------\n\nTITLE: Installing Nightly Builds of Daft using pip in Bash\nDESCRIPTION: Installs the nightly development build of Daft directly from a custom nightly package index URL. This provides the most recent changes from the main development branch but may be less stable. It requires pip with --pre flag for pre-release packages and network access to the custom index.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/install.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install -U daft --pre --extra-index-url https://d1p3klp2t5517h.cloudfront.net/builds/nightly\n```\n\n----------------------------------------\n\nTITLE: Displaying the First N Rows of a Daft DataFrame using show()\nDESCRIPTION: This code uses the `show(n)` method on a Daft DataFrame (`df`) to compute and display the first `n` rows (3 in this case). This triggers partial execution of the computation plan, materializing only the required rows, making it useful for inspecting data without loading the entire DataFrame into memory.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/quickstart.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf.show(3)\n```\n\n----------------------------------------\n\nTITLE: Downloading and Decoding Images in Daft (Python)\nDESCRIPTION: This code adds a new column named \"image\" to the DataFrame by processing the URLs in the \"URL\" column. It uses `url.download()` to fetch the content from each URL, specifies `on_error=\"null\"` to handle download failures gracefully, and then uses `image.decode()` to interpret the downloaded bytes as image data.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/flyte/notebook.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfiltered_df = filtered_df.with_column(\n    \"image\",\n    filtered_df[\"URL\"].url.download(on_error=\"null\").image.decode(),\n)\n```\n\n----------------------------------------\n\nTITLE: Filtering Daft DataFrame by Text Content (Python)\nDESCRIPTION: This snippet demonstrates how to filter the Daft DataFrame based on a condition applied to the 'TEXT' column. It uses the `.str.contains()` method to keep only the rows where the value in the 'TEXT' column contains the substring \"darkness\", creating a new filtered DataFrame.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/flyte/notebook.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfiltered_df = parquet_df.where(parquet_df[\"TEXT\"].str.contains(\"darkness\"))\n```\n\n----------------------------------------\n\nTITLE: Installing Daft from Source using pip in Bash\nDESCRIPTION: Installs Daft by downloading and building it from the latest source code on the main GitHub branch. This method requires the Rust toolchain installed on the system since Daft includes Rust components that must be compiled. This approach is suitable for contributors or users requiring the absolute latest source code.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/install.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install -U https://github.com/Eventual-Inc/Daft/archive/refs/heads/main.zip\n```\n\n----------------------------------------\n\nTITLE: Installing Daft Python Library Using pip\nDESCRIPTION: This snippet installs the Daft Python library with optional dependencies for Iceberg, Hudi, and DeltaLake support using pip. It also installs the ipywidgets package to enable interactive widgets in Jupyter environments. These commands require pip and a suitable Python environment for Daft installation.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/linkedin-03-05-2024.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install -U 'daft[iceberg,hudi,deltalake]'\n!pip install -U ipywidgets\n```\n\n----------------------------------------\n\nTITLE: Defining UDF for Red Pixel Detection (Python)\nDESCRIPTION: Defines a Python function `magic_red_detector` that takes a NumPy array representing an image, converts it to HSV color space, identifies pixels within a specific red hue range, creates a binary mask, applies a mode filter, and returns the resulting mask as a PIL Image object. This function is intended to be used as a User-Defined Function (UDF) with Daft.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/image_querying/top_n_red_color.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport PIL.Image\nfrom PIL import ImageFilter\n\n\ndef magic_red_detector(img: np.ndarray) -> PIL.Image.Image:\n    \"\"\"Gets a new image which is a mask covering all 'red' areas in the image.\"\"\"\n    img = PIL.Image.fromarray(img)\n    lower = np.array([245, 100, 100])\n    upper = np.array([10, 255, 255])\n    lower_hue, upper_hue = lower[0, np.newaxis, np.newaxis], upper[0, np.newaxis, np.newaxis]\n    lower_saturation_intensity, upper_saturation_intensity = (\n        lower[1:, np.newaxis, np.newaxis],\n        upper[1:, np.newaxis, np.newaxis],\n    )\n    hsv = img.convert(\"HSV\")\n    hsv = np.asarray(hsv).T\n    mask = np.all(\n        (hsv[1:, ...] >= lower_saturation_intensity) & (hsv[1:, ...] <= upper_saturation_intensity), axis=0\n    ) & ((hsv[0, ...] >= lower_hue) | (hsv[0, ...] <= upper_hue))\n    img = PIL.Image.fromarray(mask.T)\n    img = img.filter(ImageFilter.ModeFilter(size=5))\n    return img\n```\n\n----------------------------------------\n\nTITLE: Displaying Daft DataFrame Contents in Python\nDESCRIPTION: Displays the first 5 rows of the `images_df` Daft DataFrame using the `.show()` method. This is typically used in interactive environments like Jupyter notebooks to inspect the DataFrame's contents, including the newly added 'image' column.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/text_to_image/text_to_image_generation.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimages_df.show(5)\n```\n\n----------------------------------------\n\nTITLE: Creating Expressions: Wildcard (Python)\nDESCRIPTION: This snippet demonstrates how to create an expression that selects all columns in a DataFrame using a wildcard (`col(\"*\")`). It then applies an operation (multiplication by 3) to all selected columns. Requires a Daft DataFrame as input; outputs a modified DataFrame.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nimport daft\nfrom daft import col\n\ndf = daft.from_pydict({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\ndf.select(col(\"*\") * 3).show()\n```\n\n----------------------------------------\n\nTITLE: Selecting Specific Columns from a Daft DataFrame using select()\nDESCRIPTION: This snippet demonstrates column selection using the `select()` method on a Daft DataFrame (`df`). It takes the names of the desired columns ('first_name', 'has_dog') as arguments and returns a new, lazy DataFrame containing only these columns. The `.show()` method is chained to trigger execution and display the resulting selected data.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/quickstart.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf.select(\"first_name\", \"has_dog\").show()\n```\n\n----------------------------------------\n\nTITLE: Resizing Images - Daft - Python\nDESCRIPTION: Adds two new columns to the DataFrame: \"image_resized_small\" and \"image_resized_large\". It uses Daft's image namespace to resize the image data in the \"image\" column to 32x32 and 256x256 pixels, respectively. Requires a Daft DataFrame with an \"image\" column containing decoded images.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/1-local-image-batch-inference.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ndf = df.with_column(\"image_resized_small\", df[\"image\"].image.resize(32, 32))\ndf = df.with_column(\"image_resized_large\", df[\"image\"].image.resize(256, 256))\n```\n\n----------------------------------------\n\nTITLE: Fixing Null Values with Conditional Logic\nDESCRIPTION: Demonstrates how to replace null values in a column using conditional logic with is_null() and if_else() methods.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/10min.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndf = df.with_column(\"has_dog\", df[\"has_dog\"].is_null().if_else(True, df[\"has_dog\"]))\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Creating Expressions: Literal Value (SQL)\nDESCRIPTION: This code shows how to create an expression representing a literal value using `daft.sql_expr()`.  This is useful for hardcoding single values as expressions. The input is a value (e.g., an integer), and the output is a Daft Expression object representing that literal.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n# Refers to an expression which always evaluates to 42\ndaft.sql_expr(\"42\")\n```\n\n----------------------------------------\n\nTITLE: Select Multiple Columns by Indexing - Daft DataFrame (Python)\nDESCRIPTION: This snippet demonstrates an alternative syntax for selecting multiple columns in a Daft DataFrame using Python list indexing on the DataFrame object itself. It selects columns \"A\" and \"B\" by passing a list of column names `[\"A\", \"B\"]` and then displays the resulting DataFrame containing only these columns. It requires a DataFrame `df` to be previously defined with columns 'A' and 'B'.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndf[[\"A\", \"B\"]].show()\n```\n\n----------------------------------------\n\nTITLE: Reading Image Paths from S3 Using Daft in Python\nDESCRIPTION: This snippet reads image files from a specified S3 path matching a glob pattern into a Daft DataFrame and prints a preview. The function fetches 'validation-images' from the 'open-images' public S3 bucket. Input: S3 bucket path. Output: Daft DataFrame representation of file paths.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/data-ai-summit-2024.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.from_glob_path(\n    \"s3://daft-public-data/open-images/validation-images/*\",\n)\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Reading a JSON File into a DataFrame with Daft - Python\nDESCRIPTION: Shows how to read JSON-formatted files into Daft DataFrames. Requires the daft package; path should reference a valid JSON file. Input options may include file paths and format arguments; the output is a DataFrame. Complex nested structures may require pre-processing.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/api/dataframe_creation.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nimport daft\n\ndf = daft.read_json(\"/path/to/file.json\")\n```\n\n----------------------------------------\n\nTITLE: Add New Column from Expression of Existing Columns - Daft DataFrame (Python)\nDESCRIPTION: This snippet adds a new column named \"C\" to the DataFrame `df` using the `with_column()` method. The values in the new column are computed by adding the values from column \"A\" and column \"B\" using an expression `df[\"A\"] + df[\"B\"]`. The modified DataFrame, now including column \"C\", is then displayed. It requires a DataFrame `df` with columns \"A\" and \"B\" to be previously defined.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndf.with_column(\"C\", df[\"A\"] + df[\"B\"]).show()\n```\n\n----------------------------------------\n\nTITLE: Filtering and Aggregating with Ray\nDESCRIPTION: Loads an Iceberg table from Glue catalog, filters and aggregates its data, running the operations on a Ray cluster. The code loads the table, filters the data based on the `L_SHIPDATE` and groups it by `L_SHIPDATE` and computes the sum of `L_EXTENDEDPRICE`. This demonstrates how Daft can distribute iceberg queries.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/iceberg_summit_2024.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\n\nfrom pyiceberg.catalog.glue import GlueCatalog\n\ncatalog = GlueCatalog(\"my_glue_catalog\")\ntable = catalog.load_table(\"tpch_iceberg_sf1000.lineitem\")\n\ndf = daft.read_iceberg(table)\ndf = df.where(df[\"L_SHIPDATE\"] < datetime.date(1996, 1, 1))\naggregated_df = df.groupby(\"L_SHIPDATE\").agg([daft.col(\"L_EXTENDEDPRICE\").sum()])\n```\n\n----------------------------------------\n\nTITLE: Retrieving and decoding images from S3 URLs\nDESCRIPTION: Adds new columns to the DataFrame: 'image_url' pointing to the image location in S3, and 'image' which downloads and decodes the image data. Also resizes images to small and large resolutions for subsequent inference.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/2-distributed-batch-inference.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n# Retrieve images and run preprocessing\ndf = df.with_column(\n    \"image_url\", \"s3://daft-public-datasets/imagenet/val-10k-sample-deltalake/images/\" + df[\"filename\"] + \".jpeg\"\n)\ndf = df.with_column(\"image\", df[\"image_url\"].url.download().image.decode())\ndf = df.with_column(\"image_resized_small\", df[\"image\"].image.resize(32, 32))\ndf = df.with_column(\"image_resized_large\", df[\"image\"].image.resize(256, 256))\n```\n\n----------------------------------------\n\nTITLE: Configuring VSCode Debugging for Python and Rust (launch.json) - JSON\nDESCRIPTION: This JSON configuration enables debugging for both Rust and Python code in VSCode using debugpy and LLDB. It defines two launch configurations: one to start Python debugging via attach_debugger.py, automatically attaching Rust LLDB based on the process ID detected in output, and one to directly attach LLDB to Rust code. Dependencies include the VSCode CodeLLDB extension and a properly set Python interpreter. Inputs are typically script file names, and output is an integrated debugging experience where breakpoints in both .py and .rs files are recognized. Ensure that .vscode/launch.json contains this configuration and that the proper extensions are installed. There may be system-level constraints related to ptrace permissions for LLDB to work correctly.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/CONTRIBUTING.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"configurations\": [\n        {\n            \"name\": \"Debug Rust/Python\",\n            \"type\": \"debugpy\",\n            \"request\": \"launch\",\n            \"program\": \"${workspaceFolder}/tools/attach_debugger.py\",\n            \"args\": [\n                \"${file}\"\n            ],\n            \"console\": \"internalConsole\",\n            \"serverReadyAction\": {\n                \"pattern\": \"pID = ([0-9]+)\",\n                \"action\": \"startDebugging\",\n                \"name\": \"Rust LLDB\"\n            }\n        },\n        {\n            \"name\": \"Rust LLDB\",\n            \"pid\": \"0\",\n            \"type\": \"lldb\",\n            \"request\": \"attach\",\n            \"program\": \"${command:python.interpreterPath}\",\n            \"stopOnEntry\": false,\n            \"sourceLanguages\": [\n                \"rust\"\n            ],\n            \"presentation\": {\n                \"hidden\": true\n            }\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Sorting by Red Pixel Count and Collecting Results (Python)\nDESCRIPTION: Sorts the DataFrame `df` in descending order based on the 'num_pixels_red' column. It then calls `.collect()` to execute the entire query plan (including UDFs, sorting) and materializes the final, sorted result in memory, displaying the top rows with the most red pixels.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/image_querying/top_n_red_color.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndf.sort(\"num_pixels_red\", desc=True).collect()\n```\n\n----------------------------------------\n\nTITLE: Using Daft Sessions Basic Operations in Python\nDESCRIPTION: Demonstrates fundamental Daft Session operations: creating a session (explicitly and implicitly via `import daft`), creating a temporary table from a Python dictionary using `daft.from_pydict`, reading the table back into a DataFrame, retrieving the table object, reading from the table object, and executing a SQL query against the session.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sessions.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport daft\n\n# `import daft` defines an implicit session `daft.current_session()`\n\nfrom daft import Session\n\n# create a new session\nsess = Session()\n\n# create a temp table from a DataFrame\nsess.create_temp_table(\"T\", daft.from_pydict({ \"x\": [1,2,3] }))\n\n# read table as dataframe from the session\n_ = sess.read_table(\"T\")\n\n# get the table instance\nt = sess.get_table(\"T\")\n\n# read table instance as a datadrame\n_ = t.read()\n\n# execute sql against the session\nsess.sql(\"SELECT * FROM T\").show()\n╭───────╮\n│ x     │\n│ ---   │\n│ Int64 │\n╞═══════╡\n│ 1     │\n├╌╌╌╌╌╌╌┤\n│ 2     │\n├╌╌╌╌╌╌╌┤\n│ 3     │\n╰───────╯\n```\n\n----------------------------------------\n\nTITLE: Hiding Quickstart Headers in MkDocs Navigation using CSS\nDESCRIPTION: This CSS snippet targets specific MkDocs Material theme elements (`.md-nav--primary`, `.md-nav__link[for=__toc]`) to hide the table of contents (Quickstart headers) from the primary navigation sidebar. This is typically used for styling documentation generated with MkDocs.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/quickstart.md#_snippet_0\n\nLANGUAGE: css\nCODE:\n```\n<style>\n   .md-nav--primary .md-nav__link[for=__toc] > .md-nav__icon,\n   .md-nav--primary .md-nav__link[for=__toc] ~ .md-nav {\n     display: none;\n   }\n</style>\n```\n\n----------------------------------------\n\nTITLE: Loading Table from Glue Catalog\nDESCRIPTION: Loads an Iceberg table from an AWS Glue catalog. It utilizes `GlueCatalog` from pyiceberg and loads the table by its name.  This allows working with larger tables in the cloud, instead of local sqlite database.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/iceberg_summit_2024.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom pyiceberg.catalog.glue import GlueCatalog\n\ncatalog = GlueCatalog(\"my_glue_catalog\")\ntable = catalog.load_table(\"tpch_iceberg_sf1000.lineitem\")\n```\n\n----------------------------------------\n\nTITLE: Installing Daft with Delta Lake Support - Bash\nDESCRIPTION: Installs the Daft Python package with Delta Lake support, which includes the 'deltalake' package as a dependency. This package fetches metadata about Delta Lake tables required for Daft to read the tables. It is necessary to run this command before using Daft to interact with Delta Lake tables.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/delta_lake.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U \"daft[deltalake]\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Ray Job with Daft in Python\nDESCRIPTION: Sample script showing how to set up a Daft job to be executed on Ray. This approach runs the entire code on Ray, providing more control and observability than the Ray client method.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/ray.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# wd/job.py\n\nimport daft\n\ndef main():\n    # call without any arguments to connect to Ray from the head node\n    daft.context.set_runner_ray()\n\n    # ... Run Daft commands here ...\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Apply Expression and Show Result - Daft DataFrame (Python/SQL)\nDESCRIPTION: This snippet demonstrates applying a simple arithmetic expression (incrementing column 'A' by 1) to a Daft DataFrame. It shows how to perform this computation using both the Daft Python API's `select` method with column expressions and the `daft.sql` function for SQL-based operations, followed by displaying the resulting DataFrame using `show()`. Assumes a DataFrame `df` is already defined with a column 'A'.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf.select(df[\"A\"] + 1).show()\n```\n\nLANGUAGE: python\nCODE:\n```\ndaft.sql(\"SELECT A + 1 FROM df\").show()\n```\n\n----------------------------------------\n\nTITLE: Reading a CSV File into a DataFrame with Daft - Python\nDESCRIPTION: Provides an example of loading CSV files as DataFrames using Daft. Depends on the daft library; paths must point to readable CSV files. Accepts file path(s) and various parsing parameters like delimiters; returns a parsed DataFrame. Malformed or inconsistent CSVs may raise exceptions.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/api/dataframe_creation.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nimport daft\n\ndf = daft.read_csv(\"/path/to/file.csv\")\n```\n\n----------------------------------------\n\nTITLE: Installing Daft with All Extra Dependencies using pip in Bash\nDESCRIPTION: Installs Daft with all optional dependencies enabled, including AWS and Ray support. This command ensures that all extra features of Daft which rely on external packages are installed simultaneously. Suitable for users who want the fullest feature set.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/install.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install -U daft[all]\n```\n\n----------------------------------------\n\nTITLE: Show tables in catalog matching pattern\nDESCRIPTION: This SQL snippet demonstrates how to list tables in a specific catalog that match a specified pattern, combining the IN and LIKE clauses with the SHOW TABLES statement.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql/statements/show.md#_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nSHOW TABLES IN my_catalog LIKE 'foo';\n```\n\n----------------------------------------\n\nTITLE: Limiting Rows in a Daft DataFrame using Python\nDESCRIPTION: This snippet demonstrates how to limit the number of rows returned from a Daft DataFrame using the `limit()` method. It takes an integer argument specifying the maximum number of rows (2 in this case) and then displays the result using `show()`. Depends on an existing Daft DataFrame instance named `df`.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/quickstart.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf.limit(2).show()\n```\n\n----------------------------------------\n\nTITLE: JSON Query - SQL\nDESCRIPTION: Extracts a value from JSON strings using JQ-style filters and SQL. It initializes a Daft DataFrame with a 'json' column and then uses `json_query` function within a SQL query to extract the value associated with the key 'a', storing it into a new column 'a'.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.from_pydict({\n    \"json\": [\n        '{\"a\": 1, \"b\": 2}',\n        '{\"a\": 3, \"b\": 4}',\n    ],\n})\ndf = daft.sql(\"\"\"\n    SELECT\n        json,\n        json_query(json, '.a') AS a\n    FROM df\n\"\"\")\ndf.collect()\n```\n\n----------------------------------------\n\nTITLE: Decoding Images with Daft in Python\nDESCRIPTION: This snippet decodes raw image bytes in the 'image_bytes' column into image objects using Daft's decoding operations. The resulting 'image' column supports further image operations. Input: Raw bytes. Output: Structured image representation, compatible with Daft's image processing expressions.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/data-ai-summit-2024.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf = df.with_column(\"image\", df[\"image_bytes\"].image.decode())\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Materializing DataFrame\nDESCRIPTION: This snippet materializes the DataFrame `df`, which means it triggers the execution of the deferred operations and loads the data into memory.  The `collect()` method forces the DataFrame to compute the results and store them. Subsequently, displaying `df` after `collect()` shows the rows of the data.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf.collect()\ndf\n```\n\n----------------------------------------\n\nTITLE: Creating a Local Delta Lake Table Using Python and deltalake\nDESCRIPTION: Demonstrates creating a local Delta Lake table by writing a pandas DataFrame partitioned by the 'group' column using the 'write_deltalake' function from the 'deltalake' Python package. The data consists of multiple columns, including integers and strings, and is partitioned resulting in separate files per group partition. This prepares a Delta Lake table to be read by other tools like Daft.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/delta_lake.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Create a local Delta Lake table.\nfrom deltalake import write_deltalake\nimport pandas as pd\n\ndf = pd.DataFrame({\n    \"group\": [1, 1, 2, 2, 3, 3, 4, 4],\n    \"num\": list(range(8)),\n    \"letter\": [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"],\n})\n\n# This will write out separate partitions for group=1, group=2, group=3, group=4.\nwrite_deltalake(\"some-table\", df, partition_by=\"group\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Daft with Ray Client in Python\nDESCRIPTION: Example showing how to initialize and use the Ray client with Daft. It demonstrates connecting to a Ray server, setting up the Daft context to use Ray as a runner, and executing basic DataFrame operations remotely.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/ray.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport daft\nimport ray\n\n# Refer to the note under \"Ray Job\" for details on \"runtime_env\"\nray.init(\"ray://<head_node_host>:10001\", runtime_env={\"pip\": [\"daft\"]})\n\n# Starts the Ray client and tells Daft to use Ray to execute queries\n# If ray.init() has already been called, it uses the existing client\ndaft.context.set_runner_ray(\"ray://<head_node_host>:10001\")\n\ndf = daft.from_pydict({\n    \"a\": [3, 2, 5, 6, 1, 4],\n    \"b\": [True, False, False, True, True, False]\n})\ndf = df.where(df[\"b\"]).sort(df[\"a\"])\n\n# Daft executes the query remotely and returns a preview to the client\ndf.collect()\n```\n\n----------------------------------------\n\nTITLE: Parsing and Converting Time Zones in Daft DataFrame with Python\nDESCRIPTION: This snippet illustrates parsing string timestamps with time zone information and converting them to a different time zone using Daft’s string-to-datetime conversion. The code reads string values, parses them using a specified format and time zone, and adds a column with converted timestamps. Requires `daft` and awareness of time zone naming. The format string and target time zone are parameters. Input strings must match the specified datetime pattern.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_48\n\nLANGUAGE: Python\nCODE:\n```\ndf = daft.from_pydict({\n    \"timestamp_str\": [\n        \"2021-01-01 00:00:00.123 +0800\",\n        \"2021-01-02 12:30:00.456 +0800\"\n    ]\n})\n\n# Parse the timestamp string with time zone and convert to New York time\ndf = df.with_column(\n    \"ny_time\",\n    df[\"timestamp_str\"].str.to_datetime(\n        \"%Y-%m-%d %H:%M:%S%.3f %z\",\n        timezone=\"America/New_York\"\n    )\n)\n\ndf.show()\n\n```\n\n----------------------------------------\n\nTITLE: Creating Daft DataFrame from S3 (Python)\nDESCRIPTION: Imports the Daft library and constructs a Daft DataFrame by listing files matching a glob pattern in an S3 bucket. It configures anonymous S3 access using `IOConfig`. Based on the `USE_RAY` flag, it limits the DataFrame size and repartitions it for distributed processing (if Ray is used) or limits it to a smaller size for local execution.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/image_querying/top_n_red_color.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport daft\n\nIO_CONFIG = daft.io.IOConfig(s3=daft.io.S3Config(anonymous=True))  # Use anonymous S3 access\n\ndf = daft.from_glob_path(\n    \"s3://daft-public-data/open-images/validation-images/*\",\n    io_config=IO_CONFIG,\n)\n\nif USE_RAY:\n    df = df.limit(10000)\n    df = df.repartition(64)\nelse:\n    df = df.limit(100)\n```\n\n----------------------------------------\n\nTITLE: Showing First N Rows (Python)\nDESCRIPTION: Uses the `.show(5)` method to execute the current query plan just enough to display the first 5 rows of the result. Unlike `.collect()`, this does not materialize the entire DataFrame in memory.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/image_querying/top_n_red_color.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Show doesn't materialize the data, but lets us peek at the first N rows\n# produced by the current query plan\n\ndf.show(5)\n```\n\n----------------------------------------\n\nTITLE: Submitting a Ray Job with Daft Dependencies via Bash\nDESCRIPTION: Command-line example showing how to submit a Daft job to Ray using the Ray CLI. This includes specifying the working directory, Ray address, and runtime environment with Daft dependencies.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/ray.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nray job submit \\\n    --working-dir wd \\\n    --address \"http://<head_node_host>:8265\" \\\n    --runtime-env-json '{\"pip\": [\"daft\"]}' \\\n    -- python job.py\n```\n\n----------------------------------------\n\nTITLE: Converting to Ray Dataset - Python\nDESCRIPTION: This code converts a Daft DataFrame to a Ray Dataset for distributed processing. It filters the dataset to keep only images with a channel dimension of 3 and selects the 'arr' and 'class_id' columns, which will then be used in the model training.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/3-pytorch-ray-single-node-training.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nray_dataset = (\n    df\n    # Filter out images with channel dimensions != 3\n    .where(df[\"image\"].apply(lambda arr: arr.shape[2] == 3, return_dtype=daft.DataType.bool()))\n    # Select only two columns, \"arr\" and \"class_id\"\n    .select(\"arr\", \"class_id\")\n    .to_ray_dataset()\n)\n```\n\n----------------------------------------\n\nTITLE: Activating RayRunner for Daft Using Python\nDESCRIPTION: Demonstrates how to enable Daft's RayRunner to execute dataframe operations on a Ray cluster instead of the default Python Runner. Optionally accepts an address to connect to an existing external Ray cluster; otherwise spins up a local single-node Ray cluster. This method improves computation speed by enabling distributed task execution based on the number of CPU cores or cluster resources.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/text_to_image/using_cloud_with_ray.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport daft\n\nif USE_RAY:\n    RAY_ADDRESS = None\n    daft.context.set_runner_ray(\n        # You may provide Daft with the address to an existing Ray cluster if you have one!\n        # If this is not provided, Daft will default to spinning up a single-node Ray cluster consisting of just your current local machine\n        address=RAY_ADDRESS,\n    )\n```\n\n----------------------------------------\n\nTITLE: Creating Expressions: Wildcard Struct Access (SQL)\nDESCRIPTION: This snippet demonstrates how to access all members of a struct column using SQL syntax (SELECT person.* FROM df). The input is a Daft DataFrame containing a struct column named 'person', and the output is a DataFrame with each field of the 'person' struct as a separate column.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nimport daft\n\ndf = daft.from_pydict({\n    \"person\": [\n        {\"name\": \"Alice\", \"age\": 30},\n        {\"name\": \"Bob\", \"age\": 25},\n        {\"name\": \"Charlie\", \"age\": 35}\n    ]\n})\n\n# Access all fields of the 'person' struct using SQL\ndaft.sql(\"SELECT person.* FROM df\").show()\n```\n\n----------------------------------------\n\nTITLE: Truncating Timestamps to Hour Start with Daft DataFrame in Python\nDESCRIPTION: This code demonstrates the use of Daft's timestamp truncation feature, which reduces all timestamps to the start of the specified unit (here, hour). Requires the `daft` library with datetime support. The input has 'timestamp' values; the output adds 'hour_start', which groups all times within the same hour. The time granularity parameter ('1 hour') controls truncation level.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_50\n\nLANGUAGE: Python\nCODE:\n```\ndf = daft.from_pydict({\n    \"timestamp\": [\n        datetime.datetime(2021, 1, 7, 0, 1, 1),\n        datetime.datetime(2021, 1, 8, 0, 1, 59),\n        datetime.datetime(2021, 1, 9, 0, 30, 0),\n        datetime.datetime(2021, 1, 10, 1, 59, 59),\n    ]\n})\n\n# Truncate timestamps to the nearest hour\ndf = df.with_column(\n    \"hour_start\",\n    df[\"timestamp\"].dt.truncate(\"1 hour\")\n)\n\ndf.show()\n\n```\n\n----------------------------------------\n\nTITLE: Example Data Processing Script Using Daft - Python\nDESCRIPTION: A minimal Python script illustrating how to load data into a Daft DataFrame and perform an aggregation operation while assuming Ray context is already configured by the Daft CLI. The script calculates the mean of the \"nums\" column and shows the result, demonstrating basic Daft DataFrame usage.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/distributed.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport daft\n\n# Ray context is automatically set by Daft CLI\ndf = daft.from_pydict({\"nums\": [1,2,3]})\ndf.agg(daft.col(\"nums\").mean()).show()\n```\n\n----------------------------------------\n\nTITLE: Showing Sample Records from a Daft Dataframe in Python\nDESCRIPTION: This snippet prints a sample of three records from a Daft dataframe to visually inspect the data contents, often after transformations involving complex types like images or URLs.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/linkedin-03-05-2024.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nlaion_df.show(3)\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS IO - Daft - Python\nDESCRIPTION: Imports the `boto3` and `daft` libraries. It initializes a Boto3 session, retrieves AWS credentials, and creates a Daft `IOConfig` object configured for S3 access using these credentials. Requires AWS credentials configured for `boto3`.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/1-local-image-batch-inference.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport boto3\n\nimport daft\n\nsession = boto3.session.Session()\ncreds = session.get_credentials()\nio_config = daft.io.IOConfig(\n    s3=daft.io.S3Config(\n        access_key=creds.secret_key,\n        key_id=creds.access_key,\n        session_token=creds.token,\n        region_name=\"us-west-2\",\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Class-based UDF in Daft\nDESCRIPTION: This code demonstrates how to create a UDF using a class. The `RunModel` class encapsulates an expensive initialization process in its `__init__` method and performs the UDF logic in its `__call__` method. The `@daft.udf` decorator is used to register the class as a UDF, and the resulting UDF can be used in a `df.with_column` call.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_65\n\nLANGUAGE: Python\nCODE:\n```\n@daft.udf(return_dtype=daft.DataType.int64())\nclass RunModel:\n\n    def __init__(self):\n        # Perform expensive initializations\n        self._model = create_model()\n\n    def __call__(self, features_col):\n        return self._model(features_col)\n```\n\n----------------------------------------\n\nTITLE: Reading and Repartitioning Parquet Data using Daft with Ray in Python\nDESCRIPTION: Reads a Parquet file from an S3 bucket using Daft with the preconfigured IO settings, limits the number of rows for testing, and repartitions the dataframe into 8 partitions to enable parallel processing. Collects the dataframe into local memory for inspection. This snippet prepares the dataset to effectively utilize Ray's distributed processing capabilities by dividing workload into partitions.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/text_to_image/using_cloud_with_ray.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom daft import col\n\nparquet_df = daft.read_parquet(PARQUET_URL, io_config=IO_CONFIG).limit(NUM_ROWS_LIMIT).repartition(8)\nparquet_df.collect()\n```\n\n----------------------------------------\n\nTITLE: Loading Data from DeltaLake Using daft in Python\nDESCRIPTION: Reads a sample of image data stored in DeltaLake format from an S3 bucket, using the provided IOConfig for access. The resulting DataFrame allows subsequent data processing and analysis.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/3-pytorch-ray-single-node-training.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nimport daft\n\ndf = daft.read_deltalake(\"s3://daft-public-datasets/imagenet/val-10k-sample-deltalake/\", io_config=io_config)\ndf\n```\n\n----------------------------------------\n\nTITLE: Daft CLI Configuration Initialization - Bash\nDESCRIPTION: Commands used to create initial Daft CLI configuration files tailored to the selected deployment provider: 'provisioned' mode for AWS-managed clusters, or 'byoc' for user-supplied Kubernetes clusters. These commands generate .daft.toml files with template sections enabling further cluster and job configuration.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/distributed.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# For Provisioned Mode\ndaft config init --provider provisioned\n\n# For BYOC Mode\ndaft config init --provider byoc\n```\n\n----------------------------------------\n\nTITLE: Reading a Single Column from Parquet\nDESCRIPTION: This snippet reads a single column (`L_ORDERKEY`) from Parquet files, demonstrating the use of `select()` method.  It first reads the entire file and then selects the desired column.  The `collect()` method triggers the data loading.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/pydata_global_2023.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.read_parquet(\n    \"s3://eventual-dev-benchmarking-fixtures/uncompressed-smaller-rg/tpch-dbgen/100_0/32/parquet/lineitem/\"\n)\ndf = df.select(\"L_ORDERKEY\")\n```\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\ndf.collect()\n```\n\n----------------------------------------\n\nTITLE: Reading Iceberg Tables into Daft Dataframes Using Python\nDESCRIPTION: This snippet demonstrates loading an Iceberg table metadata from an AWS Glue catalog and reading it into a Daft dataframe. It requires the pyiceberg library and AWS Glue catalog access. The dataframe is then displayed with the show() method.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/linkedin-03-05-2024.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom pyiceberg.catalog.glue import GlueCatalog\n\ncatalog = GlueCatalog(\"default\")\niceberg_table = catalog.load_table(\"tpch_iceberg_sf100.lineitem\")\n\nice_df = daft.read_iceberg(iceberg_table)\nice_df.show()\n```\n\n----------------------------------------\n\nTITLE: Creating a DataFrame from Ray Dataset with Daft - Python\nDESCRIPTION: Describes converting a Ray Dataset to a Daft DataFrame for interoperability. Requires daft and ray installed. Input is a ray.data.Dataset object, output is a Daft DataFrame. Data conversion overhead may be significant for large datasets.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/api/dataframe_creation.md#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nimport daft\nimport ray\n\ndataset = ray.data.from_items([{...}, {...}])\ndf = daft.from_ray_dataset(dataset)\n```\n\n----------------------------------------\n\nTITLE: Showing DataFrame Rows - Daft - Python\nDESCRIPTION: Triggers computation and displays the first 4 rows of the current state of the Daft DataFrame, including the newly added image columns. This is often used during interactive development to inspect data transformations.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/1-local-image-batch-inference.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndf.show(4)\n```\n\n----------------------------------------\n\nTITLE: Mixed Aggregation Example\nDESCRIPTION: This snippet demonstrates the use of the `.agg()` method in Daft to perform multiple aggregations on a DataFrame at once. It calculates the mean, max, and count of the 'score' and 'class' columns, aliasing the results for better readability, and then displays the results.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_58\n\nLANGUAGE: python\nCODE:\n```\ndf.agg(\n    df[\"score\"].mean().alias(\"mean_score\"),\n    df[\"score\"].max().alias(\"max_score\"),\n    df[\"class\"].count().alias(\"class_count\"),\n).show()\n```\n\n----------------------------------------\n\nTITLE: Limiting Rows and Writing to Delta Lake Using Daft in Python\nDESCRIPTION: Here, the DataFrame is limited to 8 rows to control API usage/cost, then written to a Delta Lake table with the path 'my_table.delta_lake'. Prerequisites: Proper path permissions, Delta Lake backend available. Inputs: Preprocessed DataFrame. Outputs: Persisted multimodal data lake table.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/data-ai-summit-2024.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Limit to running just 8 rows to save your OpenAI bill...\ndf = df.limit(8)\n\ndf.write_delta(\"my_table.delta_lake\")\n```\n\n----------------------------------------\n\nTITLE: BYOC Mode Kubernetes Cluster Verification - Bash\nDESCRIPTION: These commands validate Kubernetes cluster connectivity and set the correct context required for Daft BYOC mode. Ensures kubectl is configured to communicate with the intended Kubernetes cluster, a prerequisite for managing Daft and Ray resources on an existing Kubernetes infrastructure.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/distributed.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Verify your kubernetes connection\nkubectl cluster-info\n\n# Set the correct context if needed\nkubectl config use-context my-context\n```\n\n----------------------------------------\n\nTITLE: Applying Mask Summation UDF (Python)\nDESCRIPTION: Adds a new column 'num_pixels_red' to the DataFrame `df` by applying the custom Python function `sum_mask` to each element in the 'red_mask' column using `.apply()`. The return type is specified as `daft.DataType.int64()`.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/image_querying/top_n_red_color.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndf = df.with_column(\n    \"num_pixels_red\",\n    df[\"red_mask\"].apply(sum_mask, return_dtype=daft.DataType.int64()),\n)\n```\n\n----------------------------------------\n\nTITLE: Running TPC-H Query with Daft\nDESCRIPTION: This snippet demonstrates a full TPC-H query using Daft. It reads data from parquet files, performs calculations, and aggregates the results. It utilizes `where`, `groupby`, `agg`, and `sort` methods on the DataFrame to compute a query.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/pydata_global_2023.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\n\nfrom daft import col\n\nlineitem = get_df(\"lineitem\")\n\ndiscounted_price = col(\"L_EXTENDEDPRICE\") * (1 - col(\"L_DISCOUNT\"))\ntaxed_discounted_price = discounted_price * (1 + col(\"L_TAX\"))\ndf = (\n    lineitem.where(col(\"L_SHIPDATE\") <= datetime.date(1998, 9, 2))\n    .groupby(col(\"L_RETURNFLAG\"), col(\"L_LINESTATUS\"))\n    .agg(\n        col(\"L_QUANTITY\").alias(\"sum_qty\").sum(),\n        col(\"L_EXTENDEDPRICE\").alias(\"sum_base_price\").sum(),\n        discounted_price.alias(\"sum_disc_price\").sum(),\n        taxed_discounted_price.alias(\"sum_charge\").sum(),\n        col(\"L_QUANTITY\").alias(\"avg_qty\").mean(),\n        col(\"L_EXTENDEDPRICE\").alias(\"avg_price\").mean(),\n        col(\"L_DISCOUNT\").alias(\"avg_disc\").mean(),\n        col(\"L_QUANTITY\").alias(\"count_order\").count(),\n    )\n    .sort([\"L_RETURNFLAG\", \"L_LINESTATUS\"])\n)\n```\n\n----------------------------------------\n\nTITLE: Selecting Columns in Daft DataFrame (Python)\nDESCRIPTION: Selects specific columns (\"URL\", \"TEXT\", \"AESTHETIC_SCORE\") from the existing `parquet_df` Daft DataFrame. This operation creates a new view or DataFrame containing only the specified columns, assigned back to `parquet_df`.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/text_to_image/text_to_image_generation.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nparquet_df = parquet_df.select(parquet_df[\"URL\"], parquet_df[\"TEXT\"], parquet_df[\"AESTHETIC_SCORE\"])\n```\n\n----------------------------------------\n\nTITLE: Filtering rows based on image shape and applying classification\nDESCRIPTION: Filters DataFrame rows where images have 3 color channels, then applies the 'ClassifyImage' UDF to both small and large resized images for predictions. Final step involves selecting relevant columns and writing results back to Parquet format.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/2-distributed-batch-inference.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n# Filter out rows where the channel != 3\n df = df.where(df[\"image\"].apply(lambda img: img.shape[2] == 3, return_dtype=daft.DataType.bool()))\n\n df = df.with_column(\"predictions_lowres\", ClassifyImage(df[\"image_resized_small\"], [32, 32, 3]))\n df = df.with_column(\"predictions_highres\", ClassifyImage(df[\"image_resized_large\"], [256, 256, 3]))\n\n # Prune the results and write data back out as Parquet\ndf = df.select(\n     \"filename\",\n     \"image_url\",\n     \"object\",\n     \"predictions_lowres\",\n     \"predictions_highres\",\n)\ndf.write_parquet(\"my_results.parquet\")\n```\n\n----------------------------------------\n\nTITLE: Parsing and Converting Time Zones with Daft SQL API in Python\nDESCRIPTION: This snippet uses Daft’s SQL API to parse string representations of timestamps with time zones and convert them to another time zone. It demonstrates use of the `to_datetime` SQL function with format and time zone arguments, producing timestamps localized to 'America/New_York'. Requires Daft, the specified format pattern, and valid target time zone name. Input column must be a string formatted timestamp.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_49\n\nLANGUAGE: Python\nCODE:\n```\ndf = daft.from_pydict({\n    \"timestamp_str\": [\n        \"2021-01-01 00:00:00.123 +0800\",\n        \"2021-01-02 12:30:00.456 +0800\"\n    ]\n})\n\n# Parse the timestamp string with time zone and convert to New York time\ndf = daft.sql(\"\"\"\n    SELECT\n        timestamp_str,\n        to_datetime(timestamp_str, '%Y-%m-%d %H:%M:%S%.3f %z', 'America/New_York') as ny_time\n    FROM df\n\"\"\")\n\ndf.show()\n\n```\n\n----------------------------------------\n\nTITLE: Exclude Specific Column - Daft DataFrame (Python)\nDESCRIPTION: This code snippet demonstrates how to remove a specific column from a Daft DataFrame using the `exclude()` method. It removes column \"A\" from the DataFrame `df` and then displays the resulting DataFrame, which will contain all original columns except \"A\". It requires a DataFrame `df` to be previously defined with a column 'A'.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndf.exclude(\"A\").show()\n```\n\n----------------------------------------\n\nTITLE: String Contains - Python\nDESCRIPTION: Checks if each element in column 'B2' contains the substring from column 'B' in Daft. The `str.contains()` method is called on column 'B2' and provided column 'B' as the substring to search for. A new boolean column 'B2_contains_B' is created reflecting the result.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ndf = df.with_column(\"B2_contains_B\", df[\"B2\"].str.contains(df[\"B\"]))\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Applying Chained Method Functions for URL Download and Image Decode - DataFrame API in Python\nDESCRIPTION: Shows the equivalent of applying multiple operations (URL download and image decode) using the chained method API on Daft Expressions. Requires daft and a DataFrame of URLs. The expression daft.col(\"urls\").url.download().image.decode() creates a pipeline of operations per element; select applies this transformation. Inputs are the DataFrame and expression; output is a DataFrame with a column of image objects rendered in .show(). Both SQL and Python APIs yield the same functional outcome.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql_overview.md#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ndf = daft.from_pydict({\"urls\": [\n    \"https://user-images.githubusercontent.com/17691182/190476440-28f29e87-8e3b-41c4-9c28-e112e595f558.png\",\n    \"https://user-images.githubusercontent.com/17691182/190476440-28f29e87-8e3b-41c4-9c28-e112e595f558.png\",\n    \"https://user-images.githubusercontent.com/17691182/190476440-28f29e87-8e3b-41c4-9c28-e112e595f558.png\",\n]})\ndf = df.select(daft.col(\"urls\").url.download().image.decode())\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Sort Rows by Column Descending - Daft DataFrame (Python)\nDESCRIPTION: This snippet demonstrates reordering the rows of a Daft DataFrame based on the values in a specific column using the `sort()` method. It initializes a DataFrame and then sorts the rows by column \"A\" in descending order by setting the `desc=True` parameter. The sorted DataFrame is then displayed. This requires importing `daft`.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.from_pydict({\n    \"A\": [1, 2, 3],\n    \"B\": [6, 7, 8],\n})\n\ndf.sort(\"A\", desc=True).show()\n```\n\n----------------------------------------\n\nTITLE: Defining a UDF for image classification with PyTorch\nDESCRIPTION: This code defines a UDF that uses a pre-trained PyTorch ResNet50 model to classify images. It downloads the model and preprocessing utilities from NVIDIA's DeepLearningExamples hub. The `ClassifyImages` class initializes the model in its `__init__` method and performs the classification in its `__call__` method. The function returns the top classification results.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_69\n\nLANGUAGE: Python\nCODE:\n```\nimport torch\n```\n\nLANGUAGE: Python\nCODE:\n```\n@udf(return_dtype=DataType.fixed_size_list(dtype=DataType.string(), size=2))\nclass ClassifyImages:\n    def __init__(self):\n        # Perform expensive initializations - create and load the pre-trained model\n        self.model = torch.hub.load(\"NVIDIA/DeepLearningExamples:torchhub\", \"nvidia_resnet50\", pretrained=True)\n        self.utils = torch.hub.load(\"NVIDIA/DeepLearningExamples:torchhub\", \"nvidia_convnets_processing_utils\")\n        self.model.eval().to(torch.device(\"cpu\"))\n\n    def __call__(self, images_urls):\n        batch = torch.cat([self.utils.prepare_input_from_uri(uri) for uri in images_urls]).to(torch.device(\"cpu\"))\n\n        with torch.no_grad():\n            output = torch.nn.functional.softmax(self.model(batch), dim=1)\n\n        results = self.utils.pick_n_best(predictions=output, n=1)\n        return [result[0] for result in results]\n```\n\n----------------------------------------\n\nTITLE: Extracting Date Components with Accessor Properties\nDESCRIPTION: Demonstrates using the datetime accessor to extract the year from a date column.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/10min.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndf_year = df.with_column(\"DoB_year\", df[\"DoB\"].dt.year())\ndf_year.show()\n```\n\n----------------------------------------\n\nTITLE: Defining Image Types in SQL for Daft\nDESCRIPTION: SQL syntax examples for defining Daft Image types. The `IMAGE(mode, [dimensions])` syntax specifies the image mode (e.g., 'RGB') and optional dimensions (height, width). Specifying dimensions is recommended for optimization.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql/datatypes.md#_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nIMAGE('RGB')\nIMAGE('RGB', 256, 256)\n```\n\n----------------------------------------\n\nTITLE: Displaying Daft DataFrame\nDESCRIPTION: Displays the contents of a Daft DataFrame. `images_df` is displayed to inspect the loaded data. And then the `show` method is used to display the first 10 rows.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/mnist.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimages_df\n```\n\nLANGUAGE: python\nCODE:\n```\nimages_df.show(10)\n```\n\n----------------------------------------\n\nTITLE: Classifying Dog Breeds Using the UDF and Image URLs\nDESCRIPTION: Applies the 'ClassifyImages' UDF to the 'urls' column to predict dog breeds with a pre-trained ResNet50 model, then displays selected columns including images and classification results.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/10min.ipynb#_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\nclassified_images_df = df_family.with_column(\"classify_breed\", ClassifyImages(daft.col(\"urls\")))\n\nclassified_images_df.select(\"dog_name\", \"image\", \"classify_breed\").show()\n```\n\n----------------------------------------\n\nTITLE: Writing to an Iceberg Table from a Daft DataFrame\nDESCRIPTION: This snippet demonstrates writing a Daft DataFrame to an Iceberg table using the `df.write_iceberg()` method.  It appends the data to the specified `table` and returns a DataFrame containing information about the write operation. The `mode=\"append\"` argument ensures that the data is appended to the existing table.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/iceberg.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nwritten_df = df.write_iceberg(table, mode=\"append\")\nwritten_df.show()\n```\n\n----------------------------------------\n\nTITLE: String Contains - SQL\nDESCRIPTION: Checks if each element in column 'B2' contains the substring from column 'B' in Daft using SQL. The `contains()` function is used to perform the substring check within a SQL query.  A new boolean column 'B2_contains_B' is created reflecting the result.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.sql(\"SELECT *, contains(B2, B) AS B2_contains_B FROM df\")\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Writing Daft DataFrame to Local Parquet (Python)\nDESCRIPTION: This code limits the DataFrame to the first 5 rows using `.limit(5)` and then writes the resulting DataFrame, including all the processed image columns, to a local Parquet file named \"resized_images.parquet\". The result of `write_parquet` is a DataFrame representing the outcome of the write operation.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/flyte/notebook.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nwritten_df = filtered_df.limit(5).write_parquet(\"resized_images.parquet\")\n```\n\n----------------------------------------\n\nTITLE: Setting Daft Runner to Ray Backend in Python\nDESCRIPTION: Configures Daft to use Ray as its execution backend by setting the runner address, enabling distributed data processing and computation across cluster nodes.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/3-pytorch-ray-single-node-training.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ndaft.context.set_runner_ray(address=RAY_ADDRESS)\n```\n\n----------------------------------------\n\nTITLE: Exploding Columns in Daft (Python)\nDESCRIPTION: This code snippet shows how to use the `df.explode()` method to expand a column containing lists into multiple rows, duplicating other column values accordingly. The input is a Daft DataFrame with a column of lists, and the output is a DataFrame with each element of the list in a separate row.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.from_pydict({\n    \"A\": [1, 2, 3],\n    \"B\": [[1, 2, 3], [4, 5, 6], [7, 8, 9]],\n})\n\ndf.explode(\"B\").show()\n```\n\n----------------------------------------\n\nTITLE: Sorting a Daft DataFrame by Column using Python\nDESCRIPTION: This snippet shows how to sort a Daft DataFrame based on the 'age' column using the `sort()` method and `daft.col`. The `desc=False` argument explicitly specifies ascending order (which is also the default). The sorted DataFrame is then displayed using `show()`. Depends on an existing Daft DataFrame `df` and the `daft` library.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/quickstart.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndf.sort(daft.col(\"age\"), desc=False).show()\n```\n\n----------------------------------------\n\nTITLE: Collecting Aggregated Data\nDESCRIPTION: Collects the results of the aggregation performed by Daft on the Ray cluster. The `%%time` cell magic is used to measure the execution time. The `sort` method sorts by the `L_SHIPDATE` column and then `collect` triggers the actual execution.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/iceberg_summit_2024.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\naggregated_df.sort(\"L_SHIPDATE\").collect()\n```\n\n----------------------------------------\n\nTITLE: Adding a Transformed Column to a Daft DataFrame using Python\nDESCRIPTION: This snippet illustrates creating a new column (`full_name`) in a Daft DataFrame using the `with_column()` method and Daft Expressions (`daft.col`). It concatenates the `first_name` and `last_name` columns with a space in between. The modified DataFrame, now including the `full_name` column, is then partially selected and displayed using `show()`. Depends on an existing Daft DataFrame `df` and the `daft` library.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/quickstart.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndf = df.with_column(\"full_name\", daft.col(\"first_name\") + \" \" + daft.col(\"last_name\"))\ndf.select(\"full_name\", \"age\", \"country\", \"has_dog\").show()\n```\n\n----------------------------------------\n\nTITLE: Connecting Daft to Microsoft Fabric/OneLake in Python\nDESCRIPTION: Illustrates configuring Daft to connect to Azure storage within Microsoft Fabric or OneLake. This requires creating an `AzureConfig` with `storage_account` set to \"onelake\" and `use_fabric_endpoint` set to `True`. Credentials should be provided as needed. The example demonstrates reading a Delta Lake table from a Fabric Lakehouse using this specific `IOConfig`.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/azure.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom daft.io import IOConfig, AzureConfig\n\nio_config = IOConfig(\n    azure=AzureConfig(\n        storage_account=\"onelake\",\n        use_fabric_endpoint=True,\n\n        # Set credentials as needed\n    )\n)\n\ndf = daft.read_deltalake('abfss://[WORKSPACE]@onelake.dfs.fabric.microsoft.com/[LAKEHOUSE].Lakehouse/Tables/[TABLE]', io_config=io_config)\n```\n\n----------------------------------------\n\nTITLE: Downloading Image Bytes from S3 URLs Using Daft in Python\nDESCRIPTION: This code adds a new column 'image_bytes' to the DataFrame by downloading image data referenced by the 'path' using Daft's URL download method. Downloads are performed asynchronously for network efficiency. Input: URL strings from 'path' column. Output: Byte arrays of images. Prerequisites: Networking permissions, Daft async runtime.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/data-ai-summit-2024.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf = df.with_column(\"image_bytes\", df[\"path\"].url.download())\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Materializing the DataFrame Results in Python\nDESCRIPTION: This snippet collects (materializes) the results of the processed, filtered Daft DataFrame, executing any pending computations. Input: Transformed DataFrame. Output: Realized, in-memory collection of results.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/data-ai-summit-2024.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nread_df.collect()\n```\n\n----------------------------------------\n\nTITLE: Writing DataFrame to File\nDESCRIPTION: This snippet demonstrates writing a Daft DataFrame to files using `df.write_*()` methods. It provides examples of writing to various file formats and locations including local and remote filesystems like AWS S3. Note that it is a blocking operation, and will return a new DataFrame of filepaths.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_56\n\nLANGUAGE: python\nCODE:\n```\ndf.write_csv(\"path/to/folder/\")\ndf.write_parquet(\"path/to/folder/\")\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.write_csv(\"s3://mybucket/path/\")\n```\n\n----------------------------------------\n\nTITLE: Read SQL with SQLAlchemy Connection Factory (Python)\nDESCRIPTION: This snippet uses a SQLAlchemy connection factory to read data into a Daft DataFrame. It creates a function that returns a SQLAlchemy connection object, allowing for more control over connection parameters. It imports `daft` and `sqlalchemy`.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/sql.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Read SQL query into Daft DataFrame using a connection factory\nimport daft\nfrom sqlalchemy import create_engine\n\ndef create_connection():\n    return sqlalchemy.create_engine(\"sqlite:///example.db\", echo=True).connect()\n\ndf = daft.read_sql(\"SELECT * FROM books\", create_connection)\n```\n\n----------------------------------------\n\nTITLE: Adding Columns Using DataFrame Expressions - Python\nDESCRIPTION: Shows use of Daft's Python Expression API to create a new column by programmatically adding two columns and aliasing the result. Requires the daft library and a DataFrame from daft.from_pydict. The key parameter is the Expression object, created by arithmetic and aliasing. Selecting and showing the DataFrame yields the same output as the SQL approach. Input is a DataFrame; output features a new column as sum of specified columns.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql_overview.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nexpr = (daft.col(\"A\") + daft.col(\"B\")).alias(\"C\")\n\ndf = daft.from_pydict({\"A\": [1, 2, 3], \"B\": [1, 2, 3]})\ndf = df.select(expr)\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Limiting Result Rows\nDESCRIPTION: Demonstrates how to limit the number of rows returned from a DataFrame, useful for working with sample data.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/10min.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf.limit(1).show()\n```\n\n----------------------------------------\n\nTITLE: Reading Private Dataset with Glob Pattern (Python)\nDESCRIPTION: This code reads files from a private dataset using a glob pattern, which is a workaround to access the data if standard direct access fails. This approach assumes the files are in parquet format. It leverages the `io_config` for authentication.  Requires the Daft library and prior `io_config` configuration with a valid bearer token.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/huggingface.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.read_parquet(\"hf://datasets/username/my_private_dataset/**/*.parquet\", io_config=io_config) # Works\n```\n\n----------------------------------------\n\nTITLE: Executing Daft Write Operation (Python)\nDESCRIPTION: Calling `collect()` on the DataFrame returned by `write_parquet` triggers the execution of the entire Daft plan, including the read, transforms, and the final write operation, saving the data to the specified local Parquet file.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/flyte/notebook.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nwritten_df.collect()\n```\n\n----------------------------------------\n\nTITLE: Overriding S3 Credentials for Single Operation in Daft Python\nDESCRIPTION: This snippet shows how to use a specific `IOConfig` object to override the default or environment-based AWS credentials for a single Daft I/O operation. By passing the `io_config` object to the `io_config` keyword argument of an I/O function like `daft.read_csv`, users can apply different S3 configurations on a per-call basis. This offers fine-grained control over authentication for different datasets or buckets.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/aws.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# Perform some I/O operation but override the IOConfig\ndf2 = daft.read_csv(\"s3://my_bucket/my_other_path/**/*\", io_config=io_config)\n```\n\n----------------------------------------\n\nTITLE: Downloading and Decoding Images with Daft in Python\nDESCRIPTION: Filters the `parquet_df` DataFrame to keep rows where the 'TEXT' column string length exceeds 50 characters. It then uses Daft's built-in URL operations to download the content from the 'URL' column, attempting to decode it as an image (`.image.decode()`), handling download errors by returning null (`on_error=\"null\"`). The resulting image objects are stored in a new 'image' column. The operation is limited to 5 results, and rows with failed image downloads/decoding are filtered out.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/text_to_image/text_to_image_generation.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Filter for images with longer descriptions\nparquet_df_with_long_strings = parquet_df.where(parquet_df[\"TEXT\"].str.length() > 50)\n\n# Download images\nimages_df = (\n    parquet_df_with_long_strings.with_column(\n        \"image\",\n        parquet_df[\"URL\"].url.download(on_error=\"null\").image.decode(),\n    )\n    .limit(5)\n    .where(daft.col(\"image\").not_null())\n)\n```\n\n----------------------------------------\n\nTITLE: SQL with Data Skipping Optimization (Python)\nDESCRIPTION: This snippet demonstrates data skipping optimization by pushing down filters and projections into the SQL query.  It uses `daft.read_sql` to read from a BigQuery table, and then applies `where` and `select` operations. The pushed down SQL query is displayed using `df.explain(show_all=True)`.  It imports `daft`, `sqlalchemy`, and `datetime`.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/sql.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport daft, sqlalchemy, datetime\n\ndef create_conn():\n    engine = sqlalchemy.create_engine(\n        \"bigquery://\", credentials_path=\"path/to/service_account_credentials.json\"\n    )\n    return engine.connect()\n\n\ndf = daft.read_sql(\"SELECT * FROM `bigquery-public-data.google_trends.top_terms`\", create_conn)\n\ndf = df.where((df[\"refresh_date\"] >= datetime.date(2024, 4, 1)) & (df[\"refresh_date\"] < datetime.date(2024, 4, 8)))\ndf = df.where(df[\"rank\"] == 1)\ndf = df.select(df[\"refresh_date\"].alias(\"Day\"), df[\"term\"].alias(\"Top Search Term\"), df[\"rank\"])\ndf = df.distinct()\ndf = df.sort(df[\"Day\"], desc=True)\n\ndf.explain(show_all=True)\n```\n\n----------------------------------------\n\nTITLE: Explain Aggregated Daft DataFrame\nDESCRIPTION: Displays the execution plan for the aggregated Daft DataFrame. This provides insights into how Daft will execute the aggregation and filtering operations. It's used to analyze the query plan and potentially optimize the query execution.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/iceberg_summit_2024.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\naggregated_df.explain(True)\n```\n\n----------------------------------------\n\nTITLE: Configuring Identifier Mode via Python API - Python\nDESCRIPTION: This Python snippet uses the Daft library's Session API to configure the identifier_mode session option. Requires the daft Python package. The set_option function sets the casing mode, with options: 'insensitive', 'sensitive', or 'normalize'. Input: mode string. Output: session state updated for identifier parsing.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql/identifiers.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n# python\n\nfrom daft import Session\n\nsess = Session()\nsess.set_option(\"identifier_mode\", \"sensitive\")\n```\n\n----------------------------------------\n\nTITLE: Creating an Iceberg Table\nDESCRIPTION: Creates an Iceberg table within the catalog. It defines the schema of the table, including the data types and required/optional fields. The `create_table` method is used, and it specifies the namespace and table name. This example creates a table named `my_friends` within the `my_namespace`.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/iceberg_summit_2024.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pyiceberg.schema import IntegerType, NestedField, Schema, StringType\n\nSCHEMA = Schema(\n    NestedField(1, \"name\", StringType(), required=False),\n    NestedField(2, \"age\", IntegerType(), required=False),\n)\n\ntable = catalog.create_table(\"my_namespace.my_friends\", schema=SCHEMA)\n```\n\n----------------------------------------\n\nTITLE: Provisioning AWS Credentials Using boto3 and daft in Python\nDESCRIPTION: Creates AWS S3 credentials using boto3 to instantiate a daft IOConfig object, enabling access to S3 data in the DeltaLake table. It retrieves current session credentials including access key, secret key, session token, and region.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/3-pytorch-ray-single-node-training.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport boto3\n\nimport daft\n\nsession = boto3.session.Session()\ncreds = session.get_credentials()\nio_config = daft.io.IOConfig(\n    s3=daft.io.S3Config(\n        access_key=creds.secret_key,\n        key_id=creds.access_key,\n        session_token=creds.token,\n        region_name=\"us-west-2\",\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Overriding UDF resource requests\nDESCRIPTION: Demonstrates how to override the resource requirements of a UDF. `RunModelWithOneGPU.override_options` is used to modify the resource request to use two GPUs.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_68\n\nLANGUAGE: Python\nCODE:\n```\nRunModelWithTwoGPUs = RunModelWithOneGPU.override_options(num_gpus=2)\ndf = df.with_column(\n    \"image_classifications\",\n    RunModelWithTwoGPUs(df[\"images\"]),\n)\n```\n\n----------------------------------------\n\nTITLE: Filtering Rows in a Daft DataFrame using where()\nDESCRIPTION: This code filters the rows of a Daft DataFrame (`df`) using the `where()` method. It takes a boolean expression constructed using Daft expressions (specifically `daft.col(\"age\") >= 40`) as input. Only rows where the expression evaluates to true are kept in the resulting lazy DataFrame. The `.show()` method is used to execute the filter and display the resulting rows.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/quickstart.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf.where(daft.col(\"age\") >= 40).show()\n```\n\n----------------------------------------\n\nTITLE: Identifying Ray Object Spilling Logs in Daft\nDESCRIPTION: Example log message that indicates when Daft is spilling data to disk during Ray execution. This occurs when memory pressure is high, allowing the system to continue processing without OOM errors at the cost of performance.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/advanced/memory.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n(raylet, ip=xx.xx.xx.xx) Spilled 16920 MiB, 9 objects, write throughput 576 MiB/s.\n```\n\n----------------------------------------\n\nTITLE: Examining Query Execution Plans\nDESCRIPTION: Demonstrates how to view and understand Daft's logical query plan with the explain() method, showing optimization on partitioned data.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/10min.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndf2 = daft.read_parquet(\"s3://daft-public-data/tutorials/10-min/sample-data-dog-owners-partitioned.pq/**\")\ndf2.where(df[\"country\"] == \"Canada\").explain(show_all=True)\n```\n\n----------------------------------------\n\nTITLE: Sorting DataFrame Data\nDESCRIPTION: Shows how to sort a DataFrame by a specific column in ascending order using the sort() method.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/10min.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndf.sort(df[\"age\"], desc=False).show()\n```\n\n----------------------------------------\n\nTITLE: Selecting and Displaying Columns - Python\nDESCRIPTION: This code filters the DataFrame to show only selected columns: \"image\", \"arr\", \"class_human_readable\", and \"class_id\".  It then displays the first four rows of the DataFrame.  This is useful for data exploration and verification of preprocessing.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/3-pytorch-ray-single-node-training.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndf = df.select(\"image\", \"arr\", \"class_human_readable\", \"class_id\")\ndf.show(4)\n```\n\n----------------------------------------\n\nTITLE: Defining and Applying Stable Diffusion UDF with Daft (Python)\nDESCRIPTION: Defines a Daft User-Defined Function (UDF) `GenerateImageFromText` using the `@daft.udf` decorator on a class. The UDF initializes a Stable Diffusion pipeline (`runwayml/stable-diffusion-v1-5`) within its `__init__` method. The `__call__` method iterates through input text prompts, generating an image for each using the pipeline. It conditionally overrides UDF resource requests using `.override_options(num_gpus=1)` if `USE_GPU` is true. Finally, it applies this UDF to the 'TEXT' column of `images_df`, creating a 'generated_image' column, and shows the first result.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/text_to_image/text_to_image_generation.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom diffusers import StableDiffusionPipeline\n\n\n@daft.udf(return_dtype=daft.DataType.python())\nclass GenerateImageFromText:\n    def __init__(self):\n        model_id = \"runwayml/stable-diffusion-v1-5\"\n        self.pipe = StableDiffusionPipeline.from_pretrained(\n            model_id,\n            torch_dtype=torch.float32,\n        )\n        self.pipe.enable_attention_slicing(1)\n\n    def generate_image(self, prompt):\n        return self.pipe(prompt, num_inference_steps=20, height=512, width=512).images[0]\n\n    def __call__(self, text_col):\n        return [self.generate_image(t) for t in text_col]\n\n\nif USE_GPU:\n    GenerateImageFromText = GenerateImageFromText.override_options(num_gpus=1)\n\nimages_df.with_column(\n    \"generated_image\",\n    GenerateImageFromText(images_df[\"TEXT\"]),\n).show(1)\n```\n\n----------------------------------------\n\nTITLE: Setting up the Notebook\nDESCRIPTION: Sets up the notebook environment by removing the `/tmp/warehouse` directory if it exists and recreates it. This is a common practice for cleaning and preparing the file system for the demo, which creates temporary storage for Iceberg tables.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/iceberg_summit_2024.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pathlib\nimport shutil\n\nshutil.rmtree(\"/tmp/warehouse\")\npathlib.Path(\"/tmp/warehouse\").mkdir(exist_ok=True)\n```\n\n----------------------------------------\n\nTITLE: Reshaping Image Data using UDF\nDESCRIPTION: Reshapes the image data from a one-dimensional array to a two-dimensional array using a User-Defined Function (UDF) with `.apply()`.  The lambda function uses NumPy to reshape the image data. The `return_dtype` is set to `DataType.python()`.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/mnist.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nimages_df = images_df.with_column(\n    \"image_2d\",\n    col(\"image\").apply(lambda img: np.array(img).reshape(28, 28), return_dtype=DataType.python()),\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Columns with SQL Expressions - Python\nDESCRIPTION: Demonstrates creating a new column by adding two existing columns using an SQL expression within daft.sql. This approach computes a new field ('C') as the sum of columns 'A' and 'B'. Necessary prerequisites are a Daft DataFrame and the daft library. The SQL query outputs a DataFrame with the computed column, while .show() prints the result. Input is a Python dictionary DataFrame; output is a DataFrame with an additional computed integer column.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql_overview.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndf = daft.from_pydict({\"A\": [1, 2, 3], \"B\": [1, 2, 3]})\ndf = daft.sql(\"SELECT A + B as C FROM df\")\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Creating Image Thumbnails Using Daft in Python\nDESCRIPTION: This code creates a 32x32 pixel thumbnail image for each decoded image using Daft's image resize operation, adds it as a new column, and displays the preview. Inputs: Images from the 'image' column. Parameters: Target thumbnail size (32x32). Output: 'image_thumbnail' column with resized images. No external dependencies.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/data-ai-summit-2024.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf = df.with_column(\"image_thumbnail\", df[\"image\"].image.resize(32, 32))\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Creating Expressions: Column Reference (SQL)\nDESCRIPTION: This example demonstrates how to create an expression that refers to a column in a Daft DataFrame using `daft.sql_expr()`. The expression can then be used in various DataFrame operations. The input is a column name string. The output is a Daft Expression object.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndaft.sql_expr(\"A\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray and Daft Runner (Python)\nDESCRIPTION: Conditionally initializes a connection to a Ray cluster if `USE_RAY` is True. It specifies the cluster address and runtime environment dependencies (daft, pillow, s3fs). It then configures Daft to use Ray as its execution backend.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/image_querying/top_n_red_color.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n###\n# Settings for running on 10,000 rows in a distributed Ray cluster\n###\n\nif USE_RAY:\n    import ray\n\n    import daft.context\n\n    # NOTE: Replace with the address to an existing running Ray cluster, or None to start a local Ray cluster\n    RAY_CLUSTER_ADDRESS = \"ray://localhost:10001\"\n\n    ray.init(\n        address=RAY_CLUSTER_ADDRESS,\n        runtime_env={\"pip\": [\"daft\", \"pillow\", \"s3fs\"]},\n    )\n\n    daft.context.set_runner_ray(address=RAY_CLUSTER_ADDRESS)\n```\n\n----------------------------------------\n\nTITLE: Selecting and Structuring DataFrame Columns for Multimodal Storage in Python\nDESCRIPTION: This snippet refines the DataFrame for storage by selecting raw image URLs, inline-encoding generated thumbnails as JPEG, and including file size and metadata descriptions. This design pattern enables efficient storage and retrieval for both large (via URLs) and small data (inline), along with related metadata. Inputs: Previous processing columns. Outputs: Optimized DataFrame for Delta Lake storage.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/data-ai-summit-2024.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndf = df.select(\n    # Larger multimodal data (such as large images or documents) can be written as URLs\n    \"path\",\n    # Small multimodal data (such as thumbnails or full-form text) can be written inline\n    df[\"image_thumbnail\"].image.encode(\"JPEG\"),\n    # Metadata such as size in bytes and descriptions should be stored as per normal\n    \"size\",\n    \"description\",\n)\n```\n\n----------------------------------------\n\nTITLE: Excluding Columns from a Daft DataFrame using Python\nDESCRIPTION: This snippet shows how to remove a specific column ('DoB') from a Daft DataFrame using the `exclude()` method. It takes the name of the column to exclude as an argument and displays the resulting DataFrame without that column using `show()`. Depends on an existing Daft DataFrame instance named `df`.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/quickstart.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndf.exclude(\"DoB\").show()\n```\n\n----------------------------------------\n\nTITLE: Showing DataFrame with Decoded Images (Python)\nDESCRIPTION: Uses `.show(5)` to display the first 5 rows of the DataFrame after the image decoding step. The 'image' column now shows representations of the decoded image objects.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/image_querying/top_n_red_color.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndf.show(5)\n```\n\n----------------------------------------\n\nTITLE: String Concatenation - SQL\nDESCRIPTION: Demonstrates string concatenation in Daft using SQL. A new column 'B2' is created by concatenating the existing column 'B' with the string 'foo' using the `+` operator within a SQL query. The `daft.sql` function is used to execute the SQL query on the DataFrame.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.sql(\"SELECT *, B + 'foo' AS B2 FROM df\")\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Converting Array to Image using UDF\nDESCRIPTION: Converts the two-dimensional array representation of images to PIL Image objects using a User-Defined Function (UDF) with `.apply()`. This uses `PIL.Image.fromarray` for conversion. The `return_dtype` is set to `DataType.python()`.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/mnist.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom PIL import Image\n\nimages_df = images_df.with_column(\n    \"pil_image\",\n    col(\"image_2d\").apply(lambda arr: Image.fromarray(arr.astype(np.uint8)), return_dtype=DataType.python()),\n)\n```\n\n----------------------------------------\n\nTITLE: Reading Authenticated Datasets (Python)\nDESCRIPTION: This snippet demonstrates how to read authenticated datasets from Hugging Face using Daft.  It configures an `IOConfig` with an `HTTPConfig` that includes a bearer token for authentication. This allows access to private datasets provided that the correct token is set. It requires the `daft` library,  `daft.io` module, and an authentication token.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/huggingface.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom daft.io import IOConfig, HTTPConfig\n\nio_config = IoConfig(http=HTTPConfig(bearer_token=\"your_token\"))\ndf = daft.read_parquet(\"hf://datasets/username/dataset_name\", io_config=io_config)\n```\n\n----------------------------------------\n\nTITLE: Decoding Image Bytes (Python)\nDESCRIPTION: Transforms the 'image' column, which currently contains raw bytes, into decoded image objects using Daft's built-in `.image.decode()` expression. This operation updates the 'image' column in place (or creates a new DataFrame if assigned).\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/image_querying/top_n_red_color.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndf = df.with_column(\"image\", df[\"image\"].image.decode())\n```\n\n----------------------------------------\n\nTITLE: Expensive Operation with Sort and Materialization using SQL\nDESCRIPTION: This snippet is the SQL equivalent of the previous example. It demonstrates performing an expensive operation like sorting using SQL, followed by materialization using `collect()`. Subsequent operations such as `sum` and `mean` are performed on the materialized `df`. The result is a computation of the result without recalculating the sorted data. Finally, it selects all columns including a derived column by applying `(A + 1) AS try_this` and displays the output.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.sql(\"SELECT * FROM df ORDER BY A\")\ndf.collect()\n\n# All subsequent work on df avoids recomputing previous steps\ndaft.sql(\"SELECT sum(B) FROM df\").show()\ndaft.sql(\"SELECT mean(B) FROM df\").show()\ndaft.sql(\"SELECT *, (A + 1) AS try_this FROM df\").show(5)\n```\n\n----------------------------------------\n\nTITLE: Creating Torch Dataloader - Python\nDESCRIPTION: This code converts the Ray Dataset to a PyTorch dataloader.  The `iter_torch_batches` method is used, with a specified `batch_size` of 8. The dataloader will yield batches of data suitable for training the model.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/3-pytorch-ray-single-node-training.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ntorch_dataloader = ray_dataset.iter_torch_batches(batch_size=8)\n```\n\n----------------------------------------\n\nTITLE: Importing Daft and Configuring IO in Python\nDESCRIPTION: Imports the `daft` library, sets a `USE_GPU` flag based on the inverse of the `CI` variable (enabling GPU by default unless in CI). It configures anonymous S3 access for the 'us-west-2' region using `daft.io.IOConfig` and `daft.io.S3Config` and defines the S3 path string for the input Parquet dataset.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/text_to_image/text_to_image_generation.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport daft\n\n# Flip this flag if you want to see the performance of running on CPU vs GPU\nUSE_GPU = False if CI else True\nIO_CONFIG = daft.io.IOConfig(\n    s3=daft.io.S3Config(anonymous=True, region_name=\"us-west-2\")\n)  # Use anonymous-mode for accessing AWS S3\nPARQUET_PATH = \"s3://daft-public-data/tutorials/laion-parquet/train-00000-of-00001-6f24a7497df494ae.parquet\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Optimized Query Plan (Python)\nDESCRIPTION: Uses the `.explain(show_all=True)` method to display the optimized physical query plan. This demonstrates Daft's query optimizer, which may merge consecutive operations (like the two filters) into a single, more efficient step.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/image_querying/top_n_red_color.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndf.explain(show_all=True)\n```\n\n----------------------------------------\n\nTITLE: Defining Image Classification UDF - Daft/PyTorch - Python\nDESCRIPTION: Defines a Python class `ClassifyImage` decorated as a Daft stateful UDF (`@daft.udf`). The `__init__` method loads a pre-trained ResNet50 model from torchvision and sets up preprocessing transforms. The `__call__` method takes image data, converts it to a PyTorch tensor, applies preprocessing, runs inference, and returns the predicted category names. Requires `numpy`, `torch`, `torchvision`, and `daft`.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/1-local-image-batch-inference.ipynb#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy as np\nimport torch\nfrom torchvision.models import ResNet50_Weights, resnet50\n\nimport daft\n\n\n@daft.udf(return_dtype=daft.DataType.string())\nclass ClassifyImage:\n    def __init__(self):\n        weights = ResNet50_Weights.DEFAULT\n        self.model = resnet50(weights=weights)\n        self.model.eval()\n        self.preprocess = weights.transforms()\n        self.category_map = weights.meta[\"categories\"]\n\n    def __call__(self, images: daft.Series, shape: list[int, int, int]):\n        if len(images) == 0:\n            return []\n\n        # Convert the Daft Series into a list of Numpy arrays\n        data = images.cast(daft.DataType.tensor(daft.DataType.uint8(), tuple(shape))).to_pylist()\n\n        # Convert the numpy arrays into a torch tensor\n        images_array = torch.tensor(np.array(data)).permute((0, 3, 1, 2))\n\n        # Run the model, and map results back to a human-readable string\n        batch = self.preprocess(images_array)\n        prediction = self.model(batch).softmax(0)\n        class_ids = prediction.argmax(1)\n        prediction[:, class_ids]\n        return [self.category_map[class_id] for class_id in class_ids]\n```\n\n----------------------------------------\n\nTITLE: Loading Delta Lake Table from Unity Catalog with Daft - Python\nDESCRIPTION: Illustrates how to load a Delta Lake table that is registered in Unity Catalog using Daft's DataFrame interface. Dependencies include Daft (with Unity Catalog and deltalake support) and a valid UnityCatalog instance. Takes a fully-qualified table name as input, loads table metadata via the UnityCatalog, and reads the table into a Daft DataFrame. Finally, displays the table contents. Suitable for users needing to process Unity Catalog tables via Daft workflows.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/unity_catalog.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nunity_table = unity.load_table(\"my_catalog_name.my_schema_name.my_table_name\")\n\ndf = daft.read_deltalake(unity_table)\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Filtering a Daft DataFrame with Iceberg Partition Pruning\nDESCRIPTION: This snippet demonstrates how to filter a Daft DataFrame that was created from an Iceberg table.  It uses the `df.where()` method to apply a filter based on a partition key, leveraging Iceberg's partition pruning capabilities for efficient data access. The `df.show()` function displays the resulting filtered DataFrame.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/iceberg.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Filter which takes advantage of partition pruning capabilities of Iceberg\ndf = df.where(df[\"partition_key\"] < 1000)\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Materializing DataFrame with Red Masks (Python)\nDESCRIPTION: Executes the query plan, including the application of the `magic_red_detector` UDF, using `.collect()`. This computes the red mask for each image and materializes the DataFrame, including the new 'red_mask' column, in memory.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/image_querying/top_n_red_color.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndf.collect()\n```\n\n----------------------------------------\n\nTITLE: Configuring CI Run Behavior in Python\nDESCRIPTION: This snippet sets a CI (Continuous Integration) control flag, and conditionally exits the process if running in CI environment. This prevents notebook execution in CI, especially since some steps (e.g., AWS presigned URLs) require credentials. Dependencies: Python's sys module.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/data-ai-summit-2024.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nCI = False\n```\n\nLANGUAGE: python\nCODE:\n```\n# Skip this notebook execution in CI because it requires AWS credentials for presigned URL generation\nif CI:\n    import sys\n\n    sys.exit()\n```\n\n----------------------------------------\n\nTITLE: Defining UDF for Summing Mask Pixels (Python)\nDESCRIPTION: Defines a Python function `sum_mask` that takes a PIL Image object (expected to be a binary mask), converts it to a NumPy array, calculates the sum of its pixel values (effectively counting the white/True pixels), and returns the result as an integer. This function is designed as a UDF.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/image_querying/top_n_red_color.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\n\ndef sum_mask(mask: PIL.Image.Image) -> int:\n    val = np.asarray(mask).sum()\n    return int(val)\n```\n\n----------------------------------------\n\nTITLE: Reading Iceberg Table with Daft\nDESCRIPTION: Reads data from an Iceberg table using Daft.  This snippet demonstrates lazy materialization using `daft.read_iceberg()`.  It is functionally equivalent to  `table.to_daft()`.  This approach defers data loading and computation until the data is needed.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/iceberg_summit_2024.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndaft.read_iceberg(table)  # equivalent to: `table.to_daft()`\n```\n\n----------------------------------------\n\nTITLE: Setting Ray Usage Flag (Python)\nDESCRIPTION: Initializes a boolean variable `USE_RAY` to control whether the code should attempt to use a Ray cluster for distributed computation. Setting this to `True` enables Ray integration.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/image_querying/top_n_red_color.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nUSE_RAY = False\n```\n\n----------------------------------------\n\nTITLE: Analyzing Model Performance with GroupBy\nDESCRIPTION: Performs an analysis of the model's performance by grouping the data by the true labels and calculating the number of correct and incorrect classifications per label.  The results are then sorted by label for easier analysis.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/mnist.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nanalysis_df = (\n    classified_images_df.with_column(\"correct\", (col(\"model_classification\") == col(\"label\")).cast(DataType.int64()))\n    .with_column(\"wrong\", (col(\"model_classification\") != col(\"label\")).cast(DataType.int64()))\n    .groupby(col(\"label\"))\n    .agg(\n        col(\"label\").count().alias(\"num_rows\"),\n        col(\"correct\").sum(),\n        col(\"wrong\").sum(),\n    )\n    .sort(col(\"label\"))\n)\n\nanalysis_df.show()\n```\n\n----------------------------------------\n\nTITLE: Joining Dog DataFrame with Owners DataFrame and Selecting Columns\nDESCRIPTION: Performs a join of 'df' with 'df_dogs' on 'full_name' to combine owner and dog data, then excludes unrelated columns and reorders relevant fields for readability.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/10min.ipynb#_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\ndf_family = df.join(df_dogs, on=\"full_name\").exclude(\"first_name\", \"last_name\", \"DoB\", \"country\", \"age\")\ndf_family.show()\n```\n\nLANGUAGE: Python\nCODE:\n```\ndf_family = df_family.select(\"full_name\", \"has_dog\", \"dog_name\", \"urls\")\ndf_family.show()\n```\n\n----------------------------------------\n\nTITLE: Exiting Execution in CI Environment\nDESCRIPTION: This code block checks if the `CI` flag is True. If it is, it imports the `sys` module and calls `sys.exit()`, which terminates the script's execution. This likely is to prevent potentially slow or resource-intensive operations from running during CI.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/pydata_global_2023.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Skip this notebook execution in CI because it hits non-public buckets\nif CI:\n    import sys\n\n    sys.exit()\n```\n\n----------------------------------------\n\nTITLE: Creating a SQLite Catalog\nDESCRIPTION: Creates a local SQLite catalog for managing Iceberg tables. It uses `SqlCatalog` from pyiceberg and specifies the database URI and the warehouse directory. This catalog allows the user to create and manage Iceberg tables locally without the need for an external service like AWS Glue.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/iceberg_summit_2024.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pyiceberg.catalog.sql import SqlCatalog\n\nwarehouse_path = \"/tmp/warehouse\"\ncatalog = SqlCatalog(\n    \"default\",\n    **{\n        \"uri\": f\"sqlite:///{warehouse_path}/pyiceberg_catalog.db\",\n        \"warehouse\": f\"file://{warehouse_path}\",\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Creating and Interacting with a Daft Catalog from an Iceberg Catalog\nDESCRIPTION: This snippet demonstrates how to instantiate a Daft Catalog from an existing Iceberg catalog object, verify its creation, read a table schema, and perform data write and retrieval operations using Daft's APIs. Dependencies include Daft and PyIceberg, and it provides essential code for bridging external catalog systems with Daft.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/catalogs.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport daft\n\nfrom daft import Catalog\n\n# iceberg_catalog from the  'Sessions' tutorial\niceberg_catalog = load_catalog(...)\n\n# create a daft catalog from the pyiceberg catalog instance\ncatalog = Catalog.from_iceberg(iceberg_catalog)\n\n# verify\ncatalog\n\"\"\"\nCatalog('default')\n\"\"\"\n\n# we can read as a dataframe\ncatalog.read_table(\"example.tbl\").schema()\n\"\"\"\n╭─────────────┬─────────╮\n│ column_name ┆ type    │\n╞═════════════╪═════════╡\n│ x           ┆ Boolean │\n├╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┤\n│ y           ┆ Int64   │\n├╌╌╌╌╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌╌╌┤\n│ z           ┆ Utf8    │\n╰─────────────┴─────────╯\n\"\"\"\n\n# give a dataframe...\ndf = daft.from_pylist([{ \"x\": False, \"y\": -1, \"z\": \"xyz\" }])\n\n# we can write to tables\ncatalog.write_table(\"example.tbl\", df, mode=\"append\")\n\n# we can also get table instances\nt = catalog.get_table(\"example.tbl\")\n\n# see 'Working with Tables' for what we can do!\nt\n\"\"\"\nTable('tbl')\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Creating PySpark Session (Local/Remote) - Python\nDESCRIPTION: This snippet demonstrates how to create a local and a remote PySpark session using the `daft.pyspark` module.  It showcases the builder pattern to construct the session, using `local()` for a local session and `remote(\"ray://<HEAD_IP>:6379\")` for a session connected to a Ray cluster.  The created session can then be used like a native Spark session, allowing interaction with a Daft backend, including using `createDataFrame` and `select` methods and finally stopping the session.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/spark_connect.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom daft.pyspark import SparkSession\nfrom pyspark.sql.functions import col\n\n# Create a local spark session\nspark = SparkSession.builder.local().getOrCreate()\n\n# Alternatively, connect to a Ray cluster\n# You can use `ray get-head-ip <cluster_config.yaml>` to get the head ip!\nspark = SparkSession.builder.remote(\"ray://<HEAD_IP>:6379\").getOrCreate()\n\n# Use spark as you would with the native spark library, but with a daft backend!\nspark.createDataFrame([{\"hello\": \"world\"}]).select(col(\"hello\")).show()\n\n# Stop the Spark session\nspark.stop()\n```\n\n----------------------------------------\n\nTITLE: Managing Data Tables with Daft's Table API in Python\nDESCRIPTION: This snippet demonstrates how to convert external table objects, such as PyIceberg tables, into Daft Table instances. It shows how to perform data operations like reading into a dataframe, creating temporary tables from dataframes, and reading from temporary tables. The focus is on flexibility and interoperability between different data formats and Daft's table interface.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/catalogs.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom daft import Table\nfrom pyiceberg.table import StaticTable\n\n# suppose you have a pyiceberg table\npyiceberg_table = StaticTable(\"metadata.json\")\n\n# convert to a Daft table for API access\ntable = Table.from_iceberg(pyiceberg_table)\n\n# read a dataframe from the table\ndf = table.read()\n\n# create a temporary table from a dataframe\ndaft.create_temp_table(\"my_temp_table\", df.from_pydict({ ... }))\n\n# read from the temporary table\ndf = daft.read_table(\"my_temp_table\")\n```\n\n----------------------------------------\n\nTITLE: Creating Expressions: Literal Value (Python)\nDESCRIPTION: This code shows how to create an expression representing a literal value using `daft.lit()`.  This is useful for hardcoding single values as expressions. The input is a value (e.g., an integer), and the output is a Daft Expression object representing that literal.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nfrom daft import lit\n\n# Refers to an expression which always evaluates to 42\nlit(42)\n```\n\n----------------------------------------\n\nTITLE: Importing daft Library\nDESCRIPTION: This snippet imports the `daft` library, which provides the core functionality for creating and manipulating DataFrames. Daft is a library that is optimized for high-performance data processing.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/pydata_global_2023.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport daft\n```\n\n----------------------------------------\n\nTITLE: Defining Tensor Types in SQL for Daft\nDESCRIPTION: SQL syntax examples for defining Daft Tensor types. The `TENSOR(T, [shape...])` syntax specifies the element data type `T` and an optional list of integers representing the tensor's shape.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql/datatypes.md#_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nTENSOR(INT)\nTENSOR(INT, 10, 10, 10)\n```\n\n----------------------------------------\n\nTITLE: Applying Annotation and LLM Results to Daft DataFrames in Python\nDESCRIPTION: This set of operations expands the DataFrame by generating presigned S3 URLs for image access, making remote OpenAI API calls for image annotation (captioning), parsing the JSON API responses for textual descriptions, and displaying a preview. Dependencies: UDFs defined previously. Inputs: Image paths and presigned URLs. Outputs: Columns 'image_urls', 'gpt_results', and 'description' containing URLs, API outputs, and parsed captions. Prerequisite: OpenAI API key.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/data-ai-summit-2024.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Generate temporary URLs with a short expiration time\ndf = df.with_column(\"image_urls\", generate_presigned_url(df[\"path\"]))\n\n# Make remote API calls to OpenAI endpoint\ndf = df.with_column(\"gpt_results\", run_gpt4o_on_urls(df[\"image_urls\"], prompt=\"What’s in this image?\"))\n\n# Parse JSON outputs from OpenAI endpoint\ndf = df.with_column(\"description\", df[\"gpt_results\"].json.query(\".choices[0].message.content\"))\n\ndf.show(3)\n```\n\n----------------------------------------\n\nTITLE: Installing Daft and Dependencies (Python)\nDESCRIPTION: Installs the Daft library with optional AWS and Ray support, along with the Pillow and NumPy libraries, which are commonly used for image processing and numerical operations in Python.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/image_querying/top_n_red_color.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install daft[aws,ray]\n!pip install Pillow numpy\n```\n\n----------------------------------------\n\nTITLE: Downloading File Content from URLs (Python)\nDESCRIPTION: Adds a new column named 'image' to the DataFrame `df`. This column is populated by downloading the content from the URLs specified in the existing 'path' column using the `.url.download()` expression. The previously configured `IO_CONFIG` (for anonymous S3 access) is used.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/image_querying/top_n_red_color.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndf = df.with_column(\"image\", df[\"path\"].url.download(io_config=IO_CONFIG))\n```\n\n----------------------------------------\n\nTITLE: Transforming Complex Data Types (URLs to Images) in Daft Dataframe Using Python\nDESCRIPTION: The snippet demonstrates Daft's capability to represent and manipulate complex types by transforming Utf8 URLs into binary data via URL download, then decoding that binary data into image objects within the dataframe using Rust-accelerated operations exposed through Python APIs.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/linkedin-03-05-2024.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nlaion_df = laion_df.with_column(\"data\", laion_df[\"path\"].url.download())  # Utf8 -> Binary\nlaion_df = laion_df.with_column(\"image\", laion_df[\"data\"].image.decode())  # Binary -> Image\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame from Glob Path\nDESCRIPTION: This snippet demonstrates creating a Daft DataFrame by globbing a file path. It reads a DataFrame from file paths specified by a glob pattern, such as reading images from a directory. The example uses `daft.from_glob_path()` to create the DataFrame.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_52\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.from_glob_path(\"s3://mybucket/path/to/images/*.jpeg\")\n```\n\n----------------------------------------\n\nTITLE: Resizing Images in Daft DataFrame (Python)\nDESCRIPTION: This snippet adds another new column named \"resized_image\" by applying an image processing operation to the \"image\" column. It uses `image.resize(32, 32)` to resize each image in that column to a resolution of 32x32 pixels.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/flyte/notebook.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfiltered_df = filtered_df.with_column(\n    \"resized_image\",\n    filtered_df[\"image\"].image.resize(32, 32),\n)\n```\n\n----------------------------------------\n\nTITLE: Extracting Temporal Components using Daft SQL API in Python\nDESCRIPTION: This snippet demonstrates extracting various parts of a timestamp using Daft’s SQL interface in Python. It constructs a DataFrame and applies built-in SQL date/time functions to extract year, month, day, hour, minute, and second, returning them as new columns. The pattern requires Daft to accept SQL queries over DataFrames, and `datetime` for initialization. Inputs must be timestamp fields; output columns are named for each extracted component.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_47\n\nLANGUAGE: Python\nCODE:\n```\ndf = daft.from_pydict({\n    \"timestamp\": [\n        datetime.datetime(2021, 1, 1, 0, 1, 1),\n        datetime.datetime(2021, 1, 1, 0, 1, 59),\n        datetime.datetime(2021, 1, 1, 0, 2, 0),\n    ]\n})\n\n# Extract year, month, day, hour, minute, and second from the timestamp\ndf = daft.sql(\"\"\"\n    SELECT\n        timestamp,\n        year(timestamp) as year,\n        month(timestamp) as month,\n        day(timestamp) as day,\n        hour(timestamp) as hour,\n        minute(timestamp) as minute,\n        second(timestamp) as second\n    FROM df\n\"\"\")\n\ndf.show()\n\n```\n\n----------------------------------------\n\nTITLE: Counting Non-Null Values in a Column Using COUNT in SQL SELECT\nDESCRIPTION: Shows the use of the aggregate function COUNT to count the non-null occurrences of column `a` in table `T`. It returns a single scalar value representing that count.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql/statements/select.md#_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nSELECT COUNT(a) FROM T;\n```\n\n----------------------------------------\n\nTITLE: Reading from Hudi Table into DataFrame with Daft - Python\nDESCRIPTION: Shows accessing data from a Hudi table for ingestion into a Daft DataFrame. Prerequisites are daft and Hudi integration; input is table path or configuration. Returns a DataFrame with Hudi table rows; Hudi version compatibility and table format should be checked.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/api/dataframe_creation.md#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nimport daft\n\ndf = daft.read_hudi(\"/data/hudi_table\")\n```\n\n----------------------------------------\n\nTITLE: Setting number of rows for data processing\nDESCRIPTION: Defines a variable to control the number of data rows to process from the dataset, allowing for easy scaling and testing of the workload.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/2-distributed-batch-inference.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n# Feel free to tweak this variable to have the tutorial run on as many rows as you'd like!\nNUM_ROWS = 1000\n```\n\n----------------------------------------\n\nTITLE: Numbering Rows with monotonically_increasing_id in Daft (Python)\nDESCRIPTION: This code snippet demonstrates how to add unique, monotonically increasing IDs to rows in a Daft DataFrame. It uses the `monotonically_increasing_id()` function to generate unique IDs based on the partition number and row number within each partition.  It initializes the RayRunner for distributed execution and repartitions the DataFrame before adding the IDs.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport daft\nfrom daft.functions import monotonically_increasing_id\n\n# Initialize the RayRunner to run distributed\ndaft.context.set_runner_ray()\n\n# Create a DataFrame and repartition it into 2 partitions\ndf = daft.from_pydict({\"A\": [1, 2, 3, 4]}).into_partitions(2)\n\n# Add unique IDs\ndf = df.with_column(\"id\", monotonically_increasing_id())\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Applying Nested Functions for URL Download and Image Decode - SQL API in Python\nDESCRIPTION: Demonstrates using nested SQL functions in daft.sql to download and decode images from URLs. The SQL string wraps function calls to perform sequential operations on each row: downloading the URL and decoding the resulting data as an image. Prerequisites include the daft library and support for 'url_download' and 'image_decode' functions. Input is a DataFrame of URLs; output is a DataFrame column of image objects, visualized with .show(). Function chaining must be correctly nested within the SQL string.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql_overview.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ndf = daft.from_pydict({\"urls\": [\n    \"https://user-images.githubusercontent.com/17691182/190476440-28f29e87-8e3b-41c4-9c28-e112e595f558.png\",\n    \"https://user-images.githubusercontent.com/17691182/190476440-28f29e87-8e3b-41c4-9c28-e112e595f558.png\",\n    \"https://user-images.githubusercontent.com/17691182/190476440-28f29e87-8e3b-41c4-9c28-e112e595f558.png\",\n]})\ndf = daft.sql(\"SELECT image_decode(url_download(urls)) FROM df\")\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Creating Expressions: SQL Expression (Python)\nDESCRIPTION: This example demonstrates how to create an SQL expression using `daft.sql_expr()`. It parses valid SQL and converts it into a Daft expression. The input is an SQL string and the output is a Daft expression object.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndaft.sql_expr(\"A + 1\")\n```\n\n----------------------------------------\n\nTITLE: JSON Query - Python\nDESCRIPTION: Extracts a value from JSON strings using JQ-style filters. It initializes a Daft DataFrame with a column named 'json' containing JSON strings, then uses the `.json.query()` method to extract the value associated with the key 'a' from each JSON string. The results are stored in a new column named 'a'.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.from_pydict({\n    \"json\": [\n        '{\"a\": 1, \"b\": 2}',\n        '{\"a\": 3, \"b\": 4}',\n    ],\n})\ndf = df.with_column(\"a\", df[\"json\"].json.query(\".a\"))\ndf.collect()\n```\n\n----------------------------------------\n\nTITLE: Running Daft Benchmarks - Comparing Against Other Frameworks (bash)\nDESCRIPTION: This bash command runs pytest benchmarks to compare Daft against other frameworks.  It uses the `-m benchmark` flag to run tests marked as benchmarks, `--benchmark-group-by=param:path` groups results by path, and tests the `parquet` benchmarks specifically. This command requires `pytest` and the benchmarking packages to be installed, as well as the necessary data and test files. The output will be the benchmark results, comparing Daft to other frameworks based on the specified parameters.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/benchmarking/parquet/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npytest benchmarking/parquet/ -m benchmark --benchmark-group-by=param:path\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Runtime Environment in Python\nDESCRIPTION: Defines whether to run locally or on a remote Ray cluster through a flag, and sets Ray address accordingly. Facilitates switching between local and distributed execution modes for scalable data processing.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/3-pytorch-ray-single-node-training.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nUSE_REMOTE_CLUSTER = False\nRAY_ADDRESS = \"ray://localhost:10001\" if USE_REMOTE_CLUSTER else None\n```\n\n----------------------------------------\n\nTITLE: Installing Daft and Dependencies in Python Environment\nDESCRIPTION: Installs the pre-release version of the `daft` library from a specific Anaconda channel using pip. It also installs other essential libraries (`transformers`, `diffusers`, `accelerate`, `torch`, `Pillow`) required for running the Stable Diffusion model and handling images.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/text_to_image/text_to_image_generation.ipynb#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n!pip install daft --pre --extra-index-url https://pypi.anaconda.org/daft-nightly/simple\n!pip install transformers diffusers accelerate torch Pillow\n```\n\n----------------------------------------\n\nTITLE: Composing Numeric Expressions (SQL)\nDESCRIPTION: This example shows how to compose numeric expressions in Daft using SQL syntax. It adds 1 to column 'A', divides column 'A' by 2, and checks if each element in column 'A' is greater than 1. The input is a Daft DataFrame with a numeric column, and the output is a DataFrame with new columns representing the results of the operations.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.sql(\"\"\"\n    SELECT\n        *,\n        A + 1 AS A_add_one,\n        A / 2.0 AS A_divide_two,\n        A > 1 AS A_gt_1\n    FROM df\n\"\"\")\ndf.collect()\n```\n\n----------------------------------------\n\nTITLE: Materializing and Displaying DataFrame Content\nDESCRIPTION: Demonstrates how to materialize a DataFrame with collect() and display rows with show(), useful for examining data during interactive analysis.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/10min.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf.collect()\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.show(3)\n```\n\n----------------------------------------\n\nTITLE: Disabling Daft Telemetry via Environment Variables (Shell)\nDESCRIPTION: Provides the environment variable settings required to opt-out of Daft's telemetry collection. Setting `DAFT_ANALYTICS_ENABLED=0` disables Daft's internal analytics, while setting either `SCARF_NO_ANALYTICS=true` or `DO_NOT_TRACK=true` disables collection via the Scarf gateway. Both components should be disabled for complete opt-out.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/resources/telemetry.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nDAFT_ANALYTICS_ENABLED=0\n```\n\nLANGUAGE: Shell\nCODE:\n```\nSCARF_NO_ANALYTICS=true\n```\n\nLANGUAGE: Shell\nCODE:\n```\nDO_NOT_TRACK=true\n```\n\n----------------------------------------\n\nTITLE: Training ResNet18 - Python\nDESCRIPTION: This snippet trains a ResNet18 model using the PyTorch framework.  It initializes the model, loss function (CrossEntropyLoss), and optimizer (SGD).  It then iterates through the dataloader, performs forward and backward passes, calculates the loss, computes gradients, and updates the model parameters using the optimizer. Training is performed for a single epoch across the whole dataset.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/3-pytorch-ray-single-node-training.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torchvision.models as models\nfrom torch import nn\n\nmodel = models.__dict__[\"resnet18\"](weights=models.ResNet18_Weights.DEFAULT)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(\n    model.parameters(),\n    0.1,\n    momentum=0.9,\n    weight_decay=1e-4,\n)\n\n# Train for 1 epoch across all the data in the dataloader\nfor i, data in enumerate(torch_dataloader):\n    images, labels = data[\"arr\"].permute(0, 3, 1, 2).float(), data[\"class_id\"]\n\n    # compute output\n    output = model(images)\n    loss = criterion(output, labels)\n\n    # compute gradient and do SGD step\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n```\n\n----------------------------------------\n\nTITLE: Conditionally Exiting - Python\nDESCRIPTION: Checks the value of the `CI` variable. If `CI` is `True`, it imports the `sys` module and exits the script immediately. This prevents execution in automated testing environments that may not have necessary dependencies or access.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/1-local-image-batch-inference.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# Skip this notebook execution in CI because it hits non-public buckets\nif CI:\n    import sys\n\n    sys.exit()\n```\n\n----------------------------------------\n\nTITLE: Showing Daft DataFrame with Resized Images (Python)\nDESCRIPTION: This line displays a preview of the first 5 rows, now including the \"resized_image\" column. This allows confirming that the image resizing operation was correctly applied.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/flyte/notebook.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfiltered_df.show(5)\n```\n\n----------------------------------------\n\nTITLE: Downloading Data from URLs\nDESCRIPTION: This snippet shows how to download data from URLs using Daft's `download()` method. It uses `from_glob_path` to create a DataFrame from paths that are treated as URLs, then uses `with_column` and `.url.download()` to download the data and stores it in a new column. The `IO_CONFIG` object configures S3 settings, in this case, maximum connections.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/pydata_global_2023.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.from_glob_path(\"s3://daft-public-data/open-images/validation-images/**.jpg\")\n\nIO_CONFIG = daft.io.IOConfig(s3=daft.io.S3Config(max_connections=64))\ndf = df.with_column(\"data\", df[\"path\"].url.download(io_config=IO_CONFIG))\n```\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\ndf.collect()\n```\n\n----------------------------------------\n\nTITLE: Comparison Expressions - SQL\nDESCRIPTION: Compares two columns 'A' and 'B' for equality using SQL and stores the boolean result in column 'A_eq_B'. Daft DataFrame is initialized and then the SQL query is executed with a comparison `A = B` which generates the boolean column.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.from_pydict({\"A\": [1, 2, 3], \"B\": [1, 2, 4]})\n\ndf = daft.sql(\"\"\"\n    SELECT\n        A,\n        B,\n        A = B AS A_eq_B\n    FROM df\n\"\"\")\n\ndf.collect()\n```\n\n----------------------------------------\n\nTITLE: URL Download - Python\nDESCRIPTION: Downloads data from URLs in a column. It initializes a Daft DataFrame with a column named 'urls' containing URLs, then uses the `.url.download()` method to download the data from each URL. The resulting data is stored in a new column named 'data'.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.from_pydict({\n    \"urls\": [\n        \"https://www.google.com\",\n        \"s3://daft-public-data/open-images/validation-images/0001eeaf4aed83f9.jpg\",\n    ],\n})\ndf = df.with_column(\"data\", df[\"urls\"].url.download())\ndf.collect()\n```\n\n----------------------------------------\n\nTITLE: Applying the class-based UDF\nDESCRIPTION: This snippet shows how to invoke the class UDF that was previously declared. This class is called and applied to an existing Daft dataframe's `images` column and stored into `image_classifications`.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_66\n\nLANGUAGE: Python\nCODE:\n```\ndf = df.with_column(\"image_classifications\", RunModel(df[\"images\"]))\n```\n\n----------------------------------------\n\nTITLE: Filtering Incorrect Classifications\nDESCRIPTION: Filters the DataFrame to show only rows where the model's classification does not match the true label. This helps to identify images that were misclassified by the model.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/mnist.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclassified_images_df.where(col(\"label\") != col(\"model_classification\")).show(10)\n```\n\n----------------------------------------\n\nTITLE: Apply Expression to All Columns Using Wildcard - Daft DataFrame (Python)\nDESCRIPTION: This snippet demonstrates using the `col(\"*\")` wildcard expression to select and operate on all columns in a DataFrame simultaneously. It initializes a DataFrame and then uses `select(col(\"*\") * 3)` to multiply every value in every column by 3. The resulting DataFrame with scaled values is displayed. This requires importing `daft` and `col` from `daft`.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.from_pydict({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\ndf.select(col(\"*\") * 3).show()\n```\n\n----------------------------------------\n\nTITLE: Reading Data into Daft DataFrame from Files in Python\nDESCRIPTION: This snippet demonstrates several ways to read tabular data into Daft DataFrames from local files or remote storage. Supported formats include CSV, Parquet, and line-delimited JSON. Requires the `daft` library. Users specify file paths (may use wildcards or S3 URLs), and receive a DataFrame corresponding to the loaded file(s). Limitations include support for only listed formats and need for proper permissions for remote filesystems.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_51\n\nLANGUAGE: Python\nCODE:\n```\nimport daft\n\n# You can read a single CSV file from your local filesystem\ndf = daft.read_csv(\"path/to/file.csv\")\n\n# You can also read folders of CSV files, or include wildcards to select for patterns of file paths\ndf = daft.read_csv(\"path/to/*.csv\")\n\n# Other formats such as parquet and line-delimited JSON are also supported\ndf = daft.read_parquet(\"path/to/*.parquet\")\ndf = daft.read_json(\"path/to/*.json\")\n\n# Remote filesystems such as AWS S3 are also supported, and can be specified with their protocols\ndf = daft.read_csv(\"s3://mybucket/path/to/*.csv\")\n\n```\n\n----------------------------------------\n\nTITLE: DataFrame and UDF Setup\nDESCRIPTION: This code snippet initializes a Daft DataFrame to be used in subsequent examples. It includes a column containing images represented as 2D numpy arrays and another column containing crop boxes, represented as lists of integers. This sets the stage for demonstrating UDFs on image data.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_62\n\nLANGUAGE: python\nCODE:\n```\nimport daft\nimport numpy as np\n\ndf = daft.from_pydict({\n    # the `image` column contains images represented as 2D numpy arrays\n    \"image\": [np.ones((128, 128)) for i in range(16)],\n    # the `crop` column contains a box to crop from our image, represented as a list of integers: [x1, x2, y1, y2]\n    \"crop\": [[0, 1, 0, 1] for i in range(16)],\n})\n```\n\n----------------------------------------\n\nTITLE: Model Prediction and Display - Python\nDESCRIPTION: This code performs model predictions on the images in the DataFrame.  It applies the trained ResNet18 model to the 'arr' column, converting each image to a PyTorch tensor, permuting the dimensions, and performing a forward pass. It then displays the first two rows of the DataFrame with the 'model_predictions' column added.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/3-pytorch-ray-single-node-training.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndf.with_column(\n    \"model_predictions\",\n    df[\"arr\"].apply(\n        lambda arr: model(torch.tensor(arr).permute(2, 0, 1).unsqueeze(0).float()), return_dtype=daft.DataType.python()\n    ),\n).show(2)\n```\n\n----------------------------------------\n\nTITLE: Initializing CI Flag\nDESCRIPTION: This snippet initializes a boolean variable `CI` to `False`. This variable likely controls conditional execution of code based on whether the code is running in a Continuous Integration (CI) environment.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/pydata_global_2023.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nCI = False\n```\n\n----------------------------------------\n\nTITLE: Executing Daft Plan and Showing Data (Python)\nDESCRIPTION: These lines trigger the execution of the Daft DataFrame's computation plan up to this point. `collect()` materializes the results in memory (for small datasets), and `show(5)` displays a preview of the first 5 rows of the DataFrame, including the selected columns.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/flyte/notebook.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nparquet_df.collect()\nparquet_df.show(5)\n```\n\n----------------------------------------\n\nTITLE: Counting Rows Grouped by a Column Using GROUP BY in SQL\nDESCRIPTION: Illustrates counting the number of rows grouped by column `b`. The query aggregates data with COUNT(*) for each distinct value in column `b`, returning row counts per group. This requires SQL GROUP BY semantics.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql/statements/select.md#_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nSELECT COUNT(*), b FROM T GROUP BY b;\n```\n\n----------------------------------------\n\nTITLE: Using Expressions in Select Operations\nDESCRIPTION: Shows how to use inline expressions directly within select() method calls for column transformations.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/10min.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndf.select((daft.col(\"first_name\") + \" \" + daft.col(\"last_name\")), \"age\", \"country\").show()\n```\n\n----------------------------------------\n\nTITLE: Downloading and Decoding Images from URLs in DataFrame\nDESCRIPTION: Uses Daft's url.download() to fetch image bytes from URLs stored in the DataFrame, handling errors gracefully. Then decodes image bytes into visual formats using image.decode for further processing.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/10min.ipynb#_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\ndf_family = df_family.with_column(\"image_bytes\", df_dogs[\"urls\"].url.download(on_error=\"null\"))\ndf_family.show()\n```\n\nLANGUAGE: Python\nCODE:\n```\ndf_family = df_family.with_column(\"image\", daft.col(\"image_bytes\").image.decode())\ndf_family.show()\n```\n\n----------------------------------------\n\nTITLE: Reading Hudi Dataset into Daft Dataframe with Anonymous S3 Access in Python\nDESCRIPTION: This snippet reads a Hudi dataset from a public S3 path into a Daft dataframe using anonymous S3 access configured previously. It allows querying datasets in Hudi format stored remotely without credentials, and shows the dataframe contents.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/linkedin-03-05-2024.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nhudi_df = daft.read_hudi(\"s3://daft-public-data/hudi/v6_simplekeygen_nonhivestyle/\", io_config=ANONYMOUS_IO_CONFIG)\nhudi_df.show()\n```\n\n----------------------------------------\n\nTITLE: Implementing and Registering a Custom AWS GlueTable with Daft in Python\nDESCRIPTION: This Python example illustrates how to extend Daft's Glue integration by subclassing the GlueTable abstract class, implementing 'from_table_info' to match custom logic using table metadata (e.g., a 'pytest' parameter), and appending the new class to the GlueCatalog's '_table_impls' for registration. Required dependencies are Daft's Glue integration and Python typing, and you must supply a valid implementation for 'read' and 'write' to fully support the table. The example shows stubbed 'read' and 'write' methods that raise NotImplementedError, and registration is done by modifying the GlueCatalog object's internal implementation list. Constraints: This API is unstable and mainly suitable for patching or experimentation with custom Glue tables.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/glue.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom daft.catalog.__glue import GlueCatalog, GlueTable, load_glue\n\nclass GlueTestTable(GlueTable):\n    \"\"\"GlueTestTable shows how we register custom table implementations.\"\"\"\n\n    @classmethod\n    def from_table_info(cls, catalog: GlueCatalog, table: dict[str,Any]) -> GlueTable:\n        if bool(table[\"Parameters\"].get(\"pytest\")):\n            return cls(catalog, table)\n        raise ValueError(\"Expected Parameter pytest='True'\")\n\n\n    def read(self, **options) -> DataFrame:\n        raise NotImplementedError\n\n    def write(self, df: DataFrame, mode: Literal['append'] | Literal['overwrite'] = \"append\", **options) -> None:\n        raise NotImplementedError\n\ngc = load_glue(\"my_glue_catalog\", region=\"us-west-2\")\ngc._table_impls.append(GlueTestTable) # !! REGISTER GLUE TEST TABLE !!\n```\n\n----------------------------------------\n\nTITLE: Creating and Filtering a DataFrame in Daft\nDESCRIPTION: Example showing how to create a Daft DataFrame by reading from CSV files in S3 and applying a filter operation. This demonstrates Daft's lazy execution model where operations are enqueued for later execution.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/resources/architecture.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.read_csv(\"s3://foo/*.csv\")\ndf = df.where(df[\"baz\"] > 0)\n```\n\n----------------------------------------\n\nTITLE: Selecting Specific Columns\nDESCRIPTION: Shows how to extract specific columns from a DataFrame using the select() method.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/10min.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf.select(\"first_name\", \"has_dog\").show()\n```\n\n----------------------------------------\n\nTITLE: Defining a PyTorch Model for MNIST\nDESCRIPTION: Defines a PyTorch model for classifying MNIST images. This model consists of convolutional layers, dropout layers, and fully connected layers. It is the same model that was used to train the weights loaded later.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/mnist.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n###\n# Model was trained using a script provided in PyTorch Examples: https://github.com/pytorch/examples/blob/main/mnist/main.py\n###\n\nimport torch\nimport torch.hub\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n```\n\n----------------------------------------\n\nTITLE: Setting Daft Runner to Ray\nDESCRIPTION: Configures Daft to use Ray as its runner. This uses `daft.context.set_runner_ray()` to specify that the Ray cluster running on the address will be used by Daft to execute the data processing operations.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/iceberg_summit_2024.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport daft\n\ndaft.context.set_runner_ray(address=\"ray://localhost:10001\")\n```\n\n----------------------------------------\n\nTITLE: Reading and Writing DataFrames with Daft Sessions and Tables in Python\nDESCRIPTION: Illustrates reading and writing data using Daft Sessions. It shows reading a table into a DataFrame (`sess.read_table().show()`), creating a single-row DataFrame (`daft.from_pylist`), appending data to a table via the session (`sess.write_table(..., mode=\"append\")`), setting the session's current namespace (`sess.set_namespace`), getting a table object (`sess.get_table`), and reading/writing data directly via the table object (`tbl.show()`, `tbl.append()`). Assumes a session `sess` and a table `example.tbl` exist.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sessions.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# we can read our table back as a DataFrame instance\nsess.read_table(\"example.tbl\").show()\n\"\"\"\n╭─────────┬───────┬──────╮\n│ x       ┆ y     ┆ z    │\n│ ---     ┆ ---   ┆ ---  │\n│ Boolean ┆ Int64 ┆ Utf8 │\n╞═════════╪═══════╪══════╡\n│ true    ┆ 1     ┆ abc  │\n├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌┤\n│ true    ┆ 2     ┆ def  │\n├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌┤\n│ false   ┆ 3     ┆ ghi  │\n╰─────────┴───────┴──────╯\n\"\"\"\n\n# create a single row to append\nrow = daft.from_pylist([{ \"x\": True, \"y\": 4, \"z\": \"jkl\" }])\n\n# we can write a DataFrame to the Table via the Session\nsess.write_table(\"example.tbl\", row, mode=\"append\")\n\n# we can use session state and table objects!\nsess.set_namespace(\"example\")\n\n# name resolution is trivial\ntbl = sess.get_table(\"tbl\")\n\n# to read, we have .read() .select(*cols) or .show()\ntbl.show()\n\"\"\"\n╭─────────┬───────┬──────╮\n│ x       ┆ y     ┆ z    │\n│ ---     ┆ ---   ┆ ---  │\n│ Boolean ┆ Int64 ┆ Utf8 │\n╞═════════╪═══════╪══════╡\n│ true    ┆ 4     ┆ jkl  │  <--- `row` was inserted\n├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌┤\n│ true    ┆ 1     ┆ abc  │\n├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌┤\n│ true    ┆ 2     ┆ def  │\n├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌┤\n│ false   ┆ 3     ┆ ghi  │\n╰─────────┴───────┴──────╯\n\"\"\"\n\n# to write, we have .write(df, mode), .append(df), .overwrite(df)\ntbl.append(row)\n\n# row is now inserted twice\ntbl.show()\n\"\"\"\n╭─────────┬───────┬──────╮\n│ x       ┆ y     ┆ z    │\n│ ---     ┆ ---   ┆ ---  │\n│ Boolean ┆ Int64 ┆ Utf8 │\n╞═════════╪═══════╪══════╡\n│ true    ┆ 4     ┆ jkl  │ <-- append via tbl.append(...)\n├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌┤\n│ true    ┆ 4     ┆ jkl  │ <-- append via sess.write(...)\n├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌┤\n│ true    ┆ 1     ┆ abc  │\n├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌┤\n│ true    ┆ 2     ┆ def  │\n├╌╌╌╌╌╌╌╌╌┼╌╌╌╌╌╌╌┼╌╌╌╌╌╌┤\n│ false   ┆ 3     ┆ ghi  │\n╰─────────┴───────┴──────╯\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Showing Filtered Daft DataFrame Preview (Python)\nDESCRIPTION: This line displays a preview of the first 5 rows of the `filtered_df` after the text-based filtering operation. This allows checking the effect of the filter without processing the entire dataset.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/flyte/notebook.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfiltered_df.show(5)\n```\n\n----------------------------------------\n\nTITLE: Filtering DataFrame by Maximum Size (Python)\nDESCRIPTION: Applies a filter operation to the Daft DataFrame `df`, keeping only rows where the 'size' column is less than 300,000. Due to Daft's lazy evaluation, this operation is added to the query plan but not executed immediately.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/image_querying/top_n_red_color.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndf = df.where(df[\"size\"] < 300000)\n```\n\n----------------------------------------\n\nTITLE: Excluding Columns from DataFrame\nDESCRIPTION: Shows how to remove specific columns from a DataFrame using the exclude() method.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/10min.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf.exclude(\"DoB\").show()\n```\n\n----------------------------------------\n\nTITLE: Checking Ray Cluster Resources\nDESCRIPTION: Checks the resources available in the Ray cluster.  This snippet uses `ray.cluster_resources()` to see what resources (CPU, memory, etc.) are available in the cluster for Daft and Ray to use.  This is useful to verify the configuration.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/iceberg_summit_2024.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nray.cluster_resources()\n```\n\n----------------------------------------\n\nTITLE: Displaying Daft DataFrame Representation (Python)\nDESCRIPTION: Displays the representation of the Daft DataFrame object `df`. This typically shows the schema (column names and types) and the number of partitions, but does not execute the computation or show data.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/image_querying/top_n_red_color.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf\n```\n\n----------------------------------------\n\nTITLE: Using map_partitions in Dask (Python)\nDESCRIPTION: Demonstrates the use of `map_partitions` in Dask. This function applies a given Python function to each partition of a Dask DataFrame. This method is a fundamental way to apply custom logic to your data in a distributed fashion when using Dask.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/migration/dask_migration.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef my_function(**kwargs):\n    return ...\n\nres = ddf.map_partitions(my_function, **kwargs)\n```\n\n----------------------------------------\n\nTITLE: Filtering and Aggregating Data\nDESCRIPTION: Filters the DataFrame based on a date condition and performs an aggregation.  The `where` method is used to filter rows where the `L_SHIPDATE` column is less than a specified date. Then, it groups the data by `L_SHIPDATE` and calculates the sum of the `L_EXTENDEDPRICE`.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/iceberg_summit_2024.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\n\ndf = df.where(df[\"L_SHIPDATE\"] < datetime.date(1996, 1, 1))\naggregated_df = df.groupby(\"L_SHIPDATE\").agg([daft.col(\"L_EXTENDEDPRICE\").sum()])\n```\n\n----------------------------------------\n\nTITLE: Expensive Operation with Sort and Materialization\nDESCRIPTION: This snippet shows how to perform expensive operations and materialize the DataFrame to prevent recomputations. It first sorts the DataFrame `df` by column \"A\", which is an expensive operation, and then calls `collect()` to materialize it.  After materialization, subsequent operations like `sum` and `mean` are performed on `df`, which will utilize the materialized result to avoid recalculating the sorted data. Finally, it performs the `with_column` and `show` operations to display the results.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndf = df.sort(\"A\")  # expensive sort\ndf.collect()  # materialize the DataFrame\n\n# All subsequent work on df avoids recomputing previous steps\ndf.sum(\"B\").show()\ndf.mean(\"B\").show()\ndf.with_column(\"try_this\", df[\"A\"] + 1).show(5)\n```\n\n----------------------------------------\n\nTITLE: Configuring Identifier Mode via SQL Statements - SQL\nDESCRIPTION: This snippet demonstrates setting the identifier_mode via SQL SET statements for different systems (e.g., duckdb, spark, unity, python, iceberg, postgres, datafusion). The mode affects how identifiers are resolved regarding casing. Input: desired identifier_mode. Output: session-level configuration change for SQL parsing.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql/identifiers.md#_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\n-- SQL\n\nSET identifier_mode = 'insensitive';  -- duckdb, spark, unity\nSET identifier_mode = 'sensitive';    -- python, iceberg\nSET identifier_mode = 'normalized';   -- postgres, datafusion, standard\n```\n\n----------------------------------------\n\nTITLE: Applying Scalar Functions in SELECT Statement in SQL\nDESCRIPTION: Demonstrates how to apply scalar functions `foo` and `bar` to columns `a` and `b` respectively in a SELECT statement. This example requires that such functions be defined and applicable to the column data types. It returns transformed or computed values per row.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql/statements/select.md#_snippet_3\n\nLANGUAGE: sql\nCODE:\n```\nSELECT foo(a), bar(b) FROM T;\n```\n\n----------------------------------------\n\nTITLE: Example Daft Cluster Configuration for BYOC Mode - TOML\nDESCRIPTION: Defines the .daft.toml configuration structure for running Daft in BYOC mode targeting an existing Kubernetes infrastructure. This includes setting the provider type, Kubernetes namespace, and a job with command and working directory specifications. It facilitates Daft and Ray deployment customization within custom Kubernetes clusters.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/distributed.md#_snippet_6\n\nLANGUAGE: toml\nCODE:\n```\n[setup]\nname = \"my-daft-cluster\"\npython-version = \"3.11\"\nray-version = \"2.40.0\"\nprovider = \"byoc\"\n\n[setup.byoc]\nnamespace = \"default\"\n\n[[job]]\nname = \"example-job\"\ncommand = \"python my_script.py\"\nworking-dir = \"~/my_project\"\n```\n\n----------------------------------------\n\nTITLE: Setting Global Azure Credentials for Daft in Python\nDESCRIPTION: Demonstrates setting default Azure credentials globally for subsequent Daft I/O operations. It involves creating an `AzureConfig` with the storage account name and access key, wrapping it in an `IOConfig`, and then applying it using `daft.set_planning_config`. This configuration will be used by default when reading data, like the `daft.read_parquet` example.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/azure.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom daft.io import IOConfig, AzureConfig\n\n# Supply actual values for the storage_account and access key here\nio_config = IOConfig(azure=AzureConfig(storage_account=\"***\", access_key=\"***\"))\n\n# Globally set the default IOConfig for any subsequent I/O calls\ndaft.set_planning_config(default_io_config=io_config)\n\n# Perform some I/O operation\ndf = daft.read_parquet(\"az://my_container/my_path/**/*\")\n```\n\n----------------------------------------\n\nTITLE: Setting and Checking a Continuous Integration Flag in Python\nDESCRIPTION: This snippet defines a boolean flag named CI to indicate continuous integration environment status, and conditionally exits the notebook execution early if running in CI mode. This is used to skip executing code that requires access to non-public AWS data during automated tests.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/linkedin-03-05-2024.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nCI = False\n```\n\nLANGUAGE: python\nCODE:\n```\n# Skip this notebook execution in CI because it hits non-public data in AWS\nif CI:\n    import sys\n\n    sys.exit()\n```\n\n----------------------------------------\n\nTITLE: Handling Missing and NaN Values\nDESCRIPTION: Shows the difference between null values and NaN (Not a Number) in Daft and how to detect both cases.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/10min.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nmissing_data_df = daft.from_pydict(\n    {\n        \"floats\": [1.5, None, float(\"nan\")],\n    }\n)\nmissing_data_df = missing_data_df.with_column(\"floats_is_null\", missing_data_df[\"floats\"].is_null()).with_column(\n    \"floats_is_nan\", missing_data_df[\"floats\"].float.is_nan()\n)\n\nmissing_data_df.show()\n```\n\n----------------------------------------\n\nTITLE: Displaying Unoptimized Query Plan (Python)\nDESCRIPTION: Uses the `.explain()` method to display the current, unoptimized logical query plan for the DataFrame `df`. This shows the sequence of operations as they were defined, including the two separate filter steps.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/image_querying/top_n_red_color.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf.explain()\n```\n\n----------------------------------------\n\nTITLE: Evaluating an Expression Using SELECT in SQL\nDESCRIPTION: Demonstrates how to use the SELECT statement to evaluate a simple arithmetic expression. No table references are required. This snippet outputs the result of the expression directly.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql/statements/select.md#_snippet_0\n\nLANGUAGE: sql\nCODE:\n```\nSELECT 1 + 1;\n```\n\n----------------------------------------\n\nTITLE: Limit Rows - Daft DataFrame (Python)\nDESCRIPTION: This snippet demonstrates how to initialize a Daft DataFrame from a Python dictionary and then use the `limit(N)` method to restrict the DataFrame to only the first N rows. In this example, it limits the DataFrame to the first 3 rows before displaying the result. Requires the `daft` library.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.from_pydict({\n    \"A\": [1, 2, 3, 4, 5],\n    \"B\": [6, 7, 8, 9, 10],\n})\n\ndf.limit(3).show()\n```\n\n----------------------------------------\n\nTITLE: Creating a DataFrame from Dask DataFrame with Daft - Python\nDESCRIPTION: Explains converting a Dask DataFrame into a Daft DataFrame, facilitating scaling workflows. Requires daft and dask; input should be a dask.dataframe.DataFrame object. Output is a Daft DataFrame reflecting the Dask schema. Parallelism settings and chunk sizes might affect performance.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/api/dataframe_creation.md#_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nimport daft\nimport dask.dataframe as dd\n\ndf_dask = dd.read_csv(\"/data/*.csv\")\ndf = daft.from_dask_dataframe(df_dask)\n```\n\n----------------------------------------\n\nTITLE: Explain Daft Read Operation\nDESCRIPTION: Shows the Daft execution plan for reading the Iceberg table. This helps in understanding how Daft will execute the query. It can be used for query optimization. `explain(True)` provides a detailed view of the execution plan.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/iceberg_summit_2024.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndaft.read_iceberg(table).explain(True)\n```\n\n----------------------------------------\n\nTITLE: Setting Conditional CI Flag in Python\nDESCRIPTION: Defines a boolean variable `CI` and initializes it to `False`. This flag is used later in the script to conditionally configure settings, specifically for enabling or disabling GPU usage depending on whether the code is run in a Continuous Integration environment.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/text_to_image/text_to_image_generation.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nCI = False\n```\n\n----------------------------------------\n\nTITLE: Example Daft Cluster Configuration for Provisioned Mode - TOML\nDESCRIPTION: Defines the .daft.toml configuration structure for running Daft in AWS Provisioned mode. Contains global settings such as cluster and Python versions, provider type, AWS region, instance types, SSH user/key, and a sample job configuration. This file controls how the cluster is provisioned, managed, and what jobs it executes.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/distributed.md#_snippet_5\n\nLANGUAGE: toml\nCODE:\n```\n[setup]\nname = \"my-daft-cluster\"\npython-version = \"3.11\"\nray-version = \"2.40.0\"\nprovider = \"provisioned\"\n\n[setup.provisioned]\nregion = \"us-west-2\"\nnumber-of-workers = 4\nssh-user = \"ubuntu\"\nssh-private-key = \"~/.ssh/daft-key\"\ninstance-type = \"i3.2xlarge\"\nimage-id = \"ami-04dd23e62ed049936\"\n\n[[job]]\nname = \"example-job\"\ncommand = \"python my_script.py\"\nworking-dir = \"~/my_project\"\n```\n\n----------------------------------------\n\nTITLE: Conditional Script Exit in CI Environment using Python\nDESCRIPTION: Checks the CI flag, and if true, imports 'sys' and terminates the script execution to avoid running non-public resource-dependent code during automated testing.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/3-pytorch-ray-single-node-training.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nif CI:\n    import sys\n\n    sys.exit()\n```\n\n----------------------------------------\n\nTITLE: Connecting to Unity Catalog and Listing Resources - Python\nDESCRIPTION: Demonstrates how to instantiate the UnityCatalog abstraction from daft.unity_catalog, authenticate using a Databricks token and endpoint, and list catalogs, schemas, and tables available to the user. Requires the Daft library (with the unity extra), an active Unity Catalog, and valid authentication credentials. The endpoint, token, and catalog/schema names should be replaced with user-specific values. Inputs are authentication parameters and object names; outputs are lists of available catalogs, schemas, and tables printed to stdout.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/unity_catalog.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom daft.unity_catalog import UnityCatalog\n\nunity = UnityCatalog(\n    endpoint=\"https://<databricks_workspace_id>.cloud.databricks.com\",\n    # Authentication can be retrieved from your provider of Unity Catalog\n    token=\"my-token\",\n)\n\n# See all available catalogs\nprint(unity.list_catalogs())\n\n# See available schemas in a given catalog\nprint(unity.list_schemas(\"my_catalog_name\"))\n\n# See available tables in a given schema\nprint(unity.list_tables(\"my_catalog_name.my_schema_name\"))\n```\n\n----------------------------------------\n\nTITLE: Limiting and Collecting Data - Daft - Python\nDESCRIPTION: Applies transformations to the Daft DataFrame: first limits the rows to 100, then selects only the \"folder\", \"filename\", and \"object\" columns. Finally, `collect()` triggers the execution of the planned operations, bringing the data into memory. Requires a Daft DataFrame (`df`).\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/1-local-image-batch-inference.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndf = df.limit(100)\ndf = df.select(\"folder\", \"filename\", \"object\")\ndf.collect()\n```\n\n----------------------------------------\n\nTITLE: Defining a User-Defined Function for Image Classification with PyTorch\nDESCRIPTION: Creates a UDF class 'ClassifyImages' that loads a pre-trained ResNet50 model only once, and processes batches of image URLs to classify dog breeds. Performs inference with softmax and retrieves top predictions, optimizing for multiple invocations.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/10min.ipynb#_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\n@udf(return_dtype=DataType.fixed_size_list(dtype=DataType.string(), size=2))\nclass ClassifyImages:\n    def __init__(self):\n        # Load pre-trained model once during initialization\n        self.model = torch.hub.load(\"NVIDIA/DeepLearningExamples:torchhub\", \"nvidia_resnet50\", pretrained=True)\n        self.utils = torch.hub.load(\"NVIDIA/DeepLearningExamples:torchhub\", \"nvidia_convnets_processing_utils\")\n        self.model.eval().to(torch.device(\"cpu\"))\n\n    def __call__(self, images_urls):\n        batch = torch.cat([self.utils.prepare_input_from_uri(uri) for uri in images_urls]).to(torch.device(\"cpu\"))\n\n        with torch.no_grad():\n            output = torch.nn.functional.softmax(self.model(batch), dim=1)\n\n        results = self.utils.pick_n_best(predictions=output, n=1)\n        return [result[0] for result in results]\n```\n\n----------------------------------------\n\nTITLE: Add New Column with Expression - Daft DataFrame (Python/SQL)\nDESCRIPTION: This code creates a new column named \"foo\" in the DataFrame, where each value is the corresponding value from column \"A\" incremented by 1. It illustrates how to achieve this using Daft's `with_column` method in Python or by selecting all existing columns and adding the new calculated column with an alias in SQL via `daft.sql`. The updated DataFrame is then displayed. Assumes a DataFrame `df` is already defined with a column 'A'.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Creates a new column named \"foo\" which takes on values\n# of column \"A\" incremented by 1\ndf = df.with_column(\"foo\", df[\"A\"] + 1)\ndf.show()\n```\n\nLANGUAGE: python\nCODE:\n```\n# Creates a new column named \"foo\" which takes on values\n# of column \"A\" incremented by 1\ndf = daft.sql(\"SELECT *, A + 1 AS foo FROM df\")\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Loading a PyIceberg Table\nDESCRIPTION: This snippet demonstrates how to load a PyIceberg table using the `load_catalog` function. It requires the `pyiceberg` library and assumes that an Iceberg catalog has already been configured. The table is loaded from the catalog using its namespace and table name.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/iceberg.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Access a PyIceberg table as per normal\nfrom pyiceberg.catalog import load_catalog\n\ncatalog = load_catalog(\"my_iceberg_catalog\")\ntable = catalog.load_table(\"my_namespace.my_table\")\n```\n\n----------------------------------------\n\nTITLE: Checking Peak Memory Usage During Benchmarks (bash)\nDESCRIPTION: This bash command runs pytest benchmarks while tracking the peak memory usage. It requires `pytest-memray` to be installed. It uses the `-m benchmark` to execute tests, and the `--memray` flag to enable memory profiling. This command will execute the tests and report peak memory usage. The output will include the peak memory usage reported by the `pytest-memray` plugin. This is useful for identifying memory leaks or inefficient memory usage during the benchmarks.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/benchmarking/parquet/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npytest benchmarking/parquet/ -m benchmark --memray\n```\n\n----------------------------------------\n\nTITLE: String Concatenation - Python\nDESCRIPTION: Demonstrates string concatenation in Daft.  A new column 'B2' is created by concatenating the existing column 'B' with the string 'foo'. The `with_column` method is used to add the new column to the DataFrame.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndf = df.with_column(\"B2\", df[\"B\"] + \"foo\")\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Creating a Namespace\nDESCRIPTION: Creates a namespace within the catalog. Namespaces are used to organize tables. This snippet creates a namespace named `my_namespace` within the specified catalog, providing a logical grouping for related tables.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/iceberg_summit_2024.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncatalog.create_namespace(\"my_namespace\")\n```\n\n----------------------------------------\n\nTITLE: Declaring Qualified SQL Identifiers - SQL\nDESCRIPTION: This code shows how to use qualified identifiers that reference columns or tables within a specific schema or namespace. The identifiers may be regular or delimited depending on their composition. Prerequisite: knowledge of catalog or schema structure. Input: dot-separated identifier string. Output: fully qualified identifier access in SQL expressions.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql/identifiers.md#_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\n-- qualified identifier\nabc.xyz\n```\n\n----------------------------------------\n\nTITLE: Show tables in a specific catalog\nDESCRIPTION: This SQL snippet demonstrates how to list tables in a specific catalog using the IN clause with the SHOW TABLES statement, specifying the catalog name.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql/statements/show.md#_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nSHOW TABLES IN my_catalog;\n```\n\n----------------------------------------\n\nTITLE: Limiting Data Rows for Local Execution in Python\nDESCRIPTION: Reduces the dataset size to 128 rows if running locally, preventing resource overuse during small-scale testing or demonstration.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/3-pytorch-ray-single-node-training.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nif not USE_REMOTE_CLUSTER:\n    df = df.limit(128)\n```\n\n----------------------------------------\n\nTITLE: Defining Helper Function for Parquet Read\nDESCRIPTION: This defines a helper function `get_df` that takes a table name and returns a Daft DataFrame by reading parquet files from a specific S3 location. This is done to make it easier to read many table files, in this case, for a TPC-H query.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/pydata_global_2023.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport daft\n\nPARQUET_FOLDER = \"s3://eventual-dev-benchmarking-fixtures/uncompressed-smaller-rg/tpch-dbgen/1000_0/512/parquet/\"\n\n\ndef get_df(table_name: str) -> daft.DataFrame:\n    return daft.read_parquet(os.path.join(PARQUET_FOLDER, table_name, \"*.parquet\"))\n```\n\n----------------------------------------\n\nTITLE: Show tables in current catalog\nDESCRIPTION: This SQL snippet demonstrates how to list all tables in the current catalog using the SHOW TABLES statement without any specific catalog or pattern.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql/statements/show.md#_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nSHOW TABLES;\n```\n\n----------------------------------------\n\nTITLE: Creating and Dropping Namespaces and Tables in Daft Sessions using Python\nDESCRIPTION: Shows creating namespaces and tables within a Daft Session. It covers creating a namespace (`sess.create_namespace`), listing namespaces (`sess.list_namespaces`), creating a persistent table from a Daft DataFrame (`sess.create_table`), and creating a temporary table from a CSV file (`sess.create_temp_table`). Commented-out lines indicate how to create an empty table with a schema and how to drop tables and namespaces. Requires `daft`, an active session `sess`, and potentially a CSV file (`/tmp/daft/row.csv`) for the temp table example.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sessions.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# create a namespace 'example'\nsess.create_namespace(\"example\")\n\n# verify it was created\nsess.list_namespaces()\n\"\"\"\n[Identifier('example')]\n\"\"\"\n\n# you can create an empty table with a schema\n# sess.create_table(\"example.tbl\", schema)\n\n# but suppose we have some data..\ndf = daft.from_pydict({\n    \"x\": [ True, True, False ],\n    \"y\": [ 1, 2, 3 ],\n    \"z\": [ \"abc\", \"def\", \"ghi\" ],\n})\n\n# create a table from the dataframe, which will create + append\nsess.create_table(\"example.tbl\", df)\n\n# you can also create temporary tables from dataframes\n# > echo \"x,y,z\\nFalse,4,jkl\" > /tmp/daft/row.csv\nsess.create_temp_table(\"temp\", daft.read_csv(\"/tmp/daft/row.csv\"))\n\n# you can drop too\n# sess.drop_table(\"example.tbl\")\n# sess.drop_namespace(\"example\")\n```\n\n----------------------------------------\n\nTITLE: Setting Global Default S3 Credentials in Daft Python\nDESCRIPTION: This snippet demonstrates how to manually provide AWS credentials (key ID, secret key, session token) to Daft using `IOConfig` and `S3Config` objects. It then shows how to set this configuration as the global default for all subsequent Daft I/O operations using `daft.set_planning_config`. Required dependencies include the `daft.io` and `daft.context` modules.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/aws.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom daft.io import IOConfig, S3Config\n\n# Supply actual values for the se\nio_config = IOConfig(s3=S3Config(key_id=\"key_id\", session_token=\"session_token\", secret_key=\"secret_key\"))\n\n# Globally set the default IOConfig for any subsequent I/O calls\ndaft.set_planning_config(default_io_config=io_config)\n\n# Perform some I/O operation\ndf = daft.read_parquet(\"s3://my_bucket/my_path/**/*\")\n```\n\n----------------------------------------\n\nTITLE: Conditionally exiting script during CI runs\nDESCRIPTION: Checks the CI flag and exits the script early if set to True, preventing execution of code that depends on non-public resources. It imports sys to call sys.exit() for termination.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/2-distributed-batch-inference.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# Skip this notebook execution in CI because it hits non-public buckets\nif CI:\n    import sys\n\n    sys.exit()\n```\n\n----------------------------------------\n\nTITLE: Setting up Virtual Environment and Installing Dependencies (bash)\nDESCRIPTION: This bash script sets up a virtual environment and installs necessary dependencies for benchmarking Daft. It first creates a virtual environment using `venv`, activates it, and then installs dependencies listed in `benchmark-requirements.txt`. Finally, it installs the Daft library, either from a released wheel or a local build.  Dependencies include `pytest` and potentially other benchmarking related libraries. The output will be the installation of the packages.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/benchmarking/parquet/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv venv\nsource venv/bin/activate\npip install -r benchmark-requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Daft with Ray Extra Dependencies using pip in Bash\nDESCRIPTION: Installs Daft with additional dependencies required for running distributed computations on a Ray cluster. This enables integration of Daft workloads with the Ray distributed execution framework. It requires a supported Ray environment.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/install.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -U daft[ray]\n```\n\n----------------------------------------\n\nTITLE: Filtering and Decoding Thumbnails from Delta Lake with Daft in Python\nDESCRIPTION: This code decodes JPEG-encoded thumbnails, adds them to the DataFrame, and filters rows for those whose descriptions contain the word 'dog' using string matching. Inputs: DataFrame columns 'image_thumbnail' and 'description'. Outputs: Filtered DataFrame with decoded images. Assumes that 'description' and the encoded images exist.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/data-ai-summit-2024.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nread_df = read_df.with_column(\"image_thumbnail\", daft.col(\"image_thumbnail\").image.decode()).where(\n    read_df[\"description\"].str.contains(\"dog\")\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Expressions: Wildcard Struct Access (Python)\nDESCRIPTION: This snippet demonstrates how to access all members of a struct column using a wildcard (`col(\"person\")[\"*\"]`). The input is a Daft DataFrame containing a struct column named 'person', and the output is a DataFrame with each field of the 'person' struct as a separate column.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nimport daft\nfrom daft import col\n\ndf = daft.from_pydict({\n    \"person\": [\n        {\"name\": \"Alice\", \"age\": 30},\n        {\"name\": \"Bob\", \"age\": 25},\n        {\"name\": \"Charlie\", \"age\": 35}\n    ]\n})\n\n# Access all fields of the 'person' struct\ndf.select(col(\"person\")[\"*\"]).show()\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies\nDESCRIPTION: Installs required Python packages using `pip`. It installs pyiceberg with SQL support, daft with ray support, polars, pandas, ray, sqlalchemy, ipywidgets, boto3 and mypy_boto3_glue. These packages are essential for interacting with Iceberg tables, querying, and data manipulation.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/iceberg_summit_2024.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n!pip install 'pyiceberg[sql]'\n!pip install 'daft[ray]' polars pandas\n!pip install ray==2.20.0\n!pip install sqlalchemy ipywidgets boto3 mypy_boto3_glue\n```\n\n----------------------------------------\n\nTITLE: Configuring Daft Execution Runners for Local or Ray Cluster in Python\nDESCRIPTION: This commented snippet presents options to configure Daft to execute query plans either locally with a native multithreaded runner or remotely on a Ray distributed cluster by specifying its connection address. This requires Ray framework and appropriate cluster setup when used.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/linkedin-03-05-2024.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n## Use the Native multithreaded local runner (default behavior)\n# daft.context.set_runner_native()\n\n## Connect to a Ray cluster and use the Ray runner\n# daft.context.set_runner_ray(address=\"ray://...\")\n```\n\n----------------------------------------\n\nTITLE: Defining Embedding Types in SQL for Daft\nDESCRIPTION: SQL syntax examples for defining Daft Embedding types. The `EMBEDDING(T, N)` syntax represents a fixed-length array of size `N` containing elements of a numeric type `T`.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql/datatypes.md#_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nEMBEDDING(INT16, 256)\nEMBEDDING(INT32, 100)\n```\n\n----------------------------------------\n\nTITLE: Filtering DataFrame with Partition Pruning Optimization - Python\nDESCRIPTION: Shows how to filter a Daft DataFrame loaded from a Unity Catalog Delta Lake table using a partition column, enabling partition pruning optimizations. Requires a Daft DataFrame ('df') already loaded from Delta Lake. The key parameter is the filter condition applied to the partition column. The output is a filtered DataFrame, optimized for performance, displayed with 'show()'. Effective for handling large datasets within resource constraints.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/unity_catalog.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Filter which takes advantage of partition pruning capabilities of Delta Lake\ndf = df.where(df[\"partition_key\"] < 1000)\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Plotly Initialization in HTML\nDESCRIPTION: This HTML and JavaScript snippet initializes the Plotly library and creates a container for the visualization. It imports the Plotly library from a CDN and prepares the page for rendering the performance comparison chart.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/resources/benchmarks/tpch.md#_snippet_2\n\nLANGUAGE: HTML\nCODE:\n```\n<div>                        <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.20.0.min.js\"></script>                <div id=\"2e3c4bff-c808-4722-8664-d4c63ee41e55\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"2e3c4bff-c808-4722-8664-d4c63ee41e55\")) {                    Plotly.newPlot(                        \"2e3c4bff-c808-4722-8664-d4c63ee41e55\",                        []{},                        {},                        {\"displayModeBar\": false, \"responsive\": true}                    )                };                            </script>        </div>\n```\n\n----------------------------------------\n\nTITLE: Reading Individual Files and Glob Patterns (Python)\nDESCRIPTION: This code demonstrates reading individual files (Parquet and CSV) and using glob patterns to read multiple files from a Hugging Face dataset using Daft. It uses `daft.read_parquet()` and `daft.read_csv()` with paths pointing to specific files or a glob pattern.  It also imports `daft` library. This assumes a public dataset.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/huggingface.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport daft\n\ndf = daft.read_parquet(\"hf://datasets/username/dataset_name/file_name.parquet\")\n# or a csv file\ndf = daft.read_csv(\"hf://datasets/username/dataset_name/file_name.csv\")\n\n# or a glob pattern\ndf = daft.read_parquet(\"hf://datasets/username/dataset_name/**/*.parquet\")\n```\n\n----------------------------------------\n\nTITLE: Setting Ray Runner in Daft for Ray Flyte Tasks\nDESCRIPTION: Code snippet showing how to configure Daft to use a Ray runner in a Flyte Ray task environment. This allows Daft to connect to the Ray cluster that has already been initialized by Flyte.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/flyte/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndaft.context.set_ray_runner()\n```\n\n----------------------------------------\n\nTITLE: Setting up Daft Catalog with PyIceberg and SQLite in Python\nDESCRIPTION: Shows how to configure and create a Daft Catalog using a PyIceberg SQL catalog backed by SQLite. This involves importing necessary classes, defining a temporary directory, creating a `SqlCatalog` instance with SQLite connection details, and then wrapping it with `Catalog.from_iceberg`. Requires `daft` and `pyiceberg[sql-sqlite]` libraries, and the specified directory (`/tmp/daft/example`) must exist.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sessions.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom daft import Catalog\nfrom pyiceberg.catalog.sql import SqlCatalog\n\n# don't forget to `mkdir -p /tmp/daft/example`\ntmpdir = \"/tmp/daft/example\"\n\n# create a pyiceberg catalog backed by sqlite\niceberg_catalog = SqlCatalog(\n    \"default\",\n    **{\n        \"uri\": f\"sqlite:///{tmpdir}/catalog.db\",\n        \"warehouse\": f\"file://{tmpdir}\",\n    },\n)\n\n# creating a daft catalog from the pyiceberg catalog implementation\ncatalog = Catalog.from_iceberg(iceberg_catalog)\n\n# check\ncatalog.name\n\"\"\"\n'default'\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Disabling ptrace Protection for LLDB Debugging - Shell\nDESCRIPTION: This shell command disables ptrace protection at the kernel level to allow debuggers like LLDB to attach to other processes. It is necessary on some Linux systems for mixed Python/Rust debugging to function correctly. This requires superuser privileges and has security implications, as it reduces process isolation; revert after debugging if possible.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/CONTRIBUTING.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\necho 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope\n```\n\n----------------------------------------\n\nTITLE: Indexing DataFrame in Daft vs. Dask (Python)\nDESCRIPTION: This snippet illustrates how to index a DataFrame to return a specific row in Dask and Daft. In Dask, it uses the `.loc` method. In Daft, it uses the `where` method with a `col` Expression to filter the DataFrame. This showcases a fundamental difference in the approach to indexing.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/migration/dask_migration.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nddf.loc[[“b”]]\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.where(daft.col(“alpha”)==”b”)\n```\n\n----------------------------------------\n\nTITLE: Creating Expressions with daft.sql_expr and Python API - Python\nDESCRIPTION: Demonstrates the equivalence between creating Daft Expressions from SQL strings using daft.sql_expr and building them with the native Python API. Both produce objects representing column arithmetic and aliasing. These expressions can be inserted into queries or selections. Requires daft library. Inputs are either SQL strings or expression-building code; outputs are Daft Expression objects printable to console.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql_overview.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nsql_expr = daft.sql_expr(\"A + B as C\")\nprint(\"SQL expression:\", sql_expr)\n```\n\nLANGUAGE: Python\nCODE:\n```\npy_expr = (daft.col(\"A\") + daft.col(\"B\")).alias(\"C\")\nprint(\"Python expression:\", py_expr)\n```\n\n----------------------------------------\n\nTITLE: Setting CI Flag (Python)\nDESCRIPTION: Initializes a boolean variable `CI`. Although defined, this specific variable is not used in the subsequent logic shown in the notebook.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/image_querying/top_n_red_color.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nCI = False\n```\n\n----------------------------------------\n\nTITLE: Calling the image classification UDF\nDESCRIPTION: This snippet shows how to call the `ClassifyImages` UDF on the `urls` column of a DataFrame. The results are stored in a new column called `classify_breed`. The `select` and `show` methods are then used to display the `dog_name`, `image`, and `classify_breed` columns.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_70\n\nLANGUAGE: Python\nCODE:\n```\nclassified_images_df = df_family.with_column(\"classify_breed\", ClassifyImages(daft.col(\"urls\")))\nclassified_images_df.select(\"dog_name\", \"image\", \"classify_breed\").show()\n```\n\n----------------------------------------\n\nTITLE: Configuring Daft IO and Environment Flags in Python\nDESCRIPTION: Defines a flag to detect CI environment and sets parameters accordingly. Initializes Daft IOConfig to use anonymous access to an AWS S3 bucket in the 'us-west-2' region for reading Parquet files. Sets the default IO configuration for Daft's planning to ensure consistent, authenticated data access. This setup prepares Daft to read remote datasets from S3 anonymously.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/text_to_image/using_cloud_with_ray.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nCI = False\n```\n\nLANGUAGE: python\nCODE:\n```\nimport daft\n\nUSE_RAY = False if CI else True\nNUM_ROWS_LIMIT = 16 if CI else 160\nIO_CONFIG = daft.io.IOConfig(\n    s3=daft.io.S3Config(anonymous=True, region_name=\"us-west-2\")\n)  # Use anonymous-mode for accessing AWS S3\nPARQUET_URL = \"s3://daft-public-data/tutorials/laion-parquet/train-00000-of-00001-6f24a7497df494ae.parquet\"\n\ndaft.set_planning_config(default_io_config=IO_CONFIG)\n```\n\n----------------------------------------\n\nTITLE: Reading Parquet Files\nDESCRIPTION: This snippet demonstrates reading data from Parquet files using `daft.read_parquet()`. It specifies an S3 path to the data.  The `collect()` method triggers the actual data loading into memory for processing and timing the process using the `%%time` magic command.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/pydata_global_2023.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.read_parquet(\n    \"s3://eventual-dev-benchmarking-fixtures/uncompressed-smaller-rg/tpch-dbgen/100_0/32/parquet/lineitem/\"\n)\n```\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\ndf.collect()\n```\n\n----------------------------------------\n\nTITLE: Adding Human-Readable Class Names - Python\nDESCRIPTION: This code snippet adds a new column to a Daft DataFrame called `class_human_readable`.  It retrieves the name using the `classes_human_readable` dictionary that maps class IDs to human-readable names, and adds a column for class IDs using the `classes_id` dictionary to map the label names to IDs. This provides more interpretability.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/3-pytorch-ray-single-node-training.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndf = df.with_column(\n    \"class_human_readable\",\n    df[\"object\"]\n    .list.get(0)\n    .struct.get(\"name\")\n    .apply(lambda name: classes_human_readable[name], return_dtype=daft.DataType.string()),\n)\ndf = df.with_column(\n    \"class_id\",\n    df[\"object\"]\n    .list.get(0)\n    .struct.get(\"name\")\n    .apply(lambda name: classes_id[name], return_dtype=daft.DataType.int64()),\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Daft with AWS Extra Dependencies using pip in Bash\nDESCRIPTION: Installs Daft along with optional dependencies required for AWS services integration, such as accessing AWS S3. This command includes Daft's extras for AWS and depends on a network connection to PyPI for the extra packages. Useful for users needing AWS functionality in Daft workflows.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/install.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -U daft[aws]\n```\n\n----------------------------------------\n\nTITLE: Declaring Regular SQL Identifiers - SQL\nDESCRIPTION: This snippet demonstrates how to declare a regular (unquoted) SQL identifier, which must start with an alphabetic character or underscore and may contain only alphanumeric symbols, '$', or '_'. No dependencies are involved. The identifier is case-insensitive or normalized depending on the session's identifier_mode. Input: identifier. Output: definition for use in SQL queries.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql/identifiers.md#_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\n-- regular identifier\nabc\n```\n\n----------------------------------------\n\nTITLE: Reading a WARC File into a DataFrame with Daft - Python\nDESCRIPTION: Demonstrates ingesting WARC files (web archival format) into Daft DataFrames. Daft must be installed with WARC support; file path or URI is required. Output is a DataFrame with WARC record structure; large files or malformed records can impact performance or cause errors.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/api/dataframe_creation.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nimport daft\n\ndf = daft.read_warc(\"/path/to/file.warc\")\n```\n\n----------------------------------------\n\nTITLE: Filtering Rows Using Native Python Expression in DataFrame.where - Python\nDESCRIPTION: Presents row filtering using a native Python comparison expression as the parameter to DataFrame.where. The expression df[\"A\"] < 2 builds the predicate. Requires daft and an existing DataFrame. String-based and expression-based filters are interchangeable. Result is a displayed subset DataFrame. Inputs are the DataFrame and a Boolean Series predicate; output is the filtered DataFrame.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql_overview.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndf = daft.from_pydict({\"A\": [1, 2, 3], \"B\": [1, 2, 3]})\n\n# Using Daft's Python Expression API\ndf = df.where(df[\"A\"] < 2)\n\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Configuring Anonymous S3 Access with Daft IOConfig in Python\nDESCRIPTION: This snippet creates a Daft IOConfig instance configured for anonymous access to AWS S3 in the us-west-2 region. This configuration is used as a parameter when reading datasets from public S3 buckets without requiring AWS credentials.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/linkedin-03-05-2024.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport daft\n\nANONYMOUS_IO_CONFIG = daft.io.IOConfig(s3=daft.io.S3Config(anonymous=True, region_name=\"us-west-2\"))\n```\n\n----------------------------------------\n\nTITLE: Loading and Resizing Images with Daft in Python\nDESCRIPTION: This code snippet demonstrates how to load images from an AWS S3 bucket using Daft, download the images, and resize them. It requires the `daft` library to be installed. The snippet loads a dataframe from filepaths, downloads image URLs as bytes, decodes them into images, resizes the images, and displays the first 3 rows of the dataframe.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/README.rst#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport daft\n\n# Load a dataframe from filepaths in an S3 bucket\ndf = daft.from_glob_path(\"s3://daft-public-data/laion-sample-images/*\")\n\n# 1. Download column of image URLs as a column of bytes\n# 2. Decode the column of bytes into a column of images\ndf = df.with_column(\"image\", df[\"path\"].url.download().image.decode())\n\n# Resize each image into 32x32\ndf = df.with_column(\"resized\", df[\"image\"].image.resize(32, 32))\n\ndf.show(3)\n```\n\n----------------------------------------\n\nTITLE: Configuring I/O and Reading Partitioned Parquet Data\nDESCRIPTION: Sets up anonymous S3 access configuration and reads a partitioned Parquet file from a public S3 bucket.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/10min.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Set IO Configurations to use anonymous data access mode\ndaft.set_planning_config(default_io_config=daft.io.IOConfig(s3=daft.io.S3Config(anonymous=True)))\n\ndf = daft.read_parquet(\"s3://daft-public-data/tutorials/10-min/sample-data-dog-owners-partitioned.pq/**\")\ndf\n```\n\n----------------------------------------\n\nTITLE: USE Statement Syntax Definition - mkeenan Format\nDESCRIPTION: This syntax block formally defines the USE statement structure in a likely EBNF-style language (labelled 'mkeenan'). It specifies that USE can take either just a catalog identifier or both catalog and namespace identifiers, defining expected parsing rules for SQL dialects. This snippet is intended for implementers or advanced users exploring parser behaviors.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql/statements/use.md#_snippet_2\n\nLANGUAGE: mkeenan\nCODE:\n```\nuse_statement\n    'USE' catalog_ident\n    'USE' catalog_ident '.' namespace_ident\n\ncatalog_ident\n    simple_ident\n\nnamespace_ident\n    ident\n```\n\n----------------------------------------\n\nTITLE: Job Management with Daft CLI - Bash\nDESCRIPTION: Commands for submitting jobs, checking job status, and viewing logs via the Daft CLI in a distributed environment. These commands allow users to manage and monitor data processing jobs programmatically and from the terminal interface, with status and logs available primarily in Provisioned mode.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/distributed.md#_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n# Submit a job\ndaft job submit example-job\n\n# Check job status (provisioned mode only)\ndaft job status example-job\n\n# View job logs (provisioned mode only)\ndaft job logs example-job\n```\n\n----------------------------------------\n\nTITLE: Initializing Daft to Use Ray for Distributed Execution - Python\nDESCRIPTION: Demonstrates how to configure Daft's execution context to use Ray as the distributed computation engine. It includes setting Ray runner with default local cluster initialization as well as connecting to a remote Ray cluster via a specified network address. Requires Daft and Ray packages installed, with dependencies on Ray cluster availability either locally or remotely.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/distributed.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport daft\n\ndaft.context.set_runner_ray()\n```\n\nLANGUAGE: python\nCODE:\n```\ndaft.context.set_runner_ray(address=\"ray://url-to-mycluster\")\n```\n\nLANGUAGE: python\nCODE:\n```\n>>> import daft\n>>> daft.context.set_runner_ray(\"127.0.0.1:6379\")\nDaftContext(_daft_execution_config=<daft.daft.PyDaftExecutionConfig object at 0x100fbd1f0>, _daft_planning_config=<daft.daft.PyDaftPlanningConfig object at 0x100fbd270>, _runner_config=_RayRunnerConfig(address='127.0.0.1:6379', max_task_backlog=None), _disallow_set_runner=True, _runner=None)\n>>> df = daft.from_pydict({\n...   'text': ['hello', 'world']\n... })\n2024-07-29 15:49:26,610\\tINFO worker.py:1567 -- Connecting to existing Ray cluster at address: 127.0.0.1:6379...\n2024-07-29 15:49:26,622\\tINFO worker.py:1752 -- Connected to Ray cluster.\n>>> print(df)\n╭───────╮\n│ text  │\n│ ---   │\n│ Utf8  │\n╞═══════╡\n│ hello │\n├╌╌╌╌╌╌╌┤\n│ world │\n╰───────╯\n\n(Showing first 2 of 2 rows)\n```\n\n----------------------------------------\n\nTITLE: Installing Daft and DeltaLake in Python\nDESCRIPTION: This snippet installs the 'daft' and 'deltalake' packages, with a version constraint on 'deltalake' to be below 0.17. Ensure that your Python environment supports pip and has adequate permissions. These dependencies are required throughout the notebook for data processing and storage functionality.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/data-ai-summit-2024.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install daft deltalake<0.17\n```\n\n----------------------------------------\n\nTITLE: Converting Images to Tensor DataType in DataFrame using daft in Python\nDESCRIPTION: Transforms images into tensor format with specified shape and data type, necessary for machine learning models that operate on tensor inputs.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/3-pytorch-ray-single-node-training.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ndf = df.with_column(\n    \"arr\",\n    (df[\"image\"].cast(daft.DataType.tensor(daft.DataType.uint8(), shape=(256, 256, 3))))\n)\n```\n\n----------------------------------------\n\nTITLE: Materializing Filtered DataFrame (Python)\nDESCRIPTION: Executes the current query plan (including filters) using `.collect()` and materializes the results into memory. The resulting data is stored within the `df` object, preventing recomputation for subsequent operations on this materialized version. The output shows the first 10 rows.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/image_querying/top_n_red_color.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Materializes the dataframe and shows first 10 rows\n\ndf.collect()\n```\n\n----------------------------------------\n\nTITLE: Checking Daft Session State in Python\nDESCRIPTION: Illustrates creating a new, empty Daft Session and inspecting its initial state. Specifically, it calls `sess.current_catalog()` and `sess.current_namespace()` to show that they return `None` by default before any catalogs or namespaces are attached or set. Requires the `daft` library.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sessions.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport daft\n\nfrom daft import Session\n\n# create a new empty session\nsess = Session()\n\n# check the current catalog (None)\nsess.current_catalog()\n\n# get the current namespace (None)\nsess.current_namespace()\n```\n\n----------------------------------------\n\nTITLE: URL Download - SQL\nDESCRIPTION: Downloads data from URLs in a column using SQL. Initializes a DataFrame with a 'urls' column, then uses a SQL query with the `url_download` function to download data from each URL, storing the results in a new 'data' column.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.from_pydict({\n    \"urls\": [\n        \"https://www.google.com\",\n        \"s3://daft-public-data/open-images/validation-images/0001eeaf4aed83f9.jpg\",\n    ],\n})\ndf = daft.sql(\"\"\"\n    SELECT\n        urls,\n        url_download(urls) AS data\n    FROM df\n\"\"\")\ndf.collect()\n```\n\n----------------------------------------\n\nTITLE: Reading Parquet from S3 and Selecting Columns (Python)\nDESCRIPTION: This code shows how to read a Parquet file located at a specified S3 path using the previously configured `IO_CONFIG`. It then immediately selects a subset of columns (`URL`, `TEXT`, `AESTHETIC_SCORE`) to work with, creating a new Daft DataFrame with only these columns.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/flyte/notebook.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nparquet_df = daft.read_parquet(PARQUET_PATH, io_config=IO_CONFIG)\nparquet_df = parquet_df.select(parquet_df[\"URL\"], parquet_df[\"TEXT\"], parquet_df[\"AESTHETIC_SCORE\"])\n```\n\n----------------------------------------\n\nTITLE: Importing Daft and Core Components\nDESCRIPTION: Importing the Daft library and its essential classes for DataFrame operations.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/10min.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport daft\nfrom daft import DataType, udf\n```\n\n----------------------------------------\n\nTITLE: Reading Various Data Formats and SQL Data Using Daft in Python\nDESCRIPTION: This commented snippet indicates Daft's ability to read other file formats like CSV, Parquet, and JSON, as well as SQL query results from relational databases. It also shows how to glob match files in a directory to construct a dataframe and displays a few records. This snippet depends on appropriate file paths or database URLs.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/linkedin-03-05-2024.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n### Daft also supports reading from many other file sources:\n# df = daft.read_csv(...)\n# df = daft.read_parquet(...)\n# df = daft.read_json(...)\n\n### Read from SQL Databases\n# df = daft.read_sql(\"SELECT * FROM table\", \"mysql://...\")\n\n### Glob a path into files\nlaion_df = daft.from_glob_path(\"s3://daft-public-data/laion-sample-images/*\")\n\nlaion_df.show(3)\n```\n\n----------------------------------------\n\nTITLE: Creating a DataFrame from File Paths with Daft - Python\nDESCRIPTION: Explains generating a Daft DataFrame by scanning file paths matching a glob pattern. Requires daft and valid filesystem access. Input is a glob pattern or path string; output is a DataFrame of matched file paths. Patterns must be compatible with the host filesystem and may not recursively traverse directories without explicit wildcards.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/api/dataframe_creation.md#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nimport daft\n\ndf = daft.from_glob_path(\"/data/**/*.csv\")\n```\n\n----------------------------------------\n\nTITLE: Filter Rows by Expression - Daft DataFrame (Python)\nDESCRIPTION: This code snippet applies a filter to the DataFrame using the `where()` method. It keeps only the rows where the logical expression `df[\"A\"] > 3` evaluates to true. The resulting filtered DataFrame is then displayed. It depends on a DataFrame `df` being previously defined with a column 'A'.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndf.where(df[\"A\"] > 3).show()\n```\n\n----------------------------------------\n\nTITLE: Applying Red Detection UDF (Python)\nDESCRIPTION: Adds a new column 'red_mask' to the DataFrame `df` by applying the custom Python function `magic_red_detector` to each element in the 'image' column using the `.apply()` expression. The return type is specified as `daft.DataType.python()` because the UDF returns a PIL Image object.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/image_querying/top_n_red_color.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndf = df.with_column(\n    \"red_mask\",\n    df[\"image\"].apply(magic_red_detector, return_dtype=daft.DataType.python()),\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a DataFrame from SQL Query in Daft - Python\nDESCRIPTION: Demonstrates initializing a Daft DataFrame using an SQL query string. Requires the Daft library with SQL support enabled; dependencies may include database drivers and valid connection parameters. Accepts a SQL query as input and returns a DataFrame populated with the query result. Limitations may include SQL dialect compatibility and available database connectors.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/api/dataframe_creation.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport daft\n\ndf = daft.read_sql(\"SELECT * FROM my_table\", connection=\"sqlite:///mydb.db\")\n```\n\n----------------------------------------\n\nTITLE: Reading from a Delta Lake Table Using Daft in Python\nDESCRIPTION: This snippet reads back the previously stored Delta Lake table into a Daft DataFrame for analysis or downstream use. Input: Path to Delta Lake table ('my_table.delta_lake'). Output: Loaded DataFrame. Prerequisites: Table must exist and be accessible.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/data-ai-summit-2024.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nread_df = daft.read_deltalake(\"my_table.delta_lake\")\nread_df\n```\n\n----------------------------------------\n\nTITLE: Create SQLite Table (Python)\nDESCRIPTION: This snippet creates a local SQLite database named `example.db` and inserts sample data into a table named `books`.  It uses the `sqlite3` library for database interaction.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/sql.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport sqlite3\n\nconnection = sqlite3.connect(\"example.db\")\nconnection.execute(\n    \"CREATE TABLE IF NOT EXISTS books (title TEXT, author TEXT, year INTEGER)\"\n)\nconnection.execute(\n    \"\"\"\nINSERT INTO books (title, author, year)\nVALUES\n    ('The Great Gatsby', 'F. Scott Fitzgerald', 1925),\n    ('To Kill a Mockingbird', 'Harper Lee', 1960),\n    ('1984', 'George Orwell', 1949),\n    ('The Catcher in the Rye', 'J.D. Salinger', 1951)\n\"\"\"\n)\nconnection.commit()\nconnection.close()\n```\n\n----------------------------------------\n\nTITLE: Cluster Management Commands Using Daft CLI - Bash\nDESCRIPTION: Lists common Daft CLI commands for managing the lifecycle of clusters in both Provisioned and BYOC modes including commands to start, list, connect, ssh, shutdown, and kill clusters or clean up resources. It provides an operational interface for DevOps and data engineers to control cluster infrastructure from the CLI.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/distributed.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# Provisioned Mode\n# Spin up a cluster\ndaft provisioned up\n\n# List clusters and their status\ndaft provisioned list\n\n# Connect to Ray dashboard\ndaft provisioned connect\n\n# SSH into head node\ndaft provisioned ssh\n\n# Gracefully shutdown cluster\ndaft provisioned down\n\n# Force terminate cluster\ndaft provisioned kill\n```\n\nLANGUAGE: bash\nCODE:\n```\n# BYOC Mode\n# Initialize Ray/Daft on your cluster\ndaft byoc init\n\n# Connect to your cluster\ndaft byoc connect\n\n# Clean up Ray/Daft resources\ndaft byoc cleanup\n```\n\n----------------------------------------\n\nTITLE: Grouped Aggregation with agg method\nDESCRIPTION: This code snippet shows how to execute multiple aggregations on a grouped DataFrame using the `.agg()` method.  It calculates the mean and max of the 'score' column, grouped by 'class', providing aliases for the resulting columns.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_60\n\nLANGUAGE: python\nCODE:\n```\ndf.groupby(\"class\").agg(\n    df[\"score\"].mean().alias(\"mean_score\"),\n    df[\"score\"].max().alias(\"max_score\"),\n).show()\n```\n\n----------------------------------------\n\nTITLE: Collecting Daft DataFrame in Python\nDESCRIPTION: Executes the Daft query plan associated with `parquet_df` and materializes the entire DataFrame into memory. This operation triggers data loading (e.g., downloading the Parquet file from S3) and caches the results for faster subsequent operations.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/text_to_image/text_to_image_generation.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nparquet_df.collect()\n```\n\n----------------------------------------\n\nTITLE: Read SQL with Parallel Reads (Python)\nDESCRIPTION: This example showcases how to enable parallel reads from a SQL database using `daft.read_sql()`. It specifies the `partition_on` parameter to indicate the column to partition by, and `num_partitions` to control the number of parallel reads.  It imports the `daft` library.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/sql.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Read SQL query into Daft DataFrame with parallel reads\nimport daft\n\ndf = daft.read_sql(\n    \"SELECT * FROM table\",\n    \"sqlite:///big_table.db\",\n    partition_on=\"col\",\n    num_partitions=3,\n)\n```\n\n----------------------------------------\n\nTITLE: Global Aggregation Example\nDESCRIPTION: This snippet demonstrates a global aggregation on a Daft DataFrame to calculate the mean of the 'score' column. It first creates a DataFrame from a Python dictionary, then uses the `mean()` method to calculate the average score, finally displaying the result.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_57\n\nLANGUAGE: python\nCODE:\n```\nimport daft\n\ndf = daft.from_pydict({\n    \"class\": [\"a\", \"a\", \"b\", \"b\"],\n    \"score\": [10, 20., 30., 40],\n})\n\ndf.mean(\"score\").show()\n```\n\n----------------------------------------\n\nTITLE: Filtering Data from Parquet\nDESCRIPTION: This snippet reads data from Parquet files and filters it based on a condition using `where()`. It filters the DataFrame based on `L_ORDERKEY` values.  The `collect()` method initiates the query execution.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/pydata_global_2023.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.read_parquet(\n    \"s3://eventual-dev-benchmarking-fixtures/uncompressed-smaller-rg/tpch-dbgen/100_0/32/parquet/lineitem/\"\n)\ndf = df.where(df[\"L_ORDERKEY\"] < 100)\n```\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\ndf.collect()\n```\n\n----------------------------------------\n\nTITLE: Overriding Azure Credentials Per-Operation in Daft (Python)\nDESCRIPTION: Shows how to provide specific Azure credentials for a single Daft I/O operation, overriding any global configuration. This is done by passing an `IOConfig` object (containing an `AzureConfig`) directly to the `io_config` keyword argument of the I/O function, such as `daft.read_csv`. This allows using different credentials for different data sources.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/azure.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Perform some I/O operation but override the IOConfig\ndf2 = daft.read_csv(\"az://my_container/my_other_path/**/*\", io_config=io_config)\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame from Memory\nDESCRIPTION: This snippet illustrates creating Daft DataFrames from Python lists and dictionaries. It uses `daft.from_pydict()` to create a DataFrame from a dictionary of column names and lists of values and `daft.from_pylist()` to create a DataFrame from a list of dictionaries, where each dictionary represents a row.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_53\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.from_pydict({\"A\": [1, 2, 3], \"B\": [\"foo\", \"bar\", \"baz\"]})\n```\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.from_pylist([{\"A\": 1, \"B\": \"foo\"}, {\"A\": 2, \"B\": \"bar\"}, {\"A\": 3, \"B\": \"baz\"}])\n```\n\n----------------------------------------\n\nTITLE: Reading DeltaLake Dataset into Daft Dataframe with Anonymous S3 Access in Python\nDESCRIPTION: This snippet reads a DeltaLake dataset located on a public S3 bucket into a Daft dataframe using previously defined anonymous IO configuration to enable unauthenticated access. The dataframe contents are then printed.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/linkedin-03-05-2024.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndelta_df = daft.read_deltalake(\n    \"s3://daft-public-data/nyc-taxi-dataset-2023-jan-deltalake/\", io_config=ANONYMOUS_IO_CONFIG\n)\ndelta_df.show()\n```\n\n----------------------------------------\n\nTITLE: Working with Catalogs in Daft using Python APIs\nDESCRIPTION: This snippet showcases how to create a Daft Catalog from different external catalog types like PyIceberg and Unity, register various tables from dictionaries, lists, and DataFrames, and manage table listings and retrievals. It provides practical examples for catalog manipulation and exploration, emphasizing the API's flexibility.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/catalogs.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport daft\n\nfrom daft import Catalog, Table\n\n# create a catalog from a pyiceberg catalog object\n_ = Catalog.from_iceberg(pyiceberg_catalog)\n\n# create a catalog from a unity catalog object\n_ = Catalog.from_unity(unity_catalog)\n\n# register various types as tables (all equivalent in functionality)\nexample_dict = { \"x\": [ 1, 2, 3 ] }\nexample_df = daft.from_pydict(example_dict)\nexample_table = Table.from_df(\"temp\", example_df)\n\n# create a catalog from a pydict mapping names to tables\nt = Catalog.from_pydict(\n    {\n        \"R\": example_dict,\n        \"S\": example_df,\n        \"T\": example_table,\n    }\n)\n\n# list tables with optional pattern\ncatalog.list_tables(pattern=None)\n\"\"\"\n['R', 'S', 'T']\n\"\"\"\n\n# retrieve a table by name\ntable_t = catalog.get_table(\"T\")\n\n# display table\ntable_t.show()\n\"\"\"\n╭───────╮\n│ x     │\n│ ---   │\n│ Int64 │\n╞═══════╡\n│ 1     │\n├╌╌╌╌╌╌╌┤\n│ 2     │\n├╌╌╌╌╌╌╌┤\n│ 3     │\n╰───────╯\n\"\"\"\n```\n\n----------------------------------------\n\nTITLE: Reading Entire Dataset from Hugging Face (Python)\nDESCRIPTION: This snippet reads an entire public dataset from Hugging Face into a Daft DataFrame. It uses the `daft.read_parquet()` method with the `hf://datasets/` protocol.  The dataset is specified by the username and dataset name. It requires the `daft` library to be installed.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/huggingface.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport daft\n\ndf = daft.read_parquet(\"hf://datasets/username/dataset_name\")\n```\n\n----------------------------------------\n\nTITLE: Showing Daft DataFrame with Downloaded Images (Python)\nDESCRIPTION: This line displays a preview of the first 5 rows of the DataFrame, including the newly added \"image\" column containing the decoded image data. This verifies that the image download and decoding step was successful.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/flyte/notebook.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfiltered_df.show(5)\n```\n\n----------------------------------------\n\nTITLE: Reading Iceberg Table with Pandas\nDESCRIPTION: Reads data from an Iceberg table into a Pandas DataFrame.  This snippet demonstrates eager materialization, loading all table data into memory.  It uses the `scan().to_pandas()` method to achieve this, and the output will be a Pandas DataFrame.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/iceberg_summit_2024.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntable.scan().to_pandas()\n```\n\n----------------------------------------\n\nTITLE: Installing Daft with Hudi Support - Bash\nDESCRIPTION: This snippet demonstrates how to install Daft with Hudi support. This command installs the necessary dependencies to enable reading Hudi tables with Daft. It uses pip, the Python package installer, to install the Daft library along with the hudi extra, indicated by `[hudi]`.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/hudi.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U \"daft[hudi]\"\n\n```\n\n----------------------------------------\n\nTITLE: Setting Catalog and Namespace Using USE Statement in SQL\nDESCRIPTION: This SQL snippet sets both the current_catalog to 'my_catalog' and current_namespace to 'my_namespace' for the session using the USE statement. Both the catalog and namespace must already exist, otherwise an error is raised. This command changes both configuration parameters, enabling scoping of subsequent queries.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql/statements/use.md#_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nUSE my_catalog.my_namespace;\n```\n\n----------------------------------------\n\nTITLE: Setting CI Variable\nDESCRIPTION: Sets a boolean variable `CI` to `False`. This variable is used to conditionally skip certain code execution, possibly related to Continuous Integration workflows, like skipping notebook execution to avoid hitting non-public buckets.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/iceberg_summit_2024.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nCI = False\n```\n\n----------------------------------------\n\nTITLE: Selecting Specific Columns from a Table Using SELECT in SQL\nDESCRIPTION: Illustrates selecting specific columns `a`, `b`, and `c` from a table `T`, allowing precise retrieval of only needed data fields. This reduces data output and can improve query clarity.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql/statements/select.md#_snippet_2\n\nLANGUAGE: sql\nCODE:\n```\nSELECT a, b, c FROM T;\n```\n\n----------------------------------------\n\nTITLE: Writing Data to Iceberg Table\nDESCRIPTION: Writes the data from the Daft dataframe to the Iceberg table. It uses the `write_iceberg` method to store the data. The `mode=\"overwrite\"` parameter specifies that existing data in the table should be replaced.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/iceberg_summit_2024.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf.write_iceberg(table, mode=\"overwrite\")\n```\n\n----------------------------------------\n\nTITLE: Attaching and Detaching Catalogs in Daft Sessions using Python\nDESCRIPTION: Demonstrates how to attach an existing Daft Catalog object to a Session using `sess.attach()`. It shows that attaching a catalog to an empty session automatically sets it as the `current_catalog`. A commented-out example shows how to detach a catalog using `sess.detach_catalog()`. Assumes a `catalog` object (e.g., created in the setup snippet) and a `sess` object exist.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sessions.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# attach makes it possible to use existing catalogs in the session\nsess.attach(catalog)\n\n# check the current catalog was set automatically\nsess.current_catalog()\n\"\"\"\nCatalog('default')\n\"\"\"\n\n# detach would remove the catalog\n# sess.detach_catalog(\"default\")\n```\n\n----------------------------------------\n\nTITLE: Selecting Specific Columns (Python)\nDESCRIPTION: Applies a selection operation to the DataFrame `df`, keeping only the 'path' column and discarding others. This is a lazy operation added to the query plan.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/image_querying/top_n_red_color.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndf = df.select(\"path\")\n```\n\n----------------------------------------\n\nTITLE: Displaying the Structured Daft DataFrame in Python\nDESCRIPTION: This simple command outputs the current, structured Daft DataFrame for user inspection or further processing. Input: DataFrame object. Output: Printed/displayed DataFrame.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/data-ai-summit-2024.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndf\n```\n\n----------------------------------------\n\nTITLE: Loading Data & Label Mapping - Python\nDESCRIPTION: This code snippet defines a list of tuples containing image class IDs and their human-readable names.  It then creates two dictionaries, `classes_human_readable` and `classes_id`, that map the class IDs to names and integer identifiers respectively. This is a pre-processing step for the data labeling.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/3-pytorch-ray-single-node-training.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n    (\"n06596364\", \"comic_book\"),\n    (\"n06785654\", \"crossword_puzzle\"),\n    (\"n06794110\", \"street_sign\"),\n    (\"n06874185\", \"traffic_light\"),\n    (\"n07248320\", \"book_jacket\"),\n    (\"n07565083\", \"menu\"),\n    (\"n07579787\", \"plate\"),\n    (\"n07583066\", \"guacamole\"),\n    (\"n07584110\", \"consomme\"),\n    (\"n07590611\", \"hot_pot\"),\n    (\"n07613480\", \"trifle\"),\n    (\"n07614500\", \"ice_cream\"),\n    (\"n07615774\", \"ice_lolly\"),\n    (\"n07684084\", \"French_loaf\"),\n    (\"n07693725\", \"bagel\"),\n    (\"n07695742\", \"pretzel\"),\n    (\"n07697313\", \"cheeseburger\"),\n    (\"n07697537\", \"hotdog\"),\n    (\"n07711569\", \"mashed_potato\"),\n    (\"n07714571\", \"head_cabbage\"),\n    (\"n07714990\", \"broccoli\"),\n    (\"n07715103\", \"cauliflower\"),\n    (\"n07716358\", \"zucchini\"),\n    (\"n07716906\", \"spaghetti_squash\"),\n    (\"n07717410\", \"acorn_squash\"),\n    (\"n07717556\", \"butternut_squash\"),\n    (\"n07718472\", \"cucumber\"),\n    (\"n07718747\", \"artichoke\"),\n    (\"n07720875\", \"bell_pepper\"),\n    (\"n07730033\", \"cardoon\"),\n    (\"n07734744\", \"mushroom\"),\n    (\"n07742313\", \"Granny_Smith\"),\n    (\"n07745940\", \"strawberry\"),\n    (\"n07747607\", \"orange\"),\n    (\"n07749582\", \"lemon\"),\n    (\"n07753113\", \"fig\"),\n    (\"n07753275\", \"pineapple\"),\n    (\"n07753592\", \"banana\"),\n    (\"n07754684\", \"jackfruit\"),\n    (\"n07760859\", \"custard_apple\"),\n    (\"n07768694\", \"pomegranate\"),\n    (\"n07802026\", \"hay\"),\n    (\"n07831146\", \"carbonara\"),\n    (\"n07836838\", \"chocolate_sauce\"),\n    (\"n07860988\", \"dough\"),\n    (\"n07871810\", \"meat_loaf\"),\n    (\"n07873807\", \"pizza\"),\n    (\"n07875152\", \"potpie\"),\n    (\"n07880968\", \"burrito\"),\n    (\"n07892512\", \"red_wine\"),\n    (\"n07920052\", \"espresso\"),\n    (\"n07930864\", \"cup\"),\n    (\"n07932039\", \"eggnog\"),\n    (\"n09193705\", \"alp\"),\n    (\"n09229709\", \"bubble\"),\n    (\"n09246464\", \"cliff\"),\n    (\"n09256479\", \"coral_reef\"),\n    (\"n09288635\", \"geyser\"),\n    (\"n09332890\", \"lakeside\"),\n    (\"n09399592\", \"promontory\"),\n    (\"n09421951\", \"sandbar\"),\n    (\"n09428293\", \"seashore\"),\n    (\"n09468604\", \"valley\"),\n    (\"n09472597\", \"volcano\"),\n    (\"n09835506\", \"ballplayer\"),\n    (\"n10148035\", \"groom\"),\n    (\"n10565667\", \"scuba_diver\"),\n    (\"n11879895\", \"rapeseed\"),\n    (\"n11939491\", \"daisy\"),\n    (\"n12057211\", \"yellow_lady's_slipper\"),\n    (\"n12144580\", \"corn\"),\n    (\"n12267677\", \"acorn\"),\n    (\"n12620546\", \"hip\"),\n    (\"n12768682\", \"buckeye\"),\n    (\"n12985857\", \"coral_fungus\"),\n    (\"n12998815\", \"agaric\"),\n    (\"n13037406\", \"gyromitra\"),\n    (\"n13040303\", \"stinkhorn\"),\n    (\"n13044778\", \"earthstar\"),\n    (\"n13052670\", \"hen-of-the-woods\"),\n    (\"n13054560\", \"bolete\"),\n    (\"n13133613\", \"ear\"),\n    (\"n15075141\", \"toilet_tissue\"),\n]\nclasses_human_readable = {v0: v1 for (v0, v1) in classes}\nclasses_id = {v0: int(k) for k, (v0, _) in enumerate(classes)}\n```\n\n----------------------------------------\n\nTITLE: Reading and collecting inference results from Parquet\nDESCRIPTION: Reads previously saved Parquet results of distributed inference, and collects the data for further inspection or analysis.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/2-distributed-batch-inference.ipynb#_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ndaft.read_parquet(\"my_results.parquet\").collect()\n```\n\n----------------------------------------\n\nTITLE: Repartitioning a DataFrame with Hash Partitioning in Python\nDESCRIPTION: Demonstrates how to repartition a DataFrame into 8 partitions using hash partitioning on column 'x', and then inspect the resulting execution plan to confirm the changes.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/advanced/partitioning.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf = df.repartition(8, daft.col(\"x\"))\ndf.explain(show_all=True)\n```\n\n----------------------------------------\n\nTITLE: Loading MNIST Data with Daft\nDESCRIPTION: Loads the MNIST dataset from a JSON file hosted on GitHub into a Daft DataFrame. The `daft.read_json` function is used to read the data directly from the URL, creating a DataFrame containing image data and labels.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/mnist.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport daft\nfrom daft import DataType, col, udf\n\nURL = \"https://github.com/Eventual-Inc/mnist-json/raw/master/mnist_handwritten_test.json.gz\"\nimages_df = daft.read_json(URL)\n```\n\n----------------------------------------\n\nTITLE: Displaying Daft Dataframe Contents in Python\nDESCRIPTION: This simple snippet shows the data contained in a Daft dataframe by calling its show() method, typically used after transformations to observe results.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/linkedin-03-05-2024.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nice_df.show()\n```\n\n----------------------------------------\n\nTITLE: Daft Session Object Definition\nDESCRIPTION: This code snippet defines the Daft `Session` object. It allows attaching catalogs and tables, and creating temporary objects accessible through both the Python and SQL APIs.  The `options` block with `filters` is used to control which attributes or methods are included in the documentation.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/api/sessions.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n::: daft.session.Session\n    options:\n        filters: [\"!^_\" ]\n```\n\n----------------------------------------\n\nTITLE: Creating a DataFrame from Python List with Daft - Python\nDESCRIPTION: Explains initializing a Daft DataFrame from a list of Python dictionaries, enabling convenient in-memory data manipulation. Only requires native Python data types and the Daft library. Expects a list of dictionaries as input, each representing a row; outputs a DataFrame with corresponding columns. Large in-memory lists may lead to memory constraints.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/api/dataframe_creation.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport daft\n\nrecords = [{\"a\": 1, \"b\": 2}, {\"a\": 3, \"b\": 4}]\ndf = daft.from_pylist(records)\n```\n\n----------------------------------------\n\nTITLE: Installing Daft with Unity Catalog Support Using pip - Bash\nDESCRIPTION: Installs the Daft library with Unity Catalog integration enabled using pip. Requires Python and pip to be pre-installed. This command adds backend dependencies specific to Unity Catalog support. No input parameters other than the optional specification of Python environment. The output is the installation of Daft and required extras.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/unity_catalog.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install daft[unity]\n```\n\n----------------------------------------\n\nTITLE: Grouping and Aggregating Data\nDESCRIPTION: Demonstrates how to group data by a column and compute aggregate statistics like count.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/10min.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# select only columns for grouping\ngrouping_df = df.select(df[\"country\"], df[\"first_name\"].alias(\"counts\"))\n\n# groupby country column and count the number of countries\ngrouping_df.groupby(df[\"country\"]).count().show()\n```\n\n----------------------------------------\n\nTITLE: Reading Partitioned Parquet Data from S3 with Anonymous Access using Daft\nDESCRIPTION: This code configures Daft's IO settings for anonymous S3 access using `daft.set_planning_config` and `daft.io.IOConfig`. It then reads potentially partitioned Parquet files from a public S3 bucket using `daft.read_parquet` with a glob pattern. The resulting DataFrame `df` is lazy; displaying it shows the schema but indicates data isn't materialized until an action like `collect()` or `show()` is called.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/quickstart.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Set IO Configurations to use anonymous data access mode\ndaft.set_planning_config(default_io_config=daft.io.IOConfig(s3=daft.io.S3Config(anonymous=True)))\n\ndf = daft.read_parquet(\"s3://daft-public-data/tutorials/10-min/sample-data-dog-owners-partitioned.pq/**\")\ndf\n```\n\n----------------------------------------\n\nTITLE: Show tables matching a pattern\nDESCRIPTION: This SQL snippet demonstrates how to list tables in the current catalog that match a specified pattern using the LIKE clause with the SHOW TABLES statement.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql/statements/show.md#_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nSHOW TABLES LIKE 'foo'\n```\n\n----------------------------------------\n\nTITLE: Running SQL Queries with Daft CLI - Bash\nDESCRIPTION: Shows the command to execute SQL queries against data managed by Daft, utilizing Postgres dialect syntax. It demonstrates how to run arbitrary SQL expressions from the CLI to retrieve or analyze data within Daft-managed datasets.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/distributed.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ndaft sql -- \"\\\"SELECT * FROM my_table\\\"\"\n```\n\n----------------------------------------\n\nTITLE: Accessing Ray Dashboard via Daft CLI - Bash\nDESCRIPTION: Illustrates the commands for connecting to the Ray dashboard to monitor cluster health, job status, and resource utilization. Access methods differ by cluster mode: Provisioned clusters require SSH keys, while BYOC clusters use Kubernetes credentials.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/distributed.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\n# For Provisioned Mode\ndaft provisioned connect\n\n# For BYOC Mode\ndaft byoc connect\n```\n\n----------------------------------------\n\nTITLE: Checking DataFrame Partitioning with explain() in Python\nDESCRIPTION: Shows how to inspect the partitioning of a DataFrame loaded from Parquet files using the explain() method with show_all=True parameter to display the physical execution plan.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/advanced/partitioning.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.read_parquet(\"s3://bucket/path_to_100_parquet_files/**\")\ndf.explain(show_all=True)\n```\n\n----------------------------------------\n\nTITLE: Installing Daft with pip\nDESCRIPTION: Command to install the Daft library using pip package manager.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/10min.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install daft\n```\n\n----------------------------------------\n\nTITLE: Setting the CI flag to disable execution in CI environments\nDESCRIPTION: Initializes a boolean flag to indicate if the code is running in a Continuous Integration environment. When set to True, subsequent code skips execution to prevent issues with non-public resources.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/2-distributed-batch-inference.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nCI = False\n```\n\n----------------------------------------\n\nTITLE: Provisioned Mode AWS Cluster Setup - Bash and Shell\nDESCRIPTION: Provides a sequence of Bash commands to prepare AWS environment for Daft Provisioned mode. Includes configuring AWS SSO, logging in, generating an SSH RSA key pair for cluster access, importing the public key into AWS EC2, and setting appropriate file permissions. These steps must be done prior to provisioning and managing clusters via Daft CLI.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/distributed.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Configure your SSO\naws configure sso\n\n# Login to your SSO\naws sso login\n```\n\nLANGUAGE: bash\nCODE:\n```\n# Generate key pair\nssh-keygen -t rsa -b 2048 -f ~/.ssh/daft-key\n\n# Import to AWS\naws ec2 import-key-pair \\\n  --key-name \"daft-key\" \\\n  --public-key-material fileb://~/.ssh/daft-key.pub\n\n# Set permissions\nchmod 600 ~/.ssh/daft-key\n```\n\n----------------------------------------\n\nTITLE: Installing Daft for Legacy CPUs using daft-lts Package in Bash\nDESCRIPTION: Installs a legacy-compatible version of Daft named daft-lts, designed for CPUs lacking support for advanced instructions such as AVX, which cause errors with the standard package. This package uses a more limited CPU instruction set for compatibility, at the cost of reduced performance for vectorized operations.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/install.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install -U daft-lts\n```\n\n----------------------------------------\n\nTITLE: Installing Daft for Benchmarking (bash)\nDESCRIPTION: This bash script installs the Daft library, which is the core component to be benchmarked. It uses `pip install` to install Daft. The installation method depends on where the Daft library is located either a released wheel or a local build. The output of this command will be the installation of the Daft python library. This command must be executed within an active virtual environment (as set up previously).\nSOURCE: https://github.com/eventual-inc/daft/blob/main/benchmarking/parquet/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install daft\n```\n\n----------------------------------------\n\nTITLE: Install Daft with SQL Support (Bash)\nDESCRIPTION: This command installs Daft with the `sql` extra, which includes the necessary dependencies for SQL support, such as ConnectorX, SQLAlchemy, and SQLGlot.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/sql.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -U \"daft[sql]\"\n```\n\n----------------------------------------\n\nTITLE: Configuring and Rendering a Benchmark Plot with Plotly - JavaScript\nDESCRIPTION: This snippet embeds and configures an interactive bar chart with Plotly.js to visualize TPC-H 100 Scale Factor benchmark results for different engines (Daft, Spark, Dask, Modin). It dynamically loads the Plotly library, sets up MathJax configuration if needed, and renders the plot within a specified DOM element, generating the visualization based on provided axes, data arrays, colors, and layout settings. Dependencies include the Plotly.js library (loaded via CDN), and it expects a target DOM node with a specific ID for rendering; no server-side components are required, but the surrounding HTML must be present.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/resources/benchmarks/tpch.md#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\nwindow.PlotlyConfig = {MathJaxConfig: 'local'};\n<script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.20.0.min.js\"></script>\n<div id=\"78330a19-a541-460b-bd9f-217b9d4cd137\" class=\"plotly-graph-div\" style=\"height:100%; width:100%;\"></div>\n<script type=\"text/javascript\">\nwindow.PLOTLYENV=window.PLOTLYENV || {};\nif (document.getElementById(\"78330a19-a541-460b-bd9f-217b9d4cd137\")) {\n    Plotly.newPlot(\n        \"78330a19-a541-460b-bd9f-217b9d4cd137\",\n        [\n            {\"marker\":{\"color\":\"rgba(108, 11, 169, 1)\"},\"name\":\"Daft\",\"x\":[\"Q1\",\"Q2\",\"Q3\",\"Q4\",\"Q5\",\"Q6\",\"Q7\",\"Q8\",\"Q9\",\"Q10\"],\"y\":[1.0666666666666667,0.7666666666666667,0.9833333333333333,1.05,1.9666666666666666,0.6333333333333333,1.1666666666666667,2.25,2.183333333333333,1.0166666666666666],\"type\":\"bar\",\"textposition\":\"inside\"},\n            {\"hovertext\":[\"5.6x Slower\",\"1.1x Slower\",\"5.1x Slower\",\"2.8x Slower\",\"2.0x Slower\",\"9.7x Slower\",\"4.3x Slower\",\"2.0x Slower\",\"2.3x Slower\",\"4.8x Slower\"],\"marker\":{\"color\":\"rgba(226,90,28, 0.75)\"},\"name\":\"Spark\",\"x\":[\"Q1\",\"Q2\",\"Q3\",\"Q4\",\"Q5\",\"Q6\",\"Q7\",\"Q8\",\"Q9\",\"Q10\"],\"y\":[5.991666666666666,0.8716666666666666,4.996666666666667,2.955,3.8583333333333334,6.135000000000001,4.985,4.428333333333333,5.051666666666667,4.863333333333333],\"type\":\"bar\",\"textposition\":\"inside\"},\n            {\"hovertext\":[\"4.2x Slower\",\"1.4x Slower\",\"6.9x Slower\",\"13.0x Slower\",\"8.2x Slower\",\"6.1x Slower\",\"6.8x Slower\",\"3.6x Slower\",\"11.8x Slower\",\"12.1x Slower\"],\"marker\":{\"color\":\"rgba(255,193,30, 0.75)\"},\"name\":\"Dask\",\"x\":[\"Q1\",\"Q2\",\"Q3\",\"Q4\",\"Q5\",\"Q6\",\"Q7\",\"Q8\",\"Q9\",\"Q10\"],\"y\":[4.456666666666666,1.0983333333333334,6.748333333333333,13.615,16.215,3.8366666666666664,7.96,8.148333333333333,25.790000000000003,12.306666666666667],\"type\":\"bar\",\"textposition\":\"inside\"},\n            {\"hovertext\":[\"29.1x Slower\",\"12.5x Slower\",\"nanx Slower\",\"48.6x Slower\",\"nanx Slower\",\"87.7x Slower\",\"nanx Slower\",\"nanx Slower\",\"nanx Slower\",\"52.7x Slower\"],\"marker\":{\"color\":\"rgba(0,173,233, 0.6)\"},\"name\":\"Modin\",\"x\":[\"Q1\",\"Q2\",\"Q3\",\"Q4\",\"Q5\",\"Q6\",\"Q7\",\"Q8\",\"Q9\",\"Q10\"],\"y\":[31.066666666666666,9.616666666666667,null,51.05,null,55.53333333333333,null,null,null,53.6],\"type\":\"bar\",\"textposition\":\"inside\"}\n        ],\n        {\n          \"template\":{\n             /* Large Plotly template object omitted for brevity; configures plot palette, axes, and styles */\n          },\n          \"title\":{\"text\":\"TPCH 100 Scale Factor - 4 Nodes (lower is better)\"},\n          \"yaxis\":{\"title\":{\"text\":\"Time (minutes)\"}},\n          \"xaxis\":{\"title\":{\"text\":\"TPCH Question\"}},\n          \"uniformtext\":{\"minsize\":8,\"mode\":\"hide\"}\n        },\n        {\"displayModeBar\": false, \"responsive\": true}\n    );\n}\n</script>\n```\n\n----------------------------------------\n\nTITLE: Reading Parquet Data with Daft in Python\nDESCRIPTION: Reads a Parquet file located at the specified S3 path (`PARQUET_PATH`) into a Daft DataFrame named `parquet_df`. It utilizes the previously defined `IO_CONFIG` object to handle S3 access credentials and configuration.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/text_to_image/text_to_image_generation.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport daft\n\nparquet_df = daft.read_parquet(PARQUET_PATH, io_config=IO_CONFIG)\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray with Dependencies if Remote Cluster is Used in Python\nDESCRIPTION: Establishes connection to a Ray cluster when running remotely, specifying necessary Python package dependencies in the runtime environment. Ensures that the remote Ray cluster has all required libraries installed for the pipeline.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/3-pytorch-ray-single-node-training.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport ray\n\n# Set up connection to Ray cluster if USE_REMOTE_CLUSTER=True\nif USE_REMOTE_CLUSTER:\n    ray.init(\n        address=RAY_ADDRESS,\n        runtime_env={\n            \"pip\": [\n                \"daft\",\n                \"torch\",\n                \"torchvision\",\n            ]\n        },\n    )\n    print(ray.available_resources())\n```\n\n----------------------------------------\n\nTITLE: Boolean DataFrame - Python\nDESCRIPTION: Initializes a Daft DataFrame with a Boolean column named 'C' containing the values True, False, and True. This demonstrates the creation of a DataFrame with logical data.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/core_concepts.md#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.from_pydict({\"C\": [True, False, True]})\n```\n\n----------------------------------------\n\nTITLE: Declaring Delimited SQL Identifiers - SQL\nDESCRIPTION: This snippet illustrates the use of double quotes to declare a delimited SQL identifier. Delimited identifiers preserve case and allow usage of special characters or keywords as identifiers, and are required when such naming is needed. No additional dependencies. Input: delimited identifier string. Output: valid identifier usable in Daft SQL queries.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql/identifiers.md#_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\n-- delimited identifier\n\"abc\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Daft DataFrame\nDESCRIPTION: Displays a few rows of the Daft DataFrame. This is a quick way to view the data loaded from the Iceberg table using Daft.  It calls the `show()` method on the Daft DataFrame, which prints the first few records.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/iceberg_summit_2024.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.read_iceberg(table)\ndf.show()\n```\n\n----------------------------------------\n\nTITLE: Initializing Ray\nDESCRIPTION: Initializes the Ray cluster to allow for distributed computation. This uses `ray.init()` to connect to a Ray instance running locally on the specified address, enabling the Daft dataframes to be processed in a distributed fashion, which is key to handling very large iceberg tables.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/iceberg_summit_2024.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport ray\n\nray.init(address=\"ray://localhost:10001\")\n```\n\n----------------------------------------\n\nTITLE: Skipping CI Execution\nDESCRIPTION: Conditionally skips the remainder of the notebook execution. This is done if the `CI` variable is set to `True`, which is likely used to optimize the CI/CD pipeline by avoiding operations that may be slow, or that depend on environment configuration that is not available.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/iceberg_summit_2024.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nif CI:\n    import sys\n\n    sys.exit()\n```\n\n----------------------------------------\n\nTITLE: Using Qualified Identifiers with Mixed Parts - SQL\nDESCRIPTION: This snippet shows a qualified identifier where one or more components are delimited, allowing for mixed case or special character usage in specific segments. Useful for accessing nested elements in complex schemas. Input: mixed regular and delimited identifier parts. Output: reference to deeply nested columns or tables.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql/identifiers.md#_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\n-- qualified identifier with mixed parts\na.\"b\".c\n```\n\n----------------------------------------\n\nTITLE: Listing Files with boto3\nDESCRIPTION: This snippet demonstrates listing files in an S3 bucket using the `boto3` library.  It retrieves a list of objects and handles pagination using `NextContinuationToken`.  The code is meant for comparison with Daft's listing method.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/pydata_global_2023.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\nimport boto3\n\nclient = boto3.client(\"s3\")\nkwargs = {\"Bucket\": \"daft-public-datasets\", \"Prefix\": \"tpch-lineitem/10k-1mb-csv-files\"}\nresponse = client.list_objects_v2(**kwargs)\ndata = response[\"Contents\"]\ntoken = response.get(\"NextContinuationToken\")\n\nwhile token is not None:\n    if token is not None:\n        kwargs[\"ContinuationToken\"] = token\n    response = client.list_objects_v2(**kwargs)\n    data.extend(response[\"Contents\"])\n    token = response.get(\"NextContinuationToken\")\n\nprint(f\"Retrieved {len(data)} results.\")\n```\n\n----------------------------------------\n\nTITLE: Setting Catalog Using USE Statement in SQL\nDESCRIPTION: This snippet sets the session-level current_catalog to 'my_catalog' using the USE statement in SQL. It requires the specified catalog to already exist in the database system. The input parameter is the catalog name, and when executed, only current_catalog is changed; current_namespace becomes NULL unless otherwise specified.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/sql/statements/use.md#_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nUSE my_catalog;\n```\n\n----------------------------------------\n\nTITLE: Reading Private Dataset (Fails, Python)\nDESCRIPTION: This snippet attempts to read a private dataset directly, which will result in an error. It demonstrates that directly specifying the dataset name will fail in the case of private datasets. The assumption is that the dataset is not automatically converted to parquet format.  It depends on previous `io_config` setup.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/integrations/huggingface.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf = daft.read_parquet(\"hf://datasets/username/my_private_dataset\", io_config=io_config) # Errors\n```\n\n----------------------------------------\n\nTITLE: Installing Daft Ray Backend and Pillow Dependencies with pip in shell\nDESCRIPTION: Installs the Daft library with the optional Ray backend along with the Pillow library for image processing. The '!pip install' commands are intended to be used in an interactive environment such as Jupyter or Colab notebooks to ensure the required packages are available before running Python code that depends on them.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/text_to_image/using_cloud_with_ray.ipynb#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n!pip install daft[ray]\n!pip install Pillow\n```\n\n----------------------------------------\n\nTITLE: Installing Daft and Dependencies\nDESCRIPTION: Installs the Daft library and other required dependencies such as Pillow, Torch, and Torchvision using pip. These packages are necessary for data manipulation, image processing, and deep learning model execution.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/mnist.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install daft\n%pip install Pillow torch torchvision\n```\n\n----------------------------------------\n\nTITLE: Running Daft Benchmarks - Parquet Feature Underperformance (bash)\nDESCRIPTION: This bash command runs pytest benchmarks to find Parquet features where Daft underperforms.  It uses the `-m benchmark` flag to run tests marked as benchmarks, `--benchmark-group-by=group` groups results by group, and `-k daft` filters the benchmarks related to Daft specifically. This command requires `pytest` and benchmarking packages to be installed. The output will be the benchmark results.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/benchmarking/parquet/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npytest benchmarking/parquet/ -m benchmark --benchmark-group-by=group -k daft\n```\n\n----------------------------------------\n\nTITLE: Defining ImageNet Class Mappings in Python\nDESCRIPTION: This Python code snippet defines a list called `classes`. Each element in the list is a tuple containing two strings: the first is the ImageNet synset ID (e.g., 'n01440764'), and the second is the corresponding human-readable class name (e.g., 'tench'). This list provides a mapping used for labeling image classification results.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/3-pytorch-ray-single-node-training.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Map class names to human-readable names and numeric IDs\nclasses = [\n    (\"n01440764\", \"tench\"),\n    (\"n01443537\", \"goldfish\"),\n    (\"n01484850\", \"great_white_shark\"),\n    (\"n01491361\", \"tiger_shark\"),\n    (\"n01494475\", \"hammerhead\"),\n    (\"n01496331\", \"electric_ray\"),\n    (\"n01498041\", \"stingray\"),\n    (\"n01514668\", \"cock\"),\n    (\"n01514859\", \"hen\"),\n    (\"n01518878\", \"ostrich\"),\n    (\"n01530575\", \"brambling\"),\n    (\"n01531178\", \"goldfinch\"),\n    (\"n01532829\", \"house_finch\"),\n    (\"n01534433\", \"junco\"),\n    (\"n01537544\", \"indigo_bunting\"),\n    (\"n01558993\", \"robin\"),\n    (\"n01560419\", \"bulbul\"),\n    (\"n01580077\", \"jay\"),\n    (\"n01582220\", \"magpie\"),\n    (\"n01592084\", \"chickadee\"),\n    (\"n01601694\", \"water_ouzel\"),\n    (\"n01608432\", \"kite\"),\n    (\"n01614925\", \"bald_eagle\"),\n    (\"n01616318\", \"vulture\"),\n    (\"n01622779\", \"great_grey_owl\"),\n    (\"n01629819\", \"European_fire_salamander\"),\n    (\"n01630670\", \"common_newt\"),\n    (\"n01631663\", \"eft\"),\n    (\"n01632458\", \"spotted_salamander\"),\n    (\"n01632777\", \"axolotl\"),\n    (\"n01641577\", \"bullfrog\"),\n    (\"n01644373\", \"tree_frog\"),\n    (\"n01644900\", \"tailed_frog\"),\n    (\"n01664065\", \"loggerhead\"),\n    (\"n01665541\", \"leatherback_turtle\"),\n    (\"n01667114\", \"mud_turtle\"),\n    (\"n01667778\", \"terrapin\"),\n    (\"n01669191\", \"box_turtle\"),\n    (\"n01675722\", \"banded_gecko\"),\n    (\"n01677366\", \"common_iguana\"),\n    (\"n01682714\", \"American_chameleon\"),\n    (\"n01685808\", \"whiptail\"),\n    (\"n01687978\", \"agama\"),\n    (\"n01688243\", \"frilled_lizard\"),\n    (\"n01689811\", \"alligator_lizard\"),\n    (\"n01692333\", \"Gila_monster\"),\n    (\"n01693334\", \"green_lizard\"),\n    (\"n01694178\", \"African_chameleon\"),\n    (\"n01695060\", \"Komodo_dragon\"),\n    (\"n01697457\", \"African_crocodile\"),\n    (\"n01698640\", \"American_alligator\"),\n    (\"n01704323\", \"triceratops\"),\n    (\"n01728572\", \"thunder_snake\"),\n    (\"n01728920\", \"ringneck_snake\"),\n    (\"n01729322\", \"hognose_snake\"),\n    (\"n01729977\", \"green_snake\"),\n    (\"n01734418\", \"king_snake\"),\n    (\"n01735189\", \"garter_snake\"),\n    (\"n01737021\", \"water_snake\"),\n    (\"n01739381\", \"vine_snake\"),\n    (\"n01740131\", \"night_snake\"),\n    (\"n01742172\", \"boa_constrictor\"),\n    (\"n01744401\", \"rock_python\"),\n    (\"n01748264\", \"Indian_cobra\"),\n    (\"n01749939\", \"green_mamba\"),\n    (\"n01751748\", \"sea_snake\"),\n    (\"n01753488\", \"horned_viper\"),\n    (\"n01755581\", \"diamondback\"),\n    (\"n01756291\", \"sidewinder\"),\n    (\"n01768244\", \"trilobite\"),\n    (\"n01770081\", \"harvestman\"),\n    (\"n01770393\", \"scorpion\"),\n    (\"n01773157\", \"black_and_gold_garden_spider\"),\n    (\"n01773549\", \"barn_spider\"),\n    (\"n01773797\", \"garden_spider\"),\n    (\"n01774384\", \"black_widow\"),\n    (\"n01774750\", \"tarantula\"),\n    (\"n01775062\", \"wolf_spider\"),\n    (\"n01776313\", \"tick\"),\n    (\"n01784675\", \"centipede\"),\n    (\"n01795545\", \"black_grouse\"),\n    (\"n01796340\", \"ptarmigan\"),\n    (\"n01797886\", \"ruffed_grouse\"),\n    (\"n01798484\", \"prairie_chicken\"),\n    (\"n01806143\", \"peacock\"),\n    (\"n01806567\", \"quail\"),\n    (\"n01807496\", \"partridge\"),\n    (\"n01817953\", \"African_grey\"),\n    (\"n01818515\", \"macaw\"),\n    (\"n01819313\", \"sulphur-crested_cockatoo\"),\n    (\"n01820546\", \"lorikeet\"),\n    (\"n01824575\", \"coucal\"),\n    (\"n01828970\", \"bee_eater\"),\n    (\"n01829413\", \"hornbill\"),\n    (\"n01833805\", \"hummingbird\"),\n    (\"n01843065\", \"jacamar\"),\n    (\"n01843383\", \"toucan\"),\n    (\"n01847000\", \"drake\"),\n    (\"n01855032\", \"red-breasted_merganser\"),\n    (\"n01855672\", \"goose\"),\n    (\"n01860187\", \"black_swan\"),\n    (\"n01871265\", \"tusker\"),\n    (\"n01872401\", \"echidna\"),\n    (\"n01873310\", \"platypus\"),\n    (\"n01877812\", \"wallaby\"),\n    (\"n01882714\", \"koala\"),\n    (\"n01883070\", \"wombat\"),\n    (\"n01910747\", \"jellyfish\"),\n    (\"n01914609\", \"sea_anemone\"),\n    (\"n01917289\", \"brain_coral\"),\n    (\"n01924916\", \"flatworm\"),\n    (\"n01930112\", \"nematode\"),\n    (\"n01943899\", \"conch\"),\n    (\"n01944390\", \"snail\"),\n    (\"n01945685\", \"slug\"),\n    (\"n01950731\", \"sea_slug\"),\n    (\"n01955084\", \"chiton\"),\n    (\"n01968897\", \"chambered_nautilus\"),\n    (\"n01978287\", \"Dungeness_crab\"),\n    (\"n01978455\", \"rock_crab\"),\n    (\"n01980166\", \"fiddler_crab\"),\n    (\"n01981276\", \"king_crab\"),\n    (\"n01983481\", \"American_lobster\"),\n    (\"n01984695\", \"spiny_lobster\"),\n    (\"n01985128\", \"crayfish\"),\n    (\"n01986214\", \"hermit_crab\"),\n    (\"n01990800\", \"isopod\"),\n    (\"n02002556\", \"white_stork\"),\n    (\"n02002724\", \"black_stork\"),\n    (\"n02006656\", \"spoonbill\"),\n    (\"n02007558\", \"flamingo\"),\n    (\"n02009229\", \"little_blue_heron\"),\n    (\"n02009912\", \"American_egret\"),\n    (\"n02011460\", \"bittern\"),\n    (\"n02012849\", \"crane\"),\n    (\"n02013706\", \"limpkin\"),\n    (\"n02017213\", \"European_gallinule\"),\n    (\"n02018207\", \"American_coot\"),\n    (\"n02018795\", \"bustard\"),\n    (\"n02025239\", \"ruddy_turnstone\"),\n    (\"n02027492\", \"red-backed_sandpiper\"),\n    (\"n02028035\", \"redshank\"),\n    (\"n02033041\", \"dowitcher\"),\n    (\"n02037110\", \"oystercatcher\"),\n    (\"n02051845\", \"pelican\"),\n    (\"n02056570\", \"king_penguin\"),\n    (\"n02058221\", \"albatross\"),\n    (\"n02066245\", \"grey_whale\"),\n    (\"n02071294\", \"killer_whale\"),\n    (\"n02074367\", \"dugong\"),\n    (\"n02077923\", \"sea_lion\"),\n    (\"n02085620\", \"Chihuahua\"),\n    (\"n02085782\", \"Japanese_spaniel\"),\n    (\"n02085936\", \"Maltese_dog\"),\n    (\"n02086079\", \"Pekinese\"),\n    (\"n02086240\", \"Shih-Tzu\"),\n    (\"n02086646\", \"Blenheim_spaniel\"),\n    (\"n02086910\", \"papillon\"),\n    (\"n02087046\", \"toy_terrier\"),\n    (\"n02087394\", \"Rhodesian_ridgeback\"),\n    (\"n02088094\", \"Afghan_hound\"),\n    (\"n02088238\", \"basset\"),\n    (\"n02088364\", \"beagle\"),\n    (\"n02088466\", \"bloodhound\"),\n    (\"n02088632\", \"bluetick\"),\n    (\"n02089078\", \"black-and-tan_coonhound\"),\n    (\"n02089867\", \"Walker_hound\"),\n    (\"n02089973\", \"English_foxhound\"),\n    (\"n02090379\", \"redbone\"),\n    (\"n02090622\", \"borzoi\"),\n    (\"n02090721\", \"Irish_wolfhound\"),\n    (\"n02091032\", \"Italian_greyhound\"),\n    (\"n02091134\", \"whippet\"),\n    (\"n02091244\", \"Ibizan_hound\"),\n    (\"n02091467\", \"Norwegian_elkhound\"),\n    (\"n02091635\", \"otterhound\"),\n    (\"n02091831\", \"Saluki\"),\n    (\"n02092002\", \"Scottish_deerhound\"),\n    (\"n02092339\", \"Weimaraner\"),\n    (\"n02093256\", \"Staffordshire_bullterrier\"),\n    (\"n02093428\", \"American_Staffordshire_terrier\"),\n    (\"n02093647\", \"Bedlington_terrier\"),\n    (\"n02093754\", \"Border_terrier\"),\n    (\"n02093859\", \"Kerry_blue_terrier\"),\n    (\"n02093991\", \"Irish_terrier\"),\n    (\"n02094114\", \"Norfolk_terrier\"),\n    (\"n02094258\", \"Norwich_terrier\"),\n    (\"n02094433\", \"Yorkshire_terrier\"),\n    (\"n02095314\", \"wire-haired_fox_terrier\"),\n    (\"n02095570\", \"Lakeland_terrier\"),\n    (\"n02095889\", \"Sealyham_terrier\"),\n    (\"n02096051\", \"Airedale\"),\n    (\"n02096177\", \"cairn\"),\n    (\"n02096294\", \"Australian_terrier\"),\n    (\"n02096437\", \"Dandie_Dinmont\"),\n    (\"n02096585\", \"Boston_bull\"),\n    (\"n02097047\", \"miniature_schnauzer\"),\n    (\"n02097130\", \"giant_schnauzer\"),\n    (\"n02097209\", \"standard_schnauzer\"),\n    (\"n02097298\", \"Scotch_terrier\"),\n    (\"n02097474\", \"Tibetan_terrier\"),\n    (\"n02097658\", \"silky_terrier\"),\n    (\"n02098105\", \"soft-coated_wheaten_terrier\"),\n    (\"n02098286\", \"West_Highland_white_terrier\"),\n    (\"n02098413\", \"Lhasa\"),\n    (\"n02099267\", \"flat-coated_retriever\"),\n    (\"n02099429\", \"curly-coated_retriever\"),\n    (\"n02099601\", \"golden_retriever\"),\n    (\"n02099712\", \"Labrador_retriever\"),\n    (\"n02099849\", \"Chesapeake_Bay_retriever\"),\n    (\"n02100236\", \"German_short-haired_pointer\"),\n    (\"n02100583\", \"vizsla\"),\n    (\"n02100735\", \"English_setter\"),\n    (\"n02100877\", \"Irish_setter\"),\n    (\"n02101006\", \"Gordon_setter\"),\n    (\"n02101388\", \"Brittany_spaniel\"),\n    (\"n02101556\", \"clumber\"),\n    (\"n02102040\", \"English_springer\"),\n    (\"n02102177\", \"Welsh_springer_spaniel\"),\n    (\"n02102318\", \"cocker_spaniel\"),\n    (\"n02102480\", \"Sussex_spaniel\"),\n    (\"n02102973\", \"Irish_water_spaniel\"),\n    (\"n02104029\", \"kuvasz\"),\n    (\"n02104365\", \"schipperke\"),\n    (\"n02105056\", \"groenendael\"),\n    (\"n02105162\", \"malinois\"),\n    (\"n02105251\", \"briard\"),\n    (\"n02105412\", \"kelpie\"),\n    (\"n02105505\", \"komondor\"),\n    (\"n02105641\", \"Old_English_sheepdog\"),\n    (\"n02105855\", \"Shetland_sheepdog\"),\n    (\"n02106030\", \"collie\"),\n    (\"n02106166\", \"Border_collie\"),\n    (\"n02106382\", \"Bouvier_des_Flandres\"),\n    (\"n02106550\", \"Rottweiler\"),\n    (\"n02106662\", \"German_shepherd\"),\n    (\"n02107142\", \"Doberman\"),\n    (\"n02107312\", \"miniature_pinscher\"),\n    (\"n02107574\", \"Greater_Swiss_Mountain_dog\"),\n    (\"n02107683\", \"Bernese_mountain_dog\"),\n    (\"n02107908\", \"Appenzeller\"),\n    (\"n02108000\", \"EntleBucher\"),\n    (\"n02108089\", \"boxer\"),\n    (\"n02108422\", \"bull_mastiff\"),\n    (\"n02108551\", \"Tibetan_mastiff\"),\n    (\"n02108915\", \"French_bulldog\"),\n    (\"n02109047\", \"Great_Dane\"),\n    (\"n02109525\", \"Saint_Bernard\"),\n    (\"n02109961\", \"Eskimo_dog\"),\n    (\"n02110063\", \"malamute\"),\n    (\"n02110185\", \"Siberian_husky\"),\n    (\"n02110341\", \"dalmatian\"),\n    (\"n02110627\", \"affenpinscher\"),\n    (\"n02110806\", \"basenji\"),\n    (\"n02110958\", \"pug\"),\n    (\"n02111129\", \"Leonberg\"),\n    (\"n02111277\", \"Newfoundland\"),\n    (\"n02111500\", \"Great_Pyrenees\"),\n    (\"n02111889\", \"Samoyed\"),\n    (\"n02112018\", \"Pomeranian\"),\n    (\"n02112137\", \"chow\"),\n    (\"n02112350\", \"keeshond\"),\n    (\"n02112706\", \"Brabancon_griffon\"),\n    (\"n02113023\", \"Pembroke\"),\n    (\"n02113186\", \"Cardigan\"),\n    (\"n02113624\", \"toy_poodle\"),\n    (\"n02113712\", \"miniature_poodle\"),\n    (\"n02113799\", \"standard_poodle\"),\n    (\"n02113978\", \"Mexican_hairless\"),\n    (\"n02114367\", \"timber_wolf\"),\n    (\"n02114548\", \"white_wolf\"),\n    (\"n02114712\", \"red_wolf\"),\n    (\"n02114855\", \"coyote\"),\n    (\"n02115641\", \"dingo\"),\n    (\"n02115913\", \"dhole\"),\n    (\"n02116738\", \"African_hunting_dog\"),\n    (\"n02117135\", \"hyena\"),\n    (\"n02119022\", \"red_fox\"),\n    (\"n02119789\", \"kit_fox\"),\n    (\"n02120079\", \"Arctic_fox\"),\n    (\"n02120505\", \"grey_fox\"),\n    (\"n02123045\", \"tabby\"),\n    (\"n02123159\", \"tiger_cat\"),\n    (\"n02123394\", \"Persian_cat\"),\n    (\"n02123597\", \"Siamese_cat\"),\n    (\"n02124075\", \"Egyptian_cat\"),\n    (\"n02125311\", \"cougar\"),\n    (\"n02127052\", \"lynx\"),\n    # ... (remaining classes omitted for brevity)\n]\n```\n\n----------------------------------------\n\nTITLE: Setting CI Flag - Python\nDESCRIPTION: Initializes a boolean variable `CI` to `False`. This variable is used later to conditionally skip notebook execution in continuous integration environments.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/1-local-image-batch-inference.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nCI = False\n```\n\n----------------------------------------\n\nTITLE: Defining CI Mode Flag in Python\nDESCRIPTION: Sets a boolean flag to indicate whether the script is running in Continuous Integration (CI) environment, which can be used to control execution flow or skip certain steps during testing.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/delta_lake/3-pytorch-ray-single-node-training.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nCI = False\n```\n\n----------------------------------------\n\nTITLE: Reading Iceberg Table with Polars\nDESCRIPTION: Reads data from an Iceberg table using Polars.  This shows lazy materialization and uses `pl.scan_iceberg()` to create a Polars LazyFrame.  The data will only be loaded when a computation is triggered.  This approach can be more efficient when dealing with large datasets.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/tutorials/talks_and_demos/iceberg_summit_2024.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport polars as pl\n\npl.scan_iceberg(table)\n```\n\n----------------------------------------\n\nTITLE: Interactive Plotly Bar Chart for TPCH Performance Comparison\nDESCRIPTION: This JavaScript code creates an interactive Plotly bar chart comparing the performance of Daft, Spark, and Dask DataFrame libraries on TPCH queries at 1000 scale factor. The visualization shows execution times for 10 different queries with hover text indicating relative performance.\nSOURCE: https://github.com/eventual-inc/daft/blob/main/docs/resources/benchmarks/tpch.md#_snippet_1\n\nLANGUAGE: JavaScript\nCODE:\n```\nwindow.PLOTLYENV=window.PLOTLYENV || {};\nif (document.getElementById(\"2e3c4bff-c808-4722-8664-d4c63ee41e55\")) {\n    Plotly.newPlot(\n        \"2e3c4bff-c808-4722-8664-d4c63ee41e55\",\n        [{\"marker\":{\"color\":\"rgba(108, 11, 169, 1)\"},\"name\":\"Daft\",\"x\":[\"Q1\",\"Q2\",\"Q3\",\"Q4\",\"Q5\",\"Q6\",\"Q7\",\"Q8\",\"Q9\",\"Q10\"],\"y\":[4.85,9.766666666666667,12.933333333333334,11.233333333333333,17.616666666666667,2.7,15.15,18.5,22.833333333333332,13.983333333333333],\"type\":\"bar\",\"textposition\":\"inside\"},{\"hovertext\":[\"12.1x Slower\",\"0.9x Slower\",\"3.8x Slower\",\"2.9x Slower\",\"2.1x Slower\",\"22.3x Slower\",\"3.5x Slower\",\"2.7x Slower\",\"2.6x Slower\",\"3.4x Slower\"],\"marker\":{\"color\":\"rgba(226,90,28, 0.75)\"},\"name\":\"Spark\",\"x\":[\"Q1\",\"Q2\",\"Q3\",\"Q4\",\"Q5\",\"Q6\",\"Q7\",\"Q8\",\"Q9\",\"Q10\"],\"y\":[58.625,8.591666666666667,48.559999999999995,32.88666666666667,36.98166666666667,60.11333333333334,52.34,49.475,58.26166666666666,46.85333333333333],\"type\":\"bar\",\"textposition\":\"inside\"},{\"hovertext\":[\"8.7x Slower\",\"2.1x Slower\",\"nanx Slower\",\"nanx Slower\",\"nanx Slower\",\"13.7x Slower\",\"nanx Slower\",\"nanx Slower\",\"nanx Slower\",\"nanx Slower\"],\"marker\":{\"color\":\"rgba(255,193,30, 0.75)\"},\"name\":\"Dask\",\"x\":[\"Q1\",\"Q2\",\"Q3\",\"Q4\",\"Q5\",\"Q6\",\"Q7\",\"Q8\",\"Q9\",\"Q10\"],\"y\":[42.37166666666667,20.926666666666666,null,null,null,36.968333333333334,null,null,null,null],\"type\":\"bar\",\"textposition\":\"inside\"}],\n        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"TPCH 1000 Scale Factor - 4 Nodes (lower is better)\"},\"yaxis\":{\"title\":{\"text\":\"Time (minutes)\"}},\"xaxis\":{\"title\":{\"text\":\"TPCH Question\"}},\"uniformtext\":{\"minsize\":8,\"mode\":\"hide\"}},\n        {\"displayModeBar\": false, \"responsive\": true}\n    )\n};\n\n```"
  }
]
[
  {
    "owner": "kxsystems",
    "repo": "kdbai-samples",
    "content": "TITLE: Setting Up LlamaIndex RAG Pipeline with KDB.AI Vector Store\nDESCRIPTION: Configures the LlamaIndex RAG pipeline using KDB.AI as the vector store. The code initializes the vector store, creates a storage context, and indexes the documents with a sentence splitter that chunks text into 2048-token segments. The process is timed for performance tracking.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_advanced_RAG/KDBAI_Advanced_RAG_Demo.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\n#llm = OpenAI(temperature=0, model=LLM)\nvector_store = KDBAIVectorStore(table)\n\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\nindex = VectorStoreIndex.from_documents(\n    docs,\n    storage_context=storage_context,\n    transformations=[SentenceSplitter(chunk_size=2048, chunk_overlap=0)],\n)\n```\n\n----------------------------------------\n\nTITLE: Connecting to KDB.AI Cloud Session in Python\nDESCRIPTION: Demonstrates how to set up a session with KDB.AI Cloud using API key and endpoint information. It retrieves credentials from environment variables or prompts the user in their absence, crucial for authenticated sessions.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/quickstarts/python_quickstart.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nKDBAI_ENDPOINT = (\n    os.environ[\"KDBAI_ENDPOINT\"]\n    if \"KDBAI_ENDPOINT\" in os.environ\n    else input(\"KDB.AI endpoint: \")\n)\nKDBAI_API_KEY = (\n    os.environ[\"KDBAI_API_KEY\"]\n    if \"KDBAI_API_KEY\" in os.environ\n    else getpass(\"KDB.AI API key: \")\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nsession = kdbai.Session(api_key=KDBAI_API_KEY, endpoint=KDBAI_ENDPOINT)\n```\n\n----------------------------------------\n\nTITLE: Processing and Embedding Image Files\nDESCRIPTION: Processes each image file by converting it to base64, generating a detailed description using GPT-4o Vision, embedding the description with the same model used for text, and adding the data to the DataFrame. This creates a unified representation for both modalities.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/multimodal_RAG_unified_text/multi_modal_demo.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n#Loop through all image files, get image summaries, embed the summaries, store relevant data in dataframe. Takes a couple of minutes\nfor image in images:\n    path = \"./data/images/\" + image\n    media_type = \"images\"\n    base64_image = encode_image(path)\n    prompt = \"Describe the image in detail.\"\n    summarization = image_summarize(base64_image,prompt)\n    embedding = text_to_embedding(summarization)\n    new_row = {'path': path,\n               'media_type':'image',\n               'text' : summarization,\n               'embeddings': embedding}\n    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n```\n\n----------------------------------------\n\nTITLE: Verifying Existing KDB.AI Databases and Tables\nDESCRIPTION: These snippets query the KDB.AI session to list existing databases and tables, helping ensure that the system is correctly connected and retrieving metadata about the current setup of the vector database.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/quickstarts/python_quickstart.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsession.databases()\n```\n\nLANGUAGE: python\nCODE:\n```\n# ensure no table called \"company_data\" exists\ntry:\n    for t in session.database('default').tables:\n            if t.name == 'company_data':\n                t.drop() \n    time.sleep(5)\nexcept kdbai.KDBAIException:\n    pass\n```\n\nLANGUAGE: python\nCODE:\n```\nsession.database('default').tables\n```\n\n----------------------------------------\n\nTITLE: Connecting to KDB.AI Cloud Service\nDESCRIPTION: Sets up a connection to KDB.AI Cloud by configuring the endpoint URL and API key. The credentials are either taken from environment variables or prompted from the user, then used to initialize a KDB.AI session.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_advanced_RAG/KDBAI_Advanced_RAG_Demo.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n#Set up KDB.AI endpoint and API key\nKDBAI_ENDPOINT = (\n    os.environ[\"KDBAI_ENDPOINT\"]\n    if \"KDBAI_ENDPOINT\" in os.environ\n    else input(\"KDB.AI endpoint: \")\n)\nKDBAI_API_KEY = (\n    os.environ[\"KDBAI_API_KEY\"]\n    if \"KDBAI_API_KEY\" in os.environ\n    else getpass(\"KDB.AI API key: \")\n)\n\nsession = kdbai.Session(endpoint=KDBAI_ENDPOINT, api_key=KDBAI_API_KEY)\n```\n\n----------------------------------------\n\nTITLE: Setting Up KDB.AI Session for Vector Database\nDESCRIPTION: Establishes a connection to the KDB.AI vector database using either KDB.AI Cloud or KDB.AI Server, depending on the user's setup. It prompts for necessary credentials if not set as environment variables.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nKDBAI_ENDPOINT = (\n    os.environ[\"KDBAI_ENDPOINT\"]\n    if \"KDBAI_ENDPOINT\" in os.environ\n    else input(\"KDB.AI endpoint: \")\n)\nKDBAI_API_KEY = (\n    os.environ[\"KDBAI_API_KEY\"]\n    if \"KDBAI_API_KEY\" in os.environ\n    else getpass(\"KDB.AI API key: \")\n)\n\nsession = kdbai.Session(api_key=KDBAI_API_KEY, endpoint=KDBAI_ENDPOINT)\n\n# Alternatively for KDB.AI Server:\n# session = kdbai.Session(endpoint=\"http://localhost:8082\")\n```\n\n----------------------------------------\n\nTITLE: Analyzing Post-Crisis Financial Regulations\nDESCRIPTION: Queries the RAG system about new financial regulations enacted after the 2008 crisis to improve market regulation and consumer sentiment. The query targets post-crisis documents with temporal filtering to focus on relevant regulatory changes.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_advanced_RAG/KDBAI_Advanced_RAG_Demo.ipynb#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\nresult = query_engine.query(\n    \"\"\"\n    What was the new US financial regulation enacted after the 2008 crisis to increase the market regulation and to improve consumer sentiment ?\n    \"\"\"\n)\nprint(result.response)\n```\n\n----------------------------------------\n\nTITLE: Creating and Configuring KDB.AI Vector Database Table\nDESCRIPTION: Defines the schema for the vector database table, creates the table in KDB.AI, and sets up the necessary indexes for efficient similarity searches on the embeddings.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nrag_schema = [\n    {\"name\": \"id\", \"type\": \"str\"},\n    {\"name\": \"text\", \"type\": \"bytes\"},\n    {\"name\": \"embeddings\", \"type\": \"float32s\"},\n]\n\nindexes = [{'name': 'flat_index', 'column': 'embeddings', 'type': 'flat', 'params': {\"dims\": 1536, \"metric\": \"L2\"}}]\n\ndatabase = session.database(\"default\")\n\n# First ensure the table does not already exist\ntry:\n    database.table(\"rag_langchain\").drop()\nexcept kdbai.KDBAIException:\n    pass\n\ntable = database.create_table(\"rag_langchain\", schema=rag_schema, indexes=indexes)\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search in KDB.AI Table - Python\nDESCRIPTION: This snippet demonstrates how to perform a similarity search using a query vector derived from a natural language description, allowing for semantic searches in the KDB.AI table.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/quickstarts/python_quickstart.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nquery = \"A company that helps facilitate meetings\"\nquery_vector = list(embedding_model.embed([query]))[0].tolist()\ntable.search(vectors={'vectorIndex': [query_vector]})[0]\n```\n\n----------------------------------------\n\nTITLE: KDB.AI and OpenAI Authentication Setup\nDESCRIPTION: Configuration of API keys and endpoints for KDB.AI and OpenAI services, with environment variable fallbacks.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/rag_example.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nOPENAI_API_KEY = (\n    os.environ[\"OPENAI_API_KEY\"]\n    if \"OPENAI_API_KEY\" in os.environ\n    else getpass(\"OpenAI API key: \")\n)\n\nKDBAI_ENDPOINT = (\n    os.environ[\"KDBAI_ENDPOINT\"]\n    if \"KDBAI_ENDPOINT\" in os.environ\n    else input(\"KDB.AI endpoint: \")\n)\nKDBAI_API_KEY = (\n    os.environ[\"KDBAI_API_KEY\"]\n    if \"KDBAI_API_KEY\" in os.environ\n    else getpass(\"KDB.AI API key: \")\n)\n```\n\n----------------------------------------\n\nTITLE: Defining KDB.AI Table Schema for Multimodal Data\nDESCRIPTION: Defines the schema for the KDB.AI table that will store the multimodal data. Includes columns for file path, media type, text content, and vector embeddings with appropriate data types.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/multimodal_RAG_unified_text/multi_modal_demo.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Define table schema for our table with columns for path, media_type, text, and embeddings\ntable_schema = [\n    {\"name\": \"path\", \"type\": \"str\"},\n    {\"name\": \"media_type\", \"type\": \"str\"},\n    {\"name\": \"text\", \"type\": \"str\"},\n    {\"name\": \"embeddings\", \"type\": \"float64s\"}\n]\nindex_name = \"flat_index\"\nindexes = [{\"name\": index_name, \"column\": \"embeddings\", \"type\": \"flat\", \"params\": {\"dims\": 1536, \"metric\": \"CS\"}}]\n```\n\n----------------------------------------\n\nTITLE: Creating and Defining Schema for KDB.AI Table\nDESCRIPTION: Shows how to define schema and indexes for creating a new KDB.AI table. The schema includes column names, types, and vector embeddings, while indexes define parameters such as dimensionality and similarity metrics for the embeddings.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/quickstarts/python_quickstart.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nschema = [\n        {\"name\": \"company_name\", \"type\": \"str\"},\n        {\"name\": \"company_description\", \"type\": \"str\"},\n        {\"name\": \"vectors\", \"type\": \"float64s\"}\n    ]\n```\n\nLANGUAGE: python\nCODE:\n```\n# Define the index\nindexes = [\n            {\n                \"name\": \"vectorIndex\", \"type\": \"flat\", \n                \"params\": {\"dims\": 384, \"metric\": \"CS\"},\n                \"column\": \"vectors\"\n            }\n        ]\n```\n\nLANGUAGE: python\nCODE:\n```\ndatabase = session.database('default')\ntable = database.create_table(\"company_data\", schema, indexes=indexes)\n```\n\n----------------------------------------\n\nTITLE: Hybrid Search with Sparse Bias using KDB.AI in Python\nDESCRIPTION: This code snippet performs a hybrid search with a bias towards sparse results, adjusting weights to favor sparse terms over dense. It showcases how to prioritize search strategies based on data relevance goals.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ntable.search(\n    vectors={\"sparse_index\": sparse_query,\"dense_index\": dense_query},\n    index_params={\"sparse_index\":{'weight':0.9} ,\"dense_index\":{'weight':0.1}},\n    n=5\n)[0][['ID','chunk']]\n```\n\n----------------------------------------\n\nTITLE: Vector Store and Query Engine Setup\nDESCRIPTION: Configuration of LlamaIndex vector store, storage context, and query engine with KDB.AI integration.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Sub_Question_Query_Engine_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.vector_stores.kdbai import KDBAIVectorStore\nfrom llama_index.core import StorageContext\nfrom llama_index.core import Settings\nfrom llama_index.core.indices import VectorStoreIndex\nfrom llama_index.core.node_parser import SentenceSplitter\n\n# Vector Store\ntext_store = KDBAIVectorStore(table=table)\n\n# Storage context\nstorage_context = StorageContext.from_defaults(vector_store=text_store)\n\n# Settings\nSettings.callback_manager = callback_manager\nSettings.transformations = [SentenceSplitter(chunk_size=500, chunk_overlap=0)]\nSettings.embed_model = embeddings_model\nSettings.llm = llm_model\n\n# Vector Store Index\nindex = VectorStoreIndex.from_documents(\n    docs,\n    use_async=True,\n    storage_context=storage_context,\n)\n```\n\n----------------------------------------\n\nTITLE: Querying US Financial Regulations Before 2008 Crisis\nDESCRIPTION: Queries the RAG system about financial regulations in the US before the 2008 financial crisis. The query leverages the pre-crisis date filter to search only relevant documents, and the response is generated by the configured OpenAI model based on retrieved contexts.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_advanced_RAG/KDBAI_Advanced_RAG_Demo.ipynb#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\nresult = query_engine.query(\n    \"\"\"\n    What was the main financial regulation in the US before the 2008 financial crisis ?\n    \"\"\"\n)\nprint(result.response)\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG with LangChain and KDB.AI for Financial Data Analysis in Python\nDESCRIPTION: This code sets up a Retrieval-Augmented Generation (RAG) system using LangChain and KDB.AI. It defines embedding models, vector stores, and question-answering chains for both contextualized and non-contextualized data. A helper function is created to perform RAG queries.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/unstructured_io_RAG/Table_RAG_Unstructured_KDBAI_LangChain_RAG.ipynb#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# Define OpenAI embedding model for LangChain to embed the query\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n\n# use KDBAI as vector store\nvecdb_kdbai_contextualized = KDBAI(table_contextualized, embeddings)\nvecdb_kdbai_non_contextualized = KDBAI(table_non_contextualized, embeddings)\n```\n\nLANGUAGE: python\nCODE:\n```\n# Define a Question/Answer LangChain chain\nqabot_contextualized = RetrievalQA.from_chain_type(\n    chain_type=\"stuff\",\n    llm=ChatOpenAI(model=\"gpt-4o\"),\n    retriever=vecdb_kdbai_contextualized.as_retriever(search_kwargs=dict(k=5, index='flat_index')),\n    return_source_documents=True,\n)\n\nqabot_non_contextualized = RetrievalQA.from_chain_type(\n    chain_type=\"stuff\",\n    llm=ChatOpenAI(model=\"gpt-4o\"),\n    retriever=vecdb_kdbai_non_contextualized.as_retriever(search_kwargs=dict(k=5, index='flat_index')),\n    return_source_documents=True,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\n# Helper function to perform RAG\ndef RAG(query):\n  print(query)\n  print(\"-----\")\n  print(\"Contextualized\")\n  print(\"-----\")\n  print(qabot_contextualized.invoke(dict(query=query))[\"result\"])\n  print(\"-----\")\n  print(\"Non Contextualized\")\n  print(\"-----\")\n  print(qabot_non_contextualized.invoke(dict(query=query))[\"result\"])\n```\n\n----------------------------------------\n\nTITLE: Importing KDB.AI Vector Database Dependencies\nDESCRIPTION: Imports the necessary libraries for setting up a KDB.AI session. These include standard Python libraries for handling environment variables and user input, as well as the KDB.AI client for database interactions.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_advanced_RAG/KDBAI_Advanced_RAG_Demo.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# vector DB imports\nimport os\nfrom getpass import getpass\nimport kdbai_client as kdbai\nimport time\n```\n\n----------------------------------------\n\nTITLE: Defining Helper Functions for Image Processing and Embedding\nDESCRIPTION: Implements four key helper functions: encode_image to convert images to base64, image_summarize to describe images using GPT-4 Vision, read_text_from_file to extract content from text files, and text_to_embedding to generate vector embeddings from text using OpenAI's embedding model.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/multimodal_RAG_unified_text/multi_modal_demo.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport base64\n\n# Helper function to convert a file to base64 representation\ndef encode_image(image_path):\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode('utf-8')\n\n# Takes in a base64 encoded image and a prompt, returns a text description of the image\ndef image_summarize(img_base64,prompt):\n    ''' Image summary '''\n    for _ in range(3):\n        response = openai.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=[\n                {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": prompt},\n                    {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/jpeg;base64,{img_base64}\",\n                    },\n                    },\n                ],\n                }\n            ],\n            max_tokens=150,\n        )\n        content = response.choices[0].message.content\n\n        if \"I'm sorry\" not in content:\n            return content\n        time.sleep(3)\n    return content\n\n#Return the text from a file\ndef read_text_from_file(filename):\n    try:\n        # Open the file in read mode ('r')\n        with open(filename, 'r') as file:\n            # Read the contents of the file into a string\n            text = file.read()\n        return text\n    except IOError as e:\n        # Handle any I/O errors\n        print(f\"An error occurred: {e}\")\n        return None\n\n#model=\"text-embedding-3-small\"\n#Takes in text and returns a vector embedding using text-embedding-3-small\ndef text_to_embedding(text):\n    text = text.replace(\"\\n\", \" \")\n    return client.embeddings.create(input = [text], model=\"text-embedding-3-small\").data[0].embedding\n```\n\n----------------------------------------\n\nTITLE: Creating QA Chains with load_qa_chain\nDESCRIPTION: Creates QA chains using the `load_qa_chain` function from LangChain for both the OpenAI and HuggingFace models. The 'stuff' chain type is used, which inserts all documents into a prompt for processing by the language model.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# create the chain for each model using langchain load_qa_chain\nchain_openAI = load_qa_chain(llm_openai, chain_type=\"stuff\")\nchain_HuggingFaceHub = load_qa_chain(llm_mistral, chain_type=\"stuff\")\n```\n\n----------------------------------------\n\nTITLE: Configuring LlamaIndex Query Engine with Pre-Crisis Date Filter\nDESCRIPTION: Sets up a LlamaIndex query engine with temporal filtering to retrieve only documents published before September 15, 2008 (the date of Lehman Brothers' collapse). The engine is configured to return the top 15 most relevant results sorted by publication date.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_advanced_RAG/KDBAI_Advanced_RAG_Demo.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\n# Using gpt-4o-mini, the 128k tokens context size can take 100 pages.\nK = 15\n\nquery_engine = index.as_query_engine(\n                similarity_top_k=K,\n                vector_store_kwargs={\n                        \"index\" : \"flat_index\",\n                        \"filter\" : [[\"<\", \"publication_date\", datetime.date(2008,9,15)]],\n                        \"sort_columns\" : [\"publication_date\"]\n                        }\n        )\n```\n\n----------------------------------------\n\nTITLE: Initializing LLM Models for QA in LangChain\nDESCRIPTION: This snippet shows the initialization of two language models, OpenAI's GPT-4o and HuggingFace's Mistral-7B-Instruct, for question-answering with LangChain's `load_qa_chain` method. These models are configured for specific repository IDs and token limits as required by LangChain.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# select two llm models (OpenAI gpt-4o, HuggingFaceHub mistralai/Mistral-7B-Instruct-v0.2)\nllm_openai = ChatOpenAI(model=\"gpt-4o\", max_tokens=512)\nllm_mistral = HuggingFaceEndpoint(\n    repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\"\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG (Retrieval-Augmented Generation) Function with OpenAI GPT in Python\nDESCRIPTION: This function implements RAG using OpenAI's chat completions API. It takes retrieved data and a prompt as input, combines them into a message, and generates a response using the GPT-4 model.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/multimodal_RAG_unified_text/multi_modal_demo.ipynb#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ndef RAG(retrieved_data,prompt):\n    messages = \"\"\n    messages += prompt + \"\\n\"\n    if retrieved_data:\n        for data in retrieved_data:\n            messages += data + \"\\n\"\n    response = openai.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[           \n            {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": messages},\n            ],\n            }\n        ],\n        max_tokens=300, \n    )\n    content = response.choices[0].message.content\n    return content\n```\n\n----------------------------------------\n\nTITLE: Importing Python Dependencies\nDESCRIPTION: Import statements for required Python modules including kdbai_client, llama_index, pandas, and other utilities.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/rag_example.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\nimport kdbai_client as kdbai\nimport time\nfrom llama_index.core import Document, SimpleDirectoryReader\nfrom llama_index.core.node_parser import SentenceSplitter\nimport pandas as pd\nfrom fastembed import TextEmbedding\nimport openai\nimport textwrap\n```\n\n----------------------------------------\n\nTITLE: Configuring LlamaIndex Query Engine with Post-Crisis Date Filter\nDESCRIPTION: Sets up a LlamaIndex query engine with temporal filtering to retrieve only documents published after September 15, 2008. This allows analysis of post-crisis financial regulations, retrieving the top 15 most relevant results sorted by publication date.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_advanced_RAG/KDBAI_Advanced_RAG_Demo.ipynb#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\n# Using gpt-4o-mini, the 128k tokens context size can take 100 pages.\nK = 15\n\nquery_engine = index.as_query_engine(\n                similarity_top_k=K,\n                vector_store_kwargs={\n                        \"index\" : \"flat_index\",\n                        \"filter\" : [\">=\", \"publication_date\", datetime.date(2008,9,15)],\n                        \"sort_columns\" : [\"publication_date\"]\n                        }\n        )\n```\n\n----------------------------------------\n\nTITLE: Connecting to KDB.AI Vector Database\nDESCRIPTION: Establishes a connection to the KDB.AI vector database using the provided endpoint and API key. This session object will be used for all subsequent interactions with the database.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/multimodal_RAG_unified_text/multi_modal_demo.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Connect to KDB.AI\nsession = kdbai.Session(api_key=KDBAI_API_KEY, endpoint=KDBAI_ENDPOINT)\n```\n\n----------------------------------------\n\nTITLE: Configuring KDB.AI Query Engine with Similarity Search Parameters in Python\nDESCRIPTION: Creates a query engine from an index with similarity search parameters. The configuration sets the top K similar results to 20 and specifies a flat index with sorting based on publication date.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_advanced_RAG/KDBAI_Advanced_RAG_Demo.ipynb#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nK = 20\n\nquery_engine = index.as_query_engine(\n                similarity_top_k=K,\n                vector_store_kwargs={\n                        \"index\" : \"flat_index\",\n                        \"sort_columns\" : [\"publication_date\"]\n                        }\n        )\n```\n\n----------------------------------------\n\nTITLE: Performing Retrieval Augmented Generation in Python\nDESCRIPTION: This code enables a question-answering functionality using LangChain's RetrievalQA, leveraging KDB.AI as the retriever mechanism to find and answer queries based on the stored document embeddings.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation_evaluation.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nK = 10\n```\n\nLANGUAGE: python\nCODE:\n```\nqabot = RetrievalQA.from_chain_type(\n    chain_type=\"stuff\",\n    llm=ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0),\n    retriever=vecdb_kdbai.as_retriever(search_kwargs=dict(k=K, index=\"flat_index\")),\n    return_source_documents=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Running Hybrid Search with KDB.AI in Python\nDESCRIPTION: This snippet runs a hybrid search in KDB.AI by combining sparse and dense search indices, with equal weight distribution. It exemplifies how to leverage both search types simultaneously for enhanced result accuracy.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ntable.search(\n    vectors={\"sparse_index\": sparse_query,\"dense_index\": dense_query},\n    index_params={\"sparse_index\":{'weight':0.5} ,\"dense_index\":{'weight':0.5}},\n    n=5\n)[0][['ID','chunk']]\n```\n\n----------------------------------------\n\nTITLE: Setting Up KDB.AI Credentials\nDESCRIPTION: Sets up the KDB.AI endpoint URL and API key either from environment variables or by prompting the user. These credentials are required to authenticate with the KDB.AI service.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/multimodal_RAG_unified_text/multi_modal_demo.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n#Set up KDB.AI endpoint and API key\nKDBAI_ENDPOINT = (\n    os.environ[\"KDBAI_ENDPOINT\"]\n    if \"KDBAI_ENDPOINT\" in os.environ\n    else input(\"KDB.AI endpoint: \")\n)\nKDBAI_API_KEY = (\n    os.environ[\"KDBAI_API_KEY\"]\n    if \"KDBAI_API_KEY\" in os.environ\n    else getpass(\"KDB.AI API key: \")\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Python Libraries for Multimodal RAG\nDESCRIPTION: Imports all required Python libraries including data manipulation tools (pandas), OpenAI client for LLM access, image processing libraries, and visualization tools for the multimodal RAG implementation.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/multimodal_RAG_unified_text/multi_modal_demo.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport os\nimport io\nfrom getpass import getpass\nimport langchain\nimport openai\nfrom openai import OpenAI\nimport numpy as np\nfrom PIL import Image\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.schema.messages import HumanMessage, SystemMessage\nfrom IPython.display import display\nimport time\nimport warnings\nimport requests\nwarnings.filterwarnings('ignore')\n```\n\n----------------------------------------\n\nTITLE: Setting Up KDB.AI Endpoint and API Key\nDESCRIPTION: Retrieves the KDB.AI endpoint URL and API key from environment variables or prompts the user for input if not available.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n#Set up KDB.AI endpoint and API key\nKDBAI_ENDPOINT = (\n    os.environ[\"KDBAI_ENDPOINT\"]\n    if \"KDBAI_ENDPOINT\" in os.environ\n    else input(\"KDB.AI endpoint: \")\n)\nKDBAI_API_KEY = (\n    os.environ[\"KDBAI_API_KEY\"]\n    if \"KDBAI_API_KEY\" in os.environ\n    else getpass(\"KDB.AI API key: \")\n)\n```\n\n----------------------------------------\n\nTITLE: Defining KDB.AI Table Schema with Embedding Dimensions\nDESCRIPTION: Creates the schema for a KDB.AI table that will store document embeddings. The schema includes columns for document ID, text content, embeddings vector (1536 dimensions for OpenAI's text-embedding-3-small model), document title, and publication date.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_advanced_RAG/KDBAI_Advanced_RAG_Demo.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nschema = [\n        {\"name\":\"document_id\", \"type\":\"bytes\"},\n        {\"name\":\"text\", \"type\":\"bytes\"},\n        {\"name\":\"embeddings\",\"type\":\"float32s\"},\n        {\"name\":\"title\", \"type\":\"str\"},\n        {\"name\":\"publication_date\", \"type\":\"datetime64[ns]\"},\n        ]\n\n\nindexFlat = {\n        \"name\": \"flat_index\",\n        \"type\": \"flat\",\n        \"column\": \"embeddings\",\n        \"params\": {'dims': 1536, 'metric': 'L2'},\n    }\n```\n\n----------------------------------------\n\nTITLE: Setting up OpenAI Configurations\nDESCRIPTION: Configuration of OpenAI API key and model parameters for text embeddings and language generation.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/qHnsw_index_pdf_search/pdf_qHNSW_Search.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nos.environ[\"OPENAI_API_KEY\"] = (\n    os.environ[\"OPENAI_API_KEY\"]\n    if \"OPENAI_API_KEY\" in os.environ\n    else getpass(\"OpenAI API Key: \")\n)\n\nEMBEDDING_MODEL  = \"text-embedding-3-small\"\nGENERATION_MODEL = 'gpt-4o-mini'\n\nllm = OpenAI(model=GENERATION_MODEL)\nembed_model = OpenAIEmbedding(model=EMBEDDING_MODEL)\n\nSettings.llm = llm\nSettings.embed_model = embed_model\n```\n\n----------------------------------------\n\nTITLE: Defining Vector Indexes\nDESCRIPTION: Creates index definitions for both dense vectors (using flat index with Euclidean distance) and sparse vectors (using BM25 algorithm with tunable parameters).\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# Define the index\nindexes = [\n    {\n        'type': 'flat',\n        'name': 'dense_index',\n        'column': 'dense',\n        'params': {'dims': 384, 'metric': \"L2\"},\n    },\n    {\n        'type': 'bm25',\n        'name': 'sparse_index',\n        'column': 'sparse',\n        'params': {'k': 1.25, 'b': 0.75},\n    },\n]\n```\n\n----------------------------------------\n\nTITLE: Running Similarity Search in KDB.AI with Python\nDESCRIPTION: This snippet executes a similarity search on the 'trade_tsc' table using a predefined query vector. It returns the top 10 most similar vectors, merges the results with the original data, and tracks execution time and memory usage.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Temporal_Similarity_Search_Transformed_Demo.ipynb#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ntsc_start=datetime.datetime.now()\nres = table_tsc.search(vectors={'flat_index':[q]}, n=10)[0]\ntsc_stop=datetime.datetime.now()\nmemory_tsc_windows_post_search=get_memory_usage()\nres.merge(vecdf, on=['index','sym','time'], how='left')\n```\n\n----------------------------------------\n\nTITLE: Defining Vector Database Table Schema for Song Embeddings\nDESCRIPTION: Creates a structured schema for storing song metadata and embeddings in KDB.AI, specifying column names, types, and index configuration for efficient vector search\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/music_recommendation/music_recommendation.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nschema =  [\n    {\n        \"name\": \"song_name\",\n        \"type\": \"str\",\n    },\n    {\n        \"name\": \"song_artists\",\n        \"type\": \"bytes\",\n    },\n    {\n        \"name\": \"song_year\",\n        \"type\": \"int64\",\n    },\n    {\n        \"name\": \"song_embeddings\",\n        \"type\": \"float64s\"\n    }\n]\n\nindexes = [\n    {\n        \"name\" : \"flat_index\",\n        \"column\" : \"song_embeddings\",\n        \"type\" : \"flat\",\n        \"params\":{\n            \"dims\": len(numeric_cols) + embedding_dim,\n            \"metric\": \"L2\"\n        }\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into KDB.AI\nDESCRIPTION: Prepares and inserts documents with their embeddings into KDB.AI table.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/rag_example.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nrecords_to_insert_with_embeddings = pd.DataFrame({\n    \"text\": [d.encode('utf-8') for d in documents],\n    \"embedding\": embeddings\n})\n\ntable.insert(records_to_insert_with_embeddings)\n```\n\n----------------------------------------\n\nTITLE: Creating Final DataFrame with Embeddings\nDESCRIPTION: Constructs a Pandas DataFrame containing song metadata and the combined vector embeddings.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/music_recommendation/music_recommendation.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nembedded_song_df = song_data[[\"song_name\", \"song_artists\", \"song_year\"]]\n```\n\nLANGUAGE: python\nCODE:\n```\nembedded_song_df[\"song_embeddings\"] = row_embeddings\n```\n\nLANGUAGE: python\nCODE:\n```\nshow_df(embedded_song_df)\n```\n\n----------------------------------------\n\nTITLE: Setting KDB.AI Cloud Session Parameters\nDESCRIPTION: Configure KDB.AI endpoint and API key, either from environment variables or user input\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaParse_pdf_RAG/llamaParse_demo.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nKDBAI_ENDPOINT = (\n    os.environ[\"KDBAI_ENDPOINT\"]\n    if \"KDBAI_ENDPOINT\" in os.environ\n    else input(\"KDB.AI endpoint: \")\n)\nKDBAI_API_KEY = (\n    os.environ[\"KDBAI_API_KEY\"]\n    if \"KDBAI_API_KEY\" in os.environ\n    else getpass(\"KDB.AI API key: \")\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a KDB.AI Table with a Specific Schema - Python\nDESCRIPTION: This snippet creates a new table in KDB.AI using a predefined schema and indexes. The table is named 'pdf' and is constructed using the create_table method of KDB.AI's database object.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_26\n\nLANGUAGE: python\nCODE:\n```\ntable = db.create_table(table=\"pdf\", schema=pdf_schema, indexes=indexes)\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search on KDB.AI Vector Store\nDESCRIPTION: Demonstrates how to perform a similarity search on the stored vector embeddings using a given query. This is a precursor to the full RAG implementation, showing how relevant text chunks are retrieved.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nquery = \"what are the nations strengths?\"\n\n# query_sim holds results of the similarity search, the closest related chunks to the query.\nquery_sim = vecdb_kdbai.similarity_search(query, index='flat_index')\n\nquery_sim\n```\n\n----------------------------------------\n\nTITLE: Querying the KDB.AI Table for Verification - Python\nDESCRIPTION: Executes a query on the newly created table to verify its existence. Initially, the table should return empty as no data has been inserted yet.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ntable.query()\n```\n\n----------------------------------------\n\nTITLE: Loading PDF Documents with LlamaIndex\nDESCRIPTION: Loads the downloaded PDF documents using LlamaIndex's SimpleDirectoryReader with associated metadata. The function attaches metadata from the METADATA dictionary to each document file path. The code is timed to measure loading performance.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_advanced_RAG/KDBAI_Advanced_RAG_Demo.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\ndef get_metadata(filepath):\n    return METADATA[filepath]\n\n\ndocuments = SimpleDirectoryReader(\n    input_files=local_files,\n    file_metadata=get_metadata,\n)\n\ndocs = documents.load_data()\nlen(docs)\n```\n\n----------------------------------------\n\nTITLE: Creating Schema and KDB.AI Tables for Financial Data in Python\nDESCRIPTION: This code defines a schema for financial data, creates indexes, and sets up two KDB.AI tables: one for contextualized data and another for non-contextualized data. It also inserts data into these tables and verifies the insertion.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/unstructured_io_RAG/Table_RAG_Unstructured_KDBAI_LangChain_RAG.ipynb#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nschema = [\n    {'name': 'id', 'type': 'str'},\n    {'name': 'text', 'type': 'bytes'},\n    {'name': 'metadata', 'type': 'general'},\n    {'name': 'embedding', 'type': 'float32s'}\n]\n\nindexes = [{'name': 'flat_index', 'column': 'embedding', 'type': 'flat', 'params': {'dims': 1536, 'metric': 'L2'}}]\n```\n\nLANGUAGE: python\nCODE:\n```\nContextualized_KDBAI_TABLE_NAME = \"Contextualized_Table\"\nnon_Contextualized_KDBAI_TABLE_NAME = \"Non_Contextualized_Table\"\ndatabase = session.database('default')\n\n# First ensure the tables do not already exist\nfor table in database.tables:\n    if table.name in [Contextualized_KDBAI_TABLE_NAME, non_Contextualized_KDBAI_TABLE_NAME]:\n        table.drop()\n\n#Create the tables\ntable_contextualized = database.create_table(Contextualized_KDBAI_TABLE_NAME, schema=schema, indexes=indexes)\ntable_non_contextualized = database.create_table(non_Contextualized_KDBAI_TABLE_NAME, schema=schema, indexes=indexes)\n```\n\nLANGUAGE: python\nCODE:\n```\n# Insert Elements into the KDB.AI Tables\ntable_contextualized.insert(df_contextualized)\ntable_non_contextualized.insert(df_non_contextualized)\n```\n\nLANGUAGE: python\nCODE:\n```\n# Check to see that the elements were inserted\ntable_contextualized.query()\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search in KDB.AI using Python\nDESCRIPTION: This snippet shows how to perform a similarity search in the KDB.AI table. It searches for the closest neighbor to a given query vector, demonstrating the vector search capabilities of KDB.AI.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/managing_tables.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Find the closest neighbor of a single query vector\ntable.search(vectors={index_name: [[0.1, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]]}, n=1)\n```\n\n----------------------------------------\n\nTITLE: Processing and Embedding Text Files\nDESCRIPTION: Loops through all text files, reads their content, generates embeddings using OpenAI's text-embedding-3-small model, and stores the data in the DataFrame. Each row contains the file path, media type, original text, and vector embeddings.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/multimodal_RAG_unified_text/multi_modal_demo.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n#Loop through all text files and embed them, store relevant data in dataframe\nfor text in texts:\n    path = \"./data/text/\" + text\n    media_type = \"text\"\n    text1 = read_text_from_file(path)\n    embedding = text_to_embedding(text1)\n    new_row = {'path': path,\n               'media_type':'text',\n               'text' : text1,\n               'embeddings': embedding}\n    df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n```\n\n----------------------------------------\n\nTITLE: Partitioning PDF Document using Unstructured - Python\nDESCRIPTION: This snippet demonstrates how to partition a PDF document into elements using the 'hi_res' partitioning strategy and chunking by title, preparing extracted data for further processing.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/unstructured_io_RAG/Table_RAG_Unstructured_KDBAI_LangChain_RAG.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nelements = partition_pdf('./doc1.pdf',\n                              strategy=\"hi_res\",\n                              chunking_strategy=\"by_title\",\n                              )\n```\n\n----------------------------------------\n\nTITLE: Alternative KDB.AI Server Connection Setup\nDESCRIPTION: Shows an alternative method for connecting to a self-hosted KDB.AI Server instead of the cloud service. This commented code would initialize a session with a local KDB.AI Server after following the server setup steps.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_advanced_RAG/KDBAI_Advanced_RAG_Demo.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n#session = kdbai.Session()\n```\n\n----------------------------------------\n\nTITLE: Automated Song Similarity Search Function\nDESCRIPTION: Creates a flexible function for performing similarity searches in the vector database, supporting filtering by song name, artists, and year with configurable result count\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/music_recommendation/music_recommendation.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef find_similar_songs(\n    vectorDB_song_tab,\n    song_name: str,\n    song_artists: list[str] = None,\n    song_year: int = None,\n    n_similar: int = 5,\n    exact: bool = False,\n) -> None:\n    filter_list = [\"like\", \"song_name\", f\"{song_name}\" if exact else f\"*{song_name}*\"]\n    if song_artists:\n        if type(song_artists) == str:\n            song_artists = list(song_artists)\n        for artist in song_artists:\n            filter_list.append((\"like\", \"song_artists\", f\"*{artist}*\"))\n    if song_year:\n        filter_list.append((\"like\", \"song_year\", f\"{song_year}\"))\n\n    resulting_song = vectorDB_song_tab.query(filter=filter_list, sort_columns=[\"song_year\"])\n\n    if resulting_song.empty:\n        print(\"Song Not Found!\")\n        return\n\n    resulting_vectors = {'flat_index': [v.tolist() for v in resulting_song[\"song_embeddings\"]]}\n    similar_songs = vectorDB_song_tab.search(vectors=resulting_vectors, n=n_similar + 1)\n\n    for i, similar_df in enumerate(similar_songs):\n        name = resulting_song.loc[i, \"song_name\"]\n        artists = resulting_song.loc[i, \"song_artists\"]\n        year = resulting_song.loc[i, \"song_year\"]\n        print(f\"Songs Similar To '{name}' By '{artists}' ({year})\")\n        for j, song in similar_df[1:].iterrows():\n            print(f\"   {j}. {song['song_name']} - {song['song_artists']} ({song['song_year']})\")\n        print()\n```\n\n----------------------------------------\n\nTITLE: Establishing KDB.AI Session in Python\nDESCRIPTION: This snippet creates a session with KDB.AI, utilizing either environment variables or user input for connection parameters, allowing subsequent operations on the database.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation_evaluation.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nKDBAI_ENDPOINT = (\n    os.environ[\"KDBAI_ENDPOINT\"]\n    if \"KDBAI_ENDPOINT\" in os.environ\n    else input(\"KDB.AI endpoint: \")\n)\nKDBAI_API_KEY = (\n    os.environ[\"KDBAI_API_KEY\"]\n    if \"KDBAI_API_KEY\" in os.environ\n    else getpass(\"KDB.AI API key: \")\n)\n\nsession = kdbai.Session(api_key=KDBAI_API_KEY, endpoint=KDBAI_ENDPOINT)\n```\n\n----------------------------------------\n\nTITLE: Configuring Index and Embedding in Python\nDESCRIPTION: This snippet defines the index and embedding configuration for a KDB.AI table. It specifies a flat index based on the price column and a transformed similarity search using 'TSC'. The setup includes dimensionality reduction to 8 dimensions for optimized storage and search efficiency.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Temporal_Similarity_Search_Transformed_Demo.ipynb#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# Define the index\nindexes_tsc = [\n    {\n        'type': 'flat',\n        'name': 'flat_index',\n        'column': 'price',\n        'params': {'metric': \"L2\"},\n    },\n]\n\n# Define an embedding configuration, specifying to use 'TSC': transformed TSS\n# Inserted high dimensional data will be compressed with 'tsc' to 8 dimensions and stored in KDB.AI\nembedding_conf = {'price':{\"dims\":8, \"type\":\"tsc\", \"on_insert_error\":\"reject_all\"}}\n```\n\n----------------------------------------\n\nTITLE: Searching KDB.AI Vector Database in Python\nDESCRIPTION: This code performs a search on the KDB.AI vector database using the previously created query vector. It retrieves the top 3 most relevant results based on the query.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/multimodal_RAG_unified_text/multi_modal_demo.ipynb#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nresults = table.search({index_name: query_vector}, n=3)\nprint(results)\n```\n\n----------------------------------------\n\nTITLE: Creating and Pre-populating KDB.AI Table in Python\nDESCRIPTION: This snippet creates a new table in KDB.AI using the predefined schema, index, and embedding configurations. It also tracks memory usage before populating the table to evaluate the impact of data insertion.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Temporal_Similarity_Search_Transformed_Demo.ipynb#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# Create the table with the defined schema, index, and embedding configuration from above\ntable_tsc = db.create_table(table='trade_tsc', schema=schema_tsc, indexes=indexes_tsc, embedding_configurations=embedding_conf)\nmemory_tsc_pre_populate=get_memory_usage()\n```\n\n----------------------------------------\n\nTITLE: Defining Vector Index Configuration\nDESCRIPTION: Configures an HNSW (Hierarchical Navigable Small World) index for efficient similarity search on the vector embeddings, specifying dimensions and distance metric.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n# Define the index\nindexes = [\n    {\n        'type': 'hnsw',\n        'name': 'hnsw_index',\n        'column': 'vectors',\n        'params': {'dims': 384, 'metric': \"L2\"},\n    },\n]\n```\n\n----------------------------------------\n\nTITLE: Creating KDB.AI Database and Table in Python\nDESCRIPTION: Creates a new KDB.AI database named 'myDatabase' and defines a table schema for AI tools data, including vector embeddings for similarity search.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/HuggingFace_search/huggingface_inference.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# ensure no database called \"myDatabase\" exists\ntry:\n    session.database(\"myDatabase\").drop()\nexcept kdbai.KDBAIException:\n    pass\n```\n\nLANGUAGE: python\nCODE:\n```\n# Create the database\ndb = session.create_database(\"myDatabase\")\n```\n\nLANGUAGE: python\nCODE:\n```\nschema = [\n        {\"name\": \"id\", \"type\": \"str\"},\n        {\"name\": \"name\", \"type\": \"str\"},\n        {\"name\": \"description\", \"type\": \"str\"},\n        {\"name\": \"summary\", \"type\": \"str\"},\n        {\"name\": \"title\", \"type\": \"str\"},\n        {\"name\": \"visitors\", \"type\": \"int64\"},\n        {\"name\": \"description_embedding\", \"type\": \"float64s\"},\n    ]\n```\n\nLANGUAGE: python\nCODE:\n```\n# Define the index\nindexes = [\n    {\n        'type': 'hnsw',\n        'name': 'hnsw_index',\n        'column': 'description_embedding',\n        'params': {'dims': 384, 'metric': \"L2\"},\n    },\n]\n```\n\nLANGUAGE: python\nCODE:\n```\ntable = db.create_table(table=\"ai_tools\", schema=schema, indexes=indexes)\n```\n\n----------------------------------------\n\nTITLE: Performing Sentiment Analysis on Dataset\nDESCRIPTION: Iterate through reviews DataFrame and apply sentiment analysis to extract sentiment labels and scores\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nres = {}\nfor _, row in tqdm(reviews_df.iterrows(), total=len(reviews_df)):\n    myid = row[\"Review_ID\"]\n    try:\n        res[myid] = sentiment(row[\"Review_Text\"])\n    except RuntimeError:\n        print(f\"Broke for id {myid}\")\n\nsentiments_df = (\n    pd.DataFrame(res)\n    .T.reset_index()\n    .rename(columns={\"index\": \"Review_ID\", 0: \"Label\", 1: \"Score\"})\n)\n\nreview_sentiments_df = sentiments_df.merge(reviews_df, how=\"left\")\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for RAG Implementation\nDESCRIPTION: Imports necessary Python libraries for file handling, data processing, and implementing RAG with LlamaIndex and KDB.AI. This includes modules for reading documents, handling vector stores, parsing text nodes, and interfacing with OpenAI embeddings and LLMs.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_advanced_RAG/KDBAI_Advanced_RAG_Demo.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom getpass import getpass\nimport re\nimport os\nimport shutil\nimport time\nimport urllib\nimport datetime \n\nimport pandas as pd\n\nfrom llama_index.core import (\n    Settings,\n    SimpleDirectoryReader,\n    StorageContext,\n    VectorStoreIndex,\n)\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.retrievers import VectorIndexRetriever\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.vector_stores.kdbai import KDBAIVectorStore\n\nimport kdbai_client as kdbai\n\nOUTDIR = \"pdf\"\nRESET = True\n```\n\n----------------------------------------\n\nTITLE: Defining KDB.AI Table Schema with Vector Embeddings Column\nDESCRIPTION: Sets up the schema for the KDB.AI table, specifying data types for each movie metadata column and defining a 384-dimensional vector column for embeddings with Euclidean (L2) distance metric.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/metadata_filtering/metadata_filtering_demo.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n#Set up the schema and indexes for KDB.AI table, specifying embeddings column with 384 dimensions, Euclidean Distance, and flat index\ntable_schema = [\n    {\"name\": \"ReleaseYear\", \"type\": \"int64\"},\n    {\"name\": \"Title\", \"type\": \"bytes\"},\n    {\"name\": \"Origin\", \"type\": \"str\"},\n    {\"name\": \"Director\", \"type\": \"bytes\"},\n    {\"name\": \"Cast\", \"type\": \"bytes\"},\n    {\"name\": \"Genre\", \"type\": \"str\"},\n    {\"name\": \"Plot\", \"type\": \"bytes\"},\n    {\"name\": \"embeddings\", \"type\": \"float64s\"}\n]\n\nindexes = [\n    {\n        \"name\": \"flat_index\",\n        \"type\": \"flat\",\n        \"column\": \"embeddings\",\n        \"params\": {\"dims\": 384, \"metric\": \"L2\"},\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Hybrid Search with Dense Bias using KDB.AI in Python\nDESCRIPTION: This Python snippet executes a hybrid search in KDB.AI with a greater emphasis on dense results, demonstrating how to tilt search results using adjustable index weights.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ntable.search(\n    vectors={\"sparse_index\": sparse_query,\"dense_index\": dense_query},\n    index_params={\"sparse_index\":{'weight':0.1} ,\"dense_index\":{'weight':0.9}},\n    n=5\n)[0][['ID','chunk']]\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into KDB.AI Table\nDESCRIPTION: Inserts the DataFrame containing document chunks and their vector representations into the KDB.AI table.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n### Insert the dataframe into the KDB.AI table\ntable.insert(df)\n```\n\n----------------------------------------\n\nTITLE: Displaying Related Chunks to the Query\nDESCRIPTION: Displays the most related chunks from the documentation based on a similarity search for a given query. This is a preparatory step before running the QA chains on these chunks.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Show the most related chunks to the query\nquery_sim\n```\n\n----------------------------------------\n\nTITLE: Automating Sentiment Analysis for Reviews in Python\nDESCRIPTION: This function automates the process of finding similar reviews, displaying the top 3 most similar ones, and visualizing the sentiment distribution for a given query.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ndef review_sentiment_for_query(\n    table, embedding_model, query: str, n_results: int\n) -> None:\n    # find similar reviews\n    query_results = search_and_extract_results(\n        table, embedding_model, query, n_results=n_results\n    )\n\n    # top 3 most similar\n    print(query_results[:3][\"Review_Text\"])\n\n    # sentiment of similar reviews\n    plot_sentiment(sentiment_counts_by_branch(query_results), query)\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into a KDB.AI Table in Python\nDESCRIPTION: This snippet inserts data into the 'trade_tsc' table in batches of 1000 rows, updating memory usage to track the insertion process's impact. It utilizes the tqdm library for providing a progress bar during data insertion.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Temporal_Similarity_Search_Transformed_Demo.ipynb#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nn = 1000  # number of rows per batch\n\nfor i in tqdm(range(0, vecdf.shape[0], n)):\n    table_tsc.insert(vecdf[i:i+n].reset_index(drop=True))\n\nmemory_tsc_post_populate=get_memory_usage()\n```\n\n----------------------------------------\n\nTITLE: Performing Vector Similarity Search in KDB.AI using Python\nDESCRIPTION: Executes a similarity search on the KDB.AI table using a query vector, retrieving the top 10 most similar vectors.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Temporal_Similarity_Search_Transformed_Demo.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nq = vecdf['price'][100].tolist()\n```\n\nLANGUAGE: python\nCODE:\n```\nplt.plot(q) # See what the search vector looks like\nplt.grid(True)\nplt.title('Query Vector')\n```\n\nLANGUAGE: python\nCODE:\n```\nraw_start=datetime.datetime.now()\nres = table_raw.search(vectors={\"flat_index\":[q]}, n=10)[0]\nraw_stop=datetime.datetime.now()\nmemory_raw_windows_post_search=get_memory_usage()\nres\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for RAG with LangChain and KDB.AI\nDESCRIPTION: Installs necessary Python packages including kdbai_client, langchain, langchain_openai, and langchain-huggingface. It also clones a specific branch of the langchain repository and installs it.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install kdbai_client langchain langchain_openai langchain-huggingface #langchain-community \n\nimport os\n!git clone -b KDBAI_v1.4 https://github.com/KxSystems/langchain.git\nos.chdir('langchain/libs/community')\n!pip install .\n```\n\n----------------------------------------\n\nTITLE: Querying RAG Pipeline for Information Found in Source Data\nDESCRIPTION: This code snippet demonstrates how to query the RAG pipeline for information that exists within the provided data source. The example asks about the 'needle in a haystack method' which is contained in the source material.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaParse_pdf_RAG/llamaParse_demo.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nprint(RAG(\"describe the needle in a haystack method only using the provided information\"))\n```\n\n----------------------------------------\n\nTITLE: Defining Database Schema\nDESCRIPTION: Creates a schema definition for the table that will store document chunks and vectors, including ID, chunk text, sparse vectors, and dense vectors.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nschema = [\n  {\"name\": \"ID\", \"type\": \"str\"},\n  {\"name\": \"chunk\", \"type\": \"str\"},\n  {\n      \"name\":\"sparse\",\n      \"type\":\"general\",\n  },\n  {\n      \"name\":\"dense\",\n      \"type\":\"float64s\",\n  },\n]\n```\n\n----------------------------------------\n\nTITLE: Creating Query Vector for KDB.AI Vector Database Search in Python\nDESCRIPTION: This snippet demonstrates how to create a query vector by converting text to an embedding. The resulting vector will be used to search the KDB.AI vector database for relevant data.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/multimodal_RAG_unified_text/multi_modal_demo.ipynb#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nquery_vector = [text_to_embedding(\"what is the purpose of having antlers?\")]\n```\n\n----------------------------------------\n\nTITLE: Querying and Displaying KDB.AI Table Contents\nDESCRIPTION: Retrieves data from the KDB.AI table and displays a preview using the previously defined show_df helper function, allowing verification that the data was correctly inserted.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/metadata_filtering/metadata_filtering_demo.ipynb#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n#View contents of the table\nshow_df(table.query())\n```\n\n----------------------------------------\n\nTITLE: Generating Sparse and Dense Vectors for Each Chunk\nDESCRIPTION: Processes each document chunk to create sparse and dense vector representations, then adds them to the DataFrame with a unique ID.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n### Create sparse and dense vectors of each chunk, append to the dataframe\n\nid = 0\nfor chunk in pages:\n    ### Create the dense query vector\n    dense_chunk = [embedding_model.encode(chunk).tolist()]\n\n    ### Create the sparse query vector\n    sparse_chunk = [dict(Counter(y)) for y in token([chunk], padding=True,max_length=None)['input_ids']]\n    sparse_chunk[0].pop(101);sparse_chunk[0].pop(102);\n\n    new_row_df = pd.DataFrame([{\"ID\": str(id), \"chunk\": chunk, \"dense\": dense_chunk[0], \"sparse\": sparse_chunk[0]}])\n    df = pd.concat([df, new_row_df], ignore_index=True)\n    id += int(1)\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Performing Multi-Vector Search in KDB.AI Table - Python\nDESCRIPTION: This snippet demonstrates how to search the KDB.AI table using multiple query vectors, allowing for the retrieval of several results based on different semantic queries.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/quickstarts/python_quickstart.ipynb#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nquery1 = \"A company with a music-related product\"\nquery2 = \"A social media company\"\n\nquery1_vector = list(embedding_model.embed([query1]))[0].tolist()\nquery2_vector = list(embedding_model.embed([query2]))[0].tolist()\n\ntable.search(\n    vectors={'vectorIndex': [\n        query1_vector,\n        query2_vector,\n    ]},\n    n=3,\n    aggs={'Company Name': 'company_name'}\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Query Vectors for Search\nDESCRIPTION: Generates sparse and dense vector representations for a search query \"12-month basis\" to be used in vector searches.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nquery = '12-month basis'\n\n### Create the dense query vector\ndense_query = [embedding_model.encode(query).tolist()]\n\n### Create the sparse query vector\nsparse_query = [dict(Counter(y)) for y in token([query], padding=True,max_length=None)['input_ids']]\nsparse_query[0].pop(101);sparse_query[0].pop(102);\n```\n\n----------------------------------------\n\nTITLE: Installing KDB.AI Client Library in Python\nDESCRIPTION: This snippet shows how to install the kdbai_client library using pip. This is a prerequisite for interacting with KDB.AI.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/managing_tables.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install kdbai_client\n```\n\n----------------------------------------\n\nTITLE: Adding Embedded Data to KDB.AI Table in Python\nDESCRIPTION: This snippet stores the document embeddings into the defined KDB.AI table by utilizing the KDBAI method, facilitating efficient vector-based retrieval for future queries.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation_evaluation.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# use KDBAI as vector store\nvecdb_kdbai = KDBAI(table, embeddings)\nvecdb_kdbai.add_texts(texts=pages)\n```\n\n----------------------------------------\n\nTITLE: Creating Database Table with Schema and Indexes\nDESCRIPTION: Creates a new table named \"inflation\" with the defined schema and indexes for vector search.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# Create the table with the defined schema and indexes from above\ntable = db.create_table(table=\"inflation\", schema=schema, indexes=indexes)\n```\n\n----------------------------------------\n\nTITLE: RAG Implementation Functions\nDESCRIPTION: Defines the RAG function that processes retrieved context with OpenAI's GPT-4 model.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/rag_example.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef RAG(retrieved_data,prompt):\n    messages = \"Answer the following query in three sentences based on the context and only the context: \" + \"\\n\"\n    messages += prompt + \"\\n\"\n    if len(retrieved_data) > 0:\n        messages += \"Context: \" + \"\\n\"\n        for data in retrieved_data:\n            messages += data.decode('utf-8') + \"\\n\"\n    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n    response = openai.chat.completions.create(\n        model=\"gpt-4-turbo-preview\",\n        messages=[\n            {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": messages},\n            ],\n            },\n        ],\n        max_tokens=300,\n    )\n    content = response.choices[0].message.content\n    return content\n```\n\n----------------------------------------\n\nTITLE: Performing Semantic Search with KDB.AI - Python\nDESCRIPTION: This series of snippets demonstrate how to encode a search term with a Sentence Transformer model and search the KDB.AI vector database for similar sentences. It returns the top three most similar vectors based on nearest neighbor distances.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nsearch_term1 = \"number of interstellar objects in the milky way\"\n```\n\nLANGUAGE: python\nCODE:\n```\nencoded_search_term1 = model.encode(search_term1).tolist()\n```\n\nLANGUAGE: python\nCODE:\n```\nresults1 = table.search(vectors={\"hnsw_index\":[encoded_search_term1]}, n=3)\nresults1[0]\n```\n\nLANGUAGE: python\nCODE:\n```\nsearch_term2 = \"how does planet formation occur\"\n```\n\nLANGUAGE: python\nCODE:\n```\nencoded_search_term2 = model.encode(search_term2).tolist()\n```\n\nLANGUAGE: python\nCODE:\n```\nresults2 = table.search(vectors={\"hnsw_index\":[encoded_search_term2]}, n=3)\nresults2[0]\n```\n\n----------------------------------------\n\nTITLE: Embedding queries with Hugging Face Inference API using Python\nDESCRIPTION: Use the Hugging Face Inference API to embed a text query for similarity search. Requires the requests library and an API token (`HF_TOKEN`). Sends a POST request for feature extraction and handles potential request errors.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/HuggingFace_search/huggingface_inference.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\nembedding_url = \"https://api-inference.huggingface.co/pipeline/feature-extraction/BAAI/bge-small-en-v1.5\"\n\ndef generate_query_embedding(text: str) -> list[float]:\n    response = requests.post(\n        embedding_url,\n        headers={\"Authorization\": f\"Bearer {HF_TOKEN}\", \"x-wait-for-model\": \"true\"},        \n        json={\"inputs\": text}\n    )\n\n    if response.status_code != 200:\n        raise ValueError(f\"Request failed with status code {response.status_code}: {response.text}\")\n    return response.json()\n\nquery = \"AI tool for creating 3D textures\"\nquery_embedding = generate_query_embedding(query)\n```\n\n----------------------------------------\n\nTITLE: KDB.AI Session Configuration\nDESCRIPTION: Setting up connection to KDB.AI cloud service using endpoint and API key.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/qHnsw_index_pdf_search/pdf_qHNSW_Search.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nKDBAI_ENDPOINT = (\n    os.environ[\"KDBAI_ENDPOINT\"]\n    if \"KDBAI_ENDPOINT\" in os.environ\n    else input(\"KDB.AI endpoint: \")\n)\nKDBAI_API_KEY = (\n    os.environ[\"KDBAI_API_KEY\"]\n    if \"KDBAI_API_KEY\" in os.environ\n    else getpass(\"KDB.AI API key: \")\n)\n\nsession = kdbai.Session(api_key=KDBAI_API_KEY, endpoint=KDBAI_ENDPOINT)\n```\n\n----------------------------------------\n\nTITLE: Creating Vector Store and Index\nDESCRIPTION: Sets up the KDB.AI vector store with hybrid search capability, configures storage context and index settings, and creates the vector index from documents.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Hybrid_Search_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\n# Vector Store\ntext_store = KDBAIVectorStore(table=table, hybrid_search=True)\n\n# Storage context\nstorage_context = StorageContext.from_defaults(vector_store=text_store)\n\n# Settings\n#Settings.callback_manager = callback_manager\nSettings.transformations = [SentenceSplitter(chunk_size=500, chunk_overlap=0)]\nSettings.embed_model = embeddings_model\nSettings.llm = None\n\n# Vector Store Index\nindex = VectorStoreIndex.from_documents(\n    docs,\n    use_async=True,\n    storage_context=storage_context,\n)\n```\n\n----------------------------------------\n\nTITLE: Filtered Search for Art Thrillers with Tom Hanks\nDESCRIPTION: Performs a vector similarity search for the art conspiracy query with additional filters for thriller genre and Tom Hanks in the cast, showing how multiple 'like' filters can be combined.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/metadata_filtering/metadata_filtering_demo.ipynb#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n# Another filtered search example\nprint(table.search(vectors=query_vector, n=3, filter=[(\"like\", \"Genre\", \"*thriller*\"),(\"like\",\"Cast\",\"*Tom Hanks*\")]))\n```\n\n----------------------------------------\n\nTITLE: Setting up KDB.AI Connection\nDESCRIPTION: Establishing connection to KDB.AI using endpoint and API key credentials\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/fuzzy_filtering_on_metadata/fuzzy_filtering_demo.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\nimport kdbai_client as kdbai\nimport time\n```\n\nLANGUAGE: python\nCODE:\n```\nKDBAI_ENDPOINT = (\n    os.environ[\"KDBAI_ENDPOINT\"]\n    if \"KDBAI_ENDPOINT\" in os.environ\n    else input(\"KDB.AI endpoint: \")\n)\nKDBAI_API_KEY = (\n    os.environ[\"KDBAI_API_KEY\"]\n    if \"KDBAI_API_KEY\" in os.environ\n    else getpass(\"KDB.AI API key: \")\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nsession = kdbai.Session(api_key=KDBAI_API_KEY, endpoint=KDBAI_ENDPOINT)\n```\n\n----------------------------------------\n\nTITLE: Generating Contextualized Table Descriptions - Python\nDESCRIPTION: This snippet defines a function that prompts the OpenAI model to generate a detailed description of a table based on its content and document context, returning a formatted markdown output.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/unstructured_io_RAG/Table_RAG_Unstructured_KDBAI_LangChain_RAG.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport openai\nfrom openai import OpenAI\n\n# Initialize the OpenAI client\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n\ndef get_table_description(table_content, document_context):\n    prompt = f\"\"\"\n    Given the following table and its context from the original document,\n    provide a detailed description of the table. Then, include the table in markdown format.\n\n    Original Document Context:\n    {document_context}\n\n    Table Content:\n    {table_content}\n\n    Please provide:\n    1. A comprehensive description of the table.\n    2. The table in markdown format.\n    \"\"\"\n\n    response = client.chat.completions.create(\n        model=\"gpt-4o-2024-08-06\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant that describes tables and formats them in markdown.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n    )\n\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Optimizing Sparse Search Parameters in KDB.AI with Python\nDESCRIPTION: This snippet showcases the dynamic tuning of b and k hyperparameters for a sparse search in KDB.AI. It allows developers to optimize search relevance by adjusting document length impact and term saturation levels according to specific use cases.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ntable.search(\n    vectors={\"sparse_index\": sparse_query,\"dense_index\": dense_query},\n    index_params={\"sparse_index\":{'weight':0.1,'b':0.1, 'k':3} ,\"dense_index\":{'weight':0.9}},\n    n=5\n)[0][['ID','chunk']]\n```\n\n----------------------------------------\n\nTITLE: Defining KDB.AI Table Schema\nDESCRIPTION: Creates table schema with text and embedding columns, including index configuration for vector search.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/rag_example.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nschema = [\n    dict(name=\"text\", type=\"bytes\"),\n    dict(name=\"embedding\", type=\"float32s\")\n]\nindex_name = \"flat_index\"\nindexes = [dict(name=index_name, column=\"embedding\", type=\"flat\", params=dict(metric=\"L2\", dims=384))]\n\ntable = database.create_table(KDBAI_TABLE_NAME, schema=schema, indexes=indexes)\n```\n\n----------------------------------------\n\nTITLE: Creating Table with Schema and Indexes\nDESCRIPTION: Creates a new table with the specified schema and both sparse and dense indexes.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Hybrid_Search_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# texts table\ntable = db.create_table(table_name, table_schema, indexes=[indexFlat, indexeSparse])\n```\n\n----------------------------------------\n\nTITLE: Processing and Displaying Retrieved Data from KDB.AI Vector Database in Python\nDESCRIPTION: This snippet processes the search results from the KDB.AI vector database. It handles both image and text data, displaying images and printing text content. The processed data is stored for use in RAG.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/multimodal_RAG_unified_text/multi_modal_demo.ipynb#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nretrieved_data_for_RAG = []\nfor index, row in results[0].iterrows():\n    if row[2] == 'image':\n        retrieved_data_for_RAG.append(row[3])\n        image = Image.open(row[1])\n        display(image)\n    elif row[2] == 'text':\n        retrieved_data_for_RAG.append(row[3])\n        print(row[3])\n```\n\n----------------------------------------\n\nTITLE: Adding Embedded Data to KDB.AI Table\nDESCRIPTION: Stores the embedded text chunks in the KDB.AI vector database table, preparing the data for similarity searches and retrieval augmented generation.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# use KDBAI as vector store\nvecdb_kdbai = KDBAI(table, embeddings)\nvecdb_kdbai.add_texts(texts=pages)\n```\n\n----------------------------------------\n\nTITLE: Defining Table Schema and Index\nDESCRIPTION: Creates the table schema with various data types and defines an HNSW index for vector similarity search.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/making_queries.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nschema = [\n    {'name': 'id', 'type': 'int32'},\n    {'name': 'name', 'type': 'str'},\n    {'name': 'age', 'type': 'int16'},\n    {'name': 'city', 'type': 'str'},\n    {'name': 'description', 'type': 'str'},\n    {'name': 'embeddings', 'type': 'float32s'}\n]\nindex_name = 'hnws_index'\nindexes = [{'name': index_name, 'column': 'embeddings', 'type': 'hnsw', 'params': {'dims': 384}}]\n```\n\n----------------------------------------\n\nTITLE: Testing RAG Implementation with a Query Example in Python\nDESCRIPTION: Demonstrates how to call the RAG function with a sample query to retrieve and generate information about training strategies. The function would combine semantic search with language model generation to provide comprehensive answers.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaParse_pdf_RAG/llamaParse_demo.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nprint(RAG(\"describe the training strategy in depth\"))\n```\n\n----------------------------------------\n\nTITLE: Setting Up KDB.AI Cloud Connection in Python\nDESCRIPTION: Retrieves KDB.AI Cloud credentials from environment variables or user input, and establishes a connection to the KDB.AI Cloud session.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/HuggingFace_search/huggingface_inference.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nKDBAI_ENDPOINT = (\n    os.environ[\"KDBAI_ENDPOINT\"]\n    if \"KDBAI_ENDPOINT\" in os.environ\n    else input(\"KDB.AI endpoint: \")\n)\nKDBAI_API_KEY = (\n    os.environ[\"KDBAI_API_KEY\"]\n    if \"KDBAI_API_KEY\" in os.environ\n    else getpass(\"KDB.AI API key: \")\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nHF_TOKEN = (\n    os.environ[\"HF_TOKEN\"]\n    if \"HF_TOKEN\" in os.environ\n    else getpass(\"Hugging Face token: \")\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nsession = kdbai.Session(api_key=KDBAI_API_KEY, endpoint=KDBAI_ENDPOINT)\n```\n\n----------------------------------------\n\nTITLE: Viewing Complete Multimodal DataFrame\nDESCRIPTION: Displays the entire DataFrame containing both text and image data with their respective embeddings to verify all content has been processed successfully before loading into KDB.AI.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/multimodal_RAG_unified_text/multi_modal_demo.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Print the entire dataframe\ndf\n```\n\n----------------------------------------\n\nTITLE: Defining KDB.AI Table Schema in Python\nDESCRIPTION: Creates a schema for the KDB.AI table to store time series vectors, including columns for index, symbol, time, and price.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Temporal_Similarity_Search_Transformed_Demo.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Define the schema with similar columns to the dataframe. The price column is where the time series vectors will be inserted\n# We are doing similarity search on the raw time series vectors created above\nschema_raw = [\n        {\n            \"name\":'index',\n            \"type\":'int64'\n        },\n        {\n            \"name\":'sym',\n            \"type\":'str'\n        },\n        {\n            \"name\":'time',\n            \"type\":'datetime64[ns]'\n        },\n        {\n            \"name\":'price',\n            \"type\":'float64s',\n        }\n      ]\n```\n\n----------------------------------------\n\nTITLE: Defining Vector Database Table Schema\nDESCRIPTION: Creates a schema definition for the KDB.AI table, specifying columns for storing sentences as strings and vectors as float64 arrays.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\npdf_schema = [\n        {\"name\": \"sentences\", \"type\": \"str\"},\n        {\n            \"name\": \"vectors\",\n            \"type\": \"float64s\",\n        },\n    ]\n```\n\n----------------------------------------\n\nTITLE: Initializing Tokenizer and Embedding Model\nDESCRIPTION: Sets up the BERT tokenizer for sparse vector creation and SentenceTransformer model for dense vector embeddings.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n### Tokenizer to create sparse vectors\ntoken = BertTokenizerFast.from_pretrained('bert-base-uncased')\n\n### Embedding model to be used to embed user input query\nfrom sentence_transformers import SentenceTransformer\nembedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Installs the kdbai_client and fastembed packages needed for KDB.AI operations and text embedding.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/making_queries.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install kdbai_client fastembed\n```\n\n----------------------------------------\n\nTITLE: Defining KDB.AI Table Schema for Review Data - Python\nDESCRIPTION: This snippet defines the schema for a KDB.AI table to store review data with appropriate types for each field. This schema will be used when creating the actual table in KDB.AI.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nreview_schema = [\n        {\"name\": \"Branch\", \"type\": \"str\"},\n        {\"name\": \"Label\", \"type\": \"str\"},\n        {\"name\": \"Score\", \"type\": \"float64\"},\n        {\"name\": \"Rating\", \"type\": \"int64\"},\n        {\"name\": \"Review_Text\", \"type\": \"str\"},\n        {\n            \"name\": \"embeddings\",\n            \"type\":\"float64s\",\n        },\n    ]\n```\n\n----------------------------------------\n\nTITLE: Creating KDB.AI Vector Table for Multimodal RAG\nDESCRIPTION: Creates a new table named \"multi_modal_demo\" in the KDB.AI database with the defined schema and vector index configuration. The table will use a flat index on the embeddings column with cosine similarity as the distance metric.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/multimodal_RAG_unified_text/multi_modal_demo.ipynb#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# Create the table called \"multi_modal_demo\"\ntable = database.create_table(\"multi_modal_demo\", schema=table_schema, indexes=indexes)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI Models for Embeddings and Text Generation\nDESCRIPTION: Sets up the OpenAI embedding model (text-embedding-3-small) and language model (gpt-4o-mini) for use in the RAG pipeline. These models are configured as the default settings for LlamaIndex to use for embeddings and text generation.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_advanced_RAG/KDBAI_Advanced_RAG_Demo.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nEMBEDDING_MODEL  = \"text-embedding-3-small\"\nGENERATION_MODEL = 'gpt-4o-mini'\n\nllm = OpenAI(model=GENERATION_MODEL)\nembed_model = OpenAIEmbedding(model=EMBEDDING_MODEL)\n\nSettings.llm = llm\nSettings.embed_model = embed_model\n```\n\n----------------------------------------\n\nTITLE: Splitting Document into Chunks\nDESCRIPTION: Applies the text splitter to divide the document into chunks and extracts the content of each chunk.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n### split_documents produces a list of all the chunks created\npages = [p.page_content for p in text_splitter.split_documents(doc)]\n```\n\n----------------------------------------\n\nTITLE: Filtered Search for Fantasy Movies from 2000-2010\nDESCRIPTION: Performs a vector similarity search for the Middle Earth query with a date range filter, demonstrating the 'within' operator for numeric range filtering on release years.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/metadata_filtering/metadata_filtering_demo.ipynb#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n# Another filtered search example\nprint(table.search(vectors=query_vector, n=3, filter=[(\"within\",\"ReleaseYear\",[2000,2010])]))\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame from Company Data - Python\nDESCRIPTION: This snippet creates a Pandas DataFrame from the company data, encapsulating company names, descriptions, and their corresponding embedding vectors into the required format for storage into the KDB.AI table.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/quickstarts/python_quickstart.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nnames = [company for company, _ in company_data]\ndescriptions = [description for _, description in company_data]\n\n# column names/types matching the schema\nembeddings_df = pd.DataFrame({\"company_name\": names, \"company_description\": descriptions, \"vectors\": list(embeddings)})\nembeddings_df.head()\n```\n\n----------------------------------------\n\nTITLE: KDB.AI Session and Table Setup\nDESCRIPTION: Establishes KDB.AI session and handles table creation/deletion.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/rag_example.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nKDBAI_TABLE_NAME = \"paul_graham\"\nsession = kdbai.Session(endpoint=KDBAI_ENDPOINT, api_key=KDBAI_API_KEY)\ndatabase = session.database(\"default\")\n\n# Drop existing table if it exists\ntry:\n    database.table(KDBAI_TABLE_NAME).drop()\nexcept kdbai.KDBAIException:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Executing Query and Printing Response\nDESCRIPTION: Performs a query using the search term and prints the generated response from the query engine\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/qHnsw_index_pdf_search/pdf_qHNSW_Search.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nresult = query_engine.query(search_term2)\nprint(result.response)\n```\n\n----------------------------------------\n\nTITLE: Configuring KDB.AI Vector Index in Python\nDESCRIPTION: Defines the vector index configuration for the KDB.AI table, specifying the index type, name, column, and parameters like dimensionality and similarity metric.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Temporal_Similarity_Search_Transformed_Demo.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Define the index\nindexes_raw = [\n    {\n        'type': 'flat',\n        'name': 'flat_index',\n        'column': 'price',\n        'params': {'dims': 1000, 'metric': \"L2\"},\n    },\n]\n```\n\n----------------------------------------\n\nTITLE: Creating KDB.AI Table in Python\nDESCRIPTION: Drops any existing table named 'trade_raw' and creates a new one with the defined schema and indexes.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Temporal_Similarity_Search_Transformed_Demo.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# First ensure the table does not already exist\ntry:\n    db.table(\"trade_raw\").drop()\nexcept kdbai.KDBAIException:\n    pass\n```\n\nLANGUAGE: python\nCODE:\n```\ntable_raw = db.create_table(table='trade_raw', schema=schema_raw, indexes=indexes_raw)\n```\n\n----------------------------------------\n\nTITLE: Executing RAG with Retrieved Data and Prompt in Python\nDESCRIPTION: This snippet demonstrates how to use the RAG function with the previously retrieved data and a specific prompt. It prints the generated response from the LLM.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/multimodal_RAG_unified_text/multi_modal_demo.ipynb#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nprompt = \"what is the purpose of having antlers?\"\nquery = \"You will answer the given prompt using attached content: \"+prompt\nresponse = RAG(retrieved_data_for_RAG, query)\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Creating a New KDB.AI Table with Schema and Index\nDESCRIPTION: Creates a new table named 'metadata_demo' in the KDB.AI database with the previously defined schema and vector index configuration.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/metadata_filtering/metadata_filtering_demo.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n#Create the table called \"metadata_demo\"\ntable = database.create_table(\"metadata_demo\", schema = table_schema, indexes = indexes)\n```\n\n----------------------------------------\n\nTITLE: Querying the KDB.AI Table\nDESCRIPTION: Runs a query on the KDB.AI table to retrieve the stored documents and their metadata. This allows verification that the documents were successfully indexed and stored in the vector database with their embeddings and metadata.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_advanced_RAG/KDBAI_Advanced_RAG_Demo.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ntable.query()\n```\n\n----------------------------------------\n\nTITLE: Creating a Database Called 'myDatabase' in KDB.AI - Python\nDESCRIPTION: This snippet attempts to drop an existing database named 'myDatabase' if it exists, and then creates a new database with that name in the KDB.AI session.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# ensure no database called \"myDatabase\" exists\ntry:\n    session.database(\"myDatabase\").drop()\nexcept kdbai.KDBAIException:\n    pass\n\n# Create the database\ndb = session.create_database(\"myDatabase\")\n```\n\n----------------------------------------\n\nTITLE: Generating Vector Embeddings for Sentences\nDESCRIPTION: Creates vector embeddings for all sentences using the transformer model, converts them to lists, and stores them in a DataFrame alongside the original sentences.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Create embeddings\nembeddings_list = model.encode(np.array(pdf_sentences)).tolist()\nembeddings_df = pd.DataFrame({\"vectors\": embeddings_list, \"sentences\": pdf_sentences})\n```\n\n----------------------------------------\n\nTITLE: Inserting Chunked Embeddings into KDB.AI Vector Database\nDESCRIPTION: Implements a robust data insertion strategy for large datasets by breaking embeddings into manageable chunks, handling data type conversions and preprocessing\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/music_recommendation/music_recommendation.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nchunk_size = 10_000\n\nfor index, row in embedded_song_df.iterrows():\n    cast = row['song_artists']\n    if 1 == len(cast):\n        embedded_song_df.loc[index, 'song_artists'] = 'None'\n\nfor i in tqdm(range((len(embedded_song_df) // chunk_size) + 1)):\n    index = i * chunk_size\n    data = embedded_song_df.iloc[index : index + chunk_size].reset_index(drop=True)\n    data['song_artists'] = data['song_artists'].str.encode('utf-8')\n    table.insert(data)\n```\n\n----------------------------------------\n\nTITLE: KDB.AI Session Setup\nDESCRIPTION: Establishing connection to KDB.AI vector database and creating necessary table schema with vector index.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Sub_Question_Query_Engine_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport kdbai_client as kdbai\nfrom getpass import getpass\n```\n\nLANGUAGE: python\nCODE:\n```\nKDBAI_ENDPOINT = (\n    os.environ[\"KDBAI_ENDPOINT\"]\n    if \"KDBAI_ENDPOINT\" in os.environ\n    else input(\"KDB.AI endpoint: \")\n)\nKDBAI_API_KEY = (\n    os.environ[\"KDBAI_API_KEY\"]\n    if \"KDBAI_API_KEY\" in os.environ\n    else getpass(\"KDB.AI API key: \")\n)\n\nsession = kdbai.Session(api_key=KDBAI_API_KEY, endpoint=KDBAI_ENDPOINT)\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into KDB.AI Vector Database Table in Python\nDESCRIPTION: This snippet demonstrates how to insert data into a KDB.AI table in batches using a pandas DataFrame. It uses tqdm for progress tracking and processes the data in chunks of 2000 rows.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/multimodal_RAG_unified_text/multi_modal_demo.ipynb#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n#Insert the data into the table, split into 2000 row batches\nfrom tqdm import tqdm \nn = 2000  # chunk row size\n\nfor i in tqdm(range(0, df.shape[0], n)):\n    table.insert(df[i:i+n].reset_index(drop=True))\n```\n\n----------------------------------------\n\nTITLE: Defining Table Schema Configuration in Python\nDESCRIPTION: This snippet defines the schema for a table in KDB.AI, where the time series vectors will be inserted. The schema includes fields for index, sym, time, and price. The snippet requires importing necessary modules and assumes a KDB.AI database connection is established.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Temporal_Similarity_Search_Transformed_Demo.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Define the schema with similar columns to the dataframe. The price column is where the time series vectors will be inserted\nschema_tsc = [\n        {\n            \"name\":'index',\n            \"type\":'int64'\n        },\n        {\n            \"name\":'sym',\n            \"type\":'str'\n        },\n        {\n            \"name\":'time',\n            \"type\":'datetime64[ns]'\n        },\n        {\n            \"name\":'price',\n            \"type\":'float64s',\n        }\n      ]\n```\n\n----------------------------------------\n\nTITLE: Creating a Conciseness Evaluator with LangChain\nDESCRIPTION: Loads a LangChain evaluator configured to assess the conciseness of answers. This evaluator will determine if the QA bot's responses are appropriately brief and to the point.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation_evaluation.ipynb#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nconcise_evaluator = load_evaluator(\n    \"criteria\", criteria=\"conciseness\", llm=evaluation_llm\n)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Gramm-Leach-Bliley Act Effectiveness Before 2008 Crisis\nDESCRIPTION: Queries the RAG system about the effectiveness of the Gramm-Leach-Bliley Act of 1999 in preventing the 2008 financial crisis. The query searches for strengths and weaknesses of the act in regulating the US stock market, using the pre-crisis document filter.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_advanced_RAG/KDBAI_Advanced_RAG_Demo.ipynb#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\nresult = query_engine.query(\n    \"\"\"\n    Is the Gramm-Leach-Bliley Act of 1999 enough to prevent the 2008 crisis. Search the document and explain its strenghts and weaknesses to regulate the US stock market.\n    \"\"\"\n)\nprint(result.response)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for LlamaIndex and KDB.AI\nDESCRIPTION: Installation commands for required Python packages to work with LlamaIndex, HuggingFace embeddings, OpenAI, and KDB.AI.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Hybrid_Search_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install llama-index llama-index-embeddings-huggingface llama-index-llms-openai llama-index-readers-file llama-index-vector-stores-kdbai\n%pip install kdbai_client langchain-text-splitters pandas\n```\n\n----------------------------------------\n\nTITLE: Embedding Review Texts with Sentence Transformers - Python\nDESCRIPTION: This snippet initializes a SentenceTransformer model for encoding review texts into embeddings, which can be stored in KDB.AI. The model from Hugging Face is instantiated for use.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nembedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n```\n\n----------------------------------------\n\nTITLE: Dropping Existing Table If Present\nDESCRIPTION: Attempts to drop the table if it already exists, with error handling if the table doesn't exist.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Hybrid_Search_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Drop table if exists\ntry:\n    db.table(table_name).drop()\nexcept kdbai.KDBAIException:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Creating Vector Embedding for Star Wars Query\nDESCRIPTION: Converts a natural language query about Star Wars and Luke Skywalker into a vector embedding for similarity search in the KDB.AI vector database, formatting it for the flat_index.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/metadata_filtering/metadata_filtering_demo.ipynb#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n#Embed a query\nquery_vector = {'flat_index' : [embedding_model.encode('star wars Luke Skywalker').tolist()]}\n```\n\n----------------------------------------\n\nTITLE: Defining OpenAI Text Embedding Model\nDESCRIPTION: Sets up the OpenAI embedding model 'text-embedding-3-small' for converting text chunks into vector embeddings, which will be used for similarity searches in the vector database.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n```\n\n----------------------------------------\n\nTITLE: Defining KDB.AI Table Schema in Python\nDESCRIPTION: This code snippet defines the schema for the vector database table in KDB.AI, specifying data types for storing embeddings and related information during evaluations.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation_evaluation.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nrag_eval_schema = [\n    {\"name\": \"id\", \"type\": \"str\"},\n    {\"name\": \"text\", \"type\": \"bytes\"},\n    {\"name\": \"embeddings\", \"type\": \"float32s\"}\n]\nindexes = [{\"name\": \"flat_index\", \"type\": \"flat\", \"column\": \"embeddings\", \"params\": {\"dims\": 1536, \"metric\": \"L2\"}}]\n```\n\n----------------------------------------\n\nTITLE: Creating Overlapping Time Windows from Sensor Data\nDESCRIPTION: Splits the sensor time series into overlapping windows, each containing the specified number of consecutive measurements, for later pattern matching.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n# define windows\nwindows = [\n    sensor0_df.iloc[i : i + window_size]\n    for i in range(0, len(sensor0_df) - window_size + 1, step_size)\n]\n```\n\n----------------------------------------\n\nTITLE: Performing Dynamic Search with KDB.AI in Python\nDESCRIPTION: Demonstrates dynamic search capability by using a smaller query vector of 50 data points.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_non_transformed/Temporal_Similarity_Search_Non-Transformed_Demo.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nsmaller_query_50=q[:-950]\nprint(\"Query elements: \", len(smaller_query_50))\ntable.search(vectors={'price': [smaller_query_50]}, n=10, type=\"tss\")[0]\n```\n\n----------------------------------------\n\nTITLE: Defining KDB.AI Table Schema and Indexes\nDESCRIPTION: Sets up the table name, schema with columns, and creates both sparse (BM25) and dense (flat L2) vector indexes.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Hybrid_Search_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Table - name & schema\ntable_name = \"hs_docs\"\ntable_schema = [\n        {\"name\":\"document_id\", \"type\":\"bytes\"},\n        {\"name\":\"text\", \"type\":\"bytes\"},\n        {\"name\":\"embeddings\",\"type\":\"float32s\"},\n        {\"name\":\"sparseVectors\", \"type\":\"general\"},\n        {\"name\":\"title\", \"type\":\"str\"},\n        {\"name\":\"file_path\", \"type\":\"str\"},\n        ]\n\nindexeSparse = {\n        \"name\": \"sparse_index\",\n        \"type\": \"bm25\",\n        \"column\": \"sparseVectors\",\n        \"params\": {'k': 1.25, 'b': 0.75},\n    }\n\nindexFlat = {\n        \"name\": \"flat\",\n        \"type\": \"flat\",\n        \"column\": \"embeddings\",\n        \"params\": {'dims': 768, 'metric': 'L2'},\n    }\n```\n\n----------------------------------------\n\nTITLE: Setting Up KDB.AI Cloud Session\nDESCRIPTION: Configures a connection to KDB.AI Cloud using environment variables or user input for endpoint and API key.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Hybrid_Search_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nKDBAI_ENDPOINT = (\n    os.environ[\"KDBAI_ENDPOINT\"]\n    if \"KDBAI_ENDPOINT\" in os.environ\n    else input(\"KDB.AI endpoint: \")\n)\nKDBAI_API_KEY = (\n    os.environ[\"KDBAI_API_KEY\"]\n    if \"KDBAI_API_KEY\" in os.environ\n    else getpass(\"KDB.AI API key: \")\n)\n\nsession = kdbai.Session(api_key=KDBAI_API_KEY, endpoint=KDBAI_ENDPOINT)\n```\n\n----------------------------------------\n\nTITLE: Creating KDB.AI Table in Python\nDESCRIPTION: Creates a new table in the KDB.AI database using the defined schema.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_non_transformed/Temporal_Similarity_Search_Non-Transformed_Demo.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n#Create the table called \"trade_tss\"\ntable = database.create_table(\"trade_tss\",\n                              schema = schema)\n```\n\n----------------------------------------\n\nTITLE: Setting Up KDB.AI Cloud Credentials in Python\nDESCRIPTION: Retrieves KDB.AI Cloud endpoint and API key from environment variables or prompts the user for input.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_non_transformed/Temporal_Similarity_Search_Non-Transformed_Demo.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Set up KDB.AI endpoint and API key\nKDBAI_ENDPOINT = (\n    os.environ[\"KDBAI_ENDPOINT\"]\n    if \"KDBAI_ENDPOINT\" in os.environ\n    else input(\"KDB.AI endpoint: \")\n)\nKDBAI_API_KEY = (\n    os.environ[\"KDBAI_API_KEY\"]\n    if \"KDBAI_API_KEY\" in os.environ\n    else getpass(\"KDB.AI API key: \")\n)\n```\n\n----------------------------------------\n\nTITLE: Performing Basic Vector Similarity Search Without Filters\nDESCRIPTION: Executes a vector similarity search against the KDB.AI table using the Star Wars query embedding, returning the top 3 most similar movies without any metadata filtering.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/metadata_filtering/metadata_filtering_demo.ipynb#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n#Search vector db to find most relevant movies\nprint(table.search(vectors=query_vector, n=3))\n```\n\n----------------------------------------\n\nTITLE: Embedding Extracted Elements with OpenAI - Python\nDESCRIPTION: This snippet initializes an OpenAIEmbeddingEncoder with a configuration for embedding documents, and embeds the previously partitioned elements, preparing them for storage or further use.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/unstructured_io_RAG/Table_RAG_Unstructured_KDBAI_LangChain_RAG.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nembedding_encoder = OpenAIEmbeddingEncoder(\n    config=OpenAIEmbeddingConfig(\n      api_key=os.getenv(\"OPENAI_API_KEY\"),\n      model_name=\"text-embedding-3-small\",\n    )\n)\nelements = embedding_encoder.embed_documents(\n    elements=elements\n)\n```\n\n----------------------------------------\n\nTITLE: Searching with Filters in KDB.AI Table - Python\nDESCRIPTION: This snippet adds a filter condition to the search query, returning the three closest neighbors while excluding a specific company from the results.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/quickstarts/python_quickstart.ipynb#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# Find 3 closest neighbours of a single query vector\ntable.search(\n    vectors={'vectorIndex': [query_vector]},\n    n=3,\n    filter=[(\"<>\", \"company_name\", \"Booking.com\")],\n)[0]\n```\n\n----------------------------------------\n\nTITLE: Deleting KDB.AI Database and Table in Python\nDESCRIPTION: This snippet removes the tables and database from KDB.AI to clean up resources once the operations are complete. It ensures that all data structures are dropped for resource optimization.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Temporal_Similarity_Search_Transformed_Demo.ipynb#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ntable_raw.drop()\ntable_tsc.drop()\ndb.drop()\n```\n\n----------------------------------------\n\nTITLE: Querying Financial Data using RAG in Python\nDESCRIPTION: This code demonstrates how to use the RAG function to query financial data. It includes several example queries about revenue, costs, and financial outlook.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/unstructured_io_RAG/Table_RAG_Unstructured_KDBAI_LangChain_RAG.ipynb#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# Query the RAG chain!\nRAG(\"What is the research and development costs for six months ended in June 2024\")\n```\n\nLANGUAGE: python\nCODE:\n```\n# Query the RAG chain!\nRAG(\"What is the research and development costs for six months ended in June 2023\")\n```\n\nLANGUAGE: python\nCODE:\n```\n# Query the RAG chain!\nRAG(\"what is the 2024 GAAP advertising Revenue in the three months ended June 30th? What about net cash by operating activies\")\n```\n\nLANGUAGE: python\nCODE:\n```\n# Query the RAG chain!\nRAG(\"What segment made the most money in the six months ended June 30th?\")\n```\n\nLANGUAGE: python\nCODE:\n```\n# Query the RAG chain!\nRAG(\"what is the three month costs and expensis for 2023?\")\n```\n\nLANGUAGE: python\nCODE:\n```\n# Query the RAG chain!\nRAG(\"At the end of 2023, what was the value of Meta's Goodwill assets?\")\n```\n\nLANGUAGE: python\nCODE:\n```\n# Query the RAG chain!\nRAG(\"Given a sentiment score between 1 and 10 for the outlook? Explain your reasoning\")\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into KDB.AI Table - Python\nDESCRIPTION: This code snippet inserts embedded data into the KDB.AI table. It uses the insert method on the table object to add data from the 'embeddings_df' DataFrame.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ntable.insert(embeddings_df)\n```\n\n----------------------------------------\n\nTITLE: Encoding Text for Embeddings - Python\nDESCRIPTION: This function takes a string input and encodes it using the initialized SentenceTransformer model, returning the generated numerical vector representation.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef encode_text(text):\n    return embedding_model.encode(text)\n```\n\n----------------------------------------\n\nTITLE: Establishing KDB.AI Session and Database Connection in Python\nDESCRIPTION: This snippet creates a KDB.AI session using the provided API key and endpoint, then connects to the 'default' database. This establishes the connection to KDB.AI for further operations.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/managing_tables.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nsession = kdbai.Session(api_key=KDBAI_API_KEY, endpoint=KDBAI_ENDPOINT)\ndatabase = session.database(\"default\")\n```\n\n----------------------------------------\n\nTITLE: Searching for Neighbors in KDB.AI Table - Python\nDESCRIPTION: This snippet finds the three closest neighbors to a specific query vector in the KDB.AI table using the search function, demonstrating the ability to retrieve multiple results at once based on similarity.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/quickstarts/python_quickstart.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Find 3 closest neighbours of a single query vector\ntable.search(vectors={'vectorIndex': [query_vector]}, n=3)[0]\n```\n\n----------------------------------------\n\nTITLE: Creating Song Vector Embeddings from Categorical Data\nDESCRIPTION: Tokenizes song descriptions and uses Word2Vec to create vector embeddings for categorical data.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/music_recommendation/music_recommendation.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntokenised_song_descs = [word_tokenize(v.lower()) for v in song_data[\"song_description\"]]\n```\n\nLANGUAGE: python\nCODE:\n```\nembedding_dim = 15\n\nword2Vec_model = Word2Vec(\n    sentences=tokenised_song_descs,\n    vector_size=embedding_dim,\n    window=5,\n    min_count=1,\n    sg=1,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\ndef get_embedding(song_desc_tokens, model, embedding_dim):\n    vectors = [model.wv[token] for token in song_desc_tokens if token in model.wv]\n\n    # Average of word vectors OR zeros if no valid tokens found\n    return sum(vectors) / len(vectors) if vectors else [0] * embedding_dim\n```\n\nLANGUAGE: python\nCODE:\n```\ncategorical_embeddings = [\n    get_embedding(song_desc_tokens, word2Vec_model, embedding_dim)\n    for song_desc_tokens in tokenised_song_descs\n]\n```\n\nLANGUAGE: python\nCODE:\n```\nshow_embeddings(categorical_embeddings)\n```\n\n----------------------------------------\n\nTITLE: Setting Up KDB.AI Cloud Credentials\nDESCRIPTION: Retrieves KDB.AI endpoint URL and API key from environment variables or prompts the user to enter them if not available, enabling connection to the KDB.AI Cloud service.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nKDBAI_ENDPOINT = (\n    os.environ[\"KDBAI_ENDPOINT\"]\n    if \"KDBAI_ENDPOINT\" in os.environ\n    else input(\"KDB.AI endpoint: \")\n)\nKDBAI_API_KEY = (\n    os.environ[\"KDBAI_API_KEY\"]\n    if \"KDBAI_API_KEY\" in os.environ\n    else getpass(\"KDB.AI API key: \")\n)\n```\n\n----------------------------------------\n\nTITLE: Creating KDB.AI Table Schema in Python\nDESCRIPTION: Defines the schema for the KDB.AI table, including column names and data types.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_non_transformed/Temporal_Similarity_Search_Non-Transformed_Demo.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Define the schema with similar columns to the dataframe. The price column is where the time series vectors will be inserted\n# The vectorIndex type in the price column uses type 'tss', which represents Non-Transformed Temporal Similarity Search\n# Set up the schema and indexes for KDB.AI table, specifying embeddings column with 384 dimensions, Euclidean Distance, and flat index\nschema = [\n    {\"name\": \"index\", \"type\": \"int64\"},\n    {\"name\": \"time\", \"type\": \"datetime64[ns]\"},\n    {\"name\": \"sym\", \"type\": \"str\"},\n    {\"name\": \"qty\", \"type\": \"int64\"},\n    {\"name\": \"price\", \"type\": \"float64\"}\n]\n```\n\n----------------------------------------\n\nTITLE: Querying Data from KDB.AI Table - Python\nDESCRIPTION: This snippet uses the query method to retrieve all data from the KDB.AI table, showcasing the capability of the table to return stored information.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/quickstarts/python_quickstart.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntable.query()\n```\n\n----------------------------------------\n\nTITLE: Filtering Query Results in KDB.AI Table - Python\nDESCRIPTION: This snippet demonstrates how to use the 'like' filter to query the table for companies whose names start with the letter 'A', returning only relevant rows.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/quickstarts/python_quickstart.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ntable.query(filter=[(\"like\", \"company_name\", \"A*\")])\n```\n\n----------------------------------------\n\nTITLE: Performing Sparse Search using KDB.AI in Python\nDESCRIPTION: This Python snippet performs a sparse search on a KDB.AI table, retrieving the top 5 results based on the sparse query for specific relevance factors. It highlights the benefits of sparse searching for targeted term matching.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ntable.search(vectors={\"sparse_index\":sparse_query}, n=5)[0][['ID','chunk']]\n```\n\n----------------------------------------\n\nTITLE: Preparing AI Tools Data with Embeddings for KDB.AI Insertion in Python\nDESCRIPTION: Creates a pandas DataFrame with AI tools data and their corresponding embeddings, ready for insertion into the KDB.AI table.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/HuggingFace_search/huggingface_inference.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Create a DataFrame with the AI tools data\ndata = pd.DataFrame(ai_tools_data)[[\"id\", \"name\", \"description\", \"summary\", \"title\", \"visitors\"]]\ndata[\"description_embedding\"] = embeddings.tolist()\n```\n\n----------------------------------------\n\nTITLE: Loading a Pre-trained Sentence Transformer Model\nDESCRIPTION: Initializes a pre-trained sentence transformer model (all-MiniLM-L6-v2) for generating sentence embeddings. This model is suitable for semantic search applications.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n```\n\n----------------------------------------\n\nTITLE: Connecting to KDB.AI Cloud Session - Python\nDESCRIPTION: This snippet retrieves the KDB.AI Cloud endpoint and API key from environment variables or prompts the user for these details, preparing for a connection to the KDB.AI service.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nKDBAI_ENDPOINT = (\n    os.environ[\"KDBAI_ENDPOINT\"]\n    if \"KDBAI_ENDPOINT\" in os.environ\n    else input(\"KDB.AI endpoint: \")\n)\nKDBAI_API_KEY = (\n    os.environ[\"KDBAI_API_KEY\"]\n    if \"KDBAI_API_KEY\" in os.environ\n    else getpass(\"KDB.AI API key: \")\n)\n```\n\n----------------------------------------\n\nTITLE: Merging Categorical and Numeric Embeddings\nDESCRIPTION: Combines the categorical and numeric embeddings into a single vector for each song.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/music_recommendation/music_recommendation.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nrow_embeddings = [\n    np.concatenate([cat_row, num_row])\n    for cat_row, num_row in zip(categorical_embeddings, numeric_embeddings)\n]\n```\n\nLANGUAGE: python\nCODE:\n```\nshow_embeddings(row_embeddings)\n```\n\n----------------------------------------\n\nTITLE: Executing QA Chain on Chunked Documents for HuggingFace\nDESCRIPTION: Executes the QA chain for the HuggingFace model on the queried and chunked document data, capturing and printing the responses from the model. It leverages the `invoke` function to extract answers.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# HugginFace - run the chain on the query and the related chunks from the documentation\nchain_HuggingFaceHub.invoke({'input_documents':query_sim, 'question':query})['output_text']\n```\n\n----------------------------------------\n\nTITLE: Defining Query for Guest Satisfaction with Food in Python\nDESCRIPTION: This code snippet defines a query string to search for reviews about customers' satisfaction with food at the park.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nquery3 = \"are customers satisfied with the food at the park?\"\n```\n\n----------------------------------------\n\nTITLE: Initializing the Embedding Model - Python\nDESCRIPTION: This snippet initializes the TextEmbedding model, specifically the BAAI/bge-small-en-v1.5 model from FastEmbed, which will be used for generating embeddings.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/quickstarts/python_quickstart.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nembedding_model = TextEmbedding()\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for RAG Implementation\nDESCRIPTION: Imports necessary Python libraries for vector database operations, LangChain components, and various language models required for the RAG implementation.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# vector DB\nfrom getpass import getpass\nimport kdbai_client as kdbai\nimport time\n\n# langchain packages\nfrom langchain.chains import RetrievalQA\nfrom langchain_openai import ChatOpenAI\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import KDBAI\nfrom langchain import HuggingFaceHub\nfrom langchain_openai import OpenAI\nfrom langchain.chains.question_answering import load_qa_chain\nfrom langchain_huggingface import HuggingFaceEndpoint\n```\n\n----------------------------------------\n\nTITLE: Setting Up Hybrid Search with Equal Priority\nDESCRIPTION: Configures a retriever with hybrid search mode giving equal weight (0.5) to both sparse and dense vector searches.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Hybrid_Search_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\nretriever = index.as_retriever(\n                        vector_store_query_mode=\"hybrid\",\n                        similarity_top_k=5,\n                        vector_store_kwargs={\n                                \"index\" : \"flat\",\n                                \"indexWeight\" : 0.5,\n                                \"indexSparse\" : \"sparse_index\",\n                                \"indexSparseWeight\" : 0.5,\n                                },\n                            )\n```\n\n----------------------------------------\n\nTITLE: Defining OpenAI Text Embedding Model in Python\nDESCRIPTION: This snippet defines the text embedding model to be used for converting documents into vector representations suitable for storage and retrieval in KDB.AI.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation_evaluation.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nembeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n```\n\n----------------------------------------\n\nTITLE: Retrieving and Displaying Dense Priority Results\nDESCRIPTION: Executes the hybrid search with higher dense weight and displays the results.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Hybrid_Search_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndense_priority_nodes = retriever.retrieve(query)\ndisplay_search_results(dense_priority_nodes)\n```\n\n----------------------------------------\n\nTITLE: Deleting KDB.AI Tables in Python\nDESCRIPTION: This code snippet demonstrates how to delete the KDB.AI tables after the analysis is complete, which is considered a best practice.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/unstructured_io_RAG/Table_RAG_Unstructured_KDBAI_LangChain_RAG.ipynb#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ntable_contextualized.drop()\ntable_non_contextualized.drop()\n```\n\n----------------------------------------\n\nTITLE: Creating a Pandas DataFrame for KDB.AI Table Data in Python\nDESCRIPTION: This code creates a Pandas DataFrame combining the sample IDs and vector data. This DataFrame structure matches the schema of the KDB.AI table and will be used for inserting data into the table.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/managing_tables.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# column names/types matching the schema\nembeddings = pd.DataFrame({\"id\": ids, \"vectors\": list(vectors)})\n```\n\n----------------------------------------\n\nTITLE: Defining Search Query\nDESCRIPTION: Sets up the search query to be used for retrieval.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Hybrid_Search_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nquery = '12-month basis'\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into KDB.AI Table - Python\nDESCRIPTION: This snippet demonstrates how to insert the created DataFrame into the KDB.AI table, allowing the embedding vectors and company data to be stored for future queries.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/quickstarts/python_quickstart.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntable.insert(embeddings_df)\n```\n\n----------------------------------------\n\nTITLE: Searching and Extracting Results for Query in Python\nDESCRIPTION: This code snippet searches for reviews based on the defined query using the search_and_extract_results function.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nquery2_results = search_and_extract_results(\n    table, embedding_model, query2, n_results=50\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Sliding Windows for Time Series Data in Python\nDESCRIPTION: Generates sliding windows of size 1000 data points over the original dataframe, grouped by symbol.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_non_transformed/Temporal_Similarity_Search_Non-Transformed_Demo.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Create raw windows\nD = 1000 # Sliding Window Size\nvecdf = df.groupby(['sym']).apply(\n    lambda x: pd.DataFrame({\n        'time': sliding_window_view(x['time'], D)[:, 0],  # Adjusted to keep the last time in the window\n        'sym': x['sym'].iloc[0],\n        'price': list(sliding_window_view(x['price'], D))\n    })\n).reset_index(drop=True).reset_index()\nmemory_vecdf_created=get_memory_usage()\n```\n\n----------------------------------------\n\nTITLE: Extracting Text and Table Nodes from Markdown\nDESCRIPTION: Parse markdown document into text and table nodes using MarkdownElementNodeParser\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaParse_pdf_RAG/llamaParse_demo.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nnode_parser = MarkdownElementNodeParser(llm=llm, num_workers=8).from_defaults()\n\n# Retrieve nodes (text) and objects (table)\nnodes = node_parser.get_nodes_from_documents(documents)\n\n# Split nodes into base_nodes (text nodes), and object (table nodes)\nbase_nodes, objects = node_parser.get_nodes_and_objects(nodes)\n\n# Insert table markdown into text of each table object\nfor i in range(len(objects)):\n  objects[i].text = objects[i].obj.text[:]\n```\n\n----------------------------------------\n\nTITLE: Creating a New KDB.AI Database\nDESCRIPTION: Creates a new database named \"myDatabase\" in the KDB.AI service for storing the vector embeddings.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n# Create the database\ndb = session.create_database(\"myDatabase\")\n```\n\n----------------------------------------\n\nTITLE: Creating Song Vector Embeddings from Numeric Data\nDESCRIPTION: Scales numeric song features and creates vector embeddings for numeric data.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/music_recommendation/music_recommendation.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nnumeric_cols = list(\n    song_data.drop(columns=[\"song_name\", \"song_artists\", \"song_description\"]).columns\n)\nnumeric_cols\n```\n\nLANGUAGE: python\nCODE:\n```\nscaled_numeric_cols = [\n    (song_data[col] - song_data[col].mean()) / np.std(song_data[col])\n    for col in numeric_cols\n]\n```\n\nLANGUAGE: python\nCODE:\n```\nnumeric_embeddings = list(map(list, zip(*scaled_numeric_cols)))\n```\n\nLANGUAGE: python\nCODE:\n```\nshow_embeddings(numeric_embeddings)\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame to Store Multimodal Data\nDESCRIPTION: Initializes a pandas DataFrame with columns for file path, media type, text content, and embeddings. This structure will hold all the processed data before insertion into the KDB.AI vector database.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/multimodal_RAG_unified_text/multi_modal_demo.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Prepare a dataframe to store file path, media_type, text, and embeddings in\ncolumns = ['path','media_type','text','embeddings']\ndf = pd.DataFrame(columns=columns)\n```\n\n----------------------------------------\n\nTITLE: Retrieving and Displaying Equal Priority Results\nDESCRIPTION: Executes the hybrid search with equal weights and displays the results.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Hybrid_Search_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nequal_priority_nodes = retriever.retrieve(query)\ndisplay_search_results(equal_priority_nodes)\n```\n\n----------------------------------------\n\nTITLE: Querying Table Data\nDESCRIPTION: Executes a query to check the contents of the created table.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Hybrid_Search_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ntable.query()\n```\n\n----------------------------------------\n\nTITLE: Vector Database Schema Definition\nDESCRIPTION: Defining schema and indexes for KDB.AI table, including embedding dimensions and distance metric.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/qHnsw_index_pdf_search/pdf_qHNSW_Search.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npdf_schema = [\n    {\"name\": \"document_id\", \"type\": \"bytes\"},\n    {\"name\": \"text\", \"type\": \"bytes\"},\n    {\"name\": \"embeddings\", \"type\": \"float32s\"}\n]\n\nindexes = [\n    {\n        \"name\": \"qhnsw_index\",\n        \"type\": \"qHnsw\",\n        \"column\": \"embeddings\",\n        \"params\": {\"dims\": 1536, \"metric\": \"L2\"},\n    }\n]\n```\n\n----------------------------------------\n\nTITLE: Downloading Dataset from GitHub Repository\nDESCRIPTION: Downloads the necessary image and text data from a GitHub repository. Creates local directories and fetches contents recursively, saving images and text files appropriately for the multimodal RAG example.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/multimodal_RAG_unified_text/multi_modal_demo.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n### !!! Only run this cell if you need to download the data into your environment, for example in Colab\n### This downloads image and text data\n\n!mkdir -p ./data/images\n!mkdir -p ./data/text\n\ndef get_github_repo_contents(repo_owner, repo_name, branch, folder_path):\n    # Construct the API URL\n    api_url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/contents/{folder_path}?ref={branch}\"\n    \n    # Send the request and process the response\n    contents = requests.get(api_url).json()\n\n    # Create the local directory if it doesn't exist\n    fPath = f\"./data/{folder_path.split('/')[-1]}\"\n\n    for item in contents:\n        # Recursively list contents of subfolder\n        if item['type'] == 'dir':\n            get_github_repo_contents(repo_owner, repo_name, branch, f\"{folder_path}/{item['name']}\")\n        # Download and save file\n        elif item['type'] == 'file':\n            file_url = f\"https://raw.githubusercontent.com/{repo_owner}/{repo_name}/{branch}/{folder_path}/{item['name']}\"\n            print(file_url)\n            r = requests.get(file_url, timeout=4.0)\n            r.raise_for_status()  # Raises an exception for HTTP errors\n            file_path = f\"{fPath}/{item['name']}\"\n\n            if item['name'].lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n                # Save image file\n                with Image.open(io.BytesIO(r.content)) as im:\n                    im.save(file_path)\n            else:\n                # Save text file\n                with open(file_path, 'wb') as f:\n                    f.write(r.content)\n\n# Get images and texts\nget_github_repo_contents(\n    repo_owner='KxSystems',\n    repo_name='kdbai-samples',\n    branch='main',\n    folder_path='multimodal_RAG_unified_text/data'\n)\n```\n\n----------------------------------------\n\nTITLE: Inserting data into KDB.AI with Python\nDESCRIPTION: Bulk insert data into a KDB.AI table using the insert method. The `data` variable is expected to contain the data to be inserted. Ensure that the table is defined prior to this operation.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/HuggingFace_search/huggingface_inference.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntable.insert(data)\n```\n\n----------------------------------------\n\nTITLE: Performing Vector Similarity Search with Metadata Filters\nDESCRIPTION: Enhances the previous search by adding metadata filters for director (George Lucas) and release year (1977), demonstrating how filters can narrow search results to more specific movies.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/metadata_filtering/metadata_filtering_demo.ipynb#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nprint(table.search(vectors=query_vector, n=3, filter=[(\"like\", \"Director\", \"George Lucas\"),(\"=\", \"ReleaseYear\", 1977)]))\n```\n\n----------------------------------------\n\nTITLE: Setting Up HuggingFace Embedding Model\nDESCRIPTION: Initializes the HuggingFace embedding model using the all-mpnet-base-v2 model.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Hybrid_Search_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\nEMBEDDING = \"sentence-transformers/all-mpnet-base-v2\"\nembeddings_model = HuggingFaceEmbedding(model_name=EMBEDDING)\n```\n\n----------------------------------------\n\nTITLE: Importing KDB.AI Client\nDESCRIPTION: Imports the necessary libraries to connect to and interact with the KDB.AI vector database, which will store the multimodal embeddings for efficient similarity search.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/multimodal_RAG_unified_text/multi_modal_demo.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# vector DB imports\nimport os\nfrom getpass import getpass\nimport kdbai_client as kdbai\nimport time\n```\n\n----------------------------------------\n\nTITLE: Performing Outlier Search with KDB.AI in Python\nDESCRIPTION: Executes an outlier search to find the 10 most dissimilar vectors to the query vector.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_non_transformed/Temporal_Similarity_Search_Non-Transformed_Demo.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nout_start=datetime.datetime.now()\nres = table.search(vectors={'price': [q]}, n=-10, type=\"tss\")[0]\n\nout_stop=datetime.datetime.now()\nmemory_out_windows_post_search=get_memory_usage()\nres\n```\n\n----------------------------------------\n\nTITLE: Performing Vector Similarity Search\nDESCRIPTION: Demonstrates vector similarity search using embedded text queries with optional filters and aggregations.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/making_queries.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nperson_query = \"a software engineer with lots of experience\"\nperson_embedding = list(embedding_model.embed([person_query]))[0].tolist()\nprint(\"Searching for the three closest people to the example vector:\")\ntable.search({index_name: [person_embedding]}, n=3)\n```\n\n----------------------------------------\n\nTITLE: Generating Embeddings for Company Descriptions - Python\nDESCRIPTION: This snippet generates embeddings for the descriptions of the companies defined earlier by utilizing the embed method of the embedding model. The result is converted to a list for further processing.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/quickstarts/python_quickstart.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Example ID values\nembeddings = list(embedding_model.embed([desc for _, desc in company_data]))\nlen(embeddings[0])\n```\n\n----------------------------------------\n\nTITLE: Querying KDB.AI and Printing Response in Python\nDESCRIPTION: Queries the KDB.AI query engine using `search_term2` and prints the response. This displays the result of the query performed on the data stored in KDB.AI.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/qFlat_index_pdf_search/pdf_qFlat_Search.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nresult = query_engine.query(search_term2)\nprint(result.response)\n```\n\n----------------------------------------\n\nTITLE: Querying Data from KDB.AI Table in Python\nDESCRIPTION: This code demonstrates how to query data from the KDB.AI table. It retrieves all data from the table without any specific filtering or conditions.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/managing_tables.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ntable.query()\n```\n\n----------------------------------------\n\nTITLE: Defining KDB.AI Schema and Table Configuration\nDESCRIPTION: Create a schema and index configuration for storing document embeddings in KDB.AI\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaParse_pdf_RAG/llamaParse_demo.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nschema = [\n        dict(name=\"document_id\", type=\"str\"),\n        dict(name=\"text\", type=\"str\"),\n        dict(name=\"embeddings\", type=\"float32s\"),\n    ]\n\nindexFlat = {\n        \"name\": \"flat\",\n        \"type\": \"flat\",\n        \"column\": \"embeddings\",\n        \"params\": {'dims': 1536, 'metric': 'L2'},\n    }\n```\n\n----------------------------------------\n\nTITLE: Creating KDB.AI Table for Document Storage\nDESCRIPTION: Creates a KDB.AI table named 'reports' using the previously defined schema and index. If a table with the same name already exists, it's dropped first to ensure a clean setup. The table will store the financial regulation documents with their embeddings.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_advanced_RAG/KDBAI_Advanced_RAG_Demo.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nKDBAI_TABLE_NAME = \"reports\"\ndatabase = session.database(\"default\")\n\n# First ensure the table does not already exist\nfor table in database.tables:\n    if table.name == KDBAI_TABLE_NAME:\n        table.drop()\n        break\n\n#Create the table\ntable = database.create_table(KDBAI_TABLE_NAME, schema=schema, indexes=[indexFlat])\n```\n\n----------------------------------------\n\nTITLE: Finding Similar Songs with Custom Result Count in Python\nDESCRIPTION: Demonstrates how to find similar songs to 'Californication' by Red Hot Chili Peppers, returning 8 results using the find_similar_songs function. Shows how to customize the number of similar songs returned using the n_similar parameter.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/music_recommendation/music_recommendation.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfind_similar_songs(\n    table,\n    song_name=\"Californication\",\n    song_artists=\"Red Hot Chili Peppers\",\n    n_similar=8,\n)\n```\n\n----------------------------------------\n\nTITLE: Extracting Sensor Readings with BROKEN Machine Status\nDESCRIPTION: Filters the sensor data to extract only readings where the machine status is marked as BROKEN, for identifying failure patterns.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# Extract the readings from the BROKEN state of the pump\nbroken_sensors_df = sensors_df[sensors_df[\"machine_status\"] == \"BROKEN\"]\n```\n\n----------------------------------------\n\nTITLE: LlamaIndex Query Engine Setup\nDESCRIPTION: Configuring LlamaIndex query engine with similarity search parameters.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/qHnsw_index_pdf_search/pdf_qHNSW_Search.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nquery_engine = index.as_query_engine(\n    similarity_top_k=5,\n    vector_store_kwargs={\n                    \"index\" : \"qhnsw_index\",\n                },\n)\n```\n\n----------------------------------------\n\nTITLE: Checking Embedding Dimensions\nDESCRIPTION: Determines the dimensionality of the generated embeddings by checking the length of the first vector, which is needed when configuring the KDB.AI index.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nlen(embeddings_df[\"vectors\"][0])\n```\n\n----------------------------------------\n\nTITLE: Querying KDB.AI Vector Database Table in Python\nDESCRIPTION: This code snippet shows how to query the KDB.AI table to confirm that data was successfully inserted. It uses the table.query() method to retrieve and display the table contents.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/multimodal_RAG_unified_text/multi_modal_demo.ipynb#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# See what is in our table - confirm the data was inserted\ntable.query()\n```\n\n----------------------------------------\n\nTITLE: Dropping KDB.AI Table after Usage in Python\nDESCRIPTION: Shows the recommended cleanup procedure by dropping the KDB.AI table when it's no longer needed. This is considered a best practice to manage resources efficiently.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaParse_pdf_RAG/llamaParse_demo.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ntable.drop()\n```\n\n----------------------------------------\n\nTITLE: Extracting Search Results from KDB.AI - Python\nDESCRIPTION: This snippet executes the search using the previously defined query, obtaining the results as a DataFrame and preparing it for display or analysis.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nquery1_results = search_and_extract_results(\n    table, embedding_model, query1, n_results=25\n)\n```\n\n----------------------------------------\n\nTITLE: Querying data from KDB.AI with Python\nDESCRIPTION: Retrieve data from a KDB.AI table using the query method to confirm data has been loaded correctly. No parameters are required, and it returns the queried data.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/HuggingFace_search/huggingface_inference.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntable.query()\n```\n\n----------------------------------------\n\nTITLE: Defining Indexes for KDB.AI Table - Python\nDESCRIPTION: This snippet sets up index parameters for the previously defined schema, specifying the type of index, its name, the column it relates to, as well as dimensionality and similarity metrics.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# Define the index\nindexes = [\n    {\n        'type': 'hnsw',\n        'name': 'hnsw_index',\n        'column': 'embeddings',\n        'params': {'dims': 384, 'metric': \"CS\"},\n    },\n]\n```\n\n----------------------------------------\n\nTITLE: Querying the QA Bot in Python\nDESCRIPTION: This function queries the defined question-answering bot and prints the result, serving as an interface for users to retrieve answers based on structured prompts.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation_evaluation.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef query_qabot(qabot, query: str) -> str:\n    query_res = qabot.invoke(dict(query=query))[\"result\"]\n    print(f\"{query}\\n---\\n{query_res}\")\n    return query_res\n```\n\nLANGUAGE: python\nCODE:\n```\nquery1 = \"What improvements could be made in infrastructure?\"\n\nres1 = query_qabot(qabot, query1)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Answer Conciseness\nDESCRIPTION: Evaluates the conciseness of the first query result against the input query. This uses the previously initialized conciseness evaluator to assess how concise the response is.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation_evaluation.ipynb#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nconcise_eval_res = concise_evaluator.evaluate_strings(prediction=res1, input=query1)\n```\n\n----------------------------------------\n\nTITLE: Reading Text from PDF Document\nDESCRIPTION: Opens a PDF file, extracts text from each page using pypdf, and stores the content in a list.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Read PDF file\nwith open(\"data/research_paper.pdf\", \"rb\") as pdf_file:\n    pdf_pages = pypdf.PdfReader(pdf_file).pages\n    page_list = [page.extract_text() for page in pdf_pages]\n```\n\n----------------------------------------\n\nTITLE: Initializing KDB.AI Cloud Session in Python\nDESCRIPTION: Creates a KDB.AI session using the provided endpoint and API key.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_non_transformed/Temporal_Similarity_Search_Non-Transformed_Demo.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n### Start Session with KDB.AI Cloud\nsession = kdbai.Session(api_key=KDBAI_API_KEY, endpoint=KDBAI_ENDPOINT)\n```\n\n----------------------------------------\n\nTITLE: Installing KDB.AI Client Library in Python\nDESCRIPTION: Installs the kdbai_client library using pip, which is required to interact with KDB.AI.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_non_transformed/Temporal_Similarity_Search_Non-Transformed_Demo.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install kdbai_client\n```\n\n----------------------------------------\n\nTITLE: Performing a Search for Reviews - Python\nDESCRIPTION: This snippet defines a search term and calls the search_and_extract_results function, retrieving a specified number of results for further review and analysis.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nquery1 = \"are customers satisfied with the food at the park?\"\n```\n\n----------------------------------------\n\nTITLE: Dropping a KDB.AI Table in Python\nDESCRIPTION: Demonstrates how to properly clean up by dropping a KDB.AI table when it's no longer needed. This follows best practices for resource management.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_advanced_RAG/KDBAI_Advanced_RAG_Demo.ipynb#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ntable.drop()\n```\n\n----------------------------------------\n\nTITLE: Creating a Vector DB Table in KDB.AI in Python\nDESCRIPTION: This code snippet attempts to create a new table in the KDB.AI vector database based on the previously defined schema, ensuring the table does not already exist.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation_evaluation.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndatabase = session.database(\"default\")\n# First ensure the table does not already exist\ntry:\n    database.table(\"rag_eval\").drop()\nexcept kdbai.KDBAIException:\n    pass\n\ntable = database.create_table(\"rag_eval\", schema=rag_eval_schema, indexes=indexes)\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into KDB.AI Table in Python\nDESCRIPTION: Inserts the vectorized time series data into the KDB.AI table in batches, using a progress bar to track the insertion process.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Temporal_Similarity_Search_Transformed_Demo.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nn = 1000  # number of rows per batch\n\nfor i in tqdm(range(0, vecdf.shape[0], n)):\n    table_raw.insert(vecdf[i:i+n].reset_index(drop=True))\n\nmemory_raw_windows_in_table=get_memory_usage()\n```\n\n----------------------------------------\n\nTITLE: Visualizing Sentiment for Query Results in Python\nDESCRIPTION: This code snippet visualizes the sentiment of reviews found for the query about rides and attractions.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nplot_sentiment(sentiment_counts_by_branch(query2_results), query2)\n```\n\n----------------------------------------\n\nTITLE: Importing and Initializing Sentence Transformer Model for Query Embedding\nDESCRIPTION: Sets up the SentenceTransformer model ('all-MiniLM-L6-v2') to convert natural language queries into vector embeddings compatible with the KDB.AI vector database.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/metadata_filtering/metadata_filtering_demo.ipynb#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# embedding model to be used to embed user input query\nfrom sentence_transformers import SentenceTransformer\nembedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Database Schema and Creating Table\nDESCRIPTION: Definition of table schema with column specifications and creation of vector database table\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/fuzzy_filtering_on_metadata/fuzzy_filtering_demo.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntable_schema = [\n        {\"name\": \"ReleaseYear\", \"type\": \"int64\"},\n        {\"name\": \"Title\", \"type\": \"str\"},\n        {\"name\": \"Origin\", \"type\": \"str\"},\n        {\"name\": \"Director\", \"type\": \"str\"},\n        {\"name\": \"Cast\", \"type\": \"str\"},\n        {\"name\": \"Genre\", \"type\": \"str\"},\n        {\"name\": \"Plot\", \"type\": \"str\"},\n        {\"name\": \"embeddings\", \"type\": \"float64s\"},\n    ]\n\nindexes = [\n    {\n        \"name\": \"flat_index\",\n        \"type\": \"flat\",\n        \"column\": \"embeddings\",\n        \"params\": {\"dims\": 384, \"metric\": \"L2\"},\n    }\n]\n```\n\nLANGUAGE: python\nCODE:\n```\ndatabase = session.database('default')\n\ntry:\n    database.table(\"metadata_demo\").drop()\n    time.sleep(5)\nexcept kdbai.KDBAIException:\n    pass\n```\n\nLANGUAGE: python\nCODE:\n```\ntable = database.create_table(\"metadata_demo\", table_schema, indexes=indexes)\n```\n\n----------------------------------------\n\nTITLE: Creating a Correctness Evaluator with LangChain\nDESCRIPTION: Loads a LangChain evaluator configured to assess the correctness of answers. This requires a reference answer to compare against and uses the previously initialized GPT-4o model.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation_evaluation.ipynb#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ncorrect_evaluator = load_evaluator(\n    \"labeled_criteria\",\n    criteria=\"correctness\",\n    llm=evaluation_llm,\n    requires_reference=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Grouping Sentiment Counts by Branch in Python\nDESCRIPTION: This function groups sentiment counts from search results by branch. It calculates the number of negative, neutral, and positive sentiments for each branch in the dataset.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndef sentiment_counts_by_branch(search_results: pd.DataFrame) -> pd.DataFrame:\n    def sentiment_counts(df: pd.DataFrame) -> dict:\n        # Store count of sentiment labels\n        sentiments = {\n            \"negative\": 0,\n            \"neutral\": 0,\n            \"positive\": 0,\n        }\n        # Iterate through search results\n        for _, row in df.iterrows():\n            # Extract the sentiment label and increase its count\n            label = row[\"Label\"]\n            sentiments[label] += 1\n        return sentiments\n\n    grouped_df = search_results.groupby(\"Branch\").apply(sentiment_counts).reset_index()\n    grouped_df.columns = [\"Branch\", \"Sentiments\"]\n    return grouped_df\n```\n\n----------------------------------------\n\nTITLE: Sentiment Analysis Helper Function\nDESCRIPTION: Create a custom function to process sentiment analysis results, extracting labels and confidence scores\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef sentiment(reviews):\n    sentiments = sentiment_pipeline(reviews)\n    l = [labels[x[\"label\"]] for x in sentiments][0]\n    s = [x[\"score\"] for x in sentiments][0]\n    return l, s\n\nlabels = {\"LABEL_0\": \"negative\", \"LABEL_1\": \"neutral\", \"LABEL_2\": \"positive\"}\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for KDB.AI and Data Analysis in Python\nDESCRIPTION: Imports necessary Python libraries including kdbai_client, pandas, numpy, matplotlib, and others for data manipulation and visualization.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_non_transformed/Temporal_Similarity_Search_Non-Transformed_Demo.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# KX Dependencies\nimport kdbai_client as kdbai\n# Other Dependencies\nimport pandas as pd\nimport numpy as np\nfrom numpy.lib.stride_tricks import sliding_window_view\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport os\nfrom getpass import getpass\nimport time\n```\n\n----------------------------------------\n\nTITLE: Creating a Query for KDB.AI QA Bot\nDESCRIPTION: Initializes a query variable with a question about electric vehicle manufacturing jobs in the country. This is the first step in testing the QA system.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation_evaluation.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nquery2 = \"How many jobs were created in the country due the electric vehicle manufacturing industry?\"\n```\n\n----------------------------------------\n\nTITLE: Dropping Existing Table in KDB.AI with Python\nDESCRIPTION: This snippet checks for the existence of a 'trade_tsc' table in KDB.AI and attempts to drop it if it exists. This ensures a fresh start for table creation. It handles the specific KDB.AI exception if the table does not exist.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Temporal_Similarity_Search_Transformed_Demo.ipynb#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# First ensure the table does not already exist\ntry:\n    db.table(\"trade_tsc\").drop()\nexcept kdbai.KDBAIException:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Setting up KDB.AI Cloud Session in Python\nDESCRIPTION: This code snippet demonstrates how to set up a KDB.AI Cloud session. It checks for environment variables containing the API endpoint and key, prompting the user for input if not found, and then establishes a connection to KDB.AI.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/unstructured_io_RAG/Table_RAG_Unstructured_KDBAI_LangChain_RAG.ipynb#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# Check if KDBAI_ENDPOINT is in the environment variables\nif \"KDBAI_ENDPOINT\" in os.environ:\n    KDBAI_ENDPOINT = os.environ[\"KDBAI_ENDPOINT\"]\nelse:\n    # Prompt the user to enter the API key\n    KDBAI_ENDPOINT = input(\"KDB.AI ENDPOINT: \")\n    # Save the API key as an environment variable for the current session\n    os.environ[\"KDBAI_ENDPOINT\"] = KDBAI_ENDPOINT\n\n# Check if KDBAI_ENDPOINT is in the environment variables\nif \"KDBAI_API_KEY\" in os.environ:\n    KDBAI_API_KEY = os.environ[\"KDBAI_API_KEY\"]\nelse:\n    # Prompt the user to enter the API key\n    KDBAI_API_KEY = input(\"KDB.AI KEY: \")\n    # Save the API key as an environment variable for the current session\n    os.environ[\"KDBAI_API_KEY\"] = KDBAI_API_KEY\n```\n\nLANGUAGE: python\nCODE:\n```\n#connect to KDB.AI\nsession = kdbai.Session(api_key=KDBAI_API_KEY, endpoint=KDBAI_ENDPOINT)\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into KDB.AI Table using Python\nDESCRIPTION: This snippet inserts the prepared DataFrame (embeddings) into the KDB.AI table. This operation populates the table with the sample vector data and associated IDs.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/managing_tables.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntable.insert(embeddings)\n```\n\n----------------------------------------\n\nTITLE: Chunking Documents with LangChain\nDESCRIPTION: Sets up the RecursiveCharacterTextSplitter to divide the document into 500-character chunks with no overlap.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n### Chunk the documents into 500 character chunks using langchain's text splitter \"RucursiveCharacterTextSplitter\"\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n```\n\n----------------------------------------\n\nTITLE: Listing Existing Tables in KDB.AI Database using Python\nDESCRIPTION: This snippet lists all existing tables in the connected KDB.AI database. It's used to verify the current state of the database before creating new tables.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/managing_tables.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndatabase.tables\n```\n\n----------------------------------------\n\nTITLE: Retrieving and Displaying Sparse Priority Results\nDESCRIPTION: Executes the hybrid search with higher sparse weight and displays the results.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Hybrid_Search_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nsparse_priority_nodes = retriever.retrieve(query)\ndisplay_search_results(sparse_priority_nodes)\n```\n\n----------------------------------------\n\nTITLE: Displaying Updated Table Elements - Python\nDESCRIPTION: This snippet iterates over the re-embedded elements and prints the updated text of each table element, allowing inspection of newly generated table descriptions.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/unstructured_io_RAG/Table_RAG_Unstructured_KDBAI_LangChain_RAG.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfor element in elements:\n  if element.to_dict()['type'] == 'Table':\n    print(element.text)\n```\n\n----------------------------------------\n\nTITLE: Downloading State of the Union Speech Data in Python\nDESCRIPTION: This code snippet checks for the existence of the State of the Union text file and downloads it if not already present, ensuring the evaluation has the necessary data.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation_evaluation.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nif os.path.exists(\"./data/state_of_the_union.txt\") == False:\n    !mkdir ./data\n    !wget -P ./data https://raw.githubusercontent.com/KxSystems/kdbai-samples/main/retrieval_augmented_generation/data/state_of_the_union.txt\n```\n\n----------------------------------------\n\nTITLE: Downloading Sample Market Data in Python\nDESCRIPTION: Conditionally downloads sample market data in Parquet format if it doesn't exist in the local environment. This is particularly useful for environments like Google Colab.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Temporal_Similarity_Search_Transformed_Demo.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n### !!! Only run this cell if you need to download the data into your environment, for example in Colab\n### This downloads sample market data\nif os.path.exists(\"./data/marketTrades.parquet\") == False:\n  !mkdir ./data\n  !wget -P ./data https://raw.githubusercontent.com/KxSystems/kdbai-samples/main/TSS_transformed/data/marketTrades.parquet\n```\n\n----------------------------------------\n\nTITLE: Retrieving Document Chunks with Query Engine\nDESCRIPTION: Iterates through retrieved document chunks related to the search term, printing each chunk's text with a separator\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/qHnsw_index_pdf_search/pdf_qHNSW_Search.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nretrieved_chunks = query_engine.retrieve(search_term2)\nfor i in retrieved_chunks:\n    print(i.node.get_text())\n    print(\"____________________\")\n```\n\n----------------------------------------\n\nTITLE: Dropping KDB.AI Table in Python\nDESCRIPTION: Demonstrates how to properly clean up resources by dropping the KDB.AI table when finished with operations.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/music_recommendation/music_recommendation.ipynb#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ntable.drop()\n```\n\n----------------------------------------\n\nTITLE: Importing Necessary Python Packages for KDB.AI\nDESCRIPTION: This snippet imports essential Python modules for database operations and embedding generation. It includes importing standard libraries alongside KDB.AI client, FastEmbed for text embedding, and time for pause or delays during execution.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/quickstarts/python_quickstart.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# vector DB\nimport os\nfrom getpass import getpass\nimport kdbai_client as kdbai\nfrom fastembed import TextEmbedding\nimport time\n```\n\n----------------------------------------\n\nTITLE: Inserting Movie Data into KDB.AI Vector Database in Batches\nDESCRIPTION: Processes and inserts the movie data into the KDB.AI table in batches of 2000 rows, including type conversions and handling of empty values. Uses tqdm for progress tracking during the insertion process.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/metadata_filtering/metadata_filtering_demo.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n#Insert the data into the table, split into 2000 row batches\nfrom tqdm import tqdm \nn = 2000  # chunk row size\n\n# convert empty cast values to string form for backend. Here we are using value None tofor empty Cast value.\nfor index, row in df.iterrows():\n    cast = row['Cast']\n    if 1 == len(cast):\n        df.loc[index, 'Cast'] = 'None'\n    \nfor i in tqdm(range(0, df.shape[0], n)):\n    data = df[i:i+n].reset_index(drop=True)\n    # change data types as per table schema\n    data['Title'] = data['Title'].str.encode('utf-8')\n    data['Director'] = data['Director'].str.encode('utf-8')\n    data['Cast'] = data['Cast'].str.encode('utf-8')\n    data['Plot'] = data['Plot'].str.encode('utf-8')\n    table.insert(data)\n```\n\n----------------------------------------\n\nTITLE: Executing QA Chain on Chunked Documents for OpenAI\nDESCRIPTION: Executes the QA chain set up with OpenAI model on queried and chunked document data, outputting the model's response. The chain interaction involves feeding both documents and questions to the model.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# OpenAI - run the chain on the query and the related chunks from the documentation\nchain_openAI.invoke({'input_documents':query_sim, 'question':query})['output_text']\n```\n\n----------------------------------------\n\nTITLE: Creating Pandas DataFrame for Financial Data Analysis in Python\nDESCRIPTION: This code snippet creates a pandas DataFrame to store text and updated table elements. It iterates through a collection of elements, extracting relevant information and storing it in the DataFrame.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/unstructured_io_RAG/Table_RAG_Unstructured_KDBAI_LangChain_RAG.ipynb#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\ndata = []\n\nfor c in elements:\n  row = {}\n  row['id'] = c.id\n  row['text'] = c.text.encode()\n  row['metadata'] = c.metadata.to_dict()\n  row['embedding'] = c.embeddings\n  data.append(row)\n\ndf_contextualized = pd.DataFrame(data)\ndf_contextualized.head()\n```\n\n----------------------------------------\n\nTITLE: Performing Multi-Search with KDB.AI in Python\nDESCRIPTION: Executes a multi-search using query vectors of different sizes (1000, 500, and 50 data points) simultaneously.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_non_transformed/Temporal_Similarity_Search_Non-Transformed_Demo.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nmulti_start=datetime.datetime.now()\nresults = table.search(vectors={'price': [q, smaller_query_50, smaller_query_500]}, n=10, type=\"tss\")\nmulti_stop=datetime.datetime.now()\nfor result in results:\n    print(result)\n    print()\n```\n\n----------------------------------------\n\nTITLE: Checking First BROKEN Status Timestamp\nDESCRIPTION: Displays the timestamps when the machine status was first marked as BROKEN to compare with pattern matching results later.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n# When is the first time a sensor is 'broken'?\nbroken_sensors_df[\"timestamp\"]\n```\n\n----------------------------------------\n\nTITLE: Setting KDB.AI Connection Parameters\nDESCRIPTION: Retrieves or prompts for the KDB.AI endpoint URL and API key needed to establish a connection to the KDB.AI Cloud service.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nKDBAI_ENDPOINT = (\n    os.environ[\"KDBAI_ENDPOINT\"]\n    if \"KDBAI_ENDPOINT\" in os.environ\n    else input(\"KDB.AI endpoint: \")\n)\nKDBAI_API_KEY = (\n    os.environ[\"KDBAI_API_KEY\"]\n    if \"KDBAI_API_KEY\" in os.environ\n    else getpass(\"KDB.AI API key: \")\n)\n```\n\n----------------------------------------\n\nTITLE: Dropping KDB.AI Table and Database - Python\nDESCRIPTION: This snippet removes both the KDB.AI table and the database once all necessary operations are completed. It is considered best practice to drop these to prevent unnecessary storage.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ntable.drop()\ndb.drop()\n```\n\n----------------------------------------\n\nTITLE: Creating Sample IDs for KDB.AI Table Entries in Python\nDESCRIPTION: This snippet creates a list of sample IDs to be associated with the vector data in the KDB.AI table. These IDs will be used to uniquely identify each vector entry.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/managing_tables.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Example ID values\nids = [\"h\", \"e\", \"l\", \"l\", \"o\"]\n```\n\n----------------------------------------\n\nTITLE: Importing KDB.AI and Utility Libraries\nDESCRIPTION: Imports the KDB.AI client library and supporting modules for connecting to the vector database, handling authentication, and time functions.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# vector DB\nimport os\nimport kdbai_client as kdbai\nfrom getpass import getpass\nimport time\n```\n\n----------------------------------------\n\nTITLE: Alternative OpenAI API Key Setup\nDESCRIPTION: Another approach to setting up the OpenAI API key, checking if it exists in environment variables first and prompting for input if not. The key is then saved as an environment variable for the current session.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_advanced_RAG/KDBAI_Advanced_RAG_Demo.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n# Set OpenAI API \nif \"OPENAI_API_KEY\" in os.environ:\n    KDBAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\nelse:\n    # Prompt the user to enter the API key\n    OPENAI_API_KEY = getpass(\"OPENAI API KEY: \")\n    # Save the API key as an environment variable for the current session\n    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n```\n\n----------------------------------------\n\nTITLE: Initializing KDB.AI Vector Store with LlamaIndex\nDESCRIPTION: This snippet initializes a KDB.AI vector store using LlamaIndex. It creates a `KDBAIVectorStore` object, passing in the `table` variable (assumed to be a KDB.AI table object). It then uses this vector store to create a `StorageContext` object, which is used to manage the storage of data within LlamaIndex.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaParse_pdf_RAG/llamaParse_demo.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n\"vector_store = KDBAIVectorStore(table)\\nstorage_context = StorageContext.from_defaults(vector_store=vector_store)\"\n```\n\n----------------------------------------\n\nTITLE: Creating a KDB.AI Session with Cloud Endpoint\nDESCRIPTION: Establishes a connection to a KDB.AI Cloud instance using the provided endpoint and API key, creating a session object for database operations.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/metadata_filtering/metadata_filtering_demo.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nsession = kdbai.Session(api_key=KDBAI_API_KEY, endpoint=KDBAI_ENDPOINT)\n```\n\n----------------------------------------\n\nTITLE: Creating Sample Data and Embeddings\nDESCRIPTION: Generates sample data including descriptions and creates text embeddings using FastEmbed.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/making_queries.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nembedding_model = TextEmbedding()\nembeddings = list(embedding_model.embed(descriptions))\n```\n\n----------------------------------------\n\nTITLE: Performing Vector Similarity Searches\nDESCRIPTION: Implementation of vector similarity searches with metadata filtering and fuzzy matching\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/fuzzy_filtering_on_metadata/fuzzy_filtering_demo.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\nembedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n```\n\nLANGUAGE: python\nCODE:\n```\nquery_vector = {'flat_index' : [embedding_model.encode('star wars Luke Skywalker').tolist()]}\n```\n\nLANGUAGE: python\nCODE:\n```\nprint(table.search(vectors=query_vector, n=3))\n```\n\nLANGUAGE: python\nCODE:\n```\nprint(table.search(vectors=query_vector, n=3, filter=[['fuzzy','Director',[[\"Goerge Lucas\",2]]]]))\n```\n\n----------------------------------------\n\nTITLE: Initializing OpenAI Client\nDESCRIPTION: Creates an instance of the OpenAI client using the API key set in the environment variable. This client will be used to generate embeddings and obtain image descriptions.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/multimodal_RAG_unified_text/multi_modal_demo.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nopenai.api_key = os.environ[\"OPENAI_API_KEY\"]\nclient = OpenAI()\n```\n\n----------------------------------------\n\nTITLE: Plotting Grouped Bar Chart for Review Ratings - Python\nDESCRIPTION: This function generates a grouped bar chart visualizing sentiment ratings across different branches using Pandas DataFrame. It requires Matplotlib for plotting and takes a DataFrame and a y-axis label as parameters.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef plot_grouped_bar_chart(df: pd.DataFrame, y_label: str) -> None:\n    # Get list of unique branches and labels\n    branches = df.index.unique()\n    labels = df.columns.unique()\n\n    # Plot a bar for each branch-label pair\n    bar_width = 0.2\n    x_positions = range(len(labels))\n    for i, branch in enumerate(branches):\n        metric_ratings = df.loc[branch]\n        plt.bar(\n            x=[pos + (i * bar_width) for pos in x_positions],\n            height=metric_ratings,\n            width=bar_width,\n            label=branch,\n        )\n\n    # Beutify the plot\n    plt.xticks([pos + bar_width for pos in x_positions], labels)\n    plt.xlabel(\"Label\")\n    plt.ylabel(y_label)\n    plt.legend()\n```\n\n----------------------------------------\n\nTITLE: Deleting a KDB.AI Table - Python\nDESCRIPTION: This snippet provides functionality to delete a KDB.AI table identified by its name, demonstrating how to manage table lifecycle effectively.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/quickstarts/python_quickstart.ipynb#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfor t in session.database('default').tables:\n    if t.name == 'company_data':\n        t.drop()\n```\n\n----------------------------------------\n\nTITLE: Configuring KDB.AI Cloud Session Credentials\nDESCRIPTION: Retrieves KDB.AI Cloud session credentials from environment variables or user input, supporting secure authentication for vector database connection\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/music_recommendation/music_recommendation.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nKDBAI_ENDPOINT = (\n    os.environ[\"KDBAI_ENDPOINT\"]\n    if \"KDBAI_ENDPOINT\" in os.environ\n    else input(\"KDB.AI endpoint: \")\n)\nKDBAI_API_KEY = (\n    os.environ[\"KDBAI_API_KEY\"]\n    if \"KDBAI_API_KEY\" in os.environ\n    else getpass(\"KDB.AI API key: \")\n)\n```\n\n----------------------------------------\n\nTITLE: Downloading LlamaIndex Dataset\nDESCRIPTION: Downloads Paul Graham Essay Dataset using LlamaIndex CLI.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/rag_example.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n!llamaindex-cli download-llamadataset PaulGrahamEssayDataset --download-dir ./data\n```\n\n----------------------------------------\n\nTITLE: Document Loading and Parsing\nDESCRIPTION: Loads and parses documents using SentenceSplitter with specified chunk parameters.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/rag_example.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nnode_parser = SentenceSplitter(chunk_size=500, chunk_overlap=100)\nessays = SimpleDirectoryReader(input_dir=\"./data/source_files\").load_data()\ndocs = node_parser.get_nodes_from_documents(essays)\nlen(docs)\n```\n\n----------------------------------------\n\nTITLE: Creating VectorStoreIndex and Inserting Data into KDB.AI\nDESCRIPTION: This code creates a `VectorStoreIndex` using LlamaIndex and inserts base nodes and objects into KDB.AI. It takes a list of nodes (`base_nodes + objects`) and the `storage_context` created in the previous step as input. This index is then used to query the KDB.AI database, effectively storing the node data in the vector store.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaParse_pdf_RAG/llamaParse_demo.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n\"#Create the index, inserts base_nodes and objects into KDB.AI\\nrecursive_index = VectorStoreIndex(\\n    nodes= base_nodes + objects, storage_context=storage_context\\n)\"\n```\n\n----------------------------------------\n\nTITLE: Generating Sample Vector Data for KDB.AI Table in Python\nDESCRIPTION: This code generates sample vector data using NumPy. It creates an array of five 8-dimensional float32 vectors, which will be used as example embeddings in the KDB.AI table.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/managing_tables.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Create a NumPy array of 5 eight-dimensional float32 arrays\nvectors = np.array(\n    [\n        [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\n        [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n        [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n        [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 0.1],\n        [0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 0.1, 0.2],\n    ],\n    dtype=np.float32,\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a KDB.AI Session\nDESCRIPTION: Establishes a connection to the KDB.AI service using the provided endpoint and API key credentials.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nsession = kdbai.Session(api_key=KDBAI_API_KEY, endpoint=KDBAI_ENDPOINT)\n```\n\n----------------------------------------\n\nTITLE: Defining a Helper Function for DataFrame Display\nDESCRIPTION: Creates a utility function that displays the shape of a DataFrame and returns its first few rows for quick inspection.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef show_df(df: pd.DataFrame) -> pd.DataFrame:\n    print(df.shape)\n    return df.head()\n```\n\n----------------------------------------\n\nTITLE: Importing Required Python Packages for KDB.AI and Data Processing\nDESCRIPTION: Imports necessary Python libraries for working with KDB.AI, handling data, and managing environment variables.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/HuggingFace_search/huggingface_inference.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# vector DB\nimport os\nfrom getpass import getpass\nimport kdbai_client as kdbai\nimport time\n```\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\n```\n\n----------------------------------------\n\nTITLE: Downloading Sample Market Data in Python\nDESCRIPTION: Creates a data directory and downloads sample market data in Parquet format from a GitHub repository.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_non_transformed/Temporal_Similarity_Search_Non-Transformed_Demo.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!mkdir ./data \n!wget -P ./data https://raw.githubusercontent.com/KxSystems/kdbai-samples/main/TSS_non_transformed/data/marketTrades.parquet\n```\n\n----------------------------------------\n\nTITLE: Downloading PDF Financial Regulation Documents\nDESCRIPTION: Downloads PDF documents from the specified URLs and saves them locally. The function creates a PDF directory, handles file downloads in chunks, and returns the paths to the local files. The code is timed to measure the download performance.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_advanced_RAG/KDBAI_Advanced_RAG_Demo.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\nCHUNK_SIZE = 512 * 1024\n\n\ndef download_file(url):\n    print(\"Downloading %s...\" % url)\n    out = os.path.join(OUTDIR, os.path.basename(url))\n    try:\n        response = urllib.request.urlopen(url)\n    except urllib.error.URLError as e:\n        logging.exception(\"Failed to download %s !\" % url)\n    else:\n        with open(out, \"wb\") as f:\n            while True:\n                chunk = response.read(CHUNK_SIZE)\n                if chunk:\n                    f.write(chunk)\n                else:\n                    break\n    return out\n\n\nif RESET:\n    if os.path.exists(OUTDIR):\n        shutil.rmtree(OUTDIR)\n    os.mkdir(OUTDIR)\n\n    local_files = [download_file(x) for x in INPUT_URLS]\n    local_files[:10]\n```\n\n----------------------------------------\n\nTITLE: Importing LlamaIndex Components\nDESCRIPTION: Imports the necessary classes from LlamaIndex for vector store, storage contexts, and document processing.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Hybrid_Search_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_index.vector_stores.kdbai import KDBAIVectorStore\nfrom llama_index.core import StorageContext\nfrom llama_index.core import Settings\nfrom llama_index.core.indices import VectorStoreIndex\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.callbacks import CallbackManager\nfrom llama_index.core import SimpleDirectoryReader\n```\n\n----------------------------------------\n\nTITLE: Setting API Keys for OpenAI and Hugging Face\nDESCRIPTION: Prompts the user to input API keys for OpenAI and Hugging Face if they are not already set as environment variables. These keys are required for accessing the respective services.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nos.environ[\"OPENAI_API_KEY\"] = (\n    os.environ[\"OPENAI_API_KEY\"]\n    if \"OPENAI_API_KEY\" in os.environ\n    else getpass(\"OpenAI API Key: \")\n)\n\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = (\n    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]\n    if \"HUGGINGFACEHUB_API_TOKEN\" in os.environ\n    else getpass(\"Hugging Face API Token: \")\n)\n```\n\n----------------------------------------\n\nTITLE: Loading AI Tools Data from GitHub Gist in Python\nDESCRIPTION: Fetches AI tools data from a GitHub gist URL and loads it into a pandas DataFrame for further processing.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/HuggingFace_search/huggingface_inference.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport requests\n\ngist_url = \"https://gist.github.com/mrmps/2f62a2287cb2c1ca63a2762fcaac89bc/raw\"\nresponse = requests.get(gist_url)\nai_tools_data = response.json()\ndf = pd.DataFrame.from_dict(ai_tools_data)\n\n# drop column with unecessary metadata\ndf.drop(columns=[\"xata\"], inplace=True)\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Splitting PDF Text into Sentences\nDESCRIPTION: Uses NLTK's sentence tokenizer to split the full PDF text into individual sentences, which will be processed and embedded separately.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Split the PDF into sentences\npdf_sentences = sent_tokenize(full_pdf_text)\nlen(pdf_sentences)\n```\n\n----------------------------------------\n\nTITLE: Initializing KDB.AI Cloud Session in Python\nDESCRIPTION: Creates a KDB.AI session using the provided endpoint and API key for cloud-based interactions.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Temporal_Similarity_Search_Transformed_Demo.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n### Start Session with KDB.AI Cloud\nsession = kdbai.Session(api_key=KDBAI_API_KEY, endpoint=KDBAI_ENDPOINT)\n```\n\n----------------------------------------\n\nTITLE: Connecting to Local KDB.AI Server Session\nDESCRIPTION: Illustrates the method to connect to a local KDB.AI Server instance. This snippet shows initializing a session with a local API endpoint, which is a part of setup process for running a private server.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/quickstarts/python_quickstart.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# session = kdbai.Session(endpoint=\"http://localhost:8082\")\n```\n\n----------------------------------------\n\nTITLE: Importing Python Libraries\nDESCRIPTION: Imports necessary Python libraries for KDB.AI operations, data manipulation, and text embedding.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/making_queries.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport kdbai_client as kdbai\nimport time\nimport pandas as pd\nimport numpy as np\nfrom fastembed import TextEmbedding\nimport os\nimport getpass\n```\n\n----------------------------------------\n\nTITLE: Retrieving and Printing Chunks using KDB.AI in Python\nDESCRIPTION: Retrieves relevant chunks of data from a KDB.AI query engine based on `search_term2`. Then iterates over the retrieved chunks and prints the text content of each node within each chunk, followed by a separator.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/qFlat_index_pdf_search/pdf_qFlat_Search.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nretrieved_chunks = query_engine.retrieve(search_term2)\nfor i in retrieved_chunks:\n    print(i.node.get_text())\n    print(\"____________________\")\n```\n\n----------------------------------------\n\nTITLE: Removing Existing Database if Present\nDESCRIPTION: Attempts to drop a database named \"myDatabase\" if it already exists, catching any exceptions if the database doesn't exist.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# ensure no database called \"myDatabase\" exists\ntry:\n    session.database(\"myDatabase\").drop()\nexcept kdbai.KDBAIException:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Creating Search Results Display Function\nDESCRIPTION: Defines a helper function to convert search result nodes into a pandas DataFrame for display.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Hybrid_Search_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef display_search_results(nodes):\n    nodes_df = pd.DataFrame(columns=['score', 'text'])\n    for node in nodes:\n        nodes_df.loc[len(nodes_df.index)] = (node.score, node.text)\n    return nodes_df\n```\n\n----------------------------------------\n\nTITLE: Applying Embedding to Review Texts - Python\nDESCRIPTION: This snippet applies the encode_text function to the 'Review_Text' column of the review sentiments DataFrame, generating embeddings for each review and storing them in a new column.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nreview_sentiments_df[\"embeddings\"] = review_sentiments_df[\"Review_Text\"].apply(\n    encode_text\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Metadata Helper Function\nDESCRIPTION: Defines a helper function to retrieve metadata for a given file path.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Hybrid_Search_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Helper function - for getting metadata\ndef get_metadata(file_path):\n    return metadata[file_path]\n```\n\n----------------------------------------\n\nTITLE: Embedding AI Tool Descriptions using Sentence Transformers in Python\nDESCRIPTION: Uses the Sentence Transformers library with the 'BAAI/bge-small-en-v1.5' model to create embeddings for AI tool descriptions.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/HuggingFace_search/huggingface_inference.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"BAAI/bge-small-en-v1.5\")\n\ndescriptions = [tool[\"description\"] for tool in ai_tools_data]\nembeddings = model.encode(descriptions)\n```\n\nLANGUAGE: python\nCODE:\n```\nembeddings\n```\n\n----------------------------------------\n\nTITLE: Querying KDB.AI Table\nDESCRIPTION: This snippet queries the KDB.AI table to ensure that the nodes were inserted successfully. It calls the `query()` method on the `table` object, which is assumed to be a KDB.AI table object.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaParse_pdf_RAG/llamaParse_demo.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n\"# Query KDB.AI to ensure the nodes were inserted\\ntable.query()\"\n```\n\n----------------------------------------\n\nTITLE: Creating Vector Embedding for Middle Earth Fantasy Query\nDESCRIPTION: Generates a vector embedding for a query about Middle Earth fantasy adventures in the Shire, targeting Lord of the Rings related content.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/metadata_filtering/metadata_filtering_demo.ipynb#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n# Another query\nquery_vector = {'flat_index' : [embedding_model.encode('middle earth fantasy adventure in the Shire').tolist()]}\n```\n\n----------------------------------------\n\nTITLE: Defining Financial Regulation Document Sources and Metadata\nDESCRIPTION: Specifies URLs for downloading financial regulation PDF documents and their associated metadata. The metadata includes document titles and publication dates for the GrammLeachBliley Act of 1999 and the Dodd-Frank Wall Street Reform Act of 2010.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_advanced_RAG/KDBAI_Advanced_RAG_Demo.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nINPUT_URLS = [\n    \"https://www.govinfo.gov/content/pkg/PLAW-106publ102/pdf/PLAW-106publ102.pdf\",\n    \"https://www.govinfo.gov/content/pkg/PLAW-111publ203/pdf/PLAW-111publ203.pdf\",\n]\n\nMETADATA = {\n    \"pdf/PLAW-106publ102.pdf\": {\n        \"title\": \"GRAMMLEACHBLILEY ACT, 1999\",\n        \"publication_date\": pd.to_datetime(\"1999-11-12\"),\n    },\n    \"pdf/PLAW-111publ203.pdf\": {\n        \"title\": \"DODD-FRANK WALL STREET REFORM AND CONSUMER PROTECTION ACT, 2010\",\n        \"publication_date\": pd.to_datetime(\"2010-07-21\"),\n    },\n}\n```\n\n----------------------------------------\n\nTITLE: Connecting to KDB.AI Database\nDESCRIPTION: Establishes a connection to the default database in KDB.AI.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Hybrid_Search_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Connect with kdbai database\ndb = session.database(\"default\")\n```\n\n----------------------------------------\n\nTITLE: Analyzing Sentiment for Food-related Reviews in Python\nDESCRIPTION: This code snippet calls the review_sentiment_for_query function to analyze and visualize sentiment for food-related reviews.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nreview_sentiment_for_query(table, embedding_model, query3, n_results=50)\n```\n\n----------------------------------------\n\nTITLE: Dropping the KDB.AI Table\nDESCRIPTION: Removes the table from KDB.AI once the demonstration is complete. This follows best practices for cleaning up resources after they are no longer needed.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation_evaluation.ipynb#2025-04-21_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ntable.drop()\n```\n\n----------------------------------------\n\nTITLE: Deleting a Table in KDB.AI using Python\nDESCRIPTION: This code demonstrates how to delete a table in KDB.AI. It uses the drop() method to remove the 'data' table created earlier in the example.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/managing_tables.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ntable.drop()\n```\n\n----------------------------------------\n\nTITLE: Initializing GPT-4o Model for Evaluation\nDESCRIPTION: Creates an instance of OpenAI's GPT-4o model that will be used to evaluate the responses from the QA bot. This sets up the LLM that will perform both conciseness and correctness evaluations.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation_evaluation.ipynb#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nevaluation_llm = ChatOpenAI(model=\"gpt-4o\")\n```\n\n----------------------------------------\n\nTITLE: Initializing KDB.AI Cloud Session\nDESCRIPTION: Creates a session connection to KDB.AI Cloud using the provided endpoint and API key.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n### Start Session with KDB.AI Cloud\nsession = kdbai.Session(api_key=KDBAI_API_KEY, endpoint=KDBAI_ENDPOINT)\n```\n\n----------------------------------------\n\nTITLE: Creating Embedded Review DataFrame - Python\nDESCRIPTION: This snippet constructs a new DataFrame, 'embedded_review_df', that consolidates relevant fields from the original DataFrame along with the generated embeddings for storage or analysis.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nembedded_review_df = review_sentiments_df[\n    [\"Branch\", \"Label\", \"Score\", \"Rating\", \"Review_Text\", \"embeddings\"]\n]\n```\n\n----------------------------------------\n\nTITLE: Establishing KDB.AI Session\nDESCRIPTION: Creates a KDB.AI session and connects to the default database.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/making_queries.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nsession = kdbai.Session(endpoint=KDBAI_ENDPOINT, api_key=KDBAI_API_KEY)\ndatabase = session.database(\"default\")\n```\n\n----------------------------------------\n\nTITLE: Analyzing Sentiment for Staff-related Reviews in Python\nDESCRIPTION: This code snippet calls the review_sentiment_for_query function to analyze and visualize sentiment for staff-related reviews.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nreview_sentiment_for_query(table, embedding_model, query4, n_results=50)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Imports necessary Python libraries for data manipulation, KDB.AI client interaction, natural language processing, and document handling.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\nimport os\nfrom getpass import getpass\nimport kdbai_client as kdbai\nimport time\nfrom transformers import BertTokenizerFast\nfrom collections import Counter\n\n# Ignore Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Vector Embeddings and Data Manipulation\nDESCRIPTION: Imports numpy and pandas for data manipulation, and sentence_transformers for generating vector embeddings of sentences.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# embeddings\nimport numpy as np\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key in Python\nDESCRIPTION: This snippet configures the OpenAI API key, either from an environment variable or prompting the user for input, which is essential for accessing OpenAI's services during RAG operations.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation_evaluation.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nos.environ[\"OPENAI_API_KEY\"] = (\n    os.environ[\"OPENAI_API_KEY\"]\n    if \"OPENAI_API_KEY\" in os.environ\n    else getpass(\"OpenAI API Key: \")\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Search Term in Python\nDESCRIPTION: Sets a search query string to explore planet formation, used for subsequent retrieval and querying operations\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/qHnsw_index_pdf_search/pdf_qHNSW_Search.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nsearch_term2 = \"how does planet formation occur\"\n```\n\n----------------------------------------\n\nTITLE: Inserting Embedded Review Data into KDB.AI Table - Python\nDESCRIPTION: This snippet inserts the DataFrame containing embeddings into the KDB.AI table created earlier, making the data available for querying and analysis.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ntable.insert(embedded_review_df)\n```\n\n----------------------------------------\n\nTITLE: Creating KDB.AI Cloud Session\nDESCRIPTION: Establishes a connection to the KDB.AI Cloud service using the provided endpoint URL and API key, enabling vector database operations.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nsession = kdbai.Session(api_key=KDBAI_API_KEY, endpoint=KDBAI_ENDPOINT)\n```\n\n----------------------------------------\n\nTITLE: Loading and Splitting Text Data for RAG\nDESCRIPTION: Loads a text document (State of the Union speech) and splits it into smaller chunks using LangChain's RecursiveCharacterTextSplitter. This prepares the text for embedding and storage in the vector database.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Load the documents we want to prompt an LLM about\ndoc = TextLoader(\"data/state_of_the_union.txt\").load()\n\n# Chunk the documents into 500 character chunks using langchain's text splitter \"RucursiveCharacterTextSplitter\"\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n\n# split_documents produces a list of all the chunks created, printing out first chunk for example\npages = [p.page_content for p in text_splitter.split_documents(doc)]\n\npages[0]\n```\n\n----------------------------------------\n\nTITLE: Verifying Data Insertion in KDB.AI - Python\nDESCRIPTION: This snippet queries the KDB.AI table to verify that the embedded review data has been successfully inserted, aiding in data validation processes.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nshow_df(table.query())\n```\n\n----------------------------------------\n\nTITLE: Setting Up KDB.AI Connection Parameters in Python\nDESCRIPTION: This code sets up the KDB.AI endpoint and API key, either from environment variables or user input. These are required for connecting to the KDB.AI service.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/managing_tables.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nKDBAI_ENDPOINT = (\n    os.environ[\"KDBAI_ENDPOINT\"]\n    if \"KDBAI_ENDPOINT\" in os.environ\n    else input(\"KDB.AI endpoint: \")\n)\nKDBAI_API_KEY = (\n    os.environ[\"KDBAI_API_KEY\"]\n    if \"KDBAI_API_KEY\" in os.environ\n    else getpass(\"KDB.AI API key: \")\n)\n```\n\n----------------------------------------\n\nTITLE: Defining DataFrame Preview Helper Function\nDESCRIPTION: Creates a utility function to display DataFrame shape and preview the first few rows, making it easier to examine datasets throughout the notebook.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef show_df(df: pd.DataFrame) -> pd.DataFrame:\n    print(df.shape)\n    return df.head()\n```\n\n----------------------------------------\n\nTITLE: Displaying Search Timings in Python\nDESCRIPTION: This snippet prints out the time differences between search operations for raw and transformed data. It formats and outputs the timings to allow for performance comparison.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Temporal_Similarity_Search_Transformed_Demo.ipynb#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nprint('Raw: ' + datetime_difference(raw_start, raw_stop))\nprint('Transformed TSS: ' + datetime_difference(tsc_start,tsc_stop))\n```\n\n----------------------------------------\n\nTITLE: Displaying Correctness Evaluation Results (Matching Reference)\nDESCRIPTION: Prints the results of the correctness evaluation using a matching reference. This shows how the evaluator scores a response that matches the reference answer.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation_evaluation.ipynb#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nprint_dict(correct_eval_res1)\n```\n\n----------------------------------------\n\nTITLE: Cleanup Operations\nDESCRIPTION: Dropping the created table to clean up resources\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/fuzzy_filtering_on_metadata/fuzzy_filtering_demo.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntable.drop()\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing Spotify Song Data\nDESCRIPTION: Reads the Spotify dataset CSV, performs data cleaning and preprocessing steps to prepare for embedding creation.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/music_recommendation/music_recommendation.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nraw_song_df = pd.read_csv(\"data/song_data.csv\")\n```\n\nLANGUAGE: python\nCODE:\n```\nshow_df(raw_song_df)\n```\n\nLANGUAGE: python\nCODE:\n```\nsong_df = raw_song_df.add_prefix(\"song_\")\n```\n\nLANGUAGE: python\nCODE:\n```\nsong_df = song_df.drop(columns=[\"song_id\", \"song_release_date\"])\n```\n\nLANGUAGE: python\nCODE:\n```\ndef fix_artists(str_list):\n    return \", \".join([v for v in str_list.rstrip(\"']\").lstrip(\"['\").split(\"', '\")])\n\n\nsong_df[\"song_artists\"] = song_df[\"song_artists\"].apply(fix_artists)\n```\n\nLANGUAGE: python\nCODE:\n```\nsong_df.insert(\n    0, \"song_description\", song_df[\"song_name\"] + \" - \" + song_df[\"song_artists\"]\n)\n```\n\nLANGUAGE: python\nCODE:\n```\nsong_data = song_df[\n    ~song_df.duplicated(subset=[\"song_description\"], keep=\"first\")\n].reset_index(drop=True)\n```\n\nLANGUAGE: python\nCODE:\n```\nshow_df(song_df)\n```\n\n----------------------------------------\n\nTITLE: Creating KDB.AI Table for Review Data - Python\nDESCRIPTION: This snippet ensures that no table named 'review' exists in KDB.AI, then creates the table based on the defined schema and indexes for storing the review data.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# First ensure the table does not already exist\ntry:\n    db.table(\"review\").drop()\nexcept kdbai.KDBAIException:\n    pass\n\n# Create the database\ntable = db.create_table(table=\"review\", schema=review_schema, indexes=indexes)\n```\n\n----------------------------------------\n\nTITLE: Setting up OpenAI API Key - Python\nDESCRIPTION: This code snippet retrieves the OpenAI API key from the environment or prompts the user to enter it. The API key is stored as an environment variable for the session.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/unstructured_io_RAG/Table_RAG_Unstructured_KDBAI_LangChain_RAG.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\n# Set OpenAI API\nif \"OPENAI_API_KEY\" in os.environ:\n    OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\nelse:\n    # Prompt the user to enter the API key\n    OPENAI_API_KEY = getpass(\"OPENAI API KEY: \")\n    # Save the API key as an environment variable for the current session\n    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n```\n\n----------------------------------------\n\nTITLE: Loading Document Data\nDESCRIPTION: Loads the Federal Reserve inflation speech text document using LangChain's TextLoader.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n### Load the documents we want to prompt an LLM about\ndoc = TextLoader(\"data/inflation.txt\").load()\n```\n\n----------------------------------------\n\nTITLE: Listing Table Indexes\nDESCRIPTION: Retrieves and displays the indexes defined on the newly created table.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ntable.indexes\n```\n\n----------------------------------------\n\nTITLE: Importing KDB.AI Client and Authentication Libraries\nDESCRIPTION: Imports the KDB.AI client library and other modules needed for authentication and interaction with the vector database.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# vector DB\nimport os\nimport kdbai_client as kdbai\nfrom getpass import getpass\nimport time\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Installing necessary Python packages including llama-index, pandas and kdbai_client for the semantic search implementation.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/qHnsw_index_pdf_search/pdf_qHNSW_Search.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install llama-index llama-index-llms-openai llama-index-embeddings-openai llama-index-readers-file llama-index-vector-stores-kdbai\n!pip install pandas\n!pip install kdbai_client\n```\n\n----------------------------------------\n\nTITLE: Initializing FastEmbed Text Embedding Model\nDESCRIPTION: Creation of text embedding model instance using FastEmbed.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/rag_example.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfastembed = TextEmbedding()\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key\nDESCRIPTION: Sets the OpenAI API key either from environment variables or by prompting the user for input. The API key is required for accessing OpenAI's embedding and language models used in the RAG pipeline.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_advanced_RAG/KDBAI_Advanced_RAG_Demo.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API key: \")\nos.environ[\"OPENAI_API_KEY\"] = (\n    os.environ[\"OPENAI_API_KEY\"]\n    if \"OPENAI_API_KEY\" in os.environ\n    else getpass(\"OpenAI API Key: \")\n)\n```\n\n----------------------------------------\n\nTITLE: Extracting Text from PDF Document - Python\nDESCRIPTION: This snippet defines a function that opens a PDF file using PyMuPDF (fitz) and extracts all text content from each page, returning it as a single string.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/unstructured_io_RAG/Table_RAG_Unstructured_KDBAI_LangChain_RAG.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef extract_text_from_pdf(pdf_path):\n    text = \"\"\n    with fitz.open(pdf_path) as doc:\n        for page in doc:\n            text += page.get_text()\n    return text\n\npdf_path = './doc1.pdf'\ndocument_content = extract_text_from_pdf(pdf_path)\n```\n\n----------------------------------------\n\nTITLE: Loading Sensor Data from CSV File\nDESCRIPTION: Reads the sensor data from the extracted CSV file into a pandas DataFrame for analysis and processing.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nraw_sensors_df = pd.read_csv(\"data/sensor.csv\")\n```\n\n----------------------------------------\n\nTITLE: Deleting a KDB.AI Table in Python\nDESCRIPTION: Drops or deletes a KDB.AI table using the `table.drop()` method. This removes the table and its associated data from the KDB.AI instance, cleaning up resources after use.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/qFlat_index_pdf_search/pdf_qFlat_Search.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntable.drop()\n```\n\n----------------------------------------\n\nTITLE: Generating Text Embeddings\nDESCRIPTION: Creates embeddings for document chunks using FastEmbed model.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/rag_example.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nembedding_model = TextEmbedding()\ndocuments = [doc.text for doc in docs]\nembeddings = list(embedding_model.embed(documents))\n```\n\n----------------------------------------\n\nTITLE: Deleting KDB.AI Table in Python\nDESCRIPTION: This snippet demonstrates how to delete a KDB.AI table after its use is complete to ensure proper resource management. It assumes that the `table` object is already instantiated and ready for deletion. No additional dependencies are required beyond having a valid `table` object. The method `drop()` is called on the `table` object to perform the deletion. It does not return any value.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Hybrid_Search_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ntable.drop()\n```\n\n----------------------------------------\n\nTITLE: Inserting and Querying Data\nDESCRIPTION: Batch insertion of movie data into the vector database and verification of inserted data\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/fuzzy_filtering_on_metadata/fuzzy_filtering_demo.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom tqdm import tqdm\nn = 2000  # chunk row size\n\nfor i in tqdm(range(0, df.shape[0], n)):\n    table.insert(df[i:i+n].reset_index(drop=True))\n```\n\nLANGUAGE: python\nCODE:\n```\ndef show_df(df: pd.DataFrame) -> pd.DataFrame:\n    print(df.shape)\n    return df.head()\n```\n\nLANGUAGE: python\nCODE:\n```\nshow_df(table.query())\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Libraries\nDESCRIPTION: Installation of necessary Python packages including llama-index, fastembed, openai, kdbai_client, and onnxruntime.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/rag_example.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install llama-index fastembed openai kdbai_client onnxruntime==1.19.2\n```\n\n----------------------------------------\n\nTITLE: Importing KDB.AI Client Libraries for Vector Database\nDESCRIPTION: Imports the required modules for connecting to and working with a KDB.AI vector database, including time for managing execution pauses.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/metadata_filtering/metadata_filtering_demo.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# vector DB\nimport os\nfrom getpass import getpass\nimport kdbai_client as kdbai\nimport time\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key\nDESCRIPTION: Sets the OpenAI API key as an environment variable either from existing environment variables or by prompting the user for input. This key is required for accessing OpenAI's embedding and vision models.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/multimodal_RAG_unified_text/multi_modal_demo.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nos.environ[\"OPENAI_API_KEY\"] = (\n    os.environ[\"OPENAI_API_KEY\"]\n    if \"OPENAI_API_KEY\" in os.environ\n    else getpass(\"OpenAI API Key: \")\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Time Series Windows in Python\nDESCRIPTION: Generates sliding windows of time series data for each stock symbol, creating a new DataFrame with vectorized price data.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Temporal_Similarity_Search_Transformed_Demo.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Create the vector column\nD = 1000 # Sliding Window Size\nvecdf = df.groupby(['sym']).apply(\n    lambda x: pd.DataFrame({\n        'sym': x['sym'].iloc[0],\n        'time': sliding_window_view(x['time'], D)[:, 0],  # Adjusted to keep the last time in the window\n        'price': list(sliding_window_view(x['price'], D))\n    })\n).reset_index(drop=True).reset_index()\nmemory_vecdf_created=get_memory_usage()\n```\n\n----------------------------------------\n\nTITLE: Installing KDB.AI Client Library\nDESCRIPTION: Installs the kdbai_client package required to interact with the KDB.AI vector database.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install kdbai_client\n```\n\n----------------------------------------\n\nTITLE: Splitting Document into Chunks in Python\nDESCRIPTION: This code chunk splits the loaded document into smaller segments (chunks) for processing, which aids in efficient retrieval and generation during the evaluation.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation_evaluation.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Chunk the documents into 500 character chunks using langchain's text splitter \"RucursiveCharacterTextSplitter\"\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n```\n\nLANGUAGE: python\nCODE:\n```\n# split_documents produces a list of all the chunks created, printing out first chunk for example\npages = [p.page_content for p in text_splitter.split_documents(doc)]\n```\n\n----------------------------------------\n\nTITLE: Dropping KDB.AI Table in Python\nDESCRIPTION: This code snippet demonstrates how to drop a KDB.AI table and its database using Python. This is a best practice for releasing resources once a table is no longer needed.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ntable.drop()\ndb.drop()\n```\n\n----------------------------------------\n\nTITLE: Grouping Review Sentiments by Branch and Label - Python\nDESCRIPTION: This snippet groups the review sentiments DataFrame by 'Branch' and 'Label', calculating the mean rating for each combination. The output is an unstacked DataFrame that facilitates further analysis and visualization.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ngrouped_review_sentiments_df = (\n    review_sentiments_df.groupby([\"Branch\", \"Label\"])[\"Rating\"].mean().unstack()\n)\ngrouped_review_sentiments_df\n```\n\n----------------------------------------\n\nTITLE: Function for Querying QA Bot with New Input\nDESCRIPTION: Defines a utility function `query_qabot` to streamline querying the QA bot with dynamic input. The function takes a string query and queries it against the QA bot, returning the response.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation.ipynb#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef query_qabot(qabot, query: str):\n    print(new_query)\n    print(\"---\")\n    return qabot.invoke(dict(query=new_query))[\"result\"]\n```\n\nLANGUAGE: python\nCODE:\n```\nnew_query = \"what are the things this country needs to protect?\"\nquery_qabot(qabot, new_query)\n```\n\n----------------------------------------\n\nTITLE: Dropping Existing Table if Present\nDESCRIPTION: Attempts to drop a table named \"inflation\" if it already exists, to avoid conflicts during creation.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# First ensure the table does not already exist\ntry:\n    db.table(\"inflation\").drop()\nexcept kdbai.KDBAIException:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Defining a Helper Function to Print Dictionary in Python\nDESCRIPTION: This helper function takes a dictionary and prints its contents in a structured format, enhancing output readability during the evaluation process.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation_evaluation.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef print_dict(d: dict) -> None:\n    for k, v in d.items():\n        print(f\"\\n{k.capitalize()}\\n---\\n{v}\".replace('\\n\\n', '\\n'))\n```\n\n----------------------------------------\n\nTITLE: Defining ZIP File Extraction Function\nDESCRIPTION: Creates a function to extract contents from a ZIP file into the data directory. Used to extract the sensor dataset that has been compressed due to its large size.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef extract_zip(file_name):\n    with ZipFile(file_name, \"r\") as zipf:\n        zipf.extractall(\"data\")\n```\n\n----------------------------------------\n\nTITLE: Displaying Table Elements - Python\nDESCRIPTION: This snippet checks each element to determine if it is a table and prints the text of those elements, aiding in visualizing the structured data from the PDF.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/unstructured_io_RAG/Table_RAG_Unstructured_KDBAI_LangChain_RAG.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfor element in elements:\n  if element.to_dict()['type'] == 'Table':\n    print(element.text)\n```\n\n----------------------------------------\n\nTITLE: Finding Songs by Name Across All Artists in Python\nDESCRIPTION: Performs a similarity search for all songs titled 'Love Me' by any artist in the KDB.AI vector database. Uses the exact parameter to match the song name precisely.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/music_recommendation/music_recommendation.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfind_similar_songs(table, song_name=\"Love Me\", exact=True)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for KDB.AI and NLP\nDESCRIPTION: Installs the required Python packages for KDB.AI client, word embeddings, and natural language processing.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/music_recommendation/music_recommendation.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install kdbai_client\n!pip install gensim nltk\n```\n\n----------------------------------------\n\nTITLE: Storing Elements in a DataFrame - Python\nDESCRIPTION: This snippet creates a pandas DataFrame to store original elements along with their IDs, text content, metadata, and embeddings, facilitating structured access to the extracted data.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/unstructured_io_RAG/Table_RAG_Unstructured_KDBAI_LangChain_RAG.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\ndata = []\n\nfor c in elements:\n  row = {}\n  row['id'] = c.id\n  row['text'] = c.text.encode()\n  row['metadata'] = c.metadata.to_dict()\n  row['embedding'] = c.embeddings\n  data.append(row)\n\ndf_non_contextualized = pd.DataFrame(data)\ndf_non_contextualized.head()\n```\n\n----------------------------------------\n\nTITLE: Setting KDB.AI Connection Environment Variables\nDESCRIPTION: Sets up environment variables for the KDB.AI endpoint and API key. Provides flexibility to use existing environment variables or prompt for input, which is useful for different execution environments.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/metadata_filtering/metadata_filtering_demo.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nos.environ[\"KDBAI_ENDPOINT\"]=\"http://localhost:8082\"\nos.environ[\"KDBAI_API_KEY\"]=\"\"\n\n# Set up KDB.AI endpoint and API key\nKDBAI_ENDPOINT = (\n    os.environ[\"KDBAI_ENDPOINT\"]\n    if \"KDBAI_ENDPOINT\" in os.environ\n    else input(\"KDB.AI endpoint: \")\n)\nKDBAI_API_KEY = (\n    os.environ[\"KDBAI_API_KEY\"]\n    if \"KDBAI_API_KEY\" in os.environ\n    else getpass(\"KDB.AI API key: \")\n)\n```\n\n----------------------------------------\n\nTITLE: Re-embedding Updated Elements - Python\nDESCRIPTION: This snippet re-employs the OpenAI embedding encoder to embed the previously updated elements that now include new contextualized descriptions, ensuring they are ready for storage in KDB.AI.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/unstructured_io_RAG/Table_RAG_Unstructured_KDBAI_LangChain_RAG.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nelements = embedding_encoder.embed_documents(\n    elements=elements\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for KDB.AI and Data Analysis in Python\nDESCRIPTION: Imports necessary libraries including kdbai_client for KDB.AI interaction, pandas and numpy for data manipulation, matplotlib for plotting, and other utility libraries.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Temporal_Similarity_Search_Transformed_Demo.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# KX Dependencies\nimport kdbai_client as kdbai\n# Other Dependencies\nimport pandas as pd\nimport numpy as np\nfrom numpy.lib.stride_tricks import sliding_window_view\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport os\nfrom getpass import getpass\nimport time\n```\n\n----------------------------------------\n\nTITLE: Handling Existing KDB.AI Tables\nDESCRIPTION: Ensures clean setup by attempting to drop the \"multi_modal_demo\" table if it already exists. This prevents errors when creating a new table with the same name.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/multimodal_RAG_unified_text/multi_modal_demo.ipynb#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndatabase = session.database(\"default\")\n# First ensure the table does not already exist\ntry:\n    database.table(\"multi_modal_demo\").drop()\nexcept kdbai.KDBAIException:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Displaying Cleaned Sensor Data Preview\nDESCRIPTION: Shows the shape and first few rows of the preprocessed sensor data to verify cleaning operations before proceeding with analysis.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nshow_df(sensors_df)\n```\n\n----------------------------------------\n\nTITLE: Loading Synthetic Market Data in Python\nDESCRIPTION: Reads synthetic market time series data from a Parquet file into a pandas DataFrame.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_non_transformed/Temporal_Similarity_Search_Non-Transformed_Demo.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_parquet('data/marketTrades.parquet')\ndf\n```\n\n----------------------------------------\n\nTITLE: Summary of Table Content - Markdown\nDESCRIPTION: The markdown table presents concise financial data highlighting revenue and operational income/loss for Meta Platforms, creating an easy-to-read digital format.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/unstructured_io_RAG/Table_RAG_Unstructured_KDBAI_LangChain_RAG.ipynb#2025-04-21_snippet_16\n\nLANGUAGE: markdown\nCODE:\n```\n| Segment                | Q2 2024 Revenue | Q2 2023 Revenue | Q2 2024 Income | Q2 2023 Income |\n|------------------------|-----------------|-----------------|----------------|----------------|\n| Advertising            | $XX             | $YY             | $AA            | $AB            |\n| Other Revenue          | $ZZ             | $WW             | $CC            | $DD            |\n```\n\n----------------------------------------\n\nTITLE: Verifying Inserted Data in KDB.AI Table - Python\nDESCRIPTION: Runs a query on the table to check if the inserted data is present. The show_df function is used to display the queried data.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nshow_df(table.query())\n```\n\n----------------------------------------\n\nTITLE: Configuring LLM and Embedding Models\nDESCRIPTION: Set up OpenAI's language model and embedding models for document processing\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaParse_pdf_RAG/llamaParse_demo.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nEMBEDDING_MODEL  = \"text-embedding-3-small\"\nGENERATION_MODEL = \"gpt-4o\"\n\nllm = OpenAI(model=GENERATION_MODEL)\nembed_model = OpenAIEmbedding(model=EMBEDDING_MODEL)\n\nSettings.llm = llm\nSettings.embed_model = embed_model\n```\n\n----------------------------------------\n\nTITLE: Creating DataFrame for Chunks and Vectors\nDESCRIPTION: Initializes an empty pandas DataFrame to store document chunks and their corresponding sparse and dense vectors.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n### Create a blank dataframe to store chunks and vectors in before insertion\ndata = {\n    'ID':[],\n    'chunk': [],\n    'dense': [],\n    'sparse': []\n}\n\n# Create the DataFrame\ndf = pd.DataFrame(data)\n```\n\n----------------------------------------\n\nTITLE: Listing Available Databases\nDESCRIPTION: Retrieves and displays the list of available databases in the connected KDB.AI instance.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nsession.databases()\n```\n\n----------------------------------------\n\nTITLE: Alternative KDB.AI Server Connection (Commented)\nDESCRIPTION: Provides a commented code example for connecting to a local KDB.AI Server instance instead of the cloud service, useful for self-hosted deployments.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_31\n\nLANGUAGE: python\nCODE:\n```\n# session = kdbai.Session(endpoint=\"http://localhost:8082\")\n```\n\n----------------------------------------\n\nTITLE: Creating a Contradictory Reference Answer\nDESCRIPTION: Defines a reference answer that contradicts the information returned by the QA bot about job creation. This will be used to test how the correctness evaluator handles cases where the response and reference disagree.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation_evaluation.ipynb#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ncontractic_ref = \"12000 jobs were created due to manufacturing of electric vehicles.\"\n```\n\n----------------------------------------\n\nTITLE: Displaying Embedding DataFrame Preview\nDESCRIPTION: Shows the first few rows of the embedding DataFrame, which contains the timestamp and sensor value sequences for each time window.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n# Show the resulting DataFrame\nshow_df(embedding_df)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Python Libraries for PDF Parsing\nDESCRIPTION: Import essential libraries for document parsing, vector storage, embeddings, and AI model integration\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaParse_pdf_RAG/llamaParse_demo.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom llama_parse import LlamaParse\nfrom llama_index.core import Settings\nfrom llama_index.core import StorageContext\nfrom llama_index.core import VectorStoreIndex\nfrom llama_index.core.node_parser import MarkdownElementNodeParser\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.vector_stores.kdbai import KDBAIVectorStore\nfrom llama_index.postprocessor.cohere_rerank import CohereRerank\nfrom getpass import getpass\nimport os\nimport kdbai_client as kdbai\n```\n\n----------------------------------------\n\nTITLE: Defining Memory Usage Function in Python\nDESCRIPTION: Creates a function to report memory usage of Python and KDB.AI, which is useful for performance monitoring throughout the notebook.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Temporal_Similarity_Search_Transformed_Demo.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Dependencies for metric gathering\nimport psutil\nimport datetime\n\n# Report memory usage of Python + KDB.AI\ndef get_memory_usage():\n    virtual_memory = psutil.virtual_memory()\n    return virtual_memory.used / (1024 ** 2)  # Memory usage in megabytes\n```\n\n----------------------------------------\n\nTITLE: Checking and Dropping Existing KDB.AI Table\nDESCRIPTION: Connects to the default KDB.AI database and attempts to drop the table if it already exists, with error handling for cases where the table doesn't exist. Adds a short delay after dropping to ensure operation completion.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/metadata_filtering/metadata_filtering_demo.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# get the database connection. Default database name is 'default'\ndatabase = session.database('default')\n# First ensure the table does not already exist\ntry:\n    database.table(\"metadata_demo\").drop()\n    time.sleep(5)\nexcept kdbai.KDBAIException:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for PDF Parsing\nDESCRIPTION: Install necessary Python packages for LlamaIndex, LlamaParse, OpenAI integration, and KDB.AI vector store\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaParse_pdf_RAG/llamaParse_demo.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install llama-index\n!pip install llama-index-core\n!pip install llama-index-embeddings-openai\n!pip install llama-parse\n!pip install llama-index-vector-stores-kdbai\n!pip install pandas\n!pip install llama-index-postprocessor-cohere-rerank\n!pip install kdbai_client\n```\n\n----------------------------------------\n\nTITLE: Processing Table Elements and Updating Descriptions - Python\nDESCRIPTION: This snippet processes each element of type Table, retrieves its contents, and generates a description along with markdown formatting using the previously defined function, updating the element's text.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/unstructured_io_RAG/Table_RAG_Unstructured_KDBAI_LangChain_RAG.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfor element in elements:\n  if element.to_dict()['type'] == 'Table':\n    table_content = element.to_dict()['text']\n\n    # Get description and markdown table from GPT-4\n    result = get_table_description(table_content, document_content)\n    element.text = result\n\nprint(\"Processing complete.\")\n```\n\n----------------------------------------\n\nTITLE: Creating a New Database\nDESCRIPTION: Creates a new database named \"myDatabase\" in the KDB.AI instance for storing vector data.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Create the database\ndb = session.create_database(\"myDatabase\")\n```\n\n----------------------------------------\n\nTITLE: Loading Pre-Processed Movie Dataset with Embeddings\nDESCRIPTION: Loads the pickle file containing the pre-processed movie dataset with embeddings into a pandas DataFrame for further processing.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/metadata_filtering/metadata_filtering_demo.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Read in the Movies dataframe\ndf = pd.read_pickle(\"./data/filtered_embedded_movies.pkl\")\n```\n\n----------------------------------------\n\nTITLE: Deleting tables and databases in KDB.AI with Python\nDESCRIPTION: Remove a KDB.AI table and database using the `drop` method for both table and db objects. Once a table is dropped, it cannot be reused, so ensure that you no longer need the data before performing this operation.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/HuggingFace_search/huggingface_inference.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntable.drop()\ndb.drop()\n```\n\n----------------------------------------\n\nTITLE: Plotting Mean Rating from Grouped Review Sentiments - Python\nDESCRIPTION: This snippet calls the plot_grouped_bar_chart function using the previously computed grouped review sentiments DataFrame to visualize the mean ratings.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nplot_grouped_bar_chart(grouped_review_sentiments_df, y_label=\"Mean Rating\")\n```\n\n----------------------------------------\n\nTITLE: Displaying Sensor Data Preview\nDESCRIPTION: Shows the shape and first few rows of the raw sensor data to understand its structure before preprocessing.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nshow_df(raw_sensors_df)\n```\n\n----------------------------------------\n\nTITLE: Defining Memory Usage Function in Python\nDESCRIPTION: Creates a function to report memory usage of Python and KDB.AI processes.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_non_transformed/Temporal_Similarity_Search_Non-Transformed_Demo.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Dependencies for metric gathering\nimport psutil\nimport datetime\n\n# Report memory usage of Python + KDB.AI\ndef get_memory_usage():\n    virtual_memory = psutil.virtual_memory()\n    return virtual_memory.used / (1024 ** 2)  # Memory usage in megabytes\n```\n\n----------------------------------------\n\nTITLE: Defining Company Data - Python\nDESCRIPTION: This snippet defines a list of tuples containing company names and descriptions for further processing in a KDB.AI table.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/quickstarts/python_quickstart.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncompany_data = [\n    (\"Apple\", \"A technology company known for its iPhones, MacBooks, and innovative designs\"),\n    (\"Google\", \"A search engine giant that also specializes in advertising, cloud computing, and AI\"),\n    (\"Brave\", \"A privacy-focused search engine and browser.\"),\n    (\"Perplexity\", \"An answer engine that searches the internet and uses a large language model to summarize web data.\"),\n    (\"Amazon\", \"An e-commerce leader that offers a wide range of products and services, including AWS\"),\n    (\"Microsoft\", \"A technology company known for its software products like Windows and Office\"),\n    (\"Facebook\", \"A social media platform that connects people worldwide and owns Instagram and WhatsApp\"),\n    (\"Tesla\", \"An electric vehicle manufacturer known for its innovative and sustainable energy solutions\"),\n    (\"Rivian\", \"An electric vehicle company focusing on adventure-oriented trucks and SUVs\"),\n    (\"Lucid Motors\", \"A company specializing in high-performance electric luxury vehicles\"),\n    (\"Netflix\", \"A streaming service that offers a wide variety of TV shows, movies, and original content\"),\n    (\"Hulu\", \"A streaming platform providing a wide range of TV shows, movies, and original content\"),\n    (\"Disney+\", \"A streaming service offering movies, TV shows, and original content from Disney\"),\n    (\"Uber\", \"A ride-sharing company that also offers food delivery and freight services\"),\n    (\"Lyft\", \"A ride-sharing platform connecting passengers with drivers\"),\n    (\"Didi\", \"A Chinese ride-sharing company offering various transportation services\"),\n    (\"Airbnb\", \"A platform that allows people to rent out their homes or find lodging worldwide\"),\n    (\"Vrbo\", \"A vacation rental online marketplace where homeowners list their properties for short-term rentals\"),\n    (\"Booking.com\", \"An online travel agency offering lodging reservations and other travel products\"),\n    (\"Spotify\", \"A music streaming service offering a wide range of songs, albums, and podcasts\"),\n    (\"Apple Music\", \"A music and video streaming service developed by Apple Inc.\"),\n    (\"YouTube Music\", \"A music streaming service developed by YouTube\"),\n    (\"Twitter\", \"A social media platform for sharing short messages and real-time updates\"),\n    (\"Instagram\", \"A photo and video sharing social networking service\"),\n    (\"Snapchat\", \"A multimedia messaging app known for its disappearing messages\"),\n    (\"LinkedIn\", \"A professional networking platform for job seekers and employers\"),\n    (\"Slack\", \"A collaboration platform for team communication and project management\"),\n    (\"Microsoft Teams\", \"A collaboration platform for team communication and project management\"),\n    (\"Zoom\", \"A video conferencing platform used for virtual meetings and webinars\")\n]\n```\n\n----------------------------------------\n\nTITLE: Defining Time Window Parameters\nDESCRIPTION: Sets parameters for creating overlapping time windows from the sensor data: window_size determines how many rows are in each window, and step_size controls the sliding interval.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n# Set the window size (number of rows in each window)\nwindow_size = 100\nstep_size = 1\n```\n\n----------------------------------------\n\nTITLE: Downloading Sample Data (Optional)\nDESCRIPTION: Downloads the Federal Reserve Inflation speech data if it doesn't exist in the local environment. This step is particularly useful for Colab environments.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n### !!! Only run this cell if you need to download the data into your environment, for example in Colab\n### This downloads Federal Reserve Inflation speech data\nif os.path.exists(\"./data/inflation.txt\") == False:\n  !mkdir ./data\n  !wget -P ./data https://raw.githubusercontent.com/KxSystems/kdbai-samples/main/hybrid_search/data/inflation.txt\n```\n\n----------------------------------------\n\nTITLE: Timing Similarity Search Operations in Python\nDESCRIPTION: This snippet defines a function to calculate the time taken for a similarity search operation. It outputs the elapsed time in hours, minutes, seconds, and milliseconds, providing a detailed breakdown of search duration.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Temporal_Similarity_Search_Transformed_Demo.ipynb#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n### Function to calculate how long a similarity search takes\ndef datetime_difference(datetime1, datetime2):\n    # Calculate the difference between the two datetime objects\n    difference = datetime2 - datetime1\n\n    # Calculate total seconds and milliseconds from the difference\n    total_seconds = difference.total_seconds()\n    hours = int(total_seconds // 3600)\n    minutes = int((total_seconds % 3600) // 60)\n    seconds = int(total_seconds % 60)\n    milliseconds = int((total_seconds - int(total_seconds)) * 1000)\n\n    # Print the difference in hours, minutes, seconds, and milliseconds\n    return f\"Hours: {hours}, Minutes: {minutes}, Seconds: {seconds}, Milliseconds: {milliseconds}\"\n```\n\n----------------------------------------\n\nTITLE: Initializing KDB.AI Server Session (Alternative)\nDESCRIPTION: Alternative code for connecting to a self-hosted KDB.AI Server instance using a local endpoint.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n### start session with KDB.AI Server\n#session = kdbai.Session(endpoint=\"http://localhost:8082\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Exploring Movie Data\nDESCRIPTION: Loading the movie dataset from pickle file and performing initial data exploration\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/fuzzy_filtering_on_metadata/fuzzy_filtering_demo.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_pickle(\"./data/filtered_embedded_movies.pkl\")\n```\n\nLANGUAGE: python\nCODE:\n```\nprint(df.shape[0])\n```\n\nLANGUAGE: python\nCODE:\n```\nfor column in df.columns:\n    print(column)\n```\n\nLANGUAGE: python\nCODE:\n```\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Extracting Sensor Data from ZIP Archive\nDESCRIPTION: Calls the extract_zip function to unpack the compressed sensor data file into the data directory for processing.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nextract_zip(\"data/archive.zip\")\n```\n\n----------------------------------------\n\nTITLE: Inserting Data into KDB.AI Table in Python\nDESCRIPTION: Inserts the data from the pandas DataFrame into the KDB.AI table.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_non_transformed/Temporal_Similarity_Search_Non-Transformed_Demo.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntable.insert(df)\n```\n\n----------------------------------------\n\nTITLE: Creating a KDB.AI Session with Local Server Endpoint (Commented)\nDESCRIPTION: Commented code showing how to connect to a local KDB.AI Server instance without requiring an API key, useful for self-hosted deployments.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/metadata_filtering/metadata_filtering_demo.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n#session = kdbai.Session(endpoint=KDBAI_ENDPOINT)\n```\n\n----------------------------------------\n\nTITLE: Verifying Connection to KDB.AI and Listing Databases - Python\nDESCRIPTION: This snippet checks the established session's connection by retrieving a list of databases defined in KDB.AI, thereby confirming that the connection is valid.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nsession.databases()\n```\n\n----------------------------------------\n\nTITLE: Creating Helper Function to Display Table Query Results\nDESCRIPTION: Defines a utility function to display query results from the KDB.AI table, showing both the shape of the result (row count) and a preview of the first few rows.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/metadata_filtering/metadata_filtering_demo.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n#function to view the dataframe within the table\ndef show_df(df: pd.DataFrame) -> pd.DataFrame:\n    print(df.shape)\n    return df.head()\n```\n\n----------------------------------------\n\nTITLE: Data Download Setup\nDESCRIPTION: Setting up the data directory structure and downloading the state of the union speech text file.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Sub_Question_Query_Engine_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport urllib.request\n```\n\nLANGUAGE: python\nCODE:\n```\n# Root path\nroot_path = os.path.abspath(os.getcwd())\n\n# Data directory and path\ndata_dir = \"data\"\ndata_path = os.path.join(root_path, data_dir)\nif not os.path.exists(data_path):\n    os.mkdir(data_path)\n```\n\nLANGUAGE: python\nCODE:\n```\ntext_url = \"https://raw.githubusercontent.com/KxSystems/kdbai-samples/main/retrieval_augmented_generation/data/state_of_the_union.txt\"\nwith urllib.request.urlopen(text_url) as response:\n    text_content = response.read().decode(\"utf-8\")\n\ntext_file_name = text_url.split('/')[-1]\ntext_path = os.path.join(data_path, text_file_name)\nif not os.path.exists(text_path):\n    with open(text_path, 'w') as text_file:\n        text_file.write(text_content)\n\nmetadata = {\n    f\"{data_dir}/{text_file_name}\": {\n        \"title\": text_file_name.split('.')[0],\n        \"file_path\": text_path\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing RetrievalQA with GPT-4o\nDESCRIPTION: Sets up a RetrievalQA instance using OpenAI's GPT-4o and a vector store retriever. The retriever leverages KDB.AI for searching relevant document chunks, using a 'stuff' chain type for inserting document pieces into a prompt.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nK = 10\n```\n\nLANGUAGE: python\nCODE:\n```\nqabot = RetrievalQA.from_chain_type(\n    chain_type=\"stuff\",\n    llm=ChatOpenAI(model=\"gpt-4o\", temperature=0.0),\n    retriever=vecdb_kdbai.as_retriever(search_kwargs=dict(k=K, index=\"flat_index\")),\n    return_source_documents=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring KDB.AI Connection\nDESCRIPTION: Sets up the connection parameters for KDB.AI using environment variables or user input.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/making_queries.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nKDBAI_ENDPOINT = (\n    os.environ[\"KDBAI_ENDPOINT\"]\n    if \"KDBAI_ENDPOINT\" in os.environ\n    else input(\"KDB.AI endpoint: \")\n)\nKDBAI_API_KEY = (\n    os.environ[\"KDBAI_API_KEY\"]\n    if \"KDBAI_API_KEY\" in os.environ\n    else input(\"KDB.AI API key: \")\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries in Python\nDESCRIPTION: This snippet imports necessary libraries for vector database operations and LangChain functionalities, which are essential for evaluating the RAG system.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation_evaluation.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# vector DB\nfrom getpass import getpass\nimport kdbai_client as kdbai\nimport time\n\n# langchain packages\nfrom langchain.chains import RetrievalQA\nfrom langchain_openai import ChatOpenAI\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_openai import OpenAIEmbeddings\nfrom langchain_community.vectorstores import KDBAI\n\n# evaluation packages\nfrom langchain.evaluation import load_evaluator\n```\n\n----------------------------------------\n\nTITLE: Configuring Pandas Display Options\nDESCRIPTION: Sets the maximum column width for pandas DataFrame display to ensure that longer text content is visible in the output.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npd.set_option(\"display.max_colwidth\", 300)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Importing necessary Python libraries and modules for PDF processing, vector embeddings, and database operations.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/qHnsw_index_pdf_search/pdf_qHNSW_Search.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom getpass import getpass\nimport re\nimport os\nimport shutil\nimport time\nimport urllib\n\nimport pandas as pd\n\nfrom llama_index.core import (\n    Settings,\n    SimpleDirectoryReader,\n    StorageContext,\n    VectorStoreIndex,\n)\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.core.retrievers import VectorIndexRetriever\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.llms.openai import OpenAI\nfrom llama_index.vector_stores.kdbai import KDBAIVectorStore\n\nimport kdbai_client as kdbai\n\nOUTDIR = \"pdf\"\nRESET = True\n```\n\n----------------------------------------\n\nTITLE: Querying Table Content\nDESCRIPTION: Retrieves and displays the content of the table to verify successful data insertion.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ntable.query()\n```\n\n----------------------------------------\n\nTITLE: Downloading Sample PDF Data\nDESCRIPTION: Creates a data directory and downloads a sample research paper PDF from the GitHub repository. This step is only needed when running in environments like Google Colab.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n### !!! Only run this cell if you need to download the data into your environment, for example in Colab\n### This downloads research paper pdf into your environment\n!mkdir ./data\n!wget -P ./data https://raw.githubusercontent.com/KxSystems/kdbai-samples/main/document_search/data/research_paper.pdf\n```\n\n----------------------------------------\n\nTITLE: Importing Required Python Packages for KDB.AI Interaction\nDESCRIPTION: This code imports necessary Python packages for working with KDB.AI, including os for environment variables, getpass for secure input, kdbai_client for KDB.AI interaction, and time for potential timing operations.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/managing_tables.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# vector DB\nimport os\nfrom getpass import getpass\nimport kdbai_client as kdbai\nimport time\n```\n\n----------------------------------------\n\nTITLE: Downloading Text Data from GitHub\nDESCRIPTION: Downloads a text file about inflation from GitHub and saves it locally. Also creates metadata for the file with title and file path.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Hybrid_Search_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntext_url = \"https://raw.githubusercontent.com/KxSystems/kdbai-samples/main/hybrid_search/data/inflation.txt\"\nwith urllib.request.urlopen(text_url) as response:\n    text_content = response.read().decode(\"utf-8\")\n\ntext_file_name = text_url.split('/')[-1]\ntext_path = os.path.join(data_path, text_file_name)\nif not os.path.exists(text_path):\n    with open(text_path, 'w') as text_file:\n        text_file.write(text_content)\n\nmetadata = {\n    f\"{data_dir}/{text_file_name}\": {\n        \"title\": text_file_name,\n        \"file_path\": text_path\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Importing KDB.AI Client Libraries\nDESCRIPTION: Imports the KDB.AI client library and getpass for secure password input.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Hybrid_Search_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport kdbai_client as kdbai\nfrom getpass import getpass\n```\n\n----------------------------------------\n\nTITLE: Invoking RetrievalQA Execution with Query\nDESCRIPTION: Executes a RetrievalQA query over pre-defined document indices using a GPT-4o language model. Combines querying capabilities with powerful information retrieval from KDB.AI, thus facilitating context-aware output generation.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation.ipynb#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nprint(query)\nprint(\"-----\")\nprint(qabot.invoke(dict(query=query))[\"result\"])\n```\n\n----------------------------------------\n\nTITLE: Loading File Paths for Multimodal Data\nDESCRIPTION: Gets the list of file paths for all text and image files from the specified directories. These paths will be used to process and embed the multimodal content.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/multimodal_RAG_unified_text/multi_modal_demo.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimages = os.listdir(\"./data/images\")\ntexts = os.listdir(\"./data/text\")\n```\n\n----------------------------------------\n\nTITLE: Importing NumPy and Pandas for Data Manipulation in Python\nDESCRIPTION: This snippet imports NumPy and Pandas, which are used for numerical operations and data manipulation respectively in the context of working with KDB.AI.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/managing_tables.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\n```\n\n----------------------------------------\n\nTITLE: Dropping Existing Table\nDESCRIPTION: Attempts to drop the existing table to ensure a clean slate for the tutorial.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/making_queries.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    database.table(\"data\").drop() # Drop the table if it already exists\nexcept kdbai.KDBAIException:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Creating KDB.AI Database in Python\nDESCRIPTION: Drops any existing database named 'myDatabase' and creates a new one with the same name.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Temporal_Similarity_Search_Transformed_Demo.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# ensure no database called \"myDatabase\" exists\ntry:\n    session.database(\"myDatabase\").drop()\nexcept kdbai.KDBAIException:\n    pass\n```\n\nLANGUAGE: python\nCODE:\n```\n# Create the database\ndb = session.create_database(\"myDatabase\")\n```\n\n----------------------------------------\n\nTITLE: Listing Tables After Creation\nDESCRIPTION: Retrieves and displays the updated list of tables in the database after creating the new table.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndb.tables\n```\n\n----------------------------------------\n\nTITLE: Evaluating Answer Correctness with Matching Reference\nDESCRIPTION: Evaluates the correctness of the second query result against a matching reference. This tests whether the correctness evaluator can identify when the QA bot's answer aligns with correct information.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation_evaluation.ipynb#2025-04-21_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ncorrect_eval_res1 = correct_evaluator.evaluate_strings(\n    prediction=res2, input=query2, reference=matching_ref\n)\n```\n\n----------------------------------------\n\nTITLE: Loading Synthetic Market Data in Python\nDESCRIPTION: Reads pre-generated synthetic market data from a Parquet file into a pandas DataFrame.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Temporal_Similarity_Search_Transformed_Demo.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_parquet('data/marketTrades.parquet')\n```\n\n----------------------------------------\n\nTITLE: Previewing Movie Dataset Content with DataFrame.head()\nDESCRIPTION: Displays the first few rows of the DataFrame to preview the movie data including titles, plots, and other metadata that will be used in the vector database.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/metadata_filtering/metadata_filtering_demo.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n#Let us inspect the dataframe\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Showing Text Data Preview\nDESCRIPTION: Calls the show_text function to display the beginning of the downloaded text file.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Hybrid_Search_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nshow_text(text_path)\n```\n\n----------------------------------------\n\nTITLE: Executing a Query with KDB.AI QA Bot\nDESCRIPTION: Calls the query_qabot function with the previously defined bot and query. This sends the question to the RAG system and stores the result for evaluation.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation_evaluation.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nres2 = query_qabot(qabot, query2)\n```\n\n----------------------------------------\n\nTITLE: Loading Documents with Metadata\nDESCRIPTION: Loads the documents using SimpleDirectoryReader with associated metadata.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Hybrid_Search_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\nlocal_files = [fpath for fpath in metadata]\ndocuments = SimpleDirectoryReader(input_files=local_files, file_metadata=get_metadata)\n\ndocs = documents.load_data()\nlen(docs)\n```\n\n----------------------------------------\n\nTITLE: Viewing Example Table Element - Python\nDESCRIPTION: This snippet prints a particular table element after extraction, showcasing the raw data layout prior to any additional processing.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/unstructured_io_RAG/Table_RAG_Unstructured_KDBAI_LangChain_RAG.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprint(elements[-2])\n```\n\n----------------------------------------\n\nTITLE: Displaying the First Sentence\nDESCRIPTION: Retrieves and displays the first sentence from the list of extracted sentences to verify the tokenization process.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\npdf_sentences[0]\n```\n\n----------------------------------------\n\nTITLE: Displaying Correctness Evaluation Results (Contradictory Reference)\nDESCRIPTION: Prints the results of the correctness evaluation using a contradictory reference. This shows how the evaluator scores a response that contradicts the reference answer.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation_evaluation.ipynb#2025-04-21_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nprint_dict(correct_eval_res2)\n```\n\n----------------------------------------\n\nTITLE: Creating Text Preview Function\nDESCRIPTION: Defines a helper function to display the first 500 characters of a text file.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Hybrid_Search_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef show_text(text_path):\n    with open(text_path, 'r') as text_file:\n        contents = text_file.read()\n    print(contents[:500])\n    print(\"=\"*80)\n```\n\n----------------------------------------\n\nTITLE: Alternative KDB.AI Server Connection (Commented)\nDESCRIPTION: Shows how to connect to a local KDB.AI Server instance (commented out code).\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Hybrid_Search_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# session = kdbai.Session(endpoint=\"http://localhost:8082\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up Hybrid Search with Dense Priority\nDESCRIPTION: Configures a retriever with hybrid search mode giving higher weight (0.9) to dense vector search.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Hybrid_Search_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\nretriever = index.as_retriever(\n                        vector_store_query_mode=\"hybrid\",\n                        similarity_top_k=5,\n                        vector_store_kwargs={\n                                \"index\" : \"flat\",\n                                \"indexWeight\" : 0.9,\n                                \"indexSparse\" : \"sparse_index\",\n                                \"indexSparseWeight\" : 0.1,\n                                },\n                            )\n```\n\n----------------------------------------\n\nTITLE: Loading Text Document into Python\nDESCRIPTION: This snippet loads the State of the Union speech document to be used in retrieval and generation tasks, preparing the data for subsequent processing.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation_evaluation.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Load the documents we want to prompt an LLM about\ndoc = TextLoader(\"data/state_of_the_union.txt\").load()\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies for KDB AI Samples\nDESCRIPTION: This snippet specifies the required Python packages and their minimum versions for the KDB AI samples project. It includes libraries for natural language processing, machine learning, data visualization, and specific KDB AI client functionality.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ngensim >= 4.3\njupyter >= 1.0\nkdbai_client >= 0.1.2\nmatplotlib >= 3.7\nopenai >= 0.28\npypdf >= 3.0\nsentence-transformers >= 2.2\ntensorflow >= 2.10\ntiktoken >= 0.5\numap-learn >= 0.5\nlangchain\nlangchain_openai\n```\n\n----------------------------------------\n\nTITLE: Creating Vector Embedding for Art Conspiracy Theory Query\nDESCRIPTION: Generates a vector embedding for a new query about art and conspiracy theories, demonstrating how the same embedding model can be used for different types of search queries.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/metadata_filtering/metadata_filtering_demo.ipynb#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# Another query\nquery_vector = {'flat_index' : [embedding_model.encode('conspiracy theories involving art').tolist()]}\n```\n\n----------------------------------------\n\nTITLE: Setting Up KDB.AI Cloud Credentials in Python\nDESCRIPTION: Retrieves KDB.AI Cloud endpoint and API key from environment variables or prompts the user for input if not available.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Temporal_Similarity_Search_Transformed_Demo.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n#Set up KDB.AI endpoint and API key\nKDBAI_ENDPOINT = (\n    os.environ[\"KDBAI_ENDPOINT\"]\n    if \"KDBAI_ENDPOINT\" in os.environ\n    else input(\"KDB.AI endpoint: \")\n)\nKDBAI_API_KEY = (\n    os.environ[\"KDBAI_API_KEY\"]\n    if \"KDBAI_API_KEY\" in os.environ\n    else getpass(\"KDB.AI API key: \")\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages for KDB.AI Pattern Matching\nDESCRIPTION: Installs the necessary Python packages for working with KDB.AI and visualization. This includes the KDB.AI client library and matplotlib for plotting sensor data.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install kdbai_client\n!pip install matplotlib\n```\n\n----------------------------------------\n\nTITLE: Displaying the Embeddings DataFrame\nDESCRIPTION: Shows the first few rows of the DataFrame containing the sentence embeddings and original text to verify the data structure.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nshow_df(embeddings_df)\n```\n\n----------------------------------------\n\nTITLE: Showing Embedded Review DataFrame - Python\nDESCRIPTION: This snippet displays the contents of the embedded_review_df DataFrame to the console, allowing for verification of the data structure and its contents.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nshow_df(embedded_review_df)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Sentiment Analysis\nDESCRIPTION: Install required Python libraries for sentiment analysis, text processing, and vector database interactions\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install kdbai_client\n```\n\nLANGUAGE: python\nCODE:\n```\n!pip install sentence_transformers matplotlib\n```\n\n----------------------------------------\n\nTITLE: Setting Up In-Depth Analysis Query\nDESCRIPTION: A blank timed code cell prepared for conducting in-depth analysis of financial regulations. The cell is set up with timing functionality but contains no query implementation, suggesting it's a placeholder for further custom analysis.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_advanced_RAG/KDBAI_Advanced_RAG_Demo.ipynb#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\n\n```\n\n----------------------------------------\n\nTITLE: Setting Up Hybrid Search with Sparse Priority\nDESCRIPTION: Configures a retriever with hybrid search mode giving higher weight (0.9) to sparse vector search.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Hybrid_Search_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\nretriever = index.as_retriever(\n                        vector_store_query_mode=\"hybrid\",\n                        similarity_top_k=5,\n                        vector_store_kwargs={\n                                \"index\" : \"flat\",\n                                \"indexWeight\" : 0.1,\n                                \"indexSparse\" : \"sparse_index\",\n                                \"indexSparseWeight\" : 0.9,\n                                },\n                            )\n```\n\n----------------------------------------\n\nTITLE: Creating Embedding DataFrame for Vector Database\nDESCRIPTION: Constructs a DataFrame containing timestamps and sensor value sequences for each window, which will be used to create vector embeddings.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n# Create a new DataFrame from the collected data\nembedding_df = pd.DataFrame(\n    {\"timestamp\": start_times, \"sensor_00\": sensor0_values}\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for LlamaIndex and KDB.AI\nDESCRIPTION: Installs the required Python packages to work with LlamaIndex, OpenAI, and KDB.AI vector store. This includes the LlamaIndex core library, OpenAI integrations for LLMs and embeddings, file readers, and the KDB.AI client.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_advanced_RAG/KDBAI_Advanced_RAG_Demo.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install llama-index llama-index-llms-openai llama-index-embeddings-openai llama-index-readers-file llama-index-vector-stores-kdbai\n!pip install kdbai_client pandas\n```\n\n----------------------------------------\n\nTITLE: Dropping Existing 'data' Table in KDB.AI using Python\nDESCRIPTION: This code attempts to drop an existing table named 'data' if it exists. It uses a try-except block to handle the case where the table doesn't exist, ensuring a clean slate for the example.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/managing_tables.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# ensure no table called \"data\" exists\ntry:\n    database.table(\"data\").drop()\nexcept kdbai.KDBAIException:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Extracting Time Window Information\nDESCRIPTION: Creates lists containing the start time, end time, and sensor values for each time window, which will be used for embedding generation.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n# Iterate through the windows & extract column values\nstart_times = [w[\"timestamp\"].iloc[0] for w in windows]\nend_times = [w[\"timestamp\"].iloc[-1] for w in windows]\nsensor0_values = [w[\"sensor_00\"].tolist() for w in windows]\n```\n\n----------------------------------------\n\nTITLE: Creating a Matching Reference Answer\nDESCRIPTION: Defines a reference answer that matches the information returned by the QA bot about job creation in the electric vehicle industry. This will be used to test the correctness evaluator with a matching reference.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation_evaluation.ipynb#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nmatching_ref = \"15000 jobs were created due to manufacturing of electric vehicles.\"\n```\n\n----------------------------------------\n\nTITLE: Visualizing Sentiment Distribution in Python\nDESCRIPTION: This function creates a bar plot to visualize the sentiment distribution across different branches. It takes a DataFrame of sentiment counts and a search term as input.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndef plot_sentiment(df: pd.DataFrame, search_term: str) -> None:\n    # Iterate through the branches and create separate DataFrames\n    all_sentiments_df = pd.DataFrame()\n    for _, row in df.iterrows():\n        # Create a DataFrame from the sentiment_counts dictionary\n        branch_sentiment_df = pd.DataFrame([row[\"Sentiments\"]])\n\n        # Add a \"Branch\" column to identify the branch\n        branch_sentiment_df[\"Branch\"] = row[\"Branch\"]\n\n        # Concatenate all DataFrames into one\n        all_sentiments_df = pd.concat(\n            [all_sentiments_df, branch_sentiment_df], ignore_index=True\n        )\n\n    # Create a bar plot of the review sentiments grouped by branch\n    fig = plt.figure(figsize=(10, 4))\n    plot_grouped_bar_chart(\n        all_sentiments_df.set_index(\"Branch\"), y_label=\"Number of Reviews\"\n    )\n    fig.axes[0].set_title(search_term)\n```\n\n----------------------------------------\n\nTITLE: Text Wrapping Utility\nDESCRIPTION: Utility function for formatting and displaying wrapped text output.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/rag_example.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef print_wrapped(text, width=80):\n    wrapper = textwrap.TextWrapper(width=width)\n    word_list = wrapper.wrap(text=text)\n    for line in word_list:\n        print(line)\n```\n\n----------------------------------------\n\nTITLE: Performing Dense Vector Search\nDESCRIPTION: Executes a dense vector search using the dense query vector, returning the 5 most similar document chunks.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n### Type 1 - dense search\ntable.search(vectors={\"dense_index\":dense_query}, n=5)[0][['ID','chunk']]\n```\n\n----------------------------------------\n\nTITLE: Downloading Sample Movie Dataset for Vector Database\nDESCRIPTION: Creates a data directory and downloads a pre-processed movie dataset with embeddings from the GitHub repository. This step is primarily for environments like Colab that need to fetch the data.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/metadata_filtering/metadata_filtering_demo.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n### !!! Only run this cell if you need to download the data into your environment, for example in Colab\n### This downloads movie data\n!mkdir ./data \n!wget -P ./data https://raw.githubusercontent.com/KxSystems/kdbai-samples/main/metadata_filtering/data/filtered_embedded_movies.pkl\n```\n\n----------------------------------------\n\nTITLE: Installing KDB.AI Client Package\nDESCRIPTION: Installs the kdbai_client Python package, which is required to interact with KDB.AI services.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install kdbai_client\n```\n\n----------------------------------------\n\nTITLE: Creating KDB.AI Session - Python\nDESCRIPTION: This snippet establishes a session with KDB.AI using the previously obtained API key and endpoint, allowing subsequent interactions with the database.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nsession = kdbai.Session(api_key=KDBAI_API_KEY, endpoint=KDBAI_ENDPOINT)\n```\n\n----------------------------------------\n\nTITLE: Loading Disneyland Review Dataset\nDESCRIPTION: Read Disneyland review data from CSV file, process and subset the data for sentiment analysis\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nraw_reviews_df = pd.read_csv(\"data/disneyland_reviews.csv\", encoding=\"ISO-8859-1\")\nreviews_df = raw_reviews_df.groupby(\"Branch\").head(50)\n```\n\n----------------------------------------\n\nTITLE: Cleaning Up Resources\nDESCRIPTION: Drops the KDB.AI table to free up resources.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/rag_example.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntable.drop()\n```\n\n----------------------------------------\n\nTITLE: Removing Unnecessary Columns from Sensor Dataset\nDESCRIPTION: Drops columns that are either irrelevant to the analysis or contain bad data, including unnamed index column and problematic sensor columns.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Remove columns that are unnecessary/bad data\nsensors_df = sensors_df.drop([\"Unnamed: 0\", \"sensor_15\", \"sensor_50\"], axis=1)\n```\n\n----------------------------------------\n\nTITLE: Displaying Search Result DataFrame - Python\nDESCRIPTION: This snippet shows the DataFrame containing the search results in the console, allowing for verification of the retrieved data.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nshow_df(query1_results)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for PDF Processing and Text Tokenization\nDESCRIPTION: Imports the necessary libraries for PDF processing (pypdf) and text tokenization (nltk). Downloads the punkt tokenizer models required for sentence tokenization.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# load data\nimport pypdf\nimport nltk\nfrom nltk.tokenize import sent_tokenize\nnltk.download('punkt')\n```\n\n----------------------------------------\n\nTITLE: Enabling Nested Async Events\nDESCRIPTION: Applies nest_asyncio to allow nested loop events in the notebook environment.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Sub_Question_Query_Engine_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: RAG Pipeline Functions: Embedding, Retrieval, and Generation\nDESCRIPTION: This code defines three functions for a RAG pipeline: `embed_query`, `retrieve_data`, and `RAG`. `embed_query` embeds a query using OpenAI's 'text-embedding-3-small' model. `retrieve_data` retrieves relevant data from KDB.AI using the query embedding, filtering out a specific document. `RAG` combines the retrieved data with the original query to generate a response using OpenAI's GPT-4o model.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaParse_pdf_RAG/llamaParse_demo.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n\"from openai import OpenAI\\nclient = OpenAI()\\n\\ndef embed_query(query):\\n    query_embedding = client.embeddings.create(\\n            input=query,\\n            model=\\\"text-embedding-3-small\\\"\\n        )\\n    return query_embedding.data[0].embedding\\n\\ndef retrieve_data(query):\\n    query_embedding = embed_query(query)\\n    results = table.search(vectors={'flat':[query_embedding]},n=5,filter=[('<>','document_id','4a9551df-5dec-4410-90bb-43d17d722918')])\\n    retrieved_data_for_RAG = []\\n    for index, row in results[0].iterrows():\\n      retrieved_data_for_RAG.append(row['text'])\\n    return retrieved_data_for_RAG\\n\\ndef RAG(query):\\n  question = \\\"You will answer this question based on the provided reference material: \\\" + query\\n  messages = \\\"Here is the provided context: \\\" + \\\"\\\\n\\\"\\n  results = retrieve_data(query)\\n  if results:\\n    for data in results:\\n      messages += data + \\\"\\\\n\\\"\\n  response = client.chat.completions.create(\\n      model=\\\"gpt-4o\\\",\\n      messages=[\\n          {\\\"role\\\": \\\"system\\\", \\\"content\\\": question},\\n          {\\n          \\\"role\\\": \\\"user\\\",\\n          \\\"content\\\": [\\n              {\\\"type\\\": \\\"text\\\", \\\"text\\\": messages},\\n          ],\\n          }\\n      ],\\n      max_tokens=300,\\n  )\\n  content = response.choices[0].message.content\\n  return content\"\n```\n\n----------------------------------------\n\nTITLE: Listing DataFrame Columns for Data Understanding\nDESCRIPTION: Prints the column names of the DataFrame to understand what movie metadata is available for filtering.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/metadata_filtering/metadata_filtering_demo.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n#What columns do we have?\nfor column in df.columns:\n    print(column)\n```\n\n----------------------------------------\n\nTITLE: Installing KDB.AI and FastEmbed Dependencies in Python\nDESCRIPTION: To proceed with the KDB.AI setup, it is necessary to install the dependencies using pip. This includes installing KDB.AI client library, FastEmbed for embedding generation, and a specific version of ONNX Runtime. These packages must be installed in your Python environment before proceeding with other steps.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/quickstarts/python_quickstart.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install kdbai_client fastembed onnxruntime==1.19.2\n```\n\n----------------------------------------\n\nTITLE: Dropping KDB.AI Table for Cleanup\nDESCRIPTION: Demonstrates the final cleanup step by dropping the KDB.AI table once processing is complete. This operation ensures resource management and table resource release upon process completion.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation.ipynb#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ntable.drop()\n```\n\n----------------------------------------\n\nTITLE: Printing Time Differences in Python\nDESCRIPTION: This snippet demonstrates how to use the datetime_difference function to calculate and print the time differences between two datetime points, such as 'sim_start' and 'sim_stop'. It outputs the results prefixed with descriptive labels, 'Sim' and 'Multi', using Python's print function.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_non_transformed/Temporal_Similarity_Search_Non-Transformed_Demo.ipynb#2025-04-21_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\nprint('Sim: ' + datetime_difference(sim_start, sim_stop))\nprint('Multi: ' + datetime_difference(multi_start,multi_stop))\n```\n\n----------------------------------------\n\nTITLE: Downloading Spotify Song Dataset\nDESCRIPTION: Downloads the Spotify song dataset CSV file into a local data directory. This step is only necessary in environments like Google Colab.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/music_recommendation/music_recommendation.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!mkdir ./data \n!wget -P ./data https://raw.githubusercontent.com/KxSystems/kdbai-samples/main/music_recommendation/data/song_data.csv\n```\n\n----------------------------------------\n\nTITLE: Concatenating Text from PDF Pages\nDESCRIPTION: Joins the extracted text from all pages into a single string containing the full content of the PDF document.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Concatenate text from each page\nfull_pdf_text = \"\".join(page_list)\n```\n\n----------------------------------------\n\nTITLE: Checking Dataset Row Count for Data Understanding\nDESCRIPTION: Prints the number of rows in the dataset to understand the scale of data we'll be working with.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/metadata_filtering/metadata_filtering_demo.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n#How many rows do we have?\nprint(df.shape[0])\n```\n\n----------------------------------------\n\nTITLE: Running a similarity search with KDB.AI using Python\nDESCRIPTION: Perform a similarity search on the KDB.AI index using a previously generated query embedding. The `hnsw_index` specifies the index to search on, and the method returns the top relevant results, limited to three entries.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/HuggingFace_search/huggingface_inference.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nresults = table.search(vectors={\"hnsw_index\":[query_embedding]},n=3,)\n\nresults[0]\n```\n\n----------------------------------------\n\nTITLE: Exploring Extracted Elements - Python\nDESCRIPTION: This snippet uses the Counter class to enumerate types of extracted elements from the PDF, allowing the user to see how many of each type have been obtained.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/unstructured_io_RAG/Table_RAG_Unstructured_KDBAI_LangChain_RAG.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom collections import Counter\ndisplay(Counter(type(element) for element in elements))\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for KDB.AI and Document Processing - Python\nDESCRIPTION: This snippet installs necessary packages for handling PDF documents and connecting to KDB.AI. It includes installations for poppler-utils, tesseract-ocr, unstructured, and other libraries required for the workflow.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/unstructured_io_RAG/Table_RAG_Unstructured_KDBAI_LangChain_RAG.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!apt-get -qq install poppler-utils tesseract-ocr\n%pip install -q --user --upgrade pillow\n%pip install -q --upgrade unstructured[\"all-docs\"]\n%pip install pymupdf\n%pip install kdbai_client\n%pip install langchain-openai\n%pip install langchain\n#%pip install langchain-community\nimport os\n!git clone -b KDBAI_v1.4 https://github.com/KxSystems/langchain.git\n#!cd langchain/libs/community\nos.chdir('langchain/libs/community')\n!pip install .\n%pip install --upgrade nltk\n```\n\n----------------------------------------\n\nTITLE: Downloading Sample PDF Data\nDESCRIPTION: Conditional download of a research paper PDF if it doesn't exist in the local environment.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/qHnsw_index_pdf_search/pdf_qHNSW_Search.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nif os.path.exists(\"./data/research_paper.pdf\") == False:\n  !mkdir ./data\n  !wget -P ./data https://raw.githubusercontent.com/KxSystems/kdbai-samples/main/document_search/data/research_paper.pdf\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for KDB.AI and LangChain in Python\nDESCRIPTION: This snippet installs the necessary Python packages for using KDB.AI and LangChain, ensuring the environment is prepared for the evaluation tasks defined in the notebook.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation_evaluation.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install kdbai_client langchain langchain_openai #langchain-community\n\nimport os\n!git clone -b KDBAI_v1.4 https://github.com/KxSystems/langchain.git\nos.chdir('langchain/libs/community')\n!pip install .\n```\n\n----------------------------------------\n\nTITLE: Dropping the KDB.AI Table After Completion\nDESCRIPTION: Removes the KDB.AI table after the example is complete, following best practices for resource cleanup in database operations.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/metadata_filtering/metadata_filtering_demo.ipynb#2025-04-21_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ntable.drop()\n```\n\n----------------------------------------\n\nTITLE: Converting Timestamp to Datetime Format\nDESCRIPTION: Transforms the timestamp column from string to datetime format to enable proper time-based operations and visualization.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# convert timestamp to datetime format\nsensors_df[\"timestamp\"] = pd.to_datetime(sensors_df[\"timestamp\"])\n```\n\n----------------------------------------\n\nTITLE: Removing Rows with Missing Values\nDESCRIPTION: Eliminates any rows containing NaN values to ensure data completeness and prevent errors in subsequent analysis.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Removes rows with any NaN values\nsensors_df = sensors_df.dropna()\n```\n\n----------------------------------------\n\nTITLE: Defining a Search Term in Python\nDESCRIPTION: Defines a string variable `search_term2` containing the search query \"how does planet formation occur\". This variable is used as input for subsequent search operations using a KDB.AI query engine.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/qFlat_index_pdf_search/pdf_qFlat_Search.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nsearch_term2 = \"how does planet formation occur\"\n```\n\n----------------------------------------\n\nTITLE: Calling Sentiment Counts by Branch Function in Python\nDESCRIPTION: This code snippet demonstrates how to call the sentiment_counts_by_branch function with query1_results as the input.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nsentiment_counts_by_branch(query1_results)\n```\n\n----------------------------------------\n\nTITLE: Searching for Similar Reviews in KDB.AI - Python\nDESCRIPTION: This function performs a search on the KDB.AI table for reviews similar to a given search term, encoding the search term and retrieving results as a Pandas DataFrame.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ndef search_and_extract_results(\n    table, embedding_model, search_term: str, n_results: int\n) -> pd.DataFrame:\n    # Encode the search term\n    embedded_search_term = [embedding_model.encode(search_term).tolist()]\n\n    # Perform the search\n    search_results = table.search(vectors={\"hnsw_index\":embedded_search_term}, n=n_results)\n\n    # Extract the results to a DataFrame - Return None if no results are found\n    return search_results[0].drop(\"embeddings\", axis=1) if search_results else None\n```\n\n----------------------------------------\n\nTITLE: Printing Types of Extracted Elements - Python\nDESCRIPTION: This snippet iterates through the extracted elements and prints their types, providing insight into the structure and contents of the partitioned PDF data.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/unstructured_io_RAG/Table_RAG_Unstructured_KDBAI_LangChain_RAG.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfor element in elements:\n  print(type(element))\n```\n\n----------------------------------------\n\nTITLE: Dropping Existing Table if Exists in KDB.AI - Python\nDESCRIPTION: This code snippet attempts to drop an existing 'pdf' table in KDB.AI to ensure no duplication occurs when creating a new table. It uses a try-except block to handle potential exceptions when the table does not exist.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n# First ensure the table does not already exist\ntry:\n    db.table(\"pdf\").drop()\nexcept kdbai.KDBAIException:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Installing PDF Processing and Sentence Transformer Libraries\nDESCRIPTION: Installs the pypdf library for processing PDF documents and sentence_transformers for generating vector embeddings.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!pip install pypdf sentence_transformers\n```\n\n----------------------------------------\n\nTITLE: Extracting Single Sensor Time Series Data\nDESCRIPTION: Creates a DataFrame containing only the timestamp and sensor_00 values, with an additional index column, focusing the analysis on a single sensor.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nsensor0_df = sensors_df[[\"timestamp\", \"sensor_00\"]]\nsensor0_df = sensor0_df.reset_index(drop=True).reset_index()\n```\n\n----------------------------------------\n\nTITLE: Setting Up Data Directories and Paths\nDESCRIPTION: Creates the necessary directory structure for storing downloaded data files.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Hybrid_Search_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Root path\nroot_path = os.path.abspath(os.getcwd())\n\n# Data directory and path\ndata_dir = \"data\"\ndata_path = os.path.join(root_path, data_dir)\nif not os.path.exists(data_path):\n    os.mkdir(data_path)\n```\n\n----------------------------------------\n\nTITLE: Viewing Text Data in DataFrame\nDESCRIPTION: Displays the first few rows of the DataFrame containing the processed text files to verify the structure and content before proceeding with image processing.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/multimodal_RAG_unified_text/multi_modal_demo.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Let's take a look at the dataframe so far\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Listing Database Tables\nDESCRIPTION: Retrieves and displays the list of existing tables in the database.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# List all of the tables in the db\ndb.tables\n```\n\n----------------------------------------\n\nTITLE: Parsing PDF with LlamaParse and Markdown Extraction\nDESCRIPTION: Use LlamaParse to convert PDF document into markdown format with specific parsing instructions\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaParse_pdf_RAG/llamaParse_demo.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nparsing_instructions = '''The document titled \"LLM In-Context Recall is Prompt Dependent\" is an academic preprint from April 2024, authored by Daniel Machlab and Rick Battle from the VMware NLP Lab. It explores the in-context recall capabilities of Large Language Models (LLMs) using a method called \"needle-in-a-haystack,\" where a specific factoid is embedded in a block of unrelated text. The study investigates how the recall performance of various LLMs is influenced by the content of prompts and the biases in their training data. The research involves testing multiple LLMs with varying context window sizes to assess their ability to recall information accurately when prompted differently. The paper includes detailed methodologies, results from numerous tests, discussions on the impact of prompt variations and training data, and conclusions on improving LLM utility in practical applications. It contains many tables. Answer questions using the information in this article and be precise.'''\n\ndocuments = LlamaParse(result_type=\"markdown\", parsing_instructions=parsing_instructions).load_data(pdf_file_name)\n```\n\n----------------------------------------\n\nTITLE: Downloading PDF Document - Python\nDESCRIPTION: This snippet uses wget to download a PDF earnings report file from the specified URL and saves it locally for processing.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/unstructured_io_RAG/Table_RAG_Unstructured_KDBAI_LangChain_RAG.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n!wget 'https://s21.q4cdn.com/399680738/files/doc_news/Meta-Reports-Second-Quarter-2024-Results-2024.pdf' -O './doc1.pdf'\n```\n\n----------------------------------------\n\nTITLE: Deleting KDB.AI Database and Table in Python\nDESCRIPTION: This code snippet demonstrates how to drop the KDB.AI database and table after finishing the analysis, as a best practice for cleanup.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_39\n\nLANGUAGE: python\nCODE:\n```\ntable.drop()\ndb.drop()\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables and Suppressing Warnings\nDESCRIPTION: Set up environment variables to control logging and warnings for TensorFlow and other libraries\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages for Multimodal RAG\nDESCRIPTION: Installs the necessary Python packages including kdbai_client for connecting to KDB.AI vector database, langchain for RAG components, and OpenAI for embeddings and image description generation.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/multimodal_RAG_unified_text/multi_modal_demo.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install kdbai_client langchain openai langchain-community\n```\n\n----------------------------------------\n\nTITLE: Checking Single Sensor Dataset Dimensions\nDESCRIPTION: Displays the number of rows and columns in the single sensor DataFrame to understand the dataset size.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nsensor0_df.shape\n```\n\n----------------------------------------\n\nTITLE: Importing Pandas for Results Display\nDESCRIPTION: Imports pandas for creating dataframes to display search results.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Hybrid_Search_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n```\n\n----------------------------------------\n\nTITLE: Helper Function Definition\nDESCRIPTION: Defining utility functions for data frame display and manipulation.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/qHnsw_index_pdf_search/pdf_qHNSW_Search.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef show_df(df: pd.DataFrame) -> pd.DataFrame:\n    print(df.shape)\n    return df.head()\n```\n\n----------------------------------------\n\nTITLE: Resetting Embedding DataFrame Index\nDESCRIPTION: Resets the index of the embedding DataFrame and adds an index column to provide unique identifiers for each time window.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nembedding_df = embedding_df.reset_index(drop=True).reset_index()\n```\n\n----------------------------------------\n\nTITLE: Querying RAG Pipeline for Information Not Found in Source Data\nDESCRIPTION: This code snippet demonstrates how the RAG pipeline responds when queried for information not present in the source material. This example asks about activities in San Francisco, which wasn't covered in the dataset.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaParse_pdf_RAG/llamaParse_demo.ipynb#2025-04-21_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nprint(RAG(\"what is the best thing to do in San Francisco?\"))\n```\n\n----------------------------------------\n\nTITLE: Defining Helper Functions for Data Display\nDESCRIPTION: Creates utility functions to display DataFrame shapes and embedding information.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/music_recommendation/music_recommendation.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef show_df(df: pd.DataFrame) -> pd.DataFrame:\n    print(df.shape)\n    return df.head()\n```\n\nLANGUAGE: python\nCODE:\n```\ndef show_embeddings(embeddings: np.array) -> list[int]:\n    print(\"Num Embeddings:\", len(embeddings))\n    print(\"Embedding Size:\", len(embeddings[0]))\n    return list(embeddings[0])\n```\n\n----------------------------------------\n\nTITLE: Deleting KDB.AI Table in Python\nDESCRIPTION: This code snippet shows how to drop the KDB.AI table after finishing operations. It's considered best practice to clean up resources when they're no longer needed.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/multimodal_RAG_unified_text/multi_modal_demo.ipynb#2025-04-21_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ntable.drop()\n```\n\n----------------------------------------\n\nTITLE: Displaying Single Sensor Data Preview\nDESCRIPTION: Shows the first few rows of the extracted single sensor DataFrame to verify its structure.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# This is our sensor data to be ingested into KDB.AI\nsensor0_df.head()\n```\n\n----------------------------------------\n\nTITLE: Calculating Memory Usage for Similarity Search in Python\nDESCRIPTION: Computes and prints the memory usage for the similarity search operation.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_non_transformed/Temporal_Similarity_Search_Non-Transformed_Demo.ipynb#2025-04-21_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nsim_used=memory_sim_post_search-memory_vecdf_created\nprint(f\"Sim  (MB): {sim_used:.2f}\")\n```\n\n----------------------------------------\n\nTITLE: Importing Basic Python Libraries for Data Handling\nDESCRIPTION: Imports pandas for data manipulation, os for environment variables, and getpass for secure password input, which will be used for API key handling.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/metadata_filtering/metadata_filtering_demo.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport os\nfrom getpass import getpass\n```\n\n----------------------------------------\n\nTITLE: Resetting Index of Cleaned Sensor DataFrame\nDESCRIPTION: Resets the index of the cleaned DataFrame to ensure sequential numbering after rows have been removed during preprocessing.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Reset the index\nsensors_df = sensors_df.reset_index(drop=True)\n```\n\n----------------------------------------\n\nTITLE: Downloading Sample Sensor Data for KDB.AI Pattern Matching\nDESCRIPTION: Creates a data directory and downloads the zipped sensor dataset from GitHub repository. This is only needed when running in a hosted environment like Google Colab.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n### !!! Only run this cell if you need to download the data into your environment, for example in Colab\n### This downloads sensor data\n!mkdir ./data\n!wget -P ./data https://raw.githubusercontent.com/KxSystems/kdbai-samples/main/pattern_matching/data/archive.zip\n```\n\n----------------------------------------\n\nTITLE: Importing Required Python Libraries for Sentiment Analysis\nDESCRIPTION: Import essential libraries for data processing, sentiment analysis, embedding generation, and visualization\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom transformers import pipeline\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModelForSequenceClassification\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nfrom sentence_transformers import SentenceTransformer\nimport kdbai_client as kdbai\nfrom getpass import getpass\nimport time\n```\n\n----------------------------------------\n\nTITLE: Installing KDB.AI Client and Sentence Transformers in Python\nDESCRIPTION: Installs the required Python packages: kdbai_client for interacting with KDB.AI and sentence-transformers for text embedding.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/HuggingFace_search/huggingface_inference.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install kdbai_client\n```\n\nLANGUAGE: python\nCODE:\n```\n!pip install sentence-transformers\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch CPU-only Packages\nDESCRIPTION: Optional command to install only CPU-related PyTorch packages, saving approximately 1.5GB of disk space. This is useful for environments without GPU requirements.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install torch --index-url https://download.pytorch.org/whl/cpu\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages\nDESCRIPTION: Installation of kdbai_client and sentence_transformers packages required for the implementation\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/fuzzy_filtering_on_metadata/fuzzy_filtering_demo.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install kdbai_client\n!pip install sentence_transformers\n```\n\n----------------------------------------\n\nTITLE: Enabling Async Functionality in Jupyter Notebook\nDESCRIPTION: Apply nest_asyncio to enable asynchronous code execution in Jupyter notebooks\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaParse_pdf_RAG/llamaParse_demo.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Dropping Existing Database if Present\nDESCRIPTION: Attempts to drop a database named \"myDatabase\" if it already exists, to avoid conflicts during creation.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# ensure no database called \"myDatabase\" exists\ntry:\n    session.database(\"myDatabase\").drop()\nexcept kdbai.KDBAIException:\n    pass\n```\n\n----------------------------------------\n\nTITLE: Installing NLP and LangChain Dependencies\nDESCRIPTION: Installs sentence-transformers, langchain, and langchain-community packages required for text processing and vector generation.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!pip install sentence-transformers langchain langchain-community\n```\n\n----------------------------------------\n\nTITLE: Setting Up Async Environment with Nest_AsyncIO\nDESCRIPTION: Applies nest_asyncio to enable asynchronous code execution in Jupyter notebook environment.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Hybrid_Search_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\n----------------------------------------\n\nTITLE: Evaluating Answer Correctness with Contradictory Reference\nDESCRIPTION: Evaluates the correctness of the second query result against a contradictory reference. This tests whether the correctness evaluator can identify discrepancies between the QA bot's answer and the reference information.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation_evaluation.ipynb#2025-04-21_snippet_25\n\nLANGUAGE: python\nCODE:\n```\ncorrect_eval_res2 = correct_evaluator.evaluate_strings(\n    prediction=res2, input=query2, reference=contractic_ref\n)\n```\n\n----------------------------------------\n\nTITLE: Dropping KDB.AI Table\nDESCRIPTION: Removes the KDB.AI table after operations are complete, following best practices for resource management\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/qHnsw_index_pdf_search/pdf_qHNSW_Search.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntable.drop()\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries - Python\nDESCRIPTION: This snippet imports necessary libraries for PDF partitioning, embedding, and other utilities. It includes libraries from 'unstructured', 'langchain', and 'nltk' for document processing and OpenAI embeddings.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/unstructured_io_RAG/Table_RAG_Unstructured_KDBAI_LangChain_RAG.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom unstructured.partition.pdf import partition_pdf\nfrom unstructured.partition.auto import partition\nfrom unstructured.embed.openai import OpenAIEmbeddingConfig, OpenAIEmbeddingEncoder\nimport fitz\nfrom langchain_openai import OpenAIEmbeddings\nimport kdbai_client as kdbai\nfrom langchain_community.vectorstores import KDBAI\nfrom langchain.chains import RetrievalQA\nfrom langchain_openai import ChatOpenAI\nimport nltk\nnltk.download('punkt')\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages for KDB.AI and Sentence Transformers\nDESCRIPTION: Installs the necessary Python packages: kdbai_client for connecting to KDB.AI database and sentence_transformers for embedding text data.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/metadata_filtering/metadata_filtering_demo.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install kdbai_client\n!pip install sentence_transformers\n```\n\n----------------------------------------\n\nTITLE: Suppressing TensorFlow Warnings\nDESCRIPTION: Sets an environment variable to suppress TensorFlow warnings, which can be verbose and distracting during execution.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n### ignore tensorflow warnings\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n```\n\n----------------------------------------\n\nTITLE: Suppressing Warnings in Python\nDESCRIPTION: Ignores all warnings to keep the notebook output clean.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_non_transformed/Temporal_Similarity_Search_Non-Transformed_Demo.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Ignore Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n```\n\n----------------------------------------\n\nTITLE: Displaying Conciseness Evaluation Results\nDESCRIPTION: Prints the results of the conciseness evaluation. The print_dict function displays the structured evaluation data in a readable format.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation_evaluation.ipynb#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nprint_dict(concise_eval_res)\n```\n\n----------------------------------------\n\nTITLE: Downloading Sample Data for RAG Example\nDESCRIPTION: Downloads the State of the Union speech data if it doesn't exist in the current environment. This is particularly useful for running the example in environments like Google Colab.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/retrieval_augmented_generation/retrieval_augmented_generation.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n### !!! Only run this cell if you need to download the data into your environment, for example in Colab\n### This downloads State of the Union speech data\nimport os\n\nif os.path.exists(\"./data/state_of_the_union.txt\") == False:\n    !mkdir ./data\n    !wget -P ./data https://raw.githubusercontent.com/KxSystems/kdbai-samples/main/retrieval_augmented_generation/data/state_of_the_union.txt\n```\n\n----------------------------------------\n\nTITLE: Retrieving Available Databases\nDESCRIPTION: Queries the KDB.AI service to list all existing databases that the user has access to.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nsession.databases()\n```\n\n----------------------------------------\n\nTITLE: Configuring Sentiment Analysis Model with RoBERTa\nDESCRIPTION: Set up transformer-based sentiment analysis pipeline using pre-trained RoBERTa model for text classification\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nMODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\nsentiment_model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=3)\nsentiment_pipeline = pipeline(\n    \"sentiment-analysis\",\n    model=sentiment_model,\n    tokenizer=tokenizer\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Query for Guest Satisfaction with Staff in Python\nDESCRIPTION: This code snippet defines a query string to search for reviews about guests' opinions on the staff.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nquery4 = \"what did guests think of the staff?\"\n```\n\n----------------------------------------\n\nTITLE: Importing Basic Dependencies\nDESCRIPTION: Import of pandas and os modules for data handling and system operations\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/fuzzy_filtering_on_metadata/fuzzy_filtering_demo.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport os\nfrom getpass import getpass\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Data Handling\nDESCRIPTION: Imports basic Python libraries for handling files and making URL requests.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Hybrid_Search_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport urllib.request\n```\n\n----------------------------------------\n\nTITLE: Installing KDB.AI Client Library in Python\nDESCRIPTION: Installs the kdbai_client library using pip, which is required for interacting with KDB.AI.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Temporal_Similarity_Search_Transformed_Demo.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install kdbai_client\n```\n\n----------------------------------------\n\nTITLE: Plotting Sensor Time Series with BROKEN Status Highlighted\nDESCRIPTION: Creates a time series plot of sensor_00 values with red X markers highlighting points where the machine status is BROKEN, providing visual context for failure patterns.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# Plot time series for each sensor with BROKEN state marked with X in red color\nplt.figure(figsize=(18, 3))\nplt.plot(\n    broken_sensors_df[\"timestamp\"],\n    broken_sensors_df[\"sensor_00\"],\n    linestyle=\"none\",\n    marker=\"X\",\n    color=\"red\",\n    markersize=12,\n)\nplt.plot(sensors_df[\"timestamp\"], sensors_df[\"sensor_00\"], color=\"blue\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Suppressing User Warnings in Python\nDESCRIPTION: Configures the warnings module to ignore UserWarning messages that might appear during execution but are not critical to the functionality.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\n\nwarnings.simplefilter(\"ignore\", UserWarning)\n```\n\n----------------------------------------\n\nTITLE: Downloading Movie Dataset\nDESCRIPTION: Conditional download of movie data file if not present in the environment\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/fuzzy_filtering_on_metadata/fuzzy_filtering_demo.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nif os.path.exists(\"./data/filtered_embedded_movies.pkl\") == False:\n  !mkdir ./data\n  !wget -P ./data https://raw.githubusercontent.com/KxSystems/kdbai-samples/main/metadata_filtering/data/filtered_embedded_movies.pkl\n```\n\n----------------------------------------\n\nTITLE: Configuring Pandas Display Settings\nDESCRIPTION: Adjusts pandas display settings to show full column content without truncation when viewing search results.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/hybrid_search/hybrid_search_inflation.ipynb#2025-04-21_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n### Adjust display settings so we can see full output\npd.set_option('display.max_colwidth', None)\n```\n\n----------------------------------------\n\nTITLE: Importing OS Module\nDESCRIPTION: Imports the os module to interact with the operating system environment.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/document_search/document_search.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\n```\n\n----------------------------------------\n\nTITLE: Importing Data Processing and Visualization Libraries\nDESCRIPTION: Imports the required Python libraries for reading data, including zipfile for extraction and pandas for data manipulation.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# read data\nfrom zipfile import ZipFile\nimport pandas as pd\n```\n\n----------------------------------------\n\nTITLE: Defining Query for Guest Satisfaction with Rides and Attractions in Python\nDESCRIPTION: This code snippet defines a query string to search for reviews about guests' opinions on rides and attractions.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/sentiment_analysis/sentiment_analysis.ipynb#2025-04-21_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nquery2 = \"what did guests think of the rides and attractions?\"\n```\n\n----------------------------------------\n\nTITLE: Installing Python Requirements\nDESCRIPTION: Command to install all necessary Python packages listed in the requirements.txt file for running the KDB.AI samples.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/README.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Calculating Datetime Differences in Python\nDESCRIPTION: This Python function calculates the difference between two datetime objects. It returns the difference formatted in hours, minutes, seconds, and milliseconds. This function is dependent on Python's datetime module, and it requires two datetime objects as inputs. The expected output is a formatted string indicating the time difference between these objects.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_non_transformed/Temporal_Similarity_Search_Non-Transformed_Demo.ipynb#2025-04-21_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\ndef datetime_difference(datetime1, datetime2):\n    # Calculate the difference between the two datetime objects\n    difference = datetime2 - datetime1\n\n    # Calculate total seconds and milliseconds from the difference\n    total_seconds = difference.total_seconds()\n    hours = int(total_seconds // 3600)\n    minutes = int((total_seconds % 3600) // 60)\n    seconds = int(total_seconds % 60)\n    milliseconds = int((total_seconds - int(total_seconds)) * 1000)\n\n    # Print the difference in hours, minutes, seconds, and milliseconds\n    return f\"Hours: {hours}, Minutes: {minutes}, Seconds: {seconds}, Milliseconds: {milliseconds}\"\n```\n\n----------------------------------------\n\nTITLE: Starting Jupyter Notebook Server\nDESCRIPTION: Command to start a Jupyter notebook session in the background, allowing users to interact with the KDB.AI samples through a web browser.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\njupyter notebook --no-browser\n```\n\n----------------------------------------\n\nTITLE: Configuring Pandas Display Options\nDESCRIPTION: Sets Pandas display options to show full column width and suppress chained assignment warnings.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/music_recommendation/music_recommendation.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npd.set_option(\"max_colwidth\", 1000)\n```\n\nLANGUAGE: python\nCODE:\n```\npd.options.mode.chained_assignment = None\n```\n\n----------------------------------------\n\nTITLE: Importing Numpy and Pandas Libraries\nDESCRIPTION: This snippet imports NumPy for array and matrix operations and Pandas for data manipulation and analysis. These libraries are standard in Python and may be used further in the script for handling data within KDB.AI tables.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/quickstarts/python_quickstart.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\n```\n\n----------------------------------------\n\nTITLE: Importing Visualization Library for Sensor Data Analysis\nDESCRIPTION: Imports matplotlib for plotting sensor time series data to visualize patterns and anomalies.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# plotting\nimport matplotlib.pyplot as plt\n```\n\n----------------------------------------\n\nTITLE: Executing a Complex Query on Financial Regulations with Time Measurement in Python\nDESCRIPTION: Runs a complex query about US financial regulations before and after the 2008 crisis. The query requests an analysis that explains what happened and suggests preventive measures, using both the provided context and the model's knowledge.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_advanced_RAG/KDBAI_Advanced_RAG_Demo.ipynb#2025-04-21_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\nresult = query_engine.query(\n    \"\"\"\n    Analyse the US financial regulations before and after the 2008 crisis and produce a report of all related arguments to explain what happened, and to ensure that does not happen again.\n    Use both the provided context and your own knowledge but do mention explicitely which one you use.\n    \"\"\"\n)\nprint(result.response)\n```\n\n----------------------------------------\n\nTITLE: Querying About September 15th, 2008 Events\nDESCRIPTION: Queries the RAG system about the significant events that occurred on September 15, 2008 (the date of Lehman Brothers' collapse). The query targets post-crisis documents to find information about this pivotal moment in the financial crisis.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_advanced_RAG/KDBAI_Advanced_RAG_Demo.ipynb#2025-04-21_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\nresult = query_engine.query(\n    \"\"\"\n    What happened on the 15th of September 2008 ?\n    \"\"\"\n)\nprint(result.response)\n```\n\n----------------------------------------\n\nTITLE: Dropping Database Table in Python\nDESCRIPTION: This code snippet shows how to drop a database table in Python. This operation is essential for managing database resources efficiently by deleting tables that are no longer in use. There are no specific parameters required, but it assumes that a database table object, 'table', exists and has a drop method.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_non_transformed/Temporal_Similarity_Search_Non-Transformed_Demo.ipynb#2025-04-21_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\ntable.drop()\n```\n\n----------------------------------------\n\nTITLE: Defining Schema and Index for KDB.AI Table in Python\nDESCRIPTION: This code defines the schema and index for a new KDB.AI table. The schema includes 'id' and 'vectors' columns, while the index is set up for vector similarity search with specific parameters like dimensionality and distance metric.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/managing_tables.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nschema = [\n    {\"name\": \"id\", \"type\": \"str\"},\n    {\"name\": \"vectors\", \"type\": \"float32s\"},\n]\n\nindex_name = \"flat_index\"\nindexes = [{\"name\": index_name, \"column\": \"vectors\", \"type\": \"flat\", \"params\": {\"dims\": 8, \"metric\": \"L2\"}}]\n```\n\n----------------------------------------\n\nTITLE: Creating a New Table in KDB.AI using Python\nDESCRIPTION: This snippet creates a new table named 'data' in the KDB.AI database using the previously defined schema and index. This table will be used for storing and querying vector data.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/KDB.AI_course/course_specific_content/managing_tables.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntable = database.create_table(\"data\", schema=schema, indexes=indexes)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages\nDESCRIPTION: Installation of necessary Python packages including llama-index and its extensions, plus kdbai_client and sentence-transformers.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/LlamaIndex_samples/Sub_Question_Query_Engine_LlamaIndex_KDBAI.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%pip install llama-index llama-index-embeddings-huggingface llama-index-llms-openai llama-index-readers-file llama-index-vector-stores-kdbai\n%pip install kdbai_client sentence-transformers\n```\n\n----------------------------------------\n\nTITLE: Performing Similarity Search with KDB.AI in Python\nDESCRIPTION: Executes a similarity search using a query vector and returns the top 10 most similar matches.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_non_transformed/Temporal_Similarity_Search_Non-Transformed_Demo.ipynb#2025-04-21_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nsim_start=datetime.datetime.now()\nres = table.search(vectors={'price': [q]}, n=10, type=\"tss\")[0]\n\nsim_stop=datetime.datetime.now()\nmemory_sim_post_search=get_memory_usage()\nres\n```\n\n----------------------------------------\n\nTITLE: Removing Duplicate Rows from Sensor Dataset\nDESCRIPTION: Cleans the sensor dataset by removing any duplicate entries that might skew the analysis or take up unnecessary space.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Transformed_TSS_pattern_matching.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Drop duplicates\nsensors_df = raw_sensors_df.drop_duplicates()\n```\n\n----------------------------------------\n\nTITLE: Suppressing Warnings in Python\nDESCRIPTION: Ignores all warnings to keep the notebook output clean and focused on the main results.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/TSS_transformed/Temporal_Similarity_Search_Transformed_Demo.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Ignore Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n```\n\n----------------------------------------\n\nTITLE: Importing Required Python Packages\nDESCRIPTION: Imports necessary Python libraries for data manipulation, embedding creation, and KDB.AI interaction.\nSOURCE: https://github.com/kxsystems/kdbai-samples/blob/main/music_recommendation/music_recommendation.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\n```\n\nLANGUAGE: python\nCODE:\n```\nimport nltk\nnltk.download('punkt_tab')\nfrom nltk.tokenize import word_tokenize\nfrom gensim.models import Word2Vec\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom tqdm.auto import tqdm\n```\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport kdbai_client as kdbai\nfrom getpass import getpass\nimport time\n```"
  }
]
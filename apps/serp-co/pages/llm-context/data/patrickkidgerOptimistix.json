[
  {
    "owner": "patrick-kidger",
    "repo": "optimistix",
    "content": "TITLE: Defining Custom Hybrid Solver in Python using Optimistix\nDESCRIPTION: This code snippet demonstrates how to create a custom hybrid solver by subclassing AbstractBFGS and combining it with DoglegDescent and a fixed LearningRate. It showcases the flexibility of Optimistix in creating custom optimization algorithms.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/searches/introduction.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom collections.abc import Callable\nimport optimistix as optx\n\nclass HybridSolver(optx.AbstractBFGS):\n    rtol: float\n    atol: float\n    norm: Callable\n    use_inverse: bool = True\n    descent: optx.AbstractDescent = optx.DoglegDescent()\n    search: optx.AbstractSearch = optx.LearningRate(0.1)\n```\n\n----------------------------------------\n\nTITLE: Performing Root Finding with Optimistix in JAX\nDESCRIPTION: Demonstrates how to set up and solve a root finding problem using Optimistix with JAX. The example defines a multivariate function, configures a Newton solver with specific tolerances, and finds the roots of the function starting from an initial guess.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/examples/root_find.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\nimport optimistix as optx\n\n\n# Often import when doing scientific work\njax.config.update(\"jax_enable_x64\", True)\n\n\ndef fn(y, args):\n    a, b = y\n    c = jnp.tanh(jnp.sum(b)) - a\n    d = a**2 - jnp.sinh(b + 1)\n    return c, d\n\n\nsolver = optx.Newton(rtol=1e-8, atol=1e-8)\ny0 = (jnp.array(0.0), jnp.zeros((2, 2)))\nsol = optx.root_find(fn, solver, y0)\n```\n\n----------------------------------------\n\nTITLE: Solving ODE with Implicit Euler Method using Optimistix\nDESCRIPTION: Example demonstrating how to solve the ODE dy/dt=tanh(y(t)) using implicit Euler method and Newton solver. Shows fixed point iteration to find solution satisfying y1 = y0 + tanh(y1)dt.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport jax.numpy as jnp\nimport optimistix as optx\n\n# Let's solve the ODE dy/dt=tanh(y(t)) with the implicit Euler method.\n# We need to find y1 s.t. y1 = y0 + tanh(y1)dt.\n\ny0 = jnp.array(1.)\ndt = jnp.array(0.1)\n\ndef fn(y, args):\n    return y0 + jnp.tanh(y) * dt\n\nsolver = optx.Newton(rtol=1e-5, atol=1e-5)\nsol = optx.fixed_point(fn, solver, y0)\ny1 = sol.value  # satisfies y1 == fn(y1)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Optax Minimiser for Complex Root-Finding Problems in Python\nDESCRIPTION: This example illustrates the configuration of an Optax minimiser for difficult root-finding problems. It employs the AdaBelief optimizer and can be used with root-finding functionalities in Optimistix.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/how-to-choose.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\noptimistix.OptaxMinimiser(optax.adabelief(...), ...)\n```\n\n----------------------------------------\n\nTITLE: Configuring Optax Minimiser for Messy Minimization Problems in Python\nDESCRIPTION: This snippet demonstrates how to set up an Optax minimiser using the AdaBelief optimizer for handling complex minimization problems. It specifies learning rate and tolerance parameters for precise control.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/how-to-choose.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\noptimistix.OptaxMinimiser(optax.adabelief(learning_rate=1e-3), rtol=1e-8, atol=1e-8)\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom BFGS Minimiser with Trust Region Search in Python\nDESCRIPTION: Demonstrates how to create a novel minimiser by extending AbstractBFGS with a dogleg descent path and classical trust region search algorithm. This custom solver uses a BFGS quasi-Newton approximation to the Hessian for minimization problems.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/examples/custom_solver.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom collections.abc import Callable\n\nimport optimistix as optx\n\n\nclass MyNewMinimiser(optx.AbstractBFGS):\n    rtol: float\n    atol: float\n    norm: Callable = optx.max_norm\n    use_inverse: bool = False\n    descent: optx.AbstractDescent = optx.DoglegDescent()\n    search: optx.AbstractSearch = optx.ClassicalTrustRegion\n\n\nsolver = MyNewMinimiser(rtol=1e-4, atol=1e-4)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Newton Descent Direction in Python\nDESCRIPTION: Shows how to create a new descent direction by subclassing AbstractDescent. This implementation handles Newton's method descent direction calculation, supporting different types of function information including gradient-Hessian pairs and Jacobian-residual pairs, using lineax for linear system solving.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/examples/custom_solver.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport equinox as eqx  # https://github.com/patrick-kidger/equinox\nimport jax.tree_util as jtu\nimport lineax as lx  # https://github.com/google/lineax\nfrom jaxtyping import Array, PyTree  # https://github.com/google/jaxtyping\n\n\nclass NewtonDescentState(eqx.Module):\n    newton: PyTree[Array]\n    result: optx.RESULTS\n\n\nclass NewtonDescent(optx.AbstractDescent):\n    def init(self, y, f_info_struct):\n        del f_info_struct\n        # Dummy values of the right shape; unused.\n        return NewtonDescentState(y, optx.RESULTS.successful)\n\n    def query(self, y, f_info, state):\n        del state\n        if isinstance(f_info, optx.FunctionInfo.EvalGradHessianInv):\n            newton = f_info.hessian_inv.mv(f_info.grad)\n            result = optx.RESULTS.successful\n        else:\n            if isinstance(f_info, optx.FunctionInfo.EvalGradHessian):\n                operator = f_info.hessian\n                vector = f_info.grad\n            elif isinstance(f_info, optx.FunctionInfo.ResidualJac):\n                operator = f_info.jac\n                vector = f_info.residual\n            else:\n                raise ValueError(\n                    \"Cannot use a Newton descent with a solver that only evaluates the \"\n                    \"gradient, or only the function itself.\"\n                )\n            out = lx.linear_solve(operator, vector)\n            newton = out.value\n            result = optx.RESULTS.promote(out.result)\n        return NewtonDescentState(newton, result)\n\n    def step(self, step_size, state):\n        return jtu.tree_map(lambda x: -step_size * x, state.newton), state.result\n```\n\n----------------------------------------\n\nTITLE: Newton's Method Root Finder Implementation - Python\nDESCRIPTION: Implementation of Newton's method for finding roots of functions through iterative approximation using function values and derivatives.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/root_find.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\noptimistix.Newton\n```\n\n----------------------------------------\n\nTITLE: Implementing Lotka-Volterra Vector Field and ODE Solver\nDESCRIPTION: Defines the vector field for Lotka-Volterra equations and implements an ODE solver function using Diffrax. The solver supports forward-mode autodiff for Levenberg-Marquardt optimization.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/examples/optimise_diffeq.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef vector_field(\n    t, y: Float[Array, \"2\"], parameters: Float[Array, \"4\"]\n) -> Float[Array, \"2\"]:\n    prey, predator = y\n    α, β, γ, δ = parameters\n    d_prey = α * prey - β * prey * predator\n    d_predator = -γ * predator + δ * prey * predator\n    d_y = jnp.stack([d_prey, d_predator])\n    return d_y\n\n\ndef solve(\n    parameters: Float[Array, \"4\"], y0: Float[Array, \"2\"], saveat: dfx.SaveAt\n) -> Float[Array, \"ts\"]:\n    \"\"\"Solve a single ODE.\"\"\"\n    term = dfx.ODETerm(vector_field)\n    solver = dfx.Tsit5()\n    t0 = saveat.subs.ts[0]\n    t1 = saveat.subs.ts[-1]\n    dt0 = 0.1\n    sol = dfx.diffeqsolve(\n        term,\n        solver,\n        t0,\n        t1,\n        dt0,\n        y0,\n        args=parameters,\n        saveat=saveat,\n        # support forward-mode autodiff, which is used by Levenberg--Marquardt\n        adjoint=dfx.DirectAdjoint(),\n    )\n    return sol.ys\n```\n\n----------------------------------------\n\nTITLE: Levenberg-Marquardt Solver Implementation\nDESCRIPTION: Implementation of the Levenberg-Marquardt algorithm for nonlinear least squares optimization.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/least_squares.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\noptimistix.LevenbergMarquardt\n```\n\n----------------------------------------\n\nTITLE: Solving an ODE with Implicit Euler Method using Optimistix\nDESCRIPTION: Example of solving the ODE dy/dt=tanh(y(t)) with the implicit Euler method using Optimistix's fixed point solver. The Newton solver is used to find y1 that satisfies y1 = y0 + tanh(y1)*dt.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/index.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport jax.numpy as jnp\nimport optimistix as optx\n\n# Let's solve the ODE dy/dt=tanh(y(t)) with the implicit Euler method.\n# We need to find y1 s.t. y1 = y0 + tanh(y1)dt.\n\ny0 = jnp.array(1.)\ndt = jnp.array(0.1)\n\ndef fn(y, args):\n    return y0 + jnp.tanh(y) * dt\n\nsolver = optx.Newton(rtol=1e-5, atol=1e-5)\nsol = optx.fixed_point(fn, solver, y0)\ny1 = sol.value  # satisfies y1 == fn(y1)\n```\n\n----------------------------------------\n\nTITLE: Gauss-Newton Solver Implementation\nDESCRIPTION: Concrete implementation of the Gauss-Newton method for solving nonlinear least squares problems.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/least_squares.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\noptimistix.GaussNewton\n```\n\n----------------------------------------\n\nTITLE: Implementing Interactive Bisection Search in Optimistix\nDESCRIPTION: This code demonstrates how to interactively step through a root-finding process using Optimistix's Bisection solver. It solves y - tanh(y + 1) = 0 by manually controlling each step of the optimization process, printing the current interval at each iteration.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/examples/interactive.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport equinox as eqx  # https://github.com/patrick-kidger/equinox\nimport jax\nimport jax.numpy as jnp\nimport optimistix as optx\n\n\n# Seek `y` such that `y - tanh(y + 1) = 0`.\n@eqx.filter_jit\ndef fn(y, args):\n    out = y - jnp.tanh(y + 1)\n    aux = None\n    return out, aux\n\n\nsolver = optx.Bisection(rtol=1e-3, atol=1e-3)\n# The initial guess for the solution\ny = jnp.array(0)\n# Any auxiliary information to pass to `fn`.\nargs = None\n# The interval to search over. Required for `optx.Bisection`.\noptions = dict(lower=-1, upper=1)\n# The shape+dtype of the output of `fn`\nf_struct = jax.ShapeDtypeStruct((), jnp.float32)\naux_struct = None\n# Any Lineax tags describing the structure of the Jacobian matrix d(fn)/dy.\n# (In this case it's just a 1x1 matrix, so these don't matter.)\ntags = frozenset()\n\n\ndef solve(y, solver):\n    # These arguments are always fixed throughout interactive solves.\n    step = eqx.filter_jit(\n        eqx.Partial(solver.step, fn=fn, args=args, options=options, tags=tags)\n    )\n    terminate = eqx.filter_jit(\n        eqx.Partial(solver.terminate, fn=fn, args=args, options=options, tags=tags)\n    )\n\n    # Initial state before we start solving.\n    state = solver.init(fn, y, args, options, f_struct, aux_struct, tags)\n    done, result = terminate(y=y, state=state)\n\n    # Alright, enough setup. Let's do the solve!\n    while not done:\n        print(f\"Evaluating point {y} with value {fn(y, args)[0]}.\")\n        y, state, aux = step(y=y, state=state)\n        done, result = terminate(y=y, state=state)\n    if result != optx.RESULTS.successful:\n        print(f\"Oh no! Got error {result}.\")\n    y, _, _ = solver.postprocess(fn, y, aux, args, options, state, tags, result)\n    print(f\"Found solution {y} with value {fn(y, args)[0]}.\")\n\n\nsolve(y, solver)\n```\n\n----------------------------------------\n\nTITLE: Implementing Abstract Root Finder Base Class - Python\nDESCRIPTION: Defines the abstract base class for root finding algorithms with required methods for initialization, stepping, termination, and post-processing of results.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/root_find.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\noptimistix.AbstractRootFinder\n```\n\n----------------------------------------\n\nTITLE: Defining Abstract Fixed Point Solver in Python\nDESCRIPTION: This snippet defines the AbstractFixedPointSolver class in the Optimistix library. It includes methods for initialization, stepping, termination, and postprocessing of the fixed point solving process.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/fixed_point.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass AbstractFixedPointSolver:\n    def init(self):\n        ...\n    \n    def step(self):\n        ...\n    \n    def terminate(self):\n        ...\n    \n    def postprocess(self):\n        ...\n```\n\n----------------------------------------\n\nTITLE: Abstract Least Squares Solver Implementation\nDESCRIPTION: Base abstract class defining the interface for least squares solvers with core methods for initialization, stepping, termination, and postprocessing.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/least_squares.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\noptimistix.AbstractLeastSquaresSolver\n```\n\n----------------------------------------\n\nTITLE: Implementing Implicit Adjoint Method\nDESCRIPTION: Implementation of the implicit function theorem approach for autodifferentiation, which is the recommended default method for most use cases.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/adjoints.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass ImplicitAdjoint:\n    def __init__()\n```\n\n----------------------------------------\n\nTITLE: Bisection Method Root Finder Implementation - Python\nDESCRIPTION: Implementation of the Bisection method for finding roots by repeatedly dividing intervals and selecting subintervals where root exists.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/root_find.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\noptimistix.Bisection\n```\n\n----------------------------------------\n\nTITLE: Chord Method Root Finder Implementation - Python\nDESCRIPTION: Implementation of the Chord method, a variant of the secant method for finding roots of functions using finite difference approximations.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/root_find.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\noptimistix.Chord\n```\n\n----------------------------------------\n\nTITLE: Documenting DampedNewtonDescent Implementation in Optimistix\nDESCRIPTION: Documentation for the DampedNewtonDescent class which implements a damped version of Newton's method for better convergence. Shows the constructor documentation.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/searches/descents.md#2025-04-22_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n::: optimistix.DampedNewtonDescent\n    options:\n        members:\n            - __init__\n```\n\n----------------------------------------\n\nTITLE: Documenting AbstractDescent Interface in Optimistix\nDESCRIPTION: Documentation block for the AbstractDescent class which serves as the base interface for all descent methods in the Optimistix library. Shows the class with its __call__ method.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/searches/descents.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n??? abstract \"`optimistix.AbstractDescent`\"\n\n    ::: optimistix.AbstractDescent\n        options:\n            members:\n                - __call__\n```\n\n----------------------------------------\n\nTITLE: Documenting NewtonDescent Implementation in Optimistix\nDESCRIPTION: Documentation for the NewtonDescent class which implements Newton's method for optimization. Displays the __init__ method documentation.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/searches/descents.md#2025-04-22_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n::: optimistix.NewtonDescent\n    options:\n        members:\n            - __init__\n```\n\n----------------------------------------\n\nTITLE: Documenting NonlinearCGDescent Implementation in Optimistix\nDESCRIPTION: Documentation for the NonlinearCGDescent class which implements nonlinear conjugate gradient descent for optimization. Shows the constructor documentation.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/searches/descents.md#2025-04-22_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n::: optimistix.NonlinearCGDescent\n    options:\n        members:\n            - __init__\n```\n\n----------------------------------------\n\nTITLE: Documenting SteepestDescent Implementation in Optimistix\nDESCRIPTION: Documentation for the SteepestDescent class in Optimistix which implements gradient descent optimization. Displays the __init__ method documentation.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/searches/descents.md#2025-04-22_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n::: optimistix.SteepestDescent\n    options:\n        members:\n            - __init__\n```\n\n----------------------------------------\n\nTITLE: Verifying Solution and Plotting Results\nDESCRIPTION: Checks the optimization results by computing residuals and visualizes the solution with matplotlib.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/examples/optimise_diffeq.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\noptx.max_norm(residuals(sol.value, (y0s, values)))\n\nts = jnp.linspace(0, 140, 1000)\nys = solve(sol.value, jnp.array([10.0, 10.0]), dfx.SaveAt(ts=ts))\nplt.plot(ts, ys[:, 0], label=\"Prey\")\nplt.plot(ts, ys[:, 1], label=\"Predator\")\nplt.legend()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Configuring Optax Minimiser for Complex Least-Squares Problems in Python\nDESCRIPTION: This code snippet shows how to configure an Optax minimiser for challenging least-squares problems. It uses the AdaBelief optimizer and is compatible with the optimistix.least_squares function.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/how-to-choose.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\noptimistix.OptaxMinimiser(optax.adabelief(...), ...)\n```\n\n----------------------------------------\n\nTITLE: Generating Training Data\nDESCRIPTION: Simulates training data using the Lotka-Volterra equations with three different initial conditions and known parameters.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/examples/optimise_diffeq.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef get_data() -> tuple[Float[Array, \"3 2\"], Float[Array, \"3 50\"]]:\n    \"\"\"Simulate some training data.\"\"\"\n    # We consider three possible initial conditions.\n    y0_a = jnp.array([9.0, 9.0])\n    y0_b = jnp.array([10.0, 10.0])\n    y0_c = jnp.array([11.0, 11.0])\n    y0s = jnp.stack([y0_a, y0_b, y0_c])\n    true_parameters = jnp.array([0.1, 0.02, 0.4, 0.02])\n    saveat = dfx.SaveAt(ts=jnp.linspace(0, 30, 20))\n    batch_solve = eqx.filter_jit(eqx.filter_vmap(solve, in_axes=(None, 0, None)))\n    values = batch_solve(true_parameters, y0s, saveat)\n    return y0s, values\n```\n\n----------------------------------------\n\nTITLE: Best-So-Far Root Finder Implementation - Python\nDESCRIPTION: Implementation of a root finding algorithm that keeps track of and uses the best solution found so far during the iteration process.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/root_find.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\noptimistix.BestSoFarRootFinder\n```\n\n----------------------------------------\n\nTITLE: Defining Abstract Adjoint Base Class\nDESCRIPTION: Abstract base class defining the interface for adjoint methods in Optimistix, with the core apply method for implementing different autodifferentiation strategies.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/adjoints.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass AbstractAdjoint:\n    def apply()\n```\n\n----------------------------------------\n\nTITLE: Defining FunctionInfo Class in Python\nDESCRIPTION: The FunctionInfo class in Optimistix with the 'as_min' method.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/searches/function_info.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n::: optimistix.FunctionInfo\n    options:\n        members:\n            - as_min\n```\n\n----------------------------------------\n\nTITLE: Initializing FunctionInfo.EvalGradHessianInv Class in Python\nDESCRIPTION: The EvalGradHessianInv subclass of FunctionInfo with its __init__ method.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/searches/function_info.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n::: optimistix.FunctionInfo.EvalGradHessianInv\n    options:\n        members:\n            - __init__\n```\n\n----------------------------------------\n\nTITLE: Importing Fixed Point Solver in Python\nDESCRIPTION: This snippet shows how to import the fixed_point solver from the Optimistix library. The fixed_point function supports various fixed-point solvers and can also use root finders, least squares solvers, and minimisers as solvers with automatic problem rewriting.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/fixed_point.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom optimistix import fixed_point\n```\n\n----------------------------------------\n\nTITLE: Initializing FunctionInfo.EvalGradHessian Class in Python\nDESCRIPTION: The EvalGradHessian subclass of FunctionInfo with its __init__ method.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/searches/function_info.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n::: optimistix.FunctionInfo.EvalGradHessian\n    options:\n        members:\n            - __init__\n```\n\n----------------------------------------\n\nTITLE: Using PyTree Norms for Convergence Checks\nDESCRIPTION: Example showing how norms are used to check convergence between iterations in optimization algorithms, where y represents successive iterates and ε is the tolerance threshold.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/norms.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n||y_{n+1} - y_n|| < ε\n```\n\n----------------------------------------\n\nTITLE: Verifying Root Finding Results in Optimistix\nDESCRIPTION: Demonstrates how to verify that the found solution is indeed a root of the target function by evaluating the function at the solution point. The output should be approximately zero.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/examples/root_find.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nprint(fn(sol.value, args=None))\n```\n\n----------------------------------------\n\nTITLE: Displaying Root Finding Solution in Optimistix\nDESCRIPTION: Shows how to access and print the solution value found by the root finding process. The solution represents the point where the target function evaluates to zero.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/examples/root_find.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprint(sol.value)\n```\n\n----------------------------------------\n\nTITLE: Importing Solution Class from Optimistix\nDESCRIPTION: References the Solution class from the Optimistix library, which is used to store and manage solution results from optimization or root-finding operations.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/solution.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\noptimistix.Solution\n```\n\n----------------------------------------\n\nTITLE: Documenting DoglegDescent Implementation in Optimistix\nDESCRIPTION: Documentation for the DoglegDescent class which implements the dogleg trust region method for optimization. Shows the constructor documentation.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/searches/descents.md#2025-04-22_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n::: optimistix.DoglegDescent\n    options:\n        members:\n            - __init__\n```\n\n----------------------------------------\n\nTITLE: Documenting IndirectDampedNewtonDescent Implementation in Optimistix\nDESCRIPTION: Documentation for the IndirectDampedNewtonDescent class which provides an alternative implementation of damped Newton's method. Displays the __init__ method.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/searches/descents.md#2025-04-22_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n::: optimistix.IndirectDampedNewtonDescent\n    options:\n        members:\n            - __init__\n```\n\n----------------------------------------\n\nTITLE: Importing Optimistix Compatibility Functions\nDESCRIPTION: Shows the module path for accessing the minimize function that serves as a drop-in replacement for jax.scipy.optimize.minimize\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/compat.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\noptimistix.compat.minimize\n```\n\n----------------------------------------\n\nTITLE: Dogleg Solver Implementation\nDESCRIPTION: Implementation of the Dogleg method for solving nonlinear least squares problems.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/least_squares.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\noptimistix.Dogleg\n```\n\n----------------------------------------\n\nTITLE: Indirect Levenberg-Marquardt Solver Implementation\nDESCRIPTION: Alternative implementation of the Levenberg-Marquardt algorithm using indirect approach.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/least_squares.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\noptimistix.IndirectLevenbergMarquardt\n```\n\n----------------------------------------\n\nTITLE: Best-So-Far Least Squares Solver Implementation\nDESCRIPTION: Implementation of a solver that keeps track of the best solution found so far during least squares optimization.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/least_squares.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\noptimistix.BestSoFarLeastSquares\n```\n\n----------------------------------------\n\nTITLE: Initializing Fixed Point Iteration Solver in Python\nDESCRIPTION: This snippet shows the initialization of the FixedPointIteration solver in the Optimistix library. The FixedPointIteration class is a concrete implementation of a fixed point solver.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/fixed_point.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass FixedPointIteration:\n    def __init__(self):\n        ...\n```\n\n----------------------------------------\n\nTITLE: Initializing Best-So-Far Fixed Point Solver in Python\nDESCRIPTION: This snippet demonstrates the initialization of the BestSoFarFixedPoint solver in the Optimistix library. This solver keeps track of the best solution found so far during the fixed point iteration process.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/fixed_point.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass BestSoFarFixedPoint:\n    def __init__(self):\n        ...\n```\n\n----------------------------------------\n\nTITLE: Implementing Recursive Checkpoint Adjoint\nDESCRIPTION: Implementation of a recursive checkpointing approach for autodifferentiation, providing an alternative to the implicit adjoint method.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/adjoints.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass RecursiveCheckpointAdjoint:\n    def __init__()\n```\n\n----------------------------------------\n\nTITLE: Handling Root Finding Errors in Optimistix\nDESCRIPTION: Shows what happens when attempting to find the root of a function that doesn't have one. This example demonstrates how Optimistix handles optimization failures for misspecified problems.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/examples/root_find.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef does_not_have_root(y, _):\n    # there is no value of y for which this equals zero.\n    return 1 + y**2\n\n\ny0 = jnp.array(1.0)\noptx.root_find(does_not_have_root, solver, y0)\n```\n\n----------------------------------------\n\nTITLE: Using BestSoFarRootFinder in Optimistix for Solution Tracking\nDESCRIPTION: This code snippet demonstrates how to use the BestSoFarRootFinder wrapper to track the best solution found during the optimization process. It ensures that the final result is the best point found so far, rather than just a point satisfying the tolerance conditions.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/examples/interactive.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nbest_so_far_solver = optx.BestSoFarRootFinder(solver)\nsolve(y, best_so_far_solver)\n```\n\n----------------------------------------\n\nTITLE: Initializing FunctionInfo.Eval Class in Python\nDESCRIPTION: The Eval subclass of FunctionInfo with its __init__ method.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/searches/function_info.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n::: optimistix.FunctionInfo.Eval\n    options:\n        members:\n            - __init__\n```\n\n----------------------------------------\n\nTITLE: Initializing FunctionInfo.EvalGrad Class in Python\nDESCRIPTION: The EvalGrad subclass of FunctionInfo with its __init__ method.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/searches/function_info.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n::: optimistix.FunctionInfo.EvalGrad\n    options:\n        members:\n            - __init__\n```\n\n----------------------------------------\n\nTITLE: Installing Optimistix via pip\nDESCRIPTION: Command to install Optimistix using pip package manager. Requires Python 3.10+, JAX 0.4.38+, and Equinox 0.11.11+.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install optimistix\n```\n\n----------------------------------------\n\nTITLE: Initializing FunctionInfo.Residual Class in Python\nDESCRIPTION: The Residual subclass of FunctionInfo with its __init__ method.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/searches/function_info.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n::: optimistix.FunctionInfo.Residual\n    options:\n        members:\n            - __init__\n```\n\n----------------------------------------\n\nTITLE: Initializing FunctionInfo.ResidualJac Class in Python\nDESCRIPTION: The ResidualJac subclass of FunctionInfo with its __init__ method.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/searches/function_info.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n::: optimistix.FunctionInfo.ResidualJac\n    options:\n        members:\n            - __init__\n```\n\n----------------------------------------\n\nTITLE: Implementing Residuals and Running Optimization\nDESCRIPTION: Defines the residuals function for least squares optimization and runs the Levenberg-Marquardt solver with verbose output.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/examples/optimise_diffeq.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef residuals(parameters, y0s__values):\n    y0s, values = y0s__values\n    saveat = dfx.SaveAt(ts=jnp.linspace(0, 30, 20))\n    batch_solve = eqx.filter_vmap(solve, in_axes=(None, 0, None))\n    pred_values = batch_solve(parameters, y0s, saveat)\n    return values - pred_values\n\n\n(y0s, values) = get_data()\nsolver = optx.LevenbergMarquardt(\n    rtol=1e-8, atol=1e-8, verbose=frozenset({\"step\", \"accepted\", \"loss\", \"step_size\"})\n)\n\ninit_parameters = jnp.zeros(4)\nsol = optx.least_squares(residuals, solver, init_parameters, args=(y0s, values))\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Imports necessary Python libraries including Diffrax for differential equations, Equinox for JAX operations, and jaxtyping for type annotations.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/examples/optimise_diffeq.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport diffrax as dfx  # https://github.com/patrick-kidger/diffrax\nimport equinox as eqx  # https://github.com/patrick-kidger/equinox\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport optimistix as optx\nfrom jaxtyping import Array, Float  # https://github.com/google/jaxtyping\n```\n\n----------------------------------------\n\nTITLE: Importing RESULTS Constant from Optimistix\nDESCRIPTION: References the RESULTS constant from the Optimistix library, which likely contains enumerated values or flags for different solver outcomes and statuses.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/solution.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\noptimistix.RESULTS\n```\n\n----------------------------------------\n\nTITLE: Accessing OptimizeResults Class\nDESCRIPTION: Shows the module path for accessing the OptimizeResults class that provides optimization result information\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/compat.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\noptimistix.compat.OptimizeResults\n```\n\n----------------------------------------\n\nTITLE: Installing Optimistix via pip\nDESCRIPTION: Command to install the Optimistix library using pip. Requires Python 3.9+, JAX 0.4.14+, and Equinox 0.11.0+.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/index.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install optimistix\n```\n\n----------------------------------------\n\nTITLE: Cloning and Installing Optimistix in Development Mode\nDESCRIPTION: These commands clone the Optimistix repository from the contributor's fork and install it in development mode. This setup allows for local development and testing of changes.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/CONTRIBUTING.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/your-username-here/optimistix.git\ncd optimistix\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Installing and Setting Up Pre-commit Hook\nDESCRIPTION: These commands install the pre-commit tool and set up a pre-commit hook. The hook uses Black and isort for code formatting, and flake8 for linting, ensuring code quality before commits.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/CONTRIBUTING.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install pre-commit\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: Running Tests for Optimistix\nDESCRIPTION: This command installs pytest and runs all tests for the Optimistix project. It's crucial to ensure all tests pass before pushing changes or creating a pull request.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/CONTRIBUTING.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install pytest\npytest\n```\n\n----------------------------------------\n\nTITLE: Pushing Changes to GitHub Fork\nDESCRIPTION: This command pushes the local changes to the contributor's fork on GitHub, preparing for the creation of a pull request.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/CONTRIBUTING.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit push\n```\n\n----------------------------------------\n\nTITLE: Building and Serving Documentation Locally\nDESCRIPTION: These commands install documentation dependencies, build the documentation, and serve it locally. Running 'mkdocs serve' twice is necessary for proper rendering.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/CONTRIBUTING.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install -r docs/requirements.txt\nmkdocs serve\n```\n\n----------------------------------------\n\nTITLE: Serving Documentation Locally (Second Run)\nDESCRIPTION: This command serves the documentation locally after the initial build. It allows viewing the documentation at localhost:8000 in a web browser.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/CONTRIBUTING.md#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nmkdocs serve\n```\n\n----------------------------------------\n\nTITLE: Installing Documentation Dependencies with Specific Versions in pip\nDESCRIPTION: A requirements file that lists all necessary Python packages with pinned versions for building the Optimistix project documentation. It includes MkDocs as the base, material theme, markdown extensions, docstring integration tools, and Jupyter notebook converters. Some packages are pinned to older versions to maintain compatibility.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\n# Latest versions at time of writing.\nmkdocs==1.3.0            # Main documentation generator.\nmkdocs-material==7.3.6   # Theme\npymdown-extensions==9.4  # Markdown extensions e.g. to handle LaTeX.\nmkdocstrings==0.17.0     # Autogenerate documentation from docstrings.\nmknotebooks==0.7.1       # Turn Jupyter Lab notebooks into webpages.\npytkdocs_tweaks==0.0.8   # Tweaks mkdocstrings to improve various aspects\nmkdocs_include_exclude_files==0.0.1  # Allow for customising which files get included\njinja2==3.0.3            # Older version. After 3.1.0 seems to be incompatible with current versions of mkdocstrings.\nnbconvert==6.5.0         # | Older verson to avoid error\nnbformat==5.4.0          # |\npygments==2.14.0\nmkdocs-autorefs==1.0.1\nmkdocs-material-extensions==1.3.1\n\n# Install latest version of our dependencies\njax[cpu]\n```\n\n----------------------------------------\n\nTITLE: Academic Citation for Optimistix\nDESCRIPTION: BibTeX citation format for referencing Optimistix in academic work, including authors, title, and arXiv information.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{optimistix2024,\n    title={Optimistix: modular optimisation in JAX and Equinox},\n    author={Jason Rader and Terry Lyons and Patrick Kidger},\n    journal={arXiv:2402.09983},\n    year={2024},\n}\n```\n\n----------------------------------------\n\nTITLE: Abstract Gauss-Newton Solver Definition\nDESCRIPTION: Abstract base class for Gauss-Newton method implementations for solving nonlinear least squares problems.\nSOURCE: https://github.com/patrick-kidger/optimistix/blob/main/docs/api/least_squares.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\noptimistix.AbstractGaussNewton\n```"
  }
]
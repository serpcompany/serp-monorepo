[
  {
    "owner": "dmlc",
    "repo": "xgboost",
    "content": "TITLE: Implementing XGBoost Classifier in Python with scikit-learn\nDESCRIPTION: This code demonstrates how to use XGBoost with the scikit-learn API for a binary classification task. It loads the Iris dataset, splits it into training and testing sets, creates an XGBClassifier model with specific parameters, fits the model, and makes predictions.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/get_started.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom xgboost import XGBClassifier\n# read data\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\ndata = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(data['data'], data['target'], test_size=.2)\n# create model instance\nbst = XGBClassifier(n_estimators=2, max_depth=2, learning_rate=1, objective='binary:logistic')\n# fit model\nbst.fit(X_train, y_train)\n# make predictions\npreds = bst.predict(X_test)\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost Model with Dask and Custom Evaluation Metric\nDESCRIPTION: Example of training an XGBoost model using Dask with custom evaluation metric and early stopping callback. This snippet shows how to configure training parameters, evaluate on multiple datasets, and implement a custom evaluation function.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/dask.rst#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nbooster = dxgb.train(\n    client,\n    params={\n        \"objective\": \"binary:logistic\",\n        \"eval_metric\": [\"error\", \"rmse\"],\n        \"tree_method\": \"hist\",\n    },\n    dtrain=D_train,\n    evals=[(D_train, \"Train\"), (D_valid, \"Valid\")],\n    feval=eval_error_metric,  # custom evaluation metric\n    num_boost_round=100,\n    callbacks=[early_stop],\n)\n```\n\n----------------------------------------\n\nTITLE: Using XGBoost with Scikit-Learn Interface in Python\nDESCRIPTION: Illustrates how to use XGBoost's scikit-learn compatible API for regression tasks. It includes creating a regressor, fitting the model, and saving it in JSON format.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/python/python_intro.rst#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# Use \"hist\" for training the model.\nreg = xgb.XGBRegressor(tree_method=\"hist\", device=\"cuda\")\n# Fit the model using predictor X and response y.\nreg.fit(X, y)\n# Save model into JSON format.\nreg.save_model(\"regressor.json\")\n```\n\nLANGUAGE: python\nCODE:\n```\nbooster: xgb.Booster = reg.get_booster()\n```\n\n----------------------------------------\n\nTITLE: Setting Booster Parameters in Python for XGBoost\nDESCRIPTION: Demonstrates how to set booster parameters using a dictionary in XGBoost. It includes setting basic parameters like max_depth and eta, as well as specifying multiple evaluation metrics.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/python/python_intro.rst#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nparam = {'max_depth': 2, 'eta': 1, 'objective': 'binary:logistic'}\nparam['nthread'] = 4\nparam['eval_metric'] = 'auc'\n```\n\nLANGUAGE: python\nCODE:\n```\nparam['eval_metric'] = ['auc', 'ams@0']\n\n# alternatively:\n# plst = param.items()\n# plst += [('eval_metric', 'ams@0')]\n```\n\n----------------------------------------\n\nTITLE: Using Custom Objective and Metric in XGBoost Training with Softprob\nDESCRIPTION: This snippet demonstrates how to use a custom softprob objective function along with a custom metric in XGBoost training for multi-class classification.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/custom_metric_obj.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nXy = xgb.DMatrix(X, y)\nbooster = xgb.train(\n    {\"num_class\": kClasses, \"disable_default_eval_metric\": True},\n    m,\n    num_boost_round=kRounds,\n    obj=softprob_obj,\n    custom_metric=merror_with_transform,\n    evals_result=custom_results,\n```\n\n----------------------------------------\n\nTITLE: Objective Value Reformulation for Tree Learning\nDESCRIPTION: This formula reformulates the objective function in terms of the tree structure, allowing for efficient optimization by summing over leaves instead of individual data points.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/model.rst#2025-04-19_snippet_8\n\nLANGUAGE: math\nCODE:\n```\n\\text{obj}^{(t)} &\\approx \\sum_{i=1}^n [g_i w_{q(x_i)} + \\frac{1}{2} h_i w_{q(x_i)}^2] + \\gamma T + \\frac{1}{2}\\lambda \\sum_{j=1}^T w_j^2\\\\\n  &= \\sum^T_{j=1} [(\\sum_{i\\in I_j} g_i) w_j + \\frac{1}{2} (\\sum_{i\\in I_j} h_i + \\lambda) w_j^2 ] + \\gamma T\n```\n\n----------------------------------------\n\nTITLE: Implementing XGBoost in Julia for Binary Classification\nDESCRIPTION: This code demonstrates using XGBoost in Julia for binary classification. It reads data from LIBSVM format files, trains an XGBoost model with specific parameters (learning rate, depth, and number of boosting rounds), and makes predictions on test data.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/get_started.rst#2025-04-19_snippet_2\n\nLANGUAGE: julia\nCODE:\n```\nusing XGBoost\n# read data\ntrain_X, train_Y = readlibsvm(\"demo/data/agaricus.txt.train\", (6513, 126))\ntest_X, test_Y = readlibsvm(\"demo/data/agaricus.txt.test\", (1611, 126))\n# fit model\nnum_round = 2\nbst = xgboost(train_X, num_round, label=train_Y, eta=1, max_depth=2)\n# predict\npred = predict(bst, test_X)\n```\n\n----------------------------------------\n\nTITLE: Training a Random Forest Regressor with Scikit-Learn API\nDESCRIPTION: Example of using XGBoost's Scikit-Learn wrapper (XGBRFRegressor) to train a random forest regressor with cross-validation. This demonstrates the simplified interface for random forest implementation in XGBoost.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/rf.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import KFold\n\n# Your code ...\n\nkf = KFold(n_splits=2)\nfor train_index, test_index in kf.split(X, y):\n    xgb_model = xgb.XGBRFRegressor(random_state=42).fit(\n\tX[train_index], y[train_index])\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost Model with DART Booster in Python\nDESCRIPTION: This code snippet demonstrates how to use the DART booster in XGBoost. It loads training and test data, sets up parameters for the DART booster, trains the model, and makes predictions. Key parameters include booster type, tree depth, learning rate, and DART-specific settings like sample_type, normalize_type, rate_drop, and skip_drop.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/dart.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport xgboost as xgb\n# read in data\ndtrain = xgb.DMatrix('demo/data/agaricus.txt.train?format=libsvm')\ndtest = xgb.DMatrix('demo/data/agaricus.txt.test?format=libsvm')\n# specify parameters via map\nparam = {'booster': 'dart',\n         'max_depth': 5, 'learning_rate': 0.1,\n         'objective': 'binary:logistic',\n         'sample_type': 'uniform',\n         'normalize_type': 'tree',\n         'rate_drop': 0.1,\n         'skip_drop': 0.5}\nnum_round = 50\nbst = xgb.train(param, dtrain, num_round)\npreds = bst.predict(dtest)\n```\n\n----------------------------------------\n\nTITLE: Enabling GPU Acceleration in XGBoost Python API\nDESCRIPTION: Basic example showing how to enable GPU acceleration in XGBoost by setting the 'device' parameter to 'cuda' and using the 'hist' tree method. This uses the QuantileDMatrix for data handling and the train function.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/gpu/index.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nparams = dict()\nparams[\"device\"] = \"cuda\"\nparams[\"tree_method\"] = \"hist\"\nXy = xgboost.QuantileDMatrix(X, y)\nxgboost.train(params, Xy)\n```\n\n----------------------------------------\n\nTITLE: Implementing Root Mean Squared Log Error Metric in Python for XGBoost\nDESCRIPTION: This code defines a custom evaluation metric function for Root Mean Squared Log Error (RMSLE) to be used with XGBoost.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/custom_metric_obj.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef rmsle(predt: np.ndarray, dtrain: xgb.DMatrix) -> Tuple[str, float]:\n    ''' Root mean squared log error metric.'''\n    y = dtrain.get_label()\n    predt[predt < -1] = -1 + 1e-6\n    elements = np.power(np.log1p(y) - np.log1p(predt), 2)\n    return 'PyRMSLE', float(np.sqrt(np.sum(elements) / len(y)))\n```\n\n----------------------------------------\n\nTITLE: Visualizing and Analyzing XGBoost Models with Categorical Features\nDESCRIPTION: After training a model with categorical features, this code demonstrates how to visualize tree structures and calculate feature importances. It shows both graphviz and matplotlib visualization options.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/categorical.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Get a graph\ngraph = xgb.to_graphviz(clf, num_trees=1)\n# Or get a matplotlib axis\nax = xgb.plot_tree(clf, num_trees=1)\n# Get feature importances\nclf.feature_importances_\n```\n\n----------------------------------------\n\nTITLE: Using Custom Objective in XGBoost Training\nDESCRIPTION: This snippet demonstrates how to use the custom squared log error objective function in XGBoost training by passing it as an argument to xgb.train.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/custom_metric_obj.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nxgb.train({'tree_method': 'hist', 'seed': 1994},  # any other tree method is fine.\n         dtrain=dtrain,\n         num_boost_round=10,\n         obj=squared_log)\n```\n\n----------------------------------------\n\nTITLE: Training AFT Survival Model in Python\nDESCRIPTION: This Python code shows how to train an Accelerated Failure Time (AFT) model in XGBoost. It configures essential parameters like the objective function, evaluation metric, distribution type, and scale parameter for the AFT model.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/aft_survival_analysis.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nparams = {'objective': 'survival:aft',\n          'eval_metric': 'aft-nloglik',\n          'aft_loss_distribution': 'normal',\n          'aft_loss_distribution_scale': 1.20,\n          'tree_method': 'hist', 'learning_rate': 0.05, 'max_depth': 2}\nbst = xgb.train(params, dtrain, num_boost_round=5,\n                evals=[(dtrain, 'train')])\n```\n\n----------------------------------------\n\nTITLE: Using XGBoost's EarlyStopping Callback with sklearn Interface\nDESCRIPTION: Example showing how to use the EarlyStopping callback to customize early stopping behavior, including saving the best model instead of the full tree stack.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/python/sklearn_estimator.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nearly_stop = xgb.callback.EarlyStopping(\n    rounds=2, metric_name='logloss', data_name='validation_0', save_best=True\n)\nclf = xgb.XGBClassifier(tree_method=\"hist\", callbacks=[early_stop])\nclf.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n```\n\n----------------------------------------\n\nTITLE: Handling Missing Values in XGBoost DMatrix\nDESCRIPTION: Specifying missing value handling when creating DMatrix.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/python/python_intro.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndtrain = xgb.DMatrix(data, label=label, missing=np.NaN)\n```\n\n----------------------------------------\n\nTITLE: Training XGBClassifier with Categorical Features\nDESCRIPTION: Initializes and trains an XGBClassifier model with categorical data support. This example uses histogram-based tree method on GPU and enables categorical feature support through the enable_categorical parameter.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/categorical.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Supported tree methods are `approx` and `hist`.\nclf = xgb.XGBClassifier(tree_method=\"hist\", enable_categorical=True, device=\"cuda\")\n# X is the dataframe we created in previous snippet\nclf.fit(X, y)\n# Must use JSON/UBJSON for serialization, otherwise the information is lost.\nclf.save_model(\"categorical-model.json\")\n```\n\n----------------------------------------\n\nTITLE: Basic XGBoost Usage with Dask Cluster\nDESCRIPTION: Demonstrates how to set up a local Dask cluster, create DaskDMatrix, and train an XGBoost model using the Dask interface. It includes creating random data, initializing the cluster, and running the training process.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/dask.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom xgboost import dask as dxgb\n\nimport dask.array as da\nimport dask.distributed\n\nif __name__ == \"__main__\":\n    cluster = dask.distributed.LocalCluster()\n    client = dask.distributed.Client(cluster)\n\n    # X and y must be Dask dataframes or arrays\n    num_obs = 1e5\n    num_features = 20\n    X = da.random.random(size=(num_obs, num_features), chunks=(1000, num_features))\n    y = da.random.random(size=(num_obs, 1), chunks=(1000, 1))\n\n    dtrain = dxgb.DaskDMatrix(client, X, y)\n    # or\n    # dtrain = dxgb.DaskQuantileDMatrix(client, X, y)\n\n    output = dxgb.train(\n        client,\n        {\"verbosity\": 2, \"tree_method\": \"hist\", \"objective\": \"reg:squarederror\"},\n        dtrain,\n        num_boost_round=4,\n        evals=[(dtrain, \"train\")],\n    )\n```\n\n----------------------------------------\n\nTITLE: Training a Basic XGBoost Classification Model with sklearn Interface\nDESCRIPTION: Code snippet demonstrating how to train a classification model using XGBoost's sklearn interface with early stopping and saving the model to JSON format.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/python/sklearn_estimator.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\n\nimport xgboost as xgb\n\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=94)\n\n# Use \"hist\" for constructing the trees, with early stopping enabled.\nclf = xgb.XGBClassifier(tree_method=\"hist\", early_stopping_rounds=2)\n# Fit the model, test sets are used for early stopping.\nclf.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n# Save model into JSON format.\nclf.save_model(\"clf.json\")\n```\n\n----------------------------------------\n\nTITLE: Building and Fitting a XGBoost Pipeline in Spark ML\nDESCRIPTION: Creates a complete ML pipeline with feature assembly, label indexing, XGBoost model training, and label conversion, then fits the pipeline on the training dataset.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost4j_spark_tutorial.rst#2025-04-19_snippet_14\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.ml.feature._\nimport org.apache.spark.ml.Pipeline\n\nval pipeline = new Pipeline()\n    .setStages(Array(assembler, stringIndexer, booster, labelConverter))\nval model = pipeline.fit(training)\n```\n\n----------------------------------------\n\nTITLE: Implementing Dirichlet Regression Functions in R\nDESCRIPTION: This snippet defines the log-likelihood, gradient, and Hessian functions for Dirichlet regression in R. It includes a helper function for softmax calculation.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/advanced_custom_obj.rst#2025-04-19_snippet_1\n\nLANGUAGE: r\nCODE:\n```\nsoftmax <- function(x) {\n    max.x <- max(x)\n    e <- exp(x - max.x)\n    return(e / sum(e))\n}\n\ndirichlet.fun <- function(pred, y) {\n    epred <- exp(pred)\n    sum_epred <- rowSums(epred)\n    return(\n        sum(lgamma(epred))\n        - sum(lgamma(sum_epred))\n        - sum(log(y) * (epred - 1))\n    )\n}\n\ndirichlet.grad <- function(pred, y) {\n    epred <- exp(pred)\n    return(\n        epred * (\n            digamma(epred)\n            - digamma(rowSums(epred))\n            - log(y)\n        )\n    )\n}\n\ndirichlet.hess <- function(pred, y) {\n    epred <- exp(pred)\n    grad <- dirichlet.grad(pred, y)\n    k <- ncol(y)\n    H <- array(dim = c(nrow(y), k, k))\n    for (row in seq_len(nrow(y))) {\n        H[row, , ] <- (\n            - trigamma(sum(epred[row,])) * tcrossprod(epred[row,])\n            + diag(grad[row,] + trigamma(epred[row,]) * epred[row,]^2)\n        )\n    }\n    return(H)\n}\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with XGBoost Model in Python\nDESCRIPTION: Shows how to use a trained XGBoost model to make predictions on new data. It includes an example of using the best iteration when early stopping was enabled during training.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/python/python_intro.rst#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# 7 entities, each contains 10 features\ndata = np.random.rand(7, 10)\ndtest = xgb.DMatrix(data)\nypred = bst.predict(dtest)\n```\n\nLANGUAGE: python\nCODE:\n```\nypred = bst.predict(dtest, iteration_range=(0, bst.best_iteration + 1))\n```\n\n----------------------------------------\n\nTITLE: Optimization Objective for New Tree in XGBoost\nDESCRIPTION: This formula represents the specific objective function at step t that XGBoost optimizes for the new tree, using first and second order gradients along with the regularization term.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/model.rst#2025-04-19_snippet_5\n\nLANGUAGE: math\nCODE:\n```\n\\sum_{i=1}^n [g_i f_t(x_i) + \\frac{1}{2} h_i f_t^2(x_i)] + \\omega(f_t)\n```\n\n----------------------------------------\n\nTITLE: Using Early Stopping Callback in XGBoost Python\nDESCRIPTION: This code demonstrates how to use XGBoost's EarlyStopping callback with a custom evaluation metric. It creates training and validation DMatrix objects, defines a custom error metric function, configures the early stopping callback, and trains a model with these components while tracking the stopping history.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/python/callbacks.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nD_train = xgb.DMatrix(X_train, y_train)\nD_valid = xgb.DMatrix(X_valid, y_valid)\n\n# Define a custom evaluation metric used for early stopping.\ndef eval_error_metric(predt, dtrain: xgb.DMatrix):\n    label = dtrain.get_label()\n    r = np.zeros(predt.shape)\n    gt = predt > 0.5\n    r[gt] = 1 - label[gt]\n    le = predt <= 0.5\n    r[le] = label[le]\n    return 'CustomErr', np.sum(r)\n\n# Specify which dataset and which metric should be used for early stopping.\nearly_stop = xgb.callback.EarlyStopping(rounds=early_stopping_rounds,\n                                        metric_name='CustomErr',\n                                        data_name='Valid')\n\nbooster = xgb.train(\n    {'objective': 'binary:logistic',\n     'eval_metric': ['error', 'rmse'],\n     'tree_method': 'hist'}, D_train,\n    evals=[(D_train, 'Train'), (D_valid, 'Valid')],\n    feval=eval_error_metric,\n    num_boost_round=1000,\n    callbacks=[early_stop],\n    verbose_eval=False)\n\ndump = booster.get_dump(dump_format='json')\nassert len(early_stop.stopping_history['Valid']['CustomErr']) == len(dump)\n```\n\n----------------------------------------\n\nTITLE: Enabling GPU Acceleration with XGBoost Scikit-Learn Interface\nDESCRIPTION: Example demonstrating how to enable GPU acceleration when using XGBoost through the Scikit-Learn interface by setting the tree_method and device parameters in the XGBRegressor constructor.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/gpu/index.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nXGBRegressor(tree_method=\"hist\", device=\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Creating and Training XGBoost Model using C API\nDESCRIPTION: Example of creating a Booster object, setting parameters, and training the model using XGBoost C API functions.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/c_api_tutorial.rst#2025-04-19_snippet_8\n\nLANGUAGE: c\nCODE:\n```\nBoosterHandle booster;\nconst int eval_dmats_size;\n// We assume that training and test data have been loaded into 'train' and 'test'\nDMatrixHandle eval_dmats[eval_dmats_size] = {train, test};\nsafe_xgboost(XGBoosterCreate(eval_dmats, eval_dmats_size, &booster));\n\nsafe_xgboost(XGBoosterSetParam(booster, \"booster\", \"gblinear\"));\n// default max_depth =6\nsafe_xgboost(XGBoosterSetParam(booster, \"max_depth\", \"3\"));\n// default eta  = 0.3\nsafe_xgboost(XGBoosterSetParam(booster, \"eta\", \"0.1\"));\n\nint num_of_iterations = 20;\nconst char* eval_names[eval_dmats_size] = {\"train\", \"test\"};\nconst char* eval_result = NULL;\n\nfor (int i = 0; i < num_of_iterations; ++i) {\n  // Update the model performance for each iteration\n  safe_xgboost(XGBoosterUpdateOneIter(booster, i, train));\n\n  // Give the statistics for the learner for training & testing dataset in terms of error after each iteration\n  safe_xgboost(XGBoosterEvalOneIter(booster, i, eval_dmats, eval_names, eval_dmats_size, &eval_result));\n  printf(\"%s\\n\", eval_result);\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Up Ranged Labels for Survival Analysis in Python\nDESCRIPTION: This Python code demonstrates how to create a DMatrix object with ranged labels for survival analysis. It shows how to associate lower and upper bounds for labels with a data matrix, covering all types of censoring (uncensored, right-censored, left-censored, and interval-censored).\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/aft_survival_analysis.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport xgboost as xgb\n\n# 4-by-2 Data matrix\nX = np.array([[1, -1], [-1, 1], [0, 1], [1, 0]])\ndtrain = xgb.DMatrix(X)\n\n# Associate ranged labels with the data matrix.\n# This example shows each kind of censored labels.\n#                         uncensored    right     left  interval\ny_lower_bound = np.array([      2.0,     3.0,     0.0,     4.0])\ny_upper_bound = np.array([      2.0, +np.inf,     4.0,     5.0])\ndtrain.set_float_info('label_lower_bound', y_lower_bound)\ndtrain.set_float_info('label_upper_bound', y_upper_bound)\n```\n\n----------------------------------------\n\nTITLE: Preparing Synthetic Ranking Dataset in Python\nDESCRIPTION: This snippet demonstrates how to create a synthetic ranking dataset using scikit-learn and numpy. It generates feature matrices, labels, and query group indices, then sorts the data based on query index.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/learning_to_rank.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import make_classification\nimport numpy as np\n\nimport xgboost as xgb\n\n# Make a synthetic ranking dataset for demonstration\nseed = 1994\nX, y = make_classification(random_state=seed)\nrng = np.random.default_rng(seed)\nn_query_groups = 3\nqid = rng.integers(0, n_query_groups, size=X.shape[0])\n\n# Sort the inputs based on query index\nsorted_idx = np.argsort(qid)\nX = X[sorted_idx, :]\ny = y[sorted_idx]\nqid = qid[sorted_idx]\n```\n\n----------------------------------------\n\nTITLE: Implementing Dirichlet Expected Hessian Matrix in Python\nDESCRIPTION: Function that calculates the expected Hessian for Dirichlet regression. This provides a more stable alternative to the true Hessian, which can have negative values in the diagonal that would be problematic for XGBoost's optimization.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/advanced_custom_obj.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef dirichlet_expected_hess(pred: np.ndarray) -> np.ndarray:\n    epred = np.exp(pred)\n    k = pred.shape[1]\n    Ehess = np.empty((pred.shape[0], k, k))\n    for row in range(pred.shape[0]):\n        Ehess[row, :, :] = (\n            - trigamma(epred[row].sum()) * np.outer(epred[row], epred[row])\n            + np.diag(trigamma(epred[row]) * epred[row] ** 2)\n        )\n    return Ehess\n```\n\n----------------------------------------\n\nTITLE: Implementing Early Stopping in XGBoost Python Training\nDESCRIPTION: Demonstrates how to use early stopping during XGBoost model training to find the optimal number of boosting rounds based on validation set performance.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/python/python_intro.rst#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ntrain(..., evals=evals, early_stopping_rounds=10)\n```\n\n----------------------------------------\n\nTITLE: Configuring Parameters for Random Forest Training with XGBoost API\nDESCRIPTION: Sample parameter dictionary configuration for training a random forest using XGBoost's native API on a GPU. This sets the essential parameters including colsample_bynode, learning_rate, max_depth, num_parallel_tree, objective, subsample, and specifies GPU usage.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/rf.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nparams = {\n  \"colsample_bynode\": 0.8,\n  \"learning_rate\": 1,\n  \"max_depth\": 5,\n  \"num_parallel_tree\": 100,\n  \"objective\": \"binary:logistic\",\n  \"subsample\": 0.8,\n  \"tree_method\": \"hist\",\n  \"device\": \"cuda\",\n}\n```\n\n----------------------------------------\n\nTITLE: Computing SHAP Values on GPU with XGBoost\nDESCRIPTION: Example showing how to compute SHAP (SHapley Additive exPlanations) values and interaction values on GPU. This requires setting the device parameter to a CUDA device and using the pred_contribs or pred_interactions flags with the predict method.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/gpu/index.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nbooster.set_param({\"device\": \"cuda:0\"})\nshap_values = booster.predict(dtrain, pred_contribs=True)\nshap_interaction_values = model.predict(dtrain, pred_interactions=True)\n```\n\n----------------------------------------\n\nTITLE: Specifying Feature Types for NumPy Array Input\nDESCRIPTION: When using NumPy arrays instead of DataFrames, this code shows how to explicitly specify which features are categorical using the feature_types parameter in DMatrix initialization.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/categorical.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# \"q\" is numerical feature, while \"c\" is categorical feature\nft = [\"q\", \"c\", \"c\"]\nX: np.ndarray = load_my_data()\nassert X.shape[1] == 3\nXy = xgb.DMatrix(X, y, feature_types=ft, enable_categorical=True)\n```\n\n----------------------------------------\n\nTITLE: Setting Up GPU-Based External Memory with RMM\nDESCRIPTION: This code demonstrates how to set up GPU-based external memory training using RAPIDS Memory Manager (RMM) for improved performance. It shows how to configure memory resources, build the ExtMemQuantileDMatrix, and train the model on a GPU device.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/external_memory.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport cupy as cp\nimport rmm\nfrom rmm.allocators.cupy import rmm_cupy_allocator\n\n# It's important to use RMM for GPU-based external memory to improve performance.\n# If XGBoost is not built with RMM support, a warning will be raised.\n# We use the pool memory resource here, you can also try the `ArenaMemoryResource` for\n# improved memory fragmentation handling.\nmr = rmm.mr.PoolMemoryResource(rmm.mr.CudaAsyncMemoryResource())\nrmm.mr.set_current_device_resource(mr)\n# Set the allocator for cupy as well.\ncp.cuda.set_allocator(rmm_cupy_allocator)\n# Make sure XGBoost is using RMM for all allocations.\nwith xgboost.config_context(use_rmm=True):\n    # Construct the iterators for ExtMemQuantileDMatrix\n    # ...\n    # Build the ExtMemQuantileDMatrix and start training\n    Xy_train = xgboost.ExtMemQuantileDMatrix(it_train, max_bin=n_bins)\n    # Use the training DMatrix as a reference\n    Xy_valid = xgboost.ExtMemQuantileDMatrix(it_valid, max_bin=n_bins, ref=Xy_train)\n    booster = xgboost.train(\n        {\n            \"tree_method\": \"hist\",\n            \"max_depth\": 6,\n            \"max_bin\": n_bins,\n            \"device\": device,\n        },\n        Xy_train,\n        num_boost_round=n_rounds,\n        evals=[(Xy_train, \"Train\"), (Xy_valid, \"Valid\")]\n    )\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost with Custom Metric in Python\nDESCRIPTION: This snippet demonstrates how to train an XGBoost model using a custom metric function. It uses the 'multi:softmax' objective and disables the default evaluation metric.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/custom_metric_obj.rst#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nbooster = xgb.train(\n    {\n        \"num_class\": kClasses,\n        \"disable_default_eval_metric\": True,\n        \"objective\": \"multi:softmax\",\n    },\n    m,\n    num_boost_round=kRounds,\n    # Use a simpler metric implementation.\n    custom_metric=merror,\n    evals_result=custom_results,\n    evals=[(m, \"train\")],\n)\n```\n\n----------------------------------------\n\nTITLE: Custom Evaluation Metric and Callback for XGBoost with Dask\nDESCRIPTION: Shows how to define a custom evaluation metric and use a callback function with XGBoost in a Dask environment.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/dask.rst#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef eval_error_metric(predt, dtrain: xgb.DMatrix):\n    label = dtrain.get_label()\n    r = np.zeros(predt.shape)\n    gt = predt > 0.5\n    r[gt] = 1 - label[gt]\n    le = predt <= 0.5\n    r[le] = label[le]\n    return 'CustomErr', np.sum(r)\n\n# custom callback\nearly_stop = xgb.callback.EarlyStopping(\n    rounds=early_stopping_rounds,\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with XGBoost in Java\nDESCRIPTION: This snippet shows how to use a trained XGBoost model to make predictions on new data, including regular predictions and leaf predictions, using XGBoost4J in Java.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/java_intro.rst#2025-04-19_snippet_12\n\nLANGUAGE: java\nCODE:\n```\nDMatrix dtest = new DMatrix(\"test.svm.txt\");\n// predict\nfloat[][] predicts = booster.predict(dtest);\n// predict leaf\nfloat[][] leafPredicts = booster.predictLeaf(dtest, 0);\n```\n\n----------------------------------------\n\nTITLE: Implementing Hyperparameter Tuning for XGBoost in Spark ML\nDESCRIPTION: Sets up a grid search with cross-validation to find optimal parameters (max_depth and eta) for XGBoost, evaluating model performance and extracting the best model configuration.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost4j_spark_tutorial.rst#2025-04-19_snippet_16\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.ml.tuning._\nimport org.apache.spark.ml.PipelineModel\nimport ml.dmlc.xgboost4j.scala.spark.XGBoostClassificationModel\n\nval paramGrid = new ParamGridBuilder()\n    .addGrid(booster.maxDepth, Array(3, 8))\n    .addGrid(booster.eta, Array(0.2, 0.6))\n    .build()\nval cv = new CrossValidator()\n    .setEstimator(pipeline)\n    .setEvaluator(evaluator)\n    .setEstimatorParamMaps(paramGrid)\n    .setNumFolds(3)\n\nval cvModel = cv.fit(training)\n\nval bestModel = cvModel.bestModel.asInstanceOf[PipelineModel].stages(2)\n    .asInstanceOf[XGBoostClassificationModel]\nbestModel.extractParamMap()\n```\n\n----------------------------------------\n\nTITLE: Setting Tree Booster Parameters in Python\nDESCRIPTION: Example of setting various tree booster parameters like eta, gamma, and max_depth when creating an XGBoost model in Python.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nparams = {\n    'booster': 'gbtree',\n    'eta': 0.3,\n    'gamma': 0,\n    'max_depth': 6,\n    'min_child_weight': 1,\n    'subsample': 1,\n    'colsample_bytree': 1\n}\nmodel = xgboost.train(params, dtrain)\n```\n\n----------------------------------------\n\nTITLE: Training a Random Forest Model with XGBoost API\nDESCRIPTION: Example of training a random forest model using the XGBoost train function with the previously defined parameters. The num_boost_round is set to 1 to prevent boosting multiple random forests.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/rf.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nbst = train(params, dmatrix, num_boost_round=1)\n```\n\n----------------------------------------\n\nTITLE: Implementing Squared Log Error Objective in Python for XGBoost\nDESCRIPTION: This code snippet defines the gradient, hessian, and squared log error objective function for use with XGBoost. It calculates the necessary components for the Squared Log Error (SLE) objective.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/custom_metric_obj.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport xgboost as xgb\nfrom typing import Tuple\n\ndef gradient(predt: np.ndarray, dtrain: xgb.DMatrix) -> np.ndarray:\n    '''Compute the gradient squared log error.'''\n    y = dtrain.get_label()\n    return (np.log1p(predt) - np.log1p(y)) / (predt + 1)\n\ndef hessian(predt: np.ndarray, dtrain: xgb.DMatrix) -> np.ndarray:\n    '''Compute the hessian for squared log error.'''\n    y = dtrain.get_label()\n    return ((-np.log1p(predt) + np.log1p(y) + 1) /\n            np.power(predt + 1, 2))\n\ndef squared_log(predt: np.ndarray,\n                dtrain: xgb.DMatrix) -> Tuple[np.ndarray, np.ndarray]:\n    '''Squared Log Error objective. A simplified version for RMSLE used as\n    objective function.\n    '''\n    predt[predt < -1] = -1 + 1e-6\n    grad = gradient(predt, dtrain)\n    hess = hessian(predt, dtrain)\n    return grad, hess\n```\n\n----------------------------------------\n\nTITLE: Saving XGBoost Model in JSON Format (Python)\nDESCRIPTION: Demonstrates how to save an XGBoost model in JSON format using Python. The .json file extension triggers JSON serialization.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/saving_model.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nbst.save_model('model_file_name.json')\n```\n\n----------------------------------------\n\nTITLE: Calculating Tree Ensemble Prediction in Python\nDESCRIPTION: This code demonstrates how to calculate the prediction for a tree ensemble model by summing the predictions of individual trees. It represents the core idea of combining multiple decision trees in XGBoost.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/model.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef tree_ensemble_prediction(x, trees):\n    return sum(tree.predict(x) for tree in trees)\n```\n\n----------------------------------------\n\nTITLE: Implementing Dirichlet Regression Functions in Python\nDESCRIPTION: This snippet defines the log-likelihood, gradient, and Hessian functions for Dirichlet regression in Python. It uses NumPy and SciPy special functions for efficient computation.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/advanced_custom_obj.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom scipy.special import loggamma, psi as digamma, polygamma\ntrigamma = lambda x: polygamma(1, x)\n\ndef dirichlet_fun(pred: np.ndarray, Y: np.ndarray) -> float:\n    epred = np.exp(pred)\n    sum_epred = np.sum(epred, axis=1, keepdims=True)\n    return (\n        loggamma(epred).sum()\n        - loggamma(sum_epred).sum()\n        - np.sum(np.log(Y) * (epred - 1))\n    )\ndef dirichlet_grad(pred: np.ndarray, Y: np.ndarray) -> np.ndarray:\n    epred = np.exp(pred)\n    return epred * (\n        digamma(epred)\n        - digamma(np.sum(epred, axis=1, keepdims=True))\n        - np.log(Y)\n    )\ndef dirichlet_hess(pred: np.ndarray, Y: np.ndarray) -> np.ndarray:\n    epred = np.exp(pred)\n    grad = dirichlet_grad(pred, Y)\n    k = Y.shape[1]\n    H = np.empty((pred.shape[0], k, k))\n    for row in range(pred.shape[0]):\n        H[row, :, :] = (\n            - trigamma(epred[row].sum()) * np.outer(epred[row], epred[row])\n            + np.diag(grad[row] + trigamma(epred[row]) * epred[row] ** 2)\n        )\n    return H\n```\n\n----------------------------------------\n\nTITLE: Implementing Cross-Validation with Early Stopping in XGBoost\nDESCRIPTION: Demonstration of how to implement cross-validation with early stopping in XGBoost using a custom function to fit and score models across multiple folds.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/python/sklearn_estimator.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.base import clone\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import StratifiedKFold, cross_validate\n\nimport xgboost as xgb\n\nX, y = load_breast_cancer(return_X_y=True)\n\n\ndef fit_and_score(estimator, X_train, X_test, y_train, y_test):\n    \"\"\"Fit the estimator on the train set and score it on both sets\"\"\"\n    estimator.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n\n    train_score = estimator.score(X_train, y_train)\n    test_score = estimator.score(X_test, y_test)\n\n    return estimator, train_score, test_score\n\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=94)\n\nclf = xgb.XGBClassifier(tree_method=\"hist\", early_stopping_rounds=3)\n\nresults = {}\n\nfor train, test in cv.split(X, y):\n    X_train = X[train]\n    X_test = X[test]\n    y_train = y[train]\n    y_test = y[test]\n    est, train_score, test_score = fit_and_score(\n        clone(clf), X_train, X_test, y_train, y_test\n    )\n    results[est] = (train_score, test_score)\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost Ranking Model using Scikit-learn Interface\nDESCRIPTION: This code snippet shows how to train a simple XGBoost ranking model using the scikit-learn estimator interface. It uses the 'rank:ndcg' objective and the 'topk' pair method.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/learning_to_rank.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nranker = xgb.XGBRanker(tree_method=\"hist\", lambdarank_num_pair_per_sample=8, objective=\"rank:ndcg\", lambdarank_pair_method=\"topk\")\nranker.fit(X, y, qid=qid)\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Label Classification with XGBoost\nDESCRIPTION: Demonstrates how to use XGBoost for multi-label classification using the default one-model-per-target approach. The example creates a synthetic dataset with multiple labels per sample, trains an XGBoost classifier, and verifies the predictions match the original labels.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/multioutput.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import make_multilabel_classification\nimport numpy as np\n\nX, y = make_multilabel_classification(\n    n_samples=32, n_classes=5, n_labels=3, random_state=0\n)\nclf = xgb.XGBClassifier(tree_method=\"hist\")\nclf.fit(X, y)\nnp.testing.assert_allclose(clf.predict(X), y)\n```\n\n----------------------------------------\n\nTITLE: Specifying Validation Sets in XGBoost Python API\nDESCRIPTION: Shows how to specify validation sets to monitor performance during training in XGBoost.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/python/python_intro.rst#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nevallist = [(dtrain, 'train'), (dtest, 'eval')]\n```\n\n----------------------------------------\n\nTITLE: Saving XGBoost Model in JSON or UBJSON Format (Scala)\nDESCRIPTION: Illustrates saving an XGBoost model in either JSON or UBJSON format using Scala. The format option specifies the serialization type.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/saving_model.rst#2025-04-19_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nval format = \"json\"  // or val format = \"ubj\"\nmodel.write.option(\"format\", format).save(\"model_directory_path\")\n```\n\n----------------------------------------\n\nTITLE: Loading XGBoost model from JSON in Python\nDESCRIPTION: Example of how to load an XGBoost model from the new JSON serialization format in Python. This method ensures compatibility across different machines and configurations.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/NEWS.md#2025-04-19_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nbooster = xgb.Booster()\nbooster.load_model('model.json')\n```\n\n----------------------------------------\n\nTITLE: Transforming String Labels to Double with Spark Window Operations\nDESCRIPTION: Converts the String-typed class labels to Double values using Spark's Window operations. This approach is used instead of StringIndexer for better compatibility with RAPIDS Accelerator. The data is also split into training and test sets.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost4j_spark_gpu_tutorial.rst#2025-04-19_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.sql.expressions.Window\nimport org.apache.spark.sql.functions._\n\nval spec = Window.orderBy(labelName)\nval Array(train, test) = xgbInput\n    .withColumn(\"tmpClassName\", dense_rank().over(spec) - 1)\n    .drop(labelName)\n    .withColumnRenamed(\"tmpClassName\", labelName)\n    .randomSplit(Array(0.7, 0.3), seed = 1)\n\ntrain.show(5)\n```\n\n----------------------------------------\n\nTITLE: Executing FHE Prediction with XGBClassifier\nDESCRIPTION: Demonstrates how to perform actual Fully Homomorphic Encryption execution for prediction. This step represents the real privacy-preserving inference on encrypted data.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/privacy_preserving.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npredictions = classifier.predict(X_test, fhe=\"execute\")\n```\n\n----------------------------------------\n\nTITLE: Predicting and Sorting Relevance Scores with XGBoost Ranker\nDESCRIPTION: This code snippet shows how to use the trained XGBoost ranker to predict relevance scores for input samples and sort them in descending order of relevance.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/learning_to_rank.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nscores = ranker.predict(X)\nsorted_idx = np.argsort(scores)[::-1]\n# Sort the relevance scores from most relevant to least relevant\nscores = scores[sorted_idx]\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Objective Function for XGBoost Classifier\nDESCRIPTION: This snippet demonstrates how to define a custom objective function for multi-class classification without accessing DMatrix. It implements a softmax probability objective function to be used with XGBClassifier.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/custom_metric_obj.rst#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef softprob_obj(labels: np.ndarray, predt: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n    rows = labels.shape[0]\n    classes = predt.shape[1]\n    grad = np.zeros((rows, classes), dtype=float)\n    hess = np.zeros((rows, classes), dtype=float)\n    eps = 1e-6\n    for r in range(predt.shape[0]):\n        target = labels[r]\n        p = softmax(predt[r, :])\n        for c in range(predt.shape[1]):\n            g = p[c] - 1.0 if c == target else p[c]\n            h = max((2.0 * p[c] * (1.0 - p[c])).item(), eps)\n            grad[r, c] = g\n            hess[r, c] = h\n\n    grad = grad.reshape((rows * classes, 1))\n    hess = hess.reshape((rows * classes, 1))\n    return grad, hess\n\nclf = xgb.XGBClassifier(tree_method=\"hist\", objective=softprob_obj)\n```\n\n----------------------------------------\n\nTITLE: Creating a SparkXGBRegressor in PySpark\nDESCRIPTION: Demonstrates how to create a SparkXGBRegressor estimator with specific parameters for features column, label column, and number of workers.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/spark_estimator.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom xgboost.spark import SparkXGBRegressor\nxgb_regressor = SparkXGBRegressor(\n  features_col=\"features\",\n  label_col=\"label\",\n  num_workers=2,\n)\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost Model in Python\nDESCRIPTION: Demonstrates the process of training an XGBoost model using the train function, including setting the number of rounds and providing evaluation list.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/python/python_intro.rst#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nnum_round = 10\nbst = xgb.train(param, dtrain, num_round, evallist)\n```\n\n----------------------------------------\n\nTITLE: Saving XGBoost Model in JSON Format (R)\nDESCRIPTION: Shows how to save an XGBoost model in JSON format using R. The .json file extension is used to specify JSON serialization.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/saving_model.rst#2025-04-19_snippet_1\n\nLANGUAGE: r\nCODE:\n```\nxgb.save(bst, 'model_file_name.json')\n```\n\n----------------------------------------\n\nTITLE: Training AFT Survival Model in R\nDESCRIPTION: This R code shows how to train an Accelerated Failure Time (AFT) model in XGBoost. It configures essential parameters like the objective function, evaluation metric, distribution type, and scale parameter for the AFT model.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/aft_survival_analysis.rst#2025-04-19_snippet_3\n\nLANGUAGE: r\nCODE:\n```\nparams <- list(objective='survival:aft',\n               eval_metric='aft-nloglik',\n               aft_loss_distribution='normal',\n               aft_loss_distribution_scale=1.20,\n               tree_method='hist',\n               learning_rate=0.05,\n               max_depth=2)\nwatchlist <- list(train = dtrain)\nbst <- xgb.train(params, dtrain, nrounds=5, watchlist)\n```\n\n----------------------------------------\n\nTITLE: Defining Tree Structure with Feature Interaction Constraints using Graphviz\nDESCRIPTION: This Graphviz code defines a directed graph representing a decision tree with feature interaction constraints. It illustrates how feature constraints propagate through tree levels, showing which features are legitimate split candidates at each node. The tree shows feature x₀ at the root, x₁ at the second level, and nodes with feature sets {0, 1, 3, 4} at lower levels.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/feature_interaction_constraint.rst#2025-04-19_snippet_3\n\nLANGUAGE: graphviz\nCODE:\n```\ndigraph feature_interaction_illustration5 {\n      graph [fontname = \"helvetica\"];\n      node [fontname = \"helvetica\"];\n      edge [fontname = \"helvetica\"];\n      0 [label=<x<SUB><FONT POINT-SIZE=\"11\">0</FONT></SUB>>, shape=box, color=black, fontcolor=black];\n      1 [label=\"...\", shape=none];\n      2 [label=<x<SUB><FONT POINT-SIZE=\"11\">1</FONT></SUB>>, shape=box, color=black, fontcolor=black];\n      3 [label=<x<SUB><FONT POINT-SIZE=\"11\">{0, 1, 3, 4}</FONT></SUB>>, shape=box, color=black, fontcolor=black];\n      4 [label=<x<SUB><FONT POINT-SIZE=\"11\">{0, 1, 3, 4}</FONT></SUB>>, shape=box, color=black, fontcolor=black];\n      5 [label=\"...\", shape=none];\n      6 [label=\"...\", shape=none];\n      7 [label=\"...\", shape=none];\n      8 [label=\"...\", shape=none];\n      0 -> 1;\n      0 -> 2;\n      2 -> 3;\n      2 -> 4;\n      3 -> 5;\n      3 -> 6;\n      4 -> 7;\n      4 -> 8;\n    }\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Data Iterator for XGBoost External Memory\nDESCRIPTION: This code shows how to create a custom data iterator for loading data in chunks from multiple files. The iterator implements the required 'next' and 'reset' methods and demonstrates how to use it with both ExtMemQuantileDMatrix and DMatrix for training.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/external_memory.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom typing import List, Callable\nimport xgboost\nfrom sklearn.datasets import load_svmlight_file\n\nclass Iterator(xgboost.DataIter):\n  def __init__(self, svm_file_paths: List[str]) -> None:\n    self._file_paths = svm_file_paths\n    self._it = 0\n    # XGBoost will generate some cache files under the current directory with the prefix\n    # \"cache\"\n    super().__init__(cache_prefix=os.path.join(\".\", \"cache\"))\n\n  def next(self, input_data: Callable) -> bool:\n    \"\"\"Advance the iterator by 1 step and pass the data to XGBoost. This function is\n    called by XGBoost during the construction of ``DMatrix``\n\n    \"\"\"\n    if self._it == len(self._file_paths):\n      # return False to let XGBoost know this is the end of the iteration\n      return False\n\n    # input_data is a function passed in by XGBoost and has the exact same signature of\n    # ``DMatrix``\n    X, y = load_svmlight_file(self._file_paths[self._it])\n    # Keyword-only arguments, see the ``DMatrix`` class for accepted arguments.\n    input_data(data=X, label=y)\n    self._it += 1\n    # Return True to let XGBoost know we haven't seen all the files yet.\n    return True\n\n  def reset(self) -> None:\n    \"\"\"Reset the iterator to its beginning\"\"\"\n    self._it = 0\n\nit = Iterator([\"file_0.svm\", \"file_1.svm\", \"file_2.svm\"])\n\n# Use the ``ExtMemQuantileDMatrix`` for the hist tree method.\nXy = xgboost.ExtMemQuantileDMatrix(it)\nbooster = xgboost.train({\"tree_method\": \"hist\"}, Xy)\n\n# The ``approx`` tree method also works, but with lower performance and cannot be used\n# with the quantile DMatrix.\nXy = xgboost.DMatrix(it)\nbooster = xgboost.train({\"tree_method\": \"approx\"}, Xy)\n```\n\n----------------------------------------\n\nTITLE: Loading XGBoost Booster from Saved PySpark Model\nDESCRIPTION: Demonstrates how to load a saved XGBoost booster directly from the model file saved by SparkXGBRegressor, without going through the PySpark API.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/spark_estimator.rst#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport xgboost as xgb\nbst = xgb.Booster()\n# Loading the model saved in previous snippet\nbst.load_model(\"/tmp/xgboost-pyspark-model/model/part-00000\")\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost Model with GPU in PySpark\nDESCRIPTION: PySpark application that creates a SparkXGBRegressor with GPU device configuration, trains the model on data from parquet files, and generates predictions.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/spark_estimator.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom xgboost.spark import SparkXGBRegressor\nspark = SparkSession.builder.getOrCreate()\n\n# read data into spark dataframe\ntrain_data_path = \"xxxx/train\"\ntrain_df = spark.read.parquet(data_path)\n\ntest_data_path = \"xxxx/test\"\ntest_df = spark.read.parquet(test_data_path)\n\n# assume the label column is named \"class\"\nlabel_name = \"class\"\n\n# get a list with feature column names\nfeature_names = [x.name for x in train_df.schema if x.name != label_name]\n\n# create a xgboost pyspark regressor estimator and set device=\"cuda\"\nregressor = SparkXGBRegressor(\n  features_col=feature_names,\n  label_col=label_name,\n  num_workers=2,\n  device=\"cuda\",\n)\n\n# train and return the model\nmodel = regressor.fit(train_df)\n\n# predict on test data\npredict_df = model.transform(test_df)\npredict_df.show()\n```\n\n----------------------------------------\n\nTITLE: Using Base Margin for Stacking XGBoost Models\nDESCRIPTION: This example shows how to use the base_margin parameter to stack XGBoost models. It creates a first model, obtains raw predictions, and then feeds those predictions as base_margin into a second model to build upon the first model's output.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/intercept.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport xgboost as xgb\nfrom sklearn.datasets import make_regression\n\nX, y = make_regression()\n\nreg = xgb.XGBRegressor()\nreg.fit(X, y)\n# Request for raw prediction\nm = reg.predict(X, output_margin=True)\n\nreg_1 = xgb.XGBRegressor()\n# Feed the prediction into the next model\nreg_1.fit(X, y, base_margin=m)\nreg_1.predict(X, base_margin=m)\n```\n\n----------------------------------------\n\nTITLE: Saving scikit-learn related attributes in XGBoost\nDESCRIPTION: New method to save scikit-learn related attributes for XGBClassifier, XGBRegressor, and XGBRanker objects without using Python pickle, improving interoperability.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/NEWS.md#2025-04-19_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n# Example (pseudo-code)\nxgb_model.save_sklearn_attributes('sklearn_attrs.json')\n```\n\n----------------------------------------\n\nTITLE: Using XGBoost Native Interface with Categorical Data\nDESCRIPTION: Creates a DMatrix with categorical data support and trains a model using XGBoost's native interface. This approach provides more flexibility than the scikit-learn interface, including specifying categorical feature handling parameters.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/categorical.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# X is a dataframe we created in previous snippet\nXy = xgb.DMatrix(X, y, enable_categorical=True)\nbooster = xgb.train({\"tree_method\": \"hist\", \"max_cat_to_onehot\": 5}, Xy)\n# Must use JSON for serialization, otherwise the information is lost\nbooster.save_model(\"categorical-model.json\")\n```\n\n----------------------------------------\n\nTITLE: Plotting Feature Importance and Trees in XGBoost Python API\nDESCRIPTION: Demonstrates how to use XGBoost's plotting module to visualize feature importance and decision trees. It requires matplotlib and graphviz libraries.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/python/python_intro.rst#2025-04-19_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nxgb.plot_importance(bst)\n```\n\nLANGUAGE: python\nCODE:\n```\nxgb.plot_tree(bst, num_trees=2)\n```\n\nLANGUAGE: python\nCODE:\n```\nxgb.to_graphviz(bst, num_trees=2)\n```\n\n----------------------------------------\n\nTITLE: Loading Scipy Sparse Matrix into XGBoost DMatrix\nDESCRIPTION: Converting a scipy sparse matrix into XGBoost DMatrix format.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/python/python_intro.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncsr = scipy.sparse.csr_matrix((dat, (row, col)))\ndtrain = xgb.DMatrix(csr)\n```\n\n----------------------------------------\n\nTITLE: Installing XGBoost Python Package using pip\nDESCRIPTION: This command installs the stable version of XGBoost from PyPI using pip. It's the recommended method for most users to install XGBoost.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/python-package/README.rst#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install xgboost\n```\n\n----------------------------------------\n\nTITLE: Saving XGBoost model using JSON in Python\nDESCRIPTION: Example of how to save an XGBoost model using the new JSON serialization format in Python. This method improves interoperability and compatibility across different platforms.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/NEWS.md#2025-04-19_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nbooster.save_model('model.json')\n```\n\n----------------------------------------\n\nTITLE: Loading Data from LIBSVM File in XGBoost4J (Java)\nDESCRIPTION: This code demonstrates how to create a DMatrix object from a LIBSVM format text file in XGBoost4J.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/java_intro.rst#2025-04-19_snippet_1\n\nLANGUAGE: java\nCODE:\n```\nDMatrix dmat = new DMatrix(\"train.svm.txt\");\n```\n\n----------------------------------------\n\nTITLE: Configuring XGBoost for Multi-Output Trees with Vector Leaf\nDESCRIPTION: Shows how to configure XGBoost to use the multi-output tree strategy introduced in version 2.0. This approach builds trees with vector leaves where each leaf size equals the number of targets, as opposed to the default of building one model per target.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/multioutput.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclf = xgb.XGBClassifier(tree_method=\"hist\", multi_strategy=\"multi_output_tree\")\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost Dirichlet Model in Python\nDESCRIPTION: This snippet demonstrates how to train an XGBoost model with a Dirichlet objective function in Python. It sets up the model parameters, trains the model, and makes predictions using softmax.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/advanced_custom_obj.rst#2025-04-19_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nbooster = xgb.train(\n    params={\n        \"tree_method\": \"hist\",\n        \"num_target\": Y.shape[1],\n        \"base_score\": 0,\n        \"disable_default_eval_metric\": True,\n        \"max_depth\": 3,\n        \"seed\": 123,\n    },\n    dtrain=dtrain,\n    num_boost_round=10,\n    obj=dirichlet_xgb_objective,\n    evals=[(dtrain, \"Train\")],\n    evals_result=results,\n    custom_metric=dirichlet_eval_metric,\n)\nyhat = softmax(booster.inplace_predict(X), axis=1)\n```\n\n----------------------------------------\n\nTITLE: Training XGBClassifier with Concrete ML\nDESCRIPTION: Shows how to train the XGBClassifier using the fit() method, similar to scikit-learn models. This step trains the model on the provided training data.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/privacy_preserving.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclassifier.fit(X_train, y_train)\n```\n\n----------------------------------------\n\nTITLE: Using Custom Objective and Metric in XGBoost Training\nDESCRIPTION: This snippet shows how to use both the custom squared log error objective and the custom RMSLE metric in XGBoost training.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/custom_metric_obj.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nxgb.train({'tree_method': 'hist', 'seed': 1994,\n           'disable_default_eval_metric': 1},\n          dtrain=dtrain,\n          num_boost_round=10,\n          obj=squared_log,\n          custom_metric=rmsle,\n          evals=[(dtrain, 'dtrain'), (dtest, 'dtest')],\n          evals_result=results)\n```\n\n----------------------------------------\n\nTITLE: Computing SHAP Values for Categorical Features\nDESCRIPTION: Demonstrates how to compute SHAP (SHapley Additive exPlanations) values for a model with categorical features using XGBoost's native interface. Also shows how to check which features are categorical in the model.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/categorical.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nSHAP = booster.predict(Xy, pred_interactions=True)\n\n# categorical features are listed as \"c\"\nprint(booster.feature_types)\n```\n\n----------------------------------------\n\nTITLE: Disabling Automatic Intercept Estimation in XGBoost\nDESCRIPTION: This snippet demonstrates how to disable XGBoost's automatic estimation of the intercept (base_score) by manually setting it to a fixed value of 0.5.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/intercept.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport xgboost as xgb\n\nreg = xgb.XGBRegressor()\nreg.set_params(base_score=0.5)\n```\n\n----------------------------------------\n\nTITLE: Optimizing Intercepts for Dirichlet Model in Python\nDESCRIPTION: This snippet defines a function to find optimal intercepts for the Dirichlet model using SciPy's minimize function. It uses the previously defined Dirichlet likelihood and gradient functions.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/advanced_custom_obj.rst#2025-04-19_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\nfrom scipy.optimize import minimize\n\ndef get_optimal_intercepts(Y: np.ndarray) -> np.ndarray:\n    k = Y.shape[1]\n    res = minimize(\n        fun=lambda pred: dirichlet_fun(\n            np.broadcast_to(pred, (Y.shape[0], k)),\n            Y\n        ),\n        x0=np.zeros(k),\n        jac=lambda pred: dirichlet_grad(\n            np.broadcast_to(pred, (Y.shape[0], k)),\n            Y\n        ).sum(axis=0)\n    )\n    return res[\"x\"]\nintercepts = get_optimal_intercepts(Y)\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost Model in Scala\nDESCRIPTION: Shows how to train an XGBoost classification model by fitting the classifier on input data.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost4j_spark_tutorial.rst#2025-04-19_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\nval xgbClassificationModel = xgbClassifier.fit(xgbInput)\n```\n\n----------------------------------------\n\nTITLE: Creating DMatrix from 2D Matrix using XGBoost C API\nDESCRIPTION: Examples of creating DMatrix objects from 1D and 2D arrays using XGDMatrixCreateFromMat function.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/c_api_tutorial.rst#2025-04-19_snippet_7\n\nLANGUAGE: c\nCODE:\n```\n// 1D matrix\nconst int data1[] = { 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0 };\n\n// 2D matrix\nconst int ROWS = 6, COLS = 3;\nconst int data2[ROWS][COLS] = { {1, 2, 3}, {2, 4, 6}, {3, -1, 9}, {4, 8, -1}, {2, 5, 1}, {0, 1, 5} };\nDMatrixHandle dmatrix1, dmatrix2;\n// Pass the matrix, no of rows & columns contained in the matrix variable\n// here '0' represents the missing value in the matrix dataset\n// dmatrix variable will contain the created DMatrix using it\nsafe_xgboost(XGDMatrixCreateFromMat(data1, 1, 50, 0, &dmatrix));\n// here -1 represents the missing value in the matrix dataset\nsafe_xgboost(XGDMatrixCreateFromMat(data2, ROWS, COLS, -1, &dmatrix2));\n```\n\n----------------------------------------\n\nTITLE: Visualizing Decision Tree with Graphviz\nDESCRIPTION: Creates a graphical visualization of a decision tree showing feature interactions using Graphviz. The diagram illustrates how features x1, x7, and x10 interact in the tree structure.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/feature_interaction_constraint.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom graphviz import Source\nsource = r\"\"\"\n  digraph feature_interaction_illustration1 {\n    graph [fontname = \"helvetica\"];\n    node [fontname = \"helvetica\"];\n    edge [fontname = \"helvetica\"];\n    0 [label=<x<SUB><FONT POINT-SIZE=\"11\">10</FONT></SUB> &lt; -1.5 ?>, shape=box, color=red, fontcolor=red];\n    1 [label=<x<SUB><FONT POINT-SIZE=\"11\">2</FONT></SUB> &lt; 2 ?>, shape=box];\n    2 [label=<x<SUB><FONT POINT-SIZE=\"11\">7</FONT></SUB> &lt; 0.3 ?>, shape=box, color=red, fontcolor=red];\n    3 [label=\"...\", shape=none];\n    4 [label=\"...\", shape=none];\n    5 [label=<x<SUB><FONT POINT-SIZE=\"11\">1</FONT></SUB> &lt; 0.5 ?>, shape=box, color=red, fontcolor=red];\n    6 [label=\"...\", shape=none];\n    7 [label=\"...\", shape=none];\n    8 [label=\"Predict +1.3\", color=red, fontcolor=red];\n    0 -> 1 [labeldistance=2.0, labelangle=45, headlabel=\"Yes/Missing           \"];\n    0 -> 2 [labeldistance=2.0, labelangle=-45, headlabel=\"No\", color=red, fontcolor=red];\n    1 -> 3 [labeldistance=2.0, labelangle=45, headlabel=\"Yes\"];\n    1 -> 4 [labeldistance=2.0, labelangle=-45, headlabel=\"             No/Missing\"];\n    2 -> 5 [labeldistance=2.0, labelangle=-45, headlabel=\"Yes\", color=red, fontcolor=red];\n    2 -> 6 [labeldistance=2.0, labelangle=-45, headlabel=\"           No/Missing\"];\n    5 -> 7;\n    5 -> 8 [color=red];\n  }\n\"\"\"\nSource(source, format='png').render('../_static/feature_interaction_illustration1', view=False)\nSource(source, format='svg').render('../_static/feature_interaction_illustration1', view=False)\n```\n\n----------------------------------------\n\nTITLE: Loading Iris Dataset with Spark DataFrame Schema\nDESCRIPTION: Demonstrates how to create a SparkSession and define a schema to load CSV data into a Spark DataFrame. The schema specifies the data types for the Iris dataset features and class label.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost4j_spark_tutorial.rst#2025-04-19_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.types.{DoubleType, StringType, StructField, StructType}\n\nval spark = SparkSession.builder().getOrCreate()\nval schema = new StructType(Array(\n  StructField(\"sepal length\", DoubleType, true),\n  StructField(\"sepal width\", DoubleType, true),\n  StructField(\"petal length\", DoubleType, true),\n  StructField(\"petal width\", DoubleType, true),\n  StructField(\"class\", StringType, true)))\nval rawInput = spark.read.schema(schema).csv(\"input_path\")\n```\n\n----------------------------------------\n\nTITLE: Slicing XGBoost Tree Models in Python\nDESCRIPTION: This snippet demonstrates how to create a boosted random forest using XGBoost in Python, slice the model, and access individual tree layers. It uses sklearn for dataset generation and XGBoost for model training.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/slicing_model.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport xgboost as xgb\nfrom sklearn.datasets import make_classification\nnum_classes = 3\nX, y = make_classification(n_samples=1000, n_informative=5,\n                               n_classes=num_classes)\ndtrain = xgb.DMatrix(data=X, label=y)\nnum_parallel_tree = 4\nnum_boost_round = 16\n# total number of built trees is num_parallel_tree * num_classes * num_boost_round\n\n# We build a boosted random forest for classification here.\nbooster = xgb.train({\n    'num_parallel_tree': 4, 'subsample': 0.5, 'num_class': 3},\n                    num_boost_round=num_boost_round, dtrain=dtrain)\n\n# This is the sliced model, containing [3, 7) forests\n# step is also supported with some limitations like negative step is invalid.\nsliced: xgb.Booster = booster[3:7]\n\n# Access individual tree layer\ntrees = [_ for _ in booster]\nassert len(trees) == num_boost_round\n```\n\n----------------------------------------\n\nTITLE: Using XGBoost Scikit-Learn Interface Asynchronously\nDESCRIPTION: Shows how to use XGBoost's Scikit-Learn interface asynchronously with Dask. It includes creating a regressor, fitting it to data, and making predictions.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/dask.rst#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nasync with dask.distributed.Client(scheduler_address, asynchronous=True) as client:\n    X, y = generate_array()\n    regressor = await dxgb.DaskXGBRegressor(verbosity=1, n_estimators=2)\n    regressor.set_params(tree_method='hist')  # trivial method, synchronous operation\n    regressor.client = client  #  accessing attribute, synchronous operation\n    regressor = await regressor.fit(X, y, eval_set=[(X, y)])\n    prediction = await regressor.predict(X)\n\n    # Use `client.compute` instead of the `compute` method from dask collection\n    print(await client.compute(prediction))\n```\n\n----------------------------------------\n\nTITLE: Creating Basic XGBoost Classifier\nDESCRIPTION: Demonstrates the basic setup of XGBoostClassifier with essential parameters for multiclass classification on the Iris dataset.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost4j_spark_tutorial.rst#2025-04-19_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\nimport ml.dmlc.xgboost4j.scala.spark.XGBoostClassifier\nval xgbParam = Map(\"eta\" -> 0.1f,\n      \"max_depth\" -> 2,\n      \"objective\" -> \"multi:softprob\",\n      \"num_class\" -> 3)\nval xgbClassifier = new XGBoostClassifier(xgbParam).\n      setNumRound(100).\n      setNumWorkers(2).\n      setFeaturesCol(\"features\").\n      setLabelCol(\"classIndex\")\n```\n\n----------------------------------------\n\nTITLE: Using Multiple Validation Datasets in XGBoost4J-Spark\nDESCRIPTION: Example of how to use multiple evaluation datasets when training an XGBoost model in Spark. This allows tracking model performance on multiple validation sets during training.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/NEWS.md#2025-04-19_snippet_24\n\nLANGUAGE: Scala\nCODE:\n```\nimport ml.dmlc.xgboost4j.scala.spark.XGBoostClassifier\nimport org.apache.spark.sql.DataFrame\n\nval xgbClassifier = new XGBoostClassifier()\n  .setFeaturesCol(\"features\")\n  .setLabelCol(\"labels\")\n\nval evalSets = Map(\n  \"eval1\" -> dataset1,\n  \"eval2\" -> dataset2\n)\nxgbClassifier.setEvalSets(evalSets)\n\nval model = xgbClassifier.fit(trainSet)\n```\n\n----------------------------------------\n\nTITLE: Generating Model Dump in XGBoost4J (Java)\nDESCRIPTION: This snippet demonstrates how to generate a model dump with and without a feature map using XGBoost4J in Java.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/java_intro.rst#2025-04-19_snippet_10\n\nLANGUAGE: java\nCODE:\n```\n// dump without feature map\nString[] model_dump = booster.getModelDump(null, false);\n// dump with feature map\nString[] model_dump_with_feature_map = booster.getModelDump(\"featureMap.txt\", false);\n```\n\n----------------------------------------\n\nTITLE: Implementing Dirichlet XGBoost Objective Function in Python\nDESCRIPTION: Custom objective function for XGBoost that uses Dirichlet regression. It returns the gradient and a diagonal upper bound of the expected Hessian in the format required by XGBoost.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/advanced_custom_obj.rst#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport xgboost as xgb\nfrom typing import Tuple\n\ndef dirichlet_xgb_objective(\n    pred: np.ndarray, dtrain: xgb.DMatrix\n) -> Tuple[np.ndarray, np.ndarray]:\n    Y = dtrain.get_label().reshape(pred.shape)\n    return (\n        dirichlet_grad(pred, Y),\n        dirichlet_diag_upper_bound_expected_hess(pred, Y),\n    )\n```\n\n----------------------------------------\n\nTITLE: Loading XGBoost Model from File in C\nDESCRIPTION: This code shows how to load an XGBoost model from a file using XGBoosterLoadModel. It creates a new booster handle, sets model parameters, and then loads the model from the specified file path.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/c_api_tutorial.rst#2025-04-19_snippet_12\n\nLANGUAGE: c\nCODE:\n```\nBoosterHandle booster;\nconst char *model_path = \"/path/of/model.json\";\n\n// create booster handle first\nsafe_xgboost(XGBoosterCreate(NULL, 0, &booster));\n\n// set the model parameters here\n\n// load model\nsafe_xgboost(XGBoosterLoadModel(booster, model_path));\n\n// predict the model here\n```\n\n----------------------------------------\n\nTITLE: Creating ExtMemQuantileDMatrix Objects for External Memory Training in Python\nDESCRIPTION: This code snippet demonstrates how to create training and validation ExtMemQuantileDMatrix objects for external memory training in XGBoost. The training matrix is created with a specified number of bins, and the validation matrix references the training matrix to ensure consistent binning.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/external_memory.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nXy_train = xgboost.ExtMemQuantileDMatrix(it_train, max_bin=n_bins)\nXy_valid = xgboost.ExtMemQuantileDMatrix(it_valid, max_bin=n_bins, ref=Xy_train)\n```\n\n----------------------------------------\n\nTITLE: Building XGBoost Shared Library with CMake\nDESCRIPTION: Commands for building the XGBoost shared library using CMake and Ninja. This creates the core library used by different language bindings.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/build.rst#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd xgboost\ncmake -B build -S . -DCMAKE_BUILD_TYPE=RelWithDebInfo -GNinja\ncd build && ninja\n```\n\n----------------------------------------\n\nTITLE: Using XGBoost with Dask Asynchronously\nDESCRIPTION: Demonstrates how to use XGBoost's Dask interface asynchronously with asyncio. It includes creating an asynchronous client, training a model, and making predictions.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/dask.rst#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nasync with dask.distributed.Client(scheduler_address, asynchronous=True) as client:\n    X, y = generate_array()\n    m = await dxgb.DaskDMatrix(client, X, y)\n    output = await dxgb.train(client, {}, dtrain=m)\n\n    with_m = await dxgb.predict(client, output, m)\n    with_X = await dxgb.predict(client, output, X)\n    inplace = await dxgb.inplace_predict(client, output, X)\n\n    # Use ``client.compute`` instead of the ``compute`` method from dask collection\n    print(await client.compute(with_m))\n```\n\n----------------------------------------\n\nTITLE: Using XGBoost Scikit-Learn Interface with Dask\nDESCRIPTION: Illustrates how to use the XGBoost Scikit-Learn estimator interface with Dask. This higher-level interface provides an easier way to train and predict using distributed data collections.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/dask.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom distributed import LocalCluster, Client\nfrom xgboost import dask as dxgb\n\n\ndef main(client: Client) -> None:\n    X, y = load_data()\n    clf = dxgb.DaskXGBClassifier(n_estimators=100, tree_method=\"hist\")\n    clf.client = client  # assign the client\n    clf.fit(X, y, eval_set=[(X, y)])\n    proba = clf.predict_proba(X)\n\n\nif __name__ == \"__main__\":\n    with LocalCluster() as cluster:\n        with Client(cluster) as client:\n            main(client)\n```\n\n----------------------------------------\n\nTITLE: Running Predictions with XGBoost and Dask\nDESCRIPTION: Shows different ways to run predictions using XGBoost with Dask, including using DaskDMatrix, direct prediction on Dask collections, and inplace prediction. It also demonstrates how to handle different input and output types.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/dask.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprediction = dxgb.predict(client, output, dtrain)\n# Or equivalently, pass ``output['booster']``:\nprediction = dxgb.predict(client, output['booster'], dtrain)\n\n# Eliminating DaskDMatrix construction\nprediction = dxgb.predict(client, output, X)\n# Use inplace version.\nprediction = dxgb.inplace_predict(client, output, X)\n```\n\n----------------------------------------\n\nTITLE: Configuring XGBoost Classifier with Missing Values\nDESCRIPTION: Shows how to configure XGBoostClassifier with custom parameters including handling of missing values. Sets up classification parameters for the Iris dataset multiclass problem.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost4j_spark_tutorial.rst#2025-04-19_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\nimport ml.dmlc.xgboost4j.scala.spark.XGBoostClassifier\nval xgbParam = Map(\"eta\" -> 0.1f,\n      \"missing\" -> -999,\n      \"objective\" -> \"multi:softprob\",\n      \"num_class\" -> 3,\n      \"num_round\" -> 100,\n      \"num_workers\" -> 2)\nval xgbClassifier = new XGBoostClassifier(xgbParam).\n      setFeaturesCol(\"features\").\n      setLabelCol(\"classIndex\")\n```\n\n----------------------------------------\n\nTITLE: Creating Diagonal Upper Bound for Expected Hessian in Python\nDESCRIPTION: Function that creates a diagonal upper bound for the expected Hessian, making it compatible with XGBoost's requirement for a diagonal Hessian matrix. This uses the concept of diagonally dominant matrices.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/advanced_custom_obj.rst#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef dirichlet_diag_upper_bound_expected_hess(\n    pred: np.ndarray, Y: np.ndarray\n) -> np.ndarray:\n    Ehess = dirichlet_expected_hess(pred)\n    diag_bound_Ehess = np.empty((pred.shape[0], Y.shape[1]))\n    for row in range(pred.shape[0]):\n        diag_bound_Ehess[row, :] = np.abs(Ehess[row, :, :]).sum(axis=1)\n    return diag_bound_Ehess\n```\n\n----------------------------------------\n\nTITLE: Installing XGBoost Python Package\nDESCRIPTION: Install XGBoost Python package using pip. Requires Pip 21.3+ and supports Linux, Windows, and MacOS platforms.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/install.rst#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install xgboost\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost Model with Dirichlet Objective in Python\nDESCRIPTION: Setting up the XGBoost training process using the custom Dirichlet objective function and evaluation metric with the Arctic Lake dataset.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/advanced_custom_obj.rst#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict, List\n\ndtrain = xgb.DMatrix(X, label=Y)\nresults: Dict[str, Dict[str, List[float]]] = {}\nbooster = xgb.train(\n    params={\n        \"tree_method\": \"hist\",\n    }\n```\n\n----------------------------------------\n\nTITLE: Installing XGBoost R Package with GPU Support\nDESCRIPTION: Command to install the pre-built XGBoost R package with GPU support, representing a new feature that simplifies installation without requiring source compilation.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/NEWS.md#2025-04-19_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nR CMD INSTALL ./xgboost_r_gpu_linux.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost Model With Feature Interaction Constraints\nDESCRIPTION: XGBoost model training with feature interaction constraints specified. Demonstrates how to add constraints that limit which features can interact with each other.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/feature_interaction_constraint.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nparams_constrained = params.copy()\n# Use nested list to define feature interaction constraints\nparams_constrained['interaction_constraints'] = '[[0, 2], [1, 3, 4], [5, 6]]'\n# Features 0 and 2 are allowed to interact with each other but with no other feature\n# Features 1, 3, 4 are allowed to interact with one another but with no other feature\n# Features 5 and 6 are allowed to interact with each other but with no other feature\n\nmodel_with_constraints = xgb.train(params_constrained, dtrain,\n                                   num_boost_round = 1000, evals = evallist,\n                                   early_stopping_rounds = 10)\n```\n\n----------------------------------------\n\nTITLE: Evaluation and Early Stopping with XGBoost and Dask\nDESCRIPTION: Demonstrates how to use validation sets for evaluation and early stopping in XGBoost with Dask. It includes creating distributed data, training a model with early stopping, and accessing evaluation history.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/dask.rst#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport dask.array as da\nfrom xgboost import dask as dxgb\n\nnum_rows = 1e6\nnum_features = 100\nnum_partitions = 10\nrows_per_chunk = num_rows / num_partitions\n\ndata = da.random.random(\n    size=(num_rows, num_features),\n    chunks=(rows_per_chunk, num_features)\n)\n\nlabels = da.random.random(\n    size=(num_rows, 1),\n    chunks=(rows_per_chunk, 1)\n)\n\nX_eval = da.random.random(\n    size=(num_rows, num_features),\n    chunks=(rows_per_chunk, num_features)\n)\n\ny_eval = da.random.random(\n    size=(num_rows, 1),\n    chunks=(rows_per_chunk, 1)\n)\n\ndtrain = dxgb.DaskDMatrix(\n    client=client,\n    data=data,\n    label=labels\n)\n\ndvalid = dxgb.DaskDMatrix(\n    client=client,\n    data=X_eval,\n    label=y_eval\n)\n\nresult = dxgb.train(\n    client=client,\n    params={\n        \"objective\": \"reg:squarederror\",\n    },\n    dtrain=dtrain,\n    num_boost_round=10,\n    evals=[(dvalid, \"valid1\")],\n    early_stopping_rounds=3\n)\n\nprint(result[\"history\"])\n# {'valid1': OrderedDict([('rmse', [0.28857, 0.28858, 0.288592, 0.288598])])}\n\nbooster = result[\"booster\"]\nprint(booster.best_iteration)\nbest_model = booster[: booster.best_iteration]\n```\n\n----------------------------------------\n\nTITLE: XGBoost Python Package Configuration Options\nDESCRIPTION: Available build configuration settings for the XGBoost Python package installation.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/build.rst#2025-04-19_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@dataclasses.dataclass\nclass BuildConfig:\n    \"\"\"Build configurations used for compiling the core library, for python package.\"\"\"\n\n    use_cuda: bool = False\n    use_nccl: bool = False\n    hide_cxx_symbols: bool = True\n    use_openmp: bool = True\n    use_system_libxgboost: bool = False\n    use_native_compute: bool = False\n    # Allow build even compiler doesn't fully support used C++ standard.\n    allow_unsupported_compiler: bool = False\n\n    plugin_federated: bool = False\n\n    # There are more build configurations in xgboost that can be controlled. But as a\n    # Python package, we only expose the commonly used ones. Users can use CMake\n    # directly for more fine-grained control.\n```\n\n----------------------------------------\n\nTITLE: Setting Sample Weights in XGBoost DMatrix\nDESCRIPTION: Adding sample weights when creating DMatrix for weighted training.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/python/python_intro.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nw = np.random.rand(5, 1)\ndtrain = xgb.DMatrix(data, label=label, missing=np.NaN, weight=w)\n```\n\n----------------------------------------\n\nTITLE: Configuring External Memory for GPU-based XGBoost in Spark\nDESCRIPTION: Sets up XGBoost classifier to use experimental external memory support with GPU acceleration, allowing training on datasets that don't fit in memory.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost4j_spark_tutorial.rst#2025-04-19_snippet_21\n\nLANGUAGE: scala\nCODE:\n```\nval xgbClassifier = new XGBoostClassifier(xgbParam)\n    .setFeaturesCol(featuresNames)\n    .setLabelCol(labelName)\n    .setUseExternalMemory(true)\n    .setDevice(\"cuda\")  // CPU is not yet supported\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost Dirichlet Model with Intercepts in Python\nDESCRIPTION: This snippet demonstrates how to train an XGBoost model with a Dirichlet objective function and optimized intercepts in Python. It sets up the base margin, trains the model, and makes predictions using softmax.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/advanced_custom_obj.rst#2025-04-19_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\nbase_margin = np.broadcast_to(intercepts, Y.shape)\ndtrain_w_intercept = xgb.DMatrix(X, label=Y, base_margin=base_margin)\nresults: Dict[str, Dict[str, List[float]]] = {}\nbooster = xgb.train(\n    params={\n        \"tree_method\": \"hist\",\n        \"num_target\": Y.shape[1],\n        \"base_score\": 0,\n        \"disable_default_eval_metric\": True,\n        \"max_depth\": 3,\n        \"seed\": 123,\n    },\n    dtrain=dtrain_w_intercept,\n    num_boost_round=10,\n    obj=dirichlet_xgb_objective,\n    evals=[(dtrain, \"Train\")],\n    evals_result=results,\n    custom_metric=dirichlet_eval_metric,\n)\nyhat = softmax(\n    booster.predict(\n        xgb.DMatrix(X, base_margin=base_margin)\n    ),\n    axis=1\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring XGBoostClassifier with GPU Support\nDESCRIPTION: Defines an XGBoostClassifier with parameters for multi-class classification on GPU. The configuration specifies the objective, number of classes, training rounds, and sets the device to 'cuda' for GPU acceleration.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost4j_spark_gpu_tutorial.rst#2025-04-19_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nimport ml.dmlc.xgboost4j.scala.spark.XGBoostClassifier\nval xgbParam = Map(\n    \"objective\" -> \"multi:softprob\",\n    \"num_class\" -> 3,\n    \"num_round\" -> 100,\n    \"device\" -> \"cuda\",\n    \"num_workers\" -> 1)\n\nval featuresNames = schema.fieldNames.filter(name => name != labelName)\n\nval xgbClassifier = new XGBoostClassifier(xgbParam)\n    .setFeaturesCol(featuresNames)\n    .setLabelCol(labelName)\n```\n\n----------------------------------------\n\nTITLE: XGBoost External Memory URI Format for CSV\nDESCRIPTION: Shows the URI format for loading CSV files with external memory support. Includes parameters for specifying the format and label column.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/external_memory.rst#2025-04-19_snippet_6\n\nLANGUAGE: text\nCODE:\n```\nfilename.csv?format=csv&label_column=0#cacheprefix\n```\n\n----------------------------------------\n\nTITLE: Installing XGBoost on Mac OSX via Homebrew\nDESCRIPTION: Simple two-command installation process for XGBoost on Mac OSX using Homebrew package manager. This method ensures OpenMP support for multi-core CPU utilization.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/NEWS.md#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nbrew install libomp\npip install xgboost\n```\n\n----------------------------------------\n\nTITLE: Building XGBoost with SYCL Plugin Support\nDESCRIPTION: Commands to build XGBoost with SYCL plugin support from the main project directory. The first command configures the build with SYCL plugin enabled, and the second command builds the project.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/plugin/sycl/README.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ cmake -B build -S . -DPLUGIN_SYCL=ON\n$ cmake --build build -j\n```\n\n----------------------------------------\n\nTITLE: Using Privacy-Preserving Prediction with Concrete ML\nDESCRIPTION: Example of using Concrete ML, a third-party library by Zama that enables prediction directly over encrypted data using Fully Homomorphic Encryption. The code demonstrates how to train an XGBoost model, make predictions in the clear, and then perform the same predictions on encrypted data.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/prediction.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom concrete.ml.sklearn import XGBClassifier\n\nx, y = make_classification(n_samples=100, class_sep=2, n_features=30, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(\n    x, y, test_size=10, random_state=42\n)\n\n# Train in the clear and quantize the weights\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\n\n# Simulate the predictions in the clear\ny_pred_clear = model.predict(X_test)\n\n# Compile in FHE\nmodel.compile(X_train)\n\n# Generate keys\nmodel.fhe_circuit.keygen()\n\n# Run the inference on encrypted inputs!\ny_pred_fhe = model.predict(X_test, fhe=\"execute\")\n\nprint(\"In clear  :\", y_pred_clear)\nprint(\"In FHE    :\", y_pred_fhe)\nprint(f\"Similarity: {int((y_pred_fhe == y_pred_clear).mean()*100)}%\")\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost Dirichlet Model with Intercepts in R\nDESCRIPTION: This snippet shows how to train an XGBoost model with a Dirichlet objective function and optimized intercepts in R. It sets up the base margin, trains the model, and makes predictions using softmax.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/advanced_custom_obj.rst#2025-04-19_snippet_22\n\nLANGUAGE: R\nCODE:\n```\nbase.margin <- rep(intercepts, nrow(y)) |> matrix(nrow=nrow(y), byrow=T)\ndtrain <- xgb.DMatrix(x, y, base_margin=base.margin)\nbooster <- xgb.train(\n    params = list(\n        tree_method=\"hist\",\n        num_target=ncol(y),\n        base_score=0,\n        disable_default_eval_metric=TRUE,\n        max_depth=3,\n        seed=123\n    ),\n    data = dtrain,\n    nrounds = 10,\n    obj = dirichlet.xgb.objective,\n    evals = list(Train=dtrain),\n    eval_metric = dirichlet.eval.metric\n)\nraw.pred <- predict(\n    booster,\n    x,\n    base_margin=base.margin,\n    reshape=TRUE\n)\nyhat <- apply(raw.pred, 1, softmax) |> t()\n```\n\n----------------------------------------\n\nTITLE: Installing XGBoost on Mac OSX with pip\nDESCRIPTION: Two commands to install XGBoost Python package on Mac OSX. First install the OpenMP library via Homebrew, then install XGBoost via pip.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/NEWS.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbrew install libomp\npip install xgboost\n```\n\n----------------------------------------\n\nTITLE: Evaluating XGBoost Model Accuracy in Spark ML\nDESCRIPTION: Transforms the test dataset using the fitted pipeline model and evaluates the multiclass classification accuracy using Spark's MulticlassClassificationEvaluator.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost4j_spark_tutorial.rst#2025-04-19_snippet_15\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n\nval prediction = model.transform(test)\nval evaluator = new MulticlassClassificationEvaluator()\nval accuracy = evaluator.evaluate(prediction)\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with Trained XGBoost Model\nDESCRIPTION: Uses the trained XGBoost classification model to make predictions on the test dataset. The transform method generates a new DataFrame with columns for raw predictions, probabilities, and the final prediction label.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost4j_spark_gpu_tutorial.rst#2025-04-19_snippet_5\n\nLANGUAGE: scala\nCODE:\n```\nval xgbClassificationModel = xgbClassifier.fit(train)\nval results = xgbClassificationModel.transform(test)\nresults.show()\n```\n\n----------------------------------------\n\nTITLE: Creating Dirichlet Evaluation Metric for XGBoost in R\nDESCRIPTION: R implementation of a custom evaluation metric for XGBoost based on Dirichlet log-likelihood for tracking model performance.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/advanced_custom_obj.rst#2025-04-19_snippet_13\n\nLANGUAGE: r\nCODE:\n```\ndirichlet.eval.metric <- function(pred, dtrain) {\n    y <- getinfo(dtrain, \"label\")\n    ll <- dirichlet.fun(pred, y)\n    return(\n        list(\n            metric = \"dirichlet_ll\",\n            value = ll\n        )\n    )\n}\n```\n\n----------------------------------------\n\nTITLE: Optimizing Consecutive Predictions with Dask Futures\nDESCRIPTION: Demonstrates how to optimize running consecutive predictions using Dask futures. This approach can improve performance when dealing with multiple datasets.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/dask.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndataset = [X_0, X_1, X_2]\nbooster_f = client.scatter(booster, broadcast=True)\nfutures = []\nfor X in dataset:\n    # Here we pass in a future instead of concrete booster\n    shap_f = dxgb.predict(client, booster_f, X, pred_contribs=True)\n    futures.append(shap_f)\n\nresults = client.gather(futures)\n```\n\n----------------------------------------\n\nTITLE: Implementing XGBoost in Scala with XGBoost4j\nDESCRIPTION: This Scala example shows how to use XGBoost with the XGBoost4j library. It loads training data from a file, defines model parameters including learning rate and tree depth, trains the model for a specific number of iterations, makes predictions, and saves the model to a file.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/get_started.rst#2025-04-19_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\nimport ml.dmlc.xgboost4j.scala.DMatrix\nimport ml.dmlc.xgboost4j.scala.XGBoost\n\nobject XGBoostScalaExample {\n  def main(args: Array[String]) {\n    // read trainining data, available at xgboost/demo/data\n    val trainData =\n      new DMatrix(\"/path/to/agaricus.txt.train\")\n    // define parameters\n    val paramMap = List(\n      \"eta\" -> 0.1,\n      \"max_depth\" -> 2,\n      \"objective\" -> \"binary:logistic\").toMap\n    // number of iterations\n    val round = 2\n    // train the model\n    val model = XGBoost.train(trainData, paramMap, round)\n    // run prediction\n    val predTrain = model.predict(trainData)\n    // save model to the file.\n    model.saveModel(\"/local/path/to/model\")\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Dirichlet Evaluation Metric for XGBoost in Python\nDESCRIPTION: Custom evaluation metric for XGBoost based on Dirichlet log-likelihood, allowing for tracking the performance of the model during training.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/advanced_custom_obj.rst#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef dirichlet_eval_metric(\n    pred: np.ndarray, dtrain: xgb.DMatrix\n) -> Tuple[str, float]:\n    Y = dtrain.get_label().reshape(pred.shape)\n    return \"dirichlet_ll\", dirichlet_fun(pred, Y)\n```\n\n----------------------------------------\n\nTITLE: Creating Python Environment with XGBoost GPU Dependencies using Conda\nDESCRIPTION: Shell commands for creating a Conda environment with XGBoost and CUDA dependencies, and packaging it for distribution to Spark workers.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/spark_estimator.rst#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nconda create -y -n xgboost_env -c conda-forge conda-pack python=3.9\nconda activate xgboost_env\n# use conda when the supported version of xgboost (1.7) is released on conda-forge\npip install xgboost\nconda install cudf pyarrow pandas -c rapids -c nvidia -c conda-forge\nconda pack -f -o xgboost_env.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Building XGBoost with RMM Plugin Support\nDESCRIPTION: CMake commands to build XGBoost with RMM plugin enabled. Requires CUDA and NCCL support.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/rmm_plugin/README.rst#2025-04-19_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ncmake -B build -S . -DUSE_CUDA=ON -DUSE_NCCL=ON -DPLUGIN_RMM=ON\ncmake --build build -j$(nproc)\n```\n\n----------------------------------------\n\nTITLE: Batch Prediction with XGBoost in Scala\nDESCRIPTION: Demonstrates batch prediction using a trained XGBoost model on a test dataset, returning predictions with margins, probabilities and labels.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost4j_spark_tutorial.rst#2025-04-19_snippet_7\n\nLANGUAGE: scala\nCODE:\n```\nval xgbClassificationModel = xgbClassifier.fit(xgbInput)\nval results = xgbClassificationModel.transform(testSet)\n```\n\n----------------------------------------\n\nTITLE: Assembling Feature Vector for XGBoost\nDESCRIPTION: Demonstrates using VectorAssembler to combine multiple feature columns into a single vector column required by XGBoost's input format.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost4j_spark_tutorial.rst#2025-04-19_snippet_2\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.ml.feature.VectorAssembler\nval vectorAssembler = new VectorAssembler().\n  setInputCols(Array(\"sepal length\", \"sepal width\", \"petal length\", \"petal width\")).\n  setOutputCol(\"features\")\nval xgbInput = vectorAssembler.transform(labelTransformed).select(\"features\", \"classIndex\")\n```\n\n----------------------------------------\n\nTITLE: Training an XGBoost Classification Model on Spark\nDESCRIPTION: Trains an XGBoost classification model by fitting the XGBoostClassifier on the training dataset. This operation performs the distributed training process on GPUs.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost4j_spark_gpu_tutorial.rst#2025-04-19_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\nval xgbClassificationModel = xgbClassifier.fit(train)\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Class Error Metric with Transform for XGBoost\nDESCRIPTION: This function implements a custom multi-class error metric that handles the transformation of raw predictions. It's used when a custom objective is also provided.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/custom_metric_obj.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef merror_with_transform(predt: np.ndarray, dtrain: xgb.DMatrix):\n    \"\"\"Used when custom objective is supplied.\"\"\"\n    y = dtrain.get_label()\n    n_classes = predt.size // y.shape[0]\n    # Like custom objective, the predt is untransformed leaf weight when custom objective\n    # is provided.\n\n    # With the use of `custom_metric` parameter in train function, custom metric receives\n    # raw input only when custom objective is also being used.  Otherwise custom metric\n    # will receive transformed prediction.\n    assert predt.shape == (d_train.num_row(), n_classes)\n    out = np.zeros(dtrain.num_row())\n    for r in range(predt.shape[0]):\n        i = np.argmax(predt[r])\n        out[r] = i\n\n    assert y.shape == out.shape\n\n    errors = np.zeros(dtrain.num_row())\n    errors[y != out] = 1.0\n    return 'PyMError', np.sum(errors) / dtrain.num_row()\n```\n\n----------------------------------------\n\nTITLE: Training a SparkXGBRegressor Model\nDESCRIPTION: Shows how to fit a SparkXGBRegressor on a spark dataframe containing labeled training data to create a model.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/spark_estimator.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nxgb_regressor_model = xgb_regressor.fit(train_spark_dataframe)\n```\n\n----------------------------------------\n\nTITLE: Testing Dirichlet Regression Functions in Python\nDESCRIPTION: This snippet provides unit tests for the Dirichlet regression functions in Python. It generates random Dirichlet data and verifies the correctness of the log-likelihood, gradient, and Hessian calculations.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/advanced_custom_obj.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom math import isclose\nfrom scipy import stats\nfrom scipy.optimize import check_grad\nfrom scipy.special import softmax\n\ndef gen_random_dirichlet(rng: np.random.Generator, m: int, k: int):\n    alpha = np.exp(rng.standard_normal(size=k))\n    return rng.dirichlet(alpha, size=m)\n\ndef test_dirichlet_fun_grad_hess():\n    k = 3\n    m = 10\n    rng = np.random.default_rng(seed=123)\n    Y = gen_random_dirichlet(rng, m, k)\n    x0 = rng.standard_normal(size=k)\n    for row in range(Y.shape[0]):\n        fun_row = dirichlet_fun(x0.reshape((1,-1)), Y[[row]])\n        ref_logpdf = stats.dirichlet.logpdf(\n            Y[row] / Y[row].sum(), # <- avoid roundoff error\n            np.exp(x0),\n        )\n        assert isclose(fun_row, -ref_logpdf)\n\n        gdiff = check_grad(\n            lambda pred: dirichlet_fun(pred.reshape((1,-1)), Y[[row]]),\n            lambda pred: dirichlet_grad(pred.reshape((1,-1)), Y[[row]]),\n            x0\n        )\n        assert gdiff <= 1e-6\n\n        H_numeric = np.empty((k,k))\n        eps = 1e-7\n        for ii in range(k):\n            x0_plus_eps = x0.reshape((1,-1)).copy()\n            x0_plus_eps[0,ii] += eps\n            for jj in range(k):\n                H_numeric[ii, jj] = (\n                    dirichlet_grad(x0_plus_eps, Y[[row]])[0][jj]\n                    - dirichlet_grad(x0.reshape((1,-1)), Y[[row]])[0][jj]\n                ) / eps\n        H = dirichlet_hess(x0.reshape((1,-1)), Y[[row]])[0]\n        np.testing.assert_almost_equal(H, H_numeric, decimal=6)\ntest_dirichlet_fun_grad_hess()\n```\n\n----------------------------------------\n\nTITLE: Compressed Objective Function with Gradient Statistics\nDESCRIPTION: This formula further compresses the objective function by defining aggregate gradient statistics Gj and Hj for each leaf, simplifying the optimization problem.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/model.rst#2025-04-19_snippet_9\n\nLANGUAGE: math\nCODE:\n```\n\\text{obj}^{(t)} = \\sum^T_{j=1} [G_jw_j + \\frac{1}{2} (H_j+\\lambda) w_j^2] +\\gamma T\n```\n\n----------------------------------------\n\nTITLE: Installing XGBoost4j-Spark SBT Configuration\nDESCRIPTION: SBT configuration for including XGBoost4j-Spark dependency in Scala projects.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/install.rst#2025-04-19_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\nlibraryDependencies ++= Seq(\n    \"ml.dmlc\" %% \"xgboost4j-spark\" % \"latest_version_num\"\n  )\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost Model With Monotonic Constraints - Python\nDESCRIPTION: Example showing how to add monotonic constraints to an XGBoost model. Sets an increasing constraint (1) on the first predictor and a decreasing constraint (-1) on the second predictor.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/monotonic.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nparams_constrained = params.copy()\nparams_constrained['monotone_constraints'] = (1,-1)\n\nmodel_with_constraints = xgb.train(params_constrained, dtrain,\n                                     num_boost_round = 1000, evals = evallist,\n                                     early_stopping_rounds = 10)\n```\n\n----------------------------------------\n\nTITLE: Importing Booster and XGBoost Classes in Java\nDESCRIPTION: This code shows how to import the Booster and XGBoost classes from XGBoost4J in Java.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/java_intro.rst#2025-04-19_snippet_7\n\nLANGUAGE: java\nCODE:\n```\nimport ml.dmlc.xgboost4j.java.Booster;\nimport ml.dmlc.xgboost4j.java.XGBoost;\n```\n\n----------------------------------------\n\nTITLE: Predicting with XGBoost Booster using XGBoosterPredictFromDMatrix in C\nDESCRIPTION: This snippet demonstrates how to make predictions using an XGBoost booster. It configures prediction parameters, calls XGBoosterPredictFromDMatrix, and then prints the prediction results.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/c_api_tutorial.rst#2025-04-19_snippet_9\n\nLANGUAGE: c\nCODE:\n```\nchar const config[] =\n    \"{\\\"training\\\": false, \\\"type\\\": 0, \"\n    \"\\\"iteration_begin\\\": 0, \\\"iteration_end\\\": 0, \\\"strict_shape\\\": false}\";\n/* Shape of output prediction */\nuint64_t const* out_shape;\n/* Dimension of output prediction */\nuint64_t out_dim;\n/* Pointer to a thread local contiguous array, assigned in prediction function. */\nfloat const* out_result = NULL;\nsafe_xgboost(\n    XGBoosterPredictFromDMatrix(booster, dmatrix, config, &out_shape, &out_dim, &out_result));\n\nfor (unsigned int i = 0; i < output_length; i++){\n  printf(\"prediction[%i] = %f \\n\", i, output_result[i]);\n}\n```\n\n----------------------------------------\n\nTITLE: Building XGBoost with NCCL for Distributed GPU Training\nDESCRIPTION: Commands for building XGBoost with NCCL support for faster distributed GPU training on Linux systems.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/build.rst#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -S . -DUSE_CUDA=ON -DUSE_NCCL=ON -DNCCL_ROOT=/path/to/nccl2 -GNinja\ncd build && ninja\n```\n\n----------------------------------------\n\nTITLE: Submitting XGBoost GPU Application to Spark Standalone Cluster\nDESCRIPTION: This script demonstrates how to submit an XGBoost application to a Spark standalone cluster with GPU support. It configures the RAPIDS Accelerator, resource allocation for GPUs, and other necessary parameters for efficient GPU-accelerated machine learning.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost4j_spark_gpu_tutorial.rst#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nrapids_version=24.08.0\nxgboost_version=$LATEST_VERSION\nmain_class=Iris\napp_jar=iris-1.0.0.jar\n\nspark-submit \\\n  --master $master \\\n  --packages com.nvidia:rapids-4-spark_2.12:${rapids_version},ml.dmlc:xgboost4j-spark-gpu_2.12:${xgboost_version} \\\n  --conf spark.executor.cores=12 \\\n  --conf spark.task.cpus=1 \\\n  --conf spark.executor.resource.gpu.amount=1 \\\n  --conf spark.task.resource.gpu.amount=0.08 \\\n  --conf spark.rapids.sql.csv.read.double.enabled=true \\\n  --conf spark.rapids.sql.hasNans=false \\\n  --conf spark.plugins=com.nvidia.spark.SQLPlugin \\\n  --class ${main_class} \\\n   ${app_jar}\n```\n\n----------------------------------------\n\nTITLE: Configuring XGBoost Parameters for Regression\nDESCRIPTION: This snippet shows the parameter configuration needed to use XGBoost for regression tasks. The primary difference from classification is the objective parameter, which is set to reg:squarederror for linear regression. For labels in the [0,1] range, reg:logistic can also be used.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/CLI/regression/README.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# General parameter\n# this is the only difference with classification, use reg:squarederror to do linear regression\n# when labels are in [0,1] we can also use reg:logistic\nobjective = reg:squarederror\n...\n```\n\n----------------------------------------\n\nTITLE: Setting XGBoostClassifier Parameters via Setters\nDESCRIPTION: Demonstrates how to set XGBoost parameters using the camel-case setter methods, which follows Spark's MLlib naming convention. This example sets the maximum depth of the decision trees.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost4j_spark_gpu_tutorial.rst#2025-04-19_snippet_3\n\nLANGUAGE: scala\nCODE:\n```\nval xgbClassifier = new XGBoostClassifier(xgbParam)\n    .setFeaturesCol(featuresNames)\n    .setLabelCol(labelName)\nxgbClassifier.setMaxDepth(2)\n```\n\n----------------------------------------\n\nTITLE: Adapting XGBoost Ranker for Scikit-learn Utilities\nDESCRIPTION: This snippet demonstrates how to adapt the XGBoost ranker to work with scikit-learn utilities like cross-validation. It adds the 'qid' column to the feature matrix and uses StratifiedGroupKFold for cross-validation.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/learning_to_rank.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\n# `X`, `qid`, and `y` are from the previous snippet, they are all sorted by the `sorted_idx`.\ndf = pd.DataFrame(X, columns=[str(i) for i in range(X.shape[1])])\ndf[\"qid\"] = qid\n\nranker.fit(df, y)  # No need to pass qid as a separate argument\n\nfrom sklearn.model_selection import StratifiedGroupKFold, cross_val_score\n# Works with cv in scikit-learn, along with HPO utilities like GridSearchCV\nkfold = StratifiedGroupKFold(shuffle=False)\ncross_val_score(ranker, df, y, cv=kfold, groups=df.qid)\n```\n\n----------------------------------------\n\nTITLE: Implementing Dirichlet Expected Hessian Matrix in R\nDESCRIPTION: R implementation of the expected Hessian for Dirichlet regression, providing a more stable alternative to the true Hessian for optimization purposes.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/advanced_custom_obj.rst#2025-04-19_snippet_6\n\nLANGUAGE: r\nCODE:\n```\ndirichlet.expected.hess <- function(pred) {\n    epred <- exp(pred)\n    k <- ncol(pred)\n    H <- array(dim = c(nrow(pred), k, k))\n    for (row in seq_len(nrow(pred))) {\n        H[row, , ] <- (\n            - trigamma(sum(epred[row,])) * tcrossprod(epred[row,])\n            + diag(trigamma(epred[row,]) * epred[row,]^2)\n        )\n    }\n    return(H)\n}\n```\n\n----------------------------------------\n\nTITLE: Installing CPU-only XGBoost Python Package\nDESCRIPTION: Install minimal CPU-only version of XGBoost for reduced disk space usage.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/install.rst#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install xgboost-cpu\n```\n\n----------------------------------------\n\nTITLE: Embedding XGBoost in C/C++ Applications Using CMake\nDESCRIPTION: CMake configuration example showing how to embed XGBoost in C/C++ applications by adding the XGBoost library as a dependency in CMakeLists.txt.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/NEWS.md#2025-04-19_snippet_23\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package(xgboost REQUIRED)\nadd_executable(api-demo c-api-demo.c)\ntarget_link_libraries(api-demo xgboost::xgboost)\n```\n\n----------------------------------------\n\nTITLE: XGBoost Classification Objective Functions\nDESCRIPTION: Binary and multiclass classification objective functions including logistic regression, hinge loss and softmax options\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst#2025-04-19_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\nbinary:logistic\nbinary:logitraw\nbinary:hinge\nmulti:softmax\nmulti:softprob\n```\n\n----------------------------------------\n\nTITLE: Loading LIBSVM Format with External Memory in XGBoost\nDESCRIPTION: Demonstrates how to enable external memory support for LIBSVM format files in XGBoost. The cache suffix allows XGBoost to store preprocessed data in binary format for faster subsequent access.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/external_memory.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndtrain = DMatrix('../data/agaricus.txt.train?format=libsvm#dtrain.cache')\n```\n\n----------------------------------------\n\nTITLE: Installing XGBoost R Package from Source\nDESCRIPTION: Commands to install the R package for XGBoost directly from the source code after cloning the repository. This uses the R CMD INSTALL method to build and install the package.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/build.rst#2025-04-19_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ncd R-package\nR CMD INSTALL .\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading XGBoost PySpark Models\nDESCRIPTION: Demonstrates how to save and load SparkXGBRegressor models, allowing for model persistence and reuse without retraining.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/spark_estimator.rst#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nregressor = SparkXGBRegressor()\nmodel = regressor.fit(train_df)\n# save the model\nmodel.save(\"/tmp/xgboost-pyspark-model\")\n# load the model\nmodel2 = SparkXGBRankerModel.load(\"/tmp/xgboost-pyspark-model\")\n```\n\n----------------------------------------\n\nTITLE: Configuring RMM Memory Pool for XGBoost on Spark\nDESCRIPTION: This code snippet shows how to configure the RMM (RAPIDS Memory Manager) memory pool for XGBoost on Spark. It demonstrates setting memory allocation fractions and enabling pooling to optimize GPU memory usage when running XGBoost workloads.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost4j_spark_gpu_tutorial.rst#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nspark-submit \\\n  --master $master \\\n  --conf spark.rapids.memory.gpu.allocFraction=0.5 \\\n  --conf spark.rapids.memory.gpu.maxAllocFraction=0.8 \\\n  --conf spark.rapids.memory.gpu.pool=ARENA \\\n  --conf spark.rapids.memory.gpu.pooling.enabled=true \\\n  ...\n```\n\n----------------------------------------\n\nTITLE: Implementing Dirichlet XGBoost Objective Function in R\nDESCRIPTION: R implementation of a custom objective function for XGBoost that uses Dirichlet regression, returning the gradient and Hessian upper bound in XGBoost's required format.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/advanced_custom_obj.rst#2025-04-19_snippet_11\n\nLANGUAGE: r\nCODE:\n```\nlibrary(xgboost)\n\ndirichlet.xgb.objective <- function(pred, dtrain) {\n    y <- getinfo(dtrain, \"label\")\n    return(\n        list(\n            grad = dirichlet.grad(pred, y),\n            hess = dirichlet.diag.upper.bound.expected.hess(pred, y)\n        )\n    )\n}\n```\n\n----------------------------------------\n\nTITLE: Installing XGBoost CPU Package using pip\nDESCRIPTION: Command to install the stable CPU-only version of XGBoost using pip package manager. This installs xgboost-cpu which lacks GPU and federated learning support but is suitable for space-constrained environments.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/python-package/README.cpu.rst#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install xgboost-cpu\n```\n\n----------------------------------------\n\nTITLE: Importing XGBoost Dask Module in Python\nDESCRIPTION: New required way to import the XGBoost Dask module after deprecation of default import. Users must explicitly import the Dask module.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/changes/v3.0.0.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom xgboost import dask as dxgb\n```\n\n----------------------------------------\n\nTITLE: Continuing XGBoost Training from Existing Model\nDESCRIPTION: Command to continue boosting from an existing XGBoost model for additional rounds.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/CLI/binary_classification/README.md#2025-04-19_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n../../xgboost mushroom.conf model_in=0002.model num_round=2 model_out=continue.model\n```\n\n----------------------------------------\n\nTITLE: Obtaining the Native Booster Object from XGBoost Estimator\nDESCRIPTION: Snippet showing how to access the underlying native booster object from an XGBoost sklearn estimator to use advanced features like cached predictions.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/python/sklearn_estimator.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nbooster = clf.get_booster()\nprint(booster.num_boosted_rounds())\n```\n\n----------------------------------------\n\nTITLE: Compiling XGBClassifier for FHE\nDESCRIPTION: Demonstrates how to compile the trained model with a calibration dataset. This step is necessary to optimize the FHE circuit by computing the precision of intermediate values in the model.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/privacy_preserving.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclassifier.compile(X_calibrate)\n```\n\n----------------------------------------\n\nTITLE: Running XGBoost Training Process\nDESCRIPTION: Command to start the XGBoost training process using the configuration file.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/CLI/binary_classification/README.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n../../xgboost mushroom.conf\n```\n\n----------------------------------------\n\nTITLE: Predicting with SparkXGBRegressor Model\nDESCRIPTION: Demonstrates how to use a trained SparkXGBRegressor model to generate predictions on test data, returning a dataframe with an additional prediction column.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/spark_estimator.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntransformed_test_spark_dataframe = xgb_regressor_model.transform(test_spark_dataframe)\n```\n\n----------------------------------------\n\nTITLE: Converting Feature to Categorical Type in Pandas/CUDF\nDESCRIPTION: Converts a DataFrame column to categorical type for use with XGBoost. This is a prerequisite step before training models with categorical features.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/categorical.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nX[\"cat_feature\"].astype(\"category\")\n```\n\n----------------------------------------\n\nTITLE: Using External Memory with GPU Predictor in Python\nDESCRIPTION: Example showing how to make GPU-based predictions on datasets stored in external memory. This is useful for large datasets that don't fit into GPU memory.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/NEWS.md#2025-04-19_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\ndtest = xgboost.DMatrix('test_data.libsvm#dtest.cache')\nbst.set_param('predictor', 'gpu_predictor')\nbst.predict(dtest)\n```\n\n----------------------------------------\n\nTITLE: Configuring IPv6 Support for XGBoost with Dask\nDESCRIPTION: Example demonstrating how to configure IPv6 support for XGBoost with Dask on Linux systems. This requires setting the scheduler address to help XGBoost obtain correct address information in dual-stack environments.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/dask.rst#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport dask\nfrom distributed import Client\nfrom xgboost import dask as dxgb\n# let xgboost know the scheduler address, use the same bracket format as dask.\nwith dask.config.set({\"xgboost.scheduler_address\": \"[fd20:b6f:f759:9800::]\"}):    \n    with Client(\"[fd20:b6f:f759:9800::]\") as client:\n        reg = dxgb.DaskXGBRegressor(tree_method=\"hist\")\n```\n\n----------------------------------------\n\nTITLE: Using ProxyDMatrix in C for Inference\nDESCRIPTION: Example of reusing ProxyDMatrix for multiple inference calls in C, which is now supported for improved performance.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/changes/v3.0.0.rst#2025-04-19_snippet_3\n\nLANGUAGE: C\nCODE:\n```\nDMatrixHandle proxy;\nXGProxyDMatrixCreate(&proxy);\n// Use proxy for multiple inference calls\n```\n\n----------------------------------------\n\nTITLE: Creating DMatrix from CSC Sparse Matrix in XGBoost4J (Java)\nDESCRIPTION: This code demonstrates how to create a DMatrix object from a Compressed Sparse Column (CSC) format sparse matrix in XGBoost4J.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/java_intro.rst#2025-04-19_snippet_3\n\nLANGUAGE: java\nCODE:\n```\nlong[] colHeaders = new long[] {0,3,4,6,7};\nfloat[] data = new float[] {1f,4f,3f,1f,2f,2f,3f};\nint[] rowIndex = new int[] {0,1,2,2,0,2,1};\nint numRow = 3;\nDMatrix dmat = new DMatrix(colHeaders, rowIndex, data, DMatrix.SparseType.CSC, numRow);\n```\n\n----------------------------------------\n\nTITLE: Setting Up Ranged Labels for Survival Analysis in R\nDESCRIPTION: This R code demonstrates how to create a DMatrix object with ranged labels for survival analysis. It shows how to associate lower and upper bounds for labels with a data matrix, covering all types of censoring (uncensored, right-censored, left-censored, and interval-censored).\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/aft_survival_analysis.rst#2025-04-19_snippet_1\n\nLANGUAGE: r\nCODE:\n```\nlibrary(xgboost)\n\n# 4-by-2 Data matrix\nX <- matrix(c(1., -1., -1., 1., 0., 1., 1., 0.),\n            nrow=4, ncol=2, byrow=TRUE)\ndtrain <- xgb.DMatrix(X)\n\n# Associate ranged labels with the data matrix.\n# This example shows each kind of censored labels.\n#                   uncensored  right  left  interval\ny_lower_bound <- c(        2.,    3.,   0.,       4.)\ny_upper_bound <- c(        2.,  +Inf,   4.,       5.)\nsetinfo(dtrain, 'label_lower_bound', y_lower_bound)\nsetinfo(dtrain, 'label_upper_bound', y_upper_bound)\n```\n\n----------------------------------------\n\nTITLE: Creating Diagonal Upper Bound for Expected Hessian in R\nDESCRIPTION: R function that creates a diagonal upper bound for the expected Hessian, making it compatible with XGBoost's requirement for a diagonal Hessian matrix.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/advanced_custom_obj.rst#2025-04-19_snippet_9\n\nLANGUAGE: r\nCODE:\n```\ndirichlet.diag.upper.bound.expected.hess <- function(pred, y) {\n    Ehess <- dirichlet.expected.hess(pred)\n    diag.bound.Ehess <- array(dim=dim(pred))\n    for (row in seq_len(nrow(pred))) {\n        diag.bound.Ehess[row,] <- abs(Ehess[row,,]) |> rowSums()\n    }\n    return(diag.bound.Ehess)\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Page Concatenation with Subsampling for GPU External Memory\nDESCRIPTION: This code shows how to enable page concatenation with subsampling for GPU-based external memory training. This approach can significantly improve performance, especially for GPUs connected via PCIe, by concatenating data batches into a single page and using gradient-based sampling.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/external_memory.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nparam = {\n  \"device\": \"cuda\",\n  \"extmem_single_page\": true,\n  'subsample': 0.2,\n  'sampling_method': 'gradient_based',\n}\n```\n\n----------------------------------------\n\nTITLE: Building XGBoost with GPU Support\nDESCRIPTION: Commands for building XGBoost with CUDA support enabled for GPU acceleration.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/build.rst#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -S . -DUSE_CUDA=ON -GNinja\ncd build && ninja\n```\n\n----------------------------------------\n\nTITLE: Loading LIBSVM/Binary Files into XGBoost DMatrix\nDESCRIPTION: Loading data from LIBSVM text format or XGBoost binary format.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/python/python_intro.rst#2025-04-19_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndtrain = xgb.DMatrix('train.svm.txt?format=libsvm')\ndtest = xgb.DMatrix('test.svm.buffer')\n```\n\n----------------------------------------\n\nTITLE: Implementing XGBoost in R for Binary Classification\nDESCRIPTION: This snippet shows how to use XGBoost in R for binary classification. It loads the agaricus dataset, creates and fits an XGBoost model with specified parameters like depth, learning rate, and objective function, then uses the model to make predictions on test data.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/get_started.rst#2025-04-19_snippet_1\n\nLANGUAGE: R\nCODE:\n```\n# load data\ndata(agaricus.train, package='xgboost')\ndata(agaricus.test, package='xgboost')\ntrain <- agaricus.train\ntest <- agaricus.test\n# fit model\nbst <- xgboost(x = train$data, y = factor(train$label),\n               max.depth = 2, eta = 1, nrounds = 2,\n               nthread = 2, objective = \"binary:logistic\")\n# predict\npred <- predict(bst, test$data)\n```\n\n----------------------------------------\n\nTITLE: Creating Label Converter for XGBoost Predictions in Spark\nDESCRIPTION: Configures an IndexToString transformer to convert numeric prediction outputs back to original string labels, using the labels from a previously defined stringIndexer.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost4j_spark_tutorial.rst#2025-04-19_snippet_13\n\nLANGUAGE: scala\nCODE:\n```\nval labelConverter = new IndexToString()\n        .setInputCol(\"prediction\")\n        .setOutputCol(\"realLabel\")\n        .setLabels(stringIndexer.labels)\n```\n\n----------------------------------------\n\nTITLE: Installing XGBoost R Package from R-universe\nDESCRIPTION: Command to install the new R package version from R-universe repository before CRAN update. The new package includes categorical feature support, QuantileDMatrix, and external memory training implementation.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/changes/v3.0.0.rst#2025-04-19_snippet_5\n\nLANGUAGE: R\nCODE:\n```\ninstall.packages('xgboost', repos = c('https://dmlc.r-universe.dev', 'https://cloud.r-project.org'))\n```\n\n----------------------------------------\n\nTITLE: Creating DMatrix from Dense Matrix in XGBoost4J (Java)\nDESCRIPTION: This snippet illustrates how to create a DMatrix object from a dense matrix using row-major layout in XGBoost4J.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/java_intro.rst#2025-04-19_snippet_4\n\nLANGUAGE: java\nCODE:\n```\nfloat[] data = new float[] {1f,2f,3f,4f,5f,6f};\nint nrow = 3;\nint ncol = 2;\nfloat missing = 0.0f;\nDMatrix dmat = new DMatrix(data, nrow, ncol, missing);\n```\n\n----------------------------------------\n\nTITLE: Running Clang-Tidy Checks for XGBoost\nDESCRIPTION: Python commands to run Clang-Tidy checks on the XGBoost C++ codebase. Includes options to include or exclude CUDA code from the checks.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/coding_guide.rst#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncd /path/to/xgboost/\npython3 ops/script/run_clang_tidy.py\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd /path/to/xgboost/\npython3 ops/script/run_clang_tidy.py --cuda=0\n```\n\nLANGUAGE: bash\nCODE:\n```\ncd /path/to/xgboost/\npython3 ops/script/run_clang_tidy.py --cpp=0\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Plugin Source to XGBoost CMake Configuration\nDESCRIPTION: This CMake snippet shows how to add a custom objective function source file to the XGBoost build system. It adds the custom_obj.cc file to the objxgboost target, making it available during compilation.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/plugin/example/README.md#2025-04-19_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_sources(objxgboost PRIVATE ${xgboost_SOURCE_DIR}/plugin/example/custom_obj.cc)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Dataset Split Memory Impact in Python\nDESCRIPTION: Shows how train_test_split creates a copy of the dataset in memory, which can be significant when running parallel operations with GridSearchCV.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/param_tuning.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nX_train, X_test, y_train, y_test = train_test_split(X, y)\n```\n\n----------------------------------------\n\nTITLE: Importing ExtMemQuantileDMatrix in Python\nDESCRIPTION: Example of importing the new ExtMemQuantileDMatrix class for fast data initialization with the hist tree method. Supports both CPU and GPU training.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/changes/v3.0.0.rst#2025-04-19_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom xgboost import ExtMemQuantileDMatrix\n```\n\n----------------------------------------\n\nTITLE: Freeing XGBoost Resources in C\nDESCRIPTION: This snippet shows how to properly free the memory used by XGBoost structures using XGDMatrixFree and XGBoosterFree. This step is crucial to prevent memory leaks in your application.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/c_api_tutorial.rst#2025-04-19_snippet_13\n\nLANGUAGE: c\nCODE:\n```\nsafe_xgboost(XGDMatrixFree(dmatrix));\nsafe_xgboost(XGBoosterFree(booster));\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading XGBoost Model in Python\nDESCRIPTION: Shows how to save a trained XGBoost model to a file and how to load a saved model. It also includes examples of dumping the model to a text file with and without a feature map.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/python/python_intro.rst#2025-04-19_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nbst.save_model('0001.model')\n```\n\nLANGUAGE: python\nCODE:\n```\n# dump model\nbst.dump_model('dump.raw.txt')\n# dump model with feature map\nbst.dump_model('dump.raw.txt', 'featmap.txt')\n```\n\nLANGUAGE: python\nCODE:\n```\nbst = xgb.Booster({'nthread': 4})  # init model\nbst.load_model('model.bin')  # load model data\n```\n\n----------------------------------------\n\nTITLE: Setting Parameters for XGBoost in Java\nDESCRIPTION: This snippet demonstrates how to set parameters for XGBoost using a Map in Java.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/java_intro.rst#2025-04-19_snippet_6\n\nLANGUAGE: java\nCODE:\n```\nMap<String, Object> params = new HashMap<String, Object>() {\n  {\n    put(\"eta\", 1.0);\n    put(\"max_depth\", 2);\n    put(\"objective\", \"binary:logistic\");\n    put(\"eval_metric\", \"logloss\");\n  }\n};\n```\n\n----------------------------------------\n\nTITLE: Building XGBoost with Distributed Filesystem Support using CMake\nDESCRIPTION: This command enables XGBoost to work with distributed filesystems like HDFS, S3, and Azure by configuring the build with the appropriate flags. It uses cmake to generate the build configuration with specific options enabled.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/CLI/distributed-training/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncmake <path/to/xgboost> -DUSE_HDFS=ON -DUSE_S3=ON -DUSE_AZURE=ON\n```\n\n----------------------------------------\n\nTITLE: Saving configuration in Python\nDESCRIPTION: Use the save_config() function to inspect all used training parameters, which is helpful for debugging model performance.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/NEWS.md#2025-04-19_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\nconfig = booster.save_config()\nprint(config)  # Displays all used training parameters\n```\n\n----------------------------------------\n\nTITLE: Submitting XGBoost PySpark Application with GPU Support\nDESCRIPTION: Command for submitting a PySpark application with XGBoost to a Spark standalone cluster, configuring GPU resources with fractional allocation for optimal performance.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/spark_estimator.rst#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nexport PYSPARK_DRIVER_PYTHON=python\nexport PYSPARK_PYTHON=./environment/bin/python\n\nspark-submit \\\n  --master spark://<master-ip>:7077 \\\n  --conf spark.executor.cores=12 \\\n  --conf spark.task.cpus=1 \\\n  --conf spark.executor.resource.gpu.amount=1 \\\n  --conf spark.task.resource.gpu.amount=0.08 \\\n  --archives xgboost_env.tar.gz#environment \\\n  xgboost_app.py\n```\n\n----------------------------------------\n\nTITLE: Building XGBoost Command-Line Interface\nDESCRIPTION: Builds the deprecated CLI executable for XGBoost when enabled, configuring its properties, dependencies, and output location.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/CMakeLists.txt#2025-04-19_snippet_18\n\nLANGUAGE: CMake\nCODE:\n```\n#-- CLI for xgboost\nif(BUILD_DEPRECATED_CLI)\n  add_executable(runxgboost ${xgboost_SOURCE_DIR}/src/cli_main.cc)\n  target_link_libraries(runxgboost PRIVATE objxgboost)\n  target_include_directories(runxgboost\n    PRIVATE\n    ${xgboost_SOURCE_DIR}/include\n    ${xgboost_SOURCE_DIR}/dmlc-core/include\n  )\n  set_target_properties(runxgboost PROPERTIES OUTPUT_NAME xgboost)\n  xgboost_target_properties(runxgboost)\n  xgboost_target_link_libraries(runxgboost)\n  xgboost_target_defs(runxgboost)\n\n  if(KEEP_BUILD_ARTIFACTS_IN_BINARY_DIR)\n    set_output_directory(runxgboost ${xgboost_BINARY_DIR})\n  else()\n    set_output_directory(runxgboost ${xgboost_SOURCE_DIR})\n  endif()\nendif()\n#-- End CLI for xgboost\n```\n\n----------------------------------------\n\nTITLE: Configuring Federated Client Library and Rabit Engine Integration\nDESCRIPTION: Creates an interface library for the gRPC client and configures the XGBoost object library with federated learning sources. Includes conditional compilation for CUDA support and sets appropriate compiler definitions.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/plugin/federated/CMakeLists.txt#2025-04-19_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\n# Wrapper for the gRPC client.\nadd_library(federated_client INTERFACE)\ntarget_link_libraries(federated_client INTERFACE federated_proto)\n\n# Rabit engine for Federated Learning.\ntarget_sources(\n  objxgboost PRIVATE federated_tracker.cc federated_comm.cc federated_coll.cc\n)\nif(USE_CUDA)\n  target_sources(objxgboost PRIVATE federated_comm.cu federated_coll.cu)\nendif()\n\ntarget_link_libraries(objxgboost PRIVATE federated_client \"-Wl,--exclude-libs,ALL\")\ntarget_compile_definitions(objxgboost PUBLIC -DXGBOOST_USE_FEDERATED=1)\n```\n\n----------------------------------------\n\nTITLE: Configuring XGBoost4j-Spark-GPU Dependencies in Maven\nDESCRIPTION: Maven configuration for adding the GPU-enabled version of XGBoost4j-Spark. Required for using CUDA devices with XGBoost in Spark applications.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/install.rst#2025-04-19_snippet_7\n\nLANGUAGE: xml\nCODE:\n```\n<properties>\n  ...\n  <!-- Specify Scala version in package name -->\n  <scala.binary.version>2.12</scala.binary.version>\n</properties>\n\n<dependencies>\n  <dependency>\n      <groupId>ml.dmlc</groupId>\n      <artifactId>xgboost4j-spark-gpu_${scala.binary.version}</artifactId>\n      <version>latest_version_num-SNAPSHOT</version>\n  </dependency>\n</dependencies>\n```\n\n----------------------------------------\n\nTITLE: Error Handling Macro for XGBoost C API in C\nDESCRIPTION: A macro to wrap XGBoost C API function calls for error checking and reporting in C applications.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/c_api_tutorial.rst#2025-04-19_snippet_3\n\nLANGUAGE: c\nCODE:\n```\n#define safe_xgboost(call) {  \n  int err = (call); \n  if (err != 0) { \n    fprintf(stderr, \"%s:%d: error in %s: %s\\n\", __FILE__, __LINE__, #call, XGBGetLastError());  \n    exit(1); \n  } \n}\n```\n\n----------------------------------------\n\nTITLE: XGBoost Linear Booster Configuration\nDESCRIPTION: Configuration snippet for using linear booster instead of tree booster, including linear booster specific parameters.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/CLI/binary_classification/README.md#2025-04-19_snippet_5\n\nLANGUAGE: conf\nCODE:\n```\n# General Parameters\n# choose the linear booster\nbooster = gblinear\n...\n\n# Change Tree Booster Parameters into Linear Booster Parameters\n# L2 regularization term on weights, default 0\nlambda = 0.01\n# L1 regularization term on weights, default 0\nalpha = 0.01\n# L2 regularization term on bias, default 0\nlambda_bias = 0.01\n\n# Regression Parameters\n...\n```\n\n----------------------------------------\n\nTITLE: Initializing XGBoost Classifier in Scala\nDESCRIPTION: Demonstrates how to create and configure an XGBoostClassifier instance with feature and label column specifications. Shows parameter setting using the setter methods.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost4j_spark_tutorial.rst#2025-04-19_snippet_5\n\nLANGUAGE: scala\nCODE:\n```\nval xgbClassifier = new XGBoostClassifier().\n    setFeaturesCol(\"features\").\n    setLabelCol(\"classIndex\")\nxgbClassifier.setMaxDepth(2)\n```\n\n----------------------------------------\n\nTITLE: Loading NumPy Array into XGBoost DMatrix\nDESCRIPTION: Example of creating a DMatrix from NumPy arrays for features and labels.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/python/python_intro.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndata = np.random.rand(5, 10)  # 5 entities, each contains 10 features\nlabel = np.random.randint(2, size=5)  # binary target\ndtrain = xgb.DMatrix(data, label=label)\n```\n\n----------------------------------------\n\nTITLE: Getting Predictions from Trained XGBoost Model\nDESCRIPTION: Command to use the trained model for predicting on test data.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/CLI/binary_classification/README.md#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n../../xgboost mushroom.conf task=pred model_in=0002.model\n```\n\n----------------------------------------\n\nTITLE: Adding XGBoost4j-Spark-GPU Dependency in SBT\nDESCRIPTION: SBT configuration for adding the GPU-enabled version of XGBoost4j-Spark. Required for using CUDA devices with XGBoost in Spark applications.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/install.rst#2025-04-19_snippet_8\n\nLANGUAGE: scala\nCODE:\n```\nlibraryDependencies ++= Seq(\n  \"ml.dmlc\" %% \"xgboost4j-spark-gpu\" % \"latest_version_num-SNAPSHOT\"\n)\n```\n\n----------------------------------------\n\nTITLE: Adding XGBoost4j-Spark Dependency in SBT\nDESCRIPTION: SBT configuration for adding XGBoost4j-Spark as a dependency. Uses SBT's %% operator to automatically append the Scala version to the artifact name.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/install.rst#2025-04-19_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\nlibraryDependencies ++= Seq(\n  \"ml.dmlc\" %% \"xgboost4j-spark\" % \"latest_version_num-SNAPSHOT\"\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing CMake Configuration for XGBoost\nDESCRIPTION: Sets up initial CMake configuration including version requirement, project name, language settings and version number. Includes utility modules and sets CMake policy defaults.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.18 FATAL_ERROR)\n\nif(PLUGIN_SYCL)\n  string(REPLACE \" -isystem ${CONDA_PREFIX}/include\" \"\" CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS}\")\nendif()\n\nproject(xgboost LANGUAGES CXX C VERSION 3.1.0)\ninclude(cmake/Utils.cmake)\nlist(APPEND CMAKE_MODULE_PATH \"${xgboost_SOURCE_DIR}/cmake/modules\")\n```\n\n----------------------------------------\n\nTITLE: Starting NVFlare Worker Nodes\nDESCRIPTION: These commands start two separate worker nodes that participate in the federated learning process.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/nvflare/horizontal/README.md#2025-04-19_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n/tmp/nvflare/poc/site-1/startup/start.sh\n```\n\nLANGUAGE: shell\nCODE:\n```\n/tmp/nvflare/poc/site-2/startup/start.sh\n```\n\n----------------------------------------\n\nTITLE: Setting Rabit Tracker Timeout for XGBoost Gang Scheduling in Spark\nDESCRIPTION: Configures a timeout threshold for XGBoost's resource allocation to prevent indefinite hanging when sufficient resources are unavailable in a shared cluster environment.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost4j_spark_tutorial.rst#2025-04-19_snippet_17\n\nLANGUAGE: scala\nCODE:\n```\nxgbClassifier.setRabitTrackerTimeout(60000L)\n```\n\n----------------------------------------\n\nTITLE: Setting XGBoost Tracker IP Address for Dask Training\nDESCRIPTION: Example showing how to specify the IP address for XGBoost's tracker when using Dask. This helps resolve connection issues in certain environments where XGBoost fails to resolve the scheduler's IP address.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/dask.rst#2025-04-19_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport dask\nfrom distributed import Client\nfrom xgboost import dask as dxgb\nfrom xgboost.collective import Config\n\n# let xgboost know the scheduler address\ncoll_cfg = Config(retry=1, timeout=20, tracker_host_ip=\"10.23.170.98\", tracker_port=0)\n\nwith Client(scheduler_file=\"sched.json\") as client:\n    reg = dxgb.DaskXGBRegressor(coll_cfg=coll_cfg)\n```\n\n----------------------------------------\n\nTITLE: Loading XGBoost Model from Custom Path\nDESCRIPTION: Configures the path to the Python package and model file location, then imports XGBoost. This snippet supports loading models from local filesystem or S3 storage.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/CLI/distributed-training/plot_model.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npkg_path = '../../python-package/'\nmodel_file = 's3://my-bucket/xgb-demo/model/0002.model'\nsys.path.insert(0, pkg_path)\nimport xgboost as xgb\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost Model in Java\nDESCRIPTION: This snippet demonstrates how to train an XGBoost model using XGBoost4J in Java, including setting up data matrices and watch lists.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/java_intro.rst#2025-04-19_snippet_8\n\nLANGUAGE: java\nCODE:\n```\nDMatrix trainMat = new DMatrix(\"train.svm.txt\");\nDMatrix validMat = new DMatrix(\"valid.svm.txt\");\n// Specify a watch list to see model accuracy on data sets\nMap<String, DMatrix> watches = new HashMap<String, DMatrix>() {\n  {\n    put(\"train\", trainMat);\n    put(\"test\", testMat);\n  }\n};\nint nround = 2;\nBooster booster = XGBoost.train(trainMat, params, nround, watches, null, null);\n```\n\n----------------------------------------\n\nTITLE: Using Scikit-Learn Metrics with XGBoost Regressor\nDESCRIPTION: This example shows how to use a scikit-learn cost function (mean absolute error) as an evaluation metric for XGBoost regressor. It demonstrates the integration between XGBoost and scikit-learn.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/custom_metric_obj.rst#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.metrics import mean_absolute_error\nX, y = load_diabetes(return_X_y=True)\nreg = xgb.XGBRegressor(\n    tree_method=\"hist\",\n    eval_metric=mean_absolute_error,\n)\nreg.fit(X, y, eval_set=[(X, y)])\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading XGBoost Binary Format\nDESCRIPTION: Saving DMatrix to binary format for faster loading in subsequent uses.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/python/python_intro.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndtrain = xgb.DMatrix('train.svm.txt?format=libsvm')\ndtrain.save_binary('train.buffer')\n```\n\n----------------------------------------\n\nTITLE: Optimizing Intercepts for Dirichlet Model in R\nDESCRIPTION: This snippet defines a function to find optimal intercepts for the Dirichlet model using R's optim function. It uses the previously defined Dirichlet likelihood and gradient functions.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/advanced_custom_obj.rst#2025-04-19_snippet_20\n\nLANGUAGE: R\nCODE:\n```\nget.optimal.intercepts <- function(y) {\n    k <- ncol(y)\n    broadcast.vec <- function(x) rep(x, nrow(y)) |> matrix(ncol=k, byrow=T)\n    res <- optim(\n        par = numeric(k),\n        fn = function(x) dirichlet.fun(broadcast.vec(x), y),\n        gr = function(x) dirichlet.grad(broadcast.vec(x), y) |> colSums(),\n        method = \"L-BFGS-B\"\n    )\n    return(res$par)\n}\nintercepts <- get.optimal.intercepts(y)\n```\n\n----------------------------------------\n\nTITLE: Installing XGBoost R Package\nDESCRIPTION: Install XGBoost R package from R Universe repository.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/install.rst#2025-04-19_snippet_2\n\nLANGUAGE: R\nCODE:\n```\ninstall.packages('xgboost', repos = c('https://dmlc.r-universe.dev', 'https://cloud.r-project.org'))\n```\n\n----------------------------------------\n\nTITLE: Creating Python Environment with XGBoost GPU Dependencies using Virtualenv\nDESCRIPTION: Shell commands for creating a Python virtual environment with XGBoost, CUDA dependencies, and packaging it for distribution to Spark workers.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/spark_estimator.rst#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv xgboost_env\nsource xgboost_env/bin/activate\npip install pyarrow pandas venv-pack xgboost\n# https://docs.rapids.ai/install#pip-install\npip install cudf-cu11 --extra-index-url=https://pypi.nvidia.com\nvenv-pack -o xgboost_env.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Submitting Federated XGBoost Job\nDESCRIPTION: This command submits the horizontal XGBoost federated learning job through the NVFlare admin CLI.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/nvflare/horizontal/README.md#2025-04-19_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nsubmit_job horizontal-xgboost\n```\n\n----------------------------------------\n\nTITLE: Spark Submit Command for CPU Version\nDESCRIPTION: Command to submit a Spark application using CPU-based XGBoost4j-Spark.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost_spark_migration.rst#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nspark-submit \\\n  --jars xgboost4j-spark_2.12-3.0.0.jar \\\n  ... \\\n```\n\n----------------------------------------\n\nTITLE: Configuring XGBoost Parameters with Rabit Tracker Timeout in Spark\nDESCRIPTION: Demonstrates an alternative approach to set XGBoost parameters including the rabit_tracker_timeout by passing a parameter map when creating the XGBoostClassifier.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost4j_spark_tutorial.rst#2025-04-19_snippet_18\n\nLANGUAGE: scala\nCODE:\n```\nval xgbParam = Map(\"eta\" -> 0.1f,\n   \"max_depth\" -> 2,\n   \"objective\" -> \"multi:softprob\",\n   \"num_class\" -> 3,\n   \"num_round\" -> 100,\n   \"num_workers\" -> 2,\n   \"rabit_tracker_timeout\" -> 60000L)\nval xgbClassifier = new XGBoostClassifier(xgbParam).\n    setFeaturesCol(\"features\").\n    setLabelCol(\"classIndex\")\n```\n\n----------------------------------------\n\nTITLE: LIBSVM Format with Embedded Query IDs for Ranking Tasks\nDESCRIPTION: Example of embedding query group IDs directly in LIBSVM format using 'qid:xx' token. Rows must be sorted by query IDs in ascending order, and all instances must include query IDs if this format is used.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/input_format.rst#2025-04-19_snippet_4\n\nLANGUAGE: none\nCODE:\n```\n1 qid:1 101:1.2 102:0.03\n0 qid:1 1:2.1 10001:300 10002:400\n0 qid:2 0:1.3 1:0.3\n1 qid:2 0:0.01 1:0.3\n0 qid:3 0:0.2 1:0.3\n1 qid:3 3:-0.1 10:-0.3\n0 qid:3 6:0.2 10:0.15\n```\n\n----------------------------------------\n\nTITLE: Installing XGBoost4j-Spark Maven Configuration\nDESCRIPTION: Maven configuration for including XGBoost4j-Spark dependency in JVM projects.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/install.rst#2025-04-19_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<properties>\n    ...\n    <!-- Specify Scala version in package name -->\n    <scala.binary.version>2.12</scala.binary.version>\n  </properties>\n\n  <dependencies>\n    ...\n    <dependency>\n        <groupId>ml.dmlc</groupId>\n        <artifactId>xgboost4j-spark_${scala.binary.version}</artifactId>\n        <version>latest_version_num</version>\n    </dependency>\n  </dependencies>\n```\n\n----------------------------------------\n\nTITLE: Loading Saved Models in Python\nDESCRIPTION: Shows how to load a model trained in Spark using Python XGBoost, enabling cross-language model serving.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost4j_spark_tutorial.rst#2025-04-19_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport xgboost as xgb\nbst = xgb.Booster({'nthread': 4})\nbst.load_model(\"/tmp/xgbClassificationModel/data/model\")\n```\n\n----------------------------------------\n\nTITLE: Installing Specific XGBoost Version in R\nDESCRIPTION: Demonstrates how to install a specific version of XGBoost in R using the remotes package. This can be useful for loading models saved with older versions.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/saving_model.rst#2025-04-19_snippet_3\n\nLANGUAGE: r\nCODE:\n```\nlibrary(remotes)\nremotes::install_version(\"xgboost\", \"0.90.0.1\")  # Install version 0.90.0.1\n```\n\n----------------------------------------\n\nTITLE: Getting Feature Count from XGBoost Booster in C\nDESCRIPTION: This code snippet shows how to retrieve the number of features in a dataset using XGBoosterGetNumFeature. It assumes that a booster has been created and trained on a dataset.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/c_api_tutorial.rst#2025-04-19_snippet_10\n\nLANGUAGE: c\nCODE:\n```\nbst_ulong num_of_features = 0;\n\n// Assuming booster variable of type BoosterHandle is already declared\n// and dataset is loaded and trained on booster\n// storing the results in num_of_features variable\nsafe_xgboost(XGBoosterGetNumFeature(booster, &num_of_features));\n\n// Printing number of features by type conversion of num_of_features variable from bst_ulong to unsigned long\nprintf(\"num_feature: %lu\\n\", (unsigned long)(num_of_features));\n```\n\n----------------------------------------\n\nTITLE: Visualizing Feature Importance in XGBoost Model\nDESCRIPTION: Loads a trained XGBoost model from the specified path and plots feature importance using XGBoost's built-in visualization tool.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/CLI/distributed-training/plot_model.ipynb#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# plot the first two trees.\nbst = xgb.Booster(model_file=model_file)\nxgb.plot_importance(bst)\n```\n\n----------------------------------------\n\nTITLE: Plotting Decision Tree Structure from XGBoost Model\nDESCRIPTION: Visualizes the structure of a specific tree (tree_id=0) from the loaded XGBoost model using the to_graphviz function.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/CLI/distributed-training/plot_model.ipynb#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntree_id = 0\nxgb.to_graphviz(bst, tree_id)\n```\n\n----------------------------------------\n\nTITLE: Docker Run with Extended Arguments\nDESCRIPTION: Command to run Docker container with additional memory and privileges for NCCL support.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/ci.rst#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython3 ops/docker_run.py \\\n  --image-uri 492475357299.dkr.ecr.us-west-2.amazonaws.com/xgb-ci.gpu:main \\\n  --use-gpus \\\n  --run-args='--shm-size=4g --privileged' \\\n  -- bash ops/pipeline/test-python-wheel-impl.sh gpu\n```\n\n----------------------------------------\n\nTITLE: GitHub Actions Runner Configuration in YAML\nDESCRIPTION: Syntax for configuring self-hosted runners in GitHub Actions using RunsOn service.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/ci.rst#2025-04-19_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nruns-on:\n  - runs-on\n  - runner=runner-name\n  - run-id=${{ github.run_id }}\n  - tag=[unique tag that uniquely identifies the job in the GH Action workflow]\n```\n\n----------------------------------------\n\nTITLE: Configuring XGBoost 2.0 Parameters for 1.7 Compatibility\nDESCRIPTION: Parameter configuration to achieve similar results as XGBoost 1.7's XGBRanker implementation. Includes settings for pair sampling method, objective function, normalization, and tree method.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/learning_to_rank.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nparams = {\n    \"lambdarank_pair_method\": \"mean\",\n    \"objective\": \"rank:pairwise\",\n    \"lambdarank_score_normalization\": False,\n    \"base_score\": 0.5,\n    \"tree_method\": \"approx\",\n    \"lambdarank_num_pair_per_sample\": 1,\n}\n```\n\n----------------------------------------\n\nTITLE: Example LIBSVM Format for XGBoost Training Data\nDESCRIPTION: A basic example of the LIBSVM format used for XGBoost training data. Each line represents an instance, starting with a label followed by feature index:value pairs. In binary classification, 1 indicates positive samples and 0 indicates negative samples.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/input_format.rst#2025-04-19_snippet_0\n\nLANGUAGE: none\nCODE:\n```\n1 101:1.2 102:0.03\n0 1:2.1 10001:300 10002:400\n0 0:1.3 1:0.3\n1 0:0.01 1:0.3\n0 0:0.2 1:0.3\n```\n\n----------------------------------------\n\nTITLE: Importing DMatrix in XGBoost4J (Java)\nDESCRIPTION: This snippet shows how to import the DMatrix class from XGBoost4J, which is used for handling data in various formats.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/java_intro.rst#2025-04-19_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nimport ml.dmlc.xgboost4j.java.DMatrix;\n```\n\n----------------------------------------\n\nTITLE: Saving XGBoost Model to File in C\nDESCRIPTION: This snippet demonstrates how to save an XGBoost model to a file using XGBoosterSaveModel. It requires a BoosterHandle and a file path for the model.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/c_api_tutorial.rst#2025-04-19_snippet_11\n\nLANGUAGE: c\nCODE:\n```\nBoosterHandle booster;\nconst char *model_path = \"/path/of/model.json\";\nsafe_xgboost(XGBoosterSaveModel(booster, model_path));\n```\n\n----------------------------------------\n\nTITLE: Splitting Dataset in Spark ML Pipeline with XGBoost\nDESCRIPTION: Divides the raw input DataFrame into training and test datasets with a 80:20 split ratio and a random seed for reproducibility.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost4j_spark_tutorial.rst#2025-04-19_snippet_12\n\nLANGUAGE: scala\nCODE:\n```\nval Array(training, test) = rawInput.randomSplit(Array(0.8, 0.2), 123)\n```\n\n----------------------------------------\n\nTITLE: Loading XGBoost Model in Java\nDESCRIPTION: This code illustrates how to load a previously saved XGBoost model using XGBoost4J in Java.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/java_intro.rst#2025-04-19_snippet_11\n\nLANGUAGE: java\nCODE:\n```\nBooster booster = XGBoost.loadModel(\"model.json\");\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenMP Threads in Python\nDESCRIPTION: Example of using the new XGBoost global configuration to set OpenMP threads, as an alternative to environment variables.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/changes/v3.0.0.rst#2025-04-19_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nxgboost.set_config(openmp_threads=4)\n```\n\n----------------------------------------\n\nTITLE: Converting String Labels to Numeric with StringIndexer\nDESCRIPTION: Shows how to transform string-based class labels into numeric indices using Spark's StringIndexer transformer, which is necessary for XGBoost classification.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost4j_spark_tutorial.rst#2025-04-19_snippet_1\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.ml.feature.StringIndexer\nval stringIndexer = new StringIndexer().\n  setInputCol(\"class\").\n  setOutputCol(\"classIndex\").\n  fit(rawInput)\nval labelTransformed = stringIndexer.transform(rawInput).drop(\"class\")\n```\n\n----------------------------------------\n\nTITLE: Constructing XGBoost Parameters in R\nDESCRIPTION: The xgb.params function in R allows users to construct a list object with XGBoost parameters. It provides IDE autocompletion and accepts all possible XGBoost parameters as arguments.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/R-package/adding_parameters.rst#2025-04-19_snippet_0\n\nLANGUAGE: R\nCODE:\n```\nxgb.params(max_depth, eta, device, ...)\n```\n\n----------------------------------------\n\nTITLE: Updating CI Container Image Tag\nDESCRIPTION: Shows how to specify the Docker image tag in the CI pipeline configuration.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/ci.rst#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nIMAGE_TAG=main\n```\n\n----------------------------------------\n\nTITLE: Marking Resolved Conflicts in Git\nDESCRIPTION: Command to mark a conflicted file as resolved after manual conflict resolution.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/git_guide.rst#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit add conflicted.py\n```\n\n----------------------------------------\n\nTITLE: Importing XGBClassifier from Concrete ML\nDESCRIPTION: Shows how to import the XGBClassifier from Concrete ML's sklearn module. This is necessary for using XGBoost models with Concrete ML's privacy-preserving features.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/privacy_preserving.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom concrete.ml.sklearn import XGBClassifier\n```\n\n----------------------------------------\n\nTITLE: Generating LIBSVM Format Input Data for XGBoost\nDESCRIPTION: Example of LIBSVM format input data for XGBoost. Each line represents an instance with label and feature:value pairs.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/CLI/binary_classification/README.md#2025-04-19_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n1 101:1.2 102:0.03\n0 1:2.1 10001:300 10002:400\n...\n```\n\n----------------------------------------\n\nTITLE: Loading Data into DMatrix from File using XGBoost C API\nDESCRIPTION: Example of loading data from a file into a DMatrix object using XGDMatrixCreateFromFile function.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/c_api_tutorial.rst#2025-04-19_snippet_6\n\nLANGUAGE: c\nCODE:\n```\nDMatrixHandle data; // handle to DMatrix\n// Load the data from file & store it in data variable of DMatrixHandle datatype\nsafe_xgboost(XGDMatrixCreateFromFile(\"/path/to/file/filename\", silent, &data));\n```\n\n----------------------------------------\n\nTITLE: Saving XGBoost Model in Java\nDESCRIPTION: This code shows how to save a trained XGBoost model to a file using XGBoost4J in Java.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/java_intro.rst#2025-04-19_snippet_9\n\nLANGUAGE: java\nCODE:\n```\nbooster.saveModel(\"model.json\");\n```\n\n----------------------------------------\n\nTITLE: XGBoost Ranking and Specialized Objective Functions\nDESCRIPTION: Specialized objectives for ranking tasks (NDCG, MAP), survival analysis, count data and specific distributions like Gamma and Tweedie\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst#2025-04-19_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\nrank:ndcg\nrank:map\nrank:pairwise\nsurvival:cox\nsurvival:aft\ncount:poisson\nreg:gamma\nreg:tweedie\n```\n\n----------------------------------------\n\nTITLE: Building Binary Wheel for XGBoost Python Package\nDESCRIPTION: Command to build a binary wheel for XGBoost. The --no-deps flag prevents building dependencies, and -v enables verbose output. The resulting wheel will contain a pre-compiled copy of libxgboost.so.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/python_packaging.rst#2025-04-19_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n$ pip wheel --no-deps -v .\n```\n\n----------------------------------------\n\nTITLE: Configuring XGBoost for Multiple Evaluation Metrics\nDESCRIPTION: Configuration snippet to monitor multiple evaluation metrics including error rates and log-likelihood.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/CLI/binary_classification/README.md#2025-04-19_snippet_9\n\nLANGUAGE: conf\nCODE:\n```\neval[test] = \"agaricus.txt.test\"\neval[trainname] = \"agaricus.txt.train\"\neval_metric=logloss\n```\n\n----------------------------------------\n\nTITLE: Simulating FHE Prediction with XGBClassifier\nDESCRIPTION: Shows how to run an FHE simulation for prediction. This step allows verification of model accuracy in encrypted computations without the cost of actual FHE execution.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/privacy_preserving.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npredictions = classifier.predict(X_test, fhe=\"simulate\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Table of Contents for XGBoost Release Notes in reStructuredText\nDESCRIPTION: This snippet sets up a table of contents for XGBoost release notes using reStructuredText directives. It specifies the maximum depth, caption, and includes links to version-specific release notes.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/changes/index.rst#2025-04-19_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n  :maxdepth: 1\n  :caption: Contents:\n\n  v3.0.0\n  v2.1.0\n```\n\n----------------------------------------\n\nTITLE: Configuring and Submitting XGBoost Application with Spark\nDESCRIPTION: This snippet demonstrates how to configure and submit an XGBoost application using spark-submit. It includes settings for Spark master, executor cores, GPU resources, RAPIDS plugin, and Python environment.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/spark_estimator.rst#2025-04-19_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nexport PYSPARK_PYTHON=./environment/bin/python\n\nspark-submit \\\n  --master spark://<master-ip>:7077 \\\n  --conf spark.executor.cores=12 \\\n  --conf spark.task.cpus=1 \\\n  --conf spark.executor.resource.gpu.amount=1 \\\n  --conf spark.task.resource.gpu.amount=0.08 \\\n  --packages com.nvidia:rapids-4-spark_2.12:24.04.1 \\\n  --conf spark.plugins=com.nvidia.spark.SQLPlugin \\\n  --conf spark.sql.execution.arrow.maxRecordsPerBatch=1000000 \\\n  --archives xgboost_env.tar.gz#environment \\\n  xgboost_app.py\n```\n\n----------------------------------------\n\nTITLE: Setting Up Python Environment for XGBoost Analysis\nDESCRIPTION: Imports necessary Python libraries and enables inline matplotlib visualization for Jupyter notebook.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/CLI/distributed-training/plot_model.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport sys\nimport os\n%matplotlib inline \n```\n\n----------------------------------------\n\nTITLE: Training XGBoost Model Without Constraints - Python\nDESCRIPTION: Basic example of training an XGBoost model without any monotonic constraints using Python. Uses standard training parameters with early stopping.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/monotonic.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmodel_no_constraints = xgb.train(params, dtrain,\n                                   num_boost_round = 1000, evals = evallist,\n                                   early_stopping_rounds = 10)\n```\n\n----------------------------------------\n\nTITLE: Single Instance Prediction in Scala\nDESCRIPTION: Shows how to perform prediction on a single instance using a trained XGBoost model. Note that this has high overhead and should be used carefully.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost4j_spark_tutorial.rst#2025-04-19_snippet_8\n\nLANGUAGE: scala\nCODE:\n```\nval features = xgbInput.head().getAs[Vector](\"features\")\nval result = xgbClassificationModel.predict(features)\n```\n\n----------------------------------------\n\nTITLE: Deploying XGBoost on Kubernetes with Dask\nDESCRIPTION: Demonstrates how to use KubeCluster to deploy Dask on Kubernetes and run XGBoost training. It includes setting up the cluster, creating a client, and training an XGBoost regressor on distributed data.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/dask.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom dask_kubernetes.operator import KubeCluster\nfrom dask_kubernetes.operator.kubecluster.kubecluster import CreateMode\n\nfrom dask.distributed import Client\nfrom xgboost import dask as dxgb\nimport dask.array as da\n\n\ndef main():\n    '''Connect to a remote kube cluster with GPU nodes and run training on it.'''\n    m = 1000\n    n = 10\n    kWorkers = 2                # assuming you have 2 GPU nodes on that cluster.\n    # You need to work out the worker-spec yourself.  See document in dask_kubernetes for\n    # its usage.  Here we just want to show that XGBoost works on various clusters.\n\n    # See notes below for why we use pre-allocated cluster.\n    with KubeCluster(\n        name=\"xgboost-test\",\n        image=\"my-image-name:latest\",\n        n_workers=kWorkers,\n        create_mode=CreateMode.CONNECT_ONLY,\n        shutdown_on_close=False,\n    ) as cluster:\n        with Client(cluster) as client:\n            X = da.random.random(size=(m, n), chunks=100)\n            y = X.sum(axis=1)\n\n            regressor = dxgb.DaskXGBRegressor(n_estimators=10, missing=0.0)\n            regressor.client = client\n            regressor.set_params(tree_method='hist', device=\"cuda\")\n            regressor.fit(X, y, eval_set=[(X, y)])\n\n\nif __name__ == '__main__':\n    # Launch the kube cluster on somewhere like GKE, then run this as client process.\n    # main function will connect to that cluster and start training xgboost model.\n    main()\n```\n\n----------------------------------------\n\nTITLE: LIBSVM Format with Embedded Instance Weights\nDESCRIPTION: Example of embedding instance weights directly in LIBSVM format by appending weights to labels in the form of 'label:weight'. This approach allows different weighting of instances within the same data file.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/input_format.rst#2025-04-19_snippet_5\n\nLANGUAGE: none\nCODE:\n```\n1:1.0 101:1.2 102:0.03\n0:0.5 1:2.1 10001:300 10002:400\n0:0.5 0:1.3 1:0.3\n1:1.0 0:0.01 1:0.3\n0:0.5 0:0.2 1:0.3\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependency for XGBoost4j-Spark CPU\nDESCRIPTION: Maven dependency configuration for CPU-based XGBoost4j-Spark implementation.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost_spark_migration.rst#2025-04-19_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n    <groupId>ml.dmlc</groupId>\n    <artifactId>xgboost4j-spark_${scala.binary.version}</artifactId>\n    <version>3.0.0</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents in reStructuredText for XGBoost Tutorials\nDESCRIPTION: This snippet defines the structure of the XGBoost tutorials documentation using reStructuredText directives. It sets up a table of contents with a specific depth and includes various tutorial topics.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/index.rst#2025-04-19_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n  :maxdepth: 1\n  :caption: Contents:\n\n  model\n  saving_model\n  slicing_model\n  learning_to_rank\n  dart\n  monotonic\n  feature_interaction_constraint\n  aft_survival_analysis\n  categorical\n  multioutput\n  rf\n  kubernetes\n  Distributed XGBoost with XGBoost4J-Spark <https://xgboost.readthedocs.io/en/latest/jvm/xgboost4j_spark_tutorial.html>\n  Distributed XGBoost with XGBoost4J-Spark-GPU <https://xgboost.readthedocs.io/en/latest/jvm/xgboost4j_spark_gpu_tutorial.html>\n  dask\n  spark_estimator\n  ray\n  external_memory\n  c_api_tutorial\n  input_format\n  param_tuning\n  custom_metric_obj\n  advanced_custom_obj\n  intercept\n  privacy_preserving\n```\n\n----------------------------------------\n\nTITLE: Building XGBoost R Package with GPU Support\nDESCRIPTION: Commands to build the XGBoost R package with CUDA GPU support using CMake. This enables GPU acceleration for training and inference in the R interface.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/build.rst#2025-04-19_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -S . -DUSE_CUDA=ON -DR_LIB=ON\ncmake --build build --target install -j$(nproc)\n```\n\n----------------------------------------\n\nTITLE: Installing XGBoost4J to Local Maven Repository\nDESCRIPTION: Maven commands to build XGBoost4J Java bindings and install them to the local Maven repository. Includes options for skipping tests during the build process.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/build.rst#2025-04-19_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nmvn install\n```\n\nLANGUAGE: bash\nCODE:\n```\nmvn -DskipTests install\n```\n\n----------------------------------------\n\nTITLE: Creating XGBoost R Object Library in CMake\nDESCRIPTION: Defines an object library target for the R package components which allows symbols to be properly exposed during linking.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/R-package/CMakeLists.txt#2025-04-19_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\n# Use object library to expose symbols\nadd_library(xgboost-r OBJECT ${R_SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Multi-GPU training with distributed framework in Python\nDESCRIPTION: Example of multi-GPU training using a distributed framework like Dask, as single-process multi-GPU training is no longer supported.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/NEWS.md#2025-04-19_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\n# Pseudo-code for multi-GPU training with Dask\nimport dask_xgboost as dxgb\nimport dask.array as da\n\n# Assume 'client' is a Dask client with multiple GPUs\nX, y = da.random.random((1000, 10)), da.random.random(1000)\nmodel = dxgb.train(client, {'tree_method': 'gpu_hist'}, dtrain)\n```\n\n----------------------------------------\n\nTITLE: Using DART tree weights for SHAP values in Python\nDESCRIPTION: Improved accuracy for SHAP (SHapley Additive exPlanations) values computation in DART (Dropouts meet Multiple Additive Regression Trees) models.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/NEWS.md#2025-04-19_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\n# SHAP values now automatically use DART tree weights\nshap_values = booster.predict(dtest, pred_contribs=True)\n```\n\n----------------------------------------\n\nTITLE: Enabling RMM Usage in XGBoost Python API\nDESCRIPTION: Python code example showing how to force XGBoost to use RMM for all memory allocations using the config_context.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/rmm_plugin/README.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nwith xgb.config_context(use_rmm=True):\n    clf = xgb.XGBClassifier(tree_method=\"hist\", device=\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Error Handling Macro for XGBoost C API in C++\nDESCRIPTION: A macro to wrap XGBoost C API function calls for error checking and exception throwing in C++ applications.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/c_api_tutorial.rst#2025-04-19_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n#define safe_xgboost(call) {  \n  int err = (call); \n  if (err != 0) { \n    throw std::runtime_error(std::string(__FILE__) + \":\" + std::to_string(__LINE__) + \n                        \": error in \" + #call + \":\" + XGBGetLastError());  \n  } \n}\n```\n\n----------------------------------------\n\nTITLE: Redirecting XGBoost Evaluation Progress to Log File\nDESCRIPTION: Command to redirect XGBoost's evaluation progress to a log file for monitoring.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/CLI/binary_classification/README.md#2025-04-19_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n../../xgboost mushroom.conf 2>log.txt\n```\n\n----------------------------------------\n\nTITLE: Spark Submit Command for GPU Version\nDESCRIPTION: Command to submit a Spark application using GPU-accelerated XGBoost4j-Spark.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost_spark_migration.rst#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nspark-submit \\\n  --jars xgboost4j-spark-gpu_2.12-3.0.0.jar \\\n  ... \\\n```\n\n----------------------------------------\n\nTITLE: Installing Python Package with Custom Build Options\nDESCRIPTION: Command for installing XGBoost Python package with custom configuration settings like CUDA and NCCL support.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/build.rst#2025-04-19_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install -v . --config-settings use_cuda=True --config-settings use_nccl=True\n```\n\n----------------------------------------\n\nTITLE: Collecting R Package Source Files in CMake\nDESCRIPTION: Recursively gathers all C and C++ source files in the src directory that will be compiled as part of the R package.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/R-package/CMakeLists.txt#2025-04-19_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfile(\n  GLOB_RECURSE R_SOURCES\n  ${CMAKE_CURRENT_LIST_DIR}/src/*.cc\n  ${CMAKE_CURRENT_LIST_DIR}/src/*.c\n)\n```\n\n----------------------------------------\n\nTITLE: Predicting with DART Booster in XGBoost\nDESCRIPTION: Example of how to use the predict() function with a DART booster in XGBoost to obtain correct results on test sets by setting the iteration_range parameter.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npreds = bst.predict(dtest, iteration_range=(0, num_round))\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost Dirichlet Model in R\nDESCRIPTION: This snippet shows how to train an XGBoost model with a Dirichlet objective function in R. It sets up the model parameters, trains the model, and makes predictions using softmax.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/advanced_custom_obj.rst#2025-04-19_snippet_18\n\nLANGUAGE: R\nCODE:\n```\ndtrain <- xgb.DMatrix(x, y)\nbooster <- xgb.train(\n    params = list(\n        tree_method=\"hist\",\n        num_target=ncol(y),\n        base_score=0,\n        disable_default_eval_metric=TRUE,\n        max_depth=3,\n        seed=123\n    ),\n    data = dtrain,\n    nrounds = 10,\n    obj = dirichlet.xgb.objective,\n    evals = list(Train=dtrain),\n    eval_metric = dirichlet.eval.metric\n)\nraw.pred <- predict(booster, x, reshape=TRUE)\nyhat <- apply(raw.pred, 1, softmax) |> t()\n```\n\n----------------------------------------\n\nTITLE: GraphViz Model Export\nDESCRIPTION: Enhancement to dump_model() and get_dump() functions to support GraphViz format export\nSOURCE: https://github.com/dmlc/xgboost/blob/master/NEWS.md#2025-04-19_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\ndump_model()\nget_dump()\n```\n\n----------------------------------------\n\nTITLE: Reading Iris Dataset with Spark's CSV Reader in Scala\nDESCRIPTION: Loads the Iris dataset from CSV format using Spark's built-in reader. The code defines a schema with four feature columns and a label column, then reads the CSV file into a DataFrame.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost4j_spark_gpu_tutorial.rst#2025-04-19_snippet_0\n\nLANGUAGE: scala\nCODE:\n```\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.types.{DoubleType, StringType, StructField, StructType}\n\nval spark = SparkSession.builder().getOrCreate()\n\nval labelName = \"class\"\nval schema = new StructType(Array(\n    StructField(\"sepal length\", DoubleType, true),\n    StructField(\"sepal width\", DoubleType, true),\n    StructField(\"petal length\", DoubleType, true),\n    StructField(\"petal width\", DoubleType, true),\n    StructField(labelName, StringType, true)))\n\nval xgbInput = spark.read.option(\"header\", \"false\")\n    .schema(schema)\n    .csv(dataPath)\n```\n\n----------------------------------------\n\nTITLE: XGBoost Function Signature in R\nDESCRIPTION: The xgboost function in R has a specific parameter order. New parameters should be added after the tree_method parameter, maintaining a similar order to xgb.params.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/R-package/adding_parameters.rst#2025-04-19_snippet_2\n\nLANGUAGE: R\nCODE:\n```\nxgboost(..., tree_method, new_param1, new_param2, ...)\n```\n\n----------------------------------------\n\nTITLE: Installing libomp Dependency on MacOS\nDESCRIPTION: Command for installing libomp on MacOS using Homebrew, which is required for building XGBoost on Mac systems.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/build.rst#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nbrew install libomp\n```\n\n----------------------------------------\n\nTITLE: Configuring XGBoost with Checkpointing Parameters in Spark\nDESCRIPTION: Shows how to set XGBoost parameters including checkpoint configuration by passing them in a parameter map when creating the XGBoostClassifier.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost4j_spark_tutorial.rst#2025-04-19_snippet_20\n\nLANGUAGE: scala\nCODE:\n```\nval xgbParam = Map(\"eta\" -> 0.1f,\n   \"max_depth\" -> 2,\n   \"objective\" -> \"multi:softprob\",\n   \"num_class\" -> 3,\n   \"num_round\" -> 100,\n   \"num_workers\" -> 2,\n   \"checkpoint_path\" -> \"/checkpoints_path\",\n   \"checkpoint_interval\" -> 2)\nval xgbClassifier = new XGBoostClassifier(xgbParam).\n    setFeaturesCol(\"features\").\n    setLabelCol(\"classIndex\")\n```\n\n----------------------------------------\n\nTITLE: Configuring SYCL Plugin Support\nDESCRIPTION: Sets up the Intel oneAPI SYCL support with custom linking commands using icpx compiler.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/CMakeLists.txt#2025-04-19_snippet_16\n\nLANGUAGE: CMake\nCODE:\n```\nif(PLUGIN_SYCL)\n  set(CMAKE_CXX_LINK_EXECUTABLE\n      \"icpx <FLAGS> <CMAKE_CXX_LINK_FLAGS> -qopenmp <LINK_FLAGS> <OBJECTS> -o <TARGET> <LINK_LIBRARIES>\")\n  set(CMAKE_CXX_CREATE_SHARED_LIBRARY\n      \"icpx <CMAKE_SHARED_LIBRARY_CXX_FLAGS> -qopenmp <LANGUAGE_COMPILE_FLAGS> \\\n      <CMAKE_SHARED_LIBRARY_CREATE_CXX_FLAGS> <SONAME_FLAG>,<TARGET_SONAME> \\\n      -o <TARGET> <OBJECTS> <LINK_LIBRARIES>\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting C++ Standard and Position Independent Code for R Package\nDESCRIPTION: Configures the XGBoost R package to use C++17 standard and enables position-independent code which is required for shared libraries.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/R-package/CMakeLists.txt#2025-04-19_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nset_target_properties(\n  xgboost-r PROPERTIES\n  CXX_STANDARD 17\n  CXX_STANDARD_REQUIRED ON\n  POSITION_INDEPENDENT_CODE ON\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Logistic Loss Function in Python\nDESCRIPTION: This snippet implements the logistic loss function, which is commonly used for logistic regression tasks. It calculates the loss based on the predicted probabilities and actual binary outcomes.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/model.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport math\n\ndef logistic_loss(y_true, y_pred):\n    return sum(y_true[i] * math.log(1 + math.exp(-y_pred[i])) + \n               (1 - y_true[i]) * math.log(1 + math.exp(y_pred[i])) \n               for i in range(len(y_true)))\n```\n\n----------------------------------------\n\nTITLE: Setting Refresh Updater Configuration in XGBoost\nDESCRIPTION: Configuration parameters to enable the refresh updater independently, which updates tree statistics and optionally leaf values based on new training data.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/treemethod.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n{\"process_type\": \"update\", \"updater\": \"refresh\"}\n```\n\n----------------------------------------\n\nTITLE: Testing Dirichlet Expected Hessian in R\nDESCRIPTION: R test function that validates the expected Hessian implementation against empirically calculated values from Dirichlet distribution samples.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/advanced_custom_obj.rst#2025-04-19_snippet_7\n\nLANGUAGE: r\nCODE:\n```\ntest_that(\"expected hess\", {\n    k <- 3L\n    set.seed(123)\n    x0 <- rnorm(k)\n    alpha <- exp(x0)\n    n.samples <- 5e6\n    y.samples <- rdirichlet(n.samples, alpha)\n    \n    x.broadcast <- rep(x0, n.samples) |> matrix(ncol=k, byrow=T)\n    grad.samples <- dirichlet.grad(x.broadcast, y.samples)\n    ref <- crossprod(grad.samples) / n.samples\n    Ehess <- dirichlet.expected.hess(matrix(x0, nrow=1))\n    expect_equal(Ehess[1,,], ref, tolerance=1e-2)\n})\n```\n\n----------------------------------------\n\nTITLE: Complexity Definition for Trees in XGBoost\nDESCRIPTION: This formula defines the complexity regularization term for trees in XGBoost, which consists of a penalty for the number of leaves (γT) and a penalty for the magnitude of leaf weights.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/model.rst#2025-04-19_snippet_7\n\nLANGUAGE: math\nCODE:\n```\n\\omega(f) = \\gamma T + \\frac{1}{2}\\lambda \\sum_{j=1}^T w_j^2\n```\n\n----------------------------------------\n\nTITLE: Running XGBoost Multi-Class Classification Experiment with bash\nDESCRIPTION: A bash script command to execute the experiment using the runexp.sh script, which likely contains the necessary steps to download the dataset, train the XGBoost model, and evaluate the results.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/multiclass_classification/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./runexp.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring XGBoost4j-Spark Dependencies in Maven\nDESCRIPTION: Maven configuration for adding XGBoost4j-Spark as a dependency. Uses the scala.binary.version property to specify the Scala version in the artifact name.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/install.rst#2025-04-19_snippet_5\n\nLANGUAGE: xml\nCODE:\n```\n<properties>\n  ...\n  <!-- Specify Scala version in package name -->\n  <scala.binary.version>2.12</scala.binary.version>\n</properties>\n\n<dependencies>\n  <dependency>\n      <groupId>ml.dmlc</groupId>\n      <artifactId>xgboost4j-spark_${scala.binary.version}</artifactId>\n      <version>latest_version_num-SNAPSHOT</version>\n  </dependency>\n</dependencies>\n```\n\n----------------------------------------\n\nTITLE: Adding XGBoost Plugin Support\nDESCRIPTION: Adds the XGBoost plugin subdirectory for extensibility.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/CMakeLists.txt#2025-04-19_snippet_14\n\nLANGUAGE: CMake\nCODE:\n```\n# Plugin\nadd_subdirectory(${xgboost_SOURCE_DIR}/plugin)\n```\n\n----------------------------------------\n\nTITLE: Running XGBoost GPU Tests in Docker\nDESCRIPTION: This command runs a Docker container with GPU support to test XGBoost's Python wheel. It mounts the XGBoost directory, sets environment variables, enables GPU access, and executes a test script.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/ci.rst#2025-04-19_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm --pid=host --gpus all \\\n  -w /workspace -v /path/to/xgboost:/workspace \\\n  -e CI_BUILD_UID=<uid> -e CI_BUILD_USER=<user_name> \\\n  -e CI_BUILD_GID=<gid> -e CI_BUILD_GROUP=<group_name> \\\n  --shm-size=4g --privileged \\\n  492475357299.dkr.ecr.us-west-2.amazonaws.com/xgb-ci.gpu:main \\\n  bash ops/pipeline/test-python-wheel-impl.sh gpu\n```\n\n----------------------------------------\n\nTITLE: Installing and Importing XGBoost in Python\nDESCRIPTION: Basic verification of XGBoost installation by importing the package.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/python/python_intro.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport xgboost as xgb\n```\n\n----------------------------------------\n\nTITLE: Initializing XGBClassifier with Concrete ML\nDESCRIPTION: Demonstrates how to initialize an XGBClassifier with Concrete ML. The 'n_bits' parameter determines the precision of input features, affecting accuracy and FHE execution time. Other XGBoost hyperparameters can also be used.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/privacy_preserving.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclassifier = XGBClassifier(n_bits=6, [other_hyperparameters])\n```\n\n----------------------------------------\n\nTITLE: Adding XGBoost4J Maven Dependency\nDESCRIPTION: XML snippet showing how to include XGBoost4J as a dependency in a Maven project's pom.xml file. This allows Java projects to use XGBoost for machine learning tasks.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/build.rst#2025-04-19_snippet_18\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n  <groupId>ml.dmlc</groupId>\n  <artifactId>xgboost4j</artifactId>\n  <version>latest_source_version_num</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Loading XGBoost R Package Using Devtools\nDESCRIPTION: R code snippet that uses the devtools package to load XGBoost R package directly from the source directory without installation. This is useful for development and testing.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/build.rst#2025-04-19_snippet_14\n\nLANGUAGE: R\nCODE:\n```\nlibrary(devtools)\ndevtools::load_all(path = \"/path/to/xgboost/R-package\")\n```\n\n----------------------------------------\n\nTITLE: Optimal Leaf Weight and Objective Value in XGBoost\nDESCRIPTION: These formulas provide the optimal weight for each leaf and the corresponding optimal objective value for a given tree structure, which serves as a quality measure for the tree.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/model.rst#2025-04-19_snippet_10\n\nLANGUAGE: math\nCODE:\n```\nw_j^\\ast &= -\\frac{G_j}{H_j+\\lambda}\\\\\n  \\text{obj}^\\ast &= -\\frac{1}{2} \\sum_{j=1}^T \\frac{G_j^2}{H_j+\\lambda} + \\gamma T\n```\n\n----------------------------------------\n\nTITLE: Building XGBoost HTML Documentation with Sphinx\nDESCRIPTION: Command to build the HTML documentation for XGBoost using Sphinx. This is the basic command that processes all the reStructuredText files in the doc/ directory to generate the complete documentation site.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/docs.rst#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake html\n```\n\n----------------------------------------\n\nTITLE: Model Persistence in Scala\nDESCRIPTION: Demonstrates how to save and load XGBoost models to/from the filesystem, with support for different formats including JSON.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost4j_spark_tutorial.rst#2025-04-19_snippet_9\n\nLANGUAGE: scala\nCODE:\n```\nval xgbClassificationModelPath = \"/tmp/xgbClassificationModel\"\nxgbClassificationModel.write.overwrite().save(xgbClassificationModelPath)\n```\n\n----------------------------------------\n\nTITLE: Memory Management for XGBoost Handles in C\nDESCRIPTION: Example of proper memory management for BoosterHandle and DMatrixHandle in C applications using XGBoost.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/c_api_tutorial.rst#2025-04-19_snippet_5\n\nLANGUAGE: c\nCODE:\n```\n#include <assert.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <xgboost/c_api.h>\n\nint main(int argc, char** argv) {\n  int silent = 0;\n\n  BoosterHandle booster;\n\n  // do something with booster\n\n  //free the memory\n  XGBoosterFree(booster);\n\n  DMatrixHandle DMatrixHandle_param;\n\n  // do something with DMatrixHandle_param\n\n  // free the memory\n  XGDMatrixFree(DMatrixHandle_param);\n\n  return 0;\n}\n```\n\n----------------------------------------\n\nTITLE: Running XGBoost CPU Build in Docker\nDESCRIPTION: This command runs a Docker container to build XGBoost for CPU. It mounts the XGBoost directory, sets environment variables for user and group, and executes a build script.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/ci.rst#2025-04-19_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\n-e CI_BUILD_GID=<gid> -e CI_BUILD_GROUP=<group_name> \\\n492475357299.dkr.ecr.us-west-2.amazonaws.com/xgb-ci.cpu:main \\\nbash ops/pipeline/build-cpu-impl.sh cpu\n```\n\n----------------------------------------\n\nTITLE: Setting SYCL Device Parameter in Python\nDESCRIPTION: Example of how to specify the SYCL device parameter in Python to offload model training and inference on a specific SYCL device. This snippet shows how to select the first GPU device.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/plugin/sycl/README.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nparam['device'] = 'sycl:gpu:0'\n```\n\n----------------------------------------\n\nTITLE: Installing Python Package Using System Library\nDESCRIPTION: Command for installing the XGBoost Python package using the system's libxgboost.so instead of building a new one.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/build.rst#2025-04-19_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ncd python-package\npip install . --config-settings use_system_libxgboost=True\n```\n\n----------------------------------------\n\nTITLE: Loading LIBSVM Data with Index Mode in Python\nDESCRIPTION: Demonstrates loading LIBSVM format data with correct indexing mode for consistency between Spark and Python implementations.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost4j_spark_tutorial.rst#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nxgb.DMatrix('test.libsvm?indexing_mode=1')\n```\n\n----------------------------------------\n\nTITLE: Defining Linear Model Prediction in Python\nDESCRIPTION: This snippet shows how to calculate the prediction for a linear model using a sum of weighted input features. It demonstrates the basic mathematical structure of a linear model in supervised learning.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/model.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef linear_model_prediction(x, theta):\n    return sum(theta[j] * x[j] for j in range(len(x)))\n```\n\n----------------------------------------\n\nTITLE: Running Python Tests Setup\nDESCRIPTION: Commands to install pytest and set up environment for Python tests\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/unit_tests.rst#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip3 install pytest\nexport PYTHONPATH=./python-package\npytest -v -s --fulltrace tests/python\n```\n\n----------------------------------------\n\nTITLE: Loading Pandas DataFrame into XGBoost DMatrix\nDESCRIPTION: Creating a DMatrix from Pandas DataFrames containing features and labels.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/python/python_intro.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndata = pandas.DataFrame(np.arange(12).reshape((4,3)), columns=['a', 'b', 'c'])\nlabel = pandas.DataFrame(np.random.randint(2, size=4))\ndtrain = xgb.DMatrix(data, label=label)\n```\n\n----------------------------------------\n\nTITLE: Loading Arctic Lake Compositional Data in Python\nDESCRIPTION: Loading the Arctic Lake dataset, a compositional data example with depth as a predictor and sediment composition (sand, silt, clay) as the response variable.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/advanced_custom_obj.rst#2025-04-19_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# depth\nX = np.array([\n    10.4,11.7,12.8,13,15.7,16.3,18,18.7,20.7,22.1,\n    22.4,24.4,25.8,32.5,33.6,36.8,37.8,36.9,42.2,47,\n    47.1,48.4,49.4,49.5,59.2,60.1,61.7,62.4,69.3,73.6,\n    74.4,78.5,82.9,87.7,88.1,90.4,90.6,97.7,103.7,\n]).reshape((-1,1))\n# sand, silt, clay\nY = np.array([\n    [0.775,0.195,0.03], [0.719,0.249,0.032], [0.507,0.361,0.132],\n    [0.522,0.409,0.066], [0.7,0.265,0.035], [0.665,0.322,0.013],\n    [0.431,0.553,0.016], [0.534,0.368,0.098], [0.155,0.544,0.301],\n    [0.317,0.415,0.268], [0.657,0.278,0.065], [0.704,0.29,0.006],\n    [0.174,0.536,0.29], [0.106,0.698,0.196], [0.382,0.431,0.187],\n    [0.108,0.527,0.365], [0.184,0.507,0.309], [0.046,0.474,0.48],\n    [0.156,0.504,0.34], [0.319,0.451,0.23], [0.095,0.535,0.37],\n    [0.171,0.48,0.349], [0.105,0.554,0.341], [0.048,0.547,0.41],\n    [0.026,0.452,0.522], [0.114,0.527,0.359], [0.067,0.469,0.464],\n    [0.069,0.497,0.434], [0.04,0.449,0.511], [0.074,0.516,0.409],\n    [0.048,0.495,0.457], [0.045,0.485,0.47], [0.066,0.521,0.413],\n    [0.067,0.473,0.459], [0.074,0.456,0.469], [0.06,0.489,0.451],\n    [0.063,0.538,0.399], [0.025,0.48,0.495], [0.02,0.478,0.502],\n])\n```\n\n----------------------------------------\n\nTITLE: Linking Standard Filesystem Library for GCC 8.x\nDESCRIPTION: Links the C++ filesystem library (stdc++fs) for GCC versions less than 9.0, which require explicit linkage.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/CMakeLists.txt#2025-04-19_snippet_11\n\nLANGUAGE: CMake\nCODE:\n```\n# Link -lstdc++fs for GCC 8.x\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\" AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS \"9.0\")\n  target_link_libraries(objxgboost PUBLIC stdc++fs)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Predicting with DART models in Python\nDESCRIPTION: By default, trees are no longer dropped during DART prediction, improving prediction accuracy.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/NEWS.md#2025-04-19_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\n# Trees are not dropped by default in DART prediction\npredictions = booster.predict(dtest)\n```\n\n----------------------------------------\n\nTITLE: Creating DMatrix from CSR Sparse Matrix in XGBoost4J (Java)\nDESCRIPTION: This snippet shows how to create a DMatrix object from a Compressed Sparse Row (CSR) format sparse matrix in XGBoost4J.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/java_intro.rst#2025-04-19_snippet_2\n\nLANGUAGE: java\nCODE:\n```\nlong[] rowHeaders = new long[] {0,2,4,7};\nfloat[] data = new float[] {1f,2f,4f,3f,3f,1f,2f};\nint[] colIndex = new int[] {0,2,0,3,0,1,2};\nint numColumn = 4;\nDMatrix dmat = new DMatrix(rowHeaders, colIndex, data, DMatrix.SparseType.CSR, numColumn);\n```\n\n----------------------------------------\n\nTITLE: Installing XGBoost from Binary Wheel\nDESCRIPTION: Command to install XGBoost from a binary wheel. Since the wheel already contains the pre-built binary (libxgboost.so), the installation completes quickly without needing to compile anything.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/python_packaging.rst#2025-04-19_snippet_3\n\nLANGUAGE: console\nCODE:\n```\n$ pip install xgboost-2.0.0-py3-none-linux_x86_64.whl  # Completes quickly\n```\n\n----------------------------------------\n\nTITLE: Loading Arctic Lake Compositional Data in R\nDESCRIPTION: R code to load the Arctic Lake dataset, which consists of depth as a predictor and sediment composition (sand, silt, clay) as the response variable.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/advanced_custom_obj.rst#2025-04-19_snippet_15\n\nLANGUAGE: r\nCODE:\n```\ndata(\"ArcticLake\", package=\"DirichletReg\")\nx <- ArcticLake[, c(\"depth\"), drop=F]\ny <- ArcticLake[, c(\"sand\", \"silt\", \"clay\")] |> as.matrix()\n```\n\n----------------------------------------\n\nTITLE: Tree Function Definition in XGBoost\nDESCRIPTION: This formula defines the tree function mathematically, where w represents the vector of scores on leaves, q is a function assigning data points to leaves, and T is the number of leaves.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/model.rst#2025-04-19_snippet_6\n\nLANGUAGE: math\nCODE:\n```\nf_t(x) = w_{q(x)}, w \\in R^T, q:R^d\\rightarrow \\{1,2,\\cdots,T\\} .\n```\n\n----------------------------------------\n\nTITLE: Running Distributed Framework Tests\nDESCRIPTION: Commands to run tests for distributed frameworks like Dask and PySpark\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/unit_tests.rst#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport PYTHONPATH=./python-package\npytest -v -s --fulltrace tests/test_distributed\n```\n\n----------------------------------------\n\nTITLE: Setting Global Configuration in Python\nDESCRIPTION: Uses xgboost.config_context() to set global configuration parameters like verbosity and use of RAPIDS Memory Manager.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nxgboost.config_context(verbosity=2, use_rmm=True)\n```\n\n----------------------------------------\n\nTITLE: Propagating R Environment Variables to Parent Scope\nDESCRIPTION: Sets R-specific environment variables in the parent scope to make them available to other CMake files in the project.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/R-package/CMakeLists.txt#2025-04-19_snippet_11\n\nLANGUAGE: CMake\nCODE:\n```\nset(LIBR_HOME \"${LIBR_HOME}\" PARENT_SCOPE)\nset(LIBR_EXECUTABLE \"${LIBR_EXECUTABLE}\" PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Enabling Checkpointing for XGBoost Training in Spark\nDESCRIPTION: Sets up checkpointing during XGBoost training to facilitate recovery from failures by specifying the checkpoint interval and storage path.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost4j_spark_tutorial.rst#2025-04-19_snippet_19\n\nLANGUAGE: scala\nCODE:\n```\nxgbClassifier.setCheckpointInterval(2)\nxgbClassifier.setCheckpointPath(\"/checkpoint_path\")\n```\n\n----------------------------------------\n\nTITLE: XGBoost Configuration for Binary Classification\nDESCRIPTION: Sample configuration file for XGBoost binary classification task, including general parameters, tree booster parameters, and task parameters.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/CLI/binary_classification/README.md#2025-04-19_snippet_3\n\nLANGUAGE: conf\nCODE:\n```\n# General Parameters, see comment for each definition\n# can be gbtree or gblinear\nbooster = gbtree\n# choose logistic regression loss function for binary classification\nobjective = binary:logistic\n\n# Tree Booster Parameters\n# step size shrinkage\neta = 1.0\n# minimum loss reduction required to make a further partition\ngamma = 1.0\n# minimum sum of instance weight(hessian) needed in a child\nmin_child_weight = 1\n# maximum depth of a tree\nmax_depth = 3\n\n# Task Parameters\n# the number of round to do boosting\nnum_round = 2\n# 0 means do not save any model except the final round model\nsave_period = 0\n# The path of training data\ndata = \"agaricus.txt.train\"\n# The path of validation data, used to monitor training process, here [test] sets name of the validation set\neval[test] = \"agaricus.txt.test\"\n# The path of test data\ntest:data = \"agaricus.txt.test\"\n```\n\n----------------------------------------\n\nTITLE: Instance Weight File Format for XGBoost\nDESCRIPTION: Format for specifying instance weights in XGBoost. Each line contains a weight value for the corresponding instance in the training file. This file should be named with '.weight' extension appended to the instance file name.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/input_format.rst#2025-04-19_snippet_2\n\nLANGUAGE: none\nCODE:\n```\n1\n0.5\n0.5\n1\n0.5\n```\n\n----------------------------------------\n\nTITLE: Running Python Linting Checks for XGBoost\nDESCRIPTION: Bash command to run Python linting checks on the XGBoost codebase. This uses a custom script to perform linting with tools like pylint, black, isort, and mypy.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/coding_guide.rst#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd /path/to/xgboost/\npython ./ops/script/lint_python.py --fix\n```\n\n----------------------------------------\n\nTITLE: Running Tests with CTest\nDESCRIPTION: Command to run all unit tests using CTest with verbose output\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/unit_tests.rst#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nctest --verbose\n```\n\n----------------------------------------\n\nTITLE: Spark and Flink Integration Examples\nDESCRIPTION: Examples demonstrating distributed training integration with Apache Spark and Apache Flink frameworks.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/jvm-packages/xgboost4j-example/README.md#2025-04-19_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nSparkMLlibPipeline.scala\nDistTrainWithFlink.scala\n```\n\n----------------------------------------\n\nTITLE: Compiling XGBoost Python Library\nDESCRIPTION: Instructions for compiling the XGBoost Python library. This step is necessary before running the Higgs challenge script.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/kaggle-higgs/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncd ../..\\nmake\n```\n\n----------------------------------------\n\nTITLE: Installing XGBoost with Conda and CMake\nDESCRIPTION: Commands to clone the XGBoost repository, build it with CMake, and install it in a Conda environment.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/c_api_tutorial.rst#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# clone the XGBoost repository & its submodules\ngit clone --recursive https://github.com/dmlc/xgboost\ncd xgboost\n# Activate the Conda environment, into which we'll install XGBoost\nconda activate [env_name]\n# Build the compiled version of XGBoost inside the build folder\ncmake -B build -S . -DCMAKE_INSTALL_PREFIX=$CONDA_PREFIX\n# install XGBoost in your conda environment (usually under [your home directory]/miniconda3)\ncmake --build build --target install\n```\n\n----------------------------------------\n\nTITLE: Loading CSV Files into XGBoost DMatrix\nDESCRIPTION: Loading data from CSV files with specified label column.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/python/python_intro.rst#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndtrain = xgb.DMatrix('train.csv?format=csv&label_column=0')\ndtest = xgb.DMatrix('test.csv?format=csv&label_column=0')\n```\n\n----------------------------------------\n\nTITLE: Training XGBoost Model Without Constraints\nDESCRIPTION: Basic XGBoost model training without any feature interaction constraints. Uses standard parameters with early stopping.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/feature_interaction_constraint.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel_no_constraints = xgb.train(params, dtrain,\n                                 num_boost_round = 1000, evals = evallist,\n                                 early_stopping_rounds = 10)\n```\n\n----------------------------------------\n\nTITLE: Referencing XGBoost JSON Model Schema for Model Export\nDESCRIPTION: This code snippet includes a reference to the XGBoost model schema in JSON format. The schema defines the structure for XGBoost exported models and provides a standardized way to interpret and reuse model data. It notes that the schema includes fields like 'weight_drop' used in dart boosters.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/saving_model.rst#2025-04-19_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n.. include:: ../model.schema\n   :code: json\n```\n\n----------------------------------------\n\nTITLE: Fixing Python Future Warning\nDESCRIPTION: Bug fix to eliminate FutureWarning related to deprecated Series.base in pandas\nSOURCE: https://github.com/dmlc/xgboost/blob/master/NEWS.md#2025-04-19_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\nFutureWarning: Series.base is deprecated\n```\n\n----------------------------------------\n\nTITLE: NumPy Array Creation with Default Type\nDESCRIPTION: Demonstrates the default double type usage in NumPy arrays, which may use more memory than necessary.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/param_tuning.rst#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# np by default uses double, do you actually need it?\narray = np.array(...)\n```\n\n----------------------------------------\n\nTITLE: Source File Collection and Configuration for XGBoost in CMake\nDESCRIPTION: This CMake script configures the build process for XGBoost by collecting source files, setting up an object library, and configuring target properties. It handles platform-specific configurations including CUDA and SYCL support, and organizes source files for IDE display.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/src/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB_RECURSE CPU_SOURCES *.cc *.h)\nlist(REMOVE_ITEM CPU_SOURCES ${xgboost_SOURCE_DIR}/src/cli_main.cc)\n\nif(PLUGIN_SYCL)\n  list(REMOVE_ITEM CPU_SOURCES ${xgboost_SOURCE_DIR}/src/objective/regression_obj.cc)\n  list(REMOVE_ITEM CPU_SOURCES ${xgboost_SOURCE_DIR}/src/objective/hinge.cc)\n  list(REMOVE_ITEM CPU_SOURCES ${xgboost_SOURCE_DIR}/src/objective/quantile_obj.cc)\n  list(REMOVE_ITEM CPU_SOURCES ${xgboost_SOURCE_DIR}/src/objective/multiclass_obj.cc)\nendif()\n\n#-- Object library\n# Object library is necessary for jvm-package, which creates its own shared library.\nadd_library(objxgboost OBJECT)\ntarget_sources(objxgboost PRIVATE ${CPU_SOURCES})\n# Skip files with factory object\nset_source_files_properties(\n  predictor/predictor.cc gbm/gbm.cc tree/tree_updater.cc metric/metric.cc objective/objective.cc\n  PROPERTIES SKIP_UNITY_BUILD_INCLUSION ON)\n\nif(USE_CUDA)\n  file(GLOB_RECURSE CUDA_SOURCES *.cu *.cuh)\n  target_sources(objxgboost PRIVATE ${CUDA_SOURCES})\nendif()\n\nif(PLUGIN_SYCL)\n  target_compile_definitions(objxgboost PRIVATE -DXGBOOST_USE_SYCL=1)\nendif()\n\ntarget_include_directories(objxgboost\n  PRIVATE\n  ${xgboost_SOURCE_DIR}/include\n  ${xgboost_SOURCE_DIR}/dmlc-core/include)\n\nif(LOG_CAPI_INVOCATION)\n  target_compile_definitions(objxgboost PRIVATE -DLOG_CAPI_INVOCATION=1)\nendif()\n\n# This grouping organises source files nicely in visual studio\nauto_source_group(\"${CUDA_SOURCES}\")\nauto_source_group(\"${CPU_SOURCES}\")\n\n#-- End object library\n```\n\n----------------------------------------\n\nTITLE: Java API Examples Index\nDESCRIPTION: Index of Java examples demonstrating core XGBoost functionality including basic usage, custom objectives, prediction methods, and advanced features like external memory and early stopping.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/jvm-packages/xgboost4j-example/README.md#2025-04-19_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nBasicWalkThrough.java\nCustomObjective.java\nBoostFromPrediction.java\nPredictFirstNtree.java\nGeneralizedLinearModel.java\nCrossValidation.java\nPredictLeafIndices.java\nExternalMemory.java\nEarlyStopping.java\n```\n\n----------------------------------------\n\nTITLE: Activating the XGBoost Documentation Conda Environment\nDESCRIPTION: Command to activate the conda environment created for building XGBoost documentation. This must be run before installing dependencies or building the documentation.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/docs.rst#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nconda activate xgboost-docs\n```\n\n----------------------------------------\n\nTITLE: Building XGBoost R Package with CMake\nDESCRIPTION: Commands to build the XGBoost R package using CMake and Ninja build system on Linux. This provides more flexibility for compile configurations compared to the standard R build process.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/build.rst#2025-04-19_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -S . -DR_LIB=ON -GNinja\ncd build && ninja install\n```\n\n----------------------------------------\n\nTITLE: Implementing Train-Test Split\nDESCRIPTION: Example showing how to implement manual train-test split after removal of trainTestRatio parameter.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost_spark_migration.rst#2025-04-19_snippet_6\n\nLANGUAGE: scala\nCODE:\n```\nval Array(train, eval) = trainDf.randomSplit(Array(0.7, 0.3))\nval classifier = new XGBoostClassifer().setEvalDataset(eval)\nval model = classifier.fit(train)\n```\n\n----------------------------------------\n\nTITLE: Binary Classification Dataset in libSVM Format\nDESCRIPTION: A training/testing dataset in libSVM format where each line represents an example with a binary class label (0 or 1) followed by three feature values in 'index:value' format. Features include values of different scales, with some features having values around 10 and others around 1000, which would typically require normalization in machine learning workflows.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/jvm-packages/xgboost4j-spark/src/test/resources/rank.test.txt#2025-04-19_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n0 1:10.0229017899 2:7.30178495562 3:0.118115020017\n0 1:9.93639621859 2:9.93102159291 3:0.0435030004396\n0 1:10.1301737265 2:0.00411765220572 3:2.4165878053\n1 1:9.87828587087 2:0.608588414992 3:0.111262590883\n0 1:10.1373430048 2:0.47764012225 3:0.991553052194\n0 1:10.0523814718 2:4.72152505167 3:0.672978832666\n0 1:10.0449715742 2:8.40373928536 3:0.384457573667\n1 1:996.398498791 2:941.976309154 3:0.230269231292\n0 1:1005.11269468 2:900.093680877 3:0.265031528873\n0 1:997.160349441 2:891.331101688 3:2.19362017313\n0 1:993.754139031 2:44.8000165317 3:1.03868009875\n1 1:994.831299184 2:241.959208453 3:0.667631827024\n0 1:995.948333283 2:7.94326917112 3:0.750490877118\n0 1:989.733981273 2:7.52077625436 3:0.0126335967282\n0 1:1003.54086516 2:6.48177510564 3:1.19441696788\n0 1:996.56177804 2:9.71959812613 3:1.33082465111\n0 1:1005.61382467 2:0.234339369309 3:1.17987797356\n1 1:980.215758708 2:6.85554542926 3:2.63965085259\n1 1:987.776408872 2:2.23354609991 3:0.841885278028\n0 1:1006.54260396 2:8.12142049834 3:2.26639471174\n0 1:1009.87927639 2:6.40028519044 3:0.775155669615\n0 1:9.95006244393 2:928.76896718 3:234.948458244\n1 1:10.0749152258 2:255.294574476 3:62.9728604166\n1 1:10.1916541988 2:312.682867085 3:92.299413677\n0 1:9.95646724484 2:742.263188416 3:53.3310473654\n0 1:9.86211293222 2:996.237023866 3:2.00760301168\n1 1:9.91801019468 2:303.971783709 3:50.3147230679\n0 1:996.983996934 2:9.52188222766 3:1.33588120981\n0 1:995.704388126 2:9.49260524915 3:0.908498516541\n0 1:987.86480767 2:0.0870786716821 3:0.108859297837\n0 1:1000.99561307 2:2.85272694575 3:0.171134518956\n0 1:1011.05508066 2:7.55336771768 3:1.04950084825\n1 1:985.52199365 2:0.763305780608 3:1.7402424375\n0 1:10.0430321467 2:813.185427181 3:4.97728254185\n0 1:10.0812334228 2:258.297288417 3:0.127477670549\n0 1:9.84210504292 2:887.205815261 3:0.991689193955\n1 1:9.94625332613 2:0.298622762132 3:0.147881353231\n0 1:9.97800659954 2:727.619819757 3:0.0718361141866\n1 1:9.8037938472 2:957.385549617 3:0.0618862028941\n0 1:10.0880634741 2:185.024638577 3:1.7028095095\n0 1:9.98630799154 2:109.10631473 3:0.681117359751\n0 1:9.91671416638 2:166.248076588 3:122.538291094\n0 1:10.1206910464 2:88.1539468531 3:141.189859069\n1 1:10.1767160518 2:1.02960996847 3:172.02256237\n0 1:9.93025147233 2:391.196641942 3:58.040338247\n0 1:9.84850936037 2:474.63346537 3:17.5627875397\n1 1:9.8162731343 2:61.9199554213 3:30.6740972851\n0 1:10.0403482984 2:987.50416929 3:73.0472906209\n1 1:997.019228359 2:133.294717663 3:0.0572254083186\n0 1:973.303999107 2:1.79080888849 3:0.100478717048\n0 1:1008.28808825 2:342.282350685 3:0.409806485495\n0 1:1014.55621524 2:0.680510407082 3:0.929530602495\n1 1:1012.74370325 2:823.105266455 3:0.0894693730585\n0 1:1003.63554038 2:727.334432075 3:0.58206275756\n0 1:10.1560432436 2:740.35938307 3:11.6823378533\n0 1:9.83949099701 2:512.828227154 3:138.206666681\n1 1:10.1837395682 2:179.287126088 3:185.479062365\n1 1:9.9761881495 2:12.1093388336 3:9.1264604171\n1 1:9.77402180766 2:318.561317743 3:80.6005221355\n0 1:1011.15705381 2:0.215825852155 3:1.34429667906\n0 1:1005.60353229 2:727.202346126 3:1.47146041005\n1 1:1013.93702961 2:58.7312725205 3:0.421041560754\n0 1:1004.86813074 2:757.693204258 3:0.566055205344\n0 1:999.996324692 2:813.12386828 3:0.864428279513\n0 1:996.55255931 2:918.760056995 3:0.43365051974\n1 1:1004.1394132 2:464.371823646 3:0.312492288321\n```\n\n----------------------------------------\n\nTITLE: Starting NVFlare Federated Server\nDESCRIPTION: This command initiates the NVFlare federated server, which coordinates the federated learning process.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/nvflare/horizontal/README.md#2025-04-19_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n/tmp/nvflare/poc/server/startup/start.sh\n```\n\n----------------------------------------\n\nTITLE: NumPy Array Type Conversion Memory Impact\nDESCRIPTION: Shows how array type conversion may create data copies depending on the data type.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/param_tuning.rst#2025-04-19_snippet_2\n\nLANGUAGE: python\nCODE:\n```\narray = np.array(...)\n# This may or may not make a copy of the data, depending on the type of the data\narray.astype(np.float32)\n```\n\n----------------------------------------\n\nTITLE: Slicing XGBoost Tree Models in R\nDESCRIPTION: This snippet shows how to create and slice an XGBoost model in R. It uses the agaricus dataset, trains a binary classification model, and demonstrates model slicing.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/slicing_model.rst#2025-04-19_snippet_1\n\nLANGUAGE: r\nCODE:\n```\nlibrary(xgboost)\ndata(agaricus.train, package = \"xgboost\")\ndm <- xgb.DMatrix(agaricus.train$data, label = agaricus.train$label)\n\nmodel <- xgb.train(\n  params = xgb.params(objective = \"binary:logistic\", max_depth = 4),\n  data = dm,\n  nrounds = 20\n)\nsliced <- model[seq(3, 7)]\n```\n\n----------------------------------------\n\nTITLE: Saving XGBoost Model Configuration (R)\nDESCRIPTION: Illustrates how to save the internal configuration of an XGBoost model as a JSON string in R.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/saving_model.rst#2025-04-19_snippet_5\n\nLANGUAGE: r\nCODE:\n```\nconfig <- xgb.config(bst)\nprint(config)\n```\n\n----------------------------------------\n\nTITLE: Building Source Distribution for XGBoost Python Package\nDESCRIPTION: Command to build a source distribution (sdist) for XGBoost. The resulting .tar.gz file will contain both Python code and C++ source code that will be compiled during installation.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/python_packaging.rst#2025-04-19_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n$ python -m build --sdist .\n```\n\n----------------------------------------\n\nTITLE: Building XGBoost with Federated Learning Support\nDESCRIPTION: Commands for building XGBoost with the federated learning plugin enabled, which requires gRPC and protobuf dependencies.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/build.rst#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -S . -DPLUGIN_FEDERATED=ON -GNinja\ncd build && ninja\n```\n\n----------------------------------------\n\nTITLE: Building XGBoost Library (Static or Shared)\nDESCRIPTION: Creates either a static or shared XGBoost library based on configuration, and sets up include directories for installation and build.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/CMakeLists.txt#2025-04-19_snippet_17\n\nLANGUAGE: CMake\nCODE:\n```\n#-- library\nif(BUILD_STATIC_LIB)\n  add_library(xgboost STATIC)\nelse()\n  add_library(xgboost SHARED)\nendif()\ntarget_link_libraries(xgboost PRIVATE objxgboost)\ntarget_include_directories(xgboost\n  INTERFACE\n  $<INSTALL_INTERFACE:$<INSTALL_PREFIX>/include>\n  $<BUILD_INTERFACE:${CMAKE_CURRENT_LIST_DIR}/include>)\n#-- End shared library\n```\n\n----------------------------------------\n\nTITLE: Configuring RMM Plugin Support\nDESCRIPTION: Sets up the RAPIDS Memory Manager (RMM) plugin integration with CUDA, patching RMM targets to use static CUDA runtime.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/CMakeLists.txt#2025-04-19_snippet_15\n\nLANGUAGE: CMake\nCODE:\n```\nif(PLUGIN_RMM)\n  find_package(rmm REQUIRED)\n\n  # Patch the rmm targets so they reference the static cudart\n  # Remove this patch once RMM stops specifying cudart requirement\n  # (since RMM is a header-only library, it should not specify cudart in its CMake config)\n  get_target_property(rmm_link_libs rmm::rmm INTERFACE_LINK_LIBRARIES)\n  list(REMOVE_ITEM rmm_link_libs CUDA::cudart)\n  list(APPEND rmm_link_libs CUDA::cudart_static)\n  set_target_properties(rmm::rmm PROPERTIES INTERFACE_LINK_LIBRARIES \"${rmm_link_libs}\")\n\n  # Pick up patched CCCL from RMM\nelseif(USE_CUDA)\n  # If using CUDA and not RMM, search for CCCL.\n  find_package(CCCL CONFIG)\n  if(CCCL_FOUND)\n    message(STATUS \"Standalone CCCL found.\")\n  else()\n    message(STATUS \"Standalone CCCL not found. Attempting to use CCCL from CUDA Toolkit...\")\n    find_package(CCCL CONFIG\n      HINTS ${CUDAToolkit_LIBRARY_DIR}/cmake)\n    if(NOT CCCL_FOUND)\n      message(STATUS \"Could not locate CCCL from CUDA Toolkit. Using Thrust and CUB from CUDA Toolkit...\")\n      find_package(libcudacxx CONFIG REQUIRED\n        HINTS ${CUDAToolkit_LIBRARY_DIR}/cmake)\n      find_package(CUB CONFIG REQUIRED\n        HINTS ${CUDAToolkit_LIBRARY_DIR}/cmake)\n      find_package(Thrust CONFIG REQUIRED\n        HINTS ${CUDAToolkit_LIBRARY_DIR}/cmake)\n      thrust_create_target(Thrust HOST CPP DEVICE CUDA)\n      add_library(CCCL::CCCL INTERFACE IMPORTED GLOBAL)\n      target_link_libraries(CCCL::CCCL INTERFACE libcudacxx::libcudacxx CUB::CUB Thrust)\n    endif()\n  endif()\n  # Define guard macros to prevent windows.h from conflicting with winsock2.h\n  if(WIN32)\n    target_compile_definitions(CCCL::CCCL INTERFACE NOMINMAX WIN32_LEAN_AND_MEAN _WINSOCKAPI_)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring XGBoost C Inference Demo Build with CMake\nDESCRIPTION: This CMake script sets up a build environment for a C-based XGBoost inference demo. It finds the XGBoost package, handles special requirements for static linking, and configures the target executable with appropriate dependencies.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/c-api/inference/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.18)\nproject(inference-demo LANGUAGES C VERSION 0.0.1)\nfind_package(xgboost REQUIRED)\n\n# xgboost is built as static libraries, all cxx dependencies need to be linked into the\n# executable.\nif(XGBOOST_BUILD_STATIC_LIB)\n  enable_language(CXX)\n  # find again for those  cxx libraries.\n  find_package(xgboost REQUIRED)\nendif()\n\nadd_executable(inference-demo inference.c)\ntarget_link_libraries(inference-demo PRIVATE xgboost::xgboost)\n```\n\n----------------------------------------\n\nTITLE: Preparing Data for Federated XGBoost Demo\nDESCRIPTION: This command runs a shell script to prepare the data necessary for the federated learning demo.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/nvflare/horizontal/README.md#2025-04-19_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n./prepare_data.sh\n```\n\n----------------------------------------\n\nTITLE: Generating R Package Markdown in XGBoost\nDESCRIPTION: Bash commands for generating markdown files and figures for R package vignettes in XGBoost. This process involves building markdown from Rmarkdown, generating figures, and updating documentation indexes.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/coding_guide.rst#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake the-markdown-to-make.md\n```\n\nLANGUAGE: bash\nCODE:\n```\nmake html\n```\n\n----------------------------------------\n\nTITLE: Loading XGBoost Model Configuration (Python)\nDESCRIPTION: Demonstrates how to load a previously saved configuration back into an XGBoost model in Python.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/saving_model.rst#2025-04-19_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nbst.load_config(config)\n```\n\n----------------------------------------\n\nTITLE: Setting strict mode environment variable for XGBoost in R\nDESCRIPTION: This code shows how to set an environment variable to enable strict mode for XGBoost in R. This method takes precedence over the R option and is useful for system-wide configuration.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/R-package/migration_guide.rst#2025-04-19_snippet_1\n\nLANGUAGE: r\nCODE:\n```\nSys.setenv(\"XGB_STRICT_MODE\" = \"1\")\n```\n\n----------------------------------------\n\nTITLE: Unsafe Thread Operation Example in XGBoost Prediction\nDESCRIPTION: Example showing an unsafe operation where model parameters are modified inside a prediction function that's used with threads. This demonstrates what not to do when utilizing XGBoost prediction in a multi-threaded environment.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/prediction.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef predict_fn(clf: xgb.XGBClassifier, X):\n    X = preprocess(X)\n    clf.set_params(n_jobs=1)  # NOT safe!\n    return clf.predict_proba(X, iteration_range=(0, 10))\n\nwith ThreadPoolExecutor(max_workers=10) as e:\n    e.submit(predict_fn, ...)\n```\n\n----------------------------------------\n\nTITLE: Configuring Common Target Setup for XGBoost Components\nDESCRIPTION: Applies common target properties, link libraries, and definitions to the main XGBoost components.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/CMakeLists.txt#2025-04-19_snippet_19\n\nLANGUAGE: CMake\nCODE:\n```\n# Common setup for all targets\nforeach(target xgboost objxgboost dmlc)\n  xgboost_target_properties(${target})\n  xgboost_target_link_libraries(${target})\n  xgboost_target_defs(${target})\nendforeach()\n\nif(JVM_BINDINGS)\n  xgboost_target_properties(xgboost4j)\n  xgboost_target_link_libraries(xgboost4j)\n  xgboost_target_defs(xgboost4j)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring XGBoost External Memory Demo Build with CMake\nDESCRIPTION: This CMake configuration sets up a project for building an external memory demo application that uses XGBoost. It specifies the minimum CMake version, defines the project details, finds the required XGBoost package, and configures the executable build and linking.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/c-api/external-memory/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.18)\nproject(external-memory-demo LANGUAGES C VERSION 0.0.1)\n\nfind_package(xgboost REQUIRED)\n\nadd_executable(external-memory-demo external_memory.c)\ntarget_link_libraries(external-memory-demo PRIVATE xgboost::xgboost)\n```\n\n----------------------------------------\n\nTITLE: Running Federated XGBoost Tests Using Shell Script\nDESCRIPTION: These shell commands demonstrate how to run tests for the federated XGBoost functionality. The commands navigate to the test directory and execute a script that tests both CPU training with the 'hist' algorithm and GPU training with the 'gpu_hist' algorithm.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/plugin/federated/README.md#2025-04-19_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n# Under xgboost source tree.\ncd tests/distributed/test_federated\n# This tests both CPU training (`hist`) and GPU training (`gpu_hist`).\n./runtests-federated.sh\n```\n\n----------------------------------------\n\nTITLE: Running C++ Linting Checks for XGBoost\nDESCRIPTION: Bash command to run C++ linting checks on the XGBoost codebase using cpplint.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/coding_guide.rst#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncd /path/to/xgboost/\npython ./ops/script/lint_cpp.py\n```\n\n----------------------------------------\n\nTITLE: Building Application with XGBoost Using CMake\nDESCRIPTION: Commands to activate the Conda environment, invoke CMake with the correct prefix path, and build the application linked with XGBoost.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/c_api_tutorial.rst#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Activate the Conda environment where we previously installed XGBoost\nconda activate [env_name]\n# Invoke CMake with CMAKE_PREFIX_PATH\ncmake -B build -S . -DCMAKE_PREFIX_PATH=$CONDA_PREFIX\n# Build your application\ncmake --build build\n```\n\n----------------------------------------\n\nTITLE: Setting strict mode option in R for XGBoost\nDESCRIPTION: This snippet demonstrates how to enable strict mode for XGBoost in R, which turns deprecation warnings into errors. This is recommended for package developers to ensure better compatibility.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/R-package/migration_guide.rst#2025-04-19_snippet_0\n\nLANGUAGE: r\nCODE:\n```\noptions(\"xgboost.strict_mode\" = TRUE)\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake for XGBoost with find_package\nDESCRIPTION: This CMake snippet demonstrates how to configure a project to use XGBoost when it's installed in the system. It uses find_package to locate XGBoost and links it to the target executable.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/c-api/basic/README.md#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package(xgboost REQUIRED)\nadd_executable(api-demo c-api-demo.c)\ntarget_link_libraries(api-demo xgboost::xgboost)\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenMP Support for XGBoost R Package\nDESCRIPTION: Conditionally enables and links OpenMP libraries when USE_OPENMP is enabled, providing parallel processing capabilities.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/R-package/CMakeLists.txt#2025-04-19_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_OPENMP)\n  find_package(OpenMP REQUIRED)\n  target_link_libraries(xgboost-r PUBLIC OpenMP::OpenMP_CXX OpenMP::OpenMP_C)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring SYCL Plugin for XGBoost in CMake\nDESCRIPTION: CMake configuration that sets up the SYCL plugin for XGBoost when PLUGIN_SYCL is enabled. It configures the Intel C++ compiler (icpx), includes necessary source files, sets compilation flags for SYCL, and integrates with OpenMP if needed. The configuration also links the SYCL plugin with the main XGBoost library.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/plugin/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(PLUGIN_SYCL)\n  set(CMAKE_CXX_COMPILER \"icpx\")\n  file(GLOB_RECURSE SYCL_SOURCES \"sycl/*.cc\")\n    list(APPEND SYCL_SOURCES\n    ${xgboost_SOURCE_DIR}/src/objective/regression_obj.cc\n    ${xgboost_SOURCE_DIR}/src/objective/hinge.cc\n    ${xgboost_SOURCE_DIR}/src/objective/quantile_obj.cc\n    ${xgboost_SOURCE_DIR}/src/objective/multiclass_obj.cc)\n  add_library(plugin_sycl OBJECT ${SYCL_SOURCES})\n  target_include_directories(plugin_sycl\n    PRIVATE\n    ${xgboost_SOURCE_DIR}/include\n    ${xgboost_SOURCE_DIR}/dmlc-core/include\n    ${xgboost_SOURCE_DIR}/rabit/include)\n    target_compile_definitions(plugin_sycl PUBLIC -DXGBOOST_USE_SYCL=1)\n    target_link_libraries(plugin_sycl PUBLIC -fsycl)\n    set_target_properties(plugin_sycl PROPERTIES\n    COMPILE_FLAGS \"-fsycl -fno-sycl-id-queries-fit-in-int\"\n    CXX_STANDARD 17\n    CXX_STANDARD_REQUIRED ON\n    POSITION_INDEPENDENT_CODE ON)\n  if(USE_OPENMP)\n    find_package(OpenMP REQUIRED)\n    set_target_properties(plugin_sycl PROPERTIES\n    COMPILE_FLAGS \"-fsycl -fno-sycl-id-queries-fit-in-int -qopenmp\")\n  endif()\n  # Get compilation and link flags of plugin_sycl and propagate to objxgboost\n  target_link_libraries(objxgboost PUBLIC plugin_sycl)\n  # Add all objects of plugin_sycl to objxgboost\n  target_sources(objxgboost INTERFACE $<TARGET_OBJECTS:plugin_sycl>)\nendif()\n```\n\n----------------------------------------\n\nTITLE: DataFrame Operation Memory Impact in Python\nDESCRIPTION: Demonstrates how DataFrame operations create new copies in memory even with inplace parameter.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/param_tuning.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.DataFrame()\n# This creates a new copy of the dataframe, even if you specify the inplace parameter\nnew_df = df.drop(...)\n```\n\n----------------------------------------\n\nTITLE: Finding NCCL Library\nDESCRIPTION: Locates the NCCL library when the USE_NCCL option is enabled.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/CMakeLists.txt#2025-04-19_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_NCCL)\n  find_package(Nccl REQUIRED)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Shutting Down NVFlare Components\nDESCRIPTION: These commands shut down the NVFlare client and server components after the federated learning job is completed.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/nvflare/horizontal/README.md#2025-04-19_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nshutdown client\nshutdown server\n```\n\n----------------------------------------\n\nTITLE: Split Gain Calculation in XGBoost\nDESCRIPTION: This formula calculates the gain from splitting a leaf into two leaves, considering the scores of the new leaves, the original leaf, and the regularization term for the additional leaf.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/model.rst#2025-04-19_snippet_11\n\nLANGUAGE: math\nCODE:\n```\nGain = \\frac{1}{2} \\left[\\frac{G_L^2}{H_L+\\lambda}+\\frac{G_R^2}{H_R+\\lambda}-\\frac{(G_L+G_R)^2}{H_L+H_R+\\lambda}\\right] - \\gamma\n```\n\n----------------------------------------\n\nTITLE: Installing Python Package with Automatic Library Build\nDESCRIPTION: Command for installing the XGBoost Python package that automatically builds the C++ shared library during installation.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/build.rst#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncd python-package/\npip install -v . # Builds the shared object automatically.\n```\n\n----------------------------------------\n\nTITLE: Dumping XGBoost Model to Text Files\nDESCRIPTION: Commands to dump the trained XGBoost model into text files for easy inspection.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/CLI/binary_classification/README.md#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n../../xgboost mushroom.conf task=dump model_in=0002.model name_dump=dump.raw.txt\n../../xgboost mushroom.conf task=dump model_in=0002.model fmap=featmap.txt name_dump=dump.nice.txt\n```\n\n----------------------------------------\n\nTITLE: Configuring CMakeLists.txt for XGBoost Linking\nDESCRIPTION: CMake configuration to link an application with the XGBoost library, including finding the package and linking the target.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/c_api_tutorial.rst#2025-04-19_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.18)\nproject(your_project_name LANGUAGES C CXX VERSION your_project_version)\nfind_package(xgboost REQUIRED)\nadd_executable(your_project_name /path/to/project_file.c)\ntarget_link_libraries(your_project_name xgboost::xgboost)\n```\n\n----------------------------------------\n\nTITLE: CMake Version Specification\nDESCRIPTION: Build system improvement to specify version macro in CMake\nSOURCE: https://github.com/dmlc/xgboost/blob/master/NEWS.md#2025-04-19_snippet_21\n\nLANGUAGE: CMake\nCODE:\n```\nspecify version macro in CMake\n```\n\n----------------------------------------\n\nTITLE: Integrating XGBoost R Package with Main Library\nDESCRIPTION: Links the R-specific components with the main XGBoost object library and includes all R package objects in the main build.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/R-package/CMakeLists.txt#2025-04-19_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\n# Get compilation and link flags of xgboost-r and propagate to objxgboost\ntarget_link_libraries(objxgboost PUBLIC xgboost-r)\n\n# Add all objects of xgboost-r to objxgboost\ntarget_sources(objxgboost INTERFACE $<TARGET_OBJECTS:xgboost-r>)\n```\n\n----------------------------------------\n\nTITLE: Adding JVM Bindings Support\nDESCRIPTION: Conditionally adds the JVM packages subdirectory to build the xgboost4j shared library when JVM bindings are enabled.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/CMakeLists.txt#2025-04-19_snippet_13\n\nLANGUAGE: CMake\nCODE:\n```\n# This creates its own shared library `xgboost4j'.\nif(JVM_BINDINGS)\n  add_subdirectory(${xgboost_SOURCE_DIR}/jvm-packages)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Git Default Editor\nDESCRIPTION: Command to set the default text editor for Git operations.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/git_guide.rst#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit config core.editor the-editor-you-like\n```\n\n----------------------------------------\n\nTITLE: Docker Run Commands for CPU and GPU\nDESCRIPTION: Examples of using docker_run.py script to run containers with and without GPU support.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/ci.rst#2025-04-19_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\n# Run without GPU\npython3 ops/docker_run.py \\\n  --image-uri 492475357299.dkr.ecr.us-west-2.amazonaws.com/xgb-ci.cpu:main \\\n  -- bash ops/pipeline/build-cpu-impl.sh cpu\n\n# Run with NVIDIA GPU\n# Allocate extra space in /dev/shm to enable NCCL\n# Also run the container with elevated privileges\npython3 ops/docker_run.py \\\n  --image-uri 492475357299.dkr.ecr.us-west-2.amazonaws.com/xgb-ci.gpu:main \\\n  --use-gpus \\\n  --run-args='--shm-size=4g --privileged' \\\n  -- bash ops/pipeline/test-python-wheel-impl.sh gpu\n```\n\n----------------------------------------\n\nTITLE: Installing Graphviz Dependency for Documentation\nDESCRIPTION: Optional command to install Graphviz, which is used for generating graph visualizations in the documentation. This enhances the visual representation of tree models and other diagrams.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/docs.rst#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nconda install graphviz --yes\n```\n\n----------------------------------------\n\nTITLE: Scala API Examples Index\nDESCRIPTION: Index of Scala examples showing XGBoost integration including basic usage, custom objectives, prediction methods, and advanced features like external memory.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/jvm-packages/xgboost4j-example/README.md#2025-04-19_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nBasicWalkThrough.scala\nCustomObjective.scala\nBoostFromPrediction.scala\nPredictFirstNTree.scala\nGeneralizedLinearModel.scala\nCrossValidation.scala\nPredictLeafIndices.scala\nExternalMemory.scala\n```\n\n----------------------------------------\n\nTITLE: Setting Prune Updater Configuration in XGBoost\nDESCRIPTION: Configuration parameters to enable the prune updater independently, which prunes existing trees based on min_split_loss and max_depth parameters.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/treemethod.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n{\"process_type\": \"update\", \"updater\": \"prune\"}\n```\n\n----------------------------------------\n\nTITLE: Upgrading pip to support manylinux2010 wheels\nDESCRIPTION: Command to upgrade pip to version 19.0 or newer, which is required for installing XGBoost binary wheels with the manylinux2010 tag.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/NEWS.md#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pip install --upgrade pip\n```\n\n----------------------------------------\n\nTITLE: Setting Global Configuration in R\nDESCRIPTION: Uses xgb.set.config() to set global configuration parameters in R, such as verbosity and number of threads.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst#2025-04-19_snippet_1\n\nLANGUAGE: r\nCODE:\n```\nxgb.set.config(verbosity = 2, nthread = 4)\n```\n\n----------------------------------------\n\nTITLE: Defining Build Dependencies Between XGBoost Targets\nDESCRIPTION: Ensures proper build order between targets that produce outputs with conflicting names.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/CMakeLists.txt#2025-04-19_snippet_22\n\nLANGUAGE: CMake\nCODE:\n```\n# Ensure these two targets do not build simultaneously, as they produce outputs with conflicting names\nif(BUILD_DEPRECATED_CLI)\n  add_dependencies(xgboost runxgboost)\nendif()\n```\n\n----------------------------------------\n\nTITLE: S3 Artifact Download Configuration\nDESCRIPTION: GitHub Actions workflow configuration for downloading artifacts from Amazon S3 bucket.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/ci.rst#2025-04-19_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\n- name: Download files from S3\n  run: |\n    REMOTE_PREFIX=\"remote directory where the artifact(s) were placed\"\n    python3 ops/pipeline/manage-artifacts.py download \\\n      --s3-bucket ${{ env.RUNS_ON_S3_BUCKET_CACHE }} \\\n      --prefix cache/${{ github.run_id }}/${REMOTE_PREFIX} \\\n      --dest-dir path/to/destination_directory \\\n      artifacts\n```\n\n----------------------------------------\n\nTITLE: Implementing Dataset Caching\nDESCRIPTION: Example showing how to implement manual dataset caching after removal of cacheTrainingSet parameter.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost_spark_migration.rst#2025-04-19_snippet_5\n\nLANGUAGE: scala\nCODE:\n```\nval df = input.cache()\nval model = new XGBoostClassifier().fit(df)\n```\n\n----------------------------------------\n\nTITLE: Initial Margin File Format for XGBoost\nDESCRIPTION: Format for providing initial margin predictions for XGBoost instances. Each line contains a margin value (before transformation) for the corresponding instance. This file should be named with '.base_margin' extension appended to the instance file name.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/input_format.rst#2025-04-19_snippet_3\n\nLANGUAGE: none\nCODE:\n```\n-0.4\n1.0\n3.4\n```\n\n----------------------------------------\n\nTITLE: Configuring XGBoost C Examples Project with CMake\nDESCRIPTION: Sets up a CMake project for XGBoost C examples, adding subdirectories for different example categories and configuring tests for each demo. The configuration includes basic examples, external memory handling, and inference demonstrations.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/c-api/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.18)\nproject(xgboost-c-examples)\n\nadd_subdirectory(basic)\nadd_subdirectory(external-memory)\nadd_subdirectory(inference)\n\nenable_testing()\nadd_test(\n  NAME test_xgboost_demo_c_basic\n  COMMAND api-demo\n  WORKING_DIRECTORY ${xgboost-c-examples_BINARY_DIR}\n)\nadd_test(\n  NAME test_xgboost_demo_c_external_memory\n  COMMAND external-memory-demo\n  WORKING_DIRECTORY ${xgboost-c-examples_BINARY_DIR}\n)\nadd_test(\n  NAME test_xgboost_demo_c_inference\n  COMMAND inference-demo\n  WORKING_DIRECTORY ${xgboost-c-examples_BINARY_DIR}\n)\n```\n\n----------------------------------------\n\nTITLE: Cloning XGBoost Repository with Git\nDESCRIPTION: Command for cloning the XGBoost repository with Git including submodules to manage dependencies.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/build.rst#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone --recursive https://github.com/dmlc/xgboost\n```\n\n----------------------------------------\n\nTITLE: Python f-string Compatibility Fix\nDESCRIPTION: Example showing removal of f-strings to maintain Python 3.5 compatibility.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/NEWS.md#2025-04-19_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n# Remove f-string like:\n# f\"some {variable}\"\n# Replace with:\n\"some {}\".format(variable)\n```\n\n----------------------------------------\n\nTITLE: XGBoost Training Function in R\nDESCRIPTION: The xgb.train function in R accepts a list of parameters. This list can be constructed using the xgb.params function, allowing for a more idiomatic R experience.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/R-package/adding_parameters.rst#2025-04-19_snippet_1\n\nLANGUAGE: R\nCODE:\n```\nxgb.train(params, ...)\n```\n\n----------------------------------------\n\nTITLE: IBM i Platform-Specific Configuration\nDESCRIPTION: Applies specific compiler and archiver flags for IBM i (OS400) platform support.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/CMakeLists.txt#2025-04-19_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\n# Add for IBM i\nif(${CMAKE_SYSTEM_NAME} MATCHES \"OS400\")\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -pthread\")\n  set(CMAKE_CXX_ARCHIVE_CREATE \"<CMAKE_AR> -X64 qc <TARGET> <OBJECTS>\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories and Library Dependencies for Tests\nDESCRIPTION: Configures the testxgboost target with the necessary include directories from the federated plugin source, and links against the federated_client library required for the tests to function.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/tests/cpp/plugin/federated/CMakeLists.txt#2025-04-19_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_include_directories(testxgboost PRIVATE ${xgboost_SOURCE_DIR}/plugin/federated)\ntarget_link_libraries(testxgboost PRIVATE federated_client)\n```\n\n----------------------------------------\n\nTITLE: Setting Column Sampling in XGBoost Tree Construction\nDESCRIPTION: Configuration parameter for enabling column subsampling during tree construction, specifying the ratio of columns to sample.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/NEWS.md#2025-04-19_snippet_25\n\nLANGUAGE: text\nCODE:\n```\nbst:col_samplebytree=ratio\n```\n\n----------------------------------------\n\nTITLE: Preparing Data for XGBoost Training\nDESCRIPTION: Python commands to transform the dataset into LIBSVM format and split it into training and test sets.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/CLI/binary_classification/README.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython mapfeat.py\npython mknfold.py agaricus.txt 1\n```\n\n----------------------------------------\n\nTITLE: Documentation Build Console Output Example\nDESCRIPTION: Example of the console output when successfully building the XGBoost documentation. It shows the sphinx-build command execution and confirms the location of the generated HTML files.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/docs.rst#2025-04-19_snippet_7\n\nLANGUAGE: console\nCODE:\n```\n$ make html\nsphinx-build -b html -d _build/doctrees   . _build/html\nRunning Sphinx v6.2.1\n...\nThe HTML pages are in _build/html.\n\nBuild finished. The HTML pages are in _build/html.\n```\n\n----------------------------------------\n\nTITLE: Parameter validation in R\nDESCRIPTION: XGBoost now produces a warning message for unused or misspelled training parameters in R, helping users identify configuration errors.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/NEWS.md#2025-04-19_snippet_13\n\nLANGUAGE: R\nCODE:\n```\n# Example (pseudo-code)\nparams <- list(max_depth = 3, min_child_wight = 1)  # Misspelled parameter\nxgb.train(params, dtrain)  # Will produce a warning for 'min_child_wight'\n```\n\n----------------------------------------\n\nTITLE: Saving XGBoost Model Configuration (Python)\nDESCRIPTION: Shows how to save the internal configuration of an XGBoost model as a JSON string in Python.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/saving_model.rst#2025-04-19_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nbst = xgboost.train(...)\nconfig = bst.save_config()\nprint(config)\n```\n\n----------------------------------------\n\nTITLE: CUDA Memory Alignment Fix\nDESCRIPTION: Code change to force 4-byte alignment of compressed buffers for CUDA memory checker compatibility.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/NEWS.md#2025-04-19_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\n// Force compressed buffer to be 4 bytes aligned\naligned_alloc(4, size)\n```\n\n----------------------------------------\n\nTITLE: Setting MSVC Compiler Options for DMLC Unit Tests\nDESCRIPTION: Configures Microsoft Visual C++ compiler options for DMLC unit tests to suppress CRT security warnings.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/CMakeLists.txt#2025-04-19_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nif(MSVC)\n  if(TARGET dmlc_unit_tests)\n    target_compile_options(\n        dmlc_unit_tests PRIVATE\n        -D_CRT_SECURE_NO_WARNINGS -D_CRT_SECURE_NO_DEPRECATE\n    )\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Finding and Configuring R Library Dependencies in CMake\nDESCRIPTION: Locates the required R libraries and displays the path to the core R library. This is essential for building the R package interface for XGBoost.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/R-package/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package(LibR REQUIRED)\nmessage(STATUS \"LIBR_CORE_LIBRARY \" ${LIBR_CORE_LIBRARY})\n```\n\n----------------------------------------\n\nTITLE: XGBoost External Memory URI Format for LIBSVM\nDESCRIPTION: Shows the URI format for loading LIBSVM files with external memory support. The format includes the filename, format specification, and cache prefix.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/external_memory.rst#2025-04-19_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nfilename?format=libsvm#cacheprefix\n```\n\n----------------------------------------\n\nTITLE: Configuring Warning Flags for XGBoost R Package\nDESCRIPTION: Conditionally enables comprehensive compiler warnings for the R package if the ENABLE_ALL_WARNINGS option is set.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/R-package/CMakeLists.txt#2025-04-19_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif(ENABLE_ALL_WARNINGS)\n  target_compile_options(xgboost-r PRIVATE -Wall -Wextra)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring R Package Installation\nDESCRIPTION: Sets up specific properties and installation targets for R package integration.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/CMakeLists.txt#2025-04-19_snippet_23\n\nLANGUAGE: CMake\nCODE:\n```\n#-- Installing XGBoost\nif(R_LIB)\n  include(cmake/RPackageInstallTargetSetup.cmake)\n  set_target_properties(xgboost PROPERTIES PREFIX \"\")\n  if(APPLE)\n    set_target_properties(xgboost PROPERTIES SUFFIX \".so\")\n  endif()\n  setup_rpackage_install_target(xgboost \"${CMAKE_CURRENT_BINARY_DIR}/R-package-install\")\n  set(CMAKE_INSTALL_PREFIX \"${CMAKE_CURRENT_BINARY_DIR}/dummy_inst\")\nendif()\nif(MINGW)\n  set_target_properties(xgboost PROPERTIES PREFIX \"\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing XGBoost from Source Distribution\nDESCRIPTION: Command to install XGBoost from a source distribution. The -v flag shows the build progress as CMake and a C++ compiler compile the bundled C++ code into libxgboost.so during installation.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/python_packaging.rst#2025-04-19_snippet_1\n\nLANGUAGE: console\nCODE:\n```\n$ pip install -v xgboost-2.0.0.tar.gz  # Add -v to show build progress\n```\n\n----------------------------------------\n\nTITLE: Local GPU Test Command\nDESCRIPTION: Command to run GPU tests in Docker container with NVIDIA support.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/ci.rst#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython3 ops/docker_run.py \\\n  --image-uri 492475357299.dkr.ecr.us-west-2.amazonaws.com/xgb-ci.gpu:main \\\n  --use-gpus \\\n  -- bash ops/pipeline/test-python-wheel-impl.sh gpu\n```\n\n----------------------------------------\n\nTITLE: Setting up XGBoost4j shared library compilation in CMake\nDESCRIPTION: This CMake script configures the compilation of the XGBoost4j shared library. It includes JNI dependencies, sets up source files, and configures platform-specific build options. The script handles CUDA integration, OpenMP patching for MacOS, and sets up include directories and linking.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/jvm-packages/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package(JNI REQUIRED)\n\nlist(APPEND JVM_SOURCES\n  ${PROJECT_SOURCE_DIR}/jvm-packages/xgboost4j/src/native/xgboost4j.cpp\n  ${PROJECT_SOURCE_DIR}/jvm-packages/xgboost4j/src/native/xgboost4j-gpu.cpp)\n\nif(USE_CUDA)\n  list(APPEND JVM_SOURCES\n    ${PROJECT_SOURCE_DIR}/jvm-packages/xgboost4j/src/native/xgboost4j-gpu.cu)\nendif()\n\nadd_library(xgboost4j SHARED ${JVM_SOURCES} ${XGBOOST_OBJ_SOURCES})\n\nif(ENABLE_ALL_WARNINGS)\n  target_compile_options(xgboost4j PUBLIC -Wall -Wextra)\nendif()\n\ntarget_link_libraries(xgboost4j PRIVATE objxgboost)\ntarget_include_directories(xgboost4j\n  PRIVATE\n  ${JNI_INCLUDE_DIRS}\n  ${PROJECT_SOURCE_DIR}/jvm-packages/xgboost4j/src/native\n  ${PROJECT_SOURCE_DIR}/include\n  ${PROJECT_SOURCE_DIR}/dmlc-core/include)\n\nset_output_directory(xgboost4j ${PROJECT_SOURCE_DIR}/lib)\n\n# MacOS: Patch libxgboost4j.dylib to use @rpath/libomp.dylib\nif(USE_OPENMP AND APPLE)\n  patch_openmp_path_macos(xgboost4j libxgboost4j)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating XGBoost CMake Package Configuration\nDESCRIPTION: Configures and installs the CMake package configuration files for XGBoost, enabling other projects to use find_package(xgboost).\nSOURCE: https://github.com/dmlc/xgboost/blob/master/CMakeLists.txt#2025-04-19_snippet_26\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(CMakePackageConfigHelpers)\nconfigure_package_config_file(\n  ${CMAKE_CURRENT_LIST_DIR}/cmake/xgboost-config.cmake.in\n  ${CMAKE_CURRENT_BINARY_DIR}/cmake/xgboost-config.cmake\n  INSTALL_DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/xgboost)\nwrite_basic_package_version_file(\n  ${CMAKE_BINARY_DIR}/cmake/xgboost-config-version.cmake\n  VERSION ${XGBOOST_VERSION}\n  COMPATIBILITY AnyNewerVersion)\ninstall(\n  FILES\n  ${CMAKE_CURRENT_BINARY_DIR}/cmake/xgboost-config.cmake\n  ${CMAKE_BINARY_DIR}/cmake/xgboost-config-version.cmake\n  DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/xgboost)\n```\n\n----------------------------------------\n\nTITLE: Linking R Core Library to XGBoost R Package\nDESCRIPTION: Links the R core library with the XGBoost R package target to provide access to R's API functions.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/R-package/CMakeLists.txt#2025-04-19_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_link_libraries(xgboost-r PUBLIC ${LIBR_CORE_LIBRARY})\n```\n\n----------------------------------------\n\nTITLE: Generating C API Documentation\nDESCRIPTION: Sets up Doxygen documentation generation for the C API when enabled.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/CMakeLists.txt#2025-04-19_snippet_24\n\nLANGUAGE: CMake\nCODE:\n```\nif(BUILD_C_DOC)\n  include(cmake/Doc.cmake)\n  run_doxygen()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Version Change Script Usage - Python\nDESCRIPTION: Reference to a helper script used for changing XGBoost version numbers during the release process\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/release.rst#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nops/script/change_version.py\n```\n\n----------------------------------------\n\nTITLE: Setting Compiler Definitions for XGBoost R Integration\nDESCRIPTION: Defines several important compile-time flags that control XGBoost behavior in the R environment, including strict R mode and customized logging.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/R-package/CMakeLists.txt#2025-04-19_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_compile_definitions(\n  xgboost-r PUBLIC\n  -DXGBOOST_STRICT_R_MODE=1\n  -DDMLC_LOG_BEFORE_THROW=0\n  -DDMLC_DISABLE_STDIN=1\n  -DDMLC_LOG_CUSTOMIZE=1\n)\n```\n\n----------------------------------------\n\nTITLE: DaskQuantileDMatrix GPU Training Reference\nDESCRIPTION: Reference to the DaskQuantileDMatrix class for optimized GPU memory usage.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/dask.rst#2025-04-19_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nxgboost.dask.DaskQuantileDMatrix\n```\n\n----------------------------------------\n\nTITLE: S3 Artifact Upload Configuration in YAML\nDESCRIPTION: GitHub Actions workflow configuration for uploading artifacts to Amazon S3 bucket using manage-artifacts.py script.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/ci.rst#2025-04-19_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\n- name: Upload files to S3\n  run: |\n    REMOTE_PREFIX=\"remote directory to place the artifact(s)\"\n    python3 ops/pipeline/manage-artifacts.py upload \\\n      --s3-bucket ${{ env.RUNS_ON_S3_BUCKET_CACHE }} \\\n      --prefix cache/${{ github.run_id }}/${REMOTE_PREFIX} \\\n      path/to/file\n```\n\n----------------------------------------\n\nTITLE: Configuring SYCL Plugin Tests in XGBoost CMake\nDESCRIPTION: Sets up the SYCL plugin test configuration when PLUGIN_SYCL is enabled. Configures the Intel C++ compiler, creates a SYCL plugin test library, defines necessary compilation flags, and links it to the main test executable.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/tests/cpp/CMakeLists.txt#2025-04-19_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif(PLUGIN_SYCL)\n  set(CMAKE_CXX_COMPILER \"icpx\")\n  file(GLOB_RECURSE SYCL_TEST_SOURCES \"plugin/test_sycl_*.cc\")\n  add_library(plugin_sycl_test OBJECT ${SYCL_TEST_SOURCES})\n\n  target_include_directories(plugin_sycl_test\n    PRIVATE\n    ${gtest_SOURCE_DIR}/include\n    ${xgboost_SOURCE_DIR}/include\n    ${xgboost_SOURCE_DIR}/dmlc-core/include)\n\n  target_compile_definitions(plugin_sycl_test PUBLIC -DXGBOOST_USE_SYCL=1)\n  target_link_libraries(plugin_sycl_test PUBLIC -fsycl)\n  target_link_libraries(plugin_sycl_test PRIVATE ${GTEST_LIBRARIES})\n\n  set_target_properties(plugin_sycl_test PROPERTIES\n    COMPILE_FLAGS -fsycl\n    CXX_STANDARD 17\n    CXX_STANDARD_REQUIRED ON\n    POSITION_INDEPENDENT_CODE ON)\n  if(USE_OPENMP)\n    find_package(OpenMP REQUIRED)\n    set_target_properties(plugin_sycl_test PROPERTIES\n    COMPILE_FLAGS \"-fsycl -qopenmp\")\n  endif()\n  # Get compilation and link flags of plugin_sycl and propagate to testxgboost\n  target_link_libraries(testxgboost PUBLIC plugin_sycl_test)\n  # Add all objects of plugin_sycl to testxgboost\n  target_sources(testxgboost INTERFACE $<TARGET_OBJECTS:plugin_sycl_test>)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Include Directories for XGBoost R Package\nDESCRIPTION: Sets up the necessary include paths for the R package, including R's headers, XGBoost's main include directory, and the DMLC core headers.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/R-package/CMakeLists.txt#2025-04-19_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_include_directories(\n  xgboost-r PRIVATE\n  ${LIBR_INCLUDE_DIRS}\n  ${PROJECT_SOURCE_DIR}/include\n  ${PROJECT_SOURCE_DIR}/dmlc-core/include\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring RabitTracker\nDESCRIPTION: Example showing how to configure RabitTracker after removal of tracker_conf parameter.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost_spark_migration.rst#2025-04-19_snippet_7\n\nLANGUAGE: scala\nCODE:\n```\nval classifier = new XGBoostClassifer()\n  .setRabitTrackerTimeout(100)\n  .setRabitTrackerHostIp(\"192.168.0.2\")\n  .setRabitTrackerPort(19203)\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependency for XGBoost4j-Spark GPU\nDESCRIPTION: Maven dependency configuration for GPU-accelerated XGBoost4j-Spark implementation.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost_spark_migration.rst#2025-04-19_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<dependency>\n    <groupId>ml.dmlc</groupId>\n    <artifactId>xgboost4j-spark-gpu_${scala.binary.version}</artifactId>\n    <version>3.0.0</version>\n</dependency>\n```\n\n----------------------------------------\n\nTITLE: Setting Weights for DMatrix in XGBoost4J (Java)\nDESCRIPTION: This code shows how to set weights for a DMatrix object in XGBoost4J.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/java_intro.rst#2025-04-19_snippet_5\n\nLANGUAGE: java\nCODE:\n```\nfloat[] weights = new float[] {1f,2f,1f};\ndmat.setWeight(weights);\n```\n\n----------------------------------------\n\nTITLE: Setting up Protobuf and gRPC Dependencies for XGBoost\nDESCRIPTION: Configures CMake to find and use Protobuf and gRPC libraries, with fallback mechanisms to ensure proper detection of these dependencies. Sets Protobuf to be module-compatible and build shared libraries.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/plugin/federated/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\n# gRPC needs to be installed first. See README.md.\nset(protobuf_MODULE_COMPATIBLE TRUE)\nset(protobuf_BUILD_SHARED_LIBS TRUE)\n\nfind_package(Protobuf CONFIG)\nif(NOT Protobuf_FOUND)\n  find_package(Protobuf)\nendif()\nif(NOT Protobuf_FOUND)\n  # let CMake emit error\n  find_package(Protobuf CONFIG REQUIRED)\nendif()\n\nfind_package(gRPC CONFIG REQUIRED)\nmessage(STATUS \"Found gRPC: ${gRPC_CONFIG}\")\n```\n\n----------------------------------------\n\nTITLE: S3 Multiple Artifacts Upload Configuration\nDESCRIPTION: GitHub Actions workflow configuration for uploading multiple files to S3 with wildcard globbing support.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/ci.rst#2025-04-19_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\n- name: Upload files to S3\n  run: |\n    python3 ops/pipeline/manage-artifacts.py upload \\\n      --s3-bucket ${{ env.RUNS_ON_S3_BUCKET_CACHE }} \\\n      --prefix cache/${{ github.run_id }}/build-cuda \\\n      build/testxgboost python-package/dist/*.whl\n```\n\n----------------------------------------\n\nTITLE: Configuring GTest Dependencies in XGBoost CMake\nDESCRIPTION: Sets up Google Test dependencies for XGBoost testing. Either uses the DMLC bundled GTest or finds an external GTest package based on the USE_DMLC_GTEST flag.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/tests/cpp/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_DMLC_GTEST)\n  if(NOT TARGET gtest)\n    message(FATAL_ERROR \"USE_DMLC_GTEST=ON but dmlc-core didn't bundle gtest\")\n  endif()\n  set(GTEST_LIBRARIES gtest gmock)\nelse()\n  find_package(GTest REQUIRED)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting XGBoost Test Output Directory and Source Groups\nDESCRIPTION: Configures the output directory for the testxgboost executable and organizes source files into groups for Visual Studio.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/tests/cpp/CMakeLists.txt#2025-04-19_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nset_output_directory(testxgboost ${xgboost_BINARY_DIR})\n\n# This grouping organises source files nicely in visual studio\nauto_source_group(\"${TEST_SOURCES}\")\n```\n\n----------------------------------------\n\nTITLE: MSVC-Specific Compatibility Fix for R Complex Types\nDESCRIPTION: Adds a special compiler definition for MSVC to handle anonymous types in R complex structures, which otherwise cause syntax errors.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/R-package/CMakeLists.txt#2025-04-19_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif(MSVC)\n  # https://github.com/microsoft/LightGBM/pull/6061\n  # MSVC doesn't work with anonymous types in structs. (R complex)\n  #\n  # syntax error: missing ';' before identifier 'private_data_c'\n  #\n  target_compile_definitions(xgboost-r PRIVATE -DR_LEGACY_RCOMPLEX)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Checking Kubernetes Pod Status\nDESCRIPTION: Shows how to check the status of Kubernetes pods using kubectl.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/dask.rst#2025-04-19_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\nkubectl get pods\n```\n\n----------------------------------------\n\nTITLE: Building and Running C++ Tests\nDESCRIPTION: CMake commands to build and run C++ unit tests with Google Test\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/unit_tests.rst#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -S . -GNinja -DGOOGLE_TEST=ON -DUSE_DMLC_GTEST=ON -DUSE_CUDA=ON -DUSE_NCCL=ON\ncmake --build build\ncd ./build\n./testxgboost\n```\n\n----------------------------------------\n\nTITLE: Configuring Installation of XGBoost Headers and Libraries\nDESCRIPTION: Sets up installation rules for XGBoost headers, libraries, and targets with proper CMake export configurations.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/CMakeLists.txt#2025-04-19_snippet_25\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(CPack)\n\ninclude(GNUInstallDirs)\n# Install all headers.  Please note that currently the C++ headers does not form an \"API\".\ninstall(DIRECTORY ${xgboost_SOURCE_DIR}/include/xgboost\n  DESTINATION ${CMAKE_INSTALL_INCLUDEDIR})\n\n# Install libraries. If `xgboost` is a static lib, specify `objxgboost` also, to avoid the\n# following error:\n#\n#  > install(EXPORT ...) includes target \"xgboost\" which requires target \"objxgboost\" that is not\n#  > in any export set.\n#\n# https://github.com/dmlc/xgboost/issues/6085\nif(BUILD_STATIC_LIB)\n  if(BUILD_DEPRECATED_CLI)\n    set(INSTALL_TARGETS xgboost runxgboost objxgboost dmlc)\n  else()\n    set(INSTALL_TARGETS xgboost objxgboost dmlc)\n  endif()\nelse()\n  if(BUILD_DEPRECATED_CLI)\n    set(INSTALL_TARGETS xgboost runxgboost)\n  else()\n    set(INSTALL_TARGETS xgboost)\n  endif()\nendif()\n\ninstall(TARGETS ${INSTALL_TARGETS}\n  EXPORT XGBoostTargets\n  ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR}\n  LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}\n  RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR}\n  INCLUDES DESTINATION ${LIBLEGACY_INCLUDE_DIRS})\ninstall(EXPORT XGBoostTargets\n  FILE XGBoostTargets.cmake\n  NAMESPACE xgboost::\n  DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/xgboost)\n```\n\n----------------------------------------\n\nTITLE: Adding XGBoost4J SBT Dependency\nDESCRIPTION: Scala build.sbt configuration to include XGBoost4J as a dependency in an SBT project. This connects to the local Maven repository and adds the XGBoost library for Scala projects.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/build.rst#2025-04-19_snippet_19\n\nLANGUAGE: scala\nCODE:\n```\nresolvers += \"Local Maven Repository\" at \"file://\"+Path.userHome.absolutePath+\"/.m2/repository\"\n\n\"ml.dmlc\" % \"xgboost4j\" % \"latest_source_version_num\"\n```\n\n----------------------------------------\n\nTITLE: Docker Run Translation Example\nDESCRIPTION: Example of how docker_run.py commands are translated to actual docker run commands.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/ci.rst#2025-04-19_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm --pid=host \\\n  -w /workspace -v /path/to/xgboost:/workspace \\\n  -e CI_BUILD_UID=<uid> -e CI_BUILD_USER=<user_name>\n```\n\n----------------------------------------\n\nTITLE: Setting CUDA Include Directories for RMM Plugin in XGBoost CMake\nDESCRIPTION: Includes CUDA directories when both USE_CUDA and PLUGIN_RMM are enabled.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/tests/cpp/CMakeLists.txt#2025-04-19_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_CUDA AND PLUGIN_RMM)\n  target_include_directories(testxgboost PRIVATE ${CUDA_INCLUDE_DIRS})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Gathering Test Sources in XGBoost CMake\nDESCRIPTION: Collects all test source files using GLOB_RECURSE and handles special cases for CUDA, Federated, and SYCL plugins. Removes specific test sources that will be handled separately.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/tests/cpp/CMakeLists.txt#2025-04-19_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB_RECURSE TEST_SOURCES \"*.cc\")\n\nif(USE_CUDA)\n  file(GLOB_RECURSE CUDA_TEST_SOURCES \"*.cu\")\n  list(APPEND TEST_SOURCES ${CUDA_TEST_SOURCES})\nendif()\n\n# We will add them back later to separate the definition.\nfile(GLOB_RECURSE FEDERATED_TEST_SOURCES \"plugin/federated/*.*\")\nlist(REMOVE_ITEM TEST_SOURCES ${FEDERATED_TEST_SOURCES})\n\nfile(GLOB_RECURSE SYCL_TEST_SOURCES \"plugin/test_sycl_*.cc\")\nlist(REMOVE_ITEM TEST_SOURCES ${SYCL_TEST_SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Installing XGBoost R Package from CRAN\nDESCRIPTION: Command to install the stable version of XGBoost R package from CRAN. This is the recommended installation method for Windows and OS X users who want pre-compiled versions.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/R-package/README.md#2025-04-19_snippet_0\n\nLANGUAGE: r\nCODE:\n```\ninstall.packages('xgboost')\n```\n\n----------------------------------------\n\nTITLE: Starting Admin CLI\nDESCRIPTION: Command to launch the NVFlare admin command-line interface.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/nvflare/vertical/README.md#2025-04-19_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n/tmp/nvflare/poc/admin/startup/fl_admin.sh\n```\n\n----------------------------------------\n\nTITLE: Installing NVFlare Package\nDESCRIPTION: Command to install the NVFlare package using pip package manager.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/nvflare/vertical/README.md#2025-04-19_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install nvflare\n```\n\n----------------------------------------\n\nTITLE: Creating a Conda Environment for XGBoost Documentation\nDESCRIPTION: Command to create a dedicated conda environment for building XGBoost documentation. It specifically uses Python 3.10 which is required by the xgboost_ray package dependency.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/docs.rst#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n xgboost-docs --yes python=3.10\n```\n\n----------------------------------------\n\nTITLE: Force Pushing Git Changes\nDESCRIPTION: Command to force push local changes to remote repository after rebase.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/git_guide.rst#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit push --force\n```\n\n----------------------------------------\n\nTITLE: Adding CUDA-Specific Test Sources for Federated Plugin\nDESCRIPTION: Conditionally adds CUDA test files to the testxgboost target when CUDA support is enabled. These files specifically test collective operations and communication group functionality with GPU acceleration.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/tests/cpp/plugin/federated/CMakeLists.txt#2025-04-19_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_CUDA)\n  target_sources(\n    testxgboost PRIVATE\n    ${xgboost_SOURCE_DIR}/tests/cpp/plugin/federated/test_federated_coll.cu\n    ${xgboost_SOURCE_DIR}/tests/cpp/plugin/federated/test_federated_comm_group.cu\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating Protobuf Library for Federated Learning\nDESCRIPTION: Sets up a static library using the federated protocol definition. Links against Protobuf and gRPC libraries and configures include directories for the generated code.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/plugin/federated/CMakeLists.txt#2025-04-19_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\n# Generated code from the protobuf definition.\nadd_library(federated_proto STATIC federated.proto)\ntarget_link_libraries(federated_proto PUBLIC protobuf::libprotobuf gRPC::grpc gRPC::grpc++)\ntarget_include_directories(federated_proto PUBLIC ${CMAKE_CURRENT_BINARY_DIR})\nxgboost_target_properties(federated_proto)\n```\n\n----------------------------------------\n\nTITLE: Configuring Test Executable Include Directories and Libraries in XGBoost CMake\nDESCRIPTION: Sets up include directories and links necessary libraries for the testxgboost executable, including XGBoost headers, DMLC core, optional RMM libraries, and GTest/GMock.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/tests/cpp/CMakeLists.txt#2025-04-19_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_include_directories(testxgboost\n  PRIVATE\n  ${xgboost_SOURCE_DIR}/include\n  ${xgboost_SOURCE_DIR}/dmlc-core/include)\ntarget_link_libraries(testxgboost\n  PRIVATE\n  $<TARGET_NAME_IF_EXISTS:rmm::rmm_logger>\n  $<TARGET_NAME_IF_EXISTS:rmm::rmm_logger_impl>\n  GTest::gtest GTest::gmock)\n```\n\n----------------------------------------\n\nTITLE: Submitting Training Job\nDESCRIPTION: Admin CLI command to submit and start the vertical XGBoost training job.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/nvflare/vertical/README.md#2025-04-19_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nsubmit_job vertical-xgboost\n```\n\n----------------------------------------\n\nTITLE: Running Python GPU Tests\nDESCRIPTION: Commands to run Python GPU-specific tests\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/unit_tests.rst#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nexport PYTHONPATH=./python-package\npytest -v -s --fulltrace tests/python-gpu\n```\n\n----------------------------------------\n\nTITLE: Launching NVFlare Admin CLI\nDESCRIPTION: This command starts the NVFlare admin command-line interface for managing the federated learning job.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/nvflare/horizontal/README.md#2025-04-19_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n/tmp/nvflare/poc/admin/startup/fl_admin.sh\n```\n\n----------------------------------------\n\nTITLE: Including a Plugin in XGBoost with CMake\nDESCRIPTION: This snippet demonstrates how to add a plugin to XGBoost by modifying the CMakeLists.txt file. It shows the required CMake code to include a plugin source file named 'plugin_a.cc' into the build process.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/plugin/README.md#2025-04-19_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(PLUGIN_SOURCES ${PLUGIN_SOURCES}\n    ${xgboost_SOURCE_DIR}/plugin/plugin_a.cc PARENT_SCOPE)\n```\n\n----------------------------------------\n\nTITLE: Overriding XGBoost Configuration via Command Line\nDESCRIPTION: Example of how to override a configuration parameter (max_depth) via command line argument.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/CLI/binary_classification/README.md#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n../../xgboost mushroom.conf max_depth=6\n```\n\n----------------------------------------\n\nTITLE: Generating Protobuf and gRPC Code for Federated Protocol\nDESCRIPTION: Configures the generation of C++ code from the Protobuf definition file. Generates both standard Protobuf code and gRPC service code using the Protobuf compiler with the gRPC plugin.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/plugin/federated/CMakeLists.txt#2025-04-19_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nprotobuf_generate(\n    TARGET federated_proto\n    LANGUAGE cpp\n    PROTOC_OUT_DIR \"${PROTO_BINARY_DIR}\")\nprotobuf_generate(\n    TARGET federated_proto\n    LANGUAGE grpc\n    GENERATE_EXTENSIONS .grpc.pb.h .grpc.pb.cc\n    PLUGIN \"protoc-gen-grpc=\\$<TARGET_FILE:gRPC::grpc_cpp_plugin>\"\n    PROTOC_OUT_DIR \"${PROTO_BINARY_DIR}\")\n```\n\n----------------------------------------\n\nTITLE: Configuring DMLC Core Dependency\nDESCRIPTION: Sets up the DMLC core library dependency with options for shared C runtime.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/CMakeLists.txt#2025-04-19_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\n# dmlc-core\nset(DMLC_FORCE_SHARED_CRT ${FORCE_SHARED_CRT})\nadd_subdirectory(${xgboost_SOURCE_DIR}/dmlc-core)\n```\n\n----------------------------------------\n\nTITLE: Configuring Federated Learning Plugin for XGBoost in CMake\nDESCRIPTION: Simple CMake configuration that conditionally adds the Federated Learning subdirectory when the PLUGIN_FEDERATED option is enabled. This allows for optional inclusion of federated learning capabilities in the XGBoost build.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/plugin/CMakeLists.txt#2025-04-19_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\n# Add the Federate Learning plugin if enabled.\nif(PLUGIN_FEDERATED)\n  add_subdirectory(federated)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Client Scatter Operation Reference\nDESCRIPTION: Reference to the client.scatter operation for distributing data from client process to workers.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/dask.rst#2025-04-19_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nclient.scatter\n```\n\n----------------------------------------\n\nTITLE: Docker Build Arguments\nDESCRIPTION: Command line arguments for building Docker container with specific versions.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/ci.rst#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n--build-arg CUDA_VERSION_ARG=12.4.1 --build-arg NCCL_VERSION_ARG=2.23.4-1 \\\n  --build-arg RAPIDS_VERSION_ARG=24.10\n```\n\n----------------------------------------\n\nTITLE: Adding R Package Support\nDESCRIPTION: Conditionally adds the R package subdirectory when R library support is enabled.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/CMakeLists.txt#2025-04-19_snippet_12\n\nLANGUAGE: CMake\nCODE:\n```\n# Exports some R specific definitions and objects\nif(R_LIB)\n  add_subdirectory(${xgboost_SOURCE_DIR}/R-package)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring R Compiler for XGBoost Testing\nDESCRIPTION: Example of how to configure R to use a different compiler (clang++) for testing XGBoost. This involves modifying the ~/.R/Makevars file to specify alternative compilers.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/coding_guide.rst#2025-04-19_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nCC=clang-15\nCXX17=clang++-15\n```\n\nLANGUAGE: sh\nCODE:\n```\nR CMD config CXX17\n```\n\n----------------------------------------\n\nTITLE: Adding XGBoost Core Source and Linking with DMLC\nDESCRIPTION: Adds the main XGBoost source directory and links the object library with DMLC.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/CMakeLists.txt#2025-04-19_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\n# core xgboost\nadd_subdirectory(${xgboost_SOURCE_DIR}/src)\ntarget_link_libraries(objxgboost PUBLIC dmlc)\n```\n\n----------------------------------------\n\nTITLE: Custom Action Usage Example\nDESCRIPTION: Example of using a custom composite action from xgboost-devops repository in GitHub Actions workflow.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/ci.rst#2025-04-19_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\n- uses: dmlc/xgboost-devops/actions/miniforge-setup@main\n  with:\n    environment-name: cpp_test\n    environment-file: ops/conda_env/cpp_test.yml\n```\n\n----------------------------------------\n\nTITLE: Setting Output Directory for XGBoost Library\nDESCRIPTION: Configures the output directory for the XGBoost library based on build settings.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/CMakeLists.txt#2025-04-19_snippet_21\n\nLANGUAGE: CMake\nCODE:\n```\nif(KEEP_BUILD_ARTIFACTS_IN_BINARY_DIR)\n  set_output_directory(xgboost ${xgboost_BINARY_DIR}/lib)\nelse()\n  set_output_directory(xgboost ${xgboost_SOURCE_DIR}/lib)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Group Information File Format for XGBoost Ranking Tasks\nDESCRIPTION: Format for the group information file used in ranking tasks. Each number indicates the count of instances in each group. This file should be named with '.group' extension appended to the instance file name.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/input_format.rst#2025-04-19_snippet_1\n\nLANGUAGE: none\nCODE:\n```\n2\n3\n```\n\n----------------------------------------\n\nTITLE: Configuring MSVC Runtime Library Linkage\nDESCRIPTION: Sets up the Microsoft Visual C++ runtime library linkage based on FORCE_SHARED_CRT option, either as dynamic or static.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/CMakeLists.txt#2025-04-19_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nif(MSVC)\n  if(FORCE_SHARED_CRT)\n    message(STATUS \"XGBoost: Using dynamically linked MSVC runtime...\")\n    set(CMAKE_MSVC_RUNTIME_LIBRARY \"MultiThreaded$<$<CONFIG:Debug>:Debug>DLL\")\n  else()\n    message(STATUS \"XGBoost: Using statically linked MSVC runtime...\")\n    set(CMAKE_MSVC_RUNTIME_LIBRARY \"MultiThreaded$<$<CONFIG:Debug>:Debug>\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Testing Dirichlet Regression Functions in R\nDESCRIPTION: This snippet provides unit tests for the Dirichlet regression functions in R. It uses the testthat package to verify the correctness of the log-likelihood, gradient, and Hessian calculations.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/advanced_custom_obj.rst#2025-04-19_snippet_3\n\nLANGUAGE: r\nCODE:\n```\nlibrary(DirichletReg)\nlibrary(testthat)\n\ntest_that(\"dirichlet formulae\", {\n    k <- 3L\n    m <- 10L\n    set.seed(123)\n    alpha <- exp(rnorm(k))\n    y <- rdirichlet(m, alpha)\n    x0 <- rnorm(k)\n    \n    for (row in seq_len(m)) {\n        logpdf <- dirichlet.fun(matrix(x0, nrow=1), y[row,,drop=F])\n        ref_logpdf <- ddirichlet(y[row,,drop=F], exp(x0), log = T)\n        expect_equal(logpdf, -ref_logpdf)\n        \n        eps <- 1e-7\n        grad_num <- numeric(k)\n        for (col in seq_len(k)) {\n            xplus <- x0\n            xplus[col] <- x0[col] + eps\n            grad_num[col] <- (\n                dirichlet.fun(matrix(xplus, nrow=1), y[row,,drop=F])\n                - dirichlet.fun(matrix(x0, nrow=1), y[row,,drop=F])\n            ) / eps\n        }\n        \n        grad <- dirichlet.grad(matrix(x0, nrow=1), y[row,,drop=F])\n        expect_equal(grad |> as.vector(), grad_num, tolerance=1e-6)\n        \n        H_numeric <- array(dim=c(k, k))\n        for (ii in seq_len(k)) {\n            xplus <- x0\n            xplus[ii] <- x0[ii] + eps\n            for (jj in seq_len(k)) {\n                # Code truncated due to character limit\n            }\n        }\n    }\n})\n```\n\n----------------------------------------\n\nTITLE: Calculating Mean Squared Error Loss in Python\nDESCRIPTION: This code calculates the mean squared error loss, which is a common choice for the training loss function in supervised learning. It measures how well the model's predictions match the actual target values.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/model.rst#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef mse_loss(y_true, y_pred):\n    return sum((y_true[i] - y_pred[i])**2 for i in range(len(y_true)))\n```\n\n----------------------------------------\n\nTITLE: Installing NVFlare for Federated XGBoost\nDESCRIPTION: This command installs the NVFlare package, which is required for running the federated learning demo.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/nvflare/horizontal/README.md#2025-04-19_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install nvflare\n```\n\n----------------------------------------\n\nTITLE: Docker Container Configuration\nDESCRIPTION: YAML configuration for GPU-enabled Docker container build settings.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/ci.rst#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nxgb-ci.gpu:\n  container_def: gpu\n  build_args:\n    CUDA_VERSION_ARG: \"12.4.1\"\n    NCCL_VERSION_ARG: \"2.23.4-1\"\n    RAPIDS_VERSION_ARG: \"24.10\"\n```\n\n----------------------------------------\n\nTITLE: Running R Tests Directly\nDESCRIPTION: Commands to run R package tests directly using Rscript\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/unit_tests.rst#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd R-package/tests/\nRscript testthat.R\n```\n\n----------------------------------------\n\nTITLE: Setting CMake Policy Defaults\nDESCRIPTION: Configures CMake policy default settings for compatibility with submodules using lower minimum versions.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/CMakeLists.txt#2025-04-19_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(CMAKE_POLICY_DEFAULT_CMP0063 NEW)\nset(CMAKE_POLICY_DEFAULT_CMP0069 NEW)\nset(CMAKE_POLICY_DEFAULT_CMP0076 NEW)\nset(CMAKE_POLICY_DEFAULT_CMP0077 NEW)\nset(CMAKE_POLICY_DEFAULT_CMP0079 NEW)\n```\n\n----------------------------------------\n\nTITLE: Preparing Training Data\nDESCRIPTION: Script execution to prepare the HIGGS dataset for training. Downloads and processes 2.6GB compressed (7.5GB uncompressed) data.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/nvflare/vertical/README.md#2025-04-19_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n./prepare_data.sh\n```\n\n----------------------------------------\n\nTITLE: Adding pkg-config Support\nDESCRIPTION: Creates and installs a pkg-config file for XGBoost when enabled, allowing other projects to find XGBoost via pkg-config.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/CMakeLists.txt#2025-04-19_snippet_28\n\nLANGUAGE: CMake\nCODE:\n```\n# Add xgboost.pc\nif(ADD_PKGCONFIG)\n  configure_file(${xgboost_SOURCE_DIR}/cmake/xgboost.pc.in ${xgboost_BINARY_DIR}/xgboost.pc @ONLY)\n\n  install(\n    FILES ${xgboost_BINARY_DIR}/xgboost.pc\n    DESTINATION ${CMAKE_INSTALL_LIBDIR}/pkgconfig)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring C++ Test Sources for XGBoost Federated Plugin\nDESCRIPTION: Adds C++ test source files for the federated plugin to the testxgboost target. These test files cover various aspects of federated learning such as collective operations, communication, grouping, tracking, learning, and data handling.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/tests/cpp/plugin/federated/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_sources(\n  testxgboost PRIVATE\n  ${xgboost_SOURCE_DIR}/tests/cpp/plugin/federated/test_federated_coll.cc\n  ${xgboost_SOURCE_DIR}/tests/cpp/plugin/federated/test_federated_comm.cc\n  ${xgboost_SOURCE_DIR}/tests/cpp/plugin/federated/test_federated_comm_group.cc\n  ${xgboost_SOURCE_DIR}/tests/cpp/plugin/federated/test_federated_tracker.cc\n  ${xgboost_SOURCE_DIR}/tests/cpp/plugin/federated/test_federated_learner.cc\n  ${xgboost_SOURCE_DIR}/tests/cpp/plugin/federated/test_federated_data.cc\n)\n```\n\n----------------------------------------\n\nTITLE: Resolving Git Conflicts with Master Branch\nDESCRIPTION: Commands for rebasing local branch against upstream master. Includes adding remote, fetching updates, and performing rebase operation.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/git_guide.rst#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# The first two steps can be skipped after you do it once.\ngit remote add upstream https://github.com/dmlc/xgboost\ngit fetch upstream\ngit rebase upstream/master\n```\n\n----------------------------------------\n\nTITLE: Starting Second Worker Node\nDESCRIPTION: Command to start the second worker node in the federated setup.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/nvflare/vertical/README.md#2025-04-19_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n/tmp/nvflare/poc/site-2/startup/start.sh\n```\n\n----------------------------------------\n\nTITLE: Handling Federated Plugin Tests in XGBoost CMake\nDESCRIPTION: Adds the federated plugin tests subdirectory when PLUGIN_FEDERATED is enabled.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/tests/cpp/CMakeLists.txt#2025-04-19_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif(PLUGIN_FEDERATED)\n  add_subdirectory(${xgboost_SOURCE_DIR}/tests/cpp/plugin/federated)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Applying Kubernetes Cluster Specification\nDESCRIPTION: Demonstrates how to apply the generated Kubernetes cluster specification using kubectl.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/dask.rst#2025-04-19_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nkubectl apply -f ./cluster-spec.json\n```\n\n----------------------------------------\n\nTITLE: Installing XGBoost R Package\nDESCRIPTION: Command to install XGBoost R package from CRAN repository. The 1.0.0 version supports multi-core processing out of the box.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/NEWS.md#2025-04-19_snippet_9\n\nLANGUAGE: R\nCODE:\n```\ninstall.packages('xgboost')\n```\n\n----------------------------------------\n\nTITLE: Verifying System Library Path for XGBoost\nDESCRIPTION: Python code to check if libxgboost.so exists in the system library path, useful for package managers.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/build.rst#2025-04-19_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport sys\nimport pathlib\nlibpath = pathlib.Path(sys.base_prefix).joinpath(\"lib\", \"libxgboost.so\")\nassert libpath.exists()\n```\n\n----------------------------------------\n\nTITLE: Python XGBoost Early Stopping Limitation Example\nDESCRIPTION: Example showing how to handle early stopping behavior in XGBoost predictions by manually passing best iteration to ntree_limit parameter.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/NEWS.md#2025-04-19_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nbst.predict(X, ntree_limit=bst.best_iteration)\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake with Custom RMM Locations\nDESCRIPTION: CMake commands showing how to specify custom RMM installation locations for both Conda and custom installations.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/rmm_plugin/README.rst#2025-04-19_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\n# If using Conda:\ncmake -B build -S . -DUSE_CUDA=ON -DUSE_NCCL=ON -DPLUGIN_RMM=ON -DCMAKE_PREFIX_PATH=$CONDA_PREFIX\n# If using RMM installed with a custom location\ncmake -B build -S . -DUSE_CUDA=ON -DUSE_NCCL=ON -DPLUGIN_RMM=ON -DCMAKE_PREFIX_PATH=/path/to/rmm\n```\n\n----------------------------------------\n\nTITLE: Displaying Build Status and License Badges in RST\nDESCRIPTION: This RST code snippet uses raw HTML to display the Travis CI build status badge and the Apache 2.0 license badge for the XGBoost project.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/index.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n  <a href=\"https://travis-ci.org/dmlc/xgboost\">\n  <img alt=\"Build Status\" src=\"https://travis-ci.org/dmlc/xgboost.svg?branch=master\">\n  </a>\n  <a href=\"https://github.com/dmlc/xgboost/blob/master/LICENSE\">\n  <img alt=\"GitHub license\" src=\"https://dmlc.github.io/img/apache2.svg\">\n  </a>\n```\n\n----------------------------------------\n\nTITLE: Building XGBoost Documentation After Environment Setup\nDESCRIPTION: Command to build the HTML documentation after all dependencies have been installed. This processes all the documentation files and creates the final HTML output.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/docs.rst#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nmake html\n```\n\n----------------------------------------\n\nTITLE: Installing XGBoost directly from Homebrew\nDESCRIPTION: Direct installation of XGBoost using Homebrew package manager on Mac OSX.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/NEWS.md#2025-04-19_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nbrew install xgboost\n```\n\n----------------------------------------\n\nTITLE: Finding OpenMP on macOS and Other Platforms\nDESCRIPTION: Handles OpenMP discovery for different platforms, with a special case for macOS that uses a custom finder module.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/CMakeLists.txt#2025-04-19_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\n# -- OpenMP\ninclude(cmake/FindOpenMPMacOS.cmake)\nif(USE_OPENMP)\n  if(APPLE)\n    find_openmp_macos()\n  else()\n    find_package(OpenMP REQUIRED)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Interactive Git Rebase for Combining Commits\nDESCRIPTION: Command to start an interactive rebase for combining multiple commits into one.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/git_guide.rst#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngit rebase -i HEAD~3\n```\n\n----------------------------------------\n\nTITLE: Installing XGBoost Python Package as Editable Installation\nDESCRIPTION: Commands for creating an editable installation of the XGBoost Python package, which links to the source code for rapid development.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/build.rst#2025-04-19_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\n# Build shared library libxgboost.so\ncmake -B build -S . -GNinja\ncd build && ninja\n# Install as editable installation\ncd ../python-package\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: XGBoost Custom Metric Example Location\nDESCRIPTION: Reference to an example file showing how to implement custom evaluation metrics with raw predictions in Python.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/NEWS.md#2025-04-19_snippet_1\n\nLANGUAGE: text\nCODE:\n```\ndemo/guide-python/custom_softmax.py\n```\n\n----------------------------------------\n\nTITLE: Local CPU Build Command\nDESCRIPTION: Command to run CPU build tests in Docker container locally.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/ci.rst#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 ops/docker_run.py \\\n  --image-uri 492475357299.dkr.ecr.us-west-2.amazonaws.com/xgb-ci.cpu:main \\\n  -- bash ops/pipeline/build-cpu-impl.sh cpu\n```\n\n----------------------------------------\n\nTITLE: Installing Python Package Using Pre-built Shared Library\nDESCRIPTION: Command for installing the XGBoost Python package using a previously built shared library to speed up installation.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/build.rst#2025-04-19_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncd python-package/\npip install .  # Will re-use lib/libxgboost.so\n```\n\n----------------------------------------\n\nTITLE: Setting Up XGBoost Testing\nDESCRIPTION: Configures the XGBoost test suite with unit tests and CLI tests when Google Test is enabled.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/CMakeLists.txt#2025-04-19_snippet_27\n\nLANGUAGE: CMake\nCODE:\n```\n#-- Test\nif(GOOGLE_TEST)\n  enable_testing()\n  # Unittests.\n  add_executable(testxgboost)\n  target_link_libraries(testxgboost PRIVATE objxgboost)\n  xgboost_target_properties(testxgboost)\n  xgboost_target_link_libraries(testxgboost)\n  xgboost_target_defs(testxgboost)\n\n  add_subdirectory(${xgboost_SOURCE_DIR}/tests/cpp)\n\n  add_test(\n    NAME TestXGBoostLib\n    COMMAND testxgboost\n    WORKING_DIRECTORY ${xgboost_BINARY_DIR})\n  # CLI tests\n  configure_file(\n    ${xgboost_SOURCE_DIR}/tests/cli/machine.conf.in\n    ${xgboost_BINARY_DIR}/tests/cli/machine.conf\n    @ONLY\n    NEWLINE_STYLE UNIX)\n  if(BUILD_DEPRECATED_CLI)\n    add_test(\n      NAME TestXGBoostCLI\n      COMMAND runxgboost ${xgboost_BINARY_DIR}/tests/cli/machine.conf\n      WORKING_DIRECTORY ${xgboost_BINARY_DIR})\n    set_tests_properties(TestXGBoostCLI\n      PROPERTIES\n      PASS_REGULAR_EXPRESSION \".*test-rmse:0.087.*\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Compiler Version Checks\nDESCRIPTION: Verifies that compiler versions meet minimum requirements for building XGBoost, including checks for MSVC, GCC, AppleClang, and Clang.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/CMakeLists.txt#2025-04-19_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif(MSVC)\n  if(MSVC_VERSION LESS 1920)\n    message(FATAL_ERROR \"Need Visual Studio 2019 or newer to build XGBoost\")\n  endif()\nelseif(CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\")\n  if(CMAKE_CXX_COMPILER_VERSION VERSION_LESS \"8.1\")\n    message(FATAL_ERROR \"Need GCC 8.1 or newer to build XGBoost\")\n  endif()\nelseif(CMAKE_CXX_COMPILER_ID STREQUAL \"AppleClang\")\n  if(CMAKE_CXX_COMPILER_VERSION VERSION_LESS \"11.0\")\n    message(FATAL_ERROR \"Need Xcode 11.0 (AppleClang 11.0) or newer to build XGBoost\")\n  endif()\nelseif(CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\")\n  if(CMAKE_CXX_COMPILER_VERSION VERSION_LESS \"9.0\")\n    message(FATAL_ERROR \"Need Clang 9.0 or newer to build XGBoost\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Testing Dirichlet Expected Hessian in Python\nDESCRIPTION: Test function that validates the expected Hessian implementation by comparing it to empirically calculated values from samples drawn from a Dirichlet distribution.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/advanced_custom_obj.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef test_dirichlet_expected_hess():\n    k = 3\n    rng = np.random.default_rng(seed=123)\n    x0 = rng.standard_normal(size=k)\n    y_sample = rng.dirichlet(np.exp(x0), size=5_000_000)\n    x_broadcast = np.broadcast_to(x0, (y_sample.shape[0], k))\n    g_sample = dirichlet_grad(x_broadcast, y_sample)\n    ref = (g_sample.T @ g_sample) / y_sample.shape[0]\n    Ehess = dirichlet_expected_hess(x0.reshape((1,-1)))[0]\n    np.testing.assert_almost_equal(Ehess, ref, decimal=2)\ntest_dirichlet_expected_hess()\n```\n\n----------------------------------------\n\nTITLE: Running R Package Tests\nDESCRIPTION: Command to build and check the XGBoost R package using Python script\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/unit_tests.rst#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython ./ops/script/test_r_package.py --task=check\n```\n\n----------------------------------------\n\nTITLE: Patching OpenMP Path on macOS\nDESCRIPTION: Applies OpenMP path fixes specifically for macOS builds when OpenMP is enabled.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/CMakeLists.txt#2025-04-19_snippet_20\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_OPENMP AND APPLE)\n  patch_openmp_path_macos(xgboost libxgboost)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Generating Kubernetes Cluster Specification for XGBoost\nDESCRIPTION: Shows how to generate a Kubernetes cluster specification for XGBoost using Dask Kubernetes. This specification can be used to pre-allocate a cluster before running Dask workflows.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/dask.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport json\n\nfrom dask_kubernetes.operator import make_cluster_spec\n\nspec = make_cluster_spec(name=\"xgboost-test\", image=\"my-image-name:latest\", n_workers=16)\nwith open(\"cluster-spec.json\", \"w\") as fd:\n    json.dump(spec, fd, indent=2)\n```\n\n----------------------------------------\n\nTITLE: Configuring Test Executable Source Files in XGBoost CMake\nDESCRIPTION: Adds the collected test sources and the custom objective example to the testxgboost executable.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/tests/cpp/CMakeLists.txt#2025-04-19_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_sources(\n  testxgboost PRIVATE\n  ${TEST_SOURCES}\n  ${xgboost_SOURCE_DIR}/plugin/example/custom_obj.cc\n)\n```\n\n----------------------------------------\n\nTITLE: Defining a Table of Contents Tree in RST\nDESCRIPTION: This RST code snippet defines a table of contents tree for the XGBoost JVM Package documentation, including links to various tutorials and examples.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/index.rst#2025-04-19_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n  :maxdepth: 2\n\n  java_intro\n  XGBoost4J-Spark Tutorial <xgboost4j_spark_tutorial>\n  XGBoost4J-Spark-GPU Tutorial <xgboost4j_spark_gpu_tutorial>\n  Code Examples <https://github.com/dmlc/xgboost/tree/master/jvm-packages/xgboost4j-example>\n  API docs <api>\n  How to migrate to XGBoost-Spark jvm 3.x <xgboost_spark_migration>\n```\n\n----------------------------------------\n\nTITLE: Implementing Simple Multi-Class Error Metric for XGBoost\nDESCRIPTION: This function implements a simpler version of the multi-class error metric, used when there's no custom objective and XGBoost handles the prediction transformation internally.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/custom_metric_obj.rst#2025-04-19_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef merror(predt: np.ndarray, dtrain: xgb.DMatrix):\n    \"\"\"Used when there's no custom objective.\"\"\"\n    # No need to do transform, XGBoost handles it internally.\n    errors = np.zeros(dtrain.num_row())\n    errors[y != out] = 1.0\n    return 'PyMError', np.sum(errors) / dtrain.num_row()\n```\n\n----------------------------------------\n\nTITLE: Shutting Down System\nDESCRIPTION: Admin CLI commands to shutdown both client and server components.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/nvflare/vertical/README.md#2025-04-19_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nshutdown client\nshutdown server\n```\n\n----------------------------------------\n\nTITLE: Listing Python Dependencies for XGBoost Documentation\nDESCRIPTION: This is a requirements file listing all Python packages needed for building XGBoost documentation, running examples, and tests. It includes documentation tools like Sphinx, data science libraries like NumPy and scikit-learn, and distributed computing frameworks like Ray and Dask.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/requirements.txt#2025-04-19_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nsphinx\nmock\nsphinx_rtd_theme>=1.0.0\nbreathe\nscikit-learn\nsh\nmatplotlib\ngraphviz\nnumpy\nscipy\nmyst-parser\nray[train]\nsphinx-gallery\nsphinx-issues\nsphinx-tabs\ndask\npyspark\ncloudpickle\nsetuptools\n```\n\n----------------------------------------\n\nTITLE: Alternative for Installing Documentation Dependencies with Conda\nDESCRIPTION: Command to install dependencies using conda instead of pip, though it is noted this won't work for all dependencies since xgboost_ray is not available in conda channels.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/docs.rst#2025-04-19_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nconda install --file requirements.txt --yes -c conda-forge\n```\n\n----------------------------------------\n\nTITLE: Running Higgs Challenge Script\nDESCRIPTION: Command to execute the Higgs challenge script after setting up the environment and data files.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/kaggle-higgs/README.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./run.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring XGBoost C API Demo Project Build\nDESCRIPTION: CMake configuration that sets up a C project to demonstrate XGBoost API usage. It handles finding and linking the XGBoost library, with special handling for static builds that require C++ dependencies.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/c-api/basic/CMakeLists.txt#2025-04-19_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nproject(api-demo LANGUAGES C VERSION 0.0.1)\nfind_package(xgboost REQUIRED)\n\n# xgboost is built as static libraries, all cxx dependencies need to be linked into the\n# executable.\nif(XGBOOST_BUILD_STATIC_LIB)\n  enable_language(CXX)\n  # find again for those  cxx libraries.\n  find_package(xgboost REQUIRED)\nendif()\n\nadd_executable(api-demo c-api-demo.c)\ntarget_link_libraries(api-demo PRIVATE xgboost::xgboost)\n```\n\n----------------------------------------\n\nTITLE: Running XGBoost Experiments for Million Song Dataset in Bash\nDESCRIPTION: This bash script executes the Year Prediction experiment on the Million Song Dataset using XGBoost. The script likely handles data preparation, model training, and evaluation of results.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/CLI/yearpredMSD/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./runexp.sh\n```\n\n----------------------------------------\n\nTITLE: Installing Python Documentation Dependencies with pip\nDESCRIPTION: Command to install all required Python packages for building XGBoost documentation using pip. This uses a requirements.txt file that lists all necessary dependencies.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/docs.rst#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Starting First Worker Node\nDESCRIPTION: Command to start the first worker node in the federated setup.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/nvflare/vertical/README.md#2025-04-19_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n/tmp/nvflare/poc/site-1/startup/start.sh\n```\n\n----------------------------------------\n\nTITLE: Exporting XGBoost Booster from PySpark Model\nDESCRIPTION: Shows how to extract the underlying XGBoost booster object from a trained SparkXGBRegressor model for use with other XGBoost interfaces or for saving in standard XGBoost formats.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/spark_estimator.rst#2025-04-19_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nregressor = SparkXGBRegressor()\nmodel = regressor.fit(train_df)\n# the same booster object returned by xgboost.train\nbooster: xgb.Booster = model.get_booster()\nbooster.predict(...)\nbooster.save_model(\"model.json\") # or model.ubj, depending on your choice of format.\n```\n\n----------------------------------------\n\nTITLE: RST Section Header Definition\nDESCRIPTION: ReStructuredText markup defining a reference label and section header for Dask examples documentation\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/dask/README.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _dask-examples:\n\nXGBoost Dask Feature Walkthrough\n================================\n```\n\n----------------------------------------\n\nTITLE: Starting NVFlare Server\nDESCRIPTION: Command to initialize and start the NVFlare federated server.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/nvflare/vertical/README.md#2025-04-19_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n/tmp/nvflare/poc/server/startup/start.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake for XGBoost as a Subdirectory\nDESCRIPTION: This CMake snippet shows how to configure a project to use XGBoost when it's included as a subdirectory in the project. It adds XGBoost as a subdirectory and links it to the target executable.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/demo/c-api/basic/README.md#2025-04-19_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(xgboost)\nadd_executable(api-demo c-api-demo.c)\ntarget_link_libraries(api-demo xgboost)\n```\n\n----------------------------------------\n\nTITLE: Calculating Second-Order Derivative for Loss Function in XGBoost\nDESCRIPTION: This formula shows how XGBoost calculates the second-order derivative (Hessian) of the loss function with respect to the prediction from the previous iteration.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/model.rst#2025-04-19_snippet_4\n\nLANGUAGE: math\nCODE:\n```\nh_i &= \\partial_{\\hat{y}_i^{(t-1)}}^2 l(y_i, \\hat{y}_i^{(t-1)})\n```\n\n----------------------------------------\n\nTITLE: S3 Wildcard Download Configuration\nDESCRIPTION: GitHub Actions workflow for downloading multiple artifacts using wildcard pattern matching.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/ci.rst#2025-04-19_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\n- name: Download files from S3\n  run: |\n    python3 ops/pipeline/manage-artifacts.py download \\\n      --s3-bucket ${{ env.RUNS_ON_S3_BUCKET_CACHE }} \\\n      --prefix cache/${{ github.run_id }}/${REMOTE_PREFIX} \\\n      --dest-dir wheelhouse/ \\\n      *.whl\n```\n\n----------------------------------------\n\nTITLE: Configuring Build Options\nDESCRIPTION: Defines various build options including documentation, OpenMP support, static library building, and other configuration flags.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/CMakeLists.txt#2025-04-19_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\noption(BUILD_C_DOC \"Build documentation for C APIs using Doxygen.\" OFF)\noption(USE_OPENMP \"Build with OpenMP support.\" ON)\noption(BUILD_STATIC_LIB \"Build static library\" OFF)\noption(BUILD_DEPRECATED_CLI \"Build the deprecated command line interface\" OFF)\noption(FORCE_SHARED_CRT \"Build with dynamic CRT on Windows (/MD)\" OFF)\n```\n\n----------------------------------------\n\nTITLE: Running JVM Package Tests\nDESCRIPTION: Maven command to run JVM package tests\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/unit_tests.rst#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmvn test\n```\n\n----------------------------------------\n\nTITLE: XGBoost Regression Objective Functions\nDESCRIPTION: List of regression-focused objective functions including squared error, log error, logistic, Huber loss and absolute error regression options\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst#2025-04-19_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\nreg:squarederror\nreg:squaredlogerror\nreg:logistic\nreg:pseudohubererror\nreg:absoluteerror\nreg:quantileerror\n```\n\n----------------------------------------\n\nTITLE: Running CMake Linting Checks for XGBoost\nDESCRIPTION: Bash command to run linting checks on CMake scripts in the XGBoost project.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/coding_guide.rst#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbash ./ops/script/lint_cmake.sh\n```\n\n----------------------------------------\n\nTITLE: Mathematical Expression for Floating-Point Non-associativity\nDESCRIPTION: Demonstrates the non-associative property of floating-point arithmetic operations that affects reproducibility in distributed computing.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/dask.rst#2025-04-19_snippet_15\n\nLANGUAGE: math\nCODE:\n```\n(a + b) + c ≠ a + (b + c)\n```\n\n----------------------------------------\n\nTITLE: Running R Linting Checks for XGBoost\nDESCRIPTION: R commands to install the XGBoost R package and run linting checks on the R code. This uses a custom R script for linting.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/coding_guide.rst#2025-04-19_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd /path/to/xgboost/\nR CMD INSTALL R-package/\nRscript ops/script/lint_r.R $(pwd)\n```\n\n----------------------------------------\n\nTITLE: Creating a Table of Contents in RST\nDESCRIPTION: This RST code snippet creates a local table of contents for the Installation section, with no backlinks.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/index.rst#2025-04-19_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. contents::\n  :local:\n  :backlinks: none\n```\n\n----------------------------------------\n\nTITLE: DataFrame Merge Operation Reference\nDESCRIPTION: Reference to non-deterministic DataFrame merge operations in parallel hardware environments.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/tutorials/dask.rst#2025-04-19_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nDataFrame.merge\n```\n\n----------------------------------------\n\nTITLE: RST Link Structure for XGBoost JVM Documentation\nDESCRIPTION: ReStructuredText formatted links to XGBoost JVM package documentation, including paths to Java and Scala API documentation for different implementations.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/api.rst#2025-04-19_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n#############################\nAPI Docs for the JVM packages\n#############################\n\n* `XGBoost4J Java API <../jvm_docs/javadocs/index.html>`_\n* `XGBoost4J Scala API <../jvm_docs/scaladocs/xgboost4j/index.html>`_\n* `XGBoost4J-Spark Scala API <../jvm_docs/scaladocs/xgboost4j-spark/index.html>`_\n* `XGBoost4J-Spark-GPU Scala API <../jvm_docs/scaladocs/xgboost4j-spark-gpu/index.html>`_\n* `XGBoost4J-Flink Scala API <../jvm_docs/scaladocs/xgboost4j-flink/index.html>`_\n```\n\n----------------------------------------\n\nTITLE: Continuing Git Rebase Operation\nDESCRIPTION: Command to continue the rebase process after resolving conflicts.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/contrib/git_guide.rst#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit rebase --continue\n```\n\n----------------------------------------\n\nTITLE: Installing XGBoost via pip\nDESCRIPTION: Command to install the stable version of XGBoost Python package using pip package manager from PyPI repository.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/python-package/README.dft.rst#2025-04-19_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install xgboost\n```\n\n----------------------------------------\n\nTITLE: Migrating XGBoost Ranking Implementation\nDESCRIPTION: Code example showing migration from XGBoostRegressor to dedicated XGBoostRanker for ranking tasks.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/jvm/xgboost_spark_migration.rst#2025-04-19_snippet_4\n\nLANGUAGE: scala\nCODE:\n```\n// before xgboost4j-spark 3.0\nval regressor = new XGBoostRegressor().setObjective(\"rank:ndcg\")\n\n// after xgboost4j-spark 3.0\nval ranker = new XGBoostRanker()\n```\n\n----------------------------------------\n\nTITLE: Safe YAML Loading\nDESCRIPTION: Security improvement to use safe_load instead of load for YAML processing\nSOURCE: https://github.com/dmlc/xgboost/blob/master/NEWS.md#2025-04-19_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\nyaml.safe_load()\n# Instead of\nyaml.load()\n```\n\n----------------------------------------\n\nTITLE: Using XGBoosterReset in C++\nDESCRIPTION: Example of using the new XGBoosterReset function to release memory held by internal tree methods in C++.\nSOURCE: https://github.com/dmlc/xgboost/blob/master/doc/changes/v3.0.0.rst#2025-04-19_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nXGBoosterReset(booster_handle);\n```"
  }
]
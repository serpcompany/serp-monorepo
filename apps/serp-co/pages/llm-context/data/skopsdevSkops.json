[
  {
    "owner": "skops-dev",
    "repo": "skops",
    "content": "TITLE: Saving and Loading Models with Skops\nDESCRIPTION: Demonstrates how to save and load machine learning models using skops.io functions. Shows working with XGBoost classifier, handling untrusted types, and both file-based and in-memory serialization.\nSOURCE: https://github.com/skops-dev/skops/blob/main/docs/persistence.rst#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.datasets import load_iris\nfrom skops.io import dump, load, get_untrusted_types\n\nX, y = load_iris(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\nparam_grid = {\"tree_method\": [\"exact\", \"approx\", \"hist\"]}\nclf = GridSearchCV(XGBClassifier(), param_grid=param_grid).fit(X_train, y_train)\nprint(clf.score(X_test, y_test))\n0.9666666666666667\ndump(clf, \"my-model.skops\")\n# ...\nunknown_types = get_untrusted_types(file=\"my-model.skops\")\nprint(unknown_types)\n['sklearn.metrics._scorer._passthrough_scorer',\n'xgboost.core.Booster', 'xgboost.sklearn.XGBClassifier']\nloaded = load(\"my-model.skops\", trusted=unknown_types)\nprint(loaded.score(X_test, y_test))\n0.9666666666666667\n\n# in memory\nfrom skops.io import dumps, loads\nserialized = dumps(clf)\nloaded = loads(serialized, trusted=unknown_types)\n```\n\n----------------------------------------\n\nTITLE: Using CLIP Model with Transformers Library for Image-Text Similarity\nDESCRIPTION: This snippet demonstrates how to use the CLIP model with the Transformers library to calculate similarity between images and text. It loads a pre-trained CLIP model, processes an image and text prompts, and calculates probability scores for text-image matches.\nSOURCE: https://github.com/skops-dev/skops/blob/main/skops/card/tests/examples/clip-vit-large-patch14.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom PIL import Image\nimport requests\n\nfrom transformers import CLIPProcessor, CLIPModel\n\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n\noutputs = model(**inputs)\nlogits_per_image = outputs.logits_per_image # this is the image-text similarity score\nprobs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities\n```\n\n----------------------------------------\n\nTITLE: Using BERT for Masked Language Modeling in Python\nDESCRIPTION: This snippet demonstrates how to use the BERT base uncased model with a pipeline for masked language modeling. It shows how to create an unmasker and predict masked words in a sentence.\nSOURCE: https://github.com/skops-dev/skops/blob/main/skops/card/tests/examples/bert-base-uncased.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-uncased')\n>>> unmasker(\"Hello I'm a [MASK] model.\")\n[{'sequence': \"[CLS] hello i'm a fashion model. [SEP]\",\n  'score': 0.1073106899857521,\n  'token': 4827,\n  'token_str': 'fashion'},\n {'sequence': \"[CLS] hello i'm a role model. [SEP]\",\n  'score': 0.08774490654468536,\n  'token': 2535,\n  'token_str': 'role'},\n {'sequence': \"[CLS] hello i'm a new model. [SEP]\",\n  'score': 0.05338378623127937,\n  'token': 2047,\n  'token_str': 'new'},\n {'sequence': \"[CLS] hello i'm a super model. [SEP]\",\n  'score': 0.04667217284440994,\n  'token': 3565,\n  'token_str': 'super'},\n {'sequence': \"[CLS] hello i'm a fine model. [SEP]\",\n  'score': 0.027095865458250046,\n  'token': 2986,\n  'token_str': 'fine'}]\n```\n\n----------------------------------------\n\nTITLE: Using Vision Transformer Model with Feature Extractor in Python\nDESCRIPTION: This code demonstrates how to load a pre-trained ViT model and feature extractor, process an image, and extract features. It shows the complete workflow from loading an image from a URL to obtaining the model's output representation.\nSOURCE: https://github.com/skops-dev/skops/blob/main/skops/card/tests/examples/vit-base-patch32-224-in21k.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import ViTFeatureExtractor, ViTModel\nfrom PIL import Image\nimport requests\nurl = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\nfeature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch32-224-in21k')\nmodel = ViTModel.from_pretrained('google/vit-base-patch32-224-in21k')\ninputs = feature_extractor(images=image, return_tensors=\"pt\")\noutputs = model(**inputs)\nlast_hidden_state = outputs.last_hidden_state\n```\n\n----------------------------------------\n\nTITLE: Generating Text with GPT-2 using Transformers Pipeline in Python\nDESCRIPTION: This snippet demonstrates how to use the Hugging Face Transformers pipeline for text generation with the GPT-2 model. It sets a seed for reproducibility and generates five different text completions based on a prompt.\nSOURCE: https://github.com/skops-dev/skops/blob/main/skops/card/tests/examples/gpt2.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='gpt2')\n>>> set_seed(42)\n>>> generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)\n\n[{'generated_text': \"Hello, I'm a language model, a language for thinking, a language for expressing thoughts.\"},\n {'generated_text': \"Hello, I'm a language model, a compiler, a compiler library, I just want to know how I build this kind of stuff. I don\"},\n {'generated_text': \"Hello, I'm a language model, and also have more than a few of your own, but I understand that they're going to need some help\"},\n {'generated_text': \"Hello, I'm a language model, a system model. I want to know my language so that it might be more interesting, more user-friendly\"},\n {'generated_text': 'Hello, I\\'m a language model, not a language model\"\\n\\nThe concept of \"no-tricks\" comes in handy later with new'}]\n```\n\n----------------------------------------\n\nTITLE: Defining Model Card Metadata in YAML\nDESCRIPTION: This snippet shows an example of the metadata section in a README.md file for a model card. It includes information about the library, tags, license, datasets, and metrics.\nSOURCE: https://github.com/skops-dev/skops/blob/main/docs/model_card.rst#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\nlibrary_name: sklearn\ntags:\n- tabular-classification\nlicense: mit\ndatasets:\n- breast-cancer\nmetrics:\n- accuracy\n---\n```\n\n----------------------------------------\n\nTITLE: Extracting Features with BERT in PyTorch\nDESCRIPTION: This code snippet shows how to use the BERT base uncased model to extract features from a given text using PyTorch. It demonstrates tokenization and model inference.\nSOURCE: https://github.com/skops-dev/skops/blob/main/skops/card/tests/examples/bert-base-uncased.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import BertTokenizer, BertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\n----------------------------------------\n\nTITLE: Extracting Features with GPT-2 in PyTorch\nDESCRIPTION: This code example shows how to load the GPT-2 model and tokenizer using PyTorch to extract features from a given text input. It demonstrates the basic setup for inference with the model.\nSOURCE: https://github.com/skops-dev/skops/blob/main/skops/card/tests/examples/gpt2.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import GPT2Tokenizer, GPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2Model.from_pretrained('gpt2')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='pt')\noutput = model(**encoded_input)\n```\n\n----------------------------------------\n\nTITLE: Saving and Loading Model Cards in Python\nDESCRIPTION: These snippets show how to save a model card to a README.md file and how to load an existing model card for further modifications using the skops library.\nSOURCE: https://github.com/skops-dev/skops/blob/main/docs/model_card.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncard.save(\"README.md\")\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom skops import card\nmodel_card = card.parse_modelcard(\"README.md\")\nmodel_card.add(**{\"A new section\": \"Some new content\"})\nmodel_card.save(\"README.md\")\n```\n\n----------------------------------------\n\nTITLE: Visualizing Skops Files\nDESCRIPTION: Shows how to visualize skops files to understand their structure. The visualization helps identify trusted and untrusted components within a serialized model.\nSOURCE: https://github.com/skops-dev/skops/blob/main/docs/persistence.rst#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport skops.io as sio\nsio.visualize(\"my-model.skops\")\n```\n\n----------------------------------------\n\nTITLE: Adding Subsections to Model Card in Python\nDESCRIPTION: This code demonstrates how to add subsections to an existing section in a model card using the Card.add method. It also shows how to add plots to specific subsections.\nSOURCE: https://github.com/skops-dev/skops/blob/main/docs/model_card.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncard.add(**{\"Model description/Figures\": \"Here are some nice figures\"})\ncard.add_plot(**{\n    \"Model description/Figures/Confusion Matrix\": \"path-to-confusion-matrix.png\",\n    \"Model description/Figures/ROC\": \"path-to-roc.png\",\n})\n```\n\n----------------------------------------\n\nTITLE: Extracting Features with BERT in TensorFlow\nDESCRIPTION: This snippet demonstrates how to use the BERT base uncased model to extract features from a given text using TensorFlow. It includes tokenization and model inference steps.\nSOURCE: https://github.com/skops-dev/skops/blob/main/skops/card/tests/examples/bert-base-uncased.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import BertTokenizer, TFBertModel\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = TFBertModel.from_pretrained(\"bert-base-uncased\")\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n----------------------------------------\n\nTITLE: Extracting Features with GPT-2 in TensorFlow\nDESCRIPTION: This snippet demonstrates how to use GPT-2 with TensorFlow to extract features from text. It shows the TensorFlow implementation equivalent to the PyTorch example, using the same tokenizer but with the TensorFlow model variant.\nSOURCE: https://github.com/skops-dev/skops/blob/main/skops/card/tests/examples/gpt2.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import GPT2Tokenizer, TFGPT2Model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = TFGPT2Model.from_pretrained('gpt2')\ntext = \"Replace me by any text you'd like.\"\nencoded_input = tokenizer(text, return_tensors='tf')\noutput = model(encoded_input)\n```\n\n----------------------------------------\n\nTITLE: Selecting and Deleting Sections in Model Card using Python\nDESCRIPTION: This snippet shows how to select existing sections and subsections using Card.select method, and how to delete sections using Card.delete method.\nSOURCE: https://github.com/skops-dev/skops/blob/main/docs/model_card.rst#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsection = card.select(\"Model description/Figures\")\nprint(section.content)  # 'Here are some nice figures'\nprint(section.subsections)\ncard.delete(\"Model description/Figures/ROC\")\n```\n\n----------------------------------------\n\nTITLE: Using Compression with Skops\nDESCRIPTION: Shows how to compress model files using zlib compression when saving with skops.io.dump. Compression can reduce file size when storage space is a concern.\nSOURCE: https://github.com/skops-dev/skops/blob/main/docs/persistence.rst#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom zipfile import ZIP_DEFLATED\ndump(clf, \"my-model.skops\", compression=ZIP_DEFLATED, compresslevel=9)\n```\n\n----------------------------------------\n\nTITLE: Converting Files Using Skops CLI\nDESCRIPTION: Command line examples for converting pickle files to skops format. The skops convert command loads pickle files and saves them to the more secure skops format.\nSOURCE: https://github.com/skops-dev/skops/blob/main/docs/persistence.rst#2025-04-22_snippet_2\n\nLANGUAGE: console\nCODE:\n```\nskops convert my_model.pkl\n```\n\nLANGUAGE: console\nCODE:\n```\nfor FILE in *.pkl; do skops convert FILE; done\n```\n\n----------------------------------------\n\nTITLE: Folding Sections in Model Card using Python\nDESCRIPTION: This code demonstrates how to fold a section in a model card, which collapses it by default when the card is rendered. It uses the Section.folded property.\nSOURCE: https://github.com/skops-dev/skops/blob/main/docs/model_card.rst#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nsection = card.select(\"Model description/Figures\")\nsection.folded = True\n```\n\n----------------------------------------\n\nTITLE: Demonstrating BERT's Bias in Masked Language Modeling\nDESCRIPTION: This code example illustrates potential biases in BERT's predictions for masked words, comparing results for sentences about men and women in job-related contexts.\nSOURCE: https://github.com/skops-dev/skops/blob/main/skops/card/tests/examples/bert-base-uncased.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> from transformers import pipeline\n>>> unmasker = pipeline('fill-mask', model='bert-base-uncased')\n>>> unmasker(\"The man worked as a [MASK].\")\n[{'sequence': '[CLS] the man worked as a carpenter. [SEP]',\n  'score': 0.09747550636529922,\n  'token': 10533,\n  'token_str': 'carpenter'},\n {'sequence': '[CLS] the man worked as a waiter. [SEP]',\n  'score': 0.0523831807076931,\n  'token': 15610,\n  'token_str': 'waiter'},\n {'sequence': '[CLS] the man worked as a barber. [SEP]',\n  'score': 0.04962705448269844,\n  'token': 13362,\n  'token_str': 'barber'},\n {'sequence': '[CLS] the man worked as a mechanic. [SEP]',\n  'score': 0.03788609802722931,\n  'token': 15893,\n  'token_str': 'mechanic'},\n {'sequence': '[CLS] the man worked as a salesman. [SEP]',\n  'score': 0.037680890411138535,\n  'token': 18968,\n  'token_str': 'salesman'}]\n>>> unmasker(\"The woman worked as a [MASK].\")\n[{'sequence': '[CLS] the woman worked as a nurse. [SEP]',\n  'score': 0.21981462836265564,\n  'token': 6821,\n  'token_str': 'nurse'},\n {'sequence': '[CLS] the woman worked as a waitress. [SEP]',\n  'score': 0.1597415804862976,\n  'token': 13877,\n  'token_str': 'waitress'},\n {'sequence': '[CLS] the woman worked as a maid. [SEP]',\n  'score': 0.1154729500412941,\n  'token': 10850,\n  'token_str': 'maid'},\n {'sequence': '[CLS] the woman worked as a prostitute. [SEP]',\n  'score': 0.037968918681144714,\n  'token': 19215,\n  'token_str': 'prostitute'},\n {'sequence': '[CLS] the woman worked as a cook. [SEP]',\n  'score': 0.03042375110089779,\n  'token': 5660,\n  'token_str': 'cook'}]\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Bias in GPT-2 Text Generation\nDESCRIPTION: This example illustrates potential biases in the GPT-2 model by comparing text completions for prompts with different racial identifiers. It shows how the model can produce different types of professions or contexts based on racial descriptors.\nSOURCE: https://github.com/skops-dev/skops/blob/main/skops/card/tests/examples/gpt2.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n>>> from transformers import pipeline, set_seed\n>>> generator = pipeline('text-generation', model='gpt2')\n>>> set_seed(42)\n>>> generator(\"The White man worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The White man worked as a mannequin for'},\n {'generated_text': 'The White man worked as a maniser of the'},\n {'generated_text': 'The White man worked as a bus conductor by day'},\n {'generated_text': 'The White man worked as a plumber at the'},\n {'generated_text': 'The White man worked as a journalist. He had'}]\n\n>>> set_seed(42)\n>>> generator(\"The Black man worked as a\", max_length=10, num_return_sequences=5)\n\n[{'generated_text': 'The Black man worked as a man at a restaurant'},\n {'generated_text': 'The Black man worked as a car salesman in a'},\n {'generated_text': 'The Black man worked as a police sergeant at the'},\n {'generated_text': 'The Black man worked as a man-eating monster'},\n {'generated_text': 'The Black man worked as a slave, and was'}]\n```\n\n----------------------------------------\n\nTITLE: Updating Skops Files Using CLI\nDESCRIPTION: Command line example for updating skops files to the latest protocol version. The skops update command checks and updates files when necessary.\nSOURCE: https://github.com/skops-dev/skops/blob/main/docs/persistence.rst#2025-04-22_snippet_3\n\nLANGUAGE: console\nCODE:\n```\nskops update my_model.skops -o my_model-updated.skops\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for skops Documentation and Development\nDESCRIPTION: A requirements file specifying the necessary Python packages for the skops project, including visualization, data handling, and documentation generation tools. It sets minimum version requirements for packages like matplotlib, pandas, and Sphinx-related extensions, and installs the project itself with the 'rich' extra.\nSOURCE: https://github.com/skops-dev/skops/blob/main/docs/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# to be synced with the versions in pyproject.toml\nmatplotlib>=3.3\npandas>=1\nfairlearn>=0.7.0\nsphinx>=3.2.0\nsphinx-gallery>=0.7.0\nsphinx-rtd-theme>=1\nnumpydoc>=1.0.0\nsphinx-prompt>=1.3.0\nsphinx-issues>=1.2.0\n-e .[rich]\n```\n\n----------------------------------------\n\nTITLE: Listing Python Package Dependencies for skops Project\nDESCRIPTION: A requirements file that specifies the Python package dependencies needed to run the skops project. It includes machine learning libraries like scikit-learn, XGBoost, LightGBM, and CatBoost (with a version constraint for Python < 3.13). The file also includes huggingface_hub for model sharing and references the skops package itself from GitHub.\nSOURCE: https://github.com/skops-dev/skops/blob/main/spaces/skops_model_card_creator/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# remove python constraint when catboost supports 3.13\n# https://github.com/catboost/catboost/issues/2748\ncatboost; python_version < \"3.13\"\nhuggingface_hub\nlightgbm\npandas\nscikit-learn\nxgboost\ngit+https://github.com/skops-dev/skops.git\n```\n\n----------------------------------------\n\nTITLE: Installing SKOPS Library with pip\nDESCRIPTION: Command to install the skops Python library using pip. This is the recommended installation method for users who want to use the library for model persistence and model card creation.\nSOURCE: https://github.com/skops-dev/skops/blob/main/README.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install skops\n```\n\n----------------------------------------\n\nTITLE: Installing Skops using pip\nDESCRIPTION: This command installs the Skops package from PyPI using pip. It should be run in the user's Python environment.\nSOURCE: https://github.com/skops-dev/skops/blob/main/docs/installation.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install skops\n```\n\n----------------------------------------\n\nTITLE: Installing Skops using conda\nDESCRIPTION: This command installs the Skops package from conda-forge using conda. It provides an alternative installation method for users who prefer conda package management.\nSOURCE: https://github.com/skops-dev/skops/blob/main/docs/installation.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda install conda-forge::skops\n```\n\n----------------------------------------\n\nTITLE: Running Tests with Pixi in skops\nDESCRIPTION: Commands for running tests using the pixi environment manager. Pixi is recommended for CI and development workflows in the skops project.\nSOURCE: https://github.com/skops-dev/skops/blob/main/CONTRIBUTING.rst#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npixi run tests\n```\n\n----------------------------------------\n\nTITLE: Running Tests in Specific Environment with Pixi\nDESCRIPTION: Command for running tests in a specific environment using pixi. This allows selecting different test environments like ci-sklearn15.\nSOURCE: https://github.com/skops-dev/skops/blob/main/CONTRIBUTING.rst#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npixi run -e ci-sklearn15 tests\n```\n\n----------------------------------------\n\nTITLE: Setting up Pre-commit Hooks with Pixi\nDESCRIPTION: Command for setting up pre-commit hooks by running the linter once through pixi. This initializes the pre-commit configuration for the project.\nSOURCE: https://github.com/skops-dev/skops/blob/main/CONTRIBUTING.rst#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npixi run -e lint lint\n```\n\n----------------------------------------\n\nTITLE: Starting Interactive Shell with Pixi\nDESCRIPTION: Command for getting an interactive shell with the nightly build of scikit-learn and all required dependencies using pixi.\nSOURCE: https://github.com/skops-dev/skops/blob/main/CONTRIBUTING.rst#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npixi shell\n```\n\n----------------------------------------\n\nTITLE: Running pytest for skops\nDESCRIPTION: Command for running pytest from the project root directory to execute the test suite for skops.\nSOURCE: https://github.com/skops-dev/skops/blob/main/CONTRIBUTING.rst#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npytest\n```\n\n----------------------------------------\n\nTITLE: Searching for TODOs Before Release\nDESCRIPTION: Git grep command to search for TODO comments that need addressing before a release. This helps identify any incomplete tasks or deprecations.\nSOURCE: https://github.com/skops-dev/skops/blob/main/CONTRIBUTING.rst#2025-04-22_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngit grep -n TODO\n```\n\n----------------------------------------\n\nTITLE: BibTeX Entry for BERT Paper Citation\nDESCRIPTION: This BibTeX entry contains the citation information for the BERT paper, including authors, title, journal, volume, year, and various online identifiers. It can be used to properly cite the BERT paper in academic publications.\nSOURCE: https://github.com/skops-dev/skops/blob/main/skops/card/tests/examples/bert-base-uncased.md#2025-04-22_snippet_4\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{DBLP:journals/corr/abs-1810-04805,\n  author    = {Jacob Devlin and\n               Ming{-}Wei Chang and\n               Kenton Lee and\n               Kristina Toutanova},\n  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language\n               Understanding},\n  journal   = {CoRR},\n  volume    = {abs/1810.04805},\n  year      = {2018},\n  url       = {http://arxiv.org/abs/1810.04805},\n  archivePrefix = {arXiv},\n  eprint    = {1810.04805},\n  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},\n  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-04805.bib},\n  bibsource = {dblp computer science bibliography, https://dblp.org}\n}\n```\n\n----------------------------------------\n\nTITLE: Citation for Visual Transformers Paper in BibTeX Format\nDESCRIPTION: BibTeX entry for citing the Visual Transformers paper by Wu et al. (2020). This reference is necessary when using or referencing the Vision Transformer model in academic or research contexts.\nSOURCE: https://github.com/skops-dev/skops/blob/main/skops/card/tests/examples/vit-base-patch32-224-in21k.md#2025-04-22_snippet_1\n\nLANGUAGE: bibtex\nCODE:\n```\n@misc{wu2020visual,\n      title={Visual Transformers: Token-based Image Representation and Processing for Computer Vision},\n      author={Bichen Wu and Chenfeng Xu and Xiaoliang Dai and Alvin Wan and Peizhao Zhang and Zhicheng Yan and Masayoshi Tomizuka and Joseph Gonzalez and Kurt Keutzer and Peter Vajda},\n      year={2020},\n      eprint={2006.03677},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV}\n}\n```\n\n----------------------------------------\n\nTITLE: Citation for ImageNet Dataset in BibTeX Format\nDESCRIPTION: BibTeX entry for citing the ImageNet dataset paper by Deng et al. (2009). This reference is important when discussing the training data used for the Vision Transformer model.\nSOURCE: https://github.com/skops-dev/skops/blob/main/skops/card/tests/examples/vit-base-patch32-224-in21k.md#2025-04-22_snippet_2\n\nLANGUAGE: bibtex\nCODE:\n```\n@inproceedings{deng2009imagenet,\n  title={Imagenet: A large-scale hierarchical image database},\n  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},\n  booktitle={2009 IEEE conference on computer vision and pattern recognition},\n  pages={248--255},\n  year={2009},\n  organization={Ieee}\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Streamlit App Configuration\nDESCRIPTION: YAML configuration for the Streamlit application that defines the app's appearance, SDK version, license and tags.\nSOURCE: https://github.com/skops-dev/skops/blob/main/spaces/skops_model_card_creator/README.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ntitle: Skops Model Card Creator\nemoji: ðŸ¨\ncolorFrom: indigo\ncolorTo: blue\nsdk: streamlit\nsdk_version: 1.17.0\napp_file: app.py\npinned: false\nlicense: bsd-3-clause\ntags:\n  - sklearn\n  - skops\n  - model card\n```\n\n----------------------------------------\n\nTITLE: Version Support Matrix in Markdown\nDESCRIPTION: Markdown table showing supported and unsupported versions of skops. Version 0.11 is currently supported while all versions below 0.11 are not supported.\nSOURCE: https://github.com/skops-dev/skops/blob/main/SECURITY.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Version       | Supported          |\n| ------------- | ------------------ |\n| 0.11          | :white_check_mark: |\n| < 0.11        | :x:                |\n```\n\n----------------------------------------\n\nTITLE: Citation BibTeX Template\nDESCRIPTION: A placeholder for the BibTeX citation information for the model, which allows users to properly cite the model in academic or professional contexts.\nSOURCE: https://github.com/skops-dev/skops/blob/main/skops/card/default_template.md#2025-04-22_snippet_0\n\nLANGUAGE: bibtex\nCODE:\n```\n{{ citation_bibtex | default(\"[More Information Needed]\", true)}}\n```\n\n----------------------------------------\n\nTITLE: Defining Hugging Face Spaces Project Structure\nDESCRIPTION: Markdown header and description defining the purpose of the directory for Hugging Face Spaces related code and scripts.\nSOURCE: https://github.com/skops-dev/skops/blob/main/spaces/README.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Hugging Face Spaces\\n\\nCode and script for creating Hugging Face Spaces go here.\n```\n\n----------------------------------------\n\nTITLE: Defining raw-html role in reStructuredText\nDESCRIPTION: Defines a custom role called 'raw-html' that allows embedding raw HTML content within reStructuredText documents. This role specifies that the content should be formatted as HTML.\nSOURCE: https://github.com/skops-dev/skops/blob/main/docs/_authors.rst#2025-04-22_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. role:: raw-html(raw)\n   :format: html\n```\n\n----------------------------------------\n\nTITLE: Setting up hyperlinks to contributor GitHub profiles in reStructuredText\nDESCRIPTION: Creates named external hyperlinks to GitHub profiles of four contributors to the skops project. These link references can be used throughout the document to create clickable links to these profiles.\nSOURCE: https://github.com/skops-dev/skops/blob/main/docs/_authors.rst#2025-04-22_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. _Adrin Jalali: https://github.com/adrinjalali\n\n.. _Benjamin Bossan: https://github.com/BenjaminBossan\n\n.. _Merve Noyan: https://github.com/merveenoyan\n\n.. _Erin Aho: https://github.com/E-Aho\n```\n\n----------------------------------------\n\nTITLE: RST Include Directive\nDESCRIPTION: Include directive to import authors list from external file\nSOURCE: https://github.com/skops-dev/skops/blob/main/docs/changes.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: _authors.rst\n```\n\n----------------------------------------\n\nTITLE: RST Section Definition\nDESCRIPTION: RST markup defining the changelog section and table of contents\nSOURCE: https://github.com/skops-dev/skops/blob/main/docs/changes.rst#2025-04-22_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. contents:: Table of Contents\n    :depth: 1\n    :local:\n```\n\n----------------------------------------\n\nTITLE: Referencing Example Files in RST\nDESCRIPTION: RST syntax for referencing example files that demonstrate skops functionality with Hugging Face Hub, including model card creation, tabular regression, text classification, and a practical housing dataset exercise.\nSOURCE: https://github.com/skops-dev/skops/blob/main/docs/examples.rst#2025-04-22_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n:ref:`sphx_glr_auto_examples_plot_model_card.py`\n:ref:`sphx_glr_auto_examples_plot_tabular_regression.py`\n:ref:`sphx_glr_auto_examples_plot_text_classification.py`\n:ref:`sphx_glr_auto_examples_plot_california_housing.py`\n```\n\n----------------------------------------\n\nTITLE: Basic Python Functions Example in Markdown Code Block\nDESCRIPTION: A simple Python code block demonstrating two basic functions: foo() which returns 0 and bar() which returns 1. This illustrates syntax highlighting in markdown code blocks with language specification.\nSOURCE: https://github.com/skops-dev/skops/blob/main/skops/card/tests/examples/toy-example.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef foo():\n  return 0\n  \ndef bar():\n  return 1\n```"
  }
]
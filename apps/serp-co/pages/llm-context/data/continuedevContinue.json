[
  {
    "owner": "continuedev",
    "repo": "continue",
    "content": "TITLE: Configuring Continue Dependency in package.json\nDESCRIPTION: This snippet shows how to add Continue as an extension dependency in the package.json file of a VSCode extension.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/tutorials/build-your-own-context-provider.mdx#2025-04-19_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"extensionDependencies\": [\"continue.continue\"]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Models in Continue Assistant YAML Configuration\nDESCRIPTION: This snippet demonstrates how to define models in a Continue assistant configuration, including importing a hub model and defining a local Ollama model.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/reference.md#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - uses: anthropic/claude-3.5-sonnet # an imported model block\n  - model: deepseek-reasoner # an explicit model block\n    provider: ollama\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Slash Command Using TypeScript Function\nDESCRIPTION: Example of creating a custom 'commit' slash command in config.ts that generates commit messages based on git diff. This implementation uses the ContinueSDK to access IDE functionality and the language model to generate commit messages in the imperative mood.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/tutorials/build-your-own-slash-command.md#2025-04-19_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nexport function modifyConfig(config: Config): Config {\n  config.slashCommands?.push({\n    name: \"commit\",\n    description: \"Write a commit message\",\n    run: async function* (sdk) {\n      // The getDiff function takes a boolean parameter that indicates whether\n      // to include unstaged changes in the diff or not.\n      const diff = await sdk.ide.getDiff(false); // Pass false to exclude unstaged changes\n      for await (const message of sdk.llm.streamComplete(\n        `${diff}\\n\\nWrite a commit message for the above changes. Use no more than 20 tokens to give a brief description in the imperative mood (e.g. 'Add feature' not 'Added feature'):`,\n        new AbortController().signal,\n        {\n          maxTokens: 20,\n        },\n      )) {\n        yield message;\n      }\n    },\n  });\n  return config;\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Continue with Advanced Features (JSON)\nDESCRIPTION: JSON configuration for Continue, specifying context providers, documentation sources, reranker settings, and embeddings provider. It includes settings for both public and private documentation, and advanced features like custom reranking and embeddings models.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/deep-dives/docs.mdx#2025-04-19_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"docs\",\n      \"params\": {\n        \"githubToken\": \"github_...\",\n        \"nRetrieve\": 25,\n        \"nFinal\": 5,\n        \"useReranking\": true\n      }\n    }\n  ],\n  \"docs\": [\n    {\n      \"title\": \"Nest.js\",\n      \"startUrl\": \"https://docs.nestjs.com/\"\n    },\n    {\n      \"title\": \"My Private Docs\",\n      \"startUrl\": \"http://10.2.1.2/docs\",\n      \"faviconUrl\": \"http://10.2.1.2/docs/assets/favicon.ico\",\n      \"maxDepth\": 4,\n      \"useLocalCrawling\": true\n    }\n  ],\n  \"reranker\": {\n    \"name\": \"voyage\",\n    \"params\": {\n      \"model\": \"rerank-2\",\n      \"apiKey\": \"<YOUR_VOYAGE_API_KEY>\"\n    }\n  },\n  \"embeddingsProvider\": {\n    \"provider\": \"lmstudio\",\n    \"model\": \"nomic-ai/nomic-embed-text-v1.5-GGUF\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Continue with Advanced Features (YAML)\nDESCRIPTION: YAML configuration for Continue, including custom models, embeddings provider, reranker, and documentation sources. It sets up both public and private documentation crawling, and includes GitHub token for extended functionality.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/deep-dives/docs.mdx#2025-04-19_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: LMStudio Nomic Text\n    provider: lmstudio\n    model: nomic-ai/nomic-embed-text-v1.5-GGUF\n    roles:\n      - embed\n  - name: Voyage Rerank-2\n    provider: voyage\n    apiKey: <VOYAGE_API_KEY>\n    model: rerank-2\n    roles:\n      - rerank\ncontext:\n  - provider: docs\n    params:\n       githubToken: <GITHUB_TOKEN>\n       nRetrieve: 25\n       nFinal: 5\n       useReranking: true\ndocs:\n  - title: Nest.js\n    startUrl: https://docs.nestjs.com/\n  - title: My Private Docs\n    startUrl: http://10.2.1.2/docs\n    faviconUrl: http://10.2.1.2/docs/assets/favicon.ico\n    maxDepth: 4\n    useLocalCrawling: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Ollama Model with API Key in YAML\nDESCRIPTION: Example YAML configuration for setting up an Ollama model with basic API key authentication in Continue's config.yaml file.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/tutorials/how-to-self-host-a-model.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Ollama\n    provider: ollama\n    model: llama2-7b\n    apiKey: <YOUR_CUSTOM_OLLAMA_SERVER_API_KEY>\n```\n\n----------------------------------------\n\nTITLE: Configuring Codebase Context Provider in JSON\nDESCRIPTION: JSON configuration example for the codebase context provider with identical functionality to the YAML version, showing how to set parameters to control result retrieval and reranking.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/deep-dives/codebase.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"codebase\",\n      \"params\": {\n        \"nRetrieve\": 25,\n        \"nFinal\": 5,\n        \"useReranking\": true\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Context Provider for Continue in TypeScript\nDESCRIPTION: This code snippet shows a complete implementation of a custom context provider for Continue. It includes the necessary imports, the implementation of the IContextProvider interface, and the registration of the provider using the Continue API.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/tutorials/build-your-own-context-provider.mdx#2025-04-19_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nimport * as vscode from \"vscode\";\nimport {\n  IContextProvider,\n  ContextProviderDescription,\n  ContextProviderExtras,\n  ContextItem,\n  LoadSubmenuItemsArgs,\n  ContextSubmenuItem,\n} from \"@continuedev/core\";\n\nclass MyCustomProvider implements IContextProvider {\n  get description(): ContextProviderDescription {\n    return {\n      title: \"Custom\",\n      displayTitle: \"Custom\",\n      description: \"my custom context provider\",\n      type: \"normal\",\n    };\n  }\n\n  async getContextItems(\n    query: string,\n    extras: ContextProviderExtras,\n  ): Promise<ContextItem[]> {\n    return [\n      {\n        name: \"Custom\",\n        description: \"Custom description\",\n        content: \"Custom content\",\n      },\n    ];\n  }\n\n  async loadSubmenuItems(\n    args: LoadSubmenuItemsArgs,\n  ): Promise<ContextSubmenuItem[]> {\n    return [];\n  }\n}\n\n// create an instance of your custom provider\nconst customProvider = new MyCustomProvider();\n\n// get Continue extension using vscode API\nconst continueExt = vscode.extensions.getExtension(\"Continue.continue\");\n\n// get the API from the extension\nconst continueApi = continueExt?.exports;\n\n// register your custom provider\ncontinueApi?.registerCustomContextProvider(customProvider);\n```\n\n----------------------------------------\n\nTITLE: Configuring Qwen2.5-Coder 1.5B as Autocomplete Model in YAML\nDESCRIPTION: This YAML configuration sets up Qwen2.5-Coder 1.5B as the autocomplete model using Ollama provider. It specifies the model name, provider, model, and assigns it the autocomplete role.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/ollama.mdx#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Qwen2.5-Coder 1.5B\n    provider: ollama\n    model: qwen2.5-coder:1.5b-base\n    roles:\n      - autocomplete\n```\n\n----------------------------------------\n\nTITLE: FastAPI Server Implementation for Continue Context Provider\nDESCRIPTION: Reference implementation of a FastAPI server that handles requests from Continue's HTTP context provider, processes queries, and returns results in the expected format.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/tutorials/custom-code-rag.mdx#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n\"\"\"\nThis is an example of a server that can be used with the \\\"http\\\" context provider.\n\"\"\"\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\n\nclass ContextProviderInput(BaseModel):\n    query: str\n    fullInput: str\n\n\napp = FastAPI()\n\n\n@app.post(\"/retrieve\")\nasync def create_item(item: ContextProviderInput):\n    results = [] # TODO: Query your vector database here.\n\n    # Construct the \\\"context item\\\" format expected by Continue\n    context_items = []\n    for result in results:\n        context_items.append({\n            \"name\": result.filename,\n            \"description\": result.filename,\n            \"content\": result.text,\n        })\n\n    return context_items\n```\n\n----------------------------------------\n\nTITLE: Configuring Claude 3.5 Sonnet V2 Model in YAML\nDESCRIPTION: YAML configuration for setting up Claude 3.5 Sonnet V2 using the regional model ID format to resolve throughput errors. Uses the us-prefixed model ID to access the model.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/bedrock.mdx#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Claude 3.5 Sonnet\n    provider: bedrock\n    model: us.anthropic.claude-3-5-sonnet-20241022-v2:0\n    env:\n      region: us-east-1\n      profile: bedrock\n    roles:\n      - chat\n```\n\n----------------------------------------\n\nTITLE: Configuring Models in Continue JSON Configuration\nDESCRIPTION: This snippet demonstrates how to configure models in the Continue config.json file. It shows how to set up multiple models with different providers, including Ollama and OpenAI, specifying various properties such as context length and API keys.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/json-reference.md#2025-04-19_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Ollama\",\n      \"provider\": \"ollama\",\n      \"model\": \"AUTODETECT\"\n    },\n    {\n      \"model\": \"gpt-4o\",\n      \"contextLength\": 128000,\n      \"title\": \"GPT-4o\",\n      \"provider\": \"openai\",\n      \"apiKey\": \"YOUR_API_KEY\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Vector Database with LanceDB and Voyage AI\nDESCRIPTION: Example of setting up LanceDB with Voyage AI embeddings for code chunks storage and retrieval. Demonstrates model configuration, schema definition, data insertion, and basic search functionality.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/tutorials/custom-code-rag.mdx#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom lancedb.pydantic import LanceModel, Vector\nfrom lancedb.embeddings import get_registry\n\ndb = lancedb.connect(\"/tmp/db\")\nfunc = get_registry().get(\"openai\").create(\n    name=\"voyage-code-3\",\n    base_url=\"https://api.voyageai.com/v1/\",\n    api_key=os.environ[\"VOYAGE_API_KEY\"],\n)\n\nclass CodeChunks(LanceModel):\n    filename: str\n    text: str = func.SourceField()\n    # 1024 is the default dimension for `voyage-code-3`: https://docs.voyageai.com/docs/embeddings#model-choices\n    vector: Vector(1024) = func.VectorField()\n\ntable = db.create_table(\"code_chunks\", schema=CodeChunks, mode=\"overwrite\")\ntable.add([\n    {\"text\": \"print('hello world!')\", filename: \"hello.py\"},\n    {\"text\": \"print('goodbye world!')\", filename: \"goodbye.py\"}\n])\n\nquery = \"greetings\"\nactual = table.search(query).limit(1).to_pydantic(CodeChunks)[0]\nprint(actual.text)\n```\n\n----------------------------------------\n\nTITLE: Configuring Claude 3.5 Sonnet Chat Model in YAML\nDESCRIPTION: YAML configuration for setting up Claude 3.5 Sonnet as a chat model in Continue.dev. Requires specifying the model name, provider, model ID, and environment variables for Google Cloud project ID and region.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/vertexai.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Claude 3.5 Sonnet\n    provider: vertexai\n    model: claude-3-5-sonnet-20240620\n    env:\n      projectId: <PROJECT_ID>\n      region: us-east5\n```\n\n----------------------------------------\n\nTITLE: Configuring GPT-4o Chat Model in YAML\nDESCRIPTION: YAML configuration for setting up GPT-4o as the chat model in Continue.dev. Requires an OpenAI API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/openai.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: GPT-4o\n    provider: openai\n    model: gpt-4o\n    apiKey: <YOUR_OPENAI_API_KEY>\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI Embeddings Model in JSON\nDESCRIPTION: JSON configuration for setting up text-embedding-3-large as the embeddings model in Continue.dev.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/openai.mdx#2025-04-19_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"embeddingsProvider\": {\n    \"provider\": \"openai\",\n    \"model\": \"text-embedding-3-large\", \n    \"apiKey\": \"<YOUR_OPENAI_API_KEY>\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Claude 3.5 Sonnet from Anthropic in JSON\nDESCRIPTION: JSON configuration for using Claude 3.5 Sonnet from Anthropic as a chat model in Continue. Requires an Anthropic API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/chat.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Claude 3.5 Sonnet\",\n      \"provider\": \"anthropic\",\n      \"model\": \"claude-3-5-sonnet-latest\",\n      \"apiKey\": \"<YOUR_ANTHROPIC_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Using .continuerc.json for Workspace-Level Configuration\nDESCRIPTION: Example of a .continuerc.json file that disables tab autocomplete options and uses the \"overwrite\" merge behavior to replace properties from the main config.json file.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/deep-dives/configuration.md#2025-04-19_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"tabAutocompleteOptions\": {\n    \"disable\": true\n  },\n  \"mergeBehavior\": \"overwrite\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Models in Continue config.json\nDESCRIPTION: Example configuration for defining chat models used in Continue. Shows setup for both Ollama and OpenAI GPT-4 models with their respective settings.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/i18n/zh-CN/docusaurus-plugin-content-docs/current/reference.md#2025-04-19_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Ollama\",\n      \"provider\": \"ollama\",\n      \"model\": \"AUTODETECT\"\n    },\n    {\n      \"model\": \"gpt-4o\",\n      \"contextLength\": 128000,\n      \"title\": \"GPT-4o\",\n      \"provider\": \"openai\",\n      \"apiKey\": \"YOUR_API_KEY\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Groq API for Llama 3.3 70b Versatile in YAML\nDESCRIPTION: This YAML configuration snippet sets up the Llama 3.3 70b Versatile model using the Groq provider. It requires a Groq API key and specifies the model's roles.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/groq.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Llama 3.3 70b Versatile\n    provider: groq\n    model: llama-3.3-70b-versatile\n    apiKey: <YOUR_GROQ_API_KEY>\n    roles:\n      - chat\n```\n\n----------------------------------------\n\nTITLE: Configuring Models in YAML Format for Continue\nDESCRIPTION: Demonstrates the YAML configuration for various models including GPT-4, rerankers, embedders, and custom OpenAI-compatible models.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/yaml-migration.md#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: GPT-4\n    provider: openai\n    model: gpt-4\n    apiKey: <YOUR_OPENAI_KEY>\n    defaultCompletionOptions:\n      temperature: 0.5\n      maxTokens: 2000\n    roles:\n      - chat\n      - edit\n\n  - name: My Voyage Reranker\n    provider: voyage\n    apiKey: <YOUR_VOYAGE_KEY>\n    roles:\n      - rerank\n\n  - name: My Starcoder\n    provider: ollama\n    model: starcoder2:3b\n    roles:\n      - autocomplete\n\n  - name: My Ada Embedder\n    provider: openai\n    apiKey: <YOUR_ADA_API_KEY>\n    roles:\n      - embed\n    embedOptions:\n      - maxChunkSize: 256\n      - maxBatchSize: 5\n\n  - name: Ollama Autodetect\n    provider: ollama\n    model: AUTODETECT\n\n  - name: My Open AI Compatible Model - Apply\n    provider: openai\n    model: my-openai-compatible-model\n    apiBase: http://3.3.3.3/v1\n    requestOptions:\n      headers:\n        X-Auth-Token: <MY_API_KEY>\n    roles:\n      - chat\n      - apply\n```\n\n----------------------------------------\n\nTITLE: Configuring watsonx Chat Model in YAML\nDESCRIPTION: YAML configuration for setting up a watsonx chat model in Continue. It includes essential parameters such as model name, provider, API base, API key, and environment variables.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/watsonx.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: watsonx - Model Name\n    provider: watsonx\n    model: model ID\n    apiBase: https://us-south.ml.cloud.ibm.com\n    apiKey: API_KEY/ZENAPI_KEY/USERNAME:PASSWORD\n    env:\n      projectId: PROJECT_ID\n      apiVersion: 2024-03-14\n```\n\n----------------------------------------\n\nTITLE: Configuring Gemini 2.0 Flash Chat Model\nDESCRIPTION: Configuration setup for using Gemini 2.0 Flash as a chat model. Requires a Gemini API key from Google AI Studio.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/gemini.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Gemini 2.0 Flash\n    provider: gemini\n    model: gemini-2.0-flash\n    apiKey: <YOUR_GEMINI_API_KEY>\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Gemini 2.0 Flash\", \n      \"provider\": \"gemini\",\n      \"model\": \"gemini-2.0-flash\",\n      \"apiKey\": \"<YOUR_GEMINI_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring watsonx Model with Advanced Options in YAML\nDESCRIPTION: YAML configuration for a watsonx model with advanced options such as template, context length, and various generation parameters like temperature, topP, and stop sequences.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/watsonx.mdx#2025-04-19_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Granite Code 20b\n    provider: watsonx\n    model: ibm/granite-20b-code-instruct\n    apiBase: watsonx endpoint e.g. https://us-south.ml.cloud.ibm.com\n    apiKey: API_KEY/ZENAPI_KEY/USERNAME:PASSWORD\n    template: granite\n    defaultCompletionOptions:\n      contextLength: 8000\n      temperature: 0.1\n      topP: 0.3\n      topK: 20\n      maxTokens: 2000\n      frequencyPenalty: 1.1\n      stop:\n        - Question:\n        - \"\\n\\n\\n\"\n    env:\n      projectId: PROJECT_ID\n      apiVersion: 2024-03-14\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using Continue.dev API Client in Python\nDESCRIPTION: Example code demonstrating how to initialize and use the Continue.dev API client to list available IDE assistants. Shows configuration setup, authentication with Bearer token, and handling the API response with proper error handling.\nSOURCE: https://github.com/continuedev/continue/blob/main/packages/continue-sdk/clients/python/docs/DefaultApi.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport openapi_client\nfrom openapi_client.models.list_assistants200_response_inner import ListAssistants200ResponseInner\nfrom openapi_client.rest import ApiException\nfrom pprint import pprint\n\n# Defining the host is optional and defaults to https://api.continue.dev\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = openapi_client.Configuration(\n    host = \"https://api.continue.dev\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure Bearer authorization: apiKeyAuth\nconfiguration = openapi_client.Configuration(\n    access_token = os.environ[\"BEARER_TOKEN\"]\n)\n\n# Enter a context with an instance of the API client\nwith openapi_client.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = openapi_client.DefaultApi(api_client)\n    always_use_proxy = 'always_use_proxy_example' # str | Whether to always use the Continue-managed proxy for model requests (optional)\n    organization_id = 'organization_id_example' # str | ID of the organization to scope assistants to. If not provided, personal assistants are returned. (optional)\n\n    try:\n        # List assistants for IDE\n        api_response = api_instance.list_assistants(always_use_proxy=always_use_proxy, organization_id=organization_id)\n        print(\"The response of DefaultApi->list_assistants:\\n\")\n        pprint(api_response)\n    except Exception as e:\n        print(\"Exception when calling DefaultApi->list_assistants: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Creating a Security Review Prompt in Continue.dev\nDESCRIPTION: A prompt that instructs the model to review files for security best practices, including architecture principles, vulnerability assessment, and sensitive data handling.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/deep-dives/prompts.md#2025-04-19_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n@open - Review these files for the following security best practices:\n- Does the architecture follow security-by-design principles?\n- Are there potential security vulnerabilities in the system design?\n- Is sensitive data handled appropriately throughout the lifecycle?\n```\n\n----------------------------------------\n\nTITLE: Configuring GPT-4o Chat Model in JSON\nDESCRIPTION: JSON configuration for setting up GPT-4o as the chat model in Continue.dev. Requires an OpenAI API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/openai.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"GPT-4o\",\n      \"provider\": \"openai\", \n      \"model\": \"gpt-4o\",\n      \"apiKey\": \"<YOUR_OPENAI_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Claude 3.5 Sonnet Chat Model in YAML\nDESCRIPTION: YAML configuration example for setting up Claude 3.5 Sonnet as a chat model in Continue. Requires your Anthropic API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/anthropic.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Claude 3.5 Sonnet\n    provider: anthropic\n    model: claude-3-5-sonnet-latest\n    apiKey: <YOUR_ANTHROPIC_API_KEY>\n```\n\n----------------------------------------\n\nTITLE: Configuring SambaNova Cloud with YAML\nDESCRIPTION: YAML configuration for setting up SambaNova Qwen2.5 coder model access. Requires a valid SambaNova API key and specifies the model name, provider, and model version.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/SambaNova.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: SambaNova Qwen2.5 coder\n    provider: sambanova\n    model: qwen2.5-coder-32b\n    apiKey: <YOUR_SAMBANOVA_API_KEY>\n```\n\n----------------------------------------\n\nTITLE: Configuring Llama3.1 8B as Chat Model in YAML\nDESCRIPTION: This YAML configuration sets up Llama3.1 8B as the chat model using Ollama provider. It specifies the model name, provider, and the specific model to use.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/ollama.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Llama3.1 8B\n    provider: ollama\n    model: llama3.1:8b\n```\n\n----------------------------------------\n\nTITLE: Configuring Claude 3.5 Sonnet Chat Model in YAML\nDESCRIPTION: YAML configuration for setting up Claude 3.5 Sonnet as a chat model in Continue. Specifies the model provider, model ID, AWS region, and profile for authentication.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/bedrock.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Claude 3.5 Sonnet\n    provider: bedrock\n    model: anthropic.claude-3-5-sonnet-20240620-v1:0\n    env:\n      region: us-east-1\n      profile: bedrock\n    roles:\n      - chat\n```\n\n----------------------------------------\n\nTITLE: Configuring Open Files Context Provider in JSON\nDESCRIPTION: JSON configuration example for adding the @Open context provider with a parameter to only include pinned files.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"open\",\n      \"params\": {\n        \"onlyPinned\": true\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Claude 3.7 Sonnet from Anthropic in YAML\nDESCRIPTION: YAML configuration for using Claude 3.7 Sonnet from Anthropic as a chat model in Continue. Requires an Anthropic API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/chat.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Claude 3.7 Sonnet\n    provider: anthropic\n    model: claude-3-7-sonnet-latest\n    apiKey: <YOUR_ANTHROPIC_API_KEY>\n```\n\n----------------------------------------\n\nTITLE: Enabling Claude Prompt Caching in YAML Configuration\nDESCRIPTION: YAML configuration example showing how to enable Anthropic prompt caching for system messages and conversation history. This optimizes performance by caching these components.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/anthropic.mdx#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Anthropic\n    provider: anthropic\n    model: claude-3-5-sonnet-latest\n    apiKey: <YOUR_ANTHROPIC_API_KEY>\n    roles:\n      - chat\n    cacheBehavior:\n      cacheSystemMessage: true\n      cacheConversation: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Together AI for Llama 3.1\nDESCRIPTION: Configuration settings for using Llama 3.1 405b model with Together AI provider in Continue. Requires Together AI account and API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/tutorials/llama3.1.mdx#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Llama 3.1 405b\n    provider: together\n    model: llama3.1-405b\n    apiKey: <YOUR_TOGETHER_API_KEY>\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"models\": [\n     {\n       \"title\": \"Llama 3.1 405b\",\n       \"provider\": \"together\",\n       \"model\": \"llama3.1-405b\",\n       \"apiKey\": \"<YOUR_TOGETHER_API_KEY>\"\n     }\n   ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Continue to use remote IPEX-LLM Ollama backend in JSON\nDESCRIPTION: JSON configuration example for setting up Continue to connect to a remote IPEX-LLM accelerated Ollama provider. This includes the apiBase parameter to specify the remote service IP address and port.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/ipex_llm.mdx#2025-04-19_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"IPEX-LLM\",\n      \"provider\": \"ollama\",\n      \"model\": \"AUTODETECT\",\n      \"apiBase\": \"http://your-ollama-service-ip:11434\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Imported Bedrock Models in JSON\nDESCRIPTION: JSON configuration for using custom imported models through Bedrock. Includes the model ARN, AWS region, and profile for authentication.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/bedrock.mdx#2025-04-19_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"AWS Bedrock deepseek-coder-6.7b-instruct\",\n      \"provider\": \"bedrockimport\",\n      \"model\": \"deepseek-coder-6.7b-instruct\",\n      \"modelArn\": \"arn:aws:bedrock:us-west-2:XXXXX:imported-model/XXXXXX\", \n      \"region\": \"us-west-2\",\n      \"profile\": \"bedrock\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Qwen2.5-Coder-32B-Instruct as Chat Model in JSON\nDESCRIPTION: JSON configuration for setting up Qwen/Qwen2.5-Coder-32B-Instruct as a chat model in Continue. Requires a SiliconFlow API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/siliconflow.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Qwen\",\n      \"provider\": \"siliconflow\",\n      \"model\": \"Qwen/Qwen2.5-Coder-32B-Instruct\",\n      \"apiKey\": \"<YOUR_SILICONFLOW_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining React Best Practices in .continuerules\nDESCRIPTION: Example of a .continuerules file that specifies best practices to follow when writing React code, including component structure, state management, and styling approaches.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/deep-dives/rules.md#2025-04-19_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nWhenever you are writing React code, make sure to\n- use functional components instead of class components\n- use hooks for state management\n- define an interface for your component props\n- use Tailwind CSS for styling\n- modularize components into smaller, reusable pieces\n```\n\n----------------------------------------\n\nTITLE: Customizing Apply Prompt Template with Handlebars in YAML Configuration\nDESCRIPTION: Example configuration that customizes how the 'apply' model formats prompts. This template uses Anthropic's Claude 3.5 Sonnet model and provides a simplified format for comparing original and new code versions, instructing the model to generate clean output without markers or explanations.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/apply.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: My Custom Apply Template\n    provider: anthropic\n    model: claude-3-5-sonnet-latest\n    promptTemplates:\n      apply: |\n        Original: {{{original_code}}}\n        New: {{{new_code}}}\n\n        Please generate the final code without any markers or explanations.\n```\n\n----------------------------------------\n\nTITLE: Setting up AWS Credentials for Bedrock\nDESCRIPTION: AWS credentials configuration for authenticating with Bedrock services. Shows how to set up access key, secret key, and optional session token in the credentials file.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/bedrock.mdx#2025-04-19_snippet_8\n\nLANGUAGE: text\nCODE:\n```\n[bedrock]\naws_access_key_id = abcdefg\naws_secret_access_key = hijklmno\naws_session_token = pqrstuvwxyz # Optional: means short term creds.\n```\n\n----------------------------------------\n\nTITLE: Configuring Claude 3.5 Sonnet Chat Model in JSON\nDESCRIPTION: JSON configuration for setting up Claude 3.5 Sonnet as a chat model in Continue.dev. Requires specifying the model title, provider, model ID, project ID, and region.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/vertexai.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Claude 3.5 Sonnet\",\n      \"provider\": \"vertexai\",\n      \"model\": \"claude-3-5-sonnet-20240620\",\n      \"projectId\": \"[PROJECT_ID]\",\n      \"region\": \"us-east5\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Codebase Context Provider in YAML\nDESCRIPTION: YAML configuration example for the codebase context provider, specifying retrieval parameters such as nRetrieve, nFinal, and useReranking to control how many results are initially retrieved and ultimately used.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/deep-dives/codebase.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: codebase\n    params:\n      nRetrieve: 25\n      nFinal: 5\n      useReranking: true\n```\n\n----------------------------------------\n\nTITLE: Configuring Embeddings for LM Studio in JSON\nDESCRIPTION: JSON configuration for setting up a Nomic Embed Text embeddings model through LM Studio. This configuration uses the 'embeddingsProvider' property to specify the model for embedding operations.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/lmstudio.mdx#2025-04-19_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"embeddingsProvider\": {\n    \"provider\": \"lmstudio\",\n    \"model\": \"nomic-ai/nomic-embed-text-v1.5-GGUF\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS SageMaker Models in YAML\nDESCRIPTION: YAML configuration for setting up both chat and embedding models using AWS SageMaker. The example configures a deepseek-coder model for chat functionality and an mxbai-embed model for embedding functionality, with appropriate region and role settings.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/sagemaker.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: deepseek-6.7b-instruct\n    provider: sagemaker\n    model: lmi-model-deepseek-coder-xxxxxxx\n    region: us-west-2\n    roles:\n      - chat\n  - name: mxbai-embed\n    provider: sagemaker\n    model: mxbai-embed-large-v1-endpoint\n    roles:\n      - embed\n```\n\n----------------------------------------\n\nTITLE: Configuring Text Embedding-3 Large Model in Azure AI Foundry\nDESCRIPTION: Configuration settings for implementing text-embedding-3-large as an embeddings model. Includes provider settings, model specification, and API configuration.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/azure.mdx#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Text Embedding-3 Large\n    provider: azure\n    model: text-embedding-3-large\n    apiBase: <YOUR_DEPLOYMENT_BASE>\n    apiKey: <YOUR_AZURE_API_KEY>\n    env:\n      deployment: <YOUR_DEPLOYMENT_NAME>\n      apiType: azure\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"embeddingsProvider\": {\n    \"provider\": \"azure\",\n    \"model\": \"text-embedding-3-large\",\n    \"apiBase\": \"<YOUR_DEPLOYMENT_BASE>\",\n    \"deployment\": \"<YOUR_DEPLOYMENT_NAME>\",\n    \"apiKey\": \"<YOUR_AZURE_API_KEY>\",\n    \"apiType\": \"azure-foundry\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: CustomContextProvider Interface Definition\nDESCRIPTION: TypeScript interface definition for implementing custom context providers with required methods and properties.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/tutorials/build-your-own-context-provider.mdx#2025-04-19_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\ninterface CustomContextProvider {\n  title: string;\n  displayTitle?: string;\n  description?: string;\n  renderInlineAs?: string;\n  type?: ContextProviderType;\n  getContextItems(\n    query: string,\n    extras: ContextProviderExtras,\n  ): Promise<ContextItem[]>;\n  loadSubmenuItems?: (\n    args: LoadSubmenuItemsArgs,\n  ) => Promise<ContextSubmenuItem[]>;\n}\n```\n\n----------------------------------------\n\nTITLE: Importing and Configuring a Custom Model in Continue Assistant YAML\nDESCRIPTION: This snippet demonstrates how to import a custom model block into an assistant configuration, passing API keys and overriding specific properties.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/reference.md#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nname: myprofile/custom-assistant\nmodels:\n  - uses: myprofile/custom-model\n    with:\n      ANTHROPIC_API_KEY: ${{ secrets.MY_ANTHROPIC_API_KEY }}\n      TEMP: 0.9\n    override:\n      roles:\n        - chat\n```\n\n----------------------------------------\n\nTITLE: Configuring Codestral in YAML for Continue\nDESCRIPTION: This YAML configuration snippet sets up Codestral as a model for both autocomplete and chat functionalities in Continue. It specifies the model name, provider, API key, and base URL.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/tutorials/set-up-codestral.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Codestral\n    provider: mistral\n    model: codestral-latest\n    apiKey: <YOUR_CODESTRAL_API_KEY>\n    apiBase: https://codestral.mistral.ai/v1\n    roles:\n      - chat\n      - autocomplete\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Models in Continue Assistant YAML\nDESCRIPTION: This example shows how to configure multiple models with different providers, roles, and completion options in a Continue assistant configuration.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/reference.md#2025-04-19_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: GPT-4o\n    provider: openai\n    model: gpt-4o\n    roles:\n      - chat\n      - edit\n      - apply\n    defaultCompletionOptions:\n      temperature: 0.7\n      maxTokens: 1500\n\n  - name: Codestral\n    provider: mistral\n    model: codestral-latest\n    roles:\n      - autocomplete\n\n  - name: My Model - OpenAI-Compatible\n    provider: openai\n    apiBase: http://my-endpoint/v1\n    model: my-custom-model\n    capabilities:\n      - tool_use\n      - image_input\n    roles:\n      - chat\n      - edit\n```\n\n----------------------------------------\n\nTITLE: Configuring NVIDIA Retrieval QA Mistral 7B Embeddings Model in YAML\nDESCRIPTION: YAML configuration for setting up the NVIDIA Retrieval QA Mistral 7B model as an embeddings model in Continue. Requires a valid NVIDIA API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/nvidia.mdx#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Nvidia Embedder\n    provider: nvidia\n    model: nvidia/nv-embedqa-mistral-7b-v2\n    apiKey: <YOUR_NVIDIA_API_KEY>\n    roles:\n      - embed\n```\n\n----------------------------------------\n\nTITLE: Configuring Codestral Autocomplete Model in YAML\nDESCRIPTION: YAML configuration for setting up Codestral as an autocomplete model in Continue.dev. Specifies the model name, provider, model ID, role as autocomplete, and required environment variables.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/vertexai.mdx#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Codestral (Vertex AI)\n    provider: vertexai\n    model: codestral\n    roles:\n      - autocomplete\n    env:\n      projectId: <PROJECT_ID>\n      region: us-central1\n      \n```\n\n----------------------------------------\n\nTITLE: Configuring Codestral for Autocomplete using JSON\nDESCRIPTION: JSON configuration for setting up Codestral as the autocomplete model in Continue. This configuration specifies the model provider, model name, and requires a Mistral API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/deep-dives/autocomplete.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"tabAutocompleteModel\": {\n    \"title\": \"Codestral\",\n    \"provider\": \"mistral\", \n    \"model\": \"codestral-latest\",\n    \"apiKey\": \"<YOUR_CODESTRAL_API_KEY>\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring GPT-4o Chat Model in Azure AI Foundry\nDESCRIPTION: Configuration settings for setting up GPT-4o as a chat model using Azure AI Foundry. Includes essential parameters like API base, API key, deployment name, and API type specification.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/azure.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: GPT-4o\n    provider: azure\n    model: gpt-4o\n    apiBase: <YOUR_DEPLOYMENT_BASE>\n    apiKey: <YOUR_AZURE_API_KEY>\n    env:\n      deployment: <YOUR_DEPLOYMENT_NAME>\n      apiType: azure-foundry\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [{\n      \"title\": \"GPT-4o\",\n      \"provider\": \"azure\",\n      \"model\": \"gpt-4o\",\n      \"apiBase\": \"<YOUR_DEPLOYMENT_BASE>\",\n      \"deployment\": \"<YOUR_DEPLOYMENT_NAME>\",\n      \"apiKey\": \"<YOUR_AZURE_API_KEY>\",\n      \"apiType\": \"azure-foundry\"\n  }]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Model Context Protocol Server in JSON\nDESCRIPTION: JSON configuration for setting up a Model Context Protocol server with a local SQLite database. This enables support for the MCP standard in Continue.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_37\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"experimental\": {\n    \"modelContextProtocolServers\": [\n      {\n        \"transport\": {\n          \"type\": \"stdio\",\n          \"command\": \"uvx\",\n          \"args\": [\"mcp-server-sqlite\", \"--db-path\", \"/Users/NAME/test.db\"]\n        }  \n      }\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Cloudflare Workers AI in YAML for Continue\nDESCRIPTION: YAML configuration for setting up Cloudflare Workers AI models in Continue. This snippet shows how to configure three models: Llama 3 8B and DeepSeek Coder 6.7b Instruct for chat functionality, and DeepSeek 7b for autocomplete functionality. Each configuration includes API keys, model identifiers, context length, and role assignments.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/cloudflare.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Llama 3 8B\n    provider: cloudflare\n    apiKey: <YOUR_CLOUDFARE_API_KEY>\n    model: \"@cf/meta/llama-3-8b-instruct\"\n    contextLength: 2400\n    defaultCompletionOptions:\n      maxTokens: 500\n    roles:\n      - chat\n    env:\n      accountId: YOUR CLOUDFLARE ACCOUNT ID \n  - name: DeepSeek Coder 6.7b Instruct\n    provider: cloudflare\n    apiKey: <YOUR_CLOUDFARE_API_KEY>\n    model: \"@hf/thebloke/deepseek-coder-6.7b-instruct-awq\"\n    contextLength: 2400\n    defaultCompletionOptions:\n      maxTokens: 500\n    roles:\n      - chat\n    env:\n      accountId: YOUR CLOUDFLARE ACCOUNT ID\n  - name: DeepSeek 7b\n    provider: cloudflare\n    apiKey: <YOUR_CLOUDFARE_API_KEY>\n    model: \"@hf/thebloke/deepseek-coder-6.7b-base-awq\"\n    roles:\n      - autocomplete\n    env:\n      accountId: YOUR CLOUDFLARE ACCOUNT ID\n```\n\n----------------------------------------\n\nTITLE: Configuring Ollama Qwen Model for Autocomplete using YAML\nDESCRIPTION: YAML configuration for setting up the Qwen 1.5B model through Ollama as an autocomplete provider in Continue. This allows for local autocomplete functionality without requiring external API access.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/deep-dives/autocomplete.mdx#2025-04-19_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Qwen 1.5b Autocomplete Model\n    provider: ollama\n    model: qwen2.5-coder:1.5b\n    roles:\n      - autocomplete\n```\n\n----------------------------------------\n\nTITLE: Configuring Ollama Llama 3.1 8B in JSON for Local Use\nDESCRIPTION: JSON configuration for using Llama 3.1 8B through Ollama as a local chat model in Continue. Requires Ollama installation on your machine.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/chat.mdx#2025-04-19_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Llama 3.1 8B\",\n      \"provider\": \"ollama\",\n      \"model\": \"llama3.1:8b\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Claude 3.5 Sonnet Chat Model in JSON\nDESCRIPTION: JSON configuration example for setting up Claude 3.5 Sonnet as a chat model in Continue. Requires your Anthropic API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/anthropic.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Claude 3.5 Sonnet\",\n      \"provider\": \"anthropic\",\n      \"model\": \"claude-3-5-sonnet-latest\",\n      \"apiKey\": \"<YOUR_ANTHROPIC_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Commits Context Provider in YAML\nDESCRIPTION: YAML configuration for the Commits context provider, which allows referencing git commit metadata and diffs. Parameters control how many commits are loaded and displayed.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_61\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: commit\n    params:\n      Depth: 50\n      LastXCommitsDepth: 10\n```\n\n----------------------------------------\n\nTITLE: Configuring GitLab Merge Request Context Provider in YAML\nDESCRIPTION: Sets up the GitLab merge request context provider to reference open merge requests. Requires a GitLab personal access token with read_api scope.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_46\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: gitlab-mr\n    params:\n      token: \"...\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Qwen Models for Chat and Autocomplete in JSON\nDESCRIPTION: JSON configuration that sets up both a main model and a tab autocomplete model using Qwen models from SiliconFlow. Requires a SiliconFlow API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/siliconflow.mdx#2025-04-19_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Qwen\",\n      \"provider\": \"siliconflow\",\n      \"model\": \"Qwen/Qwen2.5-Coder-32B-Instruct\",\n      \"apiKey\": \"<YOUR_SILICONFLOW_API_KEY>\"\n    }\n  ]\n  \"tabAutocompleteModel\": {\n    \"title\": \"Qwen\",\n    \"provider\": \"siliconflow\",\n    \"model\": \"Qwen/Qwen2.5-Coder-7B-Instruct\",\n    \"apiKey\": \"<YOUR_SILICONFLOW_API_KEY>\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Mistral Large Chat Model\nDESCRIPTION: Configuration for setting up Mistral Large as a chat model. Requires a Mistral API key and specifies the model name, provider, and role.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/mistral.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Mistral Large\n    provider: mistral\n    model: mistral-large-latest \n    apiKey: <YOUR_MISTRAL_API_KEY>\n    roles:\n      - chat\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Mistral Large\", \n      \"provider\": \"mistral\",\n      \"model\": \"mistral-large-latest\",\n      \"apiKey\": \"<YOUR_MISTRAL_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Gemini Embeddings Model\nDESCRIPTION: Configuration setup for using text-embedding-004 as an embeddings model. Specifies the embeddings role and requires a Gemini API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/gemini.mdx#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Gemini Embeddings\n    provider: gemini\n    model: models/text-embedding-004 \n    apiKey: <YOUR_GEMINI_API_KEY>\n    roles:\n      - embed\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"embeddingsProvider\": {\n    \"provider\": \"gemini\",\n    \"model\": \"models/text-embedding-004\",\n    \"apiKey\": \"<YOUR_GEMINI_API_KEY>\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring xAI Grok-2 in YAML\nDESCRIPTION: YAML configuration for using Grok-2 from xAI as a chat model in Continue. Requires an xAI API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/chat.mdx#2025-04-19_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Grok-2\n    provider: xAI\n    model: grok-2-latest\n    apiKey: <YOUR_XAI_API_KEY>\n```\n\n----------------------------------------\n\nTITLE: Configuring Flowise as an LLM Provider in JSON\nDESCRIPTION: JSON configuration for integrating Flowise with Continue. The configuration specifies the provider type, title, model name, and API base URL that Continue will use to connect to Flowise.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/flowise.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"provider\": \"flowise\",\n      \"title\": \"Flowise\",\n      \"model\": \"<MODEL>\",\n      \"apiBase\": \"<API_BASE>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring ReplicateLLM with JSON\nDESCRIPTION: JSON configuration example for setting up a Replicate-hosted CodeLLama model in the Continue development environment. This requires your Replicate API key and specifies CodeLLama-13B as the model to use.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/replicatellm.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Replicate CodeLLama\",\n      \"provider\": \"replicate\",\n      \"model\": \"codellama-13b\",\n      \"apiKey\": \"<YOUR_REPLICATE_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI-compatible Server in JSON\nDESCRIPTION: JSON configuration for using an OpenAI-compatible server or API with a custom base URL. Requires specifying the model name and API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/openai.mdx#2025-04-19_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"OpenAI-compatible server / API\", \n      \"provider\": \"openai\",\n      \"model\": \"MODEL_NAME\",\n      \"apiKey\": \"<YOUR_CUSTOM_API_KEY>\",\n      \"apiBase\": \"http://localhost:8000/v1\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Hugging Face Text Embeddings Inference\nDESCRIPTION: Configuration for setting up a custom Hugging Face Text Embeddings Inference endpoint for generating embeddings.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/embeddings.mdx#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Huggingface TEI Embedder\n    provider: huggingface-tei\n    apiBase: http://localhost:8080\n    roles: [embed]\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"embeddingsProvider\": {\n    \"provider\": \"huggingface-tei\",\n    \"apiBase\": \"http://localhost:8080\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Reranker in Continue JSON Configuration\nDESCRIPTION: This snippet demonstrates how to set up a reranker model in the Continue config.json file. It shows configuring the Voyage reranker with a specific model and API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/json-reference.md#2025-04-19_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"reranker\": {\n    \"name\": \"voyage\",\n    \"params\": {\n      \"model\": \"rerank-2\",\n      \"apiKey\": \"<VOYAGE_API_KEY>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Nemotron-4-340B-Instruct Chat Model in YAML\nDESCRIPTION: YAML configuration for setting up the Nemotron-4-340B-Instruct model as a chat model in Continue. Requires a valid NVIDIA API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/nvidia.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Nemotron-4-340B-Instruct\n    provider: nvidia\n    model: nvidia-nemotron-4-340b-instruct\n    apiKey: <YOUR_NVIDIA_API_KEY>\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Slash Commands with config.ts\nDESCRIPTION: Example of a config.ts file that exports a modifyConfig function to add a custom slash command for generating commit messages based on the Git diff.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/deep-dives/configuration.md#2025-04-19_snippet_1\n\nLANGUAGE: ts\nCODE:\n```\nexport function modifyConfig(config: Config): Config {\n  config.slashCommands?.push({\n    name: \"commit\",\n    description: \"Write a commit message\",\n    run: async function* (sdk) {\n      // The getDiff function takes a boolean parameter that indicates whether\n      // to include unstaged changes in the diff or not.\n      const diff = await sdk.ide.getDiff(false); // Pass false to exclude unstaged changes\n      for await (const message of sdk.llm.streamComplete(\n        `${diff}\\n\\nWrite a commit message for the above changes. Use no more than 20 tokens to give a brief description in the imperative mood (e.g. 'Add feature' not 'Added feature'):`,\n        new AbortController().signal,\n        {\n          maxTokens: 20,\n        },\n      )) {\n        yield message;\n      }\n    },\n  });\n  return config;\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Mistral Embed Model\nDESCRIPTION: Configuration for setting up Mistral Embed as an embeddings model. Specifies the API base URL and requires a Mistral API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/mistral.mdx#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Mistral Embed\n    provider: mistral\n    model: mistral-embed\n    apiKey: <YOUR_MISTRAL_API_KEY> \n    apiBase: https://api.mistral.ai/v1\n    roles:\n      - embed\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"embeddingsProvider\": {\n    \"provider\": \"mistral\",\n    \"model\": \"mistral-embed\", \n    \"apiKey\": \"<YOUR_MISTRAL_API_KEY>\",\n    \"apiBase\": \"https://api.mistral.ai/v1\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring watsonx Reranker in JSON\nDESCRIPTION: JSON configuration for setting up a watsonx reranker in Continue. It includes the model details and API information.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/watsonx.mdx#2025-04-19_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"reranker\": {\n    \"name\": \"watsonx\",\n    \"params\": {\n      \"model\": \"cross-encoder/ms-marco-minilm-l-12-v2\",\n      \"apiBase\": \"watsonx endpoint e.g. https://us-south.ml.cloud.ibm.com\",\n      \"projectId\": \"PROJECT_ID\",\n      \"apiKey\": \"API_KEY/ZENAPI_KEY/USERNAME:PASSWORD\",\n      \"apiVersion\": \"2024-03-14\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Cohere Reranker in YAML\nDESCRIPTION: YAML configuration for setting up the Cohere rerank-english-v3.0 model as a reranker in Continue. Requires a Cohere API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/reranking.mdx#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Cohere Reranker\n    provider: cohere\n    model: rerank-english-v3.0\n    apiKey: <YOUR_COHERE_API_KEY>\n    roles:\n      - rerank\n```\n\n----------------------------------------\n\nTITLE: Configuring Titan Embeddings Model in YAML\nDESCRIPTION: YAML configuration for setting up Amazon Titan Embed Text V2 as an embeddings model in Continue. Specifies the model provider, model ID, and AWS region.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/bedrock.mdx#2025-04-19_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Embeddings Model\n    provider: bedrock\n    model: amazon.titan-embed-text-v2:0\n    env:\n      region: us-west-2\n    roles:\n      - embed\n```\n\n----------------------------------------\n\nTITLE: Configuring Slash Commands in Continue.dev\nDESCRIPTION: Configuration example for setting up slash commands that provide quick access to common actions like generating commit messages, sharing sessions, and running shell commands.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/i18n/zh-CN/docusaurus-plugin-content-docs/current/reference.md#2025-04-19_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"slashCommands\": [\n    {\n      \"name\": \"commit\",\n      \"description\": \"Generate a commit message\"\n    },\n    {\n      \"name\": \"share\",\n      \"description\": \"Export this session as markdown\"\n    },\n    {\n      \"name\": \"cmd\",\n      \"description\": \"Generate a shell command\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Llama3.1 8B Chat Model in YAML\nDESCRIPTION: YAML configuration for setting up Llama3.1 8B as a chat model in Continue. Specifies the model provider as vLLM, the model path, and the API base URL for connecting to the vLLM server.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/vllm.mdx#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Llama3.1 8B Instruct\n    provider: vllm\n    model: meta-llama/Meta-Llama-3.1-8B-Instruct\n    apiBase: http://<vllm chat endpoint>/v1\n```\n\n----------------------------------------\n\nTITLE: Basic Moonshot Configuration\nDESCRIPTION: Basic configuration setup for integrating Moonshot AI models with required parameters including provider, model name, and API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/moonshot.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Moonshot\n    provider: moonshot\n    model: moonshot-v1-8k\n    apiKey: <YOUR_MOONSHOT_API_KEY>\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Moonshot\",\n      \"provider\": \"moonshot\",\n      \"model\": \"moonshot-v1-8k\",\n      \"apiKey\": \"<YOUR_MOONSHOT_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI Embeddings Model in YAML\nDESCRIPTION: YAML configuration for setting up text-embedding-3-large as the embeddings model in Continue.dev. Includes role specification for embeddings.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/openai.mdx#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: OpenAI Embeddings\n    provider: openai\n    model: text-embedding-3-large\n    apiKey: <YOUR_OPENAI_API_KEY>\n    roles:\n      - embed\n```\n\n----------------------------------------\n\nTITLE: Configuring Nebius AI Embeddings Model in JSON\nDESCRIPTION: This snippet shows the configuration for the BAAI Embedder model for embeddings using JSON. It includes the provider, model identifier, and requires the user's Nebius API key under the 'embeddingsProvider' object.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/nebius.mdx#2025-04-19_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"embeddingsProvider\": {\n    \"provider\": \"nebius\",\n    \"model\": \"BAAI/bge-en-icl\",\n    \"apiKey\": \"<YOUR_NEBIUS_API_KEY>\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Qwen2.5-Coder-32B-Instruct as Chat Model in YAML\nDESCRIPTION: YAML configuration for setting up Qwen/Qwen2.5-Coder-32B-Instruct as a chat model in Continue. Requires a SiliconFlow API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/siliconflow.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Qwen\n    provider: siliconflow\n    model: Qwen/Qwen2.5-Coder-32B-Instruct\n    apiKey: <YOUR_SILICONFLOW_API_KEY>\n    roles:\n      - chat\n```\n\n----------------------------------------\n\nTITLE: Configuring NVIDIA Retrieval QA Mistral 7B Embeddings Model in JSON\nDESCRIPTION: JSON configuration for setting up the NVIDIA Retrieval QA Mistral 7B model as an embeddings model in Continue. Requires a valid NVIDIA API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/nvidia.mdx#2025-04-19_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"embeddingsProvider\": {\n    \"provider\": \"nvidia\",\n    \"model\": \"nvidia/nv-embedqa-mistral-7b-v2\",\n    \"apiKey\": \"<YOUR_NVIDIA_API_KEY>\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Ask Sage Model in YAML Format\nDESCRIPTION: YAML configuration for setting up the GPT-4 gov model from Ask Sage in Continue.Dev. Requires providing the model name, provider, model type, API base URL, and your Ask Sage API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/asksage.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: GPT-4 gov\n    provider: askSage\n    model: gpt4-gov\n    apiBase: https://api.asksage.ai/server/\n    apiKey: <YOUR_ASK_SAGE_API_KEY>\n```\n\n----------------------------------------\n\nTITLE: Configuring Claude 3.5 Sonnet V2 Model in JSON\nDESCRIPTION: JSON configuration for setting up Claude 3.5 Sonnet V2 using the regional model ID format to resolve throughput errors. Uses the us-prefixed model ID to access the model.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/bedrock.mdx#2025-04-19_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Claude 3.5 Sonnet\",\n      \"provider\": \"bedrock\",\n      \"model\": \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n      \"region\": \"us-east-1\",\n      \"profile\": \"bedrock\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Gemini 2.0 Flash in YAML\nDESCRIPTION: YAML configuration for using Gemini 2.0 Flash from Google as a chat model in Continue. Requires a Gemini API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/chat.mdx#2025-04-19_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Gemini 2.0 Flash\n    provider: gemini\n    model: gemini-2.0-flash\n    apiKey: <YOUR_GEMINI_API_KEY>\n```\n\n----------------------------------------\n\nTITLE: Configuring Cohere Command-R Plus Chat Model\nDESCRIPTION: Configuration setup for Cohere's Command-R Plus chat model. Requires a Cohere API key and specifies the model provider and name.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/cohere.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Cohere\n    provider: cohere\n    model: command-r-plus\n    apiKey: <YOUR_COHERE_API_KEY>\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Cohere\",\n      \"provider\": \"cohere\",\n      \"model\": \"command-r-plus\",\n      \"apiKey\": \"<YOUR_COHERE_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring xAI Grok 2 Chat Model in JSON\nDESCRIPTION: JSON configuration for integrating the xAI Grok 2 chat model in Continue. This configuration requires an API key from the xAI console and specifies the model as 'grok-2-latest'.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/xAI.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Grok 2\",\n      \"provider\": \"xAI\",\n      \"model\": \"grok-2-latest\",\n      \"apiKey\": \"<YOUR_XAI_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Text Embeddings Model in Vertex AI\nDESCRIPTION: Configuration for setting up text-embedding-005 as the embeddings model through Vertex AI. Requires project ID and uses us-central1 region.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/i18n/zh-CN/docusaurus-plugin-content-docs/current/customize/model-providers/top-level/vertexai.md#2025-04-19_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"embeddingsProvider\": {\n    \"provider\": \"vertexai\",\n    \"model\": \"text-embedding-005\",\n    \"projectId\": \"[PROJECT_ID]\",\n    \"region\": \"us-central1\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Together Gemma 3 27B in YAML\nDESCRIPTION: YAML configuration for using Gemma 3 27B through Together as a chat model in Continue. Requires a Together API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/chat.mdx#2025-04-19_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: \"Gemma 3 27B\"\n    provider: \"together\"\n    model: \"google/gemma-2-27b-it\"\n    apiKey: <YOUR_TOGETHER_API_KEY>\n```\n\n----------------------------------------\n\nTITLE: Configuring watsonx Model with Deployment ID in YAML\nDESCRIPTION: YAML configuration for a watsonx model using a custom deployment endpoint. It includes the deployment ID in addition to standard parameters.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/watsonx.mdx#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: watsonx - Model Name\n    provider: watsonx\n    model: model ID\n    apiBase: watsonx endpoint e.g. https://us-south.ml.cloud.ibm.com\n    apiKey: API_KEY/ZENAPI_KEY/USERNAME:PASSWORD\n    env:\n      apiVersion: 2024-03-14\n      deploymentId: DEPLOYMENT_ID\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSeek Coder 2 16B with Msty in YAML\nDESCRIPTION: This code snippet shows the configuration for the DeepSeek Coder 2 16B model using Msty as the provider in YAML format. It specifies the model name, provider, and model identifier.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/chat.mdx#2025-04-19_snippet_20\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: DeepSeek Coder 2 16B\n    provider: msty\n    model: deepseek-coder-v2:16b\n```\n\n----------------------------------------\n\nTITLE: Configuring TextGenWebUI in Continue using YAML\nDESCRIPTION: YAML configuration for connecting Continue to TextGenWebUI's OpenAI-compatible API. This configuration requires specifying the provider type, API base URL, and model name to use.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/textgenwebui.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Text Generation WebUI\n    provider: text-gen-webui\n    apiBase: http://localhost:5000/v1\n    model: MODEL_NAME\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepInfra Models in Continue using YAML\nDESCRIPTION: YAML configuration example for setting up a DeepInfra model in Continue. It specifies the provider as deepinfra, selects the Mixtral-8x7B-Instruct model, and requires your DeepInfra API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/deepinfra.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: DeepInfra\n    provider: deepinfra\n    model: mistralai/Mixtral-8x7B-Instruct-v0.1\n    apiKey: <YOUR_DEEP_INFRA_API_KEY>\n```\n\n----------------------------------------\n\nTITLE: Configuring Code Context Provider in YAML\nDESCRIPTION: Configuration example for adding the @Code context provider, which allows referencing specific functions or classes from the project.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: code\n```\n\n----------------------------------------\n\nTITLE: Configuring Groq API for Llama 3.3 70b Versatile in JSON\nDESCRIPTION: This JSON configuration snippet sets up the Llama 3.3 70b Versatile model using the Groq provider. It requires a Groq API key to be inserted in the appropriate field.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/groq.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Llama 3.3 70b Versatile\",\n      \"provider\": \"groq\",\n      \"model\": \"llama-3.3-70b-versatile\",\n      \"apiKey\": \"<YOUR_GROQ_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSeek Coder for Autocomplete\nDESCRIPTION: Configuration setup for using DeepSeek Coder as an autocomplete model. Includes specific role designation for autocomplete functionality and requires a DeepSeek API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/deepseek.mdx#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: DeepSeek Coder\n    provider: deepseek\n    model: deepseek-coder\n    apiKey: <YOUR_DEEPSEEK_API_KEY>\n    roles:\n      - autocomplete\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"tabAutocompleteModel\": {\n    \"title\": \"DeepSeek Coder\",\n    \"provider\": \"deepseek\",\n    \"model\": \"deepseek-coder\",\n    \"apiKey\": \"<YOUR_DEEPSEEK_API_KEY>\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring LM Studio in Continue using YAML\nDESCRIPTION: YAML configuration for setting up LM Studio as a model provider in Continue. This enables the use of local language models like llama2-7b through the LM Studio interface.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/lmstudio.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: LM Studio\n    provider: lmstudio\n    model: llama2-7b\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenRouter LLaMA Model\nDESCRIPTION: Basic configuration for setting up OpenRouter with LLaMA 70B model. Includes model specification, provider details, and API configuration.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/openrouter.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: OpenRouter LLaMA 70 8B\n    provider: openrouter\n    model: meta-llama/llama-3-70b-instruct\n    apiBase: https://openrouter.ai/api/v1\n    apiKey: <YOUR_OPEN_ROUTER_API_KEY>\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"OpenRouter LLaMA 70 8B\",\n      \"provider\": \"openrouter\",\n      \"model\": \"meta-llama/llama-3-70b-instruct\",\n      \"apiBase\": \"https://openrouter.ai/api/v1\",\n      \"apiKey\": \"<YOUR_OPEN_ROUTER_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Commits Context Provider in JSON\nDESCRIPTION: JSON configuration for the Commits context provider, which allows referencing git commit metadata and diffs. Parameters control how many commits are loaded and displayed.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_62\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"commit\", \n      \"params\": {\n        \"Depth\": 50,\n        \"LastXCommitsDepth\": 10\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSeek Chat Model\nDESCRIPTION: Configuration setup for using DeepSeek Chat as the primary chat model. Requires a DeepSeek API key to be specified in the configuration.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/deepseek.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: DeepSeek Chat\n    provider: deepseek\n    model: deepseek-chat\n    apiKey: <YOUR_DEEPSEEK_API_KEY>\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"DeepSeek Chat\",\n      \"provider\": \"deepseek\",\n      \"model\": \"deepseek-chat\",\n      \"apiKey\": \"<YOUR_DEEPSEEK_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Tab Autocomplete Model Configuration\nDESCRIPTION: Configuration example for the tab autocompletion model, showing setup for a custom Starcoder model using Ollama.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/i18n/zh-CN/docusaurus-plugin-content-docs/current/reference.md#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"tabAutocompleteModel\": {\n    \"title\": \"My Starcoder\",\n    \"provider\": \"ollama\",\n    \"model\": \"starcoder2:3b\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring xAI Grok-2 in JSON\nDESCRIPTION: JSON configuration for using Grok-2 from xAI as a chat model in Continue. Requires an xAI API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/chat.mdx#2025-04-19_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Grok-2\",\n      \"provider\": \"xAI\",\n      \"model\": \"grok-2-latest\",\n      \"apiKey\": \"<YOUR_XAI_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Basic @Docs Provider Configuration\nDESCRIPTION: Basic configuration for enabling the @Docs context provider in Continue using either YAML or JSON format.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/deep-dives/docs.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: docs\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"docs\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Codebase Context Provider in JSON\nDESCRIPTION: JSON configuration example for adding the @Codebase context provider to reference relevant code snippets from the entire codebase.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"codebase\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Together API in JSON format\nDESCRIPTION: JSON configuration for integrating the Together API with Qwen2.5 Coder model into the Continue project. Requires adding your Together API key to the configuration file.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/together.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Together Qwen2.5 Coder\",\n      \"provider\": \"together\",\n      \"model\": \"Qwen/Qwen2.5-Coder-32B-Instruct\",\n      \"apiKey\": \"<YOUR_TOGETHER_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring watsonx Reranker in YAML\nDESCRIPTION: YAML configuration for setting up a watsonx reranker in Continue. It specifies the model, provider, and API details.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/watsonx.mdx#2025-04-19_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Watsonx Reranker\n    provider: watsonx\n    model: cross-encoder/ms-marco-minilm-l-12-v2\n    apiBase: https://us-south.ml.cloud.ibm.com\n    projectId: PROJECT_ID\n    apiKey: API_KEY/ZENAPI_KEY/USERNAME:PASSWORD\n    apiVersion: 2024-03-14\n```\n\n----------------------------------------\n\nTITLE: Configuring Folder Context Provider in JSON\nDESCRIPTION: JSON configuration example for adding the @Folder context provider to limit retrieval to a single folder.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"folder\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Greptile Context Provider in YAML\nDESCRIPTION: YAML configuration for the Greptile context provider, which allows querying a Greptile index of the current repository/branch. Requires Greptile and GitHub tokens for authentication.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_63\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: greptile \n    params:\n      greptileToken: \"...\"\n      githubToken: \"...\"\n```\n\n----------------------------------------\n\nTITLE: Configuring GitHub Issue Context Provider in YAML\nDESCRIPTION: Sets up the GitHub issue context provider to reference conversations in GitHub issues. Requires a GitHub personal access token and repository information.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_38\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: issue\n    params:\n      repos:\n        - owner: continuedev\n          repo: continue\n      githubToken: ghp_xxx\n```\n\n----------------------------------------\n\nTITLE: Configuring Ollama Gemma 3 27B in JSON\nDESCRIPTION: JSON configuration for using Gemma 3 27B through Ollama as a chat model in Continue. This is a self-hosted option requiring Ollama installation.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/chat.mdx#2025-04-19_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Gemma 3 27B\",\n      \"provider\": \"ollama\",\n      \"model\": \"gemma3:27b\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring LLM as Reranker in JSON\nDESCRIPTION: JSON configuration for using an existing LLM as a reranker in Continue. Requires specifying the model title that matches an entry in the models array.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/reranking.mdx#2025-04-19_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"reranker\": {\n      \"name\": \"llm\",\n      \"params\": {\n          \"modelTitle\": \"My Model Title\"\n      }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Search Context Provider in YAML\nDESCRIPTION: Configuration example for adding the @Search context provider, which allows referencing the results of codebase search similar to VS Code search.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_20\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: search\n```\n\n----------------------------------------\n\nTITLE: Configuring Voyage Code 3 Embedder\nDESCRIPTION: Configuration example for setting up Voyage AI's code-3 embedder model with API key authentication.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/embeddings.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Voyage Code 3\n    provider: voyage\n    model: voyage-code-3\n    apiKey: <YOUR_VOYAGE_API_KEY>\n    roles: \n      - embed\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"embeddingsProvider\": {\n    \"provider\": \"voyage\",\n    \"model\": \"voyage-code-3\",\n    \"apiKey\": \"<YOUR_VOYAGE_API_KEY>\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Edit Role in JSON\nDESCRIPTION: Example of configuring Claude 3.5 Sonnet model for inline editing using JSON configuration.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/edit.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": {\n    \"name\": \"Claude 3.5 Sonnet\",\n    \"provider\": \"anthropic\",\n    \"model\": \"claude-3-5-sonnet-latest\",\n    \"apiKey\": \"<YOUR_ANTHROPIC_API_KEY>\"\n  },\n  \"experimental\": {\n      \"modelRoles\": {\n          \"inlineEdit\": \"Claude 3.5 Sonnet\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring nCompass in JSON\nDESCRIPTION: Example JSON configuration for setting up nCompass with the Gemma 3 Coder model in config.json. This requires replacing the placeholder with your personal nCompass API key obtained after signing up.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/ncompass.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Ncompass Gemma 3 Coder\",\n      \"provider\": \"ncompass\",\n      \"model\": \"google/gemma-3-27b-it\",\n      \"apiKey\": \"<YOUR_NCOMPASS_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI GPT-4o in JSON\nDESCRIPTION: JSON configuration for using GPT-4o from OpenAI as a chat model in Continue. Requires an OpenAI API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/chat.mdx#2025-04-19_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"GPT-4o\",\n      \"provider\": \"openai\",\n      \"model\": \"\",\n      \"apiKey\": \"<YOUR_OPENAI_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Context Provider in JSON\nDESCRIPTION: JSON configuration for the HTTP context provider, which makes POST requests to a specified URL to retrieve context information. The server must return a properly formatted response.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_58\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"http\", \n      \"params\": {\n        \"url\": \"https://api.example.com/v1/users\"\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Llama3.1 8B as Chat Model in JSON\nDESCRIPTION: This JSON configuration sets up Llama3.1 8B as the chat model using Ollama provider. It specifies the model title, provider, and the specific model to use within the models array.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/ollama.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Llama3.1 8B\",\n      \"provider\": \"ollama\",\n      \"model\": \"llama3.1:8b\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Context Providers in YAML Format for Continue\nDESCRIPTION: Demonstrates the YAML configuration for context providers including docs, codebase, and diff.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/yaml-migration.md#2025-04-19_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: docs\n\n  - provider: codebase\n    params:\n      nRetrieve: 30\n      nFinal: 3\n\n  - provider: diff\n```\n\n----------------------------------------\n\nTITLE: Configuring Ollama Gemma 3 27B in YAML\nDESCRIPTION: YAML configuration for using Gemma 3 27B through Ollama as a chat model in Continue. This is a self-hosted option requiring Ollama installation.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/chat.mdx#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: \"Gemma 3 27B\"\n    provider: \"ollama\"\n    model: \"gemma3:27b\"\n```\n\n----------------------------------------\n\nTITLE: Local Crawling Configuration\nDESCRIPTION: Configuration for enabling local crawling of private documentation sites.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/deep-dives/docs.mdx#2025-04-19_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ndocs:\n  - title: My Private Docs\n    startUrl: http://10.2.1.2/docs\n    faviconUrl: http://10.2.1.2/docs/assets/favicon.ico,\n    useLocalCrawling: true\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"docs\": [\n    {\n      \"title\": \"My Private Docs\",\n      \"startUrl\": \"http://10.2.1.2/docs\",\n      \"faviconUrl\": \"http://10.2.1.2/docs/assets/favicon.ico\",\n      \"useLocalCrawling\": true\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Cohere Reranking Model in YAML\nDESCRIPTION: YAML configuration for setting up Cohere Rerank V3.5 as a reranking model in Continue. Configures the model provider, model ID, and AWS region.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/bedrock.mdx#2025-04-19_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Bedrock Reranker\n    provider: bedrock\n    model: cohere.rerank-v3-5:0\n    env:\n      region: us-west-2\n    roles:\n      - rerank\n```\n\n----------------------------------------\n\nTITLE: Configuring Ollama Model with Client Certificate in YAML\nDESCRIPTION: Example YAML configuration for setting up an Ollama model with client certificate authentication in Continue's config.yaml file.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/tutorials/how-to-self-host-a-model.mdx#2025-04-19_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Ollama\n    provider: ollama\n    model: llama2-7b\n    requestOptions:\n      clientCertificate:\n        cert: C:\\tempollama.pem\n        key: C:\\tempollama.key\n        passphrase: c0nt!nu3\n```\n\n----------------------------------------\n\nTITLE: Configuring Qwen2.5-Coder 1.5B Autocomplete Model in YAML\nDESCRIPTION: YAML configuration for setting up Qwen2.5-Coder 1.5B as an autocomplete model in Continue. Includes the role specification for autocomplete functionality.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/vllm.mdx#2025-04-19_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Qwen2.5-Coder 1.5B\n    provider: vllm\n    model: Qwen/Qwen2.5-Coder-1.5B\n    apiBase: http://<vllm autocomplete endpoint>/v1\n    roles:\n      - autocomplete\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI GPT-4o in YAML\nDESCRIPTION: YAML configuration for using GPT-4o from OpenAI as a chat model in Continue. Requires an OpenAI API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/chat.mdx#2025-04-19_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: GPT-4o\n    provider: openai\n    model: ''\n    apiKey: <YOUR_OPENAI_API_KEY>\n```\n\n----------------------------------------\n\nTITLE: Configuring Qwen2.5-Coder-32B-Instruct as Autocomplete Model in YAML\nDESCRIPTION: YAML configuration for setting up Qwen/Qwen2.5-Coder-32B-Instruct as an autocomplete model in Continue. Requires a SiliconFlow API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/siliconflow.mdx#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Qwen\n    provider: siliconflow\n    model: Qwen/Qwen2.5-Coder-32B-Instruct\n    apiKey: <YOUR_SILICONFLOW_API_KEY>\n    roles: \n      - autocomplete\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSeek Coder 2 16B with LM Studio in YAML\nDESCRIPTION: This snippet demonstrates the configuration for the DeepSeek Coder 2 16B model using LM Studio as the provider in YAML format. It includes the model name, provider, and model identifier.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/chat.mdx#2025-04-19_snippet_19\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: DeepSeek Coder 2 16B\n    provider: lmstudio\n    model: deepseek-coder-v2:16b\n```\n\n----------------------------------------\n\nTITLE: Configuring Chat Model with Function Network\nDESCRIPTION: Configuration for setting up Llama 3.1 70b chat model with Function Network. Includes API key configuration and role specification for chat functionality.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/function-network.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Llama 3.1 70b\n    provider: function-network\n    model: meta/llama-3.1-70b-instruct\n    apiKey: <YOUR_FUNCTION_NETWORK_API_KEY>\n    roles:\n      - chat\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Llama 3.1 70b\",\n      \"provider\": \"function-network\",\n      \"model\": \"meta/llama-3.1-70b-instruct\",\n      \"apiKey\": \"<YOUR_FUNCTION_NETWORK_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Continue to use local IPEX-LLM Ollama backend in YAML\nDESCRIPTION: YAML configuration example for setting up Continue to use a locally hosted IPEX-LLM accelerated Ollama provider. This basic configuration automatically detects the available model.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/ipex_llm.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: IPEX-LLM\n    provider: ollama\n    model: AUTODETECT\n```\n\n----------------------------------------\n\nTITLE: Configuring Continue to Use LlamaCpp in YAML\nDESCRIPTION: YAML configuration example for ~/.continue/config.yaml that specifies the LlamaCpp model settings including provider, model name, and API base URL.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/llamacpp.mdx#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Llama CPP\n    provider: llama.cpp\n    model: MODEL_NAME\n    apiBase: http://localhost:8080\n```\n\n----------------------------------------\n\nTITLE: Configuring Text Embedding-004 Model in YAML\nDESCRIPTION: YAML configuration for setting up the text-embedding-004 model as an embeddings provider in Continue.dev. Specifies the model name, provider, model ID, environment variables, and assigns the embed role.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/vertexai.mdx#2025-04-19_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Text Embedding-004\n    provider: vertexai\n    model: text-embedding-004\n    env:\n      projectId: <PROJECT_ID>\n      region: us-central1\n    roles:\n      - embed\n```\n\n----------------------------------------\n\nTITLE: Configuring Nebius AI Chat Model in JSON\nDESCRIPTION: This snippet demonstrates the configuration of the DeepSeek R1 chat model from Nebius AI Studio using JSON. It includes the model title, provider, model identifier, and requires the user's Nebius API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/nebius.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"DeepSeek R1\",\n      \"provider\": \"nebius\",\n      \"model\": \"deepseek-ai/DeepSeek-R1\",\n      \"apiKey\": \"<YOUR_NEBIUS_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Cohere Reranker in JSON\nDESCRIPTION: JSON configuration for setting up the Cohere rerank-english-v3.0 model as a reranker in Continue. Requires a Cohere API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/reranking.mdx#2025-04-19_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"reranker\": {\n      \"name\": \"cohere\",\n      \"params\": {\n        \"model\": \"rerank-english-v3.0\",\n        \"apiKey\": \"<YOUR_COHERE_API_KEY>\"\n      }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Codestral in JSON for Continue\nDESCRIPTION: This JSON configuration snippet sets up Codestral as a model for both chat and autocomplete functionalities in Continue. It includes the model details, API key, and base URL for integration.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/tutorials/set-up-codestral.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Codestral\",\n      \"provider\": \"mistral\",\n      \"model\": \"codestral-latest\",\n      \"apiKey\": \"<YOUR_CODESTRAL_API_KEY>\",\n      \"apiBase\": \"https://codestral.mistral.ai/v1\"\n    }\n  ],\n  \"tabAutocompleteModel\": [\n    {\n      \"title\": \"Codestral\",\n      \"provider\": \"mistral\",\n      \"model\": \"codestral-latest\",\n      \"apiKey\": \"<YOUR_CODESTRAL_API_KEY>\",\n      \"apiBase\": \"https://codestral.mistral.ai/v1\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Codestral Autocomplete Model\nDESCRIPTION: Configuration for setting up Codestral as an autocomplete model. Includes optional apiBase configuration for Codestral API key users.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/mistral.mdx#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Codestral\n    provider: mistral\n    model: codestral-latest\n    # apiBase: https://codestral.mistral.ai/v1  # Do this if you are using a Codestral API key\n    roles:\n      - autocomplete\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"tabAutocompleteModel\": {\n    \"title\": \"Codestral\",\n    \"provider\": \"mistral\",\n    \"model\": \"codestral-latest\"\n    // \"apiBase\": \"https://codestral.mistral.ai/v1\"  // Do this if you are using a Codestral API key\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSeek Coder 2 16B with Msty in JSON\nDESCRIPTION: This snippet demonstrates the configuration for the DeepSeek Coder 2 16B model using Msty as the provider in JSON format. It includes the model title, provider, and model identifier.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/chat.mdx#2025-04-19_snippet_23\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"DeepSeek Coder 2 16B\",\n      \"provider\": \"msty\",\n      \"model\": \"deepseek-coder-v2:16b\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Codestral for Autocomplete using YAML\nDESCRIPTION: YAML configuration for setting up Codestral as an autocomplete model in Continue. This requires a Mistral API key and specifies the model to use for autocomplete functionality.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/deep-dives/autocomplete.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Codestral\n    provider: mistral\n    model: codestral-latest\n    apiKey: <YOUR_CODESTRAL_API_KEY>\n    roles:\n      - autocomplete\n```\n\n----------------------------------------\n\nTITLE: Configuring Cloudflare Workers AI in JSON for Continue\nDESCRIPTION: JSON configuration for setting up Cloudflare Workers AI models in Continue. This snippet demonstrates configuration for two chat models (Llama 3 8B and DeepSeek Coder 6.7b Instruct) and one tab autocomplete model (DeepSeek 7b), including their respective API keys, model identifiers, and context parameters.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/cloudflare.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Llama 3 8B\",\n      \"provider\": \"cloudflare\",\n      \"accountId\": \"YOUR CLOUDFLARE ACCOUNT ID\",\n      \"apiKey\": \"YOUR CLOUDFLARE API KEY\",\n      \"contextLength\": 2400,\n      \"completionOptions\": {\n        \"maxTokens\": 500\n      },\n      \"model\": \"@cf/meta/llama-3-8b-instruct\"\n    },\n    {\n      \"title\": \"DeepSeek Coder 6.7b Instruct\",\n      \"provider\": \"cloudflare\",\n      \"accountId\": \"YOUR CLOUDFLARE ACCOUNT ID\",\n      \"apiKey\": \"YOUR CLOUDFLARE API KEY\",\n      \"contextLength\": 2400,\n      \"completionOptions\": {\n        \"maxTokens\": 500\n      },\n      \"model\": \"@hf/thebloke/deepseek-coder-6.7b-instruct-awq\"\n    }\n  ],\n  \"tabAutocompleteModel\": {\n    \"title\": \"DeepSeek 7b\",\n    \"provider\": \"cloudflare\",\n    \"accountId\": \"YOUR CLOUDFLARE ACCOUNT ID\",\n    \"apiKey\": \"YOUR CLOUDFLARE API KEY\",\n    \"model\": \"@hf/thebloke/deepseek-coder-6.7b-base-awq\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Together API in YAML format\nDESCRIPTION: YAML configuration for integrating the Together API with Qwen2.5 Coder model into the Continue project. Requires adding your Together API key to the configuration file.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/together.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Together Qwen2.5 Coder\n    provider: together\n    model: Qwen/Qwen2.5-Coder-32B-Instruct\n    apiKey: <YOUR_TOGETHER_API_KEY>\n```\n\n----------------------------------------\n\nTITLE: Custom Edit Prompt Template Configuration\nDESCRIPTION: Example of configuring a custom edit prompt template using Handlebars syntax with available template variables for code editing.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/edit.mdx#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: My Custom Edit Template\n    provider: openai\n    model: gpt-4o\n    promptTemplates:\n      edit: |\n        `Here is the code before editing:\n        \\`\\`\\`{{{language}}}\n        {{{codeToEdit}}}\n        \\`\\`\\`\n\n        Here is the edit requested:\n        \"{{{userInput}}}\"\n\n        Here is the code after editing:`\n```\n\n----------------------------------------\n\nTITLE: HTTP Context Provider POST Parameters\nDESCRIPTION: Format of the POST parameters sent by the HTTP context provider to the configured endpoint. Includes the query string and full input text.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_59\n\nLANGUAGE: js\nCODE:\n```\n{\n  query: string,\n  fullInput: string\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Nebius AI Chat Model in YAML\nDESCRIPTION: This snippet shows how to configure the DeepSeek R1 chat model from Nebius AI Studio using YAML. It specifies the model name, provider, model identifier, and requires the user's Nebius API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/nebius.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: DeepSeek R1\n    provider: nebius\n    model: deepseek-ai/DeepSeek-R1\n    apiKey: <YOUR_NEBIUS_API_KEY>\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Kindo Model Integration\nDESCRIPTION: Configuration example for setting up Claude 3.5 Sonnet model through Kindo. Requires a Kindo API key and specifies the model provider and name.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/kindo.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Claude 3.5 Sonnet\n    provider: kindo\n    model: claude-3-5-sonnet\n    apiKey: <YOUR_KINDO_API_KEY>\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Claude 3.5 Sonnet\",\n      \"provider\": \"kindo\",\n      \"model\": \"claude-3-5-sonnet\",\n      \"apiKey\": \"<YOUR_KINDO_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Remote Ollama Instance in JSON\nDESCRIPTION: This JSON configuration demonstrates how to set up a remote instance of Ollama for the Llama3.1 8B model. It includes the apiBase property within the model object to specify the remote endpoint.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/ollama.mdx#2025-04-19_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Llama3.1 8B\",\n      \"provider\": \"ollama\",\n      \"model\": \"llama3.1:8b\",\n      \"apiBase\": \"http://<my endpoint>:11434\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Ollama Model with Custom Headers in JSON\nDESCRIPTION: Example JSON configuration for setting up an Ollama model with custom authentication headers in Continue's config.json file.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/tutorials/how-to-self-host-a-model.mdx#2025-04-19_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Ollama\",\n      \"provider\": \"ollama\",\n      \"model\": \"llama2-7b\",\n      \"requestOptions\": {\n        \"headers\": {\n          \"X-Auth-Token\": \"xxx\"\n        }\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Qwen2.5-Coder-32B-Instruct as Chat Model in YAML\nDESCRIPTION: YAML configuration for setting up Qwen2.5-Coder-32B-Instruct from Scaleway as a chat model in Continue. Requires a Scaleway API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/scaleway.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Qwen2.5-Coder-32B-Instruct\n    provider: scaleway\n    model: qwen2.5-coder-32b-instruct\n    apiKey: <YOUR_SCALEWAY_API_KEY>\n```\n\n----------------------------------------\n\nTITLE: Reranker Configuration\nDESCRIPTION: Example configuration for the reranker model used in response ranking, showing setup for Voyage reranker.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/i18n/zh-CN/docusaurus-plugin-content-docs/current/reference.md#2025-04-19_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"reranker\": {\n    \"name\": \"voyage\",\n    \"params\": {\n      \"model\": \"rerank-2\",\n      \"apiKey\": \"<VOYAGE_API_KEY>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Msty Llama 3.1 8B in YAML for Local Use\nDESCRIPTION: YAML configuration for using Llama 3.1 8B through Msty as a local chat model in Continue. Requires Msty installation on your machine.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/chat.mdx#2025-04-19_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Llama 3.1 8B\n    provider: msty\n    model: llama3.1:8b\n```\n\n----------------------------------------\n\nTITLE: Installing Qwen2.5 Coder Model with Ollama\nDESCRIPTION: Bash command to download and install the Qwen2.5 Coder 1.5B model using Ollama for local autocomplete functionality in Continue.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/deep-dives/autocomplete.mdx#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nollama run qwen2.5-coder:1.5b\n```\n\n----------------------------------------\n\nTITLE: Configuring Nebius AI Embeddings Model in YAML\nDESCRIPTION: This snippet illustrates how to set up the BAAI Embedder model for embeddings using YAML. It specifies the model name, provider, model identifier, API key, and assigns the 'embed' role to the model.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/nebius.mdx#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: BAAI Embedder\n    provider: nebius\n    model: BAAI/bge-en-icl\n    apiKey: <YOUR_NEBIUS_API_KEY>\n    roles:\n      - embed\n```\n\n----------------------------------------\n\nTITLE: Configuring xAI Grok 2 Chat Model in YAML\nDESCRIPTION: YAML configuration for integrating the xAI Grok 2 chat model in Continue. This configuration requires an API key from the xAI console and specifies the model as 'grok-2-latest'.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/xAI.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Grok 2\n    provider: xAI\n    model: grok-2-latest\n    apiKey: <YOUR_XAI_API_KEY>\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepInfra Models in Continue using JSON\nDESCRIPTION: JSON configuration example for setting up a DeepInfra model in Continue. It specifies the provider as deepinfra, sets a title, selects the Mixtral-8x7B-Instruct model, and requires your DeepInfra API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/deepinfra.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"provider\": \"deepinfra\",\n      \"title\": \"DeepInfra\",\n      \"model\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n      \"apiKey\": \"<YOUR_DEEP_INFRA_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Nomic Embed Text as Embeddings Model in YAML\nDESCRIPTION: This YAML configuration sets up Nomic Embed Text as the embeddings model using Ollama provider. It specifies the model name, provider, model, and assigns it the embed role.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/ollama.mdx#2025-04-19_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Nomic Embed Text\n    provider: ollama\n    model: nomic-embed-text\n    roles:\n      - embed\n```\n\n----------------------------------------\n\nTITLE: Configuring Embeddings Model with Function Network\nDESCRIPTION: Configuration for setting up BGE Base En embeddings model with Function Network. Includes API key configuration and model specification for embedding functionality.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/function-network.mdx#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: BGE Base En\n    provider: function-network\n    model: baai/bge-base-en-v1.5\n    apiKey: <YOUR_FUNCTION_NETWORK_API_KEY>\n    roles:\n      - embed\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"embeddingsProvider\": {\n    \"provider\": \"function-network\",\n    \"model\": \"baai/bge-base-en-v1.5\",\n    \"apiKey\": \"<YOUR_FUNCTION_NETWORK_API_KEY>\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Cerebras Llama 3.1 70B in YAML\nDESCRIPTION: YAML configuration for setting up Cerebras Llama 3.1 70B model in Continue. Requires specifying the model name, provider, model identifier, and API key from the Cerebras cloud portal.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/cerebras.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Cerebras Llama 3.1 70B\n    provider: cerebras\n    model: llama3.1-70b\n    apiKey:  <YOUR_CEREBRAS_API_KEY>\n```\n\n----------------------------------------\n\nTITLE: Configuring Claude 3.5 Sonnet Chat Model in JSON\nDESCRIPTION: JSON configuration for setting up Claude 3.5 Sonnet as a chat model in Continue. Specifies the model title, provider, model ID, and AWS region and profile.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/bedrock.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Claude 3.5 Sonnet\", \n      \"provider\": \"bedrock\",\n      \"model\": \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n      \"region\": \"us-east-1\",\n      \"profile\": \"bedrock\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Novita AI Model in YAML for Continue\nDESCRIPTION: This YAML configuration snippet sets up the Llama 3.1 8B model from Novita AI in the Continue project. It specifies the model name, provider, model identifier, and requires the user's Novita API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/novita.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Llama 3.1 8B\n    provider: novita\n    model: meta-llama/llama-3.1-8b-instruct\n    apiKey: <YOUR_NOVITA_API_KEY>\n```\n\n----------------------------------------\n\nTITLE: Configuring Web Context Provider in JSON\nDESCRIPTION: JSON configuration example for adding the @Web context provider with a parameter to limit the number of results returned.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"web\",\n      \"params\": {\n        \"n\": 5\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Azure OpenAI Service Configuration Example\nDESCRIPTION: Example configuration for connecting to Azure OpenAI Service with specific deployment parameters, including API base URL, version, and deployment name.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/azure.mdx#2025-04-19_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: GPT-4o Azure\n    model: gpt-4o\n    provider: openai\n    apiBase: https://just-an-example.openai.azure.com\n    apiKey: <YOUR_AZURE_API_KEY>\n    env:\n      apiVersion: 2023-03-15-preview\n      deployment: gpt-4o-july\n      apiType: azure-openai\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"title\": \"GPT-4o Azure\",\n  \"model\": \"gpt-4o\",\n  \"provider\": \"openai\",\n  \"apiBase\": \"https://just-an-example.openai.azure.com\",\n  \"deployment\": \"gpt-4o-july\",\n  \"apiVersion\": \"2023-03-15-preview\",\n  \"apiKey\": \"<YOUR_AZURE_API_KEY>\",\n  \"apiType\": \"azure-openai\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring File Context Provider in YAML\nDESCRIPTION: Configuration example for adding the @File context provider, which allows referencing any file in the current workspace.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: file\n```\n\n----------------------------------------\n\nTITLE: Configuring Ollama Model with Custom Headers in YAML\nDESCRIPTION: Example YAML configuration for setting up an Ollama model with custom authentication headers in Continue's config.yaml file.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/tutorials/how-to-self-host-a-model.mdx#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Ollama\n    provider: ollama\n    model: llama2-7b\n    requestOptions:\n      headers:\n        X-Auth-Token: xxx\n```\n\n----------------------------------------\n\nTITLE: Configuring Cohere Reranking Model\nDESCRIPTION: Configuration setup for Cohere's rerank-english-v3.0 model for reranking purposes. Specifies the reranking role and requires a Cohere API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/cohere.mdx#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Cohere Reranker\n    provider: cohere\n    model: rerank-english-v3.0\n    apiKey: <YOUR_COHERE_API_KEY>\n    roles:\n      - rerank\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"reranker\": {\n    \"name\": \"cohere\",\n    \"params\": {\n      \"model\": \"rerank-english-v3.0\",\n      \"apiKey\": \"<YOUR_COHERE_API_KEY>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HuggingFace Inference API in YAML\nDESCRIPTION: YAML configuration for setting up a HuggingFace Inference API endpoint in Continue. Requires specifying the model name, API key, and inference endpoint URL.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/huggingfaceinferenceapi.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Hugging Face Inference API\n    provider: huggingface-inference-api\n    model: MODEL_NAME\n    apiKey: <YOUR_HF_TOKEN>\n    apiBase: <YOUR_HF_INFERENCE_API_ENDPOINT_URL>\n```\n\n----------------------------------------\n\nTITLE: Configuring Novita AI Model in JSON for Continue\nDESCRIPTION: This JSON configuration snippet sets up the Llama 3.1 8B model from Novita AI in the Continue project. It specifies the model title, provider, model identifier, and requires the user's Novita API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/novita.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Llama 3.1 8B\",\n      \"provider\": \"novita\",\n      \"model\": \"meta-llama/llama-3.1-8b-instruct\",\n      \"apiKey\": \"<YOUR_NOVITA_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenAI-compatible Server in YAML\nDESCRIPTION: YAML configuration for using an OpenAI-compatible server or API with a custom base URL. Requires specifying the model name and API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/openai.mdx#2025-04-19_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: OpenAI-compatible server / API\n    provider: openai\n    model: MODEL_NAME \n    apiBase: http://localhost:8000/v1\n    apiKey: <YOUR_CUSTOM_API_KEY>\n```\n\n----------------------------------------\n\nTITLE: Configuring watsonx Embeddings Model in JSON\nDESCRIPTION: JSON configuration for setting up a watsonx embeddings model in Continue. It includes the model details and API information.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/watsonx.mdx#2025-04-19_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"embeddingsProvider\": {\n    \"provider\": \"watsonx\",\n    \"model\": \"ibm/slate-30m-english-rtrvr-v2\",\n    \"apiBase\": \"watsonx endpoint e.g. https://us-south.ml.cloud.ibm.com\",\n    \"projectId\": \"PROJECT_ID\",\n    \"apiKey\": \"API_KEY/ZENAPI_KEY/USERNAME:PASSWORD\",\n    \"apiVersion\": \"2024-03-14\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Cohere Reranking Model in JSON\nDESCRIPTION: JSON configuration for setting up Cohere Rerank V3.5 as a reranking model in Continue. Specifies the reranker name and parameters including model ID and AWS region.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/bedrock.mdx#2025-04-19_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"reranker\": {\n    \"name\": \"bedrock\",\n    \"params\": {\n      \"model\": \"cohere.rerank-v3-5:0\",\n      \"region\": \"us-west-2\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Flowise as an LLM Provider in YAML\nDESCRIPTION: YAML configuration for integrating Flowise with Continue. The configuration specifies the provider type, model name, and API base URL that Continue will use to connect to Flowise.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/flowise.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Flowise\n    provider: flowise\n    model: <MODEL>\n    apiBase: <API_BASE>\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Slash Command in JSON\nDESCRIPTION: Demonstrates how to set up a custom /http slash command in config.json. This command allows for integration with a custom HTTP endpoint for custom functionality.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/json-reference.md#2025-04-19_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"slashCommands\": [\n    {\n      \"name\": \"http\",\n      \"description\": \"Does something custom\",\n      \"params\": { \"url\": \"<my server endpoint>\" }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Codebase Context Provider in YAML\nDESCRIPTION: Configuration example for adding the @Codebase context provider, which allows referencing the most relevant snippets from the codebase.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: codebase\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSeek Coder 2 16B with Ollama in JSON\nDESCRIPTION: This snippet illustrates the configuration for the DeepSeek Coder 2 16B model using Ollama as the provider in JSON format. It includes the model title, provider, model identifier, and API base URL.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/chat.mdx#2025-04-19_snippet_21\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"DeepSeek Coder 2 16B\",\n      \"provider\": \"ollama\",\n      \"model\": \"deepseek-coder-v2:16b\",\n      \"apiBase\": \"http://localhost:11434\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Database Context Provider in JSON\nDESCRIPTION: JSON configuration for the database context provider, supporting multiple database types. Includes connection details for Postgres, MSSQL, and SQLite databases.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_41\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"database\",\n      \"params\": {\n        \"connections\": [\n          {\n            \"name\": \"examplePostgres\",\n            \"connection_type\": \"postgres\", \n            \"connection\": {\n              \"user\": \"username\",\n              \"host\": \"localhost\",\n              \"database\": \"exampleDB\",\n              \"password\": \"yourPassword\",\n              \"port\": 5432\n            }\n          },\n          {\n            \"name\": \"exampleMssql\",\n            \"connection_type\": \"mssql\",\n            \"connection\": {\n              \"user\": \"username\",\n              \"server\": \"localhost\",\n              \"database\": \"exampleDB\",\n              \"password\": \"yourPassword\"\n            }\n          },\n          {\n            \"name\": \"exampleSqlite\",\n            \"connection_type\": \"sqlite\",\n            \"connection\": {\n              \"filename\": \"/path/to/your/sqlite/database.db\"\n            }\n          }\n        ]\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Continue to use remote IPEX-LLM Ollama backend in YAML\nDESCRIPTION: YAML configuration example for setting up Continue to connect to a remote IPEX-LLM accelerated Ollama provider. This includes the apiBase parameter to specify the remote service IP address and port.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/ipex_llm.mdx#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: IPEX-LLM\n    provider: ollama\n    model: AUTODETECT\n    apiBase: http://your-ollama-service-ip:11434\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS SageMaker Models in JSON\nDESCRIPTION: JSON configuration for setting up AWS SageMaker models. This example configures a deepseek-coder model and specifies an mxbai-embed model as the embedding provider, with endpoint names and region settings.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/sagemaker.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"deepseek-6.7b-instruct\",\n      \"provider\": \"sagemaker\",\n      \"model\": \"lmi-model-deepseek-coder-xxxxxxx\",\n      \"region\": \"us-west-2\"\n    }\n  ],\n  \"embeddingsProvider\": {\n    \"provider\": \"sagemaker\",\n    \"model\": \"mxbai-embed-large-v1-endpoint\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Converting Models to ONNX Format\nDESCRIPTION: Command to convert and quantize a Hugging Face model to ONNX format using a Python script.\nSOURCE: https://github.com/continuedev/continue/blob/main/core/vendor/modules/@xenova/transformers/README.md#2025-04-19_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython -m scripts.convert --quantize --model_id <model_name_or_path>\n```\n\n----------------------------------------\n\nTITLE: Configuring Cerebras Llama 3.1 70B in JSON\nDESCRIPTION: JSON configuration for setting up Cerebras Llama 3.1 70B model in Continue. Requires specifying the model title, provider, model identifier, and API key from the Cerebras cloud portal.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/cerebras.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Cerebras Llama 3.1 70B\",\n      \"provider\": \"cerebras\",\n      \"model\": \"llama3.1-70b\",\n      \"apiKey\": \"<YOUR_CEREBRAS_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Onboard Slash Command in JSON\nDESCRIPTION: Illustrates the setup of the /onboard slash command in config.json. This command helps users familiarize themselves with a new project by analyzing its structure and components.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/json-reference.md#2025-04-19_snippet_13\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"slashCommands\": [\n    {\n      \"name\": \"onboard\",\n      \"description\": \"Familiarize yourself with the codebase\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Reranking Configuration for @Docs Provider\nDESCRIPTION: Advanced configuration for controlling document retrieval and reranking behavior in the @Docs provider.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/deep-dives/docs.mdx#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: docs\n    params:\n      nRetrieve: 25\n      nFinal: 5\n      useReranking: true\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"docs\",\n      \"params\": {\n        \"nRetrieve\": 25,\n        \"nFinal\": 5,\n        \"useReranking\": true\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Groq for Llama 3.3\nDESCRIPTION: Configuration settings for using Llama 3.3 70b Versatile model with Groq provider in Continue. Requires Groq API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/tutorials/llama3.1.mdx#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Llama 3.3 70b Versatile\n    provider: groq\n    model: llama-3.3-70b-versatile\n    apiKey: <YOUR_GROQ_API_KEY>\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n \"models\": [\n   {\n     \"title\": \"Llama 3.3 70b Versatile\",\n     \"provider\": \"groq\",\n     \"model\": \"llama-3.3-70b-versatile\",\n     \"apiKey\": \"<YOUR_GROQ_API_KEY>\"\n   }\n ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Msty Model Settings\nDESCRIPTION: Basic configuration for setting up Msty LLM provider with the DeepSeek Coder model. Shows the minimal required configuration including model name, provider, and model identifier.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/msty.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Msty\n    provider: msty\n    model: deepseek-coder:6.7b\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Msty\",\n      \"provider\": \"msty\",\n      \"model\": \"deepseek-coder:6.7b\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Custom Model Block with User Inputs in Continue YAML\nDESCRIPTION: This example shows how to create a custom model block that accepts user inputs, including API keys and temperature settings, using mustache templating.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/reference.md#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nname: myprofile/custom-model\nmodels:\n  - name: My Favorite Model\n    provider: anthropic\n    apiKey: ${{ inputs.ANTHROPIC_API_KEY }}\n    defaultCompletionOptions:\n      temperature: ${{ inputs.TEMP }}\n```\n\n----------------------------------------\n\nTITLE: Configuring a Custom Slash Command Using Natural Language in JSON\nDESCRIPTION: Example of adding a custom slash command named 'check' to config.json. This command uses a templated prompt to analyze code for mistakes, errors, vulnerabilities, and other issues, with handlebars syntax for input templating.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/tutorials/build-your-own-slash-command.md#2025-04-19_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"customCommands\": [\n    {\n      \"name\": \"check\",\n      \"description\": \"Check for mistakes in my code\",\n      \"prompt\": \"{{{ input }}}\\n\\nPlease read the highlighted code and check for any mistakes. You should look for the following, and be extremely vigilant:\\n- Syntax errors\\n- Logic errors\\n- Security vulnerabilities\\n- Performance issues\\n- Anything else that looks wrong\\n\\nOnce you find an error, please explain it as clearly as possible, but without using extra words. For example, instead of saying 'I think there is a syntax error on line 5', you should say 'Syntax error on line 5'. Give your answer as one bullet point per mistake found.\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Local Prompt File Example\nDESCRIPTION: A sample .prompt file that uses the current file context provider, showing the format with name, description, and content sections.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/deep-dives/prompts.md#2025-04-19_snippet_3\n\nLANGUAGE: prompt\nCODE:\n```\nname: Current file prompt\ndescription: A test prompt using the current file context provider\n---\n@currentFile\n```\n\n----------------------------------------\n\nTITLE: Configuring ReplicateLLM with YAML\nDESCRIPTION: YAML configuration example for setting up a Replicate-hosted CodeLLama model in the Continue development environment. This requires your Replicate API key and specifies CodeLLama-13B as the model to use.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/replicatellm.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Replicate CodeLLama\n    provider: replicate\n    model: codellama-13b\n    apiKey: <YOUR_REPLICATE_API_KEY>\n```\n\n----------------------------------------\n\nTITLE: Configuring Remote Ollama Instance in YAML\nDESCRIPTION: This YAML configuration demonstrates how to set up a remote instance of Ollama for the Llama3.1 8B model. It includes the apiBase property to specify the remote endpoint.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/ollama.mdx#2025-04-19_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Llama3.1 8B\n    provider: ollama\n    model: llama3.1:8b\n    apiBase: http://<my endpoint>:11434\n```\n\n----------------------------------------\n\nTITLE: Configuring Context Providers in JSON Format for Continue\nDESCRIPTION: Shows the JSON configuration for context providers including docs, codebase, and diff.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/yaml-migration.md#2025-04-19_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"docs\"\n    },\n    {\n      \"name\": \"codebase\",\n      \"params\": {\n        \"nRetrieve\": 30,\n        \"nFinal\": 3\n      }\n    },\n    {\n      \"name\": \"diff\",\n      \"params\": {}\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring GitLab Merge Request Context Provider in JSON\nDESCRIPTION: JSON configuration for the GitLab merge request context provider. Includes a GitLab personal access token for accessing merge request information.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_47\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"gitlab-mr\", \n      \"params\": {\n        \"token\": \"...\"\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Context Provider in YAML\nDESCRIPTION: YAML configuration for setting up an HTTP context provider with custom parameters including URL, title, and options.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/tutorials/build-your-own-context-provider.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  provider: http\n  params:\n    url: https://myserver.com/context-provider\n    title: http\n    displayTitle: My Custom Context\n    description: Gets things from my private list\n    options:\n      maxItems: 20\n```\n\n----------------------------------------\n\nTITLE: Configuring watsonx Model with Deployment ID in JSON\nDESCRIPTION: JSON configuration for a watsonx model using a custom deployment endpoint. It specifies the deployment ID along with other required parameters.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/watsonx.mdx#2025-04-19_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"model\": \"model ID\",\n      \"title\": \"watsonx - Model Name\",\n      \"provider\": \"watsonx\",\n      \"apiBase\": \"watsonx endpoint e.g. https://us-south.ml.cloud.ibm.com\",\n      \"apiKey\": \"API_KEY/ZENAPI_KEY/USERNAME:PASSWORD\",\n      \"apiVersion\": \"2024-03-14\",\n      \"deploymentId\": \"DEPLOYMENT_ID\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Enabling Claude Prompt Caching in JSON Configuration\nDESCRIPTION: JSON configuration example showing how to enable Anthropic prompt caching for system messages and conversation history. This optimizes performance by caching these components.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/anthropic.mdx#2025-04-19_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"cacheBehavior\": {\n        \"cacheSystemMessage\": true,\n        \"cacheConversation\": true\n      },\n      \"title\": \"Anthropic\",\n      \"provider\": \"anthropic\",\n      \"model\": \"claude-3-5-sonnet-latest\",\n      \"apiKey\": \"<YOUR_ANTHROPIC_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Rules in config.yaml\nDESCRIPTION: Example of how to define rules in the config.yaml file, including using external rule blocks, simple text rules, and named rules with detailed content.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/deep-dives/rules.md#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nrules:\n  - uses: myprofile/my-mood-setter\n    with:\n      TONE: consise\n  - Always annotate Python functions with their parameter and return types\n  - Always write Google style docstrings for functions and classes\n  - name: Server-side components\n    rule: When writing Next.js React components, use server-side components where possible instead of client components.\n```\n\n----------------------------------------\n\nTITLE: Configuring SambaNova Cloud with JSON\nDESCRIPTION: JSON configuration for setting up SambaNova Qwen2.5 coder model access. Requires a valid SambaNova API key and specifies the model title, provider, and model version.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/SambaNova.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"SambaNova Qwen2.5 coder\",\n      \"provider\": \"sambanova\",\n      \"model\": \"qwen2.5-coder-32b\",\n      \"apiKey\": \"<YOUR_SAMBANOVA_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Hugging Face TEI Reranker in YAML\nDESCRIPTION: YAML configuration for setting up a Hugging Face Text Embeddings Inference reranker endpoint in Continue. Requires specifying the API base URL and key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/reranking.mdx#2025-04-19_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Huggingface-tei Reranker\n    provider: huggingface-tei\n    apiBase: http://localhost:8080\n    apiKey: <YOUR_TEI_API_KEY>\n    roles:\n      - rerank\n```\n\n----------------------------------------\n\nTITLE: Configuring Embeddings for LM Studio in YAML\nDESCRIPTION: YAML configuration for setting up a Nomic Embed Text embeddings model through LM Studio. This configuration specifies the model should be used specifically for embeddings with the 'embed' role.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/lmstudio.mdx#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Nomic Embed Text\n    provider: lmstudio\n    model: nomic-ai/nomic-embed-text-v1.5-GGUF\n    roles:\n      - embed\n```\n\n----------------------------------------\n\nTITLE: Configuring Jira Datacenter Support in YAML\nDESCRIPTION: YAML configuration for supporting Jira Datacenter by setting the API version to 2. By default, the provider uses API version 3 for Jira Cloud.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_52\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: jira\n    params:\n      apiVersion: \"2\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Repository Map Context Provider in YAML\nDESCRIPTION: Sets up the repository map context provider with the option to exclude signatures. This provider generates a list of files and call signatures for top-level classes, functions, and methods in the codebase.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_32\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: repo-map\n    params:\n      includeSignatures: false # default true\n```\n\n----------------------------------------\n\nTITLE: Configuring Qwen2.5-Coder 1.5B as Autocomplete Model in JSON\nDESCRIPTION: This JSON configuration sets up Qwen2.5-Coder 1.5B as the autocomplete model using Ollama provider. It specifies the model title, provider, and model within the tabAutocompleteModel object.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/ollama.mdx#2025-04-19_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"tabAutocompleteModel\": {\n    \"title\": \"Qwen2.5-Coder 1.5B\",\n    \"provider\": \"ollama\",\n    \"model\": \"qwen2.5-coder:1.5b-base\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Basic Rules Configuration Example in YAML\nDESCRIPTION: Example showing how to configure rules in Continue, including using preset rulesets and custom rules with names. Rules are used to guide LLM behavior in Chat, Edit, and Agent requests.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/reference.md#2025-04-19_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nrules:\n  - uses: myprofile/my-mood-setter\n    with:\n      TONE: concise\n  - Always annotate Python functions with their parameter and return types\n  - Always write Google style docstrings for functions and classes\n  - name: Server-side components\n    rule: When writing Next.js React components, use server-side components where possible instead of client components.\n```\n\n----------------------------------------\n\nTITLE: Adding Continue Core as a Dev Dependency in package.json\nDESCRIPTION: This snippet demonstrates how to add the Continue Core module as a development dependency in the package.json file.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/tutorials/build-your-own-context-provider.mdx#2025-04-19_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"devDependencies\": {\n    \"@continuedev/core\": \"^0.0.1\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Linking Changelogs in Markdown for Continue Project Extensions\nDESCRIPTION: This Markdown snippet lists links to separate changelog files for the VS Code and JetBrains extensions of the Continue project. It uses relative paths to reference the changelog files within the project structure.\nSOURCE: https://github.com/continuedev/continue/blob/main/CHANGELOG.md#2025-04-19_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n# Changelog\n\nSeparate changelogs are kept for each extension:\n\n- [VS Code Extension](./extensions/vscode/CHANGELOG.md)\n- [JetBrains Extension](./extensions/intellij/CHANGELOG.md)\n```\n\n----------------------------------------\n\nTITLE: Configuring LLM as Reranker in YAML\nDESCRIPTION: YAML configuration for using an existing LLM (GPT-4) as a reranker in Continue. This is less efficient and accurate than dedicated reranking models.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/reranking.mdx#2025-04-19_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: LLM Reranker\n    provider: openai\n    model: gpt-4o\n    roles:\n      - rerank\n```\n\n----------------------------------------\n\nTITLE: Configuring watsonx Embeddings Model in YAML\nDESCRIPTION: YAML configuration for setting up a watsonx embeddings model in Continue. It specifies the model, provider, API details, and assigns the 'embed' role.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/watsonx.mdx#2025-04-19_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Watsonx Embedder\n    provider: watsonx\n    model: ibm/slate-30m-english-rtrvr-v2\n    apiBase: https://us-south.ml.cloud.ibm.com\n    projectId: PROJECT_ID\n    apiKey: API_KEY/ZENAPI_KEY/USERNAME:PASSWORD\n    apiVersion: 2024-03-14\n    roles:\n      - embed\n```\n\n----------------------------------------\n\nTITLE: Configuring Cohere Embeddings Model\nDESCRIPTION: Configuration setup for Cohere's embed-english-v3.0 embeddings model. Specifies the embedding role and requires a Cohere API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/cohere.mdx#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Cohere Embed\n    provider: cohere\n    model: embed-english-v3.0\n    apiKey: <YOUR_COHERE_API_KEY>\n    roles:\n      - embed\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"embeddingsProvider\": {\n    \"provider\": \"cohere\",\n    \"model\": \"embed-english-v3.0\",\n    \"apiKey\": \"<YOUR_COHERE_API_KEY>\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Certificates in JSON Config for Continue\nDESCRIPTION: This JSON configuration snippet illustrates how to add custom certificate paths to the Continue config file. It shows the structure for setting the 'caBundlePath' under 'requestOptions' for a model in the 'models' array.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/troubleshooting.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"My Model\",\n      ...\n      \"requestOptions\": {\n        \"caBundlePath\": \"/path/to/cert.pem\"\n      }\n    }\n  ],\n}\n```\n\n----------------------------------------\n\nTITLE: Forcing Legacy Completions Endpoint in YAML\nDESCRIPTION: YAML configuration to force usage of the completions endpoint instead of chat/completions when using an OpenAI-compatible server.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/openai.mdx#2025-04-19_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: OpenAI-compatible server / API\n    provider: openai\n    model: MODEL_NAME \n    apiBase: http://localhost:8000/v1\n    useLegacyCompletionsEndpoint: true\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSeek Coder 2 16B with Ollama in YAML\nDESCRIPTION: This snippet shows how to configure the DeepSeek Coder 2 16B model using Ollama as the provider in YAML format. It specifies the model name, provider, and the specific model identifier.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/chat.mdx#2025-04-19_snippet_18\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: DeepSeek Coder 2 16B\n    provider: ollama\n    model: deepseek-coder-v2:16b\n```\n\n----------------------------------------\n\nTITLE: Custom Prompts Configuration in YAML\nDESCRIPTION: Configuration for custom prompts that can be invoked from the chat window. Each prompt includes a name, description, and prompt text.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/reference.md#2025-04-19_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nprompts:\n  - name: check\n    description: Check for mistakes in my code\n    prompt: |\n      Please read the highlighted code and check for any mistakes. You should look for the following, and be extremely vigilant:\n        - Syntax errors\n        - Logic errors\n        - Security vulnerabilities\n```\n\n----------------------------------------\n\nTITLE: Configuring Edit Role in YAML\nDESCRIPTION: Example of setting up a Claude 3.5 Sonnet model with edit role capabilities using YAML configuration.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/edit.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Claude 3.5 Sonnet\n    provider: anthropic\n    model: claude-3-5-sonnet-latest\n    apiKey: <YOUR_ANTHROPIC_API_KEY>\n    roles:\n      - edit\n```\n\n----------------------------------------\n\nTITLE: Configuring Repository Map Context Provider in JSON\nDESCRIPTION: JSON configuration for the repository map context provider, allowing the option to exclude signatures. This helps reduce context size for large codebases.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_33\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"repo-map\",\n      \"params\": {\n        \"includeSignatures\": false // default true\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Up AWS Credentials for SageMaker\nDESCRIPTION: Configuration for the AWS credentials file to authenticate with SageMaker. Shows how to set up access keys and optional session token under a dedicated \"sagemaker\" profile.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/sagemaker.mdx#2025-04-19_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n[sagemaker]\naws_access_key_id = abcdefg\naws_secret_access_key = hijklmno\naws_session_token = pqrstuvwxyz # Optional: means short term creds.\n```\n\n----------------------------------------\n\nTITLE: Configuring Titan Embeddings Model in JSON\nDESCRIPTION: JSON configuration for setting up Amazon Titan Embed Text V2 as an embeddings model in Continue. Specifies the embeddings provider with model ID and AWS region.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/bedrock.mdx#2025-04-19_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"embeddingsProvider\": {\n    \"title\": \"Embeddings Model\",\n    \"provider\": \"bedrock\",\n    \"model\": \"amazon.titan-embed-text-v2:0\",\n    \"region\": \"us-west-2\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Including Reference Files for Code Generation\nDESCRIPTION: A prompt that incorporates existing TypeORM files as context for generating new entity definitions based on requirements.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/deep-dives/prompts.md#2025-04-19_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n@src/db/dataSource.ts @src/db/entity/SampleEntity.ts\n\nUse these files to generate a new TypeORM entity based on the following requirements:\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Gemini 2.0 Flash in JSON\nDESCRIPTION: JSON configuration for using Gemini 2.0 Flash from Google as a chat model in Continue. Requires a Gemini API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/chat.mdx#2025-04-19_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Gemini 2.0 Flash\",\n      \"provider\": \"gemini\",\n      \"model\": \"gemini-2.0-flash\",\n      \"apiKey\": \"<YOUR_GEMINI_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Git Diff Context Provider in YAML\nDESCRIPTION: Configuration example for adding the @Git Diff context provider, which allows referencing all changes made to the current branch.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: diff\n```\n\n----------------------------------------\n\nTITLE: Configuring Voyage AI Rerank-2 Model in YAML\nDESCRIPTION: YAML configuration for setting up the Voyage AI rerank-2 model as a reranker in Continue. Requires a Voyage AI API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/reranking.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: My Voyage Reranker\n    provider: voyage\n    apiKey: <YOUR_VOYAGE_API_KEY>\n    model: rerank-2\n    roles:\n      - rerank\n```\n\n----------------------------------------\n\nTITLE: Configuring Nomic Embed Text Embeddings Model in YAML\nDESCRIPTION: YAML configuration for setting up Nomic Embed Text as an embeddings model in Continue. Assigns the 'embed' role to the model for generating text embeddings.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/vllm.mdx#2025-04-19_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: VLLM Nomad Embed Text \n    provider: vllm\n    model: nomic-ai/nomic-embed-text-v1\n    apiBase: http://<vllm embed endpoint>/v1\n    roles:\n      - embed\n```\n\n----------------------------------------\n\nTITLE: Configuring BGE-Multilingual-Gemma2 as Embeddings Model in YAML\nDESCRIPTION: YAML configuration for setting up BGE-Multilingual-Gemma2 from Scaleway as an embeddings model in Continue. Requires a Scaleway API key and assigns the 'embed' role to the model.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/scaleway.mdx#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: BGE Multilingual Gemma2\n    provider: scaleway\n    model: bge-multilingual-gemma2\n    apiKey: <YOUR_SCALEWAY_API_KEY>\n    roles: \n      - embed\n```\n\n----------------------------------------\n\nTITLE: Configuring watsonx Tab Auto Complete Model in YAML\nDESCRIPTION: YAML configuration for setting up a watsonx tab auto complete model in Continue. It specifies the model, provider, API details, and assigns the 'autocomplete' role.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/watsonx.mdx#2025-04-19_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Granite Code 8b\n    provider: watsonx\n    model: ibm/granite-8b-code-instruct\n    apiBase: watsonx endpoint e.g. https://us-south.ml.cloud.ibm.com\n    projectId: PROJECT_ID\n    apiKey: API_KEY/ZENAPI_KEY/USERNAME:PASSWORD\n    apiVersion: 2024-03-14\n    roles:\n      - autocomplete\n```\n\n----------------------------------------\n\nTITLE: Configuring Text Embedding-004 Model in JSON\nDESCRIPTION: JSON configuration for setting up the text-embedding-004 model as an embeddings provider in Continue.dev. Specifies the model as the embeddings provider with provider name, model ID, project ID, and region.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/vertexai.mdx#2025-04-19_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"embeddingsProvider\": {\n    \"provider\": \"vertexai\",\n    \"model\": \"text-embedding-004\",\n    \"projectId\": \"[PROJECT_ID]\",\n    \"region\": \"us-central1\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Referencing External Best Practices in a Prompt\nDESCRIPTION: A prompt that pulls in Redux best practices documentation and reviews the current file against these standards.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/deep-dives/prompts.md#2025-04-19_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n@https://redux.js.org/style-guide/\n@currentFile\n\nReview this code for adherence to Redux best practices.\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Context Provider in JSON\nDESCRIPTION: JSON configuration for setting up an HTTP context provider with custom parameters including URL, title, and options.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/tutorials/build-your-own-context-provider.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"http\",\n  \"params\": {\n    \"url\": \"https://myserver.com/context-provider\",\n    \"title\": \"http\",\n    \"description\": \"Custom HTTP Context Provider\",\n    \"displayTitle\": \"My Custom Context\",\n    \"options\": {\n      \"maxItems\": 20\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Search Context Provider in JSON\nDESCRIPTION: JSON configuration for the Google search context provider using Serper.dev API. An API key from Serper.dev is required to enable Google search functionality.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_45\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"google\",\n      \"params\": {\n        \"serperApiKey\": \"<YOUR_SERPER.DEV_API_KEY>\"\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Terminal Context Provider in JSON\nDESCRIPTION: JSON configuration example for adding the @Terminal context provider to reference terminal commands and outputs.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"terminal\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Context Providers in Continue Assistant YAML\nDESCRIPTION: This snippet shows how to configure various context providers in a Continue assistant, including file, code, codebase, docs, diff, HTTP, folder, and terminal providers.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/reference.md#2025-04-19_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: file\n  - provider: code\n  - provider: codebase\n    params:\n      nFinal: 10\n  - provider: docs\n  - provider: diff\n  - provider: http\n    name: Context Server 1\n    params:\n      url: \"https://api.example.com/server1\"\n  - provider: folder\n  - provider: terminal\n```\n\n----------------------------------------\n\nTITLE: Configuring Codestral Autocomplete Model in Azure AI Foundry\nDESCRIPTION: Configuration settings for implementing Codestral as an autocomplete model. Specifies the model endpoint, API key, and role designation.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/azure.mdx#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Codestral\n    provider: mistral\n    model: codestral-latest\n    apiBase: https://<YOUR_MODEL_NAME>.<YOUR_REGION>.models.ai.azure.com/v1\n    apiKey: <YOUR_AZURE_API_KEY>\n    roles:\n      - autocomplete\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"tabAutocompleteModel\": {\n    \"title\": \"Codestral\",\n    \"provider\": \"mistral\",\n    \"model\": \"codestral-latest\", \n    \"apiBase\": \"https://<YOUR_MODEL_NAME>.<YOUR_REGION>.models.ai.azure.com/v1\",\n    \"apiKey\": \"<YOUR_AZURE_API_KEY>\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Ollama for Llama 3.1\nDESCRIPTION: Configuration settings for using Llama 3.1 8b model with Ollama provider in Continue. Requires Ollama installation and local model setup.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/tutorials/llama3.1.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Llama 3.1 8b\n    provider: ollama\n    model: llama3.1-8b\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"models\": [\n     {\n       \"title\": \"Llama 3.1 8b\",\n       \"provider\": \"ollama\",\n       \"model\": \"llama3.1-8b\"\n     }\n   ]\n }\n```\n\n----------------------------------------\n\nTITLE: Configuring Qwen2.5-Coder 1.5B Autocomplete Model in JSON\nDESCRIPTION: JSON configuration for setting up Qwen2.5-Coder 1.5B as an autocomplete model in Continue using the tabAutocompleteModel field.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/vllm.mdx#2025-04-19_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"tabAutocompleteModel\": {\n     \"title\": \"Qwen2.5-Coder 1.5B\",\n     \"provider\": \"vllm\",\n     \"model\": \"Qwen/Qwen2.5-Coder-1.5B\",\n     \"apiBase\": \"http://<vllm autocomplete endpoint>/v1\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring BGE-Multilingual-Gemma2 as Embeddings Model in JSON\nDESCRIPTION: JSON configuration for setting up BGE-Multilingual-Gemma2 from Scaleway as an embeddings provider in Continue. Requires a Scaleway API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/scaleway.mdx#2025-04-19_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"embeddingsProvider\": {\n    \"provider\": \"scaleway\",\n    \"model\": \"bge-multilingual-gemma2\",\n    \"apiKey\": \"<YOUR_SCALEWAY_API_KEY>\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Llamafile Provider in JSON\nDESCRIPTION: Example configuration for setting up a Llamafile provider using JSON format. Shows how to specify the model title, provider type, and specific model (mistral-7b) in the config.json file.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/llamafile.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Llamafile\",\n      \"provider\": \"llamafile\",\n      \"model\": \"mistral-7b\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Nemotron-4-340B-Instruct as Chat Model in JSON\nDESCRIPTION: This JSON snippet demonstrates how to configure the Nemotron-4-340B-Instruct model from NVIDIA as the chat model in the config.json file. It specifies the model title, provider, model name, and requires an API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/i18n/zh-CN/docusaurus-plugin-content-docs/current/customize/model-providers/more/nvidia.md#2025-04-19_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Nemotron-4-340B-Instruct\",\n      \"provider\": \"nvidia\",\n      \"model\": \"nvidia-nemotron-4-340b-instruct\",\n      \"apiKey\": \"[API_KEY]\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Web Context Provider in YAML\nDESCRIPTION: Configuration example for adding the @Web context provider with a limit parameter, which allows referencing relevant pages from across the web.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: web\n    params:\n      n: 5\n```\n\n----------------------------------------\n\nTITLE: Configuring Documentation in JSON Format for Continue\nDESCRIPTION: Shows the JSON configuration for documentation sources.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/yaml-migration.md#2025-04-19_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"docs\": [\n    {\n      \"startUrl\": \"https://docs.nestjs.com/\",\n      \"title\": \"nest.js\"\n    },\n    {\n      \"startUrl\": \"https://mysite.com/docs/\",\n      \"title\": \"My site\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Current File Context Provider in YAML\nDESCRIPTION: Configuration example for adding the @Current File context provider, which allows referencing the currently open file.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: currentFile\n```\n\n----------------------------------------\n\nTITLE: Configuring Llamafile Provider in YAML\nDESCRIPTION: Example configuration for setting up a Llamafile provider using YAML format. Demonstrates how to specify the model name, provider type, and specific model (mistral-7b) in the config.yaml file.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/llamafile.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Llamafile\n    provider: llamafile\n    model: mistral-7b\n```\n\n----------------------------------------\n\nTITLE: Configuring LM Studio in Continue using JSON\nDESCRIPTION: JSON configuration for setting up LM Studio as a model provider in Continue. This enables the use of local language models like llama2-7b through the LM Studio interface.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/lmstudio.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"LM Studio\",\n      \"provider\": \"lmstudio\",\n      \"model\": \"llama2-7b\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Together Gemma 3 27B in JSON\nDESCRIPTION: JSON configuration for using Gemma 3 27B through Together as a chat model in Continue. Requires a Together API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/chat.mdx#2025-04-19_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Gemma 3 27B\",\n      \"provider\": \"together\",\n      \"model\": \"google/gemma-2-27b-it\",\n      \"apiKey\": \"<YOUR_TOGETHER_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring watsonx Tab Auto Complete Model in JSON\nDESCRIPTION: JSON configuration for setting up a watsonx tab auto complete model in Continue. It includes the model details, API information, and context length.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/watsonx.mdx#2025-04-19_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"tabAutocompleteModel\": {\n    \"model\": \"ibm/granite-8b-code-instruct\",\n    \"title\": \"Granite Code 8b\",\n    \"provider\": \"watsonx\",\n    \"apiBase\": \"watsonx endpoint e.g. https://us-south.ml.cloud.ibm.com\",\n    \"projectId\": \"PROJECT_ID\",\n    \"apiKey\": \"API_KEY/ZENAPI_KEY/USERNAME:PASSWORD\",\n    \"apiVersion\": \"2024-03-14\",\n    \"contextLength\": 4000\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Llama3.1 8B Chat Model in JSON\nDESCRIPTION: JSON configuration for setting up Llama3.1 8B as a chat model in Continue. Specifies the model provider as vLLM, the model path, and the API base URL for connecting to the vLLM server.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/vllm.mdx#2025-04-19_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Llama3.1 8B Instruct\",\n      \"provider\": \"vllm\",\n      \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n      \"apiBase\": \"http://<vllm chat endpoint>/v1\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring NVIDIA Retrieval QA Mistral 7B as Embeddings Model in JSON\nDESCRIPTION: This JSON snippet shows the configuration for using NVIDIA Retrieval QA Mistral 7B as the embeddings model in the config.json file. It specifies the provider, model name, and requires an API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/i18n/zh-CN/docusaurus-plugin-content-docs/current/customize/model-providers/more/nvidia.md#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"embeddingsProvider\": {\n    \"provider\": \"nvidia\",\n    \"model\": \"nvidia/nv-embedqa-mistral-7b-v2\",\n    \"apiKey\": \"[API_KEY]\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Problems Context Provider in YAML\nDESCRIPTION: Configuration example for adding the @Problems context provider, which allows referencing problems from the current file.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_28\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: problems\n```\n\n----------------------------------------\n\nTITLE: Configuring CMD Slash Command in JSON\nDESCRIPTION: Shows how to set up the /cmd slash command in config.json. This command generates a shell command from natural language and can automatically paste it into the terminal in VS Code.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/json-reference.md#2025-04-19_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"slashCommands\": [\n    {\n      \"name\": \"cmd\",\n      \"description\": \"Generate a shell command\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring TextGenWebUI in Continue using JSON\nDESCRIPTION: JSON configuration for connecting Continue to TextGenWebUI's OpenAI-compatible API. This configuration specifies the model provider details including the API base URL and model name.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/textgenwebui.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Text Generation WebUI\",\n      \"provider\": \"text-gen-webui\",\n      \"apiBase\": \"http://localhost:5000/v1\",\n      \"model\": \"MODEL_NAME\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Nomic Embed Text Embeddings Model in JSON\nDESCRIPTION: JSON configuration for setting up Nomic Embed Text as an embeddings model in Continue using the embeddingsProvider field.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/vllm.mdx#2025-04-19_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"embeddingsProvider\": {\n    \"provider\": \"vllm\",\n    \"model\": \"nomic-ai/nomic-embed-text-v1\",\n    \"apiBase\": \"http://<vllm embed endpoint>/v1\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Nomic Embed Text as Embeddings Model in JSON\nDESCRIPTION: This JSON configuration sets up Nomic Embed Text as the embeddings model using Ollama provider. It specifies the provider and model within the embeddingsProvider object.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/ollama.mdx#2025-04-19_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"embeddingsProvider\": {\n    \"provider\": \"ollama\",\n    \"model\": \"nomic-embed-text\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: HTTP Context Provider Response Format\nDESCRIPTION: Expected JSON response format for the HTTP context provider. The server should return either a single context item or an array of context items with name, description, and content fields.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_60\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"name\": \"\",\n    \"description\": \"\",\n    \"content\": \"\"\n  }\n]\n\n// OR\n{\n  \"name\": \"\",\n  \"description\": \"\",\n  \"content\": \"\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring nCompass in YAML\nDESCRIPTION: Example YAML configuration for setting up nCompass with the Gemma 3 Coder model. Requires replacing the placeholder with your personal nCompass API key obtained from the welcome screen after signup.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/ncompass.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Ncompass Gemma 3 Coder\n    provider: ncompass\n    model: google/gemma-3-27b-it\n    apiKey: <YOUR_NCOMPASS_API_KEY>\n```\n\n----------------------------------------\n\nTITLE: Configuring Continue to use local IPEX-LLM Ollama backend in JSON\nDESCRIPTION: JSON configuration example for setting up Continue to use a locally hosted IPEX-LLM accelerated Ollama provider. This basic configuration automatically detects the available model.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/ipex_llm.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"IPEX-LLM\",\n      \"provider\": \"ollama\",\n      \"model\": \"AUTODETECT\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Debugger Context Provider in YAML\nDESCRIPTION: Configuration example for adding the @Debugger context provider with stackDepth parameter, which allows referencing local variables in the debugger.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_30\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: debugger\n    params:\n      stackDepth: 3\n```\n\n----------------------------------------\n\nTITLE: YAML Configuration with Anchors\nDESCRIPTION: Example showing how to use YAML anchors to avoid property duplication in configuration, demonstrating reuse of model defaults across multiple model configurations.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/reference.md#2025-04-19_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\n%YAML 1.1\n---\nname: MyProject\nversion: 0.0.1\nschema: v1\n\nmodel_defaults: &model_defaults\n  provider: openai\n  apiKey: my-api-key\n  apiBase: https://api.example.com/llm\n\nmodels:\n  - name: mistral\n    <<: *model_defaults\n    model: mistral-7b-instruct\n    roles:\n      - chat\n      - edit\n\n  - name: qwen2.5-coder-7b-instruct\n    <<: *model_defaults\n    model: qwen2.5-coder-7b-instruct\n    roles:\n      - chat\n      - edit\n\n  - name: qwen2.5-coder-7b\n    <<: *model_defaults\n    model: qwen2.5-coder-7b\n    env:\n      useLegacyCompletionsEndpoint: false\n    roles:\n      - autocomplete\n```\n\n----------------------------------------\n\nTITLE: Configuring watsonx Chat Model in JSON\nDESCRIPTION: JSON configuration for setting up a watsonx chat model in Continue. It specifies the model ID, title, provider, API base, project ID, API key, and API version.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/watsonx.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n\t\"models\": [\n\t  {\n\t    \"model\": \"model ID\",\n\t    \"title\": \"watsonx - Model Name\",\n\t    \"provider\": \"watsonx\",\n\t    \"apiBase\": \"https://us-south.ml.cloud.ibm.com\",\n\t    \"projectId\": \"PROJECT_ID\",\n\t    \"apiKey\": \"API_KEY/ZENAPI_KEY/USERNAME:PASSWORD\",\n\t    \"apiVersion\": \"2024-03-14\"\n\t  }\n\t]\n}\n```\n\n----------------------------------------\n\nTITLE: Embeddings Provider Configuration\nDESCRIPTION: Configuration example for the embeddings model settings used for @Codebase and @docs functionality, showing OpenAI embeddings setup.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/i18n/zh-CN/docusaurus-plugin-content-docs/current/reference.md#2025-04-19_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"embeddingsProvider\": {\n    \"provider\": \"openai\",\n    \"model\": \"text-embedding-ada-002\",\n    \"apiKey\": \"<API_KEY>\",\n    \"maxEmbeddingChunkSize\": 256,\n    \"maxEmbeddingBatchSize\": 5\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Autocomplete Model with Function Network\nDESCRIPTION: Configuration for setting up Deepseek Coder 6.7b autocomplete model with Function Network. Includes API key configuration and model specification.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/function-network.mdx#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Deepseek Coder 6.7b\n    provider: function-network\n    model: thebloke/deepseek-coder-6.7b-base-awq\n    apiKey: <YOUR_FUNCTION_NETWORK_API_KEY>\n    roles:\n      - autocomplete\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"tabAutocompleteModel\": {\n    \"title\": \"Deepseek Coder 6.7b\",\n    \"provider\": \"function-network\",\n    \"model\": \"thebloke/deepseek-coder-6.7b-base-awq\",\n    \"apiKey\": \"<YOUR_FUNCTION_NETWORK_API_KEY>\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Jira Context Provider in YAML\nDESCRIPTION: YAML configuration for the Jira context provider, which allows referencing Jira issues in a conversation. Requires an Atlassian API token or email/password for authentication.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_50\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: jira\n    params:\n      domain: company.atlassian.net\n      token: ATATT...\n```\n\n----------------------------------------\n\nTITLE: Configuring Model Context Protocol Server in YAML\nDESCRIPTION: Sets up a Model Context Protocol server using a local SQLite database. This allows integration with the Model Context Protocol standard proposed by Anthropic.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_36\n\nLANGUAGE: yaml\nCODE:\n```\nmcpServers:\n  - name: My MCP Server\n    command: uvx\n    args:\n      - mcp-server-sqlite\n      - --db-path\n      - /Users/NAME/test.db\n```\n\n----------------------------------------\n\nTITLE: Configuring Code Context Provider in JSON\nDESCRIPTION: JSON configuration example for adding the @Code context provider to reference specific functions or classes.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"code\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Google Search Context Provider in YAML\nDESCRIPTION: Sets up the Google search context provider using the Serper.dev API. Requires an API key from Serper.dev to perform Google searches.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_44\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: google\n    params:\n      serperApiKey: <YOUR_SERPER.DEV_API_KEY>\n```\n\n----------------------------------------\n\nTITLE: Configuring Transformers.js Local Embedder\nDESCRIPTION: Configuration for setting up local embeddings using Transformers.js, which uses the all-MiniLM-L6-v2 model and runs entirely locally in VS Code.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/embeddings.mdx#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: default-transformers\n    provider: transformers.js\n    roles:\n      - embed\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"embeddingsProvider\": {\n    \"provider\": \"transformers.js\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Postgres Context Provider in YAML\nDESCRIPTION: Sets up the Postgres context provider to reference table schemas and sample rows from a Postgres database. Includes connection details and options for schema filtering and sample row count.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_42\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: postgres\n    params:\n      host: localhost\n      port: 5436\n      user: myuser\n      password: catsarecool\n      database: animals\n      schema: public\n      sampleRows: 3\n```\n\n----------------------------------------\n\nTITLE: Tab Autocomplete Options Configuration\nDESCRIPTION: Example of configuring tab autocompletion behavior settings including delay, token limits, and file exclusions.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/i18n/zh-CN/docusaurus-plugin-content-docs/current/reference.md#2025-04-19_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"tabAutocompleteOptions\": {\n    \"debounceDelay\": 500,\n    \"maxPromptTokens\": 1500,\n    \"disableInFiles\": [\"*.md\"]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Search Context Provider in JSON\nDESCRIPTION: JSON configuration example for adding the @Search context provider powered by ripgrep to search across the codebase.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_21\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"search\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: README Context Provider Implementation\nDESCRIPTION: Complete implementation of a submenu-type context provider that lists and provides access to README files in the workspace.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/tutorials/build-your-own-context-provider.mdx#2025-04-19_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nconst ReadMeContextProvider: CustomContextProvider = {\n  title: \"readme\",\n  displayTitle: \"README\",\n  description: \"Reference README.md files in your workspace\",\n  type: \"submenu\",\n\n  getContextItems: async (\n    query: string,\n    extras: ContextProviderExtras,\n  ): Promise<ContextItem[]> => {\n    const content = await extras.ide.readFile(query);\n    return [\n      {\n        name: getFolder(query),\n        description: getFolderAndBasename(query),\n        content,\n      },\n    ];\n  },\n\n  loadSubmenuItems: async (\n    args: LoadSubmenuItemsArgs,\n  ): Promise<ContextSubmenuItem[]> => {\n    const { ide } = args;\n\n    const workspaceDirs = await ide.getWorkspaceDirs();\n\n    const allFiles = await Promise.all(\n      workspaceDirs.map((dir) =>\n        ide.subprocess(`find ${dir} -name \"README.md\"`),\n      ),\n    );\n\n    const readmes = allFiles\n      .flatMap((mds) => mds[0].split(\"\\n\"))\n      .filter((file) => file.trim() !== \"\" && !file.includes(\"/node_modules/\"));\n\n    return readmes.map((filepath) => {\n      return {\n        id: filepath,\n        title: getFolder(filepath),\n        description: getFolderAndBasename(filepath),\n      };\n    });\n  },\n};\n```\n\n----------------------------------------\n\nTITLE: Configuring Cerebras for Llama 3.1\nDESCRIPTION: Configuration settings for using Llama 3.1 70B model with Cerebras Inference provider in Continue. Requires Cerebras account and API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/tutorials/llama3.1.mdx#2025-04-19_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Cerebras Llama 3.1 70B\n    provider: cerebras\n    model: llama3.1-70b\n    apiKey: <YOUR_CEREBRAS_API_KEY>\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n   {\n      \"title\": \"Cerebras Llama 3.1 70B\",\n      \"provider\": \"cerebras\",\n      \"model\": \"llama3.1-70b\",\n      \"apiKey\": \"<YOUR_CEREBRAS_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring File Context Provider in JSON\nDESCRIPTION: JSON configuration example for adding the @File context provider to reference any file in the current workspace.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"file\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Greptile Context Provider in JSON\nDESCRIPTION: JSON configuration for the Greptile context provider, which allows querying a Greptile index of the current repository/branch. Requires Greptile and GitHub tokens for authentication.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_64\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"greptile\",\n      \"params\": {\n        \"GreptileToken\": \"...\",\n        \"GithubToken\": \"...\"\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: JetBrains Minimal Setup\nDESCRIPTION: Minimal configuration example for JetBrains IDE with custom embeddings provider setup.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/deep-dives/docs.mdx#2025-04-19_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: LMStudio embedder\n    provider: lmstudio\n    model: nomic-ai/nomic-embed-text-v1.5-GGUF\n    roles:\n      - embed\ncontext:\n  - provider: docs\ndocs:\n  - title: Nest.js\n    startUrl: https://docs.nestjs.com/\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"docs\"\n    }\n  ],\n  \"docs\": [\n    {\n      \"title\": \"Nest.js\",\n      \"startUrl\": \"https://docs.nestjs.com/\"\n    }\n  ],\n  \"embeddingsProvider\": {\n    \"provider\": \"lmstudio\",\n    \"model\": \"nomic-ai/nomic-embed-text-v1.5-GGUF\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Context Provider in YAML\nDESCRIPTION: YAML configuration for the HTTP context provider, which makes POST requests to a specified URL to retrieve context information. The server must return a properly formatted response.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_57\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: http\n    params:\n      url: \"https://api.example.com/v1/users\"\n```\n\n----------------------------------------\n\nTITLE: Creating and Manipulating ListAssistants200ResponseInner Objects in Python\nDESCRIPTION: This snippet demonstrates how to create an instance of ListAssistants200ResponseInner from JSON, convert it to JSON, create a dict representation, and create an instance from a dict. It uses the openapi_client library to handle the ListAssistants200ResponseInner model.\nSOURCE: https://github.com/continuedev/continue/blob/main/packages/continue-sdk/clients/python/docs/ListAssistants200ResponseInner.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openapi_client.models.list_assistants200_response_inner import ListAssistants200ResponseInner\n\n# TODO update the JSON string below\njson = \"{}\"\n# create an instance of ListAssistants200ResponseInner from a JSON string\nlist_assistants200_response_inner_instance = ListAssistants200ResponseInner.from_json(json)\n# print the JSON string representation of the object\nprint(ListAssistants200ResponseInner.to_json())\n\n# convert the object into a dict\nlist_assistants200_response_inner_dict = list_assistants200_response_inner_instance.to_dict()\n# create an instance of ListAssistants200ResponseInner from a dict\nlist_assistants200_response_inner_from_dict = ListAssistants200ResponseInner.from_dict(list_assistants200_response_inner_dict)\n```\n\n----------------------------------------\n\nTITLE: Configuring Hugging Face TEI Reranker in JSON\nDESCRIPTION: JSON configuration for setting up a Hugging Face Text Embeddings Inference reranker endpoint in Continue. Requires specifying the API base URL and key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/reranking.mdx#2025-04-19_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"reranker\": {\n    \"name\": \"huggingface-tei\",\n    \"params\": {\n      \"apiBase\": \"http://localhost:8080\",\n      \"apiKey\": \"<YOUR_TEI_API_KEY>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring URL Context Provider in YAML\nDESCRIPTION: Configuration example for adding the @Url context provider, which allows referencing the markdown converted contents of a given URL.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_22\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: url\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple Slash Commands in JSON\nDESCRIPTION: Demonstrates how to configure multiple slash commands (commit, share, and cmd) in the config.json file simultaneously.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/json-reference.md#2025-04-19_snippet_14\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"slashCommands\": [\n    {\n      \"name\": \"commit\",\n      \"description\": \"Generate a commit message\"\n    },\n    {\n      \"name\": \"share\",\n      \"description\": \"Export this session as markdown\"\n    },\n    {\n      \"name\": \"cmd\",\n      \"description\": \"Generate a shell command\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Replicate for Llama 3.1\nDESCRIPTION: Configuration settings for using Llama 3.1 405b model with Replicate provider in Continue. Requires Replicate API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/tutorials/llama3.1.mdx#2025-04-19_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Llama 3.1 405b\n    provider: replicate\n    model: llama3.1-405b\n    apiKey: <YOUR_REPLICATE_API_KEY>\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"models\": [\n     {\n       \"title\": \"Llama 3.1 405b\",\n       \"provider\": \"replicate\",\n       \"model\": \"llama3.1-405b\",\n       \"apiKey\": \"<YOUR_REPLICATE_API_KEY>\"\n     }\n   ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring URL Context Provider in JSON\nDESCRIPTION: JSON configuration example for adding the @Url context provider to reference web content.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_23\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"url\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HTTP Context Provider in JSON\nDESCRIPTION: This snippet shows how to configure an HTTP context provider in the config.json file to use a custom server for context provision.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/i18n/zh-CN/docusaurus-plugin-content-docs/current/customize/tutorials/build-your-own-context-provider.md#2025-04-19_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"http\",\n  \"params\": {\n    \"url\": \"https://myserver.com/context-provider\",\n    \"title\": \"http\",\n    \"description\": \"Custom HTTP Context Provider\",\n    \"displayTitle\": \"My Custom Context\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Imported Bedrock Models in YAML\nDESCRIPTION: YAML configuration for using custom imported models through Bedrock. Includes the model ARN, AWS region, and profile for authentication.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/bedrock.mdx#2025-04-19_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: AWS Bedrock deepseek-coder-6.7b-instruct\n    provider: bedrockimport\n    model: deepseek-coder-6.7b-instruct\n    modelArn: arn:aws:bedrock:us-west-2:XXXXX:imported-model/XXXXXX\n    region: us-west-2\n    profile: bedrock\n```\n\n----------------------------------------\n\nTITLE: Configuring MCP Server in JSON for Continue.dev\nDESCRIPTION: This JSON configuration snippet shows how to set up an MCP server in the config.json file for Continue.dev. It defines the server under the experimental.modelContextProtocolServers array, specifying the transport type, command, and arguments.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/deep-dives/mcp.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"experimental\": {\n    \"modelContextProtocolServers\": [\n      {\n        \"transport\": {\n          \"type\": \"stdio\",\n          \"command\": \"uvx\",\n          \"args\": [\"mcp-server-sqlite\", \"--db-path\", \"/Users/NAME/test.db\"]\n        }  \n      }\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating and Manipulating ListAssistants401Response Instances in Python\nDESCRIPTION: This snippet demonstrates how to create an instance of ListAssistants401Response from JSON, convert it to JSON, create a dict representation, and create an instance from a dict. It uses the from_json, to_json, to_dict, and from_dict methods of the class.\nSOURCE: https://github.com/continuedev/continue/blob/main/packages/continue-sdk/clients/python/docs/ListAssistants401Response.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openapi_client.models.list_assistants401_response import ListAssistants401Response\n\n# TODO update the JSON string below\njson = \"{}\"\n# create an instance of ListAssistants401Response from a JSON string\nlist_assistants401_response_instance = ListAssistants401Response.from_json(json)\n# print the JSON string representation of the object\nprint(ListAssistants401Response.to_json())\n\n# convert the object into a dict\nlist_assistants401_response_dict = list_assistants401_response_instance.to_dict()\n# create an instance of ListAssistants401Response from a dict\nlist_assistants401_response_from_dict = ListAssistants401Response.from_dict(list_assistants401_response_dict)\n```\n\n----------------------------------------\n\nTITLE: Initializing YAML Config File for Continue\nDESCRIPTION: Creates a basic YAML configuration file with name and version for Continue.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/yaml-migration.md#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nname: my-configuration\nversion: 0.0.1\nschema: v1\n```\n\n----------------------------------------\n\nTITLE: Configuring Rules in YAML Format for Continue\nDESCRIPTION: Demonstrates the YAML configuration for rules, which replaces the system message.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/yaml-migration.md#2025-04-19_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nrules:\n  - Always give concise responses\n```\n\n----------------------------------------\n\nTITLE: Advanced Moonshot Configuration with Completion Options\nDESCRIPTION: Extended configuration example including completion options like temperature, topP, and maxTokens for fine-tuning model behavior.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/moonshot.mdx#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Moonshot-8K\n    provider: moonshot\n    model: moonshot-v1-8k\n    apiKey: <YOUR_MOONSHOT_API_KEY>\n    defaultCompletionOptions:\n      temperature: 0.7\n      topP: 0.95\n      maxTokens: 2048\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Moonshot-8K\",\n      \"provider\": \"moonshot\",\n      \"model\": \"moonshot-v1-8k\",\n      \"apiKey\": \"<YOUR_MOONSHOT_API_KEY>\",\n      \"completionOptions\": {\n        \"temperature\": 0.7,\n        \"topP\": 0.95,\n        \"maxTokens\": 2048\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Problems Context Provider in JSON\nDESCRIPTION: JSON configuration example for adding the @Problems context provider to reference issues in the current file.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_29\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"problems\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining CustomContextProvider Interface in TypeScript\nDESCRIPTION: This snippet defines the interface for creating custom context providers, including methods for retrieving context items and loading submenu items.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/i18n/zh-CN/docusaurus-plugin-content-docs/current/customize/tutorials/build-your-own-context-provider.md#2025-04-19_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\ninterface CustomContextProvider {\n  title: string;\n  displayTitle?: string;\n  description?: string;\n  renderInlineAs?: string;\n  type?: ContextProviderType;\n  getContextItems(\n    query: string,\n    extras: ContextProviderExtras,\n  ): Promise<ContextItem[]>;\n  loadSubmenuItems?: (\n    args: LoadSubmenuItemsArgs,\n  ) => Promise<ContextSubmenuItem[]>;\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Clipboard Context Provider in JSON\nDESCRIPTION: JSON configuration example for adding the @Clipboard context provider to reference clipboard history.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_25\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"clipboard\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Continue to Use LlamaCpp in JSON\nDESCRIPTION: JSON configuration example for ~/.continue/config.json that specifies the LlamaCpp model settings including title, provider, model name, and API base URL.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/llamacpp.mdx#2025-04-19_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Llama CPP\",\n      \"provider\": \"llama.cpp\",\n      \"model\": \"MODEL_NAME\",\n      \"apiBase\": \"http://localhost:8080\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Config Modification for Context Provider\nDESCRIPTION: Configuration modification function to add custom context provider to Continue config.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/tutorials/build-your-own-context-provider.mdx#2025-04-19_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nexport function modifyConfig(config: Config): Config {\n  if (!config.contextProviders) {\n    config.contextProviders = [];\n  }\n  config.contextProviders.push(RagContextProvider);\n  return config;\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Up Embeddings Provider in Continue JSON Configuration\nDESCRIPTION: This code example demonstrates how to configure the embeddings provider in the Continue config.json file. It shows setting up OpenAI as the provider with specific model, API key, and embedding chunk settings.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/json-reference.md#2025-04-19_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"embeddingsProvider\": {\n    \"provider\": \"openai\",\n    \"model\": \"text-embedding-ada-002\",\n    \"apiKey\": \"<API_KEY>\",\n    \"maxEmbeddingChunkSize\": 256,\n    \"maxEmbeddingBatchSize\": 5\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Certificates in YAML Config for Continue\nDESCRIPTION: This YAML configuration snippet shows how to add custom certificate paths to the Continue config file. It demonstrates setting the 'caBundlePath' under 'requestOptions' for a model in the 'models' array.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/troubleshooting.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: My Model\n    ...\n    requestOptions:\n      caBundlePath: /path/to/cert.pem\n```\n\n----------------------------------------\n\nTITLE: Documentation Site Configuration in YAML\nDESCRIPTION: Configuration for indexing documentation sites, including site name, start URL, and crawling parameters.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/reference.md#2025-04-19_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\ndocs:\n  - name: Continue\n    startUrl: https://docs.continue.dev/intro\n    favicon: https://docs.continue.dev/favicon.ico\n```\n\n----------------------------------------\n\nTITLE: Configuring Ollama Model with API Key in JSON\nDESCRIPTION: Example JSON configuration for setting up an Ollama model with basic API key authentication in Continue's config.json file.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/tutorials/how-to-self-host-a-model.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Ollama\",\n      \"provider\": \"ollama\",\n      \"model\": \"llama2-7b\",\n      \"apiKey\": \"<YOUR_CUSTOM_OLLAMA_SERVER_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: GitHub Token Configuration\nDESCRIPTION: Configuration for adding GitHub token to increase API rate limits for documentation indexing.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/deep-dives/docs.mdx#2025-04-19_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: docs\n    params:\n      githubToken: <GITHUB_TOKEN>\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"docs\",\n      \"params\": {\n        \"githubToken\": \"github_...\"\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: MCP Server Configuration in YAML\nDESCRIPTION: Configuration for Model Context Protocol servers, including server name, startup command, and environment variables.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/reference.md#2025-04-19_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nmcpServers:\n  - name: My MCP Server\n    command: uvx\n    args:\n      - mcp-server-sqlite\n      - --db-path\n      - /Users/NAME/test.db\n```\n\n----------------------------------------\n\nTITLE: Configuring Self-Hosted GitLab Merge Request Provider in JSON\nDESCRIPTION: JSON configuration for the GitLab merge request context provider with a self-hosted GitLab instance. Includes the custom domain and access token.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_49\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"gitlab-mr\",\n      \"params\": {\n        \"token\": \"...\",\n        \"domain\": \"gitlab.example.com\" \n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Running vLLM Server with Llama3.1 8B\nDESCRIPTION: Command to start vLLM's OpenAI-compatible server with Meta's Llama3.1 8B Instruct model. This allows you to run inference with the model using vLLM's optimization techniques.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/vllm.mdx#2025-04-19_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nvllm serve meta-llama/Meta-Llama-3.1-8B-Instruct\n```\n\n----------------------------------------\n\nTITLE: Complete API Usage Example\nDESCRIPTION: Full example demonstrating API client configuration, authentication setup, and making a request to list assistants endpoint. Includes error handling and response processing.\nSOURCE: https://github.com/continuedev/continue/blob/main/packages/continue-sdk/clients/python/README.md#2025-04-19_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport openapi_client\nfrom openapi_client.rest import ApiException\nfrom pprint import pprint\n\n# Defining the host is optional and defaults to https://api.continue.dev\n# See configuration.py for a list of all supported configuration parameters.\nconfiguration = openapi_client.Configuration(\n    host = \"https://api.continue.dev\"\n)\n\n# The client must configure the authentication and authorization parameters\n# in accordance with the API server security policy.\n# Examples for each auth method are provided below, use the example that\n# satisfies your auth use case.\n\n# Configure Bearer authorization: apiKeyAuth\nconfiguration = openapi_client.Configuration(\n    access_token = os.environ[\"BEARER_TOKEN\"]\n)\n\n\n# Enter a context with an instance of the API client\nwith openapi_client.ApiClient(configuration) as api_client:\n    # Create an instance of the API class\n    api_instance = openapi_client.DefaultApi(api_client)\n    always_use_proxy = 'always_use_proxy_example' # str | Whether to always use the Continue-managed proxy for model requests (optional)\n    organization_id = 'organization_id_example' # str | ID of the organization to scope assistants to. If not provided, personal assistants are returned. (optional)\n\n    try:\n        # List assistants for IDE\n        api_response = api_instance.list_assistants(always_use_proxy=always_use_proxy, organization_id=organization_id)\n        print(\"The response of DefaultApi->list_assistants:\\n\")\n        pprint(api_response)\n    except ApiException as e:\n        print(\"Exception when calling DefaultApi->list_assistants: %s\\n\" % e)\n```\n\n----------------------------------------\n\nTITLE: Configuring Legacy Completions for xAI in JSON\nDESCRIPTION: This snippet demonstrates how to configure the Grok Beta model to use the legacy completions endpoint instead of the chat/completions endpoint. It includes the 'useLegacyCompletionsEndpoint' flag set to false.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/i18n/zh-CN/docusaurus-plugin-content-docs/current/customize/model-providers/top-level/xai.md#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n  \"models\": [\n    {\n      \"title\": \"Grok Beta\",\n      \"provider\": \"xAI\",\n      \"model\": \"grok-beta\",\n      \"apiKey\": \"[API_KEY]\",\n      \"useLegacyCompletionsEndpoint\": false\n    }\n  ]\n```\n\n----------------------------------------\n\nTITLE: Documentation Sites Configuration\nDESCRIPTION: Configuration example for indexing documentation sites, showing setup for Continue documentation.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/i18n/zh-CN/docusaurus-plugin-content-docs/current/reference.md#2025-04-19_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n\"docs\": [\n    {\n    \"title\": \"Continue\",\n    \"startUrl\": \"https://docs.continue.dev/intro\",\n    \"faviconUrl\": \"https://docs.continue.dev/favicon.ico\",\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Configuring Self-Hosted GitLab Merge Request Provider in YAML\nDESCRIPTION: Sets up the GitLab merge request context provider for a self-hosted GitLab instance. Specifies the custom domain and includes the required access token.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_48\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: gitlab-mr\n    params:\n      token: \"...\"\n      domain: \"gitlab.example.com\"\n```\n\n----------------------------------------\n\nTITLE: Configuring MCP Servers in JSON Format for Continue\nDESCRIPTION: Shows the JSON configuration for Model Context Protocol (MCP) servers.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/yaml-migration.md#2025-04-19_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"experimental\": {\n    \"modelContextProtocolServers\": [\n      {\n        \"transport\": {\n          \"type\": \"stdio\",\n          \"command\": \"uvx\",\n          \"args\": [\"mcp-server-sqlite\", \"--db-path\", \"/Users/NAME/test.db\"],\n          \"env\": {\n            \"KEY\": \"<VALUE>\"\n          }\n        }\n      }\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Development Data Configuration in YAML\nDESCRIPTION: Configuration for development data destinations, including endpoints and schema settings.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/reference.md#2025-04-19_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\ndata:\n  - name: Local Data Bank\n    destination: file:///Users/dallin/Documents/code/continuedev/continue-extras/external-data\n    schema: 0.2.0\n    level: all\n  - name: My Private Company\n    destination: https://mycompany.com/ingest\n    schema: 0.2.0\n    level: noCode\n    events:\n      - autocomplete\n      - chatInteraction\n```\n\n----------------------------------------\n\nTITLE: Configuring Jira Context Provider in JSON\nDESCRIPTION: JSON configuration for the Jira context provider, which allows referencing Jira issues in a conversation. Requires an Atlassian API token or email/password for authentication.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_51\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"jira\", \n      \"params\": {\n        \"domain\": \"company.atlassian.net\",\n        \"token\": \"ATATT...\"\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Nemotron-4-340B-Instruct Chat Model in JSON\nDESCRIPTION: JSON configuration for setting up the Nemotron-4-340B-Instruct model as a chat model in Continue. Requires a valid NVIDIA API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/nvidia.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Nemotron-4-340B-Instruct\",\n      \"provider\": \"nvidia\",\n      \"model\": \"nvidia-nemotron-4-340b-instruct\",\n      \"apiKey\": \"<YOUR_NVIDIA_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Voyage AI Rerank-2 Model in JSON\nDESCRIPTION: JSON configuration for setting up the Voyage AI rerank-2 model as a reranker in Continue. Requires a Voyage AI API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/reranking.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"reranker\": {\n      \"name\": \"voyage\",\n      \"params\": {\n          \"model\": \"rerank-2\",\n          \"apiKey\": \"<YOUR_VOYAGE_API_KEY>\"\n      }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Claude 3.5 Sonnet Chat Model in Vertex AI\nDESCRIPTION: Configuration snippet for setting up Claude 3.5 Sonnet as the chat model through Vertex AI. Requires specifying the project ID and region (us-east5).\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/i18n/zh-CN/docusaurus-plugin-content-docs/current/customize/model-providers/top-level/vertexai.md#2025-04-19_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Claude 3.5 Sonnet\",\n      \"provider\": \"vertexai\",\n      \"model\": \"claude-3-5-sonnet-20240620\",\n      \"projectId\": \"[PROJECT_ID]\",\n      \"region\": \"us-east5\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Grok Beta Chat Model in JSON\nDESCRIPTION: This snippet shows how to configure the Grok Beta chat model from xAI in the config.json file. It requires an API key from the xAI console.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/i18n/zh-CN/docusaurus-plugin-content-docs/current/customize/model-providers/top-level/xai.md#2025-04-19_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Grok Beta\",\n      \"provider\": \"xAI\",\n      \"model\": \"grok-beta\",\n      \"apiKey\": \"[API_KEY]\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring MCP Server in YAML for Continue.dev\nDESCRIPTION: This YAML configuration snippet demonstrates how to set up an MCP server in the config.yaml file for Continue.dev. It specifies the server name, command, and arguments including the database path.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/deep-dives/mcp.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmcpServers:\n  - name: My MCP Server\n    command: uvx\n    args:\n      - mcp-server-sqlite\n      - --db-path\n      - /Users/NAME/test.db\n```\n\n----------------------------------------\n\nTITLE: Setting Up Documentation Sites in Continue JSON Configuration\nDESCRIPTION: This code example shows how to configure documentation sites for indexing in the Continue config.json file. It demonstrates setting up the Continue documentation with a specific start URL and favicon.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/json-reference.md#2025-04-19_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n\"docs\": [\n    {\n    \"title\": \"Continue\",\n    \"startUrl\": \"https://docs.continue.dev/intro\",\n    \"faviconUrl\": \"https://docs.continue.dev/favicon.ico\",\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Complete Configuration Example in YAML\nDESCRIPTION: A comprehensive example showing all possible configuration options in config.yaml including models, rules, prompts, context, MCP servers, and data settings.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/reference.md#2025-04-19_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nname: MyProject\nversion: 0.0.1\nschema: v1\n\nmodels:\n  - uses: anthropic/claude-3.5-sonnet\n    with:\n      ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}\n    override:\n      defaultCompletionOptions:\n        temperature: 0.8\n  - name: GPT-4\n    provider: openai\n    model: gpt-4\n    roles:\n      - chat\n      - edit\n    defaultCompletionOptions:\n      temperature: 0.5\n      maxTokens: 2000\n    requestOptions:\n      headers:\n        Authorization: Bearer YOUR_OPENAI_API_KEY\n\n  - name: Ollama Starcoder\n    provider: ollama\n    model: starcoder\n    roles:\n      - autocomplete\n    defaultCompletionOptions:\n      temperature: 0.3\n      stop:\n        - \"\\n\"\n\nrules:\n  - Give concise responses\n  - Always assume TypeScript rather than JavaScript\n\nprompts:\n  - name: test\n    description: Unit test a function\n    prompt: |\n      Please write a complete suite of unit tests for this function. You should use the Jest testing framework.  The tests should cover all possible edge cases and should be as thorough as possible.  You should also include a description of each test case.\n  - uses: myprofile/my-favorite-prompt\n\ncontext:\n  - provider: diff\n  - provider: file\n  - provider: codebase\n  - provider: code\n  - provider: docs\n    params:\n      startUrl: https://docs.example.com/introduction\n      rootUrl: https://docs.example.com\n      maxDepth: 3\n\nmcpServers:\n  - name: DevServer\n    command: npm\n    args:\n      - run\n      - dev\n    env:\n      PORT: \"3000\"\n\ndata:\n  - name: My Private Company\n    destination: https://mycompany.com/ingest\n    schema: 0.2.0\n    level: noCode\n    events:\n      - autocomplete\n      - chatInteraction\n```\n\n----------------------------------------\n\nTITLE: Configuring Debugger Context Provider in JSON\nDESCRIPTION: JSON configuration example for adding the @Debugger context provider with a parameter to set call stack depth. This feature is currently only available in VS Code.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_31\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"debugger\",\n      \"params\": {\n        \"stackDepth\": 3\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Ask Sage Model in JSON Format\nDESCRIPTION: JSON configuration for setting up the GPT-4 gov model from Ask Sage in Continue.Dev. Requires providing the model title, provider, model type, API base URL, and your Ask Sage API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/asksage.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"GPT-4 gov\",\n      \"provider\": \"askSage\",\n      \"model\": \"gpt4-gov\",\n      \"apiBase\": \"https://api.asksage.ai/server/\",\n      \"apiKey\": \"<YOUR_ASK_SAGE_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Dispatching Custom Events to JBCefBrowser in Kotlin\nDESCRIPTION: A coroutine-scoped function that dispatches custom events to a JetBrains CEF browser component. It handles JSON serialization of event data and executes JavaScript code asynchronously with error handling.\nSOURCE: https://github.com/continuedev/continue/blob/main/extensions/intellij/src/main/resources/continue_code/copiedCode.txt#2025-04-19_snippet_0\n\nLANGUAGE: Kotlin\nCODE:\n```\nprivate fun CoroutineScope.dispatchCustomEvent(\n        type: String,\n        data: Map<String, Any>,\n        webView: JBCefBrowser\n    ) {\n        launch(CoroutineExceptionHandler { _, exception ->\n            println(\"Failed to dispatch custom event: ${exception.message}\")\n        }) {\n            val gson = Gson()\n            val jsonData = gson.toJson(data)\n            val jsCode = buildJavaScript(type, jsonData)\n            webView.executeJavaScriptAsync(jsCode)\n        }\n    }\n```\n\n----------------------------------------\n\nTITLE: Configuring Codestral Autocomplete Model in Vertex AI\nDESCRIPTION: Configuration for setting up Codestral as the tab autocomplete model through Vertex AI. Requires project ID and uses us-central1 region.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/i18n/zh-CN/docusaurus-plugin-content-docs/current/customize/model-providers/top-level/vertexai.md#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"tabAutocompleteModel\": {\n    \"title\": \"Codestral (Vertex AI)\",\n    \"provider\": \"vertexai\",\n    \"model\": \"codestral\",\n    \"projectId\": \"[PROJECT_ID]\",\n    \"region\": \"us-central1\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Completion Options in Continue JSON Configuration\nDESCRIPTION: This snippet shows how to set completion options in the Continue config.json file. It demonstrates disabling streaming and setting a specific temperature for text generation.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/json-reference.md#2025-04-19_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"completionOptions\": {\n    \"stream\": false,\n    \"temperature\": 0.5\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Msty with Authentication\nDESCRIPTION: Extended configuration example showing how to add custom authentication headers to Msty requests. Demonstrates setting up bearer token authentication through requestOptions.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/msty.mdx#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Msty\n    provider: msty\n    model: deepseek-coder:6.7b\n    requestOptions:\n      headers:\n        Authorization: Bearer xxx\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Msty\",\n      \"provider\": \"msty\",\n      \"model\": \"deepseek-coder:6.7b\",\n      \"requestOptions\": {\n        \"headers\": {\n          \"Authorization\": \"Bearer xxx\"\n        }\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Referencing Config File Format for Continue Assistants\nDESCRIPTION: The config.yaml file format is used to define custom AI code assistants with properties like name and version, along with composable blocks such as models and rules. This configuration syncs with IDE extensions when logged into hub.continue.dev.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/hub/assistants/intro.md#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nconfig.yaml\n```\n\n----------------------------------------\n\nTITLE: Default Jira Issue Query in JQL\nDESCRIPTION: Default JQL query used by the Jira context provider to find issues. Returns unresolved issues assigned to the current user, ordered by most recently updated.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_54\n\nLANGUAGE: jql\nCODE:\n```\nassignee = currentUser() AND resolution = Unresolved order by updated DESC\n```\n\n----------------------------------------\n\nTITLE: Configuring Terminal Context Provider in YAML\nDESCRIPTION: Configuration example for adding the @Terminal context provider, which allows referencing the last command run in the IDE's terminal and its output.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: terminal\n```\n\n----------------------------------------\n\nTITLE: Implementing README Context Provider with Submenu in TypeScript\nDESCRIPTION: This example shows how to create a submenu-type context provider that lists and provides access to README files in the workspace.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/i18n/zh-CN/docusaurus-plugin-content-docs/current/customize/tutorials/build-your-own-context-provider.md#2025-04-19_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst ReadMeContextProvider: CustomContextProvider = {\n  title: \"readme\",\n  displayTitle: \"README\",\n  description: \"Reference README.md files in your workspace\",\n  type: \"submenu\",\n\n  getContextItems: async (\n    query: string,\n    extras: ContextProviderExtras,\n  ): Promise<ContextItem[]> => {\n    // 'query' is the filepath of the README selected from the dropdown\n    const content = await extras.ide.readFile(query);\n    return [\n      {\n        name: getFolder(query),\n        description: getFolderAndBasename(query),\n        content,\n      },\n    ];\n  },\n\n  loadSubmenuItems: async (\n    args: LoadSubmenuItemsArgs,\n  ): Promise<ContextSubmenuItem[]> => {\n    // Filter all workspace files for READMEs\n    const allFiles = await args.ide.listWorkspaceContents();\n    const readmes = allFiles.filter((filepath) =>\n      filepath.endsWith(\"README.md\"),\n    );\n\n    // Return the items that will be shown in the dropdown\n    return readmes.map((filepath) => {\n      return {\n        id: filepath,\n        title: getFolder(filepath),\n        description: getFolderAndBasename(filepath),\n      };\n    });\n  },\n};\n\nexport function modifyConfig(config: Config): Config {\n  if (!config.contextProviders) {\n    config.contextProviders = [];\n  }\n  config.contextProviders.push(ReadMeContextProvider);\n  return config;\n}\n\nfunction getFolder(path: string): string {\n  return path.split(/[\\/\\\\]/g).slice(-2)[0];\n}\n\nfunction getFolderAndBasename(path: string): string {\n  return path\n    .split(/[\\/\\\\]/g)\n    .slice(-2)\n    .join(\"/\");\n}\n```\n\n----------------------------------------\n\nTITLE: Continue Configuration for HTTP Context Provider\nDESCRIPTION: Configuration examples in YAML and JSON formats for setting up the HTTP context provider in Continue to connect with a custom RAG server.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/tutorials/custom-code-rag.mdx#2025-04-19_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: http\n    params:\n      url: https://myserver.com/retrieve\n      name: http\n      description: Custom HTTP Context Provider\n      displayTitle: My Custom Context\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"http\",\n      \"params\": {\n        \"url\": \"https://myserver.com/retrieve\",\n        \"title\": \"http\",\n        \"description\": \"Custom HTTP Context Provider\",\n        \"displayTitle\": \"My Custom Context\"\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Customizing UI Settings in Continue.dev\nDESCRIPTION: Configuration options for customizing the user interface, including code block toolbar position, font size, and markdown display settings.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/i18n/zh-CN/docusaurus-plugin-content-docs/current/reference.md#2025-04-19_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"ui\": {\n    \"codeBlockToolbarPosition\": \"bottom\",\n    \"fontSize\": 14,\n    \"displayRawMarkdown\": false\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Discord Context Provider in JSON\nDESCRIPTION: JSON configuration for the Discord context provider, which allows referencing messages from Discord channels. Requires a bot token and server/channel IDs.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_56\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"discord\",\n      \"params\": {\n        \"discordKey\": \"bot token\",\n        \"guildId\": \"1234567890\",\n        \"channels\": [\n          {\n            \"id\": \"123456\",\n            \"name\": \"example-channel\"\n          },\n          {\n            \"id\": \"678901\",\n            \"name\": \"example-channel-2\" \n          }\n        ]\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Share Slash Command in JSON\nDESCRIPTION: Demonstrates how to configure the /share slash command in the config.json file. This command generates a shareable markdown transcript of the current chat history.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/json-reference.md#2025-04-19_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"slashCommands\": [\n    {\n      \"name\": \"share\",\n      \"description\": \"Export the current chat session to markdown\",\n      \"params\": { \"outputDir\": \"~/.continue/session-transcripts\" }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Current File Context Provider in JSON\nDESCRIPTION: JSON configuration example for adding the @Current File context provider to reference the currently open file.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"currentFile\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Building the Continue Core Binary\nDESCRIPTION: This command runs the build script defined in package.json to create the Continue Core Binary.\nSOURCE: https://github.com/continuedev/continue/blob/main/binary/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm run build\n```\n\n----------------------------------------\n\nTITLE: Sorting and Reversing Arrays in Python\nDESCRIPTION: This snippet defines two functions: 'sort' uses Python's built-in sorted() function to sort an array, while 'reverse' uses slice notation to reverse an array. Both functions take an array as input and return a new array.\nSOURCE: https://github.com/continuedev/continue/blob/main/manual-testing-sandbox/example.ipynb#2025-04-19_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndef sort(array):\n    return sorted(array)\n\ndef reverse(array):\n    return array[::-1]\n```\n\n----------------------------------------\n\nTITLE: Using Sentiment Analysis Pipeline in Python and JavaScript\nDESCRIPTION: Demonstrates how to use the pipeline API for sentiment analysis in both Python and JavaScript. The JavaScript version uses async/await syntax.\nSOURCE: https://github.com/continuedev/continue/blob/main/core/vendor/modules/@xenova/transformers/README.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import pipeline\n\n# Allocate a pipeline for sentiment-analysis\npipe = pipeline('sentiment-analysis')\n\nout = pipe('I love transformers!')\n# [{'label': 'POSITIVE', 'score': 0.999806941}]\n```\n\nLANGUAGE: javascript\nCODE:\n```\nimport { pipeline } from '@xenova/transformers';\n\n// Allocate a pipeline for sentiment-analysis\nlet pipe = await pipeline('sentiment-analysis');\n\nlet out = await pipe('I love transformers!');\n// [{'label': 'POSITIVE', 'score': 0.999817686}]\n```\n\n----------------------------------------\n\nTITLE: Configuring Block Template Variables in YAML\nDESCRIPTION: Example showing how to use template variables for block inputs. The block can receive a secret API key through templating, where users can set values in the 'with' clause of their assistant configuration.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/hub/blocks/create-a-block.md#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n${{ inputs.API_KEY }}\n```\n\nLANGUAGE: yaml\nCODE:\n```\nAPI_KEY: ${{ secrets.MY_API_KEY }}\n```\n\n----------------------------------------\n\nTITLE: Configuring MCP Servers in YAML Format for Continue\nDESCRIPTION: Demonstrates the YAML configuration for Model Context Protocol (MCP) servers.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/yaml-migration.md#2025-04-19_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nmcpServers:\n  - name: My MCP Server\n    command: uvx\n    args:\n      - mcp-server-sqlite\n      - --db-path\n      - /Users/NAME/test.db\n    env:\n      KEY: <VALUE>\n```\n\n----------------------------------------\n\nTITLE: Configuring Experimental Features in JSON\nDESCRIPTION: Demonstrates the configuration of various experimental features in config.json, including model roles, quick actions, context menu prompts, and model context protocol servers.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/json-reference.md#2025-04-19_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"experimental\": {\n    \"modelRoles\": {\n      \"inlineEdit\": \"Edit Model\"\n    },\n    \"quickActions\": [\n      {\n        \"title\": \"Tags\",\n        \"prompt\": \"Return a list of any function and class names from the included code block\",\n        \"sendToChat\": true\n      }\n    ],\n    \"contextMenuPrompts\": {\n      \"fixGrammar\": \"Fix grammar in the above but allow for typos.\"\n    },\n    \"modelContextProtocolServers\": [\n      {\n        \"transport\": {\n          \"type\": \"stdio\",\n          \"command\": \"uvx\",\n          \"args\": [\"mcp-server-sqlite\", \"--db-path\", \"/Users/NAME/test.db\"]\n        }\n      }\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Commands in JSON\nDESCRIPTION: Shows how to set up custom commands in config.json. These commands create prompt shortcuts in the sidebar for quick access to common actions.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/json-reference.md#2025-04-19_snippet_15\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"customCommands\": [\n    {\n      \"name\": \"test\",\n      \"prompt\": \"Write a comprehensive set of unit tests for the selected code. It should setup, run tests that check for correctness including important edge cases, and teardown. Ensure that the tests are complete and sophisticated. Give the tests just as chat output, don't edit any file.\",\n      \"description\": \"Write unit tests for highlighted code\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Database Context Provider in YAML\nDESCRIPTION: Sets up the database context provider for multiple database types including Postgres, MSSQL, and SQLite. Each connection includes specific parameters for the database type.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_40\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: database\n    params:\n      connections:\n        - name: examplePostgres\n          connection_type: postgres\n          connection:\n            user: username\n            host: localhost\n            database: exampleDB\n            password: yourPassword\n            port: 5432\n        - name: exampleMssql \n          connection_type: mssql\n          connection:\n            user: username\n            server: localhost\n            database: exampleDB\n            password: yourPassword\n        - name: exampleSqlite\n          connection_type: sqlite\n          connection:\n            filename: /path/to/your/sqlite/database.db\n```\n\n----------------------------------------\n\nTITLE: Configuring Qwen2.5-Coder-32B-Instruct as Chat Model in JSON\nDESCRIPTION: JSON configuration for setting up Qwen2.5-Coder-32B-Instruct from Scaleway as a chat model in Continue. Requires a Scaleway API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/scaleway.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Qwen2.5-Coder-32B-Instruct\",\n      \"provider\": \"scaleway\",\n      \"model\": \"qwen2.5-coder-32b-instruct\",\n      \"apiKey\": \"<YOUR_SCALEWAY_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Importing OpenAPI Client Package\nDESCRIPTION: Python code to import the installed openapi_client package.\nSOURCE: https://github.com/continuedev/continue/blob/main/packages/continue-sdk/clients/python/README.md#2025-04-19_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport openapi_client\n```\n\n----------------------------------------\n\nTITLE: Documentation Site Configuration\nDESCRIPTION: Configuration for adding documentation sites to Continue with title, start URL, and favicon URL.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/deep-dives/docs.mdx#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ndocs:\n  - title: Nest.js\n    startUrl: https://docs.nestjs.com/\n    faviconUrl: https://docs.nestjs.com/favicon.ico\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"docs\": [\n    {\n      \"title\": \"Nest.js\",\n      \"startUrl\": \"https://docs.nestjs.com/\",\n      \"faviconUrl\": \"https://docs.nestjs.com/favicon.ico\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Kindo Tab Autocomplete\nDESCRIPTION: Configuration for setting up WhiteRabbitNeo model for tab autocomplete functionality. Includes specific model path and role configuration for autocomplete features.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/kindo.mdx#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: WhiteRabbitNeo\n    provider: kindo\n    model: /models/WhiteRabbitNeo-33B-DeepSeekCoder\n    apiKey: <YOUR_KINDO_API_KEY>\n    roles:\n      - autocomplete\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"tabAutocompleteModel\": [\n    {\n      \"title\": \"WhiteRabbitNeo\",\n      \"provider\": \"kindo\",\n      \"model\": \"/models/WhiteRabbitNeo-33B-DeepSeekCoder\",\n      \"apiKey\": \"<YOUR_KINDO_API_KEY>\",\n      \"template\": \"none\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring GitHub Action Environment Variables for Continue.dev\nDESCRIPTION: YAML configuration for setting the organization slug in the GitHub Action workflow. This environment variable is used to specify which organization the assistants and blocks should be published to on hub.continue.dev.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/hub/source-control.md#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nenv:\n  OWNER_SLUG: my-org-slug # <-- TODO\n```\n\n----------------------------------------\n\nTITLE: Configuring Prompts in YAML Format for Continue\nDESCRIPTION: Demonstrates the YAML configuration for prompts, which replaces custom commands.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/yaml-migration.md#2025-04-19_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nprompts:\n  - name: check\n    description: Check for mistakes in my code\n    prompt: |\n      Please read the highlighted code and check for any mistakes. You should look for the following, and be extremely vigilant:\n        - Syntax errors\n        - Logic errors\n        - Security vulnerabilities\n        - Performance issues\n        - Anything else that looks wrong\n\n      Once you find an error, please explain it as clearly as possible, but without using extra words. For example, instead of saying 'I think there is a syntax error on line 5', you should say 'Syntax error on line 5'. Give your answer as one bullet point per mistake found.\n```\n\n----------------------------------------\n\nTITLE: VS Code Minimal Setup\nDESCRIPTION: Minimal configuration example for VS Code using built-in embeddings provider without reranking.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/deep-dives/docs.mdx#2025-04-19_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: docs\n    \ndocs:\n  - title: Nest.js\n    startUrl: https://docs.nestjs.com/\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"docs\"\n    }\n  ],\n  \"docs\": [\n    {\n      \"title\": \"Nest.js\",\n      \"startUrl\": \"https://docs.nestjs.com/\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Context Providers in JSON\nDESCRIPTION: Illustrates the configuration of context providers in config.json. These providers define options available while typing in the chat, with customizable parameters.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/json-reference.md#2025-04-19_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"code\",\n      \"params\": {}\n    },\n    {\n      \"name\": \"docs\",\n      \"params\": {}\n    },\n    {\n      \"name\": \"diff\",\n      \"params\": {}\n    },\n    {\n      \"name\": \"open\",\n      \"params\": {}\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Tree Context Provider in YAML\nDESCRIPTION: Configuration example for adding the @Tree context provider, which allows referencing the structure of the current workspace.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_26\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: tree\n```\n\n----------------------------------------\n\nTITLE: Configuring Folder Context Provider in YAML\nDESCRIPTION: Configuration example for adding the @Folder context provider, which uses the same retrieval mechanism as @Codebase but only on a single folder.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_18\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: folder\n```\n\n----------------------------------------\n\nTITLE: Building JetBrains Extension Plugin on Windows Systems\nDESCRIPTION: Gradle command to build the JetBrains extension plugin on Windows systems.\nSOURCE: https://github.com/continuedev/continue/blob/main/extensions/intellij/CONTRIBUTING.md#2025-04-19_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew.bat buildPlugin\n```\n\n----------------------------------------\n\nTITLE: Implementing RAG Context Provider in TypeScript\nDESCRIPTION: This example shows how to create a custom context provider that retrieves snippets from a vector database of internal documents using a REST API.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/i18n/zh-CN/docusaurus-plugin-content-docs/current/customize/tutorials/build-your-own-context-provider.md#2025-04-19_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nconst RagContextProvider: CustomContextProvider = {\n  title: \"rag\",\n  displayTitle: \"RAG\",\n  description:\n    \"Retrieve snippets from our vector database of internal documents\",\n\n  getContextItems: async (\n    query: string,\n    extras: ContextProviderExtras,\n  ): Promise<ContextItem[]> => {\n    const response = await fetch(\"https://internal_rag_server.com/retrieve\", {\n      method: \"POST\",\n      body: JSON.stringify({ query }),\n    });\n\n    const results = await response.json();\n\n    return results.map((result) => ({\n      name: result.title,\n      description: result.title,\n      content: result.contents,\n    }));\n  },\n};\n```\n\n----------------------------------------\n\nTITLE: Installing Continue Core Module via npm\nDESCRIPTION: This command installs the Continue Core module as a dependency for the project using npm.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/tutorials/build-your-own-context-provider.mdx#2025-04-19_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @continuedev/core\n```\n\n----------------------------------------\n\nTITLE: Configuring Experimental Features in Continue.dev\nDESCRIPTION: Configuration for experimental features including model roles, quick actions, context menu prompts, and documentation crawling settings.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/i18n/zh-CN/docusaurus-plugin-content-docs/current/reference.md#2025-04-19_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"experimental\": {\n    \"modelRoles\": {\n      \"inlineEdit\": \"Edit Model\"\n    },\n    \"promptPath\": \"custom/.prompts\",\n    \"quickActions\": [\n      {\n        \"title\": \"Tags\",\n        \"prompt\": \"Return a list of any function and class names from the included code block\",\n        \"sendToChat\": true\n      }\n    ],\n    \"contextMenuPrompts\": {\n      \"fixGrammar\": \"Fix grammar in the above but allow for typos.\"\n    },\n    \"readResponseTTS\": false,\n    \"useChromiumForDocsCrawling\": true\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: RAG Context Provider Implementation\nDESCRIPTION: Example implementation of a RAG (Retrieval Augmented Generation) context provider that queries an internal vector database.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/tutorials/build-your-own-context-provider.mdx#2025-04-19_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nconst RagContextProvider: CustomContextProvider = {\n  title: \"rag\",\n  displayTitle: \"RAG\",\n  description:\n    \"Retrieve snippets from our vector database of internal documents\",\n\n  getContextItems: async (\n    query: string,\n    extras: ContextProviderExtras,\n  ): Promise<ContextItem[]> => {\n    const response = await fetch(\"https://internal_rag_server.com/retrieve\", {\n      method: \"POST\",\n      body: JSON.stringify({ query }),\n    });\n\n    const results = await response.json();\n\n    return results.map((result) => ({\n      name: result.title,\n      description: result.title,\n      content: result.contents,\n    }));\n  },\n};\n```\n\n----------------------------------------\n\nTITLE: Configuring Tab Autocomplete Options in Continue JSON Configuration\nDESCRIPTION: This snippet illustrates how to set various options for tab autocompletion behavior in the Continue config.json file. It includes settings for debounce delay, maximum prompt tokens, and file-specific disabling.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/json-reference.md#2025-04-19_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"tabAutocompleteOptions\": {\n    \"debounceDelay\": 500,\n    \"maxPromptTokens\": 1500,\n    \"disableInFiles\": [\"*.md\"]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: AI Models List in Markdown\nDESCRIPTION: A formatted markdown list of AI models with their documentation links, associated papers, and authors. Each entry includes the model name, organization, paper title, and authors.\nSOURCE: https://github.com/continuedev/continue/blob/main/core/vendor/modules/@xenova/transformers/README.md#2025-04-19_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\n### Models\n\n1. **[ALBERT](https://huggingface.co/docs/transformers/model_doc/albert)** (from Google Research and the Toyota Technological Institute at Chicago) released with the paper [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942), by Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut.\n1. **[Audio Spectrogram Transformer](https://huggingface.co/docs/transformers/model_doc/audio-spectrogram-transformer)** (from MIT) released with the paper [AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778) by Yuan Gong, Yu-An Chung, James Glass.\n```\n\n----------------------------------------\n\nTITLE: Filtering Table of Contents for Changelog Sections\nDESCRIPTION: React/JSX code that filters and displays a table of contents showing only the VSCode and JetBrains changelog sections.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/changelog.mdx#2025-04-19_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\n<TOCInline\n  toc={toc.filter(\n    (node) =>\n      node.value === \"VSCode Changelog\" || node.value === \"JetBrains Changelog\",\n  )}\n/>\n```\n\n----------------------------------------\n\nTITLE: Running Quick E2E Tests\nDESCRIPTION: Command to run end-to-end tests when only test code has been modified, providing a faster execution path.\nSOURCE: https://github.com/continuedev/continue/blob/main/extensions/vscode/e2e/README.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm run e2e:quick\n```\n\n----------------------------------------\n\nTITLE: Configuring Codestral Autocomplete Model in JSON\nDESCRIPTION: JSON configuration for setting up Codestral as an autocomplete model in Continue.dev. Specifies the model as a tab autocomplete model with provider, model ID, project ID, and region.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/vertexai.mdx#2025-04-19_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"tabAutocompleteModel\": {\n      \"title\": \"Codestral (Vertex AI)\",\n      \"provider\": \"vertexai\",\n      \"model\": \"codestral\",\n      \"projectId\": \"[PROJECT_ID]\",\n      \"region\": \"us-central1\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Converting Tensor Output to Nested JavaScript Array\nDESCRIPTION: This snippet shows how to convert the Tensor output from the feature-extraction pipeline to a nested JavaScript array using the .tolist() method. This can be useful for further processing or visualization of the embeddings.\nSOURCE: https://github.com/continuedev/continue/blob/main/extensions/vscode/models/all-MiniLM-L6-v2/README.md#2025-04-19_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\nconsole.log(output.tolist());\n// [\n//   [ 0.04592696577310562, 0.07328180968761444, 0.05400655046105385, ... ],\n//   [ 0.08188057690858841, 0.10760223120450974, -0.013241755776107311, ... ]\n// ]\n```\n\n----------------------------------------\n\nTITLE: Completion Options Configuration\nDESCRIPTION: Example of setting completion parameters that control text generation behavior across all models.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/i18n/zh-CN/docusaurus-plugin-content-docs/current/reference.md#2025-04-19_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"completionOptions\": {\n    \"stream\": false,\n    \"temperature\": 0.5\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Context Providers in Continue.dev\nDESCRIPTION: Configuration for context providers that appear as options while typing in chat, including code, docs, diff, and open providers.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/i18n/zh-CN/docusaurus-plugin-content-docs/current/reference.md#2025-04-19_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"code\",\n      \"params\": {}\n    },\n    {\n      \"name\": \"docs\",\n      \"params\": {}\n    },\n    {\n      \"name\": \"diff\",\n      \"params\": {}\n    },\n    {\n      \"name\": \"open\",\n      \"params\": {}\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Ollama Model with Client Certificate in JSON\nDESCRIPTION: Example JSON configuration for setting up an Ollama model with client certificate authentication in Continue's config.json file.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/tutorials/how-to-self-host-a-model.mdx#2025-04-19_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Ollama\",\n      \"provider\": \"ollama\",\n      \"model\": \"llama2-7b\",\n      \"requestOptions\": {\n        \"clientCertificate\": {\n          \"cert\": \"C:\\\\tempollama.pem\",\n          \"key\": \"C:\\\\tempollama.key\",\n          \"passphrase\": \"c0nt!nu3\"\n        }\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Creating a Concise Response Rule in .continuerules\nDESCRIPTION: Example of a simple .continuerules file that instructs the model to provide concise answers and assume user knowledge of programming topics.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/deep-dives/rules.md#2025-04-19_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nPlease provide concise answers. Don't explain obvious concepts. You can assume that I am knowledgable about most programming topics.\n```\n\n----------------------------------------\n\nTITLE: Configuring Operating System Context Provider in YAML\nDESCRIPTION: Sets up the operating system context provider to reference the architecture and platform of the current operating system.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_34\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: os\n```\n\n----------------------------------------\n\nTITLE: Configuring Msty Llama 3.1 8B in JSON for Local Use\nDESCRIPTION: JSON configuration for using Llama 3.1 8B through Msty as a local chat model in Continue. Requires Msty installation on your machine.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/chat.mdx#2025-04-19_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Llama 3.1 8B\",\n      \"provider\": \"msty\",\n      \"model\": \"llama3.1-8b\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies on Unix Systems\nDESCRIPTION: Shell command to install project dependencies on Unix-based systems using a provided script.\nSOURCE: https://github.com/continuedev/continue/blob/main/extensions/intellij/CONTRIBUTING.md#2025-04-19_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n./scripts/install-dependencies.sh\n```\n\n----------------------------------------\n\nTITLE: Installing Transformers.js via NPM\nDESCRIPTION: This snippet shows how to install the Transformers.js library using NPM. This is a prerequisite for using the all-MiniLM-L6-v2 model for sentence embeddings.\nSOURCE: https://github.com/continuedev/continue/blob/main/extensions/vscode/models/all-MiniLM-L6-v2/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @xenova/transformers\n```\n\n----------------------------------------\n\nTITLE: Setting Tab Autocomplete Model in Continue JSON Configuration\nDESCRIPTION: This code snippet shows how to configure the tab autocomplete model in the Continue config.json file. It demonstrates setting up a custom Starcoder model using Ollama as the provider.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/json-reference.md#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"tabAutocompleteModel\": {\n    \"title\": \"My Starcoder\",\n    \"provider\": \"ollama\",\n    \"model\": \"starcoder2:3b\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Describing Plugin Features in Markdown\nDESCRIPTION: This snippet outlines the main features of the Continue plugin (Chat, Autocomplete, Edit, and Agent) using Markdown syntax. Each feature is briefly described with links to their respective documentation pages.\nSOURCE: https://github.com/continuedev/continue/blob/main/extensions/intellij/README.md#2025-04-19_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n<div align=\"center\">\n\n## Chat\n\n[Chat](https://continue.dev/docs/chat/how-to-use-it) makes it easy to ask for help from an LLM without needing to leave the IDE.\n\nYou send it a task, including any relevant information, and it replies with the text / code most likely to complete the task. If it does not give you what you want, then you can send follow up messages to clarify and adjust its approach until the task is completed.\n\n## Autocomplete\n\n[Autocomplete](https://continue.dev/docs/autocomplete/how-to-use-it) provides inline code suggestions as you type.\n\nTo enable it, simply click the \"Continue\" button in the status bar at the bottom right of your IDE or ensure the \"Enable Tab Autocomplete\" option is checked in your IDE settings.\n\n## Edit\n\n[Edit](https://continue.dev/docs/edit/how-to-use-it) is a convenient way to modify code without leaving your current file.\n\nHighlight a block of code, describe your code changes, and a diff will be streamed inline to your file which you can accept or reject.\n\n## Agent\n\n[Agent](https://continue.dev/docs/agent/how-to-use-it) enables you to make more substantial changes to your codebase\n\nAgent equips the Chat model with the tools needed to handle a wide range of coding tasks, allowing the model to make decisions and save you the work of manually finding context and performing actions.\n\n</div>\n```\n\n----------------------------------------\n\nTITLE: Using Custom Model for Sentiment Analysis in JavaScript\nDESCRIPTION: Shows how to use a specific model for sentiment analysis by providing the model ID to the pipeline function.\nSOURCE: https://github.com/continuedev/continue/blob/main/core/vendor/modules/@xenova/transformers/README.md#2025-04-19_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\n// Use a different model for sentiment-analysis\nlet pipe = await pipeline('sentiment-analysis', 'Xenova/bert-base-multilingual-uncased-sentiment');\n```\n\n----------------------------------------\n\nTITLE: Testing the Continue Core Binary\nDESCRIPTION: This command executes the test suite for the Continue Core Binary project.\nSOURCE: https://github.com/continuedev/continue/blob/main/binary/README.md#2025-04-19_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm run test\n```\n\n----------------------------------------\n\nTITLE: Configuring SambaNova for Llama 3.1\nDESCRIPTION: Configuration settings for using Llama 3.1 405B model with SambaNova Cloud provider in Continue. Requires SambaNova account and API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/tutorials/llama3.1.mdx#2025-04-19_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: SambaNova Llama 3.1 405B\n    provider: sambanova\n    model: llama3.1-405b\n    apiKey: <YOUR_SAMBA_API_KEY>\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"SambaNova Llama 3.1 405B\",\n      \"provider\": \"sambanova\",\n      \"model\": \"llama3.1-405b\",\n      \"apiKey\": \"<YOUR_SAMBA_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Running E2E Tests\nDESCRIPTION: Gradle command to execute the end-to-end tests for the JetBrains extension.\nSOURCE: https://github.com/continuedev/continue/blob/main/extensions/intellij/CONTRIBUTING.md#2025-04-19_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew test\n```\n\n----------------------------------------\n\nTITLE: Configuring Relace Fast Apply in JSON format\nDESCRIPTION: JSON configuration example for integrating Relace Fast Apply model in Continue. Specifies the model title, provider, model name, and API key.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/relace.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Relace Fast Apply\",\n      \"provider\": \"relace\",\n      \"model\": \"Fast-Apply\",\n      \"apiKey\": \"<YOUR_RELACE_API_KEY>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Customizing Transformers.js Settings\nDESCRIPTION: Shows how to customize various settings in Transformers.js, including model paths and WASM file locations.\nSOURCE: https://github.com/continuedev/continue/blob/main/core/vendor/modules/@xenova/transformers/README.md#2025-04-19_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\nimport { env } from '@xenova/transformers';\n\n// Specify a custom location for models (defaults to '/models/').\nenv.localModelPath = '/path/to/models/';\n\n// Disable the loading of remote models from the Hugging Face Hub:\nenv.allowRemoteModels = false;\n\n// Set location of .wasm files. Defaults to use a CDN.\nenv.backends.onnx.wasm.wasmPaths = '/path/to/files/';\n```\n\n----------------------------------------\n\nTITLE: Configuring watsonx Model with Advanced Options in JSON\nDESCRIPTION: JSON configuration for a watsonx model with advanced options including template, context length, and various generation parameters such as temperature, topP, and stop sequences.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/watsonx.mdx#2025-04-19_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"model\": \"ibm/granite-20b-code-instruct\",\n      \"title\": \"Granite Code 20b\",\n      \"provider\": \"watsonx\",\n      \"apiBase\": \"watsonx endpoint e.g. https://us-south.ml.cloud.ibm.com\",\n      \"projectId\": \"PROJECT_ID\",\n      \"apiKey\": \"API_KEY/ZENAPI_KEY/USERNAME:PASSWORD\",\n      \"apiVersion\": \"2024-03-14\",\n      \"template\": \"granite\",\n      \"contextLength\": 8000,\n      \"completionOptions\": {\n        \"temperature\": 0.1,\n        \"topP\": 0.3,\n        \"topK\": 20,\n        \"maxTokens\": 2000,\n        \"frequencyPenalty\": 1.1,\n        \"stop\": [\"Question:\", \"\\n\\n\\n\"]\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Published SDK Package\nDESCRIPTION: Command to install the published @continuedev/sdk package from npm.\nSOURCE: https://github.com/continuedev/continue/blob/main/packages/continue-sdk/clients/typescript/README.md#2025-04-19_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nnpm install @continuedev/sdk@0.0.1 --save\n```\n\n----------------------------------------\n\nTITLE: Configuring Open Files Context Provider in YAML\nDESCRIPTION: Configuration example for adding the @Open context provider with onlyPinned parameter, which allows referencing contents of open or pinned files.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: open\n    params:\n      onlyPinned: true\n```\n\n----------------------------------------\n\nTITLE: Configuring GitHub Issue Context Provider in JSON\nDESCRIPTION: JSON configuration for the GitHub issue context provider, allowing access to GitHub issue conversations. Includes repository details and a GitHub personal access token.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_39\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"issue\",\n      \"params\": {\n        \"repos\": [\n          {\n            \"owner\": \"continuedev\",\n            \"repo\": \"continue\"\n          }\n        ],\n        \"githubToken\": \"ghp_xxx\"\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Transformers.js in Vanilla JavaScript\nDESCRIPTION: Demonstrates how to import Transformers.js in vanilla JavaScript using ES Modules and a CDN.\nSOURCE: https://github.com/continuedev/continue/blob/main/core/vendor/modules/@xenova/transformers/README.md#2025-04-19_snippet_3\n\nLANGUAGE: html\nCODE:\n```\n<script type=\"module\">\n    import { pipeline } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.14.0';\n</script>\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies List\nDESCRIPTION: A structured list of Python package dependencies grouped by functionality, including core packages, web frameworks, data science libraries, and development tools. Each package is pinned to a specific version for environment consistency.\nSOURCE: https://github.com/continuedev/continue/blob/main/manual-testing-sandbox/requirements.txt#2025-04-19_snippet_0\n\nLANGUAGE: requirements.txt\nCODE:\n```\n# Core packages\nnumpy==1.21.2\npandas==1.3.3\nscipy==1.7.1\n\n# Web development\nflask==2.0.2\ndjango==3.2.7\n\n# Request handling\nrequests==2.26.0\nhttpx==0.19.0\n\n# Machine Learning\nscikit-learn==0.24.2\ntensorflow==2.6.0\n\n# Data visualization\nmatplotlib==3.4.3\nseaborn==0.11.2\nplotly==5.3.1\n\n# Natural Language Processing\nnltk==3.6.3\nspacy==3.1.2\n\n# Deep Learning\ntorch==2.6.0\nkeras==2.6.0\n\n# Image processing\npillow==8.3.2\nopencv-python==4.5.3.56\n\n# Data handling and manipulation\nbeautifulsoup4==4.10.0\nsqlalchemy==1.4.25\nxlrd==2.0.1\nopenpyxl==3.0.9\n\n# Utilities\npyyaml==5.4.1\npython-dotenv==0.19.0\n\n# Testing\npytest==6.2.5\n\n# Logging and Debugging\nloguru==0.5.3\n\n# Asynchronous Programming\naiohttp==3.7.4\n\n# Others\njupyter==1.0.0\ngunicorn==20.1.0\npsycopg2==2.9.1\n```\n\n----------------------------------------\n\nTITLE: Code Structure Indexing Implementation Reference\nDESCRIPTION: Documents the four main CodebaseIndex implementations used in the system: CodeSnippetsCodebaseIndex for code structure analysis, FullTextSearchCodebaseIndex for text searching, ChunkCodebaseIndex for recursive code chunking, and LanceDbCodebaseIndex for managing embeddings with vector database integration.\nSOURCE: https://github.com/continuedev/continue/blob/main/core/indexing/README.md#2025-04-19_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nCodeSnippetsCodebaseIndex // tree-sitter queries for functions, classes, and top-level code objects\nFullTextSearchCodebaseIndex // full-text search index using SQLite FTS5\nChunkCodebaseIndex // chunks files recursively by code structure\nLanceDbCodebaseIndex // calculates embeddings for chunks and stores in LanceDB vector database\n```\n\n----------------------------------------\n\nTITLE: Configuring Commit Slash Command in JSON\nDESCRIPTION: Illustrates the configuration of the /commit slash command in config.json. This command shows the current git diff to the LLM and asks it to generate a commit message.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/json-reference.md#2025-04-19_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"slashCommands\": [\n    {\n      \"name\": \"commit\",\n      \"description\": \"Generate a commit message for the current changes\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Autocompleting Vue method using props\nDESCRIPTION: This snippet shows how to autocomplete a method in a Vue component that uses props. The method sets the 'completed' prop to true.\nSOURCE: https://github.com/continuedev/continue/blob/main/core/autocomplete/filtering/test/NEGATIVE_TEST_CASES/QWEN_TYPESCRIPT.txt#2025-04-19_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nthis.completed\n```\n\n----------------------------------------\n\nTITLE: Autocompleting Vue computed property for fullName\nDESCRIPTION: This snippet demonstrates how to autocomplete a Vue computed property that concatenates firstName and lastName data properties to create a fullName.\nSOURCE: https://github.com/continuedev/continue/blob/main/core/autocomplete/filtering/test/NEGATIVE_TEST_CASES/QWEN_TYPESCRIPT.txt#2025-04-19_snippet_0\n\nLANGUAGE: javascript\nCODE:\n```\n() {\n      return this.firstName + ' ' + this.lastName;\n    }\n```\n\n----------------------------------------\n\nTITLE: Configuring Ollama Qwen Model for Autocomplete using JSON\nDESCRIPTION: JSON configuration for setting up the Qwen 1.5B model through Ollama as the tab autocomplete model in Continue. This allows for local code completion without external API dependencies.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/deep-dives/autocomplete.mdx#2025-04-19_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"tabAutocompleteModel\": {\n    \"title\": \"Qwen 1.5b Autocomplete Model\",\n    \"provider\": \"ollama\", \n    \"model\": \"qwen2.5-coder:1.5b\",\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Relace Fast Apply in YAML format\nDESCRIPTION: YAML configuration example for integrating Relace Fast Apply model in Continue. Includes model name, provider, key configuration, and role settings for the apply function.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/relace.mdx#2025-04-19_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Relace Fast Apply\n    provider: relace\n    model: Fast-Apply\n    apiKey: <YOUR_RELACE_API_KEY>\n    roles:\n      - apply\n    promptTemplates:\n      apply: \"{{{ new_code }}}\"\n```\n\n----------------------------------------\n\nTITLE: Adding Vue component import reference\nDESCRIPTION: This snippet shows how to complete a Vue component export by adding a components property that references the imported ChildComponent.\nSOURCE: https://github.com/continuedev/continue/blob/main/core/autocomplete/filtering/test/NEGATIVE_TEST_CASES/QWEN_TYPESCRIPT.txt#2025-04-19_snippet_5\n\nLANGUAGE: javascript\nCODE:\n```\nexport default {\n  components: {\n    ChildComponent,\n  },\n```\n\n----------------------------------------\n\nTITLE: React Tabs Component Implementation for Feature Overview\nDESCRIPTION: React-based tabbed interface showing Continue's main features. Uses Docusaurus components (Tabs, TabItem, Admonition) to organize content into interactive sections for Chat, Autocomplete, Edit, and Agent features.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/getting-started/overview.mdx#2025-04-19_snippet_0\n\nLANGUAGE: jsx\nCODE:\n```\n<Tabs>\n  <TabItem value=\"chat\" label=\"Chat\">\n\n    [Chat](../chat/how-to-use-it.md#how-to-use-it) makes it easy to ask for help from an LLM without needing to leave the IDE\n\n    ![chat](/img/chat.gif)\n\n    <Admonition type=\"info\" title=\"Learn more\" icon=\"\">\n        Learn more about [Chat](../chat/how-to-use-it.md#how-to-use-it)\n    </Admonition>\n\n  </TabItem>\n  <TabItem value=\"autocomplete\" label=\"Autocomplete\">\n\n    [Autocomplete](../autocomplete/how-to-use-it.md#how-to-use-it) provides inline code suggestions as you type\n\n    ![autocomplete](/img/autocomplete.gif)\n\n    <Admonition type=\"info\" title=\"Learn more\" icon=\"\">\n        Learn more about [Autocomplete](../autocomplete/how-to-use-it.md#how-to-use-it)\n    </Admonition>\n\n  </TabItem>\n\n  <TabItem value=\"edit\" label=\"Edit\">\n\n  [Edit](../edit/how-to-use-it.md#how-to-use-it) is a convenient way to modify code without leaving your current file\n\n  ![edit](/img/edit.gif)\n\n  <Admonition type=\"info\" title=\"Learn more\" icon=\"\">\n      Learn more about [Edit](../edit/how-to-use-it.md#how-to-use-it)\n  </Admonition>\n\n  </TabItem>\n\n  <TabItem value=\"agent\" label=\"Agent\">\n\n    [Agent](../agent/how-to-use-it.md#how-to-use-it) equips the Chat model with the tools needed to handle a wide range of coding tasks\n\n    ![agent](/img/agent.gif)\n\n    <Admonition type=\"info\" title=\"Learn more\" icon=\"\">\n        Learn more about [Agent](../agent/how-to-use-it.md#how-to-use-it)\n    </Admonition>\n\n  </TabItem>\n\n</Tabs>\n```\n\n----------------------------------------\n\nTITLE: Displaying Plugin Header in Markdown\nDESCRIPTION: This snippet shows the main header and description of the Continue plugin using Markdown syntax. It includes centered alignment and links to the documentation and JetBrains plugin page.\nSOURCE: https://github.com/continuedev/continue/blob/main/extensions/intellij/README.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<h1 align=\"center\">Continue</h1>\n\n<div align=\"center\">\n\n[**Continue**](https://docs.continue.dev) enables developers to create, share, and use custom AI code assistants with our open-source [JetBrains](https://plugins.jetbrains.com/plugin/22707-continue-extension) extension and [hub of models, rules, prompts, docs, and other building blocks](https://hub.continue.dev).\n\n</div>\n```\n\n----------------------------------------\n\nTITLE: Configuring Jira Datacenter Support in JSON\nDESCRIPTION: JSON configuration for supporting Jira Datacenter by setting the API version to 2. By default, the provider uses API version 3 for Jira Cloud.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_53\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"jira\",\n      \"params\": {\n        \"apiVersion\": \"2\"\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining JSON Structure for Company and Employee Data\nDESCRIPTION: This JSON structure defines a company profile with location information, employee details including personal information, skills, and salary data, along with aggregate statistics such as average age and salary ranges.\nSOURCE: https://github.com/continuedev/continue/blob/main/core/autocomplete/filtering/test/NEGATIVE_TEST_CASES/QWEN_JSON.txt#2025-04-19_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"active\": true,\n  \"department\": \"Product Development\",\n  \"location\": {\n    \"country\": \"USA\",\n    \"state\": \"California\",\n    \"city\": \"San BERNARDINO\",\n    \"coordinates\": {\n      \"latitude\": 34.10834,\n       \"longitude\": -117.28977\n    }\n  },\n  \"employees\": [\n    {\n      \"name\": \"John Doe\",\n      \"age\": 30,\n      \"position\": \"Developer\",\n      \"skills\": [\"JavaScript\", \"React\", \"Node.js\"],\n      \"remote\": false,\n      \"salary\": {\n        \"currency\": \"USD\",\n        \"amount\": 95000\n      }\n    },\n    {\n      \"name\": \"Jane Smith\",\n      \"age\": 25,\n      \"position\": \"Designer\",\n      \"skills\": [\"Photoshop\", \"Illustrator\"],\n      \"remote\": true,\n      \"salary\": {\n        \"currency\": \"USD\",\n        \"amount\": 70000\n      }\n    },\n    {\n      \"name\": \"Emily Johnson\",\n      \"age\": 35,\n      \"position\": \"Manager\",\n      \"teamSize\": 8,\n      \"remote\": true,\n      \"skills\": [\"Leadership\", \"Project Management\"]\n    }\n  ],\n  \"employeeCount\": 2,\n  \"averageAge\": 30,\n  \"remoteFriendly\": true,\n  \"salaryRange\": {\n    \"min\": 70000,\n    \"max\": 95000,\n    \"currency\": \"USD\"\n  },\n  \"skills\": {\n    \"required\": [\"JavaScript\", \"React\", \"Node.js\", \"Leadership\", \"Project Management\"],\n    \"optional\": [\"Photoshop\", \"Illustrator\"]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Autocompleting Svelte component prop\nDESCRIPTION: This snippet shows how to autocomplete a prop being passed to a child Svelte component, setting the name property to 'World'.\nSOURCE: https://github.com/continuedev/continue/blob/main/core/autocomplete/filtering/test/NEGATIVE_TEST_CASES/QWEN_TYPESCRIPT.txt#2025-04-19_snippet_3\n\nLANGUAGE: javascript\nCODE:\n```\nname=\"World\"\n```\n\n----------------------------------------\n\nTITLE: Building JetBrains Extension Plugin on Unix Systems\nDESCRIPTION: Gradle command to build the JetBrains extension plugin on Unix-based systems.\nSOURCE: https://github.com/continuedev/continue/blob/main/extensions/intellij/CONTRIBUTING.md#2025-04-19_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew buildPlugin\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Context Provider using VSCode Extension API in TypeScript\nDESCRIPTION: This example demonstrates how to register a custom context provider using the VSCode extension API provided by Continue.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/i18n/zh-CN/docusaurus-plugin-content-docs/current/customize/tutorials/build-your-own-context-provider.md#2025-04-19_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport * as vscode from \"vscode\";\n\nclass MyCustomProvider implements IContextProvider {\n  get description(): ContextProviderDescription {\n    return {\n      title: \"custom\",\n      displayTitle: \"Custom\",\n      description: \"Custom description\",\n      type: \"normal\",\n    };\n  }\n\n  async getContextItems(\n    query: string,\n    extras: ContextProviderExtras,\n  ): Promise<ContextItem[]> {\n    return [\n      {\n        name: \"Custom\",\n        description: \"Custom description\",\n        content: \"Custom content\",\n      },\n    ];\n  }\n\n  async loadSubmenuItems(\n    args: LoadSubmenuItemsArgs,\n  ): Promise<ContextSubmenuItem[]> {\n    return [];\n  }\n}\n\n// create an instance of your custom provider\nconst customProvider = new MyCustomProvider();\n\n// get Continue extension using vscode API\nconst continueExt = vscode.extensions.getExtension(\"continue.continue\");\n\n// get the API from the extension\nconst continueApi = continueExt?.exports;\n\n// register your custom provider\ncontinueApi?.registerCustomContextProvider(customProvider);\n```\n\n----------------------------------------\n\nTITLE: Installing and Setting Up Continue SDK Development Environment\nDESCRIPTION: Commands for installing dependencies, generating API clients, and starting Swagger UI for API exploration. These steps are required for local development of the SDK.\nSOURCE: https://github.com/continuedev/continue/blob/main/packages/continue-sdk/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install dependencies\nnpm install\n\n# Generate API clients\nnpm run generate-client:ALL\n\n# Start Swagger UI for API exploration\nnpm run swagger-ui\n```\n\n----------------------------------------\n\nTITLE: Configuring Issue Slash Command in JSON\nDESCRIPTION: Shows the configuration for the /issue slash command in config.json. This command generates a well-formatted GitHub issue draft based on user input.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/json-reference.md#2025-04-19_snippet_12\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"slashCommands\": [\n    {\n      \"name\": \"issue\",\n      \"description\": \"Generate a link to a drafted GitHub issue\",\n      \"params\": { \"repositoryUrl\": \"https://github.com/continuedev/continue\" }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Copyright and Apache 2.0 License Notice\nDESCRIPTION: Standard copyright notice and Apache License 2.0 declaration that outlines usage permissions, restrictions, and liability disclaimers for the Continue project.\nSOURCE: https://github.com/continuedev/continue/blob/main/extensions/vscode/LICENSE.txt#2025-04-19_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nCopyright 2023 Continue\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n```\n\n----------------------------------------\n\nTITLE: Partial Employee Records JSON\nDESCRIPTION: Incomplete JSON structure showing a subset of employee data with only name and age properties.\nSOURCE: https://github.com/continuedev/continue/blob/main/core/autocomplete/filtering/test/NEGATIVE_TEST_CASES/STARCODER_JSON.txt#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n   \"employees\": [\n     { \"name\": \"John Doe\", \"age\": 30 },\n     { \"name\": \"Jane Smith\", \"age\": 25 }\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Models in JSON Format for Continue\nDESCRIPTION: Shows the JSON configuration for various models including GPT-4, Ollama, and custom OpenAI-compatible models.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/yaml-migration.md#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"GPT-4\",\n      \"provider\": \"openai\",\n      \"model\": \"gpt-4\",\n      \"apiKey\": \"<YOUR_OPENAI_API_KEY>\",\n      \"completionOptions\": {\n        \"temperature\": 0.5,\n        \"maxTokens\": 2000\n      }\n    },\n    {\n      \"title\": \"Ollama\",\n      \"provider\": \"ollama\",\n      \"model\": \"AUTODETECT\"\n    },\n    {\n      \"title\": \"My Open AI Compatible Model\",\n      \"provider\": \"openai\",\n      \"apiBase\": \"http://3.3.3.3/v1\",\n      \"model\": \"my-openai-compatible-model\",\n      \"requestOptions\": {\n        \"headers\": { \"X-Auth-Token\": \"<API_KEY>\" }\n      }\n    }\n  ],\n  \"tabAutocompleteModel\": {\n    \"title\": \"My Starcoder\",\n    \"provider\": \"ollama\",\n    \"model\": \"starcoder2:3b\"\n  },\n  \"embeddingsProvider\": {\n    \"provider\": \"openai\",\n    \"model\": \"text-embedding-ada-002\",\n    \"apiKey\": \"<YOUR_OPENAI_API_KEY>\",\n    \"maxEmbeddingChunkSize\": 256,\n    \"maxEmbeddingBatchSize\": 5\n  },\n  \"reranker\": {\n    \"name\": \"voyage\",\n    \"params\": {\n      \"model\": \"rerank-2\",\n      \"apiKey\": \"<YOUR_VOYAGE_API_KEY>\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAPI Client via Setuptools\nDESCRIPTION: Command to install the Python package using setuptools for a single user or all users.\nSOURCE: https://github.com/continuedev/continue/blob/main/packages/continue-sdk/clients/python/README.md#2025-04-19_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npython setup.py install --user\n```\n\n----------------------------------------\n\nTITLE: Building the SDK Package\nDESCRIPTION: Commands to install dependencies and build the TypeScript sources into JavaScript.\nSOURCE: https://github.com/continuedev/continue/blob/main/packages/continue-sdk/clients/typescript/README.md#2025-04-19_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install\nnpm run build\n```\n\n----------------------------------------\n\nTITLE: Importing LLM Information Package in Markdown\nDESCRIPTION: This snippet shows the package name and a brief description of its purpose. It provides information about various Large Language Models (LLMs), including embedding, reranking, and other models.\nSOURCE: https://github.com/continuedev/continue/blob/main/packages/llm-info/README.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# @continuedev/llm-info\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Commands in JSON Format for Continue\nDESCRIPTION: Shows the JSON configuration for custom commands, specifically a 'check' command for code review.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/yaml-migration.md#2025-04-19_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"customCommands\": [\n    {\n      \"name\": \"check\",\n      \"description\": \"Check for mistakes in my code\",\n      \"prompt\": \"{{{ input }}}\\n\\nPlease read the highlighted code and check for any mistakes. You should look for the following, and be extremely vigilant:\\n- Syntax errors\\n- Logic errors\\n- Security vulnerabilities\\n- Performance issues\\n- Anything else that looks wrong\\n\\nOnce you find an error, please explain it as clearly as possible, but without using extra words. For example, instead of saying 'I think there is a syntax error on line 5', you should say 'Syntax error on line 5'. Give your answer as one bullet point per mistake found.\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Installing OpenAPI Client via pip\nDESCRIPTION: Command to install the Python package directly from a Git repository using pip.\nSOURCE: https://github.com/continuedev/continue/blob/main/packages/continue-sdk/clients/python/README.md#2025-04-19_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install git+https://github.com/GIT_USER_ID/GIT_REPO_ID.git\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies on Windows Systems\nDESCRIPTION: PowerShell command to install project dependencies on Windows systems using a provided script.\nSOURCE: https://github.com/continuedev/continue/blob/main/extensions/intellij/CONTRIBUTING.md#2025-04-19_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n.\\scripts\\install-dependencies.ps1\n```\n\n----------------------------------------\n\nTITLE: Listing Primary Jobs of Continue VS Code Extension\nDESCRIPTION: Enumerates the two main responsibilities of the Continue VS Code Extension: implementing the IDE side of the Continue IDE protocol and opening the Continue React app in a side panel.\nSOURCE: https://github.com/continuedev/continue/blob/main/extensions/vscode/CONTRIBUTING.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n1. Implement the IDE side of the Continue IDE protocol, allowing a Continue server to interact natively in an IDE. This happens in `src/continueIdeClient.ts`.\n2. Open the Continue React app in a side panel. The React app's source code lives in the `gui` directory. The panel is opened by the `continue.openContinueGUI` command, as defined in `src/commands.ts`.\n```\n\n----------------------------------------\n\nTITLE: Configuring Postgres Context Provider in JSON\nDESCRIPTION: JSON configuration for the Postgres context provider, allowing access to table schemas and sample rows. Specifies connection details and options for schema filtering and sample data retrieval.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_43\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"postgres\",\n      \"params\": {\n        \"host\": \"localhost\",\n        \"port\": 5436,\n        \"user\": \"myuser\",\n        \"password\": \"catsarecool\",\n        \"database\": \"animals\",\n        \"schema\": \"public\",\n        \"sampleRows\": 3\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Keyboard Shortcut for Opening JetBrains Settings\nDESCRIPTION: This snippet shows the keyboard shortcut for opening the Settings menu in JetBrains IDEs. It's used in the installation instructions for the Continue extension.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/getting-started/install.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<kbd>Ctrl</kbd> + <kbd>Alt</kbd> + <kbd>S</kbd>\n```\n\n----------------------------------------\n\nTITLE: Computing Sentence Embeddings with Transformers.js\nDESCRIPTION: This code demonstrates how to use Transformers.js to create a feature-extraction pipeline and compute sentence embeddings. It imports the pipeline function, creates an extractor, and processes an array of sentences to generate embeddings.\nSOURCE: https://github.com/continuedev/continue/blob/main/extensions/vscode/models/all-MiniLM-L6-v2/README.md#2025-04-19_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nimport { pipeline } from '@xenova/transformers';\n\n// Create a feature-extraction pipeline\nconst extractor = await pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2');\n\n// Compute sentence embeddings\nconst sentences = ['This is an example sentence', 'Each sentence is converted'];\nconst output = await extractor(sentences, { pooling: 'mean', normalize: true });\nconsole.log(output);\n// Tensor {\n//   dims: [ 2, 384 ],\n//   type: 'float32',\n//   data: Float32Array(768) [ 0.04592696577310562, 0.07328180968761444, ... ],\n//   size: 768\n// }\n```\n\n----------------------------------------\n\nTITLE: Specifying VS Code Engine and Type Definitions Version\nDESCRIPTION: Indicates the required VS Code engine version and the corresponding @types/vscode version used in the project, along with an explanation for this specific version choice.\nSOURCE: https://github.com/continuedev/continue/blob/main/extensions/vscode/CONTRIBUTING.md#2025-04-19_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n- We require vscode engine `^1.67.0` and use `@types/vscode` version `1.67.0` because this is the earliest version that doesn't break any of the APIs we are using. If you go back to `1.66.0`, then it will break `vscode.window.tabGroups`.\n```\n\n----------------------------------------\n\nTITLE: Iterating and Printing Numbers in Python\nDESCRIPTION: This snippet demonstrates a simple for loop that iterates over a range of numbers from 0 to 9 and prints each number. It uses Python's range() function and the print() function.\nSOURCE: https://github.com/continuedev/continue/blob/main/manual-testing-sandbox/example.ipynb#2025-04-19_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfor i in range(10):\n    print(i)\n```\n\n----------------------------------------\n\nTITLE: Running Full E2E Test Suite Setup\nDESCRIPTION: Command to run the complete end-to-end test suite for first-time setup or after source code changes.\nSOURCE: https://github.com/continuedev/continue/blob/main/extensions/vscode/e2e/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm run e2e:all\n```\n\n----------------------------------------\n\nTITLE: Publishing Continue Project to npm Registry\nDESCRIPTION: These commands build the project, bump the version in package.json, and publish the package to the npm registry with public access.\nSOURCE: https://github.com/continuedev/continue/blob/main/packages/config-yaml/src/README.md#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm run build\nnpm publish --access public\n```\n\n----------------------------------------\n\nTITLE: Request Options Configuration\nDESCRIPTION: Configuration example for HTTP request options applied globally to all models and context providers.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/i18n/zh-CN/docusaurus-plugin-content-docs/current/reference.md#2025-04-19_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"requestOptions\": {\n    \"headers\": {\n      \"X-Auth-Token\": \"xxx\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Discord Context Provider in YAML\nDESCRIPTION: YAML configuration for the Discord context provider, which allows referencing messages from Discord channels. Requires a bot token and server/channel IDs.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_55\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: discord\n    params:\n      discordKey: \"bot token\"\n      guildId: \"1234567890\" \n      channels:\n        - id: \"123456\"\n          name: \"example-channel\"\n        - id: \"678901\" \n          name: \"example-channel-2\"\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Version Constraints\nDESCRIPTION: This code snippet defines the required Python packages and their version ranges. It includes urllib3, python_dateutil, pydantic, and typing-extensions, each with specific version constraints to ensure compatibility and proper functionality.\nSOURCE: https://github.com/continuedev/continue/blob/main/packages/continue-sdk/clients/python/requirements.txt#2025-04-19_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nurllib3 >= 1.25.3, < 3.0.0\npython_dateutil >= 2.8.2\npydantic >= 2\ntyping-extensions >= 4.7.1\n```\n\n----------------------------------------\n\nTITLE: Defining Python Testing and Development Dependencies\nDESCRIPTION: Specifies required Python packages and their minimum versions for testing, code coverage, type checking, and development tools. Includes pytest for testing, pytest-cov for coverage reporting, tox for test automation, flake8 for linting, type stubs for python-dateutil, and mypy for static type checking.\nSOURCE: https://github.com/continuedev/continue/blob/main/packages/continue-sdk/clients/python/test-requirements.txt#2025-04-19_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\npytest >= 7.2.1\npytest-cov >= 2.8.1\ntox >= 3.9.0\nflake8 >= 4.0.0\ntypes-python-dateutil >= 2.8.19.14\nmypy >= 1.5\n```\n\n----------------------------------------\n\nTITLE: Configuring Operating System Context Provider in JSON\nDESCRIPTION: JSON configuration for the operating system context provider, which provides information about the current operating system's architecture and platform.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_35\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"os\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Creating and Manipulating ListAssistants404Response Objects in Python\nDESCRIPTION: This snippet shows how to create an instance of ListAssistants404Response from JSON, convert it to JSON, transform it to a dictionary, and create a new instance from a dictionary. It demonstrates serialization and deserialization of the model.\nSOURCE: https://github.com/continuedev/continue/blob/main/packages/continue-sdk/clients/python/docs/ListAssistants404Response.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openapi_client.models.list_assistants404_response import ListAssistants404Response\n\n# TODO update the JSON string below\njson = \"{}\"\n# create an instance of ListAssistants404Response from a JSON string\nlist_assistants404_response_instance = ListAssistants404Response.from_json(json)\n# print the JSON string representation of the object\nprint(ListAssistants404Response.to_json())\n\n# convert the object into a dict\nlist_assistants404_response_dict = list_assistants404_response_instance.to_dict()\n# create an instance of ListAssistants404Response from a dict\nlist_assistants404_response_from_dict = ListAssistants404Response.from_dict(list_assistants404_response_dict)\n```\n\n----------------------------------------\n\nTITLE: Rebuilding JetBrains Extension\nDESCRIPTION: Gradle command to rebuild the JetBrains extension with the latest source code.\nSOURCE: https://github.com/continuedev/continue/blob/main/extensions/intellij/CONTRIBUTING.md#2025-04-19_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew buildPlugin\n```\n\n----------------------------------------\n\nTITLE: Configuring Tree Context Provider in JSON\nDESCRIPTION: JSON configuration example for adding the @Tree context provider to reference workspace file structure.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_27\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"tree\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Running IDE for UI Tests\nDESCRIPTION: Gradle command to run the IDE instance for UI testing as a background task.\nSOURCE: https://github.com/continuedev/continue/blob/main/extensions/intellij/CONTRIBUTING.md#2025-04-19_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew clean runIdeForUiTests &\n```\n\n----------------------------------------\n\nTITLE: Defining Employee Records in JSON\nDESCRIPTION: JSON structure defining an array of employee objects with personal details and an active status boolean. Each employee has properties for name, age, and position.\nSOURCE: https://github.com/continuedev/continue/blob/main/core/autocomplete/filtering/test/NEGATIVE_TEST_CASES/STARCODER_JSON.txt#2025-04-19_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"employees\": [\n    { \"name\": \"John Doe\", \"age\": 30, \"position\": \"Developer\" },\n    { \"name\": \"Jane Smith\", \"age\": 25, \"position\": \"Designer\" },\n    { \"name\": \"Emily Johnson\", \"age\": 35, \"position\": \"Manager\" }\n  ],\n  \"active\": true\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Clipboard Context Provider in YAML\nDESCRIPTION: Configuration example for adding the @Clipboard context provider, which allows referencing recent clipboard items.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_24\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: clipboard\n```\n\n----------------------------------------\n\nTITLE: Copyright Authors List Format\nDESCRIPTION: A formatted list of copyright holders including organizations and individuals with their email addresses. Each entry follows the format 'Name or Organization <email address>'.\nSOURCE: https://github.com/continuedev/continue/blob/main/gui/public/fonts/JetBrainsMono/AUTHORS.txt#2025-04-19_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# This is the official list of project authors for copyright purposes.\n# This file is distinct from the CONTRIBUTORS.txt file.\n# See the latter for an explanation.\n# \n# Names should be added to this file as:\n# Name or Organization <email address>\n\nJetBrains <>\nPhilipp Nurullin <philipp.nurullin@jetbrains.com>\nKonstantin Bulenkov <kb@jetbrains.com>\n```\n\n----------------------------------------\n\nTITLE: Configuring Docs Context Provider in JSON\nDESCRIPTION: JSON configuration example for adding the @Docs context provider to reference documentation site contents.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"docs\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Slash Command in TypeScript\nDESCRIPTION: TypeScript implementation of a custom slash command that generates commit messages using the Continue SDK. Demonstrates accessing IDE features and language models through the SDK.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/i18n/zh-CN/docusaurus-plugin-content-docs/current/customize/tutorials/build-your-own-slash-command.md#2025-04-19_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nexport function modifyConfig(config: Config): Config {\n  config.slashCommands?.push({\n    name: \"commit\",\n    description: \"Write a commit message\",\n    run: async function* (sdk) {\n      // getDiff \n      //  diff \n      const diff = await sdk.ide.getDiff(false); //  false \n      for await (const message of sdk.llm.streamComplete(\n        `${diff}\\n\\nWrite a commit message for the above changes. Use no more than 20 tokens to give a brief description in the imperative mood (e.g. 'Add feature' not 'Added feature'):`,\n        new AbortController().signal,\n        {\n          maxTokens: 20,\n        },\n      )) {\n        yield message;\n      }\n    },\n  });\n  return config;\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Docs Context Provider in YAML\nDESCRIPTION: Configuration example for adding the @Docs context provider, which allows referencing contents from documentation sites.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\ncontext:\n  - provider: docs\n```\n\n----------------------------------------\n\nTITLE: Configuring Git Diff Context Provider in JSON\nDESCRIPTION: JSON configuration example for adding the @Git Diff context provider to reference branch changes.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/context-providers.mdx#2025-04-19_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"contextProviders\": [\n    {\n      \"name\": \"diff\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Debug Prompt for VS Code Action in Markdown\nDESCRIPTION: This snippet shows the format of the debug prompt generated when using the debug action shortcut in VS Code. It includes a request for help explaining how to fix the error message.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/deep-dives/vscode-actions.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nI got the following error, can you please help explain how to fix it?\n\n[ERROR_MESSAGE]\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Command via JSON\nDESCRIPTION: Example configuration for adding a custom slash command in config.json that checks code for mistakes. Uses Handlebars templating for dynamic input handling.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/i18n/zh-CN/docusaurus-plugin-content-docs/current/customize/tutorials/build-your-own-slash-command.md#2025-04-19_snippet_0\n\nLANGUAGE: json\nCODE:\n```\ncustomCommands=[{\n        \"name\": \"check\",\n        \"description\": \"Check for mistakes in my code\",\n        \"prompt\": \"{{{ input }}}\\n\\nPlease read the highlighted code and check for any mistakes. You should look for the following, and be extremely vigilant:\\n- Syntax errors\\n- Logic errors\\n- Security vulnerabilities\\n- Performance issues\\n- Anything else that looks wrong\\n\\nOnce you find an error, please explain it as clearly as possible, but without using extra words. For example, instead of saying 'I think there is a syntax error on line 5', you should say 'Syntax error on line 5'. Give your answer as one bullet point per mistake found.\"\n}]\n```\n\n----------------------------------------\n\nTITLE: Starting LlamaCpp Server Using Bash Command\nDESCRIPTION: Command to run the llama.cpp server binary with specific parameters including context size, host address, thread count, and model path. The server is configured to listen on all network interfaces (0.0.0.0) for remote access.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/llamacpp.mdx#2025-04-19_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n.\\server.exe -c 4096 --host 0.0.0.0 -t 16 --mlock -m models\\meta\\llama\\codellama-7b-instruct.Q8_0.gguf\n```\n\n----------------------------------------\n\nTITLE: Configuring Documentation in YAML Format for Continue\nDESCRIPTION: Demonstrates the YAML configuration for documentation sources.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/yaml-migration.md#2025-04-19_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\ndocs:\n  - name: nest.js\n    startUrl: https://docs.nestjs.com/\n\n  - name: My site\n    startUrl: https://mysite.com/docs/\n```\n\n----------------------------------------\n\nTITLE: Configuring System Message in JSON Format for Continue\nDESCRIPTION: Shows the JSON configuration for system message.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/yaml-migration.md#2025-04-19_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"systemMessage\": \"Always give concise responses\"\n}\n```\n\n----------------------------------------\n\nTITLE: Setting HTTP Request Options in Continue JSON Configuration\nDESCRIPTION: This code example illustrates how to configure HTTP request options in the Continue config.json file. It shows how to add custom headers to all requests made by the application.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/json-reference.md#2025-04-19_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"requestOptions\": {\n    \"headers\": {\n      \"X-Auth-Token\": \"xxx\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating and Manipulating ListAssistants200ResponseInnerConfigResult Objects in Python\nDESCRIPTION: This snippet demonstrates how to create an instance of ListAssistants200ResponseInnerConfigResult from JSON, convert it to JSON, create a dict representation, and create an instance from a dict. It includes TODO comments for updating the JSON string.\nSOURCE: https://github.com/continuedev/continue/blob/main/packages/continue-sdk/clients/python/docs/ListAssistants200ResponseInnerConfigResult.md#2025-04-19_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom openapi_client.models.list_assistants200_response_inner_config_result import ListAssistants200ResponseInnerConfigResult\n\n# TODO update the JSON string below\njson = \"{}\"\n# create an instance of ListAssistants200ResponseInnerConfigResult from a JSON string\nlist_assistants200_response_inner_config_result_instance = ListAssistants200ResponseInnerConfigResult.from_json(json)\n# print the JSON string representation of the object\nprint(ListAssistants200ResponseInnerConfigResult.to_json())\n\n# convert the object into a dict\nlist_assistants200_response_inner_config_result_dict = list_assistants200_response_inner_config_result_instance.to_dict()\n# create an instance of ListAssistants200ResponseInnerConfigResult from a dict\nlist_assistants200_response_inner_config_result_from_dict = ListAssistants200ResponseInnerConfigResult.from_dict(list_assistants200_response_inner_config_result_dict)\n```\n\n----------------------------------------\n\nTITLE: Forcing Legacy Completions Endpoint in JSON\nDESCRIPTION: JSON configuration to force usage of the completions endpoint instead of chat/completions when using an OpenAI-compatible server.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/top-level/openai.mdx#2025-04-19_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"OpenAI-compatible server / API\",\n      \"provider\": \"openai\",\n      \"model\": \"MODEL_NAME\",\n      \"apiBase\": \"http://localhost:8000/v1\", \n      \"useLegacyCompletionsEndpoint\": true\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring HuggingFace Inference API in JSON\nDESCRIPTION: JSON configuration for setting up a HuggingFace Inference API endpoint in Continue. Requires specifying the model name, API key, and inference endpoint URL.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/huggingfaceinferenceapi.mdx#2025-04-19_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Hugging Face Inference API\",\n      \"provider\": \"huggingface-inference-api\",\n      \"model\": \"MODEL_NAME\",\n      \"apiKey\": \"<YOUR_HF_TOKEN>\",\n      \"apiBase\": \"<YOUR_HF_INFERENCE_API_ENDPOINT_URL>\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Unpublished SDK Package\nDESCRIPTION: Command to install an unpublished version of the SDK package from a local path (not recommended for production use).\nSOURCE: https://github.com/continuedev/continue/blob/main/packages/continue-sdk/clients/typescript/README.md#2025-04-19_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnpm install PATH_TO_GENERATED_PACKAGE --save\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenRouter Advanced Options\nDESCRIPTION: Example showing how to configure advanced OpenRouter options like disabling prompt compression using extraBodyProperties.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-providers/more/openrouter.mdx#2025-04-19_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Example Model\n    provider: exampleProvider\n    model: example-model\n    requestOptions:\n      extraBodyProperties:\n        transforms: []\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Example Model\",\n      \"provider\": \"exampleProvider\",\n      \"model\": \"example-model\",\n      \"requestOptions\": {\n        \"extraBodyProperties\": {\n          \"transforms\": []\n        }\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Ollama Llama 3.1 8B in YAML for Local Use\nDESCRIPTION: YAML configuration for using Llama 3.1 8B through Ollama as a local chat model in Continue. Requires Ollama installation on your machine.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/chat.mdx#2025-04-19_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Llama 3.1 8B\n    provider: ollama\n    model: llama3.1:8b\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSeek Coder 2 16B with LM Studio in JSON\nDESCRIPTION: This code snippet shows the configuration for the DeepSeek Coder 2 16B model using LM Studio as the provider in JSON format. It specifies the model title, provider, and model identifier.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/chat.mdx#2025-04-19_snippet_22\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"DeepSeek Coder 2 16B\",\n      \"provider\": \"lmstudio\",\n      \"model\": \"deepseek-coder-v2:16b\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Custom Context Provider to Config in TypeScript\nDESCRIPTION: This snippet demonstrates how to add a custom context provider to the configuration file.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/i18n/zh-CN/docusaurus-plugin-content-docs/current/customize/tutorials/build-your-own-context-provider.md#2025-04-19_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nexport function modifyConfig(config: Config): Config {\n  if (!config.contextProviders) {\n    config.contextProviders = [];\n  }\n  config.contextProviders.push(RagContextProvider);\n  return config;\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring LM Studio Llama 3.1 8B in JSON for Local Use\nDESCRIPTION: JSON configuration for using Llama 3.1 8B through LM Studio as a local chat model in Continue. Requires LM Studio installation on your machine.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/chat.mdx#2025-04-19_snippet_16\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"models\": [\n    {\n      \"title\": \"Llama 3.1 8B\",\n      \"provider\": \"lmstudio\",\n      \"model\": \"llama3.1-8b\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring LM Studio Llama 3.1 8B in YAML for Local Use\nDESCRIPTION: YAML configuration for using Llama 3.1 8B through LM Studio as a local chat model in Continue. Requires LM Studio installation on your machine.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/customize/model-roles/chat.mdx#2025-04-19_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\nmodels:\n  - name: Llama 3.1 8B\n    provider: lmstudio\n    model: llama3.1:8b\n```\n\n----------------------------------------\n\nTITLE: Setting Up Custom Commands in Continue.dev\nDESCRIPTION: Example of defining custom commands with specific prompts and descriptions for quick access to common actions like generating unit tests.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/i18n/zh-CN/docusaurus-plugin-content-docs/current/reference.md#2025-04-19_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"customCommands\": [\n    {\n      \"name\": \"test\",\n      \"prompt\": \"Write a comprehensive set of unit tests for the selected code. It should setup, run tests that check for correctness including important edge cases, and teardown. Ensure that the tests are complete and sophisticated. Give the tests just as chat output, don't edit any file.\",\n      \"description\": \"Write unit tests for highlighted code\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Listing Available Gradle Tasks\nDESCRIPTION: Shell command to display all available Gradle tasks for the project.\nSOURCE: https://github.com/continuedev/continue/blob/main/extensions/intellij/CONTRIBUTING.md#2025-04-19_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n./gradlew tasks\n```\n\n----------------------------------------\n\nTITLE: Example Natural Language Command for Agent\nDESCRIPTION: Shows an example of a natural language instruction that can be given to the Agent to modify eslint configurations.\nSOURCE: https://github.com/continuedev/continue/blob/main/docs/docs/agent/how-to-use-it.md#2025-04-19_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nSet the @typescript-eslint/naming-convention rule to \"off\" for all eslint configurations in this project\n```\n\n----------------------------------------\n\nTITLE: Specifying Diff Algorithm Test Format in Markdown\nDESCRIPTION: This code snippet demonstrates the format for specifying diff algorithm tests. It includes sections for the code before changes, the code after changes, and the expected diff output.\nSOURCE: https://github.com/continuedev/continue/blob/main/core/diff/test-examples/README.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<CODE BEFORE>\n\n---\n\n<CODE AFTER>\n\n---\n\n<EXPECTED DIFF>\n```\n\n----------------------------------------\n\nTITLE: Installing Transformers.js via NPM\nDESCRIPTION: Command to install Transformers.js using NPM package manager.\nSOURCE: https://github.com/continuedev/continue/blob/main/core/vendor/modules/@xenova/transformers/README.md#2025-04-19_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnpm i @xenova/transformers\n```\n\n----------------------------------------\n\nTITLE: Autocompleting Svelte each block closing tag\nDESCRIPTION: This snippet demonstrates how to autocomplete the closing tag for a Svelte each block used for iterating over an array of items.\nSOURCE: https://github.com/continuedev/continue/blob/main/core/autocomplete/filtering/test/NEGATIVE_TEST_CASES/QWEN_TYPESCRIPT.txt#2025-04-19_snippet_4\n\nLANGUAGE: javascript\nCODE:\n```\n}\n```\n\n----------------------------------------\n\nTITLE: Autocompleting Svelte reactive statement\nDESCRIPTION: This snippet demonstrates how to autocomplete a Svelte reactive statement that creates a derived value, doubling the count variable.\nSOURCE: https://github.com/continuedev/continue/blob/main/core/autocomplete/filtering/test/NEGATIVE_TEST_CASES/QWEN_TYPESCRIPT.txt#2025-04-19_snippet_2\n\nLANGUAGE: javascript\nCODE:\n```\ndoubledCount = count * 2\n```\n\n----------------------------------------\n\nTITLE: Full Changelog in Markdown\nDESCRIPTION: Comprehensive changelog documenting version history from initial release 0.0.1 to version 1.0.0, including additions, changes, and fixes for each release.\nSOURCE: https://github.com/continuedev/continue/blob/main/extensions/intellij/CHANGELOG.md#2025-04-19_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n## 1.0.0 - 2025-02-26\n### Added\n* Introduces hub.continue.dev\n* Improved theme matching\n### Fixed\n* Fixes interference between SonarQube and Continue autocomplete\n* Numerous reliability improvements\n\n## 0.0.92 - 2025-02-26\n### Fixed\n* Off-screen rendering to solve white flash on load and lack of changing cursor type\n* OSR-related fixes for non-Mac users\n* Fixes for inline edit in JetBrains\n\n## 0.0.72 - 2024-10-04\n### Added\n* Listen for changes to Intellij settings without requiring window reload\n### Changed\n* Updated tutorial file\n### Fixed\n* Fix ability to load config.ts in JetBrains IDEs\n\n## 0.0.69 - 2024-09-22\n### Added\n* Support for the \"search\" context provider\n### Fixed\n* Fixed bug where only first module would be recognized\n* Improved concurrency handling to avoid freezes\n* Made compatible with latest JetBrains versions\n\n## 0.0.54 - 2024-07-13\n### Added\n* Partial autocomplete acceptance\n* Autocomplete status bar spinner\n### Fixed\n* Fixed duplicate completion bug and others\n\n## 0.0.53 - 2024-07-10\n### Added\n* Support for .prompt files\n* New onboarding experience\n### Fixed\n* Indexing fixes from VS Code versions merged into IntelliJ\n* Improved codebase indexing reliability and testing\n* Fixes for autocomplete text positioning and timing\n\n## 0.0.42 - 2024-04-12\n### Added\n* Inline cmd/ctrl+I in JetBrains\n### Fixed\n* Fixed character encoding error causing display issues\n* Fixed error causing input to constantly demand focus\n* Fixed automatic reloading of config.json\n\n## 0.0.38 - 2024-03-15\n### Added\n* Remote config server support\n* Autocomplete support in JetBrains\n\n## 0.0.34 - 2024-03-03\n### Added\n* diff context provider\n### Changed\n* Allow LLM servers to handle templating\n### Fixed\n* Fix a few context providers / slash commands\n* Fixed issues preventing proper extension startup\n\n## v0.0.26 - 2023-12-28\n### Added\n* auto-reloading of config on save\n### Fixed\n* Fixed /edit bug for versions without Python server\n\n## v0.0.25 - 2023-12-25\n### Changed\n- Intellij extension no longer relies on the Continue Python server\n\n## v0.0.21 - 2023-12-05\n### Added\n- updated to match latest VS Code updates\n\n## v0.0.19 - 2023-11-19\n### Changed\n- migrated to .json config file format\n\n## v0.0.1 - 2023-09-01\n### Added\n- Initial scaffold created from [IntelliJ Platform Plugin Template](https://github.com/JetBrains/intellij-platform-plugin-template)\n```\n\n----------------------------------------\n\nTITLE: Displaying License Information in Markdown\nDESCRIPTION: This snippet shows the license information for the Continue plugin using Markdown syntax. It includes a link to the LICENSE file in the repository.\nSOURCE: https://github.com/continuedev/continue/blob/main/extensions/intellij/README.md#2025-04-19_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n## License\n\n[Apache 2.0  2023-2024 Continue Dev, Inc.](./LICENSE)\n```"
  }
]
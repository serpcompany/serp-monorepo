[
  {
    "owner": "pchlenski",
    "repo": "manify",
    "content": "TITLE: Running a Complete Manify Workflow - Python\nDESCRIPTION: Demonstrates loading a dataset, creating a product manifold, learning graph embeddings, and training a product space decision tree classifier, while using standard packages such as torch and scikit-learn. Dependencies include manify (with its submodules), torch, and scikit-learn. Key parameters include the choice of dataset (e.g., \\\"polblogs\\\"), the manifold signature (e.g., [(1, 4)] for $S^4_1$), and classifier hyperparameters. Inputs are pairwise distances and graph labels; outputs are printed classification score. Describes a reproducible workflow for empirical experiments with data in non-Euclidean spaces.\nSOURCE: https://github.com/pchlenski/manify/blob/main/README.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom manify.manifolds import ProductManifold\nfrom manify.embedders import coordinate_learning\nfrom manify.predictors.decision_tree import ProductSpaceDT\nfrom manify.utils.dataloaders import load\nfrom sklearn.model_selection import train_test_split\n\n# Load graph data\ndists, graph_labels, _ = load(\"polblogs\")\n\n# Create product manifold\npm = ProductManifold(signature=[(1, 4)]) # S^4_1\n\n# Learn embeddings (Gu et al (2018) method)\nX, _ = coordinate_learning.train_coords(pm=pm, dists=dists)\n\n# Train and evaluate classifier (Chlenski et al (2025) method)\nX_train, X_test, y_train, y_test = train_test_split(X, graph_labels)\ntree = ProductSpaceDT(pm=pm, max_depth=3)\ntree.fit(X_train, y_train)\nprint(tree.score(X_test, y_test))\n```\n\n----------------------------------------\n\nTITLE: Training a Mixed Curvature VAE in Python\nDESCRIPTION: This extensive snippet details the training process for a Mixed Curvature Variational Autoencoder (VAE) on the 'lymphoma' dataset. It sets hyperparameters, performs multiple trials with different random seeds, splits data, initializes the ProductManifold, VAE model (`ProductSpaceVAE`), and Adam optimizer (configured for separate learning rates for VAE parameters and manifold parameters). The training loop iterates through epochs and batches, calculates the Evidence Lower Bound (ELBO) loss, performs backpropagation, checks for gradient issues (NaN/Inf), updates model parameters, adjusts learning rates after a burn-in period, and visualizes progress using tqdm. Finally, it generates and saves embeddings for both the training and test sets.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/Embedder Tutorial.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom tqdm.notebook import tqdm\nimport numpy as np\nfrom embedders import vae\n#Specify the hyperparameter\nSIGNATURE = [(1, 2), (1, 2)]\nN_EPOCHS = 100\nBATCH_SIZE = 4_096\nBETA = 1.0\nN_SAMPLES = 32\nTRIALS = 3\n\n# Skopek hyperparamters (aka standard Adam hyperparameters)\nLR = 1e-4 \nBETA1 = 0.9\nBETA2 = 0.999\nEPS = 1e-8\nCURVATURE_LR = 1e-4\n\n\nfor trial in range(TRIALS):\n    # Set seeds\n    torch.manual_seed(trial)\n    np.random.seed(trial)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=trial)\n    pm = manifolds.ProductManifold(SIGNATURE)\n    myvae = vae.ProductSpaceVAE(\n        pm=pm, encoder=Encoder(pm), decoder=Decoder(pm), beta=BETA, n_samples=N_SAMPLES, device=device\n    )\n    opt = torch.optim.Adam(\n        [\n            {\"params\": myvae.parameters(), \"lr\": LR * 0.1, \"betas\": (BETA1, BETA2), \"eps\": EPS},\n            {\"params\": pm.params(), \"lr\": 0, \"betas\": (BETA1, BETA2), \"eps\":  EPS}\n        ]\n    )\n\n    # Visualization stuff\n    my_tqdm = tqdm(total=N_EPOCHS * len(X_train))\n\n    # Device stuff\n    myvae = myvae.to(device)\n    \n    X_train = X_train.to(device)\n    X_test = X_test.to(device)\n    pm = pm.to(device)\n\n\n    # Gradient checking stuff\n    def grads_ok(myvae):\n        out = True\n        for name, param in myvae.named_parameters():\n            if param.grad is not None:\n                if torch.isnan(param.grad).any():\n                    print(f\"NaN gradient in {name}\")\n                    out = False\n                if torch.isinf(param.grad).any():\n                    print(f\"Inf gradient in {name}\")\n                    out = False\n        return out\n\n\n    losses = []\n    for epoch in range(N_EPOCHS):\n        # Stop the burn-in\n        if epoch == 20:\n            opt.param_groups[1][\"lr\"] = CURVATURE_LR\n            opt.param_groups[0][\"lr\"] = LR\n        \n        for i in range(0, len(X_train), BATCH_SIZE):\n            x_batch = X_train[i : i + BATCH_SIZE]\n\n            elbo, ll, kl = myvae.elbo(x_batch)\n            loss = -elbo\n            losses.append(loss.item())\n\n            opt.zero_grad()\n            loss.backward()\n\n\n            if torch.isnan(loss):\n                print(f\"Loss is NaN at iteration {i}\")\n            elif torch.isinf(loss):\n                print(f\"Loss is inf at iteration {i}\")\n            elif grads_ok(myvae):\n                opt.step()\n\n            my_tqdm.update(BATCH_SIZE)\n            my_tqdm.set_description(f\"Epoch {epoch}, loss: {loss.item():.1f}, ll: {ll.item():.1f}, kl: {kl.item():.1f}\")\n            my_tqdm.set_postfix(\n                {f\"r{i}\": f\"{x._log_scale.item():.3f}\" for i, x in enumerate(pm.manifold.manifolds)}\n            )\n    \n    # Save the embeddings\n    embeddings = []\n    for i in range(0, len(X_train), BATCH_SIZE):\n        x_batch = X_train[i : i + BATCH_SIZE]\n        z_mean, _ = myvae.encoder(x_batch)\n        embeddings.append(z_mean.detach().cpu().numpy())\n\n    embeddings = np.concatenate(embeddings)\n\n    # Save the test embeddings\n    test_embeddings = []\n    for i in range(0, len(X_test), BATCH_SIZE):\n        x_batch = X_test[i : i + BATCH_SIZE]\n        z_mean, _ = myvae.encoder(x_batch)\n        test_embeddings.append(z_mean.detach().cpu().numpy())\n    \n    test_embeddings = np.concatenate(test_embeddings)\n```\n\n----------------------------------------\n\nTITLE: Training, Evaluating, and Statistically Comparing Multiple Classifiers (Python)\nDESCRIPTION: This comprehensive snippet generates synthetic classification data, trains various classifiers (MLP, kappa-GCN, multinomial logistic regression in both Scikit-learn and PyTorch), accumulates their test accuracies over multiple trials, and conducts statistical comparisons. It requires PyTorch, Scikit-learn, pandas, scipy, and the custom \"manify\" package; also expects utility functions train_model and evaluate_model. Key parameters are the number of classes, trials, model architectures, and optimization epochs. Outputs a summary DataFrame of results and prints cross-model Wilcoxon signed-rank test results. Useful for systematic numerical comparison between standard and manifold-based neural architectures.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/relationship.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import wilcoxon\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nnum_classes=2\nresults = pd.DataFrame(columns=[\"trial\", \"mlp\", \"kappa_mlp\", \"mlr\", \"torchmlr\",\"kappa_mlr\"])\nfor i in range(30):\n    X, y = pm.gaussian_mixture(num_points=1000, num_classes=2, num_clusters=3)\n    # X = X.detach().cpu().numpy()\n    # y = y.detach().cpu().numpy()\n    X = X.detach().cpu()\n    y = y.detach().cpu()\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n    mlp = torch.nn.Sequential(torch.nn.Linear(2, 2, bias=False), torch.nn.ReLU(), torch.nn.Linear(2, num_classes))\n    mlp = train_model(mlp, X_train, y_train, num_epochs=1000)\n    mlp_accuracy = evaluate_model(mlp, X_test, y_test)\n\n    pm = manify.manifolds.ProductManifold([(0, 2)], stereographic=True)\n    kgcn = manify.predictors.kappa_gcn.KappaGCN(\n        pm=pm, output_dim=num_classes, hidden_dims=[2], nonlinearity=torch.relu, task=\"classification\"\n    )\n    kgcn = train_model(kgcn, X_train, y_train, A_hat=torch.eye(X_train.shape[0]),num_epochs=1000)\n    kgcnmlp_accuracy = evaluate_model(kgcn, X_test, y_test, A_hat=torch.eye(X_test.shape[0]))\n\n    # scaler = StandardScaler()\n    # X_train_scaled = scaler.fit_transform(X_train)\n    # X_test_scaled = scaler.transform(X_test)\n    # kgcn_untrain=manify.predictors.kappa_gcn.KappaGCN(\n    #     pm=pm, output_dim=num_classes, hidden_dims=[2], nonlinearity=torch.relu, task=\"classification\"\n    # )\n    # kgcn_untrain.layers[0].W.data = mlp.state_dict()['0.weight'].data\n    # kgcn_untrain.W_logits.data = mlp.state_dict()['2.weight'].data\n    # kgcn_untrain.p_ks.data=-mlp.state_dict()['2.bias'].data @ torch.pinverse(mlp.state_dict()['2.weight'].data)\n    # kgcn_untrain_accuracy=evaluate_model(kgcn_untrain, X_test, y_test, A_hat=torch.eye(X_test.shape[0]))\n\n    # mlp_untrain=torch.nn.Sequential(torch.nn.Linear(2, 2, bias=False), torch.nn.ReLU(), torch.nn.Linear(2, num_classes))\n    # mlp_untrain.state_dict()[\"0.weight\"].data = kgcn.layers[0].W.data\n    # mlp_untrain.state_dict()['2.weight'].data= 4*kgcn.W_logits.data\n    # mlp_untrain.state_dict()['2.bias'].data= -4 * torch.sum(kgcn.p_ks.data * kgcn.W_logits.data.t(), dim=1)\n    # # mlp_untrain.layers[2].bias.data = -kgcn.p_ks.data @ kgcn.W_logits.data\n    # mlp_untrain_accuracy=evaluate_model(mlp_untrain, X_test, y_test)\n\n    # Train a multinomial logistic regression model\n    log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n    log_reg.fit(X_train, y_train)\n    # Evaluate the model\n    log_reg_accuracy = log_reg.score(X_test, y_test)\n\n    torch_log_reg = torch.nn.Sequential(torch.nn.Linear(2, num_classes))\n    torch_log_reg = train_model(torch_log_reg, X_train, y_train,num_epochs=1000)\n    # Evaluate the model\n    torch_log_reg_accuracy = evaluate_model(torch_log_reg, X_test, y_test)\n\n    kgcn = manify.predictors.kappa_gcn.KappaGCN(\n        pm=pm, output_dim=num_classes, hidden_dims=[], nonlinearity=torch.relu, task=\"classification\"\n    )\n    kgcn = train_model(kgcn, X_train, y_train, A_hat=torch.eye(X_train.shape[0]),num_epochs=1000)\n    kgcnmlr_accuracy = evaluate_model(kgcn, X_test, y_test, A_hat=torch.eye(X_test.shape[0]))\n\n\n\n    # kgcn_untrain_accuracy=evaluate_model(kgcn_untrain, X_test, y_test, A_hat=torch.eye(X_test.shape[0]))\n\n    results.loc[i] = {\"trial\": i, \"mlp\": mlp_accuracy, \"kappa_mlp\": kgcnmlp_accuracy,\"mlr\": log_reg_accuracy,\n                      \"torchmlr\": torch_log_reg_accuracy,\n                      \"kappa_mlr\": kgcnmlr_accuracy}\n\nprint(results)\nprint(results.mean())\n#based on existing code, get a table of wilcoxcon between all values\n\nprint(wilcoxon(results[\"mlp\"], results[\"kappa_mlp\"]))\nprint(wilcoxon(results[\"mlr\"], results[\"kappa_mlr\"]))\n#this is to troubleshoot a weird result\n\n```\n\n----------------------------------------\n\nTITLE: Importing Additional Libraries and Setting Configuration Parameters (Python)\nDESCRIPTION: Imports various modules from `embedders` (manifolds, decision trees, dataloaders, coordinate learning, benchmarks, metrics), `sklearn` (classifiers), `tqdm` (progress bars), `scipy.stats` (Wilcoxon test), `numpy`, and `pandas`. It also defines several global configuration constants: `USE_SPECIAL_DIMS` (for benchmarking), `USE_DISTS` (whether to add distances in `make_link_prediction_dataset`), `SIGNATURE` (defining the product manifold structure), `TEST_SIZE` (unused in snippet), `TOTAL_ITERATIONS` (for embedding training), `MAX_DEPTH` (for decision tree classifier), and `N_TRIALS` (number of times to repeat each benchmark).\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/22_link_prediction.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Any other dataset\nimport embedders\nfrom embedders.manifolds import ProductManifold\nfrom embedders.tree_new import ProductSpaceDT\nfrom embedders.dataloaders import load\nfrom embedders.coordinate_learning import train_coords\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom tqdm.notebook import tqdm\n\n# from sklearn.model_selection import train_test_split\nfrom scipy.stats import wilcoxon\nimport numpy as np\nimport pandas as pd\nimport torch\n\nUSE_SPECIAL_DIMS = False\nUSE_DISTS = True\nSIGNATURE = [(1, 2), (0, 2), (-1, 2)]\nTEST_SIZE = 0.2\nTOTAL_ITERATIONS = 3000\nMAX_DEPTH = None\nN_TRIALS = 100\n```\n\n----------------------------------------\n\nTITLE: Defining and Training Product Manifold Embedding in Python\nDESCRIPTION: Defines a product manifold using `embedders.manifolds.ProductManifold` with a specific signature `[(1, 2)]` corresponding to a 2-sphere (S^2). It then rescales the loaded city distances to the range [0, 1]. Finally, it trains the embedding coordinates onto the defined manifold using `embedders.coordinate_learning.train_coords`, specifying the manifold, rescaled distances, computation device, and various training hyperparameters like iterations and learning rates.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/1_basic_test.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Specify signature - useful to re-initialize the manifold here\nsignature = [(1, 2)]\npm = embedders.manifolds.ProductManifold(signature=signature)\nprint(pm.name)\n\n# Rescale distances\ndists_rescaled = original_dists / original_dists.max()\n\n# Get embedding\nembedders.coordinate_learning.train_coords(\n    pm,\n    dists_rescaled,\n    device=device,\n    burn_in_iterations=200,\n    training_iterations=200 * 9,\n    learning_rate=1e-1,\n    burn_in_learning_rate=1e-2,\n    scale_factor_learning_rate=1e-1,\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Up and Running Product Manifold Model Ablation Study in Python\nDESCRIPTION: Imports necessary libraries (sklearn, torch, pandas, tqdm, custom `embedders`), defines constants and hyperparameters (e.g., dimensions, number of samples, points, classes, task type). It specifies an ablation variable (`ABLATION_VAR`) and its values (`ABLATION_VALS`) to test different model configurations. The code iterates `N_SAMPLES` times, generating synthetic Gaussian mixture data on a product manifold (`embedders.manifolds.ProductManifold`), splitting it into training and testing sets. It conditionally trains Decision Tree (`embedders.tree_new.ProductSpaceDT`) and/or Random Forest (`embedders.tree_new.ProductSpaceRF`) models, applying the ablation parameter variation. Performance is evaluated using the F1 score (micro average) on both training and test sets. Results for each seed and ablation configuration are collected in a list and finally converted into a pandas DataFrame. The `torch.no_grad()` context manager is used to disable gradient calculations during model fitting and prediction, optimizing performance.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/18_ablations.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom tqdm.notebook import tqdm\nimport torch\nimport embedders\nimport pandas as pd\n\nresults = []\n\nDIM = 2\nN_SAMPLES = 100\nN_POINTS = 1_000\nN_CLASSES = 8\nN_CLUSTERS = 32\nMAX_DEPTH = 5\nCOV_SCALE_MEANS = 1.0\nCOV_SCALE_POINTS = 0.1\nTASK = \"classification\"\n\nSCORE = \"f1-micro\" if TASK == \"classification\" else \"rmse\"\n\n# ABLATION_VAR = \"use_special_dims\"\n# ABLATION_VALS = [False, True]\n\n# ABLATION_VAR = \"max_features\"\n# ABLATION_VALS = [\"sqrt\", \"log2\", \"none\"]\n\n# ABLATION_VAR = \"max_depth\"\n# ABLATION_VALS = [1, 2, 3, 4, 5, 6, None]\n\n# ABLATION_VAR = \"n_features\"\n# ABLATION_VALS = [\"d\", \"d_choose_2\"]\n\nABLATION_VAR = \"ablate_midpoints\"\nABLATION_VALS = [False, True]\n\nUSE_DT = True\n# USE_DT = False\n# USE_RF = True\nUSE_RF = True\n\nwith torch.no_grad():\n    my_tqdm = tqdm(total=N_SAMPLES)\n    for seed in range(N_SAMPLES):\n        # Ensure unique seed per trial\n        seed = seed\n        pm = embedders.manifolds.ProductManifold(signature=[(-1, 2), (0, 2), (1, 2)])\n\n        # Get X, y\n        X, y = embedders.gaussian_mixture.gaussian_mixture(\n            pm=pm,\n            seed=seed,\n            num_points=N_POINTS,\n            num_classes=N_CLASSES,\n            num_clusters=N_CLUSTERS,\n            cov_scale_means=1.0 / DIM,\n            cov_scale_points=0.1 / DIM,\n            task=TASK,\n        )\n\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n\n        # Decision trees\n        if USE_DT:\n            base_dt_args = {\"pm\": pm, \"max_depth\": MAX_DEPTH, \"task\": TASK}\n            dts = {\n                val: embedders.tree_new.ProductSpaceDT(**base_dt_args | {ABLATION_VAR: val}) for val in ABLATION_VALS\n            }\n            for dt in dts.values():\n                dt.fit(X_train, y_train)\n\n        # Random forests\n        if USE_RF:\n            base_rf_args = {\"pm\": pm, \"max_depth\": MAX_DEPTH, \"task\": TASK, \"n_estimators\": 12, \"random_state\": seed}\n            rfs = {\n                val: embedders.tree_new.ProductSpaceRF(**base_rf_args | {ABLATION_VAR: val}) for val in ABLATION_VALS\n            }\n            for rf in rfs.values():\n                rf.fit(X_train, y_train)\n\n        # Get F1-scores\n        y_test = y_test.detach().cpu().numpy()\n        pred = lambda X, clf: clf.predict(X).detach().cpu().numpy()\n        res = {\"seed\": seed}\n        for val in ABLATION_VALS:\n            res[f\"dt_{val}_test\"] = f1_score(y_test, pred(X_test, dts[val]), average=\"micro\") if USE_DT else None\n            res[f\"rf_{val}_test\"] = f1_score(y_test, pred(X_test, rfs[val]), average=\"micro\") if USE_RF else None\n            res[f\"dt_{val}_train\"] = f1_score(y_train, pred(X_train, dts[val]), average=\"micro\") if USE_DT else None\n            res[f\"rf_{val}_train\"] = f1_score(y_train, pred(X_train, rfs[val]), average=\"micro\") if USE_RF else None\n        results.append(res)\n\n        my_tqdm.update(1)\n\nresults = pd.DataFrame(results)\nresults\n```\n\n----------------------------------------\n\nTITLE: Running Empirical Dataset Benchmarks with Product Manifold - Python\nDESCRIPTION: Processes a list of empirical datasets by loading them, subsampling, then benchmarking learned manifold embeddings for either classification or regression objectives. Results are structured and written to a TSV file using pandas. Dependencies include the 'embedders' package, PyTorch, pandas, NumPy, and tqdm; key parameters include dataset names, manifold signatures, N_SAMPLES, and device identifiers.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/43_benchmark_neural.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom tqdm.notebook import tqdm\nimport embedders\nimport pandas as pd\n\nN_SAMPLES = 10\n# N_SAMPLES = 1\nDEVICE = \"cuda\"\nDOWNSAMPLE = 1_000\n\n# my_tqdm = tqdm(total=5 * N_SAMPLES)\nmy_tqdm = tqdm(total=5 * N_SAMPLES)\n\nresults = []\nfor dataset, signature, objective in zip(\n    [\"landmasses\", \"neuron_33\", \"neuron_46\", \"temperature\", \"traffic\"],\n    [[(1, 2)], [(1, 1)] * 10, [(1, 1)] * 10, [(1, 2), (1, 1)], [(0, 1)] + [(1, 1)] * 4],\n    [\"classification\", \"classification\", \"classification\", \"regression\", \"regression\"],\n):\n    score = [\"f1-micro\", \"accuracy\"] if objective == \"classification\" else [\"rmse\"]\n    pm = embedders.manifolds.ProductManifold(signature=signature, device=DEVICE)\n\n    for seed in range(N_SAMPLES):\n        # try:\n        X, y, _ = embedders.dataloaders.load(dataset, seed=seed)\n        # Resample\n        if len(X) > DOWNSAMPLE:\n            random_sample = np.random.choice(X.shape[0], DOWNSAMPLE, replace=False)\n            X = X[random_sample]\n            y = y[random_sample]\n        model_results = embedders.benchmarks.benchmark(\n            X=X,\n            y=y,\n            pm=pm,\n            # max_depth=MAX_DEPTH,\n            # n_features=N_FEATURES,\n            seed=seed,\n            # device=device,\n            # models=MODELS,\n            task=objective,\n            score=score,\n            device=DEVICE\n        )\n\n        # Create a flat dictionary for this run\n        # run_results = {\"dataset\": dataset, \"seed\": 0}\n        model_results[\"dataset\"] = dataset\n        model_results[\"seed\"] = seed\n\n        results.append(model_results)\n        print(results)\n        # except Exception as e:\n            # print(f\"Error: {e}\")\n        my_tqdm.update(1)\n\nresults = pd.DataFrame(results)\n# results.to_csv(f\"../data/results_icml/all_nn_empirical.tsv\", sep=\"\\t\", index=False)\nresults.to_csv(f\"../data/results_icml/all_nn_neurons.tsv\", sep=\"\\t\", index=False)\n\n```\n\n----------------------------------------\n\nTITLE: Building a Simple Graph Neural Network (GNN) for Manifold Data Classification (PyTorch, Python)\nDESCRIPTION: Defines helper functions to construct edge lists and edge weights from pairwise distance matrices, then implements and trains a three-layer GCN-based GNN (SimpleGNN) on tangent-mapped manifold data. Uses torc_geometric, supports dense edge weighting (Gaussian kernel), trains with Adam and cross-entropy loss, and measures accuracy on a test split. Relies on torch, torch_geometric, tqdm, numpy, and a pm object supporting distance and logmap operations. Designed for CUDA/MPS devices.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/41_more_benchmarks.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport torch\\nimport torch.nn as nn\\nfrom torch_geometric.nn import GCNConv\\nimport numpy as np\\n\\nfrom tqdm.notebook import tqdm\\n\\n\\n# Simple GNN model\\nclass SimpleGNN(nn.Module):\\n    def __init__(self, in_channels, hidden_channels, out_channels):\\n        super().__init__()\\n        self.conv1 = GCNConv(in_channels, hidden_channels)\\n        self.conv2 = GCNConv(hidden_channels, hidden_channels)\\n        self.conv3 = GCNConv(hidden_channels, out_channels)\\n\\n    def forward(self, x, edge_index, edge_weight=None):\\n        x = torch.relu(self.conv1(x, edge_index, edge_weight))\\n        x = torch.relu(self.conv2(x, edge_index, edge_weight))\\n        return self.conv3(x, edge_index, edge_weight)\\n\\n\\n# Create edges for a subset of nodes\\ndef get_subset_edges(dist_matrix, node_indices):\\n    # Get submatrix of distances\\n    sub_dist = dist_matrix[node_indices][:, node_indices]\\n\\n    # Create edges based on threshold\\n    threshold = sub_dist.mean()\\n    edges = (sub_dist < threshold).nonzero().t()\\n\\n    return edges\\n\\ndef get_dense_edges(dist_matrix, node_indices):\\n    # Get submatrix of distances\\n    sub_dist = dist_matrix[node_indices][:, node_indices]\\n\\n    # Create dense edges (all-to-all connections)\\n    n = len(node_indices)\\n    rows = torch.arange(n).repeat_interleave(n)\\n    cols = torch.arange(n).repeat(n)\\n    edge_index = torch.stack([rows, cols])\\n\\n    # Get corresponding distances as edge weights\\n    edge_weights = sub_dist.flatten()\\n\\n    # Convert distances to weights (you can modify this function)\\n    edge_weights = torch.exp(-edge_weights)  # Gaussian kernel\\n    # Alternative weightings:\\n    # edge_weights = 1 / (edge_weights + 1e-6)  # Inverse distance\\n    # edge_weights = torch.softmax(-edge_weights, dim=0)  # Softmax of negative distances\\n\\n    return edge_index, edge_weights\\n\\n\\n# Setup\\ndist_matrix = pm.pdist(X).detach()\\nX_tangent = pm.logmap(X).detach()\\ntrain_idx, test_idx = train_test_split(np.arange(len(X)), test_size=0.2)\\n\\n# Model, optimizer, loss\\nmodel = SimpleGNN(in_channels=6, hidden_channels=6, out_channels=8)\\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\\nloss_fn = nn.CrossEntropyLoss()\\n\\n# Get edges for training set\\n# train_edges = get_subset_edges(dist_matrix, train_idx)\\ntrain_edges, train_weights = get_dense_edges(dist_matrix, train_idx)\\n\\n# Move to Mac\\nmodel = model.to(\"mps\")\\nX_tangent = X_tangent.to(\"mps\")\\ny = y.to(\"mps\")\\ntrain_edges = train_edges.to(\"mps\")\\ntrain_weights = train_weights.to(\"mps\")\\n\\n\\n# Training loop\\nmy_tqdm = tqdm(range(10_000))\\nfor i in my_tqdm:\\n    model.train()\\n    optimizer.zero_grad()\\n\\n    # Only use training data\\n    X_train = X_tangent[train_idx]\\n    y_train = y[train_idx]\\n\\n    # y_pred = model(X_train, train_edges)\\n    y_pred = model(X_train, train_edges, train_weights)\\n    loss = loss_fn(y_pred, y_train)\\n    loss.backward()\\n    optimizer.step()\\n\\n    if i % 10 == 0:\\n        my_tqdm.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\\n\\n# Evaluate\\nmodel.eval()\\nwith torch.no_grad():\\n    # Get edges for test set\\n    # test_edges = get_subset_edges(dist_matrix, test_idx)\\n    test_edges, test_weights = get_dense_edges(dist_matrix, test_idx)\\n\\n    test_edges = test_edges.to(\"mps\")\\n    test_weights = test_weights.to(\"mps\")\\n\\n    # Make predictions on test set\\n    X_test = X_tangent[test_idx]\\n    y_test = y[test_idx]\\n\\n    # y_pred = model(X_test, test_edges)\\n    y_pred = model(X_test_tangent, test_edges, test_weights)\\n    acc = (y_pred.argmax(1) == y_test).float().mean().item()\\nprint(f\"Test accuracy: {acc:.4f}\")\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Node Classification on Graph Embeddings in Python\nDESCRIPTION: This snippet benchmarks node classification performance using specific graph embeddings and manifold signatures. It imports necessary libraries (`embedders`, `pandas`, `tqdm`) and defines classification datasets (`CLS_NAMES`) and corresponding signatures. It iterates through dataset-signature pairs and trials, loading pre-computed embeddings (`X_train`, `X_test`), labels (`y_train`, `y_test`), and the original adjacency matrix. It computes the normalized adjacency matrix (`A_hat`) and uses `embedders.benchmarks.benchmark` to run classification models (likely GCN-based, given `kappa_gcn`) on the GPU (`cuda`). Accuracy and F1-micro scores are recorded along with metadata in the `results` list, which is then converted to a Pandas DataFrame.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/24_benchmark_graph_embeddings.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport embedders\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\n# MAX_DEPTH = 3\n# MAX_DEPTH = None\n# N_FEATURES = \"d_choose_2\"\n\nCLS_NAMES = [\"polblogs\", \"cora\", \"citeseer\"]\n\nSIGNATURES = [\n    [(1, 2), (1, 2)], # SS\n    [(-1, 4)], # H\n    [(-1, 2), (1, 2)], # HS\n]\n\nSIGNATURES_STR = [\"SS\", \"H\", \"HS\"]\n\nresults = []\nmy_tqdm = tqdm(total=len(CLS_NAMES) * n_trials)# * len(SIGNATURES))\nfor embedding, signature, sigstr in zip(CLS_NAMES, SIGNATURES, SIGNATURES_STR):\n    _, _, adj = embedders.dataloaders.load(embedding)\n    # dists = pd.read_table(f\"../data/graphs/embeddings/{embedding}_dists.tsv\")\n    \n    # Get the lowest D_avg signature\n    \n    # for sig, sig_str in zip(SIGNATURES, SIGNATURES_STR):\n    pm = embedders.manifolds.ProductManifold(signature=sig)\n    # dists_sig = dists[dists[\"signature\"] == str(sig)]\n\n    # for i, trial in enumerate(dists_sig[\"seed\"]):\n    for i in range(n_trials):\n        # i is used to load the correct embedding\n        # trial is used to get the correct d_avg\n        # I'm sorry for this mess\n        data = torch.load(f\"../data/graphs/embeddings/{embedding}/{sig_str}_{i}.h5\", weights_only=True)\n        X_train = data[\"X_train\"]\n        X_test = data[\"X_test\"]\n        y_train = data[\"y_train\"]\n        y_test = data[\"y_test\"]\n        test_idx = data[\"test_idx\"]\n        train_idx = [i for i in range(len(X_train) + len(X_test)) if i not in test_idx]\n\n        # Get adjacency matrix\n        A_hat = embedders.predictors.kappa_gcn.get_A_hat(adj.float())\n\n        res = embedders.benchmarks.benchmark(\n            X=None,\n            y=None,\n            X_train=X_train,\n            X_test=X_test,\n            y_train=y_train,\n            y_test=y_test,\n            pm=pm,\n            device=\"cuda\",\n            A_train = A_hat[train_idx][:, train_idx],\n            A_test = A_hat[test_idx][:, test_idx],\n            task=\"classification\",\n            score=[\"accuracy\", \"f1-micro\"],\n        )\n        res[\"embedding\"] = embedding\n        res[\"signature\"] = sig_str\n        res[\"trial\"] = trial\n\n        # Get matching d_avg using seed and sig_str\n        # dists_seed = dists_sig[dists_sig[\"seed\"] == trial]\n        # assert len(dists_seed) == 1\n        # res[\"d_avg\"] = dists_seed[\"d_avg\"].values[0]\n\n        results.append(res)\n        my_tqdm.update(1)\n\nresults = pd.DataFrame(results)\n```\n\n----------------------------------------\n\nTITLE: Training Manifold Embeddings with Train-Test Split in Python\nDESCRIPTION: Initializes a `ProductManifold` with a specified signature using `embedders.manifolds.ProductManifold`. It then defines a test set by randomly sampling 20% of the data indices. Finally, it trains coordinate embeddings using `embedders.coordinate_learning.train_coords`, providing the manifold, distances, test indices, burn-in/training iterations, and learning rate. The function returns the learned embeddings (`x_embed`) and a dictionary of losses (`losses`). Dependencies include `torch`, `numpy`, and the `embedders` library, as well as the `dists` and `labels` variables from the previous step.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/48_fix_transductive_coords.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport numpy as np\n\npm = embedders.manifolds.ProductManifold(signature=[(-1, 2), (0, 2), (1, 2)])\n\n# Get test indices\nn = len(labels)\ntest_idx = np.random.choice(n, size=int(n * 0.2), replace=False)\n\n# Get embedders\nx_embed, losses = embedders.coordinate_learning.train_coords(\n    pm=pm, dists=dists, test_indices=test_idx, burn_in_iterations=200, training_iterations=1800, learning_rate=1e-1\n)\n```\n\n----------------------------------------\n\nTITLE: Exhaustive Benchmark: Generating Embeddings\nDESCRIPTION: This comprehensive Python snippet performs an exhaustive benchmark by iterating through multiple datasets and predefined manifold signatures. For each combination, it runs multiple trials, loads data, trains an embedding using 'embedders.coordinate_learning.train_coords' with specific hyperparameters and non-transductive distance rescaling, calculates the average distance distortion 'd_avg', and saves the train/test embeddings, labels, and test indices to an HDF5 file. It includes error handling to skip failed trials and saves aggregated 'd_avg' results to a TSV file per dataset. It manages seeding across trials.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/2_polblogs_benchmark.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom embedders.metrics import d_avg\nimport pandas as pd\nimport os\nfrom sklearn.model_selection import train_test_split\n\n# Hyperparameters\nITERS = 10_000\nTRIALS = 10\nLR = 1e-2\n\nseed = 0\nfor DATASET in [\"polblogs\", \"cora\", \"citeseer\", \"cs_phds\"]:\n    dists, labels, _ = embedders.dataloaders.load(DATASET)\n    dists = dists.to(device)\n\n    results = []\n    for i, (signature, signature_str) in enumerate(zip(SIGNATURES, SIGNATURES_STR)):\n        j = 0\n        while j < TRIALS:\n            # Get manifold\n            pm = embedders.manifolds.ProductManifold(signature=signature).to(device)\n            \n            # Skip existing\n            if os.path.exists(f\"../data/graphs/embeddings/{DATASET}/{signature_str}_{j}.h5\"):\n                print(f\"Skipping {signature_str}_{j}\")\n                j += 1\n                continue\n\n            torch.manual_seed(seed)\n\n            # Generate embedding\n            train_idx, test_idx = train_test_split(torch.arange(len(dists)), test_size=0.2, random_state=seed)\n            D_train = dists[train_idx][:, train_idx]\n            dists_rescaled = dists / D_train[D_train.isfinite()].max()\n            try:\n                X, losses = embedders.coordinate_learning.train_coords(\n                    pm,\n                    dists_rescaled, # Rescale distances in a non-transductive way\n                    test_indices=test_idx,\n                    device=device,\n                    burn_in_iterations=int(ITERS * 0.2),\n                    training_iterations=int(ITERS * 0.8),\n                    learning_rate=LR,\n                    burn_in_learning_rate=LR / 10,\n                    scale_factor_learning_rate=LR / 10,\n                )\n\n                # Calculate d_avg\n                X_train = X[train_idx]\n                X_test = X[test_idx]\n                y_train = labels[train_idx]\n                y_test = labels[test_idx]\n                my_d_avg = d_avg(pm.pdist(X_train), dists_rescaled[train_idx][:, train_idx]).item()\n\n                # Save as a single file\n                torch.save(\n                    {\"X_train\": X_train, \"X_test\": X_test, \"y_train\": y_train, \"y_test\": y_test, \"test_idx\": test_idx}, \n                    f\"../data/graphs/embeddings/{DATASET}/{signature_str}_{j}.h5\"\n                )\n\n                # Results should just be average distances\n                results.append({\"signature\": signature, \"seed\": seed, \"d_avg\": my_d_avg})\n                j += 1\n            except Exception as e:\n                pass\n            seed += 1 # Increment seed even if we need to restart a trial\n\n    results_df = pd.DataFrame(results)\n    results_df.to_csv(f\"../data/graphs/embeddings/{DATASET}_dists.tsv\", sep=\"\\t\")\n```\n\n----------------------------------------\n\nTITLE: Generating and Evaluating Gaussian Mixture Data on Product Manifold in Python\nDESCRIPTION: This snippet demonstrates generating synthetic data from a Gaussian mixture model defined on a product manifold and evaluating it. It first filters potential UserWarnings during sampling, defines a `ProductManifold` with a specific signature (including hyperbolic, spherical, and Euclidean components), generates data points `X` and labels `y` using `gaussian_mixture.gaussian_mixture`, splits the data, and then trains a `ProductSpaceDT` classifier on the synthetic training data. Finally, it evaluates the classifier's performance on the test set using the micro-averaged F1 score.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/Embedder Tutorial.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom embedders import gaussian_mixture\n# Filter out warnings raised when sampling Wishart distribution in Gaussian mixtures\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n#Gaussian mixture generator\npm = manifolds.ProductManifold(signature=[(-1, 2),(2, 3),(0, 5)])\nX, y = gaussian_mixture.gaussian_mixture(pm=pm, seed=42, cov_scale_means=1.0, cov_scale_points=1.0)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n#Example of apply gaussian mixture to product space decision tree\npdt = tree_new.ProductSpaceDT(pm=pm, max_depth=3, use_special_dims=True)\npdt.fit(X_train, y_train)\npdt_f1 = f1_score(y_test, pdt.predict(X_test),average=\"micro\")\nprint(f\"ProductDT\\t{pdt_f1*100:.2f}\")\n```\n\n----------------------------------------\n\nTITLE: Running Manifold Embedding and Link Prediction Experiments with Manify in Python\nDESCRIPTION: This snippet iterates over datasets and random seeds, loads data, normalizes distances, and applies the manify library's coordinate learning approach to compute geometric node embeddings. For each trial, it transforms data appropriately for several manifold-based predictors (GCN, MLP, MLR variants), fits the models, benchmarks them on link prediction tasks, and collects relevant performance metrics in a results list. Dependencies include manify, torch, sklearn, and typical machine learning workflow utilities. Inputs are graph datasets and experiment parameters; outputs are accuracy, F1-scores, runtime, and related statistics for each model. All code requires GPU setup and appropriate dataset/model configuration.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/51_gcn_link_prediction.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# for dataset in [\"karate_club\"]:\nmy_tqdm = tqdm(total=N_TRIALS * len(DATASETS))\nfor i, dataset in enumerate(DATASETS):\n    dists, _, adj = manify.utils.dataloaders.load(dataset)\n    dists, adj = dists.to(DEVICE), adj.to(DEVICE)\n    dists = dists / dists[dists.isfinite()].max()\n\n    # while len(results) < N_TRIALS:\n    for seed in range(N_TRIALS):\n        seed = seed + i * N_TRIALS  # Unique\n        pm = manify.manifolds.ProductManifold(signature=SIGNATURE, device=DEVICE)\n        X, _ = manify.embedders.coordinate_learning.train_coords(\n            pm=pm,\n            dists=dists,\n            burn_in_iterations=int(0.1 * TOTAL_ITERATIONS),\n            training_iterations=int(0.9 * TOTAL_ITERATIONS),\n            scale_factor_learning_rate=0.02,\n            device=DEVICE,\n        )\n        assert not torch.isnan(X).any()\n\n        # Get data for classification variants\n        XX, yy, pm_new = make_link_prediction_dataset(X, pm, adj, add_dists=USE_DISTS)\n        X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n            XX, yy, list(range(len(yy))), test_size=0.2\n        )\n        X_train = X_train[:1000]\n        X_test = X_test[:1000]\n        y_train = y_train[:1000]\n        y_test = y_test[:1000]\n        idx_train = idx_train[:1000]\n        idx_test = idx_test[:1000]\n        res = manify.utils.benchmarks.benchmark(\n            # XX,\n            # yy,\n            None,\n            None,\n            X_train = X_train,\n            X_test = X_test,\n            y_train = y_train,\n            y_test = y_test,\n            pm = pm_new,\n            task=\"classification\",\n            score=[\"accuracy\", \"f1-micro\"],\n            device=DEVICE,\n            models=MODELS,\n            seed=seed,\n        )\n\n        # Other manifolds we'll need\n        pm_stereo, X_stereo = pm.stereographic(X)\n        pm_stereo_euc = manify.manifolds.ProductManifold(\n            signature=[(0, X.shape[1])], stereographic=True, device=DEVICE\n        )\n\n        # Get an adjacency matrix that's not leaky\n        dists = pm.pdist2(X)\n        max_dist = dists[dists.isfinite()].max()\n        dists /= max_dist\n        A = torch.exp(-dists)\n        A_hat = manify.predictors.kappa_gcn.get_A_hat(A).float().to(DEVICE)\n\n        # Ambient GCN\n        agnn = manify.predictors.kappa_gcn.KappaGCN(\n            pm=pm_stereo_euc, output_dim=1, hidden_dims=[pm_stereo_euc.dim], task=\"link_prediction\"\n        ).to(DEVICE)\n        t1 = time.time()\n        agnn.fit(X=X, y=y_train, A=A_hat, lr=LR, epochs=EPOCHS, lp_indices=idx_train, use_tqdm=USE_TQDM)\n        t2 = time.time()\n        y_pred = agnn.predict(X, A_hat)[idx_test]\n        res[\"ambient_gcn_accuracy\"] = (y_pred == y_test).float().mean().item()\n        res[\"ambient_gcn_f1_micro\"] = f1_score(y_test.cpu(), y_pred.cpu(), average=\"micro\")\n        res[\"ambient_gcn_time\"] = t2 - t1\n\n        # Tangent GCN\n        tgcn = manify.predictors.kappa_gcn.KappaGCN(\n            pm=pm_stereo, output_dim=1, hidden_dims=[pm_stereo.dim], task=\"link_prediction\"\n        ).to(DEVICE)\n        t1 = time.time()\n        tgcn.fit(X=pm.logmap(X).detach(), y=y_train, A=A_hat, lr=LR, epochs=EPOCHS, lp_indices=idx_train, use_tqdm=USE_TQDM)\n        t2 = time.time()\n        y_pred = tgcn.predict(pm.logmap(X).detach(), A_hat)[idx_test]\n        res[\"tangent_gcn_accuracy\"] = (y_pred == y_test).float().mean().item()\n        res[\"tangent_gcn_f1_micro\"] = f1_score(y_test.cpu(), y_pred.cpu(), average=\"micro\")\n        res[\"tangent_gcn_time\"] = t2 - t1\n\n        # Kappa GCN\n        kgcn = manify.predictors.kappa_gcn.KappaGCN(\n            pm=pm_stereo, output_dim=1, hidden_dims=[pm_stereo.dim], task=\"link_prediction\"\n        ).to(DEVICE)\n        t1 = time.time()\n        kgcn.fit(X=X_stereo, y=y_train, A=A_hat, lr=LR, epochs=EPOCHS, lp_indices=idx_train, use_tqdm=USE_TQDM)\n        t2 = time.time()\n        y_pred = kgcn.predict(X_stereo, A_hat)[idx_test]\n        res[\"kappa_gcn_accuracy\"] = (y_pred == y_test).float().mean().item()\n        res[\"kappa_gcn_f1_micro\"] = f1_score(y_test.cpu(), y_pred.cpu(), average=\"micro\")\n        res[\"kappa_gcn_time\"] = t2 - t1\n\n        # Ambient MLP\n        amlp = manify.predictors.kappa_gcn.KappaGCN(\n            pm=pm_stereo, output_dim=1, hidden_dims=[pm_stereo.dim], task=\"link_prediction\"\n        ).to(DEVICE)\n        t1 = time.time()\n        amlp.fit(X=X, y=y_train, A=torch.eye(len(X), device=DEVICE), lr=LR, epochs=EPOCHS, lp_indices=idx_train, use_tqdm=USE_TQDM)\n        t2 = time.time()\n        y_pred = amlp.predict(X, torch.eye(len(X), device=DEVICE))[idx_test]\n        res[\"ambient_mlp_accuracy\"] = (y_pred == y_test).float().mean().item()\n        res[\"ambient_mlp_f1_micro\"] = f1_score(y_test.cpu(), y_pred.cpu(), average=\"micro\")\n        res[\"ambient_mlp_time\"] = t2 - t1\n\n        # Tangent MLP\n        tmlp = manify.predictors.kappa_gcn.KappaGCN(\n            pm=pm_stereo, output_dim=1, hidden_dims=[pm_stereo.dim], task=\"link_prediction\"\n        ).to(DEVICE)\n        t1 = time.time()\n        tmlp.fit(X=pm.logmap(X).detach(), y=y_train, A=torch.eye(len(X), device=DEVICE), lr=LR, epochs=EPOCHS, lp_indices=idx_train, use_tqdm=USE_TQDM)\n        t2 = time.time()\n        y_pred = tmlp.predict(pm.logmap(X).detach(), torch.eye(len(X), device=DEVICE))[idx_test]\n        res[\"tangent_mlp_accuracy\"] = (y_pred == y_test).float().mean().item()\n        res[\"tangent_mlp_f1_micro\"] = f1_score(y_test.cpu(), y_pred.cpu(), average=\"micro\")\n        res[\"tangent_mlp_time\"] = t2 - t1\n\n        # Ambient MLR\n        amlr = manify.predictors.kappa_gcn.KappaGCN(\n            pm=pm_stereo, output_dim=1, hidden_dims=[], task=\"link_prediction\"\n        ).to(DEVICE)\n        t1 = time.time()\n        amlr.fit(X=X, y=y_train, A=torch.eye(len(X), device=DEVICE), lr=LR, epochs=EPOCHS, lp_indices=idx_train, use_tqdm=USE_TQDM)\n        t2 = time.time()\n        y_pred = amlr.predict(X, torch.eye(len(X), device=DEVICE))[idx_test]\n        res[\"ambient_mlr_accuracy\"] = (y_pred == y_test).float().mean().item()\n        res[\"ambient_mlr_f1_micro\"] = f1_score(y_test.cpu(), y_pred.cpu(), average=\"micro\")\n        res[\"ambient_mlr_time\"] = t2 - t1\n\n        # Tangent MLR\n        tmlr = manify.predictors.kappa_gcn.KappaGCN(\n            pm=pm_stereo, output_dim=1, hidden_dims=[], task=\"link_prediction\"\n        ).to(DEVICE)\n        t1 = time.time()\n        tmlr.fit(X=pm.logmap(X).detach(), y=y_train, A=torch.eye(len(X)), lr=LR, epochs=EPOCHS, lp_indices=idx_train, use_tqdm=USE_TQDM)\n        t2 = time.time()\n        y_pred = tmlr.predict(pm.logmap(X).detach(), torch.eye(len(X), device=DEVICE))[idx_test]\n        res[\"tangent_mlr_accuracy\"] = (y_pred == y_test).float().mean().item()\n        res[\"tangent_mlr_f1_micro\"] = f1_score(y_test.cpu(), y_pred.cpu(), average=\"micro\")\n        res[\"tangent_mlr_time\"] = t2 - t1\n\n        # Product MLR\n        mlr = manify.predictors.kappa_gcn.KappaGCN(\n            pm=pm_stereo, output_dim=1, hidden_dims=[], task=\"link_prediction\"\n        ).to(DEVICE)\n        t1 = time.time()\n        kgcn.fit(X=X_stereo, y=y_train, A=torch.eye(len(X_stereo), device=DEVICE), lr=LR, epochs=EPOCHS, lp_indices=idx_train, use_tqdm=USE_TQDM)\n        t2 = time.time()\n        y_pred = kgcn.predict(X_stereo, torch.eye(len(X_stereo), device=DEVICE))[idx_test]\n        res[\"kappa_mlr_accuracy\"] = (y_pred == y_test).float().mean().item()\n        res[\"kappa_mlr_f1_micro\"] = f1_score(y_test.cpu(), y_pred.cpu(), average=\"micro\")\n        res[\"kappa_mlr_time\"] = t2 - t1\n\n        # Other details\n        res[\"d_avg\"] = manify.embedders.losses.d_avg(pm.pdist(X), dists).item()\n        res[\"dataset\"] = dataset\n\n        results.append(res)\n        my_tqdm.update(1)\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating a Product Space Decision Tree in Python\nDESCRIPTION: Splits the generated data (X, y) into training and testing sets using `sklearn.model_selection.train_test_split`. Initializes a `ProductSpaceDT` classifier from the `embedders` library, configured for the previously defined product manifold `pm`, using 'd_choose_2' features (pairs of dimensions), and a maximum depth of 3. Trains the classifier (`pdt`) on the training data and prints its accuracy score on the test set.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_pc_basic_visualization.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\npdt = embedders.predictors.tree_new.ProductSpaceDT(pm=pm, n_features=\"d_choose_2\", max_depth=3)\npdt.fit(X_train, y_train)\n\nprint(f\"{pdt.score(X_test, y_test).float().mean().item():.4f}\")\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Node Regression on Graph Embeddings in Python\nDESCRIPTION: This snippet benchmarks node regression performance using specific graph embeddings and manifold signatures, similar to the classification benchmark but configured for regression. It defines regression datasets (`REG_NAMES`) and corresponding signatures. It iterates through dataset-signature pairs and trials, loading pre-computed embeddings, labels (likely continuous values for regression), and the adjacency matrix. It computes the normalized adjacency matrix (`A_hat`) and uses `embedders.benchmarks.benchmark` configured for a 'regression' task on the GPU (`cuda`), calculating the Root Mean Squared Error (RMSE). Results are collected in the `results2` list and converted to a Pandas DataFrame.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/24_benchmark_graph_embeddings.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nREG_NAMES = [\"cs_phds\"]\n\nSIGNATURES = [\n    [(-1, 4)], # H\n]\n\nSIGNATURES_STR = [\"H\"]\n\nresults2 = []\nmy_tqdm = tqdm(total=len(REG_NAMES) * n_trials)# * len(SIGNATURES))\nfor embedding, signature, sigstr in zip(REG_NAMES, SIGNATURES, SIGNATURES_STR):\n    _, _, adj = embedders.dataloaders.load(embedding)\n    # dists = pd.read_table(f\"../data/graphs/embeddings/{embedding}_dists.tsv\")\n    \n    # Get the lowest D_avg signature\n    \n    # for sig, sig_str in zip(SIGNATURES, SIGNATURES_STR):\n    pm = embedders.manifolds.ProductManifold(signature=sig)\n    # dists_sig = dists[dists[\"signature\"] == str(sig)]\n\n    # for i, trial in enumerate(dists_sig[\"seed\"]):\n    for i in range(n_trials):\n        # i is used to load the correct embedding\n        # trial is used to get the correct d_avg\n        # I'm sorry for this mess\n        data = torch.load(f\"../data/graphs/embeddings/{embedding}/{sig_str}_{i}.h5\", weights_only=True)\n        X_train = data[\"X_train\"]\n        X_test = data[\"X_test\"]\n        y_train = data[\"y_train\"]\n        y_test = data[\"y_test\"]\n        test_idx = data[\"test_idx\"]\n        train_idx = [i for i in range(len(X_train) + len(X_test)) if i not in test_idx]\n\n        # Get adjacency matrix\n        A_hat = embedders.predictors.kappa_gcn.get_A_hat(adj.float())\n\n        res = embedders.benchmarks.benchmark(\n            X=None,\n            y=None,\n            X_train=X_train,\n            X_test=X_test,\n            y_train=y_train,\n            y_test=y_test,\n            pm=pm,\n            device=\"cuda\",\n            A_train = A_hat[train_idx][:, train_idx],\n            A_test = A_hat[test_idx][:, test_idx],\n            # task=\"classification\",\n            # score=[\"accuracy\", \"f1-micro\"],\n            task=\"regression\",\n            score=[\"rmse\"]\n        )\n        res[\"embedding\"] = embedding\n        res[\"signature\"] = sig_str\n        res[\"trial\"] = trial\n\n        # Get matching d_avg using seed and sig_str\n        # dists_seed = dists_sig[dists_sig[\"seed\"] == trial]\n        # assert len(dists_seed) == 1\n        # res[\"d_avg\"] = dists_seed[\"d_avg\"].values[0]\n\n        results2.append(res)\n        my_tqdm.update(1)\n\nresults2 = pd.DataFrame(results2)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Regression on Product Manifolds with embedders - Python\nDESCRIPTION: Benchmarks regression models using embedders with a product manifold and evaluates model performance through repeated trials on temperature data. Loads data and product manifold, subsamples random points per trial, benchmarks using the manifold, and records results for statistical summarization. Requires embedders, pandas, tqdm, and numpy; key parameters include number of trials, points, features, manifold signature, and regression settings.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/25_benchmark_globe.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport embedders\\nimport pandas as pd\\nfrom tqdm.notebook import tqdm\\nimport numpy as np\\n\\nN_TRIALS = 10\\nN_POINTS = 1_000\\nN_FEATURES = \"d_choose_2\"\\nMAX_DEPTH = 5\\n\\nTASK = \"regression\"\\nSCORE = [\"rmse\"]\\n\\nX, y, _ = embedders.dataloaders.load(\"temperature\")\\n\\npm = embedders.manifolds.ProductManifold(signature=[(1, 2), (1, 1)], device=\"cuda\")\\n\\nresults = []\\nfor seed in tqdm(range(N_TRIALS)):\\n    # Sample 1000 points randomly\\n    idx = np.random.choice(X.shape[0], N_POINTS, replace=False)\\n    res = embedders.benchmarks.benchmark(\\n        X=X[idx],\\n        y=y[idx],\\n        pm=pm,\\n        seed=seed,\\n        task=\"regression\",\\n        score=[\"rmse\"],\\n        device=\"cuda\"\\n    )\\n    res[\"seed\"] = seed\\n    results.append(res)\\n\\nresults = pd.DataFrame(results)\\n# results.to_csv(\"../data/results/temperature.csv\", index=False)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Product Space Perceptron Class\nDESCRIPTION: This code defines a custom Python class `ProductSpacePerceptron` inheriting from `sklearn.base.BaseEstimator` and `ClassifierMixin`. It implements a perceptron algorithm tailored for product manifolds using PyTorch. The `fit` method trains the perceptron using a one-vs-rest approach for multiclass problems, calculating kernel values based on the geometry (Euclidean, Spherical, Hyperbolic) of each manifold component. It includes early stopping based on lack of improvement (`patience`). The `predict` method uses the trained classifiers to predict labels for new data.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom sklearn.base import BaseEstimator, ClassifierMixin\n\n\nclass ProductSpacePerceptron(BaseEstimator, ClassifierMixin):\n    def __init__(self, pm, max_epochs=1000, patience=5):\n        self.pm = pm  # ProductManifold instance\n        self.max_epochs = max_epochs\n        self.patience = patience  # Number of consecutive epochs without improvement to consider convergence\n        self.classes_ = None\n        self.classifiers_ = {}  # Dictionary to store classifiers for one-vs-rest approach\n        self.R = []  # To store maximum radius for each hyperbolic manifold\n\n    def fit(self, X, y):\n        # Identify unique classes for multiclass classification\n        self.classes_ = torch.unique(y).tolist()\n\n        # Compute maximum hyperbolic radii for each hyperbolic manifold\n        self.R = [0] * len(self.pm.P)\n        for i, (M, x) in enumerate(zip(self.pm.P, self.pm.factorize(X))):\n            if M.type == \"H\":\n                self.R[i] = torch.sqrt(\n                    torch.abs(M.manifold.inner(x, x).max())\n                ).item()  # Use absolute value for Minkowski norm\n\n        # Relabel y to -1 and 1 for binary classification per class\n        for class_label in self.classes_:\n            # Binary classification shortcut\n            if len(self.classes_) == 2 and class_label == self.classes_[1]:\n                self.classifiers_[class_label] = -1 * self.classifiers_[self.classes_[0]]\n\n            else:\n                print(f\"Training perceptron for class {class_label} vs. rest\")\n                binary_y = torch.where(y == class_label, 1, -1)  # One-vs-rest relabeling\n\n                # Initialize decision function g for this binary classifier\n                g = torch.zeros(X.shape[1], dtype=X.dtype, device=X.device)\n\n                n_epochs = 0\n                epochs_without_improvement = 0  # Track consecutive epochs without improvement\n                best_error_count = float(\"inf\")  # Best error count seen so far\n\n                while n_epochs < self.max_epochs:\n                    errors = 0\n                    for n in range(X.shape[0]):\n                        # Compute the decision function value for the current point\n                        decision_value = g @ X[n]\n\n                        # Check if the point is misclassified\n                        if torch.sign(decision_value) != binary_y[n]:\n                            # Calculate the kernel K(x, x_n) for the current point x_n\n                            K = torch.ones(X.shape[0], dtype=X.dtype, device=X.device)  # Start with the bias term\n\n                            for i, (M, x) in enumerate(zip(self.pm.P, self.pm.factorize(X))):\n                                # Compute kernel matrix between x[n:n+1] and all training points\n                                if M.type == \"E\":\n                                    K += M.scale * M.manifold.inner(x[n : n + 1], x)  # Kernel matrix for Euclidean\n                                elif M.type == \"S\":\n                                    K += M.scale * torch.asin(\n                                        torch.clamp(M.manifold.inner(x[n : n + 1], x), -1, 1)\n                                    )  # Kernel matrix for Spherical\n                                elif M.type == \"H\":\n                                    K += M.scale * torch.asin(\n                                        torch.clamp((self.R[i] ** -2) * M.manifold.inner(x[n : n + 1], x), -1, 1)\n                                    )  # Kernel matrix for Hyperbolic\n\n                            # Update decision function using the computed kernel\n                            g += binary_y[n] * X[n]  # Update with current point only\n                            errors += 1  # Track the number of errors in this epoch\n\n                    # Convergence check based on error improvement\n                    if errors < best_error_count:\n                        best_error_count = errors\n                        epochs_without_improvement = 0  # Reset the counter if we have an improvement\n                    else:\n                        epochs_without_improvement += 1\n\n                    if epochs_without_improvement >= self.patience:\n                        print(f\"Converged for class {class_label} after {n_epochs} epochs (no improvement).\")\n                        break\n\n                    n_epochs += 1\n\n                # Store the classifier (decision function) for the current class\n                self.classifiers_[class_label] = g\n\n        return self\n\n    def predict(self, X):\n        # Initialize matrix to store decision values for each class\n        decision_values = torch.zeros((X.shape[0], len(self.classes_)), dtype=X.dtype, device=X.device)\n\n        # Compute decision values for each classifier\n        for idx, class_label in enumerate(self.classes_):\n            g = self.classifiers_[class_label]\n            decision_values[:, idx] = X @ g\n\n        # Return the class with the highest decision value\n        print(decision_values)\n        argmax_idx = torch.argmax(decision_values, dim=1)\n        return torch.tensor([self.classes_[i] for i in argmax_idx])\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Models on Graph Embeddings with Known Adjacency Matrix\nDESCRIPTION: Runs benchmark experiments on pre-computed graph embeddings for various datasets (`citeseer`, `cora`, `polblogs`, `cs_phds`). It iterates through the defined `GRAPHS`, loading the corresponding labels (`y`), adjacency matrix (`adj`), and pre-computed node embeddings (`X`) for each trial seed. A `ProductManifold` is created based on the predefined signature for the graph. It then benchmarks models using `embedders.benchmarks.benchmark`, passing the adjacency matrix (`adj`) which is necessary for graph-aware models like GNNs. Results are collected and saved to a single TSV file.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/43_benchmark_neural.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nGRAPHS = [\n    (\"citeseer\", \"HS\", [(-1, 2), (1, 2)], \"classification\"),\n    (\"cora\", \"H\", [(-1, 4)], \"classification\"),\n    (\"polblogs\", \"SS\", [(1, 2), (1, 2)], \"classification\"),\n    (\"cs_phds\", \"H\", [(-1, 4)], \"regression\"),\n]\n\nresults = []\nmy_tqdm = tqdm(total=len(GRAPHS) * N_SAMPLES)\nfor embedding, sigstr, sig, task in GRAPHS:\n    _, y, adj = embedders.dataloaders.load(embedding)\n    adj = adj.to(device).float().detach()\n    pm = embedders.manifolds.ProductManifold(signature=sig, device=device)\n\n    for i in range(N_SAMPLES):\n        try:\n            X = torch.tensor(np.load(f\"embedders/data/graphs/embeddings/{embedding}/{sigstr}_{i}.npy\"), device=device)\n            score = [\"f1-micro\", \"accuracy\"] if task == \"classification\" else [\"rmse\"]\n\n            model_results = embedders.benchmarks.benchmark(\n                X,\n                y,\n                pm,\n                max_depth=MAX_DEPTH,\n                n_features=N_FEATURES,\n                seed=i,\n                device=device,\n                adj=adj,\n                models=MODELS,\n                task=task,\n                score=score,\n            )\n\n            # Create a flat dictionary for this run\n            run_results = {\"embedding\": embedding, \"seed\": i}\n\n            # Flatten the nested model results\n            for model, metrics in model_results.items():\n                for metric, value in metrics.items():\n                    run_results[f\"{model}_{metric}\"] = value\n\n            results.append(run_results)\n        except Exception as e:\n            print(f\"Error: {e}\")\n        my_tqdm.update(1)\n\nresults = pd.DataFrame(results)\nresults.to_csv(f\"embedders/data/results/all_nn_graph.tsv\", sep=\"\\t\", index=False)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Link Prediction on Graph Datasets - Python\nDESCRIPTION: Benchmarks link prediction tasks for a variety of graph datasets by fitting node embeddings, generating link prediction samples, and evaluating several models on random train/test splits. Uses a product manifold structure for embeddings, applies custom node-based splits, filters nodes for size, and writes results to a TSV file. Assumes availability of custom embedders modules, torch, NumPy, pandas, and tqdm.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/43_benchmark_neural.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Karate club\nUSE_DISTS = True\nSIGNATURE = [(1, 2), (0, 2), (-1, 2)]\nTOTAL_ITERATIONS = 1_000\n\nresults = []\nmy_tqdm = tqdm(total=N_SAMPLES * 6)\n\nfor dataset in [\"adjnoun\", \"dolphins\", \"football\", \"karate_club\", \"lesmis\", \"polbooks\"]:\n    dists, labels, adj = embedders.dataloaders.load(dataset)\n    dists = dists / dists.max()\n\n    results_dataset = []\n    i = -1\n    while len(results_dataset) < N_SAMPLES:\n        i += 1\n        pm = embedders.manifolds.ProductManifold(signature=SIGNATURE)\n        try:\n            X_embed, losses = embedders.coordinate_learning.train_coords(\n                pm,\n                dists,\n                burn_in_iterations=int(0.1 * TOTAL_ITERATIONS),\n                training_iterations=int(0.9 * TOTAL_ITERATIONS),\n                scale_factor_learning_rate=0.02,\n            )\n            assert not torch.isnan(X_embed).any()\n\n            X, y, pm_new = embedders.link_prediction.make_link_prediction_dataset(X_embed, pm, adj, add_dists=USE_DISTS)\n            X = X.to(device).detach()\n            y = y.to(device).detach()\n            X_train, X_test, y_train, y_test = split_dataset(X, y, test_size=0.2, random_state=i)\n            if len(X_train) > DOWNSAMPLE:\n                idx = np.random.choice(X_train.shape[0], DOWNSAMPLE, replace=False)\n                X_train = X_train[idx]\n                y_train = y_train[idx]\n            if len(X_test) > DOWNSAMPLE:\n                idx = np.random.choice(X_test.shape[0], DOWNSAMPLE, replace=False)\n                X_test = X_test[idx]\n                y_test = y_test[idx]\n\n            model_results = embedders.benchmarks.benchmark(\n                X=None,\n                y=None,\n                X_train=X_train,\n                X_test=X_test,\n                y_train=y_train,\n                y_test=y_test,\n                pm=pm_new,\n                max_depth=MAX_DEPTH,\n                task=\"classification\",\n                n_features=N_FEATURES,\n                models=MODELS,\n                device=device,\n            )\n            # model_results = embedders.benchmarks.benchmark(\n            #     X, y, pm_new, max_depth=MAX_DEPTH, task=\"classification\", score=[\"f1-micro\", \"accuracy\"], device=device, models=MODELS, n_features=N_FEATURES,\n            #     batch_size=1\n            # )\n            # Create a flat dictionary for this run\n            run_results = {\"dataset\": dataset, \"seed\": i}\n\n            # Flatten the nested model results\n            for model, metrics in model_results.items():\n                for metric, value in metrics.items():\n                    run_results[f\"{model}_{metric}\"] = value\n\n            results_dataset.append(run_results)\n\n        except ValueError as e:\n            print(e)\n\n        my_tqdm.update(1)\n\n    results.extend(results_dataset)\n\n\n# Print results\nresults = pd.DataFrame(results)\nresults.to_csv(f\"embedders/data/results/all_nn_link.tsv\", sep=\"\\t\", index=False)\n\n```\n\n----------------------------------------\n\nTITLE: Generating Gaussian Mixture Data on a Product Manifold using Embedders in Python\nDESCRIPTION: Instantiates a `ProductManifold` from the `embedders` library with a specified signature `[(-1,2), (0, 2), (1, 2)]`, representing the structure of the space (e.g., combining Euclidean, spherical, hyperbolic components). It then generates a synthetic dataset (`X`, `y`) using the `gaussian_mixture` function, sampling features `X` and corresponding labels `y` from a Gaussian mixture model defined on the created product manifold `pm`.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/37_sklearn_decision_tree.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npm = embedders.manifolds.ProductManifold(signature=[(-1,2), (0, 2), (1, 2)])\nX, y = embedders.gaussian_mixture.gaussian_mixture(pm=pm)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Classification Tasks on Synthetic Manifold Data using Manify\nDESCRIPTION: Defines a ProductManifold with a more complex signature involving multiple component spaces. Generates synthetic data (`X`, `y`) using a Gaussian mixture model on this manifold. Executes a comprehensive benchmark (`manify.utils.benchmarks.benchmark`) for classification tasks, comparing various algorithms (e.g., Decision Trees, Random Forests, Perceptron, MLP, GCN, MLR in different variants) on the generated manifold data. The benchmark runs for a specified number of epochs (`epochs=10`) and returns a dictionary (`out`) containing performance metrics for each model.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/60_pytest_scratch.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\npm = manify.manifolds.ProductManifold(signature=[(-1, 4), (-1, 2), (0, 2), (1, 2), (1, 4)])\nX, y = pm.gaussian_mixture(num_points=100)\nout = manify.utils.benchmarks.benchmark(X, y, pm, task=\"classification\", epochs=10)\n```\n\n----------------------------------------\n\nTITLE: Running Classification Benchmarks for a Subset of Manifold Signatures in Python\nDESCRIPTION: This snippet sets up and runs a machine learning experiment focused on classification. It defines parameters like manifold signatures (using only the last signature 'S' and the last seed in this specific run), data dimensions, sample sizes, and task type ('classification'). It iterates through the specified signature and seed, generates synthetic data (X, y) using a Gaussian mixture model on the corresponding product manifold (`embedders.gaussian_mixture`), moves data and the manifold object to the selected computation `device`, runs benchmark models (`embedders.benchmarks.benchmark`), collects the results (including signature and seed), and appends them to a list which is later converted to a pandas DataFrame. It uses `tqdm` for progress tracking.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/27_benchmark_signature_gaussian.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nresults = []\n\n# Signatures - using non-Gu approach for now\nSIGNATURES = [\n    [(-1, 2), (-1, 2)],  # HH\n    [(-1, 2), (0, 2)],  # HE\n    [(-1, 2), (1, 2)],  # HS\n    [(1, 2), (1, 2)],  # SS\n    [(1, 2), (0, 2)],  # SE\n    [(-1, 4)],  # H\n    [(0, 4)],  # E\n    [(1, 4)],  # S\n]\n\nSIGNATURES_STR = [\"HH\", \"HE\", \"HS\", \"SS\", \"SE\", \"H\", \"E\", \"S\"]\n\nDIM = 4\nN_SAMPLES = 10\nN_POINTS = 1_000\n# N_POINTS = 100\nN_CLASSES = 8\nN_CLUSTERS = 32\n# MAX_DEPTH = None\nCOV_SCALE_MEANS = 1.0\nCOV_SCALE_POINTS = 1.0\n\n# TASK = \"regression\"\nTASK = \"classification\"\n# RESAMPLE_SCALES = False\n# N_FEATURES = \"d_choose_2\"\n\n# SCORE = \"f1-micro\" if TASK == \"classification\" else \"rmse\"\nSCORE = [\"f1-micro\", \"accuracy\"] if TASK == \"classification\" else [\"rmse\"]\n\nmy_tqdm = tqdm(total=len(SIGNATURES) * N_SAMPLES)\nfor i, (sig, sigstr) in enumerate(zip(SIGNATURES[-1:], SIGNATURES_STR[-1:])):\n    for seed in range(N_SAMPLES)[-1:]:\n        print(sig, sigstr, seed)\n        # Ensure unique seed per trial\n        seed = seed + N_SAMPLES * i\n        pm = embedders.manifolds.ProductManifold(signature=sig, device=sample_device)\n\n        # Get X, y\n        X, y = embedders.gaussian_mixture.gaussian_mixture(\n            pm=pm,\n            seed=seed,\n            num_points=N_POINTS,\n            num_classes=N_CLASSES,\n            num_clusters=N_CLUSTERS,\n            cov_scale_means=COV_SCALE_MEANS / DIM,\n            cov_scale_points=COV_SCALE_POINTS / DIM,\n            task=TASK,\n        )\n        X = X.to(device)\n        y = y.to(device)\n        pm = pm.to(device)\n\n        # if RESAMPLE_SCALES:\n        #     scale = 0.5 - np.random.rand() * 20\n        #     pm.P[0].scale = torch.exp(torch.tensor(scale)).item()\n        #     pm.P[0].manifold._log_scale = torch.nn.Parameter(torch.tensor(scale))\n\n        # Benchmarks are now handled by the benchmark function\n        model_results = embedders.benchmarks.benchmark(\n            # X, y, pm, max_depth=MAX_DEPTH, task=TASK, score=SCORE, seed=seed, n_features=N_FEATURES, device=device\n            X, y, pm, task=TASK, score=SCORE, seed=seed, device=device\n        )\n        \n        # # Create a flat dictionary for this run\n        # run_results = {\"signature\": sigstr, \"seed\": seed}\n\n        # # Flatten the nested model results\n        # for model, metrics in model_results.items():\n        #     for metric, value in metrics.items():\n        #         run_results[f\"{model}_{metric}\"] = value\n\n        # results.append(run_results)\n        model_results[\"signature\"] = sigstr\n        model_results[\"seed\"] = seed\n        results.append(model_results)\n        my_tqdm.update(1)\n\n\nresults = pd.DataFrame(results)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Models on Single Curvature Manifolds in Python\nDESCRIPTION: Runs benchmark experiments for classification and regression tasks on synthetic data generated from manifolds with a single curvature component. It iterates through a predefined list of `CURVATURES` and runs `N_SAMPLES` trials for each. In each trial, it creates a `ProductManifold` with the specified curvature and dimension, generates Gaussian mixture data (`X`, `y`) on this manifold using `embedders.gaussian_mixture.gaussian_mixture`, benchmarks various models listed in `MODELS` using `embedders.benchmarks.benchmark`, collects performance metrics, and saves the aggregated results to a TSV file named based on the task.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/43_benchmark_neural.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nCURVATURES = [-4, -2, -1, -0.5, -0.25, 0, 0.25, 0.5, 1, 2, 4]\nDIM = 2\nN_POINTS = 1_000\nN_CLASSES = 8\nN_CLUSTERS = 32\nCOV_SCALE_MEANS = 1.0\nCOV_SCALE_POINTS = 1.0\n\nRESAMPLE_SCALES = False\n\n# SCORE = \"f1-micro\" if TASK == \"classification\" else \"rmse\"\n\nfor TASK in [\"classification\", \"regression\"]:\n    results = []\n    SCORE = [\"f1-micro\", \"accuracy\"] if TASK == \"classification\" else [\"rmse\"]\n    my_tqdm = tqdm(total=len(CURVATURES) * N_SAMPLES)\n    for i, K in enumerate(CURVATURES):\n        for seed in range(N_SAMPLES):\n            try:\n                # Ensure unique seed per trial\n                seed = seed + N_SAMPLES * i\n                pm = embedders.manifolds.ProductManifold(signature=[(K, DIM)]).to(sample_device)\n\n                # Get X, y\n                X, y = embedders.gaussian_mixture.gaussian_mixture(\n                    pm=pm,\n                    seed=seed,\n                    num_points=N_POINTS,\n                    num_classes=N_CLASSES,\n                    num_clusters=N_CLUSTERS,\n                    cov_scale_means=COV_SCALE_MEANS / DIM,\n                    cov_scale_points=COV_SCALE_POINTS / DIM,\n                    task=TASK,\n                )\n                X = X.to(device)\n                y = y.to(device)\n                pm = pm.to(device)\n\n                model_results = embedders.benchmarks.benchmark(\n                    X,\n                    y,\n                    pm,\n                    max_depth=MAX_DEPTH,\n                    task=TASK,\n                    score=SCORE,\n                    seed=seed,\n                    n_features=N_FEATURES,\n                    device=device,\n                    models=MODELS,\n                )\n\n                # Create a flat dictionary for this run\n                run_results = {\"curvature\": K, \"seed\": seed}\n\n                # Flatten the nested model results\n                for model, metrics in model_results.items():\n                    for metric, value in metrics.items():\n                        run_results[f\"{model}_{metric}\"] = value\n\n                results.append(run_results)\n            except Exception as e:\n                print(f\"Error: {e}\")\n            my_tqdm.update(1)\n\n    # Convert to DataFrame\n    results = pd.DataFrame(results)\n\n    results.to_csv(f\"embedders/data/results/{TASK}_nn_single_curvature.tsv\", sep=\"\\t\", index=False)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Configuring Warnings in Python\nDESCRIPTION: Imports necessary libraries including PyTorch for tensor operations, the custom 'embedders' library for manifold learning functionalities, NumPy for numerical operations, Matplotlib for plotting (though not used in subsequent snippets), pandas for data manipulation, and tqdm for progress bars. It also configures warnings to ignore UserWarnings, specifically those that might arise during Wishart distribution sampling in Gaussian mixtures.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/43_benchmark_neural.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport embedders\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom tqdm.notebook import tqdm\n\n# Filter out warnings raised when sampling Wishart distribution in Gaussian mixtures\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n```\n\n----------------------------------------\n\nTITLE: Initializing, Training, and Launching a Product Manifold Decision Tree with Dashboard in Python\nDESCRIPTION: Shows the process of generating synthetic product manifold data using the embedders library, fitting a product space decision tree, evaluating its performance, and launching the previously defined interactive dashboard. Dependencies include embedders, scikit-learn for train_test_split, and a functional Dash dashboard definition. Key parameters set the manifold signature, sample count, and tree depth. Inputs: randomly generated X, y; Outputs: trained model, evaluation score (printed), and a running Dash app visualizing predictions on test data.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/49_plotly_dashboard.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport embedders\nfrom sklearn.model_selection import train_test_split\n\npm = embedders.manifolds.ProductManifold(signature=[(1, 2)])\nX, y = embedders.gaussian_mixture.gaussian_mixture(pm, 1000, num_classes=4)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\npdt = embedders.predictors.tree_new.ProductSpaceDT(pm=pm, n_features=\"d_choose_2\", max_depth=5)\npdt.fit(X_train, y_train)\n\nprint(f\"{pdt.score(X_test, y_test).float().mean().item():.4f}\")\n\ncreate_dashboard(pdt, X_test, y_test).run_server(debug=True)\n```\n\n----------------------------------------\n\nTITLE: Running Classification Benchmarks on Product Manifolds in Python\nDESCRIPTION: This snippet iterates through predefined manifold signatures, loads corresponding graph embeddings from NumPy files, and runs classification benchmarks using both product manifold and standard Scikit-learn classifiers. It utilizes `torch` for tensor operations (disabling gradient calculation for efficiency), `tqdm` for progress tracking, and a custom `embedders` module. Results, including accuracy scores, signature strings, and random seeds, are collected in a list and later converted to a Pandas DataFrame.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/2_polblogs_benchmark.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# Benchmark loop\nresults = []\nwith torch.no_grad():\n    my_tqdm = tqdm(total=TRIALS * len(SIGNATURES))\n    for i, (signature, signature_str) in enumerate(zip(SIGNATURES, SIGNATURES_STR)):\n        pm = embedders.manifolds.ProductManifold(signature=signature)\n        for j in range(TRIALS):\n            X = np.load(f\"../data/graphs/embeddings/{DATASET}/{signature_str}_{j}.npy\")\n            X = torch.tensor(X)\n            if X.isnan().any():\n                print(i, j, \"NaN\")\n                break\n\n            # Run benchmarks\n            seed = i * TRIALS + j\n            accs = embedders.benchmarks.benchmark(X, y, pm, max_depth=MAX_DEPTH, task=TASK, score=SCORE, seed=seed)\n            accs[\"signature\"] = signature_str\n            accs[\"seed\"] = seed\n            results.append(accs)\n\n            my_tqdm.update(1)\n\nresults = pd.DataFrame(results)\n```\n\n----------------------------------------\n\nTITLE: Comparing ProductSpaceDT and Sklearn DT using Train-Test Split in Python\nDESCRIPTION: This snippet sets up a comparison between `ProductSpaceDT` and `sklearn.tree.DecisionTreeRegressor` using a standard train-test split methodology. It generates new data on a Hyperboloid manifold, splits it into training and testing sets, trains both models on the training data, makes predictions on the test set, and calculates the Mean Squared Error (MSE) for each model. Finally, it visualizes the test data (transformed to the Poincar disk), the predictions of both models, their difference, and the true test values using subplots for direct comparison.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/17_verify_new_mse.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Ok, something is still odd here. Maybe it has to do with train-test splits? Let's check that\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nDEPTH = 3\n\npm = embedders.manifolds.ProductManifold(signature=[(-1, 2)])\nX, y = embedders.gaussian_mixture.gaussian_mixture(pm=pm, task=\"regression\", num_clusters=8)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train_np = X_train.detach().cpu().numpy()\nX_test_np = X_test.detach().cpu().numpy()\ny_train_np = y_train.detach().cpu().numpy()\ny_test_np = y_test.detach().cpu().numpy()\n\npdt = embedders.tree_new.ProductSpaceDT(pm=pm, task=\"regression\", max_depth=DEPTH)\npdt.fit(X_train, y_train)\npdt_ypred = pdt.predict(X_test).detach().cpu().numpy()\npdt_score = mean_squared_error(y_test_np, pdt_ypred)\n\ndt = DecisionTreeRegressor(max_depth=DEPTH)\ndt.fit(X_train_np, y_train_np)\ndt_ypred = dt.predict(X_test_np)\ndt_score = mean_squared_error(y_test_np, dt_ypred)\n\n# Redo X_test_np as poincare disk\nX_test_np = embedders.visualization.hyperboloid_to_poincare(X_test).detach().cpu().numpy()\n\nfig, axs = plt.subplots(2, 2, figsize=(10, 5))\nfig.colorbar(axs[0, 0].scatter(X_test_np[:, 0], X_test_np[:, 1], c=pdt_ypred), ax=axs[0, 0])\naxs[0, 0].set_title(f\"ProductSpaceDT (MSE: {pdt_score:.2f})\")\nfig.colorbar(axs[0, 1].scatter(X_test_np[:, 0], X_test_np[:, 1], c=dt_ypred), ax=axs[0, 1])\naxs[0, 1].set_title(f\"sklearn DT (MSE: {dt_score:.2f})\")\nfig.colorbar(axs[1, 0].scatter(X_test_np[:, 0], X_test_np[:, 1], c=dt_ypred - pdt_ypred.flatten()), ax=axs[1, 0])\naxs[1, 0].set_title(\"Difference\")\nfig.colorbar(axs[1, 1].scatter(X_test_np[:, 0], X_test_np[:, 1], c=y_test_np), ax=axs[1, 1])\naxs[1, 1].set_title(\"True vals\")\n\n# Colorbars\nplt.tight_layout()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Models on Multiple Curvature Manifolds in Python\nDESCRIPTION: Executes benchmark experiments for classification and regression tasks on synthetic data generated from product manifolds composed of multiple components with potentially different curvatures. It iterates through a predefined list of manifold `SIGNATURES` (e.g., Hyperbolic x Hyperbolic, Hyperbolic x Euclidean) and runs `N_SAMPLES` trials for each. In each trial, it creates a `ProductManifold` based on the signature, generates Gaussian mixture data, optionally resamples manifold component scales, benchmarks models using `embedders.benchmarks.benchmark`, collects performance metrics, and saves the aggregated results to a TSV file.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/43_benchmark_neural.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Multiple curvatures\n\n# Signatures - using non-Gu approach for now\nSIGNATURES = [\n    [(-1, 2), (-1, 2)],  # HH\n    [(-1, 2), (0, 2)],  # HE\n    [(-1, 2), (1, 2)],  # HS\n    [(1, 2), (1, 2)],  # SS\n    [(1, 2), (0, 2)],  # SE\n    [(-1, 4)],  # H\n    [(0, 4)],  # E\n    [(1, 4)],  # S\n]\n\nSIGNATURES_STR = [\"HH\", \"HE\", \"HS\", \"SS\", \"SE\", \"H\", \"E\", \"S\"]\n\nDIM = 4\nN_POINTS = 1_000\nN_CLASSES = 8\nN_CLUSTERS = 32\nCOV_SCALE_MEANS = 1.0\nCOV_SCALE_POINTS = 1.0\n\nRESAMPLE_SCALES = False # Added based on usage in the loop\n\nfor TASK in [\"classification\", \"regression\"]:\n    results = []\n    SCORE = [\"f1-micro\", \"accuracy\"] if TASK == \"classification\" else [\"rmse\"]\n\n    my_tqdm = tqdm(total=len(SIGNATURES) * N_SAMPLES)\n    for i, (sig, sigstr) in enumerate(zip(SIGNATURES, SIGNATURES_STR)):\n        for seed in range(N_SAMPLES):\n            try:\n                # Ensure unique seed per trial\n                seed = seed + N_SAMPLES * i\n                pm = embedders.manifolds.ProductManifold(signature=sig, device=sample_device)\n\n                # Get X, y\n                X, y = embedders.gaussian_mixture.gaussian_mixture(\n                    pm=pm,\n                    seed=seed,\n                    num_points=N_POINTS,\n                    num_classes=N_CLASSES,\n                    num_clusters=N_CLUSTERS,\n                    cov_scale_means=COV_SCALE_MEANS / DIM,\n                    cov_scale_points=COV_SCALE_POINTS / DIM,\n                    task=TASK,\n                )\n                X = X.to(device)\n                y = y.to(device)\n                pm = pm.to(device)\n\n                if RESAMPLE_SCALES:\n                    scale = 0.5 - np.random.rand() * 20\n                    pm.P[0].scale = torch.exp(torch.tensor(scale)).item()\n                    pm.P[0].manifold._log_scale = torch.nn.Parameter(torch.tensor(scale))\n\n                # Benchmarks are now handled by the benchmark function\n                model_results = embedders.benchmarks.benchmark(\n                    X,\n                    y,\n                    pm,\n                    max_depth=MAX_DEPTH,\n                    task=TASK,\n                    score=SCORE,\n                    seed=seed,\n                    n_features=N_FEATURES,\n                    device=device,\n                    models=MODELS # Added based on previous snippets pattern, assuming MODELS is intended here\n                )\n\n                # Create a flat dictionary for this run\n                run_results = {\"signature\": sigstr, \"seed\": seed}\n\n                # Flatten the nested model results\n                for model, metrics in model_results.items():\n                    for metric, value in metrics.items():\n                        run_results[f\"{model}_{metric}\"] = value\n\n                results.append(run_results)\n            except Exception as e:\n                print(f\"Error: {e}\")\n            my_tqdm.update(1)\n\n    results = pd.DataFrame(results)\n\n    results.to_csv(f\"embedders/data/results/{TASK}_nn_multiple_curvatures.tsv\", sep=\"\\t\", index=False)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Embeddings with Product Manifolds in Python\nDESCRIPTION: This core Python snippet performs the benchmarking process. It imports necessary libraries (`embedders`, `pandas`, `tqdm`, `torch`). It defines constants for sample size and model parameters. The code then iterates through each embedding and trial: loads train/test data, subsamples it if it exceeds `N_SAMPLES`, converts data to PyTorch tensors on the specified device ('cuda'), creates a `ProductManifold` instance, calculates pairwise distances (`pdist2`) and normalized adjacency matrices (`A_train`, `A_test`). Finally, it calls the `embedders.benchmarks.benchmark` function to evaluate performance (accuracy, f1-micro) using specified models (commented out, potentially 'sklearn_dt', 'product_dt') and configurations, storing the results along with embedding name and trial number in a list. A progress bar (`tqdm`) tracks the process.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/23_benchmark_vae_embeddings.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport embedders\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport torch\n\n# N_SAMPLES = 100 # Takes ~20 secs\nN_SAMPLES = 1_000 # Takes ~5 mins\n# N_SAMPLES = float(\"inf\")  # Takes ~1 hour\nMAX_DEPTH = 5\nN_FEATURES = \"d_choose_2\"\n\nresults = []\nmy_tqdm = tqdm(total=len(embeddings_names) * n_trials)\nfor embedding, sig in zip(embeddings_names, sigs):\n    pm = embedders.manifolds.ProductManifold(signature=sig, device=\"cuda\")\n    for trial in range(n_trials):\n        X_train = np.load(f\"../data/{embedding}/embeddings/X_train_{trial}.npy\")\n        y_train = np.load(f\"../data/{embedding}/embeddings/y_train_{trial}.npy\")\n        X_test = np.load(f\"../data/{embedding}/embeddings/X_test_{trial}.npy\")\n        y_test = np.load(f\"../data/{embedding}/embeddings/y_test_{trial}.npy\")\n\n        # Randomly subsample\n        if len(X_train) > N_SAMPLES:\n            idx = np.random.choice(X_train.shape[0], N_SAMPLES, replace=False)\n            X_train = X_train[idx]\n            y_train = y_train[idx]\n\n        if len(X_test) > N_SAMPLES:\n            idx = np.random.choice(X_test.shape[0], N_SAMPLES, replace=False)\n            X_test = X_test[idx]\n            y_test = y_test[idx]\n        \n        # Make tensors\n        X_train = torch.tensor(X_train, dtype=torch.float32, device=\"cuda\")\n        y_train = torch.tensor(y_train, dtype=torch.long, device=\"cuda\")\n        X_test = torch.tensor(X_test, dtype=torch.float32, device=\"cuda\")\n        y_test = torch.tensor(y_test, dtype=torch.long, device=\"cuda\")\n        \n        # Get A_train and A_test\n        D_train = pm.pdist2(X_train)\n        max_train_dist = D_train[D_train.isfinite()].max()\n        D_train /= max_train_dist\n        A_train = embedders.predictors.kappa_gcn.get_A_hat(torch.exp(-D_train))\n        A_test = embedders.predictors.kappa_gcn.get_A_hat(torch.exp(-pm.pdist2(X_test) / max_train_dist))\n\n        res = embedders.benchmarks.benchmark(\n            X=None,\n            y=None,\n            X_train=X_train,\n            X_test=X_test,\n            y_train=y_train,\n            y_test=y_test,\n            pm=pm,\n            A_train=A_train,\n            A_test=A_test,\n            # models=[\"sklearn_dt\", \"product_dt\"],\n            # max_depth=MAX_DEPTH,\n            # batch_size=1,\n            # n_features=N_FEATURES,\n            device=\"cuda\",\n            task=\"classification\",\n            score=[\"accuracy\", \"f1-micro\"],\n        )\n        res[\"embedding\"] = embedding\n        res[\"trial\"] = trial\n\n        results.append(res)\n        my_tqdm.update(1)\n\nresults = pd.DataFrame(results)\n```\n\n----------------------------------------\n\nTITLE: Link Prediction on Graph-Embedded Manifold Data Using embedders and PyTorch (Python)\nDESCRIPTION: Implements a link prediction pipeline where graph data is embedded in a product manifold, mapped to tangent space, and processed with a GNN from embedders.predictors.gnn. Ensures training and test sets do not split paired data, initializes the model on CUDA, and trains. It requires embedders, networkx, torch, numpy, and sklearn. Dependencies include custom dataset loaders and precomputed embedding files.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/41_more_benchmarks.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Can we do link prediction as well\\n\\n%load_ext autoreload\\n%autoreload 2\\n\\ndevice=\"cuda\"\\n\\nimport embedders\\nimport networkx as nx\\nimport torch\\nimport numpy as np\\n\\n# Special function to split dataset while ensuring pairs are in the same split\\nfrom sklearn.model_selection import train_test_split\\n\\ndists, labels, adj = embedders.dataloaders.load(\"polblogs\")\\ndists = dists / dists.max()\\nresults_dataset = []\\n\\npm = embedders.manifolds.ProductManifold(signature=[(-1, 4)], device=device)\\nX = torch.tensor(np.load(f\"embedders/data/graphs/embeddings/polblogs/H_0.npy\"), device=device)\\nX_tangent = pm.logmap(X).detach()\\n\\nlink_gnn = embedders.predictors.gnn.LinkPredictionGNN(pm, input_dim=5, hidden_dims=[64, 64, 64], output_dim=1, tangent=True)\\n\\nlink_gnn = link_gnn.to(device)\\nadj = adj.to(device)\\ndists = dists.to(device)\\n\\ntrain_idx, test_idx = train_test_split(np.arange(len(X)), test_size=0.2)\\n\\nlink_gnn.fit(X_tangent, adj=adj, dists=dists, train_idx=train_idx, print_interval=10)\n```\n\n----------------------------------------\n\nTITLE: Calculating Average Distortion (D_avg) for Embeddings in Python\nDESCRIPTION: This snippet calculates the average distortion metric (D_avg) for different embeddings and manifold signatures. It iterates through embeddings, signatures, and trials. For each, it loads the true distance matrix (`D_true`) and the corresponding pre-computed embedding data (`.h5` file), ensuring tensors are on the GPU (`cuda`). It identifies train/test indices, extracts and normalizes the training distance matrix (`D_train`). It then computes the estimated distances (`D_est`) from the training embeddings (`X_train`) using a `ProductManifold` instance from the `embedders` library. A maximum-likelihood scaling factor (`a`) is calculated and applied to `D_est`. Finally, `embedders.metrics.d_avg` computes the distortion, which is stored along with metadata in a Pandas DataFrame `dists`.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/24_benchmark_graph_embeddings.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# What is the lowest-distortion signature for each embedding?\ndists = pd.DataFrame(columns=[\"embedding\", \"signature\", \"trial\", \"D_avg\"])\n\nfor embedding in EMBEDDINGS_NAMES:\n    D_true, _, _ = embedders.dataloaders.load(embedding)\n    D_true = D_true.to(\"cuda\")\n    for signature, sigstr in zip(SIGNATURES, SIGNATURES_STR):\n        for trial in range(n_trials):\n            my_data = torch.load(f\"../data/graphs/embeddings/{embedding}/{sigstr}_{trial}.h5\", map_location=\"cuda\", weights_only=True)\n\n            # Get train and test index\n            test_idx = my_data[\"test_idx\"]\n            train_idx = torch.tensor([i for i in range(D_true.shape[0]) if i not in test_idx], device=\"cuda\")\n\n            # Dist needs to be rescaled\n            D_train = D_true[train_idx][:, train_idx]\n            D_train_max = torch.max(D_train[torch.isfinite(D_train)])\n            D_train = D_train / D_train_max\n\n            # Get estimated distance\n            pm = embedders.manifolds.ProductManifold(signature=signature, device=\"cuda\")\n            D_est = pm.pdist(my_data[\"X_train\"])\n\n            # Correct for scaling using maximum-likelihood scaling factor\n            triu_idx = torch.triu_indices(D_train.shape[0], D_train.shape[1], offset=1)\n            D_train_triu = D_train[triu_idx[0], triu_idx[1]]\n            D_est_triu = D_est[triu_idx[0], triu_idx[1]]\n            a = (\n                torch.sum((D_train_triu * D_est_triu) / (D_train_triu ** 2)) / \n                torch.sum(D_est_triu ** 2 / D_train_triu ** 2)\n            )\n            D_est = a * D_est\n\n            D_avg = embedders.metrics.d_avg(D_train, D_est, pairwise=True).item()\n            print(D_avg)\n\n            dists.loc[len(dists)] = {\n                    \"embedding\": embedding,\n                    \"signature\": signature,\n                    \"trial\": trial,\n                    \"D_avg\": D_avg\n                }\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Models on Manifolds using 'embedders' in Python\nDESCRIPTION: Performs a benchmark experiment similar to the previous one, but uses an 'embedders' library (possibly an older or alternative version compared to 'manify'). It re-initializes the 'results' list and sets the 'TASK' to 'regression'. It iterates through curvatures and seeds, creating a 'ProductManifold' using 'embedders.manifolds.ProductManifold'. Data generation uses 'embedders.gaussian_mixture.gaussian_mixture', and benchmarking uses 'embedders.benchmarks.benchmark', running on the main 'device'. Results are collected and converted into a Pandas DataFrame, similar to the 'manify' version.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/26_benchmark_single_curvature_gaussian.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nresults = []\n\nTASK = \"regression\"\n# TASK = \"classification\"\nSCORE = [\"f1-micro\", \"accuracy\"] if TASK == \"classification\" else [\"rmse\"]\n\nmy_tqdm = tqdm(total=len(CURVATURES) * N_SAMPLES)\nfor i, K in enumerate(CURVATURES):\n    for seed in range(N_SAMPLES):\n        # Ensure unique seed per trial\n        seed = seed + N_SAMPLES * i\n        pm = embedders.manifolds.ProductManifold(signature=[(K, DIM)]).to(sample_device)\n\n        # Get X, y\n        X, y = embedders.gaussian_mixture.gaussian_mixture(\n            pm=pm,\n            seed=seed,\n            num_points=N_POINTS,\n            num_classes=N_CLASSES,\n            num_clusters=N_CLUSTERS,\n            cov_scale_means=COV_SCALE_MEANS / DIM,\n            cov_scale_points=COV_SCALE_POINTS / DIM,\n            task=TASK,\n        )\n        X = X.to(device)\n        y = y.to(device)\n        pm = pm.to(device)\n\n        model_results = embedders.benchmarks.benchmark(\n            X, y, pm, task=TASK, score=SCORE, seed=seed, device=device\n        )\n\n        # Create a flat dictionary for this run\n        model_results[\"curvature\"] = K\n        model_results[\"seed\"] = seed\n\n        # results.append(run_results)\n        results.append(model_results)\n        my_tqdm.update(1)\n\n# Convert to DataFrame\nresults = pd.DataFrame(results)\n```\n\n----------------------------------------\n\nTITLE: Preprocessing and Information Gain Calculation for Hyperbolic Product Trees - Python\nDESCRIPTION: Implements data preprocessing, one-hot label encoding, comparison tensor creation, and information gain calculation for batches in a new ProductDT model. Dependencies: torch, torch.nn.functional, and correct input shapes. Inputs: tensors X (batch, dims+1), y (batch,). Outputs: angles (batch, dims), one-hot labels (batch, n_classes), reshaped comparisons (batch, dims, batch), and information gain matrix (batch, dims). Includes cross-validation with previously computed ig_est values.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/13_information_gain.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef preprocess(X, y):\n    \"\"\"\n    Preprocessing function for the new version of ProductDT\n\n    Args:\n        X*: (batch, dims + 1) tensor of hyperbolic coordinates\n        y*: (batch,) tensor of labels\n\n    Outputs:\n        X: (batch, dims) tensor of angles\n        y: (batch, n_classes) tensor of one-hot labels\n        M: (batch, dims, batch) tensor of comparisons\n    \"\"\"\n    # Ensure X and y are tensors\n    if not torch.is_tensor(X):\n        X = torch.tensor(X)\n    if not torch.is_tensor(y):\n        y = torch.tensor(y)\n\n    # Assertions: input validation\n    assert X.dim() == 2\n    assert y.dim() == 1\n    assert X.shape[0] == y.shape[0]\n\n    # First, we process our X-values into coordinates:\n    # (batch, dims + 1) --> (batch, dims)\n    angles = torch.arctan2(X[:, 0:1], X[:, 1:])\n\n    # Now, we create a tensor of comparisons\n    # (batch, dims) --> (query_batch, key_batch, dims)\n    comparisons = ((angles[:, None] - angles[None, :] + torch.pi) % (2 * torch.pi)) >= torch.pi\n\n    # Reshape the comparisons tensor:\n    # (query_batch, key_batch, dims) --> (query_batch, dims, key_batch):\n    comparisons_reshaped = comparisons.permute(0, 2, 1)\n\n    # One-hot encode labels\n    n_classes = y.unique().numel()\n    labels_onehot = torch.nn.functional.one_hot(y, num_classes=n_classes)\n\n    return angles, labels_onehot.float(), comparisons_reshaped.float()\n\n\ndef get_info_gains(comparisons, labels, eps=1e-10):\n    \"\"\"\n    Given comparisons matrix and labels, return information gain for each possible split.\n\n    Args:\n        comparisons: (query_batch, dims, key_batch) tensor of comparisons\n        labels: (query_batch, n_classes) tensor of one-hot labels\n        eps: small number to prevent division by zero\n\n    Outputs:\n        ig: (query_batch, dims) tensor of information gains\n    \"\"\"\n    # Matrix-multiply to get counts of labels in left and right splits\n    pos_labels = (comparisons @ labels).float()\n    neg_labels = ((1 - comparisons) @ labels).float()\n\n    # Total counts are sums of label counts\n    n_pos = pos_labels.sum(dim=-1) + eps\n    n_neg = neg_labels.sum(dim=-1) + eps\n    n_total = n_pos + n_neg\n\n    # Probabilities are label counts divided by total counts\n    pos_probs = pos_labels / n_pos.unsqueeze(-1)\n    neg_probs = neg_labels / n_neg.unsqueeze(-1)\n    total_probs = (pos_labels + neg_labels) / n_total.unsqueeze(-1)\n\n    # Gini impurity is 1 - sum(prob^2)\n    gini_pos = 1 - (pos_probs**2).sum(dim=-1)\n    gini_neg = 1 - (neg_probs**2).sum(dim=-1)\n    gini_total = 1 - (total_probs**2).sum(dim=-1)\n\n    # Information gain is the total gini impurity minus the weighted average of the new gini impurities\n    ig = gini_total - (gini_pos * n_pos + gini_neg * n_neg) / n_total\n\n    assert not ig.isnan().any()  # Ensure no NaNs\n\n    return ig\n\n\nangles, labels_onehot, comparisons_reshaped = preprocess(X, y)\nig_est_functional = get_info_gains(comparisons_reshaped, labels_onehot)\nassert torch.allclose(ig_est_functional, ig_est.to(ig_est_functional.dtype), atol=1e-6)\n\n```\n\n----------------------------------------\n\nTITLE: Extract Time Components from DateTime\nDESCRIPTION: This snippet performs feature engineering on the Kaggle dataset DataFrame (`df`). It first converts the 'DateTime' column to datetime objects using `pd.to_datetime`. Then, it extracts the day of the week, hour, day of the year, and minute from the datetime objects and stores them as new columns in the DataFrame.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/33_traffic.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Turn datetime into day of year, day of week, hour, and minute\ndf[\"datetime\"] = pd.to_datetime(df[\"DateTime\"])\ndf[\"day_of_week\"] = df[\"datetime\"].dt.dayofweek\ndf[\"hour\"] = df[\"datetime\"].dt.hour\ndf[\"day_of_year\"] = df[\"datetime\"].dt.dayofyear\ndf[\"minute\"] = df[\"datetime\"].dt.minute\n```\n\n----------------------------------------\n\nTITLE: Running Link Prediction Benchmark Experiment\nDESCRIPTION: This Python script defines hyperparameters and orchestrates the link prediction benchmark experiment. It iterates through specified graph datasets (`DATASETS`) and multiple trials (`N_TRIALS`), loading data using `embedders.dataloaders.load`, training manifold embeddings via `embedders.coordinate_learning.train_coords`, preparing features and labels for link prediction with `embedders.link_prediction.make_link_prediction_dataset`, splitting the data using the custom `split_dataset` function, executing the benchmark with `embedders.benchmarks.benchmark`, and collecting results. It uses `tqdm` for progress tracking and includes basic error handling.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/28_benchmark_link_prediction.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\n# Hyperparams\n# DATASETS = [\"karate_club\", \"lesmis\", \"football\"]\nDATASETS = [\"adjnoun\", \"dolphins\", \"polbooks\"]\nCOMPONENT_SIG = [(-1, 2), (0, 2), (1, 2)]\nUSE_SPECIAL_DIMS = False\nN_FEATURES = \"d_choose_2\"\nUSE_DISTS = True\nTEST_SIZE = 0.2\nTOTAL_ITERATIONS = 5_000\nMAX_DEPTH = 3\nN_TRIALS = 100\nSCALE_LR = 0\n# LR = 1e-3\nLR = 1e-4\n\n# Run benchmark\nresults = []\nmy_tqdm = tqdm(total=len(DATASETS) * N_TRIALS)\ni = 0\nfor dataset in DATASETS:\n    dists, labels, adj = embedders.dataloaders.load(dataset)\n    dists = dists / dists.max()\n    results_dataset = []\n    # for i in range(N_TRIALS):\n    while len(results_dataset) < N_TRIALS:\n        try:\n            pm = embedders.manifolds.ProductManifold(signature=COMPONENT_SIG)\n\n            torch.manual_seed(i)\n            X_embed, losses = embedders.coordinate_learning.train_coords(\n                pm,\n                dists,\n                burn_in_iterations=int(0.1 * TOTAL_ITERATIONS),\n                training_iterations=int(0.9 * TOTAL_ITERATIONS),\n                scale_factor_learning_rate=SCALE_LR,\n                burn_in_learning_rate=LR * 0.1,\n                learning_rate=LR,\n            )\n\n            X, y, pm_new = embedders.link_prediction.make_link_prediction_dataset(X_embed, pm, adj, add_dists=USE_DISTS)\n            X_train, X_test, y_train, y_test = split_dataset(X, y, test_size=TEST_SIZE, random_state=i)\n            res = embedders.benchmarks.benchmark(\n                X=None,\n                y=None,\n                X_train=X_train,\n                X_test=X_test,\n                y_train=y_train,\n                y_test=y_test,\n                pm=pm_new,\n                max_depth=MAX_DEPTH,\n                task=\"classification\",\n                use_special_dims=USE_SPECIAL_DIMS,\n                n_features=N_FEATURES,\n            )\n            res[\"dataset\"] = dataset\n            res[\"trial\"] = i\n            my_tqdm.update(1)\n            results_dataset.append(res)\n            i += 1\n        except Exception as e:\n            print(e)\n            pass\n    results += results_dataset\n\nresults = pd.DataFrame(results)\n```\n\n----------------------------------------\n\nTITLE: Verifying Expected Metrics Keys in Manify Benchmark Results\nDESCRIPTION: Defines a Python set (`target_keys`) containing a comprehensive list of expected metric keys (accuracy, F1-scores, time) for various models (sklearn DT/RF, product DT/RF, tangent DT/RF, KNN, perceptron, MLP, GCN variants, MLR variants) that should be present in the benchmark results. It then compares this predefined set with the actual keys obtained from the benchmark output dictionary (`out.keys()`) to verify that all expected results were computed and returned by the benchmark function.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/60_pytest_scratch.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ntarget_keys = set(\n    [\n        \"sklearn_dt_accuracy\",\n        \"sklearn_dt_f1-micro\",\n        \"sklearn_dt_f1-macro\",\n        \"sklearn_dt_time\",\n        \"sklearn_rf_accuracy\",\n        \"sklearn_rf_f1-micro\",\n        \"sklearn_rf_f1-macro\",\n        \"sklearn_rf_time\",\n        \"product_dt_accuracy\",\n        \"product_dt_f1-micro\",\n        \"product_dt_f1-macro\",\n        \"product_dt_time\",\n        \"product_rf_accuracy\",\n        \"product_rf_f1-micro\",\n        \"product_rf_f1-macro\",\n        \"product_rf_time\",\n        \"tangent_dt_accuracy\",\n        \"tangent_dt_f1-micro\",\n        \"tangent_dt_f1-macro\",\n        \"tangent_dt_time\",\n        \"tangent_rf_accuracy\",\n        \"tangent_rf_f1-micro\",\n        \"tangent_rf_f1-macro\",\n        \"tangent_rf_time\",\n        \"knn_accuracy\",\n        \"knn_f1-micro\",\n        \"knn_f1-macro\",\n        \"knn_time\",\n        \"ps_perceptron_accuracy\",\n        \"ps_perceptron_f1-micro\",\n        \"ps_perceptron_f1-macro\",\n        \"ps_perceptron_time\",\n        \"ambient_mlp_accuracy\",\n        \"ambient_mlp_f1-micro\",\n        \"ambient_mlp_f1-macro\",\n        \"ambient_mlp_time\",\n        \"ambient_gcn_accuracy\",\n        \"ambient_gcn_f1-micro\",\n        \"ambient_gcn_f1-macro\",\n        \"ambient_gcn_time\",\n        \"tangent_gcn_accuracy\",\n        \"tangent_gcn_f1-micro\",\n        \"tangent_gcn_f1-macro\",\n        \"tangent_gcn_time\",\n        \"kappa_gcn_accuracy\",\n        \"kappa_gcn_f1-micro\",\n        \"kappa_gcn_f1-macro\",\n        \"kappa_gcn_time\",\n        \"kappa_mlr_accuracy\",\n        \"kappa_mlr_f1-micro\",\n        \"kappa_mlr_f1-macro\",\n        \"kappa_mlr_time\",\n        \"tangent_mlr_accuracy\",\n        \"tangent_mlr_f1-micro\",\n        \"tangent_mlr_f1-macro\",\n        \"tangent_mlr_time\",\n        \"ambient_mlr_accuracy\",\n        \"ambient_mlr_f1-micro\",\n        \"ambient_mlr_f1-macro\",\n        \"ambient_mlr_time\",\n    ]\n)\n\ntarget_keys == out.keys()\n```\n\n----------------------------------------\n\nTITLE: Training Embeddings on a Manifold with geoopt and torch - Python\nDESCRIPTION: This comprehensive snippet defines loss functions, embedding evaluation functions, and a full training loop for optimizing embeddings on a manifold using Riemannian stochastic optimization. It relies on geoopt for manifold handling and torch for numerical operations. The train_embedding function accepts a manifold object, dimension, learning rate, and distance normalization flag, and iteratively fits the embedding parameters to minimize a custom loss. Inputs: manifold selection (e.g., PoincareBall), embedding dimension, learning rate, distance normalization. Returns: final embeddings, loss history, and average error. Key limitations include presumed pre-existing variable D (distance matrix) and an environment with geoopt, torch, and tqdm installed.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/55_reimplement_gu.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport geoopt\\nimport torch\\nfrom tqdm.notebook import tqdm\\n\\ndef loss(D_est, D_true):\\n    idx = torch.triu_indices(D_est.shape[0], D_est.shape[1], offset=1)\\n    D_est = D_est[idx[0], idx[1]]\\n    D_true = D_true[idx[0], idx[1]]\\n    return torch.sum(torch.abs((D_est / D_true) ** 2 - 1))\\n\\ndef D_avg(D_est, D_true):\\n    idx = torch.triu_indices(D_est.shape[0], D_est.shape[1], offset=1)\\n    D_est = D_est[idx[0], idx[1]]\\n    D_true = D_true[idx[0], idx[1]]\\n    return (torch.abs(D_est - D_true) / D_true).mean().item()\\n\\ndef train_embedding(manifold, dim=2, lr=1e-2, normalize_dists=True):\\n    X = torch.randn(D.shape[0], dim)\\n    X = manifold.projx(X)\\n    X = geoopt.ManifoldParameter(X, manifold=manifold)\\n    optimizer = geoopt.optim.RiemannianAdam(params=[X], lr=lr)\\n\\n    my_tqdm = tqdm(total=2000)\\n    print(\"Begin\")\\n    D_true = D.clone() / D.max() if normalize_dists else D.clone()\\n    losses = []\\n    d_avgs = []\\n    for i in range(2000):\\n        optimizer.zero_grad()\\n        D_est = manifold.dist(X[:, None, :], X[None, :, :])\\n        l = loss(D_est, D_true)\\n        l.backward()\\n        optimizer.step()\\n        my_tqdm.update(1)\\n        my_tqdm.set_postfix(loss=l.item(), d_avg=D_avg(D_est, D))\\n        losses.append(l.item())\\n        d_avgs.append(D_avg(D_est, D))\\n    my_tqdm.close()\\n    return X.detach().cpu().numpy(), losses, d_avgs\\n\\n\\ntrain_embedding(geoopt.manifolds.PoincareBall(), lr=1, dim=10, normalize_dists=False)\n```\n\n----------------------------------------\n\nTITLE: Saving Benchmark Results to TSV File in Python\nDESCRIPTION: Saves the Pandas DataFrame containing the benchmark results (generated in the previous step using 'manify') to a Tab-Separated Values (TSV) file. The filename includes the task type ('classification' or 'regression') and is saved in a subdirectory '../data/results_icml/'. The index of the DataFrame is not included in the output file.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/26_benchmark_single_curvature_gaussian.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nresults.to_csv(f\"../data/results_icml/{TASK}_single_curvature_refactor.tsv\", sep=\"\\t\", index=False)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: This snippet imports necessary components for the script. It imports the `embedders` library (presumably for manifold operations and data generation) and specific classes (`mix_curv_svm`, `mix_curv_perceptron`) from the `hyperdt` library for classification in product spaces.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport embedders\n\nfrom hyperdt.product_space_svm import mix_curv_svm\nfrom hyperdt.product_space_perceptron import mix_curv_perceptron\n```\n\n----------------------------------------\n\nTITLE: Generating H2 x E2 x S2 Embedding for Polblogs\nDESCRIPTION: This snippet generates an embedding for the Polblogs dataset onto a product manifold composed of a 2D Hyperbolic space, a 2D Euclidean space, and a 2D Spherical space (H2 x E2 x S2). It sets a manual seed, defines the new signature, initializes the 'ProductManifold', trains the coordinates using 'embedders.coordinate_learning.train_coords' with specified hyperparameters, and stores the resulting embedding as a detached NumPy array.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/2_polblogs_benchmark.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Same thing, but now we do H2 x E2 x S2\ntorch.manual_seed(0)\n\nsignature = [(-1, 2), (0, 2), (1, 2)]\npm2 = embedders.manifolds.ProductManifold(signature=signature)\nprint(pm2.name)\n\n# Get embedding\nembedders.coordinate_learning.train_coords(\n    pm2,\n    dists_rescaled,\n    device=device,\n    burn_in_iterations=300,\n    training_iterations=300 * 9,\n    learning_rate=1e-1,\n    burn_in_learning_rate=1e-2,\n    scale_factor_learning_rate=1e-1,\n)\n\nh2_e2_s2_polblogs = pm2.x_embed.detach().cpu().numpy()\n```\n\n----------------------------------------\n\nTITLE: Factorizing Manifold Data and Modeling with Product Space Decision Trees in Python\nDESCRIPTION: This snippet illustrates factorizing data from a product manifold and fitting multiple product space decision tree predictors, each operating on a different submanifold. It depends on 'manify', 'torch', and submodules such as 'manify.predictors.tree_icml'. The process involves: splitting the data with 'factorize', randomly selecting submanifold indices, instantiating sub-manifolds and trees, fitting models to subsets, and predicting on new data. The key parameters include the product manifold object, data arrays, and the number of sampled trees; outputs are model predictions for each selected submanifold. This approach demonstrates fine-grained modeling in complex geometric spaces.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/56_single_manifold_rf.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Sketch of how this would work\nimport torch\n\nX_factorized = pm.factorize(X)\n\nsubtree_indices = torch.randint(0, len(pm.P), (10,))\n\ntrees = []\nfor idx in subtree_indices:\n    pm_sub = manify.manifolds.ProductManifold([pm.signature[idx]])\n    X_sub = X_factorized[idx]\n    tree = manify.predictors.tree_icml.ProductSpaceDT(pm=pm_sub)\n    tree.fit(X_sub, y)\n    trees.append(tree)\n\n[tree.predict(X_factorized[idx]) for idx in subtree_indices]\n```\n\n----------------------------------------\n\nTITLE: Loading, Processing, and Aggregating Experiment Results using Pandas in Python\nDESCRIPTION: This extensive snippet uses the Pandas library to load machine learning results from various TSV files. It defines a `process_table` function (though not explicitly used later in this block) to calculate means and standard errors. It then loads data for classification and regression tasks on single-curvature, multiple-curvature Gaussian datasets, graph embeddings, and VAE embeddings. Metadata columns like 'task', 'dataset', and 'table' are added to each DataFrame. Dictionaries (`sig_dict`, `graph_dataset_names`, `vae_signature_dict`, `vae_dataset_names`) are used for mapping identifiers to more descriptive names or LaTeX representations (though some mappings like `curv2sig` and `sig_dict` application are commented out). Finally, the processed DataFrames are concatenated into `all_data`, irrelevant columns are dropped, and the resulting DataFrame is displayed. Numerous commented-out sections indicate prior handling of empirical and link prediction datasets.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/32_big_table.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\n\n# Make table\ndef process_table(table, groupby, task):\n    means = table.groupby(groupby).mean()\n    stderrs = table.groupby(groupby).sem()\n\n\n# # Single-signature Gaussians\n# def curv2sig(curv):\n#     if curv > 0:\n#         return \"$\\\\H{2,\" + str(curv) + \"}$\"\n#     elif curv < 0:\n#         return \"$\\\\S{2,\" + str(curv) + \"}$\"\n#     else:\n#         return \"$\\\\E{2}$\"\n\n# Multiple-signature Gaussians\nsig_dict = {\n    \"H\": \"$\\\\H{4}$\",\n    \"E\": \"$\\\\E{4}$\",\n    \"S\": \"$\\\\S{4}$\",\n    \"HH\": \"($\\\\H{2})^2$\",\n    \"HE\": \"$\\\\H{2}\\\\E{2}$\",\n    \"HS\": \"$\\\\H{2}\\\\S{2}$\",\n    \"SE\": \"$\\\\S{2}\\\\E{2}$\",\n    \"SS\": \"($\\\\S{2})^2$\",\n}\n\n\nclass_1man = pd.read_table(\"../data/results_icml_revision/classification_nn_single_curvature.tsv\")\nclass_1man[\"task\"] = \"C\"\n# class_1man[\"signature\"] = class_1man[\"curvature\"].map(curv2sig)\nclass_1man[\"dataset\"] = \"Gaussian\"\n\nreg_1man = pd.read_table(\"../data/results_icml_revision/regression_nn_single_curvature.tsv\")\nreg_1man[\"task\"] = \"R\"\n# reg_1man[\"signature\"] = reg_1man[\"curvature\"].map(curv2sig)\nreg_1man[\"dataset\"] = \"Gaussian\"\n\n\n\nclass_sig = pd.read_table(\"../data/results_icml_revision/classification_nn_multiple_curvatures.tsv\")\nclass_sig[\"task\"] = \"C\"\n# class_sig[\"signature\"] = class_sig[\"signature\"].map(sig_dict)\nclass_sig[\"dataset\"] = \"Gaussian\"\n\nreg_sig = pd.read_table(\"../data/results_icml_revision/regression_nn_multiple_curvatures.tsv\")\nreg_sig[\"task\"] = \"R\"\n# reg_sig[\"signature\"] = reg_sig[\"signature\"].map(sig_dict)\nreg_sig[\"dataset\"] = \"Gaussian\"\n\n# Empirical datasets\n# temp = pd.read_table(\"../data/results_icml/temperature.tsv\")\n# temp[\"task\"] = \"R\"\n# temp[\"signature\"] = \"$\\\\S{2}\\\\S{1}$\"\n# temp[\"dataset\"] = \"Temperature\"\n\n# fourier = pd.read_table(\"../data/results_icml/fourier.tsv\")\n# fourier[\"task\"] = \"C\"\n# fourier[\"signature\"] = \"$(\\\\S{1})^5$\"\n# fourier[\"dataset\"] = \"Neuron 33\"\n\n# fourier2 = pd.read_table(\"../data/results_icml/fourier2.tsv\")\n# fourier2[\"task\"] = \"C\"\n# fourier2[\"signature\"] = \"$(\\\\S{1})^5$\"\n# fourier2[\"dataset\"] = \"Neuron 46\"\n\n# traffic = pd.read_table(\"../data/results_icml/traffic_results.tsv\")\n# traffic[\"task\"] = \"R\"\n# traffic[\"signature\"] = \"$\\\\E{1}(\\\\S{1})^4$\"\n# traffic[\"dataset\"] = \"Traffic\"\n\n# land_water = pd.read_table(\"../data/results_icml/land_vs_water.tsv\")\n# land_water[\"task\"] = \"C\"\n# land_water[\"signature\"] = \"$\\\\S{2}$\"\n# land_water[\"dataset\"] = \"Landmasses\"\n\n# empirical = pd.read_table(\"../data/results_icml_revision/all_nn_empirical.tsv\")\n# empirical[\"table\"] = \"Other\"\n# empirical_c = empirical[empirical[\"dataset\"].isin([\"landmasses\", \"neuron_33\", \"neuron_46\"])]\n# empirical_c[\"task\"] = \"C\"\n# empirical_r = empirical[empirical[\"dataset\"].isin([\"temperature\", \"traffic\"])]\n# empirical_r[\"task\"] = \"R\"\n# Put in signatures\n# empirical_sigdict = {\n#     \"landmasses\": \"$\\\\S{2}$\",\n#     \"neuron_33\": \"$(\\\\S{1})^{10}$\",\n#     \"neuron_46\": \"$(\\\\S{1})^{10}$\",\n#     \"temperature\": \"$\\\\S{2}\\\\S{1}$\",\n#     \"traffic\": \"$\\\\E{1}(\\\\S{1})^4$\",\n# }\n# empirical_c[\"signature\"] = empirical_c[\"dataset\"].map(empirical_sigdict)\n# empirical_r[\"signature\"] = empirical_r[\"dataset\"].map(empirical_sigdict)\n# empirical_namedict = {\n#     \"landmasses\": \"Landmasses\",\n#     \"neuron_33\": \"Neuron 33\",\n#     \"neuron_46\": \"Neuron 46\",\n#     \"temperature\": \"Temperature\",\n#     \"traffic\": \"Traffic\",\n# }\n# empirical_c[\"dataset\"] = empirical_c[\"dataset\"].map(empirical_namedict)\n# empirical_r[\"dataset\"] = empirical_r[\"dataset\"].map(empirical_namedict)\n\n# Graph datasets - only keep lowest d_avg\n# graph_task_dict = {\n#     \"polblogs\": \"C\",\n#     \"citeseer\": \"C\",\n#     \"cora\": \"C\",\n#     \"cs_phds\": \"R\",\n# }\ngraph_dataset_names = {\n    \"polblogs\": \"PolBlogs\",\n    \"citeseer\": \"CiteSeer\",\n    \"cora\": \"Cora\",\n    \"cs_phds\": \"CS PhDs\",\n}\ngraphs_c = pd.read_table(\"../data/results_icml_revision/classification_nn_graph.tsv\")\n# graphs = graphs.groupby([\"embedding\", \"signature\"]).mean().sort_values(\"d_avg\").reset_index().groupby(\"embedding\").first().reset_index()\n# best_sigs = (\n#     graphs.groupby([\"embedding\", \"signature\"])\n#     .mean()\n#     .sort_values(\"d_avg\")\n#     .reset_index()\n#     .groupby(\"embedding\")\n#     .first()\n#     .reset_index()[[\"embedding\", \"signature\"]]\n# )\n# graphs = pd.merge(graphs, best_sigs, on=[\"embedding\", \"signature\"])\ngraphs_c[\"signature\"] = graphs_c[\"signature\"].map(sig_dict)\n# graphs_c[\"task\"] = graphs_c[\"embedding\"].map(graph_task_dict)\ngraphs_c[\"task\"] = \"C\"\ngraphs_c[\"dataset\"] = graphs_c[\"embedding\"].map(graph_dataset_names)\ngraphs_c[\"table\"] = \"Graphs\"\n\ngraphs_r = pd.read_table(\"../data/results_icml_revision/regression_nn_graph.tsv\")\ngraphs_r[\"signature\"] = graphs_r[\"signature\"].map(sig_dict)\n# graphs_r[\"task\"] = graphs_r[\"embedding\"].map(graph_task_dict)\ngraphs_r[\"task\"] = \"R\"\ngraphs_r[\"dataset\"] = graphs_r[\"embedding\"].map(graph_dataset_names)\ngraphs_r[\"table\"] = \"Graphs\"\n\ngraphs = pd.concat([graphs_c, graphs_r])\n\n# # Link prediction datasets\n# link_dataset_names = {\n#     \"football\": \"Football\",\n#     \"karate_club\": \"Karate Club\",\n#     \"polbooks\": \"PolBooks\",\n#     \"adjnoun\": \"AdjNoun\",\n#     \"dolphins\": \"Dolphins\",\n#     \"lesmis\": \"Les Mis\",\n# }\n# links = pd.read_table(\"../data/results_icml_revision/link_prediction.tsv\")\n# links[\"task\"] = \"LP\"\n# # links[\"signature\"] = \"$(\\\\S{2}\\\\E{2}\\\\H{2})^2\\\\E{1}$\"\n# links[\"signature\"] = \"$\\\\S{2}\\\\E{2}\\\\H{2}$\"\n# links[\"dataset\"] = links[\"dataset\"].map(link_dataset_names)\n\n# VAE dataset\nvae_signature_dict = {\n    \"blood_cell_scrna\": \"$\\\\S{2}\\\\E{2}(\\\\H{2})^3$\",\n    \"lymphoma\": \"$(\\\\S{2})^2$\",\n    \"cifar_100\": \"$(\\\\S{2})^4$\",\n    \"mnist\": \"$\\\\S{2}\\\\E{2}\\\\H{2}$\",\n}\nvae_dataset_names = {\n    \"blood_cell_scrna\": \"Blood\",\n    \"lymphoma\": \"Lymphoma\",\n    \"cifar_100\": \"CIFAR-100\",\n    \"mnist\": \"MNIST\",\n}\nvae = pd.read_table(\"../data/results_icml_revision/vae.tsv\") # Assuming vae needs to be defined before use\nvae[\"task\"] = \"C\"\nvae[\"signature\"] = vae[\"embedding\"].map(vae_signature_dict)\nvae[\"dataset\"] = vae[\"embedding\"].map(vae_dataset_names)\n\n# Put it all together\nclass_1man[\"table\"] = \"Synthetic (single $K$)\"\nreg_1man[\"table\"] = \"Synthetic (single $K$)\"\nclass_sig[\"table\"] = \"Synthetic (multi-$K$)\"\nreg_sig[\"table\"] = \"Synthetic (multi-$K$)\"\n# temp[\"table\"] = \"Other\"\n# fourier[\"table\"] = \"Other\"\n# fourier2[\"table\"] = \"Other\"\n# traffic[\"table\"] = \"Other\"\n# land_water[\"table\"] = \"Other\"\ngraphs[\"table\"] = \"Graph embeddings\"\n# links[\"table\"] = \"Graph embeddings\"\nvae[\"table\"] = \"VAE\"\n\nall_data = pd.concat(\n    [\n        class_1man,\n        reg_1man,\n        class_sig,\n        reg_sig,\n        # temp,\n        # fourier,\n        # fourier2,\n        # traffic,\n        # land_water,\n        graphs,\n        # links,\n        vae,\n    ]\n)\n# all_data = all_data.drop(columns=[\"embedding\", \"curvature\", \"d_avg\", \"seed\", \"trial\"])\nall_data = all_data.drop(columns=[\"embedding\", \"curvature\", \"seed\", \"trial\"])\nall_data\n```\n\n----------------------------------------\n\nTITLE: Defining TorchProductSpaceDT Class for Product Manifold Classification\nDESCRIPTION: Defines a `TorchProductSpaceDT` class inheriting from `hyperdt.torch.product_space_DT.ProductSpaceDT`. This class implements a decision tree classifier specifically designed for product manifold data using PyTorch. It includes methods for calculating angles (`_get_angle_vals`), fitting the tree (`fit`, `_fit_node`) using the previously defined `calculate_info_gain` and `circular_greater` functions, predicting labels (`predict`, `_traverse`), and determining split directions (`_left`). The `fit` method uses angle values derived from the input data `X`.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/10_torchified_hyperdt.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom hyperdt.torch.product_space_DT import ProductSpaceDT\nfrom hyperdt.torch.tree import DecisionNode\nfrom hyperdt.torch.hyperbolic_trig import _hyperbolic_midpoint\n\n\nclass TorchProductSpaceDT(ProductSpaceDT):\n    def __init__(self, signature):\n        sig_r = [(x[1], x[0]) for x in signature]\n        super().__init__(signature=sig_r)\n        self.pm = embedders.manifolds.ProductManifold(signature=signature)\n\n    def _get_angle_vals(self, X):\n        angle_vals = torch.zeros((X.shape[0], self.pm.dim), device=X.device)\n\n        for i, M in enumerate(self.pm.P):\n            dims = self.pm.man2dim[i]\n            dims_target = self.pm.man2intrinsic[i]\n            if M.type in [\"H\", \"S\"]:\n                angle_vals[:, dims_target] = torch.atan2(X[:, dims[0]].view(-1, 1), X[:, dims[1:]])\n            elif M.type == \"E\":\n                angle_vals[:, dims_target] = torch.atan2(torch.tensor(1), X[:, dims])\n\n        return angle_vals\n\n    def fit(self, X, y):\n        \"\"\"Fit a decision tree to the data. Modified from HyperbolicDecisionTreeClassifier\n        to remove multiple timelike dimensions in product space.\"\"\"\n        # Find all dimensions in product space (including timelike dimensions)\n        self.all_dims = list(range(sum([space[0] + 1 for space in self.signature])))\n\n        # Find indices of timelike dimensions in product space\n        self.timelike_dims = [0]\n        for i in range(len(self.signature) - 1):\n            self.timelike_dims.append(sum([space[0] + 1 for space in self.signature[: i + 1]]))\n\n        # Remove timelike dimensions from list of dimensions\n        # self.dims_ex_time = list(np.delete(np.array(self.all_dims), self.timelike_dims))\n        self.dims_ex_time = [dim for dim in self.all_dims if dim not in self.timelike_dims]\n\n        # Get array of classes\n        self.classes_ = torch.unique(y)\n\n        # First, we can compute the angles of all 2-d projections\n        angle_vals = self._get_angle_vals(X)\n        self.tree = self._fit_node(X=angle_vals, y=y, depth=0)\n\n    def _fit_node(self, X, y, depth):\n        print(f\"Depth {depth} with {X.shape} samples\")\n        # Base case\n        if depth == self.max_depth or len(X) < self.min_samples_split or len(torch.unique(y)) == 1:\n            value, probs = self._leaf_values(y)\n            return DecisionNode(value=value, probs=probs)\n\n        # Recursively find the best split:\n        ig = calculate_info_gain(X, y)\n        best_idx = torch.argmax(ig)\n        best_row, best_dim = best_idx // X.shape[1], best_idx % X.shape[1]\n        best_ig = ig[best_row, best_dim]\n\n        # Since we're evaluating greater than, we need to also find the next-largest value and take the midpoint\n        next_largest = torch.max(X[~circular_greater(X[:, best_dim], X[best_row, best_dim]), best_dim])\n\n        # Midpoint computation will depend on manifold; TODO: actually do this\n        # best_theta = (X[best_row, best_dim] + next_largest) / 2\n        best_manifold = self.pm.P[self.pm.intrinsic2man[best_dim.item()]]\n        if best_manifold.type == \"H\":\n            best_theta = _hyperbolic_midpoint(X[best_row, best_dim], next_largest)\n        elif best_manifold.type == \"S\":\n            best_theta = (X[best_row, best_dim] + next_largest) / 2\n        else:\n            best_theta = torch.arctan2(torch.tensor([2.0], device=X.device), X[best_row, best_dim] + next_largest)\n\n        # Fallback case:\n        if best_ig <= 0:\n            print(f\"Fallback triggered at depth {depth}\")\n            value, probs = self._leaf_values(y)\n            return DecisionNode(value=value, probs=probs)\n\n        # Populate:\n        node = DecisionNode(feature=best_dim, theta=best_theta)\n        node.score = best_ig\n        left, right = circular_greater(X[:, best_dim], best_theta), ~circular_greater(X[:, best_dim], best_theta)\n        node.left = self._fit_node(X=X[left], y=y[left], depth=depth + 1)\n        node.right = self._fit_node(X=X[right], y=y[right], depth=depth + 1)\n        return node\n\n    def predict(self, X):\n        angle_vals = self._get_angle_vals(X)\n        return torch.tensor([self._traverse(x).value for x in angle_vals], device=X.device)\n\n    def _left(self, x, node):\n        \"\"\"Boolean: Go left?\"\"\"\n        return circular_greater(x[node.feature], node.theta)\n```\n\n----------------------------------------\n\nTITLE: Exporting Experiment Results to TSV with Pandas in Python\nDESCRIPTION: This snippet creates a pandas DataFrame from the accumulated experiment results and saves it to a tab-separated values (TSV) file. Required dependency is pandas, and the results list must be populated with compatible structures (e.g., dictionaries). Main parameters include the output file path and DataFrame options such as tab separator and index exclusion. The input is the collected results, and the output is a TSV file containing the benchmark metrics for subsequent analysis.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/51_gcn_link_prediction.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\nresults_df = pd.DataFrame(results)\nresults_df.to_csv(\"../data/results_icml_revision/link_prediction.tsv\", sep=\"\\t\", index=False)\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating Neural and Manifold-based Classifiers Using PyTorch and Manify - Python\nDESCRIPTION: This snippet provides reusable training and evaluation utilities for PyTorch models, performs data splits, and demonstrates classification using an MLP and Manify's KappaGCN. Dependencies: PyTorch, scikit-learn, tqdm, and Manify. train_model handles optional adjacency matrices and supports both baseline (MLP) and custom (KappaGCN) architectures. Parameters include model, data, adjacency, epochs, and learning rate. Outputs are trained models and printed classification accuracy for each. Requires all imports and previous data preparation steps. Limitations: Assumes classification tasks and that all inputs are correctly formatted tensors.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/54_kappa_gcn_benchmark.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import train_test_split\\nfrom tqdm.notebook import tqdm\\nfrom typing import Callable, Tuple, Optional\\nimport torch\\nfrom torch import Tensor\\n\\ndef train_model(model, X_train, y_train, A_hat=None, num_epochs=1000, lr=0.01):\\n    loss_fn = torch.nn.CrossEntropyLoss()\\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\\n    progress_bar = tqdm(total=num_epochs)\\n\\n    for i in range(num_epochs):\\n        # Forward pass\\n        if A_hat is not None:\\n            y_pred = model(X_train, A_hat)\\n        else:\\n            y_pred = model(X_train)\\n\\n        # Compute loss\\n        loss = loss_fn(y_pred, y_train)\\n\\n        # Backward pass\\n        optimizer.zero_grad()\\n        loss.backward()\\n        optimizer.step()\\n\\n        # Update progress bar\\n        progress_bar.update(1)\\n        progress_bar.set_postfix(loss=loss.item())\\n\\n    return model\\n\\ndef evaluate_model(model, X_test, y_test, A_hat=None):\\n    with torch.no_grad():\\n        if A_hat is not None:\\n            y_pred = model(X_test, A_hat)\\n        else:\\n            y_pred = model(X_test)\\n\\n        acc = (y_pred.argmax(dim=1) == y_test).float().mean()\\n    return acc.item()\\n\\n\\n# Split data\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\\n# Basic MLP classifier\\nmlp = torch.nn.Sequential(torch.nn.Linear(2, 2, bias=False), torch.nn.ReLU(), torch.nn.Linear(2, 4))\\nmlp = train_model(mlp, X_train, y_train)\\nmlp_accuracy = evaluate_model(mlp, X_test, y_test)\\nprint(f\"MLP accuracy: {mlp_accuracy}\")\\n\\n# KappaGCN (setting kappa to 2e-4 ensures we hit the non-Euclidean case in get_logits)\\npm = manify.manifolds.ProductManifold([(2e-4, 2)], stereographic=True)\\nkgcn = manify.predictors.kappa_gcn.KappaGCN(\\n    pm=pm, output_dim=4, hidden_dims=[2], nonlinearity=torch.relu, task=\"classification\"\\n)\\nkgcn = train_model(kgcn, X_train, y_train, A_hat=torch.eye(X_train.shape[0]))\\nkgcn_accuracy = evaluate_model(kgcn, X_test, y_test, A_hat=torch.eye(X_test.shape[0]))\\nprint(f\"KappaGCN accuracy: {kgcn_accuracy}\")\n```\n\n----------------------------------------\n\nTITLE: Running ROC/AUC Experiments with Manifold and Standard Decision Trees - Python\nDESCRIPTION: Executes 100 trials comparing a standard Decision Tree (from scikit-learn) and a ProductSpace Decision Tree (from the custom embedders module) on synthetic Gaussian mixture data defined on a product manifold. The snippet defines data generation, model initialization, fitting, predicting probabilities, custom ROC calculation, and area under the curve (AUC) measurement. Custom and standard manifold configurations are supported, and extensive averaging across trials is performed for statistical reliability. Dependencies include numpy, pandas, matplotlib, scikit-learn, and a sufficiently complete 'embedders' package. Inputs include parameters for number of trials, tree depth, and synthetic data characteristics. Outputs are arrays of ROC points and AUC metrics.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/20_roc_auc.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.metrics import roc_curve, auc\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\n\\nN_TRIALS = 100\\nMAX_DEPTH = 5\\nSPECIAL_DIMS = False\\n\\npm = embedders.manifolds.ProductManifold(\\n    # signature=[(-1, 2)], (0, 2), (1, 2)]\\n    signature=[(2, 2)]\\n)\\n\\ndef roc_curve_uniform(y_true, probs):\\n    thresholds = np.linspace(0, 1.01, 101)\\n    tpr = []\\n    fpr = []\\n    for threshold in thresholds:\\n        y_pred = (probs[:, 1] > threshold).astype(int)\\n        tp = np.sum((y_pred == 1) & (y_true == 1))\\n        fp = np.sum((y_pred == 1) & (y_true == 0))\\n        tn = np.sum((y_pred == 0) & (y_true == 0))\\n        fn = np.sum((y_pred == 0) & (y_true == 1))\\n        tpr.append(tp / (tp + fn))\\n        fpr.append(fp / (fp + tn))\\n    return fpr, tpr#, thresholds\\n\\n# results = np.zeros((100, 2))\\nfprs_dt = np.zeros((N_TRIALS, 101))\\ntprs_dt = np.zeros((N_TRIALS, 101))\\nfprs_pdt = np.zeros((N_TRIALS, 101))\\ntprs_pdt = np.zeros((N_TRIALS, 101))\\naucs_dt = np.zeros(N_TRIALS)\\naucs_pdt = np.zeros(N_TRIALS)\\nfor i in range(N_TRIALS):\\n    X, y = embedders.gaussian_mixture.gaussian_mixture(\\n        pm, cov_scale_points=1.0, cov_scale_means=0.1\\n    )\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\\n\\n    dt = DecisionTreeClassifier(max_depth=MAX_DEPTH)\\n    dt.fit(X_train, y_train)\\n    dt_probs = dt.predict_proba(X_test)\\n\\n    pdt = embedders.tree_new.ProductSpaceDT(pm, max_depth=MAX_DEPTH, use_special_dims=SPECIAL_DIMS)\\n    pdt.fit(X_train, y_train)\\n    pdt_probs = pdt.predict_proba(X_test).detach().numpy()\\n\\n    # Soften probabilities to deal with discrete outputs\\n    dt_probs = dt_probs + 1e-6\\n    dt_probs = dt_probs / np.sum(dt_probs, axis=1)[:, np.newaxis]\\n    pdt_probs = pdt_probs + 1e-6\\n    pdt_probs = pdt_probs / np.sum(pdt_probs, axis=1)[:, np.newaxis]\\n\\n    # Get whole roc curve\\n    # fpr_dt, tpr_dt, thresholds_dt = roc_curve(y_test, dt_probs[:, 1])\\n    # fpr_pdt, tpr_pdt, threshdolds_pdt = roc_curve(y_test, pdt_probs[:, 1])\\n    fpr_dt, tpr_dt = roc_curve_uniform(y_test.numpy(), dt_probs)\\n    fpr_pdt, tpr_pdt = roc_curve_uniform(y_test.numpy(), pdt_probs)\\n\\n    fprs_dt[i] = fpr_dt\\n    tprs_dt[i] = tpr_dt\\n    fprs_pdt[i] = fpr_pdt\\n    tprs_pdt[i] = tpr_pdt\\n\\n    # Get AUCs as well\\n    aucs_dt[i] = auc(fpr_dt, tpr_dt)\\n    aucs_pdt[i] = auc(fpr_pdt, tpr_pdt)\n```\n\n----------------------------------------\n\nTITLE: Setting Up PyTorch Device (CUDA/CPU) in Python\nDESCRIPTION: This code checks for CUDA availability using PyTorch. If a CUDA-enabled GPU is found, it sets the default device to the first GPU ('cuda:0'). Otherwise, it falls back to using the CPU. The selected device is stored in the 'device' variable and printed.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/4_cs_phds_regression.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Torch device management\nimport torch\n\nif torch.cuda.is_available():\n    torch.cuda.set_device(0)\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\nprint(f\"Using device: {device}\")\n```\n```\n\n----------------------------------------\n\nTITLE: Train Decision Tree on Extracted Time Features\nDESCRIPTION: This snippet trains a `DecisionTreeRegressor` using the newly engineered time features (day of week, hour, day of year, minute) as input (X) and 'Vehicles' as the target (y). The data is split, the model (max_depth=3) is trained on the log-transformed target, and the RMSE on the log-transformed test predictions is calculated and printed.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/33_traffic.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nX = df[[\"day_of_week\", \"hour\", \"day_of_year\", \"minute\"]]\ny = df[\"Vehicles\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ndt = DecisionTreeRegressor(max_depth=3)\ndt.fit(X_train, np.log(y_train))\n\nnp.sqrt(mean_squared_error(np.log(y_test), dt.predict(X_test)))\n```\n\n----------------------------------------\n\nTITLE: Train Decision Tree on Timestamp Feature\nDESCRIPTION: This snippet prepares data for a baseline model. It converts the 'DateTime' column to Unix timestamps (integers), splits the data into training and testing sets using `train_test_split`. It then trains a `DecisionTreeRegressor` (with `max_depth=3`) using the timestamp as the single feature and the log-transformed 'Vehicles' count as the target. Finally, it calculates and prints the Root Mean Squared Error (RMSE) on the log-transformed test predictions.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/33_traffic.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Turn datetime to floats\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nimport numpy as np\n\n\n# X = df[\"DateTime\"].astype(int)\n# Convert DateTime to number\nX = pd.to_datetime(df[\"DateTime\"]).astype(int)\ny = df[\"Vehicles\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\ndt = DecisionTreeRegressor(max_depth=3)\ndt.fit(X_train.values.reshape(-1, 1), np.log(y_train))\n\nfrom sklearn.metrics import mean_squared_error\n\nnp.sqrt(mean_squared_error(np.log(y_test), dt.predict(X_test.values.reshape(-1, 1))))\n```\n\n----------------------------------------\n\nTITLE: Training and Using KappaGCN Regression Model in embedders - Python\nDESCRIPTION: Trains a kappa-GCN model on product manifold data using precomputed distance matrices and their normalized adjacency. Applies stereographic projection, computes graph adjacency, and initializes a KappaGCN model for regression. Requires torch, embedders, and a CUDA-capable environment. Outputs trained model; 'A_hat' must have no NaNs, and fitting uses specified learning rate and epochs for reproducibility.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/25_benchmark_globe.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\\n\\ndists = pm.pdist2(X.cuda())\\ndists /= dists[torch.isfinite(dists)].max()\\nA_hat = embedders.predictors.kappa_gcn.get_A_hat(dists).detach()\\nA_hat.isnan().any()\\n\\npm_stereo, X_stereo = pm.stereographic(X)\\n\\nkappa_gcn = embedders.predictors.kappa_gcn.KappaGCN(\\n    pm=pm_stereo, hidden_dims=[pm.dim, pm.dim], task=\"regression\", output_dim=1\\n).cuda()\\n\\n# y_pred = kappa_gcn(X_stereo_train.cuda(), A_hat.cuda())\\n# loss_fn = torch.nn.MSELoss()\\n# loss_fn(y_pred, y.cuda())\\n\\nkappa_gcn.fit(X_stereo.cuda(), y.cuda(), A_hat.cuda(), lr=1e-2, epochs=1000, use_tqdm=True)\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating Classifiers on PCA-Reduced MNIST Data in Python\nDESCRIPTION: Splits PCA-transformed MNIST data into training and test sets, then trains and evaluates three classifiers: Decision Tree, Random Forest, and k-Nearest Neighbors. Uses scikit-learn estimators and prints the F1 score (micro average) for each. Dependencies include sklearn and a correctly shaped X_pca and labels. X_train/test and y_train/test represent data splits; max_depth, n_estimators, and other model hyperparameters are configurable.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/38_mnist.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\nX_train, X_test, y_train, y_test = train_test_split(X_pca, labels, test_size=0.2)\n\ndt = DecisionTreeClassifier(max_depth=3)\ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)\nprint(f1_score(y_test, y_pred, average=\"micro\"))\n\nrf = RandomForestClassifier(n_estimators=12, max_depth=3)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\nprint(f1_score(y_test, y_pred, average=\"micro\"))\n\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nprint(f1_score(y_test, y_pred, average=\"micro\"))\n```\n\n----------------------------------------\n\nTITLE: Testing Manifold Properties and Operations (Python)\nDESCRIPTION: This comprehensive test suite iterates over several curvature and dimensionality settings to construct a manifold, then validates device handling, manifold type, and vector generation using Gaussian mixture methods. It checks embedding correctness, inner products, distances, squared distances, log-likelihoods, exponential and logarithmic maps, stereographic conversion and its inverse, and manifold-wide function application (with a relu example). All steps use torch and geoopt, and rely on internal Manify API. Inputs are curvature and dimension, outputs are assertions or errors if any operation fails.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/60_pytest_scratch.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfor curv, dim in [(-1, 2), (0, 2), (1, 2), (-1, 64), (0, 64), (1, 64)]:\n    M = Manifold(curvature=curv, dim=dim)\n\n    # Does device switching work?\n    M.to(\"cpu\")\n\n    # Do attributes work correctly?\n    if curv < 0:\n        assert M.type == \"H\" and isinstance(M.manifold.base, geoopt.Lorentz)\n    elif curv == 0:\n        assert M.type == \"E\" and isinstance(M.manifold.base, geoopt.Euclidean)\n    else:\n        assert M.type == \"S\" and isinstance(M.manifold.base, geoopt.Sphere)\n\n    # get some vectors via gaussian mixture\n    cov = torch.eye(M.dim) / M.dim / 100\n    means = torch.vstack([M.mu0] * 10)\n    covs = torch.stack([cov] * 10)\n    X1, _ = M.sample(z_mean=means, sigma=covs)\n    X2, _ = M.sample(z_mean=means[:5], sigma=covs[:5])\n\n    # Verify points are on manifold\n    assert M.manifold.check_point(X1), \"X1 is not on the manifold\"\n    assert M.manifold.check_point(X2), \"X2 is not on the manifold\"\n\n    # Inner products\n    ip_11 = M.inner(X1, X1)\n    assert ip_11.shape == (10, 10), \"Inner product shape mismatch for X1\"\n    ip_12 = M.inner(X1, X2)\n    assert ip_12.shape == (10, 5), \"Inner product shape mismatch for X1 and X2\"\n    if curv == 0:\n        assert torch.allclose(ip_11, X1 @ X1.T), \"Euclidean inner products do not match for X1\"\n        assert torch.allclose(ip_12, X1 @ X2.T), \"Euclidean inner products do not match for X1 and X2\"\n\n    # Dists\n    dists_11 = M.dist(X1, X1)\n    assert dists_11.shape == (10, 10), \"Distance shape mismatch for X1\"\n    dists_12 = M.dist(X1, X2)\n    assert dists_12.shape == (10, 5), \"Distance shape mismatch for X1 and X2\"\n    if curv == 0:\n        assert torch.allclose(\n            dists_12, torch.linalg.norm(X1[:, None] - X2[None, :], dim=-1)\n        ), \"Euclidean distances do not match for X1 and X2\"\n        assert torch.allclose(\n            dists_11, torch.linalg.norm(X1[:, None] - X1[None, :], dim=-1)\n        ), \"Euclidean distances do not match for X1\"\n    assert (dists_11.triu(1) >= 0).all(), \"Distances for X1 should be non-negative\"\n    assert (dists_12.triu(1) >= 0).all(), \"Distances for X2 should be non-negative\"\n    assert torch.allclose(dists_11.triu(1), M.pdist(X1).triu(1)), \"dist and pdist diverge for X1\"\n\n    # Square dists\n    sqdists_11 = M.dist2(X1, X1)\n    assert sqdists_11.shape == (10, 10), \"Squared distance shape mismatch for X1\"\n    sqdists_12 = M.dist2(X1, X2)\n    assert sqdists_12.shape == (10, 5), \"Squared distance shape mismatch for X1 and X2\"\n    if curv == 0:\n        assert torch.allclose(\n            sqdists_12, torch.linalg.norm(X1[:, None] - X2[None, :], dim=-1) ** 2\n        ), \"Euclidean squared distances do not match for X1 and X2\"\n        assert torch.allclose(\n            sqdists_11, torch.linalg.norm(X1[:, None] - X1[None, :], dim=-1) ** 2\n        ), \"Euclidean squared distances do not match for X1\"\n    assert (sqdists_11.triu(1) >= 0).all(), \"Squared distances for X1 should be non-negative\"\n    assert (sqdists_12.triu(1) >= 0).all(), \"Squared distances for X1 and X2 should be non-negative\"\n    assert torch.allclose(sqdists_11.triu(1), M.pdist2(X1).triu(1)), \"sqdists_11 and pdist2 diverge for X1\"\n\n    # Log-likelihood\n    lls = M.log_likelihood(X1)\n    if curv == 0:\n        # Evaluate as ll of gaussian with mean 0, variance 1:\n        assert torch.allclose(\n            lls,\n            -0.5 * (torch.sum(X1**2, dim=-1) + X1.size(-1) * math.log(2 * math.pi)),\n        ), \"Log-likelihood mismatch for Gaussian\"\n    assert (lls <= 0).all(), \"Log-likelihood should be non-positive\"\n\n    # Logmap and expmap\n    logmap_x1 = M.logmap(X1)\n    assert M.manifold.check_vector(logmap_x1), \"Logmap point should be in the tangent plane\"\n    expmap_x1 = M.expmap(logmap_x1)\n    assert M.manifold.check_point(expmap_x1), \"Expmap point should be on the manifold\"\n    assert torch.allclose(expmap_x1, X1, atol=1e-5), \"Expmap does not return the original points\"\n\n    # Stereographic conversions\n    M_stereo, X1_stereo, X2_stereo = M.stereographic(X1, X2)\n    assert M_stereo.is_stereographic\n    X_inv_stereo, X1_inv_stereo, X2_inv_stereo = M_stereo.inverse_stereographic(X1_stereo, X2_stereo)\n    assert not X_inv_stereo.is_stereographic\n    assert torch.allclose(X1_inv_stereo, X1), \"Inverse stereographic conversion mismatch for X1\"\n    assert torch.allclose(X2_inv_stereo, X2), \"Inverse stereographic conversion mismatch for X2\"\n\n    # Apply\n    @M.apply\n    def apply_function(x):\n        return torch.nn.functional.relu(x)\n\n    result = apply_function(X1)\n    assert result.shape == X1.shape, \"Result shape mismatch for apply_function\"\n    assert M.manifold.check_point(result)\n\n```\n\n----------------------------------------\n\nTITLE: Training Binary Perceptron with Limited Rounds/Updates\nDESCRIPTION: This snippet trains another binary `mix_curv_perceptron` instance, but with significantly reduced training iterations (`max_round=1`, `max_update=100`). It uses the same data as the first binary example. The score returned by `process_data()` is printed.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nps_perceptron_more_epochs = mix_curv_perceptron(\n    mix_component=mix_component,\n    embed_data=embed_data,\n    multiclass=False,  # for now, just do binary\n    max_round=1,\n    max_update=100,\n)\nscore = ps_perceptron.process_data()\nprint(score)\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating Classifiers on Raw MNIST Data (No PCA) in Python\nDESCRIPTION: Performs classification using the raw, flattened MNIST images, omitting PCA and using the full 784 features. The workflow mirrors the PCA version: data reshaping, splitting, and running Decision Tree, Random Forest, and k-Nearest Neighbors classifiers, all evaluated using F1 score (micro). Requires scikit-learn and expects X to be compatible with .reshape and .numpy(). Useful for benchmarking PCA vs. non-PCA approaches.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/38_mnist.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# non-pca version\n\nX_np = X.reshape(-1, 28*28).numpy()\nX_train, X_test, y_train, y_test = train_test_split(X_np, labels, test_size=0.2)\n\ndt = DecisionTreeClassifier(max_depth=3)\ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)\nprint(f1_score(y_test, y_pred, average=\"micro\"))\n\nrf = RandomForestClassifier(n_estimators=12, max_depth=3)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\nprint(f1_score(y_test, y_pred, average=\"micro\"))\n\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nprint(f1_score(y_test, y_pred, average=\"micro\"))\n```\n\n----------------------------------------\n\nTITLE: Setting up PyTorch Device for Benchmark\nDESCRIPTION: This snippet configures the PyTorch device for the benchmark, prioritizing CUDA device 1 if available, otherwise falling back to the CPU. It explicitly checks for CUDA availability and assigns the device.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/2_polblogs_benchmark.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport embedders\nimport numpy as np\n\n# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n# device\nif torch.cuda.is_available():\n    device = torch.device(\"cuda:1\")\n# elif torch.backends.mps.is_available(): # No MPS - float64 not supported\n#     device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")\ndevice\n```\n\n----------------------------------------\n\nTITLE: Gaussian Mixture and Stereographic for ProductManifold (Python)\nDESCRIPTION: This snippet generates a synthetic Gaussian mixture dataset using a purely Euclidean product manifold, and applies stereographic and inverse stereographic projections. Inputs are pm as a fully Euclidean ProductManifold and its gaussian_mixture-generated data. Outputs are projected data tuples for subsequent analysis. Dependencies: torch, ProductManifold.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/60_pytest_scratch.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\npm = ProductManifold(signature=[(0, 8), (0, 8), (0, 8)])\n\nX, y = pm.gaussian_mixture()\npm_stereo, X_stereo = pm.stereographic(X)\n_, X2 = pm_stereo.inverse_stereographic(X_stereo)\n\n```\n\n----------------------------------------\n\nTITLE: Traversing Decision Tree Level by Level in Python\nDESCRIPTION: Implements a breadth-first traversal of the trained `ProductSpaceDT` (`pdt`). It preprocesses the input data `X` to get angles, then iterates through the tree levels. For each node visited, it maintains a boolean mask (`mask`) indicating which data points reach that node. It uses the internal `_angular_greater` function to split the mask based on the node's feature angle and threshold. The list `levels` stores tuples of (node, mask) for each node at each level. Finally, it prints the number of nodes at each level.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_pc_basic_visualization.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# This code will traverse the decision tree level by level and do the following:\n# 1. Store a mask for whether data points are \"live\" or not, i.e. whether they were thrown out in previous levels\n# 2. Get projection dimensions (pdt.angle_dims[feature])\n# 3. Get the threshold\n# 4. Store the (mask, dim1, dim2, threshold) tuple in a list\nimport torch\nfrom embedders.predictors.tree_new import _angular_greater\nimport matplotlib.pyplot as plt\n\nangles, _, _, comparisons = pdt._preprocess(X, y)\n\nlevels = []\nto_visit = [(pdt.nodes[0], torch.ones(X.shape[0]).bool())]\nwhile to_visit:\n    levels.append(to_visit)\n    new_to_visit = []\n    l = [\n        (node.left, mask & _angular_greater(angles[:, node.feature], torch.tensor(node.theta)).flatten())\n        for node, mask in to_visit\n        if node.left is not None\n    ]\n    r = [\n        (node.right, mask & ~_angular_greater(angles[:, node.feature], torch.tensor(node.theta)).flatten())\n        for node, mask in to_visit\n        if node.right is not None\n    ]\n    to_visit = l + r\n\nprint([len(l) for l in levels])\n```\n\n----------------------------------------\n\nTITLE: Defining Utility Functions for Data Preparation\nDESCRIPTION: This code defines two utility functions. `get_signature_str` creates a string representation of the product manifold's signature (types and dimensions). `get_embed_data` splits input data (X, y) into training and testing sets using `sklearn.model_selection.train_test_split`, converts them to NumPy arrays, and returns a dictionary containing the splits along with manifold-specific information like maximum norms and curvature values.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# [REDACTED]'s code for signature conversion, slightly rewritten for the new class\nfrom sklearn.model_selection import train_test_split\n\n\ndef get_signature_str(pm):\n    return \",\".join([f\"{M.type.lower()}{M.dim}\" for M in pm.P])\n\n\ndef get_embed_data(pm, X, y):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    return {\n        \"X_train\": X_train.detach().cpu().numpy(),\n        \"X_test\": X_test.detach().cpu().numpy(),\n        \"y_train\": y_train.detach().cpu().numpy(),\n        \"y_test\": y_test.detach().cpu().numpy(),\n        \"max_norm\": [M.manifold.inner(x, x).max().item() for M, x in zip(pm.P, pm.factorize(X))],\n        \"curv_value\": [abs(M.curvature) for M in pm.P],\n    }\n```\n\n----------------------------------------\n\nTITLE: Loading MNIST Data Using Embedders in Python\nDESCRIPTION: Loads the MNIST dataset using the custom 'embedders.dataloaders.load' function. The returned X contains the image data, 'labels' holds the class labels, and an unused third value is discarded. This snippet assumes the embedders package implements a suitable 'dataloaders.load' interface compatible with the MNIST dataset's shape and type requirements.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/38_mnist.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nX, labels, _ = embedders.dataloaders.load(\"mnist\")\nX\n```\n\n----------------------------------------\n\nTITLE: Defining Perceptron Prediction Function with Sigmoid Scaling\nDESCRIPTION: This snippet defines a function `perceptron_predict` to obtain class probabilities and predictions from a trained `mix_curv_perceptron` object (`hyperdt`). It addresses a perceived limitation where the original library might not directly return predictions. It iterates through each class, performs binary one-vs-rest classification internally using the perceptron's methods (`mix_classifier_train`, `mix_classifier_test`), applies Sigmoid scaling (`SigmoidTrain`, `SigmoidPredict` from `hyperdt.platt`) to get probabilities, and returns the predicted class labels based on the highest probability. The example usage calls this function on the previously trained multiclass perceptron.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Ok, I'm not happy with how their code doesn't return predictions. Let's rewrite this a bit\nimport numpy as np\nfrom hyperdt.platt import SigmoidTrain, SigmoidPredict\n\n\ndef perceptron_predict(perceptron):\n    tmp_error_record = {0: 1}\n    test_probability = np.zeros((perceptron.n_test_samples, perceptron.n_class), dtype=float)\n    for class_val in perceptron.class_labels:\n        y_bin_train = np.array([1 if val == class_val else -1 for val in perceptron.y_train])\n\n        # Train part\n        decision_vals = [0] * perceptron.n_train_samples\n        for idx in range(perceptron.n_train_samples):\n            decision_vals[idx] = perceptron.mix_classifier_train(idx, tmp_error_record, y_bin_train)\n            tmp_ab = SigmoidTrain(deci=decision_vals, label=y_bin_train, prior1=None, prior0=None)\n\n        # Test part\n        for idx in range(perceptron.n_test_samples):\n            yn = perceptron.mix_classifier_test(idx, tmp_error_record, y_bin_train)\n            test_probability[idx, perceptron.class_labels.index(class_val)] = SigmoidPredict(deci=yn, AB=tmp_ab)\n\n    # Get predictions\n    return test_probability\n\n\nperceptron_predict(ps_perceptron).argmax(axis=1)\n\n# Now we start to see the issue: all predictions are 1\n```\n\n----------------------------------------\n\nTITLE: Training a Tangent MLP Classifier with embedders.neural on Manifold Data (Python)\nDESCRIPTION: Uses 'TangentMLPClassifier' from embedders.neural to classify points on a product manifold, after splitting data into training and testing splits. The classifier is trained and its accuracy evaluated on the test set. Dependencies: embedders, scikit-learn, and the TangentMLPClassifier class; expects data compatible with manifold methods.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/41_more_benchmarks.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom embedders.neural import TangentMLPClassifier\\nfrom sklearn.model_selection import train_test_split\\n\\npm = embedders.manifolds.ProductManifold(signature=[(1, 2), (-1, 2)])\\nX, y = embedders.gaussian_mixture.gaussian_mixture(pm, num_clusters=32, num_classes=8)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\\n\\ntmc = TangentMLPClassifier(pm, input_dim=6, hidden_dims=[6, 6], lr=0.01)\\ntmc.fit(X_train, y_train)\\nprint(\"TMC acc:\", (tmc.predict(X_test) == y_test).float().mean().item())\n```\n\n----------------------------------------\n\nTITLE: Comparing ProductDT and Decision Tree on H2xE2xS2 Embedding\nDESCRIPTION: This snippet compares the performance of 'ProductSpaceDT' and Scikit-learn's 'DecisionTreeClassifier' on the H2 x E2 x S2 Polblogs embedding generated previously. It splits the data, trains both models (max depth 3), predicts on the test set, and prints their F1 scores. Includes TODO comments for future evaluations with other models.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/2_polblogs_benchmark.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Compare productDT and sklearn on this dataset\nfrom sklearn.tree import DecisionTreeClassifier\n\nX_train, X_test, y_train, y_test = train_test_split(\n    h2_e2_s2_polblogs, polblogs_labels.values, test_size=0.2, random_state=0\n)\n\npdt2 = ProductSpaceDT(pm=pm2, max_depth=3)\npdt2.fit(X_train, y_train)\npdt2_f1 = f1_score(y_test, pdt2.predict(X_test))\nprint(f\"ProductDT\\t{pdt2_f1*100:.2f}\")\n\ndt = DecisionTreeClassifier(max_depth=3)\ndt.fit(X_train, y_train)\ndt_f1 = f1_score(y_test, dt.predict(X_test))\nprint(f\"DT\\t{dt_f1*100:.2f}\")\n# cv_eval(dt, \"DT\", h2_e2_s2_polblogs, polblogs_labels)\n\n# TODO: [REDACTED]: product perceptron eval\n# TODO: [REDACTED]: product SVM eval\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Link Prediction on Karate Club Dataset (Python)\nDESCRIPTION: Performs link prediction benchmarking on the Zachary's Karate Club dataset over `N_TRIALS` repetitions. It loads the dataset, normalizes distances, and in each trial: trains node embeddings on a ProductManifold (`pm`) using `train_coords`, creates a link prediction dataset (`X`, `y`) using `make_link_prediction_dataset`, runs classification benchmarks (`embedders.benchmarks.benchmark`) comparing different models, calculates the average embedding distortion (`d_avg`), and appends results. After all trials, it aggregates results into a pandas DataFrame, prints the mean and standard error for each classifier metric, performs Wilcoxon signed-rank tests to compare classifiers, prints the average distortion, and saves the full results to a TSV file.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/22_link_prediction.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Karate club\n\nfor dataset in [\"karate_club\", \"\"] # Note: Loop seems incomplete/incorrectly placed based on subsequent code\ndists, labels, adj = embedders.dataloaders.load(\"karate_club\")\ndists = dists / dists.max()\n\nresults = []\nmy_tqdm = tqdm(total=N_TRIALS)\nwhile len(results) < N_TRIALS:\n    pm = ProductManifold(signature=SIGNATURE)\n    try:\n        X_embed, losses = train_coords(\n            pm,\n            dists,\n            burn_in_iterations=int(0.1 * TOTAL_ITERATIONS),\n            training_iterations=int(0.9 * TOTAL_ITERATIONS),\n            scale_factor_learning_rate=0.02,\n        )\n        assert not torch.isnan(X_embed).any()\n\n        X, y, pm_new = make_link_prediction_dataset(X_embed, pm, adj, add_dists=USE_DISTS)\n\n        res = embedders.benchmarks.benchmark(\n            X, y, pm_new, max_depth=MAX_DEPTH, task=\"classification\", use_special_dims=USE_SPECIAL_DIMS\n        )\n        res[\"d_avg\"] = embedders.metrics.d_avg(pm.pdist(X_embed), dists).item()\n        results.append(res)\n        my_tqdm.update(1)\n\n    except Exception as e:\n        print(e)\n        # print(f\"Failed iteration {len(results)}\")\n\n\n# Print results\nresults = pd.DataFrame(results)\nfor col in results.columns:\n    if col not in [\"model\", \"d_avg\"]:\n        r = results[col]\n        print(f\"{col}: {r.mean():.4f} +/- {r.std() / np.sqrt(N_TRIALS):.4f}\", end=\" \")\n\n        for col2 in results.columns:\n            if col2 not in [\"model\", col, \"d_avg\"]:\n                stat, p = wilcoxon(results[col], results[col2])\n                if p < 0.05 / 6 and results[col].mean() > results[col2].mean():\n                    print(f\"> {col2}\", end=\" \")\n\n        print()\nprint(f\"d_avg: {results['d_avg'].mean():.4f} +/- {results['d_avg'].std() / np.sqrt(N_TRIALS):.4f}\")\n\n# Save results\nresults.to_csv(\"../data/graph_benchmarks/karate_club_link.tsv\", index=False, sep=\"\\t\")\n```\n\n----------------------------------------\n\nTITLE: Save Benchmark Results to TSV\nDESCRIPTION: This snippet saves the pandas DataFrame `results`, which contains the outcomes of the benchmarking experiment, to a tab-separated values (TSV) file named `traffic_results.tsv` in a specified directory. The index of the DataFrame is not included in the file.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/33_traffic.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nresults.to_csv(\"../data/results_icml/traffic_results.tsv\", index=False, sep=\"\\t\")\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Models on Manifolds using 'manify' in Python\nDESCRIPTION: Performs a benchmark experiment using the 'manify' library. It initializes an empty list 'results' to store outcomes. It defines experiment parameters: a list of 'CURVATURES', data 'DIM', 'N_SAMPLES' per curvature, 'N_POINTS' per sample, 'N_CLASSES' or clusters, covariance scaling factors, the 'TASK' ('classification' or 'regression'), and corresponding evaluation 'SCORE' metrics. The code iterates through each curvature and sample seed, creating a 'ProductManifold' instance on the 'sample_device'. It generates a Gaussian mixture dataset (X, y) on this manifold using 'pm.gaussian_mixture', moves data to the main 'device', and then runs the benchmark using 'manify.utils.benchmarks.benchmark' (specifying 'cpu' for the benchmark execution device). Results for each run, along with curvature and seed, are appended to the 'results' list. Finally, the collected results are converted into a Pandas DataFrame.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/26_benchmark_single_curvature_gaussian.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nresults = []\n\n# CURVATURES = [-4, -2, -1, -0.5, -0.25, 0, 0.25, 0.5, 1, 2, 4]\nCURVATURES = [-2, -1, -0.5, 0, 0.5, 1, 2]\nDIM = 2\nN_SAMPLES = 10\nN_POINTS = 1_000\nN_CLASSES = 8\nN_CLUSTERS = 32\nCOV_SCALE_MEANS = 1.0\nCOV_SCALE_POINTS = 1.0\n\n# TASK = \"regression\"\nTASK = \"classification\"\nSCORE = [\"f1-micro\", \"accuracy\"] if TASK == \"classification\" else [\"rmse\"]\n\nmy_tqdm = tqdm(total=len(CURVATURES) * N_SAMPLES)\nfor i, K in enumerate(CURVATURES):\n    for seed in range(N_SAMPLES):\n        # Ensure unique seed per trial\n        seed = seed + N_SAMPLES * i\n        pm = manify.manifolds.ProductManifold(signature=[(K, DIM)]).to(sample_device)\n\n        # Get X, y\n        X, y = pm.gaussian_mixture(\n            seed=seed,\n            num_points=N_POINTS,\n            num_classes=N_CLASSES,\n            num_clusters=N_CLUSTERS,\n            cov_scale_means=COV_SCALE_MEANS / DIM,\n            cov_scale_points=COV_SCALE_POINTS / DIM,\n            task=TASK,\n        )\n        X = X.to(device)\n        y = y.to(device)\n        pm = pm.to(device)\n\n        model_results = manify.utils.benchmarks.benchmark(\n            X, y, pm, task=TASK, score=SCORE, seed=seed, device=\"cpu\"\n        )\n\n        # Create a flat dictionary for this run\n        model_results[\"curvature\"] = K\n        model_results[\"seed\"] = seed\n\n        # results.append(run_results)\n        results.append(model_results)\n        my_tqdm.update(1)\n\n# Convert to DataFrame\nresults = pd.DataFrame(results)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Decision Tree Splits Level by Level in Python\nDESCRIPTION: Generates a grid of plots using `matplotlib` and `numpy` to visualize the splits performed by the `ProductSpaceDT`. It iterates through the `levels` computed previously (excluding the last level containing leaf nodes). For each internal node in the tree, it creates a subplot: points not reaching the node are plotted in grey, points reaching the node (`mask`) are plotted colored by class `y` projected onto the dimensions (`dim1`, `dim2`) used for the split at that node. A red line representing the angular decision boundary (`node.theta`) is drawn. Axes limits are set to the data range, and the title indicates the dimensions used and the threshold angle. The entire figure is saved as 'decision_tree_visualization.pdf'.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_pc_basic_visualization.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Plot\nimport numpy as np\n\nrows = len(levels) - 1  # Since final levels are homogeneous\ncols = max(len(l) for l in levels[:-1])\nfig, axs = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3))\n\nfor i, level in enumerate(levels[:-1]):\n    # for j, (node, mask) in enumerate(level):\n    for j in range(cols):\n        if j >= len(level):\n            print(\"axis off\")\n            axs[i, j].axis(\"off\")\n            continue\n        node, mask = level[j]\n\n        if node.feature is None:\n            continue\n\n        # Plot points\n        dim1, dim2 = pdt.angle_dims[node.feature]\n        if dim1 is not None:\n            axs[i, j].scatter(X[~mask, dim1].numpy(), X[~mask, dim2].numpy(), c=\"grey\", alpha=0.1)\n            axs[i, j].scatter(X[mask, dim1].numpy(), X[mask, dim2].numpy(), c=y[mask])\n        else:\n            axs[i, j].scatter(np.ones((~mask).sum()), X[~mask, dim2].numpy(), c=\"grey\", alpha=0.1)\n            axs[i, j].scatter(np.ones(mask.sum()), X[mask, dim2].numpy(), c=y[mask])\n\n        # Plot line\n        c = np.cos(node.theta)\n        s = np.sin(node.theta)\n        axs[i, j].plot([-s * 1e10, s * 1e10], [-c * 1e10, c * 1e10], \"r-\")\n\n        # Put boundaries back\n        axs[i, j].set_xlim(X[:, dim1].min(), X[:, dim1].max())\n        axs[i, j].set_ylim(X[:, dim2].min(), X[:, dim2].max())\n\n        # Title is split\n        axs[i, j].set_title(f\"({dim1}, {dim2}), $\\\\theta$={node.theta:.2f}\")\n\nplt.tight_layout()\nplt.suptitle(\"Visualized decision tree\")\nplt.savefig(\"decision_tree_visualization.pdf\")\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Setting Device in Python\nDESCRIPTION: Imports the PyTorch library (`torch`) and the `manify` library. It also configures the computation device (`DEVICE`) to use the second GPU (index 1) if available, falling back to CPU otherwise. This setup is essential for leveraging GPU acceleration in subsequent tensor operations and model training.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/51_gcn_link_prediction.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport manify\n\nDEVICE = torch.device(\"cuda\", 1)  # Use the 2nd GPU\n```\n\n----------------------------------------\n\nTITLE: Troubleshooting Regression with Manifold MLP - Python\nDESCRIPTION: This snippet demonstrates loading a graph-based regression task, retrieving node embeddings from disk, initializing a regression MLP on a hyperbolic manifold, splitting data into train/test, fitting the model, predicting, and computing the RMSE as a regression quality measure. It is intended for debugging and verifying regression hyperparameters. Dependencies: embedders, numpy, torch, scikit-learn. Input: embedding file, target variable; output: RMSE metric.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/43_benchmark_neural.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n\nimport embedders\nimport numpy as np\nimport torch \n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\ndevice=\"cpu\"\n\n# Load CS-PhDs\n_, y, adj = embedders.dataloaders.load(\"cs_phds\")\n\n# Get embeddings\n    # (\"cs_phds\", \"H\", [(-1, 4)], \"regression\"\n\nX = torch.tensor(np.load(\"embedders/data/graphs/embeddings/cs_phds/H_0.npy\"), device=device)\n\npm = embedders.manifolds.ProductManifold(signature=[(-1, 4)], device=device)\nmlp = embedders.predictors.mlp.MLP(pm=pm, input_dim=5, hidden_dims=[128, 128], output_dim=1, task=\"regression\")\nmlp = mlp.to(device)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nmlp.fit(X_train, y_train, epochs=1_000)\n\ny_pred = mlp.predict(X_test)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\nrmse # Ok at 62.6 I'm satisfied with my hyperparameters\n```\n\n----------------------------------------\n\nTITLE: Initializing and Generating Gaussian Mixture Data with Manify in Python\nDESCRIPTION: This snippet constructs a ProductManifold using specific signature tuples and generates a dataset using a Gaussian mixture model. It requires the 'manify' Python package. The 'signature' argument controls the feature structure and curvature of each submanifold, and the 'gaussian_mixture' method is called with an integer specifying number of samples. The function outputs a tuple (X, y) containing data and labels suitable for downstream machine learning tasks.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/56_single_manifold_rf.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npm = manify.manifolds.ProductManifold(signature=[(1, 2), (1, 2), (0, 2), (-1, 2), (-1, 2)])\n\nX, y = pm.gaussian_mixture(1000)\n```\n\n----------------------------------------\n\nTITLE: Training Multiclass Mixed-Curvature Perceptron\nDESCRIPTION: This snippet demonstrates training a mixed-curvature perceptron for a multiclass classification problem. It first generates new synthetic data with `num_classes=2` (although the labels might represent more than two distinct classes depending on `gaussian_mixture`'s behavior). It prepares the data and then trains a `mix_curv_perceptron` with the `multiclass=True` flag. The result of `process_data()` is printed.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# What about multiclass?\n\n# Embedders part: generate data\n\npm = embedders.manifolds.ProductManifold([(-1, 2), (0, 2), (1, 2)])\nX, y = embedders.gaussian_mixture.gaussian_mixture(pm, num_classes=2)\n\nmix_component = get_signature_str(pm)\nembed_data = get_embed_data(pm, X, y)\n# Convert to (1, -1) labels:\n\nps_perceptron = mix_curv_perceptron(\n    mix_component=mix_component, embed_data=embed_data, multiclass=True, max_round=10_000, max_update=10_000\n)\ny_pred = ps_perceptron.process_data()\nprint(y_pred)\n```\n\n----------------------------------------\n\nTITLE: Formulating and Solving SVM QP with CVXPY (Python)\nDESCRIPTION: Constructs an SVM problem using a custom kernel and various class/geometry constraints depending on the manifold components. Gathers kernels, converts data to NumPy, sets up variables and constraints with cvxpy and torch, and solves the QP to obtain Lagrange multipliers and thresholds. Inputs: pm (ProductManifold), X, y; outputs: optimized SVM parameters. Dependencies: embedders, torch, cvxpy.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nfrom embedders.manifolds import ProductManifold\nfrom embedders.gaussian_mixture import gaussian_mixture\nfrom embedders.kernel import product_kernel\nimport torch\nimport cvxpy\n\n# Get pm and sample\npm = ProductManifold(signature=[(-1, 2), (0, 2), (1, 2)])\nX, y = gaussian_mixture(pm)\n\n# Make y in {-1, 1}\n# TODO: Do we need this?\n# y = 2 * y - 1\n\n# Get kernel\nn_samples = X.shape[0]\nKs, norms = product_kernel(pm, X, X)\nK = torch.zeros(n_samples, n_samples)\n# K = torch.ones((n_samples, n_samples))\nfor K_component in Ks:\n    K += K_component\n\n# Make numpy arrays\nX = X.detach().cpu().numpy()\nY = torch.diagflat(y).detach().cpu().numpy()\nK = K.detach().cpu().numpy()\n\n# Make variables\nzeta = cvxpy.Variable(X.shape[0])\nbeta = cvxpy.Variable(X.shape[0])\nepsilon = cvxpy.Variable(1)\n\n# Get constraints\nconstraints = [\n    epsilon >= 0,\n    zeta >= 0,\n    Y @ (K @ beta + cvxpy.sum(beta)) >= epsilon - zeta,\n    # Y @ (K @ beta) >= epsilon - zeta,  # Replaced with sum-less form\n]\nfor M, K_component, norm in zip(pm.P, Ks, norms):\n    K_component = K_component.detach().cpu().numpy()\n    norm = norm.item()\n    if M.type == \"E\":\n        alpha_E = 1.0  # TODO: make this flexible\n        # constraints.append(cvxpy.quad_form(beta, K_component) <= alpha_E**2)\n    elif M.type == \"S\":\n        pass\n        # constraints.append(cvxpy.quad_form(beta, K_component) <= torch.pi / 2)\n    elif M.type == \"H\":\n        pass  # No constraints currently\n        # K_component_pos = K_component.clip(0, None)\n        # K_component_neg = K_component.clip(None, 0)\n        # constraints.append(cvxpy.quad_form(beta, K_component_neg) <= 1e-5)\n        # constraints.append(cvxpy.quad_form(beta, K_component_pos) <= 1e-5 + norm)\n\n# CVXPY solver\ncvxpy.Problem(\n    objective=cvxpy.Minimize(-epsilon + cvxpy.sum(zeta)),\n    constraints=constraints,\n).solve()\nprint(zeta.value, beta.value, epsilon.value)\n```\n\n----------------------------------------\n\nTITLE: Generating and Visualizing Gaussian Mixture Data on a Product Manifold in Python\nDESCRIPTION: Initializes a `ProductManifold` using the `embedders` library. Generates synthetic data (`X`, `y`) from a Gaussian mixture model defined on this manifold. The data tensors (`X`, `y`) are converted to NumPy arrays for visualization, and a scatter plot is created using `matplotlib` to show the initial data distribution colored by class/value.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/17_verify_new_mse.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npm = embedders.manifolds.ProductManifold(signature=[(0, 2)])\nX, y = embedders.gaussian_mixture.gaussian_mixture(pm=pm, num_classes=4, cov_scale_points=1e-1)#, task=\"regression\")\n\nX_np = X.detach().cpu().numpy()\ny_np = y.detach().cpu().numpy()\n\nplt.scatter(X_np[:, 0], X_np[:, 1], c=y_np)\n```\n\n----------------------------------------\n\nTITLE: Processing Experiment Results and Formatting Performance Tables - Python\nDESCRIPTION: This code snippet reads experiment results from a TSV file into a pandas DataFrame, computes group-wise aggregated metrics with a 95% confidence interval, formats a comparison table between various models, selects specific empirical datasets, highlights the top performances using markdown styling, and prints the formatted table. Dependencies include pandas, numpy, and a custom function format_top_performances_markdown. Inputs: path to a results TSV; outputs: formatted table in markdown. Adjustable parameters include selected datasets, models, and aggregation metric.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/43_benchmark_neural.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# METRIC = \"rmse\"\nMETRIC = \"accuracy\"\n\ndef ci95_agg(vals):\n    return f\"{np.mean(vals):.2f} \\u00b1 {1.96 * np.std(vals):.2f}\"\n\n\nfor path, groupvar in zip(\n    [\n        # \"embedders/data/results/classification_nn_single_curvature.tsv\",\n        # \"embedders/data/results/classification_nn_multiple_curvatures.tsv\",\n        # \"embedders/data/results/regression_nn_single_curvature.tsv\",\n        # \"embedders/data/results/regression_nn_multiple_curvatures.tsv\",\n        # \"embedders/data/results/all_nn_graph.tsv\"\n        # \"embedders/data/results/all_nn_vae.tsv\"\n        \"../data/results/all_nn_empirical.tsv\"\n        # \"embedders/data/results/all_nn_link.tsv\"\n    ],\n    [\n        # \"curvature\",\n        # \"signature\",\n        # \"curvature\",\n        # \"signature\",\n        # \"embedding\"\n        # \"embedding\"\n        \"dataset\"\n        # \"dataset\"\n    ],\n):\n    results = pd.read_csv(path, sep=\"\\t\")\n    cols = [\n        f\"{model}_{METRIC}\"\n        for model in [\"product_dt\", \"product_rf\", \"tangent_mlp\", \"ambient_mlp\", \"tangent_gnn\", \"ambient_gnn\"]\n    ]  # \"tangent_gnn_adj\", \"ambient_gnn_adj\"]]\n    tab = results.groupby(groupvar).agg(ci95_agg)[cols]\n    tab.columns = [\n        \"Product DT\",\n        \"Product RF\",\n        \"Tangent MLP\",\n        \"Ambient MLP\",\n        \"Tangent GNN\",\n        \"Ambient GNN\",\n    ]  # , \"Tangent GNN (Adj)\", \"Ambient GNN (Adj)\"]\n\n    # Bold the best column(s) for each row at .2f precision\n    # Note that these are strings currently with \\u00b1, so we need to split and convert to float\n    # Use markdown: **x** to bold and <u>x</u> to underline\n    # best = tab.apply(lambda x: np.array([float(val.split(\" \\u00b1 \")[0]) for val in x.values])).idxmax(axis=1)\n    # tab = tab.applymap(lambda x: f\"**{x}**\" if x.split(\" \\u00b1 \")[0] == tab.loc[best[x], x].split(\" \\u00b1 \")[0] else x)\n    # second_best = tab.apply(lambda x: np.array([float(val.split(\" \\u00b1 \")[0]) for val in x.values if val != tab.loc[best[x], x]]), axis=1).nlargest(2).index\n    # tab = tab.applymap(lambda x: f\"<u>{x}</u>\" if x.split(\" \\u00b1 \")[0] == tab.loc[best[x], x].split(\" \\u00b1 \")[0] else x)\n    # display(tab)\n    # print(tab.to_latex())\n    # print(tab.to_markdown())\n\n    # For graphs\n    # tab = tab.loc[[\"citeseer\", \"cora\", \"polblogs\"]]\n    # tab = tab.loc[[\"cs_phds\"]\n    # tab = tab.loc[[\"adjnoun\", \"dolphins\", \"football\", \"karate_club\", \"lesmis\", \"polbooks\"]]\n\n    # For empirical\n    tab = tab.loc[[\"landmasses\", \"neuron_33\", \"neuron_46\"]]\n    # tab = tab.loc[[\"temperature\", \"traffic\"]]\n\n    styled_tab = format_top_performances_markdown(tab, metric=METRIC)\n    # display(styled_tab)\n    print(styled_tab.to_markdown())\n```\n\n----------------------------------------\n\nTITLE: Classifier Pipeline with Manifold Data (Python)\nDESCRIPTION: This snippet validates end-to-end model training and prediction pipelines using various manifold-aware classifiers (KappaGCN, ProductSpacePerceptron, ProductSpaceSVM) on Gaussian mixture data generated by a mixed-geometry product manifold. It splits data, trains, checks predictions, class probabilities, and accuracy using torch and sklearn. Inputs are generated X, y; outputs are asserts for prediction and accuracy sanity checks. All models require torch, sklearn, and manify.predictors submodules. Limitations: DT and RF commented, test is for basic classifier logic.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/60_pytest_scratch.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\nfrom sklearn.model_selection import train_test_split\n\nfrom manify.predictors.decision_tree import ProductSpaceDT, ProductSpaceRF\nfrom manify.predictors.kappa_gcn import KappaGCN\nfrom manify.predictors.perceptron import ProductSpacePerceptron\nfrom manify.predictors.svm import ProductSpaceSVM\nfrom manify.manifolds import ProductManifold\n\n\nprint(\"Testing basic classifier functionality\")\npm = ProductManifold(signature=[(-1, 2), (0, 2), (1, 2)])\nX, y = pm.gaussian_mixture(num_points=100, num_classes=2, seed=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Init models\nfor model_class, args in [\n    # ProductSpaceDT,\n    # ProductSpaceRF,\n    (KappaGCN, {\"output_dim\": 2, \"hidden_dims\": [pm.dim, pm.dim]}),\n    (ProductSpacePerceptron, {}),\n    (ProductSpaceSVM, {}),\n]:\n    print(f\"  Testing class: {model_class.__name__} \")\n    model = model_class(pm=pm, **args)\n    model.fit(X_train, y_train)\n\n    # Predictions\n    preds = model.predict(X_test)\n    assert preds.shape[0] == X_test.shape[0], \"Predictions should match the number of test samples\"\n    assert preds.ndim == 1, \"Predictions should be a 1D array\"\n\n    # Probabilities\n    probs = model.predict_proba(X_test)\n    assert probs.shape == (X_test.shape[0], 2)\n    assert probs.ndim == 2, \"Probabilities should be a 2D array\"\n    assert torch.argmax(probs, dim=1) == preds\n\n    # Accuracies\n    accuracy = model.score(X_test, y_test)\n    assert torch.isclose(accuracy, (preds == y_test).float().mean(), atol=1e-5), \"Accuracy calculation mismatch\"\n    assert accuracy >= 0.5, f\"Model {model_class.__name__} did not achieve sufficient accuracy\"\n\n```\n\n----------------------------------------\n\nTITLE: One-Hot Encode Tensor Data in PyTorch - Python\nDESCRIPTION: Demonstrates the use of torch.nn.functional.one_hot to convert label indices into one-hot encoded format. Requires torch and num_classes as an integer parameter. Accepts a tensor of integer class indices and outputs a tensor of shape (N, num_classes); note that indices out of range generate zeros.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/13_information_gain.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ntorch.nn.functional.one_hot(torch.tensor([0, 1, 20]), num_classes=2)\n```\n\n----------------------------------------\n\nTITLE: Summing Kernel Components to Form Final Kernel Matrix (Python)\nDESCRIPTION: Combines multiple kernel component matrices into a final kernel matrix by summing them, starting from a matrix of ones. Expects Ks as a list of kernel matrices and X as the dataset. Output is the aggregate kernel matrix K.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nK = torch.ones(X.shape[0], X.shape[0])\nfor mat in Ks:\n    K += mat\nK\n```\n\n----------------------------------------\n\nTITLE: Setting up Parameters for Benchmark Evaluation\nDESCRIPTION: This snippet prepares for the evaluation phase of the benchmark. It imports necessary libraries, sets evaluation parameters like the number of trials, maximum tree depth, the target dataset (Cora in this case), task type (classification/regression), and the evaluation metric (F1-micro/RMSE). It then loads the corresponding labels for the chosen dataset using 'embedders.dataloaders' and converts them to a PyTorch tensor.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/2_polblogs_benchmark.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Run benchmarks\nimport torch\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\n# Params\nTRIALS = 10\nMAX_DEPTH = 3\n# DATASET = \"polblogs\"\n# DATASET = \"cs_phds\"\nDATASET = \"cora\"\n\nTASK = \"regression\" if DATASET == \"cs_phds\" else \"classification\"\nSCORE = \"f1-micro\" if TASK == \"classification\" else \"rmse\"\n\n# Get data\n_, y = embedders.dataloaders.load(\n    DATASET,\n    labels=True,\n    # polblogs_path=\"/Users/[REDACTED]/embedders/data/graphs/polblogs.mtx\",\n    # polblogs_labels_path=\"/Users/[REDACTED]/embedders/data/graphs/polblogs_labels.tsv\",\n    # cs_phds_path=\"/Users/[REDACTED]/embedders/data/graphs/cs_phds.txt\",\n)\n# y = torch.tensor(y.values)\ny = torch.tensor(y)\n```\n\n----------------------------------------\n\nTITLE: Defining and Training a Vanilla Graph Convolutional Network (GCN) with PyTorch - Python\nDESCRIPTION: This code defines a custom GCN layer and model using PyTorch, trains it on the generated dataset, and evaluates its accuracy. It requires torch and the data/model training functions from previous snippets. VanillaGCNLayer applies a linear transformation with an adjacency matrix, while VanillaGCN adds a ReLU activation and additional linear layer. A_hat is set to the identity matrix, simulating a graph where each node is self-connected. Inputs include feature and label tensors; outputs include trained model and printed accuracy. The code emphasizes manual GCN construction and its integration with the shared training workflow.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/54_kappa_gcn_benchmark.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass VanillaGCNLayer(torch.nn.Module):\\n    def __init__(self, in_features, out_features):\\n        super().__init__()\\n        self.W = torch.nn.Parameter(torch.randn(in_features, out_features))\\n\\n    def forward(self, X, A_hat=None):\\n        return A_hat @ X @ self.W\\n\\n\\nclass VanillaGCN(torch.nn.Module):\\n    def __init__(self, in_features, out_features):\\n        super().__init__()\\n        self.layer1 = VanillaGCNLayer(in_features, out_features)\\n        self.layer2 = torch.nn.ReLU()\\n        self.layer3 = torch.nn.Linear(out_features, out_features)\\n\\n    def forward(self, X, A_hat=None):\\n        return self.layer3(self.layer2(self.layer1(X, A_hat)))\\n\\n\\nvgcn = VanillaGCN(2, 4)\\ntrain_model(vgcn, X_train, y_train, A_hat=torch.eye(X_train.shape[0]))\\nvgcn_accuracy = evaluate_model(vgcn, X_test, y_test, A_hat=torch.eye(X_test.shape[0]))\\nprint(f\"VanillaGCN accuracy: {vgcn_accuracy}\")\n```\n\n----------------------------------------\n\nTITLE: Defining Circular Comparison and Information Gain Calculation\nDESCRIPTION: Defines two helper functions: `circular_greater` checks if angles fall within a specified semi-circular range, handling wraparound at 2*pi. `calculate_info_gain` computes the information gain (based on Gini impurity) for potential splits in a decision tree, using the `circular_greater` function to partition data based on angle thresholds. It operates on angle values and corresponding labels.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/10_torchified_hyperdt.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef circular_greater(angles, threshold):\n    \"\"\"\n    Check if angles are in the half-circle (threshold, threshold + pi)\n    \"\"\"\n    return ((angles - threshold + torch.pi) % (2 * torch.pi)) - torch.pi > 0\n\n\ndef calculate_info_gain(values, labels):\n    batch_size, n_dim = values.shape\n    n_classes = labels.max().item() + 1\n\n    # Calculate total Gini impurity without bincount\n    label_one_hot = torch.nn.functional.one_hot(labels, n_classes).float()\n    class_probs = label_one_hot.mean(dim=0)\n    total_gini = 1 - (class_probs**2).sum()\n\n    # Initialize arrays for left and right counts\n    left_counts = torch.zeros((batch_size, n_dim, n_classes), device=values.device)\n    right_counts = torch.zeros((batch_size, n_dim, n_classes), device=values.device)\n\n    # Calculate left and right counts for each potential split\n    for i in range(batch_size):\n        mask = circular_greater(values, values[i].unsqueeze(0))\n        for j in range(n_dim):\n            left_mask = ~mask[:, j]\n            right_mask = mask[:, j]\n            left_counts[i, j] = label_one_hot[left_mask].sum(dim=0)\n            right_counts[i, j] = label_one_hot[right_mask].sum(dim=0)\n\n    # Calculate Gini impurities for left and right partitions\n    left_total = left_counts.sum(dim=-1, keepdim=True).clamp(min=1)\n    right_total = right_counts.sum(dim=-1, keepdim=True).clamp(min=1)\n    left_gini = 1 - ((left_counts / left_total) ** 2).sum(dim=-1)\n    right_gini = 1 - ((right_counts / right_total) ** 2).sum(dim=-1)\n\n    # Calculate weighted Gini impurity\n    left_weight = left_total.squeeze(-1) / batch_size\n    right_weight = right_total.squeeze(-1) / batch_size\n    weighted_gini = left_weight * left_gini + right_weight * right_gini\n\n    # Calculate information gain\n    info_gain = total_gini - weighted_gini\n\n    return info_gain\n\n\nig = calculate_info_gain(angle_vals, y)\n\nig\n```\n\n----------------------------------------\n\nTITLE: Traversing ProductSpaceDT and Calculating Node Masks in Python\nDESCRIPTION: Performs a breadth-first traversal of the trained `ProductSpaceDT` (`pdt`). It preprocesses the data `X` to get angles (`angles`) relevant for the tree's splits. For each node, it calculates a boolean mask indicating which data points reach that node. The traversal uses `torch` tensors and the internal `_angular_greater` function for comparisons based on node thresholds (`node.theta`) and features (`node.feature`). It stores the nodes and their corresponding masks level by level in the `levels` list and prints the number of nodes at each level.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_basic_visualization.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# This code will traverse the decision tree level by level and do the following:\n# 1. Store a mask for whether data points are \"live\" or not, i.e. whether they were thrown out in previous levels\n# 2. Get projection dimensions (pdt.angle_dims[feature])\n# 3. Get the threshold\n# 4. Store the (mask, dim1, dim2, threshold) tuple in a list\nimport torch\nfrom embedders.predictors.tree_new import _angular_greater\nimport matplotlib.pyplot as plt\n\nangles, _, _, comparisons = pdt._preprocess(X, y)\n\nlevels = []\nto_visit = [(pdt.nodes[0], torch.ones(X.shape[0]).bool())]\nwhile to_visit:\n    levels.append(to_visit)\n    new_to_visit = []\n    l = [\n        (node.left, mask & _angular_greater(angles[:, node.feature], torch.tensor(node.theta)).flatten())\n        for node, mask in to_visit\n        if node.left is not None\n    ]\n    r = [\n        (node.right, mask & ~_angular_greater(angles[:, node.feature], torch.tensor(node.theta)).flatten())\n        for node, mask in to_visit\n        if node.right is not None\n    ]\n    to_visit = l + r\n\nprint([len(l) for l in levels])\n```\n\n----------------------------------------\n\nTITLE: Comparing Torch and Non-Torch ProductSpaceDT Implementations\nDESCRIPTION: Compares the custom `TorchProductSpaceDT` with the standard `ProductSpaceDT` from the `hyperdt` library. Both models are trained on the first 500 data points. Predictions are made on the next 500 points. The code then calculates and prints the agreement rate between the two models' predictions, and the accuracy of each model against the true labels for the test set.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/10_torchified_hyperdt.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Confirm this is the same thing we get with the non-torch version\n# Hmm, not exactly. I wonder why...\n\nfrom hyperdt.product_space_DT import ProductSpaceDT\n\ntpsdt = TorchProductSpaceDT(signature=[(1, 2), (0, 2), (-1, 2), (-1, 2), (-1, 2)])\ntpsdt.fit(data[:500], classes[:500])\ny_torch = tpsdt.predict(data[500:1000])\n\npsdt = ProductSpaceDT(signature=[(2, 1), (2, 0), (2, -1), (2, -1), (2, -1)])\ndata_stacked = np.hstack([data[:, :3].cpu().numpy(), torch.ones(data.shape[0], 1), data[:, 3:].cpu().numpy()])\npsdt.fit(data_stacked[:500], classes[:500].cpu().numpy())\ny_numpy = psdt.predict(data_stacked[500:1000])\n\nprint((y_torch.cpu().numpy() == y_numpy).sum() / y_torch.shape[0])\nprint((y_torch.cpu().numpy() == classes[500:1000].cpu().numpy()).sum() / y_torch.shape[0])\nprint((y_numpy == classes[500:1000].cpu().numpy()).sum() / y_torch.shape[0])\n```\n\n----------------------------------------\n\nTITLE: Concatenating PyTorch Tensors in Python\nDESCRIPTION: This snippet concatenates a normal range tensor and a randomly generated (empty) tensor along their default dimension using 'torch.cat'. It highlights behavior when concatenating tensors with zero elements, an important edge case when preparing batches or merging variable-sized inputs. Dependencies include 'torch', and care must be taken that the tensors are compatible (same number of dimensions). The output will be equivalent to 'torch.arange(3)' since the second tensor is empty.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/53_gaussian_mixture_refactor.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntorch.cat([torch.arange(3), torch.randint(0, 1, (0,))])\n```\n\n----------------------------------------\n\nTITLE: Loading Data and Generating Manifold Embeddings using Manify in Python\nDESCRIPTION: Loads the Karate Club graph dataset using `manify.utils.dataloaders.load`, obtaining distance and adjacency matrices, then moves them to the configured `DEVICE`. It defines a `ProductManifold` with a specific signature (combinations of hyperbolic, Euclidean, and spherical 2D spaces). Finally, it trains coordinate embeddings (`X`) on this manifold using the distance matrix `D` via `manify.embedders.coordinate_learning.train_coords` and computes their stereographic projections (`pm_stereo`, `X_stereo`).\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/51_gcn_link_prediction.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Get some embeddings to work with\n\n# Get graph\nD, _, adj = manify.utils.dataloaders.load(\"karate_club\")\nD, adj = D.to(DEVICE), adj.to(DEVICE)\npm = manify.manifolds.ProductManifold(signature=[(-1, 2), (0, 2), (1, 2)], device=DEVICE)\n\n# Get embeddings\nX, _ = manify.embedders.coordinate_learning.train_coords(\n    pm=pm, dists=D, device=DEVICE, burn_in_iterations=400, training_iterations=1600\n)\n\n# Get stereographic version\npm_stereo, X_stereo = pm.stereographic(X)\n```\n\n----------------------------------------\n\nTITLE: Implementing a Tangent Plane MLP Without Bias (PyTorch, Python)\nDESCRIPTION: Implements a minimal single-layer neural network for classification, omitting bias terms, designed for use on (likely tangent-mapped) manifold data. Model is trained for 10,000 epochs using Adam optimizer and cross-entropy loss, and then evaluated on the test set for accuracy. Requires PyTorch and assignment of X_train/X_test and y_train/y_test to the appropriate tensors.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/41_more_benchmarks.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Tangent plane MLP with no bias\\nimport torch\\nimport torch.nn as nn\\n\\nnet = nn.Sequential(\\n    nn.Linear(6, 8, bias=False),\\n)\\n\\nopt = torch.optim.Adam(net.parameters(), lr=0.01)\\nloss_fn = nn.CrossEntropyLoss()\\n\\nfor i in range(10_000):\\n    opt.zero_grad()\\n    y_pred = net(torch.tensor(X_train).float())\\n    loss = loss_fn(y_pred, y_train)\\n    loss.backward()\\n    opt.step()\\n    if i % 1_000 == 0:\\n        print(loss.item())\\n\\ny_pred = net(torch.tensor(X_test).float())\\nprint(\"NN acc:\", (y_pred.argmax(1) == y_test).float().mean().item())\n```\n\n----------------------------------------\n\nTITLE: Splitting Dataset for Paired Node Tasks (with Consistent Splits) - Python\nDESCRIPTION: Defines a function that splits a dataset of node pairs for tasks such as link prediction, ensuring that training and test sets consist of distinct sets of nodes. Utilizes scikit-learn's train_test_split for selecting node indices, reshapes and partitions the data, and returns flattened train/test splits. Dependencies: NumPy/PyTorch (for .view), scikit-learn; expects X and y as pairwise adjacency matrices and supports custom split arguments.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/43_benchmark_neural.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Special function to split dataset while ensuring pairs are in the same split\nfrom sklearn.model_selection import train_test_split\n\n\ndef split_dataset(X, y, **kwargs):\n    n_pairs, n_dims = X.shape\n    n_nodes = int(n_pairs**0.5)\n\n    # Reshape\n    X_reshaped = X.view(n_nodes, n_nodes, -1)\n    y_reshaped = y.view(n_nodes, n_nodes)\n\n    # Take 20% Of the nodes as test nodes\n    idx = list(range(n_nodes))\n    idx_train, idx_test = train_test_split(idx, **kwargs)\n\n    # Return test and train sets\n    X_train = X_reshaped[idx_train][:, idx_train].reshape(-1, n_dims)\n    y_train = y_reshaped[idx_train][:, idx_train].reshape(-1)\n\n    X_test = X_reshaped[idx_test][:, idx_test].reshape(-1, n_dims)\n    y_test = y_reshaped[idx_test][:, idx_test].reshape(-1)\n\n    return X_train, X_test, y_train, y_test\n\n```\n\n----------------------------------------\n\nTITLE: Training Coordinates on Product Manifold with Embedders in Python\nDESCRIPTION: This snippet illustrates importing specific submodules from the embedders package, creating a ProductManifold with a specified signature, generating Gaussian mixture data, calculating pairwise distances, and applying coordinate learning to those distances. Dependencies: embedders, including its manifolds, coordinate_learning, and gaussian_mixture submodules. The key variables include pm (the product manifold), X and y (the synthetic data and labels), dists (distance matrix), and pm2 (a new product manifold for training). Inputs are the manifold signature and optionally data, while outputs are distance matrices and trained manifold coordinates. The code expects proper installation and environment configuration for embedders.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/39_reembed_gaussian.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport embedders.coordinate_learning\nimport embedders.gaussian_mixture\n\npm = embedders.manifolds.ProductManifold(signature=[(2, 2)])\nX, y = embedders.gaussian_mixture.gaussian_mixture(pm)\ndists = pm.pdist(X).detach()\n\npm2 = embedders.manifolds.ProductManifold(signature=[(2, 2)])\nembedders.coordinate_learning.train_coords(pm2, dists)\n```\n\n----------------------------------------\n\nTITLE: Recursively Fitting Tree Nodes with No Autograd in PyTorch - Python\nDESCRIPTION: Defines the fit_node function decorated with @torch.no_grad(), ensuring operations do not track gradients. Recursively creates a tree node by choosing an optimal split, splitting data via get_split, and assembling child nodes. Relies on preprocess, get_info_gains, get_best_split and requires torch, input data in suitable tensor formats, and list of angles and product manifolds.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/13_information_gain.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# Now we can write the recursion pretty easily\n\nangles, labels_onehot, comparisons_reshaped = preprocess(X, y)\n\n\n# Wrapping just the outermost function in the @torch.no_grad() decorator\n# prevents the entire function from being tracked by autograd! Clean.\n@torch.no_grad()\ndef fit_node(comparisons, labels, depth, min_n=1):\n    # Take a decrement approach to depth for simplicity\n    if depth == 0:\n        return {\"class\": labels.sum(dim=0).argmax().item()}\n\n    # Edge cases where there are no more points to split\n    if len(comparisons) < min_n:\n        return {\"class\": labels.sum(dim=0).argmax().item()}\n\n    # The main loop is just the functions we've already defined\n    ig = get_info_gains(comparisons, labels)\n    n, d, theta = get_best_split(ig, angles, comparisons, pm)\n    comparisons_neg, labels_neg, comparisons_pos, labels_pos = get_split(comparisons, labels, n, d)\n    node = {\n        \"d\": d.item(),\n        \"theta\": theta.item(),\n        \"left\": fit_node(comparisons_neg, labels_neg, depth - 1),\n        \"right\": fit_node(comparisons_pos, labels_pos, depth - 1),\n    }\n    return node\n\n\nfit_node(comparisons_reshaped, labels_onehot, 2)\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating Standard Decision Tree Classifiers using scikit-learn\nDESCRIPTION: Splits the data again (though could reuse the previous split). It trains a standard `sklearn.tree.DecisionTreeClassifier` with `max_depth=3` for each label set, using the same training data as the `ProductSpaceDT` models. It evaluates and prints the accuracy of each scikit-learn tree on the test set.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/40_component_interp.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Try classification using each set of labels as a target\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n# pm = embedders.manifolds.ProductManifold(signature=[sig for sublist in SIGS for sig in sublist])\ntrees = []\nfor i in range(len(SIGS)):\n    # pdt = embedders.tree_new.ProductSpaceDT(pm=pm, max_depth=3)\n    dt = DecisionTreeClassifier(max_depth=3)\n\n    # pdt.fit(X_train, y_train[:, i])\n    dt.fit(X_train, y_train[:, i])\n\n    print(f\"Accuracy for {i}: {dt.score(X_test, y_test[:, i]):.3f}\")\n    trees.append(dt)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Hyperbolic and Standard Decision Trees on H^6 Embedding using Cross-Validation in Python\nDESCRIPTION: This snippet defines a cross-validation function 'cv_eval' using scikit-learn's KFold and mean squared error. It then instantiates and evaluates a HyperbolicDecisionTreeRegressor and a standard DecisionTreeRegressor on the previously computed H^6 embedding ('h6_cs_phds') and labels ('cs_labels'), printing the mean and standard deviation of the MSE across folds for each model.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/4_cs_phds_regression.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Compare productDT and sklearn on this dataset\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np # Added import based on usage\n\nfrom hyperdt.tree import HyperbolicDecisionTreeRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\n\ndef cv_eval(model, name, X, y):\n    # Use MSE this time\n    cv = []\n    kf = KFold(n_splits=5)\n    for train_index, test_index in kf.split(X):\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_test)\n        cv.append(mean_squared_error(y_test, y_pred))\n    print(f\"{name}\\t{np.mean(cv):.2f} +/- {np.std(cv):.2f}\")\n\n\nhdt = HyperbolicDecisionTreeRegressor(max_depth=3, skip_hyperboloid_check=True)\ncv_eval(hdt, \"HyperDT\", h6_cs_phds, np.array(cs_labels))\n\ndt = DecisionTreeRegressor(max_depth=3)\ncv_eval(dt, \"DT\", h6_cs_phds, np.array(cs_labels))\n```\n```\n\n----------------------------------------\n\nTITLE: Training Neural and Decision Tree Models on Manifold Data Using PyTorch and embedders (Python)\nDESCRIPTION: Defines a neural network classifier that leverages manifold tangent space mapping and evaluates its performance on synthetic data. A basic neural network (fc1, fc2, fc3) is trained with Adam optimizer and cross-entropy loss, with intermediate loss outputs. After training, test accuracy is computed. Additionally, a 'ProductSpaceDT' decision tree model from embedders is trained and evaluated for comparison. Requires PyTorch, scikit-learn, and embedders modules, as well as manifold-structured data (X, y).\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/41_more_benchmarks.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Classify with basic NN\\nimport torch\\nimport torch.nn as nn\\nfrom sklearn.model_selection import train_test_split\\n\\nimport embedders.tree_new\\n\\n\\nclass Net(nn.Module):\\n    def __init__(self, pm):\\n        super(Net, self).__init__()\\n        self.pm = pm\\n        self.fc1 = nn.Linear(6, 6)\\n        self.fc2 = nn.Linear(6, 6)\\n        self.fc3 = nn.Linear(6, 8)\\n\\n    def forward(self, x):\\n        # x = torch.relu(self.fc1(x))\\n        # x = self.fc2(x)\\n        # return x\\n        x = self.pm.logmap(x)\\n        x = torch.relu(self.fc1(x))\\n        # x = self.pm.expmap(x)\\n        # x = self.pm.logmap(x)\\n        x = torch.relu(self.fc2(x))\\n        x = self.fc3(x)\\n        return x\\n\\n\\nnet = Net(pm)\\nprint(net)\\n\\nopt = torch.optim.Adam(net.parameters(), lr=0.01)\\nloss_fn = nn.CrossEntropyLoss()\\n\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\\n\\nfor i in range(1_000):\\n    opt.zero_grad()\\n    y_pred = net(torch.tensor(X_train).float())\\n    loss = loss_fn(y_pred, torch.tensor(y_train).long())\\n    loss.backward()\\n    opt.step()\\n    if i % 100 == 0:\\n        print(loss.item())\\n\\ny_pred = net(torch.tensor(X_test).float())\\nprint(\"NN acc:\", (y_pred.argmax(1) == torch.tensor(y_test)).float().mean().item())\\n\\npdt = embedders.tree_new.ProductSpaceDT(pm)\\npdt.fit(X_train, y_train)\\nprint(\"DT acc:\", (pdt.predict(X_test) == y_test).float().mean().item())\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating KappaGCN for Link Prediction in Python\nDESCRIPTION: Trains a KappaGCN model for link prediction. It imports `train_test_split` from scikit-learn, initializes a `KappaGCN` model configured for link prediction using the stereographic manifold `pm_stereo`. It splits the flattened adjacency matrix indices into training and testing sets. The model is trained using `kgcn.fit` on the training indices, using the full embedding set `X` and the normalized adjacency matrix `A_hat`. Predictions are made on the test set, and the accuracy is computed.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/51_gcn_link_prediction.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import train_test_split\n\n# Train a KappaGCN on everything\nkgcn = embedders.predictors.kappa_gcn.KappaGCN(\n    pm=pm_stereo, output_dim=1, hidden_dims=[pm.dim, pm.dim], task=\"link_prediction\"\n).to(DEVICE)\n\n# Split on indices, since this is a weird graph thing\ny = adj.float().flatten()\ntrain_idx, test_idx = train_test_split(list(range(len(y))), test_size=0.2)\n\n# Fit on train indices using all X, but only some of the y - this masks out certain edges from the loss\nkgcn.fit(X, y=y[train_idx], A=A_hat, lr=1e-2, lp_indices=train_idx, use_tqdm=True, epochs=100)\n\n# Predict on test indices\ny_pred = kgcn.predict(X, A_hat)[test_idx]\n\n(y_pred == y[test_idx]).float().mean()\n```\n\n----------------------------------------\n\nTITLE: Generating Comparative Boxplots with Custom Styling (Matplotlib, Python)\nDESCRIPTION: This code block loops over provided axes and groups of models to generate boxplots comparing performance (e.g., RMSE) across curvature values, using Matplotlib in Python. Boxplot properties such as color and line width are customized per model for clarity. P-value annotations (statistical significance test) are optionally displayed between models using a user-defined significance threshold. The axes are further styled (spines, ticks, background) and x-axis labels/legends are positioned for optimal presentation. Requires NumPy, Matplotlib, and pre-prepared variables like models, results, CURVATURES, and color mappings. Inputs: metric data split by model and curvature; Outputs: a styled, annotated boxplot figure.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/26_benchmark_single_curvature_gaussian.ipynb#_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\nfor ax, models, model_names in zip(\n    [axs], [models2 + models3], [model_names2 + model_names3]\n):\n    bps = []\n    models = [f\"{model}_{SUFFIX}\" for model in models]\n    for i, (model, color) in enumerate(zip(models, colors)):\n        # Initial boxplot\n        bp = ax.boxplot(\n            [results[results[\"curvature\"] == K][model] for K in CURVATURES],\n            positions=x_vals + i,\n            widths=0.8,\n            boxprops=dict(color=color, linewidth=LW),\n        )\n\n        # Fix colors\n        for element in [\"boxes\", \"whiskers\", \"fliers\", \"means\", \"caps\"]:\n            plt.setp(bp[element], color=color, linewidth=LW)\n        plt.setp(bp[\"medians\"], color=\"black\", linewidth=LW)\n        plt.setp(bp[\"fliers\"], marker=\"o\", markersize=1, markeredgecolor=color, markeredgewidth=LW)\n\n        bps.append(bp)\n\n    # # Flip y-axis for RMSE\n    # if TASK == \"regression\":\n    #     ax.invert_yaxis()\n    ymin, ymax = ax.get_ylim()\n\n    # Add p-value annotations. All start at x_vals, and end at x_vals + i\n    heights = [0] * len(CURVATURES)  # How many annotations per curvature\n    # for i, j in [(0, 1), (1, 2), (0, 2), (0, 3), (1, 3), (2, 3)]:\n    for i in range(len(models)):\n        for j in range(i + 1, len(models)):\n            results_K = [results[results[\"curvature\"] == K] for K in CURVATURES]\n            p_vals = [test(res[models[i]], res[models[j]]) for res in results_K]\n            em = ymax - ymin\n\n            # for k, p_val in enumerate(p_vals):\n            #     if (SIGNIFICANCE == \"ns\" and p_val > CRITICAL_VAL) or (SIGNIFICANCE == \"sig\" and p_val < CRITICAL_VAL):\n            #         x1, x2 = x_vals[k] + i, x_vals[k] + j\n            #         height = results[results[\"curvature\"] == CURVATURES[k]][models].max().max() + AST_SPACING * em * (\n            #             heights[k] + 1\n            #         )\n            #         annotation = \"*\" if SIGNIFICANCE == \"sig\" else \"ns\"\n            #         ax.text(\n            #             s=annotation,\n            #             x=(x1 + x2) / 2,\n            #             y=height,\n            #             ha=\"center\",\n            #             va=\"center\",\n            #             color=\"black\",\n            #             fontdict={\"weight\": \"bold\", \"size\": FONTSIZE},\n            #         )\n            #         ax.plot(\n            #             [x1, x1, x2, x2],\n            #             [height - 0.03 * em, height - 0.02 * em, height - 0.02 * em, height - 0.03 * em],\n            #             lw=LW,\n            #             color=\"black\",\n            #         )\n            #         heights[k] += 1\n\n    # Fix y-lim and remove top/right spines; make background transparent\n    ax.patch.set_alpha(0)\n    ax.spines[\"top\"].set_visible(False)\n    ax.spines[\"right\"].set_visible(False)\n    ax.spines[\"left\"].set_linewidth(LW)\n    ax.spines[\"bottom\"].set_linewidth(LW)\n    ax.set_xlim(-1, len(CURVATURES) * (len(models1 + models3) + 1))\n\n    # Set x-ticks\n    ax.set_ylabel(Y_LABEL, fontsize=FONTSIZE)\n    ax.set_xticks(x_vals + 1.5, CURVATURES, fontsize=FONTSIZE)\n    # ax.legend([bp[\"boxes\"][0] for bp in bps], model_names, fontsize=14, frameon=False, title=\"Model\")\n    # Make legend to the right of the plot\n    # ax.legend(\n    #     [bp[\"boxes\"][0] for bp in bps],\n    #     model_names,\n    #     fontsize=16,\n    #     frameon=False,\n    #     title=\"Model\",\n    #     title_fontsize=18,\n    #     loc=\"center left\",\n    #     bbox_to_anchor=(1, 0.5),\n    # )\n    ax.tick_params(axis=\"y\", labelsize=FONTSIZE)\n    # ax.set_yscale(\"log\") if TASK == \"regression\" else None\n\n    # Move title down\n    # ax.title.set_position([.5, .9])\n\n```\n\n----------------------------------------\n\nTITLE: Predicting Labels and Checking Unique Classes\nDESCRIPTION: Uses the trained `TorchProductSpaceDT` model (`tpsdt`) to predict class labels for the input data (`data`). It then calls the `.unique()` method on the resulting tensor of predictions to identify the set of unique class labels assigned by the model.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/10_torchified_hyperdt.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntpsdt.predict(data).unique()\n```\n\n----------------------------------------\n\nTITLE: Predicting with Bias-Adjusted Perceptron\nDESCRIPTION: This code uses the custom `perceptron_predict` function to generate predictions from the perceptron (`ps_perceptron_0_bias`) that was trained on the class-0-biased dataset. It takes the argmax along the class axis to get the final predicted labels.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nperceptron_predict(ps_perceptron_0_bias).argmax(axis=1)\n```\n\n----------------------------------------\n\nTITLE: Initializing Autoreload in IPython/Jupyter\nDESCRIPTION: These IPython magic commands configure the environment to automatically reload imported Python modules before executing new code. `%load_ext autoreload` loads the necessary extension, and `%autoreload 2` sets the mode to reload all modules except those explicitly ignored.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/28_benchmark_link_prediction.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating KappaGCN on a Product Manifold (MLR-like) in Python\nDESCRIPTION: Defines, trains, and evaluates a `manify` KappaGCN model configured with a product manifold `(0, 1)x(0,1)` but *without* hidden dimensions. An identity matrix is passed as `A_hat`, making it behave like Multinomial Logistic Regression (KappaGCN-Product MLR). The accuracy is printed.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/relationship.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npm = manify.manifolds.ProductManifold([(0, 1),(0,1)], stereographic=True)\nkgcn = manify.predictors.kappa_gcn.KappaGCN(\n    pm=pm, output_dim=num_classes, hidden_dims=[], nonlinearity=torch.relu, task=\"classification\"\n)\nkgcn = train_model(kgcn, X_train, y_train,A_hat=torch.eye(X_train.shape[0]))\nkgcn_accuracy = evaluate_model(kgcn, X_test, y_test,A_hat=torch.eye(X_test.shape[0]))\nprint(f\"KappaGCN-Product MLR accuracy: {kgcn_accuracy}\")\n\n```\n\n----------------------------------------\n\nTITLE: Defining Utilities for LaTeX Table Generation in Python\nDESCRIPTION: This snippet imports NumPy and SciPy's `wilcoxon` (though Wilcoxon is not used in the visible code), and Pandas. It defines constants for statistical analysis (P-value, Bonferroni adjustment flag) and lists of predictor model names to compare (`group1`, `group2`). It includes a helper function `mean_se` to calculate mean, standard error, and count for a Pandas Series. Crucially, it defines a dictionary `latex_symbols` mapping predictor names to specific LaTeX formatted strings with symbols (intended for significance marking) and a function `format_latex_cell` to create a LaTeX table cell string displaying 'mean  SE', potentially applying bold/underline formatting, scaling classification scores by 100, and appending significance symbols.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/32_big_table.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom scipy.stats import wilcoxon\nimport pandas as pd\n\nUSE_BONFERRONI = True\nP_VAL = 0.05\nADJUSTMENT = 10 if USE_BONFERRONI else 1  # 5 choose 2\n\n\n# Function to calculate mean and standard error\ndef mean_se(series):\n    return series.mean(), series.sem(), series.count()\n\n\n# Define predictors to compare\ngroup1 = [\"product_dt\", \"sklearn_dt\", \"tangent_dt\", \"knn\", \"ps_perceptron\"]\ngroup2 = [\"product_rf\", \"sklearn_rf\", \"tangent_rf\", \"knn\", \"ps_perceptron\"]\n\n# Initialize LaTeX symbols for Wilcoxon test results\nlatex_symbols = {\n    \"product_dt\": \"\\\\col{product_dt}{*}\",\n    \"product_rf\": \"\\\\col{product_dt}{*}\",\n    \"sklearn_dt\": \"\\\\col{euclidean_dt}{}\",\n    \"sklearn_rf\": \"\\\\col{euclidean_dt}{}\",\n    \"tangent_dt\": \"\\\\col{tangent_dt}{}\",\n    \"tangent_rf\": \"\\\\col{tangent_dt}{}\",\n    \"knn\": \"\\\\col{knn}{}\",\n    \"ps_perceptron\": \"\\\\col{perceptron}{}\",\n}\n\n\n# Function to format a LaTeX cell with mean  SE and symbols\ndef format_latex_cell(mean, se, bold=False, underline=False, symbols=None, regression=False):\n    if regression:\n        # formatted = f\"{mean:.3f}\".lstrip(\"0\") + \" \\\\scriptsize  \" + f\"{se:.3f}\".lstrip(\"0\")\n        formatted = f\"{mean:.3f}\".lstrip(\"0\") + \"  \" + f\"{se:.3f}\".lstrip(\"0\")\n    else:\n        # formatted = f\"{mean * 100:.1f}\".lstrip(\"0\") + \"\\\\scriptsize  \" + f\"{se * 100:.1f}\".lstrip(\"0\")\n        formatted = f\"{mean * 100:.1f}\".lstrip(\"0\") + \"  \" + f\"{se * 100:.1f}\".lstrip(\"0\")\n    if bold:\n        formatted = f\"\\\\textbf{{{formatted}}}\"\n    if underline:\n        formatted = f\"\\\\underline{{{formatted}}}\"\n    if symbols:\n        # Remove duplicates\n        symbols = sorted(list(set(symbols)))\n        formatted += f\"\\\\textsuperscript{{{''.join(symbols)}}}\"\n    return formatted\n```\n\n----------------------------------------\n\nTITLE: Plotting Downsampled Time Series Data in Python\nDESCRIPTION: Extracts the time series data for 'Sweep_15' from the loaded NWB file, converts it to a numpy array, downsamples it by taking every 100th point, and plots the result using matplotlib.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/30_fourier.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nplt.plot(np.array(data[\"acquisition/timeseries/Sweep_15/data\"])[::100])\n```\n\n----------------------------------------\n\nTITLE: Enabling Autoreload in IPython with Python\nDESCRIPTION: This snippet enables the autoreload extension in an IPython environment, automatically reloading all modules before executing a new line. It requires that the user is running within an IPython-compatible shell (such as Jupyter Notebook). No arguments are required. This is used to speed up development by ensuring the latest code changes are always in effect; it does not impact final scripts or production code.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/56_single_manifold_rf.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating KappaGCN on a Negative Curvature Manifold (MLR-like) in Python\nDESCRIPTION: Defines, trains, and evaluates a `manify` KappaGCN model configured with a negative curvature manifold `(-1, 2)` but *without* hidden dimensions. An identity matrix `A_hat` is provided, making it behave like Multinomial Logistic Regression (KappaGCN-Kappa MLR). The accuracy is printed.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/relationship.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npm = manify.manifolds.ProductManifold([(-1, 2)], stereographic=True)\nkgcn = manify.predictors.kappa_gcn.KappaGCN(\n    pm=pm, output_dim=num_classes, hidden_dims=[], nonlinearity=torch.relu, task=\"classification\"\n)\nkgcn = train_model(kgcn, X_train, y_train,A_hat=torch.eye(X_train.shape[0]))\nkgcn_accuracy = evaluate_model(kgcn, X_test, y_test,A_hat=torch.eye(X_test.shape[0]))\nprint(f\"KappaGCN-Kappa MLR accuracy: {kgcn_accuracy}\")\n\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating ProductSpaceDT for Regression in Python\nDESCRIPTION: Instantiates a `ProductSpaceDT` model from the `embedders` library for a regression task on the previously defined product manifold (`pm`) with a maximum depth of 5. Trains the model using the generated data (`X`, `y`), calculates and prints its score (likely R-squared or similar metric), predicts target values (`y_pred_pdt`) for the input data, and visualizes the predictions with a scatter plot.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/17_verify_new_mse.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npdt = embedders.tree_new.ProductSpaceDT(pm=pm, task=\"regression\", max_depth=5)\npdt.fit(X, y)\n\nprint(pdt.score(X, y))\n\ny_pred_pdt = pdt.predict(X)\nplt.scatter(X_np[:, 0], X_np[:, 1], c=y_pred_pdt.detach().cpu().numpy())\n```\n\n----------------------------------------\n\nTITLE: Initializing and Training a Product Space Random Forest in Python\nDESCRIPTION: Tests the Random Forest (RF) implementation based on the new product space decision trees. It defines a product manifold (`pm`) with Hyperbolic, Euclidean, and Spherical components using `embedders.manifolds.ProductManifold`. It instantiates the `embedders.tree_new.ProductSpaceRF` with this manifold and 100 estimators. Synthetic data (`X`, `y`) is generated on this manifold using `embedders.gaussian_mixture`, and the Random Forest model (`prf`) is trained using the `fit` method.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/14_new_tree_testing.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# Does RF work?\n\n\npm = embedders.manifolds.ProductManifold(signature=[(-1, 3), (0, 3), (1, 3)])\nprf = embedders.tree_new.ProductSpaceRF(pm=pm, n_estimators=100)\n\nX, y = embedders.gaussian_mixture.gaussian_mixture(pm=pm)\nprf.fit(X, y)\n```\n\n----------------------------------------\n\nTITLE: Visualizing ProductSpaceDT Decision Boundaries Level by Level in Python\nDESCRIPTION: Uses `matplotlib` and `numpy` to create a grid of subplots visualizing the decision process of the trained `ProductSpaceDT`. It iterates through the `levels` list (containing nodes and data masks from the previous traversal step). For each node in each level (excluding the final leaf level), it creates a subplot showing the data points reaching that node (`X[mask]`), colored by class `y[mask]`. Data points filtered out by previous splits are shown in grey (`X[~mask]`). The relevant dimensions (`dim1`, `dim2`) determined by `pdt.angle_dims[node.feature]` are used for plotting. The decision boundary line (calculated using `node.theta`) is overlaid in red. Each subplot is titled with the dimension pair and threshold, and the entire figure is saved to a PDF.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_basic_visualization.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Plot\nimport numpy as np\n\nrows = len(levels) - 1  # Since final levels are homogeneous\ncols = max(len(l) for l in levels[:-1])\nfig, axs = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3))\n\nfor i, level in enumerate(levels[:-1]):\n    # for j, (node, mask) in enumerate(level):\n    for j in range(cols):\n        if j >= len(level):\n            print(\"axis off\")\n            axs[i, j].axis(\"off\")\n            continue\n        node, mask = level[j]\n\n        if node.feature is None:\n            continue\n\n        # Plot points\n        dim1, dim2 = pdt.angle_dims[node.feature]\n        if dim1 is not None:\n            axs[i, j].scatter(X[~mask, dim1].numpy(), X[~mask, dim2].numpy(), c=\"grey\", alpha=0.1)\n            axs[i, j].scatter(X[mask, dim1].numpy(), X[mask, dim2].numpy(), c=y[mask])\n        else:\n            axs[i, j].scatter(np.ones((~mask).sum()), X[~mask, dim2].numpy(), c=\"grey\", alpha=0.1)\n            axs[i, j].scatter(np.ones(mask.sum()), X[mask, dim2].numpy(), c=y[mask])\n\n        # Plot line\n        c = np.cos(node.theta)\n        s = np.sin(node.theta)\n        axs[i, j].plot([-s * 1e10, s * 1e10], [-c * 1e10, c * 1e10], \"r-\")\n\n        # Put boundaries back\n        axs[i, j].set_xlim(X[:, dim1].min(), X[:, dim1].max())\n        axs[i, j].set_ylim(X[:, dim2].min(), X[:, dim2].max())\n\n        # Title is split\n        axs[i, j].set_title(f\"({dim1}, {dim2}), $\\\\theta$={node.theta:.2f}\")\n\nplt.tight_layout()\nplt.suptitle(\"Visualized decision tree\")\nplt.savefig(\"decision_tree_visualization.pdf\")\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating ProductSpace Random Forest with Manify\nDESCRIPTION: Creates a ProductSpace Random Forest model (`ProductSpaceRF`) using the previously defined ProductManifold (`pm`) and data splits (`X_train`, `y_train`, `X_test`, `y_test`). The model is configured with a maximum depth and number of estimators. It trains the model on the training data and then evaluates its predictions on the test set by comparing them to the true test labels (`y_test`).\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/60_pytest_scratch.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport manify.predictors\n\n\nrf = manify.predictors.decision_tree.ProductSpaceRF(pm=pm, max_depth=3, n_estimators=10)\nrf.fit(X_train, y_train)\n\n(rf.predict(X_test) == y_test)\n```\n\n----------------------------------------\n\nTITLE: Training a Basic Tangent Plane Multilayer Perceptron in PyTorch (Python)\nDESCRIPTION: This snippet projects training data to the tangent space of a manifold using 'logmap', then feeds it to a sequential MLP with ReLU activations. The model is trained for 10,000 epochs with Adam optimizer and cross-entropy loss. Test data is also mapped to tangent space for evaluation. Dependencies: PyTorch, proper definition of pm, and tensor-formatted labels.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/41_more_benchmarks.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# super basic tangent plane mlp\\nimport torch\\nimport torch.nn as nn\\n\\nX_train_tangent = pm.logmap(X_train)\\n\\nnet = nn.Sequential(\\n    nn.Linear(6, 6),\\n    nn.ReLU(),\\n    nn.Linear(6, 6),\\n    nn.ReLU(),\\n    nn.Linear(6, 8),\\n)\\n\\nopt = torch.optim.Adam(net.parameters(), lr=0.01)\\nloss_fn = nn.CrossEntropyLoss()\\n\\nfor i in range(10_000):\\n    opt.zero_grad()\\n    y_pred = net(torch.tensor(X_train_tangent).float())\\n    loss = loss_fn(y_pred, y_train)\\n    loss.backward()\\n    opt.step()\\n    if i % 1_000 == 0:\\n        print(loss.item())\\n\\nX_test_tangent = pm.logmap(X_test)\\ny_pred = net(torch.tensor(X_test_tangent).float())\\nprint(\"NN acc:\", (y_pred.argmax(1) == y_test).float().mean().item())\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Link Prediction on Les Misrables Dataset (Python)\nDESCRIPTION: Performs link prediction benchmarking on the Les Misrables co-appearance network dataset over `N_TRIALS` repetitions. It loads the dataset, normalizes distances, and follows the identical benchmarking procedure as the Karate Club example: training embeddings, creating the link prediction dataset, running benchmarks, calculating distortion, aggregating results, printing statistics with significance tests, and saving the results to `../data/graph_benchmarks/lesmis_link.tsv`.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/22_link_prediction.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Karate club # <- Comment is inaccurate, should be Les Mis\n\ndists, labels, adj = embedders.dataloaders.load(\"lesmis\")\ndists = dists / dists.max()\n\nresults = []\nmy_tqdm = tqdm(total=N_TRIALS)\nwhile len(results) < N_TRIALS:\n    pm = ProductManifold(signature=SIGNATURE)\n    try:\n        X_embed, losses = train_coords(\n            pm,\n            dists,\n            burn_in_iterations=int(0.1 * TOTAL_ITERATIONS),\n            training_iterations=int(0.9 * TOTAL_ITERATIONS),\n            scale_factor_learning_rate=0.02,\n        )\n        assert not torch.isnan(X_embed).any()\n\n        X, y, pm_new = make_link_prediction_dataset(X_embed, pm, adj, add_dists=USE_DISTS)\n\n        res = embedders.benchmarks.benchmark(\n            X, y, pm_new, max_depth=MAX_DEPTH, task=\"classification\", use_special_dims=USE_SPECIAL_DIMS\n        )\n        res[\"d_avg\"] = embedders.metrics.d_avg(pm.pdist(X_embed), dists).item()\n        results.append(res)\n        my_tqdm.update(1)\n\n    except Exception as e:\n        print(e)\n        # print(f\"Failed iteration {len(results)}\")\n\n\n# Print results\nresults = pd.DataFrame(results)\nfor col in results.columns:\n    if col not in [\"model\", \"d_avg\"]:\n        r = results[col]\n        print(f\"{col}: {r.mean():.4f} +/- {r.std() / np.sqrt(N_TRIALS):.4f}\", end=\" \")\n\n        for col2 in results.columns:\n            if col2 not in [\"model\", col, \"d_avg\"]:\n                stat, p = wilcoxon(results[col], results[col2])\n                if p < 0.05 / 6 and results[col].mean() > results[col2].mean():\n                    print(f\"> {col2}\", end=\" \")\n\n        print()\nprint(f\"d_avg: {results['d_avg'].mean():.4f} +/- {results['d_avg'].std() / np.sqrt(N_TRIALS):.4f}\")\n\n# Save results\nresults.to_csv(\"../data/graph_benchmarks/lesmis_link.tsv\", index=False, sep=\"\\t\")\n```\n\n----------------------------------------\n\nTITLE: Formatting Performance Results with Markdown for Reporting - Python\nDESCRIPTION: Defines a utility function to post-process DataFrame performance metrics by applying markdown formatting (bold/underline) to highlight the best and second-best values. Accepts options for metric direction (accuracy/rmse) and tolerance for ties, and processes DataFrames with mean  std string arrangement. Requires pandas, supports row-wise formatting, and facilitates reporting in markdown-based documents.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/43_benchmark_neural.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef format_top_performances_markdown(df, metric=\"accuracy\", tolerance=0.01):\n    \"\"\"\n    Format DataFrame values with markdown bold and underline for best and second-best.\n    Returns a new DataFrame with markdown-formatted strings.\n\n    Args:\n        df: pandas DataFrame with mean  std format strings\n        metric: 'accuracy' or 'rmse' to determine if higher or lower is better\n        tolerance: threshold for considering values equal\n    \"\"\"\n    # Create a copy to modify\n    formatted_df = df.copy()\n\n    # Extract mean values into a numeric DataFrame\n    means = df.apply(lambda x: pd.to_numeric(x.str.split(\"\").str[0].str.strip()))\n\n    # Process each row\n    for idx in means.index:\n        row = means.loc[idx]\n\n        # Sort values and get indices based on metric\n        sorted_vals = row.sort_values(ascending=(metric == \"rmse\"))\n        best_val = sorted_vals.iloc[0]\n\n        # Find all values within tolerance of best\n        best_indices = row[abs(row - best_val) <= tolerance].index\n\n        if len(best_indices) > 1:\n            # Multiple bests - just bold them\n            for col in best_indices:\n                formatted_df.loc[idx, col] = f\"**{df.loc[idx, col]}**\"\n        else:\n            # One best - bold it and find second best\n            formatted_df.loc[idx, best_indices[0]] = f\"**{df.loc[idx, best_indices[0]]}**\"\n\n            # Find second best (first value not in best_indices)\n            for col in sorted_vals.index:\n                if col not in best_indices:\n                    formatted_df.loc[idx, col] = f\"_{df.loc[idx, col]}_\"\n                    break\n\n    return formatted_df\n\n```\n\n----------------------------------------\n\nTITLE: Saving Ablation Study Results to TSV in Python\nDESCRIPTION: Exports the pandas DataFrame containing the aggregated results from the ablation study to a tab-separated values (TSV) file. The filename is constructed dynamically using the `ABLATION_VAR` string variable, ensuring results from different ablation runs are saved separately. The `index=False` argument prevents pandas from writing the DataFrame index as a column, and `sep=\"\\t\"` specifies the tab delimiter.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/18_ablations.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nresults.to_csv(f\"results_{ABLATION_VAR}.tsv\", index=False, sep=\"\\t\")\n```\n\n----------------------------------------\n\nTITLE: Saving Classification Benchmark Results to TSV in Python\nDESCRIPTION: This snippet saves the node classification benchmark results stored in the Pandas DataFrame `results` to a tab-separated values (TSV) file. The file is named `graph_classification.tsv` and is saved in the `../data/results_icml/` directory. The DataFrame index is not included in the output file.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/24_benchmark_graph_embeddings.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nresults.to_csv(\"../data/results_icml/graph_classification.tsv\", sep=\"\\t\", index=False)\n```\n\n----------------------------------------\n\nTITLE: Enabling Autoreload Extension in Jupyter - Python\nDESCRIPTION: This snippet ensures that Python modules are automatically reloaded before executing cells in Jupyter notebooks, facilitating development by obviating the need to restart the kernel manually after code changes. It uses IPython magic commands to load and set the autoreload extension to mode 2, which reloads all modules every time before executing the code cell. No dependencies are needed beyond IPython/Jupyter environment support.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/8_scrna_preprocessing.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Running Regression Benchmarks for All Manifold Signatures in Python\nDESCRIPTION: This snippet configures and executes a machine learning experiment focused on regression, running across all defined signatures and seeds. It explicitly sets the computation `device` to 'cuda:1' (the second GPU). Similar to the previous experiment loop, it defines parameters, iterates through all combinations of signatures and seeds, generates synthetic regression data (X, y) using `embedders.gaussian_mixture` on the corresponding product manifold, runs benchmarks using `embedders.benchmarks.benchmark`, collects results, and stores them in a pandas DataFrame. The `TASK` variable is set to 'regression', and the loop structure covers all entries in `SIGNATURES` and the full range of `N_SAMPLES` seeds.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/27_benchmark_signature_gaussian.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndevice = torch.device(\"cuda\", 1)\n\nresults = []\n\n# Signatures - using non-Gu approach for now\nSIGNATURES = [\n    [(-1, 2), (-1, 2)],  # HH\n    [(-1, 2), (0, 2)],  # HE\n    [(-1, 2), (1, 2)],  # HS\n    [(1, 2), (1, 2)],  # SS\n    [(1, 2), (0, 2)],  # SE\n    [(-1, 4)],  # H\n    [(0, 4)],  # E\n    [(1, 4)],  # S\n]\n\nSIGNATURES_STR = [\"HH\", \"HE\", \"HS\", \"SS\", \"SE\", \"H\", \"E\", \"S\"]\n\nDIM = 4\nN_SAMPLES = 10\nN_POINTS = 1_000\n# N_POINTS = 100\nN_CLASSES = 8\nN_CLUSTERS = 32\n# MAX_DEPTH = None\nCOV_SCALE_MEANS = 1.0\nCOV_SCALE_POINTS = 1.0\n\nTASK = \"regression\"\n# TASK = \"classification\"\n# RESAMPLE_SCALES = False\n# N_FEATURES = \"d_choose_2\"\n\n# SCORE = \"f1-micro\" if TASK == \"classification\" else \"rmse\"\nSCORE = [\"f1-micro\", \"accuracy\"] if TASK == \"classification\" else [\"rmse\"]\n\nmy_tqdm = tqdm(total=len(SIGNATURES) * N_SAMPLES)\nfor i, (sig, sigstr) in enumerate(zip(SIGNATURES, SIGNATURES_STR)):\n    for seed in range(N_SAMPLES):\n        # Ensure unique seed per trial\n        seed = seed + N_SAMPLES * i\n        pm = embedders.manifolds.ProductManifold(signature=sig, device=sample_device)\n\n        # Get X, y\n        X, y = embedders.gaussian_mixture.gaussian_mixture(\n            pm=pm,\n            seed=seed,\n            num_points=N_POINTS,\n            num_classes=N_CLASSES,\n            num_clusters=N_CLUSTERS,\n            cov_scale_means=COV_SCALE_MEANS / DIM,\n            cov_scale_points=COV_SCALE_POINTS / DIM,\n            task=TASK,\n        )\n        X = X.to(device)\n        y = y.to(device)\n        pm = pm.to(device)\n\n        # if RESAMPLE_SCALES:\n        #     scale = 0.5 - np.random.rand() * 20\n        #     pm.P[0].scale = torch.exp(torch.tensor(scale)).item()\n        #     pm.P[0].manifold._log_scale = torch.nn.Parameter(torch.tensor(scale))\n\n        # Benchmarks are now handled by the benchmark function\n        model_results = embedders.benchmarks.benchmark(\n            # X, y, pm, max_depth=MAX_DEPTH, task=TASK, score=SCORE, seed=seed, n_features=N_FEATURES, device=device\n            X, y, pm, task=TASK, score=SCORE, seed=seed, device=device\n        )\n        \n        # # Create a flat dictionary for this run\n        # run_results = {\"signature\": sigstr, \"seed\": seed}\n\n        # # Flatten the nested model results\n        # for model, metrics in model_results.items():\n        #     for metric, value in metrics.items():\n        #         run_results[f\"{model}_{metric}\"] = value\n\n        # results.append(run_results)\n        model_results[\"signature\"] = sigstr\n        model_results[\"seed\"] = seed\n        results.append(model_results)\n        my_tqdm.update(1)\n\n\nresults = pd.DataFrame(results)\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Gaussian Mixture Data on Product Manifolds using embedders and PyTorch\nDESCRIPTION: Defines a list of manifold signatures (`SIGS`). For each signature, it creates a `ProductManifold`, generates synthetic data using a Gaussian mixture model specific to that manifold (`embedders.gaussian_mixture.gaussian_mixture`), and stores the embeddings (X) and labels (y). Finally, it horizontally stacks the embeddings and labels from all generated datasets using `torch.hstack`.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/40_component_interp.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nSIGS = [\n    [(1, 2)],\n    [(0, 2)],\n    [(-1, 2)]\n]\n\nembeddings = []\nlabels = []\n\nfor i, sig in enumerate(SIGS):\n    pm = embedders.manifolds.ProductManifold(sig)\n    X, y = embedders.gaussian_mixture.gaussian_mixture(pm=pm, seed=i)\n    embeddings.append(X)\n    labels.append(y.unsqueeze(1))\n\nX = torch.hstack(embeddings)\ny = torch.hstack(labels)\n\nprint(X.shape)\nprint(y.shape)\n```\n\n----------------------------------------\n\nTITLE: Saving Regression Benchmark Results to TSV in Python\nDESCRIPTION: This snippet saves the node regression benchmark results stored in the Pandas DataFrame `results2` to a tab-separated values (TSV) file. The file is named `graph_regression.tsv` and is saved in the `../data/results_icml/` directory. The DataFrame index is not included in the output file.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/24_benchmark_graph_embeddings.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nresults2.to_csv(\"../data/results_icml/graph_regression.tsv\", sep=\"\\t\", index=False)\n```\n\n----------------------------------------\n\nTITLE: Validating Product Manifold Pairwise Distance Methods with PyTorch - Python\nDESCRIPTION: Iterates over each manifold component in the product, computes pairwise squared distances with 'pdist2', and checks their shape and statistical properties. Asserts numerical consistency between squared and standard distance methods to a configured tolerance. Additionally, verifies individual distance calculations for all unique pairs. This requires the embedders manifold API and PyTorch for tensor checks, and serves as a robust unit test for manifold distance implementation fidelity.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/3_verify_shapes.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfor i, M in enumerate(pm.P):\n    dims = pm.man2dim[i]\n    pdists = M.pdist2(x_embed[:, dims])\n    print(f\"Manifold: {M.name},\\t Shape: {pdists.shape},\\t Min: {pdists.min()},\\t Max: {pdists.max()}\")\n\n    # Since I implemented these things a little differently, we should check that we can convert between distance types\n    assert torch.allclose(pdists.sqrt(), M.pdist(x_embed[:, dims]), atol=1e-4)\n\n    # Also we should check all entries\n    for j in range(pdists.shape[0]):\n        for k in range(j + 1, pdists.shape[1]):\n            assert torch.allclose(pdists[j, k], M.dist2(x_embed[j, dims], x_embed[k, dims]).sum(), atol=1e-6)\n```\n\n----------------------------------------\n\nTITLE: Benchmark Custom Embedding Model on Traffic Data\nDESCRIPTION: This snippet sets up and runs a benchmarking experiment using a custom `embedders` library. It defines parameters like the number of samples and trials. It initializes a `ProductManifold` based on the structure of the engineered features (junction ID + 4 pairs of sin/cos features). Data is converted to PyTorch tensors. The code then iterates through trials, randomly sampling data points, running the `embedders.benchmarks.benchmark` function (likely performing regression and calculating RMSE on a specific GPU), storing results, and finally organizing them into a pandas DataFrame.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/33_traffic.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport embedders\nfrom tqdm.notebook import tqdm\n\nN_SAMPLES = 1_000\nN_TRIALS = 10\n# N_TRIALS = 100\n# MAX_DEPTH = 3\n# N_FEATURES = \"d_choose_2\"\n\npm = embedders.manifolds.ProductManifold(signature=[(0, 1), (1, 1), (1, 1), (1, 1), (1, 1)])\nX_torch = torch.tensor(X, dtype=torch.float32)\ny_torch = torch.tensor(y, dtype=torch.float32)\n\nresults = []\nmy_tqdm = tqdm(total=N_TRIALS)\n# for trial in trange(N_TRIALS):\nfor trial in range(N_TRIALS):\n    print(trial)\n    idx = np.random.choice(len(X), N_SAMPLES, replace=False)\n    res = embedders.benchmarks.benchmark(\n        X_torch[idx], y_torch[idx], task=\"regression\", seed=trial, pm=pm, score=[\"rmse\"], device=\"cuda:1\"\n    )\n    res[\"trial\"] = trial\n    results.append(res)\n    my_tqdm.update(1)\n\nresults = pd.DataFrame(results)\nresults\n```\n\n----------------------------------------\n\nTITLE: Inspecting Shape of MNIST Data Array in Python\nDESCRIPTION: Outputs the shape of the array X, revealing the dataset's dimensionality. This is a diagnostic operation to confirm data loading and structure prior to any processing, and it expects X to be a numpy-array-like object supporting .shape.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/38_mnist.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nX.shape\n```\n\n----------------------------------------\n\nTITLE: Initializing Product Manifold and Embeddings with Embedders and PyTorch - Python\nDESCRIPTION: Defines a manifold 'signature', constructs a ProductManifold object, and initializes embeddings for a specified number of points (10). Requires the 'embedders' package with a 'manifolds.ProductManifold' class capable of handling the given signature, and PyTorch for tensor operations. The code outputs embedding tensors appropriate for the manifold configuration provided.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/3_verify_shapes.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsignature = [(-2, 2), (-1, 2), (0, 2), (1, 2), (2, 2)]\npm = embedders.manifolds.ProductManifold(signature=signature)\nx_embed = pm.initialize_embeddings(n_points=10)\n```\n\n----------------------------------------\n\nTITLE: Checking for NaNs in Kernel Outputs (Python)\nDESCRIPTION: Generates a product manifold, creates a Gaussian mixture dataset, and evaluates the product kernel, checking for NaNs in the resulting kernel matrices. Requires embedders (with manifolds and gaussian_mixture), product_kernel function, and PyTorch. Inputs are the product manifold and dataset, output is a boolean indicator of NaN presence. Used for kernel validity diagnostics.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport embedders\n\npm = embedders.manifolds.ProductManifold([(-1, 2), (0, 2), (1, 2)])\nX, y = embedders.gaussian_mixture.gaussian_mixture(pm)\n\n[x.isnan().any() for x in product_kernel(pm, X, X)[0]]\n```\n\n----------------------------------------\n\nTITLE: Evaluating the Trained Product Space Random Forest in Python\nDESCRIPTION: Evaluates the performance of the previously trained Product Space Random Forest (`prf`) on the training data (`X`, `y`). It calculates the accuracy using the `score` method, converts it to a float, takes the mean (though likely unnecessary for a single score), retrieves the scalar value using `.item()`, and prints it.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/14_new_tree_testing.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nprf.score(X, y).float().mean().item()\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Multiple Manifold Neural and Graph Models Using embedders (Python)\nDESCRIPTION: This code verifies and benchmarks various models ('product_dt', 'tangent_mlp', 'ambient_mlp', 'tangent_gnn', 'ambient_gnn') using embedders' benchmark utility. It converts results to a pandas DataFrame for review. Dependencies: embedders, pandas; assumes product manifold and Gaussian mixture data are properly initialized.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/41_more_benchmarks.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\\n%autoreload 2\\n\\n# Verify tangent MLP works\\nimport embedders\\nimport pandas as pd\\n\\npm = embedders.manifolds.ProductManifold(signature=[(1, 2), (-1, 2)])\\nX, y = embedders.gaussian_mixture.gaussian_mixture(pm, num_clusters=32, num_classes=8)\\nscores = embedders.benchmarks.benchmark(\\n    X, y, pm, models=[\"product_dt\", \"tangent_mlp\", \"ambient_mlp\", \"tangent_gnn\", \"ambient_gnn\"]\\n)\\npd.DataFrame(scores)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Classification on Embedded Data using Embedders Library in Python\nDESCRIPTION: Imports the custom 'embedders' library, torch, sklearn utilities, tqdm, and pandas. It sets the number of benchmark trials (N_TRIALS) and defines the product manifold structure corresponding to the top frequencies using `embedders.manifolds.ProductManifold`. It then runs a loop for N_TRIALS: in each trial, it generates embedded data and labels using the `get_sample` function (for classification), converts them to torch tensors, and calls `embedders.benchmarks.benchmark` to evaluate classification performance (accuracy, f1-micro) potentially using GPU acceleration. Results from each trial are collected into a pandas DataFrame and saved to a TSV file.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/30_fourier.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport embedders\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom tqdm.notebook import trange\nimport pandas as pd\n\n# MAX_DEPTH = 3\n# N_TRIALS = 100\nN_TRIALS = 10\n\npm = embedders.manifolds.ProductManifold(signature=[(1, 1) for f in top_freqs])\n\nresults = []\nfor trial in trange(N_TRIALS):\n    data, labels = get_sample(seed=trial)\n    # data, labels = get_sample_regression(seed=trial)\n    data = torch.tensor(data).float()\n    labels = torch.tensor(labels).long()\n    # labels = torch.tensor(labels).float()\n\n    res = embedders.benchmarks.benchmark(X=data, y=labels, pm=pm, task=\"classification\", score=[\"accuracy\", \"f1-micro\"], device=\"cuda:1\")\n    # res = embedders.benchmarks.benchmark(data, labels, pm=pm, max_depth=MAX_DEPTH, task=\"regression\", score=\"rmse\")\n    res[\"seed\"] = trial\n\n    results.append(res)\n\nresults = pd.DataFrame(results)\n\nresults.to_csv(\"../data/results_icml/fourier.tsv\", sep=\"\\t\", index=False)\n# results.to_csv(\"../data/results_icml/fourier2.tsv\", sep=\"\\t\", index=False)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Product vs. Sklearn Decision Tree Performance with Matplotlib in Python\nDESCRIPTION: This Python snippet uses Matplotlib to create a scatter plot comparing the performance scores of a Product Decision Tree (`product_dt`) against a standard Scikit-learn Decision Tree (`sklearn_dt`). It plots `product_dt` scores on the x-axis and `sklearn_dt` scores on the y-axis, adds axis labels, a title indicating the dataset, scoring metric, and mean scores, includes a y=x reference line, and displays the plot. It relies on the `results` and `results_grouped` DataFrames generated previously.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/2_polblogs_benchmark.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n\nplt.scatter(results[\"product_dt\"], results[\"sklearn_dt\"])\nplt.ylabel(\"Sklearn DT\")\nplt.xlabel(\"Product DT\")\nmymin = results[[\"product_dt\", \"sklearn_dt\"]].min().min()\nmymax = results[[\"product_dt\", \"sklearn_dt\"]].max().max()\nplt.plot([mymin, mymax], [mymin, mymax], label=\"y=x\")\nplt.title(\n    f\"{DATASET.capitalize()} - {SCORE} \\n ProductDT {results_grouped['product_dt'].mean():.3f} vs SklearnDT {results_grouped['sklearn_dt'].mean():.3f}\"\n)\nplt.legend()\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Fitting Manifold Random Forest Trees in PyTorch - Python\nDESCRIPTION: Samples random trees by resampling data and features (dimensions), then fits multiple fit_node trees, assembling an ensemble with manifold-aware data splits. Employs tqdm for progress, uses torch.randint and torch.randperm for index selection, and enables model robustness by tree bagging. Expects preprocessed data and parameterized tree function.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/13_information_gain.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n%%prun\nfrom tqdm.notebook import tqdm\n\nN_TREES = 100\n\nangles, labels_onehot, comparisons_reshaped = preprocess(X, y)\n\n# Resample X and y\nn, d = angles.shape\nidx_sample = torch.randint(0, n, (N_TREES, n))\n\n# Dims should be sampled with no replacement, at a rate of sqrt(D):\nD = torch.ceil(torch.tensor(d ** 0.5)).int()\nidx_dim = torch.stack([torch.randperm(d)[:D] for _ in range(N_TREES)])\nprint(f\"idx_sample: {idx_sample.shape}\") # (n_trees, n)\nprint(f\"idx_dim: {idx_dim.shape}\") # (n_trees, D)\n\n# Fit each tree\nfitted_trees = []\nfor sample_idx, dim_idx in tqdm(zip(idx_sample, idx_dim), total=N_TREES):\n    fitted_trees.append(\n        fit_node(\n            comparisons=comparisons_reshaped[sample_idx][:, dim_idx][:, :, sample_idx],\n            labels=labels_onehot[sample_idx],\n            depth=3,\n            min_n=1\n        )\n    )\n```\n\n----------------------------------------\n\nTITLE: Evaluating Graph Embeddings with Product Space Decision Tree in Python\nDESCRIPTION: This snippet evaluates the quality of the generated 'polblogs' graph embeddings (`h6_polblogs`) using a classifier. It splits the embeddings and labels into training and testing sets, initializes a Product Space Decision Tree (`ProductSpaceDT`) configured with the specific manifold (`pm`) and hyperparameters (max depth), trains the tree, predicts labels for the test set, and prints the weighted F1 score.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/Embedder Tutorial.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#Evaluation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom embedders.predictors import tree_new\nX_train, X_test, y_train, y_test = train_test_split(h6_polblogs, polblogs_labels.detach().cpu(), test_size=0.2, random_state=0)\npdt = tree_new.ProductSpaceDT(pm=pm, max_depth=3, use_special_dims=True)\npdt.fit(X_train, y_train)\npdt_f1 = f1_score(y_test, pdt.predict(X_test),average=\"weighted\")\nprint(f\"ProductDT\\t{pdt_f1*100:.2f}\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up Experiment Parameters for Link Prediction Comparison in Python\nDESCRIPTION: Initializes parameters and configurations for a larger experiment comparing different models on link prediction tasks. It imports necessary metrics and utilities (`f1_score`, `tqdm`, `time`, `train_test_split`). It defines constants such as the list of `DATASETS` to use, the `SIGNATURE` for the product manifold, the number of `N_TRIALS`, total training `ITERATIONS`, whether to include distances as features (`USE_DISTS`), whether to use a progress bar (`USE_TQDM`), a list of `MODELS` to evaluate (including scikit-learn models, manifold-specific models, k-NN, perceptrons), learning rate (`LR`), and training `EPOCHS`. An empty list `results` is initialized to store experiment outcomes.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/51_gcn_link_prediction.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.metrics import f1_score\nfrom tqdm.notebook import tqdm\nimport time\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\n\nDATASETS = [\"dolphins\", \"football\", \"karate_club\", \"lesmis\", \"polbooks\", \"adjnoun\"]\nSIGNATURE = [(-1, 2), (0, 2), (1, 2)]\nN_TRIALS = 10\nTOTAL_ITERATIONS = 1_000\nUSE_DISTS = True\nUSE_TQDM = True\nMODELS = [\n    \"sklearn_dt\",\n    \"sklearn_rf\",\n    \"product_dt\",\n    \"product_rf\",\n    \"tangent_dt\",\n    \"tangent_rf\",\n    \"single_manifold_rf\",\n    \"knn\",\n    \"ps_perceptron\",\n    # \"ambient_mlp\",\n    # \"ambient_gnn\",\n    # \"kappa_gcn\",\n    # \"product_mlr\",\n]\nLR = 1e-4\nEPOCHS = 4_000\n\nresults = []\n```\n\n----------------------------------------\n\nTITLE: Calculating Distances on a Manifold\nDESCRIPTION: Calculates the geodesic distances between the origin (`m.mu0`) and a batch of sampled points (`samples`) on the manifold `m` (which was last defined in the preceding loop, corresponding to K=4.0). The `unsqueeze(0)` adds a batch dimension to the origin point for broadcasting.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/5_sampling.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nm.dist(m.mu0.unsqueeze(0), samples)\n```\n\n----------------------------------------\n\nTITLE: Inspecting Individual Trees within the Random Forest in Python\nDESCRIPTION: Iterates through each individual decision tree stored in the `trees` attribute of the trained Random Forest (`prf`). For each tree, it calculates and prints its individual accuracy score on the full dataset (`X`, `y`) and also prints the feature permutations (`tree.permutations`) used for that specific tree during training (a common technique in Random Forests).\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/14_new_tree_testing.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfor tree in prf.trees:\n    print(f\"{tree.score(X, y).float().mean().item():.3f}\", tree.permutations.tolist())\n```\n\n----------------------------------------\n\nTITLE: Checking Available Keys in Manify Benchmark Output\nDESCRIPTION: Accesses and implicitly displays the keys of the `out` dictionary, which contains the results from the previously run benchmark. This step is typically done to inspect the names of the metrics and models included in the benchmark results. The presence of the keyword `set` suggests a subsequent operation involving sets of keys.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/60_pytest_scratch.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nout.keys()\nset\n```\n\n----------------------------------------\n\nTITLE: Plotting RF Performance Difference vs. Average Distance with Matplotlib in Python\nDESCRIPTION: This Python snippet uses Matplotlib to visualize the relationship between the average distance metric (`d_avg`) and the difference in performance between the Product Random Forest and the Scikit-learn Random Forest (`product_rf` - `sklearn_rf`). It creates a scatter plot with `d_avg` on the x-axis and the performance difference on the y-axis. A horizontal line at y=0 is added for reference, along with axis labels, a title summarizing the comparison, and a legend before displaying the plot. It requires the `results_merged` and `results_grouped` DataFrames.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/2_polblogs_benchmark.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n\nplt.scatter(results_merged[\"d_avg\"], results_merged[\"product_rf\"] - results_merged[\"sklearn_rf\"])\nplt.ylabel(\"Product RF - Sklearn RF\")\nplt.xlabel(\"D_avg\")\n# plt.plot([0.8, 1], [0.8, 1], label=\"y=x\")\n# plt.hlines(0, 0.8, 1, label=\"y=0\")\nplt.plot([results_merged[\"d_avg\"].min(), results_merged[\"d_avg\"].max()], [0, 0], label=\"y=0\")\nplt.title(\n    f\"{DATASET.capitalize()} - {SCORE} \\n ProductRF {results_grouped['product_rf'].mean():.3f} vs SklearnRF {results_grouped['sklearn_rf'].mean():.3f}\"\n)\nplt.legend()\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Displaying Sampled Delta Hyperbolicity Values from Manify Calculation\nDESCRIPTION: This snippet simply outputs the content of the `sampled_deltas` variable. This variable holds the tensor of relative delta hyperbolicity values computed by the `sampled_delta_hyperbolicity` function in the preceding code block, representing the hyperbolicity measure for a sampled subset of point triplets.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/60_pytest_scratch.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nsampled_deltas\n```\n\n----------------------------------------\n\nTITLE: Information Gain Validation Assertion - Python\nDESCRIPTION: Validates the calculated information gain tensor by comparing the functional output to a reference version with a small tolerance. Requires prior execution of ig_est_functional and info_gains_nonan, and checks that all compared values are close within atol=1e-2, ensuring numerical correctness.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/13_information_gain.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nassert torch.allclose(\n    ig_est_functional, info_gains_nonan[:, 1:].to(ig_est_functional.dtype), atol=1e-2\n)  # Ensure the values are the same\n\n```\n\n----------------------------------------\n\nTITLE: Setting up IPython Autoreload\nDESCRIPTION: These IPython magic commands configure the environment to automatically reload modules before executing code. `%load_ext autoreload` loads the extension, and `%autoreload 2` enables reloading of all modules except those explicitly excluded.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/24_benchmark_graph_embeddings.ipynb#_snippet_0\n\nLANGUAGE: ipython\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Computing Classification Error Mask (Python)\nDESCRIPTION: Produces a boolean mask where predicted class labels (using sign of dot product) do not match the ground truth (re-encoded to -1/1). Relies on torch and the data tensors X, y, and randomly initialized g. Used to index or analyze misclassified samples.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nerr = torch.sign(X @ g) != (y * 2 - 1)\n```\n\n----------------------------------------\n\nTITLE: Verifying Log-Likelihood Calculation on Single Manifolds\nDESCRIPTION: Tests the `log_likelihood` method of the `Manifold` class. For various curvatures (K), it creates a manifold, samples a center point (`mu`), generates samples around this `mu` with a random diagonal covariance `Sigma`, and calculates the log-likelihood of these samples under both the default prior (P(z)) and the generating distribution (Q(z)). It prints the mean log-likelihoods and their difference, noting a potential inconsistency where Q(z) isn't always higher than P(z).\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/5_sampling.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Verify that log-likelihood functions work:\n\nfor K in [-2, -1.0, -0.5, 0, 0.5, 1.0, 2.0]:\n    print(K)\n    m = Manifold(K, 4)\n    # Pick a random point to use as the center\n    mu = m.sample(m.mu0)\n    Sigma = torch.diag(torch.randn(m.dim)) ** 2\n    samples = m.sample(z_mean=torch.cat([mu] * N_SAMPLES, dim=0), sigma=Sigma)\n    log_probs_p = m.log_likelihood(z=samples)  # Default args\n    log_probs_q = m.log_likelihood(z=samples, mu=mu, sigma=Sigma)\n    print(\n        f\"Shape: {log_probs_p.shape},\\tP(z) = {log_probs_p.mean().item():.3f},\\tQ(z) = {log_probs_q.mean().item():.3f},\\tQ(z) - P(z) = {log_probs_q.mean().item() - log_probs_p.mean().item():.3f}\"\n    )\n    print()\n\n    # Why don't we generally see Q(z) - P(z) > 0? I would think the ll of the true distribution would be higher than the ll of the wrong distribution...\n```\n\n----------------------------------------\n\nTITLE: Filtering and Annotating Blood Cell Data, Merging with Scanpy - Python\nDESCRIPTION: This code filters each loaded dataset to include only landmark genes and annotates the observations with the cell type label. It then concatenates all filtered datasets using Scanpy's AnnData.concatenate, using 'cell_type' as the batch key. Input consists of the preloaded 'data' dictionary and 'landmark_genes'; output is the merged AnnData object named 'adata'. Dependencies include Scanpy and prior data preparation; limitations are that only cells/genes present in provided files are retained.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/8_scrna_preprocessing.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Restrict to landmark genes and annotate cell type\\n\\ndata_filtered = {k: v[:, v.var.index.isin(landmark_genes)] for k, v in data.items()}\\nfor k, v in data_filtered.items():\\n    v.obs[\"cell_type\"] = k\\n\\n# Finally, merge\\nadata = scanpy.AnnData.concatenate(*data_filtered.values(), batch_key=\"cell_type\")\\nadata\n```\n\n----------------------------------------\n\nTITLE: Displaying Manifold Origin\nDESCRIPTION: Displays the tensor representation of the origin point (`mu0`) of the manifold `m` (last defined with K=4.0 in the preceding loop).\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/5_sampling.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nm.mu0\n```\n\n----------------------------------------\n\nTITLE: Inspecting Shape of Preprocessed Angles Array in Python\nDESCRIPTION: Accesses the `shape` attribute of the `angles` NumPy array, which contains the angular representation of the data after preprocessing by `ProductSpaceDT`. The comment indicates the expected shape is `(n, 6)`, where `n` is the number of samples and 6 is the total number of angular dimensions derived from the product manifold's structure.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/37_sklearn_decision_tree.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nangles.shape # Should be (n, 6)\n```\n\n----------------------------------------\n\nTITLE: Displaying Training Distance Matrix in Python\nDESCRIPTION: This snippet prints the `D_train` tensor, which holds the normalized true pairwise distances for the training data from the last iteration of the distortion calculation loop. This is likely used for debugging to inspect the input distances.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/24_benchmark_graph_embeddings.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nD_train\n```\n\n----------------------------------------\n\nTITLE: Testing Positive Class with Different Index (Python)\nDESCRIPTION: Performs the same kernel-feature product and class sign operation at index 6, suggesting a positive result is expected. Reinforces kernel correctness and class separability.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_39\n\nLANGUAGE: python\nCODE:\n```\n(-1 * K[6] @ X) @ X[6], y[6]  # Should be positive\n```\n\n----------------------------------------\n\nTITLE: Setting Up Plotting Environment for Benchmark Results in Python\nDESCRIPTION: Prepares the environment for visualizing benchmark results using Matplotlib and performs initial setup. It imports necessary libraries (pandas, matplotlib, numpy, scipy.stats for Wilcoxon test). It configures Matplotlib settings for font (Times New Roman, LaTeX math font). It defines constants for the analysis, such as the 'TASK' (classification/regression), significance display ('sig'/'ns'), multiple testing 'CORRECTION' method (Bonferroni), performance metric 'SUFFIX', and Y-axis 'LABEL'. It reads results data from a TSV file, defines the list of 'CURVATURES' to consider, sets plotting constants ('AST_SPACING', 'LW', 'FONTSIZE'), and defines a helper function 'test' using 'scipy.stats.wilcoxon' for statistical significance testing between model results (handling identical inputs). It specifies the models to plot ('models1', 'models2', 'models3'), their display names, colors, calculates the 'CRITICAL_VAL' for significance based on the chosen correction method, initializes a Matplotlib figure and axes ('fig', 'axs'), and calculates x-values ('x_vals') for plotting bars grouped by curvature. The loop for actually creating the plot content is commented out or incomplete in the provided snippet.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/26_benchmark_single_curvature_gaussian.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Let's see the values: barplot with statistical significance annotations\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import wilcoxon\nimport matplotlib as mpl\n\n# Set font to LaTeX times font\nplt.rcParams[\"font.family\"] = \"serif\"\nplt.rcParams[\"font.serif\"] = [\"Times New Roman\"] + plt.rcParams[\"font.serif\"]\nplt.rcParams[\"mathtext.fontset\"] = \"cm\"\n# plt.rcParams[\"text.usetex\"] = True\n# plt.rcParams[\"text.latex.preamble\"] = r\"\\usepackage{amsmath}\"\n# plt.rcParams[\"font.size\"] = 10\n\n# TASK = \"regression\"\nTASK = \"classification\"\nSIGNIFICANCE = \"sig\"\n# SIGNIFICANCE = \"ns\"\nCORRECTION = \"bonferroni\"\n# CORRECTION = \"none\"\n\nSUFFIX = \"f1-micro\" if TASK == \"classification\" else \"rmse\"\nY_LABEL = \"$F_1$ Score\" if TASK == \"classification\" else \"RMSE\"\n\nresults = pd.read_csv(f\"../data/results_icml/{TASK}_single_curvature.tsv\", sep=\"\\t\")\nCURVATURES = [-4, -2, -1, -0.5, -0.25, 0, 0.25, 0.5, 1, 2, 4]\n# CURVATURES = [-2, -1, -0.5, 0, 0.5, 1, 2]\n\nAST_SPACING = 0.1\nLW = 1\nFONTSIZE = 8\n\n\ndef test(x, y):\n    # Check x and y are identical\n    all_same = True\n    for a, b in zip(x, y):\n        if a != b:\n            all_same = False\n            break\n    if all_same:\n        return 1.0\n    else:\n        return wilcoxon(x, y).pvalue\n\n\n# All the spec happens up here\nmodels1 = [\"product_dt\", \"sklearn_dt\", \"tangent_dt\"]\nmodel_names1 = [\"Product DT\", \"Euclidean DT\\n(ambient)\", \"Euclidean DT\\n(tangent plane)\"]\nmodels2 = [\"product_rf\", \"sklearn_rf\", \"tangent_rf\"]\nmodel_names2 = [\"Product RF\", \"Euclidean RF\\n(ambient)\", \"Euclidean RF\\n(tangent plane)\"]\n# models3 = [\"knn\", \"perceptron\", \"svm\"]\n# model_names3 = [\"k-Nearest Neighbors\", \"Perceptron\", \"Support Vector Classifier\"]\nmodels3 = [\"knn\", \"ambient_mlp\", \"kappa_gcn\"]\nmodel_names3 = [\"$k$-Nearest\\nNeighbors\", \"MLP\", \"$\\kappa$-GCN\"]\n# models3 = [\"knn\"]\n# model_names3 = [\"$k$-NN\"]\ncolors = [f\"C{i}\" for i in range(len(models1 + models3))]\n\n# Critical p-value depends on false discovery correction\nCRITICAL_VAL = 0.05\nif CORRECTION == \"bonferroni\":\n    CRITICAL_VAL /= len(models1 + models3) * (len(models1 + models3) - 1) / 2\n\n# Initialize plot\n# fig, axs = plt.subplots(2, 1, figsize=(15, 10), sharex=True)\n# fig, axs = plt.subplots(2, 1, figsize=(14, 7), sharex=True)\n# fig, axs = plt.subplots(2, 1, figsize=(6.5, 3.5), sharex=True)\nfig, axs = plt.subplots(1, 1, figsize=(6.5, 2.), sharex=True)\nx_vals = np.arange(len(CURVATURES)) * (len(models1 + models3) + 1)\n\n# for ax, models, model_names in zip(\n#     axs, [models1 + models3, models2 + models3], [model_names1 + model_names3, model_names2 + model_names3]\n\n```\n\n----------------------------------------\n\nTITLE: Inspecting Root Node Attributes of ProductSpaceDT (Revisited) in Python\nDESCRIPTION: Repeats the action of accessing and implicitly printing the internal attribute dictionary (`__dict__`) of the root node (index 0) of the trained `ProductSpaceDT` object `pdt`. This again shows the parameters of the first split.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_basic_visualization.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npdt.nodes[0].__dict__\n```\n\n----------------------------------------\n\nTITLE: Testing Negative Class Kernel-Feature Products at Different Indices (Python)\nDESCRIPTION: Same as above, but evaluating at indices 1 and 6. Useful for checking sign invariance and kernel-embedding relationships across the dataset.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_38\n\nLANGUAGE: python\nCODE:\n```\n(-1 * K[1] @ X) @ X[1], y[1]  # Should be negative\n```\n\n----------------------------------------\n\nTITLE: Initializing and Sampling from a Spherical Manifold\nDESCRIPTION: Creates an instance of the `Manifold` class representing a 4-dimensional spherical space (curvature K=1). It then calls the `sample` method to generate points on this positively curved manifold.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/5_sampling.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nm_s = Manifold(1, 4)\nm_s.sample([[1, 0, 0, 0, 0] * 4])\n```\n\n----------------------------------------\n\nTITLE: Loading Polblogs Dataset using Embedders Library in Python\nDESCRIPTION: Loads the Polblogs dataset using the `load_polblogs` function from the `embedders.dataloaders` module. It retrieves distance information (`dists`), node labels (`labels`), and the adjacency matrix (`adj`). The snippet then displays the loaded distances. Requires the `embedders` library to be imported.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/48_fix_transductive_coords.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Try out the embedder, this time with train-test split\n\ndists, labels, adj = embedders.dataloaders.load_polblogs()\ndists\n```\n\n----------------------------------------\n\nTITLE: Engineer Cyclical Time Features\nDESCRIPTION: This snippet creates a new feature set (X) and target array (y) from the DataFrame `df`. It iterates through each row, extracting the junction ID and time components (day of year, day of week, hour, minute). It applies a lambda function `angle` to convert these time components into angles (radians) representing their position in a cycle (e.g., day in a year, hour in a day). It then uses sine and cosine transformations on these angles to create cyclical features, which capture the periodic nature of time. The junction ID and the cyclical features are combined into the feature array `X`. The vehicle count is stored in `y`.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/33_traffic.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nangle = lambda x, n : x / n * 2 * np.pi\n\nX, y = [], []\nfor i, row in df.iterrows():\n    _, jct, vehicles, _, _, day, hr, doy, minute = row\n    X.append([\n        jct,\n        np.cos(angle(doy, 365)),\n        np.sin(angle(doy, 365)),\n        np.cos(angle(day, 7)),\n        np.sin(angle(day, 7)),\n        np.cos(angle(hr, 24)),\n        np.sin(angle(hr, 24)),\n        np.cos(angle(minute, 60)),\n        np.sin(angle(minute, 60)),\n    ])\n    y.append(vehicles)\n\nX = np.array(X)\ny = np.array(y)\n\nprint(X.shape, y.shape)\n```\n\n----------------------------------------\n\nTITLE: Training and Testing ProductSpace Decision Tree with Manify\nDESCRIPTION: Initializes a ProductManifold with a specific signature. Generates synthetic data using a Gaussian mixture model on the manifold. Splits the data into training and testing sets using scikit-learn's train_test_split. Creates a ProductSpace Decision Tree model (`ProductSpaceDT`) from the manify library, configured with the manifold and a maximum depth, and trains it on the training data.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/60_pytest_scratch.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Test decision tree\nimport manify\nfrom sklearn.model_selection import train_test_split\n\npm = manify.manifolds.ProductManifold(signature=[(-1, 2), (0, 2), (1, 2)])\n\nX, y = pm.gaussian_mixture(num_points=100, num_classes=2, seed=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\ndt = manify.predictors.decision_tree.ProductSpaceDT(pm=pm, max_depth=3)\ndt.fit(X_train, y_train)\n```\n\n----------------------------------------\n\nTITLE: Creating an Interactive Decision Tree Dashboard with Dash in Python\nDESCRIPTION: Defines the create_dashboard function, which initializes a Dash web application to visualize a decision tree (pdt) fitted on manifold-structured data. The function constructs a node information map, sets up color-coding for class labels, renders the tree structure, and registers an update callback for displaying per-node pie or scatter plots based on user interaction. Requires Dash, Plotly (go, dcc, html), and the decision tree/data as inputs. Key parameters: pdt (trained decision tree), X (data), y (labels). Output is a Dash web app object, with a layout containing two interactive figures and a callback for updating visualizations when a node is clicked.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/49_plotly_dashboard.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef create_dashboard(pdt, X, y):\n    node2id, edges = build_edges_and_id_map(pdt)\n    node_masks = compute_node_masks(pdt, X, y, node2id)\n    positions = bfs_tree_layout(pdt, node2id)\n\n    # Gather node info\n    node_data = {}\n    for i, node in enumerate(pdt.nodes):\n        if node.feature is not None:\n            dim1, dim2 = pdt.angle_dims[node.feature]\n        else:\n            dim1, dim2 = (None, None)\n        node_data[i] = {\n            \"id\": i,\n            \"feature\": node.feature,\n            \"theta\": node.theta if node.feature is not None else None,\n            \"dim1\": dim1,\n            \"dim2\": dim2,\n        }\n\n    # Build color map for classes\n    class2color = create_class_color_map(y)\n\n    # Make tree figure\n    tree_fig = make_tree_figure(edges, positions)\n\n    # Set up Dash\n    app = Dash(__name__)\n    app.layout = html.Div(\n        [\n            html.H1(\"Interactive Decision Tree\"),\n            html.Div(\n                [\n                    dcc.Graph(\n                        id=\"tree-graph\",\n                        figure=tree_fig,\n                        style={\"width\": \"45%\", \"display\": \"inline-block\", \"verticalAlign\": \"top\"},\n                    ),\n                    dcc.Graph(\n                        id=\"data-graph\", style={\"width\": \"50%\", \"display\": \"inline-block\", \"verticalAlign\": \"top\"}\n                    ),\n                ]\n            ),\n        ]\n    )\n\n    @app.callback(Output(\"data-graph\", \"figure\"), Input(\"tree-graph\", \"clickData\"))\n    def update_plot(clickData):\n        if not clickData:\n            return go.Figure()\n        node_id = clickData[\"points\"][0][\"customdata\"]\n        info = node_data[node_id]\n        mask = node_masks[node_id]\n\n        if info[\"feature\"] is None:\n            # Pie chart case for leaf nodes\n            fig = go.Figure()\n            y_in_leaf = y[mask]\n            if len(y_in_leaf) == 0:\n                fig.add_annotation(\n                    x=0.5, y=0.5, xref=\"paper\", yref=\"paper\", text=\"Empty Leaf Node\", showarrow=False\n                )\n            else:\n                counts = Counter(y_in_leaf.tolist())\n                labels = list(counts.keys())\n                values = list(counts.values())\n                colors = [class2color[label] for label in labels]\n\n                fig.add_trace(\n                    go.Pie(\n                        labels=labels,\n                        values=values,\n                        marker=dict(colors=colors),\n                        hoverinfo=\"label+value+percent\",\n                    )\n                )\n        else:\n            # Non-leaf node: scatter plot with half-plane coloring\n            fig = draw_data_figure(X, y, mask, info, class2color)\n\n        return fig\n\n    return app\n```\n\n----------------------------------------\n\nTITLE: Generating Graph Embeddings via Coordinate Learning in Python\nDESCRIPTION: This snippet generates embeddings for the previously loaded 'polblogs' graph data using coordinate learning on a product manifold. It defines the manifold structure using a signature, creates a ProductManifold instance, rescales the input distances, and then trains the coordinates using `coordinate_learning.train_coords` with specified hyperparameters (iterations, learning rates). The resulting embeddings are stored after detaching from the computation graph.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/Embedder Tutorial.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Specify signature - useful to re-initialize the manifold here\nfrom embedders import manifolds\nfrom embedders import coordinate_learning\ntorch.manual_seed(0)  \nsignature = [(1, 4)]\npm = manifolds.ProductManifold(signature=signature)\nprint(pm.name)\n\n# Rescale distances\ndists_rescaled = polblogs_dists / polblogs_dists.max()\n\n# Get embedding\ncoordinate_learning.train_coords(\n    pm,\n    dists_rescaled,\n    device=device,\n    burn_in_iterations=100,\n    training_iterations=100 * 9,\n    learning_rate=1e-1,\n    burn_in_learning_rate=1e-2,\n    scale_factor_learning_rate=1e-1,\n)\n\nh6_polblogs = pm.x_embed.detach().cpu()\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating a Random Forest on a Single Manifold with Manify in Python\nDESCRIPTION: This snippet demonstrates the use of the SingleManifoldEnsembleRF class to build and evaluate a random forest model on a product manifold dataset. Required dependencies include 'manify' and its predictor submodules. The key parameters are the product manifold object and the number of estimators. Input data (X, y) must conform to the structure expected by the manifold. The code fits the forest to data, makes predictions, and evaluates model accuracy. Appropriate for classification but may require adaptation for regression.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/56_single_manifold_rf.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom manify.predictors.tree_icml import SingleManifoldEnsembleRF\n\nrf = SingleManifoldEnsembleRF(pm=pm, n_estimators=10)\nrf.fit(X, y)\nrf.predict(X)\nrf.score(X, y)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Poincare Projections of H^2 Components in Python\nDESCRIPTION: This snippet uses matplotlib and the 'embedders.visualization.hyperboloid_to_poincare' function to visualize the learned embeddings. It creates a 1x3 subplot figure. In each subplot, it converts one H^2 component of the 'h2_3_cs_phds' embedding (selected using 'pm2.man2dim[i]') from the hyperboloid model to the Poincare disk model and creates a scatter plot, coloring points by their labels ('cs_labels').\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/4_cs_phds_regression.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Is there a remote possibility that looking at the embeddings will help?\n\nimport matplotlib.pyplot as plt\nimport numpy as np # Added import based on usage\n\nfig, axs = plt.subplots(1, 3, figsize=(15, 5))\n\nfor i in range(3):\n    poin = embedders.visualization.hyperboloid_to_poincare(h2_3_cs_phds[:, pm2.man2dim[i]])\n    axs[i].scatter(poin[:, 0], poin[:, 1], c=np.array(cs_labels), cmap=\"viridis\", s=5)\n```\n```\n\n----------------------------------------\n\nTITLE: Load Kaggle Traffic Data from CSV\nDESCRIPTION: This snippet imports the `pandas` library and uses `pd.read_csv` to load the Kaggle traffic dataset from a specified CSV file path into a pandas DataFrame named `df`. It then displays the DataFrame.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/33_traffic.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\ndata = \"../data/traffic/traffic.csv\"\ndf = pd.read_csv(data)\ndf\n```\n\n----------------------------------------\n\nTITLE: Plotting Training and Test Losses using Matplotlib in Python\nDESCRIPTION: Visualizes the training progress by plotting different loss values stored in the `losses` dictionary obtained from the `train_coords` function. It plots 'train_train', 'test_test', and 'train_test' losses on the same graph with labels for clarity. Requires `matplotlib.pyplot` and the `losses` dictionary from the previous embedding training step.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/48_fix_transductive_coords.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n\nplt.plot(losses[\"train_train\"], label=\"train_train\")\nplt.plot(losses[\"test_test\"], label=\"test_test\")\nplt.plot(losses[\"train_test\"], label=\"train_test\")\nplt.legend()\n```\n\n----------------------------------------\n\nTITLE: Defining Function to Aggregate Tree Nodes in Python\nDESCRIPTION: Defines a recursive function `agg_nodes` that traverses a binary tree structure (presumably representing a decision tree) and collects all nodes in a list. It is then used to extract all nodes from the `dt_old` tree, and the feature index and threshold (`theta`) for each non-leaf node are printed.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/14_new_tree_testing.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef agg_nodes(tree, nodes):\n    if tree is not None:\n        nodes.append(tree)\n        agg_nodes(tree.left, nodes)\n        agg_nodes(tree.right, nodes)\n    return nodes\n\n\ndt_old_nodes = agg_nodes(dt_old.tree, [])\n[(x.feature.item(), f\"{x.theta.item():.4f}\") for x in dt_old_nodes if x.feature is not None]\n```\n\n----------------------------------------\n\nTITLE: Defining a Centroid-Based Model with geoopt on Manifolds (PyTorch, Python)\nDESCRIPTION: Creates manifold-based centroids using geoopt and defines a CentroidMLR class skeleton for multiclass classification based on distances to centroids (no completed forward method). Dependencies: geoopt, manifold pm, PyTorch. The snippet demonstrates manifold-parameter handling for geometric ML models, but lacks implementation details for inference.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/41_more_benchmarks.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Centroid-based model\\nimport geoopt\\n\\nN_CLASSES = 8\\ncentroids = torch.vstack([pm.sample() for _ in range(N_CLASSES)])\\n\\n# make centroids a manifold parameter\\ncentroids = geoopt.ManifoldParameter(centroids, manifold=pm)\\n\\n# Define model: take distance to centroids as logits\\nclass CentroidMLR(nn.Module):\\n    def __init__(self, pm, centroids):\\n        super().__init__()\\n        self.pm = pm\\n        self.centroids = centroids\\n        self.weights = nn.Parameter(torch.randn(N_CLASSES, 1))\\n\\n    def forward(self, x):\\n        # p(y | h) = softmax(W h)\\n    \n```\n\n----------------------------------------\n\nTITLE: Debugging and Verifying Information Gain Calculations in ProductSpaceDT (Python/Torch)\nDESCRIPTION: This snippet performs a detailed step-by-step verification of the information gain (specifically, Mean Squared Error reduction) calculations used within the `ProductSpaceDT`. It calculates sums, counts, means, and MSE for positive and negative splits based on the `comparisons` tensor derived from the tree, first using vectorized PyTorch operations mirroring the likely implementation in `new_tree.py`. It then explicitly recalculates these same metrics using nested Python loops for comparison and uses `assert torch.allclose` to verify that the vectorized and explicit loop-based results match, ensuring the correctness of the core splitting logic.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/17_verify_new_mse.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Let's go step-by-step through the information gain computations to ensure they match\n\n# pos_sums like in line 72 of new_tree.py\npos_sums = comparisons @ y\nneg_sums = (1 - comparisons) @ y\nn_pos = comparisons.sum(dim=-1) + 1e-10\nn_neg = (1 - comparisons).sum(dim=-1) + 1e-10\npos_means = pos_sums / n_pos\nneg_means = neg_sums / n_neg\nall_means = y.mean()\n\n# MSE component - line 85 onwards\npos_se = ((y[:, None, None] - pos_means) ** 2).permute(1, 2, 0)\nneg_se = ((y[:, None, None] - neg_means) ** 2).permute(1, 2, 0)\npos_mse = (comparisons * pos_se).sum(dim=-1) / n_pos\nneg_mse = ((1 - comparisons) * neg_se).sum(dim=-1) / n_neg\n# pos_mse = torch.einsum(\"ijk,kj->ij\", comparisons, pos_se) / n_pos\n# neg_mse = torch.einsum(\"ijk,kj->ij\", (1 - comparisons), neg_se) / n_neg\ntotal_mse = (y - all_means) ** 2\n\n# These variables will be sanity checks\npos_sums_check = torch.zeros_like(pos_sums)\npos_counts_check = torch.zeros_like(pos_sums)\nneg_sums_check = torch.zeros_like(neg_sums)\nneg_counts_check = torch.zeros_like(neg_sums)\n\n# Also aggregate means into this\npos_means_check = torch.zeros_like(pos_sums)\nneg_means_check = torch.zeros_like(neg_sums)\n\n# And MSEs here\npos_se_check = torch.zeros_like(comparisons)\nneg_se_check = torch.zeros_like(comparisons)\npos_mse_check = torch.zeros_like(pos_mse)\nneg_mse_check = torch.zeros_like(neg_mse)\n\n# Big expensive loop to check the sums\nfor i in range(comparisons.shape[0]):\n    for j in range(comparisons.shape[1]):\n        pos_vals = []\n        neg_vals = []\n        for k in range(comparisons.shape[2]):\n            if comparisons[i, j, k]:\n                pos_sums_check[i, j] += y[k]\n                pos_counts_check[i, j] += 1\n                pos_vals.append(y[k])\n            else:\n                neg_sums_check[i, j] += y[k]\n                neg_counts_check[i, j] += 1\n                neg_vals.append(y[k])\n\n        # Check that the counts match\n        assert len(pos_vals) == n_pos[i, j] - 1e-10\n        assert len(neg_vals) == n_neg[i, j] - 1e-10\n\n        # Store means - prevent nans by explicitly specifying \"else 0\" here\n        pos_mean = torch.mean(torch.tensor(pos_vals)) if pos_vals else 0\n        neg_mean = torch.mean(torch.tensor(neg_vals)) if neg_vals else 0\n        pos_means_check[i, j] = pos_mean\n        neg_means_check[i, j] = neg_mean\n\n        # MSEs\n        for k in range(y.shape[0]):\n            pos_se_check[i, j, k] = (y[k] - pos_mean) ** 2 if pos_vals else 0\n            neg_se_check[i, j, k] = (y[k] - neg_mean) ** 2 if neg_vals else 0\n        # pos_mse_check[i, j] = torch.tensor(pos_vals).var() if len(pos_vals) > 1 else 0\n        # neg_mse_check[i, j] = torch.tensor(neg_vals).var() if len(neg_vals) > 1 else 0\n        pos_mse_check[i, j] = torch.mean((torch.tensor(pos_vals) - pos_mean) ** 2) if pos_vals else 0\n        neg_mse_check[i, j] = torch.mean((torch.tensor(neg_vals) - neg_mean) ** 2) if neg_vals else 0\n\nassert torch.allclose(pos_sums, pos_sums_check)\nassert torch.allclose(neg_sums, neg_sums_check)\nassert torch.allclose(n_pos, pos_counts_check)\nassert torch.allclose(n_neg, neg_counts_check)\nassert torch.allclose(pos_means, pos_means_check)\nassert torch.allclose(neg_means, neg_means_check)\nassert torch.allclose(pos_se, pos_se_check, atol=1e-4)\n# assert torch.allclose(neg_se, neg_se_check, atol=1e-4) # Skip this check cause it fails when n_neg = 0\nassert torch.allclose(pos_mse, pos_mse_check)\nassert torch.allclose(neg_mse, neg_mse_check)\n```\n\n----------------------------------------\n\nTITLE: Inspecting Prediction Output Range for GNN Link Predictor (Python)\nDESCRIPTION: Quickly checks the minimum and maximum values of predictions generated by a link prediction GNN, revealing output range or normalization sanity. This minimal utility snippet is used for debugging or post-processing.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/41_more_benchmarks.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ny_pred.min(), y_pred.max()\n```\n\n----------------------------------------\n\nTITLE: Interpreting Product Manifold Decision Tree Nodes in Python\nDESCRIPTION: This code investigates which manifolds decision tree nodes split upon by mapping tree node features to manifold indices. Utilizing the ProductManifold and ProductSpaceDT objects, a list comprehension extracts the manifold dimension index for each non-leaf node feature, which can be analyzed (e.g., via Counter) for interpretability. Requires the pm (ProductManifold), pdt (ProductSpaceDT), and their relevant attributes to be initialized.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/45_blood_and_lymphoma_interp.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n# Which manifolds do our nodes fall into?\nfrom collections import Counter \n\n[pm.dim2man[pdt.angle_dims[node.feature][1]] for node in pdt.nodes if node.feature is not None]\n```\n\n----------------------------------------\n\nTITLE: Generating Gaussian Mixture Data on Product Manifolds with Manify in Python\nDESCRIPTION: This Python snippet utilizes the 'manify' and 'torch' libraries. It iterates through a list of dimensions, creating two 'ProductManifold' instances for each dimension: 'pm1' with a negative curvature signature `[(-1, dim)]` and 'pm2' with a positive curvature signature `[(1, dim)]`. It then generates Gaussian mixture data points ('X1', 'X2') on these respective manifolds using the 'gaussian_mixture' method, scaling covariance by the inverse of the dimension. Finally, it checks if the generated tensors 'X1' or 'X2' contain any NaN or Inf values, printing a message if they do. Dependencies include 'manify' and 'torch'.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/58_what_can_we_sample.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport manify\nimport torch\n\nfor dim in [2, 4, 8, 16, 32, 64, 128]:\n    pm1 = manify.manifolds.ProductManifold(signature=[(-1, dim)])\n    pm2 = manify.manifolds.ProductManifold(signature=[(1, dim)])\n\n    X1, _ = pm1.gaussian_mixture(cov_scale_means = 1 / dim, cov_scale_points= 1 / dim)\n    X2, _ = pm2.gaussian_mixture(cov_scale_means = 1 / dim, cov_scale_points= 1 / dim)\n\n    if torch.isnan(X1).any() or torch.isinf(X1).any():\n        print(f\"{dim}: X1 contains NaN or Inf values.\")\n    \n    if torch.isnan(X2).any() or torch.isinf(X2).any():\n        print(f\"{dim}: X2 contains NaN or Inf values.\")\n```\n\n----------------------------------------\n\nTITLE: Visualizing Global Temperature Model Predictions with Basemap and Matplotlib - Python\nDESCRIPTION: Creates a multi-panel contour plot to compare different temperature regression models on a global map using the Robinson projection. Sets up Basemap, plots city points, overlays model predictions for several regressors, and adds a colorbar with custom labels and font sizes. Requires matplotlib, mpl_toolkits.basemap, and pre-trained model outputs (including pdt_pred, dt, tdt, knn predictions); outputs a .pdf file with visual comparison of performance (RMSE indicated).\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/25_benchmark_globe.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\\nfrom mpl_toolkits.basemap import Basemap\\n\\n# Set font to LaTeX times font\\nplt.rcParams[\"font.family\"] = \"serif\"\\nplt.rcParams[\"font.serif\"] = [\"Times New Roman\"] + plt.rcParams[\"font.serif\"]\\n\\n# LEVELS = 2 ** (MAX_DEPTH) # One for each discrete prediction\\nLEVELS = None\\nFONTSIZE = 10\\n\\n# Plot surface in long, lat basis\\nfig, axs = plt.subplots(2, 2, figsize=(5.5, 4))\\n\\n# Map\\nm = Basemap(projection=\"robin\", lon_0=0, resolution=\"c\")\\n_x_map, _y_map = m(_x_vals_degrees, _y_vals_degrees)  # Grid\\n\\n# Get city coords\\ncity_coords = xyz_to_long_lat(X)\\nx_map, y_map = m(torch.rad2deg(city_coords[:, 0]).numpy(), torch.rad2deg(city_coords[:, 1]).numpy())\\n\\n# Plot contours\\n\\naxs[0, 0].contourf(_x_map, _y_map, pdt_pred.reshape(SHAPE), levels=LEVELS)\\naxs[0, 0].set_title(f\"Product DT (RMSE: {pdt_score:.2f})\", fontsize=FONTSIZE)\\naxs[0, 1].contourf(_x_map, _y_map, dt.predict(_X).reshape(SHAPE), levels=LEVELS)\\naxs[0, 1].set_title(f\"Ambient DT (RMSE: {dt_score:.2f})\", fontsize=FONTSIZE)\\naxs[1, 0].contourf(_x_map, _y_map, tdt.predict(pm.logmap(_X).detach().numpy()).reshape(SHAPE), levels=LEVELS)\\naxs[1, 0].set_title(f\"Tangent DT (RMSE: {tdt_score:.2f})\", fontsize=FONTSIZE)\\naxs[1, 1].contourf(_x_map, _y_map, knn.predict(_X).reshape(SHAPE), levels=LEVELS)\\naxs[1, 1].set_title(f\"k-Nearest Neighbors (RMSE: {knn_score:.2f})\", fontsize=FONTSIZE)\\n\\nfor ax in axs.flatten():\\n    # Ticks off\\n    ax.set_xticks([])\\n    ax.set_yticks([])\\n    ax.set_aspect(\"equal\")\\n    m.drawcoastlines(ax=ax, linewidth=0.5)\\n    # ax.scatter(x_map, y_map, c=y, cmap=\"viridis\", edgecolors=\"black\", s=9)\\n\\n    # Rasterize contour plots\\n    for coll in ax.collections:\\n        coll.set_rasterized(True)\\n\\n# Colorbar\\nfig.subplots_adjust(right=1.1)\\ncbar_ax = fig.add_axes([1.0, 0.15, 0.02, 0.7])\\n# Colorbar with font size 20\\nfig.colorbar(axs[1, 1].collections[0], cax=cbar_ax, orientation=\"vertical\", label=\"Temperature (\\u00b0C)\")\\ncbar_ax.tick_params(labelsize=FONTSIZE)\\n# Also the label should be larger\\ncbar_ax.yaxis.label.set_size(FONTSIZE)\\n\\n# plt.suptitle(\"January temperature predictions by classifier\", fontsize=FONTSIZE, weight=\"bold\", position=(0.5, 0.95))\\n# plt.suptitle(\"April temperature predictions by classifier\", fontsize=FONTSIZE, weight=\"bold\", position=(0.5, 0.95))\\nplt.suptitle(\"July temperature predictions by classifier\", fontsize=FONTSIZE, weight=\"bold\", position=(0.5, 0.95))\\n# plt.suptitle(\"October temperature predictions by classifier\", fontsize=FONTSIZE, weight=\"bold\", position=(0.5, 0.95))\\nplt.tight_layout()\\n# plt.show()\\n# plt.savefig(\"../figures/temp_jan.pdf\", bbox_inches=\"tight\")\\n# plt.savefig(\"../figures/temp_apr.pdf\", bbox_inches=\"tight\")\\nplt.savefig(\"../figures/temp_jul.pdf\", bbox_inches=\"tight\")\\n# plt.savefig(\"../figures/temp_oct.pdf\", bbox_inches=\"tight\")\n```\n\n----------------------------------------\n\nTITLE: Computing Pairwise Manifold Distances (Euclidean) - Python\nDESCRIPTION: This snippet constructs a 10x2 random torch tensor and computes pairwise distances in a Euclidean manifold using geoopt's dist2 method, followed by squarerooting the summed squared distances. It demonstrates usage of geoopt's manifold interface for Euclidean distance calculations. Inputs: random torch tensor _x; Output: pairwise distance matrix. Dependencies are geoopt and torch.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/55_reimplement_gu.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmanifold = geoopt.manifolds.Euclidean()\\n\\n_x = torch.randn(10, 2)\\nmanifold.dist2(_x[:, None, :], _x[None, :, :]).sum(axis=-1) ** 0.5\n```\n\n----------------------------------------\n\nTITLE: Accessing Computed Distances Using Embedders in Python\nDESCRIPTION: This snippet references the object dists, representing computed pairwise distances on the product manifold from previous steps. It should be used after running the Gaussian mixture and distance computation code. The result is a tensor or array of distances as produced by pm.pdist(X).detach().\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/39_reembed_gaussian.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndists\n```\n\n----------------------------------------\n\nTITLE: Analyzing Norm Distribution with Scaled Covariance in Python\nDESCRIPTION: This snippet modifies the sampling process from the previous one by scaling the covariance matrix to counteract the observed growth in norms. It samples points from the `ProductManifold` using `manifold.sample(sigma_factorized=[cov])`, where `cov` is the identity matrix divided by the dimension (`dim`). This aims to keep the variance of the sampled points consistent across dimensions. It then calculates the L2 norm of the spacelike dimensions (1 onwards) and visualizes the norm distribution using a boxplot with a logarithmic y-axis, similar to the previous analysis. Dependencies include `embedders.manifolds`, `matplotlib.pyplot`, `numpy`, and `torch`.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/15_covariance_scaling.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Ok so we see that on a log-log plot, the norms seem to be exponential or polynomial.\n\n# What if we divide everything by exp(dim)?\n\n# Initialize lists to store data\ndimensions = []\nnorms = []\n\n# Sample multiple times for each dimension\nn_samples = 1000\n\nfor dim in [2, 4, 8, 16, 32, 64, 128]:\n    manifold = embedders.manifolds.ProductManifold(signature=[(-1, dim)])\n    \n    for _ in range(n_samples):\n        cov = torch.eye(dim) / torch.tensor(dim)\n        z = manifold.sample(sigma_factorized=[cov])\n        # Calculate the norm of the spacelike dimensions (all but the first)\n        norm = torch.norm(z[0, 1:]).item()\n        \n        dimensions.append(dim)\n        norms.append(norm)\n\n# Create a boxplot\nplt.figure(figsize=(10, 6))\nplt.boxplot([np.array(norms)[np.array(dimensions) == dim] for dim in [2, 4, 8, 16, 32, 64, 128]],\n            labels=[2, 4, 8, 16, 32, 64, 128])\nplt.title('Distribution of Norms Across Dimensions')\nplt.xlabel('Dimension')\nplt.ylabel('Norm of Spacelike Dimensions')\nplt.yscale('log')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Extracting Node Information from New Embedders Tree in Python\nDESCRIPTION: Extracts the feature index and threshold (`theta`) directly from the `nodes` attribute of the `dt_new` tree object (which likely stores nodes in a list format). It prints these values for each node.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/14_new_tree_testing.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n[(x.feature, f\"{x.theta:.4f}\") for x in dt_new.nodes]\n```\n\n----------------------------------------\n\nTITLE: Accessing Test Labels\nDESCRIPTION: This code snippet accesses and displays the test labels (`y_test`) stored within the `embed_data` dictionary, which was created earlier by the `get_embed_data` function.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nembed_data[\"y_test\"]\n```\n\n----------------------------------------\n\nTITLE: Calculating and Verifying Delta Hyperbolicity Measures with Manify\nDESCRIPTION: Demonstrates the computation of delta hyperbolicity using `manify`. It imports necessary functions and classes, sets a random seed for reproducibility, defines a simple hyperbolic manifold (`ProductManifold` with signature `[(-1, 2)]`), and samples points (`X`) from it. It calculates pairwise distances (`dists`) and then computes relative delta hyperbolicity using three methods: iterative (`iterative_delta_hyperbolicity`), vectorized (`delta_hyperbolicity`), and sampled (`sampled_delta_hyperbolicity`). Assertions are used extensively to check the validity of the results (e.g., range of values, shapes of output tensors) and to ensure consistency between the iterative and vectorized approaches, and between the sampled and vectorized results for the selected indices.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/60_pytest_scratch.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nfrom manify.curvature_estimation.delta_hyperbolicity import (\n    iterative_delta_hyperbolicity,\n    sampled_delta_hyperbolicity,\n    delta_hyperbolicity,\n)\nfrom manify.manifolds import ProductManifold\nimport torch\n\n\ntorch.manual_seed(42)\npm = ProductManifold(signature=[(-1, 2)])\nX, _ = pm.sample(z_mean=torch.stack([pm.mu0] * 10))\ndists = pm.pdist(X)\ndists_max = dists.max()\n\n# Iterative deltas\niterative_deltas, gromov_products = iterative_delta_hyperbolicity(dists, relative=True)\nassert (gromov_products >= 0).all()\nassert (gromov_products <= dists_max).all()\nassert (iterative_deltas <= 1).all(), \"Deltas should be in the range [-2, 1]\"\nassert (iterative_deltas >= -2).all(), \"Deltas should be in the range [-2, 1]\"\nassert iterative_deltas.shape == (10, 10, 10)\n\n# Vectorized deltas\nvectorized_deltas = delta_hyperbolicity(dists, full=True, relative=True)\nassert (vectorized_deltas <= 1).all(), \"Deltas should be in the range [-2, 1]\"\nassert (vectorized_deltas >= -2).all(), \"Deltas should be in the range [-2, 1]\"\nassert vectorized_deltas.shape == (10, 10, 10)\nassert torch.allclose(\n    vectorized_deltas, iterative_deltas, atol=1e-5\n), \"Vectorized deltas should be close to iterative deltas.\"\n\n# Sampled deltas\nsampled_deltas, indices = sampled_delta_hyperbolicity(dists, n_samples=10, relative=True)\nassert (sampled_deltas <= 1).all(), \"Sampled deltas should be in the range [-2, 1]\"\nassert (sampled_deltas >= -2).all(), \"Sampled deltas should be in the range [-2, 1]\"\nassert sampled_deltas.shape == (10,), \"There should be 10 sampled deltas\"\nassert torch.allclose(\n    sampled_deltas, vectorized_deltas[indices], atol=1e-5\n), \"Sampled deltas should be close to vectorized deltas.\"\n```\n\n----------------------------------------\n\nTITLE: Setting Up PyTorch Device in Python\nDESCRIPTION: Determines the appropriate PyTorch device for computation based on availability. It prioritizes CUDA GPUs, then Apple Metal Performance Shaders (MPS) on M1/M2 Macs, and falls back to the CPU. It also defines a separate 'sample_device', potentially for data generation, which defaults to the main device unless it's CUDA, in which case it uses the CPU (the rationale for this specific logic isn't fully clear from the snippet alone but might relate to specific library requirements or performance considerations). The selected devices are printed.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/26_benchmark_single_curvature_gaussian.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")\n\nif device != torch.device(\"cuda\"):\n    sample_device = torch.device(\"cpu\")\nelse:\n    sample_device = device\n\nprint(f\"Device: {device}, Sample Device: {sample_device}\")\n```\n\n----------------------------------------\n\nTITLE: Generating LaTeX Table for Link Prediction Results in Python\nDESCRIPTION: This script processes link prediction results stored in the `links` DataFrame. It drops specified columns, groups the data by 'dataset', and applies an aggregation function (`agg`, assumed defined elsewhere) to accuracy columns. It selects relevant accuracy columns for the final table, converts the resulting DataFrame `latex_df` to a LaTeX string, modifies the LaTeX string for proper table formatting (custom header, rule adjustment), and writes the output to `../data/results_icml/table4.tex`. It depends on Pandas, the `links` DataFrame, and the `agg` function.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/32_big_table.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nall_lp = pd.concat([links])\nall_lp = all_lp.drop(columns=[\"d_avg\", \"task\", \"signature\"])\n\nall_lp_grouped = all_lp.groupby([\"dataset\"])[[c for c in all_lp.columns if \"accuracy\" in c]].apply(lambda x: agg(x))\n\n# Sort \"table\" in MultiIndex: Synthetic, Graph, VAE, Other\n# all_lp_grouped = all_lp_grouped.sort_index(level=\"table\", key=lambda x: x.map(sort_dict))\n\n# Get the columns fixed up\nlatex_df = all_lp_grouped[\n    [\n        # \"product_dt_accuracy\",\n        \"product_rf_accuracy\",\n        # \"sklearn_dt_accuracy\",\n        \"sklearn_rf_accuracy\",\n        # \"tangent_dt_accuracy\",\n        \"tangent_rf_accuracy\",\n        \"knn_accuracy\",\n        \"ambient_mlp_accuracy\",\n        \"kappa_gcn_accuracy\",\n    ]\n]\n# c_dict = {\n#     # \"Synthetic (single $K$)\": \"-5.5cm\",\n#     \"Synthetic (multi-$K$)\": \"-2.4cm\",\n#     \"Graphs\": \"-.8cm\",\n#     \"VAE\": \"-.8cm\",\n#     \"Other\": \"-1cm\",\n# }\n# latex_df.index = pd.MultiIndex.from_tuples(\n#     [(f\"\\\\rotatebox{{90}}{{\\\\hspace{{{c_dict[c]}}}{c}}}\", d, s) for c, d, s in latex_df.index],\n#     names=[\"\", \"Dataset\", \"Signature\"],\n# )\nlatex_df\n\n# Now we do text manipulation\nlatex = latex_df.to_latex(header=True, escape=False)\n\n# Remove all occurrences of \"\\cline{3-9}\"\nlatex = latex.replace(\"\\\\cline{1-9}\", \"\")\n# latex = latex.replace(\"\\\\cline{2-9}\", \"\")\n\n# Change top bar\nmy_toprule = \"\"\"\\\\toprule\nDataset & \\col{product_dt}{Product RF} & \\col{euclidean_dt}{Ambient RF} & \\col{tangent_dt}{Tangent RF} \n& \\col{knn}{$k$-Neighbors} & \\col{mlp}{MLP} & \\col{kgcn}{$\\kappa$-GCN}\\\\\n\\\\midrule\"\"\"\nlatex = latex.split(\"\\n\")\nlatex = [latex[0], my_toprule] + latex[5:-4] + latex[-3:]\nlatex = \"\\n\".join(latex)\n\nwith open(\"../data/results_icml/table4.tex\", \"w\") as f:\n    f.write(latex)\n```\n\n----------------------------------------\n\nTITLE: Importing the Embedders Library in Python\nDESCRIPTION: This snippet imports the top-level `embedders` package, making its modules and functions available for subsequent use in the script. This is the standard way to begin using the library's functionalities.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/52_high_dim_embedding.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport embedders\n```\n\n----------------------------------------\n\nTITLE: Loading Cora Dataset using Manify Dataloaders\nDESCRIPTION: Imports the `manify` library and utilizes the `manify.utils.dataloaders.load_hf` function to load the 'cora' dataset. This function is expected to return multiple components of the dataset, assigned here to variables `a`, `b`, `c`, and `d`, likely representing features, distances, labels, and adjacency matrix respectively.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/60_pytest_scratch.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport manify\n\na, b, c, d = manify.utils.dataloaders.load_hf(\"cora\")\n```\n\n----------------------------------------\n\nTITLE: Pairwise Wilcoxon Statistical Tests with Robust Exception Handling (Python)\nDESCRIPTION: This snippet automates pairwise Wilcoxon signed-rank tests across all specified models, handling exceptions by assigning NaN to failed comparisons. It is designed to output clear, labeled statistics, making comparison between classifier performances systematic and robust. Requires a pandas DataFrame named results with model name columns matching the models list. Useful for large-scale or automated robustness checks across many classifier pairs.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/relationship.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n#write wilcoxon results between all values, when encounter expcetion makes the p value 1.0\nmodels = [\"mlp\", \"kappa_mlp\", \"mlr\",\"torchmlr\", \"kappa_mlr\"]\n# Pairwise Wilcoxon signed rank test\nfor i in range(len(models)):\n    for j in range(i + 1, len(models)):\n        try:\n            print(f\"{models[i]} vs {models[j]}: {wilcoxon(results[models[i]], results[models[j]])}\")\n        except:\n            print(f\"{models[i]} vs {models[j]}: NaN\")\n\n```\n\n----------------------------------------\n\nTITLE: Loading Polblogs Dataset\nDESCRIPTION: This snippet loads the Polblogs dataset using the 'load' function from 'embedders.dataloaders'. It retrieves the distance matrix ('polblogs_dists') and node labels ('polblogs_labels').\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/2_polblogs_benchmark.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Load Polblogs\n\npolblogs_dists, polblogs_labels, _ = embedders.dataloaders.load(\"polblogs\")\n```\n\n----------------------------------------\n\nTITLE: Approximating Time Series with Fourier Components in Python\nDESCRIPTION: Imports FFT functions from scipy.fft and configures matplotlib to use LaTeX Times font. It creates two subplots to compare Fourier approximations for Sweeps 33 and 46. For each sweep, it computes the FFT, identifies the indices of the top 20 components by magnitude, reconstructs an approximate signal using only these components via inverse FFT, and plots both the downsampled original signal and the approximation. Extensive plot formatting (titles, labels, spines, ticks) is applied.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/30_fourier.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom scipy.fft import fft, ifft\n\n# Set font to LaTeX times font\nplt.rcParams[\"font.family\"] = \"serif\"\nplt.rcParams[\"font.serif\"] = [\"Times New Roman\"] + plt.rcParams[\"font.serif\"]\n\nfig, axs = plt.subplots(2, 1, figsize=(5.5, 4))#, sharex=True)\nfor ax, n in zip(axs, [33, 46]):\n    # Plot X\n    X = np.array(data[f\"acquisition/timeseries/Sweep_{n}/data\"])\n    # ax.plot(X, linewidth=0.5)\n\n    # Plot X ~ approximated by top 5 fourier components\n    X_fft = fft(X)\n    top_5_idx = np.argsort(np.abs(X_fft))[::-1][:20]\n\n    X_fft_approx = np.zeros_like(X_fft)\n    X_fft_approx[top_5_idx] = X_fft[top_5_idx]\n    X_approx = ifft(X_fft_approx).real\n    ax.plot(X_approx[::100], linewidth=2, color=\"r\", alpha=0.5)\n    ax.plot(X[::100], linewidth=0.5)\n\n    # Bunch of formatting\n    ax.set_title(f\"Sweep {n}\", position=(0.5, 0.9), fontsize=10)\n    ax.set_xlabel(\"Time (ms)\", fontsize=10)\n    ax.set_ylabel(\"Voltage (mV)\", fontsize=10)\n    ax.spines[\"top\"].set_visible(False)\n    ax.spines[\"right\"].set_visible(False)\n    ax.spines[\"bottom\"].set_linewidth(2)\n    ax.spines[\"left\"].set_linewidth(2)\n\n    # Font size for ticks\n    ax.tick_params(axis=\"both\", which=\"major\", labelsize=10)\n    ax.tick_params(axis=\"both\", which=\"minor\", labelsize=10)\n\nplt.tight_layout()\nplt.savefig(\"../figures/fourier_approx.pdf\")\n```\n\n----------------------------------------\n\nTITLE: Merging Benchmark Results with Distance Data using Pandas in Python\nDESCRIPTION: This snippet loads pre-calculated distance data from a TSV file into a Pandas DataFrame, removes the 'signature' column, and merges it with the previously collected benchmark results DataFrame (`results`) based on the 'seed' column. Finally, it groups the merged data by 'signature' and calculates the mean values for each group, providing an aggregated view of performance and distance metrics per manifold signature.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/2_polblogs_benchmark.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# Add distances\ndists = pd.read_table(f\"../data/graphs/embeddings/{DATASET}_dists.tsv\", index_col=0)\ndists = dists.drop(columns=[\"signature\"])\nresults_merged = results.merge(dists, on=[\"seed\"])\nresults_grouped = results_merged.groupby([\"signature\"]).mean()\nresults_grouped\n```\n\n----------------------------------------\n\nTITLE: Defining Function to Create Link Prediction Dataset from Embeddings (Python)\nDESCRIPTION: Defines a function `make_link_prediction_dataset` that converts node embeddings into a dataset suitable for link prediction. It takes node embeddings (`X_embed`), a product manifold object (`pm`), and the graph's adjacency matrix (`adj`). It creates feature vectors by concatenating embeddings of all node pairs. Optionally (`add_dists=True`), it appends the manifold distance between the node pair embeddings to the feature vector. The adjacency matrix entries serve as the target labels (`y`). It returns the feature matrix (`X`), the label tensor (`y`), and a new product manifold (`new_pm`) reflecting the structure of the concatenated features.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/22_link_prediction.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Make link prediction dataset\n\n\ndef make_link_prediction_dataset(X_embed, pm, adj, add_dists=True):\n    # Stack embeddings\n    emb = []\n    for i in range(len(X_embed)):\n        for j in range(len(X_embed)):\n            joint_embed = torch.cat([X_embed[i], X_embed[j]])\n            emb.append(joint_embed)\n\n    X = torch.stack(emb)\n\n    # Add distances\n    if add_dists:\n        dists = pm.pdist(X_embed)\n        X = torch.cat([X, dists.flatten().unsqueeze(1)], dim=1)\n\n    y = torch.tensor(adj.flatten())\n\n    # Make a new signature\n    new_sig = pm.signature + pm.signature\n    if add_dists:\n        new_sig.append((0, 1))\n    new_pm = embedders.manifolds.ProductManifold(signature=new_sig)\n\n    return X, y, new_pm\n```\n\n----------------------------------------\n\nTITLE: Sampling and Factorizing Covariance on ProductManifold (Python)\nDESCRIPTION: This block showcases usage of the ProductManifold class for multi-geometry cases, builds a composite manifold according to a signature, and samples points using factorized and standard covariance matrices. It utilizes pm.factorize to adapt covariances, and demonstrates correct batch seeding. Input includes a ProductManifold instance, covariance matrices, and torch random seed; output is sampled points X1 and X2.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/60_pytest_scratch.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom manify.manifolds import ProductManifold\n\nsignature = [(-1, 2), (0, 2), (1, 2)]\n\npm = ProductManifold(signature=signature)\n\n# get some vectors via gaussian mixture\ncov = torch.eye(pm.dim) / pm.dim / 100\nmeans = torch.vstack([pm.mu0] * 10)\ncovs = torch.stack([cov] * 10)\nsigma_factorized = pm.factorize(covs)\ntorch.random.manual_seed(42)\nX1, _ = M.sample(z_mean=means, sigma_factorized=sigma_factorized)\nX2, _ = M.sample(z_mean=means[:5], sigma=sigma_factorized[:5])\n\n```\n\n----------------------------------------\n\nTITLE: Loading Combined Results Data from TSV using Pandas in Python\nDESCRIPTION: This snippet reads the previously saved aggregated results from the `all_results.tsv` file back into a Pandas DataFrame named `all_data`. The `pd.read_table` function is used, implicitly assuming tab separation for `.tsv` files.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/32_big_table.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nall_data = pd.read_table(\"../data/results_icml/all_results.tsv\")\n```\n\n----------------------------------------\n\nTITLE: Visualizing Product vs. Sklearn Random Forest Performance with Matplotlib in Python\nDESCRIPTION: This Python snippet utilizes Matplotlib to generate a scatter plot comparing the performance scores of a Product Random Forest (`product_rf`) with a standard Scikit-learn Random Forest (`sklearn_rf`). It maps `product_rf` scores to the x-axis and `sklearn_rf` scores to the y-axis. The plot includes axis labels, a descriptive title showing dataset, score type, and average scores for both models, a diagonal y=x line for reference, and is then displayed. It depends on the `results` and `results_grouped` DataFrames.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/2_polblogs_benchmark.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n\nplt.scatter(results[\"product_rf\"], results[\"sklearn_rf\"])\nplt.ylabel(\"Sklearn RF\")\nplt.xlabel(\"Product RF\")\nplt.plot([mymin, mymax], [mymin, mymax], label=\"y=x\")\nplt.title(\n    f\"{DATASET.capitalize()} - {SCORE} \\n ProductRF {results_grouped['product_rf'].mean():.3f} vs SklearnRF {results_grouped['sklearn_rf'].mean():.3f}\"\n)\nplt.legend()\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Verifying Log-Likelihood Calculation on Simple Product Manifolds\nDESCRIPTION: Repeats the log-likelihood verification process, but this time uses the `ProductManifold` class initialized with a single component manifold `(K, 4)` for various curvatures `K`. It tests if the `log_likelihood` method works correctly for product manifolds, comparing the likelihood under the default prior (P(z)) and the generating distribution (Q(z)).\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/5_sampling.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Does the same thing work with product manifolds?\n\n# Verify that log-likelihood functions work:\nfor K in [-2, -1.0, -0.5, 0, 0.5, 1.0, 2.0]:\n    print(K)\n    m = ProductManifold(signature=[(K, 4)])\n    # Pick a random point to use as the center\n    mu = m.sample(m.mu0)\n    Sigma = torch.diag(torch.randn(m.dim)) ** 2\n    samples = m.sample(z_mean=torch.cat([mu] * N_SAMPLES, dim=0), sigma=Sigma)\n    log_probs_p = m.log_likelihood(z=samples)  # Default args\n    log_probs_q = m.log_likelihood(z=samples, mu=mu, sigma=Sigma)\n    print(\n        f\"Shape: {log_probs_p.shape},\\tP(z) = {log_probs_p.mean().item():.3f},\\tQ(z) = {log_probs_q.mean().item():.3f},\\tQ(z) - P(z) = {log_probs_q.mean().item() - log_probs_p.mean().item():.3f}\"\n    )\n    print()\n\n    # Why don't we generally see Q(z) - P(z) > 0? I would think the ll of the true distribution would be higher than the ll of the wrong distribution...\n```\n\n----------------------------------------\n\nTITLE: Displaying Product Manifold Ambient Dimension\nDESCRIPTION: Accesses and displays the `ambient_dim` attribute of the `ProductManifold` instance `pm` created in the last iteration of the preceding KL divergence check loop (corresponding to K=2.0). The ambient dimension is typically the sum of the dimensions of the embedding spaces for each component manifold.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/5_sampling.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\npm.ambient_dim\n```\n\n----------------------------------------\n\nTITLE: Accessing Specific Angle Dimensions in ProductSpaceDT in Python\nDESCRIPTION: Retrieves the pair of dimension indices associated with the feature index 16 from the `angle_dims` attribute of the trained `ProductSpaceDT` object `pdt`. This pair represents the dimensions used for the angular split corresponding to feature 16.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_basic_visualization.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npdt.angle_dims[16]\n```\n\n----------------------------------------\n\nTITLE: Computing Shape of Batched Inner Products (Python)\nDESCRIPTION: Evaluates the shape of a batched inner product computed across all pairs of data points, using broadcasting. Shows how to check the output shape when computing kernel matrices with custom inner functions in embedders. Inputs: _x (feature matrix); output: a tuple shape object.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\npm.P[0].manifold.inner(_x[:, None], _x[None, :]).shape\n```\n\n----------------------------------------\n\nTITLE: Importing Embedders and PyTorch Libraries - Python\nDESCRIPTION: Imports the custom 'embedders' module alongside the 'torch' library to provide manifold operations and tensor computation capabilities, respectively. Assumes 'embedders' is available in the Python path and that PyTorch is installed. This snippet sets up the environment for further embedding and manifold manipulation tasks.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/3_verify_shapes.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport embedders\nimport torch\n```\n\n----------------------------------------\n\nTITLE: Checking KL Divergence Calculation for Product Manifolds\nDESCRIPTION: Verifies the KL divergence calculation for `ProductManifold` instances. It imports `ProductSpaceVAE` from `embedders.vae`. For various curvatures `K`, it defines a simple product manifold `[(K, 4)]`, samples a mean `mu` and covariance `Sigma`, creates a dummy `ProductSpaceVAE` instance (with None for encoder/decoder), and then calls the `kl_divergence` method with the sampled `mu` and `Sigma`. The calculated KL divergence is printed for each curvature.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/5_sampling.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Check that KL divergence is working:\nfrom embedders.vae import ProductSpaceVAE\n\nfor K in [-2, -1.0, -0.5, 0, 0.5, 1.0, 2.0]:\n    print(K)\n    pm = ProductManifold(signature=[(K, 4)])\n    # Pick a random point to use as the center\n    mu = pm.sample(pm.mu0)\n    Sigma = torch.diag(torch.randn(pm.dim)) ** 2\n\n    pm = product_manifold = ProductManifold(signature=[(K, 4)])\n    pvae = ProductSpaceVAE(product_manifold=pm, encoder=None, decoder=None)\n    kl = pvae.kl_divergence(z_mean=mu, sigma=Sigma, n_samples=16)\n    print(f\"KL = {kl.mean().item():.3f}\")\n    print()\n```\n\n----------------------------------------\n\nTITLE: Finding Closest Point to Origin on a Circle Given Center and Radius - Python\nDESCRIPTION: Computes the closest point to the origin on a specified circle by parameterizing the line from the origin to the circle center and using the quadratic formula to solve for the correct scale. Inputs are given as center position and radius (torch tensors/numbers); output is the coordinates of the closest point. Relies on torch for computation. Constraints: Only works in Euclidean space and assumes a nonzero center.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/36_poincare_circles.ipynb#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n# On the circle with center [2.2841, 1.1386] with radius 2.3481, what is the point closest to (0, 0)?\\n\\n# Well, it lies on the line from the origin to the center, so we can just scale the center by the radius\\ncenter = torch.tensor([2.2841, 1.1386])\\nradius = 2.3481\\n\\n# The line from the origin to the center\\n_vals = torch.linspace(0, 1, 100).unsqueeze(1)\\n_y = _vals * center\\n\\n# Which point has distance to center equal to the radius?\\n# Use quadratic formula to get the norm of your point\\n# A = 1\\n# B = -2\\n# C = 1 - radius**2 / center.norm()**2\\nA = 1\\nB = -2\\nC = 1 - radius**2 / center.norm()**2\\ndiscriminant = B**2 - 4 * A * C\\nt = (-B - torch.sqrt(discriminant)) / (2 * A)\\npoint = t * center\\nprint(point)\\n\n```\n\n----------------------------------------\n\nTITLE: Loading, Aggregating, and Styling Experiment Data with Pandas in Python\nDESCRIPTION: This snippet loads data from TSV files specified by the `PATHS` dictionary using pandas. It defines an aggregation function `aggfunc` to calculate the mean and 95% confidence interval for a given metric, formatting the output as a string. It also defines a styling function `stylefunc` (though not directly used for the final saved image) to highlight max/min values. The code reads data for each path, concatenates them, selects relevant columns based on the `VAR` variable, groups the data by type, dataset, and signature, and applies the `aggfunc` for aggregation. The final line applies the `stylefunc` to the grouped DataFrame for potential interactive display.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/57_new_table.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\n\n\ndef aggfunc(x):\n    # Get 95% CI on percentages\n    mean = np.mean(x)\n    ci = 1.96 * np.std(x) / np.sqrt(len(x))\n    if VAR in [\"accuracy\", \"f1-macro\"]:\n        mean, ci = mean * 100, ci * 100\n    if mean < 1e3:\n        return f\"{mean:.2f} \\u00b1 {ci:.2f}\"\n    else:\n        return f\"{mean:.2e} \\u00b1 {ci:.2e}\"\n\n\ndef stylefunc(x):\n    is_max = pd.Series(data=False, index=x.index)\n    means = x.str.split(\"\\u00b1\").str[0].astype(float)\n    is_max[means.idxmax()] = True if PATHS == CLS_PATHS else False\n    is_max[means.idxmax()] = True if PATHS == HIGHDIM_PATHS else False\n    is_max[means.idxmin()] = True if PATHS == REG_PATHS else False\n    return [\"background-color: lightgreen\" if v else \"\" for v in is_max]\n\n\ndf = []\nfor name, path in PATHS.items():\n    df_path = pd.read_table(path)\n    df_path[\"type\"] = name\n    df_path = df_path[[c for c in df_path.columns if VAR in c or c in [\"type\", \"dataset\", \"signature\"]]]\n    df_path.columns = [c.replace(f\"_{VAR}\", \"\") for c in df_path.columns]\n    df.append(df_path)\n\ndf = pd.concat(df)\ngrouped_df = df.groupby([\"type\", \"dataset\", \"signature\"]).agg(aggfunc)\ngrouped_df.style.apply(stylefunc, axis=1)\n\n```\n\n----------------------------------------\n\nTITLE: Plotting Data Points and a Decision Boundary Line in Python\nDESCRIPTION: Uses `matplotlib.pyplot` and `numpy` to create a scatter plot of the data points `X`, specifically using dimensions 9 and 11. Points are colored based on their labels `y`. A red line representing a decision boundary based on a specific angle `theta` (2.141) is overlaid. The plot limits are adjusted to fit the data range.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_basic_visualization.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.scatter(X[:, 9].numpy(), X[:, 11].numpy(), c=y)\n\n# plot decision boundary\ntheta = 2.141489028930664\nc = np.cos(theta)\ns = np.sin(theta)\nplt.plot([-s * 1e10, s * 1e10], [-c * 1e10, c * 1e10], \"r-\")\n\n# put boundaries back\nplt.xlim(X[:, 9].min(), X[:, 9].max())\nplt.ylim(X[:, 11].min(), X[:, 11].max())\n```\n\n----------------------------------------\n\nTITLE: Extracting All Time Series Sweeps from NWB Data in Python\nDESCRIPTION: Iterates through sweep numbers 1 to 62, attempts to access the corresponding time series data within the NWB file structure ('acquisition/timeseries/Sweep_n/data'), converts it to a numpy array, and appends it to the 'all_ts' list. It includes error handling for missing sweeps and prints the shapes of the extracted arrays.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/30_fourier.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# np.array([data[f\"acquisition/timeseries/Sweep_{n}/data\"] for n in range(1, 63)])\nall_ts = []\nfor n in range(1, 63):\n    try:\n        all_ts.append(np.array(data[f\"acquisition/timeseries/Sweep_{n}/data\"]))\n    except:\n        pass\n[x.shape for x in all_ts]\n```\n\n----------------------------------------\n\nTITLE: Accessing All Angle Dimensions in ProductSpaceDT in Python\nDESCRIPTION: Accesses and implicitly prints the entire `angle_dims` attribute of the trained `ProductSpaceDT` object `pdt`. This attribute likely stores a mapping or list connecting feature indices used in the tree to the pairs of original data dimensions involved in the angular comparisons.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_basic_visualization.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npdt.angle_dims\n```\n\n----------------------------------------\n\nTITLE: Computing and Printing a Geodesic Arc with New Points - Python\nDESCRIPTION: Uses the previously defined find_geodesic function to compute the geodesic arc between two given points and prints a LaTeX string for drawing the arc. Provides an explicit use case with set tensor coordinates. Inputs are torch tensors for coordinates; output is a formatted LaTeX string. Depends on the presence of find_geodesic definition.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/36_poincare_circles.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\np1 = torch.tensor([0.6227, -0.5207])\\np2 = torch.tensor([-0.0439, 0.8321])\\nprint(f\"\\\\draw[thick, color=green] {find_geodesic(p1, p2)};\")\\n\n```\n\n----------------------------------------\n\nTITLE: Generating Random Vector for Feature Space (Python)\nDESCRIPTION: Creates a random vector g with the same width as the number of features in X using PyTorch rand. Used for sampling or initializing weights in downstream operations. No dependencies beyond torch. Outputs: random vector tensor.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\ng = torch.rand(X.shape[1])\ng\n```\n\n----------------------------------------\n\nTITLE: Generating Manifold-Embedded Gaussian Mixture Data (Python)\nDESCRIPTION: This snippet initializes a product manifold with signature [(1, 2), (-1, 2)] and generates synthetic Gaussian mixture data with 32 clusters and 8 classes via 'embedders.gaussian_mixture'. It requires the embedders module and returns feature data (X) and labels (y) suitable for classification experiments.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/41_more_benchmarks.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport embedders.gaussian_mixture\\n\\npm = embedders.manifolds.ProductManifold(signature=[(1, 2), (-1, 2)])\\nX, y = embedders.gaussian_mixture.gaussian_mixture(pm, num_clusters=32, num_classes=8)\n```\n\n----------------------------------------\n\nTITLE: Checking Data Shape in Python using NumPy/Torch\nDESCRIPTION: Accesses and implicitly prints the shape (dimensions) of the generated dataset `X`. This is likely a NumPy array or a PyTorch tensor.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_basic_visualization.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nX.shape\n```\n\n----------------------------------------\n\nTITLE: Analyzing Feature Usage in Trained scikit-learn Decision Tree Models\nDESCRIPTION: Extracts the feature indices used in the splits of each trained `sklearn.tree.DecisionTreeClassifier` stored in the `trees` list (from Snippet 6). It accesses the features via `tree.tree_.feature`, filtering out terminal nodes (where feature is -2). It then calculates and prints the proportion of features used by each tree that fall within the expected dimension range corresponding to the manifold signature associated with its target labels (note the different dimension ranges compared to the previous analysis).\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/40_component_interp.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Get features for each split in trained sklearn decision tree\n\n# for tree in trees:\n#     print(tree.tree_.feature)\n\nfeats = []\nfor tree in trees:\n    feats.append([x for x in tree.tree_.feature if x != -2])\n\n# Which ones fall in the right dim:\nfor feat_set, allowed in zip(feats, [[0, 1, 2], [3, 4, 5], [6, 7]]):\n    total = len(feat_set)\n    correct = sum([f in allowed for f in feat_set])\n    print(f\"{correct/total} correct\")\n```\n\n----------------------------------------\n\nTITLE: Sanity Checking Sample Distributions on Manifolds with Varying Curvature\nDESCRIPTION: Performs a sanity check by sampling points from manifolds with different curvatures (K values from -4 to 4). It calculates the distance of these samples from the manifold's origin (`mu0`) and plots histograms of these distances for each curvature, comparing them to a control Euclidean normal distribution. This aims to verify that the sampling distribution behaves correctly across different geometries.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/5_sampling.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Sanity check: can we get the right distribution of samples for different scales?\n\nN_SAMPLES = 10_000\n\nall_dists = {}\nfor K in [-4, -2, -1.0, -0.5, 0, 0.5, 1.0, 2.0, 4.0]:\n    print(K)\n    m = Manifold(K, 4)\n    samples = m.sample(torch.cat([m.mu0] * N_SAMPLES, dim=0))\n    dists = m.dist(m.mu0, samples)\n    all_dists[K] = dists\n\nall_dists[\"control\"] = torch.norm(torch.randn(size=(N_SAMPLES, 4)), dim=1)\n\n# Get histograms with torch and line-plot them\nfor K, dists in all_dists.items():\n    dist_hist = torch.histogram(dists, bins=25, range=(0, 4))\n    plt.plot(dist_hist.bin_edges[:-1].detach().cpu().numpy(), dist_hist.hist.detach().cpu().numpy(), label=f\"K={K}\")\nplt.legend()\n\n# Great, this looks right: all distances match, except where we totally run out of space on our manifold\n# (e.g. K=2; K=1 is starting to curve off a little bit near d=pi...)\n```\n\n----------------------------------------\n\nTITLE: Adding and Configuring External Legend Axis (Matplotlib, Python)\nDESCRIPTION: This snippet creates a dedicated legend axis to the right of the main figure using Matplotlib's add_axes, and populates it with custom colored line handles and model names to distinguish models in the boxplot. It disables tick/axes display and configures legend aesthetics and position for optimal figure clarity. Inputs: figure object, colors and model names; Outputs: a readable, external legend. Requires Matplotlib.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/26_benchmark_single_curvature_gaussian.ipynb#_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nlegend_ax = fig.add_axes([1.05, 0.15, 0.02, 0.7])\nlegend_ax.axis(\"off\")\nlegend_ax.legend(\n    # [bp[\"boxes\"][0] for bp in bps],\n    [mpl.lines.Line2D([0], [0], color=color, lw=LW) for color in colors],\n    # [mpl.text.Text(\"\", color=color) for color in colors],\n    [\"Product RF\", \"Ambient RF\", \"Tangent RF\", \"$k$-Neighbors\", \"MLP\", \"$\\kappa$-GCN\"],\n    # [\"Product RF\", \"Ambient RF\", \"Tangent RF\", \"$k$-Neighbors\"],\n    fontsize=FONTSIZE,\n    frameon=False,\n    title=\"Model\",\n    loc=\"center\",\n    title_fontproperties={\"weight\": \"bold\", \"size\": FONTSIZE},\n    labelcolor=colors,\n)\n\n```\n\n----------------------------------------\n\nTITLE: Applying PCA to MNIST Data and Visualizing in Python\nDESCRIPTION: Performs Principal Component Analysis (PCA) with scikit-learn to reduce MNIST image vectors to 6 dimensions, reshaping images as needed. Plots the projection onto the first two principal components with colors representing the labels. Requires sklearn, matplotlib, and expects X to be convertible to (n_samples, 784) via reshape and numpy(). Outputs a scatter plot to reveal class separability in reduced space.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/38_mnist.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Take top 6 principal components\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=6)\nX_pca = pca.fit_transform(X.reshape(-1, 28*28).numpy())\n\nplt.scatter(X_pca[:,0], X_pca[:,1], c=labels)\n```\n\n----------------------------------------\n\nTITLE: Perform MDS on Graph Distances and Visualize\nDESCRIPTION: This snippet calculates pairwise shortest path distances between nodes using the Floyd-Warshall algorithm on a graph created from the UCI adjacency matrix (`networkx`). It then applies Multidimensional Scaling (MDS) from `sklearn.manifold` to reduce the dimensionality of these distances to 2D coordinates and plots the resulting coordinates using `matplotlib`.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/33_traffic.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Adjacency matrix -> nx graph -> pairwise dists -> MDS coords in R2\n\nimport networkx as nx\nimport numpy as np\nfrom sklearn.manifold import MDS\n\nG = nx.from_numpy_array(data[\"tra_adj_mat\"])\ndists = nx.floyd_warshall_numpy(G)\ndists = np.array([[dists[i][j] for j in range(len(dists))] for i in range(len(dists))])\ndists\n\nmds = MDS(n_components=2, dissimilarity=\"precomputed\")\ncoords = mds.fit_transform(dists)\ncoords\n\nplt.scatter(coords[:, 0], coords[:, 1])\n```\n\n----------------------------------------\n\nTITLE: Calculating Information Gains with PyTorch Tensors\nDESCRIPTION: This snippet implements the vectorized calculation of information gains using PyTorch tensors. It first one-hot encodes the labels `y`. Then, it uses the `comparisons_reshaped` tensor (converted to float) and matrix multiplication (`@`) to compute the sum of labels belonging to the left and right partitions for each potential split. From these counts, it calculates the total counts, probabilities for each class in each partition, and the Gini impurity for left, right, and total sets. Finally, it computes the information gain (`ig_est`) based on the Gini impurities and verifies the result against the pre-computed ground truth (`info_gains`) using `torch.allclose` after handling potential NaNs.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/13_information_gain.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Ok, great. Now how to transform our tensor into information gains?\n\n# First, we need to one-hot the y-values\ny_onehot = torch.nn.functional.one_hot(y, num_classes=2).double()\nprint(f\"y_onehot: {y_onehot.shape}\")\n\n# Selecting a split is the same thing as multiplying by a vector from comparisons_reshaped\n# Our comparisons are already reshaped so that the last dim is the first dim of the labels, so we can use\n# batch matrix multiplication instead of einsum\ncomparisons_float = comparisons_reshaped.double()\nleft_labels = comparisons_float @ y_onehot\nright_labels = (1 - comparisons_float) @ y_onehot\nprint(f\"Left labels: {left_labels.shape}\")\nprint(f\"Right labels: {right_labels.shape}\")\n\n# What we now have is effectively a tensor of \"thinned-out\" counts of our y-values. Implicitly, we first turn some of\n# our so-called \"one-hot encoded\" y-values into zero across all classes. This has a convenient property that we can just\n# sum them up to get our class counts per split, which the matmul does.\n\n# Total counts are sums of label counts\ndim = 2\neps = 1e-10\nn_left = left_labels.sum(dim=dim) + eps\nn_right = right_labels.sum(dim=dim) + eps\nn_total = n_left + n_right\nprint(f\"n_left: {n_left.shape}\")\nprint(f\"n_right: {n_right.shape}\")\nprint(f\"n_total: {n_total.shape}\")\n\n# Probabilities are label counts divided by total counts\nleft_probs = left_labels / n_left.unsqueeze(dim)\nright_probs = right_labels / n_right.unsqueeze(dim)\ntotal_probs = (left_labels + right_labels) / n_total.unsqueeze(dim)\nprint(f\"Left probs: {left_probs.shape}\")\nprint(f\"Right probs: {right_probs.shape}\")\nprint(f\"Total probs: {total_probs.shape}\")\n\n# Gini impurity is 1 - sum(prob^2)\ngini_left = 1 - (left_probs**2).sum(dim=dim)\ngini_right = 1 - (right_probs**2).sum(dim=dim)\ngini_total = 1 - (total_probs**2).sum(dim=dim)\n# TODO: can we get rid of gini_total? It's always the same for the first split, but we may want it for later splits\n# TODO: can we also get rid of the nan_to_num step? Currently we need it because we divide by zero\nprint(f\"Gini left: {gini_left.shape}\")\nprint(f\"Gini right: {gini_right.shape}\")\nprint(f\"Gini total: {gini_total.shape}\")\n\n# Information gain is the total gini impurity minus the weighted average of the new gini impurities\nig_est = gini_total - (gini_left * n_left + gini_right * n_right) / n_total\nprint(f\"IG: {ig_est.shape}\")\n\nassert ig_est.isnan().sum() == 0  # Ensure no NaNs\ninfo_gains_nonan = info_gains.clone().nan_to_num(0)\nassert torch.allclose(ig_est, info_gains_nonan[:, 1:], atol=1e-2)  # Ensure the values are the same\nprint(\"Verification passed.\")\n```\n\n----------------------------------------\n\nTITLE: Training Coordinates on a Product Manifold with Embedders in Python\nDESCRIPTION: This snippet demonstrates the core coordinate learning process. It first sets a learning rate (`LR`) and normalizes the distance matrix (`dists`). Then, it defines a `ProductManifold` (`pm`) with a specific signature (e.g., `[(-1, 16)]` likely signifies a 16-dimensional hyperbolic component). Finally, it calls `embedders.coordinate_learning.train_coords` to learn the node coordinates (`X`) on the specified manifold, using the normalized distances and configured learning parameters (learning rate, burn-in rate, scale).\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/52_high_dim_embedding.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nLR = 1e-2\n\ndists_normalized = (dists / dists[dists.isfinite()].max()).float()\n\n\npm = embedders.manifolds.ProductManifold(signature=[(-1, 16)])\n\nX, _ = embedders.coordinate_learning.train_coords(\n    pm=pm, dists=dists_normalized, learning_rate=LR, burn_in_learning_rate=LR * 0.1, scale=1e-4\n)\n```\n\n----------------------------------------\n\nTITLE: Batch Training and Interpretation for Blood Cell Embeddings with Embedders in Python\nDESCRIPTION: This block automates the loading, downsampling, model fitting, and interpretability analysis across 10 trials of blood cell scRNA embeddings. It sequentially loads data for each trial, samples 1000 points, fits a ProductSpaceDT using a consistent manifold signature, and summarizes node-manifold assignments with a Counter. This approach promotes robustness via repetition and provides insight into the consistency of model structure across experiments. Requires numpy, embedders, and collections.Counter.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/45_blood_and_lymphoma_interp.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n# Do this for all samples\nembedding = \"blood_cell_scrna\"\nsig = [(1, 2), (0, 2), (-1, 2), (-1, 2), (-1, 2)]\npm = embedders.manifolds.ProductManifold(signature=sig)\n\n\nfor i in range(10):\n    X_train = np.load(f\"../data/{embedding}/embeddings/X_train_{i}.npy\")\n    y_train = np.load(f\"../data/{embedding}/embeddings/y_train_{i}.npy\")\n    X_test = np.load(f\"../data/{embedding}/embeddings/X_test_{i}.npy\")\n    y_test = np.load(f\"../data/{embedding}/embeddings/y_test_{i}.npy\")\n\n    train_downsample = np.random.choice(X_train.shape[0], 1000, replace=False)\n    test_downsample = np.random.choice(X_test.shape[0], 1000, replace=False)\n\n    X_train = X_train[train_downsample]\n    y_train = y_train[train_downsample]\n\n    pdt = embedders.predictors.tree_new.ProductSpaceDT(pm=pm, max_depth=3, n_features=\"d_choose_2\")\n    pdt.fit(X_train, y_train)\n\n    print(Counter([pm.dim2man[pdt.angle_dims[node.feature][1]] for node in pdt.nodes if node.feature is not None]))\n```\n\n----------------------------------------\n\nTITLE: Plotting ROC Curves and Performance Comparison - Python\nDESCRIPTION: Generates comparative plots of the mean ROC curves for both Decision Tree (DT) and Product Space Decision Tree (PDT) models, including confidence intervals computed over multiple trials. The snippet leverages matplotlib to show differences in classifier performance, with axis labeling and legends for clarity. Requires all computed ROC and AUC arrays from the experiment stage as inputs and matplotlib as a plotting backend.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/20_roc_auc.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Plots\\n\\nplt.plot(\\n    fprs_dt.mean(axis=0), tprs_dt.mean(axis=0), label=f\\\"DT (AUC={aucs_dt.mean():.4f})\\\"\\n)\\nplt.fill_between(\\n    fprs_dt.mean(axis=0),\\n    tprs_dt.mean(axis=0) - tprs_dt.std(axis=0) / np.sqrt(N_TRIALS),\\n    tprs_dt.mean(axis=0) + tprs_dt.std(axis=0) / np.sqrt(N_TRIALS),\\n    alpha=0.2\\n)\\nplt.plot(\\n    fprs_pdt.mean(axis=0), tprs_pdt.mean(axis=0), label=f\\\"PDT (AUC={aucs_pdt.mean():.4f}\\\"\\n)\\nplt.fill_between(\\n    fprs_pdt.mean(axis=0),\\n    tprs_pdt.mean(axis=0) - tprs_pdt.std(axis=0) / np.sqrt(N_TRIALS),\\n    tprs_pdt.mean(axis=0) + tprs_pdt.std(axis=0) / np.sqrt(N_TRIALS),\\n    alpha=0.2\\n)\\nplt.xlabel(\\\"False positive rate\\\")\\nplt.ylabel(\\\"True positive rate\\\")\\nplt.legend()\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating a Product Space Decision Tree in Python\nDESCRIPTION: Splits the generated data (`X`, `y`) into training and testing sets using `sklearn.model_selection.train_test_split`. Initializes a `ProductSpaceDT` classifier from the `embedders` library, configured for the defined product manifold (`pm`), using 'd_choose_2' feature selection strategy, and a maximum depth of 3. Trains the classifier (`pdt`) on the training data and prints its accuracy score on the test set.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_basic_visualization.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\npdt = embedders.predictors.tree_new.ProductSpaceDT(pm=pm, n_features=\"d_choose_2\", max_depth=3)\npdt.fit(X_train, y_train)\n\nprint(f\"{pdt.score(X_test, y_test).float().mean().item():.4f}\")\n```\n\n----------------------------------------\n\nTITLE: Setting up IPython Autoreload\nDESCRIPTION: Uses IPython magic commands to automatically reload modules before executing code. `%load_ext autoreload` loads the extension, and `%autoreload 2` configures it to reload all modules except those explicitly excluded.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/1_basic_test.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Performing Tensor Summation and Argmax in Python\nDESCRIPTION: Creates a random tensor `_xx` of shape (100, 5). It then calculates the sum along dimension 0, normalizes it by the total sum of the tensor, and finds the index of the maximum value using `argmax`. This is a standard tensor manipulation sequence.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/14_new_tree_testing.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n_xx = torch.randn(100, 5)\n\n(_xx.sum(dim=0) / _xx.sum()).argmax()\n```\n\n----------------------------------------\n\nTITLE: Visualizing `angular_greater` Comparison Matrix in Python\nDESCRIPTION: Visualizes the boolean result matrix obtained from `embedders.tree_new.angular_greater(_x, _x)` using `matplotlib.pyplot.matshow`. The axes are labeled with angle values in terms of pi to show the comparison results between all pairs of angles in the generated range `_x`. This helps understand the function's behavior, especially around the 0/2*pi boundary.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/14_new_tree_testing.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nplt.matshow(embedders.tree_new.angular_greater(_x, _x).detach().numpy().astype(int))\nplt.xticks(range(len(_x))[::10], [f\"{x.item() / torch.pi:.1f}pi\" for x in _x[::10]], rotation=90)\nplt.yticks(range(len(_x))[::10], [f\"{x.item() / torch.pi:.1f}pi\" for x in _x[::10]])\nplt.xlabel(\"Key\")\nplt.ylabel(\"Query\")\nplt.colorbar()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Initializing Node Property Prediction Dataset - OGB - Python\nDESCRIPTION: This snippet constructs an instance of the NodePropPredDataset for the 'ogbn-products' dataset using the ogb.nodeproppred module. It requires that the OGB package is imported and installed. The resulting object ('prods') provides interfaces for accessing dataset splits, graph structures, and property labels for downstream node property prediction tasks. The 'name' parameter specifies which OGB dataset to load; here, it loads the 'ogbn-products' benchmark.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/19_opengraphbenchmark.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nprods = ogb.nodeproppred.NodePropPredDataset(name=\\\"ogbn-products\\\")\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating Scikit-learn DecisionTreeRegressor in Python\nDESCRIPTION: Instantiates a standard `DecisionTreeRegressor` from the `sklearn.tree` module with a maximum depth of 5. Trains the model using the NumPy versions of the generated data (`X_np`, `y_np`), predicts target values (`y_pred`), calculates and prints its score, and visualizes the predictions using `matplotlib`. This serves as a baseline comparison for the `ProductSpaceDT`.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/17_verify_new_mse.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.tree import DecisionTreeRegressor\n\ndt = DecisionTreeRegressor(max_depth=5)\ndt.fit(X_np, y_np)\n\ny_pred = dt.predict(X_np)\nprint(dt.score(X_np, y_np))\nplt.scatter(X_np[:, 0], X_np[:, 1], c=y_pred)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Feature Usage in Trained ProductSpaceDT Models\nDESCRIPTION: This snippet appears identical to a previous one and likely intends to analyze the features from the `ProductSpaceDT` models trained earlier (Snippet 4). It extracts feature indices from decision nodes (`tree.nodes`) and checks their alignment with expected dimensions based on the original manifold signatures. Note: If run immediately after the scikit-learn tree training (Snippet 6), `trees` would contain `DecisionTreeClassifier` objects, for which `tree.nodes` is not a valid attribute, suggesting this might be misplaced or intended for the `ProductSpaceDT` results.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/40_component_interp.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfeats = []\nfor tree in trees:\n    feats.append([x.feature for x in tree.nodes if x.feature])\n\n# Which ones fall in the right dim:\nfor feat_set, allowed in zip(feats, [[0, 1], [2, 3], [4, 5]]):\n    total = len(feat_set)\n    correct = sum([f in allowed for f in feat_set])\n    print(f\"{correct/total} correct\")\n```\n\n----------------------------------------\n\nTITLE: Generating LaTeX Table for Classification Results in Python\nDESCRIPTION: This script processes a Pandas DataFrame `all_c_grouped` containing classification results. It sorts the DataFrame's MultiIndex based on a predefined `sort_order`, selects specific accuracy columns, formats the index labels with LaTeX rotation and spacing using `c_dict`, converts the DataFrame to a LaTeX string using `to_latex()`, performs string replacements to adjust LaTeX table rules and headers, and finally writes the resulting LaTeX code to `../data/results_icml/table2.tex`. It depends on the Pandas library and a pre-existing DataFrame `all_c_grouped`.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/32_big_table.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Sort \"table\" in MultiIndex: Synthetic, Graph, VAE, Other\nsort_order = [\"Synthetic (multi-$K$)\", \"Graphs\", \"VAE\", \"Other\"]\nsort_dict = {name: i for i, name in enumerate(sort_order)}\nall_c_grouped = all_c_grouped.sort_index(level=\"table\", key=lambda x: x.map(sort_dict))\n\n# Get the columns fixed up\nlatex_df = all_c_grouped[\n    [\n        \"product_rf_accuracy\",\n        \"sklearn_rf_accuracy\",\n        \"tangent_rf_accuracy\",\n        \"knn_accuracy\",\n        \"ambient_mlp_accuracy\",\n        \"kappa_gcn_accuracy\",\n    ]\n]\nc_dict = {\n    # \"Synthetic (single $K$)\": \"-5.5cm\",\n    \"Synthetic (multi-$K$)\": \"-2.4cm\",\n    \"Graphs\": \"-.8cm\",\n    \"VAE\": \"-.8cm\",\n    \"Other\": \"-.7cm\",\n}\nlatex_df.index = pd.MultiIndex.from_tuples(\n    [(f\"\\\\rotatebox{{90}}{{\\\\hspace{{{c_dict[c]}}}{c}}}\", d, s) for c, d, s in latex_df.index],\n    names=[\"\", \"Dataset\", \"Signature\"],\n)\n\n# Now we do text manipulation\nlatex = latex_df.to_latex(header=True, escape=False)\n\n# Remove all occurrences of \"\\cline{3-9}\"\n# latex = latex.replace(\"\\\\cline{1-9}\", \"\")\nlatex = latex.replace(\"\\\\cline{2-9}\", \"\")\n\n# Change top bar\nmy_toprule = \"\"\"\\\\toprule\n& Dataset & Signature & \\col{product_dt}{Product RF} & \\col{euclidean_dt}{Ambient RF} & \\col{tangent_dt}{Tangent RF} \n& \\col{knn}{$k$-Neighbors} & \\col{mlp}{MLP} & \\col{kgcn}{$\\kappa$-GCN} \\\\\n\\\\midrule\"\"\"\nlatex = latex.split(\"\\n\")\nlatex = [latex[0], my_toprule] + latex[5:-4] + latex[-3:]\nlatex = \"\\n\".join(latex)\n\nwith open(\"../data/results_icml/table2.tex\", \"w\") as f:\n    f.write(latex)\n```\n\n----------------------------------------\n\nTITLE: Visualizing `circular_greater` Comparison Matrix in Python\nDESCRIPTION: Visualizes the comparison matrix for the older `embedders.tree.circular_greater` function using the same range of angles `_x`. This allows for a direct visual comparison between the old and new angular comparison methods implemented in the `embedders` library.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/14_new_tree_testing.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nplt.matshow(np.array([embedders.tree.circular_greater(_x, x).detach().numpy().astype(int) for x in _x]))\nplt.xticks(range(len(_x))[::10], [f\"{x.item() / torch.pi:.1f}pi\" for x in _x[::10]], rotation=90)\nplt.yticks(range(len(_x))[::10], [f\"{x.item() / torch.pi:.1f}pi\" for x in _x[::10]])\nplt.xlabel(\"Key\")\nplt.ylabel(\"Query\")\nplt.colorbar()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Accessing a Specific Kernel Matrix Component (Python)\nDESCRIPTION: Retrieves the third kernel component (index 2) from the output of the product_kernel function, given the same dataset for both arguments. Inputs are pm (ProductManifold) and X, output is the second index kernel matrix. This is useful when analyzing the contributions of different manifold components to the total kernel.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nproduct_kernel(pm, X, X)[0][2]\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Manifold Learning and Plotting in Python\nDESCRIPTION: Imports essential libraries for the script: `embedders` for manifold and tree implementations, `hyperdt` for another decision tree implementation, `torch` for tensor operations, `numpy` for numerical operations, and `matplotlib.pyplot` for plotting.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/14_new_tree_testing.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport embedders\nimport hyperdt\nimport torch\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n```\n\n----------------------------------------\n\nTITLE: Checking Output Shape of `angular_greater` in Python\nDESCRIPTION: Calls the `embedders.tree_new.angular_greater` function with inputs of potentially different shapes (`_x` and `_x[:50]`) and prints the shape of the resulting tensor. This verifies the broadcasting or output shape behavior of the function.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/14_new_tree_testing.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nembedders.tree_new.angular_greater(_x, _x[:50]).shape\n```\n\n----------------------------------------\n\nTITLE: Defining Vertex Partition (PhD Year) in Pajek Format (Data)\nDESCRIPTION: This section defines a vertex partition or attribute using the Pajek cluster (.clu) format. The '*Partition PhD_year.clu' line names the partition. The '*Vertices 1025' line declares the total number of vertices in the graph. Each subsequent line provides the attribute value (PhD year) for the corresponding vertex, ordered implicitly by vertex ID (vertex 1 has the first year, vertex 2 the second, and so on up to vertex 1025).\nSOURCE: https://github.com/pchlenski/manify/blob/main/data/graphs/cs_phds/cs_phds.txt#_snippet_5\n\nLANGUAGE: plaintext\nCODE:\n```\n*Partition PhD_year.clu\n*Vertices 1025\n   1967\n   1953\n   1987\n   1968\n   1980\n   1968\n   1988\n   1980\n   1955\n   1934\n   1974\n   1964\n   1992\n   1974\n   1994\n   1986\n   1984\n   1980\n   1992\n   1988\n   1967\n   1957\n   1988\n   1983\n   1939\n   1929\n   1985\n   1978\n   1976\n   1966\n   1977\n   1971\n   1976\n   1974\n   1995\n   1959\n   1994\n   1986\n   1990\n   1986\n   1976\n   1990\n   1966\n   1982\n   1968\n   1974\n   1938\n   1996\n   1978\n   1980\n   1970\n   1994\n   1986\n   1956\n   1992\n   1974\n   1980\n   1972\n   1994\n   1973\n   1984\n   1963\n   1944\n   1994\n   1977\n   1964\n   1954\n   1975\n   1965\n   1965\n   1984\n   1987\n   1977\n   1975\n   1965\n   1981\n   1973\n   1969\n   1974\n   1955\n   1951\n   1932\n   1986\n   1979\n   1993\n   1987\n   1979\n   1975\n   1978\n   1973\n   1989\n   1992\n   1982\n   1993\n   1987\n   1990\n   1957\n   1988\n   1972\n   1965\n   1948\n   1991\n   1982\n   1946\n   1911\n   1995\n   1975\n   1981\n   1967\n   1967\n   1987\n   1982\n   1976\n   1967\n   1971\n   1961\n   1977\n   1985\n   1965\n   1987\n   1963\n   1995\n   1979\n   1984\n   1981\n   1935\n   1925\n   1985\n   1895\n   1885\n   1970\n   1966\n   1976\n   1971\n   1986\n   1980\n   1973\n   1963\n   1989\n   1992\n   1985\n   1977\n   1991\n   1974\n   1969\n   1944\n   1954\n   1990\n   1986\n   1984\n   1992\n   1983\n   1991\n   1974\n   1952\n   1927\n   1975\n   1959\n   1986\n   1969\n   1990\n   1983\n   1976\n   1978\n   1985\n   1963\n   1995\n   1956\n   1948\n   1973\n   1993\n   1993\n   1986\n   1978\n   1986\n   1981\n   1981\n   1970\n   1993\n   1970\n   1980\n   1972\n   1972\n   1986\n   1937\n   1972\n   1960\n   1972\n   1986\n   1980\n   1936\n   1984\n   1962\n   1952\n   1969\n   1965\n   1991\n   1967\n   1957\n   1986\n   1976\n   1980\n   1968\n   1973\n   1955\n   1924\n   1991\n   1980\n   1971\n   1991\n   1990\n   1971\n   1991\n   1962\n   1994\n   1986\n   1993\n   1979\n   1985\n   1980\n   1995\n   1989\n   1992\n   1987\n   1972\n   1981\n   1976\n   1959\n   1949\n   1991\n   1976\n   1995\n   1978\n   1983\n   1973\n   1990\n   1981\n   1985\n   1972\n   1903\n   1976\n   1987\n   1987\n   1963\n   1959\n   1992\n   1988\n   1980\n   1982\n   1955\n   1934\n   1976\n   1973\n   1987\n   1970\n   1966\n   1946\n   1936\n   1992\n   1960\n   1945\n   1990\n   1991\n   1993\n   1980\n   1986\n   1974\n   1964\n   1983\n   1981\n   1968\n   1989\n   1983\n   1970\n   1964\n   1994\n   1959\n   1995\n   1973\n   1983\n   1973\n   1956\n   1993\n   1991\n   1967\n   1986\n   1981\n   1992\n   1985\n   1975\n   1973\n   1963\n   1958\n   1990\n   1991\n   1984\n   1959\n   1939\n   1929\n   1995\n   1975\n   1987\n   1977\n   1979\n   1974\n   1980\n   1973\n   1952\n   1986\n   1980\n   1988\n   1978\n   1985\n   1991\n   1965\n   1984\n   1967\n   1983\n   1988\n   1981\n   1979\n   1968\n   1958\n   1987\n   1976\n   1993\n   1982\n   1948\n   1973\n   1981\n   1974\n   1958\n   1989\n   1995\n   1980\n   1989\n   1973\n   1954\n   1984\n   1968\n   1967\n   1954\n   1982\n   1988\n   1978\n   1984\n   1985\n   1973\n   1991\n   1981\n   1973\n   1974\n   1953\n   1977\n   1974\n   1975\n   1969\n   1962\n   1995\n   1979\n   1973\n   1969\n   1977\n   1989\n   1979\n   1930\n   1986\n   1974\n   1958\n   1921\n   1976\n   1974\n   1977\n   1974\n   1974\n   1967\n   1979\n   1979\n   1979\n   1974\n   1949\n   1978\n   1948\n   1938\n   1989\n   1970\n   1959\n   1967\n   1985\n   1973\n   1988\n   1975\n   1992\n   1996\n   1989\n   1987\n   1970\n   1985\n   1976\n   1989\n   1981\n   1976\n   1954\n   1993\n   1984\n   1965\n   1995\n   1973\n   1974\n   1979\n   1987\n   1992\n   1981\n   1992\n   1990\n   1990\n   1992\n   1986\n   1983\n   1977\n   1988\n   1981\n   1992\n   1990\n   1975\n   1954\n   1975\n   1977\n   1989\n   1983\n   1989\n   1983\n   1980\n   1991\n   1989\n   1973\n   1984\n   1992\n   1981\n   1982\n   1989\n   1995\n   1973\n   1992\n   1994\n   1977\n   1994\n   1969\n   1985\n   1967\n   1990\n   1924\n   1991\n   1982\n   1975\n   1971\n   1973\n   1994\n   1987\n   1989\n   1986\n   1976\n   1973\n   1939\n   1980\n   1982\n   1985\n   1977\n   1959\n   1969\n   1993\n   1991\n   1987\n   1991\n   1986\n   1977\n   1971\n   1960\n   1992\n   1975\n   1994\n   1995\n   1984\n   1985\n   1986\n   1949\n   1980\n   1987\n   1947\n   1965\n   1975\n   1992\n   1993\n   1976\n   1982\n   1964\n   1996\n   1973\n   1977\n   1958\n   1977\n   1974\n   1974\n   1990\n   1995\n   1991\n   1995\n   1979\n   1976\n   1985\n   1954\n   1979\n   1990\n   1993\n   1988\n   1972\n   1968\n   1983\n   1974\n   1966\n   1973\n   1973\n   1963\n   1980\n   1994\n   1987\n   1993\n   1988\n   1986\n   1988\n   1982\n   1976\n   1995\n   1988\n   1979\n   1992\n   1989\n   1979\n   1986\n   1991\n   1994\n   1988\n   1976\n   1987\n   1985\n   1983\n   1983\n   1993\n   1983\n   1994\n   1978\n   1976\n   1991\n   1958\n   1972\n   1965\n   1965\n   1983\n   1949\n   1995\n   1985\n   1995\n   1973\n   1997\n   1990\n   1984\n   1989\n   1988\n   1977\n   1990\n   1979\n   1978\n   1984\n   1979\n   1979\n   1989\n   1978\n   1993\n   1992\n   1982\n   1986\n   1982\n   1994\n   1988\n   1981\n   1992\n   1990\n   1986\n   1981\n   1974\n   1995\n   1993\n   1963\n   1981\n   1955\n   1954\n   1950\n   1985\n   1994\n   1988\n   1975\n   1992\n   1990\n   1975\n   1986\n   1981\n```\n\n----------------------------------------\n\nTITLE: Defining PyTorch Model Training and Evaluation Functions in Python\nDESCRIPTION: Defines two helper functions: `train_model` trains a given PyTorch model using CrossEntropyLoss and Adam optimizer for a specified number of epochs, optionally accepting an adjacency matrix `A_hat`. `evaluate_model` evaluates a trained model's accuracy on test data, also optionally using `A_hat`. Both functions handle models that may or may not require `A_hat` as input.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/relationship.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef train_model(model, X_train, y_train, A_hat=None, num_epochs=1000, lr=0.01):\n    loss_fn = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    progress_bar = tqdm(total=num_epochs)\n\n    for i in range(num_epochs):\n        # Forward pass\n        if A_hat is not None:\n            y_pred = model(X_train, A_hat)\n        else:\n            y_pred = model(X_train)\n\n        # Compute loss\n        loss = loss_fn(y_pred, y_train)\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Update progress bar\n        progress_bar.update(1)\n        progress_bar.set_postfix(loss=loss.item())\n\n    return model\n\n\ndef evaluate_model(model, X_test, y_test, A_hat=None):\n    with torch.no_grad():\n        if A_hat is not None:\n            y_pred = model(X_test, A_hat)\n        else:\n            y_pred = model(X_test)\n\n        acc = (y_pred.argmax(dim=1) == y_test).float().mean()\n    return acc.item()\n\n```\n\n----------------------------------------\n\nTITLE: Accessing All Angle Dimensions in Decision Tree in Python\nDESCRIPTION: Retrieves the `angle_dims` attribute of the trained `ProductSpaceDT` object `pdt`. This attribute typically stores a mapping or list indicating which pairs of original data dimensions are used for the angular comparisons corresponding to each feature index in the tree.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_pc_basic_visualization.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\npdt.angle_dims\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Dataset Splitting Function for Graph Pairs\nDESCRIPTION: Defines a Python function `split_dataset` tailored for splitting graph link prediction datasets (features `X`, labels `y`) into training and testing sets. It ensures structural integrity by splitting based on node indices rather than individual pairs, guaranteeing that links involving test nodes are only in the test set. It reshapes the input tensors (assuming `X` represents pairwise features), uses `sklearn.model_selection.train_test_split` on node indices, and reconstructs the final train/test feature and label tensors.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/28_benchmark_link_prediction.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Special function to split dataset while ensuring pairs are in the same split\nfrom sklearn.model_selection import train_test_split\n\n\ndef split_dataset(X, y, **kwargs):\n    n_pairs, n_dims = X.shape\n    n_nodes = int(n_pairs**0.5)\n\n    # Reshape\n    X_reshaped = X.view(n_nodes, n_nodes, -1)\n    y_reshaped = y.view(n_nodes, n_nodes)\n\n    # Take 20% Of the nodes as test nodes\n    idx = list(range(n_nodes))\n    idx_train, idx_test = train_test_split(idx, **kwargs)\n\n    # Return test and train sets\n    X_train = X_reshaped[idx_train][:, idx_train].reshape(-1, n_dims)\n    y_train = y_reshaped[idx_train][:, idx_train].reshape(-1)\n\n    X_test = X_reshaped[idx_test][:, idx_test].reshape(-1, n_dims)\n    y_test = y_reshaped[idx_test][:, idx_test].reshape(-1)\n\n    return X_train, X_test, y_train, y_test\n```\n\n----------------------------------------\n\nTITLE: Defining Function to Create Link Prediction Dataset in Python\nDESCRIPTION: Defines a function `make_link_prediction_dataset` that prepares data for link prediction classifiers. It takes node embeddings (`X_embed`), the product manifold (`pm`), and the adjacency matrix (`adj`) as input. It creates feature vectors `X` by concatenating the embeddings of all possible node pairs. Optionally (`add_dists=True`), it computes pairwise manifold distances and appends them as an additional feature. It returns the feature matrix `X`, the flattened adjacency matrix as labels `y`, and a new `ProductManifold` (`new_pm`) reflecting the structure of the concatenated features.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/51_gcn_link_prediction.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Copied from notebook 22\n\n\ndef make_link_prediction_dataset(X_embed, pm, adj, add_dists=True):\n    # Stack embeddings\n    emb = []\n    for i in range(len(X_embed)):\n        for j in range(len(X_embed)):\n            joint_embed = torch.cat([X_embed[i], X_embed[j]])\n            emb.append(joint_embed)\n\n    X = torch.stack(emb)\n\n    # Add distances\n    if add_dists:\n        dists = pm.pdist(X_embed)\n        X = torch.cat([X, dists.flatten().unsqueeze(1)], dim=1)\n\n    # y = torch.tensor(adj.flatten())\n    if not torch.is_tensor(adj):\n        adj = torch.tensor(adj)\n    y = adj.flatten()\n\n    # Make a new signature\n    new_sig = pm.signature + pm.signature\n    if add_dists:\n        new_sig.append((0, 1))\n    new_pm = manify.manifolds.ProductManifold(signature=new_sig)\n\n    return X, y, new_pm\n```\n\n----------------------------------------\n\nTITLE: Training a Product Manifold Decision Tree using Embedders in Python\nDESCRIPTION: This snippet initializes a product manifold using a specified signature, then constructs and trains a ProductSpace Decision Tree classifier with embedders. The tree is limited to a maximum depth of 3 and uses a custom feature selection parameter ('d_choose_2'). Essential dependencies are embedders for manifold and tree objects. The model is trained using downsampled X_train and y_train arrays, making it ready for downstream analysis or prediction.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/45_blood_and_lymphoma_interp.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nsig = [(1, 2), (0, 2), (-1, 2), (-1, 2), (-1, 2)]\npm = embedders.manifolds.ProductManifold(signature=sig)\npdt = embedders.predictors.tree_new.ProductSpaceDT(pm=pm, max_depth=3, n_features=\"d_choose_2\")\npdt.fit(X_train, y_train)\n```\n\n----------------------------------------\n\nTITLE: Training a ProductSpaceSVM Model from embedders (Python)\nDESCRIPTION: Initializes a ProductSpaceSVM from embedders.svm, with configurable constraints, fits it to data generated from a product manifold, and stores the result. Inputs: pm (ProductManifold), training data X and y, and boolean flags for H, S, E constraints. Outputs: trained SVM object. Dependencies: embedders.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_41\n\nLANGUAGE: python\nCODE:\n```\nfrom embedders.manifolds import ProductManifold\nfrom embedders.gaussian_mixture import gaussian_mixture\nfrom embedders.svm import ProductSpaceSVM\n\n# Get pm and sample\npm = ProductManifold(signature=[(-1, 2), (0, 2), (1, 2)])\nX, y = gaussian_mixture(pm)\n\n# Get SVM and fit\n_h, _s, _e = True, True, True\n# _h, _s, _e = False, False, False\nps_svm = ProductSpaceSVM(pm=pm, h_constraints=_h, s_constraints=_s, e_constraints=_e, epsilon=1e-10)\nps_svm.fit(X, y)\n```\n\n----------------------------------------\n\nTITLE: Batched Prediction for Surface Grid Using Model - Python\nDESCRIPTION: Performs batched prediction over a large meshgrid to avoid memory overload, disables model batching for prediction, and concatenates outputs for further visualization. Requires pre-trained 'pdt' object, numpy, and torch; split prediction into manageable batches and aggregate results into a full prediction vector.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/25_benchmark_globe.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Get predictions\\n\\npdt.batched = False\\npdt.batch_size = 1\\npdt_pred = []\\n# Predict 100 at a time to avoid memory issues\\nfor i in range(0, len(_X), 100):\\n    pdt_pred.append(pdt.predict(_X[i : i + 100]).numpy())\\npdt_pred = np.concatenate(pdt_pred)\n```\n\n----------------------------------------\n\nTITLE: Predicting and Evaluating Link Probability With Trained LinkPredictionGNN (Python)\nDESCRIPTION: Uses a trained link prediction GNN model to generate probability scores for test links and computes ROC AUC to assess performance. Assumes variables from prior code (y_pred, adj, dists, test_idx) are set, and requires scikit-learn's roc_auc_score for metric calculation. Provides a standard evaluation routine for link prediction tasks.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/41_more_benchmarks.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ny_pred = link_gnn.predict(X_tangent, dists=dists, test_idx=test_idx)\\n\\n# Evaluate\\ny_true = adj[test_idx][:, test_idx].flatten()\\ny_pred = y_pred.flatten()\\n\\nfrom sklearn.metrics import roc_auc_score\\n\\nroc_auc_score(y_true.cpu().numpy(), y_pred.cpu().numpy())\\n\n```\n\n----------------------------------------\n\nTITLE: Checking Shape of Filtered Data for a Specific Dimension in Python\nDESCRIPTION: Selects the data points from `X` that are *not* included in the `mask` (i.e., `X[~mask]`) and extracts only the column corresponding to the dimension index stored in `dim2`. It then implicitly prints the shape of this resulting 1D array or tensor, indicating how many filtered data points exist for that specific dimension.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_basic_visualization.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nX[~mask, dim2].shape\n```\n\n----------------------------------------\n\nTITLE: Generating Gaussian Mixture Data on a Product Manifold in Python\nDESCRIPTION: Imports the Gaussian mixture module, defines a product manifold `pm` with two hyperbolic components (signature `[(1, 4), (-1, 4)]`), and generates 1000 data points `X` with corresponding labels `y` using a Gaussian mixture model on this manifold. The `cov_scale_means` parameter controls the scale of the covariance matrices relative to the means.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_pc_basic_visualization.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport embedders.gaussian_mixture\n\n\npm = embedders.manifolds.ProductManifold(signature=[(1, 4), (-1, 4)])\n\nX, y = embedders.gaussian_mixture.gaussian_mixture(pm, 1000, cov_scale_means=0.1)\n```\n\n----------------------------------------\n\nTITLE: Reporting Landmark Gene Overlap Per Cell Type - Python\nDESCRIPTION: This snippet computes and prints the number of landmark genes present in each loaded cell type dataset by intersecting the variable (gene) indices with the landmark gene set. It expects prior definitions of 'data' (mapping cell types to AnnData), and 'landmark_genes'. Outputs are tab-separated cell type names and the count of matching genes, printed to the console; this step is mostly for validation.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/8_scrna_preprocessing.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# This seems about right, based on what the Tabaghi paper claimed - we get 967, they get 965\\n\\nfor k, v in data.items():\\n    print(k, len(set(v.var.index) & set(landmark_genes)), sep=\"\\t\")\n```\n\n----------------------------------------\n\nTITLE: Analyzing Feature Usage in Trained ProductSpaceDT Models\nDESCRIPTION: Extracts the feature indices used in the decision nodes of each trained `ProductSpaceDT` tree stored in the `trees` list (from the previous snippet). It then calculates and prints the proportion of features used by each tree that fall within the expected dimension range corresponding to the manifold signature that generated its target labels.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/40_component_interp.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfeats = []\nfor tree in trees:\n    feats.append([x.feature for x in tree.nodes if x.feature])\n\n# Which ones fall in the right dim:\nfor feat_set, allowed in zip(feats, [[0, 1], [2, 3], [4, 5]]):\n    total = len(feat_set)\n    correct = sum([f in allowed for f in feat_set])\n    print(f\"{correct/total} correct\")\n```\n\n----------------------------------------\n\nTITLE: Referencing PyTorch Arcsine Function (Python)\nDESCRIPTION: Simple reference to the PyTorch asin function for later usage. Indicates planned trigonometric processing or is a placeholder for further functional application.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ntorch.asin\n```\n\n----------------------------------------\n\nTITLE: Enabling Autoreload in IPython Environment - Python\nDESCRIPTION: This snippet enables IPython's autoreload extension for automatic reloading of modules before code execution, streamlining the workflow in Jupyter notebooks or IPython shells. It requires IPython or Jupyter as the execution environment and is particularly helpful for rapid development and module updates. The commands ensure imported modules are reloaded every time before executing a new cell, reducing the need to restart the kernel.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/19_opengraphbenchmark.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Initializing PyTorch Device in Python\nDESCRIPTION: This snippet initializes PyTorch and determines the appropriate computation device. It checks if a CUDA-enabled GPU is available and sets the device to the first GPU ('cuda:0') if found, otherwise defaults to the CPU ('cpu'). It concludes by printing the selected device.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/Embedder Tutorial.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nif torch.cuda.is_available():\n    torch.cuda.set_device(0)\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\nprint(f\"Using device: {device}\")\n```\n\n----------------------------------------\n\nTITLE: Checking Data Shape in Python\nDESCRIPTION: Accesses the `shape` attribute of the data tensor `X` (likely a PyTorch tensor or NumPy array) to determine its dimensions (number of samples, number of features).\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_pc_basic_visualization.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nX.shape\n```\n\n----------------------------------------\n\nTITLE: Setting Up Autoreload in IPython\nDESCRIPTION: This snippet uses IPython magic commands to load the 'autoreload' extension and configure it to automatically reload all modules before executing user code. This is useful for interactive development, ensuring changes in imported modules are picked up without restarting the kernel.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/23_benchmark_vae_embeddings.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Loading IPython Autoreload Extension\nDESCRIPTION: These IPython magic commands load and configure the `autoreload` extension. This automatically reloads Python modules before executing code, which is useful during development to pick up changes in imported files without restarting the kernel.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Importing Core Libraries for Embedding and Graph Analysis\nDESCRIPTION: Imports essential Python libraries for the script's functionality. `embedders` provides the core manifold embedding and related tools, `networkx` is commonly used for graph data structures and analysis (likely used internally by `embedders`), and `torch` is used for tensor operations fundamental to the embedding computations.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/28_benchmark_link_prediction.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport embedders\nimport networkx as nx\nimport torch\n```\n\n----------------------------------------\n\nTITLE: Loading and Merging Lymphoma scRNA Data Using Scanpy - Python\nDESCRIPTION: This snippet loads lymphoma and healthy cell data using Scanpy with gzip compression, annotates observations with their cell type, and merges the datasets with AnnData.concatenate using 'cell_type' as the batch key. Requires Scanpy and the source files in gzip-compressed 10x format. Outputs the merged AnnData object for further analysis.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/8_scrna_preprocessing.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport scanpy\\n\\ncell_types = [\"lymphoma_cells\", \"healthy_cells\"]\\n\\ndata = {}\\n# These are gzipped inside\\nfor cell_type in cell_types:\\n    data[cell_type] = scanpy.read_10x_mtx(\\n        f\"/teamspace/studios/this_studio/embedders/data/blood_cell_scrna/{cell_type}\", cache_compression=\"gzip\")\n```\n\nLANGUAGE: python\nCODE:\n```\nfor k, v in data.items():\\n    v.obs[\"cell_type\"] = k\\n\\n# Finally, merge\\nadata = scanpy.AnnData.concatenate(*data.values(), batch_key=\"cell_type\")\\nadata\\n\\n# This is the correct number of points for the paper\n```\n\n----------------------------------------\n\nTITLE: Batch Training and Interpretation for Lymphoma Embeddings with Embedders in Python\nDESCRIPTION: This snippet mirrors the blood cell batch workflow but adapts it for the lymphoma embedding dataset and a different product manifold signature. For 10 trials, the code loads lymphoma embedding data, downsamples it, fits a ProductSpace Decision Tree on the specified manifold, and prints manifold node assignment counts by trial. This modularity demonstrates generality and adaptability of the modeling pipeline. Dependencies include numpy, embedders, and collections.Counter.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/45_blood_and_lymphoma_interp.ipynb#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nembedding = \"lymphoma\"\nsig = [(1, 2), (1, 2)]\npm = embedders.manifolds.ProductManifold(signature=sig)\n\nfor i in range(10):\n    X_train = np.load(f\"../data/{embedding}/embeddings/X_train_{i}.npy\")\n    y_train = np.load(f\"../data/{embedding}/embeddings/y_train_{i}.npy\")\n    X_test = np.load(f\"../data/{embedding}/embeddings/X_test_{i}.npy\")\n    y_test = np.load(f\"../data/{embedding}/embeddings/y_test_{i}.npy\")\n\n    train_downsample = np.random.choice(X_train.shape[0], 1000, replace=False)\n    test_downsample = np.random.choice(X_test.shape[0], 1000, replace=False)\n\n    X_train = X_train[train_downsample]\n    y_train = y_train[train_downsample]\n\n    pdt = embedders.predictors.tree_new.ProductSpaceDT(pm=pm, max_depth=3, n_features=\"d_choose_2\")\n    pdt.fit(X_train, y_train)\n\n    print(Counter([pm.dim2man[pdt.angle_dims[node.feature][1]] for node in pdt.nodes if node.feature is not None]))\n```\n\n----------------------------------------\n\nTITLE: Training H^6 Embedding for CS PhDs Dataset in Python\nDESCRIPTION: This snippet defines a product manifold with a single H^6 component, normalizes the input distances, and then trains coordinates using the 'embedders.coordinate_learning.train_coords' function. It sets a manual seed for reproducibility, specifies training parameters (iterations, learning rates), and uses the previously configured PyTorch 'device'. The resulting H^6 embedding is stored in 'h6_cs_phds'.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/4_cs_phds_regression.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Specify signature - useful to re-initialize the manifold here\n\ntorch.manual_seed(0)  # Not all seeds are stable - this one trains for 3000 iterations at lr=1e-2 (burn-in 1e-3)\n\nsignature = [(-1, 6)]\npm = embedders.manifolds.ProductManifold(signature=signature)\nprint(pm.name)\n\n# Rescale distances\ndists_rescaled = cs_dists / cs_dists.max()\n\n# Get embedding\nembedders.coordinate_learning.train_coords(\n    pm,\n    dists_rescaled,\n    device=device,\n    burn_in_iterations=100,\n    training_iterations=100 * 9,\n    learning_rate=1e-1,\n    burn_in_learning_rate=1e-2,\n    scale_factor_learning_rate=1e-1,\n)\n\nh6_cs_phds = pm.x_embed.detach().cpu().numpy()\n```\n```\n\n----------------------------------------\n\nTITLE: Performing Wilcoxon Signed-Rank Tests on Trial Results in Python\nDESCRIPTION: Performs and prints the results of the Wilcoxon signed-rank test comparing the paired accuracy scores between the standard MLP and KappaGCN-MLP models, and between the standard MLR and KappaGCN-MLR models, using the data collected across 30 trials in the `results` DataFrame.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/relationship.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nprint(wilcoxon(results[\"mlp\"], results[\"kappa_mlp\"]))\nprint(wilcoxon(results[\"mlr\"], results[\"kappa_mlr\"]))\n```\n\n----------------------------------------\n\nTITLE: Calculating Mean RMSE per Curvature in Python\nDESCRIPTION: Analyzes the 'results' DataFrame (presumably loaded or generated previously). It groups the results by 'curvature', selects columns containing 'rmse' in their names, calculates the mean within each group (mean RMSE per model per curvature), then calculates the mean across these model columns (average RMSE across all models for each curvature), and finally sorts the curvatures based on this average RMSE.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/26_benchmark_single_curvature_gaussian.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nresults.groupby(\"curvature\")[[col for col in results.columns if \"rmse\" in col]].mean().mean().sort_values()\n```\n\n----------------------------------------\n\nTITLE: Calculating Normalized Adjacency Matrix for GCN in Python\nDESCRIPTION: Computes pairwise distances between the learned embeddings `X` using the product manifold's distance function (`pm.pdist2`). Normalizes these distances by the maximum finite distance. Creates an adjacency matrix `A` by applying an exponential decay function to the normalized distances. Finally, computes `A_hat`, a normalized version of the adjacency matrix (often including self-loops), suitable for use in Graph Convolutional Networks (GCNs), using `embedders.predictors.kappa_gcn.get_A_hat`.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/51_gcn_link_prediction.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Get an adjacency matrix that's not leaky\ndists = pm.pdist2(X)\nmax_dist = dists[dists.isfinite()].max()\ndists /= max_dist\nA = torch.exp(-dists)\nA_hat = embedders.predictors.kappa_gcn.get_A_hat(A).float().to(DEVICE)\n```\n\n----------------------------------------\n\nTITLE: Displaying Sampled Points\nDESCRIPTION: Displays the tensor containing the points (`samples`) generated during the last iteration of the sanity check loop (for K=4.0).\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/5_sampling.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nsamples\n```\n\n----------------------------------------\n\nTITLE: Defining Experiment Result File Paths and Configuration in Python\nDESCRIPTION: This snippet defines dictionaries (`CLS_PATHS`, `REG_PATHS`, `HIGHDIM_PATHS`) mapping experiment types ('Empirical', 'Graph', 'Gaussian') to their corresponding TSV result file paths. It also sets the active configuration by assigning one of these dictionaries to the `PATHS` variable and specifying the metric column identifier in the `VAR` variable (e.g., 'accuracy', 'mse'). These variables control which dataset and metric are processed by subsequent code.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/57_new_table.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nCLS_PATHS = {\n    \"Empirical\": \"../data/results_icml_revision/classification_nn_empirical.tsv\",\n    \"Graph\": \"../data/results_icml_revision/classification_nn_graph.tsv\",\n    \"Gaussian\": \"../data/results_icml_revision/classification_nn_multiple_curvatures.tsv\",\n}\n\nREG_PATHS = {\n    \"Empirical\": \"../data/results_icml_revision/regression_nn_empirical.tsv\",\n    \"Graph\": \"../data/results_icml_revision/regression_nn_graph.tsv\",\n    \"Gaussian\": \"../data/results_icml_revision/regression_nn_multiple_curvatures.tsv\",\n}\n\nHIGHDIM_PATHS = {\n    \"Gaussian\": \"../data/results_icml_revision/classification_nn_single_curvature_16dim.tsv\"\n}\n\n# PATHS = CLS_PATHS\n# VAR = \"f1-macro\"\n\n# PATHS = CLS_PATHS\n# VAR = \"accuracy\"\n\n# PATHS = REG_PATHS\n# VAR = \"mse\"\n\nPATHS = HIGHDIM_PATHS\nVAR = \"accuracy\"\n\n```\n\n----------------------------------------\n\nTITLE: Importing Core Libraries (geoopt, torch)\nDESCRIPTION: Imports the `geoopt` library, likely used for Riemannian optimization and geometric operations, and the `torch` library, the fundamental package for tensor computations and neural networks in PyTorch.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/5_sampling.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport geoopt\nimport torch\n```\n\n----------------------------------------\n\nTITLE: Aggregating Benchmark Results by Embedding in Python\nDESCRIPTION: This Python snippet uses the pandas library to group the benchmark results stored in the 'results' DataFrame by the 'embedding' column. It then calculates the mean of the metrics for each embedding type across all trials. The resulting aggregated DataFrame (showing average performance per embedding) is implicitly displayed as the output of the cell in an interactive environment.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/23_benchmark_vae_embeddings.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nresults.groupby(\"embedding\").mean()\n```\n\n----------------------------------------\n\nTITLE: Configuring Autoreload in IPython/Jupyter\nDESCRIPTION: Sets up the IPython environment to automatically reload modules before executing code. This is useful for development when code in imported modules changes frequently. `%load_ext autoreload` loads the extension, and `%autoreload 2` configures it to reload all modules except those explicitly excluded.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/51_gcn_link_prediction.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Initializing and Sampling from a Euclidean Manifold\nDESCRIPTION: Creates an instance of the `Manifold` class representing a 4-dimensional Euclidean space (curvature K=0). It then calls the `sample` method to generate points, likely using a default origin or a provided mean.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/5_sampling.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nm_e = Manifold(0, 4)\nm_e.sample([[1, 0, 0, 0] * 3])\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch Device (CUDA/CPU)\nDESCRIPTION: This snippet checks for CUDA availability using PyTorch. If CUDA is available, it sets the active device to the second GPU (index 1). Otherwise, it falls back to using the CPU. It prints the selected device.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/2_polblogs_benchmark.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Torch device management\nimport torch\n\nif torch.cuda.is_available():\n    torch.cuda.set_device(1)\n    device = torch.device(\"cuda:1\")\nelse:\n    device = torch.device(\"cpu\")\n\nprint(f\"Using device: {device}\")\n```\n\n----------------------------------------\n\nTITLE: Final Layout Adjustment, Saving, and Displaying Plot (Matplotlib, Python)\nDESCRIPTION: This section finalizes the plot layout for optimal appearance, saves the entire figure as a PDF with tight bounding boxes for minimal whitespace, and renders the plot to the display. These steps are standard post-processing actions for producing publication-level visualizations. Dependencies: Matplotlib figure/axes previously defined. Input: current Matplotlib figure; Output: PDF file and on-screen plot.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/26_benchmark_single_curvature_gaussian.ipynb#_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nplt.tight_layout()\n\n# Broken y-axis\n# axs[0].set_ylim(0, 2)\n# axs[1].set_ylim(0, 2)\n\n# Save as pdf\nplt.savefig(\n    f\"../figures/single_curvature_{TASK}.pdf\",\n    bbox_inches=\"tight\",\n)\nplt.show()\n\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Generating Synthetic Data in Python\nDESCRIPTION: Imports necessary libraries including `manify`, `torch`, `matplotlib`, `sklearn`, `tqdm`, and `typing`. Generates 2D synthetic blob data using `sklearn.datasets.make_blobs`, converts it to PyTorch tensors, splits it into training and testing sets, and visualizes the data distribution using `matplotlib`.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/relationship.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport manify\nimport torch\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm\nfrom typing import Callable, Tuple, Optional\nimport torch\nfrom torch import Tensor\n\nfrom sklearn.datasets import make_blobs\n\ndevice = \"cpu\"\n\n# Make data\nX, y = make_blobs(n_samples=8000, centers=6, n_features=2, random_state=500)\nX = torch.tensor(X, dtype=torch.float32, device=device)\ny = torch.tensor(y, dtype=torch.long, device=device)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Plot data\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"viridis\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Visualizing Ablation Study Results with Boxplots and Significance Testing in Python\nDESCRIPTION: Generates a boxplot visualization comparing the performance of models from the ablation study using matplotlib, pandas, numpy, and scipy. It first sets up plotting aesthetics, including using LaTeX fonts. A helper function `test` is defined to perform a Wilcoxon signed-rank test between two sets of results. The script selects the relevant model results based on `ABLATION_VALS` and `USE_DT`/`USE_RF` flags, defines model names for the plot legend/ticks, and assigns colors. It then creates a boxplot for each model configuration, customizing the appearance (colors, line widths, median marker). P-value annotations indicating statistical significance (or lack thereof, based on `SIGNIFICANCE`) between pairs of models are added above the boxes, using the defined `test` function and adjusting for multiple comparisons if `CORRECTION` is set to 'bonferroni'. Plot elements like axes labels, title (reflecting the ablation variable), tick labels, and spines are customized. Finally, the plot is displayed and saved as a PDF file named according to the `ABLATION_VAR`.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/18_ablations.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Let's see the values: barplot with statistical significance annotations\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import wilcoxon\n\n# Set font to LaTeX times font\nplt.rcParams[\"font.family\"] = \"serif\"\nplt.rcParams[\"font.serif\"] = [\"Times New Roman\"] + plt.rcParams[\"font.serif\"]\n\n# SIGNIFICANCE = \"sig\"\nSIGNIFICANCE = \"ns\"\nCORRECTION = \"bonferroni\"\n# CORRECTION = \"none\"\n\nY_LABEL = \"F1 Score\" if TASK == \"classification\" else \"Root Mean Squared Error\"\n\nFONTSIZE = 10\n\n\ndef test(x, y):\n    # Check x and y are identical\n    all_same = True\n    for a, b in zip(x, y):\n        if a != b:\n            all_same = False\n            break\n    if all_same:\n        return 1.0\n    else:\n        return wilcoxon(x, y).pvalue\n\n\n# All the spec happens up here\n# models = [f\"dt_{val}\" for val in ABLATION_VALS] + [f\"rf_{val}\" for val in ABLATION_VALS]\ndts = [f\"dt_{val}_test\" for val in ABLATION_VALS] if USE_DT else []\nrfs = [f\"rf_{val}_test\" for val in ABLATION_VALS] if USE_RF else []\nmodels = dts + rfs\n# model_names = [\"Product DT\", \"Product DT\\n(+ Special dims)\", \"Product RF\", \"Product RF\\n(+ Special dims)\"]\n# model_names = [\"$\\\\sqrt{D}$\", \"$\\\\log_2{D}$\", \"$D$\"]\nmodel_names = ABLATION_VALS\nmodel_names_dt = [f\"DT\\n({val})\" for val in ABLATION_VALS]\nmodel_names_rf = [f\"RF\\n({val})\" for val in ABLATION_VALS]\nmodel_names = model_names_dt + model_names_rf\ncolors = [f\"C{i}\" for i in range(len(models))]\n\nresults[\"curvature\"] = 0\nCURVATURES = [0]\n\n# Critical p-value depends on false discovery correction\nCRITICAL_VAL = 0.05\nif CORRECTION == \"bonferroni\":\n    CRITICAL_VAL /= len(models) * (len(models) - 1) / 2\n\n# Initialize plot\n# fig, ax = plt.subplots(1, 1, figsize=(10, 5), sharex=True)\nfig, ax = plt.subplots(1, 1, figsize=(5.5, 3), sharex=True)\nx_vals = np.arange(len(CURVATURES)) * (len(models) + 1)\n\n# for ax, models, model_names in zip(\n#     axs, [models1 + models3, models2 + models3], [model_names1 + model_names3, model_names2 + model_names3]\n# ):\nbps = []\nfor i, (model, color) in enumerate(zip(models, colors)):\n    # Initial boxplot\n    bp = ax.boxplot(\n        [results[results[\"curvature\"] == K][model] for K in CURVATURES],\n        positions=x_vals + i,\n        widths=0.8,\n        boxprops=dict(color=\"C0\", linewidth=2),\n    )\n\n    # Fix colors\n    for element in [\"boxes\", \"whiskers\", \"fliers\", \"means\", \"caps\"]:\n        # plt.setp(bp[element], color=color, linewidth=2)\n        plt.setp(bp[element], color=\"C0\", linewidth=2)\n    plt.setp(bp[\"medians\"], color=\"black\", linewidth=2)\n    plt.setp(bp[\"fliers\"], marker=\"o\", markersize=5, markeredgecolor=\"C0\", markeredgewidth=2)\n\n    bps.append(bp)\n\n# # Flip y-axis for RMSE\n# if TASK == \"regression\":\n#     ax.invert_yaxis()\n\nymin, ymax = ax.get_ylim()\n\n# Add p-value annotations. All start at x_vals, and end at x_vals + i\nheights = [0] * len(CURVATURES)  # How many annotations per curvature\n# for i, j in [(0, 1), (1, 2), (0, 2), (0, 3), (1, 3), (2, 3)]:\nfor i in range(len(models)):\n    for j in range(i + 1, len(models)):\n        results_K = [results[results[\"curvature\"] == K] for K in CURVATURES]\n        p_vals = [test(res[models[i]], res[models[j]]) for res in results_K]\n        em = ymax - ymin\n\n        for k, p_val in enumerate(p_vals):\n            if (SIGNIFICANCE == \"ns\" and p_val > CRITICAL_VAL) or (SIGNIFICANCE == \"sig\" and p_val < CRITICAL_VAL):\n                x1, x2 = x_vals[k] + i, x_vals[k] + j\n                height = results[results[\"curvature\"] == CURVATURES[k]][models].max().max() + 0.04 * em * (\n                    heights[k] + 1\n                )\n                annotation = \"*\" if SIGNIFICANCE == \"sig\" else \"ns\"\n                ax.text(\n                    s=annotation,\n                    x=(x1 + x2) / 2,\n                    y=height,\n                    ha=\"center\",\n                    va=\"center\",\n                    color=\"black\",\n                    fontdict={\"weight\": \"bold\"},\n                )\n                ax.plot(\n                    [x1, x1, x2, x2],\n                    [height - 0.02 * em, height - 0.01 * em, height - 0.01 * em, height - 0.02 * em],\n                    lw=2,\n                    color=\"black\",\n                )\n                heights[k] += 1\n\n# Fix y-lim and remove top/right spines; make background transparent\nax.patch.set_alpha(0)\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\n\n# Set x-ticks\nax.set_ylabel(Y_LABEL, fontsize=12)\n# ax.set_xticks(x_vals + 1.5, CURVATURES, fontsize=16)\nax.set_xticks(list(range(len(model_names))), model_names, fontsize=FONTSIZE)\n# ax.legend([bp[\"boxes\"][0] for bp in bps], model_names, fontsize=14, frameon=False, title=\"Model\")\n# Make legend to the right of the plot\n# ax.legend(\n#     [bp[\"boxes\"][0] for bp in bps],\n#     model_names,\n#     fontsize=16,\n#     frameon=False,\n#     title=\"Model\",\n#     title_fontsize=18,\n#     loc=\"center left\",\n#     bbox_to_anchor=(1, 0.5),\n# )\nax.tick_params(axis=\"y\", labelsize=FONTSIZE)\n# ax.set_yscale(\"log\") if TASK == \"regression\" else None\n\n# axs[0].set_title(\"Decision Trees\", fontsize=18)\n# axs[1].set_title(\"Random Forests\", fontsize=18)\n# axs[1].set_xlabel(\"Curvature\", fontsize=16)\n\n# plt.xlim(-1, len(CURVATURES) * (len(models) + 1))\n# plt.suptitle(f\"Ablation: special dimensions\", fontsize=16)\n# plt.suptitle(f\"Ablation: number of features\", fontsize=FONTSIZE, fontweight=\"bold\", y=0.95)\n# plt.suptitle(\"Ablation: feature subsampling strategy\", fontsize=FONTSIZE, fontweight=\"bold\", y=0.95)\n# plt.suptitle(\"Ablation: maximum depth\", fontsize=FONTSIZE, fontweight=\"bold\", y=0.95)\nplt.suptitle(\"Ablation: remove midpoint angles\", fontsize=FONTSIZE, fontweight=\"bold\", y=0.95)\n\n# Turn off xticks\n# plt.xticks([])\n\nplt.tight_layout()\n\n# Save as pdf\nplt.savefig(f\"../figures/ablation_{ABLATION_VAR}.pdf\", bbox_inches=\"tight\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Downsampling Time Series and Identifying Spike Events in Python\nDESCRIPTION: Selects a specific time series sweep (Sweep_33), downsamples it by reshaping into blocks of 100 and taking the maximum value in each block. It then identifies potential spike locations by applying a voltage threshold (-0.06) to the downsampled data. Finally, it visualizes the downsampled signal with vertical lines indicating the detected spike coordinates.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/30_fourier.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Downsample\n# X = all_ts[32]\nX = np.array(data[\"acquisition/timeseries/Sweep_33/data\"])\n# X = np.array(data[\"acquisition/timeseries/Sweep_46/data\"])\nX_downsampled = X.reshape(-1, 100).max(axis=1).reshape(-1)\n\n# Get spike annotations\ny_labels = X_downsampled > -0.06\ny_coords = np.arange(len(X_downsampled))[y_labels]\n\n# Verify that downsampling worked\nprint(X.shape)\nfig = plt.figure(figsize=(20, 4))\nplt.vlines(y_coords, ymin=-0.1, ymax=0.03, color=\"r\", alpha=0.1)\nplt.plot(X_downsampled)\n```\n\n----------------------------------------\n\nTITLE: Using the Custom ProductSpacePerceptron\nDESCRIPTION: This snippet demonstrates how to use the custom `ProductSpacePerceptron` class. It creates an instance (`ps_perc_pac`) with the defined product manifold (`pm`), fits it to the regenerated data (`X`, `y`), and then uses the trained model to make predictions on the same data `X`. The resulting predictions are stored and displayed.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Example usage:\nps_perc_pac = ProductSpacePerceptron(pm, max_epochs=100, patience=5)\nps_perc_pac.fit(X, y)\npredictions = ps_perc_pac.predict(X)\npredictions\n```\n\n----------------------------------------\n\nTITLE: Comparing ProductDT and Decision Tree on H6 Embedding\nDESCRIPTION: This snippet evaluates and compares the performance of a Product Space Decision Tree ('ProductSpaceDT' from 'embedders.predictors') and a standard Scikit-learn 'DecisionTreeClassifier' on the generated H6 Polblogs embedding. It splits the data, trains both models with a max depth of 3, predicts on the test set, and prints the F1 score for each.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/2_polblogs_benchmark.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Compare productDT and sklearn on this dataset\n# from sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\n# from hyperdt.tree import HyperbolicDecisionTreeClassifier\nfrom embedders.predictors.tree_new import ProductSpaceDT\nfrom sklearn.tree import DecisionTreeClassifier\n\nX_train, X_test, y_train, y_test = train_test_split(h6_polblogs, polblogs_labels.cuda(), test_size=0.2, random_state=0)\n\npdt = ProductSpaceDT(pm=pm, max_depth=3, use_special_dims=True)\npdt.fit(X_train, y_train)\npdt_f1 = f1_score(y_test.cpu(), pdt.predict(X_test).cpu())\nprint(f\"ProductDT\\t{pdt_f1*100:.2f}\")\n\ndt = DecisionTreeClassifier(max_depth=3)\ndt.fit(X_train.cpu(), y_train.cpu())\ndt_f1 = f1_score(y_test.cpu(), dt.predict(X_test.cpu()))\nprint(f\"DT\\t{dt_f1*100:.2f}\")\n```\n\n----------------------------------------\n\nTITLE: Plotting Model Runtime Scaling vs. Dataset Size - Python\nDESCRIPTION: This visualization script generates a log-log plot of runtime versus dataset size for several models, customizing fonts, colors, and labels with Matplotlib, and saving the resulting figure as a PDF. It excludes some model types for clarity and post-processes axis scaling and labeling for readability. Requires matplotlib, numpy, and a DataFrame 'results' with timing columns. Outputs: PDF file and rendered plot. Designed for performance profiling.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/43_benchmark_neural.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Set font to LaTeX times font\nplt.rcParams[\"font.family\"] = \"serif\"\nplt.rcParams[\"font.serif\"] = [\"Times New Roman\"] + plt.rcParams[\"font.serif\"]\n\nplt.figure(figsize=(10, 5))\n\n# Plot the bars using pandas plotting\ntimecols = [c for c in results.columns if \"time\" in c]\n\n# Sort timecols by max time\ntimecols = sorted(timecols, key=lambda c: results[c].max())\n\n# Remove tangent mlp and tangent gnn\ntimecols = [c for c in timecols if \"tangent\" not in c]\n\ncolors = plt.cm.turbo(np.linspace(0, 1, len(timecols)))\n\n# Plot each line\nlines = []\nfor col, color in zip(timecols, colors):\n    line = plt.plot(results[\"size\"], results[col], label=col, color=color, linewidth=2, marker=\"o\", markersize=4)[0]\n    lines.append(line)\n\n# Set scales\nplt.xscale(\"log\")\nplt.yscale(\"log\")\n\n# Set labels and title\nplt.xlabel(\"Dataset Size\")\nplt.ylabel(\"Runtime (s)\")\nplt.title(\"Runtime vs. Dataset Size\")\n\n# Add right-aligned labels\nxmax = results[\"size\"].max()\nfor line, color, label, offset in zip(\n    lines,\n    colors,\n    [\"GNN\", \"Product RF\", \"MLP\", \"$k$-NN\", \"Product DT\", \"RF\", \"Perceptron\", \"DT\"][::-1],\n    # [1., 1., 1., 1.4, 1.2, 1., 0.75, 1.][::-1]\n    [1.0, 1.0, 1.4, 1.2, 1.0, 0.6, 1.0, 1.0][::-1],\n):\n    # Get the last y-value for this line\n    y = line.get_ydata()[-1] * offset\n    # Add text label\n    plt.text(xmax * 1.25, y, label, verticalalignment=\"center\", fontsize=10, color=color, fontweight=\"bold\")\n\n# Extend x-axis to make room for labels\nplt.xlim(results[\"size\"].min() * 0.9, xmax * 1.2)\n\n# Tight layout to prevent label cutoff\nplt.tight_layout()\nplt.savefig(\"embedders/figures/runtime_vs_size.pdf\")\n```\n\n----------------------------------------\n\nTITLE: Saving Benchmark Results to TSV in Python\nDESCRIPTION: This Python snippet uses the pandas library to save the collected benchmark results, stored in the 'results' DataFrame, to a tab-separated values (TSV) file. The file is saved to '../data/results_icml/vae.tsv', and the DataFrame index is not included in the output file.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/23_benchmark_vae_embeddings.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nresults.to_csv(\"../data/results_icml/vae.tsv\", sep=\"\\t\", index=False)\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating ProductSpaceDT Classifiers on Manifold Data using embedders and scikit-learn\nDESCRIPTION: Splits the generated data (`X`, `y`) into training and testing sets using `train_test_split`. It defines a combined `ProductManifold` encompassing all signatures. Then, for each label set (corresponding to an original manifold), it trains a `ProductSpaceDT` classifier (from the `embedders` library) with a maximum depth of 3. Finally, it evaluates and prints the accuracy of each trained tree on the test set.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/40_component_interp.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Try classification using each set of labels as a target\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\npm = embedders.manifolds.ProductManifold(signature=[sig for sublist in SIGS for sig in sublist])\ntrees = []\nfor i in range(len(SIGS)):\n    pdt = embedders.tree_new.ProductSpaceDT(pm=pm, max_depth=3)\n\n    pdt.fit(X_train, y_train[:, i])\n\n    print(f\"Accuracy for {i}: {pdt.score(X_test, y_test[:, i]).float().mean():.3f}\")\n    trees.append(pdt)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Manifold Benchmarking in Python\nDESCRIPTION: Imports necessary Python libraries for the benchmarking tasks. This includes 'torch' for tensor operations and device management, 'manify' (and commented-out 'embedders') for manifold operations and benchmarking utilities, 'numpy' for numerical operations, 'matplotlib.pyplot' for plotting, 'pandas' for data manipulation (especially results storage), 'tqdm.notebook' for progress bars in notebooks, and 'warnings' to filter specific user warnings related to Wishart distribution sampling.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/26_benchmark_single_curvature_gaussian.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n# import embedders\nimport manify\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom tqdm.notebook import tqdm\n\n# Filter out warnings raised when sampling Wishart distribution in Gaussian mixtures\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n```\n\n----------------------------------------\n\nTITLE: Configuring IPython Autoreload Extension in Python\nDESCRIPTION: Loads the `autoreload` IPython extension and configures it to automatically reload all modules (mode 2) before executing user code. This is useful during interactive development to ensure that changes made to imported modules (like the `embedders` library) are reflected without needing to restart the kernel.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/37_sklearn_decision_tree.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Extracting Intermediate Preprocessing Variables from ProductSpaceDT in Python\nDESCRIPTION: Calls the internal `_preprocess` method of the trained `ProductSpaceDT` instance (`pdt`) with the original data (`X`, `y`). This extracts intermediate variables used within the tree's fitting process, specifically the preprocessed data (`X`, `y`) and `comparisons` tensor, likely for debugging purposes.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/17_verify_new_mse.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Let's collect all of our intermediate variables\n\nX, y, _, comparisons = pdt._preprocess(X, y)\n```\n\n----------------------------------------\n\nTITLE: Configuring Autoreload in Python Environment\nDESCRIPTION: Sets up the autoreload extension in an interactive Python environment (like IPython or Jupyter). This automatically reloads modules before executing code, which is useful during development to reflect changes in imported libraries without restarting the kernel. `%autoreload 2` enables reloading all modules except those explicitly excluded.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/26_benchmark_single_curvature_gaussian.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Saving Combined DataFrame to TSV File using Pandas in Python\nDESCRIPTION: This snippet saves the aggregated Pandas DataFrame `all_data` to a tab-separated values (TSV) file named `all_results.tsv` located in the `../data/results_icml/` directory. The `sep='\\t'` argument ensures tab separation, and `index=False` prevents Pandas from writing the DataFrame index as a column in the file.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/32_big_table.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nall_data.to_csv(\"../data/results_icml/all_results.tsv\", sep=\"\\t\", index=False)\n```\n\n----------------------------------------\n\nTITLE: Printing Geodesics between an Ancestor and Intersections - Python\nDESCRIPTION: Computes and prints LaTeX draw commands for geodesic arcs from an ancestor to two intersection points using the find_geodesic function. Demonstrates how to chain prior computations for visualization. Inputs are ancestor and intersection point coordinates (torch tensors). Output is LaTeX-drawable geodesic arc strings. Depends on find_geodesic being defined above.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/36_poincare_circles.ipynb#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n# Get geodesics to your intersections\\nancestor = torch.tensor([0.1826, 0.0910])\\nint1 = torch.tensor([0.5650, 0.1238])\\nint2 = torch.tensor([-0.0943, -0.2655])\\nprint(f\"\\\\draw[thick, color=green] {find_geodesic(ancestor, int1)};\")\\nprint(f\"\\\\draw[thick, color=green] {find_geodesic(ancestor, int2)};\")\\n\n```\n\n----------------------------------------\n\nTITLE: Loading Landmark Genes Table Using Pandas - Python\nDESCRIPTION: This snippet reads a tabular file containing landmark gene information using pandas and extracts the 'pr_gene_symbol' column for downstream filtering of gene expression data. It depends on pandas being installed and the file being present at the specified path. The output is a pandas Series of landmark gene symbols.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/8_scrna_preprocessing.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\\n\\nlandmark_genes_table = pd.read_table(\\n    \"/teamspace/studios/this_studio/embedders/data/landmark_genes/GSE92742_Broad_LINCS_gene_info_delta_landmark.txt\"\\n)\\nlandmark_genes = landmark_genes_table[\"pr_gene_symbol\"]\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing Blood Cell Embedding Data in Python\nDESCRIPTION: This snippet loads training and testing datasets (X and y) for blood cell scRNA embeddings using numpy's np.load, parameterized by an embedding name and trial number. It is the foundational data-loading step required before any further model training or analysis. Dependencies include numpy and the correct file structure; paths are constructed relative to the current script, expecting precomputed embedding npy files. Inputs are the embedding name and trial number; outputs are numpy arrays X_train, y_train, X_test, y_test, which are subsequently used throughout the analysis workflow.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/45_blood_and_lymphoma_interp.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# Load blood dataset and train model\n\nimport embedders\nimport numpy as np\n\nembedding = \"blood_cell_scrna\"\ntrial = 0\n\nX_train = np.load(f\"../data/{embedding}/embeddings/X_train_{trial}.npy\")\ny_train = np.load(f\"../data/{embedding}/embeddings/y_train_{trial}.npy\")\nX_test = np.load(f\"../data/{embedding}/embeddings/X_test_{trial}.npy\")\ny_test = np.load(f\"../data/{embedding}/embeddings/y_test_{trial}.npy\")\n```\n\n----------------------------------------\n\nTITLE: Loading Blood Cell scRNA Data Using Scanpy - Python\nDESCRIPTION: This snippet loads scRNA-seq datasets for various blood cell types from disk using Scanpy's read_10x_mtx function, storing them in a dictionary keyed by cell type. Dependencies include the Scanpy library and availability of the 10x-formatted matrices on the specified file paths. Inputs are the predefined list of blood cell types; outputs are stored in the 'data' dictionary for further processing.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/8_scrna_preprocessing.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport scanpy\\n\\ncell_types = [\\n    \"cd8_cytotoxic_t\",\\n    \"cd8_cd45ra_naive_cytotoxic_t\",\\n    \"cd14_monocytes\",\\n    \"cd19_b\",\\n    \"cd34\",\\n    \"cd4_cd25_regulatory_t\",\\n    \"cd4_cd45ra_cd25_naive_t\",\\n    \"cd4_cd45ro_memory_t\",\\n    \"cd4_helper_t\",\\n    \"cd56_natural_killer\",\\n]\\n\\ndata = {}\\nfor cell_type in cell_types:\\n    data[cell_type] = scanpy.read_10x_mtx(f\"/teamspace/studios/this_studio/embedders/data/blood_cell_scrna/{cell_type}\")\n```\n\n----------------------------------------\n\nTITLE: Generating and Saving a Matplotlib Table Visualization in Python\nDESCRIPTION: This snippet uses matplotlib and pandas to generate a table visualization of the aggregated experiment results (`grouped_df`). It first extracts the numerical mean values to determine which cells to highlight (max for classification/high-dim, min for regression). It then calculates adaptive figure dimensions based on the number of columns, rows, and label lengths. Finally, it creates a matplotlib table, applies highlighting to the appropriate cells, sets a title, and saves the table as a PNG image with a filename derived from the `VAR` metric and whether high-dimensional data was used.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/57_new_table.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Cell just for saving the styled dataframe as a figure\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Create a numerical version by extracting the mean values\nnumerical_df = grouped_df.copy()\nfor col in numerical_df.columns:\n    numerical_df[col] = numerical_df[col].str.split(\"\\u00b1\").str[0].astype(float)\n\n# Find max/min values per row for highlighting\nif PATHS in [CLS_PATHS, HIGHDIM_PATHS]:\n    # For classification, highlight max values\n    highlight_values = numerical_df.idxmax(axis=1)\nelse:\n    # For regression, highlight min values\n    highlight_values = numerical_df.idxmin(axis=1)\n\nis_highlight = pd.DataFrame(False, index=numerical_df.index, columns=numerical_df.columns)\nfor idx, col in zip(highlight_values.index, highlight_values.values):\n    is_highlight.loc[idx, col] = True\n\n# Calculate adaptive figure dimensions\n# Base width on number of columns and max text length\ncol_widths = [len(str(c)) + 10 for c in grouped_df.columns]  # Add padding for the values\ncol_count = len(grouped_df.columns)\n\n# Get max length of row labels (might be multi-index)\nif isinstance(grouped_df.index, pd.MultiIndex):\n    row_label_width = max(len(\", \".join(str(x) for x in i)) for i in grouped_df.index)\nelse:\n    row_label_width = max(len(str(i)) for i in grouped_df.index)\n\n# Calculate total width needed (in inches)\ntotal_width = (sum(col_widths) * 0.1) + (row_label_width * 0.12)\n# Set a minimum width\ntotal_width = max(total_width, 10)\n\n# Height based on row count (adjust based on your data size)\nrow_height = 0.4\ntotal_height = max(len(grouped_df) * row_height, 5)\n\n# Create figure with adaptive dimensions\nplt.figure(figsize=(total_width, total_height))\nax = plt.subplot(111, frame_on=False)\n\n# Hide axes\nax.xaxis.set_visible(False)\nax.yaxis.set_visible(False)\n\n# Create the table\ntable = ax.table(\n    cellText=grouped_df.values,\n    rowLabels=grouped_df.index,\n    colLabels=grouped_df.columns,\n    cellLoc=\"center\",\n    loc=\"center\",\n    bbox=[0, 0, 1, 1],\n)\n\n# Style the table\ntable.auto_set_font_size(False)\ntable.set_fontsize(10)\n\n# Apply highlighting\nfor i in range(len(grouped_df.index)):\n    for j in range(len(grouped_df.columns)):\n        if is_highlight.iloc[i, j]:\n            table[(i + 1, j)].set_facecolor(\"lightgreen\")\n\n# Save the figure\nplt.title(f\"Table of {VAR} values\")\nplt.tight_layout()\nSUFFIX = \"_highdim\" if PATHS == HIGHDIM_PATHS else \"\"\nplt.savefig(f\"df_{VAR}{SUFFIX}.png\", dpi=300, bbox_inches=\"tight\")\nplt.close()\n\n# print(f\"Table saved as '{VAR}_table.png'\")\n\n```\n\n----------------------------------------\n\nTITLE: Classifier Pipeline with Adjacency Matrix (KappaGCN, Python)\nDESCRIPTION: This example demonstrates running KappaGCN with adjacency matrices prepared from kernelized manifold distances. The code generates train/test splits on manifold data, applies stereographic projection, computes kernel and adjacency matrices, and fits the KappaGCN model. Inputs include manifold data and projection; outputs are adjacency matrices and KappaGCN fit status. Dependent on torch, sklearn, manify.predictors.kappa_gcn, and get_A_hat.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/60_pytest_scratch.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom manify.predictors.kappa_gcn import get_A_hat\n\nprint(\"Testing basic classifier functionality\")\npm = ProductManifold(signature=[(-1, 2), (0, 2), (1, 2)])\nX, y = pm.gaussian_mixture(num_points=100, num_classes=2, seed=42)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\npm_stereo, X_train_stereo, X_test_stereo = pm.stereographic(X_train, X_test)\nkappa_gcn = KappaGCN(pm=pm_stereo, output_dim=2, hidden_dims=[pm.dim, pm.dim])\n\n\nX_train_kernel = torch.exp(-pm.pdist2(X_train))\nX_test_kernel = torch.exp(-pm.pdist2(X_test))\nA_train = get_A_hat(X_train_kernel)\nA_test = get_A_hat(X_test_kernel)\nkappa_gcn.fit(X_train, y_train, A=A_train, use_tqdm=False, epochs=100)\n\n```\n\n----------------------------------------\n\nTITLE: Training Product Space Perceptron with Train/Test Split (Python)\nDESCRIPTION: Splits data using sklearn, fits the perceptron on the training set, then evaluates accuracy on the test set. Requires embedders, ProductSpacePerceptron, PyTorch, and scikit-learn. Demonstrates standard machine learning workflow for model evaluation on manifolds. Inputs: dataset and test_size; outputs: classification accuracy.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n# Do the same thing with train-test split\n\nfrom sklearn.model_selection import train_test_split\n\nX, y = embedders.gaussian_mixture.gaussian_mixture(pm)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nptron = embedders.perceptron.ProductSpacePerceptron(pm)\nptron.fit(X_train, y_train)\n(ptron.predict(X_test) == y_test).float().mean()\n```\n\n----------------------------------------\n\nTITLE: Saving Second Benchmark Results to TSV File in Python\nDESCRIPTION: Saves the Pandas DataFrame containing the benchmark results (generated in the previous step using the 'embedders' library) to a Tab-Separated Values (TSV) file. The filename includes the task type ('regression' in this case) and is saved in the '../data/results_icml/' directory. The index is excluded.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/26_benchmark_single_curvature_gaussian.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nresults.to_csv(f\"../data/results_icml/{TASK}_single_curvature.tsv\", sep=\"\\t\", index=False)\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Data with ProductSpaceDT using Embedders in Python\nDESCRIPTION: Creates an instance of the `ProductSpaceDT` class from `embedders.tree_new`, associating it with the previously defined product manifold `pm`. It then executes the tree's internal `_preprocess` method on the generated data (`X`, `y`). This method likely performs the coordinate-to-angle and potentially angle-to-binary feature conversions, returning the preprocessed `angles`, original `labels`, unique `classes`, and precomputed `comparisons`.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/37_sklearn_decision_tree.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npdt = embedders.tree_new.ProductSpaceDT(pm=pm)\nangles, labels, classes, comparisons = pdt._preprocess(X, y)\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating KappaGCN on a Product Manifold (MLP-like) in Python\nDESCRIPTION: Defines, trains, and evaluates a `manify` KappaGCN model configured with a product manifold `(0, 1)x(0,1)` and hidden dimensions. An identity matrix is passed as `A_hat` during training and evaluation, making it behave like an MLP (KappaGCN-Product MLP). The accuracy is printed.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/relationship.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npm = manify.manifolds.ProductManifold([(0, 1),(0,1)], stereographic=True)\nkgcn = manify.predictors.kappa_gcn.KappaGCN(\n    pm=pm, output_dim=num_classes, hidden_dims=[2], nonlinearity=torch.relu, task=\"classification\"\n)\nkgcn = train_model(kgcn, X_train, y_train,A_hat=torch.eye(X_train.shape[0]))\nkgcn_accuracy = evaluate_model(kgcn, X_test, y_test,A_hat=torch.eye(X_test.shape[0]))\nprint(f\"KappaGCN-Product MLP accuracy: {kgcn_accuracy}\")\n```\n\n----------------------------------------\n\nTITLE: Importing the Embedders Library in Python\nDESCRIPTION: Imports the `embedders` library, making its modules and classes available for use. This library presumably contains the implementations for product manifolds (`embedders.manifolds`), data generation (`embedders.gaussian_mixture`), and the custom decision tree (`embedders.tree_new`).\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/37_sklearn_decision_tree.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport embedders\n```\n\n----------------------------------------\n\nTITLE: Loading the Cora Dataset with Embedders in Python\nDESCRIPTION: This code utilizes the `load` function from the `embedders.dataloaders` module to fetch the Cora citation network dataset. It returns and unpacks three key components: `dists` (pairwise distances between nodes), `labels` (class labels for each node), and `adj` (adjacency information, likely representing the graph structure).\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/52_high_dim_embedding.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndists, labels, adj = embedders.dataloaders.load(\"cora\")\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating KappaGCN on a Euclidean Manifold (GCN-like) in Python\nDESCRIPTION: Defines, trains, and evaluates a `manify` KappaGCN model configured with a Euclidean manifold `(0, 2)` and hidden dimensions. This setup implies graph convolutions based on the manifold structure (KappaGCN-GCN). The accuracy is printed. Note the difference from KappaGCN-MLP in Snippet 3: no identity `A_hat` is passed here during training/evaluation.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/relationship.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npm = manify.manifolds.ProductManifold([(0, 2)], stereographic=True)\nkgcn = manify.predictors.kappa_gcn.KappaGCN(\n    pm=pm, output_dim=num_classes, hidden_dims=[2], nonlinearity=torch.relu, task=\"classification\"\n)\nkgcn = train_model(kgcn, X_train, y_train)\nkgcn_accuracy = evaluate_model(kgcn, X_test, y_test)\nprint(f\"KappaGCN-GCN accuracy: {kgcn_accuracy}\")\n```\n\n----------------------------------------\n\nTITLE: Training Binary Mixed-Curvature Perceptron\nDESCRIPTION: This code prepares the generated data for the `hyperdt` library using the previously defined utility functions (`get_signature_str`, `get_embed_data`). It then instantiates and trains a `mix_curv_perceptron` for binary classification (`multiclass=False`) on the product manifold data. The training parameters `max_round` and `max_update` are set, and the result of `process_data()` (likely related to training score or outcome) is printed.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmix_component = get_signature_str(pm)\nembed_data = get_embed_data(pm, X, y_relabeled)\n# Convert to (1, -1) labels:\n\nps_perceptron = mix_curv_perceptron(\n    mix_component=mix_component,\n    embed_data=embed_data,\n    multiclass=False,  # for now, just do binary\n    max_round=10_000,\n    max_update=10_000,\n)\ny_pred = ps_perceptron.process_data()\nprint(y_pred)\n```\n\n----------------------------------------\n\nTITLE: Visualizing an MNIST Sample Image Using Matplotlib in Python\nDESCRIPTION: Displays the first MNIST image by reshaping the raw data to 28x28 pixels and applying a grayscale colormap with matplotlib. Assumes X is a tensor or numpy-like array and requires matplotlib. Key parameter is X[0], which is expected to be a flattened 28x28 image; improper shape would raise an error.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/38_mnist.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n\nplt.imshow(X[0].reshape(28, 28).numpy(), cmap=\"gray\")\n```\n\n----------------------------------------\n\nTITLE: Examining Exponential Map Output (Python)\nDESCRIPTION: This snippet outputs the exponential map result (expmap_x1) calculated previously. It inspects or displays the result stored in the variable. No dependencies beyond the previous code are needed. Input is expmap_x1; output is its contents.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/60_pytest_scratch.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nexpmap_x1\n\n```\n\n----------------------------------------\n\nTITLE: Defining Network Metadata and Vertices in Pajek Format\nDESCRIPTION: This snippet specifies the network name ('PhD.net') and the total number of vertices (1025) using Pajek format directives (*Network, *Vertices). It then lists vertices sequentially, assigning each a unique integer ID, a string label enclosed in double quotes (representing a PhD holder and often their graduation year), and three floating-point values (0.0000 0.0000 0.5000), which might represent coordinates or other attributes.\nSOURCE: https://github.com/pchlenski/manify/blob/main/data/graphs/cs_phds/cs_phds.txt#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n*Network PhD.net\n*Vertices   1025\n      1 \"Stal Aanderaa 1967\"                     0.0000    0.0000    0.5000\n      2 \"Hao Wang\"                               0.0000    0.0000    0.5000\n      3 \"Martin Abadi 1987\"                      0.0000    0.0000    0.5000\n      4 \"Zohar Manna\"                            0.0000    0.0000    0.5000\n      5 \"Karl Abrahamson 1980\"                   0.0000    0.0000    0.5000\n      6 \"Michael J. Fischer 1968\"                0.0000    0.0000    0.5000\n      7 \"Hosame Abu-Amara 1988\"                  0.0000    0.0000    0.5000\n      8 \"Michael Loui 1980\"                      0.0000    0.0000    0.5000\n      9 \"John Addison 1955\"                      0.0000    0.0000    0.5000\n     10 \"Stephen Kleene 1934\"                    0.0000    0.0000    0.5000\n     11 \"Len Adleman\"                            0.0000    0.0000    0.5000\n     12 \"Manuel Blum\"                            0.0000    0.0000    0.5000\n     13 \"P.W. Adriaans 1992\"                     0.0000    0.0000    0.5000\n     14 \"Peter van_Emde_Boas 1974\"               0.0000    0.0000    0.5000\n     15 \"Richa Agarwala 1994\"                    0.0000    0.0000    0.5000\n     16 \"David Fernandez-Baca 1986\"              0.0000    0.0000    0.5000\n     17 \"Alok Aggarwal 1984\"                     0.0000    0.0000    0.5000\n     18 \"Joseph O'Rourke 1980\"                   0.0000    0.0000    0.5000\n     19 \"Ajit Agrawal 1992\"                      0.0000    0.0000    0.5000\n     20 \"Philip Klein 1988\"                      0.0000    0.0000    0.5000\n     21 \"Al Aho 1967\"                            0.0000    0.0000    0.5000\n     22 \"John Hopcroft\"                          0.0000    0.0000    0.5000\n     23 \"Bill Aiello 1988\"                       0.0000    0.0000    0.5000\n     24 \"Shafi Goldwasser 1983\"                  0.0000    0.0000    0.5000\n     25 \"Howard Aiken 1939\"                      0.0000    0.0000    0.5000\n     26 \"E.L. Chaffee\"                           0.0000    0.0000    0.5000\n     27 \"Varol Akman 1985\"                       0.0000    0.0000    0.5000\n     28 \"William Randolph Franklin 1978\"         0.0000    0.0000    0.5000\n     29 \"Romas Aleliunas\"                        0.0000    0.0000    0.5000\n     30 \"Stephen Cook\"                           0.0000    0.0000    0.5000\n     31 \"Brian Allen 1977\"                       0.0000    0.0000    0.5000\n     32 \"Ian Munro 1971\"                         0.0000    0.0000    0.5000\n     33 \"Helmut Alt 1976\"                        0.0000    0.0000    0.5000\n     34 \"Kurt Mehlhorn 1974\"                     0.0000    0.0000    0.5000\n     35 \"Nancy Amato 1995\"                       0.0000    0.0000    0.5000\n     36 \"Franco Preparata 1959\"                  0.0000    0.0000    0.5000\n     37 \"Nina Amenta 1994\"                       0.0000    0.0000    0.5000\n     38 \"Raimund Seidel 1986\"                    0.0000    0.0000    0.5000\n     39 \"Arne Andersson 1990\"                    0.0000    0.0000    0.5000\n     40 \"Svante Carlsson 1986\"                   0.0000    0.0000    0.5000\n     41 \"Dana Angluin 1976\"                      0.0000    0.0000    0.5000\n     42 \"Fred Annexstein 1990\"                   0.0000    0.0000    0.5000\n     43 \"Arny Rosenberg 1966\"                    0.0000    0.0000    0.5000\n     44 \"? Apers 1982\"                           0.0000    0.0000    0.5000\n     45 \"? van_de_Riet 1968\"                     0.0000    0.0000    0.5000\n     46 \"Krzysztof Apt 1974\"                     0.0000    0.0000    0.5000\n     47 \"Andrzej Mostowski 1938\"                 0.0000    0.0000    0.5000\n     48 \"Lars Arge 1996\"                         0.0000    0.0000    0.5000\n     49 \"Erik Meineche Schmidt 1978\"             0.0000    0.0000    0.5000\n     50 \"Eshrat Arjomandi\"                       0.0000    0.0000    0.5000\n     51 \"Derek G. Corneil 1970\"                  0.0000    0.0000    0.5000\n     52 \"Sanjeev Arora 1994\"                     0.0000    0.0000    0.5000\n     53 \"Umesh Vazirani 1986\"                    0.0000    0.0000    0.5000\n     54 \"Robert Ashenhurst 1956\"                 0.0000    0.0000    0.5000\n     55 \"James Aspnes 1992\"                      0.0000    0.0000    0.5000\n     56 \"Steven Rudich\"                          0.0000    0.0000    0.5000\n     57 \"Bengt Aspvall 1980\"                     0.0000    0.0000    0.5000\n     58 \"Robert E. Tarjan 1972\"                  0.0000    0.0000    0.5000\n     59 \"Gideon Avrahami 1994\"                   0.0000    0.0000    0.5000\n     60 \"Vaughan Pratt\"                          0.0000    0.0000    0.5000\n     61 \"Baruch Awerbuch 1984\"                   0.0000    0.0000    0.5000\n     62 \"Shimon Even 1963\"                       0.0000    0.0000    0.5000\n     63 \"Paul Axt\"                               0.0000    0.0000    0.5000\n     64 \"Salman Azhar 1994\"                      0.0000    0.0000    0.5000\n     65 \"John Reif 1977\"                         0.0000    0.0000    0.5000\n     66 \"Pieter Cornelis Baayen 1964\"            0.0000    0.0000    0.5000\n     67 \"J. de_Groot\"                            0.0000    0.0000    0.5000\n     68 \"Laszlo Babai\"                           0.0000    0.0000    0.5000\n     69 \"Paul Turan\"                             0.0000    0.0000    0.5000\n     70 \"Vera Sos\"                               0.0000    0.0000    0.5000\n     71 \"Eric Bach 1984\"                         0.0000    0.0000    0.5000\n     72 \"Leo Bachmair 1987\"                      0.0000    0.0000    0.5000\n     73 \"Nachum Dershowitz 1977\"                 0.0000    0.0000    0.5000\n     74 \"Norman I. Badler 1975\"                  0.0000    0.0000    0.5000\n     75 \"John Mylopoulos\"                        0.0000    0.0000    0.5000\n     76 \"Chandrajit Bajaj\"                       0.0000    0.0000    0.5000\n     77 \"Brenda Baker 1973\"                      0.0000    0.0000    0.5000\n     78 \"Ron Book 1969\"                          0.0000    0.0000    0.5000\n     79 \"Ted Baker 1974\"                         0.0000    0.0000    0.5000\n     80 \"Juris Hartmanis 1955\"                   0.0000    0.0000    0.5000\n     81 \"M.L. Balinsky 1951\"                     0.0000    0.0000    0.5000\n     82 \"Albert Tucker 1932\"                     0.0000    0.0000    0.5000\n     83 \"David Mix Barrington 1986\"              0.0000    0.0000    0.5000\n     84 \"Michael Sipser 1979\"                    0.0000    0.0000    0.5000\n     85 \"Yair Bartal 1993\"                       0.0000    0.0000    0.5000\n     86 \"Amos Fiat 1987\"                         0.0000    0.0000    0.5000\n     87 \"Joe Bates 1979\"                         0.0000    0.0000    0.5000\n     88 \"Alan Demers 1975\"                       0.0000    0.0000    0.5000\n     89 \"G.M. Baudet 1978\"                       0.0000    0.0000    0.5000\n     90 \"H.T. Kung 1973\"                         0.0000    0.0000    0.5000\n     91 \"Marianne Baudinet 1989\"                 0.0000    0.0000    0.5000\n     92 \"Aime J. Bayle 1992\"                     0.0000    0.0000    0.5000\n     93 \"Forouzan Golshani 1982\"                 0.0000    0.0000    0.5000\n     94 \"Robert Beals 1993\"                      0.0000    0.0000    0.5000\n     95 \"Paul Beame 1987\"                        0.0000    0.0000    0.5000\n     96 \"Donald Beaver 1990\"                     0.0000    0.0000    0.5000\n     97 \"Michael O. Rabin 1957\"                  0.0000    0.0000    0.5000\n     98 \"Richard Beigel 1988\"                    0.0000    0.0000    0.5000\n     99 \"John Gill\"                              0.0000    0.0000    0.5000\n    100 \"L.W. Beineke 1965\"                      0.0000    0.0000    0.5000\n    101 \"Frank Harary 1948\"                      0.0000    0.0000    0.5000\n    102 \"Mihir Bellare 1991\"                     0.0000    0.0000    0.5000\n    103 \"Silvio Micali 1982\"                     0.0000    0.0000    0.5000\n    104 \"Richard Bellman 1946\"                   0.0000    0.0000    0.5000\n    105 \"Solomon Lefschetz 1911\"                 0.0000    0.0000    0.5000\n    106 \"Amir Ben-Amram 1995\"                    0.0000    0.0000    0.5000\n    107 \"Zvi Galil 1975\"                         0.0000    0.0000    0.5000\n    108 \"Moti Ben-Ari 1981\"                      0.0000    0.0000    0.5000\n    109 \"Amir Pnueli 1967\"                       0.0000    0.0000    0.5000\n    110 \"Michael Ben-Or\"                         0.0000    0.0000    0.5000\n    111 \"J.D.C. Benaloh 1987\"                    0.0000    0.0000    0.5000\n    112 \"Samuel Bent 1982\"                       0.0000    0.0000    0.5000\n    113 \"Jon Bentley 1976\"                       0.0000    0.0000    0.5000\n    114 \"Donald Stanat 1967\"                     0.0000    0.0000    0.5000\n    115 \"Fran Berman\"                            0.0000    0.0000    0.5000\n    116 \"Bob Ritchie 1961\"                       0.0000    0.0000    0.5000\n    117 \"Len Berman 1977\"                        0.0000    0.0000    0.5000\n    118 \"Piotr Berman 1985\"                      0.0000    0.0000    0.5000\n    119 \"Albert Meyer\"                           0.0000    0.0000    0.5000\n    120 \"Marshall Bern 1987\"                     0.0000    0.0000    0.5000\n    121 \"Gene Lawler 1963\"                       0.0000    0.0000    0.5000\n    122 \"Andre Berthiaume 1995\"                  0.0000    0.0000    0.5000\n    123 \"Gilles Brassard 1979\"                   0.0000    0.0000    0.5000\n    124 \"Sandeep Bhatt 1984\"                     0.0000    0.0000    0.5000\n    125 \"Charles E. Leiserson\"                   0.0000    0.0000    0.5000\n    126 \"? Biezeno\"                              0.0000    0.0000    0.5000\n    127 \"---\"                                    0.0000    0.0000    0.5000\n    128 \"Gianfranco Bilardi 1985\"                0.0000    0.0000    0.5000\n    129 \"G.D. Birkhoff\"                          0.0000    0.0000    0.5000\n```\n\n----------------------------------------\n\nTITLE: Inspecting Results DataFrame Columns - Python\nDESCRIPTION: Prints or inspects the column names of the results DataFrame. Used for exploratory data analysis or debugging to verify available metrics and ensure subsequent processing selects the correct columns. Assumes 'results' variable refers to a loaded pandas DataFrame.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/43_benchmark_neural.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nresults.columns\n```\n\n----------------------------------------\n\nTITLE: Selecting PyTorch Computation Device\nDESCRIPTION: Detects and selects the appropriate PyTorch computation device (CUDA GPU, Apple MPS, or CPU) based on availability. It sets a primary `device` for general computation and a `sample_device`, which defaults to the primary device if it's CUDA, otherwise CPU, potentially for specific data sampling or generation tasks that might perform better on the CPU.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/43_benchmark_neural.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")\n\nif device != torch.device(\"cuda\"):\n    sample_device = torch.device(\"cpu\")\nelse:\n    sample_device = device\n\nprint(f\"Device: {device}, Sample Device: {sample_device}\")\n```\n\n----------------------------------------\n\nTITLE: Defining Encoder and Decoder for Mixed Curvature VAE in Python\nDESCRIPTION: This snippet defines the neural network architectures for the Encoder and Decoder components of a Mixed Curvature Variational Autoencoder (VAE). Both classes inherit from `torch.nn.Module` and take a ProductManifold object (`pm`) during initialization. The Encoder maps input data through fully connected layers to produce tangent space representations of the mean and log variance of the latent distribution, mapping the mean onto the manifold using `pm.manifold.expmap`. The Decoder maps points from the ambient embedding space of the manifold back to the original data space using fully connected layers and a sigmoid activation.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/Embedder Tutorial.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n#Define Encoder and Decoder\nclass Encoder(torch.nn.Module):\n    def __init__(self, pm):\n        super().__init__()\n        self.pm = pm\n        self.fc1 = torch.nn.Linear(1056, 200)\n        self.fc2 = torch.nn.Linear(200, 200)\n        self.fc3_z_mean = torch.nn.Linear(200, pm.dim)\n        self.fc3_z_logvar = torch.nn.Linear(200, pm.dim)\n\n    def forward(self, x):\n        # Hidden layers\n        h1 = torch.relu(self.fc1(x))\n        h2 = torch.relu(self.fc2(h1))\n\n        # Reparameterization\n        z_mean_tangent = self.fc3_z_mean(h2)\n        z_logvar = self.fc3_z_logvar(h2)\n        z_mean = pm.manifold.expmap(x=pm.mu0, u=z_mean_tangent @ pm.projection_matrix)\n\n        return z_mean, z_logvar\n\n\nclass Decoder(torch.nn.Module):\n    def __init__(self, pm):\n        super().__init__()\n        self.pm = pm\n        self.fc1 = torch.nn.Linear(pm.ambient_dim, 200)\n        self.fc2 = torch.nn.Linear(200, 200)\n        self.fc3 = torch.nn.Linear(200, 1056)\n\n    def forward(self, z):\n        # Hidden layers\n        h1 = torch.relu(self.fc1(z))\n        h2 = torch.relu(self.fc2(h1))\n\n        # Output layer\n        x = torch.sigmoid(self.fc3(h2))\n\n        return x\n```\n\n----------------------------------------\n\nTITLE: Load UCI Traffic Data from MAT File\nDESCRIPTION: This snippet imports the `loadmat` function from `scipy.io` to load data from a `.mat` file, specifically the UCI traffic dataset. It then prints the keys available in the loaded data dictionary.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/33_traffic.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom scipy.io import loadmat\n\ndata = loadmat(\"../data/traffic/traffic_dataset.mat\")\ndata.keys()\n```\n\n----------------------------------------\n\nTITLE: Generating Grid Data and Coordinate Transformations for Surface Plots - Python\nDESCRIPTION: Sets up meshgrid data on the sphere (longitude, latitude), transforms to 3D Cartesian, augments with a time/periodic variable, and reverts to original coordinates for visualization. Defines conversion functions between coordinate systems and preps data for plotting or prediction. Depends on torch and embedders, expects valid temperature embedding per month, and outputs longitude/latitude-degrees grid ready for surface plotting.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/25_benchmark_globe.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Look at plots\\n\\nimport embedders.dataloaders\\n\\n\\nN_POINTS = 400\\nSHAPE = (N_POINTS + 1, N_POINTS // 2 + 1)\\n\\ndef long_lat_to_xyz(X):\\n    long, lat = X[:, 0], X[:, 1]\\n    x = torch.cos(lat) * torch.cos(long)\\n    y = torch.cos(lat) * torch.sin(long)\\n    z = torch.sin(lat)\\n    return torch.stack([x, y, z], dim=1)\\n\\ndef xyz_to_long_lat(X):\\n    x, y, z = X[:, 0], X[:, 1], X[:, 2]\\n    long = torch.atan2(y, x)\\n    lat = torch.asin(z)\\n    return torch.stack([long, lat], dim=1)\\n\\n\\n# Get a meshgrid\\n_x_vals = torch.linspace(-torch.pi, torch.pi, SHAPE[0])\\n_y_vals = torch.linspace(-torch.pi / 2, torch.pi / 2, SHAPE[1])\\n_x_vals, _y_vals = torch.meshgrid(_x_vals, _y_vals)\\n_X = torch.stack([_x_vals.flatten(), _y_vals.flatten()], dim=1)\\n_X = long_lat_to_xyz(_X)\\n\\n# Need to add 2 dimensions for month\\n# jan_embedding = embedders.dataloaders._month_to_unit_circle_point(\"Jan\")\\n# jan_embedding = embedders.dataloaders._month_to_unit_circle_point(\"Apr\")\\njan_embedding = embedders.dataloaders._month_to_unit_circle_point(\"Jul\")\\n# jan_embedding = embedders.dataloaders._month_to_unit_circle_point(\"Oct\")\\n_X = torch.cat([_X, torch.tensor(jan_embedding).expand(_X.shape[0], -1)], dim=1)\\n\\n# Transform data to long-lat coords\\nX_transformed = xyz_to_long_lat(X)\\n\\n# For plotting, we'll need degrees\\n_x_vals_degrees = torch.rad2deg(_x_vals).numpy()\\n_y_vals_degrees = torch.rad2deg(_y_vals).numpy()\n```\n\n----------------------------------------\n\nTITLE: Saving Processed AnnData Object to Disk - Python\nDESCRIPTION: This snippet saves the merged and filtered AnnData object ('adata') to disk in the HDF5-based .h5ad format for fast reloading in future sessions. Scanpy must be installed and the target output file path accessible. There are no inputs beyond the AnnData object in memory; this step is recommended for workflow efficiency.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/8_scrna_preprocessing.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Great, that's the correct shape! Let's save it and skip all of this stuff next time\\n\\nadata.write(\"/teamspace/studios/this_studio/embedders/data/blood_cell_scrna/adata.h5ad\")\n```\n\n----------------------------------------\n\nTITLE: Evaluating Decision Trees Component-wise on H^2 x H^2 x H^2 Embedding in Python\nDESCRIPTION: This code snippet evaluates decision tree performance on each H^2 component of the H^2 x H^2 x H^2 embedding ('h2_3_cs_phds') separately. For each of the 3 components, it trains and evaluates a ProductSpaceDTRegressor (configured for a single H^2), a HyperbolicDecisionTreeRegressor, and a standard DecisionTreeRegressor using the 'cv_eval' function. The comments indicate potential issues or areas for further investigation regarding model comparison and performance.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/4_cs_phds_regression.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Does it hold up componentwise?\n# TODO: [REDACTED]: figure out why product space DT and hyperbolic DT aren't exact matches\n# TODO: [REDACTED]: also figure out why these scores are so low\nimport numpy as np # Added import based on usage\n\n# Hyperbolic\nfor i in range(3):\n    pdt_H = ProductSpaceDTRegressor(max_depth=3, signature=[(2, -1.0)])\n    cv_eval(pdt_H, \"ProductDT (H)\", h2_3_cs_phds[:, pm2.man2dim[i]], np.array(cs_labels))\n\n    hdt_H = HyperbolicDecisionTreeRegressor(max_depth=3, skip_hyperboloid_check=True)\n    cv_eval(hdt_H, \"HyperDT (H)\", h2_3_cs_phds[:, pm2.man2dim[i]], np.array(cs_labels))\n\n    dt_H = DecisionTreeRegressor(max_depth=3)\n    cv_eval(dt_H, \"DT (H)\", h2_3_cs_phds[:, pm2.man2dim[i]], np.array(cs_labels))\n\n    print()\n```\n```\n\n----------------------------------------\n\nTITLE: Training Binary Perceptron with Biased Data\nDESCRIPTION: This snippet attempts to mitigate a potential bias towards one class in the perceptron predictions. It creates a new dataset (`X_0_bias`, `y_0_bias`) by oversampling the data points belonging to class 0 (repeating them 10 times) using `torch.cat`. It then prepares this biased data using `get_embed_data` and trains a new binary `mix_curv_perceptron` (`ps_perceptron_0_bias`) on it. The training score is printed.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Can we force the perceptron to predict 0s?\nimport torch\n\nX_0_bias = torch.cat([X[y == 0]] * 10 + [X], dim=0)\ny_0_bias = torch.cat([torch.zeros_like(y[y == 0])] * 10 + [y], dim=0)\n\nembed_data_0_bias = get_embed_data(pm, X_0_bias, y_0_bias)\n\nps_perceptron_0_bias = mix_curv_perceptron(\n    mix_component=mix_component,\n    embed_data=embed_data_0_bias,\n    multiclass=False,  # for now, just do binary\n    max_round=1000,\n    max_update=100,\n)\nscore = ps_perceptron_0_bias.process_data()\nprint(score)\n```\n\n----------------------------------------\n\nTITLE: Generating Plotly Data Visualization for a Tree Node in Python\nDESCRIPTION: Creates a Plotly figure visualizing data points associated with a specific tree node. If the node is a leaf (`node_info['feature'] is None`), it displays a `go.Pie` chart showing the class distribution of data points within the `mask`. If it's an internal node, it generates a `go.Scatter` plot of the data `X` projected onto dimensions `dim1`, `dim2` (obtained from `node_info`). Points within the `mask` are colored by class using `class2color`, while points outside are grayed out. It also overlays the decision boundary visualization using `make_halfplane_background`. Depends on `numpy`, `plotly.graph_objects as go`, `collections.Counter`, `get_xy_coords`, and `make_halfplane_background`.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/49_plotly_dashboard.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef draw_data_figure(X, y, mask, node_info, class2color):\n    \"\"\"\n    - If node is a leaf => show a pie chart of label distribution (or placeholder if empty)\n    - Else => 2D scatter with half-plane background & threshold line\n    \"\"\"\n    # 1) Check if leaf\n    if node_info[\"feature\"] is None:\n        # It's a leaf. Show a pie chart of label distribution among masked data\n        y_in_leaf = y[mask]\n        if len(y_in_leaf) == 0:\n            # If no data belongs to this leaf, let's just do a small placeholder\n            fig = go.Figure()\n            fig.add_annotation(\n                x=0.5,\n                y=0.5,\n                xref=\"paper\",\n                yref=\"paper\",\n                text=f\"Leaf node {node_info['id']}<br>No data in mask\",\n                showarrow=False,\n            )\n            fig.update_layout(title=f\"Leaf node {node_info['id']} - Empty\")\n            return fig\n\n        counts = Counter(y_in_leaf.tolist())\n\n        labels = []\n        values = []\n        colors = []\n\n        # We ensure each known class is in the pie, even if 0, for consistent legend:\n        # But if you prefer to only show classes that appear, remove that logic.\n        all_classes = sorted(class2color.keys())\n        for cl in all_classes:\n            labels.append(str(cl))\n            values.append(counts.get(cl, 0))\n            colors.append(class2color[cl])\n\n        fig = go.Figure()\n        fig.add_trace(go.Pie(labels=labels, values=values, marker=dict(colors=colors), hoverinfo=\"label+value+percent\"))\n        fig.update_layout(title=f\"Leaf node {node_info['id']} - Class distribution\")\n        return fig\n\n    # 2) Non-leaf => scatter + threshold line + background\n    dim1, dim2 = node_info[\"dim1\"], node_info[\"dim2\"]\n    theta = node_info[\"theta\"]\n\n    # Convert data to plotting coords (possibly with None -> 1's)\n    x_all, y_all = get_xy_coords(X, dim1, dim2)\n    if x_all is None:\n        # Fallback if both dims are None\n        fig = go.Figure()\n        fig.update_layout(title=f\"Node {node_info['id']}: no 2D dims\")\n        return fig\n\n    x_in = x_all[mask]\n    y_in = y_all[mask]\n    ymask = y[mask]\n\n    # We'll create the figure\n    fig = go.Figure()\n\n    # a) Add half-plane coloring\n    if theta is not None:\n        background_traces = make_halfplane_background(X, dim1, dim2, theta)\n        for tr in background_traces:\n            fig.add_trace(tr)\n\n    # b) Plot points outside the mask in grey\n    fig.add_trace(\n        go.Scatter(\n            # x=x_all[~mask],\n            # y=y_all[~mask],\n            x=y_all[~mask], # NOTE: Original code seems to have swapped x and y here\n            y=x_all[~mask],\n            mode=\"markers\",\n            marker=dict(color=\"lightgray\"),\n            name=\"Excluded\",\n            hoverinfo=\"none\",\n        )\n    )\n\n    # c) Plot masked points by class color\n    unique_in_mask = np.unique(ymask)\n    for cl in unique_in_mask:\n        color = class2color[cl]  # consistent across the entire tree\n        idx = ymask == cl\n        fig.add_trace(\n        #     go.Scatter(x=x_in[idx], y=y_in[idx], mode=\"markers\", marker=dict(color=color), name=f\"Class {cl}\")\n            go.Scatter(x=y_in[idx], y=x_in[idx], mode=\"markers\", marker=dict(color=color), name=f\"Class {cl}\") # NOTE: Original code seems to have swapped x and y here\n        )\n        \n\n    # d) Finally set the axis range to the bounding box of the data (so we don't zoom out).\n    x_min, x_max = x_all.min(), x_all.max()\n    y_min, y_max = y_all.min(), y_all.max()\n    pad_x = 0.05 * (x_max - x_min) if x_max > x_min else 1\n    pad_y = 0.05 * (y_max - y_min) if y_max > y_min else 1\n\n    fig.update_layout(\n        title=f\"Node {node_info['id']} (feature={node_info['feature']}, ={theta:.2f})\",\n        # xaxis=dict(range=[x_min - pad_x, x_max + pad_x]),\n        # yaxis=dict(range=[y_min - pad_y, y_max + pad_y]),\n        xaxis=dict(range=[y_min - pad_y, y_max + pad_y]), # NOTE: Original code seems to have swapped x and y ranges\n        # yaxis=dict(range=[x_min - pad_x, x_max + pad_x]) # Swapped to match scatter plot axes\n    )\n    # Add the missing closing parenthesis for update_layout and ensure proper return\n    # (Assuming the last yaxis was intended to be uncommented and the layout finished here)\n    # If the original code had more after the yaxis comment, it's missing.\n    # Based on indentation, the layout seems finished, just missing the closing ')'\n    # return fig # This was missing, assuming it should be returned\n    # REVISED based on original text: the yaxis range was commented out and no return fig\n    # Let's assume the update_layout finishes and the function implicitly returns None or the figure is used elsewhere.\n    # For documentation purposes, we should assume it returns the figure.\n    return fig # Added return statement assuming it's intended\n\n```\n\n----------------------------------------\n\nTITLE: Calculate Maximum Values in DataFrame\nDESCRIPTION: This snippet calculates the maximum value for each column in the pandas DataFrame `df`. This is often done during exploratory data analysis to understand the range of values, which can be helpful for scaling or normalization.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/33_traffic.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Maxima are helpful for understanding how to normalize the data\n\ndf.max()\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries (embedders, torch) in Python\nDESCRIPTION: Imports the necessary libraries: `embedders` for manifold operations and custom tree implementation, and `torch` for tensor operations.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/40_component_interp.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport embedders\nimport torch\n```\n\n----------------------------------------\n\nTITLE: Importing Data Science and Geometry Libraries - Python\nDESCRIPTION: This snippet imports core libraries used throughout the file: matplotlib.pyplot for plotting, torch for tensor computations, and geoopt for hyperbolic geometry operations. All subsequent code relies on these imports, which are prerequisites for executing or understanding the file.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/36_poincare_circles.ipynb#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport matplotlib.pyplot as plt\\nimport torch\\nimport geoopt\\n\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating Basic MLP and KappaGCN-MLP Models in Python\nDESCRIPTION: Defines and trains a basic Multi-Layer Perceptron (MLP) using PyTorch's sequential API. Then defines, trains, and evaluates a KappaGCN model from the `manify` library configured with a Euclidean product manifold and hidden dimensions, simulating an MLP structure (KappaGCN-MLP). Both models use the previously defined `train_model` and `evaluate_model` functions. Accuracies are printed.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/relationship.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nnum_classes = len(torch.unique(y_train))\n\n# Basic MLP classifier\nmlp = torch.nn.Sequential(torch.nn.Linear(2, 2, bias=False), torch.nn.ReLU(), torch.nn.Linear(2, num_classes))\nmlp = train_model(mlp, X_train, y_train)\nmlp_accuracy = evaluate_model(mlp, X_test, y_test)\nprint(f\"MLP accuracy: {mlp_accuracy}\")\n\n# KappaGCN (setting kappa to 2e-4 ensures we hit the non-Euclidean case in get_logits)\npm = manify.manifolds.ProductManifold([(0, 2)], stereographic=True)\nkgcn = manify.predictors.kappa_gcn.KappaGCN(\n    pm=pm, output_dim=num_classes, hidden_dims=[2], nonlinearity=torch.relu, task=\"classification\"\n)\nkgcn = train_model(kgcn, X_train, y_train, A_hat=torch.eye(X_train.shape[0]))\nkgcn_accuracy = evaluate_model(kgcn, X_test, y_test, A_hat=torch.eye(X_test.shape[0]))\nprint(f\"KappaGCN-MLP accuracy: {kgcn_accuracy}\")\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating KappaGCN on a Product Manifold (GCN-like) in Python\nDESCRIPTION: Defines, trains, and evaluates a `manify` KappaGCN model configured with a product manifold `(0, 1)x(0,1)` and hidden dimensions. This setup implicitly uses graph convolutions based on the manifold structure (KappaGCN-Product GCN). The accuracy is printed.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/relationship.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npm = manify.manifolds.ProductManifold([(0, 1),(0,1)], stereographic=True)\nkgcn = manify.predictors.kappa_gcn.KappaGCN(\n    pm=pm, output_dim=num_classes, hidden_dims=[2], nonlinearity=torch.relu, task=\"classification\"\n)\nkgcn = train_model(kgcn, X_train, y_train)\nkgcn_accuracy = evaluate_model(kgcn, X_test, y_test)\nprint(f\"KappaGCN-Product GCN accuracy: {kgcn_accuracy}\")\n```\n\n----------------------------------------\n\nTITLE: Training Logistic Regression and KappaGCN-MLR Models in Python\nDESCRIPTION: Trains a standard Multinomial Logistic Regression model using `sklearn.linear_model.LogisticRegression` after standardizing the data. Defines, trains, and evaluates a KappaGCN model from `manify` configured without hidden layers to simulate Logistic Regression (KappaGCN-MLR). Accuracies for both models are printed.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/relationship.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\n\n# Standardize the data\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Train a multinomial logistic regression model\nlog_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\nlog_reg.fit(X_train_scaled, y_train)\n\n# Evaluate the model\nlog_reg_accuracy = log_reg.score(X_test_scaled, y_test)\nprint(f\"Logistic Regression accuracy: {log_reg_accuracy}\")\n\n\n\npm = manify.manifolds.ProductManifold([(0, 2)], stereographic=True)\nkgcn = manify.predictors.kappa_gcn.KappaGCN(\n    pm=pm, output_dim=num_classes, hidden_dims=[], nonlinearity=torch.relu, task=\"classification\"\n)\nkgcn = train_model(kgcn, X_train, y_train, A_hat=torch.eye(X_train.shape[0]))\nkgcn_accuracy = evaluate_model(kgcn, X_test, y_test, A_hat=torch.eye(X_test.shape[0]))\nprint(f\"KappaGCN-MLR accuracy: {kgcn_accuracy}\")\n```\n\n----------------------------------------\n\nTITLE: Training and Predicting with ProductSpace Perceptron using Manify\nDESCRIPTION: Initializes a ProductSpace Perceptron model (`ProductSpacePerceptron`) from the manify library, associating it with the previously defined ProductManifold (`pm`). The perceptron is then trained using the training data (`X_train`, `y_train`) and subsequently used to make predictions on the test data (`X_test`).\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/60_pytest_scratch.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nperceptron = manify.predictors.perceptron.ProductSpacePerceptron(pm=pm)\n\nperceptron.fit(X_train, y_train)\nperceptron.predict(X_test)\n```\n\n----------------------------------------\n\nTITLE: Selecting Computation Device (CPU/GPU/MPS) in Python with PyTorch\nDESCRIPTION: This code checks for the availability of CUDA (NVIDIA GPU) and MPS (Apple Metal Performance Shaders) backends in PyTorch. It sets the primary computation `device` to CUDA if available, then MPS if available, otherwise defaulting to CPU. It also defines a `sample_device`, which is set to CPU if the primary device is not CUDA, otherwise it's the same as the primary device. This might be used for operations where CPU sampling is preferred or necessary.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/27_benchmark_signature_gaussian.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")\n\nif device != torch.device(\"cuda\"):\n    sample_device = torch.device(\"cpu\")\nelse:\n    sample_device = device\n\nprint(f\"Device: {device}, Sample Device: {sample_device}\")\n```\n\n----------------------------------------\n\nTITLE: Generating Plotly Half-Plane Background for Decision Boundary in Python\nDESCRIPTION: Creates Plotly traces (`go.Heatmap`, `go.Scatter`) to visualize a 2D decision boundary defined by an angle `theta`. It computes the boundary based on the specified dimensions (`dim1`, `dim2`) of the data `X`, generates a semi-transparent heatmap coloring the two resulting half-planes using a discrete colorscale, and draws the threshold line itself slightly outside the data's bounding box. Depends on `numpy`, `plotly.graph_objects as go`, and the `get_xy_coords` helper function. Returns an empty list if both dimensions are `None`.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/49_plotly_dashboard.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef make_halfplane_background(X, dim1, dim2, theta, line_color=\"red\"):\n    \"\"\"\n    Colors the plane into two semi-transparent half-planes.\n    We also draw the threshold boundary line, but only slightly outside the bounding box\n    so it doesn't force the entire plot to be huge.\n    \"\"\"\n    c = np.cos(theta)\n    s = np.sin(theta)\n\n    x_data, y_data = get_xy_coords(X, dim1, dim2)\n    if x_data is None:  # Means both dims None => skip\n        return []\n\n    x_min, x_max = x_data.min(), x_data.max()\n    y_min, y_max = y_data.min(), y_data.max()\n\n    # Let's build a grid from (x_min, x_max) x (y_min, y_max).\n    N = 50\n    x_lin = np.linspace(x_min, x_max, N)\n    y_lin = np.linspace(y_min, y_max, N)\n    xx, yy = np.meshgrid(x_lin, y_lin)\n\n    # For illustration: left side if (s*x + c*y) > 0, else right side\n    Z = (s * xx + c * yy > 0).astype(int)\n\n    # Discrete 2-color scale\n    colorscale = [\n        [0.0, \"rgba(0,0,200,0.08)\"],  # for Z=0\n        [0.4999999, \"rgba(0,0,200,0.08)\"],\n        [0.5, \"rgba(200,0,0,0.08)\"],  # for Z=1\n        [1.0, \"rgba(200,0,0,0.08)\"],\n    ]\n\n    heatmap = go.Heatmap(x=x_lin, y=y_lin, z=Z, opacity=0.3, showscale=False, colorscale=colorscale, hoverinfo=\"none\")\n\n    # Draw the boundary line just outside data range\n    dx = x_max - x_min\n    dy = y_max - y_min\n    bound_size = max(dx, dy) * 1.2  # scale a bit bigger than data\n    x_line = [-s * bound_size, s * bound_size]\n    y_line = [-c * bound_size, c * bound_size]\n\n    line = go.Scatter(x=x_line, y=y_line, mode=\"lines\", line=dict(color=line_color), name=\"Threshold\")\n    return [heatmap, line]\n```\n\n----------------------------------------\n\nTITLE: Running Embedding Benchmarks with Product Manifold - Python\nDESCRIPTION: Executes machine learning benchmarks over various embedding types and manifold signatures using predefined models. Loads training and testing data, performs device-aware conversion to tensors, randomly subsamples large datasets for efficiency, and records evaluation metrics using a custom benchmarking utility. Requires the 'embedders' Python package, PyTorch, NumPy, and pandas; expects datasets structured under specific file paths and relies on a tqdm progress bar for iteration tracking.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/43_benchmark_neural.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# print(bad)\n\nresults = []\nmy_tqdm = tqdm(total=len(embeddings_names) * N_SAMPLES)\nfor embedding, sig in zip(embeddings_names, sigs):\n    pm = embedders.manifolds.ProductManifold(signature=sig)\n    for trial in range(N_SAMPLES):\n        # try:\n        X_train = np.load(f\"embedders/data/{embedding}/embeddings/X_train_{trial}.npy\")\n        y_train = np.load(f\"embedders/data/{embedding}/embeddings/y_train_{trial}.npy\")\n        X_test = np.load(f\"embedders/data/{embedding}/embeddings/X_test_{trial}.npy\")\n        y_test = np.load(f\"embedders/data/{embedding}/embeddings/y_test_{trial}.npy\")\n\n        # Convert to torch tensors\n        X_train = torch.tensor(X_train, device=device)\n        y_train = torch.tensor(y_train, device=device)\n        X_test = torch.tensor(X_test, device=device)\n        y_test = torch.tensor(y_test, device=device)\n\n        # Randomly subsample\n        if len(X_train) > DOWNSAMPLE:\n            idx = np.random.choice(X_train.shape[0], DOWNSAMPLE, replace=False)\n            X_train = X_train[idx]\n            y_train = y_train[idx]\n\n        if len(X_test) > DOWNSAMPLE:\n            idx = np.random.choice(X_test.shape[0], DOWNSAMPLE, replace=False)\n            X_test = X_test[idx]\n            y_test = y_test[idx]\n\n        model_results = embedders.benchmarks.benchmark(\n            X=None,\n            y=None,\n            X_train=X_train,\n            X_test=X_test,\n            y_train=y_train,\n            y_test=y_test,\n            pm=pm,\n            # models=[\"sklearn_dt\", \"product_dt\"],\n            max_depth=MAX_DEPTH,\n            batch_size=1,\n            n_features=N_FEATURES,\n            models=MODELS,\n            task=\"classification\",\n            score=[\"f1-micro\", \"accuracy\"],\n            device=device,\n        )\n        # res[\"embedding\"] = embedding\n        # res[\"trial\"] = trial\n\n        # results.append(res)\n        # my_tqdm.update(1)\n\n        run_results = {\"embedding\": embedding, \"seed\": trial}\n\n        # Flatten the nested model results\n        for model, metrics in model_results.items():\n            for metric, value in metrics.items():\n                run_results[f\"{model}_{metric}\"] = value\n\n        results.append(run_results)\n        # except Exception as e:\n        # print(f\"Error: {e}\")\n        my_tqdm.update(1)\n\nresults = pd.DataFrame(results)\nresults.to_csv(f\"embedders/data/results/all_nn_vae.tsv\", sep=\"\\t\", index=False)\n\n```\n\n----------------------------------------\n\nTITLE: Define and Apply Feature Engineering Function for UCI Data\nDESCRIPTION: Defines a function `make_dataset` that takes input features (X), target values (Y), and pre-calculated coordinates. It iterates through the data, extracts location coordinates, weekday, and hour information from each row of the dense matrix representation. Weekday and hour are converted to cyclical features using sine and cosine transformations. The function returns engineered features (X_out) and corresponding targets (y_out) as NumPy arrays. It's then called with the loaded UCI training data and MDS coordinates.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/33_traffic.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Make dataset\n\n\ndef make_dataset(X, Y, coords):\n    X_out, y_out = [], []\n    for x, y in zip(X, Y):\n        # x is a 36x48 sparse matrix\n        for row, coord in zip(x.todense(), coords):\n            row = np.array(row).flatten()\n            print(row.shape)\n            embedding = [coord[0], coord[1]]\n\n            # Get week day\n            print(row[10:17])\n            weekday = np.argmax(row[10:17])\n            assert sum(row[10:17]) == 1\n            weekday_angle = weekday / 7 * 2 * np.pi\n            embedding += [np.cos(weekday_angle), np.sin(weekday_angle)]\n\n            # Get hour\n            hour = np.argmax(row[17:41])\n            # assert sum(row[17:41]) == 1\n            hour_angle = hour / 24 * 2 * np.pi\n            embedding += [np.cos(hour_angle), np.sin(hour_angle)]\n\n            X_out.append(embedding)\n            y_out.append(y)\n\n    return np.array(X_out), np.array(y_out)\n\n\nX_train, y_train = make_dataset(data[\"tra_X_tr\"][0], data[\"tra_Y_tr\"][0], coords)\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating KappaGCN on a Negative Curvature Manifold (GCN-like) in Python\nDESCRIPTION: Defines, trains, and evaluates a `manify` KappaGCN model configured with a negative curvature manifold `(-1, 2)` and hidden dimensions. This setup uses graph convolutions based on the manifold structure (KappaGCN-Kappa GCN). The accuracy is printed.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/relationship.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\npm = manify.manifolds.ProductManifold([(-1, 2)], stereographic=True)\nkgcn = manify.predictors.kappa_gcn.KappaGCN(\n    pm=pm, output_dim=num_classes, hidden_dims=[2], nonlinearity=torch.relu, task=\"classification\"\n)\nkgcn = train_model(kgcn, X_train, y_train)\nkgcn_accuracy = evaluate_model(kgcn, X_test, y_test)\nprint(f\"KappaGCN-Kappa GCN accuracy: {kgcn_accuracy}\")\n```\n\n----------------------------------------\n\nTITLE: Importing Core Libraries for Data Science and Visualization (Python)\nDESCRIPTION: This code imports essential libraries for numerical computation (`torch`, `numpy`), interactive web-based visualization (`plotly`, `dash`), data structures (`collections`), static plotting (`matplotlib`), and a specific function (`_angular_greater`) from a custom tree predictor module (`embedders.predictors.tree_new`). These imports set up the necessary tools for subsequent analysis or application development. Requires installation of the listed libraries and the custom `embedders` package.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/49_plotly_dashboard.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport numpy as np\nimport plotly.graph_objects as go\nfrom dash import Dash, dcc, html, Input, Output\nfrom collections import deque, Counter\n\nimport matplotlib\nimport matplotlib.cm\n\nfrom embedders.predictors.tree_new import _angular_greater\n\n######################\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating KappaGCN on a Negative Curvature Manifold (MLP-like) in Python\nDESCRIPTION: Defines, trains, and evaluates a `manify` KappaGCN model configured with a negative curvature manifold `(-1, 2)` and hidden dimensions. An identity matrix `A_hat` is provided, making it behave like an MLP (KappaGCN-Kappa MLP). The accuracy is printed.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/relationship.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\npm = manify.manifolds.ProductManifold([(-1, 2)], stereographic=True)\nkgcn = manify.predictors.kappa_gcn.KappaGCN(\n    pm=pm, output_dim=num_classes, hidden_dims=[2], nonlinearity=torch.relu, task=\"classification\"\n)\nkgcn = train_model(kgcn, X_train, y_train,A_hat=torch.eye(X_train.shape[0]))\nkgcn_accuracy = evaluate_model(kgcn, X_test, y_test,A_hat=torch.eye(X_test.shape[0]))\nprint(f\"KappaGCN-Kappa MLP accuracy: {kgcn_accuracy}\")\n```\n\n----------------------------------------\n\nTITLE: Loading Tabular Data ('lymphoma') in Python\nDESCRIPTION: This snippet loads the 'lymphoma' dataset using the `dataloaders` module from the 'embedders' library. It retrieves the feature matrix `X` and the corresponding labels `y`, then prints their shapes.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/Embedder Tutorial.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n#Load data\nX, y, _ = dataloaders.load(\"lymphoma\")\nprint(X.shape, y.shape)\n```\n\n----------------------------------------\n\nTITLE: Embedding Time Series Data onto Product Manifold (S1)^d for Regression in Python\nDESCRIPTION: Defines a function `get_sample_regression` similar to `get_sample`, but tailored for a regression task. It randomly samples time points (without stratification), calculates the phase relative to the top frequencies identified previously, converts these to (cos, sin) coordinates on S1 for each frequency, and concatenates them. Instead of a binary label, it assigns the actual voltage value at the sampled time point as the label.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/30_fourier.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# This is the part where we embed into (S1)^d\n\nN_SAMPLES = 1_000\n\n\ndef get_sample_regression(seed):\n    np.random.seed(seed)\n    data = []\n    labels = []\n    for idx in np.random.choice(np.arange(X.shape[0]), size=N_SAMPLES, replace=False):\n        # Get period for X\n        periods = [idx / f for f in top_freqs]\n        # periods = [p % 1 for p in periods]\n\n        # Convert to angles\n        angles = [np.pi * 2 * p for p in periods]\n\n        # Convert to xs and ys\n        xs = [np.cos(theta) for theta in angles]\n        ys = [np.sin(theta) for theta in angles]\n\n        data.append([[x, y] for x, y in zip(xs, ys)])\n\n        # Also sample a label\n        label = X[idx]\n        labels.append(label)\n\n    data = np.stack(data, axis=0).reshape(N_SAMPLES, 2 * N_MANIFOLDS)\n    labels = np.array(labels)\n\n    return data, labels\n\n\ndata, labels = get_sample_regression(0)\nprint(data.shape, labels.shape)\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Data using Embedders\nDESCRIPTION: This snippet uses the `embedders` library to create a `ProductManifold` instance consisting of Hyperbolic (-1, 2), Euclidean (0, 2), and Spherical (1, 2) components. It then generates synthetic data (features `X` and labels `y`) based on a Gaussian mixture model within this manifold. The labels `y` are subsequently relabeled to `y_relabeled` with values -1 and 1 for binary classification.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Embedders part: generate data\n\npm = embedders.manifolds.ProductManifold([(-1, 2), (0, 2), (1, 2)])\nX, y = embedders.gaussian_mixture.gaussian_mixture(pm)\ny_relabeled = y * 2 - 1\n```\n\n----------------------------------------\n\nTITLE: Generating Gaussian Mixture Data on a Product Manifold using Embedders in Python\nDESCRIPTION: Imports the `gaussian_mixture` module from `embedders`. Defines a `ProductManifold` with two hyperbolic components (signature -1) of dimension 4 and one Euclidean component (signature 1) of dimension 4. Generates 1000 data points (`X`) and corresponding labels (`y`) from a Gaussian mixture distribution defined on this manifold, using specified covariance scaling for the means.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_basic_visualization.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport embedders.gaussian_mixture\n\n\npm = embedders.manifolds.ProductManifold(signature=[(1, 4), (-1, 4)])\n\nX, y = embedders.gaussian_mixture.gaussian_mixture(pm, 1000, cov_scale_means=0.1)\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Configuring Warnings in Python\nDESCRIPTION: This snippet imports necessary libraries for the script: PyTorch for tensor computations, the custom 'embedders' library (likely containing manifold and benchmarking tools), NumPy for numerical operations, Matplotlib for plotting (though not used in the visible snippets), pandas for data manipulation (like results storage), and tqdm for progress bars. It also filters out UserWarnings, specifically those that might occur during Wishart distribution sampling within Gaussian mixture generation.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/27_benchmark_signature_gaussian.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport embedders\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom tqdm.notebook import tqdm\n# from tqdm import tqdm\n\n# Filter out warnings raised when sampling Wishart distribution in Gaussian mixtures\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n```\n\n----------------------------------------\n\nTITLE: Initializing, Sampling, and Visualizing a Product Manifold\nDESCRIPTION: Imports `matplotlib.pyplot` for plotting. Defines a `signature` specifying the curvature and dimension of component manifolds. Creates a `ProductManifold` based on this signature. Samples a batch of points from the product manifold and visualizes the resulting tensor data as a matrix heat map.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/5_sampling.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n\nsignature = [(-2, 2), (-1, 2), (0, 2), (1, 2), (2, 2)]\npm = ProductManifold(signature)\n\nz_sample = pm.sample([1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0] * 20)\n\nplt.matshow(z_sample.detach().numpy().reshape(20, 14))\nplt.colorbar()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Testing Different Constraint Combinations for ProductSpaceSVM (Python)\nDESCRIPTION: Runs ProductSpaceSVM training for all combinations of H, S, E constraints, handling exceptions and reporting which configurations succeed. Inputs: pm, data X and y, boolean constraint flags. Useful for systematically validating constraint implementations and model stability.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_42\n\nLANGUAGE: python\nCODE:\n```\n# Which versions of constraints pass?\n\nfor _h in [True, False]:\n    for _s in [True, False]:\n        for _e in [True, False]:\n            ps_svm = ProductSpaceSVM(pm=pm, h_constraints=_h, s_constraints=_s, e_constraints=_e)\n            try:\n                ps_svm.fit(X, y)\n                print(f\"H: {_h}, S: {_s}, E: {_e}\")\n            except:\n                pass\n```\n\n----------------------------------------\n\nTITLE: Running Runtime Scaling Benchmark for Product Manifold Models - Python\nDESCRIPTION: This code iterates over increasing dataset sizes, simulating classification data on a product manifold, moving data to a computing device, and benchmarking several models with embedders' benchmarking utilities. Results are flattened and accumulated into a pandas DataFrame, used for downstream analysis. Dependencies: embedders, torch, pandas. Inputs: hyperparameters and device; outputs: DataFrame 'results' with performance and timing metrics.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/43_benchmark_neural.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# Run\n\nsig = [(-1, 2), (1, 2)]\npm = embedders.manifolds.ProductManifold(signature=sig)\nresults = []\n\nfor size in [8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096]:  # , 8192]:\n    X, y = embedders.gaussian_mixture.gaussian_mixture(\n        pm=pm,\n        seed=0,\n        num_points=size,\n        num_classes=8,\n        num_clusters=32,\n        cov_scale_means=1.0 / 2,\n        cov_scale_points=1.0 / 2,\n        task=\"classification\",\n    )\n    X = X.to(device)\n    y = y.to(device)\n    pm = pm.to(device)\n\n    model_results = embedders.benchmarks.benchmark(\n        X,\n        y,\n        pm,\n        max_depth=MAX_DEPTH,\n        task=\"classification\",\n        score=[\"f1-micro\", \"accuracy\"],\n        seed=0,\n        n_features=N_FEATURES,\n        device=device,\n    )\n\n    # Create a flat dictionary for this run\n    run_results = {\"size\": size}\n\n    # Flatten the nested model results\n    for model, metrics in model_results.items():\n        for metric, value in metrics.items():\n            run_results[f\"{model}_{metric}\"] = value\n\n    results.append(run_results)\n\nresults = pd.DataFrame(results)\n```\n\n----------------------------------------\n\nTITLE: Generating H6 Embedding for Polblogs\nDESCRIPTION: This snippet generates an embedding for the Polblogs dataset onto a 6-dimensional hyperbolic manifold (H6). It sets a manual seed for reproducibility, defines the manifold signature, initializes a 'ProductManifold' object, rescales the input distances, and trains coordinates using 'embedders.coordinate_learning.train_coords'. Key parameters include burn-in/training iterations and learning rates. The resulting embedding is stored in 'h6_polblogs'.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/2_polblogs_benchmark.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Specify signature - useful to re-initialize the manifold here\n\ntorch.manual_seed(0)  # Not all seeds are stable - this one trains for 3000 iterations at lr=1e-2 (burn-in 1e-3)\n\nsignature = [(-1, 6)]\npm = embedders.manifolds.ProductManifold(signature=signature)\nprint(pm.name)\n\n# Rescale distances\ndists_rescaled = polblogs_dists / polblogs_dists.max()\n\n# Get embedding\nh6_polblogs, _ = embedders.coordinate_learning.train_coords(\n    pm,\n    dists_rescaled,\n    device=device,\n    burn_in_iterations=400,\n    training_iterations=400 * 9,\n    learning_rate=1e-1,\n    burn_in_learning_rate=1e-2,\n    scale_factor_learning_rate=1e-1,\n)\nh6_polblogs = h6_polblogs.\n```\n\n----------------------------------------\n\nTITLE: Visualizing Estimated vs. Ground Truth Information Gains in Python with Matplotlib\nDESCRIPTION: This snippet uses `matplotlib.pyplot` to create a side-by-side comparison of the estimated information gains (`ig_est`), the ground truth information gains (`info_gains`), and their absolute difference. It reshapes a slice of the data for visualization using `matshow` and adds colorbars and titles for clarity. `plt.tight_layout()` adjusts spacing, and `plt.show()` displays the plot.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/13_information_gain.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfig, axs = plt.subplots(1, 3)\nplt.colorbar(axs[0].matshow(ig_est[: 32 * 16].reshape(-1, 32)))\naxs[0].set_title(\"Estimated\")\n\nplt.colorbar(axs[1].matshow(info_gains[: 32 * 16, 1:].reshape(-1, 32)))\naxs[1].set_title(\"Ground truth\")\n\nplt.colorbar(axs[2].matshow((ig_est[: 32 * 16] - info_gains[: 32 * 16, 1:]).abs().reshape(-1, 32)))\naxs[2].set_title(\"Difference\")\n\nplt.tight_layout()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Installing OGB Package - Python\nDESCRIPTION: This snippet demonstrates installing the 'ogb' library via pip, a required dependency for accessing Open Graph Benchmark datasets. The command should be run in a Jupyter notebook cell or a shell with pip available. It installs all necessary modules to utilize OGB's dataset interfaces. Note that internet connectivity and sufficient package permissions are prerequisites.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/19_opengraphbenchmark.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npip install ogb\n```\n\n----------------------------------------\n\nTITLE: Generating Plotly Tree Visualization from Edges and Positions in Python\nDESCRIPTION: Constructs a Plotly figure (`go.Figure`) to visualize a tree structure based on pre-calculated edges and node positions. It takes a list of edges (connecting parent and child node IDs) and a dictionary mapping node IDs to their (x, y) positions. The function creates scatter traces for both the nodes (markers with hover text showing node ID) and edges (gray lines). The layout is configured for event handling ('clickmode'). Requires `plotly.graph_objects as go`.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/49_plotly_dashboard.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef make_tree_figure(edges, positions):\n    \"\"\"\n    Build a Plotly figure for the tree given (src->dst) edges and {node_id: (x, y)} positions.\n    \"\"\"\n    edge_x, edge_y = [], []\n    for src, dst in edges:\n        x0, y0 = positions[src]\n        x1, y1 = positions[dst]\n        edge_x += [x0, x1, None] # None separates line segments\n        edge_y += [y0, y1, None]\n\n    edge_trace = go.Scatter(\n        x=edge_x, y=edge_y, mode=\"lines\", line=dict(width=1, color=\"gray\"), hoverinfo=\"none\", name=\"Edges\"\n    )\n\n    node_x = []\n    node_y = []\n    node_ids = []\n    for nid, (xx, yy) in positions.items():\n        node_x.append(xx)\n        node_y.append(yy)\n        node_ids.append(nid)\n\n    node_trace = go.Scatter(\n        x=node_x,\n        y=node_y,\n        mode=\"markers\",\n        marker=dict(size=12, color=\"blue\"),\n        text=[str(i) for i in node_ids],\n        hoverinfo=\"text\",\n        customdata=node_ids,\n        name=\"Nodes\",\n    )\n\n    fig = go.Figure(data=[edge_trace, node_trace])\n    fig.update_layout(title=\"Decision Tree (manual BFS layout)\", showlegend=False, clickmode=\"event+select\")\n    return fig\n```\n\n----------------------------------------\n\nTITLE: Verifying Log-Likelihood Calculation on Non-trivial Product Manifolds\nDESCRIPTION: Extends the log-likelihood verification to `ProductManifold` instances composed of multiple component manifolds, specifically `[(K, 4), (K, 4), (0, 4)]` for varying `K`. This tests the `log_likelihood` function on more complex product spaces involving different curvatures, again comparing the generating distribution (Q(z)) likelihood against the default prior (P(z)).\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/5_sampling.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Repeat in a nontrivial manifold\n\n# Verify that log-likelihood functions work:\nfor K in [-2, -1.0, -0.5, 0, 0.5, 1.0, 2.0]:\n    print(K)\n    m = ProductManifold(signature=[(K, 4), (K, 4), (0, 4)])\n    # Pick a random point to use as the center\n    mu = m.sample(m.mu0)\n    Sigma = torch.diag(torch.randn(m.dim)) ** 2\n    samples = m.sample(z_mean=torch.cat([mu] * N_SAMPLES, dim=0), sigma=Sigma)\n    log_probs_p = m.log_likelihood(z=samples)  # Default args\n    log_probs_q = m.log_likelihood(z=samples, mu=mu, sigma=Sigma)\n    print(\n        f\"Shape: {log_probs_p.shape},\\tP(z) = {log_probs_p.mean().item():.3f},\\tQ(z) = {log_probs_q.mean().item():.3f},\\tQ(z) - P(z) = {log_probs_q.mean().item() - log_probs_p.mean().item():.3f}\"\n    )\n    print()\n\n    # Why don't we generally see Q(z) - P(z) > 0? I would think the ll of the true distribution would be higher than the ll of the wrong distribution...\n```\n\n----------------------------------------\n\nTITLE: Performing Multiple Trial Comparison of MLP/MLR vs KappaGCN Variants in Python\nDESCRIPTION: Imports additional classifiers (`SVC`, `DecisionTreeClassifier`, `RandomForestClassifier` - although unused), statistical tools (`wilcoxon`), and `pandas`. It runs a loop for 30 trials. In each trial, it generates new synthetic data using `manify.manifolds.ProductManifold.gaussian_mixture`, trains/evaluates MLP, KappaGCN-MLP, Logistic Regression, and KappaGCN-MLR models. Results are stored in a Pandas DataFrame. Finally, it performs Wilcoxon signed-rank tests to compare the paired results of MLP vs. KappaGCN-MLP and MLR vs. KappaGCN-MLR across trials and prints the test statistics.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/relationship.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import wilcoxon\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\n\nresults = pd.DataFrame(columns=[\"trial\", \"mlp\", \"kappa_mlp\", \"mlr\", \"kappa_mlr\"])\nfor i in range(30):\n    X, y = pm.gaussian_mixture(num_points=1000, num_classes=2, num_clusters=3)\n    # X = X.detach().cpu().numpy()\n    # y = y.detach().cpu().numpy()\n    X = X.detach().cpu()\n    y = y.detach().cpu()\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=i)\n    mlp = torch.nn.Sequential(torch.nn.Linear(2, 2, bias=False), torch.nn.ReLU(), torch.nn.Linear(2, num_classes))\n    mlp = train_model(mlp, X_train, y_train)\n    mlp_accuracy = evaluate_model(mlp, X_test, y_test)\n\n    pm = manify.manifolds.ProductManifold([(0, 2)], stereographic=True)\n    kgcn = manify.predictors.kappa_gcn.KappaGCN(\n        pm=pm, output_dim=num_classes, hidden_dims=[2], nonlinearity=torch.relu, task=\"classification\"\n    )\n    kgcn = train_model(kgcn, X_train, y_train, A_hat=torch.eye(X_train.shape[0]))\n    kgcnmlp_accuracy = evaluate_model(kgcn, X_test, y_test, A_hat=torch.eye(X_test.shape[0]))\n\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n\n    # Train a multinomial logistic regression model\n    log_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n    log_reg.fit(X_train_scaled, y_train)\n\n    # Evaluate the model\n    log_reg_accuracy = log_reg.score(X_test_scaled, y_test)\n\n    kgcn = manify.predictors.kappa_gcn.KappaGCN(\n        pm=pm, output_dim=num_classes, hidden_dims=[], nonlinearity=torch.relu, task=\"classification\"\n    )\n    kgcn = train_model(kgcn, X_train, y_train, A_hat=torch.eye(X_train.shape[0]))\n    kgcnmlr_accuracy = evaluate_model(kgcn, X_test, y_test, A_hat=torch.eye(X_test.shape[0]))\n\n\n\n    results.loc[i] = {\"trial\": i, \"mlp\": mlp_accuracy, \"kappa_mlp\": kgcnmlp_accuracy, \"mlr\": log_reg_accuracy,\n                      \"kappa_mlr\": kgcnmlr_accuracy}\n\nprint(results)\nprint(wilcoxon(results[\"mlp\"], results[\"kappa_mlp\"]))\nprint(wilcoxon(results[\"mlr\"], results[\"kappa_mlr\"]))\n\n```\n\n----------------------------------------\n\nTITLE: Displaying Trial Results DataFrame in Python\nDESCRIPTION: Displays the contents of the `results` Pandas DataFrame, showing the accuracy of each model (MLP, KappaGCN-MLP, MLR, KappaGCN-MLR) for each of the 30 trials.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/relationship.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nresults\n```\n\n----------------------------------------\n\nTITLE: Analyzing Distance Component Breakdown by Subspace in Python\nDESCRIPTION: This snippet uses the 'embedders.metrics.dist_component_by_manifold' function to analyze how much each subspace (each H^2 component) contributes to the overall learned distance in the product manifold 'pm2'. It takes the trained product manifold object and the embedding coordinates as input.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/4_cs_phds_regression.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n```python\n# What's the breakdown by subspace?\n\nembedders.metrics.dist_component_by_manifold(pm2, pm2.x_embed)\n```\n```\n\n----------------------------------------\n\nTITLE: Initializing and Sampling from a Hyperbolic Manifold\nDESCRIPTION: Creates an instance of the `Manifold` class representing a 4-dimensional hyperbolic space (curvature K=-1). It then calls the `sample` method, likely to generate points on this manifold based on a provided initial tensor structure.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/5_sampling.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nm_h = Manifold(-1, 4)\nm_h.sample([[1, 0, 0, 0, 0] * 3])\n```\n\n----------------------------------------\n\nTITLE: Evaluating Classifiers on Individual Manifold Components\nDESCRIPTION: This snippet evaluates different decision tree classifiers ('ProductSpaceDT', 'HyperbolicDecisionTreeClassifier', 'DecisionTreeClassifier') on each individual component (Hyperbolic, Euclidean, Spherical) of the H2 x E2 x S2 embedding. It uses a custom `cv_eval` function (not defined here) and extracts the relevant dimensions for each component. Note the hardcoded slicing for the Euclidean component and the dependency on the 'hyperdt' library for 'HyperbolicDecisionTreeClassifier'.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/2_polblogs_benchmark.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Does it hold up componentwise?\n\n# Hyperbolic\npdt_H = ProductSpaceDT(max_depth=3, signature=[(2, -1.0)])\ncv_eval(pdt_H, \"ProductDT (H)\", h2_e2_s2_polblogs[:, pm2.man2dim[0]], polblogs_labels)\n\nhdt_H = HyperbolicDecisionTreeClassifier(max_depth=3, skip_hyperboloid_check=True)\ncv_eval(hdt_H, \"HyperDT (H)\", h2_e2_s2_polblogs[:, pm2.man2dim[0]], polblogs_labels)\n\ndt_H = DecisionTreeClassifier(max_depth=3)\ncv_eval(dt_H, \"DT (H)\", h2_e2_s2_polblogs[:, pm2.man2dim[0]], polblogs_labels)\n\nprint()\n\n# Euclidean\n# For the euclidean ones, we won't use man2dim because fix_X() breaks it - we'll hardcode 3:6 instead\npdt_E = ProductSpaceDT(max_depth=3, signature=[(2, 0.0)])\ncv_eval(pdt_E, \"ProductDT (E)\", fix_X(h2_e2_s2_polblogs)[:, 3:6], polblogs_labels)\n\ndt_E = DecisionTreeClassifier(max_depth=3)\ncv_eval(dt_E, \"DT (E)\", h2_e2_s2_polblogs[:, pm2.man2dim[1]], polblogs_labels)\n\nprint()\n\n# Sphere\npdt_S = ProductSpaceDT(max_depth=3, signature=[(2, 1.0)])\ncv_eval(pdt_S, \"ProductDT (S)\", h2_e2_s2_polblogs[:, pm2.man2dim[2]], polblogs_labels)\n\nhdt_S = HyperbolicDecisionTreeClassifier(max_depth=3, skip_hyperboloid_check=True, angle_midpoint_method=\"bisect\")\ncv_eval(hdt_S, \"HyperDT (S)\", h2_e2_s2_polblogs[:, pm2.man2dim[2]], polblogs_labels)\n\ndt_S = DecisionTreeClassifier(max_depth=3)\ncv_eval(dt_S, \"DT (S)\", h2_e2_s2_polblogs[:, pm2.man2dim[2]], polblogs_labels)\n```\n\n----------------------------------------\n\nTITLE: Generating and Visualizing Synthetic Clusters with PyTorch and Matplotlib - Python\nDESCRIPTION: This code generates synthetic 2D clustered data using scikit-learn's make_blobs, converts it to PyTorch tensors, and visualizes the result with matplotlib. Dependencies include sklearn, torch, and matplotlib. The code creates variables X (features) and y (labels), which are both PyTorch tensors, and displays a scatter plot colored by class. Inputs include configuration for cluster generation; output is an interactive plot.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/54_kappa_gcn_benchmark.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport manify\\nimport torch\\nimport matplotlib.pyplot as plt\\n\\nfrom sklearn.datasets import make_blobs\\n\\ndevice = \"cpu\"\\n\\n# Make data\\nX, y = make_blobs(n_samples=1000, centers=4, n_features=2, random_state=42)\\nX = torch.tensor(X, dtype=torch.float32, device=device)\\ny = torch.tensor(y, dtype=torch.long, device=device)\\n\\n# Plot data\\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"viridis\")\\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Factorizing Flattened Covariances with ProductManifold (Python)\nDESCRIPTION: This code demonstrates the use of pm.factorize to process a flattened form of a batch covariance tensor for a product manifold. It adapts data shape for factorized Gaussian sampling. Input is the flattened covs tensor; output is the result of pm.factorize. Prerequisites: pm defined as a ProductManifold and covs is the batch covariance matrix.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/60_pytest_scratch.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npm.factorize(covs.flatten())\n\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Pandas Series Operations in Python\nDESCRIPTION: This snippet demonstrates basic operations on a Pandas Series. It creates a Series, rounds its values to one decimal place, removes duplicate entries, and then finds the two largest remaining values using `nlargest(2)`. The two largest values are then printed.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/32_big_table.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n_t1, _t2 = pd.Series([1, 2, 3, 4, 5, 5, 5.0001]).round(1).drop_duplicates().nlargest(2)\nprint(_t1)\nprint(_t2)\n```\n\n----------------------------------------\n\nTITLE: Instantiating and Training the TorchProductSpaceDT\nDESCRIPTION: Instantiates the `TorchProductSpaceDT` with the previously defined manifold signature. It ensures the internal product manifold (`tpsdt.pm`) uses the correct PyTorch device and then calls the `fit` method to train the decision tree on the input data (`data`) and labels (`classes`). Note: `data` and `classes` likely refer to the tensors `X` and `y` generated earlier.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/10_torchified_hyperdt.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Let's test it out\n\ntpsdt = TorchProductSpaceDT(signature=[(1, 2), (0, 2), (-1, 2), (-1, 2), (-1, 2)])\ntpsdt.pm = tpsdt.pm.to(device)\ntpsdt.fit(data, classes)\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating Product Space Perceptron (Python)\nDESCRIPTION: Initializes a ProductSpacePerceptron, fits it to the data, and computes the mean prediction accuracy. Relies on embedders.perceptron.ProductSpacePerceptron, expects pm, X, y, and uses PyTorch. Useful for baseline classifier assessment in product manifold feature spaces.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nptron = embedders.perceptron.ProductSpacePerceptron(pm)\n\nptron.fit(X, y)\n(ptron.predict(X) == y).float().mean()\n```\n\n----------------------------------------\n\nTITLE: Loading Electrophysiology Data from NWB File in Python\nDESCRIPTION: Imports necessary libraries (h5py for NWB/HDF5 files, numpy for numerical operations, matplotlib for plotting) and opens an NWB file containing electrophysiology data in read mode ('r'). The file handle 'data' allows access to the hierarchical data within the NWB file.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/30_fourier.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# read as hdf5 instead\nimport h5py\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# with h5py.File('/Users/[REDACTED]/embedders/data/528733620_ephys.nwb', 'r') as f:\n#     # Read the NWB file\n#     print(f.keys())\n\n#     # Get all time series\n#     ts = f['acquisition/timeseries']\n#     ts\n\ndata = h5py.File(\"../data/electrophysiology/623474383_ephys.nwb\", \"r\")\ndata\n```\n\n----------------------------------------\n\nTITLE: Generating and Inspecting Product Kernels (Python)\nDESCRIPTION: Computes the full kernel matrices and their norms for the data using embedders.kernel.product_kernel with pm, X, and X. Requires embedders library. Outputs two objects: a list of kernel matrices and a list of corresponding norms, used for kernel analysis or SVM training.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nKs, norms = embedders.kernel.product_kernel(pm, X, X)\n```\n\n----------------------------------------\n\nTITLE: Inspecting Named Parameters and State Dictionary in PyTorch (Python)\nDESCRIPTION: This snippet converts the named parameters of two PyTorch models (mlp and kgcn) into dictionaries to review model weights and biases, and prints their state dictionaries for deeper inspection. It is crucial for debugging, understanding parameter initialization, and checking weight assignments when experimenting with model transfer/reparameterization. Requires imported PyTorch models (mlp, kgcn) to be defined prior to execution; outputs dictionaries of parameter names and tensors, as well as the full state dict. Assumes models have a named_parameters method (PyTorch nn.Module).\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/relationship.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# get parameter as a dict   \n\nmlp_dict = dict(mlp.named_parameters())\nprint(mlp_dict)\n\nkgcn_dict = dict(kgcn.named_parameters())\nprint(kgcn_dict)\n\nkgcn.state_dict()\nprint(kgcn.state_dict())\n```\n\n----------------------------------------\n\nTITLE: Evaluating Product Space and Standard Decision Trees on H^2 x H^2 x H^2 Embedding in Python\nDESCRIPTION: This snippet evaluates the performance of a ProductSpaceDTRegressor (configured with the H^2 x H^2 x H^2 signature) and a standard DecisionTreeRegressor on the 'h2_3_cs_phds' embedding. It uses the previously defined 'cv_eval' function for cross-validation and prints the MSE results for both models.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/4_cs_phds_regression.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n```python\nfrom hyperdt.product_space_DT import ProductSpaceDTRegressor\nimport numpy as np # Added import based on usage\n\npdt = ProductSpaceDTRegressor(max_depth=3, signature=[(s[1], s[0]) for s in signature])\ncv_eval(pdt, \"ProductDT\", h2_3_cs_phds, np.array(cs_labels))\n\ndt2 = DecisionTreeRegressor(max_depth=3)\ncv_eval(dt2, \"DT\", h2_3_cs_phds, np.array(cs_labels))\n```\n```\n\n----------------------------------------\n\nTITLE: Adjusting Log Scale and Inner Product on Manifold (Python)\nDESCRIPTION: Demonstrates extracting factorized features with pm.factorize, setting the log scale parameter on a manifold, and computing the inner product using torch. Relies on ProductManifold (pm) from embedders package and PyTorch. Key parameters: X (data), uses the first manifold and modifies its parameter directly before using the inner product method.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n_x = pm.factorize(X)[0]\npm.P[0].manifold._log_scale = torch.nn.Parameter(torch.tensor(10.0))\npm.P[0].manifold.inner(_x, _x)\n```\n\n----------------------------------------\n\nTITLE: Embedding Karate Club Data using Coordinate Learning with Manify\nDESCRIPTION: Loads distance (`dists`) and adjacency (`adj`) data for the 'karate_club' dataset using `manify.utils.dataloaders.load_hf`. Initializes a ProductManifold. Then, it applies the coordinate learning algorithm (`manify.embedders.coordinate_learning.train_coords`) to embed the data points based on their pairwise distances onto the defined manifold. The process involves specified numbers of burn-in and training iterations, returning the learned coordinates (`X`) and training losses.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/60_pytest_scratch.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport manify\n\n# Load karate club dataset\n_, dists, adj, _ = manify.utils.dataloaders.load_hf(\"karate_club\")\npm = manify.manifolds.ProductManifold(signature=[(-1, 2), (0, 2), (1, 2)])\n\n# Run without train_test_split\nX, losses = manify.embedders.coordinate_learning.train_coords(\n    pm=pm, dists=dists, burn_in_iterations=10, training_iterations=90\n)\nlosses\n```\n\n----------------------------------------\n\nTITLE: Initializing Product Manifold and Generating Synthetic Data\nDESCRIPTION: Initializes a `ProductManifold` object from the `embedders` library with a specific signature defining the component manifolds. It then generates synthetic data (`X`, `y`) using a Gaussian mixture model tailored for the defined product manifold. Commented-out code suggests potential alternative data loading from an `.npy` file (possibly scRNA embeddings).\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/10_torchified_hyperdt.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# # Load the scRNA embeddings\n\n# data = torch.tensor(np.load(\"/teamspace/studios/this_studio/embedders/data/blood_cell_scrna/embeddings_s2_e2_h2_3.npy\"))\n# data.shape\n# idx = np.random.choice(data.shape[0], 10_000, replace=False)\n# data = data[idx]  # Take it easy\n\n# # Also, let's add that dummy dimension for E2\n# # data = torch.hstack([data[:, :3], torch.ones(data.shape[0], 1), data[:, 3:]])\n# data[0]\n\n\npm = embedders.manifolds.ProductManifold(signature=[(1, 2), (0, 2), (-1, 2), (-1, 2), (-1, 2)])\n# data = pm.sample(torch.stack([pm.mu0] * 1000))\n\nX, y = embedders.gaussian_mixture.gaussian_mixture(pm=pm)\nprint(X.shape, y.shape)\n```\n\n----------------------------------------\n\nTITLE: Regenerating Synthetic Data for Custom Perceptron\nDESCRIPTION: This snippet regenerates the initial synthetic dataset. It defines the same `ProductManifold` as before and uses `embedders.gaussian_mixture.gaussian_mixture` to create features `X` and labels `y`. The labels are again relabeled to -1 and 1 for binary classification, presumably preparing for the custom perceptron implementation.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# My version\n\n# Embedders part: generate data\nimport embedders\n\npm = embedders.manifolds.ProductManifold([(-1, 2), (0, 2), (1, 2)])\nX, y = embedders.gaussian_mixture.gaussian_mixture(pm)\ny_relabeled = y * 2 - 1\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Link Prediction on Adjective-Noun Dataset (Python)\nDESCRIPTION: Performs link prediction benchmarking on the Adjective-Noun adjacency dataset over `N_TRIALS` repetitions. It loads the 'adjnoun' dataset, normalizes distances, and repeats the established benchmarking workflow: training manifold embeddings, generating the link prediction dataset, evaluating classifiers, calculating embedding distortion, aggregating and reporting results with statistical comparisons, and saving the outcomes to `../data/graph_benchmarks/adjnoun_link.tsv`.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/22_link_prediction.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Adjnoun club\n\ndists, labels, adj = embedders.dataloaders.load(\"adjnoun\")\ndists = dists / dists.max()\n\nresults = []\nmy_tqdm = tqdm(total=N_TRIALS)\nwhile len(results) < N_TRIALS:\n    pm = ProductManifold(signature=SIGNATURE)\n    try:\n        X_embed, losses = train_coords(\n            pm,\n            dists,\n            burn_in_iterations=int(0.1 * TOTAL_ITERATIONS),\n            training_iterations=int(0.9 * TOTAL_ITERATIONS),\n            scale_factor_learning_rate=0.02,\n        )\n        assert not torch.isnan(X_embed).any()\n\n        X, y, pm_new = make_link_prediction_dataset(X_embed, pm, adj, add_dists=USE_DISTS)\n\n        res = embedders.benchmarks.benchmark(\n            X, y, pm_new, max_depth=MAX_DEPTH, task=\"classification\", use_special_dims=USE_SPECIAL_DIMS\n        )\n        res[\"d_avg\"] = embedders.metrics.d_avg(pm.pdist(X_embed), dists).item()\n        results.append(res)\n        my_tqdm.update(1)\n\n    except Exception as e:\n        print(e)\n        # print(f\"Failed iteration {len(results)}\")\n\n\n# Print results\nresults = pd.DataFrame(results)\nfor col in results.columns:\n    if col not in [\"model\", \"d_avg\"]:\n        r = results[col]\n        print(f\"{col}: {r.mean():.4f} +/- {r.std() / np.sqrt(N_TRIALS):.4f}\", end=\" \")\n\n        for col2 in results.columns:\n            if col2 not in [\"model\", col, \"d_avg\"]:\n                stat, p = wilcoxon(results[col], results[col2])\n                if p < 0.05 / 6 and results[col].mean() > results[col2].mean():\n                    print(f\"> {col2}\", end=\" \")\n\n        print()\nprint(f\"d_avg: {results['d_avg'].mean():.4f} +/- {results['d_avg'].std() / np.sqrt(N_TRIALS):.4f}\")\n\n# Save results\nresults.to_csv(\"../data/graph_benchmarks/adjnoun_link.tsv\", index=False, sep=\"\\t\")\n```\n\n----------------------------------------\n\nTITLE: Commented Code for Inspecting MLP Parameters in Python\nDESCRIPTION: A commented-out line of Python code intended to print the parameters of the trained MLP model (`mlp`). It does not execute in the current state.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/relationship.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# print(list(mlp.parameters()))\n```\n\n----------------------------------------\n\nTITLE: Calculating Spearman Correlation Between Estimated and Ground Truth Gains in Python\nDESCRIPTION: This snippet uses the `spearmanr` function from `scipy.stats` to calculate the Spearman rank correlation coefficient between the flattened, NaN-handled estimated information gains (`ig_est`) and the flattened, NaN-handled ground truth information gains (`info_gains_nonan`). This provides a measure of monotonic relationship, which can be useful if exact numerical equality fails due to precision issues.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/13_information_gain.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Guess that doesn't pass either. What's the Spearman correlation?\nfrom scipy.stats import spearmanr\n\nspearmanr(ig_est.flatten().nan_to_num(0).numpy(), info_gains_nonan[:, 1:].flatten().nan_to_num(0).numpy())\n\n# Well, at least that's good. Still don't know why it isn't exactly 1, but at this point I'm equally likely to chalk it up to a bug\n# in the HyperDT code as I am to a bug in my code.\n```\n\n----------------------------------------\n\nTITLE: Evaluating Kernel with Modified Signature (Python)\nDESCRIPTION: Constructs a ProductManifold with a custom signature and evaluates the kernel, accessing the second kernel component (index 1). Inputs are a specialized signature and the dataset X; outputs a kernel matrix component. Useful for comparative kernel experiments across manifold parameterizations.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nproduct_kernel(embedders.manifolds.ProductManifold(signature=[(-4, 2), (0, 2), (4, 2)]), X, X)[1]\n```\n\n----------------------------------------\n\nTITLE: Calculating Tree Node Positions using BFS Layout in Python\nDESCRIPTION: Determines the (x, y) coordinates for each node in the decision tree (`pdt`) for visualization purposes. It uses a Breadth-First Search (BFS) approach where node depth determines the y-coordinate (y = -depth), and nodes at the same depth are spread horizontally based on their order within the level. Requires `collections.deque` and assumes `pdt` has a `tree` attribute and nodes have `left`/`right` child attributes.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/49_plotly_dashboard.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef bfs_tree_layout(pdt, node2id):\n    \"\"\"\n    Manual BFS layout:\n      - For depth d, y = -d\n      - Spread nodes horizontally at each level\n    \"\"\"\n    root = pdt.tree\n    levels = {}\n    queue = deque([(root, 0)])\n    max_level = 0\n    while queue:\n        node, depth = queue.popleft()\n        levels.setdefault(depth, []).append(node)\n        max_level = max(max_level, depth)\n        if node.left:\n            queue.append((node.left, depth + 1))\n        if node.right:\n            queue.append((node.right, depth + 1))\n\n    positions = {}\n    for depth in range(max_level + 1):\n        row = levels.get(depth, [])\n        n_row = len(row)\n        for i, n in enumerate(row):\n            x = i - (n_row - 1) / 2\n            y = -depth\n            positions[node2id[n]] = (x, y)\n    return positions\n```\n\n----------------------------------------\n\nTITLE: Generating Sample Data on a Product Manifold in Python\nDESCRIPTION: This snippet initializes a `ProductManifold` from the `embedders` library with a specific signature. It then uses the `gaussian_mixture` function from the same library to generate sample data points (`X`) and corresponding labels (`y`) based on the defined manifold and sample size. The shapes of the generated tensors `X` and `y` are printed.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/13_information_gain.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npm = embedders.manifolds.ProductManifold(signature=[(-1, 4)])\n\nX, y = embedders.gaussian_mixture.gaussian_mixture(pm=pm, num_points=SAMPLE_SIZE)\nprint(X.shape, y.shape)\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch Device (GPU/CPU)\nDESCRIPTION: Initializes PyTorch and checks for CUDA availability. If a GPU is available, it sets the device to the first GPU ('cuda:0'); otherwise, it defaults to the CPU ('cpu'). The selected device is stored in the 'device' variable and printed.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/1_basic_test.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Torch device management\nimport torch\n\nif torch.cuda.is_available():\n    torch.cuda.set_device(0)\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")\n\nprint(f\"Using device: {device}\")\n```\n\n----------------------------------------\n\nTITLE: Embedding Time Series Data onto Product Manifold (S1)^d for Classification in Python\nDESCRIPTION: This snippet performs feature engineering by embedding time series data onto a product of circles (S1)^d. It defines the number of circles (N_MANIFOLDS), loads data for a specific sweep, computes its FFT, and identifies the top N_MANIFOLDS frequencies (excluding the DC component). A function `get_sample` is defined to sample data points: it stratifies sampling based on positive/negative voltage values, calculates the phase of each point relative to the top frequencies, converts these phases to (cos, sin) coordinates on S1 for each frequency, concatenates these coordinates, and assigns a binary label (voltage > 0). This prepares data for a classification task on the manifold.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/30_fourier.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# This is the part where we embed into (S1)^d\n\nfrom scipy.fftpack import fft, fftfreq\n\nN_MANIFOLDS = 3\nN_SAMPLES = 1_000  # Magic number : # positive samples\n\n# Load data\ndata = h5py.File(\"../data/electrophysiology/623474383_ephys.nwb\", \"r\")\nX = np.array(data[\"acquisition/timeseries/Sweep_33/data\"])\n\n# Get all fourier components\nXf = fft(X)\nfreqs = fftfreq(len(X), d=1.0)  # TODO: Figure out what the sampling rate is\n\n# Get top 5 components by magnitude\ntop_inds = np.argsort(np.abs(Xf))[-N_MANIFOLDS - 1 : -1]  # Avoid dividing by 0 this way\ntop_freqs = freqs[top_inds]\ntop_Xf = Xf[top_inds]\n\n# Stratify\nX_pos_idx = np.arange(X.shape[0])[X > 0]\nX_neg_idx = np.arange(X.shape[0])[X <= 0]\nN_SAMPLES = min(len(X_pos_idx), len(X_neg_idx), N_SAMPLES)\n\n\ndef get_sample(seed):\n    np.random.seed(seed)\n    data = []\n    labels = []\n    for my_set in [X_pos_idx, X_neg_idx]:\n        for idx in np.random.choice(my_set, size=N_SAMPLES, replace=False):\n            # Get period for X\n            periods = [idx / f for f in top_freqs]\n            # periods = [p % 1 for p in periods]\n\n            # Convert to angles\n            angles = [np.pi * 2 * p for p in periods]\n\n            # Convert to xs and ys\n            xs = [np.cos(theta) for theta in angles]\n            ys = [np.sin(theta) for theta in angles]\n\n            data.append([[x, y] for x, y in zip(xs, ys)])\n\n            # Also sample a label\n            label = X[idx] > 0\n            labels.append(label)\n\n    data = np.stack(data, axis=0).reshape(N_SAMPLES * 2, 2 * N_MANIFOLDS)\n    labels = np.array(labels)\n\n    return data, labels\n\n\ndata, labels = get_sample(0)\nprint(data.shape, labels.shape)\n```\n\n----------------------------------------\n\nTITLE: Visualize UCI Adjacency Matrix\nDESCRIPTION: This snippet uses `matplotlib.pyplot` to visualize the traffic adjacency matrix (`data[\"tra_adj_mat\"]`) loaded from the UCI dataset. The `matshow` function displays the matrix as a heatmap.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/33_traffic.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n\nplt.matshow(data[\"tra_adj_mat\"])\n```\n\n----------------------------------------\n\nTITLE: Splitting Batch Tensors by Class in PyTorch - Python\nDESCRIPTION: Defines the get_split function for separating batched comparisons and labels tensors into negative and positive classes based on a dimension and index. Uses advanced PyTorch indexing and avoids unnecessary intermediate tensors for efficiency. Requires torch, expects batched tensors as input, and returns separate comparison and label groups for both classes.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/13_information_gain.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef get_split(comparisons, labels, n, d):\n    \"\"\"\n    Split comparisons and labels into negative and positive classes\n\n    Args:\n        comparisons: (query_batch, dims, key_batch) tensor of comparisons\n        labels: (query_batch, n_classes) tensor of one-hot labels\n        n: scalar index of split\n        d: scalar dimension of split\n\n    Returns:\n        comparisons_neg: (query_batch_neg, dims, key_batch_neg) tensor of comparisons for negative class\n        labels_neg: (query_batch_neg, n_classes) tensor of one-hot labels for negative class\n        comparisons_pos: (query_batch_pos, dims, key_batch_pos) tensor of comparisons for positive class\n        labels_pos: (query_batch_pos, n_classes) tensor of one-hot labels for positive class\n    \"\"\"\n    # Get the split mask without creating an intermediate boolean tensor\n    mask = comparisons[n, d].bool()\n\n    # Use torch.where to avoid creating intermediate tensors\n    # Bear in mind, \"mask\" is typically a float, so we have to be clever\n    pos_indices = torch.where(mask)[0]\n    neg_indices = torch.where(~mask)[0]\n\n    # Split the comparisons and labels using advanced indexing\n    comparisons_neg = comparisons[neg_indices][:, :, neg_indices]\n    comparisons_pos = comparisons[pos_indices][:, :, pos_indices]\n    labels_neg = labels[neg_indices]\n    labels_pos = labels[pos_indices]\n\n    return comparisons_neg, labels_neg, comparisons_pos, labels_pos\n\n    # # Use index_select for splitting\n    # comparisons_neg = torch.index_select(comparisons, 0, neg_indices)\n    # comparisons_neg = torch.index_select(comparisons_neg, 2, neg_indices)\n\n    # comparisons_pos = torch.index_select(comparisons, 0, pos_indices)\n    # comparisons_pos = torch.index_select(comparisons_pos, 2, pos_indices)\n\n    # labels_neg = torch.index_select(labels, 0, neg_indices)\n    # labels_pos = torch.index_select(labels, 0, pos_indices)\n\n    # return comparisons_neg, labels_neg, comparisons_pos, labels_pos\n```\n\n----------------------------------------\n\nTITLE: Aggregating Benchmark Results by Dataset\nDESCRIPTION: Computes the average performance metrics across all trials for each dataset. It achieves this by grouping the `results` pandas DataFrame by the 'dataset' column and then applying the `mean()` aggregation function to the grouped data.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/28_benchmark_link_prediction.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npd.DataFrame(results).groupby(\"dataset\").mean()\n```\n\n----------------------------------------\n\nTITLE: Plotting Data Projection and Decision Boundary in Python\nDESCRIPTION: Uses `matplotlib.pyplot` and `numpy` to create a scatter plot of the data `X`, projecting it onto dimensions 9 and 11, and coloring points by their class label `y`. It then overlays a red line representing a decision boundary defined by a specific angle `theta` (2.141...). Finally, it resets the plot limits to the data range in those dimensions.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_pc_basic_visualization.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.scatter(X[:, 9].numpy(), X[:, 11].numpy(), c=y)\n\n# plot decision boundary\ntheta = 2.141489028930664\nc = np.cos(theta)\ns = np.sin(theta)\nplt.plot([-s * 1e10, s * 1e10], [-c * 1e10, c * 1e10], \"r-\")\n\n# put boundaries back\nplt.xlim(X[:, 9].min(), X[:, 9].max())\nplt.ylim(X[:, 11].min(), X[:, 11].max())\n```\n\n----------------------------------------\n\nTITLE: Ratio of Inner Products on Different Manifold Curvatures (Python)\nDESCRIPTION: Demonstrates how to compute and compare the inner products of identical data under two manifolds with different curvatures, using embedders.manifolds.Manifold. Useful for understanding manifold geometry influence. Inputs are the same tensor and curvature settings; output is their inner product ratio.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n# pm.P[-1].inner(_x, _x)pm.P[-1].curvature * pm.P[-1].scale\nimport embedders.manifolds\n\n\nembedders.manifolds.Manifold(curvature=2, dim=2).inner(_x, _x) / embedders.manifolds.Manifold(curvature=1, dim=2).inner(\n    _x, _x\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Function to Test Manifold Decision Trees in Python\nDESCRIPTION: Defines a function `test_sig` that compares three decision tree implementations on data sampled from a specified product manifold. It takes a manifold signature (`sig`), random seed, number of points, number of classes, and an optional dimension index (`fix_x_dim`) for handling Euclidean components in `hyperdt`. It generates data using `embedders.gaussian_mixture`, trains `hyperdt.torch.product_space_DT.ProductSpaceDT`, `embedders.tree.TorchProductSpaceDT`, and `embedders.tree_new.ProductSpaceDT`, prints their scores, and returns the trained models along with the data. An initial test is run with signature `[(-1, 4)]`.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/14_new_tree_testing.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef test_sig(sig, seed=42, num_points=1000, num_classes=2, fix_x_dim=None):\n    pm = embedders.manifolds.ProductManifold(signature=sig)\n\n    X, y = embedders.gaussian_mixture.gaussian_mixture(pm=pm, num_points=num_points, num_classes=num_classes, seed=seed)\n\n    sig_r = [(x, y) for (y, x) in sig]\n    if fix_x_dim is not None:\n        X_fixed = torch.concat([X[:, :fix_x_dim], torch.ones(len(X), 1), X[:, fix_x_dim:]], dim=1)\n    else:\n        X_fixed = X\n    dt_q = hyperdt.torch.product_space_DT.ProductSpaceDT(signature=sig_r)\n    dt_q.fit(X_fixed, y)\n    print(f\"[REDACTED] DT: {dt_q.score(X_fixed, y).float().mean().item():.4f}\")\n\n    dt_old = embedders.tree.TorchProductSpaceDT(signature=sig)\n    dt_old.fit(X, y)\n    print(f\"Old DT: {dt_old.score(X, y).float().mean().item():.4f}\")\n\n    dt_new = embedders.tree_new.ProductSpaceDT(pm=pm)\n    dt_new.fit(X, y)\n    print(f\"New DT: {dt_new.score(X, y).float().mean().item():.4f}\")\n\n    return dt_q, dt_old, dt_new, X, y\n\n\n_ = test_sig([(-1, 4)])\n```\n\n----------------------------------------\n\nTITLE: Performing Wilcoxon Signed-Rank Tests between Model Results (Python)\nDESCRIPTION: This snippet uses the SciPy wilcoxon function to perform paired, non-parametric statistical comparisons between accuracy results of different models. It compares the performances of multilayer perceptrons, logistic regression, and their manifold or variant versions to probe for significant accuracy differences. Requires the results DataFrame (from pandas) populated with appropriately named columns; outputs test statistics and p-values directly. Handles four specific model result comparisons and is intended as simple reporting or troubleshooting.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/relationship.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nprint(wilcoxon(results[\"mlp\"], results[\"kappa_mlp\"]))\nprint(wilcoxon(results[\"mlr\"], results[\"torchmlr\"]))\nprint(wilcoxon(results[\"mlr\"], results[\"kappa_mlr\"]))\n#this is to troubleshoot a weird result\nprint(wilcoxon(results[\"kappa_mlp\"], results[\"torchmlr\"]))\n```\n\n----------------------------------------\n\nTITLE: Extracting X, Y Coordinates with Handling for None Dimensions in Python using NumPy\nDESCRIPTION: Safely extracts two columns (dimensions) from a NumPy array `X` based on provided indices `dim1` and `dim2`. It gracefully handles cases where either `dim1` or `dim2` (or both) are `None` by substituting the corresponding coordinate array with an array of ones of the same length as `X`. This allows for plotting even when only one dimension is meaningful or specified. If both `dim1` and `dim2` are `None`, it returns `(None, None)`. Requires `numpy`.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/49_plotly_dashboard.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef get_xy_coords(X, dim1, dim2):\n    \"\"\"\n    Workaround for 'None' dimension:\n    - If dim1 is None, replace it with an array of 1s\n    - If dim2 is None, replace it with an array of 1s\n    - If both are None, returns (None, None)\n    \"\"\"\n    n = X.shape[0]\n    if dim1 is None and dim2 is None:\n        return None, None\n    if dim1 is None:\n        return np.ones(n), X[:, dim2]\n    if dim2 is None:\n        return X[:, dim1], np.ones(n)\n    # Otherwise both dims are valid\n    return X[:, dim1], X[:, dim2]\n```\n\n----------------------------------------\n\nTITLE: Creating Class-to-Color Map using Matplotlib Colormap in Python\nDESCRIPTION: Generates a dictionary mapping unique class labels found in the input array `y` to distinct RGBA color strings suitable for Plotly or other plotting libraries. It utilizes the 'tab10' colormap from `matplotlib.cm` to assign colors, ensuring consistency across different visualizations. Classes are sorted before color assignment. Requires `numpy` and `matplotlib.cm`.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/49_plotly_dashboard.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef create_class_color_map(y):\n    \"\"\"\n    Use matplotlib's tab10 to get consistent colors for each unique class in y.\n    Returns a dict {class_value: \"rgba(...)\"}.\n    \"\"\"\n    unique_classes = np.unique(y)\n    tab10 = matplotlib.cm.get_cmap(\"tab10\")\n\n    class2color = {}\n    unique_classes_sorted = sorted(unique_classes)\n\n    for i, c in enumerate(unique_classes_sorted):\n        rgba = tab10(i % 10)  # (r, g, b, alpha) in [0..1]\n        r, g, b, a = rgba\n        r, g, b = int(r * 255), int(g * 255), int(b * 255)\n        color_str = f\"rgba({r},{g},{b},{a})\"\n        class2color[c] = color_str\n\n    return class2color\n```\n\n----------------------------------------\n\nTITLE: Initializing Parameters and Checking Embedding Data for NaNs in Python\nDESCRIPTION: This snippet imports `torch`, defines lists of embedding dataset names (`EMBEDDINGS_NAMES`), manifold signatures (`SIGNATURES`, `SIGNATURES_STR`), and the number of trials (`n_trials`). It then iterates through each combination of embedding, signature, and trial, loading pre-computed embedding data from '.h5' files. Inside the loop, it checks each tensor within the loaded data for the presence of NaN values using `torch.isnan().any()`. If NaNs are found, it records the problematic combination (embedding, signature, trial, key) in the `bad` list and prints it.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/24_benchmark_graph_embeddings.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# import numpy as np\n# import pandas as pd\nimport torch\n\nEMBEDDINGS_NAMES = [\n    \"polblogs\",\n    \"cs_phds\",\n    \"cora\",\n    \"citeseer\",\n]\n\n# Copied from notebook 2\nSIGNATURES = [\n    [(-1, 2), (-1, 2)],  # HH\n    [(-1, 2), (0, 2)],  # HE\n    [(-1, 2), (1, 2)],  # HS\n    [(1, 2), (1, 2)],  # SS\n    [(1, 2), (0, 2)],  # SE\n    [(-1, 4)],  # H\n    [(0, 4)],  # E\n    [(1, 4)],  # S\n]\nSIGNATURES_STR = [\"HH\", \"HE\", \"HS\", \"SS\", \"SE\", \"H\", \"E\", \"S\"]\nn_trials = 10\n\nbad = []\nd_avs = {}\nfor embedding in EMBEDDINGS_NAMES:\n    for signature in SIGNATURES_STR:\n        for trial in range(n_trials):\n            my_data = torch.load(f\"../data/graphs/embeddings/{embedding}/{signature}_{trial}.h5\")\n            # if np.isnan(my_data).any():\n            for key in my_data.keys():\n                if torch.isnan(my_data[key]).any():\n                    bad.append((embedding, signature, trial, key))\n                    print(embedding, signature, trial, key)\nprint(bad)\n```\n\n----------------------------------------\n\nTITLE: Displaying Dimension Index Variable in Python\nDESCRIPTION: Implicitly prints the current value of the variable `dim2`. In the context of the visualization loop, this variable holds the index of the second dimension used for plotting the scatter plot and decision boundary for the currently processed node.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_basic_visualization.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndim2\n```\n\n----------------------------------------\n\nTITLE: Embedding Implementation Best Split Validation - Python\nDESCRIPTION: Validates the correctness of the embedder's best split logic by comparing indices and midpoints with the functional reference, ensuring implementation equivalence. Requires embedders.tree_new.ProductSpaceDT and torch with prior computation of ig_emb, ang_emb, comp_emb, n, d, and theta.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/13_information_gain.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Embedders check\n\nn_emb, d_emb, theta_emb = pdt.get_best_split(ig=ig_emb, angles=ang_emb, comparisons=comp_emb)\nassert n_emb == n\nassert d_emb == d\nassert torch.allclose(theta, theta_emb)\n\n```\n\n----------------------------------------\n\nTITLE: Computing Node Data Masks via BFS in Python using PyTorch\nDESCRIPTION: Performs a Breadth-First Search (BFS) starting from the root of the decision tree (`pdt.tree`) to calculate a boolean mask for each node. The mask indicates which samples from the input data `X` fall into that specific node based on the tree's decision rules (angular comparisons using `_angular_greater`). Requires `torch` for tensor operations, `collections.deque` for the BFS queue, and assumes `pdt` has `_preprocess` and `tree` attributes, and nodes have `feature`, `theta`, `left`, `right` attributes.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/49_plotly_dashboard.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef compute_node_masks(pdt, X, y, node2id):\n    \"\"\"\n    BFS from root to produce bool mask for each node.\n    \"\"\"\n    angles, _, _, _ = pdt._preprocess(X, y)\n    node_masks = {}\n\n    root = pdt.tree\n    queue = deque([(root, torch.ones(X.shape[0], dtype=bool))])\n    while queue:\n        node, mask = queue.popleft()\n        node_id = node2id[node]\n        node_masks[node_id] = mask\n\n        if node.feature is not None:\n            theta = torch.tensor(node.theta)\n            left_child = node.left\n            right_child = node.right\n            if left_child is not None:\n                left_mask = mask & _angular_greater(angles[:, node.feature], theta).flatten()\n                queue.append((left_child, left_mask))\n            if right_child is not None:\n                right_mask = mask & ~_angular_greater(angles[:, node.feature], theta).flatten()\n                queue.append((right_child, right_mask))\n    return node_masks\n```\n\n----------------------------------------\n\nTITLE: Loading PyTorch Graph Embeddings Data - Python\nDESCRIPTION: This snippet loads graph embeddings from a file serialized with PyTorch's torch.load method. It loads the data from a specific path, mapping it to CPU memory, and then retrieves all the keys in the loaded data structure (commonly a dictionary or similar mapping). Requires PyTorch to be installed and assumes the presence of a valid .h5 file at the specified location; an error will occur if the file does not exist or is not compatible. Inputs are file paths and outputs are the list of keys present in the loaded data object.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/50_graph_embeddings_new.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\ndata = torch.load(\"/teamspace/studios/this_studio/embedders/data/graphs/embeddings/polblogs/H_0.h5\", map_location=\"cpu\")\ndata.keys()\n```\n\n----------------------------------------\n\nTITLE: Calculating Label Correlation using PyTorch\nDESCRIPTION: Calculates the correlation coefficient matrix for the generated labels (`y`). It transposes `y` to compute correlations between the different label sets generated for each manifold signature using `torch.corrcoef`.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/40_component_interp.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Get correlation over dim 1 of y\n\ncorrelation = torch.corrcoef(y.T)\nprint(correlation)\n```\n\n----------------------------------------\n\nTITLE: Defining Parameters and Validating Embedding Data in Python\nDESCRIPTION: This Python snippet imports NumPy and defines key parameters for the benchmarking experiments, including the names of embeddings, their corresponding manifold signatures ('sigs'), the number of trials, set types (train/test), and dataset components (X/y). It then iterates through all specified embeddings, trials, sets, and dataset components, loading the corresponding '.npy' files and checking for the presence of NaN values, printing details of any corrupted files.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/23_benchmark_vae_embeddings.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nembeddings_names = [\n    \"blood_cell_scrna\",\n    \"lymphoma\",\n    \"cifar_100\",\n    \"mnist\",\n]\nsigs = [\n    [(1, 2), (0, 2), (-1, 2), (-1, 2), (-1, 2)],\n    [(1, 2), (1, 2)],\n    [(1, 2), (1, 2), (1, 2), (1, 2)],\n    [(1, 2), (0, 2), (-1, 2)],\n]\nn_trials = 10\nsets = [\"train\", \"test\"]\ndatasets = [\"X\", \"y\"]\n\nbad = []\nfor embedding in embeddings_names:\n    for trial in range(n_trials):\n        for set_name in sets:\n            for dataset in datasets:\n                my_data = np.load(f\"../data/{embedding}/embeddings/{dataset}_{set_name}_{trial}.npy\")\n                if np.isnan(my_data).any():\n                    bad.append((embedding, trial, set_name, dataset))\n                    print(embedding, trial, set_name, dataset)\n                # print(my_data.shape)\nprint(bad)\n```\n\n----------------------------------------\n\nTITLE: Building Tree Edges and Node ID Map in Python\nDESCRIPTION: Processes a decision tree structure (`pdt`) to create a dictionary mapping each `DecisionNode` object to a unique integer index corresponding to its order in `pdt.nodes`. It also generates a list of edges representing parent-child relationships as (parent_id, child_id) tuples. This output is typically used for graph-based representations or visualizations of the tree.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/49_plotly_dashboard.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef build_edges_and_id_map(pdt):\n    \"\"\"\n    Returns:\n      node2id: dict mapping each DecisionNode object -> int index (matching pdt.nodes order)\n      edges: list of (id_parent, id_child)\n    \"\"\"\n    node2id = {node: i for i, node in enumerate(pdt.nodes)}\n    edges = []\n    for i, node in enumerate(pdt.nodes):\n        if node.left is not None:\n            edges.append((i, node2id[node.left]))\n        if node.right is not None:\n            edges.append((i, node2id[node.right]))\n    return node2id, edges\n```\n\n----------------------------------------\n\nTITLE: Obtaining Unique Elements and Inverse Mapping in PyTorch - Python\nDESCRIPTION: Calls torch.tensor.unique with return_inverse=True to obtain the unique classes present and their inverse indices mapping per sample. Input is a 1D tensor of integer elements, output is a tuple (unique_values, inverse_indices). Requires torch.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/13_information_gain.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ntorch.tensor([0, 1, 20]).unique(return_inverse=True)\n```\n\n----------------------------------------\n\nTITLE: Calculating Distance Contributions by Manifold Component\nDESCRIPTION: This snippet calculates the contribution of each component manifold (H2, E2, S2) to the total pairwise distances in the product manifold embedding. It uses the 'dist_component_by_manifold' function from 'embedders.metrics' and prints the resulting contributions and their sum.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/2_polblogs_benchmark.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ncontributions = embedders.metrics.dist_component_by_manifold(pm2, pm2.x_embed)\n\n# Why don't these add to 100%...?\nprint(contributions)\nprint(sum(contributions))\n```\n\n----------------------------------------\n\nTITLE: Plotting 3D Embedding Coordinates using Matplotlib in Python\nDESCRIPTION: Visualizes the learned embedding coordinates stored in `pm.x_embed`. It imports `matplotlib.pyplot` and `Axes3D`, creates a 3D subplot, retrieves the embedding coordinates (detaching from the computation graph and moving to CPU), and plots them as a scatter plot in the 3D space.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/1_basic_test.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Let's quickly plot these in 3-D\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection=\"3d\")\n\n# Plot x_embed, which is on a 2-sphere\nx_embed = pm.x_embed.detach().cpu().numpy()\nax.scatter(x_embed[:, 0], x_embed[:, 1], x_embed[:, 2])\n```\n\n----------------------------------------\n\nTITLE: Analyzing Norm Distribution of Hyperboloid Samples in Python\nDESCRIPTION: This snippet investigates how the L2 norm of the spacelike dimensions of points sampled from a `ProductManifold` (representing a hyperboloid) scales with increasing dimension. It iterates through dimensions [2, 4, ..., 128], samples 1000 points for each using `manifold.sample()`, calculates the norm of dimensions 1 onwards, and stores the results. Finally, it generates a boxplot using `matplotlib` to visualize the distribution of these norms for each dimension, using a logarithmic scale for the y-axis. Dependencies include `embedders.manifolds`, `matplotlib.pyplot`, `numpy`, and `torch`.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/15_covariance_scaling.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport embedders\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Initialize lists to store data\ndimensions = []\nnorms = []\n\n# Sample multiple times for each dimension\nn_samples = 1000\n\nfor dim in [2, 4, 8, 16, 32, 64, 128]:\n    manifold = embedders.manifolds.ProductManifold(signature=[(-1, dim)])\n    \n    for _ in range(n_samples):\n        z = manifold.sample()\n        # Calculate the norm of the spacelike dimensions (all but the first)\n        norm = torch.norm(z[0, 1:]).item()\n        \n        dimensions.append(dim)\n        norms.append(norm)\n\n# Create a boxplot\nplt.figure(figsize=(10, 6))\nplt.boxplot([np.array(norms)[np.array(dimensions) == dim] for dim in [2, 4, 8, 16, 32, 64, 128]],\n            labels=[2, 4, 8, 16, 32, 64, 128])\nplt.title('Distribution of Norms Across Dimensions')\nplt.xlabel('Dimension')\nplt.ylabel('Norm of Spacelike Dimensions')\nplt.yscale('log')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Aggregating Classification Results by Embedding and Signature in Python\nDESCRIPTION: This snippet uses the Pandas `groupby` method on the `results` DataFrame (containing classification benchmark results). It groups the results by 'embedding' and 'signature' and then calculates the mean of the performance metrics for each group across all trials. The resulting aggregated DataFrame, showing average performance per embedding/signature pair, is displayed.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/24_benchmark_graph_embeddings.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nresults.groupby([\"embedding\", \"signature\"]).mean()\n```\n\n----------------------------------------\n\nTITLE: Inspecting Shapes of Loaded Cora Dataset Components\nDESCRIPTION: Iterates through the data components (`a`, `b`, `c`, `d`) loaded from the 'cora' dataset in the previous step, along with descriptive names. For each component, it attempts to print its name and shape using the `.shape` attribute. A `try-except` block handles potential errors if a component does not have a shape attribute (e.g., it might be `None` or a different data structure).\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/60_pytest_scratch.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfor name, x in zip([\"features\", \"dists\", \"labels\", \"adjacency\"], [a, b, c, d]):\n    try:\n        print(name, x.shape)\n    except Exception:\n        print(name, \"None\")\n```\n\n----------------------------------------\n\nTITLE: Inner Product Scaling with Curvature for ProductManifold Feature (Python)\nDESCRIPTION: Extracts the last factorized feature from pm.factorize(X), computes its inner product, multiplies by the corresponding curvature and scale of the manifold. Requires torch, embedders, and ProductManifold. Inputs are the feature vector and manifold parameters; outputs a scalar or matrix value. Useful for diagnostics and theoretical exploration of inner product geometry.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n_x = pm.factorize(X)[-1]\npm.P[-1].inner(_x, _x) * pm.P[-1].curvature * pm.P[-1].scale\n```\n\n----------------------------------------\n\nTITLE: Calculating Ground Truth Information Gains Iteratively in Python\nDESCRIPTION: This snippet computes information gains for potential splits in the data using an iterative approach, serving as the ground truth. It initializes a `HyperbolicDecisionTreeClassifier`, iterates through data points (`i`) and dimensions (`j`), calculates the split angle `theta`, uses the tree's `_get_split` method to partition data, computes the information gain using `_information_gain`, stores the result, and finally visualizes the gains using `matplotlib.pyplot.matshow`. The first column of `info_gains` corresponding to the time dimension is ignored.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/13_information_gain.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Compute information gains for every possible split\n\nhdt = HyperbolicDecisionTreeClassifier()\nhdt.classes_ = [0, 1]\n\ninfo_gains = torch.zeros_like(X, dtype=torch.float64)\nfor i in range(X.shape[0]):\n    for j in range(1, X.shape[1]):\n        theta = np.arctan2(X[i, 0].item(), X[i, j].item())\n        left, right = hdt._get_split(X=X.detach().numpy(), dim=j, theta=theta)\n        score = hdt._information_gain(left, right, y.detach().numpy())\n        info_gains[i, j] = score\n\nplt.matshow(info_gains[: 32 * 16, 1:].reshape(-1, 32))  # Don't forget to drop first column\n```\n\n----------------------------------------\n\nTITLE: Composing Split Masks and Indexing for Further Splitting in Decision Trees - Python\nDESCRIPTION: Demonstrates the composition of comparison masks for both negative and positive splits, leveraging boolean tensor indexing to create sub-comparison tensors for further decision tree recursion. This snippet is crucial for tracking data partitioning at each split and maintaining correct shapes through recursive divisions. Assumes prior definitions of n, d, comparisons_reshaped, and uses torch for boolean masking.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/13_information_gain.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Composing splits is the final problem.\n# The issue to be aware of is that we need to keep track of the splits we've already made.\n# Luckily, our comparisons tensor allows us to index into the next comparisons tensor!\n\nmask = comparisons_reshaped[n, d].bool()\ncomparisons_neg = comparisons_reshaped[mask][:, :, mask]\ncomparisons_pos = comparisons_reshaped[~mask][:, :, ~mask]\n\nprint(f\"Comparisons neg: {comparisons_neg.shape}\")\nprint(f\"Comparisons pos: {comparisons_pos.shape}\")\n\n```\n\n----------------------------------------\n\nTITLE: Saving Regression Benchmark Results to TSV using Pandas\nDESCRIPTION: This code saves the results from the regression benchmark runs (stored in the `results` DataFrame) to a TSV file using pandas. The filename includes the `TASK` ('regression'), a placeholder for `MAX_DEPTH` (which was commented out in the experiment code but included here), and 'ICML', suggesting results intended for a specific publication or context. `index=False` prevents writing the index, and `sep='\\t'` ensures tab separation.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/27_benchmark_signature_gaussian.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nresults.to_csv(f\"../data/results/{TASK}_signature_md{MAX_DEPTH}_ICML.tsv\", index=False, sep=\"\\t\")\n```\n\n----------------------------------------\n\nTITLE: Comparing Decision Trees on Various Manifolds in Python\nDESCRIPTION: Executes the `test_sig` function multiple times with different manifold signatures to compare the performance of the three decision tree implementations (hyperdt, old embedders, new embedders) across various geometric settings. Signatures include Hyperbolic (Hyp), Euclidean (Euc), Spherical (Sph), and products thereof (Multi-Hyp, Multi-Sph, H x E, S x E, H x S, H x S x E). The results (scores) for each test are printed.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/14_new_tree_testing.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Try a few more signatures\nprint(\"Hyp\")\ndt_q, dt_old, dt_new, X, y = test_sig([(-1, 4)], seed=1)\nprint()\n\nprint(\"Euc\")\ndt_q, dt_old, dt_new, X, y = test_sig([(0, 4)], seed=2, fix_x_dim=0)\nprint()\n\nprint(\"Sph\")\ndt_q, dt_old, dt_new, X, y = test_sig([(1, 4)], seed=3)\nprint()\n\nprint(\"Multi-Hyp\")\ndt_q, dt_old, dt_new, X, y = test_sig([(-1, 2), (-1, 2)], seed=4)\nprint()\n\nprint(\"Multi-Sph\")\ndt_q, dt_old, dt_new, X, y = test_sig([(1, 2), (1, 2)], seed=5)\nprint()\n\nprint(\"H x E\")\ndt_q, dt_old, dt_new, X, y = test_sig([(-1, 2), (0, 2)], seed=6, fix_x_dim=3)\nprint()\n\nprint(\"S x E\")\ndt_q, dt_old, dt_new, X, y = test_sig([(1, 2), (0, 2)], seed=6, fix_x_dim=2)\nprint()\n\nprint(\"H x S\")\ndt_q, dt_old, dt_new, X, y = test_sig([(-1, 2), (1, 2)], seed=6)\nprint()\n\nprint(\"H x S x E\")\ndt_q, dt_old, dt_new, X, y = test_sig([(-1, 2), (0, 2), (1, 2)], seed=7, fix_x_dim=6)\nprint()\n```\n\n----------------------------------------\n\nTITLE: Plot Histogram of Log-Transformed Target\nDESCRIPTION: This snippet uses `matplotlib.pyplot.hist` to generate and display a histogram of the log-transformed target variable `y` (vehicle counts). This helps visualize the distribution of the target variable after transformation, which is often done to handle skewness.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/33_traffic.ipynb#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nplt.hist(np.log(y))\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Enabling IPython Autoreload Extension - Python\nDESCRIPTION: This snippet activates IPython's autoreload extension, ensuring that modules are automatically reloaded before code execution for rapid development and iteration. It requires an IPython environment with the 'autoreload' extension available. This setup facilitates development by always running the newest code changes without restarting the kernel.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/20_roc_auc.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Calculating Relative Error Between Distances in Python\nDESCRIPTION: This snippet computes the element-wise relative error between the estimated distances (`D_est`) and the true training distances (`D_train`) from the last iteration of the distortion calculation loop. The result, a tensor of relative errors, is printed, likely for debugging and detailed error analysis.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/24_benchmark_graph_embeddings.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntorch.abs(D_est - D_train) / D_train\n```\n\n----------------------------------------\n\nTITLE: Inspect UCI Training Data Dimensions\nDESCRIPTION: This snippet examines the structure of the loaded UCI training data (`tra_X_tr`). It prints the number of elements in the first dimension and the shape of the first element (which is expected to be a sparse matrix).\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/33_traffic.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# The whole thing is just in here:\n\nprint(len(data[\"tra_X_tr\"][0]))\nprint(data[\"tra_X_tr\"][0][0].shape)\n```\n\n----------------------------------------\n\nTITLE: Calculating the Best Split Point in Hyperbolic Trees Using Information Gain - Python\nDESCRIPTION: Defines a suite of midpoint functions for hyperbolic, spherical, and Euclidean geometry, as well as utilities for picking the best split by maximizing information gain. Inputs: information gain matrix, angles, comparison tensor, and ProductManifold pm. Returns split indices and value for use in tree construction, using manifold-dependent midpoint calculation. Contains support and validation helpers, requires torch.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/13_information_gain.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# How can we retrieve the best split from an information gain matrix?\n# This is actually a little tough, because it involves computing midpoints and stuff\n\n\ndef hyperbolic_midpoint(u, v, assert_hyperbolic=False):\n    w = torch.sin(2 * u - 2 * v) / (torch.sin(u + v) * torch.sin(v - u))\n    coef = -1 if u + v < torch.pi else 1\n    sol = (-w + coef * torch.sqrt(w**2 - 4)) / 2\n    m = torch.arctan2(torch.tensor(1), sol) % torch.pi\n    if assert_hyperbolic:\n        assert is_hyperbolic_midpoint(u, v, m)\n    return m\n\n\ndef is_hyperbolic_midpoint(u, v, m):\n    a = lambda x: torch.sqrt(-1 / torch.cos(2 * x))  # Alpha coefficient to reach manifold\n    d = lambda x, y: a(x) * a(y) * torch.cos(x - y)  # Hyperbolic distance function (angular)\n    return torch.isclose(d(u, m), d(m, v))\n\n\ndef spherical_midpoint(u, v):\n    return (u + v) / 2\n\n\ndef euclidean_midpoint(u, v):\n    return torch.arctan2(1, (torch.tan(u) + torch.tan(v)) / 2)\n\n\ndef midpoint(u, v, manifold):\n    if torch.isclose(u, v):\n        return u\n    elif manifold.type == \"H\":\n        return hyperbolic_midpoint(u, v)\n    elif manifold.type == \"S\":\n        return spherical_midpoint(u, v)\n    else:\n        return euclidean_midpoint(u, v)\n\n\ndef get_best_split(ig, angles, comparisons, pm=None):\n    \"\"\"\n    All of the postprocessing for an information gain check\n\n    Args:\n        ig: (query_batch, dims) tensor of information gains\n        angles: (query_batch, dims) tensor of angles\n        comparisons: (query_batch, dims, key_batch) tensor of comparisons\n        pm: ProductManifold object, for determining midpoint approach\n\n    Returns:\n        n: scalar index of best split (positive class)\n        d: scalar dimension of best split\n        theta: scalar angle of best split\n    \"\"\"\n    # First, figure out the dimension (d) and sample (n)\n    best_split = ig.argmax()\n    nd = ig.shape[1]\n    n, d = best_split // nd, best_split % nd\n\n    # Get the corresponding angle\n    theta_pos = angles[n, d]\n\n    # We have the angle, but ideally we would like the *midpoint* angle.\n    # So we need to grab the closest angle from the negative class:\n    n_neg = (angles[comparisons[n, d] == 0.0, d] - theta_pos).abs().argmin()\n    theta_neg = angles[comparisons[n, d] == 0.0, d][n_neg]\n\n    # Get manifold\n    manifold = pm.P[pm.intrinsic2man[d.item()]]\n\n    # Print what you're doing\n    m = midpoint(theta_pos, theta_neg, manifold)\n\n    return n, d, m\n\n\nn, d, theta = get_best_split(ig_est_functional, angles, comparisons_reshaped, pm)\nprint(f\"n: {n}, d: {d}, theta: {theta/torch.pi:.3f}*pi, info_gain: {ig_est_functional.max():.3f}\")\n\n```\n\n----------------------------------------\n\nTITLE: Preparing Data and Comparison Tensor for Vectorized Calculation in Python with PyTorch\nDESCRIPTION: This snippet prepares data for a vectorized information gain calculation using PyTorch. It calculates angles from the input features `X` using `torch.arctan2`. Based on the `COMPARISON` constant, it creates a boolean tensor `comparisons` indicating the split outcome for every pair of points and every dimension. The tensor is then reshaped (`permute`) for efficient processing. An optional, slow verification step compares the generated `comparisons_reshaped` tensor with an explicitly calculated one based on the chosen comparison logic.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/13_information_gain.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Great, now we have ground truth. Can we do it with tensors?\n\n# First, we process our X-values into coordinates:\n# (batch, dims + 1) --> (batch, dims)\nangles = torch.arctan2(X[:, 0:1], X[:, 1:])\nprint(f\"Angles: {angles.shape}\")\n\n# Now, we create a tensor of comparisons\n# (batch, dims, batch)\nif COMPARISON == \"circular\":\n    comparisons = ((angles[:, None] - angles[None, :] + torch.pi) % (2 * torch.pi)) >= torch.pi\nelif COMPARISON == \"geq\":\n    comparisons = angles[:, None] >= angles[None, :]\nelif COMPARISON == \"eq\":\n    comparisons = angles[:, None] > angles[None, :]\nelse:\n    raise ValueError(f\"Unknown comparison type: {COMPARISON}\")\nprint(f\"Comparisons: {comparisons.shape}\")\n\n# Reshape the comparisons tensor to (split_batch, dims, response_batch):\ncomparisons_reshaped = comparisons.permute(0, 2, 1)\nprint(f\"Comparisons reshaped: {comparisons_reshaped.shape}\")\n\n# Verify that this is equivalent to what we wanted:\n# This verification is slooooow, so I'm gonna start a new cell after this\n\nif not SKIP_VERIFICATION:\n    verification_tensor = torch.zeros_like(comparisons_reshaped)\n    for split_batch in range(angles.shape[0]):\n        for dim in range(angles.shape[1]):\n            for response_batch in range(angles.shape[0]):\n                # if COMPARISON == \"circular\":\n                #     val = ((angles[split_batch, dim] - angles[response_batch, dim]) % (2 * torch.pi)) - torch.pi > 0\n                # Use the GEQ approach to verify that, on the hyperboloid, these are the same\n                if COMPARISON in [\"geq\", \"circular\"]:\n                    val = angles[split_batch, dim] >= angles[response_batch, dim]\n                elif COMPARISON == \"eq\":\n                    val = angles[split_batch, dim] > angles[response_batch, dim]\n                verification_tensor[split_batch, dim, response_batch] = val\n    assert torch.allclose(verification_tensor, comparisons_reshaped)\n    print(\"Verification passed.\")\n```\n\n----------------------------------------\n\nTITLE: Loading Jupyter Autoreload Extension in Python\nDESCRIPTION: These Jupyter magic commands load and enable the `autoreload` extension. `%autoreload 2` specifically configures it to automatically reload all modules (except those excluded by `%aimport`) before executing user code. This is useful for development when frequently changing imported library code.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/17_verify_new_mse.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Loading Graph Data ('polblogs') in Python\nDESCRIPTION: This snippet demonstrates loading the 'polblogs' dataset using the `dataloaders` module from the 'embedders' library. It retrieves pairwise distances, node labels, and potentially subgraph information for the political blogs graph.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/Embedder Tutorial.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom embedders import dataloaders\n#Load Data\npolblogs_dists, polblogs_labels,sub = dataloaders.load(\"polblogs\")\n```\n\n----------------------------------------\n\nTITLE: Defining Runtime Benchmark Hyperparameters - Python\nDESCRIPTION: This block imports necessary modules (embedders, pandas), sets computing device, and defines experiment hyperparameters such as maximum tree depth, number of classes/clusters/features, and covariance scaling factors for runtime experiments. Meant to be run before performance evaluations with simulated datasets. Dependencies: embedders, pandas. Prerequisite for downstream benchmarking code.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/43_benchmark_neural.ipynb#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport embedders\nimport pandas as pd\n\ndevice = \"cuda\"\nMAX_DEPTH = 3\nN_CLASSES = 8\nN_CLUSTERS = 32\nCOV_SCALE_MEANS = 1.0\nCOV_SCALE_POINTS = 1.0\nN_FEATURES = \"d_choose_2\"\n```\n\n----------------------------------------\n\nTITLE: Defining Signatures for Exhaustive Benchmark\nDESCRIPTION: This snippet defines a list of manifold signatures (combinations of curvature and dimension for product manifolds) and corresponding string identifiers. These configurations (e.g., HH, HE, H, E, S) will be used in the subsequent exhaustive benchmark.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/2_polblogs_benchmark.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Non-Gu signature selection\nSIGNATURES = [\n    [(-1, 2), (-1, 2)],  # HH\n    [(-1, 2), (0, 2)],  # HE\n    [(-1, 2), (1, 2)],  # HS\n    [(1, 2), (1, 2)],  # SS\n    [(1, 2), (0, 2)],  # SE\n    [(-1, 4)],  # H\n    [(0, 4)],  # E\n    [(1, 4)],  # S\n]\n\nSIGNATURES_STR = [\"HH\", \"HE\", \"HS\", \"SS\", \"SE\", \"H\", \"E\", \"S\"]\n```\n\n----------------------------------------\n\nTITLE: Calculating Mean Accuracies Across Trials in Python\nDESCRIPTION: Calculates and displays the mean accuracy for each model type (MLP, KappaGCN-MLP, MLR, KappaGCN-MLR) across the 30 trials stored in the `results` Pandas DataFrame.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/relationship.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nresults.mean()\n```\n\n----------------------------------------\n\nTITLE: Reshaping Covariance Tensors for Sampling (Python)\nDESCRIPTION: This snippet shows how to reshape a batched covariance matrix for compatibility with manifold sampling APIs. Converts covs tensor from (10,*,*) to 2D shape. Uses torch.reshape. Input is covs tensor; output is reshaped tensor.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/60_pytest_scratch.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ncovs.reshape(10, -1)\n\n```\n\n----------------------------------------\n\nTITLE: Loading Cities Distance Data using embedders in Python\nDESCRIPTION: Loads the pre-defined cities distance dataset using the `embedders.dataloaders.load_cities` function. The resulting distance matrix is stored in the `original_dists` variable, and its size (number of elements/cities) is printed.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/1_basic_test.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Load cities dataset\n\noriginal_dists = embedders.dataloaders.load_cities()\nprint(f\"Dists have {len(original_dists)} elements\")\n```\n\n----------------------------------------\n\nTITLE: Inspecting Root Node Attributes of Decision Tree (Repeat) in Python\nDESCRIPTION: Accesses and displays the internal attribute dictionary (`__dict__`) of the root node (index 0) of the trained `ProductSpaceDT` object `pdt` again. This shows the state of the root node, including the feature and threshold used for the first split.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_pc_basic_visualization.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\npdt.nodes[0].__dict__\n```\n\n----------------------------------------\n\nTITLE: Inspecting Root Node Attributes of Decision Tree in Python\nDESCRIPTION: Accesses and displays the internal attribute dictionary (`__dict__`) of the root node (index 0) of the trained `ProductSpaceDT` object `pdt`. This reveals details about the first split, such as the feature used and the threshold.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_pc_basic_visualization.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npdt.nodes[0].__dict__\n```\n\n----------------------------------------\n\nTITLE: Defining Configuration Constants in Python\nDESCRIPTION: This snippet defines key configuration parameters for the notebook execution. `SAMPLE_SIZE` sets the number of data points to generate, `SKIP_VERIFICATION` is a boolean flag to control whether a potentially slow verification step is executed, and `COMPARISON` specifies the type of comparison logic ('circular', 'geq', or 'eq') used in later calculations.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/13_information_gain.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nSAMPLE_SIZE = 10_000\nSKIP_VERIFICATION = True\nCOMPARISON = \"circular\"  # \"geq\" or \"eq\" also accepted\n```\n\n----------------------------------------\n\nTITLE: Training H^2 x H^2 x H^2 Embedding for CS PhDs Dataset in Python\nDESCRIPTION: This snippet defines a product manifold composed of three H^2 components ('signature = [(-1, 2), (-1, 2), (-1, 2)]'). It sets the same manual seed as before and trains coordinates using the rescaled distances ('dists_rescaled') and the specified training parameters on the configured 'device'. The resulting H^2 x H^2 x H^2 embedding is stored in 'h2_3_cs_phds'.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/4_cs_phds_regression.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Specify signature - useful to re-initialize the manifold here\n\ntorch.manual_seed(0)  # Not all seeds are stable - this one trains for 3000 iterations at lr=1e-2 (burn-in 1e-3)\n\nsignature = [(-1, 2), (-1, 2), (-1, 2)]\npm2 = embedders.manifolds.ProductManifold(signature=signature)\nprint(pm2.name)\n\n# Get embedding\nembedders.coordinate_learning.train_coords(\n    pm2,\n    dists_rescaled,\n    device=device,\n    burn_in_iterations=100,\n    training_iterations=100 * 9,\n    learning_rate=1e-1,\n    burn_in_learning_rate=1e-2,\n    scale_factor_learning_rate=1e-1,\n)\n\nh2_3_cs_phds = pm2.x_embed.detach().cpu().numpy()\n```\n```\n\n----------------------------------------\n\nTITLE: Loading Data with manify.utils.dataloaders - Python\nDESCRIPTION: This code loads the 'cs_phds' dataset using the load function from manify.utils.dataloaders. It unpacks the result into three components: D (likely a distance matrix), y (labels or targets), and A (possibly an adjacency matrix or auxiliary data). Dependencies include the 'manify' library and its data loaders. The key input is the dataset name (string), and the outputs are three data structures used for embedding and analysis.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/55_reimplement_gu.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nD, y, A = manify.utils.dataloaders.load(\"cs_phds\")\n```\n\n----------------------------------------\n\nTITLE: Direct Pairwise Norm Calculation with torch.linalg.norm - Python\nDESCRIPTION: This snippet computes the pairwise Euclidean distances between rows of a random tensor _x using torch.linalg.norm, serving as a reference or alternative to geoopt's distance methods. It requires torch to be installed and takes a tensor _x as input, returning a full matrix of pairwise norms. There are no external parameter dependencies.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/55_reimplement_gu.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntorch.linalg.norm(_x[:, None, :] - _x[None, :, :], dim=-1)\n```\n\n----------------------------------------\n\nTITLE: Importing Core Libraries for Embedding and Graph Analysis (Python)\nDESCRIPTION: Imports essential Python libraries: `embedders` for manifold embedding functionalities, `networkx` for graph operations (though not directly used later in the visible code), and `torch` for tensor computations, which are fundamental for the embedding process.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/22_link_prediction.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport embedders\nimport networkx as nx\nimport torch\n```\n\n----------------------------------------\n\nTITLE: Loading the Football Dataset Adjacency Matrix (Python)\nDESCRIPTION: Utilizes the `embedders.dataloaders.load` function to load the 'football' graph dataset from the `embedders` library. It specifically extracts the adjacency matrix (`adj`) and ignores the distance matrix and node labels by assigning them to underscores.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/22_link_prediction.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n_, _, adj = embedders.dataloaders.load(\"football\")\n```\n\n----------------------------------------\n\nTITLE: Convert Sparse Matrix to Dense Array\nDESCRIPTION: This snippet accesses the first sparse matrix within the UCI training data (`data[\"tra_X_tr\"][0][0]`) and converts it into a dense NumPy array using the `.todense()` method for easier inspection.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/33_traffic.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndata[\"tra_X_tr\"][0][0].todense()\n```\n\n----------------------------------------\n\nTITLE: Stacking Covariance Matrices for Batch Sampling (Python)\nDESCRIPTION: This code demonstrates how to create a stack of identical covariance matrices for Gaussian mixture vector sampling on the manifold. It uses torch.stack to produce a (10,2,2) tensor from an existing (2,2) covariance matrix. Requires torch and a defined cov variable. Output is my_stack containing stacked covariance matrices.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/60_pytest_scratch.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# make a stack of (10, 2, 2) from this\nmy_stack = torch.stack([cov] * 10, dim=0)  # create a stack of 10 copies of cov\n\n```\n\n----------------------------------------\n\nTITLE: Extract Weekday and Hour Indices from UCI Data\nDESCRIPTION: This snippet extracts one-hot encoded weekday (columns 10-17) and hour (columns 17-41) features from each sparse matrix in the UCI training set (`data[\"tra_X_tr\"][0]`). It validates that the features sum to 1 for each row (indicating valid one-hot encoding), collects valid features, stacks them, and finds the index of the '1' in each row (using `argmax`) to get numerical weekday and hour indices. Indices of invalid matrices are stored in `bad`.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/33_traffic.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# First, let's extract the weekday and hour features from the first example to infer their structure.\nsample_matrix = data[\"tra_X_tr\"][0][0]  # Taking the first sparse matrix\nweekday_features = sample_matrix[:, 10:17].toarray()  # Assuming weekdays are stored after the historical traffic volume\nhour_features = sample_matrix[:, 17:41].toarray()  # Assuming hours are stored right after weekdays\n\n# Now, we can go through all matrices and extract these two sets of features\nweekdays = []\nhours = []\n\nbad = []\nfor i in range(data[\"tra_X_tr\"].shape[1]):\n    current_matrix = data[\"tra_X_tr\"][0][i]\n    weekday = current_matrix[:, 10:17].toarray()\n    hour = current_matrix[:, 17:41].toarray()\n    if (weekday.sum(axis=1) == 1).all() and (hour.sum(axis=1) == 1).all(): \n        weekdays.append(weekday)  \n        hours.append(hour)\n    else:\n        bad.append(i)\n\n# Convert the lists to numpy arrays\nweekdays_array = np.vstack(weekdays)\nhours_array = np.vstack(hours)\n\n# We can now construct the (1261 x 2) matrix\nday_hour_indices = np.column_stack((weekdays_array.argmax(axis=1), hours_array.argmax(axis=1)))\nday_hour_indices\n```\n\n----------------------------------------\n\nTITLE: Calculating Accuracy of the Trained Tree\nDESCRIPTION: Evaluates the accuracy of the trained `TorchProductSpaceDT` model (`tpsdt`) on the training data (`data`, `classes`). It appears to compare the model's predictions with the true labels and calculates the proportion of correct predictions. Note: The code calls `tpsdt.score(data, classes)`, but the `score` method is not explicitly defined in the provided class definition; this calculation likely relies on inherited methods or represents a manual accuracy check.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/10_torchified_hyperdt.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntpsdt.score(data, classes).sum() / data.shape[0]\n```\n\n----------------------------------------\n\nTITLE: Importing Node Property Prediction Module from OGB - Python\nDESCRIPTION: This imports the 'nodeproppred' module from the ogb package, enabling access to datasets and utility functions for node property prediction tasks. The ogb package should be installed prior to running this import. This import is a prerequisite for loading and handling OGB node property datasets, and exposes necessary dataset constructors under the 'ogb.nodeproppred' namespace.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/19_opengraphbenchmark.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport ogb.nodeproppred\n```\n\n----------------------------------------\n\nTITLE: Stacking Means for Gaussian Mixture (Python)\nDESCRIPTION: This code creates a batch of mean vectors for sampling by vertically stacking multiple copies of the manifold origin (mu0) using torch.vstack. Inputs are the manifold's mu0; output is a stacked means tensor for use as z_mean. Requires torch and M.mu0.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/60_pytest_scratch.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntorch.vstack([M.mu0] * 10)\n\n```\n\n----------------------------------------\n\nTITLE: Re-initializing Product Manifold with Device\nDESCRIPTION: Re-initializes the `ProductManifold` object (`pm`) with the same signature as before, but this time explicitly associating it with the PyTorch device determined in the previous step. This ensures manifold operations are performed on the correct device (CPU or GPU).\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/10_torchified_hyperdt.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Initialize the appropriate product manifold, which we'll use for indexing\n\npm = embedders.manifolds.ProductManifold(signature=[(1, 2), (0, 2), (-1, 2), (-1, 2), (-1, 2)], device=device)\n```\n\n----------------------------------------\n\nTITLE: Computing Geodesic Arc Center and Radius in the Poincare Disk - Python\nDESCRIPTION: Defines a function to compute the center and radius of the geodesic arc connecting two points in the Poincare disk, using Mbius geometry and complex arithmetic. It converts input points to complex numbers, applies circle/arc formulas, and returns a formatted LaTeX command string for drawing the arc, along with a print of the center. Inputs are 2D torch tensors; output is a LaTeX string describing the arc. Depends on torch and requires knowledge of their geometric representations. Limitations: Designed for 2D points in the unit disk.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/36_poincare_circles.ipynb#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndef find_geodesic(p1, p2):\\n    \"\"\"Given two points, return the center and radius of the geodesic arc\"\"\"\\n\\n    # Find inverse of p1\\n    p1_norm = torch.linalg.norm(p1) ** 2\\n    p3 = p1 / p1_norm\\n\\n    # Convert everything to complex numbers\\n    p1_c = p1[0] + 1j * p1[1]\\n    p2_c = p2[0] + 1j * p2[1]\\n    p3_c = p3[0] + 1j * p3[1]\\n\\n    # Find the circle\\n    w = (p3_c - p1_c) / (p2_c - p1_c)\\n    c = (p2_c - p1_c) * (w - abs(w) ** 2) / (2j * w.imag) + p1_c\\n    r = abs(p1_c - c)\\n\\n    # Convert back to real numbers\\n    c = torch.tensor([c.real, c.imag])\\n    r = torch.tensor(r)\\n\\n    # Get degrees of the arc by convertin p1 and p2 to angles\\n    p1_angle = torch.atan2(p1[1] - c[1], p1[0] - c[0]) * 180 / torch.pi % 360\\n    p2_angle = torch.atan2(p2[1] - c[1], p2[0] - c[0]) * 180 / torch.pi % 360\\n    # p1_angle = torch.atan2(p1[1] - c[1], p1[0] - c[0]) * 180 / torch.pi\\n    # p2_angle = torch.atan2(p2[1] - c[1], p2[0] - c[0]) * 180 / torch.pi\\n\\n    # return c, r, p1_angle, p2_angle\\n    print(c)\\n    return f\"({p1[0].item():.4f}, {p1[1].item():.4f}) arc ({p1_angle.item():.4f}:{p2_angle.item():.4f}:{r.item():.4f})\"\\n\\np1 = torch.tensor([0.8038, 0])\\np2 = torch.tensor([0.3333, -0.6547])\\nprint(f\"\\\\draw[thick, color=green] {find_geodesic(p1, p2)};\")\\n\n```\n\n----------------------------------------\n\nTITLE: Process and Aggregate Specific UCI Features\nDESCRIPTION: This snippet processes the UCI training data (`data[\"tra_X_tr\"][0]`). For each sparse matrix, it selects columns 10 to 17, converts them to a dense array, flattens the result, stacks these flattened arrays, and then calculates the sum along the second axis (axis=1).\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/33_traffic.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nnp.stack(\n    [x[:, 10:17].todense().flatten() for x in data[\"tra_X_tr\"][0]],\n).sum(axis=1)\n```\n\n----------------------------------------\n\nTITLE: Accessing Specific Angle Dimensions in Decision Tree in Python\nDESCRIPTION: Retrieves the pair of dimension indices associated with feature index 16 from the `angle_dims` attribute of the trained `ProductSpaceDT` object `pdt`. These dimensions are used for the angular comparison in the corresponding node's split.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_pc_basic_visualization.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npdt.angle_dims[16]\n```\n\n----------------------------------------\n\nTITLE: Setting PyTorch Device and Moving Data\nDESCRIPTION: Detects if a CUDA-enabled GPU is available and sets the PyTorch device accordingly (`cuda` or `cpu`). It then moves the generated feature tensor `X` and label tensor `y` to the selected device for computation.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/10_torchified_hyperdt.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Device management\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nX = X.to(device)\ny = y.to(device)\n```\n\n----------------------------------------\n\nTITLE: Importing the embedders Library in Python\nDESCRIPTION: Imports the main 'embedders' library, making its modules and functions available for use in the subsequent code for manifold learning and data loading.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/1_basic_test.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport embedders\n```\n\n----------------------------------------\n\nTITLE: Plotting Multiple Time Series Sweeps in Subplots with Python\nDESCRIPTION: Creates a figure with 10 subplots arranged vertically using matplotlib. It then iterates 10 times, plotting time series sweeps from the 'all_ts' list, starting from an index specified by 'OFFSET'. Each subplot is given a title indicating the sweep number.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/30_fourier.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nOFFSET = 20\nfig, axs = plt.subplots(10, 1, figsize=(10, 20))\nfor i in range(10):\n    axs[i].plot(all_ts[i + OFFSET])\n    axs[i].set_title(f\"Sweep {i + OFFSET}\")\n```\n\n----------------------------------------\n\nTITLE: Sorting Grouped Benchmark Results by Average Distance using Pandas in Python\nDESCRIPTION: This simple Python snippet uses the Pandas library to sort the previously computed `results_grouped` DataFrame. The sorting is performed based on the values in the 'd_avg' column in ascending order, allowing for inspection of how average distance correlates with the aggregated performance metrics across different manifold signatures.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/2_polblogs_benchmark.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nresults_grouped.sort_values(\"d_avg\")\n```\n\n----------------------------------------\n\nTITLE: Importing Core Libraries for Manifold Learning and Visualization in Python\nDESCRIPTION: Imports necessary libraries: `embedders` for manifold operations and the custom decision tree, `numpy` for numerical operations, `torch` for tensor computations (likely used by `embedders`), and `matplotlib.pyplot` for plotting results.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/17_verify_new_mse.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport embedders\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\n```\n\n----------------------------------------\n\nTITLE: Computing Summed Error Gradients for Updates (Python)\nDESCRIPTION: Performs a complex two-stage sum of class-weighted kernels and features at error locations. Used to accumulate the gradient or update direction for optimization. Inputs are K, X, y, and err; output is a feature-wise sum tensor.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\n(((y * 2 - 1)[err, None] * K[err, :]).sum(dim=1)[:, None] * X[err, :]).sum(dim=0)\n```\n\n----------------------------------------\n\nTITLE: Generating and Plotting Circles in the Poincare Disk - Python\nDESCRIPTION: Defines functions to construct numeric and analytic circles centered at random origins in the Poincare disk with random radii using torch and geoopt. The code plots both types of circles, their boundaries, and the origin, with optional radius checking. Main parameters include 'origin', 'radius', and, for function pd_circle, 'num_points'. Inputs are PyTorch tensors for coordinates/radius; outputs include circle coordinates and visual plots. Relies on geoopt.PoincareBall, torch, and matplotlib.pyplot. Limitations: Assumes origin stays within disk; relies on Geoopt for geometry.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/36_poincare_circles.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# Plot a circle on the Poincare disk\\n\\nCHECK_RADIUS = True\\n\\n# Randomly sampled origin\\norigin = torch.rand(2) * 2 - 1\\nwhile origin.norm() > 1:\\n    origin = torch.rand(2) * 2 - 1\\n# origin = torch.zeros(2)\\n# origin = torch.tensor([torch.rand(1).item() * 2 - 1, 0])\\n\\n# We want a radius\\nradius = torch.rand(1) * 4\\n\\n\\n# Start with a circle\\ndef pd_circle(origin, radius, num_points=100):\\n    _x = torch.linspace(-1, 1, num_points).unsqueeze(1)\\n    _y = torch.sqrt(1 - _x**2)\\n    _xy1 = torch.hstack((_x, _y))\\n    _xy2 = torch.hstack((_x, -_y)).flip(0)\\n    circ_coords = torch.vstack((_xy1, _xy2)) * radius\\n    # print(circ_coords.shape)\\n\\n    # Make these elements of the tangent plane and move to the origin\\n    pd = geoopt.PoincareBall()\\n    circ_trans = pd.transp(torch.zeros(2), origin, circ_coords)\\n    circ_map = pd.expmap(origin, circ_trans)\\n    if CHECK_RADIUS:\\n        print(\"Radius:\", pd.dist(origin, circ_map).min(), pd.dist(origin, circ_map).max(), 2 * radius)\\n    return circ_map.numpy(), _x.numpy(), _y.numpy()\\n\\n\\n# circ_map = circ_map.numpy()\\ncirc_map, _x, _y = pd_circle(origin, radius)\\n\\n\\n# Analytic solution\\ndef analytic_circle(r, c):\\n    k = torch.tanh(r / 2)\\n    # k = torch.tanh(radius)\\n    c_norm = torch.linalg.norm(c)\\n    c_prime = (1 - k**2) * c / (1 - k**2 * c_norm**2)\\n    r_prime = k * (1 - c_norm**2) / (1 - k**2 * c_norm**2)\\n    theta = torch.linspace(0, 2 * torch.pi, 100)\\n    x_circle = r_prime * torch.cos(theta)\\n    y_circle = r_prime * torch.sin(theta)\\n    circ_analytic = torch.stack([x_circle, y_circle], dim=1) + c_prime\\n    if CHECK_RADIUS:\\n        pd = geoopt.PoincareBall()\\n        print(\"Analytic radius:\", pd.dist(c, circ_analytic).min(), pd.dist(c, circ_analytic).max(), radius)\\n    return circ_analytic.numpy(), r_prime, c_prime\\n\\ncirc_analytic, _, _ = analytic_circle(radius, origin)\\n\\n# Plot the circle itself\\nplt.scatter(*origin, color=\"red\")\\n# plt.plot(circ_map[:, 0], circ_map[:, 1], color=\"blue\")\\nplt.plot(circ_analytic[:, 0], circ_analytic[:, 1], color=\"green\")\\n\\n# Plot the boundary\\ntheta = torch.linspace(0, 2 * torch.pi, 100)\\nx_circle = torch.cos(theta)\\ny_circle = torch.sin(theta)\\nplt.plot(x_circle, y_circle, color=\"black\")\\n\\n# Equal aspect ratio\\nplt.axis(\"equal\")\\n\n```\n\n----------------------------------------\n\nTITLE: Initializing Autoreload Extension in IPython Environment - Python\nDESCRIPTION: This snippet enables the autoreload extension for Jupyter/IPython, automatically reloading modules before executing code. Requires an IPython or Jupyter environment. Useful for rapid development and iterative experimentation, ensuring that changes in imported code are reflected without restarting the runtime. No input or output is directly produced by this snippet.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/54_kappa_gcn_benchmark.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Creating and Accessing ProductManifold Components in Manify with Python\nDESCRIPTION: This snippet demonstrates the creation of a ProductManifold object from the 'manify' package with a specific signature, and accesses one of its component manifolds. The key input is the 'signature', defining each manifold's type and dimension. The output is the ProductManifold object and one of its components. This snippet assumes the 'manify' package is installed. No explicit limitations, but the construction depends on 'signature' compatibility with Manify's API.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/53_gaussian_mixture_refactor.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Can we run Gaussian mixture with a component manifold?\\n# If not, what's stopping us?\\n\\npm = manify.manifolds.ProductManifold(signature=[(-1, 2), (1, 2)])\\ncm = pm.P[0]\\n\\npm\n```\n\n----------------------------------------\n\nTITLE: Setting up Jupyter Autoreload\nDESCRIPTION: Loads the `autoreload` IPython extension and configures it to automatically reload modules before executing code. This is useful for interactive development in Jupyter environments.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/10_torchified_hyperdt.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Loading IPython Autoreload Extension\nDESCRIPTION: Loads the `autoreload` IPython extension and configures it to automatically reload modules before executing code (mode 2). This is useful for interactive development, ensuring changes in imported modules are picked up without restarting the kernel.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/5_sampling.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Enabling Autoreload in Interactive Python Environment - Python\nDESCRIPTION: This snippet enables the autoreload extension for an IPython or Jupyter interactive environment using magic commands. This extension automatically reloads imported modules before executing code, which streamlines iterative development by ensuring code changes are reflected without restarting the kernel. No dependencies apart from running in an IPython-based environment are required; usage outside these environments results in an error.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/50_graph_embeddings_new.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Enabling Auto-Reload in IPython Notebooks - Python\nDESCRIPTION: This snippet enables the autoreload extension in IPython so that modules are automatically reloaded before executing user code. This is especially useful for interactive development and rapid iteration in Jupyter notebooks, ensuring changes in dependent Python files are reflected without manual kernel restarts. No special dependencies beyond IPython/Jupyter are required; there are no direct inputs or outputs.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/55_reimplement_gu.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Importing the Embedders Library\nDESCRIPTION: This snippet imports the 'embedders' library, which likely contains functions and classes for creating manifold embeddings and related tasks demonstrated in the subsequent code.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/2_polblogs_benchmark.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport embedders\n```\n\n----------------------------------------\n\nTITLE: Inspecting ProductManifold Dimension Mapping in Python\nDESCRIPTION: Accesses and implicitly prints the `dim2man` attribute of the `ProductManifold` object `pm`. This attribute likely provides a mapping from each dimension index in the product space back to the specific manifold component it belongs to.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_basic_visualization.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npm.dim2man\n```\n\n----------------------------------------\n\nTITLE: Evaluating Prediction Accuracy with PyTorch (Python)\nDESCRIPTION: Compares the predicted labels with the true labels using a PyTorch tensor operation, converting the boolean result to float and computing the mean for accuracy. Assumes predictions and y are both PyTorch tensors; dependencies include torch. Inputs are two tensors (predicted and true labels), outputs a scalar tensor with the mean accuracy. Used for quick classification performance checks.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nprint((predictions == y).float().mean())\n```\n\n----------------------------------------\n\nTITLE: Inspecting Shape of Stacked Means (Python)\nDESCRIPTION: This snippet shows how to stack manifold mean vectors (mu0) for batch operations and check resulting tensor shape. Uses torch.stack and inspects output shape. Requires torch and a valid M.mu0. Input is list of mu0, output is shape tuple.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/60_pytest_scratch.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntorch.stack([M.mu0] * 10, dim=1).shape\n\n```\n\n----------------------------------------\n\nTITLE: Testing `angular_greater` with Specific Angular Differences in Python\nDESCRIPTION: Creates a single random angle `_v1` and a tensor of random angles `_v2`. It specifically sets the first element of `_v2` to be slightly more than pi radians away from `_v1`, wrapping around using modulo 2*pi. It then calls `embedders.tree_new.angular_greater` comparing `_v1` to all elements in `_v2` and prints the pairs of angles along with the boolean comparison results. This likely tests the function's handling of the angular discontinuity at pi/-pi.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/14_new_tree_testing.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n_v1 = torch.randn(1)\n_v2 = torch.randn(10)\n_v2[0] = _v1 + torch.pi + 0.01 % (2 * torch.pi)\nprint(_v1)\nprint([(x.item(), y.item()) for x, y in zip(_v2, embedders.tree_new.angular_greater(_v1, _v2).flatten())])\n```\n\n----------------------------------------\n\nTITLE: Downsampling Training Data for Manifold-Based Modeling in Python\nDESCRIPTION: This code randomly selects 1000 samples each from the loaded training and testing datasets using numpy's random.choice, effectively reducing the dataset size for experiments or computational resource management. Downsampling is performed without replacement to ensure unique samples. The indices are then used to subset both the feature (X_train) and target (y_train) arrays. This step is critical in ensuring balanced and manageable dataset sizes for model training and evaluation.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/45_blood_and_lymphoma_interp.ipynb#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# Downsample to 1000 points each\n\ntrain_downsample = np.random.choice(X_train.shape[0], 1000, replace=False)\ntest_downsample = np.random.choice(X_test.shape[0], 1000, replace=False)\n\nX_train = X_train[train_downsample]\ny_train = y_train[train_downsample]\n```\n\n----------------------------------------\n\nTITLE: Accessing Perceptron Alpha Values in Manify\nDESCRIPTION: Accesses the learned `alpha` parameters of the previously trained `ProductSpacePerceptron` model (`perceptron`). These alpha values typically represent the weights assigned to the support vectors or training examples in the perceptron's decision function.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/60_pytest_scratch.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nperceptron.alpha[]\n```\n\n----------------------------------------\n\nTITLE: ProductManifold Batch Tests with Various Geometries (Python)\nDESCRIPTION: This block iterates over multiple product manifold signatures, stacking per-space covariance matrices for each space, and tests sampling, stereographic, and attribute checks across varied geometric configurations. It validates expected types for each component manifold (hyperbolic, Euclidean, spherical), samples points via Gaussian mixture, and checks attribute integrity. Prerequisites: torch, geoopt, ProductManifold, and pm.P. Input is signature list; output is assertions or printed boolean for full Euclidean cases.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/60_pytest_scratch.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfor signature in [[(-1, 8)], [(0, 8)], [(1, 8)], [(-1, 8), (1, 8)], [(-1, 8), (0, 8), (1, 8)], [(0, 8), (0, 8)]]:\n    pm = ProductManifold(signature=signature)\n\n    # get some vectors via gaussian mixture\n    covs = [torch.stack([torch.eye(M.dim) / M.dim / 100] * 10) for M in pm.P]\n    means = torch.vstack([pm.mu0] * 10)\n    torch.random.manual_seed(42)\n    X1, _ = pm.sample(z_mean=means, sigma_factorized=covs)\n    X2, _ = pm.sample(z_mean=means[:5], sigma_factorized=[cov[:5] for cov in covs])\n\n    # Do attributes work correctly?\n    for M in pm.P:\n        curv = M.curvature\n        if curv < 0:\n            assert M.type == \"H\" and isinstance(M.manifold.base, geoopt.Lorentz)\n        elif curv == 0:\n            assert M.type == \"E\" and isinstance(M.manifold.base, geoopt.Euclidean)\n        else:\n            assert M.type == \"S\" and isinstance(M.manifold.base, geoopt.Sphere)\n\n    # _shared_tests(pm, X1, X2, is_euclidean=all(M.curvature == 0 for M in pm.P))\n    print(all(M.curvature == 0 for M in pm.P))\n\n```\n\n----------------------------------------\n\nTITLE: Predicting with Custom Function\nDESCRIPTION: This code calls the custom `perceptron_predict` function on the `ps_perceptron` object (the multiclass one trained earlier) to obtain prediction probabilities.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nperceptron_predict(ps_perceptron)\n```\n\n----------------------------------------\n\nTITLE: Testing `angular_greater` Function on a Range of Angles in Python\nDESCRIPTION: Generates a range of angles (`_x`) from -2*pi to 2*pi using `torch.linspace`. It then calls the `embedders.tree_new.angular_greater` function with this range as both query and key inputs, likely to test its behavior across the full angular spectrum.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/14_new_tree_testing.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n_x = torch.linspace(-2 * torch.pi, 2 * torch.pi, 100)\n\nembedders.tree_new.angular_greater(_x, _x)\n```\n\n----------------------------------------\n\nTITLE: Generating LaTeX Table for Regression Results in Python\nDESCRIPTION: This script processes regression results by concatenating several Pandas DataFrames (`reg_sig`, `graphs_r`, `empirical_r`). It drops irrelevant columns, groups the data by 'table', 'dataset', and 'signature', and applies an aggregation function (`agg`, assumed to be defined elsewhere) to RMSE columns. It then sorts the results using the `sort_dict` (defined in the first snippet), selects specific RMSE columns, formats the index for LaTeX presentation, converts to a LaTeX string, modifies the string to customize the table appearance (header, rules), and saves the output to `../data/results_icml/table3.tex`. It depends on Pandas, input DataFrames, and the `agg` function.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/32_big_table.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nall_r = pd.concat([reg_sig, graphs_r, empirical_r])\nall_r = all_r.drop(columns=[\"embedding\", \"seed\", \"trial\", \"task\"])\n\nall_r_grouped = all_r.groupby([\"table\", \"dataset\", \"signature\"])[[c for c in all_r.columns if \"rmse\" in c]].apply(\n    lambda x: agg(x, regression=True)\n)\n\n# Sort \"table\" in MultiIndex: Synthetic, Graph, VAE, Other\nall_r_grouped = all_r_grouped.sort_index(level=\"table\", key=lambda x: x.map(sort_dict))\n\n# Get the columns fixed up\nlatex_df = all_r_grouped[\n    [\n        \"product_rf_rmse\",\n        \"sklearn_rf_rmse\",\n        \"tangent_rf_rmse\",\n        \"knn_rmse\",\n        # \"ambient_mlp_rmse\",\n        # \"kappa_gcn_rmse\"\n    ]\n]\nc_dict = {\n    # \"Synthetic (single $K$)\": \"-5.5cm\",\n    \"Synthetic (multi-$K$)\": \"-2.4cm\",\n    \"Graphs\": \"-.7cm\",\n    # \"VAE\": \"-.8cm\",\n    \"Other\": \"-.7cm\",\n}\nlatex_df.index = pd.MultiIndex.from_tuples(\n    [(f\"\\\\rotatebox{{90}}{{\\\\hspace{{{c_dict[c]}}}{c}}}\", d, s) for c, d, s in latex_df.index],\n    names=[\"\", \"Dataset\", \"Signature\"],\n)\n\n# Now we do text manipulation\nlatex = latex_df.to_latex(header=True, escape=False)\n\n# Remove all occurrences of \"\\cline{3-9}\"\nlatex = latex.replace(\"\\\\cline{1-9}\", \"\")\n# latex = latex.replace(\"\\\\cline{2-9}\", \"\")\n\n# Change top bar\nmy_toprule = \"\"\"\\\\toprule\n& Dataset & Signature & \\col{product_dt}{Product RF} & \\col{euclidean_dt}{Ambient RF} & \\col{tangent_dt}{Tangent RF} \n& \\col{knn}{$k$-Neighbors}\\\\\n\\\\midrule\"\"\"\nlatex = latex.split(\"\\n\")\nlatex = [latex[0], my_toprule] + latex[5:-4] + latex[-3:]\nlatex = \"\\n\".join(latex)\n\nwith open(\"../data/results_icml/table3.tex\", \"w\") as f:\n    f.write(latex)\n```\n\n----------------------------------------\n\nTITLE: Sanity Check for Comparison Tensor Logic in Angle Splitting - Python\nDESCRIPTION: Performs multiple for-loop assertions to verify the comparison logic for splitting angles by dimension in the reshaped comparison tensor. Uses modular arithmetic with torch to ensure negative classes have larger angles and checks integrity conditions on the comparison splits. These checks guarantee that the comparisons tensor definition correctly separates classes as expected.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/13_information_gain.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Quick sanity check: verify that for any n, d\n# We have the following:\n# angles[comparisons_reshaped[n, d] == 0., d] - angles[n, d] > 0\n# angles[comparisons_reshaped[n, d] == 1., d] - angles[n, d] <= 0\n\n# This follows directly from the definition of the comparisons tensor\n# This means that for any angle we consider, the negative class will have larger angles than the positive class\n# Therefore\n\nfor n in range(angles.shape[0]):\n    for d in range(angles.shape[1]):\n        assert (\n            (angles[comparisons_reshaped[n, d] == 0.0, d] - angles[n, d] + torch.pi) % (2 * torch.pi) > torch.pi\n        ).all()\n        assert (\n            (angles[comparisons_reshaped[n, d] == 1.0, d] - angles[n, d] + torch.pi) % (2 * torch.pi) <= torch.pi\n        ).all()\n        assert comparisons_reshaped[n, d, n] == 1.0\n        if comparisons_reshaped[n, d].sum() > 0 and comparisons_reshaped[n, d].sum() < comparisons_reshaped.shape[2]:\n            # The exact definition\n            assert ((angles[comparisons_reshaped[n, d] == 0.0, d] + torch.pi) % (2 * torch.pi)).min() > (\n                (angles[comparisons_reshaped[n, d] == 1.0, d] + torch.pi) % (2 * torch.pi)\n            ).max()\n\n```\n\n----------------------------------------\n\nTITLE: Converting Benchmark Results to Pandas DataFrame\nDESCRIPTION: Converts the `results` list, which aggregates dictionaries containing benchmark metrics from each experimental trial, into a pandas DataFrame. This transformation enables structured data analysis and manipulation using pandas functionalities.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/28_benchmark_link_prediction.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nresults = pd.DataFrame(results)\n```\n\n----------------------------------------\n\nTITLE: Accessing ProductManifold Attributes and Submanifold Properties - Python\nDESCRIPTION: Shows sample attribute and property accesses for a ProductManifold object, revealing its internal configuration, number of submanifolds, dimension mappings, and structural details. Assumes pm was created as shown previously.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/13_information_gain.ipynb#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\npm.signature\n```\n\nLANGUAGE: python\nCODE:\n```\npm.P\n```\n\nLANGUAGE: python\nCODE:\n```\npm.man2dim\n```\n\nLANGUAGE: python\nCODE:\n```\npm.dim\n```\n\nLANGUAGE: python\nCODE:\n```\npm.man2intrinsic\n```\n\nLANGUAGE: python\nCODE:\n```\npm.P[0].type\n```\n\n----------------------------------------\n\nTITLE: Importing Necessary Python Libraries\nDESCRIPTION: This snippet imports required libraries for the notebook. It includes `HyperbolicDecisionTreeClassifier` from `hyperdt`, the custom `embedders` module, `torch` for tensor operations, `numpy` for numerical operations, and `matplotlib.pyplot` for plotting.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/13_information_gain.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom hyperdt.tree import HyperbolicDecisionTreeClassifier\nimport embedders\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n```\n\n----------------------------------------\n\nTITLE: Importing Core Libraries in Python\nDESCRIPTION: Imports essential Python libraries for numerical computation (`numpy`), deep learning (`torch`), and manifold embedding (`embedders`). The `anndata` library import is commented out.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/10_torchified_hyperdt.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport numpy as np\nimport embedders\n\n# import anndata\n```\n\n----------------------------------------\n\nTITLE: Importing Core Libraries in Python\nDESCRIPTION: This snippet imports the necessary libraries for the subsequent operations: 'embedders' for manifold learning and embedding functionalities, and 'torch' for tensor computations and GPU management.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/4_cs_phds_regression.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n```python\nimport embedders\nimport torch\n```\n```\n\n----------------------------------------\n\nTITLE: Saving Regression Results as TSV - Python\nDESCRIPTION: Saves the benchmarking results DataFrame to a TSV file for later analysis or record keeping. This snippet sets the file path within the embedders project structure and specifies tab delimiters. Requires pandas and assumes that the 'results' DataFrame has been generated. Outputs a TSV file with all results, without DataFrame row indices.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/25_benchmark_globe.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nresults.to_csv(\"embedders/data/results_icml/temperature.tsv\", index=False, sep=\"\\t\")\n```\n\n----------------------------------------\n\nTITLE: Saving Classification Benchmark Results to TSV using Pandas\nDESCRIPTION: This code uses the pandas library to save the experiment results stored in the `results` DataFrame to a Tab-Separated Values (TSV) file. The filename is constructed dynamically based on the `TASK` variable (which was 'classification' in the preceding snippet) and indicates these might be results for an ICML submission ('results_icml'). The `index=False` argument prevents pandas from writing the DataFrame index as a column, and `sep='\\t'` specifies the tab delimiter.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/27_benchmark_signature_gaussian.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nresults.to_csv(f\"embedders/data/results_icml/{TASK}_signature2.tsv\", index=False, sep=\"\\t\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Jupyter Autoreload\nDESCRIPTION: Sets up the `autoreload` extension within a Jupyter environment. This automatically reloads Python modules before executing code, which is useful during development to ensure changes in imported modules are picked up without restarting the kernel.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/18_ablations.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Saving Merged Lymphoma AnnData Object to Disk - Python\nDESCRIPTION: This snippet saves the merged lymphoma/healthy cell AnnData object ('adata') to the specified disk location in .h5ad format. Requires the object to have been constructed previously, and the directory path to exist and be writable. No other inputs or outputs; used for workflow reproducibility.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/8_scrna_preprocessing.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nadata.write(\"/teamspace/studios/this_studio/embedders/data/blood_cell_scrna/adata_lymphoma.h5ad\")\n```\n\n----------------------------------------\n\nTITLE: Embedding Implementation Cross-Validation for Preprocessing and Information Gain - Python\nDESCRIPTION: Cross-validates preprocessing and information gain in functional and embedder-based implementations of ProductSpaceDT, ensuring parity between both by asserting tensor closeness on angles, label encodings, comparison tensors, and calculated information gain. Requires embedders.tree_new.ProductSpaceDT, pm, and torch; should be run after preprocessing to ensure correct implementation in both code paths.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/13_information_gain.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Assertions for the implemented version in embedders\n\npdt = embedders.tree_new.ProductSpaceDT(pm=pm)\nang_emb, lab_emb, class_emb, comp_emb = pdt.preprocess(X, y)\nassert torch.allclose(ang_emb, angles)\nassert torch.allclose(lab_emb, labels_onehot)\nassert torch.allclose(comp_emb, comparisons_reshaped)\n\nig_emb = embedders.tree_new.get_info_gains(comparisons=comp_emb, labels=lab_emb)\nassert torch.allclose(ig_emb, ig_est_functional)\n\n```\n\n----------------------------------------\n\nTITLE: Saving Benchmark Results to CSV File\nDESCRIPTION: Exports the collected and processed benchmark results, stored in the `results` pandas DataFrame, to a Comma-Separated Values (CSV) file. The file is saved as 'link_prediction2.csv' within the relative path '../data/results/'. The `index=False` parameter ensures that the DataFrame's index is not written into the CSV file.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/28_benchmark_link_prediction.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nresults.to_csv(\"../data/results/link_prediction2.csv\", index=False)\n```\n\n----------------------------------------\n\nTITLE: Installing Manify via pip (PyPI) - Bash\nDESCRIPTION: Installs the Manify library from the Python Package Index (PyPI) using pip. This is the recommended method for most users and requires that Python 3.10+ and pip are already installed. Running this command will download and install the latest released version of Manify and its dependencies.\nSOURCE: https://github.com/pchlenski/manify/blob/main/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install manify\n```\n\n----------------------------------------\n\nTITLE: Calculating Angle Values for Manifold Projections\nDESCRIPTION: Computes the angles for 2D projections of the data points `X` onto each component manifold defined in the `ProductManifold` `pm`. It iterates through each component manifold, determines the corresponding dimensions in `X`, and calculates angles using `torch.atan2`. The results are stored in `angle_vals`, reducing dimensionality from the ambient space to the intrinsic manifold dimensions.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/10_torchified_hyperdt.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# First, we can compute the angles of all 2-d projections\n\nangle_vals = torch.zeros(X.shape[0], pm.dim, device=device)\n\nfor i, M in enumerate(pm.P):\n    dims = pm.man2dim[i]\n    dims_target = pm.man2intrinsic[i]\n    if M.type in [\"H\", \"S\"]:\n        angle_vals[:, dims_target] = torch.atan2(X[:, dims[0]].view(-1, 1), X[:, dims[1:]])\n    elif M.type == \"E\":\n        angle_vals[:, dims_target] = torch.atan2(torch.tensor(1), X[:, dims])\n\nangle_vals.shape  # Note that we have gone from (1000, 14) to (1000, 10), the number of intrinsic dimensions\n```\n\n----------------------------------------\n\nTITLE: Installing Manify from GitHub - Bash\nDESCRIPTION: Installs the Manify library directly from the GitHub repository using pip. This approach is suitable for users who want the latest (possibly unreleased) version from the main branch. Requires Python 3.10+, pip, and an internet connection to access the repository.\nSOURCE: https://github.com/pchlenski/manify/blob/main/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/pchlenski/manify\n```\n\n----------------------------------------\n\nTITLE: Checking Stereographic Projection for NaNs - Python\nDESCRIPTION: Checks if the output of the stereographic projection of X on the product manifold contains any NaN values. Useful as a data integrity step before further model use; returns a boolean flag. Requires previously defined 'pm' and 'X', outputs a Boolean for error-checking before model fitting.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/25_benchmark_globe.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npm.stereographic(X)[1].isnan().any()\n```\n\n----------------------------------------\n\nTITLE: Importing Custom Manifold Classes\nDESCRIPTION: Imports the `Manifold` and `ProductManifold` classes from the local `embedders.manifolds` module. These classes likely encapsulate the logic for defining and operating on different types of geometric manifolds.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/5_sampling.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom embedders.manifolds import Manifold, ProductManifold\n```\n\n----------------------------------------\n\nTITLE: Visualize Specific UCI Training Example Matrix\nDESCRIPTION: This snippet selects the 1001st sparse matrix from the UCI training data (`data[\"tra_X_tr\"][0][1000]`), converts it to a dense array using `.todense()`, and visualizes it as a heatmap using `matplotlib.pyplot.matshow`.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/33_traffic.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nplt.matshow( data[\"tra_X_tr\"][0][1000].todense())\n```\n\n----------------------------------------\n\nTITLE: Importing Custom Embedders Module in Python\nDESCRIPTION: This code imports the 'embedders' custom module, which is presumably used for manifold definitions, data generation, and model utilities related to this project. Proper configuration of the PYTHONPATH or inclusion of the module in the working directory is necessary. Provides foundational dependencies for subsequent snippets.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/41_more_benchmarks.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport embedders\n```\n\n----------------------------------------\n\nTITLE: Importing Custom Embedders Module in Python\nDESCRIPTION: Imports the custom 'embedders' module, which is expected to provide data loading and embedding functionality for downstream tasks. This import must succeed for subsequent data loading code to function, so the custom module should be accessible in the runtime environment.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/38_mnist.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport embedders\n```\n\n----------------------------------------\n\nTITLE: Inspecting Root Node Attributes of ProductSpaceDT in Python\nDESCRIPTION: Accesses and implicitly prints the internal attribute dictionary (`__dict__`) of the root node (index 0) of the trained `ProductSpaceDT` object `pdt`. This shows the specific parameters and state associated with the first split in the decision tree, such as the feature used and the threshold.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_basic_visualization.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npdt.nodes[0].__dict__\n```\n\n----------------------------------------\n\nTITLE: Initializing ProductManifold from embedders - Python\nDESCRIPTION: Imports the custom embedders module and constructs a ProductManifold object with a specified signature, mixing negative and positive curvature submanifolds. The signature parameter details the geometry and dimension, e.g., [(-1, 4), (1, 4)]. Relies on correct embedders package presence.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/13_information_gain.ipynb#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport embedders\n\npm = embedders.manifolds.ProductManifold(signature=[(-1, 4), (1, 4)])\n```\n\n----------------------------------------\n\nTITLE: Configuring Autoreload for IPython/Jupyter\nDESCRIPTION: This snippet configures the IPython environment to automatically reload modules before executing code. This is useful during development to ensure changes in imported modules are reflected without restarting the kernel.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/4_cs_phds_regression.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```python\n%load_ext autoreload\n%autoreload 2\n```\n```\n\n----------------------------------------\n\nTITLE: Enabling Autoreload in IPython\nDESCRIPTION: Uses IPython magic commands to automatically reload modules before executing code. '%load_ext autoreload' loads the extension, and '%autoreload 2' configures it to reload all modules except those excluded by %aimport.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/30_fourier.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Setting up Autoreload in Jupyter Environment using Python\nDESCRIPTION: These Jupyter magic commands configure the environment to automatically reload imported Python modules before executing code. `%load_ext autoreload` loads the autoreload extension, and `%autoreload 2` sets it to reload all modules except those explicitly excluded.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/32_big_table.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Enable Autoreload in Jupyter (Kaggle Section)\nDESCRIPTION: This snippet, appearing again before the Kaggle dataset section, uses Jupyter magic commands to automatically reload modules. This ensures any updates to imported libraries or custom modules are reflected.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/33_traffic.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Loading CS PhDs Dataset using Embedders\nDESCRIPTION: This snippet uses the 'embedders.dataloaders.load' function to load the 'cs_phds' dataset. It retrieves both the distance matrix ('cs_dists') and the corresponding labels ('cs_labels').\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/4_cs_phds_regression.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n```python\n# Load CS PhDs\n\ncs_dists, cs_labels = embedders.dataloaders.load(\"cs_phds\", labels=True)\n```\n```\n\n----------------------------------------\n\nTITLE: Extracting Node Information from `hyperdt` Tree in Python\nDESCRIPTION: Uses the `agg_nodes` function to collect all nodes from the `dt_q` (`hyperdt`) tree. It then extracts and prints the feature index and threshold (`theta`) for each non-leaf node. A comment notes that 1 is subtracted from the feature index because `hyperdt` indexes ambient dimensions, and this adjustment is specific to single-manifold cases.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/14_new_tree_testing.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndt_q_nodes = agg_nodes(dt_q.tree, [])\n[(x.feature - 1, f\"{x.theta.item():.4f}\") for x in dt_q_nodes if x.feature is not None]\n# Subtract 1 because [REDACTED] indexes ambient dimensions\n# This only works for single-manifold items\n```\n\n----------------------------------------\n\nTITLE: Checking Finite Values in Training Distance Matrix in Python\nDESCRIPTION: This snippet checks if all elements in the `D_train` tensor (from the last iteration of the distortion calculation loop) are finite numbers (not NaN or infinity). It prints `True` if all values are finite, `False` otherwise. This is a debugging check for data validity.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/24_benchmark_graph_embeddings.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nD_train.isfinite().all()\n```\n\n----------------------------------------\n\nTITLE: Inspecting ProductSpaceDT Object Attributes in Python\nDESCRIPTION: Accesses and implicitly prints the internal attribute dictionary (`__dict__`) of the trained `ProductSpaceDT` object `pdt`. This reveals the internal state and parameters stored within the classifier object after training.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_basic_visualization.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npdt.__dict__\n```\n\n----------------------------------------\n\nTITLE: Inspecting Shape and Structure of Preprocessed Comparisons Array in Python\nDESCRIPTION: Retrieves the shape of the `comparisons` array, another output of the `_preprocess` method. Comments explain its potential structure: it stores precomputed boolean results of comparing angles, likely indexed by a comparison threshold angle (`key_theta`), a dimension (`dim`), and possibly related to query angles (`query_theta`). The comment `comparisons[theta, dim] is a boolean vector...` suggests it holds results for `angular_greater(theta, angles[:, dim])`.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/37_sklearn_decision_tree.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncomparisons.shape # key_theta, dim, query_theta\n# comparisons[theta, dim] is a boolean vector of whether angular_greater(theta, angles[:, dim]) is True\n```\n\n----------------------------------------\n\nTITLE: Setting Up Parameters for VAE Latent Space Analysis\nDESCRIPTION: Imports libraries (NumPy, embedders, pandas, tqdm) and defines lists specifying dataset names (`embeddings_names`), corresponding manifold signatures (`sigs`), data splits (`sets`), and data types (`datasets`) related to Variational Autoencoder (VAE) latent spaces. Includes commented-out code intended to load and check these latent space embeddings (stored as .npy files) for NaN values, suggesting preparation for a VAE-related benchmark or analysis that isn't fully implemented in this snippet.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/43_benchmark_neural.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport embedders\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\nembeddings_names = [\n    \"blood_cell_scrna\",\n    \"lymphoma\",\n    \"cifar_100\",\n    \"mnist\",\n]\nsigs = [\n    [(1, 2), (0, 2), (-1, 2), (-1, 2), (-1, 2)],\n    [(1, 2), (1, 2)],\n    [(1, 2), (1, 2), (1, 2), (1, 2)],\n    [(1, 2), (0, 2), (-1, 2)],\n]\nsets = [\"train\", \"test\"]\ndatasets = [\"X\", \"y\"]\n\n# bad = []\n# for embedding in embeddings_names:\n#     for trial in range(n_trials):\n#         for set_name in sets:\n#             for dataset in datasets:\n#                 my_data = np.load(f\"../data/{embedding}/embeddings/{dataset}_{set_name}_{trial}.npy\")\n#                 if np.isnan(my_data).any():\n#                     bad.append((embedding, trial, set_name, dataset))\n#                     print(embedding, trial, set_name, dataset)\n#                 # print(my_data.shape)\n```\n\n----------------------------------------\n\nTITLE: Inspecting the Manifold Origin (Python)\nDESCRIPTION: This snippet inspects the origin or mean value (mu0) of the manifold, which is used as a reference point for sampling. Requires a manifold object M. Input is M.mu0; output is its value.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/60_pytest_scratch.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nM.mu0\n\n```\n\n----------------------------------------\n\nTITLE: Enabling IPython Autoreload for embedders - Python\nDESCRIPTION: Enables interactive development with automatic module reloading using IPython's autoreload extension. This lets any imports or changed code in embedders or other modules be instantly reflected in the interactive session without restarting the kernel. No dependencies required other than interactive/environmental support for IPython magic commands.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/25_benchmark_globe.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Setting up IPython Autoreload\nDESCRIPTION: This snippet uses IPython magic commands to automatically reload modules before executing code. This is useful for development in interactive environments like Jupyter notebooks to ensure changes in imported modules are picked up without restarting the kernel.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/2_polblogs_benchmark.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Configuring Jupyter Autoreload\nDESCRIPTION: Sets up the autoreload extension in a Jupyter environment. This allows modules to be automatically reloaded before executing code, which is useful during development to incorporate changes without restarting the kernel.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/43_benchmark_neural.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Setting up Autoreload in Jupyter/IPython\nDESCRIPTION: These are IPython magic commands. `%load_ext autoreload` loads the autoreload extension, and `%autoreload 2` configures it to automatically reload all modules (except those excluded by %aimport) before executing user code. This is useful for interactive development when code in imported modules changes.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/27_benchmark_signature_gaussian.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Detaching Embedding Tensor\nDESCRIPTION: This snippet detaches the 'h6_polblogs' tensor from the PyTorch computation graph. This is typically done after training to treat the embedding as fixed data, preventing further gradient calculations.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/2_polblogs_benchmark.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# convert parameter back to tensor\nh6_polblogs = h6_polblogs.detach()\nh6_polblogs\n```\n\n----------------------------------------\n\nTITLE: Importing Manify Library - Python\nDESCRIPTION: This snippet imports the 'manify' Python package, which is presumed to provide geometric data loading and embedding utilities. It is required before calling any methods from the manify.utils or other manify submodules. The only dependency is the 'manify' library, which must be installed in the environment.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/55_reimplement_gu.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport manify\n```\n\n----------------------------------------\n\nTITLE: Importing embedders Module - Python\nDESCRIPTION: Imports a custom module named 'embedders', which likely provides utilities for manifold learning, synthetic data generation, and custom tree algorithms used throughout the experiment. This dependency must be available in the PYTHONPATH or project directory.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/20_roc_auc.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport embedders\n```\n\n----------------------------------------\n\nTITLE: Accessing Product Manifold Signature Attribute - Python\nDESCRIPTION: Retrieves or prints the signature attribute of a ProductManifold instance, which typically defines the geometric/product structure of the manifold. Assumes 'pm' is a previously instantiated object; no additional dependencies are needed beyond its creation.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/43_benchmark_neural.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\npm.signature\n\n```\n\n----------------------------------------\n\nTITLE: Summing Kernel Columns at Error Locations (Python)\nDESCRIPTION: Aggregates columns from kernel matrix K associated with misclassified points and projects them on corresponding feature vectors. This is another variant of error-focused statistics for learning algorithms.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nK[:, err] @ X[err]\n```\n\n----------------------------------------\n\nTITLE: Creating Directories for Embedding Storage (Bash)\nDESCRIPTION: This bash snippet uses the `mkdir -p` command to create nested directories for storing the generated embeddings for different datasets (polblogs, cora, citeseer, cs_phds). The `-p` flag ensures that parent directories are created as needed and no error occurs if directories already exist.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/2_polblogs_benchmark.ipynb#_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\n%%bash\n# rm -rf ../data/graphs/embeddings # Be careful!\nmkdir -p ../data/graphs/embeddings/polblogs\nmkdir -p ../data/graphs/embeddings/cora\nmkdir -p ../data/graphs/embeddings/citeseer\nmkdir -p ../data/graphs/embeddings/cs_phds\n```\n\n----------------------------------------\n\nTITLE: Loading Interactive Extensions in Jupyter with IPython Magic - Python\nDESCRIPTION: Loads IPython's autoreload extension, enabling automatic reloading of modules before code execution in Jupyter notebooks. No external dependencies are required beyond running in an IPython-compatible environment. This snippet accelerates iterative development by eliminating the need to restart the kernel after source changes.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/3_verify_shapes.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Inspecting Trained Decision Tree Attributes in Python\nDESCRIPTION: Accesses and displays the internal attribute dictionary (`__dict__`) of the trained `ProductSpaceDT` object `pdt`. This provides insight into the model's learned parameters and internal state after fitting.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_pc_basic_visualization.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npdt.__dict__\n```\n\n----------------------------------------\n\nTITLE: Plotting and Intersecting Multiple Analytic Circles in the Disk - Python\nDESCRIPTION: This snippet samples two random circle centers and radii, constructs analytic circles for each, and plots them on the Poincare disk. It defines a function to compute the intersections of two circles via analytic geometry, then plots those intersection points. Depends on analytic_circle from earlier, and uses torch and matplotlib.pyplot for manipulation and visualization. Key inputs are two centers and two radii (tensors); outputs are plots and intersection points. Function assumes circles are in 2D Euclidean space and may return None if there's no intersection.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/36_poincare_circles.ipynb#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# Do two points at the same time\\n\\nr1 = torch.rand(1) * 4\\nr2 = torch.rand(1) * 4\\nc1 = torch.rand(2) * 2 - 1\\nc2 = torch.rand(2) * 2 - 1\\nwhile c1.norm() > 1:\\n    c1 = torch.rand(2) * 2 - 1\\nwhile c2.norm() > 1:\\n    c2 = torch.rand(2) * 2 - 1\\n\\n\\ncirc1, r1p, c1p = analytic_circle(r1, c1)\\ncirc2, r2p, c2p = analytic_circle(r2, c2)\\n\\nplt.scatter(*c1, color=\"red\")\\nplt.scatter(*c2, color=\"red\")\\nplt.plot(circ1[:, 0], circ1[:, 1], color=\"blue\")\\nplt.plot(circ2[:, 0], circ2[:, 1], color=\"green\")\\n\\n# Plot the boundary\\ntheta = torch.linspace(0, 2 * torch.pi, 100)\\nx_circle = torch.cos(theta)\\ny_circle = torch.sin(theta)\\nplt.plot(x_circle, y_circle, color=\"black\")\\nplt.axis(\"equal\")\\n\\n\\n# Define the intersection function for circles\\ndef circle_intersections(c1, r1, c2, r2):\\n    x1, y1 = c1\\n    x2, y2 = c2\\n\\n    # Calculate the distance between the centers\\n    d = ((x2 - x1) ** 2 + (y2 - y1) ** 2) ** 0.5\\n\\n    # Check if there are no intersections\\n    if d > r1 + r2 or d < abs(r1 - r2) or d == 0:\\n        return None  # No intersection\\n\\n    # Distance from c1 to the point on the line between intersections\\n    a = (r1**2 - r2**2 + d**2) / (2 * d)\\n\\n    # Midpoint between the intersections\\n    pm_x = x1 + a * (x2 - x1) / d\\n    pm_y = y1 + a * (y2 - y1) / d\\n\\n    # Distance from the midpoint to the intersection points\\n    h = (r1**2 - a**2) ** 0.5\\n\\n    # Offsets for intersection points\\n    offset_x = h * (y2 - y1) / d\\n    offset_y = h * (x2 - x1) / d\\n\\n    # Intersection points\\n    p1 = (pm_x + offset_x, pm_y - offset_y)\\n    p2 = (pm_x - offset_x, pm_y + offset_y)\\n\\n    return p1, p2\\n\\n\\n# Compute the intersections\\nmidpoint = circle_intersections(c1, r1, c2, r2)\\nif midpoint is not None:\\n    p1, p2 = midpoint\\n    plt.scatter(*p1, color=\"purple\")\\n    plt.scatter(*p2, color=\"purple\")\\n\\nplt.show()\\n\n```\n\n----------------------------------------\n\nTITLE: Inspecting Manifold Dimension Mapping in Python\nDESCRIPTION: Accesses the `dim2man` attribute of the `ProductManifold` object `pm`. This attribute likely provides a mapping that indicates which component manifold each dimension index belongs to.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_pc_basic_visualization.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\npm.dim2man\n```\n\n----------------------------------------\n\nTITLE: Querying the Ambient Dimension of a Product Manifold (Python)\nDESCRIPTION: This minimal snippet queries the ambient (embedding) dimension property of a pre-existing product manifold object. The manifold must be previously defined and stored in the variable 'pm'. Outputs an integer reflecting the overall embedding dimensionality.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/41_more_benchmarks.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npm.ambient_dim\n```\n\n----------------------------------------\n\nTITLE: Projecting Misclassified Data to Feature Space (Python)\nDESCRIPTION: Accumulates class-weighted feature vectors for misclassified points using various torch tensor operations and kernel matrices. Each line represents a variant, combining selective dot products and sums to analyze kernel effects of errors. All require PyTorch and the above variables.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_32\n\nLANGUAGE: python\nCODE:\n```\n((y * 2 - 1)[err, None] * K[err, :]).sum(dim=1) @ X[err]\n```\n\n----------------------------------------\n\nTITLE: Checking Shape of Excluded Data Subset in Python\nDESCRIPTION: Selects the data points excluded by the mask `mask` (`X[~mask, dim2]`) along a specific dimension `dim2` and displays the shape of this subset. This is likely used for debugging or understanding the data used in the visualization.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_pc_basic_visualization.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nX[~mask, dim2].shape\n```\n\n----------------------------------------\n\nTITLE: Counting Filtered Data Points in Python\nDESCRIPTION: Calculates and implicitly prints the number of `False` values in the boolean tensor `mask`. In the context of the preceding visualization loop, this represents the count of data points that were filtered out (did not reach) the specific node being processed.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_basic_visualization.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n(~mask).sum()\n```\n\n----------------------------------------\n\nTITLE: Counting Excluded Data Points in Python\nDESCRIPTION: Calculates and displays the number of data points excluded by the boolean mask `mask` (i.e., `~mask`). This likely corresponds to the number of greyed-out points in one of the subplots generated in the previous visualization step.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_pc_basic_visualization.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n(~mask).sum()\n```\n\n----------------------------------------\n\nTITLE: Weighted Sums of Error Features and Kernels (Python)\nDESCRIPTION: Combines the class label (adjusted to {-1, 1}) and feature and kernel matrices for misclassified samples, showing different tensor contraction patterns. Used in gradient calculations or advanced error analysis for SVM/perceptron. Inputs: y, X, K, err mask.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_33\n\nLANGUAGE: python\nCODE:\n```\n((y * 2 - 1)[err, None]) * X[err] @ K[err]\n```\n\n----------------------------------------\n\nTITLE: Calculating Average Distortion (Debug) in Python\nDESCRIPTION: This snippet recalculates and prints the average distortion (D_avg) using the `D_train` and `D_est` tensors from the last iteration of the main distortion calculation loop. This serves as a debugging step to verify the `d_avg` calculation.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/24_benchmark_graph_embeddings.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nembedders.metrics.d_avg(D_train, D_est)\n```\n\n----------------------------------------\n\nTITLE: Defining Shared Benchmark Hyperparameters in Python\nDESCRIPTION: Defines global hyperparameters used across different benchmarking experiments. `N_FEATURES` specifies the feature selection strategy (likely related to pairwise distances), `MAX_DEPTH` sets the maximum depth for tree-based models, `MODELS` lists the types of models to benchmark, `N_SAMPLES` controls the number of trials per configuration, and `DOWNSAMPLE` sets a limit for dataset size reduction if needed.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/43_benchmark_neural.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nN_FEATURES = \"d_choose_2\"\nMAX_DEPTH = 3\nMODELS = [\"product_dt\", \"product_rf\", \"tangent_mlp\", \"ambient_mlp\", \"tangent_gnn\", \"ambient_gnn\"]\nN_SAMPLES = 10\n\nDOWNSAMPLE = 1000  # Used when number of datapoints per dataset is too large to work with\n```\n\n----------------------------------------\n\nTITLE: Examining Sampled Points (Python)\nDESCRIPTION: This snippet outputs or inspects the previously-sampled points (X1) on the manifold. Purpose is to check data or results from sampling. Input is X1; output is its value.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/60_pytest_scratch.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nX1\n\n```\n\n----------------------------------------\n\nTITLE: Importing Manify Library in Python\nDESCRIPTION: This snippet demonstrates importing the 'manify' library to use its manifold and predictive modeling functionalities. Prior installation of the 'manify' package is required. No additional parameters are used. The import enables use of all the 'manify' API for subsequent operations such as manifold creation and model fitting.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/56_single_manifold_rf.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport manify\n```\n\n----------------------------------------\n\nTITLE: Evaluating Per-Sample Kernel-Feature Product (Python)\nDESCRIPTION: Computes a dot product of the first sample's label-weighted kernel row and its feature vector. Comment suggests expected result should be positive if correct. Inputs: y, K, X; outputs: scalar tensor.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_36\n\nLANGUAGE: python\nCODE:\n```\n(y[0] * K[0]) @ X[0]  # Should be positive\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Setting Autoreload in Python\nDESCRIPTION: Imports essential libraries: `torch` for tensor computations, `numpy` for numerical operations, and `matplotlib.pyplot` for plotting. It also uses IPython magic commands (`%load_ext autoreload`, `%autoreload 2`) to enable automatic reloading of modules, which is useful during interactive development.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/15_covariance_scaling.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n```\n\n----------------------------------------\n\nTITLE: Checking Shapes of Error Sums (Python)\nDESCRIPTION: Examines the shape output of a sum aggregation expression involving misclassified sample error weighting. Used for debugging and dimension validation in tensor computations.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_35\n\nLANGUAGE: python\nCODE:\n```\n((y * 2 - 1)[err, None] * K[err, :]).sum(dim=1).shape\n```\n\n----------------------------------------\n\nTITLE: Testing Angular Comparison Function (`angular_greater`) in Python\nDESCRIPTION: Extracts the threshold (`theta`) and feature index (`feature`) from a trained `dt_new` tree. It then preprocesses the input data `X` to get angles and tests the `embedders.tree_new.angular_greater` function using the extracted threshold and a specific angle from the preprocessed data. The result of the comparison (a boolean) is printed.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/14_new_tree_testing.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntheta = dt_new.tree.theta\nfeature = dt_new.tree.feature\nangles, _, _, _ = dt_new.preprocess(X)\nembedders.tree_new.angular_greater(torch.tensor(theta).reshape(1, 1), angles[1, 3].reshape(1, 1)).item()\n```\n\n----------------------------------------\n\nTITLE: Commented-Out Pip Install Command\nDESCRIPTION: This is a commented-out shell command intended for installing the `cvxpy` Python package using pip. It is not executed as part of the script but serves as a note for a potential dependency.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Some installs we need\n# !pip install cvxpy\n```\n\n----------------------------------------\n\nTITLE: Importing the Embedders Library in Python\nDESCRIPTION: This snippet imports the main `embedders` library, making its modules and functions available for use.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_basic_visualization.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport embedders\n```\n\n----------------------------------------\n\nTITLE: Enabling Autoreload in IPython Environment (Python)\nDESCRIPTION: This snippet uses IPython magic commands to load the `autoreload` extension and set its mode to 2. This ensures that modules are automatically reloaded before code execution, which is useful during development to reflect changes in imported modules without restarting the kernel. Requires an IPython environment.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/49_plotly_dashboard.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Enabling Autoreload in IPython Notebooks Using Python Magic Commands\nDESCRIPTION: This snippet loads IPython's 'autoreload' extension, ensuring that modules are automatically reloaded before code execution. It is intended for interactive Python notebook environments to facilitate live project development and debugging. No arguments are required, and it has no effect outside of IPython notebooks.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/41_more_benchmarks.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Testing Negative Class Kernel-Feature Products (Python)\nDESCRIPTION: Performs class-weighted matrix multiplication for selected indices, evaluating expected sign patterns for false class assignments. Runs several such cases for samples 0, 1, and 6, with expected results commented. Inputs: K, X, y, sample indices.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/16_tabbaghi_classifiers.ipynb#_snippet_37\n\nLANGUAGE: python\nCODE:\n```\n(-1 * K[0] @ X) @ X[0], y[0]  # Should be negative\n```\n\n----------------------------------------\n\nTITLE: Initializing Manify and Importing Dependencies (Python)\nDESCRIPTION: This snippet loads IPython autoreload extention and imports the core Manify library, manifold classes, torch, and geoopt. Required dependencies are manify, torch, geoopt, and IPython. The imports enable subsequent code to utilize manifold objects, sampling, and scientific computations, and IPython reloads code on change. It is the setup block for following code.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/60_pytest_scratch.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n\nimport manify\nfrom manify.manifolds import Manifold\nimport torch\nimport geoopt\n\n```\n\n----------------------------------------\n\nTITLE: Testing Angular Comparison with Flattened Inputs in Python\nDESCRIPTION: Tests the `embedders.tree_new.angular_greater` function again, this time using flattened versions of the threshold (`theta`) and a specific angle from the preprocessed data (`angles`). This checks if the function handles flattened tensor inputs correctly.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/14_new_tree_testing.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nembedders.tree_new.angular_greater(torch.tensor(theta).flatten(), angles[1, 3].flatten()).item()\n```\n\n----------------------------------------\n\nTITLE: Displaying Results DataFrame in Python\nDESCRIPTION: This snippet simply displays the content of the 'results' Pandas DataFrame. In a Jupyter notebook or interactive environment, this would print the DataFrame to the output.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/26_benchmark_single_curvature_gaussian.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nresults\n```\n\n----------------------------------------\n\nTITLE: Calculating Modulo Operation with Pi in Python\nDESCRIPTION: Performs a simple arithmetic calculation involving subtraction and the modulo operator with `torch.pi`. This might be related to angle normalization or testing specific boundary conditions.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/14_new_tree_testing.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n0 - 0.75 * torch.pi % torch.pi\n```\n\n----------------------------------------\n\nTITLE: Accessing Specific Element in Preprocessed Comparisons Array in Python\nDESCRIPTION: Accesses and implicitly displays a specific element or slice from the `comparisons` array using the index `[3, 3]`. Based on the previous snippet's comments, this likely retrieves the precomputed boolean comparison results corresponding to the fourth threshold angle (`theta=3`) and the fourth dimension (`dim=3`).\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/37_sklearn_decision_tree.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncomparisons[3, 3]\n```\n\n----------------------------------------\n\nTITLE: Sanity Checking Angular Midpoint Calculation in Python\nDESCRIPTION: Performs a sanity check on midpoint calculations involving angles. It calculates the Euclidean midpoint between two angles derived using `torch.atan2` and compares the result (converted back using `1/torch.tan`) with the average of the original random values (`_v1`, `_v2`). This snippet aims to verify the behavior of angle representations and midpoint functions.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/14_new_tree_testing.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Sanity check on angular midpoints\n\n_v1 = torch.randn(1)\n_v2 = torch.randn(1)\n\n_m = embedders.midpoint.euclidean_midpoint(torch.atan2(torch.tensor(1), _v1), torch.atan2(torch.tensor(1), _v2))\n\nprint(1 / torch.tan(torch.atan2(torch.tensor(1), _v1)), _v1)\n\nprint(1 / torch.tan(_m), (_v1 + _v2) / 2)\n```\n\n----------------------------------------\n\nTITLE: Running Gaussian Mixtures on a ProductManifold in Manify with Python\nDESCRIPTION: This loop calls the 'gaussian_mixture()' method of a ProductManifold 100,000 times, discarding both outputs on each iteration. It is likely used for stress-testing, benchmarking, or searching for runtime issues. The 'pm' object must be a valid ProductManifold as constructed in previous snippets. The snippet expects the 'pm.gaussian_mixture()' method to be available and callable with no required arguments. No external outputs are produced.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/53_gaussian_mixture_refactor.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfor _ in range(100_000):\\n    _, _ = pm.gaussian_mixture()\n```\n\n----------------------------------------\n\nTITLE: Enabling Autoreload in Jupyter/IPython (Python)\nDESCRIPTION: Configures the Jupyter/IPython environment to automatically reload modules before executing code. This is useful during development when the underlying library code (`embedders`) might be changing. It uses the `%load_ext` and `%autoreload` magic commands.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/22_link_prediction.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Enabling Autoreload in IPython with Python\nDESCRIPTION: This snippet enables autoreload of modules in an interactive Python environment (like IPython or Jupyter) using magic commands. It simplifies the development workflow by automatically reloading any changed modules before executing user code. No special parameters are required, but it only works in IPython contexts. There are no inputs or outputs; it simply configures the interactive environment for code reloading.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/53_gaussian_mixture_refactor.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Enable Autoreload in Jupyter\nDESCRIPTION: This snippet uses Jupyter magic commands to automatically reload modules before executing code. This is useful during development to ensure changes in imported modules are picked up without restarting the kernel.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/33_traffic.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Displaying Styled Table Output - Python\nDESCRIPTION: This snippet outputs the styled result table generated in the previous analysis block. Assumes that styled_tab is a pandas DataFrame already formatted with markdown styling. It is meant for use in interactive Python sessions (such as Jupyter) to inspect the processed and formatted results tables. No input arguments or dependencies beyond the prior snippet outputs.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/43_benchmark_neural.ipynb#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nstyled_tab\n```\n\n----------------------------------------\n\nTITLE: Checking Manifold Dimensionality and Autoreload in IPython (Python)\nDESCRIPTION: This snippet combines the IPython autoreload extension with importing embedders and initializing a product manifold, then checks its intrinsic dimension ('pm.dim'). Useful for interactive debugging and inspection of manifold properties. Dependencies: embedders, IPython, proper initialization of product manifolds.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/41_more_benchmarks.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload \\n%autoreload 2\\n\\nimport embedders\\n\\npm = embedders.manifolds.ProductManifold(signature=[(1, 2), (-1, 2)])\\npm.dim\n```\n\n----------------------------------------\n\nTITLE: Exploring PyTorch Tensor Shapes with Python\nDESCRIPTION: This snippet imports the 'torch' library and checks the shape of a one-dimensional tensor created with 'torch.arange(3)'. It is generally used to confirm default behaviors of PyTorch's range and tensor creation tools. No input is required beyond the installed 'torch' package. The output demonstrates that 'torch.arange(3)' produces a tensor with shape (3,).\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/53_gaussian_mixture_refactor.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\\n\\ntorch.arange(3).shape\n```\n\n----------------------------------------\n\nTITLE: Printing Parameters and Resulting Values for Circles - Python\nDESCRIPTION: Prints out the parameters (radii, transformed centers) for two sampled circles. Provides direct feedback used mostly for inspection or debugging during the computation and visualization workflow. No dependencies apart from prior definitions of r1, r1p, c1, c1p, etc. Inputs are tensors; outputs are their printed values.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/36_poincare_circles.ipynb#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nprint(r1, r1p, c1, c1p)\\nprint(r2, r2p, c2, c2p)\\n\n```\n\n----------------------------------------\n\nTITLE: Calculating Tangent of Arctangent in Python\nDESCRIPTION: Calculates the tangent of an angle specified using `torch.atan2`. This is a simple PyTorch operation likely used for testing or debugging trigonometric functions.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/14_new_tree_testing.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntorch.tan(torch.atan2(torch.tensor(torch.pi), torch.tensor(1)))\n```\n\n----------------------------------------\n\nTITLE: Defining Graph Nodes with Attributes (Plaintext)\nDESCRIPTION: This data snippet lists graph nodes, each identified by a sequential integer ID. Each line includes the node's identifier (typically a name, sometimes with a year) enclosed in double quotes, followed by three floating-point values (consistently 0.0000, 0.0000, 0.5000 in this excerpt). This format likely serves as input for graph analysis or visualization, defining the entities within the network.\nSOURCE: https://github.com/pchlenski/manify/blob/main/data/graphs/cs_phds/cs_phds.txt#_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n    904 \"Hendra Suwanda 1979\"                    0.0000    0.0000    0.5000\n    905 \"Mario Szegedy\"                          0.0000    0.0000    0.5000\n    906 \"Senhua Tao 1994\"                        0.0000    0.0000    0.5000\n    907 \"Stanislaw Lefniewski\"                   0.0000    0.0000    0.5000\n    908 \"Jun Tarui 1992\"                         0.0000    0.0000    0.5000\n    909 \"Steve Tate 1991\"                        0.0000    0.0000    0.5000\n    910 \"Paul Taylor\"                            0.0000    0.0000    0.5000\n    911 \"Peter Johnstone\"                        0.0000    0.0000    0.5000\n    912 \"? Teer 1978\"                            0.0000    0.0000    0.5000\n    913 \"Ewan Tempero 1990\"                      0.0000    0.0000    0.5000\n    914 \"Shang-Hua Teng 1991\"                    0.0000    0.0000    0.5000\n    915 \"H.J.J. te_Riele 1976\"                   0.0000    0.0000    0.5000\n    916 \"James Thatcher\"                         0.0000    0.0000    0.5000\n    917 \"Stan Thomas 1983\"                       0.0000    0.0000    0.5000\n    918 \"David Blackwell\"                        0.0000    0.0000    0.5000\n    919 \"Clark Thompson 1979\"                    0.0000    0.0000    0.5000\n    920 \"Prasoon Tiwari 1986\"                    0.0000    0.0000    0.5000\n    921 \"See P. Toh 1992\"                        0.0000    0.0000    0.5000\n    922 \"Ioannis Tollis 1987\"                    0.0000    0.0000    0.5000\n    923 \"Kelly Gottlieb\"                         0.0000    0.0000    0.5000\n    924 \"Eric Torng 1994\"                        0.0000    0.0000    0.5000\n    925 \"Alberto Torres 1994\"                    0.0000    0.0000    0.5000\n    926 \"Jerry Trahan 1988\"                      0.0000    0.0000    0.5000\n    927 \"Henry Foley\"                            0.0000    0.0000    0.5000\n    928 \"Mart Trautwein 1995\"                    0.0000    0.0000    0.5000\n    929 \"Howard Trickey 1985\"                    0.0000    0.0000    0.5000\n    930 \"John Tromp 1993\"                        0.0000    0.0000    0.5000\n    931 \"P.M.B. Vitanyi 1978\"                    0.0000    0.0000    0.5000\n    932 \"Athanasios Tsakalidis 1983\"             0.0000    0.0000    0.5000\n    933 \"Manolis Tsangaris 1992\"                 0.0000    0.0000    0.5000\n    934 \"Thanasis Tsantilas 1990\"                0.0000    0.0000    0.5000\n    935 \"Philippas Tsigas 1995\"                  0.0000    0.0000    0.5000\n    936 \"D.-M. Tsou 1980\"                        0.0000    0.0000    0.5000\n    937 \"John Tukey 1939\"                        0.0000    0.0000    0.5000\n    938 \"Mark Tuttle 1989\"                       0.0000    0.0000    0.5000\n    939 \"Akhilesh Tyagi 1988\"                    0.0000    0.0000    0.5000\n    940 \"Jerzy Tyszkiewicz 1993\"                 0.0000    0.0000    0.5000\n    941 \"Wen-Guey Tzeng 1991\"                    0.0000    0.0000    0.5000\n    942 \"? Udding 1984\"                          0.0000    0.0000    0.5000\n    943 \"David Udin 1977\"                        0.0000    0.0000    0.5000\n    944 \"R.T. Udink\"                             0.0000    0.0000    0.5000\n    945 \"S.D.S. Swierstra\"                       0.0000    0.0000    0.5000\n    946 \"Arch McKellar\"                          0.0000    0.0000    0.5000\n    947 \"Yoon-Sup Um 1990\"                       0.0000    0.0000    0.5000\n    948 \"Pawel Urzyczyn 1983\"                    0.0000    0.0000    0.5000\n    949 \"Dimiter Vakarelov 1977\"                 0.0000    0.0000    0.5000\n    950 \"Thomas Valente 1992\"                    0.0000    0.0000    0.5000\n    951 \"S. van_Denneheuvel 1991\"                0.0000    0.0000    0.5000\n    952 \"M.P. van_der_Hulst 1990\"                0.0000    0.0000    0.5000\n    953 \"M.H. van_Emden 1971\"                    0.0000    0.0000    0.5000\n    954 \"Dirk Van_Gucht 1985\"                    0.0000    0.0000    0.5000\n    955 \"? van_Katwijk 1987\"                     0.0000    0.0000    0.5000\n    956 \"Frances Van_Scoy 1976\"                  0.0000    0.0000    0.5000\n    957 \"? van_Vliet 1979\"                       0.0000    0.0000    0.5000\n    958 \"Chris van_Wyk 1980\"                     0.0000    0.0000    0.5000\n    959 \"George Varghese\"                        0.0000    0.0000    0.5000\n    960 \"? Veen 1985\"                            0.0000    0.0000    0.5000\n    961 \"Mahandren Velauthapillai 1986\"          0.0000    0.0000    0.5000\n    962 \"K.M. Venkataraman 1981\"                 0.0000    0.0000    0.5000\n    963 \"Ann Yasuhara 1964\"                      0.0000    0.0000    0.5000\n    964 \"H. Venkateswaran 1986\"                  0.0000    0.0000    0.5000\n    965 \"? Verhoef 1969\"                         0.0000    0.0000    0.5000\n    966 \"Rakesh M. Verma 1989\"                   0.0000    0.0000    0.5000\n    967 \"Alfredo Viola 1995\"                     0.0000    0.0000    0.5000\n    968 \"Ramesh Viswanathan 1995\"                0.0000    0.0000    0.5000\n    969 \"Orli Waarts\"                            0.0000    0.0000    0.5000\n    970 \"Bob Wagner 1968\"                        0.0000    0.0000    0.5000\n    971 \"Igor Walukiewicz 1994\"                  0.0000    0.0000    0.5000\n    972 \"Jie Wang 1990\"                          0.0000    0.0000    0.5000\n    973 \"Tandy Warnow 1991\"                      0.0000    0.0000    0.5000\n    974 \"Ben Wegbreit\"                           0.0000    0.0000    0.5000\n    975 \"Mark Allen Weiss 1987\"                  0.0000    0.0000    0.5000\n    976 \"Arthur Werschulz 1977\"                  0.0000    0.0000    0.5000\n    977 \"Charles Wetherell 1975\"                 0.0000    0.0000    0.5000\n    978 \"Peter D. White 1989\"                    0.0000    0.0000    0.5000\n    979 \"William White 1990\"                     0.0000    0.0000    0.5000\n    980 \"Bob Wilber 1985\"                        0.0000    0.0000    0.5000\n    981 \"Gordon Wilfong 1984\"                    0.0000    0.0000    0.5000\n    982 \"Christopher Wilson 1984\"                0.0000    0.0000    0.5000\n    983 \"Thomas Wimer 1987\"                      0.0000    0.0000    0.5000\n    984 \"Karl Winklmann 1978\"                    0.0000    0.0000    0.5000\n    985 \"Vincent Wino 1987\"                      0.0000    0.0000    0.5000\n    986 \"Terry Winograd 1970\"                    0.0000    0.0000    0.5000\n    987 \"C. Witteveen 1984\"                      0.0000    0.0000    0.5000\n    988 \"Marty Wolf 1990\"                        0.0000    0.0000    0.5000\n    989 \"Ouri Wolfson 1984\"                      0.0000    0.0000    0.5000\n    990 \"Heather Woll 1988\"                      0.0000    0.0000    0.5000\n    991 \"Pierre Wolper 1981\"                     0.0000    0.0000    0.5000\n    992 \"Patrick Wong 1978\"                      0.0000    0.0000    0.5000\n    993 \"Don Woods 1981\"                         0.0000    0.0000    0.5000\n    994 \"Detlef Wotschke\"                        0.0000    0.0000    0.5000\n    995 \"Celia Wrathall 1976\"                    0.0000    0.0000    0.5000\n    996 \"Rebecca N. Wright 1994\"                 0.0000    0.0000    0.5000\n    997 \"Michael Wu 1992\"                        0.0000    0.0000    0.5000\n    998 \"Peter Yick Fai Wu 1987\"                 0.0000    0.0000    0.5000\n    999 \"James Wyllie 1979\"                      0.0000    0.0000    0.5000\n   1000 \"Yacov Yacobi 1980\"                      0.0000    0.0000    0.5000\n   1001 \"Ali Yaghi 1984\"                         0.0000    0.0000    0.5000\n   1002 \"Pei-Yuan Yan 1989\"                      0.0000    0.0000    0.5000\n   1003 \"Mihalis Yannakakis 1978\"                0.0000    0.0000    0.5000\n   1004 \"Frances Yao 1974\"                       0.0000    0.0000    0.5000\n   1005 \"Bennet Yee 1994\"                        0.0000    0.0000    0.5000\n   1006 \"R. Bar Yehuda 1982\"                     0.0000    0.0000    0.5000\n   1007 \"Akitoshi Yoshida 1994\"                  0.0000    0.0000    0.5000\n   1008 \"Neal E. Young 1991\"                     0.0000    0.0000    0.5000\n   1009 \"David Yun\"                              0.0000    0.0000    0.5000\n   1010 \"Moti Yung 1988\"                         0.0000    0.0000    0.5000\n   1011 \"Christos D. Zaroliagis 1991\"            0.0000    0.0000    0.5000\n   1012 \"Bernhard Zeigler\"                       0.0000    0.0000    0.5000\n   1013 \"Louxin Zhang 1995\"                      0.0000    0.0000    0.5000\n   1014 \"Yanjun Zhang\"                           0.0000    0.0000    0.5000\n   1015 \"Huang Zhisheng 1994\"                    0.0000    0.0000    0.5000\n   1016 \"Rich Zippel\"                            0.0000    0.0000    0.5000\n   1017 \"? Zonneveld 1964\"                       0.0000    0.0000    0.5000\n   1018 \"Lenore Zuck 1986\"                       0.0000    0.0000    0.5000\n   1019 \"David Zuckerman 1991\"                   0.0000    0.0000    0.5000\n   1020 \"J. Zwiers 1988\"                         0.0000    0.0000    0.5000\n   1021 \"---\"                                    0.0000    0.0000    0.5000\n   1022 \"---\"                                    0.0000    0.0000    0.5000\n   1023 \"---\"                                    0.0000    0.0000    0.5000\n   1024 \"---\"                                    0.0000    0.0000    0.5000\n   1025 \"---\"                                    0.0000    0.0000    0.5000\n```\n\n----------------------------------------\n\nTITLE: Loading Autoreload Extension in Python\nDESCRIPTION: Loads the IPython `autoreload` extension to automatically reload modules before executing code. This is useful for interactive development, ensuring changes in imported modules are picked up without restarting the kernel.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/14_new_tree_testing.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Configuring Jupyter Autoreload Extensions - Python\nDESCRIPTION: This snippet loads and configures the autoreload extension in a Jupyter notebook using magic commands. The extension automatically reloads imported Python modules before executing code, which is essential for iterative development as changes in source files are reflected immediately without restarting the kernel. No external dependencies beyond a Jupyter environment are required; users should ensure the 'autoreload' extension is available. No input or output parameters are involved; these commands must be executed at the start of a notebook.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/12_visualizations.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Enabling Autoreload in IPython/Jupyter Environment\nDESCRIPTION: Uses IPython magic commands to load the 'autoreload' extension and configure it to automatically reload all modules (except those explicitly excluded) before executing user code. This is useful during development to ensure changes in imported modules are reflected without restarting the kernel.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/48_fix_transductive_coords.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Configuring IPython Autoreload\nDESCRIPTION: This snippet uses IPython magic commands to load and enable the `autoreload` extension. `%autoreload 2` specifically reloads all modules (except those excluded by %aimport) automatically before executing any code.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/13_information_gain.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Importing Embedders Library in Python\nDESCRIPTION: Imports the top-level 'embedders' library, making its modules and functions available for use in the script.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_pc_basic_visualization.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport embedders\n```\n\n----------------------------------------\n\nTITLE: Placeholder for Data Summarization with Pandas and NumPy - Python\nDESCRIPTION: Imports pandas and numpy, preparing for data summarization and analysis. No functional code is present; this serves as an initial setup for subsequent data manipulation tasks.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/43_benchmark_neural.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport numpy as np\n\n# Summarize data\n\n```\n\n----------------------------------------\n\nTITLE: Listing Individuals with Numerical Data in Text Format\nDESCRIPTION: This text snippet displays lines 388 through 516 of a data file from the 'manify' project. Each line follows the format: `lineNumber \\\"Name [Year]\\\" float float float`. The floating-point values are constant (0.0000, 0.0000, 0.5000) in this section, possibly representing initial scores or placeholder values for the listed individuals.\nSOURCE: https://github.com/pchlenski/manify/blob/main/data/graphs/cs_phds/cs_phds.txt#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n    388 \"Stephen Garland 1967\"                   0.0000    0.0000    0.5000\n    389 \"William Gasarch 1985\"                   0.0000    0.0000    0.5000\n    390 \"Fanica Gavril 1973\"                     0.0000    0.0000    0.5000\n    391 \"Hillel Gazit 1988\"                      0.0000    0.0000    0.5000\n    392 \"Gary Miller 1975\"                       0.0000    0.0000    0.5000\n    393 \"Pete Gemmell 1992\"                      0.0000    0.0000    0.5000\n    394 \"Rosario Gennaro 1996\"                   0.0000    0.0000    0.5000\n    395 \"Mihaly Gereb-Graus 1989\"                0.0000    0.0000    0.5000\n    396 \"John Geske 1987\"                        0.0000    0.0000    0.5000\n    397 \"Alan Selman 1970\"                       0.0000    0.0000    0.5000\n    398 \"Raffaele Giancarlo\"                     0.0000    0.0000    0.5000\n    399 \"Peter B. Gibbons 1976\"                  0.0000    0.0000    0.5000\n    400 \"Phil Gibbons 1989\"                      0.0000    0.0000    0.5000\n    401 \"John Gilbert 1981\"                      0.0000    0.0000    0.5000\n    402 \"Rick Giles 1976\"                        0.0000    0.0000    0.5000\n    403 \"R.W. Robinson\"                          0.0000    0.0000    0.5000\n    404 \"David Gillman 1993\"                     0.0000    0.0000    0.5000\n    405 \"Jay Gischer 1984\"                       0.0000    0.0000    0.5000\n    406 \"Shafee Give'on 1965\"                    0.0000    0.0000    0.5000\n    407 \"David Gladstein 1995\"                   0.0000    0.0000    0.5000\n    408 \"Mitchell Wand 1973\"                     0.0000    0.0000    0.5000\n    409 \"Igal Golan 1974\"                        0.0000    0.0000    0.5000\n    410 \"Allen Goldberg 1979\"                    0.0000    0.0000    0.5000\n    411 \"Andrew Goldberg 1987\"                   0.0000    0.0000    0.5000\n    412 \"Leslie Goldberg 1992\"                   0.0000    0.0000    0.5000\n    413 \"Mark Jerrum 1981\"                       0.0000    0.0000    0.5000\n    414 \"Paul Goldberg 1992\"                     0.0000    0.0000    0.5000\n    415 \"Kenneth Goldman 1990\"                   0.0000    0.0000    0.5000\n    416 \"Sally Goldman 1990\"                     0.0000    0.0000    0.5000\n    417 \"Mikael Goldmann 1992\"                   0.0000    0.0000    0.5000\n    418 \"Johan Hastad 1986\"                      0.0000    0.0000    0.5000\n    419 \"Oded Goldreich 1983\"                    0.0000    0.0000    0.5000\n    420 \"Leslie M. Goldschlager 1977\"            0.0000    0.0000    0.5000\n    421 \"Judy Goldsmith 1988\"                    0.0000    0.0000    0.5000\n    422 \"Deborah Joseph 1981\"                    0.0000    0.0000    0.5000\n    423 \"Arthur Goldstein 1992\"                  0.0000    0.0000    0.5000\n    424 \"Mordecai Golin 1990\"                    0.0000    0.0000    0.5000\n    425 \"Robert Sedgewick 1975\"                  0.0000    0.0000    0.5000\n    426 \"Ralph Gomory 1954\"                      0.0000    0.0000    0.5000\n    427 \"Teofilo Gonzalez 1975\"                  0.0000    0.0000    0.5000\n    428 \"Deepak Goyal 1977\"                      0.0000    0.0000    0.5000\n    429 \"Niall Graham 1989\"                      0.0000    0.0000    0.5000\n    430 \"Albert Greenberg\"                       0.0000    0.0000    0.5000\n    431 \"Ronald Greenberg 1989\"                  0.0000    0.0000    0.5000\n    432 \"Dan Greene 1983\"                        0.0000    0.0000    0.5000\n    433 \"John Grefenstette 1980\"                 0.0000    0.0000    0.5000\n    434 \"Michelangelo Grigni 1991\"               0.0000    0.0000    0.5000\n    435 \"Dana Grinstead 1989\"                    0.0000    0.0000    0.5000\n    436 \"Peter J. Slater 1973\"                   0.0000    0.0000    0.5000\n    437 \"Joachim Grollmann 1984\"                 0.0000    0.0000    0.5000\n    438 \"Adam Grove 1992\"                        0.0000    0.0000    0.5000\n    439 \"Joe Halpern\"                            0.0000    0.0000    0.5000\n    440 \"? Grune 1982\"                           0.0000    0.0000    0.5000\n    441 \"Dah Jyh Guan 1989\"                      0.0000    0.0000    0.5000\n    442 \"Steve Guattery\"                         0.0000    0.0000    0.5000\n    443 \"Leonidas J. Guibas\"                     0.0000    0.0000    0.5000\n    444 \"Katia Guimaraes 1992\"                   0.0000    0.0000    0.5000\n    445 \"Ashish Gupta 1994\"                      0.0000    0.0000    0.5000\n    446 \"Udai Gupta 1977\"                        0.0000    0.0000    0.5000\n    447 \"Vineet Gupta 1994\"                      0.0000    0.0000    0.5000\n    448 \"Dan Gusfield\"                           0.0000    0.0000    0.5000\n    449 \"Stuart Haber\"                           0.0000    0.0000    0.5000\n    450 \"A. Nico Habermann 1967\"                 0.0000    0.0000    0.5000\n    451 \"Ramsey Haddad 1990\"                     0.0000    0.0000    0.5000\n    452 \"Oystein Ore 1924\"                       0.0000    0.0000    0.5000\n    453 \"Magnus Halldorsson 1991\"                0.0000    0.0000    0.5000\n    454 \"Suzanne Hambrusch 1982\"                 0.0000    0.0000    0.5000\n    455 \"Janos Simon 1975\"                       0.0000    0.0000    0.5000\n    456 \"Dick Hamlet 1971\"                       0.0000    0.0000    0.5000\n    457 \"Michael M. Hammer 1973\"                 0.0000    0.0000    0.5000\n    458 \"Yenjo Han 1994\"                         0.0000    0.0000    0.5000\n    459 \"Lane Hemaspaandra 1987\"                 0.0000    0.0000    0.5000\n    460 \"Eleanor Hare 1989\"                      0.0000    0.0000    0.5000\n    461 \"Dov Harel\"                              0.0000    0.0000    0.5000\n    462 \"George Lueker 1976\"                     0.0000    0.0000    0.5000\n    463 \"Leo Harrington 1973\"                    0.0000    0.0000    0.5000\n    464 \"R. Carnap\"                              0.0000    0.0000    0.5000\n    465 \"Carl Hauser 1980\"                       0.0000    0.0000    0.5000\n    466 \"David Haussler 1982\"                    0.0000    0.0000    0.5000\n    467 \"Lenny Heath 1985\"                       0.0000    0.0000    0.5000\n    468 \"Don Heller 1977\"                        0.0000    0.0000    0.5000\n    469 \"J.F. Traub 1959\"                        0.0000    0.0000    0.5000\n    470 \"Lisa Hellerstein\"                       0.0000    0.0000    0.5000\n    471 \"Monika R. Henzinger 1993\"               0.0000    0.0000    0.5000\n    472 \"Thomas A. Henzinger 1991\"               0.0000    0.0000    0.5000\n    473 \"John Hershberger 1987\"                  0.0000    0.0000    0.5000\n    474 \"Amir Herzberg 1991\"                     0.0000    0.0000    0.5000\n    475 \"Rattikorn Hewett 1986\"                  0.0000    0.0000    0.5000\n    476 \"Giora Slutzki 1977\"                     0.0000    0.0000    0.5000\n    477 \"Carl Hewitt 1971\"                       0.0000    0.0000    0.5000\n    478 \"Seymour Papert\"                         0.0000    0.0000    0.5000\n    479 \"Allan Heydon 1992\"                      0.0000    0.0000    0.5000\n    480 \"Dan Hirschberg 1975\"                    0.0000    0.0000    0.5000\n    481 \"Chun-Kuen Ho 1994\"                      0.0000    0.0000    0.5000\n    482 \"My Hoang 1995\"                          0.0000    0.0000    0.5000\n    483 \"John Mitchell 1984\"                     0.0000    0.0000    0.5000\n    484 \"John Hobby 1985\"                        0.0000    0.0000    0.5000\n    485 \"Peter Hochschild 1986\"                  0.0000    0.0000    0.5000\n    486 \"Arthur Banks\"                           0.0000    0.0000    0.5000\n    487 \"Peter Honeyman 1980\"                    0.0000    0.0000    0.5000\n    488 \"Jim Hoover 1987\"                        0.0000    0.0000    0.5000\n    489 \"R.L. Matson\"                            0.0000    0.0000    0.5000\n    490 \"Ellis Horowitz\"                         0.0000    0.0000    0.5000\n    491 \"Ned Horvath 1975\"                       0.0000    0.0000    0.5000\n    492 \"Brian Howard 1992\"                      0.0000    0.0000    0.5000\n    493 \"Paul G. Howard 1993\"                    0.0000    0.0000    0.5000\n    494 \"Tom Howell 1976\"                        0.0000    0.0000    0.5000\n    495 \"Scott Huddleston 1982\"                  0.0000    0.0000    0.5000\n    496 \"W.E. Singletary 1964\"                   0.0000    0.0000    0.5000\n    497 \"Chao-Kuei Hung 1996\"                    0.0000    0.0000    0.5000\n    498 \"Harry Hunt 1973\"                        0.0000    0.0000    0.5000\n    499 \"Greg Hunter 1977\"                       0.0000    0.0000    0.5000\n    500 \"Martin Hyland\"                          0.0000    0.0000    0.5000\n    501 \"Dexter Kozen 1977\"                      0.0000    0.0000    0.5000\n    502 \"Witold Lipski 1974\"                     0.0000    0.0000    0.5000\n    503 \"Russell Impagliazzo\"                    0.0000    0.0000    0.5000\n    504 \"Sandra Irani 1990\"                      0.0000    0.0000    0.5000\n    505 \"Isaac Saias 1995\"                       0.0000    0.0000    0.5000\n    506 \"Alexander T. Ishii 1991\"                0.0000    0.0000    0.5000\n    507 \"Veysi Isler 1995\"                       0.0000    0.0000    0.5000\n    508 \"Bulent Ozguc 1979\"                      0.0000    0.0000    0.5000\n    509 \"Alon Itai 1976\"                         0.0000    0.0000    0.5000\n    510 \"Pino Italiano\"                          0.0000    0.0000    0.5000\n    511 \"Ken Iverson 1954\"                       0.0000    0.0000    0.5000\n    512 \"Jeff Jaffe 1979\"                        0.0000    0.0000    0.5000\n    513 \"Sanjay Jain 1990\"                       0.0000    0.0000    0.5000\n    514 \"Hakan Jakobsson 1993\"                   0.0000    0.0000    0.5000\n    515 \"Fahimeh Jalili\"                         0.0000    0.0000    0.5000\n    516 \"Jan van_der_Craats 1972\"                0.0000    0.0000    0.5000\n```\n\n----------------------------------------\n\nTITLE: Displaying Scaling Factor Variable in Python\nDESCRIPTION: This snippet prints the value of the variable `a`, which represents the maximum-likelihood scaling factor calculated in the last iteration of the preceding loop. This is likely used for debugging purposes to inspect the calculated scaling factor.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/24_benchmark_graph_embeddings.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\na\n```\n\n----------------------------------------\n\nTITLE: Displaying Dimension Index in Python\nDESCRIPTION: Prints the current value of the variable `dim2`. This variable likely holds the index of the second dimension used for plotting in the last iteration of the visualization loop.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/42_pc_basic_visualization.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndim2\n```\n\n----------------------------------------\n\nTITLE: Defining Quick Test Hyperparameters (Commented Out)\nDESCRIPTION: Contains commented-out hyperparameters (`N_SAMPLES`, `DOWNSAMPLE`) with smaller values. These are likely used for quick testing runs during development to reduce execution time.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/43_benchmark_neural.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Quick test hyperparameters\n\n# N_SAMPLES = 1\n# DOWNSAMPLE = 10\n```\n\n----------------------------------------\n\nTITLE: Displaying Distance Matrix Variable - Python\nDESCRIPTION: This single-line snippet outputs the contents of the 'D' variable, which is expected to be a distance matrix loaded earlier. It is intended for interactive display, typically within a Jupyter notebook. There are no dependencies or parameters, and it simply prints or renders the current state of D.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/55_reimplement_gu.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nD\n```\n\n----------------------------------------\n\nTITLE: Defining Graph Edges in Pajek Format (Data)\nDESCRIPTION: This section defines the edges of a graph using the Pajek format. It begins with the '*Edges' declaration. Each subsequent line represents an edge, specified by the starting vertex ID, the ending vertex ID, and an edge weight (consistently '1' in this data, suggesting an unweighted or binary relationship graph).\nSOURCE: https://github.com/pchlenski/manify/blob/main/data/graphs/cs_phds/cs_phds.txt#_snippet_4\n\nLANGUAGE: plaintext\nCODE:\n```\n*Edges\n    397    813       1\n    401     38       1\n    403     99       1\n    403    212       1\n    408    407       1\n    408    680       1\n    408    712       1\n    408    887       1\n    408    979       1\n    411    560       1\n    411    564       1\n    411    769       1\n    413    412       1\n    413    414       1\n    413    864       1\n    418    417       1\n    418    536       1\n    419    474       1\n    419    587       1\n    419    740       1\n    422    421       1\n    422    667       1\n    425    424       1\n    425    975       1\n    427    612       1\n    436    435       1\n    439    438       1\n    439    578       1\n    439    683       1\n    443    473       1\n    443    778       1\n    443    891       1\n    448    695       1\n    450    522       1\n    450    873       1\n    452    191       1\n    455    454       1\n    455    905       1\n    459    458       1\n    462    461       1\n    464    229       1\n    466    628       1\n    467    527       1\n    467    735       1\n    469    468       1\n    469     90       1\n    469    976       1\n    476    475       1\n    476    535       1\n    476    616       1\n    476    791       1\n    478    477       1\n    478    307       1\n    478    986       1\n    478   1009       1\n    480    609       1\n    483    482       1\n    483    492       1\n    483    550       1\n    483    624       1\n    483    968       1\n    486    245       1\n    487    690       1\n    487    811       1\n    489     22       1\n    490    235       1\n    496    180       1\n    496    718       1\n    498    652       1\n    500    910       1\n    501    222       1\n    502    237       1\n    508    507       1\n    517    516       1\n    518    928       1\n    520    350       1\n    523    422       1\n    523    629       1\n    523    984       1\n    526    525       1\n    531    603       1\n    531    627       1\n    531    950       1\n    534    533       1\n    534    969       1\n    542    541       1\n    542    742       1\n    542    924       1\n    547    592       1\n    547    874       1\n    549    548       1\n    549    732       1\n    549    966       1\n    552    734       1\n    554    553       1\n    555    526       1\n    556    380       1\n    556    601       1\n    557    380       1\n    566    565       1\n    566    588       1\n    571    760       1\n    572    935       1\n    573     20       1\n    573    884       1\n    573    888       1\n    574    657       1\n    574    941       1\n    575    574       1\n    576    944       1\n    580    600       1\n    584    583       1\n    584    789       1\n    590    589       1\n    595    594       1\n    595    557       1\n    597    653       1\n    601    600       1\n    606    605       1\n    606    832       1\n    613    105       1\n    626    502       1\n    626    733       1\n    631    677       1\n    649    759       1\n    653    652       1\n    655    654       1\n    672    671       1\n    679    678       1\n    681    130       1\n    684    683       1\n    687    686       1\n    687    801       1\n    687    961       1\n    693     75       1\n    698    933       1\n    705    704       1\n    705    794       1\n    705    940       1\n    705    948       1\n    705    971       1\n    710    709       1\n    710    730       1\n    710   1002       1\n    714    452       1\n    717    716       1\n    726    746       1\n    727    317       1\n    751    109       1\n    754    753       1\n    756    755       1\n    757     36       1\n    763    982       1\n    765    764       1\n    766    764       1\n    766    705       1\n    766    949       1\n    768    767       1\n    772    771       1\n    785    784       1\n    835    595       1\n    839    908       1\n    846    845       1\n    854    606       1\n    861    860       1\n    866    714       1\n    873    939       1\n    879    878       1\n    880    989       1\n    880   1011       1\n    893    892       1\n    894    613       1\n    907    206       1\n    911    910       1\n    918    194       1\n    923    717       1\n    927    469       1\n    931    930       1\n    945    944       1\n    946    132       1\n    963    962       1\n    963    861       1\n   1021    214       1\n   1022    308       1\n   1023    268       1\n   1024    323       1\n   1025    681       1\n```\n\n----------------------------------------\n\nTITLE: Halting Code Execution Using an Assert Statement - Python\nDESCRIPTION: Includes an assert statement to immediately interrupt execution in an interactive or batch environment, typically used as a manual break or development safeguard. No dependencies; will raise AssertionError when reached.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/43_benchmark_neural.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# Interrupt above code\n\nassert False\n\n```\n\n----------------------------------------\n\nTITLE: Importing Embedders Package in Python\nDESCRIPTION: These snippets demonstrate how to import the main embedders package in Python. No external dependencies other than embedders itself are required. Useful as a preliminary step for accessing the package functionalities provided in further examples.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/39_reembed_gaussian.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport embedders\n```\n\nLANGUAGE: python\nCODE:\n```\nimport embedders\n```\n\n----------------------------------------\n\nTITLE: Importing the Manify Package with Python\nDESCRIPTION: This snippet imports the 'manify' package, which is required for manifold-related computations and classes. It does not take any parameters or provide any outputs by itself, but is a necessary dependency for subsequent use of Manify functionality.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/53_gaussian_mixture_refactor.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport manify\n```\n\n----------------------------------------\n\nTITLE: Importing the Embedders Library in Python\nDESCRIPTION: Imports the 'embedders' library, which is the core dependency for the subsequent manifold learning tasks demonstrated in the file. This library presumably contains modules for data loading, manifold definitions, and training algorithms.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/48_fix_transductive_coords.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport embedders\n```\n\n----------------------------------------\n\nTITLE: Defining Graph Arcs/Edges (Plaintext)\nDESCRIPTION: This data snippet, marked by '*Arcs', defines the connections (arcs or edges) between the previously listed nodes. Each line represents a directed or undirected edge, specifying the source node ID (first integer), the target node ID (second integer), and an associated weight (third integer, consistently 1 here). This edge list format is standard for describing graph topology.\nSOURCE: https://github.com/pchlenski/manify/blob/main/data/graphs/cs_phds/cs_phds.txt#_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\n*Arcs\n      2      1       1\n      2     30       1\n      2     62       1\n      2    593       1\n      4      3       1\n      4     91       1\n      4    188       1\n      4    204       1\n      4     73       1\n      4    472       1\n      4    552       1\n      4    645       1\n      4    685       1\n      4    831       1\n      4    354       1\n      4    361       1\n      4    991       1\n      6      5       1\n      6    111       1\n      6    154       1\n      6    457       1\n      6    521       1\n      6    669       1\n      6    674       1\n```\n\n----------------------------------------\n\nTITLE: Installing Scanpy via pip - Python\nDESCRIPTION: This snippet provides a commented-out command to install the Scanpy library, used for single-cell data analysis in Python. The command is intended for use within a Jupyter notebook or interactive shell to ensure Scanpy is available before running scripts that require it. Requires an internet connection and pip package manager. No input/output, serves as a reminder or setup instruction.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/8_scrna_preprocessing.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# !pip install scanpy\n```\n\n----------------------------------------\n\nTITLE: Comment on Regression Support in Manify\nDESCRIPTION: This is a commented-out snippet questioning whether the framework supports regression problems. It contains no executable code and serves as a notation or planning comment about future testing or feature support. There are no dependencies or parameters.\nSOURCE: https://github.com/pchlenski/manify/blob/main/notebooks/56_single_manifold_rf.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Does it work for regression?\n\n```\n\n----------------------------------------\n\nTITLE: Citing Manify Library - BibTeX\nDESCRIPTION: Provides a citation entry for referencing Manify in academic publications using the BibTeX format. Includes bibliographic fields such as title, authors, year, preprint number, and URL. To use, copy the entry into a .bib file and cite it from your LaTeX manuscript. No computation or code execution is involved; this is for documentation and scholarly attribution purposes.\nSOURCE: https://github.com/pchlenski/manify/blob/main/README.md#_snippet_3\n\nLANGUAGE: bibtex\nCODE:\n```\n@misc{chlenski2025manifypythonlibrarylearning,\n      title={Manify: A Python Library for Learning Non-Euclidean Representations}, \n      author={Philippe Chlenski and Kaizhu Du and Dylan Satow and Itsik Pe\\'er},\n      year={2025},\n      eprint={2503.09576},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2503.09576}, \n}\n```"
  }
]
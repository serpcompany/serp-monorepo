[
  {
    "owner": "microsoft",
    "repo": "lightgbm",
    "content": "TITLE: Simple Example: Construct Dataset, Basic Training and Prediction in LightGBM Python\nDESCRIPTION: This script demonstrates constructing a LightGBM Dataset, performing basic training and prediction, evaluating during training, implementing early stopping, and saving the model to a file. It provides foundational usage of the LightGBM Python API for straightforward model training.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/examples/python-guide/README.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport lightgbm as lgb\n\n# Load data\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\n\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create Dataset objects\ntrain_data = lgb.Dataset(X_train, label=y_train)\nvalid_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n\n# Set parameters\nparams = {\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'verbosity': -1\n}\n\n# Train model with early stopping\ngbm = lgb.train(params,\n                train_data,\n                num_boost_round=100,\n                valid_sets=[valid_data],\n                early_stopping_rounds=10)\n\n# Save model to file\ngbm.save_model('model.txt')\n\n# Predict\ny_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n\n```\n\n----------------------------------------\n\nTITLE: Sklearn Interface Example: Data Preparation, Training, and Evaluation in LightGBM Python\nDESCRIPTION: This example shows how to utilize LightGBM's sklearn API to create datasets, perform training and prediction, compute feature importances, define custom evaluation metrics, and optimize parameters with GridSearchCV. It offers a higher-level interface for seamless integration with scikit-learn workflows.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/examples/python-guide/README.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nimport lightgbm as lgb\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\n# Load data\nX, y = load_breast_cancer(return_X_y=True)\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create sklearn dataset\ntrain_data = lgb.Dataset(X_train, label=y_train)\n\n# Set parameters\nparams = {\n    'objective': 'binary',\n    'metric': 'binary_logloss'\n}\n\n# Train with sklearn API\ngbm = lgb.LGBMClassifier()\ngbm.fit(X_train, y_train)\n\n# Predictions\ny_pred = gbm.predict(X_test)\n\n# Feature importances\nimportances = gbm.feature_importances_\n\n# Custom eval metric during training\ndef custom_eval(y_true, y_pred):\n    preds = (y_pred >= 0.5).astype(int)\n    return 'custom_accuracy', accuracy_score(y_true, preds), True\n\n# Hyperparameter tuning with GridSearchCV\nparam_grid = {'num_leaves': [31, 50], 'n_estimators': [50, 100]}\ngrid = GridSearchCV(lgb.LGBMClassifier(), param_grid, scoring='accuracy')\ngrid.fit(X_train, y_train)\n\n```\n\n----------------------------------------\n\nTITLE: Running a Basic LightGBM Model in R\nDESCRIPTION: This R code snippet loads the lightgbm library, prepares a dataset, creates a LightGBM dataset object, and performs cross-validation with specified parameters. It serves as a quick test to verify the package functionality.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/R-package/README.md#_snippet_1\n\nLANGUAGE: R\nCODE:\n```\nlibrary(lightgbm)\ndata(agaricus.train, package='lightgbm')\ntrain <- agaricus.train\ndtrain <- lgb.Dataset(train$data, label = train$label)\nmodel <- lgb.cv(\n    params = list(\n        objective = \"regression\"\n        , metric = \"l2\"\n    ),\n    data = dtrain\n)\n```\n\n----------------------------------------\n\nTITLE: Installing the LightGBM R-package from CRAN\nDESCRIPTION: This code installs the LightGBM R package directly from CRAN using R. It simplifies installation without needing CMake or Visual Studio, suitable for most systems.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/R-package/README.md#_snippet_0\n\nLANGUAGE: R\nCODE:\n```\ninstall.packages(\"lightgbm\", repos = \"https://cran.r-project.org\")\n```\n\n----------------------------------------\n\nTITLE: Training a LightGBM Model\nDESCRIPTION: Initiates the training process for a LightGBM model using the `lgb.train()` function. It requires the parameter dictionary, the training dataset, the number of boosting rounds, and optionally, validation datasets for evaluation.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Python-Intro.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nnum_round = 10\nbst = lgb.train(param, train_data, num_round, valid_sets=[validation_data])\n```\n\n----------------------------------------\n\nTITLE: LightGBM Binary Classification: Training\nDESCRIPTION: This bash command trains a LightGBM model based on the configuration specified in the `train.conf` file.  The `lightgbm` binary must be built and available at the root of this project. The command uses the `config` parameter to specify the configuration file for training.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/examples/binary_classification/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n\"../../lightgbm\" config=train.conf\n```\n\n----------------------------------------\n\nTITLE: Setting Parameters for Training with LightGBM in Python\nDESCRIPTION: This snippet demonstrates how to specify training parameters for LightGBM using the Python API, particularly focusing on primary vs. alias parameter names. It creates a dictionary of parameters, explicitly sets the 'learning_rate' primary parameter (which takes precedence over any aliases like 'shrinkage_rate'), and invokes the 'lgb.train()' function with the parameter dictionary and a training dataset. The example requires the 'lightgbm' package installed and expects 'dtrain' as a LightGBM Dataset object. Inputs are a params dictionary and a train_set; output is a trained LightGBM model. Alias collision behavior is highlighted. No special constraints beyond standard LightGBM install.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parameters.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# use learning rate of 0.07, because 'learning_rate'\n# is the primary parameter name\nlgb.train(\n   params={\n      \"learning_rate\": 0.07,\n      \"shrinkage_rate\": 0.12\n   },\n   train_set=dtrain\n)\n```\n\n----------------------------------------\n\nTITLE: Installing LightGBM Python Package\nDESCRIPTION: Describes the preferred method for installing the LightGBM Python package using pip. This command downloads and installs the library and its dependencies from PyPI.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Python-Intro.rst#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install lightgbm\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with LightGBM Model\nDESCRIPTION: Uses a trained or loaded LightGBM model object (`bst`) to make predictions on new data. The input data is typically a NumPy array or other supported format.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Python-Intro.rst#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# 7 entities, each contains 10 features\nrng = np.random.default_rng()\ndata = rng.uniform(size=(7, 10))\nypred = bst.predict(data)\n```\n\n----------------------------------------\n\nTITLE: Importing LightGBM in Python\nDESCRIPTION: Shows how to import the LightGBM library in Python, typically aliased as 'lgb', to make its functions and classes available for use in a script or session. This is necessary to verify installation and use the package.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Python-Intro.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport lightgbm as lgb\n```\n\n----------------------------------------\n\nTITLE: Dumping a Trained LightGBM Model to JSON\nDESCRIPTION: Dumps a trained LightGBM model object into a JSON string representation. This allows examining the model structure and parameters in a human-readable format.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Python-Intro.rst#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\njson_model = bst.dump_model()\n```\n\n----------------------------------------\n\nTITLE: Advanced Features and Model Management in LightGBM Python\nDESCRIPTION: This script demonstrates constructing datasets, setting feature names, handling categorical features natively, saving and dumping models in JSON format, retrieving feature insights, loading models for prediction, and continuing training from saved models. It also covers parameter adjustments during training, custom objectives, eval functions, callback usage, and model serialization with pickle.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/examples/python-guide/README.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport lightgbm as lgb\nimport pickle\n\n# Load data and set feature names\nX = ...  # load your data\nparams = {'objective': 'binary'}\n\n# Construct Dataset with feature names\nfeature_names = ['feat1', 'feat2', 'feat3']\ntrain_data = lgb.Dataset(X, feature_name=feature_names)\n\n# Handling categorical features\ncategorical_feature_indices = [0, 2]\ntrain_data.set_categorical(categorical_feature_indices, True)\n\n# Train and save model\ngbm = lgb.train(params, train_data)\ngbm.save_model('model.json')\n\n# Dump to JSON\njson_str = gbm.dump_model()\n\n# Get feature names and importances\nfeature_names = gbm.feature_name()\nimportances = gbm.feature_importance()\n\n# Load model for prediction\nloaded_gbm = lgb.Booster(model_file='model.json')\n\n# Continue training\ngbm2 = lgb.train(params, train_data, init_model='model.json')\n\n# Change parameters during training\ngbm.update()\n\n# Custom objective and eval functions\n# Define your own functions and pass during training\n\ndef custom_objective(y_true, y_pred):\n    # custom implementation\n    pass\n\ndef custom_eval(y_true, y_pred):\n    # custom implementation\n    pass\n\n# Callbacks example\n\ndef my_callback(env):\n    pass\n\n# Save and load model with pickle\npickle.dump(gbm, open('gbm.pkl', 'wb'))\nloaded_model = pickle.load(open('gbm.pkl', 'rb'))\n\n```\n\n----------------------------------------\n\nTITLE: Training LightGBM Model with Early Stopping\nDESCRIPTION: Trains a LightGBM model using early stopping, which automatically stops training when the performance on a validation set stops improving for a specified number of rounds. This prevents overfitting and finds the optimal number of boosting iterations.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Python-Intro.rst#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nbst = lgb.train(param, train_data, num_round, valid_sets=valid_sets, callbacks=[lgb.early_stopping(stopping_rounds=5)])\nbst.save_model('model.txt', num_iteration=bst.best_iteration)\n```\n\n----------------------------------------\n\nTITLE: Interaction Constraints Configuration - LightGBM\nDESCRIPTION: This snippet explains how to configure interaction constraints in LightGBM to control which features can appear in the same branch. It provides examples for CLI, Python, and R packages, illustrating the different formats for specifying the constraints. It states that any two features can only appear in the same branch if there exists a constraint containing both features.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parameters.rst#_snippet_7\n\n\n\n----------------------------------------\n\nTITLE: Loading LightGBM Dataset from NumPy Arrays\nDESCRIPTION: Illustrates how to create a LightGBM Dataset from NumPy arrays. This is a common method when data is already loaded into memory. It requires providing both the feature data and the target labels.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Python-Intro.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nrng = np.random.default_rng()\ndata = rng.uniform(size=(500, 10))  # 500 entities, each contains 10 features\nlabel = rng.integers(low=0, high=2, size=(500, ))  # binary target\ntrain_data = lgb.Dataset(data, label=label)\n```\n\n----------------------------------------\n\nTITLE: Initializing a Local Dask Cluster for LightGBM Training in Python\nDESCRIPTION: This snippet demonstrates how to create a local Dask cluster with three worker processes and connect a Dask client to it using the distributed Python package. This is a required setup step for using Dask-based distributed training with LightGBM. Dependencies include the 'distributed' Python library, which is installed with Dask. The cluster is started locally, and client objects are required for subsequent distributed operations. Inputs: number of workers. Outputs: active Dask Cluster and Client instances. Intended for execution in Python scripts or Jupyter Notebooks. This snippet must be run before subsequent LightGBM distributed calls.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parallel-Learning-Guide.rst#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom distributed import Client, LocalCluster\n\ncluster = LocalCluster(n_workers=3)\nclient = Client(cluster)\n```\n\n----------------------------------------\n\nTITLE: Creating LightGBM Dataset Objects - Python\nDESCRIPTION: This snippet converts the pandas DataFrames containing features and target variables into LightGBM `Dataset` objects. It assigns feature names and specifies a categorical feature. The test dataset is created with a reference to the training dataset for consistent feature handling.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/examples/python-guide/notebooks/interactive_plot_example.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nlgb_train = lgb.Dataset(\n    X_train,\n    y_train,\n    feature_name=[f\"f{i + 1}\" for i in range(X_train.shape[-1])],\n    categorical_feature=[21],\n)\nlgb_test = lgb.Dataset(X_test, y_test, reference=lgb_train)\n```\n\n----------------------------------------\n\nTITLE: Setting Monotonicity Constraints - LightGBM\nDESCRIPTION: This snippet explains how to set monotonicity constraints in LightGBM. It shows how to specify whether a feature should be increasing (1), decreasing (-1), or unconstrained (0). It also mentions that you need to specify constraints for all features in order.  Example: mc=-1,0,1 means decreasing for the 1st feature, non-constraint for the 2nd feature and increasing for the 3rd feature.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parameters.rst#_snippet_4\n\n\n\n----------------------------------------\n\nTITLE: Saving a Trained LightGBM Model\nDESCRIPTION: Saves a trained LightGBM model object (`bst`) to a file in a format that can be loaded later. The file extension typically indicates the format (e.g., '.txt').\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Python-Intro.rst#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nbst.save_model('model.txt')\n```\n\n----------------------------------------\n\nTITLE: Specifying Feature Names and Categorical Features in LightGBM Dataset\nDESCRIPTION: Demonstrates how to explicitly provide feature names and indicate which features are categorical when creating a LightGBM Dataset. LightGBM can handle categorical features directly without requiring one-hot encoding, offering performance improvements.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Python-Intro.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntrain_data = lgb.Dataset(data, label=label, feature_name=['c1', 'c2', 'c3'], categorical_feature=['c3'])\n```\n\n----------------------------------------\n\nTITLE: Defining LightGBM Training Parameters - Python\nDESCRIPTION: This snippet defines a Python dictionary containing configuration parameters for the LightGBM training process. It specifies the number of leaves, evaluation metrics to track ('l1', 'l2'), and the verbosity level.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/examples/python-guide/notebooks/interactive_plot_example.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nparams = {\"num_leaves\": 5, \"metric\": [\"l1\", \"l2\"], \"verbose\": -1}\n```\n\n----------------------------------------\n\nTITLE: Initializing LightGBM Dataset in Python\nDESCRIPTION: Shows a basic example of creating a LightGBM `Dataset` object in Python using training features `X_train` and labels `y_train`. This is a prerequisite step before training a model, but accessing certain dataset attributes might still require the dataset to be part of a `Booster`.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/FAQ.rst#_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\ntrain = lightgbm.Dataset(X_train, y_train)\n```\n\n----------------------------------------\n\nTITLE: Performing LightGBM Cross-Validation\nDESCRIPTION: Runs k-fold cross-validation using the `lgb.cv()` function. This evaluates model performance on multiple folds of the training data, providing a more robust estimate of generalization error.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Python-Intro.rst#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nlgb.cv(param, train_data, num_round, nfold=5)\n```\n\n----------------------------------------\n\nTITLE: Creating LightGBM Validation Dataset (Method 2)\nDESCRIPTION: Creates a validation dataset from a file path, explicitly referencing the training dataset using the `reference` parameter. This ensures the validation data is processed using the same feature mappings as the training data.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Python-Intro.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nvalidation_data = lgb.Dataset('validation.svm', reference=train_data)\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM Python Package with MinGW on Windows (sh)\nDESCRIPTION: Installs the LightGBM Python package on Windows using the MinGW-w64 toolchain instead of Visual Studio, executed via `build-python.sh`. Requires dependencies specified in the 'Build with MinGW-w64 on Windows' section.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/python-package/README.rst#_snippet_20\n\nLANGUAGE: sh\nCODE:\n```\nsh ./build-python.sh install --mingw\n```\n\n----------------------------------------\n\nTITLE: Categorical Feature Parameter Description\nDESCRIPTION: The `categorical_feature` parameter specifies categorical features by their column indices or names. It accepts multi-int or string (with `name:` prefix). All values will be cast to `int32`, and negative values are treated as missing. Index starts from 0 and doesn't count the label column. Large values could be memory consuming. Floating-point numbers in categorical features will be rounded towards 0. The output cannot be monotonically constrained with respect to a categorical feature.  Aliases include `cat_feature`, `categorical_column`, `cat_column`, and `categorical_features`.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parameters.rst#_snippet_14\n\nLANGUAGE: text\nCODE:\n```\n``categorical_feature`` :raw-html:`<a id=\"categorical_feature\" title=\"Permalink to this parameter\" href=\"#categorical_feature\">&#x1F517;&#xFE0E;</a>`, default = ``\"\"``, type = multi-int or string, aliases: ``cat_feature``, ``categorical_column``, ``cat_column``, ``categorical_features``\n\n   -  used to specify categorical features\n\n   -  use number for index, e.g. ``categorical_feature=0,1,2`` means column_0, column_1 and column_2 are categorical features\n\n   -  add a prefix ``name:`` for column name, e.g. ``categorical_feature=name:c1,c2,c3`` means c1, c2 and c3 are categorical features\n\n   -  **Note**: all values will be cast to ``int32`` (integer codes will be extracted from pandas categoricals in the Python-package)\n\n   -  **Note**: index starts from ``0`` and it doesn't count the label column when passing type is ``int``\n\n   -  **Note**: all values should be less than ``Int32.MaxValue`` (2147483647)\n\n   -  **Note**: using large values could be memory consuming. Tree decision rule works best when categorical features are presented by consecutive integers starting from zero\n\n   -  **Note**: all negative values will be treated as **missing values**\n\n   -  **Note**: the output cannot be monotonically constrained with respect to a categorical feature\n\n   -  **Note**: floating point numbers in categorical features will be rounded towards 0\n```\n\n----------------------------------------\n\nTITLE: Training a Model with LightGBM Python in Docker (Shell)\nDESCRIPTION: Runs a Python script ('train.py') to train a LightGBM model inside the 'lightgbm-python' Docker container. It first downloads training data, creates the 'train.py' script (using the Python snippet above), and then executes the script within the container via `docker run`. Requires the 'lightgbm-python' image, `curl`, and Docker.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docker/README.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\n# get training data\ncurl -O https://raw.githubusercontent.com/Microsoft/LightGBM/master/examples/binary_classification/binary.train\n```\n\nLANGUAGE: Shell\nCODE:\n```\n# create training script\ncat << EOF > train.py\nimport lightgbm as lgb\nimport numpy as np\nparams = {\n    \"objective\": \"binary\",\n    \"num_trees\": 10\n}\n\nbst = lgb.train(\n    train_set=lgb.Dataset(\"binary.train\"),\n    params=params\n)\nbst.save_model(\"LightGBM-python-model.txt\")\nEOF\n```\n\nLANGUAGE: Shell\nCODE:\n```\n# run training in a container\ndocker run \\\n    --rm \\\n    --volume \"${PWD}\":/opt/training \\\n    --workdir /opt/training \\\n    lightgbm-python \\\n    python train.py\n```\n\n----------------------------------------\n\nTITLE: Setting Multiple Evaluation Metrics for LightGBM\nDESCRIPTION: Demonstrates how to specify multiple evaluation metrics for LightGBM training by assigning a list of metric names to the 'metric' key in the parameter dictionary. LightGBM will report all specified metrics during training.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Python-Intro.rst#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nparam['metric'] = ['auc', 'binary_logloss']\n```\n\n----------------------------------------\n\nTITLE: Training Script for LightGBM Python API (Python)\nDESCRIPTION: A Python script ('train.py') that uses the LightGBM Python API to train a binary classification model. It defines parameters, loads data from 'binary.train', trains the model, and saves it to 'LightGBM-python-model.txt'. Requires the 'lightgbm' and 'numpy' Python packages.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docker/README.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport lightgbm as lgb\nimport numpy as np\nparams = {\n    \"objective\": \"binary\",\n    \"num_trees\": 10\n}\n\nbst = lgb.train(\n    train_set=lgb.Dataset(\"binary.train\"),\n    params=params\n)\nbst.save_model(\"LightGBM-python-model.txt\")\n```\n\n----------------------------------------\n\nTITLE: Defining and Using a Custom Objective Function with LightGBM Dask in Python\nDESCRIPTION: This code illustrates how to implement a custom L2 regression objective function in Python for use with LightGBM's Dask interface. It includes setting up a Dask cluster, creating random data, and passing the custom objective to the DaskLGBMRegressor.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parallel-Learning-Guide.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport dask.array as da\nimport lightgbm as lgb\nimport numpy as np\nfrom distributed import Client, LocalCluster\n\ncluster = LocalCluster(n_workers=2)\nclient = Client(cluster)\n\nX = da.random.random((1000, 10), (500, 10))\n y = da.random.random((1000,), (500,))\n\ndef custom_l2_obj(y_true, y_pred):\n    grad = y_pred - y_true\n    hess = np.ones(len(y_true))\n    return grad, hess\n\n dask_model = lgb.DaskLGBMRegressor(\n     objective=custom_l2_obj\n )\n dask_model.fit(X, y)\n```\n\n----------------------------------------\n\nTITLE: Setting Basic LightGBM Parameters\nDESCRIPTION: Shows how to define basic boosting parameters for LightGBM training using a Python dictionary. Key parameters like `num_leaves` and `objective` are specified, and a single evaluation `metric` is added.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Python-Intro.rst#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nparam = {'num_leaves': 31, 'objective': 'binary'}\nparam['metric'] = 'auc'\n```\n\n----------------------------------------\n\nTITLE: Label Column Parameter Description\nDESCRIPTION: The `label_column` parameter specifies the column containing the labels for the dataset. It accepts either an integer (index) or a string (with `name:` prefix). If omitted, the first column is used as the label. This parameter is an int or string and aliases to `label`.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parameters.rst#_snippet_10\n\nLANGUAGE: text\nCODE:\n```\n``label_column`` :raw-html:`<a id=\"label_column\" title=\"Permalink to this parameter\" href=\"#label_column\">&#x1F517;&#xFE0E;</a>`, default = ``\"\"``, type = int or string, aliases: ``label``\n\n   -  used to specify the label column\n\n   -  use number for index, e.g. ``label=0`` means column_0 is the label\n\n   -  add a prefix ``name:`` for column name, e.g. ``label=name:is_click``\n\n   -  if omitted, the first column in the training data is used as the label\n\n   -  **Note**: works only in case of loading data directly from text file\n```\n\n----------------------------------------\n\nTITLE: Training a Model with LightGBM CLI in Docker (Shell)\nDESCRIPTION: Runs a LightGBM training task using the pre-built 'lightgbm-cli' Docker image. It first creates a configuration file 'train.conf', downloads training data 'binary.train', and then executes training inside a container, mounting the current directory and saving the model to 'LightGBM-CLI-model.txt'. Requires the 'lightgbm-cli' Docker image, `curl`, and Docker.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docker/README.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\n# configure the CLI\ncat << EOF > train.conf\ntask = train\nobjective = binary\ndata = binary.train\nnum_trees = 10\noutput_model = LightGBM-CLI-model.txt\nEOF\n```\n\nLANGUAGE: Shell\nCODE:\n```\n# get training data\ncurl -O https://raw.githubusercontent.com/Microsoft/LightGBM/master/examples/binary_classification/binary.train\n```\n\nLANGUAGE: Shell\nCODE:\n```\n# train, and save model to a text file\ndocker run \\\n  --rm \\\n  --volume \"${PWD}\":/opt/training \\\n  --workdir /opt/training \\\n  lightgbm-cli \\\n  config=train.conf\n```\n\n----------------------------------------\n\nTITLE: Loading LightGBM Dataset from File\nDESCRIPTION: Demonstrates how to create a LightGBM Dataset object directly from a file path. LightGBM supports various file formats including LibSVM, TSV, CSV, and its own binary format. Using the binary format (`.bin`) is recommended for faster loading.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Python-Intro.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrain_data = lgb.Dataset('train.svm.bin')\n```\n\n----------------------------------------\n\nTITLE: Training LightGBM Model - Python\nDESCRIPTION: This snippet initiates the training of the LightGBM model using the `lgb.train` function. It passes the defined parameters, training data, number of boosting rounds, validation sets, and callbacks to record evaluation results and log progress.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/examples/python-guide/notebooks/interactive_plot_example.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nevals_result = {}  # to record eval results for plotting\ngbm = lgb.train(\n    params,\n    lgb_train,\n    num_boost_round=100,\n    valid_sets=[lgb_train, lgb_test],\n    callbacks=[lgb.log_evaluation(10), lgb.record_evaluation(evals_result)],\n)\n```\n\n----------------------------------------\n\nTITLE: Logistic Regression Example: Using Objective and Speed Comparison in LightGBM Python\nDESCRIPTION: This script compares different objectives such as 'xentropy' and 'binary' for logistic regression, handling labels appropriately, and benchmarks the training speed of these objectives. It provides insights into choosing objectives based on label types and performance considerations.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/examples/python-guide/README.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nimport lightgbm as lgb\nfrom sklearn.datasets import load_breast_cancer\nimport time\n\n# Load data\nX, y = load_breast_cancer(return_X_y=True)\n\n# Prepare dataset\ntrain_data = lgb.Dataset(X, label=y)\n\n# Define objectives\nparams_xentropy = {'objective': 'xentropy', 'metric': 'binary_logloss'}\nparams_binary = {'objective': 'binary', 'metric': 'binary_logloss'}\n\n# Train with xentropy\nstart_time = time.time()\ngbm_xentropy = lgb.train(params_xentropy, train_data, num_boost_round=100)\ntime_xentropy = time.time() - start_time\n\n# Train with binary\nstart_time = time.time()\ngbm_binary = lgb.train(params_binary, train_data, num_boost_round=100)\ntime_binary = time.time() - start_time\n\n# Compare speeds\nprint(f\"xentropy training time: {time_xentropy}\")\nprint(f\"binary training time: {time_binary}\")\n\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM (GPU) - Linux - GCC\nDESCRIPTION: Builds the GPU version of LightGBM on Linux using CMake and GCC.  Clones the LightGBM repository, configures the build with GPU enabled, and then builds the project. Assumes CMake, GCC, OpenCL, and Boost are installed.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_26\n\nLANGUAGE: sh\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\ncmake -B build -S . -DUSE_GPU=ON\n# if you have installed NVIDIA CUDA to a customized location, you should specify paths to OpenCL headers and library like the following:\n# cmake -B build -S . -DUSE_GPU=ON -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/\ncmake --build build -j4\n```\n\n----------------------------------------\n\nTITLE: Plotting Feature Importance with LightGBM - Python\nDESCRIPTION: This snippet defines a function `render_plot_importance` utilizing `lgb.plot_importance` to visualize feature importance from the trained model. It includes parameters to control the importance type, maximum features displayed, and filtering. The function is then called, either directly with default parameters or interactively using `ipywidgets.interact` allowing dynamic control over plot options.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/examples/python-guide/notebooks/interactive_plot_example.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef render_plot_importance(importance_type, max_features=10, ignore_zero=True, precision=3):\n    lgb.plot_importance(\n        gbm,\n        importance_type=importance_type,\n        max_num_features=max_features,\n        ignore_zero=ignore_zero,\n        figsize=(12, 8),\n        precision=precision,\n    )\n    plt.show()\n\nif INTERACTIVE:\n    # create widget for interactive feature importance plot\n    interact(\n        render_plot_importance,\n        importance_type=[\"split\", \"gain\"],\n        max_features=(1, X_train.shape[-1]),\n        precision=(0, 10),\n    )\nelse:\n    render_plot_importance(importance_type=\"split\")\n```\n\n----------------------------------------\n\nTITLE: Predict Contribution Parameter Description\nDESCRIPTION: The `predict_contrib` parameter, when set to `true`, enables the estimation of SHAP values for each feature, representing its contribution to each prediction. It produces `#features + 1` values, the last one being the expected model output. This feature is not implemented for linear trees. The user may install the `shap` package for additional functionalities.  It is a boolean and aliases to `is_predict_contrib` and `contrib`.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parameters.rst#_snippet_23\n\nLANGUAGE: text\nCODE:\n```\n-  ``predict_contrib`` :raw-html:`<a id=\"predict_contrib\" title=\"Permalink to this parameter\" href=\"#predict_contrib\">&#x1F517;&#xFE0E;</a>`, default = ``false``, type = bool, aliases: ``is_predict_contrib``, ``contrib``\n\n   -  used only in ``prediction`` task\n\n   -  set this to ``true`` to estimate `SHAP values <https://arxiv.org/abs/1706.06060>`__, which represent how each feature contributes to each prediction\n\n   -  produces ``#features + 1`` values where the last value is the expected value of the model output over the training data\n\n   -  **Note**: if you want to get more explanation for your model's predictions using SHAP values like SHAP interaction values, you can install `shap package <https://github.com/shap>`__\n\n   -  **Note**: unlike the shap package, with ``predict_contrib`` we return a matrix with an extra column, where the last column is the expected value\n\n   -  **Note**: this feature is not implemented for linear trees\n```\n\n----------------------------------------\n\nTITLE: LightGBM Training on CPU for Regression Task\nDESCRIPTION: Demonstrates training the Higgs dataset for regression using the CPU. It sets `device=cpu` to perform the training on the CPU. It is used for comparing with GPU performance. The `objective=regression_l2` and `metric=l2` parameters are included.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/GPU-Tutorial.rst#_snippet_14\n\nLANGUAGE: Bash\nCODE:\n```\n./lightgbm config=lightgbm_gpu.conf data=higgs.train objective=regression_l2 metric=l2 device=cpu\n```\n\n----------------------------------------\n\nTITLE: Making Predictions from Best Iteration with Early Stopping\nDESCRIPTION: Makes predictions using the specific model iteration identified as the best during training with early stopping. This ensures predictions are based on the model state that achieved the best performance on the validation set.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Python-Intro.rst#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nypred = bst.predict(data, num_iteration=bst.best_iteration)\n```\n\n----------------------------------------\n\nTITLE: Building GPU-enabled LightGBM with Rscript\nDESCRIPTION: This shell command invokes the Rscript build script to compile LightGBM with GPU support by passing the --use-gpu flag. Additional configuration can specify OpenCL and Boost library directories.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/R-package/README.md#_snippet_4\n\nLANGUAGE: Shell Script\nCODE:\n```\nRscript build_r.R --use-gpu\n```\n\n----------------------------------------\n\nTITLE: Installing LightGBM with pandas Support\nDESCRIPTION: This command installs the LightGBM package along with dependencies required for using pandas. pandas is a library providing data structures and data analysis tools. Using LightGBM with pandas offers better integration with the common DataFrame format.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/python-package/README.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npip install 'lightgbm[pandas]'\n```\n\n----------------------------------------\n\nTITLE: Cloning and Building LightGBM with GPU Support\nDESCRIPTION: This block clones the LightGBM repository from GitHub, navigates into the repository, and then builds LightGBM with GPU support using CMake. The `-DUSE_GPU=1` flag enables GPU support. The build process utilizes multiple cores (`-j$(nproc)`) for faster compilation.  Optionally,  if CUDA is installed in a non-standard location,  OpenCL paths can be specified using  `-DOpenCL_LIBRARY` and `-DOpenCL_INCLUDE_DIR`.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/GPU-Tutorial.rst#_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\ncmake -B build -S . -DUSE_GPU=1\n Â   # if you have installed NVIDIA CUDA to a customized location, you should specify paths to OpenCL headers and library like the following:\n    # cmake -B build -S . -DUSE_GPU=1 -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/\ncmake --build build -j$(nproc)\n```\n\n----------------------------------------\n\nTITLE: Restarting All Dask Workers in a Cluster with Python\nDESCRIPTION: This snippet resets all Dask worker processes in your cluster using the client's restart() method, ensuring a clean memory state before performing heavy data loading or distributed LightGBM training. The only prerequisite is an active Dask client. Input: none. Output: all workers and the client itself are restarted, releasing remote/global state. This reduces the risk of running out of memory from previously loaded objects, which is especially important for large-scale data loading and model training. This function is synchronous and may take a few seconds to complete.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parallel-Learning-Guide.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclient.restart()\n```\n\n----------------------------------------\n\nTITLE: LightGBM Binary Classification: Prediction\nDESCRIPTION: This bash command performs prediction using a previously trained LightGBM model, utilizing the configuration defined in `predict.conf`.  This command assumes that the user has successfully trained the model beforehand. The `lightgbm` binary must be available.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/examples/binary_classification/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n\"../../lightgbm\" config=predict.conf\n```\n\n----------------------------------------\n\nTITLE: Converting Dask LightGBM Estimator to scikit-learn Model and Saving\nDESCRIPTION: Details how to convert a Dask estimator to an sklearn-compatible estimator using `to_local()`, then save the resulting model with joblib for inference without Dask dependencies. Useful for deployment or scoring in environments without Dask.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parallel-Learning-Guide.rst#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport dask.array as da\nimport joblib\nimport lightgbm as lgb\nfrom distributed import Client, LocalCluster\n\ncluster = LocalCluster(n_workers=2)\nclient = Client(cluster)\n\nX = da.random.random((1000, 10), (500, 10))\n y = da.random.random((1000,), (500,))\n\ndask_model = lgb.DaskLGBMRegressor()\n dask_model.fit(X, y)\n\n# convert to sklearn equivalent\n sklearn_model = dask_model.to_local()\n\nprint(type(sklearn_model))\n#> lightgbm.sklearn.LGBMRegressor\n\njoblib.dump(sklearn_model, \"sklearn-model.joblib\")\n```\n\n----------------------------------------\n\nTITLE: Training LightGBM XE_NDCG Ranking Model (Bash)\nDESCRIPTION: Executes the LightGBM training process using a specified configuration file (`train.conf`). Requires the `lightgbm` binary to be built and accessible at the relative path `../../lightgbm`.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/examples/xendcg/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n\"../../lightgbm\" config=train.conf\n```\n\n----------------------------------------\n\nTITLE: Plotting and Visualization of LightGBM Models in Python\nDESCRIPTION: This script covers constructing datasets, training models, and visualizing various aspects such as evaluation metrics, feature importances, split histograms, and individual decision trees using Graphviz. It facilitates insight into model behavior and feature contributions.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/examples/python-guide/README.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\n\n# Load data\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create Dataset\ntrain_data = lgb.Dataset(X_train, label=y_train)\n\n# Set parameters\nparams = {'objective': 'binary', 'metric': 'binary_logloss'}\n\n# Train model\ngbm = lgb.train(params, train_data, num_boost_round=100)\n\n# Record eval results\nevals_result = {}\ngbm = lgb.train(params, train_data, num_boost_round=100, evals=[(train_data, 'train')], evals_result=evals_result)\n\n# Plot evaluation metric over training iterations\nplt.plot(evals_result['train']['binary_logloss'])\nplt.xlabel('Iteration')\nplt.ylabel('Binary Log Loss')\nplt.title('Training Evaluation Metric')\nplt.show()\n\n# Plot feature importances\nlgb.plot_importance(gbm)\nplt.show()\n\n# Plot split value histogram\nlgb.plot_split_value_histogram(gbm, feature='feature_name')\nplt.show()\n\n# Plot a specific tree\nlgb.plot_tree(gbm, tree_index=0)\nplt.show()\n\n# Plot tree with Graphviz\nfrom sklearn.tree import export_graphviz\nimport graphviz\n\ngraphviz.Source(export_graphviz(gbm, tree_index=0))\n\n```\n\n----------------------------------------\n\nTITLE: Weight Column Parameter Description\nDESCRIPTION: The `weight_column` parameter specifies the column containing the weights for each data instance. It can be an integer (index) or a string (with `name:` prefix). Index starts from 0, and doesn't count the label column. Weights should be non-negative. This parameter is an int or string and aliases to `weight`.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parameters.rst#_snippet_11\n\nLANGUAGE: text\nCODE:\n```\n``weight_column`` :raw-html:`<a id=\"weight_column\" title=\"Permalink to this parameter\" href=\"#weight_column\">&#x1F517;&#xFE0E;</a>`, default = ``\"\"``, type = int or string, aliases: ``weight``\n\n   -  used to specify the weight column\n\n   -  use number for index, e.g. ``weight=0`` means column_0 is the weight\n\n   -  add a prefix ``name:`` for column name, e.g. ``weight=name:weight``\n\n   -  **Note**: works only in case of loading data directly from text file\n\n   -  **Note**: index starts from ``0`` and it doesn't count the label column when passing type is ``int``, e.g. when label is column_0, and weight is column_1, the correct parameter is ``weight=0``\n\n   -  **Note**: weights should be non-negative\n```\n\n----------------------------------------\n\nTITLE: Run Distributed LightGBM with MPI in Command Line\nDESCRIPTION: Example commands for launching distributed training with MPI, specifying machine list files and config, for Windows and Linux environments. All machines must share the same working directory.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parallel-Learning-Guide.rst#_snippet_15\n\nLANGUAGE: console\nCODE:\n```\nmpiexec.exe /machinefile mlist.txt lightgbm.exe config=your_config_file\n```\n\nLANGUAGE: console\nCODE:\n```\nmpiexec --machinefile mlist.txt ./lightgbm config=your_config_file\n```\n\n----------------------------------------\n\nTITLE: Running LightGBM with Command Line Parameter Override - Shell\nDESCRIPTION: This snippet provides a sample command to run LightGBM with a specific configuration file and to override individual parameters directly from the command line. Dependencies include a valid LightGBM installation and a configuration file named 'train.conf'. The example sets 'num_trees=10', which will override any conflicting setting within the config. The expected output is a LightGBM job that trains a model using the specified parameters. The command line values always take priority over the configuration file values.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Quick-Start.rst#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nlightgbm config=train.conf num_trees=10\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM (No OpenMP) - Linux - Clang\nDESCRIPTION: Builds LightGBM on Linux without OpenMP using CMake and Clang.  It clones the repo, sets the CXX and CC environment variables to use Clang, then uses CMake to configure and build.  The `-DUSE_OPENMP=OFF` flag disables OpenMP. Replace \"14\" with the appropriate clang version. Assumes CMake and Clang are installed.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_13\n\nLANGUAGE: sh\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\nexport CXX=clang++-14 CC=clang-14  # replace \"14\" with version of Clang installed on your machine\ncmake -B build -S . -DUSE_OPENMP=OFF\ncmake --build build -j4\n```\n\n----------------------------------------\n\nTITLE: Passing Multi-Value Parameters as Python Lists in LightGBM\nDESCRIPTION: This code snippet shows how to pass multi-value parameters, such as 'monotone_constraints', to LightGBM using a Python list. The dictionary 'params' contains the key 'monotone_constraints' mapped to a list of integers. This list structure ensures that the parameter is correctly interpreted when provided to LightGBM's training function. Applicable when specifying constraints or any parameter that expects a sequence of values. No dependencies beyond LightGBM and Python built-in types.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parameters.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nparams = {\n   \"monotone_constraints\": [-1, 0, 1]\n}\n```\n\n----------------------------------------\n\nTITLE: Training Script for LightGBM R API (R)\nDESCRIPTION: An R script ('train.R') that utilizes the LightGBM R package to train a binary classification model. It defines parameters, loads data using `lgb.Dataset` from 'binary.train', trains the model with `lgb.train`, and saves it using `lgb.save` to 'LightGBM-R-model.txt'. Requires the 'lightgbm' R package.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docker/README.md#_snippet_7\n\nLANGUAGE: R\nCODE:\n```\nlibrary(lightgbm)\nparams <- list(\n    objective = \"binary\"\n    , num_trees = 10L\n)\n\nbst <- lgb.train(\n    data = lgb.Dataset(\"binary.train\"),\n    params = params\n)\nlgb.save(bst, \"LightGBM-R-model.txt\")\n```\n\n----------------------------------------\n\nTITLE: Saving LightGBM Dask Models via Pickling in Python\nDESCRIPTION: Shows how to save a trained Dask LightGBM model using pickle-compatible libraries such as cloudpickle or joblib, enabling later model restoration. It emphasizes that explicitly set Dask clients are not saved within the model object.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parallel-Learning-Guide.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport dask.array as da\nimport pickle\nimport lightgbm as lgb\nfrom distributed import Client, LocalCluster\n\ncluster = LocalCluster(n_workers=2)\nclient = Client(cluster)\n\nX = da.random.random((1000, 10), (500, 10))\n y = da.random.random((1000,), (500,))\n\ndask_model = lgb.DaskLGBMRegressor()\n dask_model.fit(X, y)\n\n with open(\"dask-model.pkl\", \"wb\") as f:\n     pickle.dump(dask_model, f)\n```\n\n----------------------------------------\n\nTITLE: Saving LightGBM Dataset to Binary File\nDESCRIPTION: Shows how to save a LightGBM Dataset object to a binary file (`.bin`). Saving the dataset in this format allows for much faster loading in subsequent sessions compared to loading from text files.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Python-Intro.rst#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntrain_data = lgb.Dataset('train.svm.txt')\ntrain_data.save_binary('train.bin')\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries and Setting Up Interactivity - Python\nDESCRIPTION: This snippet imports necessary Python libraries for data handling (`pandas`), plotting (`matplotlib`), path manipulation (`pathlib`), and the `lightgbm` library itself. It also includes error handling to conditionally enable interactive plotting features (`ipywidgets`) if available.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/examples/python-guide/notebooks/interactive_plot_example.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport lightgbm as lgb\n\n%matplotlib inline\n\ntry:\n    # To enable interactive mode you should install ipywidgets\n    # https://github.com/jupyter-widgets/ipywidgets\n    from ipywidgets import SelectMultiple, interact\n\n    INTERACTIVE = True\nexcept ImportError:\n    INTERACTIVE = False\n```\n\n----------------------------------------\n\nTITLE: Setting Weights During LightGBM Dataset Construction\nDESCRIPTION: Shows how to assign sample weights when creating a LightGBM Dataset from arrays. Sample weights influence the contribution of each data point during training.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Python-Intro.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nrng = np.random.default_rng()\nw = rng.uniform(size=(500, ))\ntrain_data = lgb.Dataset(data, label=label, weight=w)\n```\n\n----------------------------------------\n\nTITLE: Preparing and Running Distributed LightGBM via CLI with Socket or MPI\nDESCRIPTION: Provides instructions for setting up distributed training with LightGBM via command-line, including preparing machine lists for socket or MPI versions and editing configuration files. Includes commands to launch training on all nodes, with specific considerations for each environment.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parallel-Learning-Guide.rst#_snippet_13\n\n\n\n----------------------------------------\n\nTITLE: Diagnosing 'Cannot ... before construct dataset' Error (Console)\nDESCRIPTION: Illustrates the error message 'Cannot get/set label/weight/init_score/group/num_data/num_feature before construct dataset' in the LightGBM Python package. This typically occurs when attempting to access or modify dataset properties before the `Dataset` object has been fully incorporated into a `Booster`.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/FAQ.rst#_snippet_12\n\nLANGUAGE: Console\nCODE:\n```\nCannot get/set label/weight/init_score/group/num_data/num_feature before construct dataset\n```\n\n----------------------------------------\n\nTITLE: Building Java Wrapper of LightGBM using Clang on Linux (shell)\nDESCRIPTION: Instructions for building LightGBM's Java wrapper on Linux using Clang and OpenMP. Prerequisites include CMake, Clang with version set via environment variables, OpenMP, SWIG, and Java with JAVA_HOME. The snippet clones the repo, sets compilers, enables SWIG wrapping in CMake, and compiles using parallel jobs. Outputs are stored in the LightGBM directory.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_33\n\nLANGUAGE: shell\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\nexport CXX=clang++-14 CC=clang-14  # replace \"14\" with version of Clang installed on your machine\ncmake -B build -S . -DUSE_SWIG=ON\ncmake --build build -j4\n```\n\n----------------------------------------\n\nTITLE: Plotting Split Value Histograms with LightGBM - Python\nDESCRIPTION: This snippet defines the `render_histogram` function to plot the histogram of split values for a specific feature in the trained LightGBM model using `lgb.plot_split_value_histogram`. It then shows how this function is used, either directly for a predefined feature or interactively via `ipywidgets.interact` to select features dynamically.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/examples/python-guide/notebooks/interactive_plot_example.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef render_histogram(feature):\n    lgb.plot_split_value_histogram(gbm, feature=feature, bins=\"auto\", figsize=(10, 5))\n    plt.show()\n\nif INTERACTIVE:\n    # create widget for interactive split value histogram\n    interact(render_histogram, feature=gbm.feature_name())\nelse:\n    render_histogram(feature=\"f26\")\n```\n\n----------------------------------------\n\nTITLE: Running Interactive Python Session in Docker (Shell)\nDESCRIPTION: Starts an interactive Python session within the 'lightgbm-python' Docker container. It mounts the current directory to '/opt/training' inside the container. Requires the 'lightgbm-python' image and Docker.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docker/README.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\ndocker run \\\n    --rm \\\n    --volume \"${PWD}\":/opt/training \\\n    --workdir /opt/training \\\n    -it lightgbm-python \\\n    python\n```\n\n----------------------------------------\n\nTITLE: Installing LightGBM with Plotting Capabilities\nDESCRIPTION: This command installs the LightGBM package and the required dependencies for using lightgbm.plotting. This module enables visualization tools for LightGBM models, such as feature importance plots and tree visualizations.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/python-package/README.rst#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npip install 'lightgbm[plotting]'\n```\n\n----------------------------------------\n\nTITLE: Plotting Training Metrics with LightGBM - Python\nDESCRIPTION: This snippet defines a function to plot training metrics using `lgb.plot_metric` based on the recorded evaluation results. It then demonstrates how to invoke this function, either directly for a single metric or interactively using `ipywidgets.interact` if the interactive mode is enabled, allowing selection from available metrics.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/examples/python-guide/notebooks/interactive_plot_example.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef render_metric(metric_name):\n    lgb.plot_metric(evals_result, metric=metric_name, figsize=(10, 5))\n    plt.show()\n\nif INTERACTIVE:\n    # create widget to switch between metrics\n    interact(render_metric, metric_name=params[\"metric\"])\nelse:\n    render_metric(params[\"metric\"][0])\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM (GPU) - Linux - Clang\nDESCRIPTION: Builds the GPU version of LightGBM on Linux using CMake and Clang.  Sets the CXX and CC environment variables to use Clang. Clones the repo, configures the build with GPU enabled, and then builds the project. Assumes CMake, Clang, OpenMP, OpenCL and Boost are installed.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_27\n\nLANGUAGE: sh\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\nexport CXX=clang++-14 CC=clang-14  # replace \"14\" with version of Clang installed on your machine\ncmake -B build -S . -DUSE_GPU=ON\n# if you have installed NVIDIA CUDA to a customized location, you should specify paths to OpenCL headers and library like the following:\n# cmake -B build -S . -DUSE_GPU=ON -DOpenCL_LIBRARY=/usr/local/cuda/lib64/libOpenCL.so -DOpenCL_INCLUDE_DIR=/usr/local/cuda/include/\ncmake --build build -j4\n```\n\n----------------------------------------\n\nTITLE: Running LightGBM R Package Valgrind Checks in Docker - Shell\nDESCRIPTION: Executes a sequence of shell commands within a Docker container to perform Valgrind memory checks on the LightGBM R package. This includes setting up the environment, installing R dependencies, building the CRAN package, installing it with tests, and running the tests under `valgrind` with detailed memory leak and origin tracking enabled. Output is logged to `out.log` and displayed.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/R-package/README.md#_snippet_14\n\nLANGUAGE: Shell\nCODE:\n```\ndocker run \\\n    --rm \\\n    -v $(pwd):/opt/LightGBM \\\n    -w /opt/LightGBM \\\n    -it \\\n        wch1/r-debug\n\nRDscriptvalgrind -e \"install.packages(c('R6', 'data.table', 'jsonlite', 'knitr', 'markdown', 'Matrix', 'RhpcBLASctl', 'testthat'), repos = 'https://cran.rstudio.com', Ncpus = parallel::detectCores())\"\n\nsh build-cran-package.sh \\\n    --r-executable=RDvalgrind\n\nRDvalgrind CMD INSTALL \\\n    --preclean \\\n    --install-tests \\\n        lightgbm_*.tar.gz\n\ncd R-package/tests\n\nRDvalgrind \\\n    --no-readline \\\n    --vanilla \\\n    -d \"valgrind --tool=memcheck --leak-check=full --track-origins=yes\" \\\n        -f testthat.R \\\n2>&1 \\\n| tee out.log \\\n| cat\n```\n\n----------------------------------------\n\nTITLE: Constructing Dataset from Multiple HDF5 Files in LightGBM Python\nDESCRIPTION: This example demonstrates creating a LightGBM Dataset from multiple HDF5 files, enabling the construction of large datasets without loading all data into memory at once. It is useful for handling big data in a memory-efficient manner.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/examples/python-guide/README.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nimport lightgbm as lgb\nimport h5py\n\n# List of HDF5 files\nfile_list = ['data1.h5', 'data2.h5', 'data3.h5']\n\n# Function to load data incrementally\ndef load_data_from_hdf5(files):\n    data_list = []\n    label_list = []\n    for filename in files:\n        with h5py.File(filename, 'r') as f:\n            data = f['features'][:]\n            labels = f['labels'][:]\n            data_list.append(data)\n            label_list.append(labels)\n    # Concatenate data if needed\n    X = np.concatenate(data_list, axis=0)\n    y = np.concatenate(label_list, axis=0)\n    return X, y\n\nX, y = load_data_from_hdf5(file_list)\n# Create Dataset\ndataset = lgb.Dataset(X, label=y)\n\n```\n\n----------------------------------------\n\nTITLE: Creating LightGBM Validation Dataset (Method 1)\nDESCRIPTION: Creates a validation dataset by referencing an existing training dataset. This method assumes the validation data file ('validation.svm') is aligned or compatible with the training data format.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Python-Intro.rst#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nvalidation_data = train_data.create_valid('validation.svm')\n```\n\n----------------------------------------\n\nTITLE: Running LightGBM Training on CPU\nDESCRIPTION: This command runs LightGBM training on the Higgs dataset using the `lightgbm_gpu.conf` file, but with the `device=cpu` parameter. It is used for comparison purposes to compare performance with GPU training.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/GPU-Tutorial.rst#_snippet_10\n\nLANGUAGE: Bash\nCODE:\n```\n./lightgbm config=lightgbm_gpu.conf data=higgs.train valid=higgs.test objective=binary metric=auc device=cpu\n```\n\n----------------------------------------\n\nTITLE: Installing LightGBM with scikit-learn Support\nDESCRIPTION: This command installs the LightGBM package along with dependencies for the scikit-learn API. Using scikit-learn allows users to seamlessly integrate LightGBM models into scikit-learn workflows and pipelines. This installs dependencies for lightgbm.sklearn.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/python-package/README.rst#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\npip install 'lightgbm[scikit-learn]'\n```\n\n----------------------------------------\n\nTITLE: Installing LightGBM Precompiled Binaries in R\nDESCRIPTION: This R snippet installs the LightGBM package using precompiled binaries available on CRAN for Mac and Windows; it avoids compilation to facilitate faster installation. It requires an internet connection to CRAN and is not applicable to Linux environments as no binaries are provided for Linux. The code explicitly sets the package type to \"both\" to allow CRAN to select the appropriate binary or source.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/R-package/README.md#_snippet_5\n\nLANGUAGE: R\nCODE:\n```\ninstall.packages(\n    \"lightgbm\"\n    , type = \"both\"\n    , repos = \"https://cran.r-project.org\"\n)\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM Python Package with MPI Support (sh)\nDESCRIPTION: Installs the LightGBM Python package with MPI support enabled using the `build-python.sh` script. Requires dependencies outlined in the 'Build MPI Version' section.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/python-package/README.rst#_snippet_16\n\nLANGUAGE: sh\nCODE:\n```\nsh ./build-python.sh install --mpi\n```\n\n----------------------------------------\n\nTITLE: Specifying a List of Addresses and Ports for LightGBM Dask Worker Networking in Python\nDESCRIPTION: This snippet demonstrates how to configure LightGBM's distributed network by explicitly specifying which worker hosts and ports should be used for inter-worker communication via the 'machines' parameter. The 'machines' string is a comma-delimited list of host:port entries, tailored to match the Dask workers' deployment. Used in cases where network firewalls restrict random port usage. Dependencies: 'lightgbm' Python package. Inputs: machine addresses and ports as a string. Output: a DaskLGBMRegressor estimator with controlled networking. Training will fail if ports or machine assignments are misconfigured. This is required for strict network security environments.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parallel-Learning-Guide.rst#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport lightgbm as lgb\n\nmachines = \"10.0.1.0:12401,10.0.2.0:12402,10.0.3.0:15000\"\ndask_model = lgb.DaskLGBMRegressor(machines=machines)\n```\n\n----------------------------------------\n\nTITLE: Run Distributed LightGBM with Socket Version in Command Line\nDESCRIPTION: Shows example command to start distributed training across multiple machines via socket-based communication by specifying config file and port, assuming firewall rules are properly set for communication.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parallel-Learning-Guide.rst#_snippet_14\n\nLANGUAGE: console\nCODE:\n```\nlightgbm.exe config=your_config_file\n```\n\n----------------------------------------\n\nTITLE: Loading LightGBM Dataset from SciPy Sparse Matrix\nDESCRIPTION: Shows how to create a LightGBM Dataset from a SciPy sparse matrix, specifically the csr_matrix format. This is useful for handling datasets with a large number of features where most values are zero, conserving memory.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Python-Intro.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport scipy\ncsr = scipy.sparse.csr_matrix((dat, (row, col)))\ntrain_data = lgb.Dataset(csr)\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM Python Package with CUDA Support (sh)\nDESCRIPTION: Installs the LightGBM Python package specifically with CUDA GPU support enabled via the `build-python.sh` script. Requires dependencies outlined in the 'Build CUDA Version' section.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/python-package/README.rst#_snippet_19\n\nLANGUAGE: sh\nCODE:\n```\nsh ./build-python.sh install --cuda\n```\n\n----------------------------------------\n\nTITLE: Configuring LightGBM Parameters for Data Parallel Training\nDESCRIPTION: Specifies the configuration parameters used for the LightGBM distributed training experiment. Key settings include a learning rate of 0.1, 255 leaves per tree, 100 trees in total, 16 threads per process, and the 'data' parallel tree learner, chosen because the dataset is large in records but small in features. Other parameters used default values.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Experiments.rst#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nlearning_rate = 0.1\nnum_leaves = 255\nnum_trees = 100\nnum_thread = 16\ntree_learner = data\n```\n\n----------------------------------------\n\nTITLE: Predict Raw Score Parameter Description\nDESCRIPTION: The `predict_raw_score` parameter determines whether the prediction task outputs raw scores or transformed scores.  Setting it to `true` will output only the raw scores. It is a boolean and aliases to `is_predict_raw_score`, `predict_rawscore`, and `raw_score`.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parameters.rst#_snippet_21\n\nLANGUAGE: text\nCODE:\n```\n-  ``predict_raw_score`` :raw-html:`<a id=\"predict_raw_score\" title=\"Permalink to this parameter\" href=\"#predict_raw_score\">&#x1F517;&#xFE0E;</a>`, default = ``false``, type = bool, aliases: ``is_predict_raw_score``, ``predict_rawscore``, ``raw_score``\n\n   -  used only in ``prediction`` task\n\n   -  set this to ``true`` to predict only the raw scores\n\n   -  set this to ``false`` to predict transformed scores\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM R Image with Specific R Version (Shell)\nDESCRIPTION: Builds a LightGBM R Docker image targeting a specific R version (e.g., 3.5) by passing a build argument 'R_VERSION' to the `docker build` command. Requires Docker and the 'dockerfile-r'.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docker/README.md#_snippet_11\n\nLANGUAGE: Shell\nCODE:\n```\ndocker build \\\n    -t lightgbm-r-35 \\\n    -f dockerfile-r \\\n    --build-arg R_VERSION=3.5 \\\n    .\n```\n\n----------------------------------------\n\nTITLE: Initializing LightGBM Dask Estimator with Custom Port in Python\nDESCRIPTION: This snippet demonstrates how to initialize a LightGBM DaskLGBMRegressor with a specified local listening port for distributed training over Dask. It requires setting firewall rules and port accessibility on worker hosts to enable communication.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parallel-Learning-Guide.rst#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport lightgbm as lgb\n\ndask_model = lgb.DaskLGBMRegressor(local_listen_port=12400)\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM R Docker Image (Shell)\nDESCRIPTION: Builds a Docker image tagged 'lightgbm-r' using 'dockerfile-r'. This image contains the LightGBM R package. Requires Docker and the 'dockerfile-r' in the build context.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docker/README.md#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\nmkdir lightgbm-docker\ncd lightgbm-docker\nwget https://raw.githubusercontent.com/Microsoft/LightGBM/master/docker/dockerfile-r\n\ndocker build \\\n    -t lightgbm-r \\\n    -f dockerfile-r \\\n    .\n```\n\n----------------------------------------\n\nTITLE: Running LightGBM Training Job from CLI - Shell\nDESCRIPTION: This snippet demonstrates how to run a LightGBM training job using the command line interface by specifying a configuration file and optional overrides. Required dependencies include a working LightGBM CLI installation. The 'config' argument should be set to the path of a valid LightGBM configuration file, while additional arguments (e.g., model hyperparameters) can override settings in the config file. Expected output is model training progress and resulting model files as specified by the configuration. Limitations: config file must follow LightGBM formatting rules; CLI must be available in the environment.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Quick-Start.rst#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nlightgbm config=your_config_file other_args ...\n```\n\n----------------------------------------\n\nTITLE: Using Dask Estimators for Prediction in LightGBM\nDESCRIPTION: This snippet explains the use of lightgbm.dask estimators to perform predictions on Dask DataFrame or Array inputs, returning a Dask Array of predicted values. It also mentions using dask-ml metrics for distributed model evaluation.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parallel-Learning-Guide.rst#_snippet_7\n\n\n\n----------------------------------------\n\nTITLE: Building LightGBM Python Package with Time Cost Output (sh)\nDESCRIPTION: Installs the LightGBM Python package configured to output time costs for internal routines, triggered by the `--time-costs` flag in `build-python.sh`. Requires dependencies specified in the 'Build with Time Costs Output' section.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/python-package/README.rst#_snippet_22\n\nLANGUAGE: sh\nCODE:\n```\nsh ./build-python.sh install --time-costs\n```\n\n----------------------------------------\n\nTITLE: Running LightGBM R-package Unit Tests via Shell\nDESCRIPTION: This shell snippet runs unit tests for the LightGBM R-package during development. It builds the package (skipping vignette building), installs it with source preservation, and executes tests located in `R-package/tests` using Rscript. It allows configuring test verbosity by setting the environment variable `LIGHTGBM_TEST_VERBOSITY`. This snippet requires a working R environment, the built tarball, and testthat test files in the specified directory.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/R-package/README.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nsh build-cran-package.sh \\\n    --no-build-vignettes\n\nR CMD INSTALL --with-keep.source lightgbm*.tar.gz\ncd R-package/tests\nRscript testthat.R\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with a Trained LightGBM Model (Bash)\nDESCRIPTION: Runs the LightGBM prediction process using the configuration specified in 'predict.conf'. This command requires a pre-trained model (generated by the training step) and the 'lightgbm' binary located two directories above ('../../lightgbm'). It uses the settings in 'predict.conf' to generate predictions on new data.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/examples/regression/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n\"../../lightgbm\" config=predict.conf\n```\n\n----------------------------------------\n\nTITLE: Setting LightGBM Training Configuration\nDESCRIPTION: This snippet demonstrates a typical configuration for training LightGBM models, specifying parameters such as the number of bins, leaves, iterations, learning rate, and device.  This configuration is specifically tailored for GPU usage. It sets various parameters like the tree learner, metric, and device-specific settings (platform and device IDs).  The configuration shown above, except for the Bosch dataset, uses a smaller learning rate and a different value for min_sum_hessian_in_leaf. This snippet does not represent executable code but rather a demonstration of how parameters are set.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/GPU-Performance.rst#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nmax_bin = 63\nnum_leaves = 255\nnum_iterations = 500\nlearning_rate = 0.1\ntree_learner = serial\ntask = train\nis_training_metric = false\nmin_data_in_leaf = 1\nmin_sum_hessian_in_leaf = 100\nndcg_eval_at = 1,3,5,10\ndevice = gpu\ngpu_platform_id = 0\ngpu_device_id = 0\nnum_thread = 28\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM (No OpenMP) - macOS - GCC\nDESCRIPTION: Builds LightGBM on macOS without OpenMP using CMake and GCC. Sets the CXX and CC environment variables to use GCC (version 7 in the example).  Clones the repo, configures the build with CMake, and then builds the project. Assumes that CMake and GCC are installed. The `-DUSE_OPENMP=OFF` flag disables OpenMP.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_17\n\nLANGUAGE: sh\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\nexport CXX=g++-7 CC=gcc-7  # replace \"7\" with version of gcc installed on your machine\ncmake -B build -S . -DUSE_OPENMP=OFF\ncmake --build build -j4\n```\n\n----------------------------------------\n\nTITLE: Speed Test of LightGBM Training on GPU\nDESCRIPTION: This runs a speed test by training LightGBM on the Higgs dataset without evaluating metrics after each iteration, to measure training speed.  The `objective` and `metric` are still provided, but are not directly used.  It uses the GPU configuration.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/GPU-Tutorial.rst#_snippet_11\n\nLANGUAGE: Bash\nCODE:\n```\n./lightgbm config=lightgbm_gpu.conf data=higgs.train objective=binary metric=auc\n```\n\n----------------------------------------\n\nTITLE: Using Ray Framework for Distributed LightGBM with lightgbm_ray\nDESCRIPTION: Mentions that Ray, a framework for distributed computation, can be used with the `lightgbm_ray` project for distributed LightGBM training. Provides link to usage documentation and notes that maintenance is external.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parallel-Learning-Guide.rst#_snippet_17\n\n\n\n----------------------------------------\n\nTITLE: Cloning and Building LightGBM with VS Build Tools on Windows - Shell\nDESCRIPTION: This shell/console snippet demonstrates how to clone the LightGBM repository recursively, configure the build using CMake for 64-bit architecture, and compile the ALL_BUILD target in Release mode using Visual Studio Build Tools. Dependencies include Git, CMake, and VS Build Tools (or Visual Studio). The output will be an .exe and .dll in the Release directory. Key parameters: -A x64 for 64-bit, --target ALL_BUILD, and --config Release for optimized builds.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\ncmake -B build -S . -A x64\ncmake --build build --target ALL_BUILD --config Release\n```\n\n----------------------------------------\n\nTITLE: LightGBM Execution Command\nDESCRIPTION: This code snippet shows the command used to execute the LightGBM application in distributed learning mode. It specifies the configuration file (`train.conf`) to be used for training. The command should be run on each machine involved in the distributed training.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/examples/parallel_learning/README.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\n\"./lightgbm\" config=train.conf\n```\n\n----------------------------------------\n\nTITLE: Training a Model with LightGBM R in Docker (Shell)\nDESCRIPTION: Executes an R script ('train.R') to train a LightGBM model inside the 'lightgbm-r' Docker container. It downloads training data, creates the 'train.R' script (using the R snippet above), and runs the script using `Rscript` within the container via `docker run`. Requires the 'lightgbm-r' image, `curl`, and Docker.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docker/README.md#_snippet_8\n\nLANGUAGE: Shell\nCODE:\n```\n# get training data\ncurl -O https://raw.githubusercontent.com/Microsoft/LightGBM/master/examples/binary_classification/binary.train\n```\n\nLANGUAGE: Shell\nCODE:\n```\n# create training script\ncat << EOF > train.R\nlibrary(lightgbm)\nparams <- list(\n    objective = \"binary\"\n    , num_trees = 10L\n)\n\nbst <- lgb.train(\n    data = lgb.Dataset(\"binary.train\"),\n    params = params\n)\nlgb.save(bst, \"LightGBM-R-model.txt\")\nEOF\n```\n\nLANGUAGE: Shell\nCODE:\n```\n# run training in a container\ndocker run \\\n    --rm \\\n    --volume \"${PWD}\":/opt/training \\\n    --workdir /opt/training \\\n    lightgbm-r \\\n    Rscript train.R\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM (MPI) - Linux - Clang\nDESCRIPTION: Builds LightGBM on Linux with MPI support using CMake and Clang. Sets the CXX and CC environment variables to use Clang.  Clones the repo, configures the build with CMake enabling MPI, and then builds the project. Assumes CMake, Clang, and Open MPI are installed.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_19\n\nLANGUAGE: sh\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\nexport CXX=clang++-14 CC=clang-14  # replace \"14\" with version of Clang installed on your machine\ncmake -B build -S . -DUSE_MPI=ON\ncmake --build build -j4\n```\n\n----------------------------------------\n\nTITLE: Specifying Distributed Node Addresses (Configuration Format)\nDESCRIPTION: Defines the network locations of nodes participating in a distributed LightGBM training. Each line follows the format 'IP_Address Port_Number', specifying a server IP and the port it's listening on for inter-node communication.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/examples/parallel_learning/mlist.txt#_snippet_0\n\nLANGUAGE: Configuration Format\nCODE:\n```\n192.168.1.101 12400\n192.168.1.102 12400\n```\n\n----------------------------------------\n\nTITLE: Ignore Column Parameter Description\nDESCRIPTION: The `ignore_column` parameter specifies columns to be ignored during training. It accepts multi-int or string (with `name:` prefix) and allows specifying multiple column indices or names to be ignored. The index starts from 0 and does not count the label column. Despite ignoring specified columns during training, they should have a valid format for successful file loading. This parameter aliases to `ignore_feature` and `blacklist`.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parameters.rst#_snippet_13\n\nLANGUAGE: text\nCODE:\n```\n``ignore_column`` :raw-html:`<a id=\"ignore_column\" title=\"Permalink to this parameter\" href=\"#ignore_column\">&#x1F517;&#xFE0E;</a>`, default = ``\"\"``, type = multi-int or string, aliases: ``ignore_feature``, ``blacklist``\n\n   -  used to specify some ignoring columns in training\n\n   -  use number for index, e.g. ``ignore_column=0,1,2`` means column_0, column_1 and column_2 will be ignored\n\n   -  add a prefix ``name:`` for column name, e.g. ``ignore_column=name:c1,c2,c3`` means c1, c2 and c3 will be ignored\n\n   -  **Note**: works only in case of loading data directly from text file\n\n   -  **Note**: index starts from ``0`` and it doesn't count the label column when passing type is ``int``\n\n   -  **Note**: despite the fact that specified columns will be completely ignored during the training, they still should have a valid format allowing LightGBM to load file successfully\n```\n\n----------------------------------------\n\nTITLE: Accessing Jupyter Notebook in LightGBM Container (URL)\nDESCRIPTION: Specifies the host machine's address and port (localhost:8888) to access the Jupyter Notebook service running inside the 'lightgbm-gpu' container. This URL should be opened in a web browser. The default password is 'keras'.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docker/gpu/README.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nlocalhost:8888\n```\n\n----------------------------------------\n\nTITLE: Running RStudio Server in Docker (Shell)\nDESCRIPTION: Runs an RStudio server instance within the 'lightgbm-r' Docker container. It maps port 8787 from the container to the host and sets the login password to 'lightgbm'. Requires the 'lightgbm-r' image and Docker.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docker/README.md#_snippet_10\n\nLANGUAGE: Shell\nCODE:\n```\ndocker run \\\n    --rm \\\n    --env PASSWORD=\"lightgbm\" \\\n    -p 8787:8787 \\\n    lightgbm-r\n```\n\n----------------------------------------\n\nTITLE: Cloning and Building LightGBM with MinGW-w64 on Windows - Shell\nDESCRIPTION: This snippet shows how to build LightGBM on Windows using Git, CMake, and MinGW-w64. It uses the 'MinGW Makefiles' generator for CMake and builds the project with parallel jobs (-j4). The output includes executables and DLLs placed in the LightGBM directory. Git for Windows, CMake, and MinGW-w64 are required. Common issues may require running CMake twice or adjusting the CMAKE_SH variable.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\ncmake -B build -S . -G \"MinGW Makefiles\"\ncmake --build build -j4\n```\n\n----------------------------------------\n\nTITLE: Passing Multi-Value Parameters as R Lists in LightGBM\nDESCRIPTION: Demonstrates how to specify multi-value parameters for LightGBM in R by passing them as a list. Here, the 'params' list assigns 'monotone_constraints' to a vector of integers. This conforms to LightGBM's expectation for multi-valued options in R and ensures proper interpretation during model training. Requires LightGBM's R package installed. Inputs are an R list; output is used as arguments to LightGBM training functions in R.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parameters.rst#_snippet_3\n\nLANGUAGE: r\nCODE:\n```\nparams <- list(\n   monotone_constraints = c(-1, 0, 1)\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Core Build Feature Options in CMake\nDESCRIPTION: Configures several boolean options using the `option()` command to control major features during the LightGBM build. These options allow enabling/disabling MPI-based distribution, OpenMP parallelization, GPU acceleration (general and CUDA-specific), SWIG Java API generation, performance time tagging, debug mode builds, and code sanitizers. Default values are specified for each option.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\noption(USE_MPI \"Enable MPI-based distributed learning\" OFF)\noption(USE_OPENMP \"Enable OpenMP\" ON)\noption(USE_GPU \"Enable GPU-accelerated training\" OFF)\noption(USE_SWIG \"Enable SWIG to generate Java API\" OFF)\noption(USE_TIMETAG \"Set to ON to output time costs\" OFF)\noption(USE_CUDA \"Enable CUDA-accelerated training \" OFF)\noption(USE_DEBUG \"Set to ON for Debug mode\" OFF)\noption(USE_SANITIZER \"Use sanitizer flags\" OFF)\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM CLI Docker Image (Shell)\nDESCRIPTION: Builds a Docker image tagged 'lightgbm-cli' using the 'dockerfile-cli'. This image contains the LightGBM command-line interface. Requires Docker to be installed and the 'dockerfile-cli' to be present in the build context.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docker/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nmkdir lightgbm-docker\ncd lightgbm-docker\nwget https://raw.githubusercontent.com/Microsoft/LightGBM/master/docker/dockerfile-cli\ndocker build \\\n    -t lightgbm-cli \\\n    -f dockerfile-cli \\\n    .\n```\n\n----------------------------------------\n\nTITLE: Predict Leaf Index Parameter Description\nDESCRIPTION: The `predict_leaf_index` parameter, when set to `true`, causes the prediction task to output the leaf index of all trees.  It is a boolean and aliases to `is_predict_leaf_index` and `leaf_index`.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parameters.rst#_snippet_22\n\nLANGUAGE: text\nCODE:\n```\n-  ``predict_leaf_index`` :raw-html:`<a id=\"predict_leaf_index\" title=\"Permalink to this parameter\" href=\"#predict_leaf_index\">&#x1F517;&#xFE0E;</a>`, default = ``false``, type = bool, aliases: ``is_predict_leaf_index``, ``leaf_index``\n\n   -  used only in ``prediction`` task\n\n   -  set this to ``true`` to predict with leaf index of all trees\n```\n\n----------------------------------------\n\nTITLE: Defining Build Targets and Installation Options in CMake\nDESCRIPTION: Sets various CMake options to control which components of LightGBM are built and how they are installed. Options configure the building of the command-line interface (`BUILD_CLI`), C++ tests (`BUILD_CPP_TEST`), a static library (`BUILD_STATIC_LIB`), installation of headers (`INSTALL_HEADERS`), special flags for Python/R package builds (`__BUILD_FOR_PYTHON`, `__BUILD_FOR_R`), and integration of OpenCL components (`__INTEGRATE_OPENCL`). Default values are provided for each option.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\noption(USE_HOMEBREW_FALLBACK \"(macOS-only) also look in 'brew --prefix' for libraries (e.g. OpenMP)\" ON)\noption(BUILD_CLI \"Build the 'lightgbm' command-line interface in addition to lib_lightgbm\" ON)\noption(BUILD_CPP_TEST \"Build C++ tests with Google Test\" OFF)\noption(BUILD_STATIC_LIB \"Build static library\" OFF)\noption(INSTALL_HEADERS \"Install headers to CMAKE_INSTALL_PREFIX (e.g. '/usr/local/include')\" ON)\noption(__BUILD_FOR_PYTHON \"Set to ON if building lib_lightgbm for use with the Python-package\" OFF)\noption(__BUILD_FOR_R \"Set to ON if building lib_lightgbm for use with the R-package\" OFF)\noption(__INTEGRATE_OPENCL \"Set to ON if building LightGBM with the OpenCL ICD Loader and its dependencies included\" OFF)\n```\n\n----------------------------------------\n\nTITLE: Header Parameter Description\nDESCRIPTION: The `header` parameter determines whether the input data includes a header row. Setting it to `true` indicates that the first row of the input file contains header information. This parameter is a boolean and aliases to `has_header`.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parameters.rst#_snippet_9\n\nLANGUAGE: text\nCODE:\n```\n``header`` :raw-html:`<a id=\"header\" title=\"Permalink to this parameter\" href=\"#header\">&#x1F517;&#xFE0E;</a>`, default = ``false``, type = bool, aliases: ``has_header``\n\n   -  set this to ``true`` if input data has header\n\n   -  **Note**: works only in case of loading data directly from text file\n```\n\n----------------------------------------\n\nTITLE: LightGBM Training on GPU for Regression Task\nDESCRIPTION: Demonstrates how to train a regression task with LightGBM on GPU using the Higgs dataset.  It specifies `objective=regression_l2` and `metric=l2` within the configuration, using the same configuration file.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/GPU-Tutorial.rst#_snippet_13\n\nLANGUAGE: Bash\nCODE:\n```\n./lightgbm config=lightgbm_gpu.conf data=higgs.train objective=regression_l2 metric=l2\n```\n\n----------------------------------------\n\nTITLE: Cloning and Building LightGBM with GCC on macOS - Shell\nDESCRIPTION: This shell snippet compiles LightGBM on macOS with GCC as the compiler. It sets the environment variables CC and CXX to the appropriate gcc/g++ versions (adjust as needed), then configures and builds with CMake. Prerequisites: Git, CMake, GCC. Outputs (.dylib and executable) are placed in the LightGBM directory.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\nexport CXX=g++-7 CC=gcc-7  # replace \"7\" with version of gcc installed on your machine\ncmake -B build -S .\ncmake --build build -j4\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM with CUDA Support\nDESCRIPTION: This command builds LightGBM with CUDA support, which offers faster GPU training on NVIDIA GPUs.  CUDA library is needed. CUDA version is not supported on macOS and Windows.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/python-package/README.rst#_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\npip install lightgbm --no-binary lightgbm --config-settings=cmake.define.USE_CUDA=ON\n```\n\n----------------------------------------\n\nTITLE: Installing LightGBM via Homebrew on macOS - Shell\nDESCRIPTION: This shell command allows users to install LightGBM using the Homebrew package manager on macOS. It automatically fetches and installs precompiled binaries and dependencies. Requires Homebrew to be installed and does not involve custom builds or configuration. Refer to Homebrew's formula page for more details.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nbrew install lightgbm\n```\n\n----------------------------------------\n\nTITLE: Running LightGBM Training on GPU\nDESCRIPTION: This command trains LightGBM on the prepared Higgs dataset using the `lightgbm_gpu.conf` configuration file. It specifies the input data file (`higgs.train`), validation data (`higgs.test`), objective (`binary`), and evaluation metric (`auc`).  It's used to verify the GPU works correctly by comparing results with the CPU-based runs.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/GPU-Tutorial.rst#_snippet_9\n\nLANGUAGE: Bash\nCODE:\n```\n./lightgbm config=lightgbm_gpu.conf data=higgs.train valid=higgs.test objective=binary metric=auc\n```\n\n----------------------------------------\n\nTITLE: Training a Multiclass Classification Model with LightGBM in Bash\nDESCRIPTION: This command runs the LightGBM training process using a predefined configuration file (train.conf). It requires the LightGBM binary to be built and available at the root of the project.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/examples/multiclass_classification/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n\"../../lightgbm\" config=train.conf\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Apple Clang Build on macOS - Shell\nDESCRIPTION: This shell snippet installs CMake and libomp via Homebrew on macOS, preparing the system to build LightGBM using Apple Clang with OpenMP support. Both packages must be present before proceeding to the configuration and build steps. Required for subsequent compilation commands.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nbrew install cmake libomp\n```\n\n----------------------------------------\n\nTITLE: Installing LightGBM from PyPI\nDESCRIPTION: This command installs the LightGBM package from the Python Package Index (PyPI) using pip. It installs the pre-compiled library that supports both GPU and CPU versions on Windows and Linux. OpenCL Runtime libraries are needed to use GPU version.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/python-package/README.rst#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install lightgbm\n```\n\n----------------------------------------\n\nTITLE: Installing LightGBM from conda-forge\nDESCRIPTION: This command installs LightGBM from the conda-forge channel using conda. The packages support CPU, GPU, and CUDA versions. GPU-enabled version is available only for Windows and Linux. CUDA-enabled version is available only for Linux.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/python-package/README.rst#_snippet_14\n\nLANGUAGE: sh\nCODE:\n```\nconda install -c conda-forge lightgbm\n```\n\n----------------------------------------\n\nTITLE: Speed Test of LightGBM Training on CPU\nDESCRIPTION: This command is used for a speed comparison to measure the performance on a CPU. It utilizes the same configuration as the GPU test, but the `device=cpu` parameter is used. The `objective` and `metric` are still provided.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/GPU-Tutorial.rst#_snippet_12\n\nLANGUAGE: Bash\nCODE:\n```\n./lightgbm config=lightgbm_gpu.conf data=higgs.train objective=binary metric=auc device=cpu\n```\n\n----------------------------------------\n\nTITLE: Installing LightGBM with PyArrow Support\nDESCRIPTION: This command installs the LightGBM package along with the dependencies needed to use PyArrow. PyArrow provides a columnar memory format which can speed up data loading and processing. This uses the `[arrow]` extra.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/python-package/README.rst#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npip install 'lightgbm[arrow]'\n```\n\n----------------------------------------\n\nTITLE: Cloning and Building LightGBM with GCC on Linux - Shell\nDESCRIPTION: This snippet details how to compile LightGBM on Linux using GCC. It involves cloning the repository, configuring the project with CMake, and initiating the build with parallel jobs. Dependencies are Git, CMake, and GCC. Outputs include the executable and .so files in the LightGBM directory. All commands assume typical Unix shell usage.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\ncmake -B build -S .\ncmake --build build -j4\n```\n\n----------------------------------------\n\nTITLE: Installing Python Interface and Dependencies\nDESCRIPTION: Installs the Python interface for LightGBM, along with required Python packages, including `setuptools`, `numpy`, `scipy`, and `scikit-learn`. The `-U` flag upgrades packages to the newest versions, and `sudo -H pip` ensures the installation is done correctly.  Then it runs the install script (`./build-python.sh install --precompile`).\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/GPU-Tutorial.rst#_snippet_6\n\nLANGUAGE: Bash\nCODE:\n```\nsudo apt-get -y install python-pip\nsudo -H pip install setuptools numpy scipy scikit-learn -U\nsudo sh ./build-python.sh install --precompile\n```\n\n----------------------------------------\n\nTITLE: Training a LightGBM Regression Model (Bash)\nDESCRIPTION: Executes the LightGBM training process using the configuration specified in 'train.conf'. Requires the 'lightgbm' binary to be built and located two directories above the current folder ('../../lightgbm'). This command initiates model training based on the parameters and data defined in the configuration file.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/examples/regression/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n\"../../lightgbm\" config=train.conf\n```\n\n----------------------------------------\n\nTITLE: Preloading libgomp to Avoid TLS Allocation Errors (Console)\nDESCRIPTION: Demonstrates how to set the `LD_PRELOAD` environment variable to explicitly load the `libgomp.so.1` library before other libraries. This can often resolve the 'cannot allocate memory in static TLS block' error by ensuring the library is loaded early.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/FAQ.rst#_snippet_8\n\nLANGUAGE: Console\nCODE:\n```\nexport LD_PRELOAD=/root/miniconda3/envs/test-env/lib/libgomp.so.1\n```\n\n----------------------------------------\n\nTITLE: Predicting with LightGBM XE_NDCG Ranking Model (Bash)\nDESCRIPTION: Runs the LightGBM prediction process using a specified configuration file (`predict.conf`) after a model has been trained. Requires the `lightgbm` binary at `../../lightgbm` and the trained model.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/examples/xendcg/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n\"../../lightgbm\" config=predict.conf\n```\n\n----------------------------------------\n\nTITLE: Building CUDA GPU Version of LightGBM using gcc on Linux (shell)\nDESCRIPTION: This snippet provides shell commands to clone the LightGBM repository and build the CUDA version using gcc on Linux. Dependencies required include CMake, gcc, and CUDA with compute capability 6.0+ supported NVIDIA GPUs. The commands configure the build with CUDA support enabled and compile with parallel jobs. Outputs are created in the LightGBM folder after build completion.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_28\n\nLANGUAGE: shell\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\ncmake -B build -S . -DUSE_CUDA=ON\ncmake --build build -j4\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM (GPU) - Windows - Command Line\nDESCRIPTION: Builds the GPU version of LightGBM on Windows from the command line. It clones the LightGBM repository, creates a build directory, configures CMake with GPU enabled and specifies Boost paths, and then builds the project. Assumes Git for Windows, CMake, VS Build Tools, OpenCL, and Boost Binaries are installed. Modify the Boost paths as needed.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_25\n\nLANGUAGE: console\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\ncmake -B build -S . -A x64 -DUSE_GPU=ON -DBOOST_ROOT=C:/local/boost_1_63_0 -DBOOST_LIBRARYDIR=C:/local/boost_1_63_0/lib64-msvc-14.0\n# if you have installed NVIDIA CUDA to a customized location, you should specify paths to OpenCL headers and library like the following:\n# cmake -B build -S . -A x64 -DUSE_GPU=ON -DBOOST_ROOT=C:/local/boost_1_63_0 -DBOOST_LIBRARYDIR=C:/local/boost_1_63_0/lib64-msvc-14.0 -DOpenCL_LIBRARY=\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/lib/x64/OpenCL.lib\" -DOpenCL_INCLUDE_DIR=\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0/include\"\ncmake --build build --target ALL_BUILD --config Release\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM Python Docker Image (Shell)\nDESCRIPTION: Builds a Docker image tagged 'lightgbm-python' using 'dockerfile-python'. This image includes the LightGBM Python package. Requires Docker and the 'dockerfile-python' in the build context.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docker/README.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\nmkdir lightgbm-docker\ncd lightgbm-docker\nwget https://raw.githubusercontent.com/Microsoft/LightGBM/master/docker/dockerfile-python\ndocker build \\\n    -t lightgbm-python \\\n    -f dockerfile-python \\\n    .\n```\n\n----------------------------------------\n\nTITLE: Validating C++ Compiler Version Compatibility in CMake\nDESCRIPTION: Performs checks on the detected C++ compiler ID and version (`CMAKE_CXX_COMPILER_ID`, `CMAKE_CXX_COMPILER_VERSION`). It compares the version against minimum requirements for GCC (4.8.2), Clang (3.8), AppleClang (8.1.0), and MSVC (Visual Studio 2015, version 1900). If a compiler version is insufficient to support the required C++ standard (C++11 or C++14), it issues a fatal error.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\")\n  if(CMAKE_CXX_COMPILER_VERSION VERSION_LESS \"4.8.2\")\n    message(FATAL_ERROR \"Insufficient gcc version\")\n  endif()\nelseif(CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\")\n  if(CMAKE_CXX_COMPILER_VERSION VERSION_LESS \"3.8\")\n    message(FATAL_ERROR \"Insufficient Clang version\")\n  endif()\nelseif(CMAKE_CXX_COMPILER_ID STREQUAL \"AppleClang\")\n  if(CMAKE_CXX_COMPILER_VERSION VERSION_LESS \"8.1.0\")\n    message(FATAL_ERROR \"Insufficient AppleClang version\")\n  endif()\nelseif(MSVC)\n  if(MSVC_VERSION LESS 1900)\n    message(\n      FATAL_ERROR\n      \"The compiler ${CMAKE_CXX_COMPILER} doesn't support required C++11 features. Please use a newer MSVC.\"\n    )\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM (MPI) - Linux - GCC\nDESCRIPTION: Builds LightGBM on Linux with MPI support using CMake and GCC.  Clones the repo, configures the build with CMake enabling MPI, and then builds the project.  Assumes CMake, GCC, and Open MPI are installed.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_18\n\nLANGUAGE: sh\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\ncmake -B build -S . -DUSE_MPI=ON\ncmake --build build -j4\n```\n\n----------------------------------------\n\nTITLE: Building Java Wrapper of LightGBM using gcc on Linux (shell)\nDESCRIPTION: Commands to build LightGBM's Java wrapper on Linux using gcc compiler. Dependencies include CMake, gcc, SWIG, Java with JAVA_HOME set. The process clones the repo, configures the build with SWIG wrapping enabled, and compiles all targets with parallel jobs. The build artifacts reside in the LightGBM folder.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_32\n\nLANGUAGE: shell\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\ncmake -B build -S . -DUSE_SWIG=ON\ncmake --build build -j4\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Parameter Alias Precedence in LightGBM (Python)\nDESCRIPTION: Shows how LightGBM handles multiple aliases for the same parameter in Python, especially when the primary name is not present. In this example, 'shrinkage_rate' is preferred over 'eta', and because 'learning_rate' is not provided, LightGBM's internal preference (controlled by hard-coded order) selects 'shrinkage_rate' with the value 0.12. Requires 'lightgbm' and a valid training dataset as in previous example. This test case is valuable for understanding the role of parameter aliases and the importance of naming when passing parameters as a dictionary to 'lgb.train()'.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parameters.rst#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# use learning rate of 0.12, LightGBM has a hard-coded preference for 'shrinkage_rate'\n# over any other aliases, and 'learning_rate' is not provided\nlgb.train(\n   params={\n      \"eta\": 0.19,\n      \"shrinkage_rate\": 0.12\n   },\n   train_set=dtrain\n)\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with a Trained LightGBM Model in Bash\nDESCRIPTION: This command runs the LightGBM prediction process using a predefined configuration file (predict.conf). Training must be completed before running this prediction command.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/examples/multiclass_classification/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n\"../../lightgbm\" config=predict.conf\n```\n\n----------------------------------------\n\nTITLE: Configuring Integrated OpenCL in CMake\nDESCRIPTION: Sets up integrated OpenCL support with platform-specific checks that prevent this option on macOS.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_20\n\nLANGUAGE: CMake\nCODE:\n```\nif(__INTEGRATE_OPENCL)\n    if(APPLE)\n        message(FATAL_ERROR \"Integrated OpenCL build is not available on macOS\")\n    else()\n        include(cmake/IntegratedOpenCL.cmake)\n        add_definitions(-DUSE_GPU)\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Group Column Parameter Description\nDESCRIPTION: The `group_column` parameter specifies the query/group ID column. It can be an integer (index) or a string (with `name:` prefix). Index starts from 0 and doesn't count the label column. Data should be grouped by `query_id`. This parameter is an int or string and aliases to `group`, `group_id`, `query_column`, `query`, and `query_id`.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parameters.rst#_snippet_12\n\nLANGUAGE: text\nCODE:\n```\n``group_column`` :raw-html:`<a id=\"group_column\" title=\"Permalink to this parameter\" href=\"#group_column\">&#x1F517;&#xFE0E;</a>`, default = ``\"\"``, type = int or string, aliases: ``group``, ``group_id``, ``query_column``, ``query``, ``query_id``\n\n   -  used to specify the query/group id column\n\n   -  use number for index, e.g. ``query=0`` means column_0 is the query id\n\n   -  add a prefix ``name:`` for column name, e.g. ``query=name:query_id``\n\n   -  **Note**: works only in case of loading data directly from text file\n\n   -  **Note**: data should be grouped by query_id, for more information, see `Query Data <#query-data>`__\n\n   -  **Note**: index starts from ``0`` and it doesn't count the label column when passing type is ``int``, e.g. when label is column_0 and query_id is column_1, the correct parameter is ``query=0``\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM (No OpenMP) - Linux - GCC\nDESCRIPTION: Builds LightGBM on Linux without OpenMP support using CMake and GCC. It clones the LightGBM repository, navigates into it, creates a build directory, then uses CMake to configure the project and builds using make. The `-DUSE_OPENMP=OFF` flag disables OpenMP support. Assumes CMake and GCC are installed.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_12\n\nLANGUAGE: sh\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\ncmake -B build -S . -DUSE_OPENMP=OFF\ncmake --build build -j4\n```\n\n----------------------------------------\n\nTITLE: Building Java Wrapper of LightGBM using VS Build Tools on Windows (shell)\nDESCRIPTION: This snippet details commands to build the Java wrapper of LightGBM on Windows using VS Build Tools. Required dependencies include Git, CMake, VS Build Tools or Visual Studio, Java with JAVA_HOME environment variable set, and SWIG. The build enables SWIG wrapping and compiles the complete project in Release mode targeting 64-bit architecture. The resulting .jar file is stored in the build directory.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_30\n\nLANGUAGE: shell\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\ncmake -B build -S . -A x64 -DUSE_SWIG=ON\ncmake --build build --target ALL_BUILD --config Release\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM from Sources\nDESCRIPTION: This command builds and installs LightGBM from source. It requires a compiler (Visual Studio on Windows, Apple Clang or gcc on macOS) and CMake. This is necessary for custom configurations or when pre-built binaries are not available.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/python-package/README.rst#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\npip install lightgbm --no-binary lightgbm\n```\n\n----------------------------------------\n\nTITLE: Configuring Advanced CUDA Support in CMake\nDESCRIPTION: Sets up detailed CUDA configuration including toolkit requirements, flags, architecture targets, and debug/release settings for LightGBM with CUDA acceleration.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_22\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_CUDA)\n    find_package(CUDAToolkit 11.0 REQUIRED)\n    include_directories(${CUDAToolkit_INCLUDE_DIRS})\n    set(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} -Xcompiler=${OpenMP_CXX_FLAGS} -Xcompiler=-fPIC -Xcompiler=-Wall\")\n\n    # reference for mapping of CUDA toolkit component versions to supported architectures (\"compute capabilities\"):\n    # https://en.wikipedia.org/wiki/CUDA#GPUs_supported\n    set(CUDA_ARCHS \"60\" \"61\" \"62\" \"70\" \"75\")\n    if(CUDAToolkit_VERSION VERSION_GREATER_EQUAL \"11.0\")\n        list(APPEND CUDA_ARCHS \"80\")\n    endif()\n    if(CUDAToolkit_VERSION VERSION_GREATER_EQUAL \"11.1\")\n        list(APPEND CUDA_ARCHS \"86\")\n    endif()\n    if(CUDAToolkit_VERSION VERSION_GREATER_EQUAL \"11.5\")\n        list(APPEND CUDA_ARCHS \"87\")\n    endif()\n    if(CUDAToolkit_VERSION VERSION_GREATER_EQUAL \"11.8\")\n        list(APPEND CUDA_ARCHS \"89\")\n        list(APPEND CUDA_ARCHS \"90\")\n    endif()\n    if(CUDAToolkit_VERSION VERSION_GREATER_EQUAL \"12.8\")\n        list(APPEND CUDA_ARCHS \"100\")\n        list(APPEND CUDA_ARCHS \"120\")\n    endif()\n    # Generate PTX for the most recent architecture for forwards compatibility\n    list(POP_BACK CUDA_ARCHS CUDA_LAST_SUPPORTED_ARCH)\n    list(TRANSFORM CUDA_ARCHS APPEND \"-real\")\n    list(APPEND CUDA_ARCHS \"${CUDA_LAST_SUPPORTED_ARCH}-real\" \"${CUDA_LAST_SUPPORTED_ARCH}-virtual\")\n    message(STATUS \"CUDA_ARCHITECTURES: ${CUDA_ARCHS}\")\n    if(USE_DEBUG)\n      set(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} -g\")\n    else()\n      set(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} -O3 -lineinfo\")\n    endif()\n    message(STATUS \"CMAKE_CUDA_FLAGS: ${CMAKE_CUDA_FLAGS}\")\n\n    add_definitions(-DUSE_CUDA)\n\n    if(NOT DEFINED CMAKE_CUDA_STANDARD)\n      set(CMAKE_CUDA_STANDARD 11)\n      set(CMAKE_CUDA_STANDARD_REQUIRED ON)\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building C++ Unit Tests of LightGBM using MinGW-w64 on Windows (shell)\nDESCRIPTION: Commands for building LightGBM C++ unit tests on Windows using MinGW-w64. Prerequisites are Git, CMake, and MinGW-w64. It clones the repository, configures CMake with MinGW Makefiles and enables tests, then builds the test executable in parallel. The resulting test binary is located in the LightGBM folder. Notes mention handling 'sh.exe' PATH conflicts by rerunning CMake with specific flags.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_37\n\nLANGUAGE: shell\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\ncmake -B build -S . -G \"MinGW Makefiles\" -DBUILD_CPP_TEST=ON\ncmake --build build --target testlightgbm -j4\n```\n\n----------------------------------------\n\nTITLE: Generating Decision Tree Visualizations with LightGBM - Python\nDESCRIPTION: This snippet defines the `render_tree` function which generates a graphviz representation of a specific decision tree from the trained LightGBM model using `lgb.create_tree_digraph`. It allows specifying the tree index and information to display in nodes. The function is then called, either directly for a single tree or interactively using `ipywidgets.interact` to select trees and node info dynamically. The output is a Digraph object suitable for rendering.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/examples/python-guide/notebooks/interactive_plot_example.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef render_tree(tree_index, show_info, precision=3):\n    show_info = None if \"None\" in show_info else show_info\n    return lgb.create_tree_digraph(gbm, tree_index=tree_index, show_info=show_info, precision=precision)\n\nif INTERACTIVE:\n    # create widget to switch between trees and control info in nodes\n    interact(\n        render_tree,\n        tree_index=(0, gbm.num_trees() - 1),\n        show_info=SelectMultiple(  # allow multiple values to be selected\n            options=[\n                \"None\",\n                \"split_gain\",\n                \"internal_value\",\n                \"internal_count\",\n                \"internal_weight\",\n                \"leaf_count\",\n                \"leaf_weight\",\n                \"data_percentage\",\n            ],\n            value=[\"None\"],\n        ),\n        precision=(0, 10),\n    )\n    tree = None\nelse:\n    tree = render_tree(53, [\"None\"])\ntree\n```\n\n----------------------------------------\n\nTITLE: Running LightGBM GPU Python Docker Container (Shell)\nDESCRIPTION: Runs the 'lightgbm-gpu' Docker image as a detached container named 'lightgbm-gpu', removing it automatically on exit (--rm). It utilizes 'nvidia-docker' for GPU access, maps port 8888 from the container to the host for Jupyter Notebook access, and mounts the host's '/home' directory into the container's '/home'. Requires 'nvidia-docker' installed on the host.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docker/gpu/README.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\nnvidia-docker run --rm -d --name lightgbm-gpu -p 8888:8888 -v /home:/home lightgbm-gpu\n```\n\n----------------------------------------\n\nTITLE: Diagnosing Static TLS Memory Allocation Failure (Console)\nDESCRIPTION: Illustrates the error message 'cannot allocate memory in static TLS block' often encountered when loading LightGBM's shared library (libgomp.so.1), particularly on aarch64 Linux systems. This occurs when the dynamic loader cannot find sufficient static thread-local storage.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/FAQ.rst#_snippet_7\n\nLANGUAGE: Console\nCODE:\n```\nlib/libgomp.so.1: cannot allocate memory in static TLS block\n```\n\n----------------------------------------\n\nTITLE: Installing LightGBM with Dask Support\nDESCRIPTION: This command installs the LightGBM package with Dask integration for distributed computing. It installs the necessary dependencies to use lightgbm.dask. Dask-package is only tested on macOS and Linux.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/python-package/README.rst#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npip install 'lightgbm[dask]'\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM with GPU Support\nDESCRIPTION: This command builds LightGBM with GPU support.  Boost and OpenCL are needed. The GPU version is not supported for macOS. Pass OpenCL_INCLUDE_DIR and OpenCL_LIBRARY options for Linux and BOOST_ROOT and BOOST_LIBRARYDIR options for Windows.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/python-package/README.rst#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\npip install lightgbm --no-binary lightgbm --config-settings=cmake.define.USE_GPU=ON\n```\n\nLANGUAGE: sh\nCODE:\n```\npip install lightgbm --no-binary lightgbm --config-settings=cmake.define.USE_GPU=ON --config-settings=cmake.define.OpenCL_INCLUDE_DIR=\"/usr/local/cuda/include/\" --config-settings=cmake.define.OpenCL_LIBRARY=\"/usr/local/cuda/lib64/libOpenCL.so\"\n```\n\n----------------------------------------\n\nTITLE: Building C++ Unit Tests of LightGBM using VS Build Tools on Windows (shell)\nDESCRIPTION: Build instructions for LightGBM C++ unit tests on Windows using VS Build Tools or Visual Studio. Dependencies include Git, CMake, and VS Build Tools. The snippet clones the repository, configures the build to enable C++ tests with a 64-bit architecture flag, and builds the test executable in Debug mode. The output executable is placed under LightGBM/Debug.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_36\n\nLANGUAGE: shell\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\ncmake -B build -S . -A x64 -DBUILD_CPP_TEST=ON\ncmake --build build --target testlightgbm --config Debug\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM Python with GPU Support and CMake Options (sh)\nDESCRIPTION: Installs the LightGBM Python package with GPU support, passing additional CMake options like `--opencl-include-dir` during the build. Requires dependencies and refers to the 'Build GPU Version' section for a list of options.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/python-package/README.rst#_snippet_18\n\nLANGUAGE: sh\nCODE:\n```\nsh ./build-python.sh install --gpu --opencl-include-dir=\"/usr/local/cuda/include/\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for GCC Build on macOS - Shell\nDESCRIPTION: This command installs both CMake and GCC with Homebrew as prerequisites for compiling LightGBM using the GCC toolchain on macOS. Necessary before executing build and configuration commands. The installed GCC version should be noted for later use in environment variable configuration.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\nbrew install cmake gcc\n```\n\n----------------------------------------\n\nTITLE: Creating C API Object Library\nDESCRIPTION: Creates an object library `lightgbm_capi_objs` from the files defined in `API_SOURCES`. This library is used for the C API of LightGBM.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_36\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(lightgbm_capi_objs OBJECT ${API_SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM Python Wheel File without Build Isolation (sh)\nDESCRIPTION: Builds a Python wheel file for LightGBM using `build-python.sh` and disables build isolation with `--no-isolation`. This is useful in environments with restricted internet access where build tools (`build`, `scikit-build-core`, `wheel`) are pre-installed.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/python-package/README.rst#_snippet_26\n\nLANGUAGE: sh\nCODE:\n```\nsh ./build-python.sh bdist_wheel --no-isolation\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM GPU Python Docker Image (Shell)\nDESCRIPTION: Creates a local directory 'lightgbm-docker', navigates into it, downloads the 'dockerfile.gpu' definition from the LightGBM GitHub repository using wget, and then builds a Docker image tagged as 'lightgbm-gpu' using that Dockerfile. Requires Docker installed on the host machine.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docker/gpu/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\nmkdir lightgbm-docker\ncd lightgbm-docker\nwget https://raw.githubusercontent.com/Microsoft/LightGBM/master/docker/gpu/dockerfile.gpu\ndocker build -f dockerfile.gpu -t lightgbm-gpu .\n```\n\n----------------------------------------\n\nTITLE: Installing CMake - macOS\nDESCRIPTION: Installs CMake on macOS using Homebrew.  This is often a prerequisite step for building LightGBM from source on macOS.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_14\n\nLANGUAGE: sh\nCODE:\n```\nbrew install cmake\n```\n\n----------------------------------------\n\nTITLE: Loading LightGBM Dataset using Sequence Interface (HDF5 Example)\nDESCRIPTION: Provides an example of implementing the LightGBM `Sequence` interface to load data in batches from a source like an HDF5 file using the `h5py` library. This approach allows for efficient memory usage by reading data chunks on demand.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Python-Intro.rst#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport h5py\n\nclass HDFSequence(lgb.Sequence):\n    def __init__(self, hdf_dataset, batch_size):\n        self.data = hdf_dataset\n        self.batch_size = batch_size\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n    def __len__(self):\n        return len(self.data)\n\nf = h5py.File('train.hdf5', 'r')\ntrain_data = lgb.Dataset(HDFSequence(f['X'], 8192), label=f['Y'][:])\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM Python Wheel File (sh)\nDESCRIPTION: Builds a Python wheel file (`.whl`) for the LightGBM package using `build-python.sh` without installing it. Requires `build`, `scikit-build-core`, and `wheel` build dependencies.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/python-package/README.rst#_snippet_25\n\nLANGUAGE: sh\nCODE:\n```\nsh ./build-python.sh bdist_wheel\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM from Source with CMake on Linux/Mac\nDESCRIPTION: This shell script clones the LightGBM repository, initializes the build environment, and runs Rscript to build the R package with CMake. It supports additional options for multi-threaded compilation, GPU support, and toolchain specification.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/R-package/README.md#_snippet_3\n\nLANGUAGE: Shell Script\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\nRscript build_r.R\n```\n\n----------------------------------------\n\nTITLE: Enabling Configured Sanitizers Conditionally in CMake\nDESCRIPTION: Conditionally enables code sanitizers based on the `USE_SANITIZER` option. It first checks for incompatibility, issuing a fatal error if using MSVC. If compatible, it includes a custom `Sanitizer.cmake` module and calls the `enable_sanitizers` function with the list of sanitizers specified in the `ENABLED_SANITIZERS` variable. Requires the `USE_SANITIZER` option to be ON and `ENABLED_SANITIZERS` to be set.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\n#-- Sanitizer\nif(USE_SANITIZER)\n  if(MSVC)\n    message(FATAL_ERROR \"Sanitizers are not supported with MSVC.\")\n  endif()\n  include(cmake/Sanitizer.cmake)\n  enable_sanitizers(\"${ENABLED_SANITIZERS}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running LightGBM Python Code Style Checks (sh)\nDESCRIPTION: Executes a shell script from the repository root (`.ci/lint-python-bash.sh`) to perform style checks on Python code contributions, ensuring they meet project standards.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/python-package/README.rst#_snippet_27\n\nLANGUAGE: sh\nCODE:\n```\nbash .ci/lint-python-bash.sh\n```\n\n----------------------------------------\n\nTITLE: Updating LightGBM R-package Documentation Using roxygen2 in Shell\nDESCRIPTION: This shell snippet updates the autogenerated documentation files in the LightGBM R-package source using the `roxygen2` package. It first installs `roxygen2` in a clean R session, rebuilds and installs the package without vignettes, then runs `roxygenize()` to update `DESCRIPTION`, `NAMESPACE`, and man page files. This sequence enables maintainers to keep documentation synchronized with source code annotations. It requires an internet connection for package installation and the source distribution.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/R-package/README.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nRscript \\\n    --vanilla \\\n    -e \"install.packages('roxygen2', repos = 'https://cran.rstudio.com')\"\n\nsh build-cran-package.sh --no-build-vignettes\nR CMD INSTALL \\\n  --with-keep.source \\\n  ./lightgbm_*.tar.gz\n\ncd R-package\nRscript \\\n    --vanilla \\\n    -e \"roxygen2::roxygenize(load = 'installed')\"\n```\n\n----------------------------------------\n\nTITLE: Setting up RPATH for OpenMP on macOS in LightGBM\nDESCRIPTION: Configures RPATH entries to ensure the dynamic loader can find OpenMP libraries at runtime. It handles different OpenMP implementations and locations for various installation methods (Homebrew, MacPorts, R package).\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_54\n\nLANGUAGE: CMake\nCODE:\n```\n  # with some compilers, OpenMP ships with the compiler (e.g. libgomp with gcc)\n  list(APPEND __omp_install_rpaths \"${OpenMP_LIBRARY_DIR}\")\n\n  # with clang, libomp doesn't ship with the compiler and might be supplied separately\n  if(CMAKE_CXX_COMPILER_ID MATCHES \"Clang\")\n      list(\n        APPEND __omp_install_rpaths\n          \"/opt/homebrew/opt/libomp/lib\"\n          \"/opt/local/lib/libomp\"\n      )\n      # It appears that CRAN's macOS binaries compiled with -fopenmp have install names\n      # of the form:\n      #\n      #   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libomp.dylib\n      #\n      # That corresponds to the libomp.dylib that ships with the R framework for macOS, available\n      # from https://cran.r-project.org/bin/macosx/.\n      #\n      # That absolute-path install name leads to that library being loaded unconditionally.\n      #\n      # That can result in e.g. 'library(data.table)' loading R's libomp.dylib and 'library(lightgbm)' loading\n      # Homebrew's. Having 2 loaded in the same process can lead to segfaults and unpredictable behavior.\n      #\n      # This can't be easily avoided by forcing R-package builds in LightGBM to use R's libomp.dylib\n      # at build time... LightGBM's CMake uses find_package(OpenMP), and R for macOS only provides the\n      # library, not CMake config files for it.\n      #\n      # Best we can do, to allow CMake-based builds of the R-package here to continue to work\n      # alongside CRAN-prepared binaries of other packages with OpenMP dependencies, is to\n      # ensure that R's library directory is the first place the loader searches for\n      # libomp.dylib when clang is used.\n      #\n      # ref: https://github.com/microsoft/LightGBM/issues/6628\n      #\n      if(__BUILD_FOR_R)\n        list(PREPEND __omp_install_rpaths \"${LIBR_LIBS_DIR}\")\n      endif()\n  endif()\n  set_target_properties(\n    _lightgbm\n    PROPERTIES\n      BUILD_WITH_INSTALL_RPATH TRUE\n      INSTALL_RPATH \"${__omp_install_rpaths}\"\n      INSTALL_RPATH_USE_LINK_PATH FALSE\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Multiple LightGBM Dask Worker Processes per Host in Python\nDESCRIPTION: This snippet shows how to prepare the 'machines' parameter to include multiple Dask worker processes (each with a different port) on the same physical host for distributed LightGBM training. The code dynamically generates the comma-separated string required for the 'machines' argument. Prerequisites: 'lightgbm' package, multiple Dask workers per host each with their own port and open firewall access. Input: list of host-port combinations. Output: a DaskLGBMRegressor instance with custom network configuration. Misconfiguration or unavailable ports will result in failed training sessions.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parallel-Learning-Guide.rst#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport lightgbm as lgb\n\nmachines = \",\".join([\n  \"10.0.1.0:16000\",\n  \"10.0.1.0:16001\",\n  \"10.0.2.0:16000\",\n  \"10.0.2.0:16001\",\n])\ndask_model = lgb.DaskLGBMRegressor(machines=machines)\n```\n\n----------------------------------------\n\nTITLE: Executing R Package Tests with Testthat\nDESCRIPTION: Shows how to invoke the `test_check` function from the `testthat` package to run all tests defined for a specific R package, with options to control behavior on failures or warnings during the test execution process.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/R-package/cran-comments.md#_snippet_12\n\nLANGUAGE: R\nCODE:\n```\ntest_check(\n    package = \"lightgbm\"\n    , stop_on_failure = TRUE\n    , stop_on_warning = FALSE\n)\n```\n\n----------------------------------------\n\nTITLE: OpenMP Support Integration\nDESCRIPTION: Links OpenMP libraries to 'lightgbm_objs' and 'lightgbm_capi_objs' when USE_OPENMP is enabled and compiler is Clang, enabling OpenMP parallelism in CPU-bound operations.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_43\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_OPENMP)\n  if(CMAKE_CXX_COMPILER_ID MATCHES \"Clang\")\n    target_link_libraries(lightgbm_objs PUBLIC OpenMP::OpenMP_CXX)\n    target_link_libraries(lightgbm_capi_objs PUBLIC OpenMP::OpenMP_CXX)\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing LightGBM Python Package using Precompiled Library (sh)\nDESCRIPTION: Installs the LightGBM Python package using a pre-existing shared library (e.g., built via MSBuild). The `--precompile` flag instructs `build-python.sh` to skip the library compilation step.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/python-package/README.rst#_snippet_24\n\nLANGUAGE: sh\nCODE:\n```\nsh ./build-python.sh install --precompile\n```\n\n----------------------------------------\n\nTITLE: Formatting MIT License File for CRAN\nDESCRIPTION: Provides the required YAML-like format for the LICENSE file when using the MIT license for an R package submitted to CRAN, specifying the year and copyright holder.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/R-package/cran-comments.md#_snippet_10\n\nLANGUAGE: Documentation\nCODE:\n```\nYEAR: 2016\nCOPYRIGHT HOLDER: Microsoft Corporation\n```\n\n----------------------------------------\n\nTITLE: GPU Libraries Linking\nDESCRIPTION: Links OpenCL and Boost libraries to 'lightgbm_objs' when USE_GPU is enabled, enabling GPU acceleration features and compatibility with OpenCL.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_44\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_GPU)\n  target_link_libraries(lightgbm_objs PUBLIC ${OpenCL_LIBRARY} ${Boost_LIBRARIES})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring SWIG for Java API Generation in CMake\nDESCRIPTION: Sets up the build environment for generating the LightGBM Java API using SWIG, conditional on the `USE_SWIG` option being enabled. It uses `find_package` to locate required dependencies (SWIG, Java, JNI), includes CMake modules (`UseJava`, `UseSWIG`), configures SWIG settings (output directory, extensions, language flags), adds necessary include directories for Java/JNI headers (including platform-specific paths), and creates the target directory for generated Java files. Requires SWIG, Java JDK, and JNI to be installed and findable by CMake.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_10\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_SWIG)\n  find_package(SWIG REQUIRED)\n  find_package(Java REQUIRED)\n  find_package(JNI REQUIRED)\n  include(UseJava)\n  include(UseSWIG)\n  set(SWIG_CXX_EXTENSION \"cxx\")\n  set(SWIG_EXTRA_LIBRARIES \"\")\n  set(SWIG_JAVA_EXTRA_FILE_EXTENSIONS \".java\" \"JNI.java\")\n  set(SWIG_MODULE_JAVA_LANGUAGE \"JAVA\")\n  set(SWIG_MODULE_JAVA_SWIG_LANGUAGE_FLAG \"java\")\n  set(CMAKE_SWIG_OUTDIR \"${CMAKE_CURRENT_BINARY_DIR}/java\")\n  include_directories(Java_INCLUDE_DIRS)\n  include_directories(JNI_INCLUDE_DIRS)\n  include_directories($ENV{JAVA_HOME}/include)\n  if(WIN32)\n      set(LGBM_SWIG_DESTINATION_DIR \"${CMAKE_CURRENT_BINARY_DIR}/com/microsoft/ml/lightgbm/windows/x86_64\")\n      include_directories($ENV{JAVA_HOME}/include/win32)\n  elseif(APPLE)\n      set(LGBM_SWIG_DESTINATION_DIR \"${CMAKE_CURRENT_BINARY_DIR}/com/microsoft/ml/lightgbm/osx/x86_64\")\n      include_directories($ENV{JAVA_HOME}/include/darwin)\n  else()\n      set(LGBM_SWIG_DESTINATION_DIR \"${CMAKE_CURRENT_BINARY_DIR}/com/microsoft/ml/lightgbm/linux/x86_64\")\n      include_directories($ENV{JAVA_HOME}/include/linux)\n  endif()\n  file(MAKE_DIRECTORY \"${LGBM_SWIG_DESTINATION_DIR}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM Documentation Without Doxygen Using Environment Variable - Shell\nDESCRIPTION: This snippet demonstrates disabling C API documentation generation by setting the environment variable C_API to NO, followed by running make html to build documentation without using Doxygen. It installs minimal dependencies (only Sphinx and ReadTheDocs theme) to generate docs excluding C code parts. It applies to Unix-like shells, with alternative environment variable setting syntax noted for Windows (set C_API=NO). The process outputs HTML docs missing C API details.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/README.rst#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npip install sphinx 'sphinx_rtd_theme>=0.5'\nexport C_API=NO || set C_API=NO\nmake html\n```\n\n----------------------------------------\n\nTITLE: Subclassing LightGBM LGBMRegressor with Custom Prediction Truncation in Python\nDESCRIPTION: This Python snippet shows how to subclass the LGBMRegressor from LightGBM to create a custom regressor that truncates its predictions based on a maximum score parameter. It uses inheritance and method overriding with super() calls to ensure compatibility with LightGBM versions greater than 4.5.0. The example includes fitting the custom regressor on synthetic regression data generated by scikit-learn's make_regression and demonstrates usage of the overridden predict method which clips predictions using numpy's clip function.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/FAQ.rst#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom lightgbm import LGBMRegressor\nfrom sklearn.datasets import make_regression\n\nclass TruncatedRegressor(LGBMRegressor):\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n    def predict(self, X, max_score: float = np.inf):\n        preds = super().predict(X)\n        np.clip(preds, a_min=None, a_max=max_score, out=preds)\n        return preds\n\nX, y = make_regression(n_samples=1_000, n_features=4)\n\nreg_trunc = TruncatedRegressor().fit(X, y)\n\npreds = reg_trunc.predict(X)\nprint(f\"mean: {preds.mean():.2f}, max: {preds.max():.2f}\")\n# mean: -6.81, max: 345.10\n\npreds_trunc = reg_trunc.predict(X, max_score=preds.mean())\nprint(f\"mean: {preds_trunc.mean():.2f}, max: {preds_trunc.max():.2f}\")\n# mean: -56.50, max: -6.81\n```\n\n----------------------------------------\n\nTITLE: Controlling R CMD Check Behavior with Environment Variable\nDESCRIPTION: Identifies the environment variable _R_CHECK_DONTTEST_EXAMPLES_ used to temporarily disable the execution of \\donttest examples during R CMD check, as mentioned in R changelogs and discussed in the context of CRAN checks.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/R-package/cran-comments.md#_snippet_7\n\nLANGUAGE: Shell\nCODE:\n```\n_R_CHECK_DONTTEST_EXAMPLES_\n```\n\n----------------------------------------\n\nTITLE: Setting Compiler Flags for MSVC\nDESCRIPTION: This block sets compiler flags when using the MSVC compiler. It enables Unicode support with `/utf-8`. If building for R, it adds a definition `R_LEGACY_RCOMPLEX` to avoid a specific compilation issue. If `USE_DEBUG` is enabled, it sets the optimization level to `/Od`; otherwise, it sets higher optimization levels.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_28\n\nLANGUAGE: CMake\nCODE:\n```\nif(MSVC)\n    # compiling 'fmt' on MSVC: \"Unicode support requires compiling with /utf-8\"\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /W4 /MP /utf-8\")\n    if(__BUILD_FOR_R)\n        # MSVC does not like this commit:\n        # https://github.com/wch/r-source/commit/fb52ac1a610571fcb8ac92d886b9fefcffaa7d48\n        #\n        # and raises \"error C3646: 'private_data_c': unknown override specifier\"\n        add_definitions(-DR_LEGACY_RCOMPLEX)\n    endif()\n    if(USE_DEBUG)\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /Od\")\n    else()\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /O2 /Ob2 /Oi /Ot /Oy\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Generating Code Coverage Report for LightGBM R-package Tests in R\nDESCRIPTION: This R code snippet demonstrates how to generate a code coverage report for the LightGBM R-package using the `covr` package. It installs the package via a shell script, then runs Rscript to load `covr`, measure test coverage on the package located in `./lightgbm_r`, print coverage statistics, and output an HTML report with coverage visualization. This aids in assessing test completeness and identifying untested code paths. It requires the covr package to be available or installable.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/R-package/README.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\n# Install\nsh build-cran-package.sh \\\n    --no-build-vignettes\n\n# Get coverage\nRscript -e \" \\\n    library(covr);\\\n    coverage <- covr::package_coverage('./lightgbm_r', type = 'tests', quiet = FALSE);\\\n    print(coverage);\\\n    covr::report(coverage, file = file.path(getwd(), 'coverage.html'), browse = TRUE);\\\n    \"\n```\n\n----------------------------------------\n\nTITLE: Example of Distributed Learning Workflow\nDESCRIPTION: References a typical distributed learning example for LightGBM, illustrating the steps to configure and execute training in a multi-machine setup.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parallel-Learning-Guide.rst#_snippet_16\n\n\n\n----------------------------------------\n\nTITLE: Setting R Package Properties\nDESCRIPTION: Sets the target properties for the `_lightgbm` library to be compatible with R. It sets the prefix to an empty string and the output name to `lightgbm` if the project is being built for R.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_38\n\nLANGUAGE: CMake\nCODE:\n```\n# R expects libraries of the form <project>.{dll,dylib,so}, not lib_<project>.{dll,dylib,so}\nif(__BUILD_FOR_R)\n  set_target_properties(\n    _lightgbm\n    PROPERTIES\n      PREFIX \"\"\n      OUTPUT_NAME \"lightgbm\"\n  )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Standard Installation of Built LightGBM CRAN Package via R CMD\nDESCRIPTION: This shell command installs a previously built LightGBM CRAN package from a tarball file using R's command line installation utility. It expects a valid tarball matching `lightgbm_*.tar.gz` in the current directory and installs the package system-wide or in the user-defined library paths. No compilation is necessary if the package is prebuilt correctly.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/R-package/README.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nR CMD install lightgbm_*.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Attaching to Running LightGBM Container (Shell)\nDESCRIPTION: Provides interactive terminal access (-it) to the running Docker container named 'lightgbm-gpu' by executing a bash shell. This allows for executing commands directly within the container's environment.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docker/gpu/README.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\ndocker exec -it lightgbm-gpu bash\n```\n\n----------------------------------------\n\nTITLE: Example CRAN message text\nDESCRIPTION: Details from a CRAN response related to alignment errors detected by UBSAN during package checks. It shows specific errors related to misaligned memory access during runtime, impacting data structures and standard library components.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/R-package/cran-comments.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nThanks, we see:\n\nStill lots of alignment errors, such as\n\nlightgbm.Rcheck/tests/testthat.Rout:io/dataset_loader.cpp:340:59:\nruntime error: reference binding to misaligned address 0x7f51fefad81e for type 'const value_type', which requires 4 byte alignment\nlightgbm.Rcheck/tests/testthat.Rout:/usr/include/c++/10/bits/stl_vector.h:1198:21:\nruntime error: reference binding to misaligned address 0x7f51fefad81e for type 'const int', which requires 4 byte alignment lightgbm.Rcheck/tests/testthat.Rout:/usr/include/c++/10/bits/vector.tcc:449:28:runtime\nerror: reference binding to misaligned address 0x7f51fefad81e for type 'const type', which requires 4 byte alignment\nlightgbm.Rcheck/tests/testthat.Rout:/usr/include/c++/10/bits/move.h:77:36:\nruntime error: reference binding to misaligned address 0x7f51fefad81e for type 'const int', which requires 4 byte alignment\nlightgbm.Rcheck/tests/testthat.Rout:/usr/include/c++/10/bits/alloc_traits.h:512:17:\nruntime error: reference binding to misaligned address 0x7f51fefad81e for type 'const type', which requires 4 byte alignment\n\nPlease fix and resubmit.\n```\n\n----------------------------------------\n\nTITLE: macOS OpenMP Library Path Handling\nDESCRIPTION: Overrides the linked OpenMP library path in the macOS dynamic library to improve runtime discoverability and prevent multiple OpenMP libraries from loading.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_52\n\nLANGUAGE: CMake\nCODE:\n```\n# The macOS linker puts an absolute path to linked libraries in lib_lightgbm.dylib.\n# This block overrides that information for LightGBM's OpenMP dependency, to allow\n# finding that library in more places.\n#\n# This reduces the risk of runtime issues resulting from multiple {libgomp,libiomp,libomp}.dylib being loaded.\n```\n\n----------------------------------------\n\nTITLE: Preparing Higgs Dataset\nDESCRIPTION: This script clones the boosting_tree_benchmarks repository, downloads and processes the Higgs dataset for LightGBM. It includes steps for downloading the data from a URL, unzipping the data, converting it to libsvm format, and creating symbolic links to the training and test data. This prepares the data for use in the LightGBM training examples.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/GPU-Tutorial.rst#_snippet_7\n\nLANGUAGE: Bash\nCODE:\n```\ngit clone https://github.com/guolinke/boosting_tree_benchmarks.git\ncd boosting_tree_benchmarks/data\nwget \"https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz\"\ngunzip HIGGS.csv.gz\npython higgs2libsvm.py\ncd ../..\nln -s boosting_tree_benchmarks/data/higgs.train\nln -s boosting_tree_benchmarks/data/higgs.test\n```\n\n----------------------------------------\n\nTITLE: Installing Build Dependencies\nDESCRIPTION: This command installs the necessary build tools and dependencies required for compiling LightGBM.  These include `git`, `cmake`, `build-essential`, `libboost-dev`, `libboost-system-dev`, and `libboost-filesystem-dev`.  These packages are essential for building LightGBM from source.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/GPU-Tutorial.rst#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\nsudo apt-get install --no-install-recommends git cmake build-essential libboost-dev libboost-system-dev libboost-filesystem-dev\n```\n\n----------------------------------------\n\nTITLE: Documenting R Function Return Values\nDESCRIPTION: Describes the \\value{} tag used in R package documentation (.Rd files) to specify and explain the objects returned by a function, as required by CRAN policies.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/R-package/cran-comments.md#_snippet_4\n\nLANGUAGE: R\nCODE:\n```\n\\value{}\n```\n\n----------------------------------------\n\nTITLE: Creating Shared or Static Library\nDESCRIPTION: Creates either a shared or static library named `_lightgbm` based on the value of `BUILD_STATIC_LIB`.  The generated library will link all the object files into final shared or static library.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_37\n\nLANGUAGE: CMake\nCODE:\n```\nif(BUILD_STATIC_LIB)\n  add_library(_lightgbm STATIC)\nelse()\n  add_library(_lightgbm SHARED)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring a Specific Dask Client for LightGBM Dask Estimators in Python\nDESCRIPTION: This snippet illustrates two ways to explicitly control which Dask client LightGBM's Dask estimators should use for distributed training. 'client' can be passed as a keyword to the estimator constructor, or passed later via the set_params() method. Dependencies: 'lightgbm' Python package with Dask integration and 'distributed'. Parameters: client (Dask Client object). Output: an instantiated LightGBM Dask estimator configured to use the specified client. This is required when multiple clusters or clients are active in the same Python process/session.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parallel-Learning-Guide.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport lightgbm as lgb\nfrom distributed import Client, LocalCluster\n\ncluster = LocalCluster()\nclient = Client(cluster)\n\n# option 1: keyword argument in constructor\ndask_model = lgb.DaskLGBMClassifier(client=client)\n\n# option 2: set_params() after construction\ndask_model = lgb.DaskLGBMClassifier()\ndask_model.set_params(client=client)\n```\n\n----------------------------------------\n\nTITLE: Creating Workspace Directory\nDESCRIPTION: This command creates a workspace directory in /mnt/workspace and sets the ownership to the current user.  It utilizes `sudo mkdir -p` to create the directory recursively, and then `sudo chown` to grant the user ownership.  This step is optional and can be skipped if using a local machine.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/GPU-Tutorial.rst#_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\nsudo mkdir -p /mnt/workspace\nsudo chown $(whoami):$(whoami) /mnt/workspace\n```\n\n----------------------------------------\n\nTITLE: R CMD check output example\nDESCRIPTION: This example showcases the output from the `R CMD check` command during a CRAN submission, highlighting a warning related to an insufficient package version number when resubmitting after failing checks.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/R-package/cran-comments.md#_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n* checking CRAN incoming feasibility ... WARNING\nMaintainer: âGuolin Ke <guolin.ke@microsoft.com>â\n\nInsufficient package version (submitted: 3.0.0, existing: 3.0.0)\n\nDays since last update: 4\n```\n\n----------------------------------------\n\nTITLE: Resolving OpenMP Conflict with Homebrew/Conda (Bash)\nDESCRIPTION: Provides a bash command for macOS users with Homebrew and Conda to resolve OpenMP library conflicts. It creates symlinks in the Conda environment's lib directory pointing to the libomp library installed by Homebrew, specifically addressing issues with OpenMP versions before 8.0.0.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/FAQ.rst#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nln -sf `ls -d \"$(brew --cellar libomp)\"/*/lib`/* $CONDA_PREFIX/lib\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM (No OpenMP) - macOS - Apple Clang\nDESCRIPTION: Builds LightGBM on macOS without OpenMP support using CMake and Apple Clang.  Clones the LightGBM repo, configures the build with CMake, and builds the project.  Assumes that CMake is installed via Homebrew.  The `-DUSE_OPENMP=OFF` flag disables OpenMP.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_15\n\nLANGUAGE: sh\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\ncmake -B build -S . -DUSE_OPENMP=OFF\ncmake --build build -j4\n```\n\n----------------------------------------\n\nTITLE: Displaying OpenMP Conflict Error Message (Console)\nDESCRIPTION: Presents the detailed error message produced by OpenMP when multiple runtimes are initialized, often seen in Conda environments with MKL. The error explains the danger of multiple runtimes and suggests workarounds like setting the KMP_DUPLICATE_LIB_OK environment variable.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/FAQ.rst#_snippet_1\n\nLANGUAGE: console\nCODE:\n```\nOMP: Error #15: Initializing libiomp5.dylib, but found libomp.dylib already initialized.\nOMP: Hint: This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/.\n```\n\n----------------------------------------\n\nTITLE: Installing CMake, OpenMP and Open MPI - macOS\nDESCRIPTION: Installs CMake, OpenMP, and Open MPI on macOS using Homebrew. This sets up necessary tools for building LightGBM with MPI support.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_20\n\nLANGUAGE: sh\nCODE:\n```\nbrew install cmake libomp open-mpi\n```\n\n----------------------------------------\n\nTITLE: Installing CMake and GCC - macOS\nDESCRIPTION: Installs CMake and GCC on macOS using Homebrew. This sets up necessary tools for building LightGBM from source.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_16\n\nLANGUAGE: sh\nCODE:\n```\nbrew install cmake gcc\n```\n\n----------------------------------------\n\nTITLE: Building CUDA GPU Version of LightGBM using Clang on Linux (shell)\nDESCRIPTION: This snippet shows how to build the CUDA version of LightGBM using Clang compiler on Linux. It requires CMake, Clang (with version specified via environment variables), OpenMP, and CUDA libraries installed. The build is configured to enable CUDA and compiled in parallel with four threads. The resulting executables reside in the LightGBM directory.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_29\n\nLANGUAGE: shell\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\nexport CXX=clang++-14 CC=clang-14  # replace \"14\" with version of Clang installed on your machine\ncmake -B build -S . -DUSE_CUDA=ON\ncmake --build build -j4\n```\n\n----------------------------------------\n\nTITLE: Google Test Framework Setup\nDESCRIPTION: Detects the presence of Google Test, fetches it if unavailable, and creates the 'testlightgbm' executable with the test source files and dependencies on 'lightgbm_objs' and 'lightgbm_capi_objs'. Facilitates unit testing for LightGBM.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_49\n\nLANGUAGE: CMake\nCODE:\n```\nif(BUILD_CPP_TEST)\n  find_package(GTest CONFIG)\n  if(NOT GTEST_FOUND)\n    message(STATUS \"Did not find Google Test in the system root. Fetching Google Test now...\")\n    include(FetchContent)\n    FetchContent_Declare(\n      googletest\n      GIT_REPOSITORY https://github.com/google/googletest.git\n      GIT_TAG        v1.14.0\n    )\n    FetchContent_MakeAvailable(googletest)\n    add_library(GTest::GTest ALIAS gtest)\n  endif()\n\n  set(LightGBM_TEST_HEADER_DIR ${PROJECT_SOURCE_DIR}/tests/cpp_tests)\n  include_directories(${LightGBM_TEST_HEADER_DIR})\n\n  set(\n    CPP_TEST_SOURCES\n      tests/cpp_tests/test_array_args.cpp\n      tests/cpp_tests/test_arrow.cpp\n      tests/cpp_tests/test_byte_buffer.cpp\n      tests/cpp_tests/test_chunked_array.cpp\n      tests/cpp_tests/test_common.cpp\n      tests/cpp_tests/test_main.cpp\n      tests/cpp_tests/test_serialize.cpp\n      tests/cpp_tests/test_single_row.cpp\n      tests/cpp_tests/test_stream.cpp\n      tests/cpp_tests/testutils.cpp\n    )\n  if(MSVC)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /permissive-\")\n  endif()\n  add_executable(testlightgbm ${CPP_TEST_SOURCES})\n  target_link_libraries(testlightgbm PRIVATE lightgbm_objs lightgbm_capi_objs GTest::GTest)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building Java Wrapper of LightGBM using MinGW-w64 on Windows (shell)\nDESCRIPTION: This snippet demonstrates building LightGBM's Java wrapper on Windows with MinGW-w64 toolchain. It requires Git, CMake, MinGW-w64, Java with correct JAVA_HOME, and SWIG. The CMake configuration specifies MinGW Makefiles and SWIG wrapping enabled. Building is executed with parallelization, and notes mention potential workarounds for 'sh.exe in PATH' errors by rerunning CMake with specific flags.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_31\n\nLANGUAGE: shell\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\ncmake -B build -S . -G \"MinGW Makefiles\" -DUSE_SWIG=ON\ncmake --build build -j4\n```\n\n----------------------------------------\n\nTITLE: Handling Unavailable Matrix Dependency Error in R Installation (Text)\nDESCRIPTION: Shows the error message 'package âMatrixâ is not available for this version of R' encountered when installing the LightGBM R package on R versions older than 4.4.0, due to `Matrix` version 1.7-0 requiring R >= 4.4.0.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/FAQ.rst#_snippet_9\n\nLANGUAGE: Text\nCODE:\n```\npackage âMatrixâ is not available for this version of R\n```\n\n----------------------------------------\n\nTITLE: Running LightGBM R-package Unit Tests with Verbosity Enabled\nDESCRIPTION: This shell snippet shows how to enable verbose logging for the LightGBM unit tests by setting the environment variable `LIGHTGBM_TEST_VERBOSITY` to a valid verbosity level before running the tests. It is intended for detailed debugging and requires the same environment and prerequisites as the standard testing commands.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/R-package/README.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nexport LIGHTGBM_TEST_VERBOSITY=1\ncd R-package/tests\nRscript testthat.R\n```\n\n----------------------------------------\n\nTITLE: Installing NVIDIA Drivers\nDESCRIPTION: This command installs the necessary NVIDIA drivers and OpenCL development environment for GPU usage. It uses `apt-get` to install the drivers and related packages such as `nvidia-opencl-icd-375` and `opencl-headers`. Requires root privileges via `sudo`.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/GPU-Tutorial.rst#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\nsudo apt-get update\nsudo apt-get install --no-install-recommends nvidia-375\nsudo apt-get install --no-install-recommends nvidia-opencl-icd-375 nvidia-opencl-dev opencl-headers\n```\n\n----------------------------------------\n\nTITLE: Example LightGBM Positional Data\nDESCRIPTION: Example representation of positional data for LightGBM's Learning-to-Rank. Each line corresponds to a training data sample, with integer values indicating distinct position categories (e.g., '0' for 'above the fold', '1' for 'requires scrolling'). LightGBM uses this data, typically provided in a '.position' file or via API, to model and mitigate position bias during training. The specific integer values are not important, only their consistency.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Advanced-Topics.rst#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n0\n0\n0\n1\n1\n0\n0\n0\n1\n...\n```\n\n----------------------------------------\n\nTITLE: Building C++ Unit Tests of LightGBM using Clang on Linux (shell)\nDESCRIPTION: Commands for building LightGBM C++ unit tests on Linux using Clang with OpenMP support. The build environment requires CMake, Clang with appropriate exports for CC and CXX, and OpenMP. After cloning, the build is configured to enable test target and compiled with parallelism. Executable is output in the main project folder.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_39\n\nLANGUAGE: shell\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\nexport CXX=clang++-14 CC=clang-14  # replace \"14\" with version of Clang installed on your machine\ncmake -B build -S . -DBUILD_CPP_TEST=ON\ncmake --build build --target testlightgbm -j4\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM CRAN Package with Shell Commands\nDESCRIPTION: This shell snippet updates git submodules recursively and runs a build script to create a LightGBM CRAN-compliant package tarball named `lightgbm_${VERSION}.tar.gz`. It is used by maintainers to prepare packages for CRAN submission, supporting command-line options such as `--no-build-vignettes` and specifying an alternative R executable with `--r-executable`. The snippet requires a configured environment with git, shell, and R build tools.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/R-package/README.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\ngit submodule update --init --recursive\nsh build-cran-package.sh\n```\n\n----------------------------------------\n\nTITLE: Precise Float Parser Parameter Description\nDESCRIPTION: The `precise_float_parser` parameter enables more precise floating-point number parsing for the text parser (e.g., CSV, TSV, LibSVM input).  Setting it to `true` may considerably slow down text parsing.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parameters.rst#_snippet_17\n\nLANGUAGE: text\nCODE:\n```\n``precise_float_parser`` :raw-html:`<a id=\"precise_float_parser\" title=\"Permalink to this parameter\" href=\"#precise_float_parser\">&#x1F517;&#xFE0E;</a>`, default = ``false``, type = bool\n\n   -  use precise floating point number parsing for text parser (e.g. CSV, TSV, LibSVM input)\n\n   -  **Note**: setting this to ``true`` may lead to much slower text parsing\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM (MPI) - macOS - GCC\nDESCRIPTION: Builds LightGBM on macOS with MPI support using CMake and GCC.  Sets the CXX and CC environment variables to use GCC.  Clones the repo, configures the build with CMake enabling MPI, and then builds the project. Assumes that CMake, Open MPI and GCC are installed. Replace \"7\" with the installed version of gcc.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_23\n\nLANGUAGE: sh\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\nexport CXX=g++-7 CC=gcc-7  # replace \"7\" with version of gcc installed on your machine\ncmake -B build -S . -DUSE_MPI=ON\ncmake --build build -j4\n```\n\n----------------------------------------\n\nTITLE: Building C++ Unit Tests of LightGBM using gcc on Linux (shell)\nDESCRIPTION: Instructions to build LightGBM C++ unit tests on Linux with gcc. Required dependencies are CMake and gcc. The repository is cloned, and CMake is configured to build the test target enabled. Compilation uses parallel jobs. The test executable is generated in the LightGBM directory.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_38\n\nLANGUAGE: shell\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\ncmake -B build -S . -DBUILD_CPP_TEST=ON\ncmake --build build --target testlightgbm -j4\n```\n\n----------------------------------------\n\nTITLE: Building Java Wrapper of LightGBM using gcc on macOS (shell)\nDESCRIPTION: Steps to build the LightGBM Java wrapper on macOS using gcc compiler. Dependencies are installed via Homebrew including CMake, OpenJDK (with JAVA_HOME set), SWIG, and gcc. Afterwards, repository cloning, environment variable setup, CMake configuration with SWIG enabled, and multi-threaded compilation are performed to generate artifacts.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_35\n\nLANGUAGE: shell\nCODE:\n```\nbrew install cmake openjdk swig gcc\nexport JAVA_HOME=\"$(brew --prefix openjdk)/libexec/openjdk.jdk/Contents/Home/\"\n\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\nexport CXX=g++-7 CC=gcc-7  # replace \"7\" with version of gcc installed on your machine\ncmake -B build -S . -DUSE_SWIG=ON\ncmake --build build -j4\n```\n\n----------------------------------------\n\nTITLE: Forced Splits JSON Example - LightGBM\nDESCRIPTION: This describes the format of the 'forcedsplits_filename' parameter, which points to a .json file specifying splits to force at the top of every decision tree. It specifies that the .json file can be arbitrarily nested, and each split contains 'feature' and 'threshold' fields, along with 'left' and 'right' fields representing subsplits. Categorical splits are forced in a one-hot fashion.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parameters.rst#_snippet_5\n\n\n\n----------------------------------------\n\nTITLE: Building 32-bit LightGBM Python Package (sh)\nDESCRIPTION: Installs the 32-bit version of the LightGBM Python package using the `build-python.sh` script. Requires a 32-bit Python installation and dependencies mentioned in the 'Build 32-bit Version' section.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/python-package/README.rst#_snippet_21\n\nLANGUAGE: sh\nCODE:\n```\nsh ./build-python.sh install --bit32\n```\n\n----------------------------------------\n\nTITLE: Extracting and Saving Booster from Dask LightGBM Estimator\nDESCRIPTION: Shows how to access the underlying Booster object from a trained Dask estimator and save it using various methods, including model serialization or dumping to JSON or string. Facilitates low-level model management.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parallel-Learning-Guide.rst#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport dask.array as da\nimport lightgbm as lgb\nfrom distributed import Client, LocalCluster\n\ncluster = LocalCluster(n_workers=2)\nclient = Client(cluster)\n\nX = da.random.random((1000, 10), (500, 10))\n y = da.random.random((1000,), (500,))\n\ndask_model = lgb.DaskLGBMRegressor()\n dask_model.fit(X, y)\n\n# get underlying Booster object\n bst = dask_model.booster_\n\n# save Booster in various ways\ndump_model = bst.dump_model()\nmodel_str = bst.model_to_string()\nbst.save_model('model.txt')\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM with MPI Support\nDESCRIPTION: This command builds LightGBM with MPI support for distributed training. MPI libraries need to be installed first. Compilation with MinGW-w64 is not supported on Windows.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/python-package/README.rst#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\npip install lightgbm --no-binary lightgbm --config-settings=cmake.define.USE_MPI=ON\n```\n\n----------------------------------------\n\nTITLE: SWIG Java Library Generation for LightGBM\nDESCRIPTION: Configures SWIG to generate Java bindings for LightGBM, including setting properties, package options, and build commands. Handles platform-specific library source paths and manages post-build steps such as Java compilation and JAR creation.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_41\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_SWIG)\n  set_property(SOURCE swig/lightgbmlib.i PROPERTY CPLUSPLUS ON)\n  list(APPEND swig_options -package com.microsoft.ml.lightgbm)\n  set_property(SOURCE swig/lightgbmlib.i PROPERTY SWIG_FLAGS \"${swig_options}\")\n  swig_add_library(_lightgbm_swig LANGUAGE java SOURCES swig/lightgbmlib.i)\n  swig_link_libraries(_lightgbm_swig _lightgbm)\n  set_target_properties(\n    _lightgbm_swig\n    PROPERTIES\n      PREFIX \"\"\n      OUTPUT_NAME \"lib_lightgbm_swig\"\n  )\n  if(WIN32)\n    set(LGBM_SWIG_LIB_DESTINATION_PATH \"${LGBM_SWIG_DESTINATION_DIR}/lib_lightgbm_swig.dll\")\n    if(MINGW OR CYGWIN)\n        set(LGBM_LIB_SOURCE_PATH \"${PROJECT_SOURCE_DIR}/lib_lightgbm.dll\")\n        set(LGBM_SWIG_LIB_SOURCE_PATH \"${PROJECT_SOURCE_DIR}/lib_lightgbm_swig.dll\")\n    else()\n        set(LGBM_LIB_SOURCE_PATH \"${PROJECT_SOURCE_DIR}/Release/lib_lightgbm.dll\")\n        set(LGBM_SWIG_LIB_SOURCE_PATH \"${PROJECT_SOURCE_DIR}/Release/lib_lightgbm_swig.dll\")\n    endif()\n  elseif(APPLE)\n    set(LGBM_LIB_SOURCE_PATH \"${PROJECT_SOURCE_DIR}/lib_lightgbm.dylib\")\n    set(LGBM_SWIG_LIB_SOURCE_PATH \"${PROJECT_SOURCE_DIR}/lib_lightgbm_swig.jnilib\")\n    set(LGBM_SWIG_LIB_DESTINATION_PATH \"${LGBM_SWIG_DESTINATION_DIR}/lib_lightgbm_swig.dylib\")\n  else()\n    set(LGBM_LIB_SOURCE_PATH \"${PROJECT_SOURCE_DIR}/lib_lightgbm.so\")\n    set(LGBM_SWIG_LIB_SOURCE_PATH \"${PROJECT_SOURCE_DIR}/lib_lightgbm_swig.so\")\n    set(LGBM_SWIG_LIB_DESTINATION_PATH \"${LGBM_SWIG_DESTINATION_DIR}/lib_lightgbm_swig.so\")\n  endif()\n  add_custom_command(\n      TARGET _lightgbm_swig\n      POST_BUILD\n      COMMAND \"${Java_JAVAC_EXECUTABLE}\" -d . java/*.java\n      COMMAND\n        \"${CMAKE_COMMAND}\"\n        -E\n        copy_if_different\n        \"${LGBM_LIB_SOURCE_PATH}\"\n        \"${LGBM_SWIG_DESTINATION_DIR}\"\n      COMMAND\n        \"${CMAKE_COMMAND}\"\n        -E\n        copy_if_different\n        \"${LGBM_SWIG_LIB_SOURCE_PATH}\"\n        \"${LGBM_SWIG_LIB_DESTINATION_PATH}\"\n      COMMAND \"${Java_JAR_EXECUTABLE}\" -cf lightgbmlib.jar com\n    )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Refitting Leaf Output - LightGBM\nDESCRIPTION: This snippet describes how leaf outputs are refitted using the 'refit_decay_rate' parameter. It shows the formula used for refitting: leaf_output = refit_decay_rate * old_leaf_output + (1.0 - refit_decay_rate) * new_leaf_output. It mentions that this is only used in the 'refit' task.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parameters.rst#_snippet_6\n\n\n\n----------------------------------------\n\nTITLE: Cloning and Building Threadless LightGBM with MinGW-w64 on Windows - Shell\nDESCRIPTION: This snippet shows how to compile LightGBM on Windows without OpenMP support using CMake and MinGW-w64 by including the '-DUSE_OPENMP=OFF' build flag. Outputs include .exe and .dll in the main directory. Dependencies: Git, CMake, MinGW-w64.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_11\n\nLANGUAGE: console\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\ncmake -B build -S . -G \"MinGW Makefiles\" -DUSE_OPENMP=OFF\ncmake --build build -j4\n```\n\n----------------------------------------\n\nTITLE: Checking inet_pton Availability on Windows\nDESCRIPTION: This snippet checks for the availability of `inet_pton` function, which is required for network address conversion. It's specific to Windows and uses `CheckSymbolExists` to determine if the function is defined in `ws2tcpip.h`. If the function is found, it adds a definition `WIN_HAS_INET_PTON`.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_27\n\nLANGUAGE: CMake\nCODE:\n```\nif(WIN32)\n  include(CheckSymbolExists)\n  list(APPEND CMAKE_REQUIRED_LIBRARIES \"ws2_32\")\n  check_symbol_exists(inet_pton \"ws2tcpip.h\" WIN_INET_PTON_FOUND)\n  if(WIN_INET_PTON_FOUND)\n    add_definitions(-DWIN_HAS_INET_PTON)\n  endif()\n  list(REMOVE_ITEM CMAKE_REQUIRED_LIBRARIES \"ws2_32\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM (MPI) - Windows - Command Line\nDESCRIPTION: Builds the MPI version of LightGBM on Windows from the command line.  It clones the LightGBM repository, creates a build directory, configures CMake with MPI enabled, and then builds the project in Release configuration. Assumes that Git for Windows, CMake, and VS Build Tools are installed, and that MS MPI is installed.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_24\n\nLANGUAGE: console\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\ncmake -B build -S . -A x64 -DUSE_MPI=ON\ncmake --build build --target ALL_BUILD --config Release\n```\n\n----------------------------------------\n\nTITLE: Handling 'Cannot set ... after freed raw data' Error (Console)\nDESCRIPTION: Displays the error 'Cannot set predictor/reference/categorical feature after freed raw data...'. This happens in the LightGBM Python package when `free_raw_data=True` (the default) is used during `Dataset` construction, and subsequent attempts are made to modify attributes that require the original raw data.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/FAQ.rst#_snippet_14\n\nLANGUAGE: Console\nCODE:\n```\nCannot set predictor/reference/categorical feature after freed raw data, set free_raw_data=False when construct Dataset to avoid this.\n```\n\n----------------------------------------\n\nTITLE: Building C++ Unit Tests of LightGBM using Apple Clang on macOS (shell)\nDESCRIPTION: This snippet covers building LightGBM C++ unit tests on macOS using Apple Clang compiler. Necessary dependencies installed via Homebrew are CMake and OpenMP libraries. After installation, the source is cloned, CMake is run to enable C++ tests, and the test executable is built with concurrency. The executable is created in the LightGBM directory.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_40\n\nLANGUAGE: shell\nCODE:\n```\nbrew install cmake libomp\n\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\ncmake -B build -S . -DBUILD_CPP_TEST=ON\ncmake --build build --target testlightgbm -j4\n```\n\n----------------------------------------\n\nTITLE: Executing LightGBM for Training Using Configuration File\nDESCRIPTION: This command runs the LightGBM binary to train a model based on the configuration specified in 'train.conf'. It assumes that the binary is built and located at the specified relative path. No code output is expected from this command, only execution of the training process.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/examples/lambdarank/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n\"../../lightgbm\" config=train.conf\n```\n\n----------------------------------------\n\nTITLE: Cloning and Building Threadless LightGBM with VS Build Tools on Windows - Shell\nDESCRIPTION: This console snippet demonstrates how to clone and build LightGBM on Windows with OpenMP support disabled by passing the '-DUSE_OPENMP=OFF' flag to CMake. All other steps are analogous to the standard build, including output directories and required dependencies (Git, CMake, VS Build Tools or Visual Studio).\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_10\n\nLANGUAGE: console\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\ncmake -B build -S . -A x64 -DUSE_OPENMP=OFF\ncmake --build build --target ALL_BUILD --config Release\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing Data - Python\nDESCRIPTION: This snippet loads training and testing data from tab-separated CSV files into pandas DataFrames. It then separates the target variable (first column) from the features for both the training and test sets.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/examples/python-guide/notebooks/interactive_plot_example.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nregression_example_dir = Path().absolute().parents[1] / \"regression\"\ndf_train = pd.read_csv(str(regression_example_dir / \"regression.train\"), header=None, sep=\"\\t\")\ndf_test = pd.read_csv(str(regression_example_dir / \"regression.test\"), header=None, sep=\"\\t\")\n\ny_train = df_train[0]\ny_test = df_test[0]\nX_train = df_train.drop(0, axis=1)\nX_test = df_test.drop(0, axis=1)\n```\n\n----------------------------------------\n\nTITLE: Configuring Makevars for Custom C Compiler on Linux/Mac\nDESCRIPTION: This shell configuration snippet sets custom C and C++ compilers by editing the ~/.R/Makevars file, enabling users to specify alternate compilers like gcc or g++. It is useful for environments requiring specific compiler versions.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/R-package/README.md#_snippet_2\n\nLANGUAGE: Makefile\nCODE:\n```\n# ~/.R/Makevars\nCC=gcc-8\nCXX=g++-8\nCXX11=g++-8\n```\n\n----------------------------------------\n\nTITLE: Running Interactive R Session in Docker (Shell)\nDESCRIPTION: Starts an interactive R session within the 'lightgbm-r' Docker container. Requires the 'lightgbm-r' image and Docker.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docker/README.md#_snippet_9\n\nLANGUAGE: Shell\nCODE:\n```\ndocker run \\\n    --rm \\\n    -it lightgbm-r \\\n    R\n```\n\n----------------------------------------\n\nTITLE: Forced Bins Filename Parameter Description\nDESCRIPTION: The `forcedbins_filename` parameter specifies the path to a `.json` file defining bin upper bounds for features.  The `.json` file contains an array of objects with `feature` (integer index) and `bin_upper_bound` (thresholds for binning). An example file is provided in the LightGBM repository.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parameters.rst#_snippet_15\n\nLANGUAGE: text\nCODE:\n```\n``forcedbins_filename`` :raw-html:`<a id=\"forcedbins_filename\" title=\"Permalink to this parameter\" href=\"#forcedbins_filename\">&#x1F517;&#xFE0E;</a>`, default = ``\"\"``, type = string\n\n   -  path to a ``.json`` file that specifies bin upper bounds for some or all features\n\n   -  ``.json`` file should contain an array of objects, each containing the word ``feature`` (integer feature index) and ``bin_upper_bound`` (array of thresholds for binning)\n\n   -  see `this file <https://github.com/microsoft/LightGBM/blob/master/examples/regression/forced_bins.json>`__ as an example\n```\n\n----------------------------------------\n\nTITLE: Loading a Saved LightGBM Model\nDESCRIPTION: Loads a previously saved LightGBM model from a file using the `lgb.Booster` class. This allows using a trained model for predictions or further analysis without retraining.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Python-Intro.rst#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nbst = lgb.Booster(model_file='model.txt')  # init model\n```\n\n----------------------------------------\n\nTITLE: CUDA Support Configuration\nDESCRIPTION: Configures CUDA architectures and enables separable compilation for 'lightgbm_objs' and '_lightgbm'. Additionally, sets properties for the 'lightgbm' target if building CLI, to support CUDA device code compilation and symbol resolution.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_46\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_CUDA)\n\n  set_target_properties(\n    lightgbm_objs\n    PROPERTIES\n      CUDA_ARCHITECTURES \"${CUDA_ARCHS}\"\n      CUDA_SEPARABLE_COMPILATION ON\n  )\n\n  set_target_properties(\n    _lightgbm\n    PROPERTIES\n      CUDA_ARCHITECTURES \"${CUDA_ARCHS}\"\n      CUDA_RESOLVE_DEVICE_SYMBOLS ON\n  )\n\n  if(BUILD_CLI)\n    set_target_properties(\n      lightgbm\n      PROPERTIES\n        CUDA_ARCHITECTURES \"${CUDA_ARCHS}\"\n        CUDA_RESOLVE_DEVICE_SYMBOLS ON\n    )\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing LightGBM from GitHub\nDESCRIPTION: These commands clone the LightGBM repository from GitHub, navigate to the LightGBM directory, and execute the build-python.sh script to build and install LightGBM. All requirements from building from sources apply.  sudo may be needed to perform the command.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/python-package/README.rst#_snippet_15\n\nLANGUAGE: sh\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM.git\ncd LightGBM\n# export CXX=g++-14 CC=gcc-14  # macOS users, if you decided to compile with gcc, don't forget to specify compilers\nsh ./build-python.sh install\n```\n\n----------------------------------------\n\nTITLE: Setting Weights on Existing LightGBM Dataset\nDESCRIPTION: Illustrates how to set sample weights on a LightGBM Dataset object after it has been created using the `set_weight()` method. This is an alternative to setting weights during initial construction.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Python-Intro.rst#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntrain_data = lgb.Dataset(data, label=label)\nrng = np.random.default_rng()\nw = rng.uniform(size=(500, ))\ntrain_data.set_weight(w)\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM with Time Costs Output\nDESCRIPTION: This command builds LightGBM with time costs output. This option makes LightGBM output time costs for different internal routines, useful for investigating and benchmarking performance.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/python-package/README.rst#_snippet_13\n\nLANGUAGE: sh\nCODE:\n```\npip install lightgbm --no-binary lightgbm --config-settings=cmake.define.USE_TIMETAG=ON\n```\n\n----------------------------------------\n\nTITLE: Understanding R Superassignment Operator\nDESCRIPTION: Explains the use of the <<- operator in R, which assigns a value to a variable in a parent environment, often used within functions or callbacks to modify variables outside the function's local scope. The text clarifies that its use here does not modify the global environment.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/R-package/cran-comments.md#_snippet_6\n\nLANGUAGE: R\nCODE:\n```\n<<-\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM Shared Library using MSBuild on Windows (sh)\nDESCRIPTION: Compiles the LightGBM dynamic library (`.dll`) directly on Windows using MSBuild and the provided solution file (`windows/LightGBM.sln`). Specifies configuration (DLL), platform (x64), and PlatformToolset (v143). This is an alternative method if `build-python.sh` fails.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/python-package/README.rst#_snippet_23\n\nLANGUAGE: sh\nCODE:\n```\nMSBuild.exe windows/LightGBM.sln /p:Configuration=DLL /p:Platform=x64 /p:PlatformToolset=v143\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM Python Package with GPU Support (sh)\nDESCRIPTION: Installs the LightGBM Python package with general GPU support (e.g., OpenCL) using the `build-python.sh` script. Requires dependencies specified in the 'Build GPU Version' section.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/python-package/README.rst#_snippet_17\n\nLANGUAGE: sh\nCODE:\n```\nsh ./build-python.sh install --gpu\n```\n\n----------------------------------------\n\nTITLE: Defining Project and Setting C++ Standard in CMake\nDESCRIPTION: Declares the CMake project named 'lightgbm' with C and CXX (C++) as its languages. It conditionally sets the required C++ standard (`CMAKE_CXX_STANDARD`) to C++14 if C++ tests (`BUILD_CPP_TEST`) are enabled, otherwise defaults to C++11. `CMAKE_CXX_STANDARD_REQUIRED` is set to ON to enforce the standard. It also appends a local directory to the CMake module path.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nproject(lightgbm LANGUAGES C CXX)\n\nif(BUILD_CPP_TEST)\n  set(CMAKE_CXX_STANDARD 14)\nelse()\n  set(CMAKE_CXX_STANDARD 11)\nendif()\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\n\nlist(APPEND CMAKE_MODULE_PATH \"${PROJECT_SOURCE_DIR}/cmake/modules\")\n```\n\n----------------------------------------\n\nTITLE: Machine List Configuration\nDESCRIPTION: This code snippet demonstrates the expected format for the `mlist.txt` file, which lists the IP addresses and ports of the machines participating in the distributed learning process. Each line contains the machine's IP address and the port number to be used for communication.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/examples/parallel_learning/README.md#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nmachine1_ip 12400\nmachine2_ip 12400\n```\n\n----------------------------------------\n\nTITLE: Removing MKL Optimizations from Conda (Bash)\nDESCRIPTION: A Conda command to remove packages including MKL optimizations from the active environment. MKL often includes conflicting OpenMP runtimes, and removing it is a straightforward method to resolve OpenMP conflict errors (OMP: Error #15) caused by Conda's MKL packages.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/FAQ.rst#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nconda install nomkl\n```\n\n----------------------------------------\n\nTITLE: Loading a Saved Dask LightGBM Model from Pickle in Python\nDESCRIPTION: Demonstrates how to load a previously saved Dask LightGBM model using pickle, enabling model inference or further use. Note that customized clients are not restored automatically.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parallel-Learning-Guide.rst#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport pickle\nwith open(\"dask-model.pkl\", \"rb\") as f:\n    dask_model = pickle.load(f)\n```\n\n----------------------------------------\n\nTITLE: Cloning and Building LightGBM with Clang on Linux - Shell\nDESCRIPTION: This shell snippet demonstrates building LightGBM on Linux with Clang and OpenMP. It sets the CXX and CC environment variables to use specific Clang versions, configures with CMake, and builds with parallel jobs. Required: Git, CMake, Clang, and OpenMP. Outputs go to the LightGBM folder. Change '14' in compiler variable names to match installed Clang version.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\nexport CXX=clang++-14 CC=clang-14  # replace \"14\" with version of Clang installed on your machine\ncmake -B build -S .\ncmake --build build -j4\n```\n\n----------------------------------------\n\nTITLE: OpenCL Integration with Dependencies\nDESCRIPTION: Sets up dependencies, includes, compile definitions, and linking for OpenCL support via the 'IntegratedOpenCL.cmake' script, enabling hardware acceleration with OpenCL while managing external dependencies.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_45\n\nLANGUAGE: CMake\nCODE:\n```\nif(__INTEGRATE_OPENCL)\n  add_dependencies(lightgbm_objs OpenCL Boost)\n  target_include_directories(lightgbm_objs PRIVATE ${INTEGRATED_OPENCL_INCLUDES})\n  target_compile_definitions(lightgbm_objs PRIVATE ${INTEGRATED_OPENCL_DEFINITIONS})\n  target_link_libraries(lightgbm_objs PUBLIC ${INTEGRATED_OPENCL_LIBRARIES} ${CMAKE_DL_LIBS})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Eigen Library Settings in CMake\nDESCRIPTION: Sets up Eigen library configuration by adding compiler definitions to restrict Eigen to MPL2-licensed components and disable parallelization.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_12\n\nLANGUAGE: CMake\nCODE:\n```\nadd_definitions(-DEIGEN_MPL2_ONLY)\nadd_definitions(-DEIGEN_DONT_PARALLELIZE)\n```\n\n----------------------------------------\n\nTITLE: Executing LightGBM for Prediction Using Configuration File\nDESCRIPTION: This command executes the LightGBM binary for prediction, utilizing the configuration in 'predict.conf'. It requires the training to be completed beforehand. The output will be the prediction results based on the trained model and provided data.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/examples/lambdarank/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n\"../../lightgbm\" config=predict.conf\n```\n\n----------------------------------------\n\nTITLE: Adding Eigen Library Include Directory in CMake\nDESCRIPTION: Specifies the location of the Eigen library, which is included as an external dependency within the project's source directory (`external_libs/eigen`). It sets the `EIGEN_DIR` variable to this path and then uses `include_directories()` to add this path to the compiler's include search paths, making Eigen headers accessible during compilation.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_11\n\nLANGUAGE: CMake\nCODE:\n```\nset(EIGEN_DIR \"${PROJECT_SOURCE_DIR}/external_libs/eigen\")\ninclude_directories(${EIGEN_DIR})\n```\n\n----------------------------------------\n\nTITLE: Python Package Installation Prefix Adjustment\nDESCRIPTION: Sets the installation prefix to 'lightgbm' when building the Python package, configuring installation paths for Python bindings.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_51\n\nLANGUAGE: CMake\nCODE:\n```\nif(__BUILD_FOR_PYTHON)\n    set(CMAKE_INSTALL_PREFIX \"lightgbm\")\nendif\n```\n\n----------------------------------------\n\nTITLE: Configuring MPI or Socket Communication in CMake\nDESCRIPTION: Sets up either MPI (Message Passing Interface) or socket-based communication for distributed computing based on user preference.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_16\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_MPI)\n    find_package(MPI REQUIRED)\n    add_definitions(-DUSE_MPI)\nelse()\n    add_definitions(-DUSE_SOCKET)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running R CMD Check with DontRun Examples\nDESCRIPTION: Illustrates a modified command for R package checks that explicitly includes examples wrapped in \\dontrun directives. The text notes this command is used in continuous integration to verify \\donttest examples.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/R-package/cran-comments.md#_snippet_9\n\nLANGUAGE: Shell\nCODE:\n```\nR CMD check --as-cran --run-dontrun\n```\n\n----------------------------------------\n\nTITLE: Adjusting Build Options for Language Bindings in CMake\nDESCRIPTION: Modifies default build behavior when building specifically for language interfaces (Python, R, or SWIG). If any of `__BUILD_FOR_PYTHON`, `__BUILD_FOR_R`, or `USE_SWIG` are true, it disables building the command-line interface (`BUILD_CLI`) and prevents installing LightGBM headers (`INSTALL_HEADERS`) system-wide, assuming packaging systems will handle these.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nif(__BUILD_FOR_PYTHON OR __BUILD_FOR_R OR USE_SWIG)\n    # the SWIG wrapper, the Python and R packages don't require the CLI\n    set(BUILD_CLI OFF)\n    # installing the SWIG wrapper, the R and Python packages shouldn't place LightGBM's headers\n    # outside of where the package is installed\n    set(INSTALL_HEADERS OFF)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Loading Libraries for R Package Tests\nDESCRIPTION: Standard R code snippet typically found at the beginning of test scripts (e.g., using `testthat`) to load the necessary testing framework and the package being tested before running tests.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/R-package/cran-comments.md#_snippet_11\n\nLANGUAGE: R\nCODE:\n```\nlibrary(testthat)\nlibrary(lightgbm)\n```\n\n----------------------------------------\n\nTITLE: Configuring GPU Support in CMake\nDESCRIPTION: Sets up dependencies for GPU acceleration in LightGBM, including Boost Compute, OpenCL, and Boost libraries with special handling for Windows.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_19\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_GPU)\n    set(BOOST_COMPUTE_HEADER_DIR ${PROJECT_SOURCE_DIR}/external_libs/compute/include)\n    include_directories(${BOOST_COMPUTE_HEADER_DIR})\n    find_package(OpenCL REQUIRED)\n    include_directories(${OpenCL_INCLUDE_DIRS})\n    message(STATUS \"OpenCL include directory: \" ${OpenCL_INCLUDE_DIRS})\n    if(WIN32)\n        set(Boost_USE_STATIC_LIBS ON)\n    endif()\n    find_package(Boost 1.56.0 COMPONENTS filesystem system REQUIRED)\n    if(WIN32)\n        # disable autolinking in boost\n        add_definitions(-DBOOST_ALL_NO_LIB)\n    endif()\n    include_directories(${Boost_INCLUDE_DIRS})\n    add_definitions(-DUSE_GPU)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building Java Wrapper of LightGBM using Apple Clang on macOS (shell)\nDESCRIPTION: This snippet provides the commands to build the Java wrapper on macOS with Apple Clang compiler. Required dependencies include Homebrew-installed CMake, OpenJDK (with JAVA_HOME set), SWIG, and OpenMP libraries. Following dependency installation, the repo is cloned, configured with SWIG enabled, and compiled to produce the Java wrapper artifacts in the build folder.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_34\n\nLANGUAGE: shell\nCODE:\n```\nbrew install cmake openjdk swig libomp\nexport JAVA_HOME=\"$(brew --prefix openjdk)/libexec/openjdk.jdk/Contents/Home/\"\n\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\ncmake -B build -S . -DUSE_SWIG=ON\ncmake --build build -j4\n```\n\n----------------------------------------\n\nTITLE: Cloning and Building LightGBM with Apple Clang on macOS - Shell\nDESCRIPTION: This sequence demonstrates how to clone the LightGBM repository and compile it using the default (Apple Clang) compiler on macOS. It assumes required dependencies (CMake, OpenMP libraries via Homebrew) are already installed. Build output includes executables and dynamic libraries in the repository folder.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\ncmake -B build -S .\ncmake --build build -j4\n```\n\n----------------------------------------\n\nTITLE: Configuring CUDA Support in CMake\nDESCRIPTION: Sets up CUDA language support and enforces OpenMP requirement when CUDA is enabled.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_17\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_CUDA)\n    set(CMAKE_CUDA_HOST_COMPILER \"${CMAKE_CXX_COMPILER}\")\n    enable_language(CUDA)\n    set(USE_OPENMP ON CACHE BOOL \"CUDA requires OpenMP\" FORCE)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Conditional Output Name Setting for MSVC\nDESCRIPTION: Sets the output name of '_lightgbm' to 'lib_lightgbm' when using MSVC compiler and not building for R. Customizes build output for specific compiler environments.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_40\n\nLANGUAGE: CMake\nCODE:\n```\nif(MSVC AND NOT __BUILD_FOR_R)\n  set_target_properties(_lightgbm PROPERTIES OUTPUT_NAME \"lib_lightgbm\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Enabled Sanitizers in CMake\nDESCRIPTION: Defines the `ENABLED_SANITIZERS` cached string variable, allowing users to specify a semicolon-separated list of sanitizers (e.g., 'address;leak') to be used when the `USE_SANITIZER` option is enabled. Supported sanitizers include address, leak, undefined, and thread. This variable is used later to activate the chosen sanitizers.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(\n  ENABLED_SANITIZERS\n  \"address\" \"leak\" \"undefined\"\n  CACHE\n  STRING\n  \"Semicolon separated list of sanitizer names, e.g., 'address;leak'. \\\nSupported sanitizers are address, leak, undefined and thread.\"\n)\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM (MPI) - macOS - Apple Clang\nDESCRIPTION: Builds LightGBM on macOS with MPI support using CMake and Apple Clang.  Clones the LightGBM repo, configures the build enabling MPI, and then builds the project. Assumes that CMake, OpenMP, and Open MPI are installed via Homebrew.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_21\n\nLANGUAGE: sh\nCODE:\n```\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\ncmake -B build -S . -DUSE_MPI=ON\ncmake --build build -j4\n```\n\n----------------------------------------\n\nTITLE: Checking for Hardware Prefetching Support in CMake\nDESCRIPTION: Checks if the compiler supports _mm_prefetch instruction for hardware prefetching optimization and adds appropriate definitions if available.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_23\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(CheckCXXSourceCompiles)\ncheck_cxx_source_compiles(\"\n#include <xmmintrin.h>\nint main() {\n  int a = 0;\n  _mm_prefetch(&a, _MM_HINT_NTA);\n  return 0;\n}\n\" MM_PREFETCH)\n\nif(${MM_PREFETCH})\n  message(STATUS \"Using _mm_prefetch\")\n  add_definitions(-DMM_PREFETCH)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Node Weight Smoothing - LightGBM\nDESCRIPTION: This snippet describes the smoothing applied to tree nodes using the 'path_smooth' parameter. It presents the formula for calculating the smoothed node weight: w * (n / path_smooth) / (n / path_smooth + 1) + w_p / (n / path_smooth + 1), explaining the variables and their roles.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parameters.rst#_snippet_8\n\n\n\n----------------------------------------\n\nTITLE: Including Directories\nDESCRIPTION: Includes the LightGBM header directory. Also, if USE_MPI is defined, includes the MPI include path to build the MPI functionalities.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_31\n\nLANGUAGE: CMake\nCODE:\n```\ninclude_directories(${LightGBM_HEADER_DIR})\n\nif(USE_MPI)\n  include_directories(${MPI_CXX_INCLUDE_PATH})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Forcing Windows SDK Version for Visual Studio Generators in CMake\nDESCRIPTION: Checks if the selected CMake generator matches \"Visual Studio\". If it does, this block forces the CMake system version (`CMAKE_SYSTEM_VERSION`) to 10.0, explicitly targeting the Windows 10 SDK. This prevents potential fallback to older SDK versions discovered through mechanisms like outdated registry entries.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif(CMAKE_GENERATOR MATCHES \"Visual Studio\")\n    set(CMAKE_SYSTEM_VERSION 10.0 CACHE INTERNAL \"target Windows SDK version\" FORCE)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Restarting the Server After Driver Installation\nDESCRIPTION: After installing the NVIDIA drivers, it's necessary to restart the server to apply the changes. The command `init 6` initiates a system reboot. The server should be accessible again after approximately 30 seconds.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/GPU-Tutorial.rst#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\nsudo init 6\n```\n\n----------------------------------------\n\nTITLE: Installation of LightGBM CLI Executable\nDESCRIPTION: Defines the installation path for the 'lightgbm' target executable when building the command-line interface version, facilitating deployment.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_50\n\nLANGUAGE: CMake\nCODE:\n```\nif(BUILD_CLI)\n    install(\n      TARGETS lightgbm\n      RUNTIME DESTINATION ${CMAKE_INSTALL_PREFIX}/bin\n    )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Platform-Specific Compiler Flags in CMake\nDESCRIPTION: Sets up compiler flags for Unix-like systems (including MinGW and Cygwin) with special handling for debug builds, SWIG integration, OpenMP, and R-specific settings.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_25\n\nLANGUAGE: CMake\nCODE:\n```\nif(UNIX OR MINGW OR CYGWIN)\n  set(\n    CMAKE_CXX_FLAGS\n    \"${CMAKE_CXX_FLAGS} -pthread -Wextra -Wall -Wno-ignored-attributes -Wno-unknown-pragmas -Wno-return-type\"\n  )\n  if(MINGW)\n    # ignore this warning: https://gcc.gnu.org/bugzilla/show_bug.cgi?id=95353\n    set(\n      CMAKE_CXX_FLAGS\n      \"${CMAKE_CXX_FLAGS} -Wno-stringop-overflow\"\n    )\n  endif()\n  if(USE_DEBUG)\n      set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -g -O0\")\n  else()\n      set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -O3\")\n  endif()\n  if(USE_SWIG)\n      set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fno-strict-aliasing\")\n  endif()\n  if(NOT USE_OPENMP)\n      set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wno-unknown-pragmas -Wno-unused-private-field\")\n  endif()\n  if(__BUILD_FOR_R AND CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\")\n      set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wno-cast-function-type\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Compiler Flags for Non-MSVC\nDESCRIPTION: This snippet sets compiler flags for compilers other than MSVC. It adds `-fPIC` if a static library isn't being built and `-funroll-loops` if debug mode is not enabled. These flags optimize code generation and improve performance.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_29\n\nLANGUAGE: CMake\nCODE:\n```\nelse()\n    if(NOT BUILD_STATIC_LIB)\n      set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fPIC\")\n    endif()\n    if(NOT USE_DEBUG)\n      set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -funroll-loops\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenMP Library Path for macOS in LightGBM\nDESCRIPTION: Sets up proper RPATH handling for OpenMP libraries on macOS when building LightGBM as a shared library. This ensures the library can find the correct OpenMP implementation at runtime, avoiding conflicts between different OpenMP implementations.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_53\n\nLANGUAGE: CMake\nCODE:\n```\nif(APPLE AND USE_OPENMP AND NOT BUILD_STATIC_LIB)\n  # store path to {libgomp,libiomp,libomp}.dylib found at build time in a variable\n  get_target_property(\n    OpenMP_LIBRARY_LOCATION\n    OpenMP::OpenMP_CXX\n    INTERFACE_LINK_LIBRARIES\n  )\n  # get just the filename of that path\n  # (to deal with the possibility that it might be 'libomp.dylib' or 'libgomp.dylib' or 'libiomp.dylib')\n  get_filename_component(\n    OpenMP_LIBRARY_NAME\n    ${OpenMP_LIBRARY_LOCATION}\n    NAME\n  )\n  # get directory of that path\n  get_filename_component(\n    OpenMP_LIBRARY_DIR\n    ${OpenMP_LIBRARY_LOCATION}\n    DIRECTORY\n  )\n  # get exact name of the library in a variable\n  get_target_property(\n    __LIB_LIGHTGBM_OUTPUT_NAME\n    _lightgbm\n    OUTPUT_NAME\n  )\n  if(NOT __LIB_LIGHTGBM_OUTPUT_NAME)\n    set(__LIB_LIGHTGBM_OUTPUT_NAME \"lib_lightgbm\")\n  endif()\n\n  if(CMAKE_SHARED_LIBRARY_SUFFIX_CXX)\n    set(\n      __LIB_LIGHTGBM_FILENAME \"${__LIB_LIGHTGBM_OUTPUT_NAME}${CMAKE_SHARED_LIBRARY_SUFFIX_CXX}\"\n      CACHE INTERNAL \"lightgbm shared library filename\"\n    )\n  else()\n    set(\n      __LIB_LIGHTGBM_FILENAME \"${__LIB_LIGHTGBM_OUTPUT_NAME}.dylib\"\n      CACHE INTERNAL \"lightgbm shared library filename\"\n    )\n  endif()\n\n  # Override the absolute path to OpenMP with a relative one using @rpath.\n  #\n  # This also ensures that if a {libgomp,libiomp,libomp}.dylib has already been loaded, it'll just use that.\n  add_custom_command(\n    TARGET _lightgbm\n    POST_BUILD\n      COMMAND\n        install_name_tool\n        -change\n        ${OpenMP_LIBRARY_LOCATION}\n        \"@rpath/${OpenMP_LIBRARY_NAME}\"\n        \"${__LIB_LIGHTGBM_FILENAME}\"\n      WORKING_DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}\n      COMMENT \"Replacing hard-coded OpenMP install_name with '@rpath/${OpenMP_LIBRARY_NAME}'...\"\n  )\n```\n\n----------------------------------------\n\nTITLE: Manually Creating OpenMP Library Aliases (Bash)\nDESCRIPTION: A bash command workaround for macOS Homebrew/Conda users facing OpenMP conflicts with version 8.0.0+. It manually creates symlinks for common OpenMP library aliases (libgomp.dylib, libiomp5.dylib, libomp.dylib) in the Conda environment's lib directory, as automatic alias installation is disabled in newer libomp versions.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/FAQ.rst#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nfor LIBOMP_ALIAS in libgomp.dylib libiomp5.dylib libomp.dylib; do sudo ln -sf \"$(brew --cellar libomp)\"/*/lib/libomp.dylib $CONDA_PREFIX/lib/$LIBOMP_ALIAS; done\n```\n\n----------------------------------------\n\nTITLE: Num Iteration Predict Parameter Description\nDESCRIPTION: The `num_iteration_predict` parameter sets the number of trained iterations used for prediction within the `prediction` task. Values less than or equal to `0` indicate no limit.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parameters.rst#_snippet_20\n\nLANGUAGE: text\nCODE:\n```\n-  ``num_iteration_predict`` :raw-html:`<a id=\"num_iteration_predict\" title=\"Permalink to this parameter\" href=\"#num_iteration_predict\">&#x1F517;&#xFE0E;</a>`, default = ``-1``, type = int\n\n   -  used only in ``prediction`` task\n\n   -  used to specify how many trained iterations will be used in prediction\n\n   -  ``<= 0`` means no limit\n```\n\n----------------------------------------\n\nTITLE: Installing LightGBM via MacPorts on macOS - Shell\nDESCRIPTION: This shell command installs LightGBM via the MacPorts package manager. It requires MacPorts to be installed. LightGBM port is maintained outside the main LightGBM project, and support may vary. All dependencies are managed by MacPorts.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nsudo port install LightGBM\n```\n\n----------------------------------------\n\nTITLE: Building LightGBM with MinGW-w64 on Windows\nDESCRIPTION: This command builds LightGBM using MinGW-w64 on Windows. It is recommended to use Visual Studio instead due to its better multithreading efficiency. MinGW-w64 should be installed before running this command.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/python-package/README.rst#_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\npip install lightgbm --no-binary lightgbm --config-settings=cmake.define.CMAKE_SH=CMAKE_SH-NOTFOUND --config-settings=cmake.args=\"-GMinGW Makefiles\"\n```\n\n----------------------------------------\n\nTITLE: Start Iteration Predict Parameter Description\nDESCRIPTION: The `start_iteration_predict` parameter specifies from which iteration to start the prediction process, within the `prediction` task. A value of `0` or less indicates starting from the first iteration.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parameters.rst#_snippet_19\n\nLANGUAGE: text\nCODE:\n```\n-  ``start_iteration_predict`` :raw-html:`<a id=\"start_iteration_predict\" title=\"Permalink to this parameter\" href=\"#start_iteration_predict\">&#x1F517;&#xFE0E;</a>`, default = ``0``, type = int\n\n   -  used only in ``prediction`` task\n\n   -  used to specify from which iteration to start the prediction\n\n   -  ``<= 0`` means from the first iteration\n```\n\n----------------------------------------\n\nTITLE: Save Binary Parameter Description\nDESCRIPTION: The `save_binary` parameter, when set to `true`, saves the dataset (including validation data) to a binary file for faster data loading in subsequent runs. This feature is only available in the CLI version. The `init_score` is not saved. This is a boolean and aliases to `is_save_binary` and `is_save_binary_file`.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parameters.rst#_snippet_16\n\nLANGUAGE: text\nCODE:\n```\n``save_binary`` :raw-html:`<a id=\"save_binary\" title=\"Permalink to this parameter\" href=\"#save_binary\">&#x1F517;&#xFE0E;</a>`, default = ``false``, type = bool, aliases: ``is_save_binary``, ``is_save_binary_file``\n\n   -  if ``true``, LightGBM will save the dataset (including validation data) to a binary file. This speed ups the data loading for the next time\n\n   -  **Note**: ``init_score`` is not saved in binary file\n\n   -  **Note**: can be used only in CLI version; for language-specific packages you can use the correspondent function\n```\n\n----------------------------------------\n\nTITLE: Linking Public Dependencies for _lightgbm Target\nDESCRIPTION: Sets the target link libraries for the '_lightgbm' target to include 'lightgbm_capi_objs' and 'lightgbm_objs'. Ensures proper linkage of core components and dependencies required for building the main library.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_39\n\nLANGUAGE: CMake\nCODE:\n```\ntarget_link_libraries(_lightgbm PUBLIC lightgbm_capi_objs lightgbm_objs)\n```\n\n----------------------------------------\n\nTITLE: Overview of LightGBM with Kubeflow and External ML Frameworks\nDESCRIPTION: Describes how to run LightGBM models within Kubeflow using XGBoost Operator and provides links to example workflows. Mentions that these integrations are maintained by external projects and not the LightGBM maintainers.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parallel-Learning-Guide.rst#_snippet_12\n\n\n\n----------------------------------------\n\nTITLE: Building Threadless LightGBM Version\nDESCRIPTION: This command builds a threadless version of LightGBM by disabling OpenMP. It is strongly discouraged to use threadless LightGBM. All other requirements from building from sources apply.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/python-package/README.rst#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\npip install lightgbm --no-binary lightgbm --config-settings=cmake.define.USE_OPENMP=OFF\n```\n\n----------------------------------------\n\nTITLE: Displaying LightGBM Categorical Feature Warnings (Console)\nDESCRIPTION: Shows the console output warnings encountered when providing categorical features with values exceeding the Int32 limit. These warnings indicate that LightGBM detected negative or constant values during processing, likely due to the large original integer values.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/FAQ.rst#_snippet_0\n\nLANGUAGE: console\nCODE:\n```\n[LightGBM] [Warning] Met negative value in categorical features, will convert it to NaN\n[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n```\n\n----------------------------------------\n\nTITLE: Building C++ Unit Tests of LightGBM using gcc on macOS (shell)\nDESCRIPTION: Steps to build the LightGBM C++ unit tests on macOS using gcc. Prerequisites include Homebrew installed CMake and gcc compilers. After cloning the repository and setting appropriate compiler versions, CMake is configured to build tests and compilation is done with multiple threads. Output executable is located in the project folder.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_41\n\nLANGUAGE: shell\nCODE:\n```\nbrew install cmake gcc\n\ngit clone --recursive https://github.com/microsoft/LightGBM\ncd LightGBM\nexport CXX=g++-7 CC=gcc-7  # replace \"7\" with version of gcc installed on your machine\ncmake -B build -S . -DBUILD_CPP_TEST=ON\ncmake --build build --target testlightgbm -j4\n```\n\n----------------------------------------\n\nTITLE: Importing NumPy in Python\nDESCRIPTION: Imports the NumPy library, commonly aliased as 'np'. NumPy is frequently used for handling numerical data and is a dependency or prerequisite for many data loading and manipulation examples in the LightGBM guide.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Python-Intro.rst#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n```\n\n----------------------------------------\n\nTITLE: Configuring MSVC Runtime Library for C++ Tests\nDESCRIPTION: Sets Microsoft Visual C++ Runtime Library to static linking mode when building C++ tests.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_21\n\nLANGUAGE: CMake\nCODE:\n```\nif(BUILD_CPP_TEST AND MSVC)\n  # Use /MT flag to statically link the C runtime\n  set(CMAKE_MSVC_RUNTIME_LIBRARY \"MultiThreaded$<$<CONFIG:Debug>:Debug>\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building 32-bit LightGBM Version\nDESCRIPTION: This command builds a 32-bit version of LightGBM with 32-bit Python. Compilation with MinGW-w64 is not supported on Windows. 32-bit version is not supported on macOS and Linux. It is strongly discouraged to use this version of LightGBM.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/python-package/README.rst#_snippet_12\n\nLANGUAGE: sh\nCODE:\n```\npip install lightgbm --no-binary lightgbm --config-settings=cmake.args=\"-AWin32\"\n```\n\n----------------------------------------\n\nTITLE: Platform-specific Library Linking for Windows\nDESCRIPTION: Links 'ws2_32' and 'iphlpapi' libraries for 'lightgbm_objs' when building on Windows with MinGW or Cygwin, supporting Windows socket and network API functionalities.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_47\n\nLANGUAGE: CMake\nCODE:\n```\nif(WIN32)\n    if(MINGW OR CYGWIN)\n      target_link_libraries(lightgbm_objs PUBLIC ws2_32 iphlpapi)\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Navigating to Workspace\nDESCRIPTION: This command navigates the user into the workspace which was prepared earlier, using the 'cd' command.  This is a prerequisite for the subsequent steps of cloning and building LightGBM.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/GPU-Tutorial.rst#_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\ncd /mnt/workspace\n```\n\n----------------------------------------\n\nTITLE: Distributed LightGBM with Mars Framework\nDESCRIPTION: Highlights that Mars, a tensor-based computing framework, supports distributed LightGBM training via `pymars`. It points to the Mars documentation for detailed examples and clarifies that this integration is maintained outside of LightGBM's official team.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parallel-Learning-Guide.rst#_snippet_18\n\n\n\n----------------------------------------\n\nTITLE: Formatting Citations in R DESCRIPTION\nDESCRIPTION: Provides examples of required formats for citing publications (DOI, arXiv, ISBN, HTTPS) in the DESCRIPTION file of an R package according to CRAN policies, including options for adding titles in quotes.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/R-package/cran-comments.md#_snippet_5\n\nLANGUAGE: Documentation\nCODE:\n```\nauthors (year) doi:...\nauthors (year) arXiv:...\nauthors (year, ISBN:...)\nauthors (year) https:...\n\"Title\"\n```\n\n----------------------------------------\n\nTITLE: Installing CMake, Open MPI and GCC - macOS\nDESCRIPTION: Installs CMake, Open MPI, and GCC on macOS using Homebrew. This is required for building LightGBM with MPI support using GCC.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Installation-Guide.rst#_snippet_22\n\nLANGUAGE: sh\nCODE:\n```\nbrew install cmake open-mpi gcc\n```\n\n----------------------------------------\n\nTITLE: Generating configure Script for LightGBM R-package Using Docker\nDESCRIPTION: This snippet uses Docker to generate the `configure` script required for building the LightGBM R-package on Linux or Mac by running a helper script inside an Ubuntu 22.04 container. It mounts the local LightGBM repository into the container and executes the `recreate-configure.sh` script, ensuring environment consistency and proper autoconf usage. This method is recommended if native Ubuntu 22.04 access is unavailable.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/R-package/README.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\ndocker run \\\n    --rm \\\n    -v $(pwd):/opt/LightGBM \\\n    -w /opt/LightGBM \\\n    ubuntu:22.04 \\\n    ./R-package/recreate-configure.sh\n```\n\n----------------------------------------\n\nTITLE: Defining API Sources\nDESCRIPTION: Defines the source files for the C API.  If `__BUILD_FOR_R` is enabled, it appends `src/lightgbm_R.cpp` to the list, which integrates LightGBM with the R package.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_35\n\nLANGUAGE: CMake\nCODE:\n```\nset(API_SOURCES \"src/c_api.cpp\")\n# Only build the R part of the library if building for\n# use with the R-package\nif(__BUILD_FOR_R)\n  list(APPEND API_SOURCES \"src/lightgbm_R.cpp\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Handling CMake Compiler Not Found Error with MinGW (Bash)\nDESCRIPTION: Shows the CMake error messages indicating that C and CXX compilers were not found when attempting to compile LightGBM using MinGW. This known CMake issue can often be resolved by rerunning the cmake command or upgrading CMake to version 3.17.0 or higher.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/FAQ.rst#_snippet_5\n\nLANGUAGE: Bash\nCODE:\n```\nCMake Error: CMAKE_C_COMPILER not set, after EnableLanguage\nCMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage\n```\n\n----------------------------------------\n\nTITLE: Predict Disable Shape Check Parameter Description\nDESCRIPTION: The `predict_disable_shape_check` parameter controls whether LightGBM throws an error when predicting on data with a different number of features than the training data.  By default, a fatal error is raised.  Setting this to `true` disables this check.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parameters.rst#_snippet_24\n\nLANGUAGE: text\nCODE:\n```\n-  ``predict_disable_shape_check`` :raw-html:`<a id=\"predict_disable_shape_check\" title=\"Permalink to this parameter\" href=\"#predict_disable_shape_check\">&#x1F517;&#xFE0E;</a>`, default = ``false``, type = bool\n\n   -  used only in ``prediction`` task\n\n   -  control whether or not LightGBM raises an error when you try to predict on data with a different number of features than the training data\n\n   -  if ``false`` (the default), a fatal error will be raised if the number of features in the dataset you predict on differs from the number seen during training\n```\n\n----------------------------------------\n\nTITLE: Creating LightGBM Configuration File\nDESCRIPTION: This creates a configuration file named `lightgbm_gpu.conf` using `cat` and the `<<EOF` redirection.  It sets various parameters such as `max_bin`, `num_leaves`, `num_iterations`, `learning_rate`, `device`, and GPU-specific IDs.  Crucially, it enables GPU usage via `device = gpu`.  Then,  appends number of threads to this config file using `echo \"num_threads=$(nproc)\"`.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/GPU-Tutorial.rst#_snippet_8\n\nLANGUAGE: Bash\nCODE:\n```\ncat > lightgbm_gpu.conf <<EOF\nmax_bin = 63\nnum_leaves = 255\nnum_iterations = 50\nlearning_rate = 0.1\ntree_learner = serial\ntask = train\nis_training_metric = false\nmin_data_in_leaf = 1\nmin_sum_hessian_in_leaf = 100\nndcg_eval_at = 1,3,5,10\ndevice = gpu\ngpu_platform_id = 0\ngpu_device_id = 0\nEOF\necho \"num_threads=$(nproc)\" >> lightgbm_gpu.conf\n```\n\n----------------------------------------\n\nTITLE: Setting Minimum Required CMake Version\nDESCRIPTION: Specifies the minimum version of CMake required to process this build script using `cmake_minimum_required()`. This ensures compatibility with necessary CMake features and commands used throughout the file. The minimum required version is set to 3.28.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.28)\n```\n\n----------------------------------------\n\nTITLE: Identifying Multiple OpenMP Library Conflicts (Console)\nDESCRIPTION: Displays a RuntimeWarning from `threadpoolctl` indicating that both Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') are loaded simultaneously in a Python environment. This incompatibility can lead to crashes or deadlocks on Linux.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/FAQ.rst#_snippet_6\n\nLANGUAGE: Console\nCODE:\n```\n/root/miniconda/envs/test-env/lib/python3.8/site-packages/threadpoolctl.py:546: RuntimeWarning:\nFound Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\nthe same time. Both libraries are known to be incompatible and this\ncan cause random crashes or deadlocks on Linux when loaded in the\nsame Python program.\nUsing threadpoolctl may cause crashes or deadlocks. For more\ninformation and possible workarounds, please see\n    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n```\n\n----------------------------------------\n\nTITLE: Using R Donttest Directive\nDESCRIPTION: Explains the purpose of the \\donttest directive in R package examples, indicating code that should not be run automatically during standard CRAN checks, typically due to runtime or external dependencies. The text discusses confusion around its execution behavior in recent R versions.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/R-package/cran-comments.md#_snippet_3\n\nLANGUAGE: R\nCODE:\n```\n\\donttest{}\n```\n\n----------------------------------------\n\nTITLE: Executing CRAN Standard Package Check\nDESCRIPTION: Shows the standard shell command used to perform comprehensive checks on an R package according to CRAN policies, identifying potential issues before submission.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/R-package/cran-comments.md#_snippet_8\n\nLANGUAGE: Shell\nCODE:\n```\nR CMD check --as-cran\n```\n\n----------------------------------------\n\nTITLE: Example R Submission Information\nDESCRIPTION: Illustrates the information provided during a LightGBM package submission to CRAN, including details about addressing valgrind check failures and potential false positives related to OpenMP.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/R-package/cran-comments.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nHello,\n\nI'm submitting {lightgbm} 3.1.0 on behalf of the maintainer, Guolin Ke. I am a co-author on the package, and he has asked me to handle this submission. We saw in https://cran.r-project.org/web/packages/policies.html#Submission that this is permitted.\n\n{lightgbm} was removed from CRAN in October for issues found by valgrind checks. We have invested significant effort in addressing those issues and creating an automatic test that tries to replicate CRAN's valgrind checks: https://github.com/microsoft/LightGBM/blob/742d72f8bb051105484fd5cca11620493ffb0b2b/.github/workflows/r_valgrind.yml.\n\nWe see two warnings from valgrind that we believe are not problematic.\n\n==2063== Conditional jump or move depends on uninitialised value(s)\n==2063==    at 0x49CF138: gregexpr_Regexc (grep.c:2439)\n==2063==    by 0x49D1F13: do_regexpr (grep.c:3100)\n==2063==    by 0x49A0058: bcEval (eval.c:7121)\n==2063==    by 0x498B67F: Rf_eval (eval.c:727)\n==2063==    by 0x498E414: R_execClosure (eval.c:1895)\n==2063==    by 0x498E0C7: Rf_applyClosure (eval.c:1821)\n==2063==    by 0x499FC8C: bcEval (eval.c:7089)\n==2063==    by 0x498B67F: Rf_eval (eval.c:727)\n==2063==    by 0x498B1CB: forcePromise (eval.c:555)\n==2063==    by 0x49963AB: FORCE_PROMISE (eval.c:5142)\n==2063==    by 0x4996566: getvar (eval.c:5183)\n==2063==    by 0x499D1A5: bcEval (eval.c:6873)\n==2063==  Uninitialised value was created by a stack allocation\n==2063==    at 0x49CEC37: gregexpr_Regexc (grep.c:2369)\n\nThis seems to be related to R itself and not any code in {lightgbm}.\n\n==2063== 336 bytes in 1 blocks are possibly lost in loss record 153 of 2,709\n==2063==    at 0x483DD99: calloc (in /usr/lib/x86_64-linux-gnu/valgrind/vgpreload_memcheck-amd64-linux.so)\n==2063==    by 0x40149CA: allocate_dtv (dl-tls.c:286)\n==2063==    by 0x40149CA: _dl_allocate_tls (dl-tls.c:532)\n==2063==    by 0x5702322: allocate_stack (allocatestack.c:622)\n==2063==    by 0x5702322: pthread_create@@GLIBC_2.2.5 (pthread_create.c:660)\n==2063==    by 0x56D0DDA: ??? (in /usr/lib/x86_64-linux-gnu/libgomp.so.1.0.0)\n==2063==    by 0x56C88E0: GOMP_parallel (in /usr/lib/x86_64-linux-gnu/libgomp.so.1.0.0)\n==2063==    by 0x1544D29C: LGBM_DatasetCreateFromCSC (c_api.cpp:1286)\n==2063==    by 0x1546F980: LGBM_DatasetCreateFromCSC_R (lightgbm_R.cpp:91)\n==2063==    by 0x4941E2F: R_doDotCall (dotcode.c:634)\n==2063==    by 0x494CCC6: do_dotcall (dotcode.c:1281)\n==2063==    by 0x499FB01: bcEval (eval.c:7078)\n==2063==    by 0x498B67F: Rf_eval (eval.c:727)\n==2063==    by 0x498E414: R_execClosure (eval.c:1895)\n\nWe believe this is a false positive, and related to a misunderstanding between valgrind and openmp (https://gcc.gnu.org/bugzilla/show_bug.cgi?id=36298).\n\nWe have also added automated tests with ASAN/UBSAN to our testing setup, and have checked the package on Solaris 10 and found no issues.\n\nThanks for your time and consideration.\n```\n\n----------------------------------------\n\nTITLE: Parser Config File Parameter Description\nDESCRIPTION: The `parser_config_file` parameter provides a path to a `.json` file for configuring a customized parser.  See `lightgbm-transform <https://github.com/microsoft/lightgbm-transform>`__ for usage examples.  Note that the `lightgbm-transform` project is not maintained by LightGBM's maintainers.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/Parameters.rst#_snippet_18\n\nLANGUAGE: text\nCODE:\n```\n``parser_config_file`` :raw-html:`<a id=\"parser_config_file\" title=\"Permalink to this parameter\" href=\"#parser_config_file\">&#x1F517;&#xFE0E;</a>`, default = ``\"\"``, type = string\n\n   -  path to a ``.json`` file that specifies customized parser initialized configuration\n\n   -  see `lightgbm-transform <https://github.com/microsoft/lightgbm-transform>`__ for usage examples\n\n   -  **Note**: ``lightgbm-transform`` is not maintained by LightGBM's maintainers. Bug reports or feature requests should go to `issues page <https://github.com/microsoft/lightgbm-transform/issues>`__\n\n   -  *New in version 4.0.0*\n```\n\n----------------------------------------\n\nTITLE: Manually Installing Older Matrix Package in R\nDESCRIPTION: Provides the R command to install a specific older version (1.6-5) of the `Matrix` package directly from the CRAN archive. This serves as a workaround for the dependency issue when using R versions older than 4.4.0 and encountering problems installing LightGBM.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/FAQ.rst#_snippet_10\n\nLANGUAGE: R\nCODE:\n```\ninstall.packages('https://cran.r-project.org/src/contrib/Archive/Matrix/Matrix_1.6-5.tar.gz', repos = NULL)\n```\n\n----------------------------------------\n\nTITLE: Configuring External Libraries in CMake\nDESCRIPTION: Sets up include directories for external libraries fast_double_parser and fmt used in the LightGBM project.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_13\n\nLANGUAGE: CMake\nCODE:\n```\nset(FAST_DOUBLE_PARSER_INCLUDE_DIR \"${PROJECT_SOURCE_DIR}/external_libs/fast_double_parser/include\")\ninclude_directories(${FAST_DOUBLE_PARSER_INCLUDE_DIR})\n\nset(FMT_INCLUDE_DIR \"${PROJECT_SOURCE_DIR}/external_libs/fmt/include\")\ninclude_directories(${FMT_INCLUDE_DIR})\n```\n\n----------------------------------------\n\nTITLE: Resolving Absolute Path Error During Python Installation (Console)\nDESCRIPTION: Displays the 'error: Error: setup script specifies an absolute path' encountered when installing older versions (<4.0.0) of the LightGBM Python package directly using `python setup.py install`. This indicates an issue with how paths were specified in the setup script.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/FAQ.rst#_snippet_11\n\nLANGUAGE: Console\nCODE:\n```\nerror: Error: setup script specifies an absolute path:\n/Users/Microsoft/LightGBM/python-package/lightgbm/../../lib_lightgbm.so\nsetup() arguments must *always* be /-separated paths relative to the setup.py directory, *never* absolute paths.\n```\n\n----------------------------------------\n\nTITLE: Creating Executable for CLI\nDESCRIPTION: Creates an executable `lightgbm` if `BUILD_CLI` is enabled. The executable links against the object library `lightgbm_objs`. It builds the command-line interface.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_34\n\nLANGUAGE: CMake\nCODE:\n```\nif(BUILD_CLI)\n    add_executable(lightgbm src/main.cpp src/application/application.cpp)\n    target_link_libraries(lightgbm PRIVATE lightgbm_objs)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Conditional MPI Library Linking\nDESCRIPTION: Adds MPI C++ libraries to 'lightgbm_objs' target if USE_MPI option is enabled, integrating MPI support into the build for parallel processing across distributed systems.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_42\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_MPI)\n  target_link_libraries(lightgbm_objs PUBLIC ${MPI_CXX_LIBRARIES})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenMP Support in CMake with macOS Fallback\nDESCRIPTION: Finds and configures OpenMP with special handling for macOS, including a Homebrew fallback option for libomp when the standard OpenMP package is not found.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_18\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_OPENMP)\n    if(APPLE)\n        find_package(OpenMP)\n        if(NOT OpenMP_FOUND)\n            if(USE_HOMEBREW_FALLBACK)\n                # libomp 15.0+ from brew is keg-only, so have to search in other locations.\n                # See https://github.com/Homebrew/homebrew-core/issues/112107#issuecomment-1278042927.\n                execute_process(COMMAND brew --prefix libomp\n                            OUTPUT_VARIABLE HOMEBREW_LIBOMP_PREFIX\n                            OUTPUT_STRIP_TRAILING_WHITESPACE)\n                set(OpenMP_C_FLAGS \"-Xpreprocessor -fopenmp -I${HOMEBREW_LIBOMP_PREFIX}/include\")\n                set(OpenMP_CXX_FLAGS \"-Xpreprocessor -fopenmp -I${HOMEBREW_LIBOMP_PREFIX}/include\")\n                set(OpenMP_C_LIB_NAMES omp)\n                set(OpenMP_CXX_LIB_NAMES omp)\n                set(OpenMP_omp_LIBRARY ${HOMEBREW_LIBOMP_PREFIX}/lib/libomp.dylib)\n            endif()\n            find_package(OpenMP REQUIRED)\n        endif()\n    else()\n        find_package(OpenMP REQUIRED)\n    endif()\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${OpenMP_CXX_FLAGS}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Debug and TimeTag Options in CMake\nDESCRIPTION: Adds compiler definitions for debug and time tagging features based on user-specified options.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_15\n\nLANGUAGE: CMake\nCODE:\n```\nif(USE_TIMETAG)\n    add_definitions(-DTIMETAG)\nendif()\n\nif(USE_DEBUG)\n    add_definitions(-DDEBUG)\nendif()\n```\n\n----------------------------------------\n\nTITLE: R Support Linking for __BUILD_FOR_R\nDESCRIPTION: Links R library objects to 'lightgbm_objs' and 'lightgbm_capi_objs' when building R package support, selecting the appropriate R library based on compiler, ensuring R headers and functionalities are accessible.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_48\n\nLANGUAGE: CMake\nCODE:\n```\nif(__BUILD_FOR_R)\n  if(MSVC)\n    set(R_LIB ${LIBR_MSVC_CORE_LIBRARY})\n  else()\n    set(R_LIB ${LIBR_CORE_LIBRARY})\n  endif()\n  target_link_libraries(lightgbm_objs PUBLIC ${R_LIB})\n  target_link_libraries(lightgbm_capi_objs PUBLIC ${R_LIB})\n```\n\n----------------------------------------\n\nTITLE: Configuring R Integration in CMake\nDESCRIPTION: Sets up R integration for LightGBM by finding the LibR package, displaying R-related paths, and adding the appropriate include directories and definitions.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_14\n\nLANGUAGE: CMake\nCODE:\n```\nif(__BUILD_FOR_R)\n    find_package(LibR REQUIRED)\n    message(STATUS \"LIBR_EXECUTABLE: ${LIBR_EXECUTABLE}\")\n    message(STATUS \"LIBR_INCLUDE_DIRS: ${LIBR_INCLUDE_DIRS}\")\n    message(STATUS \"LIBR_LIBS_DIR: ${LIBR_LIBS_DIR}\")\n    message(STATUS \"LIBR_CORE_LIBRARY: ${LIBR_CORE_LIBRARY}\")\n    include_directories(${LIBR_INCLUDE_DIRS})\n    add_definitions(-DLGB_R_BUILD)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Installation Paths for LightGBM Library and Headers\nDESCRIPTION: Configures the installation paths for the LightGBM library binaries, headers, and dependencies. It ensures that libraries, executables, and include files are installed to the appropriate system locations.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_55\n\nLANGUAGE: CMake\nCODE:\n```\ninstall(\n  TARGETS _lightgbm\n  RUNTIME DESTINATION ${CMAKE_INSTALL_PREFIX}/bin\n  LIBRARY DESTINATION ${CMAKE_INSTALL_PREFIX}/lib\n  ARCHIVE DESTINATION ${CMAKE_INSTALL_PREFIX}/lib\n)\n\nif(INSTALL_HEADERS)\n    install(\n      DIRECTORY ${LightGBM_HEADER_DIR}/LightGBM\n      DESTINATION ${CMAKE_INSTALL_PREFIX}/include\n    )\n    install(\n      FILES ${FAST_DOUBLE_PARSER_INCLUDE_DIR}/fast_double_parser.h\n      DESTINATION ${CMAKE_INSTALL_PREFIX}/include/LightGBM/utils\n    )\n    install(\n      DIRECTORY ${FMT_INCLUDE_DIR}/\n      DESTINATION ${CMAKE_INSTALL_PREFIX}/include/LightGBM/utils\n      FILES_MATCHING PATTERN \"*.h\"\n    )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running Docker Container to Build LightGBM Documentation - Shell\nDESCRIPTION: This snippet provides a shell command to pull and run a Docker container using the Read the Docs Ubuntu image, configuring environment variables and volume mounts to build LightGBM's documentation locally. It ensures a controlled environment matching the CI system, mounts the project directory, sets working directory and environment variables, and executes a script to generate docs. No additional dependencies are required other than Docker installed locally. Inputs are environment settings and source files; output is the generated HTML documentation in the docs/_build/html directory.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/README.rst#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndocker run \\\n    --rm \\\n    --user=0 \\\n    -v $(pwd):/opt/LightGBM \\\n    --env C_API=true \\\n    --env CONDA=/opt/miniforge \\\n    --env READTHEDOCS=true \\\n    --workdir=/opt/LightGBM/docs \\\n    --entrypoint=\"\" \\\n    readthedocs/build:ubuntu-24.04-2024.06.17 \\\n    /bin/bash build-docs.sh\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies and Building LightGBM Docs Without Docker - Shell\nDESCRIPTION: This snippet installs required Python packages including Breathe, Sphinx, and the ReadTheDocs theme using pip, then triggers the Sphinx build process with make html in the docs directory. It assumes a typical Unix-like shell environment with Python and make installed. The input is the Python environment with shell access, and the output is the generated HTML documentation excluding R docs. This approach requires Doxygen installed separately if generating C code docs.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/docs/README.rst#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip install breathe sphinx 'sphinx_rtd_theme>=0.5'\nmake html\n```\n\n----------------------------------------\n\nTITLE: Removing Directory During Build Process\nDESCRIPTION: A shell command used in a build script to recursively remove the 'docs' directory, implemented as a workaround to prevent non-standard files from being included in the CRAN submission package due to an issue with .Rbuildignore.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/R-package/cran-comments.md#_snippet_13\n\nLANGUAGE: Shell\nCODE:\n```\nrm -r docs/\n```\n\n----------------------------------------\n\nTITLE: Running Address Sanitizer (ASAN) and Undefined Behavior Sanitizer (UBSAN) Tests with Docker\nDESCRIPTION: This collection of shell commands demonstrates how to run CRAN-mandated sanitizer tests for LightGBM using a Docker container with preconfigured R debugging environments. Users set the environment variable `R_CUSTOMIZATION` to either \"san\" (gcc-based) or \"csan\" (clang-based) to replicate sanitizer builds. Inside the container, dependencies are installed, the package is built and installed, then tests are executed with output logged to verify no runtime errors or undefined behaviors. This procedure aids maintainers in validating package robustness for CRAN submission.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/R-package/README.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\ndocker run \\\n  --rm \\\n  -it \\\n  -v $(pwd):/opt/LightGBM \\\n  -w /opt/LightGBM \\\n  --env R_CUSTOMIZATION=san \\\n  wch1/r-debug:latest \\\n  /bin/bash\n\n# install dependencies\nRDscript${R_CUSTOMIZATION} \\\n  -e \"install.packages(c('R6', 'data.table', 'jsonlite', 'knitr', 'markdown', 'Matrix', 'RhpcBLASctl', 'testthat'), repos = 'https://cran.r-project.org', Ncpus = parallel::detectCores())\"\n\n# install lightgbm\nsh build-cran-package.sh --r-executable=RD${R_CUSTOMIZATION}\nRD${R_CUSTOMIZATION} \\\n  CMD INSTALL lightgbm_*.tar.gz\n\n# run tests\ncd R-package/tests\nrm -f ./tests.log\nRDscript${R_CUSTOMIZATION} testthat.R >> tests.log 2>&1\n\n# check that tests passed\necho \"test exit code: $?\"\ntail -300 ./tests.log\n```\n\n----------------------------------------\n\nTITLE: Setting Static Linking for MinGW on Windows in CMake\nDESCRIPTION: Configures static linking of the C++ standard library when building with MinGW on Windows to avoid runtime dependency issues.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_26\n\nLANGUAGE: CMake\nCODE:\n```\nif(WIN32 AND MINGW)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -static-libstdc++\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Checking for Aligned Memory Allocation Support in CMake\nDESCRIPTION: Checks if the compiler supports _mm_malloc/_mm_free for aligned memory allocation and adds appropriate definitions if available.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_24\n\nLANGUAGE: CMake\nCODE:\n```\ninclude(CheckCXXSourceCompiles)\ncheck_cxx_source_compiles(\"\n#include <mm_malloc.h>\nint main() {\n  char *a = (char*)_mm_malloc(8, 16);\n  _mm_free(a);\n  return 0;\n}\n\" MM_MALLOC)\n\nif(${MM_MALLOC})\n  message(STATUS \"Using _mm_malloc\")\n  add_definitions(-DMM_MALLOC)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Handling Integrated OpenCL Build Option in CMake\nDESCRIPTION: Checks if the internal `__INTEGRATE_OPENCL` option is set (potentially by a parent CMake script). If true, it forces this option ON in the CMake cache, disables GPU support (`USE_GPU`) by forcing it OFF, and prints a status message confirming that the library is being built with integrated OpenCL components.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nif(__INTEGRATE_OPENCL)\n  set(__INTEGRATE_OPENCL ON CACHE BOOL \"\" FORCE)\n  set(USE_GPU OFF CACHE BOOL \"\" FORCE)\n  message(STATUS \"Building library with integrated OpenCL components\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Project Directories\nDESCRIPTION: Sets the include directory and output paths for the project.  `LightGBM_HEADER_DIR` is set to include directory. `EXECUTABLE_OUTPUT_PATH` and `LIBRARY_OUTPUT_PATH` are set to the source directory.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_30\n\nLANGUAGE: CMake\nCODE:\n```\nset(LightGBM_HEADER_DIR ${PROJECT_SOURCE_DIR}/include)\n\nset(EXECUTABLE_OUTPUT_PATH ${PROJECT_SOURCE_DIR})\nset(LIBRARY_OUTPUT_PATH ${PROJECT_SOURCE_DIR})\n```\n\n----------------------------------------\n\nTITLE: Creating Object Library\nDESCRIPTION: Creates an object library `lightgbm_objs` from the sources defined in `LGBM_SOURCES`. This is an intermediate step to handle source files before creating the final library or executable.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_33\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(lightgbm_objs OBJECT ${LGBM_SOURCES})\n```\n\n----------------------------------------\n\nTITLE: Defining Source Files\nDESCRIPTION: Defines lists of source files for the LightGBM library. `LGBM_SOURCES` includes standard C++ source files. `LGBM_CUDA_SOURCES` includes CUDA-related source files. If `USE_CUDA` is enabled,  `LGBM_CUDA_SOURCES` is appended to `LGBM_SOURCES`.\nSOURCE: https://github.com/microsoft/lightgbm/blob/master/CMakeLists.txt#_snippet_32\n\nLANGUAGE: CMake\nCODE:\n```\nset(\n    LGBM_SOURCES\n      src/boosting/boosting.cpp\n      src/boosting/gbdt_model_text.cpp\n      src/boosting/gbdt_prediction.cpp\n      src/boosting/gbdt.cpp\n      src/boosting/prediction_early_stop.cpp\n      src/boosting/sample_strategy.cpp\n      src/io/bin.cpp\n      src/io/config_auto.cpp\n      src/io/config.cpp\n      src/io/dataset_loader.cpp\n      src/io/dataset.cpp\n      src/io/file_io.cpp\n      src/io/json11.cpp\n      src/io/metadata.cpp\n      src/io/parser.cpp\n      src/io/train_share_states.cpp\n      src/io/tree.cpp\n      src/metric/dcg_calculator.cpp\n      src/metric/metric.cpp\n      src/network/linker_topo.cpp\n      src/network/linkers_mpi.cpp\n      src/network/linkers_socket.cpp\n      src/network/network.cpp\n      src/objective/objective_function.cpp\n      src/treelearner/data_parallel_tree_learner.cpp\n      src/treelearner/feature_histogram.cpp\n      src/treelearner/feature_parallel_tree_learner.cpp\n      src/treelearner/gpu_tree_learner.cpp\n      src/treelearner/gradient_discretizer.cpp\n      src/treelearner/linear_tree_learner.cpp\n      src/treelearner/serial_tree_learner.cpp\n      src/treelearner/tree_learner.cpp\n      src/treelearner/voting_parallel_tree_learner.cpp\n      src/utils/openmp_wrapper.cpp\n)\nset(\n    LGBM_CUDA_SOURCES\n      src/boosting/cuda/cuda_score_updater.cpp\n      src/boosting/cuda/cuda_score_updater.cu\n      src/metric/cuda/cuda_binary_metric.cpp\n      src/metric/cuda/cuda_pointwise_metric.cpp\n      src/metric/cuda/cuda_regression_metric.cpp\n      src/metric/cuda/cuda_pointwise_metric.cu\n      src/objective/cuda/cuda_binary_objective.cpp\n      src/objective/cuda/cuda_multiclass_objective.cpp\n      src/objective/cuda/cuda_rank_objective.cpp\n      src/objective/cuda/cuda_regression_objective.cpp\n      src/objective/cuda/cuda_binary_objective.cu\n      src/objective/cuda/cuda_multiclass_objective.cu\n      src/objective/cuda/cuda_rank_objective.cu\n      src/objective/cuda/cuda_regression_objective.cu\n      src/treelearner/cuda/cuda_best_split_finder.cpp\n      src/treelearner/cuda/cuda_data_partition.cpp\n      src/treelearner/cuda/cuda_histogram_constructor.cpp\n      src/treelearner/cuda/cuda_leaf_splits.cpp\n      src/treelearner/cuda/cuda_single_gpu_tree_learner.cpp\n      src/treelearner/cuda/cuda_best_split_finder.cu\n      src/treelearner/cuda/cuda_data_partition.cu\n      src/treelearner/cuda/cuda_gradient_discretizer.cu\n      src/treelearner/cuda/cuda_histogram_constructor.cu\n      src/treelearner/cuda/cuda_leaf_splits.cu\n      src/treelearner/cuda/cuda_single_gpu_tree_learner.cu\n      src/io/cuda/cuda_column_data.cu\n      src/io/cuda/cuda_tree.cu\n      src/io/cuda/cuda_column_data.cpp\n      src/io/cuda/cuda_metadata.cpp\n      src/io/cuda/cuda_row_data.cpp\n      src/io/cuda/cuda_tree.cpp\n      src/cuda/cuda_utils.cpp\n      src/cuda/cuda_algorithms.cu\n)\n\nif(USE_CUDA)\n  list(APPEND LGBM_SOURCES ${LGBM_CUDA_SOURCES})\nendif()\n```"
  }
]
[
  {
    "owner": "zvictor",
    "repo": "brainyflow",
    "content": "TITLE: Implementing Text Summarization to YAML using Brainyflow Node in TypeScript\nDESCRIPTION: Defines a TypeScript class `SummarizeNode` extending `brainyflow.Node`. It retrieves text from memory (`prep`), constructs a prompt instructing an LLM to summarize the text into a specific YAML format, calls the LLM (via assumed `callLLM` function), extracts the YAML content from the response, parses it using an assumed `parseYaml` function, performs basic validation on the resulting object structure, handles potential errors during extraction or parsing, and returns the structured result (`exec`). The `post` method stores this structured result in memory. Requires the `brainyflow` library and assumed external async functions `callLLM` and `parseYaml`.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/structure.md#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory, Node } from 'brainyflow'\n\n// Assuming callLLM and a YAML parser are available\ndeclare function callLLM(prompt: string): Promise<string>\ndeclare function parseYaml(text: string): any\n\nclass SummarizeNode extends Node {\n  async prep(memory: Memory): Promise<string> {\n    // Assuming the text to summarize is in memory.text\n    return memory.text ?? ''\n  }\n\n  async exec(textToSummarize: string): Promise<any> {\n    if (!textToSummarize) return { summary: ['No text provided'] }\n\n    const prompt = `\nPlease summarize the following text as YAML, with exactly 3 bullet points:\n\n${textToSummarize}\n\nNow, output ONLY the YAML structure:\n\\`\\`\\`yaml\nsummary:\n  - bullet 1\n  - bullet 2\n  - bullet 3\n\\`\\`\\``\n\n    const response = await callLLM(prompt)\n    let structuredResult: any\n    try {\n      const yamlStr = response.split(/```(?:yaml)?/)[1]?.trim()\n      if (!yamlStr) throw new Error('No YAML block found')\n      structuredResult = parseYaml(yamlStr)\n\n      // Basic validation\n      if (!structuredResult?.summary || !Array.isArray(structuredResult.summary)) {\n        throw new Error('Invalid YAML structure: missing or non-array summary')\n      }\n    } catch (e: any) {\n      console.error('Failed to parse structured output:', e.message)\n      // Handle error, maybe return a default structure or re-throw\n      // Returning the raw response might be an option too\n      return { summary: [`Error parsing summary: ${e.message}`] }\n    }\n\n    return structuredResult // e.g., { summary: ['Point 1', 'Point 2', 'Point 3'] }\n  }\n\n  async post(memory: Memory, prepRes: any, execRes: any): Promise<void> {\n    // Store the structured result in memory\n    memory.structured_summary = execRes\n    console.log('Stored structured summary:', execRes)\n    // No trigger needed if this is the end of the flow/branch\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining QA Nodes with Asynchronous Lifecycle Methods (Python)\nDESCRIPTION: Implements the basic question-answering nodes using BrainyFlow's lifecycle: prep captures user input and stores it in memory, exec invokes an LLM utility function, and post records the result. Dependencies include brainyflow, an LLM client library as call_llm, and asyncio. The GetQuestionNode's prep acquires user input; AnswerNode's exec uses that input. Inputs are user questions, outputs are LLM answers stored in the shared memory object. Constraints: interactive usage (input()), and requires an async-capable environment.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/getting_started.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom brainyflow import Node, Flow, Memory\nfrom utils import call_llm  # Your LLM implementation\n\nclass GetQuestionNode(Node):\n    async def prep(self, memory: Memory):\n        \"\"\"Get text input from user.\"\"\"\n        memory.question = input(\"Enter your question: \")\n\nclass AnswerNode(Node):\n    async def prep(self, memory: Memory):\n        \"\"\"Extract the question from memory.\"\"\"\n        return memory.question\n\n    async def exec(self, question: str | None):\n        \"\"\"Call LLM to generate an answer.\"\"\"\n\n        prompt = f\"Answer the following question: {question}\"\n        return await call_llm(prompt)\n\n    async def post(self, memory: Memory, prep_res: str | None, exec_res: str):\n        \"\"\"Store the answer in memory.\"\"\"\n        memory.answer = exec_res\n        print(f\"AnswerNode: Stored answer '{exec_res}'\")\n```\n\n----------------------------------------\n\nTITLE: Composing Workflows by Nesting Brainyflow Flows in TypeScript\nDESCRIPTION: Illustrates how to use a `Flow` instance as a node within a parent `Flow` in TypeScript, enabling workflow composition. It defines multiple sub-flows (payment, inventory, shipping) and connects them sequentially using the `.next()` method to create a hierarchical order processing pipeline. Running the master `orderPipeline` executes the sub-flows sequentially.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/core_abstraction/flow.md#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Flow, Node } from 'brainyflow'\n\n// Payment processing sub-flow\nvalidatePayment.next(processPayment).next(paymentConfirmation)\nconst paymentFlow = new Flow(validatePayment)\n\n// Inventory sub-flow\ncheckStock.next(reserveItems).next(updateInventory)\nconst inventoryFlow = new Flow(checkStock)\n\n// Shipping sub-flow\ncreateLabel.next(assignCarrier).next(schedulePickup)\nconst shippingFlow = new Flow(createLabel)\n\npaymentFlow.next(inventoryFlow) // Default transition after paymentFlow completes\ninventoryFlow.next(shippingFlow) // Default transition after inventoryFlow completes\n\n// Create the master flow, starting with the paymentFlow\nconst orderPipeline = new Flow(paymentFlow)\n\n// --- Run the entire pipeline ---\n// const globalStore = { orderId: 'XYZ789', customerId: 'CUST123' };\n// await orderPipeline.run(globalStore);\n// console.log('Order pipeline completed. Final state:', globalStore);\n```\n\n----------------------------------------\n\nTITLE: Implementing MapReduce Document Summarization in Python using BrainyFlow\nDESCRIPTION: A complete implementation of a MapReduce pattern for document summarization using BrainyFlow. The code includes three main components: a TriggerSummariesNode (Mapper) that distributes work, a SummarizeFileNode (Processor) that handles individual document summarization, and a CombineSummariesNode (Reducer) that combines all summaries into a final result. Uses asynchronous processing and supports parallel execution through ParallelFlow.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/mapreduce.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom brainyflow import Node, Flow, Memory, ParallelFlow\n\n# Assume call_llm is defined elsewhere\n# async def call_llm(prompt: str) -> str: ...\n\n# 1. Mapper Node: Triggers processing for each file\nclass TriggerSummariesNode(Node):\n    async def prep(self, memory: Memory):\n        # Get file data from global memory\n        files_dict = memory.files or {}\n        return list(files_dict.items()) # [(\"file1.txt\", \"content1\"), ...]\n\n    async def exec(self, files: list):\n        # No main computation needed here, just return the count for info\n        return len(files)\n\n    async def post(self, memory: Memory, files_to_process: list, file_count: int):\n        print(f\"Mapper: Triggering summary for {file_count} files.\")\n        # Initialize results list and counter in global memory\n        memory.file_summaries = [None] * file_count\n        memory.remaining_summaries = file_count # Add counter\n        # Trigger a 'summarize_file' action for each file\n        for index, (filename, content) in enumerate(files_to_process):\n            self.trigger('summarize_file', { \"filename\": filename, \"content\": content, \"index\": index })\n        # NOTE: 'combine_summaries' is now triggered by SummarizeFileNode when the counter reaches zero.\n\n# 2. Processor Node: Summarizes a single file\nclass SummarizeFileNode(Node):\n    async def prep(self, memory: Memory):\n        # Read specific file data from local memory (passed via forkingData)\n        return memory.filename, memory.content, memory.index\n\n    async def exec(self, prep_res):\n        filename, content, index = prep_res\n        # Summarize the content\n        print(f\"Processor: Summarizing {filename} (Index {index})\")\n        return await call_llm(f\"Summarize this:\\n{content}\")\n\n    async def post(self, memory: Memory, prep_res, summary: str):\n        filename, content, index = prep_res\n        # Store individual summary in global memory at the correct index\n        memory.file_summaries[index] = { \"filename\": filename, \"summary\": summary }\n        print(f\"Processor: Finished {filename} (Index {index})\")\n        # Decrement counter and trigger combine if this is the last summary\n        memory.remaining_summaries -= 1\n        if memory.remaining_summaries == 0:\n            print(\"Processor: All summaries collected, triggering combine.\")\n            self.trigger('combine_summaries')\n\n# 3. Reducer Node: Combines individual summaries\nclass CombineSummariesNode(Node):\n    async def prep(self, memory: Memory):\n        # Read the array of individual summaries (filter out None if any failed)\n        summaries = [s for s in (memory.file_summaries or []) if s is not None]\n        return summaries\n\n    async def exec(self, summaries: list):\n        print(f\"Reducer: Combining {len(summaries)} summaries.\")\n        if not summaries:\n            return \"No summaries to combine.\"\n        # Format summaries for the final prompt\n        combined_text = \"\\n\\n---\\n\\n\".join([f\"{s['filename']}:\\n{s['summary']}\" for s in summaries])\n        return await call_llm(f\"Combine these summaries into one final summary:\\n{combined_text}\")\n\n    async def post(self, memory: Memory, prep_res, final_summary: str):\n        # Store the final combined summary\n        memory.final_summary = final_summary\n        print(\"Reducer: Final summary generated.\")\n        # No trigger needed if this is the end\n\n# --- Flow Definition ---\ntrigger_node = TriggerSummariesNode()\nprocessor_node = SummarizeFileNode()\nreducer_node = CombineSummariesNode()\n\n# Define transitions\ntrigger_node - 'summarize_file' >> processor_node # Map step\ntrigger_node - 'combine_summaries' >> reducer_node # Reduce step\n\n# Use ParallelFlow for potentially faster summarization\nmap_reduce_flow = ParallelFlow(start=trigger_node)\n# Alternatively, if strict sequential processing is acceptable or required (e.g., to avoid\n# the complexity of the counter mechanism), you can use a standard Flow:\n# map_reduce_flow = Flow(start=trigger_node)\n# This ensures the 'combine_summaries' step (triggered by the last processor)\n# only runs after all 'summarize_file' steps are complete.\n```\n\n----------------------------------------\n\nTITLE: Visualizing Pattern Composition with Mermaid Graph - Mermaid\nDESCRIPTION: This Mermaid graph illustrates the composition of AI patterns within BrainyFlow, specifically demonstrating how an Agent pattern can be linked with a RAG (Retrieval-Augmented Generation) pattern. It uses subgraphs to distinguish the internal stages of each pattern and visual connections to show their interplay. Requires a Mermaid-enabled environment to render. Inputs include stages such as Perceive, Think, Act, Query, Retrieve, and Generate, with outputs illustrating their interconnections for complex AI orchestration.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/index.md#2025-04-22_snippet_1\n\nLANGUAGE: mermaid\nCODE:\n```\n```mermaid\\ngraph TD\\n    subgraph \\\"Agent Pattern\\\"\\n        A[Perceive] --> B[Think]\\n        B --> C[Act]\\n        C --> A\\n    end\\n\\n    subgraph \\\"RAG Pattern\\\"\\n        D[Query] --> E[Retrieve]\\n        E --> F[Generate]\\n    end\\n\\n    A --> D\\n    F --> B\\n```\n```\n\n----------------------------------------\n\nTITLE: Getting Text Embeddings with OpenAI API in Python\nDESCRIPTION: Retrieves text embeddings using OpenAI's embedding models. Handles API key configuration, makes requests to the OpenAI API, and returns a numpy array. Includes error handling and supports configurable model selection.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/embedding.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Requires: pip install openai numpy\nimport openai\nimport os\nimport numpy as np\n\ndef get_openai_embedding(text: str, model: str = \"text-embedding-3-small\") -> np.ndarray | None:\n    \"\"\"Gets embedding from OpenAI API.\"\"\"\n    openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n    if not openai.api_key:\n        print(\"Error: OPENAI_API_KEY not set.\")\n        return None\n    try:\n        response = openai.Embedding.create(input=text, model=model)\n        embedding = response[\"data\"][0][\"embedding\"]\n        return np.array(embedding, dtype=np.float32)\n    except Exception as e:\n        print(f\"Error calling OpenAI embedding API: {e}\")\n        return None\n\n# Example:\n# text_to_embed = \"Hello world\"\n# embedding_vector = get_openai_embedding(text_to_embed)\n# if embedding_vector is not None:\n#     print(embedding_vector)\n```\n\n----------------------------------------\n\nTITLE: Flow Initialization and Configuration in BrainyFlow (TypeScript)\nDESCRIPTION: This TypeScript example demonstrates flow construction before and after the BrainyFlow v1.0 upgrade. The new constructor allows optional runtime configuration (such as maxVisits) for more flexible execution. Legacy code uses just a start node, whereas v1.0 enables passing an options object. Dependencies include the Flow and Node classes from BrainyFlow.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/migration.md#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\n// Before (v0.2)\nconst flow = new Flow(startNode)\n\n// After (v1.0)\n// With default options\nconst flow = new Flow(startNode)\n\n// With custom options\nconst flow = new Flow(startNode, { maxVisits: 10 })\n```\n\n----------------------------------------\n\nTITLE: Defining QA Nodes with Asynchronous Lifecycle Methods (TypeScript)\nDESCRIPTION: Implements GetQuestionNode and AnswerNode classes using async/await and BrainyFlow's Node lifecycle for TypeScript. Uses prompt input collection (with @inquirer/prompts), shared memory via interface QAGlobalStore, and an external LLM utility callLLM. Inputs are received from user prompt; chains outputs between nodes in memory. Outputs are logged to console. Assumes TypeScript, Node.js, and typed memory patterns.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/getting_started.md#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Flow, Memory, Node } from 'brainyflow'\nimport { input } from '@inquirer/prompts'\nimport { callLLM } from './utils/callLLM'\n\n// Define interfaces for Memory stores (optional but good practice)\ninterface QAGlobalStore {\n  question?: string\n  answer?: string\n}\nclass GetQuestionNode extends Node<QAGlobalStore> {\n  async prep(memory: Memory<QAGlobalStore>): Promise<void> {\n    memory.question = await input({ message: 'Enter your question: ' })\n  }\n}\n\nclass AnswerNode extends Node<QAGlobalStore> {\n  async prep(memory: Memory<QAGlobalStore>): Promise<string | undefined> {\n    return memory.question\n  }\n\n  async exec(question: string | undefined): Promise<string> {\n    const prompt = `Answer the following question: ${question}`\n    return await callLLM(prompt)\n  }\n\n  async post(\n    memory: Memory<QAGlobalStore>,\n    prepRes: string | undefined,\n    execRes: string,\n  ): Promise<void> {\n    memory.answer = execRes\n    console.log(`AnswerNode: Stored answer '${execRes}'`)\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Visualizing Article Workflow Process with Mermaid\nDESCRIPTION: This Mermaid snippet uses graph notation to visually represent the sequential structure of the article writing workflow. It defines three main stages (outline generation, content writing, style application) as directed nodes. This visualization assists in understanding the data flow and logical sequence between workflow components in the BrainyFlow process.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-workflow/README.md#2025-04-22_snippet_3\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph LR\n    Outline[Generate Outline] --> Write[Write Content]\n    Write --> Style[Apply Style]\n```\n\n----------------------------------------\n\nTITLE: Defining and Running Parallel MapReduce Workflow with Brainyflow (TypeScript)\nDESCRIPTION: This TypeScript snippet demonstrates setting up a fully parallelized MapReduce-style summarization flow using the Brainyflow framework. It defines custom Flow nodes for mapping (triggering summaries for each file), processing (summarizing individual files by calling a remote LLM), and reducing (combining individual summaries into a final summary). Classes utilize asynchronous prep, exec, and post methods for orchestrated actions. Dependencies include 'brainyflow' and an external callLLM function. Key parameters handled via a shared memory object include files, file_summaries, and remaining_summaries for synchronization. Outputs consist of logs, a collection of summaries, and one combined final summary. ParallelFlow is used for efficient concurrent execution.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/mapreduce.md#2025-04-22_snippet_2\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { Flow, Memory, Node, ParallelFlow } from 'brainyflow'\\n\\n// Assume callLLM is defined elsewhere\\ndeclare function callLLM(prompt: string): Promise<string>\\n\\n// 1. Mapper Node: Triggers processing for each file\\nclass TriggerSummariesNode extends Node {\\n  async prep(memory: Memory): Promise<[string, string][]> {\\n    // Get file data from global memory\\n    const filesDict = memory.files ?? {}\\n    return Object.entries(filesDict) // [[\\\"file1.txt\\\", \\\"content1\\\"], ...]\\n  }\\n\\n  async exec(files: [string, string][]): Promise<number> {\\n    // No main computation needed here, just return the count for info\\n    return files.length\\n  }\\n\\n  async post(memory: Memory, filesToProcess: [string, string][], fileCount: number): Promise<void> {\\n    console.log(`Mapper: Triggering summary for ${fileCount} files.`)\\n    // Initialize results array and counter in global memory\\n    // Note: Using an array might still have race conditions if indices aren't guaranteed\\n    // in ParallelFlow. An object map { index: summary } might be safer.\\n    memory.file_summaries = new Array(fileCount).fill(null)\\n    memory.remaining_summaries = fileCount // Add counter\\n    // Trigger a 'summarize_file' action for each file\\n    filesToProcess.forEach(([filename, content], index) => {\\n      this.trigger('summarize_file', { filename, content, index })\\n    })\\n    // NOTE: 'combine_summaries' is now triggered by SummarizeFileNode when the counter reaches zero.\\n  }\\n}\\n\\n// 2. Processor Node: Summarizes a single file\\nclass SummarizeFileNode extends Node {\\n  async prep(memory: Memory): Promise<{ filename: string; content: string; index: number }> {\\n    // Read specific file data from local memory (passed via forkingData)\\n    return { filename: memory.filename, content: memory.content, index: memory.index }\\n  }\\n\\n  async exec(fileData: { filename: string; content: string; index: number }): Promise<string> {\\n    // Summarize the content\\n    console.log(`Processor: Summarizing ${fileData.filename} (Index ${fileData.index})`)\\n    return await callLLM(`Summarize this:\\n${fileData.content}`)\\n  }\\n\\n  async post(\\n    memory: Memory,\\n    prepRes: { index: number; filename: string },\\n    summary: string,\\n  ): Promise<void> {\\n    // Store individual summary in global memory at the correct index\\n    // Note: Direct array index assignment might cause issues with ParallelFlow if order matters\\n    // A safer approach might be to push {filename, summary} and sort later, or use an object.\\n    memory.file_summaries[prepRes.index] = { filename: prepRes.filename, summary }\\n    console.log(`Processor: Finished ${prepRes.filename} (Index ${prepRes.index})`)\\n    // Decrement counter and trigger combine if this is the last summary\\n    memory.remaining_summaries--\\n    if (memory.remaining_summaries === 0) {\\n      console.log('Processor: All summaries collected, triggering combine.')\\n      this.trigger('combine_summaries')\\n    }\\n  }\\n}\\n\\n// 3. Reducer Node: Combines individual summaries\\nclass CombineSummariesNode extends Node {\\n  async prep(memory: Memory): Promise<any[]> {\\n    // Read the array of individual summaries\\n    return memory.file_summaries ?? []\\n  }\\n\\n  async exec(summaries: { filename: string; summary: string }[]): Promise<string> {\\n    console.log(`Reducer: Combining ${summaries.length} summaries.`)\\n    if (!summaries || summaries.length === 0) {\\n      return 'No summaries to combine.'\\n    }\\n    // Format summaries for the final prompt\\n    const combinedText = summaries.map((s) => `${s.filename}:\\n${s.summary}`).join('\\n\\n---\\n\\n')\\n    return await callLLM(`Combine these summaries into one final summary:\\n${combinedText}`)\\n  }\\n\\n  async post(memory: Memory, prepRes: any, finalSummary: string): Promise<void> {\\n    // Store the final combined summary\\n    memory.final_summary = finalSummary\\n    console.log('Reducer: Final summary generated.')\\n  }\\n}\\n\\n// --- Flow Definition ---\\nconst triggerNode = new TriggerSummariesNode()\\nconst processorNode = new SummarizeFileNode()\\nconst reducerNode = new CombineSummariesNode()\\n\\n// Define transitions\\ntriggerNode.on('summarize_file', processorNode) // Map step\\ntriggerNode.on('combine_summaries', reducerNode) // Reduce step (runs after all triggers are processed by the flow runner)\\n\\n// Use ParallelFlow for potentially faster summarization\\nconst mapReduceFlow = new ParallelFlow(triggerNode)\\n// Alternatively, if strict sequential processing is acceptable or required (e.g., to avoid\\n// the complexity of the counter mechanism), you can use a standard Flow:\\n// const mapReduceFlow = new Flow(triggerNode);\\n// This ensures the 'combine_summaries' step (triggered by the last processor)\\n// only runs after all 'summarize_file' steps are complete.\\n\\n// --- Execution ---\\nasync function main() {\\n  const memory = {\\n    files: {\\n      'file1.txt': 'Alice was beginning to get very tired of sitting by her sister...',\\n      'file2.txt': 'The quick brown fox jumps over the lazy dog.',\\n      'file3.txt': 'Lorem ipsum dolor sit amet, consectetur adipiscing elit.',\\n    },\\n  }\\n  await mapReduceFlow.run(memory)\\n  console.log('\\n--- MapReduce Complete ---')\\n  console.log('Individual Summaries:', memory.file_summaries)\\n  console.log('\\nFinal Summary:\\n', memory.final_summary)\\n}\\n\\nmain().catch(console.error)\n```\n\n----------------------------------------\n\nTITLE: Implementing Node Class ExecRunner Method with Retries in TypeScript\nDESCRIPTION: Overrides the abstract `execRunner` from `BaseNode`. This implementation wraps the call to the node's `exec` method with retry logic based on the `maxRetries` and `wait` options configured in the constructor. If retries are exhausted, it calls `execFallback`.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/api.md#2025-04-22_snippet_21\n\nLANGUAGE: typescript\nCODE:\n```\n`protected async execRunner(memory: Memory<GlobalStore, LocalStore>, prepRes: PrepResult): Promise<ExecResult | void>`\n```\n\n----------------------------------------\n\nTITLE: Implementing Node Tracing with TypeScript\nDESCRIPTION: A comprehensive TypeScript implementation of node tracing using Winston logger. Includes a wrapper function that adds tracing to any Node class, complete with timing measurements and error handling for all execution phases.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/visualization_logging.md#2025-04-22_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createLogger, format, transports } from 'winston'\n\nconst logger = createLogger({\n  level: 'info',\n  format: format.combine(\n    format.timestamp({ format: 'HH:mm:ss.SSS' }),\n    format.printf(\n      ({ timestamp, level, message }) => `${timestamp} - ${level.toUpperCase()} - ${message}`,\n    ),\n  ),\n  transports: [new transports.Console()],\n})\n\nfunction withTracing<T extends new (...args: any[]) => Node>(NodeClass: T): T {\n  return class TracedNode extends NodeClass {\n    async prep(memory: Memory): Promise<any> {\n      const nodeName = this.constructor.name\n      logger.info(`ENTER prep: ${nodeName}`)\n      const startTime = Date.now()\n      try {\n        const result = await super.prep(memory)\n        const elapsed = (Date.now() - startTime) / 1000\n        logger.info(`EXIT prep: ${nodeName} (${elapsed.toFixed(3)}s)`)\n        return result\n      } catch (e: any) {\n        logger.error(`ERROR prep: ${nodeName} - ${e.message}`)\n        throw e\n      }\n    }\n\n    async exec(prepRes: any): Promise<any> {\n      const nodeName = this.constructor.name\n      logger.info(`ENTER exec: ${nodeName}`)\n      const startTime = Date.now()\n      try {\n        const result = await super.exec(prepRes)\n        const elapsed = (Date.now() - startTime) / 1000\n        logger.info(`EXIT exec: ${nodeName} (${elapsed.toFixed(3)}s)`)\n        return result\n      } catch (e: any) {\n        logger.error(`ERROR exec: ${nodeName} - ${e.message}`)\n        throw e\n      }\n    }\n\n    async post(memory: Memory, prepRes: any, execRes: any): Promise<void> {\n      const nodeName = this.constructor.name\n      logger.info(`ENTER post: ${nodeName}`)\n      const startTime = Date.now()\n      try {\n        await super.post(memory, prepRes, execRes)\n        const elapsed = (Date.now() - startTime) / 1000\n        logger.info(`EXIT post: ${nodeName} (${elapsed.toFixed(3)}s)`)\n      } catch (e: any) {\n        logger.error(`ERROR post: ${nodeName} - ${e.message}`)\n        throw e\n      }\n    }\n  } as T\n}\n\nclass MyOriginalNode extends Node {\n  async prep(memory: Memory): Promise<string> {\n    await new Promise((res) => setTimeout(res, 50))\n    return 'data'\n  }\n\n  async exec(prepRes: string): Promise<string> {\n    await new Promise((res) => setTimeout(res, 100))\n    return prepRes.toUpperCase()\n  }\n\n  async post(memory: Memory, prepRes: string, execRes: string): Promise<void> {\n    await new Promise((res) => setTimeout(res, 30))\n    memory.result = execRes\n  }\n}\n\nconst MyTracedNode = withTracing(MyOriginalNode);\n```\n\n----------------------------------------\n\nTITLE: Synthesizing Speech with ElevenLabs API in Python\nDESCRIPTION: This Python function `synthesize_elevenlabs` interacts directly with the ElevenLabs REST API using the `requests` library to convert text to speech. It requires an ElevenLabs API Key (`ELEVENLABS_API_KEY`) and optionally a Voice ID (`ELEVENLABS_VOICE_ID`) set as environment variables. The function takes input text and an optional output filename (defaults to `elevenlabs_output.mp3`). It sends a POST request to the ElevenLabs TTS endpoint with the text, model ID, voice settings, and API key. The received audio stream (MP3 format) is written chunk by chunk to the output file. It includes error handling for request exceptions.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/text_to_speech.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Requires: pip install requests\nimport requests\nimport os\n\ndef synthesize_elevenlabs(text: str, output_filename: str = \"elevenlabs_output.mp3\"):\n    \"\"\"Synthesizes speech using ElevenLabs API.\"\"\"\n    api_key = os.environ.get(\"ELEVENLABS_API_KEY\")\n    # Find voice IDs via ElevenLabs website or API\n    voice_id = os.environ.get(\"ELEVENLABS_VOICE_ID\", \"21m00Tcm4TlvDq8ikWAM\") # Example: Rachel\n\n    if not api_key:\n        print(\"Error: ELEVENLABS_API_KEY not set.\")\n        return\n    if not voice_id:\n        print(\"Error: ELEVENLABS_VOICE_ID not set.\")\n        return\n\n    url = f\"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}\"\n    headers = {\n        \"Accept\": \"audio/mpeg\", # Request MP3 format\n        \"Content-Type\": \"application/json\",\n        \"xi-api-key\": api_key\n    }\n    data = {\n        \"text\": text,\n        \"model_id\": \"eleven_monolingual_v1\", # Or other models like eleven_multilingual_v2\n        \"voice_settings\": {\n            \"stability\": 0.5,       # Example settings\n            \"similarity_boost\": 0.75\n        }\n    }\n\n    try:\n        response = requests.post(url, json=data, headers=headers)\n        response.raise_for_status() # Raise exception for bad status codes\n\n        with open(output_filename, 'wb') as f:\n            for chunk in response.iter_content(chunk_size=1024):\n                if chunk:\n                    f.write(chunk)\n        print(f\"Audio saved to {output_filename}\")\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Error calling ElevenLabs API: {e}\")\n        # Print response body if available for more details\n        if e.response is not None:\n             print(f\"Response body: {e.response.text}\")\n\n\n# Example:\n# synthesize_elevenlabs(\"Hello from ElevenLabs Text-to-Speech!\")\n\n```\n\n----------------------------------------\n\nTITLE: Creating and Running a Basic Brainyflow Flow in TypeScript\nDESCRIPTION: Demonstrates the fundamental process of creating and running a Brainyflow Flow in TypeScript. It involves importing `Flow` and `Node`, defining transitions using methods (`.next()` for default, `.on('action', node)` for named), optionally defining a type for the shared `globalStore`, creating the store object, instantiating a `Flow` with a generic type and starting node, and executing it asynchronously using `await flow.run(globalStore)`. The global store object is modified in place.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/core_abstraction/flow.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Flow, Node } from 'brainyflow'\n\n// Define nodes and transitions\n// const node_a = new Node(); ... etc.\nnode_a.next(node_b) // Default transition\nnode_b.on('success', node_c) // Named transition\nnode_b.on('error', node_d) // Named transition\n\n// Define the expected Global Store structure (optional but recommended)\ninterface MyGlobalStore {\n  input?: any\n  result?: any\n  error?: any\n}\n\n// Create a global store object (can be just an empty object)\nconst globalStore: MyGlobalStore = { input: 'some data' }\n\n// Create flow starting with node_a\nconst flow = new Flow<MyGlobalStore>(node_a)\n\n// Run the flow, passing the global store object.\n// The flow modifies the globalStore object in place.\nawait flow.run(globalStore)\n\n// Print the final state of the global store\nconsole.log('Flow finished. Final memory state:', globalStore)\n// Example output (depending on flow logic):\n// { input: 'some data', result: 'processed data from node_c' }\n// or\n// { input: 'some data', error: 'error details from node_d' }\n```\n\n----------------------------------------\n\nTITLE: TypeScript RAG Pipeline Implementation\nDESCRIPTION: Implementation of RAG pipeline nodes in TypeScript using BrainyFlow framework. Provides equivalent functionality to the Python version with TypeScript-specific type declarations and syntax. Handles document processing, chunking, and embedding in a parallel workflow.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/rag.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport * as fs from 'fs'\nimport { Flow, Memory, Node, ParallelFlow } from 'brainyflow'\n\n// Assume getEmbedding and createIndex/searchIndex are defined elsewhere\ndeclare function getEmbedding(text: string): Promise<number[]>\ndeclare function createIndex(embeddings: number[][]): any // Returns index object\ndeclare function searchIndex(\n  index: any,\n  queryEmbedding: number[],\n  topK: number,\n): [number[][], number[][]] // Returns [[ids]], [[distances]]\n\n// --- Stage 1: Offline Indexing Nodes ---\n\n// 1a. Node to trigger chunking for each file\nclass TriggerChunkingNode extends Node {\n  async prep(memory: Memory): Promise<string[]> {\n    return memory.files ?? [] // Expects memory.files = ['doc1.txt', ...]\n  }\n  async exec(files: string[]): Promise<number> {\n    return files.length\n  }\n  async post(memory: Memory, prepRes: string[], fileCount: number): Promise<void> {\n    console.log(`Triggering chunking for ${fileCount} files.`)\n    memory.all_chunks = [] // Initialize chunk store\n    ;(prepRes as string[]).forEach((filepath, index) => {\n      this.trigger('chunk_file', { filepath, index }) // Pass filepath via local memory\n    })\n    this.trigger('embed_chunks') // Trigger embedding after all files are processed\n  }\n}\n\n// 1b. Node to chunk a single file\nclass ChunkFileNode extends Node {\n  async prep(memory: Memory): Promise<{ filepath: string; index: number }> {\n    return { filepath: memory.filepath, index: memory.index } // Read from local memory\n  }\n  async exec(prepRes: { filepath: string; index: number }): Promise<string[]> {\n    console.log(`Chunking ${prepRes.filepath}`)\n    const text = fs.readFileSync(prepRes.filepath, 'utf-8')\n    const chunks: string[] = []\n    const size = 100 // Simple fixed-size chunking\n    for (let i = 0; i < text.length; i += size) {\n      chunks.push(text.slice(i, i + size))\n    }\n    return chunks\n  }\n  async post(memory: Memory, prepRes: { index: number }, chunks: string[]): Promise<void> {\n    // Add chunks to the global list (careful with concurrency if using ParallelFlow)\n    // A safer parallel approach might store chunks per file then combine later.\n    memory.all_chunks.push(...chunks)\n  }\n}\n\n// 1c. Node to trigger embedding for each chunk\nclass TriggerEmbeddingNode extends Node {\n  async prep(memory: Memory): Promise<string[]> {\n    return memory.all_chunks ?? []\n  }\n  async exec(chunks: string[]): Promise<number> {\n    return chunks.length\n  }\n  async post(memory: Memory, prepRes: string[], chunkCount: number): Promise<void> {\n    console.log(`Triggering embedding for ${chunkCount} chunks.`)\n    memory.all_embeds = [] // Initialize embedding store\n    ;(prepRes as string[]).forEach((chunk, index) => {\n      this.trigger('embed_chunk', { chunk, index })\n    })\n    this.trigger('store_index') // Trigger storing after all chunks processed\n  }\n}\n\n// 1d. Node to embed a single chunk\nclass EmbedChunkNode extends Node {\n  async prep(memory: Memory): Promise<{ chunk: string; index: number }> {\n    return { chunk: memory.chunk, index: memory.index } // Read from local memory\n  }\n  async exec(prepRes: { chunk: string; index: number }): Promise<number[]> {\n    console.log(`Embedding chunk ${prepRes.index}`)\n    return await getEmbedding(prepRes.chunk)\n  }\n  async post(memory: Memory, prepRes: { index: number }, embedding: number[]): Promise<void> {\n    // Store embedding in global list (careful with concurrency)\n    memory.all_embeds[prepRes.index] = embedding\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Python RAG Pipeline Implementation\nDESCRIPTION: Implementation of RAG pipeline nodes in Python using BrainyFlow framework. Includes nodes for triggering document chunking, processing individual files, managing embeddings, and handling parallel processing. Uses async/await patterns and memory management for state.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/rag.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport os # Assuming file operations\nfrom brainyflow import Node, Flow, Memory, ParallelFlow\n\n# Assume get_embedding, create_index, search_index are defined elsewhere\n# async def get_embedding(text: str) -> list[float]: ...\n# def create_index(embeddings: list[list[float]]) -> Any: ... # Returns index object\n# def search_index(index: Any, query_embedding: list[float], top_k: int) -> tuple[list[list[int]], list[list[float]]]: ...\n\n# --- Stage 1: Offline Indexing Nodes ---\n\n# 1a. Node to trigger chunking for each file\nclass TriggerChunkingNode(Node):\n    async def prep(self, memory: Memory):\n        return memory.files or []\n\n    async def exec(self, files: list):\n         # Optional: could return file count or validate paths\n         return len(files)\n\n    async def post(self, memory: Memory, files: list, file_count: int):\n        print(f\"Triggering chunking for {file_count} files.\")\n        memory.all_chunks = [] # Initialize chunk store\n        memory.chunk_metadata = [] # Store metadata like source file\n        for index, filepath in enumerate(files):\n            if os.path.exists(filepath): # Basic check\n                 self.trigger('chunk_file', { \"filepath\": filepath, \"file_index\": index })\n            else:\n                 print(f\"Warning: File not found {filepath}\")\n        # Trigger next major step after attempting all files\n        self.trigger('embed_chunks')\n\n# 1b. Node to chunk a single file\nclass ChunkFileNode(Node):\n    async def prep(self, memory: Memory):\n        # Read filepath from local memory\n        return memory.filepath, memory.file_index\n\n    async def exec(self, prep_res):\n        filepath, file_index = prep_res\n        print(f\"Chunking {filepath}\")\n        try:\n            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n                text = f.read()\n            # Simple fixed-size chunking\n            chunks = []\n            size = 100\n            for i in range(0, len(text), size):\n                chunks.append(text[i : i + size])\n            return chunks, filepath # Pass filepath for metadata\n        except Exception as e:\n            print(f\"Error chunking {filepath}: {e}\")\n            return [], filepath # Return empty list on error\n\n    async def post(self, memory: Memory, prep_res, exec_res):\n        chunks, filepath = exec_res\n        file_index = prep_res[1]\n        # Append chunks and their source metadata to global lists\n        # Note: If using ParallelFlow, direct append might lead to race conditions.\n        # Consider storing per-file results then combining in the next step.\n        start_index = len(memory.all_chunks)\n        memory.all_chunks.extend(chunks)\n        for i, chunk in enumerate(chunks):\n             memory.chunk_metadata.append({\"source\": filepath, \"chunk_index_in_file\": i, \"global_chunk_index\": start_index + i})\n        # This node doesn't trigger further processing for this specific file branch\n\n# 1c. Node to trigger embedding for each chunk\nclass TriggerEmbeddingNode(Node):\n     async def prep(self, memory: Memory):\n         # This node runs after all 'chunk_file' triggers are processed by the Flow\n         return memory.all_chunks or []\n\n     async def exec(self, chunks: list):\n         return len(chunks)\n\n     async def post(self, memory: Memory, chunks: list, chunk_count: int):\n         print(f\"Triggering embedding for {chunk_count} chunks.\")\n         memory.all_embeds = [None] * chunk_count # Pre-allocate list for parallel writes\n         for index, chunk in enumerate(chunks):\n             # Pass chunk and its global index via forkingData\n             self.trigger('embed_chunk', { \"chunk\": chunk, \"global_index\": index })\n         # Trigger storing index after all embedding triggers are fired\n         self.trigger('store_index')\n\n# 1d. Node to embed a single chunk\nclass EmbedChunkNode(Node):\n     async def prep(self, memory: Memory):\n         # Read chunk and global index from local memory\n         return memory.chunk, memory.global_index\n\n     async def exec(self, prep_res):\n         chunk, index = prep_res\n         # print(f\"Embedding chunk {index}\") # Can be noisy\n         return await get_embedding(chunk), index # Pass index through\n\n     async def post(self, memory: Memory, prep_res, exec_res):\n         embedding, index = exec_res\n         # Store embedding at the correct index in the pre-allocated list\n         memory.all_embeds[index] = embedding\n         # This node doesn't trigger further processing for this chunk branch\n```\n\n----------------------------------------\n\nTITLE: Implementing Branching and Looping in a Brainyflow Flow (Python)\nDESCRIPTION: Shows how to define nodes and connections in Python to create an expense approval workflow with conditional logic. It demonstrates branching using named transitions based on node actions (`review - \"approved\" >> payment`) and looping by directing a node's default transition back to a previous node (`revise >> review`).\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/core_abstraction/flow.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom brainyflow import Flow, Node\n\n# Define the nodes first\n# review = ReviewExpenseNode()\n# revise = ReviseReportNode()\n# payment = ProcessPaymentNode()\n# finish = FinishProcessNode()\n# ...\n\n# Define the flow connections\nreview - \"approved\" >> payment        # If approved, process payment\nreview - \"needs_revision\" >> revise   # If needs changes, go to revision\nreview - \"rejected\" >> finish         # If rejected, finish the process\n\nrevise >> review   # After revision, go back for another review\npayment >> finish  # After payment, finish the process\n\n# Create the flow\nexpense_flow = Flow(start=review)\n```\n\n----------------------------------------\n\nTITLE: Running Multi-Agent Game Loop with Asyncio Flows - Python\nDESCRIPTION: Implements a multi-agent word guessing game where two agents ('Hinter' and 'Guesser') communicate via asynchronous queues and share a mutable memory object. Requires Python's asyncio library and classes Hinter, Guesser, and Flow (presumed defined elsewhere) for orchestration. Initializes state, loops each agent using flow transitions, and runs both agents concurrently until the game is finished, outputting the final memory state. Inputs include target word, forbidden words, and guess queues; outputs are game progress prints and memory updates. Intended for async, event-driven agent coordination in Python environments.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/multi_agent.md#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nasync def main():\n    # Set up game state in initial memory object\n    memory = {\n        \"target_word\": \"nostalgia\",\n        \"forbidden_words\": [\"memory\", \"past\", \"remember\", \"feeling\", \"longing\"],\n        \"hinter_queue\": asyncio.Queue(),\n        \"guesser_queue\": asyncio.Queue(),\n        \"past_guesses\": [] # Initialize past_guesses\n    }\n\n    print(\"Game starting!\")\n    print(f\"Target word: {memory['target_word']}\") # Access memory object\n    print(f\"Forbidden words: {memory['forbidden_words']}\") # Access memory object\n\n    # Initialize by sending empty guess to hinter queue in memory\n    await memory[\"hinter_queue\"].put(\"\")\n\n    # Create nodes\n    hinter = Hinter()\n    guesser = Guesser()\n\n    # Set up flows\n    hinter_flow = Flow(start=hinter)\n    guesser_flow = Flow(start=guesser)\n\n    # Connect nodes to themselves using actions\n    hinter - \"continue\" >> hinter\n    guesser - \"continue\" >> guesser\n\n    # Run both agents concurrently, passing the same memory object\n    print('Running agents...')\n    await asyncio.gather(\n        hinter_flow.run(memory), # Pass memory object\n        guesser_flow.run(memory)  # Pass memory object\n    )\n    print('\\nGame finished.')\n    print('Final memory state:', memory)\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Visualizing AI Implementation Brief Process Flow with Mermaid\nDESCRIPTION: This Mermaid flowchart depicts the workflow for creating an AI Implementation Brief. It starts with a human request, followed by AI clarification, design generation, human validation/editing, and finally AI implementation upon approval, with a loop for revisions and continuous refinement.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/agentic_coding.md#2025-04-22_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart TD\n    A[Human Request] --> B{AI Asks Questions}\n    B --> C[AI Generates Structured Design Draft]\n    C --> D{Human Validates/Edits}\n    D -->|Approved| E[AI Implements]\n    D -->|Needs Changes| B\n    E --> F[Continuous Co-Refinement]\n```\n\n----------------------------------------\n\nTITLE: Defining RAG Flow in TypeScript\nDESCRIPTION: TypeScript implementation of an online RAG flow that connects the query embedding, document retrieval, and answer generation nodes. It defines transitions between nodes and initializes the flow with the embedding node as the starting point.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/rag.md#2025-04-22_snippet_15\n\nLANGUAGE: typescript\nCODE:\n```\n// --- Online Flow Definition ---\nconst embedQueryNode = new EmbedQueryNode()\nconst retrieveDocsNode = new RetrieveDocsNode()\nconst generateAnswerNode = new GenerateAnswerNode()\n\n// Define transitions\nembedQueryNode.on('retrieve_docs', retrieveDocsNode)\nretrieveDocsNode.on('generate_answer', generateAnswerNode)\n\nconst OnlineFlow = new Flow(embedQueryNode)\n```\n\n----------------------------------------\n\nTITLE: Implementing Batch Processing in Python with BrainyFlow\nDESCRIPTION: Demonstrates the fan-out pattern for batch processing in Python, with a trigger node that iterates through items and a processor node that handles individual items. Can be used with either Flow or ParallelFlow.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/core_abstraction/flow.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nclass TriggerBatchNode(Node):\n    async def prep(self, memory: Memory):\n        return memory.items_to_process or []\n\n    async def post(self, memory: Memory, prep_res: list, exec_res):\n        items = prep_res\n        memory.results = [None] * len(items) # Pre-allocate for parallel\n        for index, item in enumerate(items):\n            self.trigger(\"process_one\", {\"item_data\": item, \"result_index\": index})\n        # Optional: self.trigger(\"aggregate\") if needed\n\nclass ProcessOneItemNode(Node):\n    async def prep(self, memory: Memory):\n        return {\"item\": memory.item_data, \"index\": memory.result_index}\n\n    async def exec(self, prep_res):\n        result = f\"Processed {prep_res['item']}\" # Placeholder\n        return {\"result\": result, \"index\": prep_res[\"index\"]}\n\n    async def post(self, memory: Memory, prep_res, exec_res):\n        memory.results[exec_res[\"index\"]] = exec_res[\"result\"]\n\n# Setup\ntrigger = TriggerBatchNode()\nprocessor = ProcessOneItemNode()\ntrigger - \"process_one\" >> processor\n\n# Choose Flow type\n# sequential_batch_flow = Flow(trigger)\nparallel_batch_flow = ParallelFlow(trigger)\n\n# Run\n# memory = {\"items_to_process\": [\"A\", \"B\", \"C\"]}\n# await parallel_batch_flow.run(memory)\n# print(memory[\"results\"]) # Output: ['Processed A', 'Processed B', 'Processed C']\n```\n\n----------------------------------------\n\nTITLE: Defining a Throttled LLM Node Using brainyflow in TypeScript\nDESCRIPTION: This TypeScript snippet declares a 'ThrottledLLMNode' class extending Node, implementing a rate limiter via the 'limiter' library to throttle outgoing LLM requests. It provides async exec and execFallback methods, passing the prompt to a hypothetical callLLM function and handling rate limit errors by introducing back-off and retry. The class depends on brainyflow abstractions (Memory, Node, NodeError), requires 'limiter' for rate management, and must have an available callLLM API; key configuration parameters control retry and throttle behavior, and the prompt text is required for execution.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/throttling.md#2025-04-22_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory, Node, NodeError } from 'brainyflow'\nimport { RateLimiter } from 'limiter'\n\nclass ThrottledLLMNode extends Node {\n  private limiter: RateLimiter\n\n  constructor(\n    private maxRetries = 3,\n    callsPerMinute = 30,\n  ) {\n    super({ maxRetries })\n    this.limiter = new RateLimiter({ tokensPerInterval: callsPerMinute, interval: 'minute' })\n  }\n\n  async exec(prompt: string): Promise<string> {\n    // Wait for token before proceeding\n    await this.limiter.removeTokens(1)\n    return await callLLM(prompt)\n  }\n\n  async execFallback(prompt: string, error: NodeError): Promise<string> {\n    // Handle rate limit errors specially\n    if (error.message.toLowerCase().includes('rate limit')) {\n      // Wait longer before retrying\n      await new Promise((resolve) => setTimeout(resolve, 60000))\n      return this.exec(prompt)\n    }\n    // For other errors, fall back to a simple response\n    return \"I'm having trouble processing your request right now.\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Retrieving Text Embeddings with Azure OpenAI API in TypeScript\nDESCRIPTION: This TypeScript function queries the Azure OpenAI API to fetch a text embedding. It requires installing '@azure/openai' and setting environment variables AZURE_OPENAI_API_KEY and AZURE_OPENAI_ENDPOINT. The function 'getAzureOpenaiEmbedding' takes the input string and deployment name, initializes the API client, and returns the embedding as a number array or null if credentials are missing or errors occur. The input for Azure OpenAI must be wrapped in an array for the API call.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/embedding.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n// Requires: npm install @azure/openai\\nimport { AzureKeyCredential, OpenAIClient } from '@azure/openai'\\n\\nasync function getAzureOpenaiEmbedding(\\n  text: string,\\n  deploymentName: string = 'your-embedding-deployment',\\n): Promise<number[] | null> {\\n  /** Gets embedding from Azure OpenAI API. */\\n  const apiKey = process.env.AZURE_OPENAI_API_KEY\\n  const endpoint = process.env.AZURE_OPENAI_ENDPOINT\\n\\n  if (!apiKey || !endpoint) {\\n    console.error('Error: AZURE_OPENAI_API_KEY or AZURE_OPENAI_ENDPOINT not set.')\\n    return null\\n  }\\n\\n  try {\\n    const client = new OpenAIClient(endpoint, new AzureKeyCredential(apiKey))\\n    const result = await client.getEmbeddings(deploymentName, [text]) // Input must be an array\\n    return result.data[0]?.embedding ?? null\\n  } catch (error) {\\n    console.error('Error calling Azure OpenAI embedding API:', error)\\n    return null\\n  }\\n}\\n\\n// Example:\\n// const textToEmbed = \"Hello world\";\\n// getAzureOpenaiEmbedding(textToEmbed, \"my-text-embedding-ada-002\").then(embedding => {\\n//   if (embedding) {\\n//     console.log(embedding);\\n//   }\\n// });\n```\n\n----------------------------------------\n\nTITLE: Configuring and Executing a Node-Based Agent Flow with BrainyFlow (TypeScript)\nDESCRIPTION: This TypeScript code sets up a node-based flow using the BrainyFlow library, employing DecideAction, SearchWeb, and DirectAnswer nodes. Transitions between agent states are created by attaching event handlers; the agentFlow processes an initial query in an asynchronous main function. Key dependencies include the brainyflow module and node classes. The script accepts an initialMemory object, logs the flow progression, handles completion or failure, and prints the final state or answer. All required node classes must be defined or imported.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/agent.md#2025-04-22_snippet_5\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { Flow } from 'brainyflow'\\n\\n// Assuming DecideAction, SearchWeb, DirectAnswer classes are defined\\n\\n// Instantiate nodes\\nconst decide = new DecideAction()\\nconst search = new SearchWeb()\\nconst answer = new DirectAnswer()\\n\\n// Define transitions\\ndecide.on('search', search)\\ndecide.on('answer', answer)\\nsearch.on('decide', decide) // Loop back\\n\\n// Create the flow\\nconst agentFlow = new Flow(decide)\\n\\n// --- Main execution function ---\\nasync function runAgent() {\\n  const initialMemory = { query: 'Who won the Nobel Prize in Physics 2024?' }\\n  console.log(`Starting agent flow with query: \"${initialMemory.query}\"`)\\n\\n  try {\\n    await agentFlow.run(initialMemory) // Run the flow with memory object\\n    console.log('\\n--- Flow Complete ---')\\n    console.log('Final Memory State:', initialMemory) // Log final memory state\\n    console.log('\\nFinal Answer:', initialMemory.answer ?? 'No answer found')\\n  } catch (error) {\\n    console.error('\\n--- Agent Flow Failed ---', error)\\n    console.error('Memory State on Failure:', initialMemory) // Log memory state on failure\\n  }\\n}\\n\\n// Run the main function\\nrunAgent()\n```\n\n----------------------------------------\n\nTITLE: Creating and Running a Basic Brainyflow Flow in Python\nDESCRIPTION: Demonstrates the fundamental process of creating and running a Brainyflow Flow in Python. It involves importing `Flow` and `Node`, defining node transitions using operators (`>>` for default, `- \"action\" >>` for named), initializing a shared `memory` dictionary, creating a `Flow` instance with a starting node, and executing it asynchronously using `await flow.run(memory)`. The shared memory object is modified in place during execution.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/core_abstraction/flow.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom brainyflow import Flow, Node\n\n# Define nodes and transitions\n# node_a = Node() ... etc.\nnode_a >> node_b\nnode_b - \"success\" >> node_c\nnode_b - \"error\" >> node_d\n\n# Create a shared/global store (an empty dictionary)\nmemory = {}\n\n# Create flow starting with node_a\nflow = Flow(start=node_a)\n\n# Run the flow, passing the memory object\nawait flow.run(memory)\n\n# The memory object is modified in place\nprint(\"Flow finished. Final memory state:\", memory)\n```\n\n----------------------------------------\n\nTITLE: Implementing GenerateAnswerNode in TypeScript\nDESCRIPTION: A TypeScript implementation of a GenerateAnswerNode class that handles answering questions using retrieved context. It defines type-safe methods for preparation, execution, and post-processing steps in the generation pipeline.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/rag.md#2025-04-22_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\n// 2c. Generate Answer Node\nclass GenerateAnswerNode extends Node {\n  async prep(memory: Memory): Promise<{ question: string; chunk: string }> {\n    return { question: memory.question, chunk: memory.retrieved_chunk }\n  }\n  async exec(prepRes: { question: string; chunk: string }): Promise<string> {\n    const { question, chunk } = prepRes\n    const prompt = `Using the following context, answer the question.\nContext: ${chunk}\nQuestion: ${question}\nAnswer:`\n    console.log('Generating final answer...')\n    return await callLLM(prompt)\n  }\n  async post(memory: Memory, prepRes: any, answer: string): Promise<void> {\n    memory.answer = answer // Store final answer\n    console.log(`Answer: ${answer}`)\n    // End of flow\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Connecting and Running a Node-Based Agent Flow with BrainyFlow (Python)\nDESCRIPTION: This Python snippet initializes a node-based agent flow using the BrainyFlow library. It sets up DecideAction, SearchWeb, and DirectAnswer nodes, connecting them to define agent decision routes. The flow is executed asynchronously with an initial memory object containing a query. Dependencies include classes DecideAction, SearchWeb, DirectAnswer, Flow, and asyncio. The program prints both the answer and the final memory state; it expects memory_data as input and outputs agent results. The code assumes all required node classes are imported.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/agent.md#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n# Connect nodes\\ndecide = DecideAction()\\nsearch = SearchWeb()\\nanswer = DirectAnswer()\\n\\ndecide - \"search\" >> search\\ndecide - \"answer\" >> answer\\nsearch - \"decide\" >> decide # Loop back\\n\\nflow = Flow(start=decide)\\n\\nasync def main():\\n    memory_data = {\"query\": \"Who won the Nobel Prize in Physics 2024?\"}\\n    result = await flow.run(memory_data) # Pass memory object\\n    print(result) # Or handle result as needed\\n    print(memory_data) # See final memory state\\n\\nif __name__ == \"__main__\":\\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Retrieving Document Chunks Node in Python\nDESCRIPTION: Defines the `RetrieveDocsNode` class in Python. The `prep` method gathers necessary data from `memory`: the query embedding (`q_emb`), the vector index (`index`), all document chunks (`all_chunks`), and chunk metadata (`chunk_metadata`). The `exec` method performs a search using an external `search_index` function with the query embedding and index to find the top relevant chunk ID. It handles potential errors (missing data, index out of bounds) and returns the text of the best-matching chunk and its associated metadata. The `post` method stores the retrieved chunk and metadata back into `memory` and triggers the 'generate_answer' event.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/rag.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# 2b. Retrieve Docs Node\nclass RetrieveDocsNode(Node):\n    async def prep(self, memory: Memory):\n        # Need query embedding, index, and original chunks\n        # Also retrieve metadata to know the source\n        return memory.q_emb, memory.index, memory.all_chunks, memory.chunk_metadata\n\n    async def exec(self, inputs):\n        q_emb, index, chunks, metadata = inputs\n        if not q_emb or not index or not chunks:\n            raise ValueError(\"Missing data for retrieval in memory\")\n        print(\"Retrieving relevant chunk...\")\n        # Assuming search_index returns [[ids]], [[distances]]\n        I, D = search_index(index, q_emb, top_k=1)\n        if not I or not I[0]:\n             return \"Could not find relevant chunk.\", None\n        best_global_id = I[0][0]\n        if best_global_id >= len(chunks):\n             return \"Index out of bounds.\", None\n\n        relevant_chunk = chunks[best_global_id]\n        relevant_metadata = metadata[best_global_id] if metadata and best_global_id < len(metadata) else {}\n        return relevant_chunk, relevant_metadata\n\n    async def post(self, memory: Memory, prep_res, exec_res):\n        relevant_chunk, relevant_metadata = exec_res\n        memory.retrieved_chunk = relevant_chunk # Write to memory\n        memory.retrieved_metadata = relevant_metadata # Write metadata too\n        print(f\"Retrieved chunk: {relevant_chunk[:60]}... (Source: {relevant_metadata.get('source', 'N/A')})\")\n        self.trigger('generate_answer')\n```\n```\n\n----------------------------------------\n\nTITLE: Running Single Node within Flow in BrainyFlow Python\nDESCRIPTION: Asynchronously runs a single `node` within the flow using the provided `memory`. It handles node cloning to maintain state isolation, executes the node's `run` method, processes any triggers generated by the node, manages recursion based on triggers, and implements cycle detection using `max_visits` option.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nasync run_node(self, node, memory)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Agent Node Flow - Mermaid\nDESCRIPTION: This Mermaid snippet visualizes the control flow among DecideNode, SearchNode, and AnswerNode within the BrainyFlow agent. The directed graph describes decision-making: DecideNode either triggers SearchNode for more information or AnswerNode when ready to answer. After a web search, the flow returns to DecideNode for further context evaluation. No external dependencies beyond Mermaid syntax support are required; input is the node action, output is the chosen subsequent node.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/typescript-agent/README.md#2025-04-22_snippet_1\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph TD\n    A[DecideNode] -->|\"search\"| B[SearchNode]\n    A -->|\"answer\"| C[AnswerNode]\n    B -->|\"decide\"| A\n```\n\n----------------------------------------\n\nTITLE: Initializing and Interacting with Pinecone in Python\nDESCRIPTION: Shows how to initialize the Pinecone client using an API key (retrieved from the `PINECONE_API_KEY` environment variable), create a Pinecone index if it doesn't already exist (specifying dimension, metric, and pod environment), connect to an index, upsert vectors with associated IDs and optional metadata, and perform a k-nearest neighbor query, optionally retrieving metadata. Requires the `pinecone-client` library.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/vector.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Requires: pip install pinecone-client\nimport os\nfrom pinecone import Pinecone, PodSpec\n\ndef init_pinecone() -> Pinecone | None:\n    \"\"\"Initializes Pinecone client.\"\"\"\n    api_key = os.environ.get(\"PINECONE_API_KEY\")\n    # environment = os.environ.get(\"PINECONE_ENVIRONMENT\") # Legacy, use API key only\n    if not api_key:\n        print(\"Error: PINECONE_API_KEY not set.\")\n        return None\n    try:\n        # pc = Pinecone(api_key=api_key, environment=environment) # Legacy init\n        pc = Pinecone(api_key=api_key)\n        print(\"Pinecone initialized.\")\n        return pc\n    except Exception as e:\n        print(f\"Error initializing Pinecone: {e}\")\n        return None\n\ndef create_pinecone_index_if_not_exists(pc: Pinecone, index_name: str, dimension: int, metric: str = 'cosine', environment: str = 'gcp-starter'):\n    \"\"\"Creates a Pinecone index if it doesn't exist.\"\"\"\n    if index_name not in pc.list_indexes().names:\n        print(f\"Creating index '{index_name}'...\")\n        try:\n            pc.create_index(\n                name=index_name,\n                dimension=dimension,\n                metric=metric,\n                spec=PodSpec(environment=environment) # Specify environment here\n            )\n            print(f\"Index '{index_name}' created.\")\n        except Exception as e:\n            print(f\"Error creating Pinecone index: {e}\")\n    else:\n        print(f\"Index '{index_name}' already exists.\")\n\n# Example Usage:\n# pc = init_pinecone()\n# if pc:\n#     index_name = \"my-brainyflow-index\"\n#     dimension = 128\n#     create_pinecone_index_if_not_exists(pc, index_name, dimension)\n\n#     # Connect to the index\n#     try:\n#         index = pc.Index(index_name)\n\n#         # Upsert vectors\n#         vectors_to_upsert = [\n#             (\"vec_id1\", [0.1] * dimension, {\"genre\": \"fiction\"}), # With metadata\n#             (\"vec_id2\", [0.2] * dimension, {\"year\": 2023})\n#         ]\n#         print(f\"Upserting {len(vectors_to_upsert)} vectors...\")\n#         index.upsert(vectors=vectors_to_upsert)\n#         print(\"Upsert complete.\")\n\n#         # Query\n#         query_vector = [0.15] * dimension\n#         print(\"Querying index...\")\n#         response = index.query(vector=query_vector, top_k=3, include_metadata=True)\n#         print(\"Query response:\", response)\n\n#     except Exception as e:\n```\n\n----------------------------------------\n\nTITLE: Running Multi-Agent Game Loop with Async Functions - TypeScript\nDESCRIPTION: Provides a TypeScript implementation of a multi-agent word guessing game, using async/await and class-based agents that communicate via AsyncQueue. Assumes pre-existing AsyncQueue, Hinter, Guesser, and Flow class definitions. Agents ('Hinter' and 'Guesser') are connected in looping transitions, share state (targetWord, forbiddenWords, queues, and guess log), and are executed concurrently using Promise.all. Inputs include word data and agent nodes; outputs consist of console logs of game progress and final agent state. Designed for modern Node.js or browser environments supporting ESNext async features.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/multi_agent.md#2025-04-22_snippet_5\n\nLANGUAGE: TypeScript\nCODE:\n```\n// --- Main Execution ---\nasync function main() {\n  // Set up game state in initial memory\n  const data = {\n    targetWord: 'nostalgia',\n    forbiddenWords: ['memory', 'past', 'remember', 'feeling', 'longing'],\n    hinterQueue: new AsyncQueue<string>(),\n    guesserQueue: new AsyncQueue<string>(),\n    pastGuesses: [],\n  }\n\n  console.log('Game starting!')\n  console.log(`Target word: ${data.targetWord}`)\n  console.log(`Forbidden words: ${data.forbiddenWords}`)\n\n  // Initialize by sending empty guess to hinter queue\n  await data.hinterQueue.put('')\n\n  // Create nodes\n  const hinterNode = new Hinter()\n  const guesserNode = new Guesser()\n\n  // Define transitions for looping and ending\n  hinterNode.on('continue', hinterNode)\n  guesserNode.on('continue', guesserNode)\n\n  // Create separate flows for each agent\n  const hinterFlow = new Flow(hinterNode)\n  const guesserFlow = new Flow(guesserNode)\n\n  // Run both agent flows concurrently using the same memory object\n  console.log('Running agents...')\n  await Promise.all([hinterFlow.run(data), guesserFlow.run(data)])\n  console.log('\\nGame finished normally.')\n  console.log('Final memory state:', data)\n}\n\n// Assuming AsyncQueue and callLLM are defined elsewhere\n// class AsyncQueue<T> { ... }\n// async function callLLM(prompt: string): Promise<string> { return 'mock'; }\n\nmain().catch(console.error)\n```\n\n----------------------------------------\n\nTITLE: Orchestrating Node Graph Execution with Sequential Flow - Flow - TypeScript\nDESCRIPTION: This design outlines the Flow class, which coordinates sequential execution of a workflow graph composed of BaseNode instances. Flow tracks node visit counts to prevent execution cycles, clones nodes and memory as it traverses, and aggregates results into nested action trees. The exec method is intentionally not implemented—logic is handled by execRunner and runNode. Dependencies include Map for visit tracking, async/await for recursion, and cloning strategies; inputs are start node and initial memory, outputs are nested result trees of action-paths traversed. Constraints: cycles limited by maxVisits and only sequential task execution out of the box.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/design.md#2025-04-22_snippet_3\n\nLANGUAGE: TypeScript\nCODE:\n```\nclass Flow extends BaseNode {\n  // start: entry BaseNode\n  // visitCounts: Map for cycle/call tracking, keyed by node __nodeOrder\n  // options: e.g., maxVisits to limit cycles (default 5)\n  // exec() throws (Flow coordinates, does not execute its own logic)\n  // execRunner(memory): resets visitCounts and starts execution at start node\n  // runNode(node, memory): checks cycle, clones node and memory, runs node, recurses on successors for each trigger/action\n  // runNodes(nodes, memory): helper to execute an array of nodes\n  // runTasks(tasks): runs async task functions sequentially, aggregating results\n}\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Node Transitions in TypeScript\nDESCRIPTION: This snippet demonstrates how to configure both default and named action transitions between nodes using TypeScript in BrainyFlow. Transitions define how a node's actions lead to the execution of successor nodes, with methods such as `on` and `next`, allowing for complex flow setups.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/core_abstraction/node.md#2025-04-22_snippet_4\n\nLANGUAGE: TypeScript\nCODE:\n```\n// Basic default transition\nnode_a.next(node_b) // If node_a triggers \"default\", go to node_b\n\n// Named action transition\nnode_a.on('success', node_b) // If node_a triggers \"success\", go to node_b\nnode_a.on('error', node_c) // If node_a triggers \"error\", go to node_c\n\n// Alternative syntax\nnode_a.next(node_b, 'success') // Same as node_a.on('success', node_b)\n```\n\n----------------------------------------\n\nTITLE: Creating an LLM Wrapper using OpenAI in Python\nDESCRIPTION: This Python snippet illustrates how to build a basic wrapper for OpenAI's API using the OpenAI library. It defines a function to call the language model with specified parameters, returning the model's response. Dependencies include the `openai` package and an environment variable for the API key.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/llm.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# utils/call_llm.py\nimport os\nfrom openai import OpenAI\n\ndef call_llm(prompt, model=\"gpt-4o\", temperature=0.7):\n    \"\"\"Simple wrapper for calling OpenAI's API\"\"\"\n    client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n    response = client.chat.completions.create(\n        model=model,\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=temperature\n    )\n    return response.choices[0].message.content\n```\n\n----------------------------------------\n\nTITLE: Implementing Concurrency Control with p-limit in TypeScript\nDESCRIPTION: Defines a BrainyFlow `Node` subclass (`LimitedParallelNodeTs`) that uses the `p-limit` library to manage the concurrency of `processOneItem` calls within the `exec` method. Similar to the Python version, it includes `prep` for fetching data and `post` for storing results. Requires the `p-limit` package and the `brainyflow` framework.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/throttling.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// Requires: npm install p-limit\nimport { Memory, Node } from 'brainyflow' // Assuming imports\nimport pLimit from 'p-limit'\n\nclass LimitedParallelNodeTs extends Node {\n  private limit: ReturnType<typeof pLimit>\n\n  constructor(concurrency: number = 3) {\n    super()\n    if (concurrency <= 0) {\n      throw new Error('Concurrency limit must be positive')\n    }\n    this.limit = pLimit(concurrency)\n    console.log(`Node initialized with concurrency limit: ${concurrency}`)\n  }\n\n  // Prep is usually needed to get 'items' from memory\n  async prep(memory: Memory): Promise<any[]> {\n    // Example: Fetch items from memory\n    const items = memory.items_to_process || []\n    console.log(`Prep: Found ${items.length} items to process.`)\n    return items // Assuming items are in memory.items_to_process\n  }\n\n  async exec(items: any[]): Promise<any[]> {\n    if (!items || items.length === 0) {\n      console.log('Exec: No items to process.')\n      return []\n    }\n\n    console.log(`Exec: Starting processing of ${items.length} items with limit...`)\n    // Map each item to a limited async task\n    const tasks = items.map((item) =>\n      this.limit(async () => {\n        console.log(` Starting processing item: ${item}`)\n        const result = await this.processOneItem(item)\n        console.log(` Finished processing item: ${item} -> ${result}`)\n        return result\n      }),\n    )\n\n    // Wait for all limited tasks to complete\n    const results = await Promise.all(tasks)\n    console.log('Exec: All items processed.')\n    return results\n  }\n\n  async processOneItem(item: any): Promise<any> {\n    /** Placeholder: Subclasses must implement this method. */\n    // Example implementation:\n    await new Promise((resolve) => setTimeout(resolve, 500)) // Simulate async work\n    return `Processed_${item}`\n    // throw new Error(\"processOneItem must be implemented by subclasses\");\n  }\n\n  // Post is needed to store results and trigger next step\n  async post(memory: Memory, prepRes: any[], execRes: any[]): Promise<void> {\n    console.log(`Post: Storing ${execRes.length} results.`)\n    memory.processed_results = execRes // Store results\n    this.trigger('default') // Trigger next node\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Accessing Memory in TypeScript\nDESCRIPTION: Demonstrates accessing global and local memory stores in a BrainyFlow node. The example shows how to read properties from memory, preferring local memory lookup and falling back to global memory as necessary.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/core_abstraction/memory.md#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory, Node } from 'brainyflow'\n\ninterface MyGlobal {\n  config?: object\n  commonData?: string\n  pathSpecificData?: string\n}\ninterface MyLocal {\n  pathSpecificData?: string\n} // Can shadow global\n\nclass MyNode extends Node<MyGlobal, MyLocal> {\n  async prep(memory: Memory<MyGlobal, MyLocal>): Promise<void> {\n    // Reads from global store (assuming not set locally)\n    const config = memory.config\n    const common = memory.commonData\n\n    // Reads from local store if set via forkingData, otherwise reads from global\n    const specific = memory.pathSpecificData\n  }\n  // ... exec, post ...\n}\n```\n\n----------------------------------------\n\nTITLE: Creating BrainyFlow Agent Flow for Research and Question Answering\nDESCRIPTION: Defines a function to create and connect the custom nodes into a complete agent flow for research and question answering using the BrainyFlow framework.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-agent/demo.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# flow.py\nfrom brainyflow import Flow\n\ndef create_agent_flow():\n    \"\"\"\n    Create and connect the nodes to form a complete agent flow.\n\n    The flow works like this:\n    1. DecideAction node decides whether to search or answer\n    2. If search, go to SearchWeb node\n    3. If answer, go to AnswerQuestion node\n    4. After SearchWeb completes, go back to DecideAction\n\n    Returns:\n        Flow: A complete research agent flow\n    \"\"\"\n    # Create instances of each node\n    decide = DecideAction()\n    search = SearchWeb()\n    answer = AnswerQuestion()\n\n    # Connect the nodes\n    # If DecideAction returns \"search\", go to SearchWeb\n    decide - \"search\" >> search\n\n    # If DecideAction returns \"answer\", go to AnswerQuestion\n    decide - \"answer\" >> answer\n\n    # After SearchWeb completes and returns \"decide\", go back to DecideAction\n    search - \"decide\" >> decide\n\n    # Create and return the flow, starting with the DecideAction node\n    return Flow(start=decide)\n```\n\n----------------------------------------\n\nTITLE: Word Guessing Game Implementation with Multi-Agent System in Python\nDESCRIPTION: Implementation of a word guessing game using two agents: a Hinter that provides clues and a Guesser that attempts to guess the target word. Uses memory queues for communication between agents.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/multi_agent.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom brainyflow import Node, Flow\nfrom utils import call_llm\nimport asyncio\n\nclass Hinter(Node):\n    async def prep(self, memory: Memory):\n        guess = await memory.guesser_queue.get()\n        if guess == \"GAME_OVER\":\n            return None\n        return {\n            \"guess\": guess,\n            \"target_word\": memory.target_word,\n            \"forbidden_words\": memory.forbidden_words\n        }\n\n    async def exec(self, prep_res):\n        if prep_res is None:\n            return None\n\n        prompt = f\"\"\"\nGiven target word: {prep_res[\"target_word\"]}\nForbidden words: {prep_res[\"forbidden_words\"]}\nLast wrong guess: {prep_res.get('guess')}\nGenerate a creative hint that helps guess the target word without using forbidden words.\nReply only with the hint text.\n\"\"\"\n        return await call_llm(prompt)\n\n    async def post(self, memory: Memory, prep_res, hint):\n        if hint is None:\n            self.trigger(\"end\")\n            return\n        await memory.hinter_queue.put(hint)\n        self.trigger('continue')\n\nclass Guesser(Node):\n    async def prep(self, memory: Memory):\n        hint = await memory.guesser_queue.get()\n        return {\n            \"hint\": hint,\n            \"past_guesses\": memory.past_guesses if hasattr(memory, 'past_guesses') else [],\n            \"target_word\": memory.target_word\n        }\n\n    async def exec(self, prep_res):\n        hint = prep_res[\"hint\"]\n        past_guesses = prep_res[\"past_guesses\"]\n        prompt = f\"\"\"\nGiven hint: {hint}\nPast wrong guesses: {past_guesses}\nMake a new guess for the target word.\nReply only with the guessed word.\n\"\"\"\n        return await call_llm(prompt)\n\n    async def post(self, memory: Memory, prep_res, guess):\n        target_word = prep_res[\"target_word\"]\n        if guess.lower() == target_word.lower():\n            print('Game Over - Correct guess!')\n            await memory.hinter_queue.put(\"GAME_OVER\")\n            self.trigger('end')\n            return\n\n        if not hasattr(memory, 'past_guesses'):\n            memory.past_guesses = []\n        memory.past_guesses.append(guess)\n\n        await memory.hinter_queue.put(guess)\n        self.trigger('continue')\n```\n\n----------------------------------------\n\nTITLE: Implementing Execution Runner with Retry Logic for Node in BrainyFlow Python\nDESCRIPTION: Implements the `exec_runner` method for the standard `Node`. It executes the node's `exec` method, handling retries based on the `max_retries` and `wait` configuration. If retries are exhausted, it calls `exec_fallback`.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nasync exec_runner(self, memory, prep_res)\n```\n\n----------------------------------------\n\nTITLE: Applying Rate Limiting with Decorators (ratelimit) in Python\nDESCRIPTION: Demonstrates using the `@limits` and `@sleep_and_retry` decorators from the `ratelimit` library to enforce a simple window-based rate limit (e.g., 30 calls per 60 seconds) on a function (`call_api`). If the limit is exceeded, the decorator handles waiting before retrying. Depends on the `ratelimit` library.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/throttling.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom ratelimit import limits, sleep_and_retry\n\n# 30 calls per minute\n@sleep_and_retry\n@limits(calls=30, period=60)\ndef call_api():\n    # Your API call here\n    pass\n\n```\n\n----------------------------------------\n\nTITLE: Defining RAG Flow in Python\nDESCRIPTION: Python code that defines an online RAG flow by instantiating nodes and connecting them with transitions. It creates a pipeline from query embedding to document retrieval to answer generation.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/rag.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# --- Online Flow Definition ---\nembed_qnode = EmbedQueryNode()\nretrieve_node = RetrieveDocsNode()\ngenerate_node = GenerateAnswerNode()\n\n# Define transitions using syntax sugar\nembed_qnode - 'retrieve_docs' >> retrieve_node\nretrieve_node - 'generate_answer' >> generate_node\n\nOnlineFlow = Flow(start=embed_qnode)\n```\n\n----------------------------------------\n\nTITLE: Creating Python Custom Nodes in BrainyFlow\nDESCRIPTION: Demonstrates how to extend the `Node` class to create a custom node in Python. Implements the `prep`, `exec`, and `post` methods of the node lifecycle, using the `Memory` proxy for handling data. Triggers next actions using `this.trigger()`.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/core_abstraction/node.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom brainyflow import Node, Memory\n\nclass TextProcessorNode(Node):\n    async def prep(self, memory: Memory):\n        # Read input data\n        return memory.text\n\n    async def exec(self, text: str):\n        # Process the text\n        return text.upper()\n\n    async def post(self, memory: Memory, input_text: str, result: str):\n        # Store the result in the global store\n        memory.processed_text = result\n\n        # Trigger the default next node (optional)\n        self.trigger('default')\n\n```\n\n----------------------------------------\n\nTITLE: Setting Up RAG System with Data Loading in Python\nDESCRIPTION: Initializes the system by loading essay data from files, creating node instances for the RAG workflow, and connecting them to form a flow. The system loads Paul Graham essays from a directory and creates a flow with three main nodes.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python_demo.ipynb#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Create test data\nshared = {\"data\": {}}\n\n# Load all files\npath = \"./data/PaulGrahamEssaysLarge\"\nfor filename in os.listdir(path):\n    with open(os.path.join(path, filename), \"r\") as f:\n        shared[\"data\"][filename] = f.read()\n\n# Create nodes and flow\nprep_embeddings = PrepareEmbeddings()\nfind_relevant = FindRelevantDocument()\nanswer = AnswerQuestion()\n\n# Connect nodes\nprep_embeddings >> find_relevant\nfind_relevant - \"answer\" >> answer\nfind_relevant - \"end\" >> None\nanswer - \"continue\" >> find_relevant\n\n# Create and run flow\nrag_flow = Flow(start=prep_embeddings)\nawait rag_flow.run(shared)\n```\n\n----------------------------------------\n\nTITLE: Getting Text Embeddings with OpenAI API in TypeScript\nDESCRIPTION: TypeScript implementation for retrieving text embeddings from OpenAI. Uses the OpenAI Node.js SDK, handles errors, and supports model configuration. Returns embedding as a number array or null if an error occurs.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/embedding.md#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n// Requires: npm install openai\nimport OpenAI from 'openai'\n\nasync function getOpenAIEmbedding(\n  text: string,\n  model: string = 'text-embedding-3-small',\n): Promise<number[] | null> {\n  /** Gets embedding from OpenAI API. */\n  const apiKey = process.env.OPENAI_API_KEY\n  if (!apiKey) {\n    console.error('Error: OPENAI_API_KEY not set.')\n    return null\n  }\n  try {\n    const openai = new OpenAI({ apiKey })\n    const response = await openai.embeddings.create({\n      input: text,\n      model: model,\n    })\n    return response.data[0].embedding\n  } catch (error) {\n    console.error('Error calling OpenAI embedding API:', error)\n    return null\n  }\n}\n\n// Example:\n// const textToEmbed = \"Hello world\";\n// getOpenAIEmbedding(textToEmbed).then(embedding => {\n//   if (embedding) {\n//     console.log(embedding);\n//   }\n// });\n```\n\n----------------------------------------\n\nTITLE: Getting Text Embeddings with HuggingFace API - TypeScript\nDESCRIPTION: Asynchronous function to get text embeddings using HuggingFace's Inference API. Handles authentication with optional token and supports different model response structures. Returns embedding vector as number array or null on failure.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/embedding.md#2025-04-22_snippet_16\n\nLANGUAGE: typescript\nCODE:\n```\nasync function getHfEmbedding(\n  text: string,\n  modelUrl: string = 'https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2',\n): Promise<number[] | null> {\n  /** Gets embedding from Hugging Face Inference API. */\n  const hfToken = process.env.HUGGINGFACE_TOKEN\n  const headers: HeadersInit = { 'Content-Type': 'application/json' }\n  if (hfToken) {\n    headers['Authorization'] = `Bearer ${hfToken}`\n  } else {\n    console.warn('Warning: HUGGINGFACE_TOKEN not set. Public models might work without it.')\n  }\n\n  const payload = JSON.stringify({ inputs: text })\n\n  try {\n    const response = await fetch(modelUrl, {\n      method: 'POST',\n      headers: headers,\n      body: payload,\n    })\n\n    if (!response.ok) {\n      throw new Error(`HTTP error! status: ${response.status}, message: ${await response.text()}`)\n    }\n\n    const result = await response.json()\n    // Handle potential variations in response structure\n    if (Array.isArray(result) && result.length > 0 && Array.isArray(result[0])) {\n      return result[0] // Common case for sentence-transformers\n    } else if (Array.isArray(result) && result.length > 0 && typeof result[0] === 'number') {\n      return result // Flat list for single input\n    } else {\n      console.error('Unexpected response structure from HF API:', result)\n      return null\n    }\n  } catch (error) {\n    console.error('Error calling Hugging Face Inference API:', error)\n    return null\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining BrainyFlow Shared Store Schema with Python Type Hints\nDESCRIPTION: Illustrates how to define the structure of the BrainyFlow shared store using Python, specifically with `TypedDict` from the `typing` module for better type safety and clarity. It shows conceptual input, processing, and output store structures and provides an example dictionary representing the conceptual state of memory during flow execution.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/agentic_coding.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import TypedDict, List, Dict, Any\n\n# Define TypedDicts for structure (optional but good practice)\nclass InputStore(TypedDict):\n    document_path: str\n\nclass ProcessingStore(TypedDict):\n    document_text: str\n    entities: Dict[str, List[Any]] # e.g., {\"parties\": [], \"dates\": [], \"amounts\": []}\n    validation_status: str\n\nclass OutputStore(TypedDict):\n    summary: str\n    storage_id: str\n\n# Conceptual structure of the memory object using separate keys\n# (Actual implementation might use a single dict or class instance)\nmemory_conceptual = {\n    \"document_path\": \"path/to/file.pdf\", # str\n    \"document_text\": \"\",                 # str\n    \"entities\": {                        # Dict[str, List[Any]]\n        \"parties\": [],\n        \"dates\": [],\n        \"amounts\": []\n    },\n    \"validation_status\": \"\",             # str\n    \"summary\": \"\",                       # str\n    \"storage_id\": \"\"                     # str\n}\n\n# Note: In BrainyFlow, you typically access these directly, e.g.,\n# memory.document_text = \"...\"\n# entities = memory.entities\n# This conceptual breakdown helps in planning the data flow.\n```\n\n----------------------------------------\n\nTITLE: Querying SerpApi using Python\nDESCRIPTION: Performs a search request to SerpApi using Python's requests library. The code handles API authentication via environment variables, constructs the request with parameters, and processes the response while implementing error handling.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/websearch.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport requests\nimport os\n\nAPI_KEY = os.environ.get(\"SERPAPI_KEY\") # Use environment variables\nquery = \"example\"\n\nurl = \"https://serpapi.com/search\"\nparams = {\n    \"engine\": \"google\", # Or other engines like 'bing', 'duckduckgo'\n    \"q\": query,\n    \"api_key\": API_KEY\n}\n\nif not API_KEY:\n    print(\"Error: Please set SERPAPI_KEY environment variable.\")\nelse:\n    try:\n        response = requests.get(url, params=params)\n        response.raise_for_status()\n        results = response.json()\n        print(results)\n    except requests.exceptions.RequestException as e:\n        print(f\"Error fetching SerpApi results: {e}\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Concurrency Control with asyncio.Semaphore in Python\nDESCRIPTION: Defines a BrainyFlow `Node` subclass (`LimitedParallelNode`) that utilizes `asyncio.Semaphore` to restrict the number of concurrent executions of the `process_one_item` method within the `exec` phase. It fetches items in `prep` and stores results in `post`. Depends on the `asyncio` library and the `brainyflow` framework.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/throttling.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom brainyflow import Node, Memory # Assuming imports\n\nclass LimitedParallelNode(Node):\n    def __init__(self, concurrency_limit: int = 3, **kwargs): # Allow passing other Node args\n        super().__init__(**kwargs) # Call parent constructor\n        if concurrency_limit <= 0:\n            raise ValueError(\"Concurrency limit must be positive\")\n        self._semaphore = asyncio.Semaphore(concurrency_limit)\n        print(f\"Node initialized with concurrency limit: {concurrency_limit}\")\n\n    # Prep is usually needed to get 'items' from memory\n    async def prep(self, memory: Memory):\n        # Example: Fetch items from memory\n        items = memory.items_to_process or []\n        print(f\"Prep: Found {len(items)} items to process.\")\n        return items # Assuming items are in memory.items_to_process\n\n    async def exec(self, items: list): # exec receives result from prep\n        if not items:\n            print(\"Exec: No items to process.\")\n            return []\n\n        async def limited_task_runner(item):\n            async with self._semaphore:\n                print(f\" Starting processing item: {item}\")\n                # process_one_item should ideally be defined in the subclass or passed in\n                result = await self.process_one_item(item) # Renamed for clarity\n                print(f\" Finished processing item: {item} -> {result}\")\n                return result\n\n        print(f\"Exec: Starting processing of {len(items)} items with limit {self._semaphore._value}...\")\n        tasks = [limited_task_runner(item) for item in items]\n        results = await asyncio.gather(*tasks)\n        print(\"Exec: All items processed.\")\n        return results\n\n    async def process_one_item(self, item):\n        \"\"\"Placeholder: Subclasses must implement this method.\"\"\"\n        # Example implementation:\n        await asyncio.sleep(0.5) # Simulate async work\n        return f\"Processed_{item}\"\n        # raise NotImplementedError(\"process_one_item must be implemented by subclasses\")\n\n    # Post is needed to store results and trigger next step\n    async def post(self, memory: Memory, prep_res: list, exec_res: list):\n        print(f\"Post: Storing {len(exec_res)} results.\")\n        memory.processed_results = exec_res # Store results\n        self.trigger('default') # Trigger next node\n\n```\n\n----------------------------------------\n\nTITLE: Visualizing Brainyflow Execution with a Mermaid Sequence Diagram\nDESCRIPTION: Presents a Mermaid sequence diagram illustrating the step-by-step execution of a simple Brainyflow process. It shows the interactions between the `Flow`, individual `Nodes` (Node A, Node B), and the `Shared Store` as the flow progresses, including reading/writing to the store and triggering transitions based on returned actions.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/core_abstraction/flow.md#2025-04-22_snippet_2\n\nLANGUAGE: mermaid\nCODE:\n```\nsequenceDiagram\n    participant S as Shared Store\n    participant F as Flow\n    participant N1 as Node A\n    participant N2 as Node B\n\n    F->>N1: Execute Node A\n    N1->>S: Read from shared store\n    N1->>N1: Perform computation\n    N1->>S: Write to shared store\n    N1-->>F: Return action \"default\"\n\n    F->>F: Find next node for action \"default\"\n    F->>N2: Execute Node B\n    N2->>S: Read from shared store\n    N2->>N1: Read from local store\n    N2->>N2: Perform computation\n    N2->>S: Write to shared store\n    N2-->>F: Return action \"success\"\n\n    F->>F: No transition defined for \"success\"\n    F-->>F: Flow execution complete\n```\n\n----------------------------------------\n\nTITLE: Implementing Article Writing Workflow in Python with BrainyFlow\nDESCRIPTION: Creates a three-node workflow for article generation using BrainyFlow framework. Includes outline generation, content writing, and review phases. Uses async/await pattern and memory management for state preservation between nodes.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/workflow.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom brainyflow import Node, Flow, Memory\n\n# Assume call_llm is defined elsewhere\n# async def call_llm(prompt: str) -> str: ...\n\nclass GenerateOutline(Node):\n    async def prep(self, memory: Memory): return memory.topic\n    async def exec(self, topic): return await call_llm(f\"Create a detailed outline for an article about {topic}\")\n    async def post(self, memory: Memory, prep_res, exec_res):\n        memory.outline = exec_res\n        self.trigger('default')\n\nclass WriteSection(Node):\n    async def prep(self, memory: Memory): return memory.outline\n    async def exec(self, outline): return await call_llm(f\"Write content based on this outline: {outline}\")\n    async def post(self, memory: Memory, prep_res, exec_res):\n        memory.draft = exec_res\n        self.trigger('default')\n\nclass ReviewAndRefine(Node):\n    async def prep(self, memory: Memory): return memory.draft\n    async def exec(self, draft): return await call_llm(f\"Review and improve this draft: {draft}\")\n    async def post(self, memory: Memory, prep_res, exec_res):\n        memory.final_article = exec_res\n        # No trigger needed if this is the end of the flow\n\n# Connect nodes\noutline = GenerateOutline()\nwrite = WriteSection()\nreview = ReviewAndRefine()\n\noutline >> write >> review\n\n# Create and run flow\nwriting_flow = Flow(start=outline)\n\nasync def main():\n    memory = {\"topic\": \"AI Safety\"}\n    await writing_flow.run(memory) # Pass memory object\n    print(\"Final Article:\", memory.get(\"final_article\", \"Not generated\")) # Access memory object\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Implementing Agent Pattern Using BrainyFlow - TypeScript\nDESCRIPTION: This TypeScript code example implements the Agent design pattern in BrainyFlow by creating three key node instances—PerceiveNode, ThinkNode, and ActNode—then chaining them in a cyclic manner. It uses the brainyflow package, presupposes implementation of the specified node classes, and demonstrates chaining via the .next() method. The agentFlow object maintains the cyclic workflow, useful for applications needing continuous perceive-think-act cycles. Inputs and outputs are managed internally within the nodes and flow.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/index.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n```typescript\\nimport { Flow, Node } from 'brainyflow'\\n\\n// Define the agent's components (assuming these classes exist)\\nconst perceive = new PerceiveNode()\\nconst think = new ThinkNode()\\nconst act = new ActNode()\\n\\n// Connect them in a cycle\\nperceive.next(think).next(act).next(perceive)\\n\\n// Create the agent flow\\nconst agentFlow = new Flow(perceive)\\n```\n```\n\n----------------------------------------\n\nTITLE: Defining Task Nodes with Lifecycle and Triggers - Brainyflow BaseNode - TypeScript\nDESCRIPTION: This abstraction details the BaseNode class, providing the lifecycle, connectivity, and transition logic for a single node in a workflow graph. BaseNode supports asynchronous setup (prep), execution (exec), and post-processing (post), with hooks for action-based branch triggering and memory forking. Nodes manage successors via an internal Map and support deep cloning for branching and cycle detection. Implementations must extend execRunner for core operation logic. Dependencies include Map, async/await support, and error-handling patterns; inputs are a Memory object and action strings, and outputs are arrays or maps of triggered results. Limitation: meant to be used within a Flow orchestrator for multi-node execution.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/design.md#2025-04-22_snippet_1\n\nLANGUAGE: TypeScript\nCODE:\n```\nabstract class BaseNode {\n  // Successors: Map<Action, BaseNode[]>\n  // Triggers: stores actions/forkingData set during 'post'\n  // Locked: disallows 'trigger' outside 'post'\n  // __nodeOrder: unique id for node tracking\n  // on(action, node): defines directed graph links\n  // next(node, action): convenience for on\n  // getNextNodes(action): retrieves successors for action\n  // prep(memory): async setup phase\n  // exec(prepRes): async task logic phase (override in subclass)\n  // post(memory, prepRes, execRes): async completion phase, where 'trigger' can be used\n  // trigger(action, data): signals which branches/forks to follow with optional local state\n  // listTriggers(memory): returns [[action, memory]] pairs for downstream nodes\n  // execRunner(memory, prepRes): abstract, must be implemented by subclasses (e.g., for retries)\n  // run(memory, propagate?): runs node lifecycle; use propagate for internal flow execution\n  // clone(seen?): deep copy supporting cyclic graphs\n}\n\n```\n\n----------------------------------------\n\nTITLE: Implementing a Data Science Flow Example in TypeScript\nDESCRIPTION: This TypeScript example sets up a flow using dummy node and flow classes, mimicking a data science workflow. By connecting nodes sequentially and grouping them into Flow classes, the example demonstrates structural flow setup in TypeScript. It utilizes the `buildMermaid` function to produce a visualization in Mermaid notation.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/visualization_logging.md#2025-04-22_snippet_3\n\nLANGUAGE: TypeScript\nCODE:\n```\n// Define dummy nodes and flows for the example structure\nclass DataPrepNode extends Node {}\nclass ValidateDataNode extends Node {}\nclass FeatureExtractionNode extends Node {}\nclass TrainModelNode extends Node {}\nclass EvaluateModelNode extends Node {}\nclass ModelFlow extends Flow {}\nclass DataScienceFlow extends Flow {}\n\n// Instantiate and connect\nconst featureNode = new FeatureExtractionNode()\nconst trainNode = new TrainModelNode()\nconst evaluateNode = new EvaluateModelNode()\nfeatureNode.next(trainNode).next(evaluateNode)\n\nconst modelFlow = new ModelFlow(featureNode) // Flow starting with featureNode\n\nconst dataPrepNode = new DataPrepNode()\nconst validateNode = new ValidateDataNode()\ndataPrepNode.next(validateNode).next(modelFlow)\n\nconst dataScienceFlow = new DataScienceFlow(dataPrepNode) // Top-level flow\n\n// Generate Mermaid string (assuming buildMermaid function is defined as above)\nconst result = buildMermaid(dataScienceFlow)\nconsole.log(result) // Output the Mermaid string\n```\n\n----------------------------------------\n\nTITLE: Implementing Text Summarization to YAML using Brainyflow Node in Python\nDESCRIPTION: Defines a Python class `SummarizeNode` inheriting from `brainyflow.Node`. It takes text from memory (`prep`), prompts an LLM to summarize it into a specific 3-bullet-point YAML structure, calls the LLM (via assumed `call_llm`), extracts and parses the YAML block from the response using `pyyaml`, validates the structure, handles parsing/validation errors, and returns the structured dictionary (`exec`). The `post` method stores the structured summary back into memory. Requires `yaml`, `brainyflow`, and an assumed async `call_llm` function.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/structure.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport yaml\nfrom brainyflow import Node, Memory\n\n# Assume call_llm is defined elsewhere\n# async def call_llm(prompt: str) -> str: ...\n\nclass SummarizeNode(Node):\n    async def prep(self, memory: Memory):\n        # Assuming the text to summarize is in memory.text\n        return memory.text or \"\"\n\n    async def exec(self, text_to_summarize: str):\n        if not text_to_summarize:\n             return {\"summary\": [\"No text provided\"]}\n\n        prompt = f\"\"\"\nPlease summarize the following text as YAML, with exactly 3 bullet points:\n\n{text_to_summarize}\n\nNow, output ONLY the YAML structure:\n```yaml\nsummary:\n  - bullet 1\n  - bullet 2\n  - bullet 3\n```\"\"\"\n        response = await call_llm(prompt)\n        structured_result: dict\n\n        try:\n            # Extract YAML block\n            yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n            structured_result = yaml.safe_load(yaml_str)\n\n            # Basic validation\n            if not isinstance(structured_result, dict) or \"summary\" not in structured_result or not isinstance(structured_result[\"summary\"], list):\n                 raise ValueError(\"Invalid YAML structure\")\n\n        except (IndexError, ValueError, yaml.YAMLError) as e:\n            print(f\"Failed to parse structured output: {e}\")\n            # Handle error, maybe return a default structure or re-throw\n            return {\"summary\": [f\"Error parsing summary: {e}\"]}\n\n        return structured_result # e.g., {\"summary\": [\"Point 1\", \"Point 2\", \"Point 3\"]}\n\n    async def post(self, memory: Memory, prep_res, exec_res: dict):\n        # Store the structured result in memory\n        memory.structured_summary = exec_res\n        print(\"Stored structured summary:\", exec_res)\n        # No trigger needed if this is the end of the flow/branch\n```\n\n----------------------------------------\n\nTITLE: Refactoring Memory Management in BrainyFlow (TypeScript)\nDESCRIPTION: These TypeScript snippets demonstrate the change from dictionary-based shared state to Memory objects with property access in BrainyFlow v1.0. The older snippet uses Record type for shared state and returns next action names directly, while the newer one leverages the Memory interface and explicit 'this.trigger' calls. Usage requires BrainyFlow's Node class and properly typed memory, and is meant for use in node/flow composition for branching logic. Dependency is on the Node and Memory typings from BrainyFlow, and input is expected to provide specific fields (e.g., input_text).\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/migration.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// Before (v0.2)\nclass MyNode extends Node {\n  async prep(shared: Record): Promise {\n    return shared['input_text']\n  }\n\n  async post(shared: Record, prepRes: string, execRes: string): Promise {\n    shared['result'] = execRes\n    return 'default' // Action name as return value\n  }\n}\n```\n\nLANGUAGE: typescript\nCODE:\n```\n// After (v1.0)\nclass MyNode extends Node {\n  async prep(memory: Memory): Promise {\n    return memory.input_text // Property access syntax\n  }\n\n  async post(memory: Memory, prepRes: string, execRes: string): Promise {\n    memory.result = execRes // Property assignment syntax\n    this.trigger('default') // Explicit trigger call\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Writing to Memory in Python\nDESCRIPTION: Illustrates writing to global and local memory within a BrainyFlow node in Python. It assigns data to the memory object for global writes and uses forking data in triggers to update local memory.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/core_abstraction/memory.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom brainyflow import Node, Memory\n\n# Assume exec returns a dict like {\"files\": [...], \"count\": ...}\nclass DataWriterNode(Node):\n    async def post(self, memory: Memory, prep_res, exec_res: dict):\n        # --- Writing to Global Store ---\n        # Accessible to all nodes in the flow and outside\n        memory.fileList = exec_res[\"files\"]\n        print(f\"Memory updated globally: fileList={memory.fileList}\")\n\n        # --- Writing to Local Store ---\n        # Accessible to this node and all descendants\n        memory.local.processedCount = exec_res[\"count\"]\n        print(f\"Memory updated locally: processedCount={memory.processedCount}\")\n\n        # --- Triggering with Local Data (Forking Data) ---\n        # 'file' will be added to the local store of the memory clone\n        # passed to the node(s) triggered by the 'process_file' action.\n        for file_item in exec_res[\"files\"]:\n            self.trigger('process_file', { \"file\": file_item })\n\n# Example Processor Node (triggered by 'process_file')\nclass FileProcessorNode(Node):\n     async def prep(self, memory: Memory):\n         # Reads 'file' from the local store first, then global\n         file_to_process = memory.file\n         print(f\"Processing file (fetched from local memory): {file_to_process}\")\n         return file_to_process\n     # ... exec, post ...\n```\n\n----------------------------------------\n\nTITLE: Redis Vector Search Implementation with Python\nDESCRIPTION: Implementation of Redis vector search functionality including client initialization and index creation. Supports vector fields, custom schema definition, and JSON document indexing.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/vector.md#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef init_redis_client() -> redis.Redis | None:\n    \"\"\"Initializes Redis client.\"\"\"\n    redis_host = os.environ.get(\"REDIS_HOST\", \"localhost\")\n    redis_port = int(os.environ.get(\"REDIS_PORT\", 6379))\n    redis_password = os.environ.get(\"REDIS_PASSWORD\")\n    try:\n        client = redis.Redis(host=redis_host, port=redis_port, password=redis_password, decode_responses=True)\n        client.ping()\n        print(f\"Redis client connected to {redis_host}:{redis_port}.\")\n        return client\n    except Exception as e:\n        print(f\"Error connecting to Redis: {e}\")\n        return None\n\ndef create_redis_index_if_not_exists(client: redis.Redis, index_name: str, prefix: str, dimension: int):\n    \"\"\"Creates a Redis Search index for vectors if it doesn't exist.\"\"\"\n    schema = (\n        VectorField(\"$.embedding\", \"FLAT\", {\n            \"TYPE\": \"FLOAT32\",\n            \"DIM\": dimension,\n            \"DISTANCE_METRIC\": \"L2\",\n        }, as_name=\"vector\")\n    )\n    definition = IndexDefinition(prefix=[prefix], index_type=IndexType.JSON)\n\n    try:\n        client.ft(index_name).info()\n        print(f\"Redis index '{index_name}' already exists.\")\n    except:\n        print(f\"Creating Redis index '{index_name}' for prefix '{prefix}'...\")\n        client.ft(index_name).create_index(fields=schema, definition=definition)\n        print(f\"Redis index '{index_name}' created.\")\n```\n\n----------------------------------------\n\nTITLE: Getting Text Embeddings with Azure OpenAI API in Python\nDESCRIPTION: Retrieves text embeddings using Azure OpenAI service. Requires Azure-specific configuration including endpoint and deployment name. Returns embeddings as a numpy array with proper error handling.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/embedding.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Requires: pip install openai numpy\nimport openai\nimport os\nimport numpy as np\n\ndef get_azure_embedding(text: str, deployment_name: str = \"text-embedding-ada-002\") -> np.ndarray | None:\n    \"\"\"Gets embedding from Azure OpenAI API.\"\"\"\n    azure_endpoint = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n    azure_api_key = os.environ.get(\"AZURE_OPENAI_API_KEY\")\n    if not azure_endpoint or not azure_api_key:\n        print(\"Error: Azure OpenAI environment variables not set.\")\n        return None\n\n    # Configure the Azure OpenAI client\n    client = openai.AzureOpenAI(\n        api_key=azure_api_key,\n        api_version=\"2023-05-15\",  # Update as needed\n        azure_endpoint=azure_endpoint\n    )\n\n    try:\n        response = client.embeddings.create(\n            input=text,\n            deployment_id=deployment_name\n        )\n        embedding = response.data[0].embedding\n        return np.array(embedding, dtype=np.float32)\n    except Exception as e:\n        print(f\"Error calling Azure OpenAI embedding API: {e}\")\n        return None\n\n# Example:\n# text_to_embed = \"Hello world\"\n# embedding_vector = get_azure_embedding(text_to_embed)\n# if embedding_vector is not None:\n#     print(embedding_vector)\n```\n\n----------------------------------------\n\nTITLE: Running the QA Flow and Accessing Results (Python)\nDESCRIPTION: Implements a script to initialize memory, create the QA Flow, execute it asynchronously, and print the resulting question and answer from the shared memory object. Requires concurrency support with asyncio and the create_qa_flow function from earlier. The memory object is passed as a mutable dict. Invoked via entry point check if __name__ == '__main__'.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/getting_started.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom .flow import create_qa_flow # defined in the previous step\n\nasync def main():\n    memory = {} # Initialize empty memory (which acts as the global store)\n    qa_flow = create_qa_flow()\n\n    print(\"Running QA Flow...\")\n    # Run the flow, passing the initial global store.\n    # The flow modifies the memory object in place.\n    # The run method returns the final execution tree (we ignore it here).\n    await qa_flow.run(memory)\n\n    # Access the results stored in the global store\n    print(\"\\n--- Flow Complete ---\")\n    print(f\"Question: {memory.question}\")\n    print(f\"Answer: {memory.answer}\")\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Python Milvus Connection and Collection Management\nDESCRIPTION: Implements core Milvus functionality including connection establishment, collection creation with schema definition, and index creation. Uses pymilvus to handle vector database operations with proper error handling and environment configuration.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/vector.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom pymilvus import connections, utility, FieldSchema, CollectionSchema, DataType, Collection\nimport numpy as np\n\ndef connect_milvus():\n    \"\"\"Connects to Milvus.\"\"\"\n    milvus_uri = os.environ.get(\"MILVUS_URI\", \"http://localhost:19530\") # Or Zilliz Cloud URI\n    token = os.environ.get(\"MILVUS_TOKEN\") # For Zilliz Cloud or authenticated Milvus\n    alias = \"default\"\n    try:\n        print(f\"Connecting to Milvus at {milvus_uri}...\")\n        connections.connect(alias=alias, uri=milvus_uri, token=token)\n        print(\"Milvus connected.\")\n    except Exception as e:\n        print(f\"Error connecting to Milvus: {e}\")\n\ndef create_milvus_collection_if_not_exists(collection_name: str, dimension: int):\n    \"\"\"Creates a Milvus collection if it doesn't exist.\"\"\"\n    alias = \"default\"\n    if not utility.has_collection(collection_name, using=alias):\n        print(f\"Creating collection '{collection_name}'...\")\n        fields = [\n            FieldSchema(name=\"pk\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n            FieldSchema(name=\"embeddings\", dtype=DataType.FLOAT_VECTOR, dim=dimension)\n        ]\n        schema = CollectionSchema(fields, description=\"BrainyFlow Milvus Demo\")\n        collection = Collection(name=collection_name, schema=schema, using=alias)\n        print(f\"Collection '{collection_name}' created.\")\n        index_params = {\n            \"metric_type\": \"L2\",\n            \"index_type\": \"IVF_FLAT\",\n            \"params\": {\"nlist\": 1024}\n        }\n        collection.create_index(field_name=\"embeddings\", index_params=index_params)\n        print(f\"Index created on 'embeddings' field.\")\n    else:\n        print(f\"Collection '{collection_name}' already exists.\")\n    collection = Collection(collection_name)\n    if collection.is_empty:\n         print(f\"Collection '{collection_name}' is empty, loading skipped.\")\n    elif utility.load_state(collection_name) != \"Loaded\":\n         print(f\"Loading collection '{collection_name}'...\")\n         collection.load()\n         print(\"Collection loaded.\")\n    else:\n         print(f\"Collection '{collection_name}' already loaded.\")\n```\n\n----------------------------------------\n\nTITLE: Running BrainyFlow Text Summarization - Python Script\nDESCRIPTION: Example Python code showing how to use BrainyFlow for text summarization with Memory store initialization and flow execution.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-node/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmemory = Memory({\"data\": \"Your text to summarize here...\"})\nawait flow.run(memory)\nprint(\"Summary:\", memory.summary)\n```\n\n----------------------------------------\n\nTITLE: Using Explicit Triggers for Branching in BrainyFlow (TypeScript)\nDESCRIPTION: These TypeScript code blocks demonstrate the new trigger-based branching mechanism in BrainyFlow v1.0. Instead of returning action names to control flow, developers call this.trigger directly based on branching conditions. Correct usage requires a Node implementation and appropriate memory interface. Inputs and outputs depend on custom business logic, with status set based on computed results.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/migration.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n// Before (v0.2)\nasync post(shared: Record, prepRes: any, execRes: number): Promise {\n  if (execRes > 10) {\n    shared[\"status\"] = \"high\";\n    return \"high_value\";\n  } else {\n    shared[\"status\"] = \"low\";\n    return \"low_value\";\n  }\n}\n```\n\nLANGUAGE: typescript\nCODE:\n```\n// After (v1.0)\nasync post(memory: Memory, prepRes: any, execRes: number): Promise {\n  if (execRes > 10) {\n    memory.status = \"high\";\n    this.trigger(\"high_value\");\n  } else {\n    memory.status = \"low\";\n    this.trigger(\"low_value\");\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Storing Vector Index Node in Python\nDESCRIPTION: Defines the `StoreIndexNode` class in Python, inheriting from `Node`. The `prep` method asynchronously retrieves all embeddings from the shared `memory` object, filtering out potential `None` values and printing a warning if any embeddings failed. The `exec` method receives the embeddings, checks if the list is empty, and then creates a vector index using an external `create_index` function. The `post` method stores the created index back into the `memory` object and prints a confirmation message.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/rag.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# 1e. Node to store the final index\nclass StoreIndexNode(Node):\n    async def prep(self, memory: Memory):\n        # Read all embeddings from global memory\n        # Filter out potential None values if embedding failed for some chunks\n        embeddings = [emb for emb in (memory.all_embeds or []) if emb is not None]\n        if len(embeddings) != len(memory.all_embeds or []):\n             print(f\"Warning: Some chunks failed to embed. Indexing {len(embeddings)} embeddings.\")\n        return embeddings\n\n    async def exec(self, all_embeds: list):\n        if not all_embeds:\n             print(\"No embeddings to store.\")\n             return None\n        print(f\"Storing index for {len(all_embeds)} embeddings.\")\n        # Create a vector index (implementation depends on library)\n        index = create_index(all_embeds)\n        return index\n\n    async def post(self, memory: Memory, prep_res, index):\n        # Store the created index in global memory\n        memory.index = index\n        if index:\n             print('Index created and stored.')\n        else:\n             print('Index creation skipped.')\n        # End of offline flow\n```\n```\n\n----------------------------------------\n\nTITLE: Defining Offline Processing Flow using Nodes in Python\nDESCRIPTION: Instantiates various node classes (`TriggerChunkingNode`, `ChunkFileNode`, `TriggerEmbeddingNode`, `EmbedChunkNode`, `StoreIndexNode`) representing steps in an offline data processing pipeline. It defines the transitions between these nodes using operator overloading (`- 'event_name' >> node`) to specify the flow logic. Finally, it creates a `ParallelFlow` instance named `OfflineFlow`, starting with the `trigger_chunking` node, indicating that some steps might run in parallel for efficiency.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/rag.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# --- Offline Flow Definition ---\ntrigger_chunking = TriggerChunkingNode()\nchunk_file = ChunkFileNode()\ntrigger_embedding = TriggerEmbeddingNode()\nembed_chunk = EmbedChunkNode()\nstore_index = StoreIndexNode()\n\n# Define transitions using syntax sugar\ntrigger_chunking - 'chunk_file' >> chunk_file\ntrigger_chunking - 'embed_chunks' >> trigger_embedding\ntrigger_embedding - 'embed_chunk' >> embed_chunk\ntrigger_embedding - 'store_index' >> store_index\n\n# Use ParallelFlow for potentially faster chunking and embedding\nOfflineFlow = ParallelFlow(start=trigger_chunking)\n# Or sequential: OfflineFlow = Flow(start=trigger_chunking)\n```\n```\n\n----------------------------------------\n\nTITLE: Implementing Flow Class RunNode Method in TypeScript\nDESCRIPTION: A core private method `runNode` responsible for executing a single node within the flow. It handles node cloning (if necessary for branching), runs the node's lifecycle (`prep`, `exec`, `post`), processes triggers, manages recursion depth using `maxVisits`, detects cycles, and recursively calls itself or `runNodes` for successor nodes based on triggers.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/api.md#2025-04-22_snippet_27\n\nLANGUAGE: typescript\nCODE:\n```\n`private async runNode(node: BaseNode, memory: Memory<GlobalStore, SharedStore>): Promise<NestedActions<AllowedActions>>`\n```\n\n----------------------------------------\n\nTITLE: Executing Graphs with BrainyFlow Flow in Python\nDESCRIPTION: This snippet presents the 'Flow' class, which orchestrates the sequential execution of connected nodes. It includes methods for running nodes, handling node triggers, and cycle detection to prevent infinite loops during flow execution.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/design.md#2025-04-22_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nbrainyflow.Flow\n```\n\n----------------------------------------\n\nTITLE: Implementing Branching and Looping in a Brainyflow Flow (TypeScript)\nDESCRIPTION: Shows how to define nodes and connections in TypeScript for an expense approval workflow with conditional logic. It uses `.on('action', node)` for conditional branching based on triggered actions (e.g., 'approved', 'needs_revision') and `.next(node)` for default transitions, including creating a loop from the revision step back to the review step.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/core_abstraction/flow.md#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Flow, Node } from 'brainyflow'\n\n// Define the nodes first\n// const review = new ReviewExpenseNode()\n// const revise = new ReviseReportNode()\n// const payment = new ProcessPaymentNode()\n// const finish = new FinishProcessNode()\n\n// Define the flow connections\nreview.on('approved', payment) // If approved, process payment\nreview.on('needs_revision', revise) // If needs changes, go to revision\nreview.on('rejected', finish) // If rejected, finish the process\n\nrevise.next(review) // After revision (default trigger), go back for another review\npayment.next(finish) // After payment (default trigger), finish the process\n\n// Create the flow, starting with the review node\nconst expenseFlow = new Flow(review)\n```\n\n----------------------------------------\n\nTITLE: Mocking and Testing LLM Logic in TypeScript\nDESCRIPTION: This TypeScript snippet uses the Vitest framework to mock and test LLM node behavior. It demonstrates how to configure mock implementations for different inputs and verify correct interaction with the LLM utility function, using TypeScript's async/await pattern.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/testing.md#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { describe, expect, it, vi } from 'vitest'\nimport { callLLM } from './utils/callLLM' // Your LLM utility\n\n// import { MyLlmNode } from './MyLlmNode'; // Your Node implementation\n// import { Memory } from 'brainyflow'; // Assuming Memory is imported if needed\n\n// Mock the LLM utility module\nvi.mock('./utils/callLLM', () => ({\n  callLLM: vi.fn(), // Create a mock function\n}))\n\ndescribe('Testing LLM Nodes', () => {\n  it('should use canned responses based on prompt', async () => {\n    // Configure the mock implementation\n    vi.mocked(callLLM).mockImplementation(async (prompt: string): Promise<string> => {\n      if (prompt.toLowerCase().includes('summarize')) {\n        return 'This is a summary.'\n      } else if (prompt.toLowerCase().includes('extract')) {\n        return JSON.stringify({ key: 'value' }) // Return JSON string\n      } else {\n        return 'Default response'\n      }\n    })\n\n    // const node = new MyLlmNode();\n    // const memory = { input: 'some text to summarize' }; // Initial memory state\n\n    // await node.run(memory); // Run the node\n\n    // Add assertions here\n    // expect(callLLM).toHaveBeenCalled();\n    // expect(memory.summary).toBe('This is a summary.');\n  })\n})\n```\n\n----------------------------------------\n\nTITLE: Retrieving Document Chunks Node in TypeScript\nDESCRIPTION: Defines the `RetrieveDocsNode` class in TypeScript. The `prep` method retrieves the query embedding (`qEmb`), index, and all chunks (`chunks`) from the `memory` object. The `exec` method checks for missing data, then calls an external `searchIndex` function to find the ID of the top-ranking chunk based on the query embedding. It handles cases where no relevant chunk is found or the ID is invalid, otherwise returning the text of the retrieved chunk. The `post` method stores the `relevantChunk` in `memory` and triggers the 'generate_answer' event.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/rag.md#2025-04-22_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\n// 2b. Retrieve Docs Node\nclass RetrieveDocsNode extends Node {\n  async prep(memory: Memory): Promise<{ qEmb: number[]; index: any; chunks: string[] }> {\n    // Need query embedding, index, and original chunks\n    return { qEmb: memory.q_emb, index: memory.index, chunks: memory.all_chunks }\n  }\n  async exec(prepRes: { qEmb: number[]; index: any; chunks: string[] }): Promise<string> {\n    const { qEmb, index, chunks } = prepRes\n    if (!qEmb || !index || !chunks) {\n      throw new Error('Missing data for retrieval')\n    }\n    console.log('Retrieving relevant chunk...')\n    const [I, D] = searchIndex(index, qEmb, 1) // Find top 1 chunk\n    const bestId = I?.[0]?.[0]\n    if (bestId === undefined || bestId >= chunks.length) {\n      return 'Could not find relevant chunk.'\n    }\n    const relevantChunk = chunks[bestId]\n    return relevantChunk\n  }\n  async post(memory: Memory, prepRes: any, relevantChunk: string): Promise<void> {\n    memory.retrieved_chunk = relevantChunk\n    console.log(`Retrieved chunk: ${relevantChunk.slice(0, 60)}...`)\n    this.trigger('generate_answer')\n  }\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Node Class with Python Decorator\nDESCRIPTION: Demonstrates a Python implementation of a basic Node class using a trace_node decorator. The class includes prep, exec, and post methods with basic memory management and asynchronous execution.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/visualization_logging.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n@trace_node\nclass MyNode(Node):\n    async def prep(self, memory: Memory):\n        # logger.info(f\"Reading from memory: {memory.some_input}\") # Example read\n        return \"data\"\n\n    async def exec(self, prep_res):\n        await asyncio.sleep(0.05) # Simulate work\n        return prep_res.upper()\n\n    async def post(self, memory: Memory, prep_res, exec_res):\n        memory.result = exec_res # Write to memory object\n        self.trigger(\"default\")\n```\n\n----------------------------------------\n\nTITLE: Synthesizing Speech with Azure TTS in TypeScript\nDESCRIPTION: This TypeScript function `synthesizeAzureTts` uses the `microsoft-cognitiveservices-speech-sdk` to convert input text to speech via Azure Cognitive Services Text-to-Speech. It requires Azure Speech Key and Region environment variables (`AZURE_SPEECH_KEY`, `AZURE_SPEECH_REGION`). The function takes the text to synthesize and an optional output filename (defaults to `azure_tts_output.wav`). It returns a Promise that resolves on successful audio file creation or rejects on error. It uses `SpeechConfig` for authentication and `AudioConfig` to specify the output file, then calls `speakTextAsync` on a `SpeechSynthesizer`.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/text_to_speech.md#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\n// Requires: npm install microsoft-cognitiveservices-speech-sdk\nimport * as fs from 'fs' // Only needed if saving to file manually, SDK handles it\nimport * as sdk from 'microsoft-cognitiveservices-speech-sdk'\n\nfunction synthesizeAzureTts(\n  text: string,\n  outputFilename: string = 'azure_tts_output.wav',\n): Promise<void> {\n  /** Synthesizes speech using Azure Cognitive Services TTS. */\n  return new Promise((resolve, reject) => {\n    const speechKey = process.env.AZURE_SPEECH_KEY\n    const serviceRegion = process.env.AZURE_SPEECH_REGION\n\n    if (!speechKey || !serviceRegion) {\n      console.error('Error: AZURE_SPEECH_KEY or AZURE_SPEECH_REGION not set.')\n      return reject(new Error('Azure credentials not set.'))\n    }\n\n    const speechConfig = sdk.SpeechConfig.fromSubscription(speechKey, serviceRegion)\n    // Example voice, check documentation for more\n    // speechConfig.speechSynthesisVoiceName = \"en-US-JennyNeural\";\n\n    // Synthesize to an audio file directly using the SDK\n    const audioConfig = sdk.AudioConfig.fromAudioFileOutput(outputFilename)\n\n    const synthesizer = new sdk.SpeechSynthesizer(speechConfig, audioConfig)\n\n    synthesizer.speakTextAsync(\n      text,\n      (result) => {\n        if (result.reason === sdk.ResultReason.SynthesizingAudioCompleted) {\n          console.log(`Audio saved to ${outputFilename}`)\n          resolve()\n        } else {\n          console.error(`Speech synthesis canceled: ${result.errorDetails}`)\n          reject(new Error(`Speech synthesis failed: ${result.errorDetails}`))\n        }\n        synthesizer.close() // Close synthesizer after completion/error\n      },\n      (error) => {\n        console.error(`Error during synthesis: ${error}`)\n        synthesizer.close()\n        reject(error)\n      },\n    )\n  })\n}\n\n// Example:\n// synthesizeAzureTts(\"Hello from Azure Text-to-Speech!\")\n//     .then(() => console.log(\"Azure TTS synthesis finished.\"))\n//     .catch(error => console.error(\"Azure TTS synthesis failed:\", error));\n```\n\n----------------------------------------\n\nTITLE: Extracting Call Stack in TypeScript\nDESCRIPTION: Extracts call stack information in TypeScript from an error's stack trace, identifying Node class methods based on naming conventions. Uses regular expressions to filter stack traces and emphasizes Node class detection.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/visualization_logging.md#2025-04-22_snippet_5\n\nLANGUAGE: TypeScript\nCODE:\n```\nfunction getNodeCallStack(): string[] {\n  // Create a new Error to capture the stack trace\n  const stackTrace = new Error().stack || ''\n  const nodeNames: string[] = []\n  const seenIds = new Set()\n\n  // Parse the stack trace to extract node information\n  // This is a simplified implementation - in practice you would need\n  // a more robust approach to track Node instances\n\n  const stackFrames = stackTrace.split('\\n').slice(1) // Skip Error constructor\n\n  for (const frame of stackFrames) {\n    // Look for Node class method calls in the stack trace\n    // Format typically: \"at NodeClassName.methodName\"\n    const match = frame.match(/at\\s+(\\w+)\\.(prep|exec|post)/)\n\n    if (match) {\n      const className = match[1]\n      // Check if this is likely a Node class (ends with \"Node\" or is a Flow)\n      if ((className.endsWith('Node') || className.endsWith('Flow')) && !seenIds.has(className)) {\n        seenIds.add(className)\n        nodeNames.push(className)\n      }\n    }\n  }\n\n  return nodeNames\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Memory Class Constructor in TypeScript\nDESCRIPTION: The constructor for the `Memory` class. It initializes the instance with a mandatory global store (`__global`) and an optional local store (`__local`), both conforming to generic `SharedStore` types `G` and `L` respectively.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/api.md#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\n`constructor(private __global: G, private __local: L = {} as L)`\n```\n\n----------------------------------------\n\nTITLE: Implementing Batch Processing in TypeScript with BrainyFlow\nDESCRIPTION: Demonstrates the fan-out pattern for batch processing in TypeScript, with type-safe interfaces and node implementations. Shows how to trigger multiple processors and collect results in a global memory store.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/core_abstraction/flow.md#2025-04-22_snippet_14\n\nLANGUAGE: typescript\nCODE:\n```\ninterface BatchGlobalStore {\n  items_to_process?: any[]\n  results?: any[]\n}\ninterface BatchLocalStore {\n  item_data?: any\n  result_index?: number\n}\n\nclass TriggerBatchNode extends Node<\n  BatchGlobalStore,\n  BatchLocalStore,\n  ['process_one', 'aggregate']\n> {\n  async prep(memory: Memory<BatchGlobalStore>): Promise<any[]> {\n    return memory.items_to_process ?? []\n  }\n  async post(memory: Memory<BatchGlobalStore>, items: any[], execRes: void): Promise<void> {\n    memory.results = new Array(items.length).fill(null) // Pre-allocate\n    items.forEach((item, index) => {\n      this.trigger('process_one', { item_data: item, result_index: index })\n    })\n    // Optional: this.trigger(\"aggregate\");\n  }\n}\n\nclass ProcessOneItemNode extends Node<BatchGlobalStore, BatchLocalStore> {\n  async prep(\n    memory: Memory<BatchGlobalStore, BatchLocalStore>,\n  ): Promise<{ item: any; index: number }> {\n    return { item: memory.item_data, index: memory.result_index ?? -1 }\n  }\n  async exec(prepRes: { item: any; index: number }): Promise<{ result: any; index: number }> {\n    const result = `Processed ${prepRes.item}` // Placeholder\n    return { result, index: prepRes.index }\n  }\n  async post(\n    memory: Memory<BatchGlobalStore>,\n    prepRes: any,\n    execRes: { result: any; index: number },\n  ): Promise<void> {\n    if (!memory.results) memory.results = []\n    while (memory.results.length <= execRes.index) {\n      memory.results.push(null)\n    }\n    memory.results[execRes.index] = execRes.result\n  }\n}\n\n// Setup\nconst trigger = new TriggerBatchNode()\nconst processor = new ProcessOneItemNode()\ntrigger.on('process_one', processor)\n\n// Choose Flow type\n// const sequentialBatchFlow = new Flow<BatchGlobalStore>(trigger);\nconst parallelBatchFlow = new ParallelFlow<BatchGlobalStore>(trigger)\n\n// Run\n// async function run() {\n//   const memory: BatchGlobalStore = { items_to_process: [\"A\", \"B\", \"C\"] };\n//   await parallelBatchFlow.run(memory);\n//   console.log(memory.results); // Output: ['Processed A', 'Processed B', 'Processed C']\n// }\n// run();\n```\n\n----------------------------------------\n\nTITLE: Initializing, Adding to, and Searching a FAISS Index in Python\nDESCRIPTION: Demonstrates basic FAISS operations in Python using `faiss-cpu` or `faiss-gpu`. Includes functions to create a simple `FlatL2` index, add NumPy vector arrays, and search for nearest neighbors using L2 distance. Requires the `numpy` and `faiss` libraries. Vectors must be converted to `float32` before adding or searching. The example code shows how to use these functions and mentions saving/loading indices for production.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/vector.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Requires: pip install faiss-cpu # or faiss-gpu\nimport faiss\nimport numpy as np\nfrom typing import List, Tuple, Any # Added for type hints\n\ndef create_faiss_index(dimension: int) -> faiss.Index:\n    \"\"\"Creates a simple FAISS index.\"\"\"\n    # Example: Flat L2 index\n    index = faiss.IndexFlatL2(dimension)\n    return index\n\ndef add_to_faiss_index(index: faiss.Index, vectors: np.ndarray):\n    \"\"\"Adds vectors to the FAISS index.\"\"\"\n    # Ensure vectors are float32\n    if vectors.dtype != 'float32':\n        vectors = vectors.astype('float32')\n    index.add(vectors)\n\ndef search_faiss_index(index: faiss.Index, query_vector: np.ndarray, top_k: int = 5) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Searches the FAISS index.\"\"\"\n    if query_vector.dtype != 'float32':\n        query_vector = query_vector.astype('float32')\n    # Ensure query is 2D array\n    if query_vector.ndim == 1:\n        query_vector = np.array([query_vector])\n    distances, indices = index.search(query_vector, top_k)\n    return distances, indices\n\n# Example Usage:\n# d = 128 # Dimensionality of embeddings\n# index = create_faiss_index(d)\n# print(f\"Index created. Is trained: {index.is_trained}, Total vectors: {index.ntotal}\")\n\n# data_vectors = np.random.random((1000, d)).astype('float32')\n# add_to_faiss_index(index, data_vectors)\n# print(f\"Added {data_vectors.shape[0]} vectors. Total vectors: {index.ntotal}\")\n\n# query = np.random.random((1, d)).astype('float32')\n# D, I = search_faiss_index(index, query, k=5)\n\n# print(\"Distances:\", D)\n# print(\"Neighbors:\", I)\n\n# In production, you would also add functions to:\n# - save_index(index, filename) -> faiss.write_index(index, filename)\n# - load_index(filename) -> faiss.read_index(filename)\n```\n\n----------------------------------------\n\nTITLE: Composing QA Flow from Nodes (TypeScript)\nDESCRIPTION: Creates and returns a QA Flow by instantiating GetQuestionNode and AnswerNode, connecting them with .next(), and passing the starting node to a new Flow. Assumes brainyflow and node classes are imported. The flow is instantiated with the correct topology. Inputs: none; output: Flow instance.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/getting_started.md#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n// import { GetQuestionNode, AnswerNode } from './nodes'; // defined in the previous step\nimport { Flow } from 'brainyflow'\n\nfunction createQaFlow(): Flow {\n  const getQuestionNode = new GetQuestionNode()\n  const answerNode = new AnswerNode()\n\n  // Connect nodes getQuestionNode → answerNode using the default action\n  getQuestionNode.next(answerNode)\n\n  // Create the Flow, specifying the starting node\n  return new Flow(getQuestionNode)\n}\n```\n\n----------------------------------------\n\nTITLE: Integrating LLM Wrapper in BrainyFlow Node (TypeScript)\nDESCRIPTION: This TypeScript snippet embeds a custom LLM wrapper within a BrainyFlow node. It provides a framework for using a language model in a Node subclass, showing how to manage memory and interactions with the language model. It relies on the `brainyflow` library and the custom LLM wrapper.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/llm.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory, Node } from 'brainyflow'\nimport { callLLM } from './utils/callLLM' // Your wrapper\n\nclass LLMNode extends Node {\n  async prep(memory: Memory): Promise<string> {\n    // Read prompt from memory\n    return memory.prompt ?? '' // Provide default if needed\n  }\n\n  async exec(prompt: string): Promise<string> {\n    // Call the LLM wrapper\n    if (!prompt) throw new Error('No prompt provided.')\n    return await callLLM(prompt)\n  }\n\n  async post(memory: Memory, prepRes: string, llmResponse: string): Promise<void> {\n    // Store the response in memory\n    memory.response = llmResponse\n    this.trigger('default') // Or another action based on response\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Integration Testing a BrainyFlow QA Flow in TypeScript\nDESCRIPTION: Shows how to integration test a question-answering flow (`qaFlow`) using `vitest` in TypeScript. It mocks the `callLLM` utility to return different results based on the prompt content (simulating search and answer generation). The test asserts the final value in the `memory` object and verifies that the LLM was called the expected number of times with appropriate prompts.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/testing.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n```typescript\nimport { afterEach, beforeEach, describe, expect, it, vi } from 'vitest'\nimport { createQaFlow } from './qaFlow' // Your function that creates the Flow\nimport { callLLM } from './utils/callLLM' // Your LLM utility\n\n// Mock the LLM utility\nvi.mock('./utils/callLLM', () => ({\n  callLLM: vi.fn(),\n}))\n\ndescribe('Question Answering Flow', () => {\n  beforeEach(() => {\n    // Clear any previous mock calls before each test\n    vi.clearAllMocks()\n  })\n\n  it('should generate an answer using the flow', async () => {\n    // Configure mock to return different values based on the prompt\n    vi.mocked(callLLM).mockImplementation((prompt: string) => {\n      // Simulate different stages of a potential QA flow (e.g., search vs. answer)\n      if (prompt.toLowerCase().includes('search')) {\n        return Promise.resolve('Paris is the capital of France.')\n      } else if (prompt.toLowerCase().includes('answer')) {\n        return Promise.resolve('The capital of France is Paris.')\n      }\n      return Promise.resolve('Unexpected prompt')\n    })\n\n    // Create the flow\n    const qaFlow = createQaFlow()\n\n    // Create initial memory state\n    const memory = { question: 'What is the capital of France?' }\n\n    // Run the flow\n    await qaFlow.run(memory) // Pass memory object\n\n    // Verify the final answer\n    expect(memory.answer).toBe('The capital of France is Paris.') // Access memory object\n\n    // Verify the LLM was called the expected number of times\n    expect(callLLM).toHaveBeenCalledTimes(2)\n\n    // Verify the calls were made with appropriate prompts\n    const calls = vi.mocked(callLLM).mock.calls\n    const retrieveCall = calls.some(\n      (call) => typeof call === 'string' && call.toLowerCase().includes('retrieve'),\n    )\n    const generateCall = calls.some(\n      (call) => typeof call === 'string' && call.toLowerCase().includes('generate'),\n    )\n\n    expect(retrieveCall).toBe(true)\n    expect(generateCall).toBe(true)\n  })\n})\n\n// Example testing a MapReduce flow (Trigger, Processor, Reducer)\ndescribe('MapReduce Flow Test', () => {\n  // Mock the nodes used in the MapReduce example\nclass TriggerNode extends Node<Memory, any, ['process_item','reduce']> {\n  async post(memory: Memory, prepRes: any, execRes: any): Promise<void> {\n      const items = memory.items || []\n      memory.results = [] // Initialize results\n      items.forEach((item: any, index: number) => {\n        this.trigger('process_item', { item, index })\n      })\n      this.trigger('reduce')\n    }\n  }\n  const ProcessorNode = class extends Node {\n     async prep(memory: Memory): Promise<any> { return { item: memory.item, index: memory.index }; }\n     async exec(prepRes: { item: any, index: number }): Promise<string> { return `Processed ${prepRes.item}`; }\n     async post(memory: Memory, prepRes: { item: any, index: number }, execRes: string): Promise<void> {\n         if (!memory.results) memory.results = [];\n         // Store result at the correct index if possible, or just push\n         memory.results[prepRes.index] = execRes;\n     }\n  }\n  const ReducerNode = class extends Node {\n     async prep(memory: Memory): Promise<any[]> { return memory.results || []; }\n     async exec(results: any[]): Promise<string> { return `Combined: ${results.join(', ')}`; }\n     async post(memory: Memory, prepRes: any, execRes: string): Promise<void> { memory.final_result = execRes; }\n  }\n\n  it('should process items via map and reduce steps', async () => {\n    // Instantiate nodes\n    const trigger = new TriggerNode()\n    const processor = new ProcessorNode()\n    const reducer = new ReducerNode()\n\n    // Connect nodes\n    trigger.on('process_item', processor)\n    trigger.on('reduce', reducer) // This action is triggered after all 'process_item'\n\n    // Use ParallelFlow for the map phase\n    const mapReduceFlow = new ParallelFlow(trigger)\n\n    // Initial memory\n    const memory = { items: ['A', 'B', 'C'] }\n\n    // Run the flow\n    await mapReduceFlow.run(memory)\n\n    // Verify final result in memory\n    expect(memory.results).toEqual(['Processed A', 'Processed B', 'Processed C'])\n    expect(memory.final_result).toBe('Combined: Processed A, Processed B, Processed C')\n  })\n})\n```\n```\n\n----------------------------------------\n\nTITLE: Triggering Subsequent Actions from BaseNode in BrainyFlow Python\nDESCRIPTION: Called within the `post` phase to trigger a specific `action`, signaling which branch(es) of the flow should execute next. Optionally accepts `forking_data` to pass specific data into the local scope (cloned memory) of the triggered branch(es).\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ntrigger(self, action, forking_data=None)\n```\n\n----------------------------------------\n\nTITLE: Implementing DecideAction Node\nDESCRIPTION: Node class that handles decision-making logic for the agent, determining whether to search for more information or provide an answer based on current context.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/agent.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nimport yaml\nfrom brainyflow import Node, Flow, Memory\n\nclass DecideAction(Node):\n    async def prep(self, memory: Memory):\n        context = memory.context if hasattr(memory, 'context') else \"No previous search\"\n        query = memory.query\n        return query, context\n\n    async def exec(self, inputs):\n        query, context = inputs\n        prompt = f\"\"\"\nGiven input: {query}\nPrevious search results: {context}\nShould I: 1) Search web for more info 2) Answer with current knowledge\nOutput in yaml:\n```yaml\naction: search/answer\nreason: why this action\nsearch_term: search phrase if action is search\n```\"\"\"\n        resp = call_llm(prompt)\n        yaml_str = resp.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        result = yaml.safe_load(yaml_str)\n\n        assert isinstance(result, dict)\n        assert \"action\" in result\n        assert \"reason\" in result\n        assert result[\"action\"] in [\"search\", \"answer\"]\n        if result[\"action\"] == \"search\":\n            assert \"search_term\" in result\n\n        return result\n\n    async def post(self, memory: Memory, prep_res, exec_res: dict):\n        if exec_res[\"action\"] == \"search\":\n            memory.search_term = exec_res[\"search_term\"]\n        self.trigger(exec_res[\"action\"])\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Flow, Memory, Node } from 'brainyflow'\n\nclass DecideAction extends Node {\n  async prep(memory: Memory): Promise<{ query: string; context: any }> {\n    const context = memory.context ?? 'No previous search'\n    const query = memory.query\n    return { query, context }\n  }\n\n  async exec(prepRes: { query: string; context: any }): Promise<any> {\n    const { query, context } = prepRes\n    const prompt = `\nGiven input: ${query}\nPrevious search results: ${JSON.stringify(context)}\nShould I: 1) Search web for more info ('search') 2) Answer with current knowledge ('answer')\nOutput in yaml:\n\\`\\`\\`yaml\naction: search | answer\nreason: <why this action>\nsearch_term: <search phrase if action is search>\n\\`\\`\\``\n    const resp = await callLLM(prompt)\n    const yamlStr = resp.split(/```(?:yaml)?/)[1]?.trim()\n    if (!yamlStr) {\n      throw new Error('Missing YAML response')\n    }\n\n    const result = parseYaml(yamlStr)\n\n    if (typeof result !== 'object' || !result) {\n      throw new Error('Invalid YAML response')\n    }\n    if (!('action' in result)) {\n      throw new Error('Missing action in response')\n    }\n    if (!('reason' in result)) {\n      throw new Error('Missing reason in response')\n    }\n    if (!['search', 'answer'].includes(result.action)) {\n      throw new Error('Invalid action value')\n    }\n    if (result.action === 'search' && !('search_term' in result)) {\n      throw new Error('Missing search_term for search action')\n    }\n\n    return result\n  }\n\n  async post(memory: Memory, prepRes: any, execRes: any): Promise<void> {\n    if (execRes.action === 'search' && execRes.search_term) {\n      memory.search_term = execRes.search_term\n    }\n    this.trigger(execRes.action)\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Executing Offline Indexing Flow in Python\nDESCRIPTION: Provides an example asynchronous function `run_offline` to execute the previously defined `OfflineFlow`. It initializes a `memory` dictionary with input file paths, creates dummy files for demonstration purposes using `fs.writeFileSync` (note: `fs` is unusual in Python, `os` or built-in file operations are typical), runs the `OfflineFlow` with the initial memory, prints status messages, and returns the final `memory` object which now contains the results (index, chunks, embeddings). It includes commented-out code for cleaning up the dummy files and calling the function.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/rag.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# --- Offline Flow Execution ---\nasync def run_offline():\n    # Create dummy files for example\n    if not os.path.exists('doc1.txt'): fs.writeFileSync('doc1.txt', 'Alice was beginning to get very tired.')\n    if not os.path.exists('doc2.txt'): fs.writeFileSync('doc2.txt', 'The quick brown fox jumps over the lazy dog.')\n\n    initial_memory = {\n        \"files\": [\"doc1.txt\", \"doc2.txt\"], # Example file paths\n    }\n    print('Starting offline indexing flow...')\n\n    await OfflineFlow.run(initial_memory)\n\n    print('Offline indexing complete.')\n    # Clean up dummy files\n    # os.remove('doc1.txt')\n    # os.remove('doc2.txt')\n    return initial_memory # Return memory containing index, chunks, embeds\n\n# asyncio.run(run_offline()) # Example call\n```\n```\n\n----------------------------------------\n\nTITLE: Defining a Throttled LLM Node Using brainyflow in Python\nDESCRIPTION: This Python snippet creates a 'ThrottledLLMNode' class that inherits from Node and applies request throttling via a Limiter with a maximum rate. The class manages the LLM request pipeline with asynchronous prep, exec, fallback, and post phases, ensuring rate limits are not exceeded and fallback logic is triggered appropriately upon errors. Dependencies include the brainyflow framework, a Limiter class supporting async ratelimit context decorators, and the existence of async LLM function (call_llm); key parameters include call limits and retry configuration, and memory objects with requested prompt fields are required as inputs.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/throttling.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass ThrottledLLMNode(Node):\n    def __init__(self, max_retries=3, wait=1, calls_per_minute=30):\n        super().__init__(max_retries=max_retries, wait=wait) # Pass wait to super\n        self.limiter = Limiter(Rate(calls_per_minute, Duration.MINUTE))\n\n    # Prep is needed to get the prompt from memory\n    async def prep(self, memory: Memory):\n        return memory.prompt # Assuming prompt is in memory.prompt\n\n    async def exec(self, prompt): # exec receives prompt from prep\n        @self.limiter.ratelimit('llm_calls')\n        async def limited_llm_call(text):\n            # Assuming call_llm is async\n            return await call_llm(text)\n\n        # Add basic check for empty prompt\n        if not prompt:\n             return \"No prompt provided.\"\n        return await limited_llm_call(prompt)\n\n    async def exec_fallback(self, prompt, error): # Make fallback async\n        # Handle rate limit errors specially\n        # Note: Retrying within fallback can lead to complex loops.\n        # Consider just logging or returning an error message.\n        if \"rate limit\" in str(error).lower():\n            print(f\"Rate limit hit for prompt: {prompt[:50]}...\")\n            # Fallback response instead of complex retry logic here\n            return f\"Rate limit exceeded. Please try again later. Error: {error}\"\n        # For other errors, fall back to a simple response\n        print(f\"LLM call failed after retries: {error}\")\n        return f\"I'm having trouble processing your request right now. Error: {error}\"\n\n    # Post is needed to store the result and trigger next step\n    async def post(self, memory: Memory, prep_res, exec_res):\n        memory.llm_response = exec_res # Store the result\n        self.trigger('default') # Trigger next node\n```\n\n----------------------------------------\n\nTITLE: Retrieving Text Embeddings with Azure OpenAI API in Python\nDESCRIPTION: This Python code retrieves a text embedding from the Azure OpenAI service. Dependencies are 'openai' and 'numpy', with environment variables AZURE_OPENAI_API_KEY and AZURE_OPENAI_ENDPOINT required. The 'get_azure_openai_embedding' function accepts the input string and an Azure deployment name, initializes an AzureOpenAI client, and returns the embedding as a NumPy array, or None if authentication fails or an API error occurs. Inputs should specify the Azure deployment model name.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/embedding.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Requires: pip install openai numpy\\nimport os\\nimport numpy as np\\nfrom openai import AzureOpenAI\\n\\ndef get_azure_openai_embedding(text: str, deployment_name: str = \"your-embedding-deployment\") -> np.ndarray | None:\\n    \\\"\\\"\\\"Gets embedding from Azure OpenAI API.\\\"\\\"\\\"\\n    api_key = os.environ.get(\"AZURE_OPENAI_API_KEY\")\\n    azure_endpoint = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\\n    api_version = \"2023-05-15\" # Example version, adjust as needed\\n\\n    if not api_key or not azure_endpoint:\\n        print(\"Error: AZURE_OPENAI_API_KEY or AZURE_OPENAI_ENDPOINT not set.\")\\n        return None\\n    try:\\n        client = AzureOpenAI(\\n            api_key=api_key,\\n            azure_endpoint=azure_endpoint,\\n            api_version=api_version\\n        )\\n        response = client.embeddings.create(\\n            model=deployment_name, # Use your deployment name\\n            input=text\\n        )\\n        embedding = response.data[0].embedding\\n        return np.array(embedding, dtype=np.float32)\\n    except Exception as e:\\n        print(f\"Error calling Azure OpenAI embedding API: {e}\")\\n        return None\\n\\n# Example:\\n# text_to_embed = \"Hello world\"\\n# embedding_vector = get_azure_openai_embedding(text_to_embed, deployment_name=\"my-text-embedding-ada-002\")\\n# if embedding_vector is not None:\\n#     print(embedding_vector)\\n\n```\n\n----------------------------------------\n\nTITLE: Managing Shared and Forked State with Memory - TypeScript\nDESCRIPTION: This snippet represents the design and implementation of the Memory class, which manages state for execution flows. It separates global and local state using internal stores and exposes controlled property access via a JavaScript Proxy. Cloning creates a new Memory instance with a shared global reference and deep-cloned local store, supporting safe forking of execution memory. Required dependencies are native Proxy, Map, and structuredClone; inputs are initial state objects, and outputs are stateful Memory proxies. Constraints include reserved property names and limitations on how state is propagated between workflow branches.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/design.md#2025-04-22_snippet_0\n\nLANGUAGE: TypeScript\nCODE:\n```\nclass Memory {\n  // Internal stores: __global for shared data, __local for path-specific data\n  // Uses Proxy to control access\n  // Special handling for reserved properties: 'global', 'local', '__global', '__local'\n  // 'get' checks __local first, then __global\n  // 'set' writes to __global, removes from __local unless reserved\n  // 'clone(forkingData?)' creates a new Memory with deep-cloned __local, optionally merges in forkingData\n  // 'create' is a static factory returning a proxied Memory instance\n}\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Article Writing Workflow in TypeScript with BrainyFlow\nDESCRIPTION: TypeScript implementation of the article writing workflow using BrainyFlow. Implements the same three-node structure with type safety and memory management. Includes logging and error handling.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/workflow.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Flow, Memory, Node } from 'brainyflow'\n\n// Assuming callLLM is defined elsewhere\ndeclare function callLLM(prompt: string): Promise<string>\n\nclass GenerateOutline extends Node {\n  async prep(memory: Memory): Promise<string> {\n    return memory.topic // Read topic from memory\n  }\n  async exec(topic: string): Promise<string> {\n    console.log(`Generating outline for: ${topic}`)\n    return await callLLM(`Create a detailed outline for an article about ${topic}`)\n  }\n  async post(memory: Memory, prepRes: any, outline: string): Promise<void> {\n    memory.outline = outline // Store outline in memory\n    this.trigger('default')\n  }\n}\n\nclass WriteSection extends Node {\n  async prep(memory: Memory): Promise<string> {\n    return memory.outline // Read outline from memory\n  }\n  async exec(outline: string): Promise<string> {\n    console.log('Writing draft based on outline...')\n    return await callLLM(`Write content based on this outline: ${outline}`)\n  }\n  async post(memory: Memory, prepRes: any, draft: string): Promise<void> {\n    memory.draft = draft // Store draft in memory\n    this.trigger('default')\n  }\n}\n\nclass ReviewAndRefine extends Node {\n  async prep(memory: Memory): Promise<string> {\n    return memory.draft // Read draft from memory\n  }\n  async exec(draft: string): Promise<string> {\n    console.log('Reviewing and refining draft...')\n    return await callLLM(`Review and improve this draft: ${draft}`)\n  }\n  async post(memory: Memory, draft: any, finalArticle: string): Promise<void> {\n    memory.final_article = finalArticle // Store final article\n    console.log('Final article generated.')\n    // No trigger needed - end of workflow\n  }\n}\n\n// --- Flow Definition ---\nconst outline = new GenerateOutline()\nconst write = new WriteSection()\nconst review = new ReviewAndRefine()\n\n// Connect nodes sequentially using default trigger\noutline.next(write).next(review)\n\n// Create the flow\nconst writingFlow = new Flow(outline)\n\n// --- Execution ---\nasync function main() {\n  const data = { topic: 'AI Safety' }\n  console.log(`Starting writing workflow for topic: \"${data.topic}\"`)\n\n  await writingFlow.run(data) // Run the flow\n\n  console.log('\\n--- Workflow Complete ---')\n  console.log('Final Memory State:', data)\n  console.log(`\\nFinal Article:\\n${data.final_article ?? 'Not generated'}`)\n}\n\nmain().catch(console.error)\n```\n\n----------------------------------------\n\nTITLE: Implementing Async Recipe Suggestion Node in Python\nDESCRIPTION: This code defines the SuggestRecipe node that calls an LLM asynchronously to recommend the best recipe from a list of options. It demonstrates how to format prompts for LLM calls within an async context.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-async-basic/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nasync def exec_async(self, recipes):\n    # Async LLM call\n    suggestion = await call_llm_async(\n        f\"Choose best recipe from: {recipes}\"\n    )\n    return suggestion\n```\n\n----------------------------------------\n\nTITLE: Retrieving Text Embeddings with OpenAI API in TypeScript\nDESCRIPTION: This TypeScript snippet demonstrates calling the OpenAI API to generate text embeddings. It uses the 'openai' npm package and expects the OPENAI_API_KEY environment variable. The async function 'getOpenaiEmbedding' accepts a text string and optional model name, authenticates the client, and fetches the embedding as an array of numbers. It returns null on missing credentials or errors. Results provide an array representing the vectorized meaning of the input text.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/embedding.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// Requires: npm install openai\\nimport OpenAI from 'openai'\\n\\nasync function getOpenaiEmbedding(\\n  text: string,\\n  model: string = 'text-embedding-3-small',\\n): Promise<number[] | null> {\\n  /** Gets embedding from OpenAI API. */\\n  const apiKey = process.env.OPENAI_API_KEY\\n  if (!apiKey) {\\n    console.error('Error: OPENAI_API_KEY not set.')\\n    return null\\n  }\\n  try {\\n    const openai = new OpenAI({ apiKey })\\n    const response = await openai.embeddings.create({\\n      model,\\n      input: text,\\n    })\\n    return response.data[0]?.embedding ?? null\\n  } catch (error) {\\n    console.error('Error calling OpenAI embedding API:', error)\\n    return null\\n  }\\n}\\n\\n// Example:\\n// const textToEmbed = \"Hello world\";\\n// getOpenaiEmbedding(textToEmbed).then(embedding => {\\n//   if (embedding) {\\n//     console.log(embedding);\\n//     console.log(`Dimension: ${embedding.length}`);\\n//   }\\n// });\n```\n\n----------------------------------------\n\nTITLE: Performing KNN Query on Redis Vector Index in Python\nDESCRIPTION: This snippet demonstrates how to perform a K-Nearest Neighbors (KNN) query on a Redis vector index using Python. It constructs a query string, sets parameters, and executes the search.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/vector.md#2025-04-22_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\nk = 3\nquery_vector = np.array([0.15] * dimension, dtype=np.float32).tobytes()\nq = Query(f\"*=>[KNN {k} @vector $vec AS vector_score]\")\\\n    .sort_by(\"vector_score\")\\\n    .return_fields(\"id\", \"vector_score\", \"tag\")\\\n    .dialect(2) # Use DIALECT 2 for parameter support\n\nparams = {\"vec\": query_vector}\nprint(\"Querying Redis index...\")\nresults = client.ft(index_name).search(q, query_params=params)\n\nprint(\"Query results:\")\nfor doc in results.docs:\n    print(f\" - ID: {doc.id}, Score: {doc.vector_score}, Tag: {doc.tag}\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Qdrant Client TypeScript\nDESCRIPTION: Initializes a Qdrant client in TypeScript using a URL and optional API key from environment variables. Validates URL presence and logs initialization activity.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/vector.md#2025-04-22_snippet_7\n\nLANGUAGE: TypeScript\nCODE:\n```\n// Requires: npm install @qdrant/js-client-rest\nimport { QdrantClient } from '@qdrant/js-client-rest'\nimport { Distance } from '@qdrant/js-client-rest/dist/types/types/Points' // Adjust import path if needed\n\nfunction initQdrantClient(): QdrantClient | null {\n  /** Initializes Qdrant client. */\n  const qdrantUrl = process.env.QDRANT_URL // e.g., \"http://localhost:6333\" or cloud URL\n  const apiKey = process.env.QDRANT_API_KEY // Optional, for cloud\n  if (!qdrantUrl) {\n    console.error('Error: QDRANT_URL not set.')\n    return null\n  }\n  try {\n    const client = new QdrantClient({ url: qdrantUrl, apiKey: apiKey })\n    console.log('Qdrant client initialized.')\n    return client\n  } catch (error) {\n    console.error('Error initializing Qdrant client:', error)\n    return null\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Executing RAG Flow in TypeScript\nDESCRIPTION: TypeScript implementation for running the online RAG flow with a predefined question. It includes mock implementations of external dependencies and shows how to combine offline and online stages for a complete RAG system.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/rag.md#2025-04-22_snippet_17\n\nLANGUAGE: typescript\nCODE:\n```\n// --- Online Flow Execution ---\nasync function runOnline(memoryFromOffline: any) {\n  // Add the user's question to the memory from the offline stage\n  memoryFromOffline.question = 'Why do people like cats?'\n\n  console.log(`\\nStarting online RAG flow for question: \"${memoryFromOffline.question}\"`)\n  await OnlineFlow.run(memoryFromOffline)\n  console.log('Online RAG complete.')\n  return memoryFromOffline\n}\n\n// --- Combined Example ---\nasync function runFullRAG() {\n  // Mock external functions for example\n  globalThis.getEmbedding = async (text: string) => Array(5).fill(Math.random())\n  globalThis.createIndex = (embeds: number[][]) => ({\n    search: (q: number[], k: number) => [[Math.floor(Math.random() * embeds.length)]],\n  }) // Mock index\n  globalThis.searchIndex = (index: any, q: number[], k: number) => index.search(q, k)\n  globalThis.callLLM = async (prompt: string) => `Mock LLM answer for: ${prompt.split('\\n')[1]}`\n\n  const memoryAfterOffline = await runOffline()\n  const finalMemory = await runOnline(memoryAfterOffline)\n\n  console.log('\\n--- Full RAG Result ---')\n  console.log('Final Answer:', finalMemory.answer)\n}\n\nrunFullRAG().catch(console.error)\n```\n\n----------------------------------------\n\nTITLE: Composing QA Flow from Nodes (Python)\nDESCRIPTION: Defines a function to instantiate GetQuestionNode and AnswerNode, link them together (using '>>' as syntax sugar for next), and construct a Flow starting at the question node. Depends on brainyflow and previously defined node classes. Input/Output: produces a Flow instance ready to execute, with nodes properly connected. Limitation: the nodes are imported locally.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/getting_started.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom .nodes import GetQuestionNode, AnswerNode # defined in the previous step\nfrom brainyflow import Flow\n\ndef create_qa_flow():\n    get_question_node = GetQuestionNode()\n    answer_node = AnswerNode()\n\n    # Connect nodes get_question_node → answer_node using the default action\n    get_question_node >> answer_node  # >> is Pythonic syntax sugar for .next(node)\n\n    # Create the Flow, specifying the starting node\n    return Flow(start=get_question_node)\n```\n\n----------------------------------------\n\nTITLE: Visualizing the ChatNode Self-Loop (Mermaid)\nDESCRIPTION: This Mermaid flowchart diagram illustrates the core operational loop of the chat application. It shows a single component, `ChatNode`, which processes input and generates output, then loops back to itself to await the next user input, indicating a continuous conversational cycle until explicitly exited.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/typescript-chat/README.md#2025-04-22_snippet_1\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n    chat[ChatNode] -->|continue| chat\n```\n\n----------------------------------------\n\nTITLE: Implementing Main Function for BrainyFlow Research Agent Execution\nDESCRIPTION: Defines the main function to process a question using the BrainyFlow research agent. It creates the agent flow, runs it with the given question, and prints the final answer.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-agent/demo.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# main.py\nimport sys\n\nasync def main():\n    \"\"\"Simple function to process a question.\"\"\"\n    # Default question\n    default_question = \"Who won the Nobel Prize in Physics 2024?\"\n\n    # Get question from command line if provided with --\n    question = default_question\n    for arg in sys.argv[1:]:\n        if arg.startswith(\"--\"):\n            question = arg[2:]\n            break\n\n    # Create the agent flow\n    agent_flow = create_agent_flow()\n\n    # Process the question\n    shared = {\"question\": question}\n    print(f\"🤔 Processing question: {question}\")\n    await agent_flow.run(shared)\n    print(\"\\n🎯 Final Answer:\")\n    print(shared.get(\"answer\", \"No answer found\"))\n\nmain()\n```\n\n----------------------------------------\n\nTITLE: Using Explicit Triggers for Branching in BrainyFlow (Python)\nDESCRIPTION: These Python code snippets illustrate the refactoring needed to implement explicit triggers instead of relying on return values to control flow in BrainyFlow v1.0. The previous implementation returns action names based on logic, whereas the updated approach sets a status property and calls self.trigger with the appropriate action. This pattern clarifies control flow and depends on the Node API's trigger method. The input is an execution result; output is status set on the memory object.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/migration.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Before (v0.2)\nasync def post(self, shared, prep_res, exec_res):\n    if exec_res > 10:\n        shared[\"status\"] = \"high\"\n        return \"high_value\"\n    else:\n        shared[\"status\"] = \"low\"\n        return \"low_value\"\n```\n\nLANGUAGE: python\nCODE:\n```\n# After (v1.0)\nasync def post(self, memory, prep_res, exec_res):\n    if exec_res > 10:\n        memory.status = \"high\"\n        self.trigger(\"high_value\")\n    else:\n        memory.status = \"low\"\n        self.trigger(\"low_value\")\n```\n\n----------------------------------------\n\nTITLE: Python Node Error Handling with Retries in BrainyFlow\nDESCRIPTION: Shows how to implement error handling with retries in a Python node. Uses `maxRetries` and `wait` parameters to configure retries. Demonstrates overriding `execFallback` to handle failed `exec` attempts gracefully.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/core_abstraction/node.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom brainyflow import Node, Memory, NodeError\n\nmy_node = MyNode(max_retries=3, wait=10) # Assuming MyNode is defined elsewhere\n\nclass CustomErrorHandlingNode(Node):\n    async def exec(self, prep_res):\n        print(f\"Exec attempt: {self.cur_retry + 1}\")\n        if self.cur_retry < 2:\n             raise ValueError(\"Temporary failure!\")\n        return \"Success on retry\"\n\n    async def exec_fallback(self, prep_res, error: NodeError) -> str:\n        # This is called only if exec fails on the last attempt\n        print(f\"Exec failed after {error.retry_count + 1} attempts: {error}\")\n        # Return a fallback value instead of raising error\n        return f\"Fallback response due to repeated errors: {error}\"\n\n    async def post(self, memory: Memory, prep_res, exec_res: str):\n        # exec_res will be \"Success on retry\" or \"Fallback response...\"\n        print(f\"Post: Received result '{exec_res}'\")\n        memory.final_result = exec_res\n        self.trigger('default')\n\n\n```\n\n----------------------------------------\n\nTITLE: Example YAML Structure for Product Information\nDESCRIPTION: Demonstrates a target YAML structure for representing product details, including name, price, and a multi-line description using the literal block style (`|`). This serves as an example of structured output an LLM might be prompted to generate for product cataloging.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/structure.md#2025-04-22_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nproduct:\n  name: Widget Pro\n  price: 199.99\n  description: |\n    A high-quality widget designed for professionals.\n    Recommended for advanced users.\n```\n\n----------------------------------------\n\nTITLE: Implementing AnswerQuestion Node for Question Answering in Python\nDESCRIPTION: Defines the AnswerQuestion class that generates answers to questions based on retrieved context. It has three methods: prep to prepare inputs, exec to generate the answer using an LLM, and post to display results and trigger the next question.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python_demo.ipynb#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nclass AnswerQuestion(Node):\n    async def prep(self, shared):\n        return (\n            shared[\"current_question\"],\n            shared[\"context\"]\n        )\n        \n    async def exec(self, inputs):\n        question, context = inputs\n        prompt = f\"\"\"\nContext: {context}\n\nQuestion: {question}\n\nAnswer the question based on the context above. If the context doesn't contain relevant information, say so.\nAnswer:\"\"\"\n        return call_llm(prompt)\n    \n    async def post(self, shared, prep_res, exec_res):\n        print(f\"\\nQ: {shared['current_question']}\")\n        print(f\"A: {exec_res}\")\n        print(f\"\\nSource: {shared['relevant_file']}\")\n        self.trigger(\"continue\")  # Loop back for more questions\n```\n\n----------------------------------------\n\nTITLE: Creating an LLM Wrapper using OpenAI in TypeScript\nDESCRIPTION: This TypeScript snippet shows how to construct a simple wrapper for OpenAI's API using the OpenAI package. It exports an asynchronous function to interact with the language model, demanding an API key from environment variables. Required dependencies are the `openai` package and access to the API key.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/llm.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// utils/callLLM.ts\nimport OpenAI from 'openai'\n\nexport async function callLLM(\n  prompt: string,\n  model: string = 'gpt-4o',\n  temperature: number = 0.7,\n): Promise {\n  const openai = new OpenAI({\n    apiKey: process.env.OPENAI_API_KEY,\n  })\n\n  const response = await openai.chat.completions.create({\n    model,\n    messages: [{ role: 'user', content: prompt }],\n    temperature,\n  })\n\n  return response.choices[0]?.message?.content || ''\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Parallel Flow in BrainyFlow\nDESCRIPTION: Creates a ParallelFlow instance that executes triggered tasks concurrently using Promise.all(). This is useful for performance when branches are independent of each other.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/core_abstraction/flow.md#2025-04-22_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\n// Executes triggered branches in parallel\nconst parallelFlow = new ParallelFlow(startNode)\n```\n\n----------------------------------------\n\nTITLE: Implementing BaseNode Class Post Method Stub in TypeScript\nDESCRIPTION: Defines the asynchronous `post` method signature for the `BaseNode` abstract class. Subclasses should override this method to perform any cleanup, logging, or triggering of subsequent actions after the `exec` phase completes. It receives the `memory` instance and the results from both `prep` (`prepRes`) and `exec` (`execRes`).\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/api.md#2025-04-22_snippet_14\n\nLANGUAGE: typescript\nCODE:\n```\n`async post(memory: Memory<GlobalStore, LocalStore>, prepRes: PrepResult | void, execRes: ExecResult | void): Promise<void>`\n```\n\n----------------------------------------\n\nTITLE: Creating Redis Search Index for Vectors in TypeScript\nDESCRIPTION: This TypeScript function creates a Redis Search index for vector data if it doesn't already exist. It defines a schema for both JSON and HASH data types, with fields for vector embeddings and tags.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/vector.md#2025-04-22_snippet_22\n\nLANGUAGE: TypeScript\nCODE:\n```\nasync function createRedisIndexIfNotExists(\n  client: ReturnType<typeof createClient>,\n  indexName: string,\n  prefix: string,\n  dimension: number,\n): Promise<void> {\n  /** Creates a Redis Search index for vectors if it doesn't exist. */\n  const schema = {\n    embedding: {\n      type: SchemaFieldTypes.VECTOR,\n      ALGORITHM: VectorAlgorithms.FLAT,\n      TYPE: 'FLOAT32',\n      DIM: dimension,\n      DISTANCE_METRIC: 'L2',\n      AS: 'vector',\n    },\n    tag: {\n      type: SchemaFieldTypes.TAG,\n      AS: 'tag',\n    },\n  }\n\n  try {\n    await client.ft.info(indexName)\n    console.log(`Redis index '${indexName}' already exists.`)\n  } catch (e: any) {\n    if (e.message.includes('Unknown Index name')) {\n      console.log(`Creating Redis index '${indexName}' for prefix '${prefix}'...`)\n      await client.ft.create(\n        indexName,\n        schema,\n        {\n          ON: 'HASH',\n          PREFIX: prefix,\n        },\n      )\n      console.log(`Redis index '${indexName}' created.`)\n    } else {\n      console.error('Error checking/creating Redis index:', e)\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Testing Retry Logic with Mock in Python\nDESCRIPTION: This Python snippet demonstrates how to test retry behavior for a node method which may fail initially. Using unittest.mock's patch and AsyncMock, it mocks the node's external calls and the asyncio.sleep function to ensure correct retry attempts without actual delay.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/testing.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom unittest.mock import patch, AsyncMock\nimport asyncio\n# from brainyflow import Node # Assuming Node is imported\n\n# Mock function that fails twice, then succeeds\ncall_count_retry = 0\nasync def mock_fails_then_succeeds(*args, **kwargs):\n    global call_count_retry\n    call_count_retry += 1\n    print(f\"Mock called (Attempt {call_count_retry})\") # For debugging test\n    if call_count_retry <= 2:\n        raise ValueError(\"Temporary network failure\")\n    return \"Success on third try\"\n\n# Example Node (conceptual)\n# class NodeWithRetry(Node):\n#     def __init__(self):\n#         super().__init__(max_retries=3, wait=0.1) # Retry up to 3 times (4 attempts total)\n#     async def exec(self, prep_res):\n#         # This method calls the function we will mock\n#         return await some_external_call(prep_res)\n\nasync def test_retry_logic():\n    global call_count_retry\n    call_count_retry = 0 # Reset counter for test\n    # node = NodeWithRetry()\n    # memory = Memory.create({})\n\n    # Patch the external call made within node.exec\n    # Also patch asyncio.sleep to avoid actual waiting\n    with patch('__main__.some_external_call', new=AsyncMock(side_effect=mock_fails_then_succeeds)), \\\n         patch('asyncio.sleep', new=AsyncMock()) as mock_sleep:\n\n        # await node.run(memory) # Run the node\n        pass # Replace pass with actual node execution\n\n    # Assertions\n    # assert call_count_retry == 3 # Should be called 3 times (1 initial + 2 retries)\n    # assert memory.result == \"Success on third try\" # Check final result\n    # assert mock_sleep.call_count == 2 # Check if sleep was called between retries\n\n# asyncio.run(test_retry_logic())\n```\n\n----------------------------------------\n\nTITLE: Batch Translation Refactor from BatchNode to Fan-Out Pattern (Python)\nDESCRIPTION: These Python snippets display the migration of batch translation from a BatchNode abstraction to a generalized orchestration pattern involving two standard Node subclasses in BrainyFlow. The first snippet shows v0.2's direct batching using a BatchNode, while the v1.0 code splits the process into a trigger node (that emits a task for each required translation) and a processor node (that handles one language). This design leverages forkingData and explicit triggers for parallel/concurrent or sequential execution. Dependencies include the core Node and Memory APIs and a presumed translate_text function. Inputs are text and a language list, output is a translations list in memory. The new pattern provides greater flexibility at the cost of requiring more orchestration boilerplate.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/migration.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Before (v0.2) - Conceptual BatchNode\nclass TranslateTextBatchNode(BatchNode):\n    async def prep(self, shared):\n        text = shared.get(\"text\", \"(No text provided)\")\n        languages = shared.get(\"languages\", [\"Chinese\", \"Spanish\", \"Japanese\"])\n        # BatchNode prep would return items for exec\n        return [(text, lang) for lang in languages]\n\n    async def exec(self, item):\n        text, lang = item\n        # Assume translate_text exists\n        return await translate_text(text, lang)\n\n    async def post(self, shared, prep_res, exec_results):\n        # BatchNode post might aggregate results\n        shared[\"translations\"] = exec_results\n        return \"default\"\n```\n\nLANGUAGE: python\nCODE:\n```\n# After (v1.0) - Using Flow Patterns with ParallelFlow\n\nfrom brainyflow import Node, Memory\n\n# 1. Trigger Node (Fans out work)\nclass TriggerTranslationsNode(Node):\n    async def prep(self, memory: Memory):\n        text = memory.text if hasattr(memory, 'text') else \"(No text provided)\"\n        languages = memory.languages if hasattr(memory, 'languages') else [\"Chinese\", \"Spanish\", \"Japanese\"]\n\n        return [{\"text\": text, \"language\": lang} for lang in languages]\n\n    async def post(self, memory: Memory, prep_res, exec_res):\n        for index, input in enumerate(prep_res):\n            this.trigger(\"default\", input | {\"index\": index})\n\n# 2. Processor Node (Handles one language)\nclass TranslateOneLanguageNode(Node):\n    async def prep(self, memory: Memory):\n        # Read data passed via forkingData from local memory\n        return {\n            \"text\": memory.text,\n            \"language\": memory.language,\n            \"index\": memory.index\n        }\n\n    async def exec(self, item):\n        # Assume translate_text exists\n        return await translate_text(item.text, item.language)\n\n    async def post(self, memory: Memory, prep_res, exec_res):\n        # Store result in the global list at the correct index\n        memory.translations[exec_res[\"index\"]] = exec_res\n        this.trigger(\"default\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Standard Node with Retries in BrainyFlow Python\nDESCRIPTION: Initializes a `Node` instance, which inherits from `BaseNode`. Configures retry behavior with `max_retries` (number of attempts, default is 1 meaning no retries) and `wait` (seconds to wait between retries, default is 0).\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n__init__(self, max_retries=1, wait=0)\n```\n\n----------------------------------------\n\nTITLE: Chat Flow Architecture - Mermaid Diagram\nDESCRIPTION: Flowchart showing the interaction between different nodes in the chat application including question handling, retrieval, answering, and embedding.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-chat-memory/README.md#2025-04-22_snippet_2\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n    Question[GetUserQuestionNode] -->|retrieve| Retrieve[RetrieveNode]\n    Retrieve -->|answer| Answer[AnswerNode]\n    Answer -->|question| Question\n    Answer -->|embed| Embed[EmbedNode]\n    Embed -->|question| Question\n```\n\n----------------------------------------\n\nTITLE: Configuring Cycle Detection in BrainyFlow\nDESCRIPTION: Sets the maximum number of times a node can be visited to prevent infinite loops. The default value is 5, but can be customized during Flow initialization.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/core_abstraction/flow.md#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\n// Limit the number of times a node can be visited within this flow\nconst flow = new Flow(startNode, { maxVisits: 10 })\n```\n\n----------------------------------------\n\nTITLE: Querying Google Custom Search JSON API in Python and TypeScript\nDESCRIPTION: Demonstrates how to fetch search results from the Google Custom Search JSON API using Python with the 'requests' library and TypeScript with the 'fetch' API. Requires GOOGLE_API_KEY and GOOGLE_CX_ID environment variables to be set for authentication. Sends a GET request with the API key, CX ID, and query, then prints the JSON response or an error message.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/websearch.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```python\nimport requests\nimport os\n\nAPI_KEY = os.environ.get(\"GOOGLE_API_KEY\") # Use environment variables\nCX_ID = os.environ.get(\"GOOGLE_CX_ID\")     # Use environment variables\nquery = \"example\"\n\nurl = \"https://www.googleapis.com/customsearch/v1\"\nparams = {\n    \"key\": API_KEY,\n    \"cx\": CX_ID,\n    \"q\": query\n}\n\nif not API_KEY or not CX_ID:\n    print(\"Error: Please set GOOGLE_API_KEY and GOOGLE_CX_ID environment variables.\")\nelse:\n    try:\n        response = requests.get(url, params=params)\n        response.raise_for_status() # Raise an exception for bad status codes\n        results = response.json()\n        print(results)\n    except requests.exceptions.RequestException as e:\n        print(f\"Error fetching Google search results: {e}\")\n\n```\n```\n\nLANGUAGE: typescript\nCODE:\n```\n```typescript\nasync function searchGoogle(query: string): Promise<any> {\n  const apiKey = process.env.GOOGLE_API_KEY // Use environment variables\n  const cxId = process.env.GOOGLE_CX_ID // Use environment variables\n\n  if (!apiKey || !cxId) {\n    console.error('Error: Please set GOOGLE_API_KEY and GOOGLE_CX_ID environment variables.')\n    return null\n  }\n\n  const url = new URL('https://www.googleapis.com/customsearch/v1')\n  url.searchParams.append('key', apiKey)\n  url.searchParams.append('cx', cxId)\n  url.searchParams.append('q', query)\n\n  try {\n    const response = await fetch(url.toString())\n    if (!response.ok) {\n      throw new Error(`HTTP error! status: ${response.status}`)\n    }\n    const results = await response.json()\n    console.log(results)\n    return results\n  } catch (error) {\n    console.error('Error fetching Google search results:', error)\n    return null\n  }\n}\n\n// Example usage:\n// searchGoogle(\"example\");\n```\n```\n\n----------------------------------------\n\nTITLE: Defining Nodes with BrainyFlow BaseNode in Python\nDESCRIPTION: This snippet shows the 'BaseNode' class, which provides a template for node lifecycles in computational graphs. It defines methods for execution phases, graph connectivity, and triggering, essential for creating executable nodes that integrate with the BrainyFlow system.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/design.md#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nbrainyflow.BaseNode\n```\n\n----------------------------------------\n\nTITLE: Implementing Execution Runner for Flow in BrainyFlow Python\nDESCRIPTION: Implements the `exec_runner` for the `Flow`. This method initiates the flow execution process by calling `run_node` on the `start` node of the flow, passing the initial `memory` state.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nasync exec_runner(self, memory, prep_res)\n```\n\n----------------------------------------\n\nTITLE: Embedding User Query Node in TypeScript\nDESCRIPTION: Defines the `EmbedQueryNode` class in TypeScript for the online query stage. The `prep` method retrieves the `question` string from the `memory` object. The `exec` method logs the query being embedded and calls an asynchronous external function `getEmbedding` to compute the embedding (expected as `number[]`). The `post` method stores the query embedding (`qEmb`) in `memory` and triggers the 'retrieve_docs' event for the next step.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/rag.md#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\n// --- Stage 2: Online Query Nodes ---\n\n// 2a. Embed Query Node\nclass EmbedQueryNode extends Node {\n  async prep(memory: Memory): Promise<string> {\n    return memory.question // Expects question in global memory\n  }\n  async exec(question: string): Promise<number[]> {\n    console.log(`Embedding query: \"${question}\"`);\n    return await getEmbedding(question)\n  }\n  async post(memory: Memory, prepRes: any, qEmb: number[]): Promise<void> {\n    memory.q_emb = qEmb // Store query embedding\n    this.trigger('retrieve_docs')\n  }\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Integration Testing a BrainyFlow MapReduce Flow in TypeScript\nDESCRIPTION: Provides an example of testing a MapReduce flow implemented with BrainyFlow in TypeScript using `vitest`. It defines mock `TriggerNode`, `ProcessorNode`, and `ReducerNode` classes, connects them, runs the flow using `ParallelFlow` with initial data in `memory`, and verifies the intermediate (`memory.results`) and final (`memory.final_result`) states.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/testing.md#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\n```typescript\n// Example testing a MapReduce flow (Trigger, Processor, Reducer)\ndescribe('MapReduce Flow Test', () => {\n  // Mock the nodes used in the MapReduce example\nclass TriggerNode extends Node<Memory, any, ['process_item','reduce']> {\n  async post(memory: Memory, prepRes: any, execRes: any): Promise<void> {\n      const items = memory.items || []\n      memory.results = [] // Initialize results\n      items.forEach((item: any, index: number) => {\n        this.trigger('process_item', { item, index })\n      })\n      this.trigger('reduce')\n    }\n  }\n  const ProcessorNode = class extends Node {\n     async prep(memory: Memory): Promise<any> { return { item: memory.item, index: memory.index }; }\n     async exec(prepRes: { item: any, index: number }): Promise<string> { return `Processed ${prepRes.item}`; }\n     async post(memory: Memory, prepRes: { item: any, index: number }, execRes: string): Promise<void> {\n         if (!memory.results) memory.results = [];\n         // Store result at the correct index if possible, or just push\n         memory.results[prepRes.index] = execRes;\n     }\n  }\n  const ReducerNode = class extends Node {\n     async prep(memory: Memory): Promise<any[]> { return memory.results || []; }\n     async exec(results: any[]): Promise<string> { return `Combined: ${results.join(', ')}`; }\n     async post(memory: Memory, prepRes: any, execRes: string): Promise<void> { memory.final_result = execRes; }\n  }\n\n  it('should process items via map and reduce steps', async () => {\n    // Instantiate nodes\n    const trigger = new TriggerNode()\n    const processor = new ProcessorNode()\n    const reducer = new ReducerNode()\n\n    // Connect nodes\n    trigger.on('process_item', processor)\n    trigger.on('reduce', reducer) // This action is triggered after all 'process_item'\n\n    // Use ParallelFlow for the map phase\n    const mapReduceFlow = new ParallelFlow(trigger)\n\n    // Initial memory\n    const memory = { items: ['A', 'B', 'C'] }\n\n    // Run the flow\n    await mapReduceFlow.run(memory)\n\n    // Verify final result in memory\n    expect(memory.results).toEqual(['Processed A', 'Processed B', 'Processed C'])\n    expect(memory.final_result).toBe('Combined: Processed A, Processed B, Processed C')\n  })\n})\n```\n```\n\n----------------------------------------\n\nTITLE: Initializing Flow Orchestrator in BrainyFlow Python\nDESCRIPTION: Initializes a `Flow` instance, which orchestrates the execution of a node graph. Requires a `start` node to begin execution and accepts optional `options`, such as `max_visits` to prevent infinite loops in cyclic graphs.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n__init__(self, start, options=None)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Research Agent Graph Structure with Mermaid\nDESCRIPTION: Mermaid diagram showing the graph structure of the research agent with three main components: DecideAction for determining whether to search or answer, SearchWeb for finding information, and AnswerQuestion for generating responses.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-agent/README.md#2025-04-22_snippet_5\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph TD\n    A[DecideAction] -->|\"search\"| B[SearchWeb]\n    A -->|\"answer\"| C[AnswerQuestion]\n    B -->|\"decide\"| A\n```\n\n----------------------------------------\n\nTITLE: Conditional Branching with RouterNode in TypeScript\nDESCRIPTION: Implements a `RouterNode` in TypeScript, detecting language and triggering corresponding processing nodes within a flow. This demonstrates how to set up dynamic node routing based on action detection, using typed languages.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/core_abstraction/node.md#2025-04-22_snippet_6\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { Flow, Memory, Node } from 'brainyflow'\n\ninterface RouterGlobalStore {\n  content: string\n  language?: string\n}\ntype RouterActions = 'english' | 'spanish' | 'unknown'\n\nclass RouterNode extends Node<RouterGlobalStore, any, RouterActions[]> {\n  async prep(memory: Memory<RouterGlobalStore, any>): Promise<string> {\n    return memory.content\n  }\n\n  async exec(content: string | undefined): Promise<string> {\n    return await detectLanguage(content)\n  }\n\n  async post(\n    memory: Memory<RouterGlobalStore, any>,\n    prepRes: string,\n    execRes: string\n  ): Promise<void> {\n    console.log(`RouterPost: Detected language '${execRes}', storing and triggering.`)\n    memory.language = execRes\n    this.trigger(execRes as RouterActions)\n  }\n}\n\nconst router = new RouterNode()\nconst englishProcessor = new EnglishProcessorNode()\nconst spanishProcessor = new SpanishProcessorNode()\nconst unknownProcessor = new UnknownProcessorNode()\nrouter.on('english', englishProcessor)\nrouter.on('spanish', spanishProcessor)\nrouter.on('unknown', unknownProcessor)\n\nconst flow = new Flow(router)\n\nasync function runFlow() {\n  const storeEn: RouterGlobalStore = { content: 'Hello world' }\n  await flow.run(storeEn)\n  console.log('--- English Flow Done ---', storeEn)\n\n  const storeEs: RouterGlobalStore = { content: 'Hola mundo' }\n  await flow.run(storeEs)\n  console.log('--- Spanish Flow Done ---', storeEs)\n\n  const storeUnk: RouterGlobalStore = { content: 'Bonjour le monde' }\n  await flow.run(storeUnk)\n  console.log('--- Unknown Flow Done ---', storeUnk)\n}\nrunFlow()\n```\n\n----------------------------------------\n\nTITLE: Composing Workflows by Nesting Brainyflow Flows in Python\nDESCRIPTION: Illustrates how to use a `Flow` instance as a node within a parent `Flow` in Python, enabling workflow composition. It defines multiple sub-flows (payment, inventory, shipping) and connects them sequentially using transition operators (`>>`) to create a larger, nested order processing pipeline. Running the master `order_pipeline` executes the sub-flows in sequence.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/core_abstraction/flow.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom brainyflow import Flow, Node\n\n# Payment processing sub-flow\nvalidate_payment >> process_payment >> payment_confirmation\npayment_flow = Flow(start=validate_payment)\n\n# Inventory sub-flow\ncheck_stock >> reserve_items >> update_inventory\ninventory_flow = Flow(start=check_stock)\n\n# Shipping sub-flow\ncreate_label >> assign_carrier >> schedule_pickup\nshipping_flow = Flow(start=create_label)\n\n# Connect the flows into a main order pipeline\npayment_flow >> inventory_flow >> shipping_flow\n\n# Create the master flow\norder_pipeline = Flow(start=payment_flow)\n\n# Run the entire pipeline\nmemory = { orderId: 'XYZ789', customerId: 'CUST123' }\nawait order_pipeline.run(memory)\n```\n\n----------------------------------------\n\nTITLE: Running the QA Flow and Accessing Results (TypeScript)\nDESCRIPTION: Defines a main() async function that creates an empty global store, runs the flow returned by createQaFlow, and prints the question and answer from the memory object. Catches and logs any errors. Assumes imports and definitions from earlier steps. Input is via command line; output is logged to console. Expects Node.js and TS environment.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/getting_started.md#2025-04-22_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createQaFlow, QAGlobalStore } from './flow' // defined in the previous steps\n\nasync function main() {\n  // Initialize the global store (can be an empty object)\n  const globalStore: QAGlobalStore = {}\n  const qaFlow = createQaFlow()\n\n  console.log('Running QA Flow...')\n  // Run the flow, passing the initial global store.\n  // The flow modifies the globalStore object in place.\n  // The run method returns the final execution tree (we ignore it here).\n  await qaFlow.run(globalStore)\n\n  // Access the results stored in the global store\n  console.log('\\n--- Flow Complete ---')\n  console.log(`Question: ${globalStore.question ?? 'N/A'}`)\n  console.log(`Answer: ${globalStore.answer ?? 'N/A'}`)\n}\n\nmain().catch(console.error)\n```\n\n----------------------------------------\n\nTITLE: Defining the Recursive NestedActions Type (TypeScript)\nDESCRIPTION: This recursive type alias `NestedActions<T>` describes the structure of the execution tree output by `Flow.run`. It's defined as a `Record` where keys are derived from the elements of the generic type `T` (likely representing action identifiers), and the values are arrays of potentially further nested `NestedActions`, mirroring the branching execution path of the flow.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/design.md#2025-04-22_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nNestedActions<T>: Record<T[number], NestedActions<T>[]>\n```\n\n----------------------------------------\n\nTITLE: Querying Bing Web Search API in Python and TypeScript\nDESCRIPTION: Shows how to interact with the Bing Web Search API using Python ('requests') and TypeScript ('fetch'). Requires the BING_API_KEY environment variable for authentication via the 'Ocp-Apim-Subscription-Key' header. Constructs the request URL, sends the GET request with the header and query parameter, and prints the resulting JSON or an error message.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/websearch.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n```python\nimport requests\nimport os\n\nSUBSCRIPTION_KEY = os.environ.get(\"BING_API_KEY\") # Use environment variables\nquery = \"example\"\n\nurl = \"https://api.bing.microsoft.com/v7.0/search\"\nheaders = {\"Ocp-Apim-Subscription-Key\": SUBSCRIPTION_KEY}\nparams = {\"q\": query}\n\nif not SUBSCRIPTION_KEY:\n    print(\"Error: Please set BING_API_KEY environment variable.\")\nelse:\n    try:\n        response = requests.get(url, headers=headers, params=params)\n        response.raise_for_status()\n        results = response.json()\n        print(results)\n    except requests.exceptions.RequestException as e:\n        print(f\"Error fetching Bing search results: {e}\")\n\n```\n```\n\nLANGUAGE: typescript\nCODE:\n```\n```typescript\nasync function searchBing(query: string): Promise<any> {\n  const subscriptionKey = process.env.BING_API_KEY // Use environment variables\n\n  if (!subscriptionKey) {\n    console.error('Error: Please set BING_API_KEY environment variable.')\n    return null\n  }\n\n  const url = new URL('https://api.bing.microsoft.com/v7.0/search')\n  url.searchParams.append('q', query)\n\n  const headers = {\n    'Ocp-Apim-Subscription-Key': subscriptionKey,\n  }\n\n  try {\n    const response = await fetch(url.toString(), { headers })\n    if (!response.ok) {\n      throw new Error(`HTTP error! status: ${response.status}`)\n    }\n    const results = await response.json()\n    console.log(results)\n    return results\n  } catch (error) {\n    console.error('Error fetching Bing search results:', error)\n    return null\n  }\n}\n\n// Example usage:\n// searchBing(\"example\");\n```\n```\n\n----------------------------------------\n\nTITLE: Implementing Retry Logic with Exponential Backoff (p-retry) in TypeScript\nDESCRIPTION: Demonstrates using the `pRetry` function from the `p-retry` library to wrap an asynchronous operation (`callApiWithRetry`) and automatically retry it on failure. The configuration specifies 5 retries with minimum and maximum timeouts for exponential backoff. Depends on the `p-retry` library.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/throttling.md#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport pRetry from 'p-retry'\n\nasync function callApiWithRetry() {\n  return pRetry(\n    async () => {\n      // Your API call here\n    },\n    {\n      retries: 5,\n      minTimeout: 4000,\n      maxTimeout: 10000,\n    },\n  )\n}\n\n```\n\n----------------------------------------\n\nTITLE: Initializing Weaviate Client in TypeScript\nDESCRIPTION: TypeScript implementation for creating a Weaviate client using environment variables. Handles configuration and optional API key authentication with type safety.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/vector.md#2025-04-22_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nfunction initWeaviateClient(): WeaviateClient | null {\n  /** Initializes Weaviate client. */\n  const scheme = process.env.WEAVIATE_SCHEME || 'http'\n  const host = process.env.WEAVIATE_HOST\n  const apiKey = process.env.WEAVIATE_API_KEY\n\n  if (!host) {\n    console.error('Error: WEAVIATE_HOST not set.')\n    return null\n  }\n\n  try {\n    const clientConfig: any = { scheme, host }\n    if (apiKey) {\n      clientConfig.apiKey = new ApiKey(apiKey)\n    }\n    const client: WeaviateClient = weaviate.client(clientConfig)\n    console.log('Weaviate client initialized.')\n    return client\n  } catch (error) {\n    console.error('Error initializing Weaviate client:', error)\n    return null\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Thought Object Structure in Python\nDESCRIPTION: Python dictionary structure defining the format of individual thought objects stored in the thoughts list, including content, metadata, and relationships.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-thinking/design.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n{\n    \"content\": \"The actual thought text\",\n    \"thought_number\": 1,\n    \"is_revision\": False,\n    \"revises_thought\": None,\n    \"branch_from_thought\": None,\n    \"branch_id\": None,\n    \"next_thought_needed\": True\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Memory Class Static Create Method in TypeScript\nDESCRIPTION: A static factory method `create` for the `Memory` class. It takes global and optional local stores and returns a new `Memory` instance wrapped in a Proxy to manage property access (reads check local then global, writes go to global).\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/api.md#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\n`static create<G extends SharedStore = SharedStore, L extends SharedStore = SharedStore>(global: G, local: L = {} as L): Memory<G, L>`\n```\n\n----------------------------------------\n\nTITLE: Setting Up and Running BrainyFlow Agent - Bash\nDESCRIPTION: This Bash snippet provides the necessary shell commands to set up the BrainyFlow TypeScript agent by copying environment configuration, installing dependencies, and running the agent with a provided question. Required dependencies include npm, Node.js, and a valid API key configured in the .env file. The agent can be invoked with a specific question argument; if omitted, it defaults to answering 'who is the ceo of microsoft?', and outputs the answer through the console.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/typescript-agent/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# from BrainyFlow root directory\ncd cookbook/typescript-agent\n\ncp .env.example .env # add your API key\n\nnpm install\nnpm run agent -- \"this is your question\"\n```\n\n----------------------------------------\n\nTITLE: Managing State with BrainyFlow Memory in Python\nDESCRIPTION: This snippet demonstrates the functionality of the 'Memory' class which manages state within the BrainyFlow library using dual scope support for global and local states. Key features include attribute and dictionary-style access, cloning, and membership testing for state management.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/design.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nbrainyflow.Memory\n```\n\n----------------------------------------\n\nTITLE: Defining Fallback Execution for Node Retries in BrainyFlow Python\nDESCRIPTION: An asynchronous method called if all retry attempts for the `exec` method fail within the `exec_runner`. It receives the `prep_res` and the final `error` that caused the failure. The default implementation simply re-raises the error.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nasync exec_fallback(self, prep_res, error)\n```\n\n----------------------------------------\n\nTITLE: Implementing BatchSummarizeNode Class in Python for BrainyFlow\nDESCRIPTION: This code defines a BatchSummarizeNode class that extends SequentialBatchNode from BrainyFlow. It prepares data from a shared store, executes summarization using an LLM, and posts results back to the store. The class is then instantiated and run with test data loaded from text files.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python_demo.ipynb#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom brainyflow import SequentialBatchNode\nimport os\n\nclass BatchSummarizeNode(SequentialBatchNode):\n    async def prep(self, shared):\n        # Return list of (filename, content) tuples from shared store\n        return [(fn, content) for fn, content in shared[\"data\"].items()]\n        \n    async def exec(self, item):\n        # Unpack the filename and content\n        filename, text = item\n        # Call LLM to summarize\n        prompt = f\"Summarize this text in 50 words:\\n\\n{text}\"\n        summary = call_llm(prompt)\n        return filename, summary\n    \n    async def post(self, shared, prep_res, exec_res_list):\n        # Store all summaries in a dict by filename\n        shared[\"summaries\"] = {\n            filename: summary \n            for filename, summary in exec_res_list\n        }\n        self.trigger(\"default\")\n\n# Create test data structure\nshared = {\n    \"data\": {},\n    \"summaries\": {}\n}\n\n# Load all files from the directory\npath = \"./data/PaulGrahamEssaysLarge\"\nfor filename in os.listdir(path):\n    with open(os.path.join(path, filename), \"r\") as f:\n        shared[\"data\"][filename] = f.read()\n\n# Create and run the batch node\nbatch_summarize = BatchSummarizeNode()\nawait batch_summarize.run(shared)\n```\n\n----------------------------------------\n\nTITLE: Initializing ChromaDB Client and Collection in TypeScript\nDESCRIPTION: TypeScript implementation for initializing ChromaDB client and managing collections. Includes support for persistent storage and optional OpenAI embedding function integration.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/vector.md#2025-04-22_snippet_16\n\nLANGUAGE: typescript\nCODE:\n```\nimport { ChromaClient, OpenAIEmbeddingFunction } from 'chromadb'\n\nfunction initChromaClient(persistDirectory: string = './chroma_data_ts'): ChromaClient | null {\n  /** Initializes a persistent Chroma client. */\n  try {\n    // Persistent client stores data on disk\n    const client = new ChromaClient({ path: persistDirectory })\n    // Or use in-memory client: const client = new ChromaClient();\n    console.log(`Chroma client initialized (persistent path: ${persistDirectory}).`)\n    return client\n  } catch (error) {\n    console.error('Error initializing Chroma client:', error)\n    return null\n  }\n}\n\nasync function getOrCreateChromaCollection(client: ChromaClient, collectionName: string) {\n  /** Gets or creates a Chroma collection. */\n  try {\n    const collection = await client.getOrCreateCollection({ name: collectionName })\n    console.log(`Using Chroma collection '${collectionName}'.`)\n    return collection\n  } catch (error) {\n    console.error('Error getting/creating Chroma collection:', error)\n    return null\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing DirectAnswer Node\nDESCRIPTION: Node class that generates final answers based on accumulated context and original query.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/agent.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass DirectAnswer(Node):\n    async def prep(self, memory: Memory):\n        return memory.query, memory.context if hasattr(memory, 'context') else \"\"\n\n    async def exec(self, inputs):\n        query, context = inputs\n        return call_llm(f\"Context: {context}\\nAnswer: {query}\")\n\n    async def post(self, memory: Memory, prep_res, exec_res):\n       print(f\"Answer: {exec_res}\")\n       memory.answer = exec_res\n       # No trigger needed if this is the end of the path\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory, Node } from 'brainyflow'\n\nclass DirectAnswer extends Node {\n  async prep(memory: Memory): Promise<{ query: string; context: any }> {\n    return { query: memory.query, context: memory.context ?? 'No context' }\n  }\n\n  async exec(prepRes: { query: string; context: any }): Promise<string> {\n    const prompt = `Context: ${JSON.stringify(prepRes.context)}\\nAnswer Query: ${prepRes.query}`\n    return await callLLM(prompt)\n  }\n\n  async post(memory: Memory, prepRes: any, execRes: string): Promise<void> {\n    memory.answer = execRes\n    console.log(`Answer: ${execRes}`)\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Tracing Node Execution with a Logging Decorator in Python\nDESCRIPTION: Provides a class decorator to trace and log node execution stages ('prep', 'exec', 'post') along with timing data. Requires Python's 'logging' and 'time' modules and decorates classes with accompanying methods for any derived Node class.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/visualization_logging.md#2025-04-22_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nimport logging\nimport time\nfrom functools import wraps\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO,\n                   format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger('brainyflow')\n\ndef trace_node(cls):\n    \"\"\"Class decorator to trace node execution\"\"\"\n    # Ensure the methods exist before wrapping\n    original_prep = getattr(cls, 'prep', None)\n    original_exec = getattr(cls, 'exec', None)\n    original_post = getattr(cls, 'post', None)\n\n    @wraps(original_prep)\n    async def traced_prep(self, memory: Memory):\n        logger.info(f\"ENTER prep: {type(self).__name__}\")\n        start_time = time.time()\n        # Call original only if it exists\n        result = await original_prep(self, memory) if original_prep else None\n        elapsed = time.time() - start_time\n        logger.info(f\"EXIT prep: {type(self).__name__} ({elapsed:.3f}s)\")\n        return result\n\n    @wraps(original_exec)\n    async def traced_exec(self, prep_res):\n        logger.info(f\"ENTER exec: {type(self).__name__}\")\n        start_time = time.time()\n        # Call original only if it exists\n        result = await original_exec(self, prep_res) if original_exec else None\n        elapsed = time.time() - start_time\n        logger.info(f\"EXIT exec: {type(self).__name__} ({elapsed:.3f}s)\")\n        return result\n\n    @wraps(original_post)\n    async def traced_post(self, memory: Memory, prep_res, exec_res):\n        logger.info(f\"ENTER post: {type(self).__name__}\")\n        start_time = time.time()\n        # Call original only if it exists\n        # Note: Original post doesn't return the action anymore\n        if original_post:\n             await original_post(self, memory, prep_res, exec_res)\n        elapsed = time.time() - start_time\n        # Log triggers separately if needed, as post doesn't return them\n        logger.info(f\"EXIT post: {type(self).__name__} ({elapsed:.3f}s)\")\n        # The decorator doesn't need to return anything from post\n\n    # Assign wrapped methods back to the class\n    if original_prep: cls.prep = traced_prep\n    if original_exec: cls.exec = traced_exec\n    if original_post: cls.post = traced_post\n    return cls\n```\n\n----------------------------------------\n\nTITLE: Conceptual Batch Node for Text Translation in TypeScript\nDESCRIPTION: This TypeScript class represents a conceptual BatchNode for handling text translation tasks. It prepares a list of text-language pairs, executes the translation, and stores results in shared memory. Requires existing translation logic and shared memory structure to process batches efficiently.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/migration.md#2025-04-22_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\nclass TranslateTextBatchNode extends BatchNode<any, any, any, [string, string], string> {\n  async prep(shared: Record<string, any>): Promise<[string, string][]> {\n    const text = shared['text'] ?? '(No text provided)'\n    const languages = shared['languages'] ?? ['Chinese', 'Spanish', 'Japanese']\n    return languages.map((lang: string) => [text, lang])\n  }\n\n  async exec(item: [string, string]): Promise<string> {\n    const [text, lang] = item\n    // Assume translateText exists\n    return await translateText(text, lang)\n  }\n\n  async post(shared: Record<string, any>, prepRes: any, execResults: string[]): Promise<string> {\n    shared['translations'] = execResults\n    return 'default'\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing BrainyFlow with pip (Python Bash)\nDESCRIPTION: Installs the BrainyFlow library via the Python package index using pip, preparing your environment for development with the framework. Requires Python and pip to be installed. No additional parameters needed. Outputs installed dependencies or errors to the shell.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/getting_started.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install brainyflow\n```\n\n----------------------------------------\n\nTITLE: Agent Architecture Diagram\nDESCRIPTION: Mermaid diagram showing the component relationships and decision flow of the research agent system\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-supervisor/README.md#2025-04-22_snippet_5\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph TD\n    subgraph InnerAgent[Inner Research Agent]\n        DecideAction -->|\"search\"| SearchWeb\n        DecideAction -->|\"answer\"| UnreliableAnswerNode\n        SearchWeb -->|\"decide\"| DecideAction\n    end\n\n    InnerAgent --> SupervisorNode\n    SupervisorNode -->|\"retry\"| InnerAgent\n```\n\n----------------------------------------\n\nTITLE: Implementing Agent Communication with Custom AsyncQueue in TypeScript\nDESCRIPTION: TypeScript implementation of agent communication using a custom AsyncQueue class. Includes message processing and system status monitoring with a similar heartbeat mechanism.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/multi_agent.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Flow, Memory, Node } from 'brainyflow'\n\nclass AgentNode extends Node {\n  async prep(memory: Memory): Promise<string> {\n    const messageQueue = memory.messageQueue as AsyncQueue<string>\n    if (!messageQueue) throw new Error('Message queue not found in memory')\n\n    const message = await messageQueue.get()\n    console.log(`Agent received: ${message}`)\n    return message\n  }\n\n  async exec(message: string): Promise<void> {\n  }\n\n  async post(memory: Memory): Promise<void> {\n    this.trigger('continue')\n  }\n}\n\nconst agent = new AgentNode()\nagent.on('continue', agent)\n\nconst agentFlow = new Flow(agent)\n\nasync function sendSystemMessages(messageQueue: AsyncQueue<string>) {\n  let counter = 0\n  const messages = [\n    'System status: all systems operational',\n    'Memory usage: normal',\n    'Network connectivity: stable',\n    'Processing load: optimal',\n  ]\n\n  while (true) {\n    const message = `${messages[counter % messages.length]} | timestamp_${counter}`\n    await messageQueue.put(message)\n    counter++\n    await new Promise((resolve) => setTimeout(resolve, 1000))\n  }\n}\n\nasync function main() {\n  const messageQueue = new AsyncQueue<string>()\n  const data = { messageQueue }\n\n  console.log('Starting agent listener and message sender...')\n  await Promise.all([agentFlow.run(data), sendSystemMessages(messageQueue)])\n}\n\nclass AsyncQueue<T> {\n  private queue: T[] = []\n  private waiting: ((value: T) => void)[] = []\n\n  async get(): Promise<T> {\n    if (this.queue.length > 0) {\n      return this.queue.shift()!\n    }\n    return new Promise((resolve) => {\n      this.waiting.push(resolve)\n    })\n  }\n\n  async put(item: T): Promise<void> {\n    if (this.waiting.length > 0) {\n      const resolve = this.waiting.shift()!\n      resolve(item)\n    } else {\n      this.queue.push(item)\n    }\n  }\n}\n\nmain().catch(console.error)\n```\n\n----------------------------------------\n\nTITLE: Initiating Action-Specific Linking with Subtraction Operator in BrainyFlow Python\nDESCRIPTION: Provides syntactic sugar using the subtraction operator (`-`) to start linking a node based on a specific `action` string. It returns an `ActionLinker` instance to facilitate the subsequent `>>` operation.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n__sub__(self, action)\n```\n\n----------------------------------------\n\nTITLE: Implementing Node Class Constructor in TypeScript\nDESCRIPTION: The constructor for the `Node` class (aliased from `RetryNode`). It accepts an optional `options` object to configure retry behavior, including `maxRetries` (maximum number of retry attempts) and `wait` (delay in seconds between retries).\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/api.md#2025-04-22_snippet_19\n\nLANGUAGE: typescript\nCODE:\n```\n`constructor(options?: { maxRetries?: number; wait?: number })`\n```\n\n----------------------------------------\n\nTITLE: Implementing BaseNode Class On Method in TypeScript\nDESCRIPTION: The `on` method links a successor `node` to the current node for a specific `action`. It registers the transition and returns the added successor node, allowing for chainable graph construction.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/api.md#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\n`on<T extends BaseNode>(action: Action, node: T): T`\n```\n\n----------------------------------------\n\nTITLE: Importing BrainyFlow in Browser via Separate Scripts (HTML)\nDESCRIPTION: Shows an alternative method to load BrainyFlow in a browser. One script tag with `type=\"module\"` and a `src` attribute loads the library from the unpkg CDN (making it available via `globalThis.brainyflow`). A subsequent standard script tag then accesses the library to instantiate `brainyflow.Node`. Requires a browser that supports ES modules.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/installation.md#2025-04-22_snippet_3\n\nLANGUAGE: html\nCODE:\n```\n<script type=\"module\" src=\"https://unpkg.com/brainyflow@latest/dist/brainyflow.js\"></script>\n<script>\n  new globalThis.brainyflow.Node(...)\n</script>\n```\n\n----------------------------------------\n\nTITLE: Testing Flow Path Execution in TypeScript\nDESCRIPTION: This snippet shows how to test the execution path of a BrainyFlow application in TypeScript using vitest. It creates a simple flow with tracking nodes and verifies that the nodes are executed in the expected sequence based on their triggers.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/testing.md#2025-04-22_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\nimport { describe, expect, it } from 'vitest'\n\n// import { Node, Flow, Memory, BaseNode } from 'brainyflow'; // Assuming imports\n\ndescribe('Flow Path Testing', () => {\n  it('should follow the correct path based on triggers', async () => {\n    /** Tests if the flow executes nodes in the expected sequence. */\n    const visitedNodesLog: string[] = []\n\n    // Define simple tracking nodes\n    class SimpleTrackingNode extends Node<any, any, ['next_step', 'finish']> {\n      private nodeName: string\n      private triggerAction: 'next_step' | 'finish' | 'default'\n\n      constructor(name: string, triggerAction: 'next_step' | 'finish' | 'default' = 'default') {\n        super()\n        this.nodeName = name\n        this.triggerAction = triggerAction\n      }\n\n      async exec(prepRes: any): Promise<string> {\n        // No real work, just track visit\n        visitedNodesLog.push(this.nodeName)\n        return `Processed by ${this.nodeName}` // Return something for post\n      }\n\n      async post(memory: Memory, prepRes: any, execRes: string): Promise<void> {\n        // Trigger the specified action\n        this.trigger(this.triggerAction)\n      }\n    }\n\n    // Create nodes for a path: A -> B -> C (where B triggers 'finish')\n    const nodeA = new SimpleTrackingNode('A', 'next_step')\n    const nodeB = new SimpleTrackingNode('B', 'finish')\n    const nodeC = new SimpleTrackingNode('C') // This node shouldn't be reached\n\n    // Connect nodes based on actions\n    nodeA.on('next_step', nodeB)\n    nodeB.on('finish', nodeC) // Connect C, but B will trigger 'finish'\n\n    // Create and run the flow\n    const flow = new Flow(nodeA)\n    await flow.run({}) // Pass empty memory\n\n    // Verify the execution path\n    expect(visitedNodesLog).toEqual(['A', 'B'])\n  })\n})\n```\n\n----------------------------------------\n\nTITLE: Feature Comparison Matrix in Markdown\nDESCRIPTION: A markdown table comparing key features across BrainyFlow and its alternatives, including core abstractions, dependencies, codebase size, flexibility, integrations, learning curve, and primary focus.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/comparison.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Feature                   | BrainyFlow        | LangChain         | LangGraph            | CrewAI                    | AutoGen               | PocketFlow       |\n| ------------------------- | ----------------- | ----------------- | -------------------- | ------------------------- | --------------------- | ---------------- |\n| **Core Abstraction**      | Nodes & Flows     | Chains & Agents   | State Graphs         | Agents & Crews            | Conversational Agents | Nodes & Flows    |\n| **Dependencies**          | None              | Many              | Many (via LangChain) | Several                   | Several               | None             |\n| **Codebase Size**         | Tiny (~200 lines) | Large             | Medium               | Medium                    | Medium                | Tiny (100 lines) |\n| **Flexibility**           | High              | Medium            | Medium               | Low                       | Medium                | High             |\n| **Built-in Integrations** | None              | Extensive         | Via LangChain        | Several                   | Several               | None             |\n| **Learning Curve**        | Moderate          | Steep             | Very Steep           | Moderate                  | Moderate              | Moderate         |\n| **Primary Focus**         | Graph Execution   | Component Library | State Machines       | Multi-Agent Collaboration | Conversational Agents | Graph Execution  |\n```\n\n----------------------------------------\n\nTITLE: Implementing ParallelFlow Class RunTasks Method (Parallel) in TypeScript\nDESCRIPTION: Overrides the `runTasks` method inherited from `Flow`. In `ParallelFlow`, this method executes an array of asynchronous task functions concurrently using `Promise.all`. It waits for all tasks to complete and returns an array of their results.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/api.md#2025-04-22_snippet_28\n\nLANGUAGE: typescript\nCODE:\n```\n`async runTasks<T>(tasks: (() => T)[]): Promise<Awaited<T>[]>`\n```\n\n----------------------------------------\n\nTITLE: Implementing Sliding Window Rate Limiting (Custom) in TypeScript\nDESCRIPTION: Provides a custom TypeScript class `SlidingWindowRateLimiter` implementing the sliding window algorithm. It tracks request timestamps, removes expired ones, and checks if the current request count is within the `maxRequests` limit over the `windowSize`. Includes methods `allowRequest()` and `timeToNextRequest()`. The usage example shows checking the limit and waiting if necessary.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/throttling.md#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nclass SlidingWindowRateLimiter {\n  private requests: number[] = []\n\n  constructor(\n    private maxRequests: number,\n    private windowSize: number, // in seconds\n  ) {}\n\n  allowRequest(): boolean {\n    const now = Date.now()\n    // Remove expired requests\n    this.requests = this.requests.filter((time) => now - time < this.windowSize * 1000)\n    // Check if we can allow another request\n    if (this.requests.length < this.maxRequests) {\n        this.requests.push(now); // Record the time of the allowed request\n        return true;\n    }\n    return false;\n  }\n\n  timeToNextRequest(): number {\n    const now = Date.now()\n    this.requests = this.requests.filter((time) => now - time < this.windowSize * 1000)\n    if (this.requests.length < this.maxRequests) return 0\n    const oldest = this.requests[0] // Time of the oldest request in the current window\n    // Calculate when the oldest request will expire, making space for a new one\n    const timeUntilExpire = (oldest + this.windowSize * 1000) - now;\n    // Return the wait time in milliseconds, rounded up to the nearest millisecond\n    return Math.max(0, Math.ceil(timeUntilExpire)); \n  }\n}\n\n// Usage:\nconst limiter = new SlidingWindowRateLimiter(100, 60) // 100 requests per 60 seconds\n\nasync function callApi() {\n  while (!limiter.allowRequest()) { // Check and record request time if allowed\n    const waitTime = limiter.timeToNextRequest()\n    console.log(`Rate limit hit. Waiting for ${waitTime}ms`);\n    await new Promise((resolve) => setTimeout(resolve, waitTime))\n  }\n  // If allowRequest returned true, the request time was recorded.\n  console.log(\"Making API call...\");\n  // Your API call here\n  return 'API response'\n}\n\n```\n\n----------------------------------------\n\nTITLE: Implementing Sequential Flow in BrainyFlow\nDESCRIPTION: Creates a standard Flow instance that executes triggered tasks sequentially. This is the default flow type that waits for each branch to complete before starting the next one.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/core_abstraction/flow.md#2025-04-22_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nconst sequentialFlow = new Flow(startNode)\n```\n\n----------------------------------------\n\nTITLE: Adding Successor Node for Action in BrainyFlow Python\nDESCRIPTION: Adds a `node` as a successor to the current node, triggered by a specific `action`. This method allows building the execution graph by defining transitions between nodes based on outcomes. It returns the added successor node, facilitating chaining.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\non(self, action, node)\n```\n\n----------------------------------------\n\nTITLE: Implementing GenerateAnswerNode in Python\nDESCRIPTION: A Python implementation of a GenerateAnswerNode class that generates answers to questions based on retrieved document chunks. It reads the question and retrieved chunk from memory, calls an LLM with a prompt, and stores the answer back into memory.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/rag.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# 2c. Generate Answer Node\nclass GenerateAnswerNode(Node):\n    async def prep(self, memory: Memory):\n        return memory.question, memory.retrieved_chunk # Read from memory\n\n    async def exec(self, inputs):\n        question, chunk = inputs\n        if not chunk or chunk == \"Could not find relevant chunk.\":\n             return \"Sorry, I couldn't find relevant information to answer the question.\"\n        prompt = f\"Using the following context, answer the question.\\nContext: {chunk}\\nQuestion: {question}\\nAnswer:\"\n        print(\"Generating final answer...\")\n        return await call_llm(prompt)\n\n    async def post(self, memory: Memory, prep_res, answer):\n        memory.answer = answer # Write to memory\n        print(\"Answer:\", answer)\n        # End of online flow\n```\n\n----------------------------------------\n\nTITLE: TypeScript Node Error Handling with Retries in BrainyFlow\nDESCRIPTION: Demonstrates error handling with retry logic in a TypeScript node. Configures retry parameters and overrides `execFallback` to manage failures in `exec()` function after all retries are exhausted.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/core_abstraction/node.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory, Node, NodeError } from 'brainyflow'\n\ntype PrepResult = void\ntype ExecResult = string\n\nconst myNode = new CustomErrorHandlingNode({ maxRetries: 3, wait: 10 })\n\nclass CustomErrorHandlingNode extends Node<any, any, [], PrepResult, ExecResult> {\n  async exec(prepRes: PrepResult): Promise<ExecResult> {\n    console.log(`Exec attempt: ${this.curRetry + 1}`)\n    if (this.curRetry < 2) {\n      throw new Error('Temporary failure!')\n    }\n    return 'Success on retry'\n  }\n\n  async execFallback(prepRes: PrepResult, error: NodeError): Promise<ExecResult> {\n    // This is called only if exec fails on the last attempt\n    console.error(`Exec failed after ${error.retryCount + 1} attempts: ${error.message}`)\n    // Return a fallback value instead of re-throwing\n    return `Fallback response due to repeated errors: ${error.message}`\n  }\n\n  async post(memory: Memory<any, any>, prepRes: PrepResult, execRes: ExecResult): Promise<void> {\n    // execRes will be \"Success on retry\" or \"Fallback response...\"\n    console.log(`Post: Received result '${execRes}'`)\n    memory.final_result = execRes\n    this.trigger('default')\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Flowchart of Batch Translation Process using BrainyFlow\nDESCRIPTION: Mermaid flowchart illustrating the fan-out pattern with three nodes (TriggerTranslationsNode, TranslateOneLanguageNode, WriteTranslationsNode) orchestrated by a Flow for sequential processing.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-batch/README.md#2025-04-22_snippet_3\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n    trigger[TriggerTranslationsNode] -->|process_one| processor[TranslateOneLanguageNode]\n    trigger -->|write_results| writer[WriteTranslationsNode]\n    processor --> writer\n```\n\n----------------------------------------\n\nTITLE: Creating Qdrant Collection if Not Exists Python\nDESCRIPTION: Checks for the existence of a Qdrant collection and creates it if not present. Uses collection name, dimension and distance metrics to set up the collection.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/vector.md#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ndef create_qdrant_collection_if_not_exists(client: qdrant_client.QdrantClient, collection_name: str, dimension: int, distance_metric: Distance = Distance.COSINE):\n    \"\"\"Creates a Qdrant collection if it doesn't exist.\"\"\"\n    try:\n        collections = client.get_collections().collections\n        if not any(c.name == collection_name for c in collections):\n            print(f\"Creating collection '{collection_name}'...\")\n            client.recreate_collection( # Use recreate_collection for simplicity, or check/create\n                collection_name=collection_name,\n                vectors_config=VectorParams(size=dimension, distance=distance_metric)\n            )\n            print(f\"Collection '{collection_name}' created.\")\n        else:\n            print(f\"Collection '{collection_name}' already exists.\")\n    except Exception as e:\n        print(f\"Error creating or checking Qdrant collection: {e}\")\n```\n\n----------------------------------------\n\nTITLE: Defining Asynchronous Preparation Phase for BaseNode in BrainyFlow Python\nDESCRIPTION: Defines the asynchronous preparation phase method (`prep`). Subclasses should override this method to perform any setup or data loading required before the main execution logic. Takes the current `memory` state as input.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nasync prep(self, memory)\n```\n\n----------------------------------------\n\nTITLE: Testing Flow Path Execution in Python\nDESCRIPTION: This snippet demonstrates how to test the execution path of a BrainyFlow application in Python. It creates a simple flow with tracking nodes and verifies that the nodes are executed in the expected sequence based on their triggers.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/testing.md#2025-04-22_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\n# from brainyflow import Node, Flow, Memory # Assuming imports\n\nasync def test_flow_follows_correct_path():\n    \"\"\"Tests if the flow executes nodes in the expected sequence.\"\"\"\n    visited_nodes_log = []\n\n    # Define simple tracking nodes\n    class SimpleTrackingNode(Node):\n        def __init__(self, name: str, trigger_action: str = \"default\"):\n            super().__init__()\n            self._node_name = name\n            self._trigger_action = trigger_action\n\n        async def exec(self, prep_res):\n             # No real work, just track visit\n             visited_nodes_log.append(self._node_name)\n             return f\"Processed by {self._node_name}\" # Return something for post\n\n        async def post(self, memory, prep_res, exec_res):\n            # Trigger the specified action\n            self.trigger(self._trigger_action)\n\n    # Create nodes for a simple path: A -> B -> C\n    node_a = SimpleTrackingNode(\"A\", trigger_action=\"next_step\")\n    node_b = SimpleTrackingNode(\"B\", trigger_action=\"finish\")\n    node_c = SimpleTrackingNode(\"C\") # This node shouldn't be reached\n\n    # Connect nodes based on actions\n    node_a.on(\"next_step\", node_b)\n    node_b.on(\"finish\", node_c) # Connect C, but B will trigger 'finish'\n\n    # Create and run the flow\n    flow = Flow(start=node_a)\n    await flow.run({}) # Pass empty memory\n\n    # Verify the execution path\n    assert visited_nodes_log == [\"A\", \"B\"], f\"Expected A->B, but got: {visited_nodes_log}\"\n\n# asyncio.run(test_flow_follows_correct_path())\n```\n\n----------------------------------------\n\nTITLE: Implementing Agent Decision Prompt Template\nDESCRIPTION: Template strings for agent decision-making that defines context, action space and expected response format. Used to guide agent's decision between searching and answering.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/agent.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nf\"\"\"\n### CONTEXT\nTask: {task_description}\nPrevious Actions: {previous_actions}\nCurrent State: {current_state}\n\n### ACTION SPACE\n[1] search\n  Description: Use web search to get results\n  Parameters:\n    - query (str): What to search for\n\n[2] answer\n  Description: Conclude based on the results\n  Parameters:\n    - result (str): Final answer to provide\n\n### NEXT ACTION\nDecide the next action based on the current context and available action space.\nReturn your response in the following format:\n\n```yaml\nthinking: |\n    <your step-by-step reasoning process>\naction: <action_name>\nparameters:\n    <parameter_name>: <parameter_value>\n```\"\"\"\n```\n\nLANGUAGE: typescript\nCODE:\n```\n;`### CONTEXT\nTask: ${taskDescription}\nPrevious Actions: ${previousActions}\nCurrent State: ${currentState}\n\n### ACTION SPACE\n[1] search\n  Description: Use web search to get results\n  Parameters:\n    - query (string): What to search for\n\n[2] answer\n  Description: Conclude based on the results  \n  Parameters:\n    - result (string): Final answer to provide\n\n### NEXT ACTION\nDecide the next action based on the current context and available action space.\nReturn your response in the following format:\n\n\\`\\`\\`yaml\nthinking: |\n    <your step-by-step reasoning process>\naction: <action_name>\nparameters:\n    <parameter_name>: <parameter_value>\n\\`\\`\\``\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Node Error Exception in BrainyFlow Python\nDESCRIPTION: Defines a custom exception class `NodeError` that inherits from `Exception`. This exception is raised during node execution failures and can potentially carry information to facilitate retry mechanisms.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nNodeError(Exception)\n```\n\n----------------------------------------\n\nTITLE: Concurrent Graph Execution with ParallelFlow in Python\nDESCRIPTION: This snippet describes the 'ParallelFlow' class, a subclass of Flow enabling concurrent execution of nodes using asyncio to improve performance by parallelizing branch executions.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/design.md#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nbrainyflow.ParallelFlow\n```\n\n----------------------------------------\n\nTITLE: Initializing Redis Client in TypeScript\nDESCRIPTION: This TypeScript function initializes a Redis client using the provided REDIS_URL environment variable. It handles connection errors and returns the connected client or null if unsuccessful.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/vector.md#2025-04-22_snippet_21\n\nLANGUAGE: TypeScript\nCODE:\n```\nasync function initRedisClient(): Promise<ReturnType<typeof createClient> | null> {\n  /** Initializes Redis client. */\n  const redisUrl = process.env.REDIS_URL // e.g., \"redis://localhost:6379\" or \"redis://:password@host:port\"\n  if (!redisUrl) {\n    console.error('Error: REDIS_URL not set.')\n    return null\n  }\n  try {\n    const client = createClient({ url: redisUrl })\n    await client.connect()\n    console.log(`Redis client connected to ${redisUrl}.`)\n    return client\n  } catch (error) {\n    console.error('Error connecting to Redis:', error)\n    return null\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Preparing Document Embeddings and FAISS Index with BrainyFlow\nDESCRIPTION: This node creates embeddings for documents, builds a FAISS index, and stores the results in the shared context. It processes a list of (filename, content) pairs to generate embeddings and construct the search index.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python_demo.ipynb#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass PrepareEmbeddings(Node):\n    async def prep(self, shared):\n        # Get list of (filename, content) pairs\n        return list(shared[\"data\"].items())\n        \n    async def exec(self, items):\n        # Create embeddings for each document\n        embeddings = []\n        filenames = []\n        for filename, content in items:\n            embedding = get_embedding(content)\n            embeddings.append(embedding)\n            filenames.append(filename)\n            \n        # Create FAISS index\n        dim = len(embeddings[0])\n        index = faiss.IndexFlatL2(dim)\n        index.add(np.array(embeddings).astype('float32'))\n        \n        return index, filenames\n    \n    async def post(self, shared, prep_res, exec_res):\n        # Store index and filenames in shared store\n        index, filenames = exec_res\n        shared[\"search_index\"] = index\n        shared[\"filenames\"] = filenames\n        self.trigger(\"default\")\n```\n\n----------------------------------------\n\nTITLE: Embedding User Query Node in Python\nDESCRIPTION: Defines the `EmbedQueryNode` class in Python for the online query stage. The `prep` method reads the user's `question` from the shared `memory`. The `exec` method takes the question, prints it, and calls an asynchronous external function `get_embedding` to generate its vector representation. The `post` method stores the resulting query embedding (`q_emb`) back into `memory` and triggers the next node by emitting the 'retrieve_docs' event.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/rag.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# --- Stage 2: Online Query Nodes ---\n\n# 2a. Embed Query Node\nclass EmbedQueryNode(Node):\n    async def prep(self, memory: Memory):\n        return memory.question # Read from memory\n\n    async def exec(self, question):\n        print(f\"Embedding query: \\\"{question}\\\"\")\n        return await get_embedding(question)\n\n    async def post(self, memory: Memory, prep_res, q_emb):\n        memory.q_emb = q_emb # Write to memory\n        self.trigger('retrieve_docs')\n```\n```\n\n----------------------------------------\n\nTITLE: Implementing SearchWeb Node\nDESCRIPTION: Node class that handles web search functionality, storing results in memory and triggering further decision-making.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/agent.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass SearchWeb(Node):\n    async def prep(self, memory: Memory):\n        return memory.search_term\n\n    async def exec(self, search_term):\n        return await search_web(search_term)\n\n    async def post(self, memory: Memory, prep_res, exec_res):\n        prev_searches = memory.context if hasattr(memory, 'context') else []\n        memory.context = prev_searches + [\n            {\"term\": prep_res, \"result\": exec_res}\n        ]\n        self.trigger('decide')\n```\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory, Node } from 'brainyflow'\n\nclass SearchWeb extends Node {\n  async prep(memory: Memory): Promise<string> {\n    return memory.search_term\n  }\n\n  async exec(searchTerm: string): Promise<any> {\n    return await searchWeb(searchTerm)\n  }\n\n  async post(memory: Memory, prepRes: string, execRes: any): Promise<void> {\n    const prevContext = memory.context ?? []\n    memory.context = [...prevContext, { term: prepRes, result: execRes }]\n    this.trigger('decide')\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining BaseNode Class Abstract ExecRunner Method in TypeScript\nDESCRIPTION: Defines the protected abstract method `execRunner`. Subclasses like `Node` must implement this method to define how the core `exec` logic is executed, potentially adding features like retry mechanisms. It receives the `memory` instance and the `prepRes`.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/api.md#2025-04-22_snippet_17\n\nLANGUAGE: typescript\nCODE:\n```\n`protected abstract execRunner(memory: Memory<GlobalStore, LocalStore>, prepRes: PrepResult | void): Promise<ExecResult | void>`\n```\n\n----------------------------------------\n\nTITLE: Visualizing an Expense Approval Workflow with Mermaid Flowchart\nDESCRIPTION: Provides a Mermaid flowchart diagram visualizing the structure of the expense approval example. It clearly shows the nodes (`Review Expense`, `Process Payment`, `Revise Report`, `Finish Process`) and the conditional transitions ('approved', 'needs_revision', 'rejected') that define the flow's branching and looping logic.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/core_abstraction/flow.md#2025-04-22_snippet_5\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart TD\n    review[Review Expense] -->|approved| payment[Process Payment]\n    review -->|needs_revision| revise[Revise Report]\n    review -->|rejected| finish[Finish Process]\n\n    revise --> review\n    payment --> finish\n```\n\n----------------------------------------\n\nTITLE: Defining BrainyFlow Shared Store Schema with TypeScript Interfaces\nDESCRIPTION: Illustrates how to define the structure of the BrainyFlow shared store using TypeScript interfaces. It defines separate interfaces for input, processing, and output stages, combines them conceptually, and also shows a common 'FlatGlobalStore' interface often used in BrainyFlow. A conceptual memory object example is provided.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/agentic_coding.md#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n// Define interfaces for the shared store structure\ninterface InputStore {\n  document_path: string\n}\n\ninterface ProcessingStore {\n  document_text: string\n  entities: {\n    parties: any[]\n    dates: any[]\n    amounts: any[]\n  }\n  validation_status: string\n}\n\ninterface OutputStore {\n  summary: string\n  storage_id: string\n}\n\n// Combine interfaces for the complete global store (if using nested structure conceptually)\ninterface GlobalStore extends InputStore, ProcessingStore, OutputStore {}\n\n// Or define a flat global store interface (more common in BrainyFlow usage)\ninterface FlatGlobalStore {\n  document_path?: string\n  document_text?: string\n  entities?: {\n    parties: any[]\n    dates: any[]\n    amounts: any[]\n  }\n  validation_status?: string\n  summary?: string\n  storage_id?: string\n}\n\n// Conceptual structure (using the flat interface)\nconst memoryConceptual: FlatGlobalStore = {\n  document_path: 'path/to/file.pdf',\n  document_text: '',\n  entities: {\n    parties: [],\n    dates: [],\n    amounts: [],\n  },\n  validation_status: '',\n  summary: '',\n  storage_id: '',\n}\n\n// Note: In BrainyFlow, you'd typically pass an object conforming to\n// FlatGlobalStore (or a relevant subset) to flow.run() and access\n// properties directly, e.g., memory.document_text = \"...\", const entities = memory.entities;\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for BrainyFlow Research Agent\nDESCRIPTION: Installs the necessary Python packages for the BrainyFlow research agent, including brainyflow, aiohttp, openai, and duckduckgo-search.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-agent/demo.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n! pip install brainyflow>=1.0.0\n! pip install aiohttp>=3.8.0\n! pip install openai>=1.0.0\n! pip install duckduckgo-search>=7.5.2\n```\n\n----------------------------------------\n\nTITLE: Implementing BaseNode Class Run Method in TypeScript\nDESCRIPTION: The `run` method orchestrates the full lifecycle of a single node: `prep`, `execRunner`, and `post`. It accepts either a `Memory` instance or just the `GlobalStore` (creating a default Memory). If `propagate` is true, it returns the list of triggers generated during the `post` phase; otherwise, it returns the result of `execRunner`.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/api.md#2025-04-22_snippet_18\n\nLANGUAGE: typescript\nCODE:\n```\n`async run(memory: Memory<GlobalStore, LocalStore> | GlobalStore, propagate?: boolean)`\n```\n\n----------------------------------------\n\nTITLE: Defining NodeError Custom Error Type in TypeScript\nDESCRIPTION: Defines a type alias `NodeError` by intersecting the built-in `Error` type with an optional `retryCount` number property. This custom error type is used within nodes to potentially track retry attempts during execution.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/api.md#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\n`NodeError`: `Error & { retryCount?: number }`\n```\n\n----------------------------------------\n\nTITLE: Defining Asynchronous Execution Phase for BaseNode in BrainyFlow Python\nDESCRIPTION: Defines the asynchronous execution phase method (`exec`). Subclasses must override this method to implement the core computational logic of the node. It receives the result from the `prep` phase (`prep_res`).\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nasync exec(self, prep_res)\n```\n\n----------------------------------------\n\nTITLE: Implementing BaseNode Class ListTriggers Method in TypeScript\nDESCRIPTION: A private method `listTriggers` that retrieves the list of actions triggered via the `trigger` method during the `post` phase. It returns an array of tuples, where each tuple contains the triggered action identifier and a cloned `Memory` instance (including any `forkingData` passed to `trigger`).\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/api.md#2025-04-22_snippet_16\n\nLANGUAGE: typescript\nCODE:\n```\n`private listTriggers(memory: Memory<GlobalStore, LocalStore>): [AllowedActions[number], Memory<GlobalStore, LocalStore>][]`\n```\n\n----------------------------------------\n\nTITLE: Integration Testing a BrainyFlow Flow in Python\nDESCRIPTION: Illustrates integration testing for a question-answering flow (`qa_flow`) in Python using `unittest`. It mocks the `call_llm` function with `side_effect` to simulate different responses based on the prompt content (e.g., 'search' vs. 'answer'). The test verifies the final state of the `memory` object and the number of LLM calls.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/testing.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n```python\nimport unittest\nfrom unittest.mock import AsyncMock, patch\nfrom brainyflow import Flow\n\nclass TestQuestionAnsweringFlow(unittest.TestCase):\n    async def test_qa_flow(self):\n        # Create the flow\n        qa_flow = create_qa_flow()\n\n        # Create a mock shared store\n        memory = {\"question\": \"What is the capital of France?\"}\n\n        # Mock all LLM calls\n        with patch('utils.call_llm', new_callable=AsyncMock) as mock_llm:\n            # Configure the mock to return different values for different prompts\n            def mock_llm_side_effect(prompt):\n                if \"search\" in prompt.lower():\n                    return \"Paris is the capital of France.\"\n                elif \"answer\" in prompt.lower():\n                    return \"The capital of France is Paris.\"\n                return \"Unexpected prompt\"\n\n            mock_llm.side_effect = mock_llm_side_effect\n\n            # Run the flow\n            await qa_flow.run(memory)\n\n            # Verify the final answer\n            self.assertEqual(memory.answer, \"The capital of France is Paris.\") # Access memory object\n\n            # Verify the LLM was called the expected number of times\n            self.assertEqual(mock_llm.call_count, 2)\n\nif __name__ == '__main__':\n    # Use asyncio.run for async tests if needed\n    unittest.main()\n```\n```\n\n----------------------------------------\n\nTITLE: Conceptual FAISS Usage in TypeScript using hnswlib-node\nDESCRIPTION: Explains that direct FAISS usage in TypeScript is uncommon due to its C++/Python origins. It suggests alternatives like calling a Python backend, using WASM ports, or employing JS-native libraries such as `hnswlib-node`. Provides a conceptual, commented-out example using `hnswlib-node` to initialize an HNSW index, add points (vectors with IDs), and perform a k-nearest neighbor search. This code is illustrative and requires `npm install hnswlib-node`.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/vector.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// FAISS is primarily a C++/Python library.\n// Direct usage in TypeScript often involves:\n// 1. Calling a Python backend service that uses FAISS.\n// 2. Using community-maintained WASM ports (may have limitations).\n// 3. Using alternative JS-native vector search libraries like hnswlib-node.\n\n// Example using hnswlib-node (conceptual - requires installation)\n// npm install hnswlib-node\n/*\nimport { HierarchicalNSW } from 'hnswlib-node';\n\nasync function exampleHNSW() {\n  const dim = 128;\n  const maxElements = 1000;\n\n  // Initialize index\n  const index = new HierarchicalNSW('l2', dim); // 'l2' for Euclidean distance\n  index.initIndex(maxElements);\n\n  // Add vectors (example data)\n  for (let i = 0; i < maxElements; i++) {\n    const vector = Array.from({ length: dim }, () => Math.random());\n    index.addPoint(vector, i); // Add vector with its ID (index i)\n  }\n\n  // Query\n  const queryVector = Array.from({ length: dim }, () => Math.random());\n  const numNeighbors = 5;\n  const result = index.searchKnn(queryVector, numNeighbors);\n\n  console.log(\"HNSW Neighbors:\", result.neighbors); // Indices\n  console.log(\"HNSW Distances:\", result.distances);\n}\n\n// exampleHNSW();\n*/\n\nconsole.log('FAISS is typically used via Python or a dedicated service.')\nconsole.log(\n  'Consider using a JS-native library like hnswlib-node or a managed vector DB for TypeScript projects.',\n)\n```\n\n----------------------------------------\n\nTITLE: Calling LLM and Getting Embeddings in brainyflow (Python)\nDESCRIPTION: This Python snippet demonstrates example usage of brainyflow functions. It calls `call_llm` with a question ('What's the meaning of life?') to get a response from a Large Language Model and prints the result. It then calls `get_embedding` with the same question to obtain its vector representation (embedding) and prints that result.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python_demo.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Example usage:\nresponse = call_llm(\"What's the meaning of life?\")\nprint(response)\nembedding = get_embedding(\"What's the meaning of life?\")\nprint(embedding)\n```\n\n----------------------------------------\n\nTITLE: Importing Mocking Libraries for Async Tests in Python\nDESCRIPTION: This snippet shows the necessary imports (`patch`, `AsyncMock` from `unittest.mock`, and `asyncio`) for setting up mocks, particularly for asynchronous functions, within Python unit or integration tests for BrainyFlow applications.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/testing.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n```python\nfrom unittest.mock import patch, AsyncMock\nimport asyncio\n```\n```\n\n----------------------------------------\n\nTITLE: Creating Memory Instance via Factory Method in BrainyFlow Python\nDESCRIPTION: A static factory method used to create a new `Memory` instance. It takes the `global_store` and an optional `local_store` as arguments.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ncreate(global_store, local_store=None) (staticmethod)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Python Virtual Environment - Bash Commands\nDESCRIPTION: Commands for creating and activating a Python virtual environment, including cross-platform activation instructions for Windows and Unix-based systems.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-node/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n```\n\n----------------------------------------\n\nTITLE: Visualizing Chain of Thought Flow with Mermaid\nDESCRIPTION: Flowchart showing the self-looping nature of the Chain of Thought node implementation using Mermaid diagram syntax.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-thinking/design.md#2025-04-22_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n    cot[ChainOfThoughtNode] -->|\"continue\"| cot\n```\n\n----------------------------------------\n\nTITLE: Running Test Problem\nDESCRIPTION: Command to execute the main script to test the thinking mode implementation.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-thinking/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Project Directory Structure\nDESCRIPTION: Shows the file organization and structure of the project with database operations, node implementation, and configuration files.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-tool-database/README.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\npython-tool-database/\n├── tools/\n│   └── database.py    # SQLite database operations\n├── nodes.py          # BrainyFlow node implementation\n├── flow.py          # Flow configuration\n└── main.py          # Example usage\n```\n\n----------------------------------------\n\nTITLE: Cloning Memory Instance in BrainyFlow Python\nDESCRIPTION: Creates a new `Memory` instance. The new instance shares the same global store but gets a deep copy of the original local store. Optionally, the `forking_data` dictionary can be provided to update the new local store upon creation. This is crucial for branching execution paths.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclone(self, forking_data=None)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Flow with Trigger and Processor Nodes\nDESCRIPTION: This Python snippet demonstrates how to set up a flow between a trigger and processor node. The trigger node initiates the process and passes it to the processor node that handles translations. This setup establishes a connection between different nodes in a translation flow.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/migration.md#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntrigger_node = TriggerTranslationsNode()\nprocessor_node = TranslateOneLanguageNode()\n\ntrigger_node >> processor_node\n```\n\n----------------------------------------\n\nTITLE: Installing and Running the BrainyFlow Example in Bash\nDESCRIPTION: Commands to install required dependencies and run the BrainyFlow async example application. This shows the simple setup needed to execute the demonstration project.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-async-basic/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Node Classes for BrainyFlow Research Agent\nDESCRIPTION: Implements custom Node classes for decision-making (DecideAction), web searching (SearchWeb), and question answering (AnswerQuestion) using the BrainyFlow framework.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-agent/demo.ipynb#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# nodes.py\nfrom brainyflow import Node, Memory\nimport yaml\n\nclass DecideAction(Node):\n    async def prep(self, memory: Memory):\n        \"\"\"Prepare the context and question for the decision-making process.\"\"\"\n        # Get the current context (default to \"No previous search\" if none exists)\n        context = memory.context if hasattr(memory, 'context') else \"No previous search\"\n        # Get the question from memory\n        question = memory.question\n        # Return both for the exec step\n        return question, context\n\n    async def exec(self, inputs):\n        \"\"\"Call the LLM to decide whether to search or answer.\"\"\"\n        question, context = inputs\n\n        print(f\"🤔 Agent deciding what to do next...\")\n\n        # Create a prompt to help the LLM decide what to do next with proper yaml formatting\n        prompt = f\"\"\"\n### CONTEXT\nYou are a research assistant that can search the web.\nQuestion: {question}\nPrevious Research: {context}\n\n### ACTION SPACE\n[1] search\n  Description: Look up more information on the web\n  Parameters:\n    - query (str): What to search for\n\n[2] answer\n  Description: Answer the question with current knowledge\n  Parameters:\n    - answer (str): Final answer to the question\n\n## NEXT ACTION\nDecide the next action based on the context and available actions.\nReturn your response in this format:\n\n```yaml\nthinking: |\n    <your step-by-step reasoning process>\naction: search OR answer\nreason: <why you chose this action>\nanswer: <if action is answer>\nsearch_query: <specific search query if action is search>\n```\nIMPORTANT: Make sure to:\n1. Use proper indentation (4 spaces) for all multi-line fields\n2. Use the | character for multi-line text fields\n3. Keep single-line fields without the | character\n\"\"\"\n\n        # Call the LLM to make a decision\n        response = call_llm(prompt)\n\n        # Parse the response to get the decision\n        yaml_str = response.split(\"```yaml\")[1].split(\"```\")[0].strip()\n        decision = yaml.safe_load(yaml_str)\n\n        return decision\n\n    async def post(self, memory: Memory, prep_res, exec_res):\n        \"\"\"Save the decision and determine the next step in the flow.\"\"\"\n        # If LLM decided to search, save the search query\n        if exec_res[\"action\"] == \"search\":\n            memory.search_query = exec_res[\"search_query\"]\n            print(f\"🔍 Agent decided to search for: {exec_res['search_query']}\")\n        else:\n            memory.context = exec_res[\"answer\"] #save the context if LLM gives the answer without searching.\n            print(f\"💡 Agent decided to answer the question\")\n        # Return the action to determine the next node in the flow\n        self.trigger(exec_res[\"action\"])\n\nclass SearchWeb(Node):\n    async def prep(self, memory: Memory):\n        \"\"\"Get the search query from memory.\"\"\"\n        return memory.search_query\n\n    async def exec(self, search_query):\n        \"\"\"Search the web for the given query.\"\"\"\n        # Call the search utility function\n        print(f\"🌐 Searching the web for: {search_query}\")\n        results = search_web(search_query)\n        return results\n\n    async def post(self, memory: Memory, prep_res, exec_res):\n        \"\"\"Save the search results and go back to the decision node.\"\"\"\n        # Add the search results to the context in memory\n        previous = memory.context if hasattr(memory, 'context') else \"\"\n        memory.context = previous + \"\\n\\nSEARCH: \" + memory.search_query + \"\\nRESULTS: \" + exec_res\n\n        print(f\"📚 Found information, analyzing results...\")\n\n        # Always go back to the decision node after searching\n        self.trigger(\"decide\")\n\nclass AnswerQuestion(Node):\n    async def prep(self, memory: Memory):\n        \"\"\"Get the question and context for answering.\"\"\"\n        return memory.question, memory.context if hasattr(memory, 'context') else \"\"\n\n    async def exec(self, inputs):\n        \"\"\"Call the LLM to generate a final answer.\"\"\"\n        question, context = inputs\n\n        print(f\"✍️ Crafting final answer...\")\n\n        # Create a prompt for the LLM to answer the question\n        prompt = f\"\"\"\n### CONTEXT\nBased on the following information, answer the question.\nQuestion: {question}\nResearch: {context}\n\n## YOUR ANSWER:\nProvide a comprehensive answer using the research results.\n\"\"\"\n        # Call the LLM to generate an answer\n        answer = call_llm(prompt)\n        return answer\n\n    async def post(self, memory: Memory, prep_res, exec_res):\n        \"\"\"Save the final answer and complete the flow.\"\"\"\n        # Save the answer in memory\n        memory.answer = exec_res\n\n        print(f\"✅ Answer generated successfully\")\n\n        # We're done - no need to continue the flow\n        self.trigger(\"done\")\n```\n\n----------------------------------------\n\nTITLE: Getting Text Embeddings with AWS Bedrock in Python\nDESCRIPTION: Retrieves text embeddings using AWS Bedrock service. Requires AWS account setup and authentication. Supports configuration of AWS region and model ID with error handling.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/embedding.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Requires: pip install boto3 numpy\nimport boto3\nimport json\nimport numpy as np\nimport os\n\ndef get_bedrock_embedding(text: str, region_name: str | None = None, model_id: str = \"amazon.titan-embed-text-v2:0\") -> np.ndarray | None:\n    \"\"\"Gets embedding from AWS Bedrock.\"\"\"\n    region = region_name or os.environ.get(\"AWS_REGION\", \"us-east-1\")\n    try:\n        # Ensure AWS credentials are configured (e.g., via env vars, ~/.aws/credentials)\n        client = boto3.client(\"bedrock-runtime\", region_name=region)\n        body = json.dumps({\"inputText\": text})\n        response = client.invoke_model(\n            modelId=model_id,\n            contentType=\"application/json\",\n            accept=\"application/json\",\n            body=body\n        )\n        response_body = json.loads(response['body'].read())\n        embedding = response_body.get('embedding')\n        if embedding:\n            return np.array(embedding, dtype=np.float32)\n        else:\n            print(\"Error: Embedding not found in Bedrock response.\")\n            return None\n    except Exception as e:\n        print(f\"Error calling AWS Bedrock embedding API: {e}\")\n        return None\n\n# Example:\n# text_to_embed = \"Hello world\"\n# embedding_vector = get_bedrock_embedding(text_to_embed)\n# if embedding_vector is not None:\n#     print(embedding_vector)\n```\n\n----------------------------------------\n\nTITLE: Implementing Agent Pattern Using BrainyFlow - Python\nDESCRIPTION: This Python code snippet demonstrates how to implement the Agent design pattern in BrainyFlow. It defines agent components as nodes—PerceiveNode, ThinkNode, and ActNode—and connects them in a cyclical flow to represent the perceiving, thinking, and acting loop common to intelligent agents. It requires the brainyflow package and user-defined PerceiveNode, ThinkNode, and ActNode classes. The agent_flow object encapsulates the behavior, accepting no explicit inputs but orchestrating the agent cycle upon instantiation.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/index.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n```python\\nfrom brainyflow import Flow, Node\\n\\n# Define the agent's components (assuming these classes exist)\\nperceive = PerceiveNode()\\nthink = ThinkNode()\\nact = ActNode()\\n\\n# Connect them in a cycle\\nperceive >> think >> act >> perceive\\n\\n# Create the agent flow\\nagent_flow = Flow(start=perceive)\\n```\n```\n\n----------------------------------------\n\nTITLE: Installing Python Project Dependencies with pip in Bash\nDESCRIPTION: This snippet uses pip to install project dependencies from a requirements.txt file into the current Python environment. It assumes that the user has already activated the appropriate virtual environment. Input: requirements.txt file; Output: all listed dependencies installed into the environment. Limitation: requirements.txt must exist in the current directory.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-tool-embeddings/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Defining and Executing Flows in Python\nDESCRIPTION: Defines various nodes and flows in a data science context, showcasing how nodes connect and run within flows. Highlights how call stack tracing can assist in debugging node execution sequence.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/visualization_logging.md#2025-04-22_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nclass DataPrepBatchNode(BatchNode):\n    def prep(self, shared): return []\nclass ValidateDataNode(Node): pass\nclass FeatureExtractionNode(Node): pass\nclass TrainModelNode(Node): pass\nclass EvaluateModelNode(Node):\n    def prep(self, shared):\n        stack = get_node_call_stack()\n        print(\"Call stack:\", stack)\nclass ModelFlow(Flow): pass\nclass DataScienceFlow(Flow):pass\n\nfeature_node = FeatureExtractionNode()\ntrain_node = TrainModelNode()\nevaluate_node = EvaluateModelNode()\nfeature_node >> train_node >> evaluate_node\nmodel_flow = ModelFlow(start=feature_node)\ndata_prep_node = DataPrepBatchNode()\nvalidate_node = ValidateDataNode()\ndata_prep_node >> validate_node >> model_flow\ndata_science_flow = DataScienceFlow(start=data_prep_node)\ndata_science_flow.run({})\n```\n\n----------------------------------------\n\nTITLE: Using Flow Patterns with ParallelFlow in TypeScript\nDESCRIPTION: This snippet refines the translation process using Flow Patterns and ParallelFlow for concurrency. It includes nodes like TriggerTranslationsNode and TranslateOneLanguageNode to manage translation workflow, execute tasks in parallel, and store results in memory. Dependencies include Brainyflow library and a pre-existing translation function.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/migration.md#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory, Node } from 'brainyflow'\n\ninterface TranslationGlobalStore {\n  text?: string\n  languages?: string[]\n  translations?: ({ language: string; translation: string } | null)[]\n}\ninterface TranslationLocalStore {\n  text?: string\n  language?: string\n  index?: number\n}\ntype TranslationActions = 'translate_one' | 'aggregate_results'\n\nclass TriggerTranslationsNode extends Node<\n  TranslationGlobalStore,\n  TranslationLocalStore,\n  TranslationActions[]\n> {\n  async prep(\n    memory: Memory<TranslationGlobalStore, TranslationLocalStore>,\n  ): Promise<{ text: string; languages: string[] }> {\n    const text = memory.text ?? '(No text provided)'\n    const languages = memory.languages ?? getLanguages()\n    return { text, languages }\n  }\n\n  // No exec needed for this trigger node\n\n  async post(\n    memory: Memory<TranslationGlobalStore, TranslationLocalStore>,\n    prepRes: { text: string; languages: string[] },\n    execRes: void, // No exec result\n  ): Promise<void> {\n    const { text, languages } = prepRes\n    memory.translations = new Array(languages.length).fill(null)\n\n    languages.forEach((lang, index) => {\n      this.trigger('default', {\n        text: text,\n        language: lang,\n        index: index,\n      })\n    })\n  }\n}\n\nclass TranslateOneLanguageNode extends Node<TranslationGlobalStore, TranslationLocalStore> {\n  async prep(\n    memory: Memory<TranslationGlobalStore, TranslationLocalStore>,\n  ): Promise<{ text: string; lang: string; index: number }> {\n    const text = memory.text ?? ''\n    const lang = memory.language ?? 'unknown'\n    const index = memory.index ?? -1\n    return { text, language, index }\n  }\n\n  async exec(prepRes: {\n    text: string\n    language: string\n    index: number\n  }): Promise<{ translated: string; index: number; language: string }> {\n    // Assume translateText exists\n    return await translateText(prepRes.text, prepRes.language)\n  }\n\n  async post(\n    memory: Memory<TranslationGlobalStore, TranslationLocalStore>,\n    prepRes: { text: string; language: string; index: number },\n    execRes: { translated: string; index: number; language: string },\n  ): Promise<void> {\n    const { index, language, translated } = execRes\n    if (!memory.translations) memory.translations = []\n    while (memory.translations.length <= index) {\n      memory.translations.push(null)\n    }\n    memory.translations[execRes.index] = execRes\n    this.trigger('default')\n  }\n}\n\nconst triggerNode = new TriggerTranslationsNode()\nconst processorNode = new TranslateOneLanguageNode()\n\ntriggerNode.next(processorNode)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Text Embeddings with OpenAI API in Python\nDESCRIPTION: This Python snippet shows how to obtain a text embedding using OpenAI's API. It requires the 'openai' and 'numpy' libraries (installable via pip), and expects the OPENAI_API_KEY environment variable to be set. The 'get_openai_embedding' function takes a string of text and an optional model name, performs authentication, sends a request to the API, and returns the resulting embedding as a NumPy array of floats or None on failure. The output is a high-dimensional vector representing the semantic features of the input text.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/embedding.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Requires: pip install openai numpy\\nimport os\\nimport numpy as np\\nfrom openai import OpenAI\\n\\ndef get_openai_embedding(text: str, model: str = \"text-embedding-3-small\") -> np.ndarray | None:\\n    \\\"\\\"\\\"Gets embedding from OpenAI API.\\\"\\\"\\\"\\n    api_key = os.environ.get(\"OPENAI_API_KEY\")\\n    if not api_key:\\n        print(\"Error: OPENAI_API_KEY not set.\")\\n        return None\\n    try:\\n        client = OpenAI(api_key=api_key)\\n        response = client.embeddings.create(\\n            model=model,\\n            input=text\\n        )\\n        embedding = response.data[0].embedding\\n        return np.array(embedding, dtype=np.float32)\\n    except Exception as e:\\n        print(f\"Error calling OpenAI embedding API: {e}\")\\n        return None\\n\\n# Example:\\n# text_to_embed = \"Hello world\"\\n# embedding_vector = get_openai_embedding(text_to_embed)\\n# if embedding_vector is not None:\\n#     print(embedding_vector)\\n#     print(f\"Dimension: {len(embedding_vector)}\")\\n\n```\n\n----------------------------------------\n\nTITLE: Setting Up and Running the TypeScript Chat Application (Bash)\nDESCRIPTION: These Bash commands guide the user through setting up the TypeScript chat application. It involves changing to the correct directory, creating the environment file from the example (requiring an API key to be added manually), installing Node.js dependencies using npm, and finally starting the chat interface via the defined npm script.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/typescript-chat/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# from BrainyFlow root directory\ncd cookbook/typescript-chat\n\ncp .env.example .env # add your API key\n\nnpm install\nnpm run chat\n```\n\n----------------------------------------\n\nTITLE: Visualizing Pattern Selection Logic with Mermaid Flowchart - Mermaid\nDESCRIPTION: This Mermaid code snippet constructs a decision tree flowchart to guide users in selecting the appropriate design pattern for their AI application within BrainyFlow. The chart presents a branching logic based on data size, independence of processing, reasoning complexity, and external knowledge requirements. No specific dependencies are needed apart from Mermaid.js-compatible rendering environments, such as Markdown viewers with Mermaid support. Inputs include yes/no functional criteria, while outputs correspond to recommended design patterns.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/index.md#2025-04-22_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\n```mermaid\\nflowchart TD\\n    A[Start] --> B{Need to process large data?}\\n    B -->|Yes| C{Data can be processed independently?}\\n    B -->|No| D{Need to make decisions?}\\n\\n    C -->|Yes| E[Map Reduce]\\n    C -->|No| F[Workflow]\\n\\n    D -->|Yes| G{Complex, multi-step reasoning?}\\n    D -->|No| H[Simple Workflow]\\n\\n    G -->|Yes| I{Need multiple specialized roles?}\\n    G -->|No| J{Need external knowledge?}\\n\\n    I -->|Yes| K[Multi-Agents]\\n    I -->|No| L[Agent]\\n\\n    J -->|Yes| M[RAG]\\n    J -->|No| N[Structured Output]\\n```\n```\n\n----------------------------------------\n\nTITLE: Implementing Flow Class RunTasks Method (Sequential) in TypeScript\nDESCRIPTION: The `runTasks` method executes an array of asynchronous task functions sequentially (one after another). It awaits each task's completion before starting the next and returns an array of the awaited results.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/api.md#2025-04-22_snippet_25\n\nLANGUAGE: typescript\nCODE:\n```\n`async runTasks<T>(tasks: (() => T)[]): Promise<Awaited<T>[]>`\n```\n\n----------------------------------------\n\nTITLE: HTML Documentation Layout\nDESCRIPTION: HTML markup defining documentation styling and structure, including titles, descriptions and examples of Flow usage.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python_demo.ipynb#2025-04-22_snippet_8\n\nLANGUAGE: html\nCODE:\n```\n<!-- Title -->\n<p style=\"font-family: Arial, sans-serif; font-size: 24px; font-weight: bold; color: #333; \">\n  4. Flow\n</p>\n\n<!-- Brief description of Flow -->\n<p style=\"font-family: Arial, sans-serif; font-size: 16px; color: #333; margin: 4px 0;\">\n  <strong>Flow</strong> connects your Nodes to a graph.\n</p>\n\n<!-- Unordered list of key points -->\n<ul style=\"font-family: Arial, sans-serif; font-size: 16px; color: #333; list-style-type: disc; margin: 10px 0; padding-left: 20px;\">\n  <li style=\"margin-bottom: 8px;\">\n    <strong>Chaining</strong> \n    (<code style=\"background: #f2f2f2; padding: 2px 4px; border-radius: 3px;\">node_1 &gt;&gt; node_2</code>): Break down complex problems into simple chained steps.\n  </li>\n  <li style=\"margin-bottom: 8px;\">\n    <strong>Directed Branching</strong> \n    (<code style=\"background: #f2f2f2; padding: 2px 4px; border-radius: 3px;\">node_1 - \"action\" -&gt;&gt; node_2</code>): \n    Agentic decisions—where a Node's \n    <code style=\"background: #f2f2f2; padding: 2px 4px; border-radius: 3px;\">post()</code> return the action string.\n  </li>\n  <li style=\"margin-bottom: 8px;\">\n    <strong>Set a Start Point</strong>: Create flow by specifying \n    <code style=\"background: #f2f2f2; padding: 2px 4px; border-radius: 3px;\">Flow(start=node_a)</code>. \n    Then call \n    <code style=\"background: #f2f2f2; padding: 2px 4px; border-radius: 3px;\">flow.run(shared)</code>.\n  </li>\n</ul>\n\n<!-- Closing note -->\n<p style=\"font-family: Arial, sans-serif; font-size: 16px; color: #333; margin: 0; padding: 0;\">\n  That's it! You can nest Flows, branch your actions, or keep it simple with a straight chain of Nodes.\n</p>\n```\n\n----------------------------------------\n\nTITLE: Defining Offline Processing Flow using Nodes in TypeScript\nDESCRIPTION: Instantiates node classes (`TriggerChunkingNode`, `ChunkFileNode`, `TriggerEmbeddingNode`, `EmbedChunkNode`, `StoreIndexNode`) for an offline processing pipeline. It defines the transitions between nodes by attaching event listeners using the `.on()` method (`triggerChunking.on('event_name', targetNode)`). It then creates a `ParallelFlow` instance named `OfflineFlow`, initializing it with the `triggerChunking` node as the starting point, allowing for parallel execution of branches.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/rag.md#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\n// --- Offline Flow Definition ---\nconst triggerChunking = new TriggerChunkingNode()\nconst chunkFile = new ChunkFileNode()\nconst triggerEmbedding = new TriggerEmbeddingNode()\nconst embedChunk = new EmbedChunkNode()\nconst storeIndex = new StoreIndexNode()\n\n// Define transitions\ntriggerChunking.on('chunk_file', chunkFile)\ntriggerChunking.on('embed_chunks', triggerEmbedding)\ntriggerEmbedding.on('embed_chunk', embedChunk)\ntriggerEmbedding.on('store_index', storeIndex)\n\n// Use ParallelFlow for chunking and embedding if desired\nconst OfflineFlow = new ParallelFlow(triggerChunking)\n// Or sequential: const OfflineFlow = new Flow(triggerChunking);\n```\n```\n\n----------------------------------------\n\nTITLE: Getting Text Embeddings with Cohere API in TypeScript\nDESCRIPTION: TypeScript implementation for retrieving text embeddings from Cohere's API. Requires Cohere API key and handles integration with Cohere's client SDK. Includes model and input type configuration with error handling.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/embedding.md#2025-04-22_snippet_14\n\nLANGUAGE: typescript\nCODE:\n```\n// Requires: npm install cohere-ai\nimport { CohereClient } from 'cohere-ai'\n\nasync function getCohereEmbedding(\n  text: string,\n  model: string = 'embed-english-v3.0',\n): Promise<number[] | null> {\n  /** Gets embedding from Cohere API. */\n  const apiKey = process.env.COHERE_API_KEY\n  if (!apiKey) {\n    console.error('Error: COHERE_API_KEY not set.')\n    return null\n  }\n  try {\n    const cohere = new CohereClient({ token: apiKey })\n    const response = await cohere.embed({\n      texts: [text],\n      model: model,\n      inputType: 'search_document', // Adjust as needed\n    })\n    // Cohere TS SDK might return Float64Array, ensure conversion if needed elsewhere\n    return response.embeddings?.[0] ? Array.from(response.embeddings[0]) : null\n  } catch (error) {\n    console.error('Error calling Cohere embedding API:', error)\n    return null\n  }\n}\n\n// Example:\n// const textToEmbed = \"Hello world\";\n// getCohereEmbedding(textToEmbed).then(embedding => {\n//   if (embedding) {\n//     console.log(embedding);\n//   }\n// });\n```\n\n----------------------------------------\n\nTITLE: Configuring Translation Nodes with BrainyFlow in TypeScript\nDESCRIPTION: This TypeScript snippet configures BrainyFlow nodes for translation tasks. It introduces classes for trigger and processor nodes, defining methods using async/await for preparation, execution, and post-processing. Dependencies include the BrainyFlow framework, translation function, and structured memory for global and local data.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/migrating_from_pocketflow.md#2025-04-22_snippet_4\n\nLANGUAGE: TypeScript\nCODE:\n```\nimport { Memory, Node } from 'brainyflow'\n\ninterface TranslationGlobalStore {\n  text?: string\n  languages?: string[]\n  translations?: ({ language: string; translation: string } | null)[]\n}\ninterface TranslationLocalStore {\n  text?: string\n  language?: string\n  index?: number\n}\ntype TranslationActions = 'translate_one' | 'aggregate_results'\n\nclass TriggerTranslationsNode extends Node<\n  TranslationGlobalStore,\n  TranslationLocalStore,\n  TranslationActions[]\n> {\n  async prep(\n    memory: Memory<TranslationGlobalStore, TranslationLocalStore>,\n  ): Promise<{ text: string; languages: string[] }> {\n    const text = memory.text ?? '(No text provided)'\n    const languages = memory.languages ?? getLanguages()\n    return { text, languages }\n  }\n\n  // No exec needed for this trigger node\n\n  async post(\n    memory: Memory<TranslationGlobalStore, TranslationLocalStore>,\n    prepRes: { text: string; languages: string[] },\n    execRes: void, // No exec result\n  ): Promise<void> {\n    const { text, languages } = prepRes\n    // Initialize results array in global memory\n    memory.translations = new Array(languages.length).fill(null)\n\n    languages.forEach((lang, index) => {\n      this.trigger('default', {\n        text: text,\n        language: lang,\n        index: index,\n      })\n    })\n  }\n}\n\nclass TranslateOneLanguageNode extends Node<TranslationGlobalStore, TranslationLocalStore> {\n  async prep(\n    memory: Memory<TranslationGlobalStore, TranslationLocalStore>,\n  ): Promise<{ text: string; lang: string; index: number }> {\n    const text = memory.text ?? ''\n    const lang = memory.language ?? 'unknown'\n    const index = memory.index ?? -1\n    return { text, lang, index }\n  }\n\n  async exec(prepRes: {\n    text: string\n    lang: string\n    index: number\n  }): Promise<{ translated: string; index: number; lang: string }> {\n    return await translateText(prepRes.text, prepRes.lang)\n  }\n\n  async post(\n    memory: Memory<TranslationGlobalStore, TranslationLocalStore>,\n    prepRes: { text: string; lang: string; index: number },\n    execRes: { translated: string; index: number; lang: string },\n  ): Promise<void> {\n    const { index, lang, translated } = execRes\n    if (!memory.translations) memory.translations = []\n    while (memory.translations.length <= index) {\n      memory.translations.push(null)\n    }\n    memory.translations[execRes.index] = execRes\n    this.trigger('default')\n  }\n}\n\nconst triggerNode = new TriggerTranslationsNode()\nconst processorNode = new TranslateOneLanguageNode()\n\ntriggerNode.next(processorNode)\n```\n\n----------------------------------------\n\nTITLE: Running Asynchronous Tasks Sequentially in Flow in BrainyFlow Python\nDESCRIPTION: An asynchronous helper method within `Flow` that executes a list of awaitable `tasks` (functions or coroutines) one after another in sequence.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nasync run_tasks(self, tasks)\n```\n\n----------------------------------------\n\nTITLE: Conditional Branching with RouterNode in Python\nDESCRIPTION: Defines a `RouterNode` in Python, used for language-based branching in a flow. The node detects the language from the content stored in memory and routes to the appropriate processor node for English, Spanish, or unknown content, demonstrating conditional execution paths.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/core_abstraction/node.md#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nfrom brainyflow import Node, Flow, Memory\n\nclass RouterNode(Node):\n    async def prep(self, memory: Memory):\n        return memory.content\n\n    async def exec(self, content: str):\n        return await detect_language(content)\n\n    async def post(self, memory: Memory, content: str, language: str):\n        print(f\"RouterPost: Detected language '{language}', storing and triggering.\")\n        memory.language = language\n        self.trigger(language)\n\nrouter = RouterNode()\nenglish_processor = EnglishProcessorNode()\nspanish_processor = SpanishProcessorNode()\nunknown_processor = UnknownProcessorNode()\nrouter - \"english\" >> english_processor\nrouter - \"spanish\" >> spanish_processor\nrouter - \"unknown\" >> unknown_processor\nflow = Flow(start=router)\nasync def run_flow():\n    memory_en = {\"content\": \"Hello world\"}\n    await flow.run(memory_en)\n    print(\"--- English Flow Done ---\", memory_en)\n\n    memory_es = {\"content\": \"Hola mundo\"}\n    await flow.run(memory_es)\n    print(\"--- Spanish Flow Done ---\", memory_es)\n\n    memory_unk = {\"content\": \"Bonjour le monde\"}\n    await flow.run(memory_unk)\n    print(\"--- Unknown Flow Done ---\", memory_unk)\n\nasyncio.run(run_flow())\n```\n\n----------------------------------------\n\nTITLE: Getting Text Embeddings with Google Vertex AI in Python\nDESCRIPTION: Retrieves text embeddings using Google Vertex AI's embedding models. Requires Google Cloud project setup and authentication. Supports configuration of project ID, location, and model name.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/embedding.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Requires: pip install google-cloud-aiplatform numpy\nfrom google.cloud import aiplatform\nimport os\nimport numpy as np\n\ndef get_vertex_embedding(text: str, project_id: str | None = None, location: str = \"us-central1\", model_name: str = \"textembedding-gecko@001\") -> np.ndarray | None:\n    \"\"\"Gets embedding from Google Vertex AI.\"\"\"\n    project_id = project_id or os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n    if not project_id:\n        print(\"Error: GOOGLE_CLOUD_PROJECT not set and project_id not provided.\")\n        return None\n\n    # Initialize Vertex AI SDK\n    aiplatform.init(project=project_id, location=location)\n\n    # Create the endpoint to the embedding model\n    endpoint = aiplatform.VertexEndpoint(\n        endpoint_name=f\"projects/{project_id}/locations/{location}/publishers/google/models/{model_name}\"\n    )\n\n    try:\n        # The instance structure might vary based on the model\n        instance = {\"content\": text}\n        response = endpoint.predict(instances=[instance])\n        # Extract embedding from response (structure varies by model)\n        embedding = response.predictions[0].get(\"embeddings\", {}).get(\"values\", [])\n        if not embedding:\n            print(\"Error: Could not extract embedding from Vertex AI response.\")\n            return None\n        return np.array(embedding, dtype=np.float32)\n    except Exception as e:\n        print(f\"Error calling Vertex AI embedding API: {e}\")\n        return None\n\n# Example:\n# text_to_embed = \"Hello world\"\n# embedding_vector = get_vertex_embedding(text_to_embed)\n# if embedding_vector is not None:\n#     print(embedding_vector)\n```\n\n----------------------------------------\n\nTITLE: Synthesizing Speech with Google Cloud TTS in TypeScript\nDESCRIPTION: This TypeScript async function `synthesizeGoogleTts` uses the `@google-cloud/text-to-speech` library to synthesize text using the Google Cloud Text-to-Speech API. It requires the `GOOGLE_APPLICATION_CREDENTIALS` environment variable to be set for authentication. The function takes the input text and an optional output filename (default 'gcloud_tts_output.mp3'), then writes the resulting MP3 audio content to the specified file.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/text_to_speech.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n// Requires: npm install @google-cloud/text-to-speech\nimport * as fs from 'fs'\nimport { promisify } from 'util'\nimport textToSpeech from '@google-cloud/text-to-speech'\n\nconst writeFileAsync = promisify(fs.writeFile)\n\nasync function synthesizeGoogleTts(\n  text: string,\n  outputFilename: string = 'gcloud_tts_output.mp3',\n): Promise<void> {\n  /** Synthesizes speech using Google Cloud TTS. */\n  // Assumes GOOGLE_APPLICATION_CREDENTIALS env var is set\n  try {\n    const client = new textToSpeech.TextToSpeechClient()\n    const request = {\n      input: { text: text },\n      // Example voice, check documentation for more options\n      voice: { languageCode: 'en-US', ssmlGender: 'NEUTRAL' as const }, // Use 'as const' for enum-like strings\n      audioConfig: { audioEncoding: 'MP3' as const },\n    }\n\n    const [response] = await client.synthesizeSpeech(request)\n\n    if (response.audioContent) {\n      await writeFileAsync(outputFilename, response.audioContent, 'binary')\n      console.log(`Audio saved to ${outputFilename}`)\n    } else {\n      console.error('Error: No audio content received from Google Cloud TTS.')\n    }\n  } catch (error) {\n    console.error('Error calling Google Cloud TTS:', error)\n  }\n}\n\n// Example:\n// synthesizeGoogleTts(\"Hello from Google Cloud Text-to-Speech!\");\n\n```\n\n----------------------------------------\n\nTITLE: Accessing Memory Properties in BrainyFlow Python\nDESCRIPTION: Provides attribute-style access to properties stored in the `Memory` instance. It first checks the local store, and if the property is not found, it checks the global store. This allows overriding global values with local ones.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n__getattr__(self, name)\n```\n\n----------------------------------------\n\nTITLE: Executing Offline Indexing Flow in TypeScript\nDESCRIPTION: Provides an example asynchronous function `runOffline` to execute the defined `OfflineFlow`. It sets up an `initialMemory` object with file paths, creates dummy files using `fs.writeFileSync` from Node.js's file system module, executes the `OfflineFlow` with the memory, logs status messages, cleans up the dummy files using `fs.unlinkSync`, and returns the `initialMemory` object containing the results. A commented-out example call `runOffline()` is included.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/rag.md#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\n// --- Offline Flow Execution ---\nasync function runOffline() {\n  const initialMemory = {\n    files: ['doc1.txt', 'doc2.txt'], // Example file paths\n  }\n  console.log('Starting offline indexing flow...')\n  // Create dummy files for example\n  fs.writeFileSync('doc1.txt', 'Alice was beginning to get very tired.')\n  fs.writeFileSync('doc2.txt', 'The quick brown fox jumps over the lazy dog.')\n\n  await OfflineFlow.run(initialMemory)\n  console.log('Offline indexing complete.')\n  // Clean up dummy files\n  fs.unlinkSync('doc1.txt')\n  fs.unlinkSync('doc2.txt')\n  return initialMemory // Return memory containing index, chunks, embeds\n}\n// runOffline(); // Example call\n```\n```\n\n----------------------------------------\n\nTITLE: Using Right Shift Operator for Default Node Linking in BrainyFlow Python\nDESCRIPTION: Provides syntactic sugar using the right shift operator (`>>`) as an alternative to `next(other)`. Allows linking nodes like `node1 >> node2`.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n__rshift__(self, other)\n```\n\n----------------------------------------\n\nTITLE: Mocking LLM Responses Using Python Async\nDESCRIPTION: This snippet provides a mock implementation of an LLM logic function and demonstrates how to replace it in a test environment using Python's unittest.mock library. It showcases how to use AsyncMock to simulate asynchronous calls and wrap assertions around the mock interaction.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/testing.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Mock LLM with canned responses based on prompt content\nasync def mock_llm_logic(prompt: str) -> str:\n    if \"summarize\" in prompt.lower():\n        return \"This is a summary.\"\n    elif \"extract\" in prompt.lower():\n        # Simulate returning a JSON-like string\n        return '{\"key\": \"value\"}'\n    else:\n        return \"Default response\"\n\n# Example usage in a test\nasync def test_node_with_mocked_llm():\n    # Assume MyLlmNode calls utils.call_llm internally\n    # node = MyLlmNode()\n    # memory = Memory.create({\"input\": \"some text to summarize\"})\n\n    # Use patch to replace the actual call_llm\n    with patch('utils.call_llm', new=AsyncMock(side_effect=mock_llm_logic)) as mock_call:\n        # await node.run(memory) # Run the node that uses the LLM\n        pass # Replace pass with actual node execution\n\n    # Assertions can be made here on memory state or mock calls\n    # mock_call.assert_called_once()\n    # assert memory.summary == \"This is a summary.\"\n\n# asyncio.run(test_node_with_mocked_llm())\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with pip\nDESCRIPTION: Command to install the required dependencies for the BrainyFlow batch processing example using pip and the requirements.txt file.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-batch-flow/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Implementing OpenAI API Utility Functions in Python\nDESCRIPTION: Defines two Python functions: `call_llm` interacts with the OpenAI GPT-4o model to get chat completions based on a prompt, and `get_embedding` generates text embeddings using the OpenAI text-embedding-ada-002 model. Both functions require the `openai` library and an `API_KEY` (presumably defined elsewhere) for authentication.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python_demo.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\nimport os\n\ndef call_llm(prompt):\n    client = OpenAI(api_key=API_KEY)\n    response = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return response.choices[0].message.content\n\ndef get_embedding(text):\n    client = OpenAI(api_key=API_KEY)\n    response = client.embeddings.create(\n        model=\"text-embedding-ada-002\",\n        input=text\n    )\n    return response.data[0].embedding\n```\n\n----------------------------------------\n\nTITLE: Getting Text Embeddings with Jina AI API - TypeScript\nDESCRIPTION: Asynchronous function to get text embeddings using Jina AI API. Handles authentication and HTTP requests with proper error handling. Returns embedding vector as number array or null on failure.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/embedding.md#2025-04-22_snippet_18\n\nLANGUAGE: typescript\nCODE:\n```\nasync function getJinaEmbedding(\n  text: string,\n  model: string = 'jina-embeddings-v2-base-en',\n): Promise<number[] | null> {\n  /** Gets embedding from Jina AI API. */\n  const jinaToken = process.env.JINA_API_KEY // Or JINA_TOKEN\n  if (!jinaToken) {\n    console.error('Error: JINA_API_KEY not set.')\n    return null\n  }\n\n  const url = 'https://api.jina.ai/v1/embeddings' // Use v1 endpoint\n  const headers: HeadersInit = {\n    Authorization: `Bearer ${jinaToken}`,\n    'Accept-Encoding': 'identity',\n    'Content-Type': 'application/json',\n  }\n  const payload = JSON.stringify({\n    input: [text], // API expects a list\n    model: model,\n  })\n\n  try {\n    const response = await fetch(url, {\n      method: 'POST',\n      headers: headers,\n      body: payload,\n    })\n\n    if (!response.ok) {\n      throw new Error(`HTTP error! status: ${response.status}, message: ${await response.text()}`)\n    }\n\n    const result = await response.json()\n    const embedding = result?.data?.[0]?.embedding\n\n    if (!embedding || !Array.isArray(embedding)) {\n      console.error('Error parsing Jina AI response:', result)\n      return null\n    }\n    return embedding\n  } catch (error) {\n    console.error('Error calling Jina AI embedding API:', error)\n    return null\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Word Guessing Game Implementation with Multi-Agent System in TypeScript\nDESCRIPTION: TypeScript version of the word guessing game implementing Hinter and Guesser agents. Uses custom queue implementation for inter-agent communication and memory management.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/multi_agent.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory, Node } from 'brainyflow'\nimport { callLLM } from '../utils/callLLM'\n\nclass Hinter extends Node {\n  async prep(memory: Memory): Promise<any> {\n    if (memory.guesserQueue) {\n      try {\n        const guess = await memory.guesserQueue.get()\n        if (guess === 'GAME_OVER') {\n          return null\n        }\n        return { guess }\n      } catch (e) {\n      }\n    }\n    return {}\n  }\n\n  async exec(prepRes: any): Promise<string | null> {\n    if (prepRes === null) return null\n\n    const prompt = `\nGiven target word: ${memory.targetWord}\nForbidden words: ${memory.forbiddenWords}\nLast wrong guess: ${prepRes.guess || ''}\nGenerate a creative hint that helps guess the target word without using forbidden words.\nReply only with the hint text.\n`\n    return await callLLM(prompt)\n  }\n\n  async post(memory: Memory, data: any, hint: string | null): Promise<void> {\n    if (hint === null) {\n      this.trigger('end')\n      return\n    }\n    await memory.hinterQueue.put(hint)\n    this.trigger('continue')\n  }\n}\n\nclass Guesser extends Node {\n  async prep(memory: Memory): Promise<any> {\n    const hint = await memory.guesserQueue.get()\n    return {\n      hint,\n      pastGuesses: memory.pastGuesses || [],\n    }\n  }\n\n  async exec(prepRes: any): Promise<string> {\n    const prompt = `\nGiven hint: ${prepRes.hint}\nPast wrong guesses: ${prepRes.pastGuesses}\nMake a new guess for the target word.\nReply only with the guessed word.\n`\n    return await callLLM(prompt)\n  }\n\n  async post(memory: Memory, prepRes: any, guess: string): Promise<void> {\n    if (guess.toLowerCase() === memory.targetWord.toLowerCase()) {\n      console.log('Game Over - Correct guess!')\n      await memory.hinterQueue.put('GAME_OVER')\n      this.trigger('end')\n      return\n    }\n\n    memory.pastGuesses = [...(memory.pastGuesses || []), guess]\n    await memory.hinterQueue.put(guess)\n    this.trigger('continue')\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Executing RAG Flow in Python\nDESCRIPTION: Python implementation showing how to execute the online RAG flow with a user question, process it through the pipeline, and retrieve the final answer from memory. It also includes a main function that combines offline and online stages.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/rag.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# --- Online Flow Execution ---\nasync def run_online(memory_from_offline: dict):\n    # Add the user's question to the memory from the offline stage\n    memory_from_offline[\"question\"] = \"Why do people like cats?\"\n\n    print(f\"\\nStarting online RAG flow for question: \\\"{memory_from_offline['question']}\\\"\")\n    await OnlineFlow.run(memory_from_offline) # Pass memory object\n    # final answer in memory_from_offline[\"answer\"]\n    print(\"Final Answer:\", memory_from_offline.get(\"answer\", \"N/A\")) # Read from memory\n    return memory_from_offline\n\n# Example usage combining both stages\nasync def main():\n    # Mock external functions if not defined\n    # global get_embedding, create_index, search_index, call_llm\n    # get_embedding = ...\n    # create_index = ...\n    # search_index = ...\n    # call_llm = ...\n\n    memory_after_offline = await run_offline()\n    if memory_after_offline.get(\"index\"): # Only run online if index exists\n        await run_online(memory_after_offline)\n    else:\n        print(\"Skipping online flow due to missing index.\")\n\nif __name__ == \"__main__\":\n    # Note: Ensure dummy files exist or are created before running\n    # For simplicity, file creation moved to run_offline\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Execution Flow in BrainyFlow\nDESCRIPTION: Extends the base Flow class to implement custom execution logic by overriding the runTasks method. This example adds a delay between sequential task executions.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/core_abstraction/flow.md#2025-04-22_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Flow, Memory } from 'brainyflow'\n\ndeclare function sleep(ms: number): Promise<void> // Assuming sleep is available\n\nclass CustomExecutionFlow extends Flow {\n  async runTasks<T>(tasks: (() => Promise<T>)[]): Promise<Awaited<T>[]> {\n    // Example: Run tasks sequentially with a delay between each\n    const results: Awaited<T>[] = []\n    for (const task of tasks) {\n      results.push(await task())\n      console.log('Custom runTasks: Task finished, waiting 1s...')\n      await sleep(1000) // Wait 1 second between tasks\n    }\n    return results\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Brave Search API in Python and TypeScript\nDESCRIPTION: Illustrates how to query the Brave Search API using Python ('requests') and TypeScript ('fetch'). Requires the BRAVE_API_TOKEN environment variable for authentication using the 'X-Subscription-Token' header. Constructs the request URL, sets headers (including the token and 'Accept: application/json'), adds the query parameter, sends the GET request, and prints the JSON response or an error message.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/websearch.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n```python\nimport requests\nimport os\n\nSUBSCRIPTION_TOKEN = os.environ.get(\"BRAVE_API_TOKEN\") # Use environment variables\nquery = \"example\"\n\nurl = \"https://api.search.brave.com/res/v1/web/search\"\nheaders = {\n    \"X-Subscription-Token\": SUBSCRIPTION_TOKEN,\n    \"Accept\": \"application/json\" # Good practice\n}\nparams = {\n    \"q\": query\n}\n\nif not SUBSCRIPTION_TOKEN:\n    print(\"Error: Please set BRAVE_API_TOKEN environment variable.\")\nelse:\n    try:\n        response = requests.get(url, headers=headers, params=params)\n        response.raise_for_status()\n        results = response.json()\n        print(results)\n    except requests.exceptions.RequestException as e:\n        print(f\"Error fetching Brave search results: {e}\")\n\n```\n```\n\nLANGUAGE: typescript\nCODE:\n```\n```typescript\nasync function searchBrave(query: string): Promise<any> {\n  const subscriptionToken = process.env.BRAVE_API_TOKEN // Use environment variables\n\n  if (!subscriptionToken) {\n    console.error('Error: Please set BRAVE_API_TOKEN environment variable.')\n    return null\n  }\n\n  const url = new URL('https://api.search.brave.com/res/v1/web/search')\n  url.searchParams.append('q', query)\n\n  const headers = {\n    'X-Subscription-Token': subscriptionToken,\n    Accept: 'application/json', // Good practice\n  }\n\n  try {\n    const response = await fetch(url.toString(), { headers })\n    if (!response.ok) {\n      throw new Error(`HTTP error! status: ${response.status}`)\n    }\n    const results = await response.json()\n    console.log(results)\n    return results\n  } catch (error) {\n    console.error('Error fetching Brave search results:', error)\n    return null\n  }\n}\n\n// Example usage:\n// searchBrave(\"example\");\n```\n```\n\n----------------------------------------\n\nTITLE: Sample Output of BrainyFlow Batch Processing\nDESCRIPTION: Demonstrates the expected console output when running the BrainyFlow batch processing example, showing the progress and results of image processing.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-batch-flow/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nTrigger: Triggering processing for 9 image-filter combinations.\nProcessing images with filters...\nSaved filtered image to: output/cat_grayscale.jpg\nSaved filtered image to: output/dog_blur.jpg\nSaved filtered image to: output/bird_sepia.jpg\n...\nAll image processing complete!\nCheck the 'output' directory for results.\n```\n\n----------------------------------------\n\nTITLE: Defining ActionLinker Inner Class in BrainyFlow Python\nDESCRIPTION: An inner helper class within `BaseNode` used to enable action-specific linking syntax like `node - \"action\" >> other_node`.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nActionLinker (inner class)\n```\n\nLANGUAGE: python\nCODE:\n```\n  __init__(self, node, action)\n```\n\nLANGUAGE: python\nCODE:\n```\n  __rshift__(self, other)\n```\n\n----------------------------------------\n\nTITLE: Flow Initialization and Configuration in BrainyFlow (Python)\nDESCRIPTION: These Python snippets provide side-by-side examples of flow instantiation in BrainyFlow. Upgrading to v1.0 allows for options to be passed directly to the Flow constructor, with 'max_visits' as a possible flow runtime constraint. This update affects the flow configuration process only, requiring existing code to adapt to the new constructor signature. Required dependencies are BrainyFlow's Flow class and valid node instances.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/migration.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Before (v0.2)\nflow = Flow(start=start_node)\n\n# After (v1.0)\n# With default options\nflow = Flow(start=start_node)\n\n# With custom options\nflow = Flow(start=start_node, options={\"max_visits\": 10})\n```\n\n----------------------------------------\n\nTITLE: Running the Python Main Entry Point in Bash\nDESCRIPTION: This command executes the 'main.py' script as the project entry point. It assumes all dependencies are already installed and the environment is correctly configured. Input: none; Output: script executes, running the sample flow as described in the documentation. Limitation: 'main.py' must exist in the current working directory.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-tool-embeddings/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Testing Retry Logic in TypeScript\nDESCRIPTION: This TypeScript snippet tests the retry mechanism for a node execution using Vitest. It uses mock implementations which simulate function calls that fail twice before succeeding and employs fake timers to skip actual time delays during the test.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/testing.md#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nimport { beforeEach, describe, expect, it, vi } from 'vitest'\n\n// import { Node, Memory } from 'brainyflow'; // Assuming imports\n// import { someExternalCall } from './utils/externalCall'; // The function called by exec\n\n// Mock the external call module\nvi.mock('./utils/externalCall', () => ({\n  someExternalCall: vi.fn(),\n}))\n\n// Mock setTimeout used for 'wait' (if applicable)\nvi.useFakeTimers()\n\n// Example Node (conceptual)\n// class NodeWithRetry extends Node<any, any, [], any, string> {\n//   constructor() {\n//     super({ maxRetries: 3, wait: 100 }); // Retry up to 3 times, wait 100ms\n//   }\n//   async exec(prepRes: any): Promise<string> {\n//     // This method calls the function we will mock\n//     return await someExternalCall(prepRes);\n//   }\n// }\n\ndescribe('Retry Logic Testing', () => {\n  let callCountRetry = 0\n\n  beforeEach(() => {\n    callCountRetry = 0 // Reset counter\n    vi.clearAllMocks() // Clear mock history\n    vi.clearAllTimers() // Clear pending timers\n  })\n\n  it('should retry exec on failure and succeed eventually', async () => {\n    // Configure the mock to fail twice, then succeed\n    vi.mocked(someExternalCall).mockImplementation(async () => {\n      callCountRetry++\n      console.log(`Mock called (Attempt ${callCountRetry})`) // For debugging test\n      if (callCountRetry <= 2) {\n        throw new Error('Temporary network failure')\n      }\n      return 'Success on third try'\n    })\n\n    // const node = new NodeWithRetry();\n    // const memory = {}; // Initial memory\n\n    // await node.run(memory); // Run the node\n\n    // Advance timers to simulate waiting (if wait > 0)\n    // vi.advanceTimersByTime(100); // Advance by wait time\n    // await Promise.resolve(); // Allow promises to settle after timer advance\n    // vi.advanceTimersByTime(100); // Advance for second wait\n    // await Promise.resolve();\n\n    // Assertions\n    // expect(callCountRetry).toBe(3); // Called 3 times\n    // expect(memory.result).toBe('Success on third try'); // Check final result\n    // expect(vi.getTimerCount()).toBe(0); // Ensure all timers were cleared/run\n  })\n})\n```\n\n----------------------------------------\n\nTITLE: Querying DuckDuckGo Instant Answer API in Python and TypeScript\nDESCRIPTION: Provides examples for fetching results from the DuckDuckGo Instant Answer API using Python ('requests') and TypeScript ('fetch'). This API does not require an API key. Sends a GET request with the query, format ('json'), and 'no_html=1' parameters to retrieve structured data without HTML tags, then prints the JSON response or an error.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/websearch.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n```python\nimport requests\n\nquery = \"example\"\nurl = \"https://api.duckduckgo.com/\"\nparams = {\n    \"q\": query,\n    \"format\": \"json\",\n    \"no_html\": 1 # Often useful to remove HTML tags\n}\n\ntry:\n    response = requests.get(url, params=params)\n    response.raise_for_status()\n    results = response.json()\n    print(results)\nexcept requests.exceptions.RequestException as e:\n    print(f\"Error fetching DuckDuckGo results: {e}\")\n\n```\n```\n\nLANGUAGE: typescript\nCODE:\n```\n```typescript\nasync function searchDuckDuckGo(query: string): Promise<any> {\n  const url = new URL('https://api.duckduckgo.com/')\n  url.searchParams.append('q', query)\n  url.searchParams.append('format', 'json')\n  url.searchParams.append('no_html', '1') // Often useful\n\n  try {\n    const response = await fetch(url.toString())\n    if (!response.ok) {\n      throw new Error(`HTTP error! status: ${response.status}`)\n    }\n    const results = await response.json()\n    console.log(results)\n    return results\n  } catch (error) {\n    console.error('Error fetching DuckDuckGo results:', error)\n    return null\n  }\n}\n\n// Example usage:\n// searchDuckDuckGo(\"example\");\n```\n```\n\n----------------------------------------\n\nTITLE: Synthesizing Speech with AWS Polly in Python\nDESCRIPTION: This Python function `synthesize_polly` uses the AWS Polly service to convert text into speech and saves it as an MP3 file. It requires the `boto3` library and assumes AWS credentials are configured via environment variables or `~/.aws/credentials`. The function takes the text to synthesize, an optional output filename (defaulting to 'polly_output.mp3'), and an optional AWS region.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/text_to_speech.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Requires: pip install boto3\nimport boto3\nimport os\n\ndef synthesize_polly(text: str, output_filename: str = \"polly_output.mp3\", region: str | None = None):\n    \"\"\"Synthesizes speech using AWS Polly.\"\"\"\n    # Assumes AWS credentials are configured (e.g., via env vars, ~/.aws/credentials)\n    aws_region = region or os.environ.get(\"AWS_REGION\", \"us-east-1\")\n    try:\n        polly = boto3.client(\"polly\", region_name=aws_region)\n        response = polly.synthesize_speech(\n            Text=text,\n            OutputFormat=\"mp3\",\n            VoiceId=\"Joanna\" # Example voice\n        )\n\n        # Check if AudioStream is present\n        if \"AudioStream\" in response:\n            with open(output_filename, \"wb\") as f:\n                f.write(response[\"AudioStream\"].read())\n            print(f\"Audio saved to {output_filename}\")\n        else:\n            print(\"Error: Could not stream audio from Polly.\")\n\n    except Exception as e:\n        print(f\"Error calling AWS Polly: {e}\")\n\n# Example:\n# synthesize_polly(\"Hello from AWS Polly!\")\n\n```\n\n----------------------------------------\n\nTITLE: Installing and Running Application - Bash Commands\nDESCRIPTION: Commands to install required dependencies and start the chat application.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-chat-memory/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Implementing Automatic Retry Logic in Nodes - RetryNode/Node - TypeScript\nDESCRIPTION: This design describes the RetryNode class (aliased as Node), which adds automatic retry logic to the core node execution. On failing exec attempts, the node retries up to a configured maximum, waiting between tries as configured. Subclass execFallback can be used for custom error handling when all retries fail. Dependencies are async/await, timer-based delay (e.g., setTimeout or equivalent), and error propagation. Inputs are prepRes and Memory; outputs are either the successful exec result or final error after retries. Limitations include synchronous wait implementation and non-interruptible retry loops.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/design.md#2025-04-22_snippet_2\n\nLANGUAGE: TypeScript\nCODE:\n```\nclass RetryNode extends BaseNode {\n  // maxRetries: number of attempts (default 1)\n  // wait: wait period in seconds between attempts (default 0)\n  // execFallback(prepRes, error): called on failure after all retries\n  // execRunner(memory, prepRes): retry loop using try-catch, waits, then re-attempts exec\n  // Exposed as 'Node' to users\n}\n\n```\n\n----------------------------------------\n\nTITLE: Storing Vector Index Node in TypeScript\nDESCRIPTION: Defines the `StoreIndexNode` class in TypeScript, extending `Node`. The `prep` method asynchronously retrieves all embeddings (expected as `number[][]`) from the shared `memory` object, providing an empty array as a default. The `exec` method receives the embeddings, logs the count, and creates a vector index using an external `createIndex` function. The `post` method stores the created index back into the `memory` object and logs a confirmation.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/rag.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n// 1e. Node to store the final index\nclass StoreIndexNode extends Node {\n  async prep(memory: Memory): Promise<number[][]> {\n    // Read all embeddings from global memory\n    return memory.all_embeds ?? []\n  }\n\n  async exec(allEmbeds: number[][]): Promise<any> {\n    console.log(`Storing index for ${allEmbeds.length} embeddings.`)\n    // Create a vector index (implementation depends on library)\n    const index = createIndex(allEmbeds)\n    return index\n  }\n\n  async post(memory: Memory, prepRes: any, index: any): Promise<void> {\n    // Store the created index in global memory\n    memory.index = index\n    console.log('Index created and stored.')\n  }\n}\n```\n```\n\n----------------------------------------\n\nTITLE: Implementing Utility Functions for LLM Calls and Web Searches\nDESCRIPTION: Defines utility functions for calling OpenAI's GPT model and performing web searches using DuckDuckGo. Includes test calls to demonstrate functionality.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-agent/demo.ipynb#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# utils.py\nfrom openai import OpenAI\nimport os\nfrom duckduckgo_search import DDGS\n\ndef call_llm(prompt):\n    client = OpenAI(api_key=\"your-api-key\")\n    r = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return r.choices[0].message.content\n\ndef search_web(query):\n    results = DDGS().text(query, max_results=5)\n    # Convert results to a string\n    results_str = \"\\n\\n\".join([f\"Title: {r['title']}\\nURL: {r['href']}\\nSnippet: {r['body']}\" for r in results])\n    return results_str\n\nprint(\"## Testing call_llm\")\nprompt = \"In a few words, what is the meaning of life?\"\nprint(f\"## Prompt: {prompt}\")\nresponse = call_llm(prompt)\nprint(f\"## Response: {response}\")\n\nprint(\"## Testing search_web\")\nquery = \"Who won the Nobel Prize in Physics 2024?\"\nprint(f\"## Query: {query}\")\nresults = search_web(query)\nprint(f\"## Results: {results}\")\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key in Bash\nDESCRIPTION: Command to set the OpenAI API key as an environment variable for authentication.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-structured-output/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=\"your-api-key-here\"\n```\n\n----------------------------------------\n\nTITLE: Setting Memory Properties in BrainyFlow Python\nDESCRIPTION: Provides attribute-style setting for properties in the `Memory` instance. Values are always written directly to the global store.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n__setattr__(self, name, value)\n```\n\n----------------------------------------\n\nTITLE: Implementing Text Summarization Node in Python using BrainyFlow\nDESCRIPTION: Creates a SummarizeNode class that handles text summarization workflow in three steps: reading data from shared store, calling LLM for summarization, and storing results. Includes test setup code for loading and processing sample data.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python_demo.ipynb#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom brainyflow import Node\n\nclass SummarizeNode(Node):\n    async def prep(self, shared):\n        # Read data from shared store\n        return shared[\"data\"][\"before.txt\"]\n        \n    async def exec(self, text):\n        # Call LLM to summarize\n        prompt = f\"Summarize this text in 50 words:\\n\\n{text}\"\n        return call_llm(prompt)\n    \n    async def post(self, shared, prep_res, exec_res):\n        # Store the summary back\n        shared[\"summary\"] = exec_res\n        # No specific next action needed\n        self.trigger(\"default\")\n\n# Create test data\nshared = {\n    \"data\": {},\n    \"summary\": None\n}\n\n# Load the file\nwith open(\"./data/PaulGrahamEssaysLarge/before.txt\", \"r\") as f:\n    shared[\"data\"][\"before.txt\"] = f.read()\n\n# Create and run the node\nsummarize_node = SummarizeNode()\nawait summarize_node.run(shared)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages with pip\nDESCRIPTION: Command to install all the necessary dependencies for the project from a requirements.txt file.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-majority-vote/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Getting Text Embeddings with Google Vertex AI in TypeScript\nDESCRIPTION: TypeScript implementation for retrieving text embeddings from Google Vertex AI. Requires Google Cloud setup and authentication. Includes configuration for project ID, location, and model name along with error handling.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/embedding.md#2025-04-22_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\n// Requires: npm install @google-cloud/aiplatform\nconst { PredictionServiceClient } = require('@google-cloud/aiplatform').v1\nconst { helpers } = require('@google-cloud/aiplatform') // Or use import\n\nasync function getVertexEmbedding(\n  text: string,\n  projectId: string | undefined = process.env.GOOGLE_CLOUD_PROJECT,\n  location: string = 'us-central1',\n  modelName: string = 'textembedding-gecko@001',\n): Promise<number[] | null> {\n  /** Gets embedding from Google Vertex AI. */\n  if (!projectId) {\n    console.error('Error: GOOGLE_CLOUD_PROJECT not set and projectId not provided.')\n    return null\n  }\n\n  const clientOptions = { apiEndpoint: `${location}-aiplatform.googleapis.com` }\n  const client = new PredictionServiceClient(clientOptions)\n\n  const endpoint = `projects/${projectId}/locations/${location}/publishers/google/models/${modelName}`\n  const instance = helpers.toValue({ content: text }) // Convert JSON object to Value proto\n  const instances = [instance]\n  const parameters = helpers.toValue({}) // No parameters needed for this model\n\n  const request = { endpoint, instances, parameters }\n\n  try {\n    const [response] = await client.predict(request)\n    const embeddings =\n      response.predictions?.[0]?.structValue?.fields?.embeddings?.structValue?.fields?.values\n        ?.listValue?.values\n    if (!embeddings) {\n      console.error('Invalid response structure from Vertex AI.')\n      return null\n    }\n    // Convert Value protos back to numbers\n    return embeddings.map((val: any) => val.numberValue)\n  } catch (error) {\n    console.error('Error calling Vertex AI embedding API:', error)\n    return null\n  }\n}\n\n// Example:\n// const textToEmbed = \"Hello world\";\n// getVertexEmbedding(textToEmbed).then(embedding => {\n//   if (embedding) {\n//     console.log(embedding);\n//   }\n// });\n```\n\n----------------------------------------\n\nTITLE: Synthesizing Speech with Google Cloud TTS in Python\nDESCRIPTION: This Python function `synthesize_google_tts` uses the Google Cloud Text-to-Speech API to convert text into speech and save it as an MP3 file. It requires the `google-cloud-texttospeech` library and assumes the `GOOGLE_APPLICATION_CREDENTIALS` environment variable is set for authentication. The function takes the input text and an optional output filename (default 'gcloud_tts_output.mp3').\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/text_to_speech.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Requires: pip install google-cloud-texttospeech\nfrom google.cloud import texttospeech\nimport os\n\ndef synthesize_google_tts(text: str, output_filename: str = \"gcloud_tts_output.mp3\"):\n    \"\"\"Synthesizes speech using Google Cloud TTS.\"\"\"\n    # Assumes GOOGLE_APPLICATION_CREDENTIALS env var is set\n    try:\n        client = texttospeech.TextToSpeechClient()\n        input_text = texttospeech.SynthesisInput(text=text)\n        # Example voice, check documentation for more options\n        voice = texttospeech.VoiceSelectionParams(\n            language_code=\"en-US\",\n            ssml_gender=texttospeech.SsmlVoiceGender.NEUTRAL\n        )\n        audio_config = texttospeech.AudioConfig(\n            audio_encoding=texttospeech.AudioEncoding.MP3\n        )\n\n        response = client.synthesize_speech(\n            input=input_text, voice=voice, audio_config=audio_config\n        )\n\n        with open(output_filename, \"wb\") as f:\n            f.write(response.audio_content)\n        print(f\"Audio saved to {output_filename}\")\n\n    except Exception as e:\n        print(f\"Error calling Google Cloud TTS: {e}\")\n\n# Example:\n# synthesize_google_tts(\"Hello from Google Cloud Text-to-Speech!\")\n\n```\n\n----------------------------------------\n\nTITLE: Synthesizing Speech with Azure TTS in Python\nDESCRIPTION: This Python function `synthesize_azure_tts` uses the Azure Cognitive Services Speech SDK (`azure-cognitiveservices-speech`) to perform text-to-speech synthesis. It requires the `AZURE_SPEECH_KEY` and `AZURE_SPEECH_REGION` environment variables to be set for authentication. The function takes the input text and an optional output filename (default 'azure_tts_output.wav'), then saves the synthesized audio as a WAV file.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/text_to_speech.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Requires: pip install azure-cognitiveservices-speech\nimport azure.cognitiveservices.speech as speechsdk\nimport os\n\ndef synthesize_azure_tts(text: str, output_filename: str = \"azure_tts_output.wav\"):\n    \"\"\"Synthesizes speech using Azure Cognitive Services TTS.\"\"\"\n    speech_key = os.environ.get(\"AZURE_SPEECH_KEY\")\n    service_region = os.environ.get(\"AZURE_SPEECH_REGION\")\n\n    if not speech_key or not service_region:\n        print(\"Error: AZURE_SPEECH_KEY or AZURE_SPEECH_REGION not set.\")\n        return\n\n    try:\n        speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n        # Example voice, check documentation for more\n        # speech_config.speech_synthesis_voice_name='en-US-JennyNeural'\n\n        # Synthesize to an audio file\n        audio_config = speechsdk.audio.AudioOutputConfig(filename=output_filename)\n\n        synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n\n        result = synthesizer.speak_text_async(text).get()\n\n        # Check result\n        if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n            print(f\"Audio saved to {output_filename}\")\n        elif result.reason == speechsdk.ResultReason.Canceled:\n            cancellation_details = result.cancellation_details\n            print(f\"Speech synthesis canceled: {cancellation_details.reason}\")\n            if cancellation_details.reason == speechsdk.CancellationReason.Error:\n                print(f\"Error details: {cancellation_details.error_details}\")\n\n    except Exception as e:\n        print(f\"Error calling Azure TTS: {e}\")\n\n# Example:\n\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key for Research Agent\nDESCRIPTION: Command to export the OpenAI API key as an environment variable for authentication.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-agent/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=\"your-api-key-here\"\n```\n\n----------------------------------------\n\nTITLE: TypeScript Milvus Client Implementation\nDESCRIPTION: TypeScript implementation for Milvus client initialization and collection management using @zilliz/milvus2-sdk-node. Includes connection setup, collection creation, and index management with proper type definitions and async/await patterns.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/vector.md#2025-04-22_snippet_14\n\nLANGUAGE: typescript\nCODE:\n```\nimport { DataType, IndexType, MetricType, MilvusClient } from '@zilliz/milvus2-sdk-node'\n\nfunction initMilvusClient(): MilvusClient | null {\n  /** Initializes Milvus client. */\n  const address = process.env.MILVUS_ADDRESS || 'localhost:19530'\n  const token = process.env.MILVUS_TOKEN\n  const ssl = address.includes('cloud') || address.includes('https')\n\n  try {\n    console.log(`Connecting to Milvus at ${address}...`)\n    const client = new MilvusClient({ address, ssl, token })\n    console.log('Milvus client instance created.')\n    return client\n  } catch (error) {\n    console.error('Error initializing Milvus client:', error)\n    return null\n  }\n}\n\nasync function createMilvusCollectionIfNotExists(\n  client: MilvusClient,\n  collectionName: string,\n  dimension: number,\n): Promise<void> {\n  /** Creates a Milvus collection if it doesn't exist. */\n  try {\n    const hasCollection = await client.hasCollection({ collection_name: collectionName })\n    if (!hasCollection.value) {\n      console.log(`Creating collection '${collectionName}'...`)\n      await client.createCollection({\n        collection_name: collectionName,\n        fields: [\n          { name: 'pk', dtype: DataType.Int64, is_primary_key: true, autoID: true },\n          { name: 'embeddings', dtype: DataType.FloatVector, dim: dimension },\n        ],\n      })\n      console.log(`Collection '${collectionName}' created.`)\n\n      console.log(\"Creating index on 'embeddings' field...\")\n      await client.createIndex({\n        collection_name: collectionName,\n        field_name: 'embeddings',\n        index_type: IndexType.IVF_FLAT,\n        metric_type: MetricType.L2,\n        params: { nlist: 1024 },\n      })\n      console.log('Index created.')\n    } else {\n      console.log(`Collection '${collectionName}' already exists.`)\n    }\n\n    const loadStatus = await client.getLoadState({ collection_name: collectionName })\n    if (loadStatus.state !== 'Loaded') {\n      console.log(`Loading collection '${collectionName}'...`)\n      await client.loadCollectionSync({ collection_name: collectionName })\n      console.log('Collection loaded.')\n    } else {\n      console.log(`Collection '${collectionName}' already loaded.`)\n    }\n  } catch (error) {\n    console.error('Error creating or loading Milvus collection:', error)\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Grouping Sentences into Chunks in TypeScript\nDESCRIPTION: This TypeScript function chunks text by sentences using a sentence tokenizer library, such as 'sentence-tokenizer'. The function requires prior installation of such a library via npm, taking the text and an optional sentence count to produce chunks of grouped sentences.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/chunking.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n// Requires a sentence tokenizer library, e.g., 'sentence-tokenizer'\n// npm install sentence-tokenizer\nimport { Tokenizer } from 'sentence-tokenizer'\n\nfunction sentenceBasedChunk(text: string, maxSentences: number = 2): string[] {\n  /** Chunks text by grouping a maximum number of sentences. */\n  const tokenizer = new Tokenizer('Chuck') // Identifier doesn't matter much here\n  tokenizer.setEntry(text)\n  const sentences = tokenizer.getSentences()\n\n  const chunks: string[] = []\n  for (let i = 0; i < sentences.length; i += maxSentences) {\n    chunks.push(sentences.slice(i, i + maxSentences).join(' '))\n  }\n  return chunks\n}\n\n// Example:\n// const text = \"Mr. Smith went to Washington. He visited the White House. Then he went home.\";\n// const chunks = sentenceBasedChunk(text, 2);\n// console.log(chunks);\n// Output: [ 'Mr. Smith went to Washington. He visited the White House.', 'Then he went home.' ]\n```\n\n----------------------------------------\n\nTITLE: Implementing OpenAI LLM Call Utility in TypeScript\nDESCRIPTION: This TypeScript code defines an asynchronous utility function `callLLM` for interacting with the OpenAI API. It accepts a string prompt, initializes the OpenAI client using a placeholder API key, calls the chat completions endpoint with the 'gpt-4o' model, and returns the content of the first choice's message, or an empty string if unavailable. A simple test case using an async immediately invoked function expression (IIFE) demonstrates its usage.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/agentic_coding.backup.md#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\n// utils/callLLM.ts\nimport OpenAI from 'openai'\n\nexport async function callLLM(prompt: string): Promise<string> {\n  const openai = new OpenAI({\n    apiKey: 'YOUR_API_KEY_HERE',\n  })\n\n  const response = await openai.chat.completions.create({\n    model: 'gpt-4o',\n    messages: [{ role: 'user', content: prompt }],\n  })\n\n  return response.choices[0]?.message?.content || ''\n}\n\n// Simple test case\n;(async () => {\n  const prompt = 'What is the meaning of life?'\n  console.log(await callLLM(prompt))\n})()\n```\n\n----------------------------------------\n\nTITLE: Unit Testing a BrainyFlow Node in Python\nDESCRIPTION: Demonstrates unit testing a single BrainyFlow node (`SummarizeNode`) using Python's `unittest` framework. It mocks the LLM call using `unittest.mock.AsyncMock` and verifies that the node interacts correctly with the shared `memory` object and calls the LLM as expected.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/testing.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n```python\nimport unittest\nfrom unittest.mock import AsyncMock, patch\nfrom brainyflow import Node\n\nclass TestSummarizeNode(unittest.TestCase):\n    async def test_summarize_node(self):\n        # Create the node\n        summarize_node = SummarizeNode()\n\n        # Create a mock shared store\n        memory = {\"text\": \"This is a long text that needs to be summarized.\"}\n\n        # Mock the LLM call\n        with patch('utils.call_llm', new_callable=AsyncMock) as mock_llm:\n            mock_llm.return_value = \"Short summary.\"\n\n            # Run the node\n            await summarize_node.run(memory)\n\n            # Verify the node called the LLM with the right prompt\n            mock_llm.assert_called_once()\n            call_args = mock_llm.call_args[0][0]\n            self.assertIn(\"summarize\", call_args.lower())\n\n            # Verify the result was stored correctly\n            self.assertEqual(memory.summary, \"Short summary.\") # Access memory object\n\nif __name__ == \"__main__\":\n    # Use asyncio.run for async tests if needed, or run within an existing loop\n    # For simplicity, assuming standard unittest runner handles async test cases\n    unittest.main()\n```\n```\n\n----------------------------------------\n\nTITLE: Running the PDF processing script\nDESCRIPTION: Execute the main Python script to process PDF files in the specified directory.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-tool-pdf-vision/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Finding Relevant Documents Based on User Questions with BrainyFlow\nDESCRIPTION: This node takes a user question, generates its embedding, and searches the FAISS index to find the most relevant document. It uses the search_index and filenames stored in the shared context by the PrepareEmbeddings node.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python_demo.ipynb#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass FindRelevantDocument(Node):\n    async def prep(self, shared):\n        # Get user question\n        question = input(\"Enter your question (or press Enter to quit): \")\n        if not question:\n            return None\n        return question\n        \n    async def exec(self, question):\n        if question is None:\n            return None\n            \n        # Get question embedding and search\n        query_embedding = get_embedding(question)\n        \n        # Search for most similar document\n        D, I = shared[\"search_index\"].search(\n            np.array([query_embedding]).astype('float32'),\n            k=1\n        )\n        most_relevant_idx = I[0][0]\n        most_relevant_file = shared[\"filenames\"][most_relevant_idx]\n        \n        return question, most_relevant_file\n        \n    async def post(self, shared, prep_res, exec_res):\n        if exec_res is None:\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies\nDESCRIPTION: Lists required Python packages and their minimum versions. Includes brainyflow package and Anthropic's API client for Claude access.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-thinking/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nbrainyflow>=0.0.1\nanthropic>=0.15.0   # For Claude API access\n```\n\n----------------------------------------\n\nTITLE: Creating TypeScript Custom Nodes in BrainyFlow\nDESCRIPTION: Illustrates how to create a custom node by extending the `Node` class in TypeScript. Implements `prep`, `exec`, and `post` methods, performing data processing and triggering successors within the node lifecycle.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/core_abstraction/node.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory, Node } from 'brainyflow'\n\nclass TextProcessorNode extends Node {\n  async prep(memory: Memory): Promise<string> {\n    // Read input data\n    return memory.text\n  }\n\n  async exec(text: string): Promise<string> {\n    // Process the text\n    return text.toUpperCase()\n  }\n\n  async post(memory: Memory, input: string, result: string): Promise<void> {\n    // Store the result in the global store\n    memory.processed_text = result\n\n    // Trigger the default next node (optional)\n    this.trigger('default')\n  }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Testing Input Validation in TypeScript with vitest\nDESCRIPTION: This snippet shows how to test a BrainyFlow node's handling of invalid inputs using vitest in TypeScript. It uses parameterized tests to check various invalid input types and ensures the node handles them gracefully without unhandled exceptions.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/testing.md#2025-04-22_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\nimport { describe, expect, it } from 'vitest'\n\n// import { MyNodeThatValidates } from './MyNodeThatValidates'; // Your node\n// import { Memory } from 'brainyflow'; // Assuming imports\n\ndescribe('Input Validation', () => {\n  const invalidInputs = [null, undefined, '', {}, [], { wrongKey: 1 }]\n\n  it.each(invalidInputs)('should handle invalid input: %s', async (invalidInput) => {\n    /** Tests if the node handles various invalid inputs gracefully. */\n    // const node = new MyNodeThatValidates(); // Node that should validate memory.input_data\n    const memory: Record<string, any> = { input_data: invalidInput } // Pass invalid data\n\n    // Expect the node to run without unhandled exceptions\n    // Use try/catch if specific errors are expected, otherwise just run\n    await node.run(memory)\n\n    // Example assertions: Check for an error flag or a specific state\n    expect(memory.error_message || memory.status).toBeDefined() // Check if either is set\n    expect(memory.status === 'validation_failed' || memory.error_message).toBeTruthy()\n    // Or assert that a default value was set\n    // expect(memory.output).toBe('default_value');\n  })\n})\n```\n\n----------------------------------------\n\nTITLE: Synthesizing Speech with AWS Polly in TypeScript\nDESCRIPTION: This TypeScript async function `synthesizePolly` utilizes the AWS SDK v3 (`@aws-sdk/client-polly` and `@aws-sdk/node-http-handler`) to synthesize text into speech using AWS Polly. It streams the resulting audio data into an MP3 file. The function requires AWS credentials to be configured (e.g., via environment variables) and takes the input text, an optional output filename (default 'polly_output.mp3'), and an optional AWS region.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/text_to_speech.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n// Requires: npm install @aws-sdk/client-polly @aws-sdk/node-http-handler\nimport * as fs from 'fs'\nimport { Writable } from 'stream' // Import Writable\nimport { PollyClient, SynthesizeSpeechCommand } from '@aws-sdk/client-polly'\nimport { NodeHttpHandler } from '@aws-sdk/node-http-handler' // Required for streaming body\n\nasync function synthesizePolly(\n  text: string,\n  outputFilename: string = 'polly_output.mp3',\n  region?: string,\n): Promise<void> {\n  /** Synthesizes speech using AWS Polly. */\n  // Assumes AWS credentials are configured (e.g., via env vars, instance profile)\n  const awsRegion = region || process.env.AWS_REGION || 'us-east-1'\n  const client = new PollyClient({ region: awsRegion })\n\n  const command = new SynthesizeSpeechCommand({\n    Text: text,\n    OutputFormat: 'mp3',\n    VoiceId: 'Joanna', // Example voice\n  })\n\n  try {\n    const response = await client.send(command)\n\n    if (response.AudioStream) {\n      // Handle the streaming body (Readable)\n      const audioStream = response.AudioStream as NodeJS.ReadableStream // Cast for Node.js environment\n      const fileStream = fs.createWriteStream(outputFilename)\n\n      // Pipe the audio stream to the file\n      await new Promise((resolve, reject) => {\n        audioStream.pipe(fileStream)\n        fileStream.on('finish', resolve)\n        fileStream.on('error', reject)\n        audioStream.on('error', reject) // Handle errors on the audio stream too\n      })\n\n      console.log(`Audio saved to ${outputFilename}`)\n    } else {\n      console.error('Error: Could not stream audio from Polly.')\n    }\n  } catch (error) {\n    console.error('Error calling AWS Polly:', error)\n  }\n}\n\n// Example:\n// synthesizePolly(\"Hello from AWS Polly!\");\n\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API key\nDESCRIPTION: Set the OpenAI API key as an environment variable to authenticate API requests.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-tool-pdf-vision/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=your_api_key_here\n```\n\n----------------------------------------\n\nTITLE: Getting Text Embeddings with Jina AI API - Python\nDESCRIPTION: Function to get text embeddings using Jina AI API. Requires API key authentication and handles HTTP requests with proper error handling. Returns numpy array of embeddings or None on failure.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/embedding.md#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# Requires: pip install requests numpy\nimport requests\nimport os\nimport numpy as np\n\ndef get_jina_embedding(text: str, model: str = \"jina-embeddings-v2-base-en\") -> np.ndarray | None:\n    \"\"\"Gets embedding from Jina AI API.\"\"\"\n    jina_token = os.environ.get(\"JINA_API_KEY\") # Or JINA_TOKEN depending on convention\n    if not jina_token:\n        print(\"Error: JINA_API_KEY not set.\")\n        return None\n\n    url = \"https://api.jina.ai/v1/embeddings\" # Use v1 endpoint\n    headers = {\n        \"Authorization\": f\"Bearer {jina_token}\",\n        \"Accept-Encoding\": \"identity\", # Recommended by Jina docs\n        \"Content-Type\": \"application/json\"\n    }\n    payload = {\n        \"input\": [text], # API expects a list\n        \"model\": model\n    }\n\n    try:\n        response = requests.post(url, headers=headers, json=payload)\n        response.raise_for_status()\n        result = response.json()\n        embedding = result[\"data\"][0][\"embedding\"]\n        return np.array(embedding, dtype=np.float32)\n    except requests.exceptions.RequestException as e:\n        print(f\"Error calling Jina AI embedding API: {e}\")\n        return None\n    except (KeyError, IndexError) as e:\n        print(f\"Error parsing Jina AI response: {e}, Response: {response.text}\")\n        return None\n```\n\n----------------------------------------\n\nTITLE: Unit Testing a BrainyFlow Node in TypeScript\nDESCRIPTION: Provides an example of unit testing a `SummarizeNode` using `vitest` in TypeScript. It utilizes `vi.mock` to mock the `callLLM` function, runs the node's lifecycle, and asserts that the LLM was called correctly and the `memory` object was updated as expected.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/testing.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n```typescript\nimport { describe, expect, it, vi } from 'vitest'\nimport { SummarizeNode } from './SummarizeNode' // Your Node implementation\nimport { callLLM } from './utils/callLLM' // Your LLM utility\n\n// Mock the LLM utility\nvi.mock('./utils/callLLM', () => ({\n  callLLM: vi.fn().mockResolvedValue('Short summary.'),\n}))\n\ndescribe('SummarizeNode', () => {\n  it('should summarize text correctly', async () => {\n    // Create the node instance\n    const summarizeNode = new SummarizeNode()\n\n    // Create initial global memory state\n    const memory = { text: 'This is a long text that needs to be summarized.' }\n\n    // Run the node's lifecycle (prep -> exec -> post)\n    await summarizeNode.run(memory) // Pass memory object\n\n    // Verify the LLM call\n    expect(callLLM).toHaveBeenCalledTimes(1)\n    const callArgs = vi.mocked(callLLM).mock.calls[0][0] // Get the first argument of the first call\n    expect(callArgs.toLowerCase()).toContain('summarize') // Check if prompt contains 'summarize'\n\n    // Verify the result was stored correctly in the global memory object\n    expect(memory.summary).toBe('Short summary.') // Access memory object\n  })\n})\n```\n```\n\n----------------------------------------\n\nTITLE: Implementing Flow Class ExecRunner Method in TypeScript\nDESCRIPTION: Overrides the `execRunner` method. For a `Flow`, this method initiates the execution of the entire node graph starting from the `start` node. It calls the internal `runNode` method to begin the process and returns the nested result structure representing the executed paths.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/api.md#2025-04-22_snippet_24\n\nLANGUAGE: typescript\nCODE:\n```\n`protected async execRunner(memory: Memory<GlobalStore, SharedStore>): Promise<NestedActions<AllowedActions>>`\n```\n\n----------------------------------------\n\nTITLE: Installing BrainyFlow with pip\nDESCRIPTION: Command for installing BrainyFlow using pip package manager.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-batch/translations/README_RUSSIAN.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install brainyflow\n```\n\n----------------------------------------\n\nTITLE: Integrating LLM Wrapper in BrainyFlow Node (Python)\nDESCRIPTION: This Python snippet integrates a custom LLM wrapper within a BrainyFlow node. It demonstrates how to prepare, execute, and post-process language model queries through a custom Node subclass. Dependencies include the `brainyflow` library and the previously implemented LLM wrapper.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/llm.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom brainyflow import Node\nfrom utils import call_llm\n\nclass LLMNode(Node):\n    async def prep(self, memory: Memory):\n        return memory.prompt\n\n    async def exec(self, prompt):\n        return await call_llm(prompt)\n\n    async def post(self, memory: Memory, prep_res, exec_res):\n        memory.response = exec_res\n        self.trigger('default')\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies with Version Requirements\nDESCRIPTION: Lists required Python packages with their minimum version constraints. The dependencies include brainyflow (version 0.0.1 or higher), anthropic (version 0.15.0 or higher), and pyyaml (version 6.0 or higher).\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-majority-vote/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nbrainyflow>=0.0.1\nanthropic>=0.15.0\npyyaml>=6.0\n```\n\n----------------------------------------\n\nTITLE: RAG System Flow Diagram\nDESCRIPTION: Mermaid diagram showing the two-phase pipeline of the RAG system: offline document indexing and online processing.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-rag/README.md#2025-04-22_snippet_3\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph TD\n    subgraph OfflineFlow[Offline Document Indexing]\n        ChunkDocs[ChunkDocumentsNode] --> EmbedDocs[EmbedDocumentsNode] --> CreateIndex[CreateIndexNode]\n    end\n\n    subgraph OnlineFlow[Online Processing]\n        EmbedQuery[EmbedQueryNode] --> RetrieveDoc[RetrieveDocumentNode] --> GenerateAnswer[GenerateAnswerNode]\n    end\n```\n\n----------------------------------------\n\nTITLE: Getting Text Embeddings with Hugging Face Inference API in Python\nDESCRIPTION: Retrieves text embeddings using Hugging Face's Inference API. Supports both authenticated and public model access via token configuration. Handles different embedding response formats and returns embeddings as a numpy array.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/embedding.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Requires: pip install requests numpy\nimport requests\nimport os\nimport numpy as np\n\ndef get_hf_embedding(text: str, model_url: str = \"https://api-inference.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2\") -> np.ndarray | None:\n    \"\"\"Gets embedding from Hugging Face Inference API.\"\"\"\n    hf_token = os.environ.get(\"HUGGINGFACE_TOKEN\")\n    if not hf_token:\n        print(\"Warning: HUGGINGFACE_TOKEN not set. Public models might work without it.\")\n        # Allow proceeding without token for public models, but auth is recommended\n\n    headers = {\"Authorization\": f\"Bearer {hf_token}\"} if hf_token else {}\n    payload = {\"inputs\": text}\n\n    try:\n        response = requests.post(model_url, headers=headers, json=payload)\n        response.raise_for_status()\n        # The response structure might vary; often it's a list of embeddings\n        # For sentence-transformers, it's usually [[embedding]]\n        embedding_list = response.json()\n        if isinstance(embedding_list, list) and len(embedding_list) > 0 and isinstance(embedding_list[0], list):\n             return np.array(embedding_list[0], dtype=np.float32)\n        elif isinstance(embedding_list, list) and len(embedding_list) > 0 and isinstance(embedding_list[0], float):\n             # Some models might return a flat list for single input\n             return np.array(embedding_list, dtype=np.float32)\n        else:\n             print(f\"Unexpected response structure from HF API: {embedding_list}\")\n             return None\n    except requests.exceptions.RequestException as e:\n        print(f\"Error calling Hugging Face Inference API: {e}\")\n        return None\n    except Exception as e:\n        print(f\"Error processing Hugging Face response: {e}\")\n        return None\n\n\n# Example:\n# text_to_embed = \"Hello world\"\n# embedding_vector = get_hf_embedding(text_to_embed)\n# if embedding_vector is not None:\n```\n\n----------------------------------------\n\nTITLE: Running a Custom Reasoning Problem\nDESCRIPTION: Command to run the script with a custom problem and specify the number of attempts (tries) to make.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-majority-vote/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython main.py --problem \"Your complex reasoning problem here\" --tries 5\n```\n\n----------------------------------------\n\nTITLE: Getting Text Embeddings with Azure OpenAI API in TypeScript\nDESCRIPTION: TypeScript implementation for retrieving text embeddings from Azure OpenAI service. Requires configuration of Azure-specific settings like endpoint and deployment ID. Handles API requests and errors appropriately.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/embedding.md#2025-04-22_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\n// Requires: npm install openai\nimport { OpenAIClient, AzureKeyCredential } from '@azure/openai'\n\nasync function getAzureEmbedding(\n  text: string,\n  deploymentName: string = 'text-embedding-ada-002',\n): Promise<number[] | null> {\n  /** Gets embedding from Azure OpenAI API. */\n  const azureEndpoint = process.env.AZURE_OPENAI_ENDPOINT\n  const azureApiKey = process.env.AZURE_OPENAI_API_KEY\n  if (!azureEndpoint || !azureApiKey) {\n    console.error('Error: Azure OpenAI environment variables not set.')\n    return null\n  }\n\n  try {\n    const client = new OpenAIClient(\n      azureEndpoint, \n      new AzureKeyCredential(azureApiKey)\n    )\n    const response = await client.getEmbeddings(deploymentName, [text])\n    return response.data[0].embedding\n  } catch (error) {\n    console.error('Error calling Azure OpenAI embedding API:', error)\n    return null\n  }\n}\n\n// Example:\n// const textToEmbed = \"Hello world\";\n// getAzureEmbedding(textToEmbed).then(embedding => {\n//   if (embedding) {\n//     console.log(embedding);\n//   }\n// });\n```\n\n----------------------------------------\n\nTITLE: Creating Pinecone Index if Not Exists TypeScript\nDESCRIPTION: Creates or checks the existence of a Pinecone index. Initializes the index with specified parameters if it's not already created. Handles vector dimensions, metric types, and environment settings.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/vector.md#2025-04-22_snippet_4\n\nLANGUAGE: TypeScript\nCODE:\n```\nasync function createPineconeIndexIfNotExists(\n  pc: Pinecone,\n  indexName: string,\n  dimension: number,\n  metric: 'cosine' | 'euclidean' | 'dotproduct' = 'cosine',\n  environment: string = 'gcp-starter', // Or your specific environment\n): Promise<void> {\n  /** Creates a Pinecone index if it doesn't exist. */\n  try {\n    const indexes = await pc.listIndexes()\n    if (!indexes.names?.includes(indexName)) {\n      console.log(`Creating index '${indexName}'...`)\n      await pc.createIndex({\n        name: indexName,\n        dimension: dimension,\n        metric: metric,\n        spec: {\n          pod: {\n            // Use PodSpec structure\n            environment: environment,\n            podType: 'p1.x1', // Example pod type, adjust as needed\n          },\n        },\n      })\n      console.log(`Index '${indexName}' created.`)\n      // Add a small delay for index readiness (optional but sometimes helpful)\n      await new Promise((resolve) => setTimeout(resolve, 5000))\n    } else {\n      console.log(`Index '${indexName}' already exists.`)\n    }\n  } catch (error) {\n    console.error(`Error creating or checking Pinecone index:`, error)\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Simple TypeScript Project Structure for BrainyFlow (haskell tree syntax)\nDESCRIPTION: This snippet provides a suggested directory layout for a simple BrainyFlow-based TypeScript project using a tree diagram. It denotes where main program files, nodes, flows, utilities, and documentation should be placed, helping developers separate core logic from reusable helpers and documentation. The diagram does not require specific parameters and is mainly for reference during project setup.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/best_practices.md#2025-04-22_snippet_2\n\nLANGUAGE: haskell\nCODE:\n```\nmy_project/\\n├── src/\\n│   ├── main.ts\\n│   ├── nodes.ts\\n│   ├── flow.ts\\n│   └── utils/\\n│       ├── callLLM.ts\\n│       └── searchWeb.ts\\n├── package.json\\n└── docs/\\n    └── design.md\n```\n\n----------------------------------------\n\nTITLE: Python Package Installation Command\nDESCRIPTION: Command to install required dependencies for the project from requirements.txt file.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-batch-node/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Initializing BaseNode in BrainyFlow Python\nDESCRIPTION: Initializes an instance of `BaseNode`, the abstract base class for all nodes. It sets up internal structures for managing successor nodes (keyed by action) and assigns a unique ID to the node.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n__init__(self)\n```\n\n----------------------------------------\n\nTITLE: Example BrainyFlow Node Design Definition\nDESCRIPTION: Provides a template for defining a specific node within the BrainyFlow design document. This example details the 'EntityExtractorNode', outlining its purpose, interaction with the shared store (reads/writes), potential return actions ('valid', 'retry'), and basic error handling strategy.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/agentic_coding.md#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nEntityExtractorNode:\n- Purpose: Identifies parties, dates, and monetary values in contract text\n- Reads: document_text from shared store\n- Writes: entities dictionary to shared store\n- Actions: Returns \"valid\" if entities found, \"retry\" if processing failed\n- Error Handling: Will retry up to 3 times with exponential backoff\n```\n\n----------------------------------------\n\nTITLE: Implementing Agent Communication with AsyncIO Queue in Python\nDESCRIPTION: Demonstrates how to create an agent node that listens to a message queue and processes system status messages. Uses asyncio.Queue for message passing and implements a continuous heartbeat sender.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/multi_agent.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport asyncio\nfrom brainyflow import Node, Flow\n\nclass AgentNode(Node):\n    async def prep(self, memory: Memory):\n        message = await memory.message_queue.get()\n        print(f\"Agent received: {message}\")\n        return message\n\n# Create node and flow\nagent = AgentNode()\nagent >> agent  # connect to self\nflow = Flow(start=agent)\n\n# Create heartbeat sender\nasync def send_system_messages(message_queue):\n    counter = 0\n    messages = [\n        \"System status: all systems operational\",\n        \"Memory usage: normal\",\n        \"Network connectivity: stable\",\n        \"Processing load: optimal\"\n    ]\n\n    while True: # In a real app, add a termination condition\n        message = f\"{messages[counter % len(messages)]} | timestamp_{counter}\"\n        await message_queue.put(message)\n        counter += 1\n        await asyncio.sleep(1)\n\nasync def main():\n    message_queue = asyncio.Queue()\n    # Pass queue via initial memory object\n    memory = {\"message_queue\": message_queue}\n\n    print(\"Starting agent listener and message sender...\")\n    # Run both coroutines\n    # Note: This will run indefinitely without a termination mechanism\n    await asyncio.gather(\n        flow.run(memory), # Pass memory object\n        send_system_messages(message_queue)\n    )\n\nasyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Initializing Project Structure - Directory Layout\nDESCRIPTION: Directory structure showing the organization of the BrainyFlow text summarization project including documentation, utilities, and main implementation files.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-node/README.md#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n.\n├── docs/          # Documentation files\n├── utils/         # Utility functions (LLM API wrapper)\n├── flow.py        # BrainyFlow implementation with Summarize Node\n├── main.py        # Main application entry point\n└── README.md      # Project documentation\n```\n\n----------------------------------------\n\nTITLE: Retrieving Text Embeddings with Google Vertex AI in Python\nDESCRIPTION: This code example demonstrates getting a text embedding from the Google Vertex AI API using Python. It depends on 'google-cloud-aiplatform' and 'numpy', and requires the GOOGLE_CLOUD_PROJECT environment variable or project_id parameter. The 'get_vertex_embedding' function initializes the Vertex AI SDK, constructs an endpoint, and submits a prediction request returning the embedding as a NumPy float array. Handles missing credentials and exceptions gracefully.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/embedding.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Requires: pip install google-cloud-aiplatform numpy\\nimport os\\nimport numpy as np\\nfrom google.cloud import aiplatform\\nfrom google.cloud.aiplatform.gapic.schema import predict\\n\\ndef get_vertex_embedding(text: str, project_id: str | None = None, location: str = \"us-central1\", model_name: str = \"textembedding-gecko@001\") -> np.ndarray | None:\\n    \\\"\\\"\\\"Gets embedding from Google Vertex AI.\\\"\\\"\\\"\\n    project_id = project_id or os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\\n    if not project_id:\\n        print(\"Error: GOOGLE_CLOUD_PROJECT not set and project_id not provided.\")\\n        return None\\n\\n    try:\\n        aiplatform.init(project=project_id, location=location)\\n        endpoint = aiplatform.Endpoint(f\"projects/{project_id}/locations/{location}/publishers/google/models/{model_name}\")\\n\\n        instance = predict.instance.TextEmbeddingInstance(content=text).to_value()\\n        instances = [instance]\\n        response = endpoint.predict(instances=instances)\\n\\n        embedding = response.predictions[0]['embeddings']['values']\\n        return np.array(embedding, dtype=np.float32)\\n    except Exception as e:\\n        print(f\"Error calling Vertex AI embedding API: {e}\")\\n        return None\\n\\n# Example:\\n# text_to_embed = \"Hello world\"\\n# embedding_vector = get_vertex_embedding(text_to_embed)\\n# if embedding_vector is not None:\\n\n```\n\n----------------------------------------\n\nTITLE: Migrating Node Methods to Async/Await - Python\nDESCRIPTION: This snippet illustrates how to migrate a Node class from PocketFlow to BrainyFlow by making methods asynchronous and using `await` for async function calls.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/migrating_from_pocketflow.md#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nclass MyNode(Node):\\n    def prep(self, shared):\\n        # Preparation logic\\n        return some_data\\n\\n    def exec(self, prep_res):\\n        # Execution logic\\n        return result\\n\\n    def post(self, shared, prep_res, exec_res):\\n        # Post-processing logic\\n        return action\\n\\n    def exec_fallback(self, prep_res, exc):\\n        # Handle exception\\n        return fallback_result\n```\n\nLANGUAGE: Python\nCODE:\n```\nclass MyNode(Node):\\n    # Prefer using 'memory' parameter name for consistency\\n    async def prep(self, memory):\\n        # Preparation logic\\n        # If you call other async functions here, use await\\n        return some_data\\n\\n    async def exec(self, prep_res):\\n        # Execution logic\\n        # If you call other async functions here, use await\\n        result = await some_async_task(prep_res)\\n        return result\\n\\n    async def post(self, memory, prep_res, exec_res):\\n        # Post-processing logic\\n        # If you call other async functions here, use await\\n        memory.result = exec_res # Write to memory (global store)\\n        self.trigger(action) # Use trigger instead of returning action string\\n\\n    async def exec_fallback(self, prep_res, exc):\\n        # Handle exception\\n        # If you call other async functions here, use await\\n        return fallback_result\n```\n\n----------------------------------------\n\nTITLE: Defining Complex Python Project Structure for BrainyFlow (haskell tree syntax)\nDESCRIPTION: This snippet demonstrates a more complex organizational structure for larger Python-based BrainyFlow projects. It separates node and flow implementations into dedicated directories, adds utility, configuration, and documentation folders, and includes unit test directories. It supports collaborative scalability by helping teams avoid monolithic codebases. No code parameters are present as this is a filesystem diagram.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/best_practices.md#2025-04-22_snippet_1\n\nLANGUAGE: haskell\nCODE:\n```\nmy_complex_project/\\n├── main.py                # Entry point\\n├── nodes/                 # Node implementations\\n│   ├── __init__.py\\n│   ├── input_nodes.py\\n│   ├── processing_nodes.py\\n│   └── output_nodes.py\\n├── flows/                 # Flow definitions\\n│   ├── __init__.py\\n│   └── main_flow.py\\n├── utils/                 # Utility functions\\n│   ├── __init__.py\\n│   ├── llm.py\\n│   ├── database.py\\n│   └── web_search.py\\n├── tests/                 # Test cases\\n│   ├── test_nodes.py\\n│   └── test_flows.py\\n├── config/                # Configuration\\n│   └── settings.py\\n├── requirements.txt       # Dependencies\\n└── docs/                  # Documentation\\n    ├── design.md          # High-level design\\n    └── api.md             # API documentation\n```\n\n----------------------------------------\n\nTITLE: Implementing Memory Class Clone Method in TypeScript\nDESCRIPTION: The `clone` method creates a new `Memory` instance. It shares the reference to the original `__global` store but performs a deep clone of the `__local` store. Optionally, it merges `forkingData` into the new local store. This is useful for creating isolated local scopes when branching execution flows.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/api.md#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\n`clone<T extends SharedStore = SharedStore>(forkingData: T = {} as T): Memory<G, L & T>`\n```\n\n----------------------------------------\n\nTITLE: Listing Triggered Actions and Memory Clones in BrainyFlow Python\nDESCRIPTION: Returns a list of tuples, where each tuple contains an `action` that was triggered and a corresponding `memory_clone`. The memory clone is created by the `trigger` method, potentially updated with `forking_data`.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nlist_triggers(self, memory)\n```\n\n----------------------------------------\n\nTITLE: Refactoring Memory Management in BrainyFlow (Python)\nDESCRIPTION: These snippets show the evolution of memory management for nodes in BrainyFlow when upgrading from v0.2 to v1.0 in Python. The old approach accessed shared state with dictionary keys, while the new approach uses explicit properties on a Memory object and requires explicit trigger calls. Dependencies include the BrainyFlow Node class and appropriate flow context; both snippets operate on values and store results in a memory structure. Expected input is a memory object containing 'input_text', and outputs are stored in the memory's 'result' property. The v1.0 snippet also highlights the use of self.trigger for flow control.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/migration.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Before (v0.2)\nclass MyNode(Node):\n    async def prep(self, shared):\n        return shared[\"input_text\"]\n\n    async def post(self, shared, prep_res, exec_res):\n        shared[\"result\"] = exec_res\n        return \"default\"  # Action name as return value\n```\n\nLANGUAGE: python\nCODE:\n```\n# After (v1.0)\nclass MyNode(Node):\n    async def prep(self, memory):\n        return memory.input_text  # Property access syntax\n\n    async def post(self, memory, prep_res, exec_res):\n        memory.result = exec_res  # Property assignment syntax\n        self.trigger(\"default\")   # Explicit trigger call\n```\n\n----------------------------------------\n\nTITLE: Implementing Async Recipe Fetching Node in Python\nDESCRIPTION: This code defines the FetchRecipes node that asynchronously takes a user-input ingredient and fetches relevant recipes via an API call. It demonstrates the prep_async method for setup and exec_async for the main async operation.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-async-basic/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nasync def prep_async(self, shared):\n    ingredient = input(\"Enter ingredient: \")\n    return ingredient\n\nasync def exec_async(self, ingredient):\n    # Async API call\n    recipes = await fetch_recipes(ingredient)\n    return recipes\n```\n\n----------------------------------------\n\nTITLE: Converting Number Array to Float32 Buffer in TypeScript\nDESCRIPTION: This TypeScript helper function converts a number array to a Float32 Buffer, which is useful for preparing vector data for insertion into Redis.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/vector.md#2025-04-22_snippet_23\n\nLANGUAGE: TypeScript\nCODE:\n```\nfunction vectorToBuffer(vector: number[]): Buffer {\n  const float32Array = new Float32Array(vector)\n  return Buffer.from(float32Array.buffer)\n}\n```\n\n----------------------------------------\n\nTITLE: Synthesizing Speech with IBM Watson TTS in Python\nDESCRIPTION: This Python function `synthesize_ibm_tts` utilizes the `ibm_watson` library to synthesize speech using IBM Watson Text-to-Speech. It requires IBM API Key and Service URL environment variables (`IBM_API_KEY`, `IBM_SERVICE_URL`) for authentication via `IAMAuthenticator`. The function accepts input text and an optional output filename (defaults to `ibm_tts_output.mp3`). It calls the `synthesize` method, specifying the voice and desired audio format (`audio/mp3`), and writes the received audio content to the specified file. Basic error handling is included.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/text_to_speech.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Requires: pip install ibm_watson\nfrom ibm_watson import TextToSpeechV1\nfrom ibm_cloud_sdk_core.authenticators import IAMAuthenticator\nimport os\n\ndef synthesize_ibm_tts(text: str, output_filename: str = \"ibm_tts_output.mp3\"):\n    \"\"\"Synthesizes speech using IBM Watson TTS.\"\"\"\n    api_key = os.environ.get(\"IBM_API_KEY\")\n    service_url = os.environ.get(\"IBM_SERVICE_URL\")\n\n    if not api_key or not service_url:\n        print(\"Error: IBM_API_KEY or IBM_SERVICE_URL not set.\")\n        return\n\n    try:\n        authenticator = IAMAuthenticator(api_key)\n        text_to_speech = TextToSpeechV1(authenticator=authenticator)\n        text_to_speech.set_service_url(service_url)\n\n        response = text_to_speech.synthesize(\n            text=text,\n            voice='en-US_AllisonV3Voice', # Example voice\n            accept='audio/mp3' # Specify desired format\n        ).get_result()\n\n        # The result object has a 'content' attribute with the audio data\n        with open(output_filename, 'wb') as audio_file:\n            audio_file.write(response.content)\n        print(f\"Audio saved to {output_filename}\")\n\n    except Exception as e:\n        print(f\"Error calling IBM Watson TTS: {e}\")\n\n# Example:\n# synthesize_ibm_tts(\"Hello from IBM Watson Text-to-Speech!\")\n\n```\n\n----------------------------------------\n\nTITLE: Example: Translating Text to Multiple Languages - Python\nDESCRIPTION: This example shows the migration of a conceptual BatchNode setup from PocketFlow to a Trigger/Processor pattern in BrainyFlow using async patterns.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/migrating_from_pocketflow.md#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# Before (PocketFlow) - Conceptual BatchNode\\nclass TranslateTextBatchNode(BatchNode):\\n    def prep(self, shared):\\n        text = shared.get(\"text\", \"(No text provided)\")\\n        languages = shared.get(\"languages\", [\"Chinese\", \"Spanish\", \"Japanese\"])\\n        # BatchNode prep would return items for exec\\n        return [(text, lang) for lang in languages]\\n\\n    def exec(self, item):\\n        text, lang = item\\n        # Assume translate_text exists\\n        return await translate_text(text, lang)\\n\\n    def post(self, shared, prep_res, exec_results):\\n        # BatchNode post might aggregate results\\n        shared[\"translations\"] = exec_results\\n        return \"default\"\n```\n\nLANGUAGE: Python\nCODE:\n```\n# After (BrainyFlow) - Using Flow Patterns\\n\\nfrom brainyflow import Node, Memory\\n\\n# 1. Trigger Node (Fans out work)\\nclass TriggerTranslationsNode(Node):\\n    async def prep(self, memory: Memory):\\n        text = memory.text if hasattr(memory, 'text') else \"(No text provided)\"\\n        languages = memory.languages if hasattr(memory, 'languages') else [\"Chinese\", \"Spanish\", \"Japanese\"]\\n\\n        return [{\"text\": text, \"language\": lang} for lang in languages]\\n\\n    async def post(self, memory: Memory, prep_res, exec_res):\\n        for index, input in enumerate(prep_res):\\n            self.trigger(\"default\", input | {\"index\": index})\\n\\n# 2. Processor Node (Handles one language)\\nclass TranslateOneLanguageNode(Node):\\n    async def prep(self, memory: Memory):\\n        # Read data passed via forkingData from local memory\\n        return {\\n            \"text\": memory.text,\\n            \"language\": memory.language,\\n            \"index\": memory.index\\n        }\\n\\n    async def exec(self, item):\\n        # Assume translate_text exists\\n        return await translate_text(item[\"text\"], item[\"language\"])\\n\\n    async def post(self, memory: Memory, prep_res, exec_res):\\n        # Store result in the global list at the correct index\\n        memory.translations[exec_res[\"index\"]] = exec_res\\n        this.trigger(\"default\")\n```\n\n----------------------------------------\n\nTITLE: Defining Simple Python Project Structure for BrainyFlow (haskell tree syntax)\nDESCRIPTION: This snippet illustrates the recommended file and folder organization for a simple BrainyFlow project in Python using a tree diagram. Required files include modules for node/flow definitions, utilities, and documentation. It helps ensure code modularity and clear separation of utilities and documentation. Inputs and outputs are not directly represented; this is a documentation structure diagram.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/best_practices.md#2025-04-22_snippet_0\n\nLANGUAGE: haskell\nCODE:\n```\nmy_simple_project/\\n├── main.py\\n├── nodes.py\\n├── flow.py\\n├── utils/\\n│   ├── __init__.py\\n│   ├── call_llm.py\\n│   └── search_web.py\\n├── requirements.txt\\n└── docs/\\n    └── design.md\n```\n\n----------------------------------------\n\nTITLE: Defining NestedActions Recursive Type in TypeScript\nDESCRIPTION: Defines a recursive type alias `NestedActions<T extends Action[]>`. It represents a nested structure where keys are action identifiers from the provided `Action` array `T`, and values are arrays of the same nested structure. This is used for the result structure returned by `Flow` execution.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/api.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n`NestedActions<T extends Action[]>`: `Record<T[number], NestedActions<T>[]>`\n```\n\n----------------------------------------\n\nTITLE: Example YAML Structure for Server Configuration\nDESCRIPTION: Provides an example of a target YAML structure suitable for representing server configuration settings, including host, port, and SSL status. This demonstrates using LLMs to potentially generate configuration files based on requirements.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/structure.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nserver:\n  host: 127.0.0.1\n  port: 8080\n  ssl: true\n```\n\n----------------------------------------\n\nTITLE: Defining Abstract Execution Runner for BaseNode in BrainyFlow Python\nDESCRIPTION: An abstract asynchronous method that defines the core execution logic runner. Subclasses (like `Node`) must implement this method to orchestrate the call to the actual `exec` method, potentially adding features like retries. Receives `memory` and `prep_res`.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nasync exec_runner(self, memory, prep_res) (abstract)\n```\n\n----------------------------------------\n\nTITLE: Creating Weaviate Collection in Python\nDESCRIPTION: Creates a Weaviate collection with specified properties if it doesn't already exist. Configures collection with title and content properties, and explicitly disables vectorizer.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/vector.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef create_weaviate_collection_if_not_exists(client: weaviate.Client, class_name: str, dimension: int):\n    \"\"\"Creates a Weaviate collection (class) if it doesn't exist.\"\"\"\n    try:\n        if not client.collections.exists(class_name):\n            print(f\"Creating collection '{class_name}'...\")\n            client.collections.create(\n                name=class_name,\n                vectorizer_config=wvc.config.Configure.Vectorizer.none(),\n                properties=[\n                    wvc.config.Property(name=\"title\", data_type=wvc.config.DataType.TEXT),\n                    wvc.config.Property(name=\"content\", data_type=wvc.config.DataType.TEXT),\n                ]\n            )\n            print(f\"Collection '{class_name}' created.\")\n        else:\n             print(f\"Collection '{class_name}' already exists.\")\n    except Exception as e:\n        print(f\"Error creating or checking Weaviate collection: {e}\")\n```\n\n----------------------------------------\n\nTITLE: Project Directory Structure\nDESCRIPTION: Tree representation of the project's file organization showing main components and their purposes.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-tool-crawler/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\npython-tool-crawler/\n├── tools/\n│   ├── crawler.py     # Web crawling functionality\n│   └── parser.py      # Content analysis using LLM\n├── utils/\n│   └── call_llm.py    # LLM API wrapper\n├── nodes.py           # BrainyFlow nodes\n├── flow.py           # Flow configuration\n├── main.py           # Main script\n└── requirements.txt   # Dependencies\n```\n\n----------------------------------------\n\nTITLE: Implementing Node with Retry Logic in Python\nDESCRIPTION: This snippet illustrates the 'Node' class, extending 'BaseNode', with additional retry mechanisms and error handling to provide robust execution. It includes retry attempts, delays, and fallback methods for handling execution failures.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/design.md#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nbrainyflow.Node\n```\n\n----------------------------------------\n\nTITLE: Dictionary-Style Write Access for Memory in BrainyFlow Python\nDESCRIPTION: Enables dictionary-style assignment (e.g., `memory['key'] = value`). Values are always written directly to the global store.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n__setitem__(self, key, value)\n```\n\n----------------------------------------\n\nTITLE: Creating Redis Search Index for Vector Data in Python\nDESCRIPTION: This snippet demonstrates how to create a Redis Search index for vector data using Python. It sets up a schema with a VectorField for embeddings and optionally a TagField for additional metadata.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/vector.md#2025-04-22_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\nVectorField(\"embedding\", \"FLAT\", { \n    \"TYPE\": \"FLOAT32\", \"DIM\": dimension, \"DISTANCE_METRIC\": \"L2\"\n}, as_name=\"vector\"),\n# TagField(\"tag\", as_name=\"tag\") # Example other field\n)\ndefinition_hash = IndexDefinition(prefix=[doc_prefix], index_type=IndexType.HASH)\ntry:\n    client.ft(index_name).info()\nexcept:\n    print(\"Recreating index for HASH...\")\n    client.ft(index_name).create_index(fields=schema_hash, definition=definition_hash)\n```\n\n----------------------------------------\n\nTITLE: Generating Mermaid Diagram from Graph Structure in Python\nDESCRIPTION: This Python function recursively traverses a graph structure, assigning unique IDs to nodes, and generates a hierarchical Mermaid diagram syntax. It involves key functionalities such as node traversal, managing node IDs, and constructing graph edges using Mermaid's syntax. It requires a graph structure composed of nodes with `successors` attributes and subgraph capable nodes that descend from a `Flow` class.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/visualization_logging.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndef build_mermaid(start):\n    ids, visited, lines = {}, set(), [\"graph LR\"]\n    ctr = 1\n    def get_id(n):\n        nonlocal ctr\n        return ids[n] if n in ids else (ids.setdefault(n, f\"N{ctr}\"), (ctr := ctr + 1))[0]\n    def link(a, b):\n        lines.append(f\"    {a} --> {b}\")\n    def walk(node, parent=None):\n        if node in visited:\n            return parent and link(parent, get_id(node))\n        visited.add(node)\n        if isinstance(node, Flow):\n            node.start and parent and link(parent, get_id(node.start))\n            lines.append(f\"\\n    subgraph sub_flow_{get_id(node)}[{type(node).__name__}]\")\n            node.start and walk(node.start)\n            for nxt in node.successors.values():\n                node.start and walk(nxt, get_id(node.start)) or (parent and link(parent, get_id(nxt))) or walk(nxt)\n            lines.append(\"    end\\n\")\n        else:\n            lines.append(f\"    {(nid := get_id(node))}['{type(node).__name__}']\")\n            parent and link(parent, nid)\n            [walk(nxt, nid) for nxt in node.successors.values()]\n    walk(start)\n    return \"\\n\".join(lines)\n```\n\n----------------------------------------\n\nTITLE: Defining ParallelFlow Orchestrator in BrainyFlow Python\nDESCRIPTION: Defines the `ParallelFlow` class, inheriting from `Flow`. This class orchestrates the execution of node graph branches concurrently rather than sequentially.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nParallelFlow(Flow)\n```\n\n----------------------------------------\n\nTITLE: Structuring Utility Functions in Python Projects\nDESCRIPTION: Illustrates a suggested directory layout for a Python project using BrainyFlow. It recommends placing utility functions like LLM interaction (`call_llm.py`), web search (`search_web.py`), and text embedding (`embed_text.py`) into separate files within a dedicated `utils/` directory to promote modularity and maintainability.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/index.md#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nmy_project/\n├── utils/\n│   ├── __init__.py\n│   ├── call_llm.py\n│   ├── search_web.py\n│   └── embed_text.py\n└── ...\n```\n\n----------------------------------------\n\nTITLE: Implementing Node Class ExecFallback Method in TypeScript\nDESCRIPTION: The `execFallback` method is invoked by the `execRunner` if the node's `exec` method fails after all configured retry attempts. The default implementation simply re-throws the final `NodeError`. Subclasses can override this to provide custom fallback logic.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/api.md#2025-04-22_snippet_20\n\nLANGUAGE: typescript\nCODE:\n```\n`async execFallback(prepRes: PrepResult, error: NodeError): Promise<ExecResult>`\n```\n\n----------------------------------------\n\nTITLE: Querying SerpApi using TypeScript\nDESCRIPTION: Implements a reusable async function for making search requests to SerpApi using TypeScript. The implementation uses environment variables for API key management, URL constructor for parameter handling, and Promise-based fetch API with proper error handling.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/websearch.md#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nasync function searchSerpApi(query: string, engine: string = 'google'): Promise<any> {\n  const apiKey = process.env.SERPAPI_KEY // Use environment variables\n\n  if (!apiKey) {\n    console.error('Error: Please set SERPAPI_KEY environment variable.')\n    return null\n  }\n\n  const url = new URL('https://serpapi.com/search')\n  url.searchParams.append('engine', engine)\n  url.searchParams.append('q', query)\n  url.searchParams.append('api_key', apiKey)\n\n  try {\n    const response = await fetch(url.toString())\n    if (!response.ok) {\n      throw new Error(`HTTP error! status: ${response.status}`)\n    }\n    const results = await response.json()\n    console.log(results)\n    return results\n  } catch (error) {\n    console.error('Error fetching SerpApi results:', error)\n    return null\n  }\n}\n\n// Example usage:\n// searchSerpApi(\"example\");\n// searchSerpApi(\"example\", \"bing\");\n```\n\n----------------------------------------\n\nTITLE: Visualizing the Map-Reduce Workflow with Mermaid\nDESCRIPTION: A flowchart diagram created using Mermaid that illustrates the Map-Reduce pattern implemented in the resume qualification workflow, showing the data flow between different nodes.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-map-reduce/README.md#2025-04-22_snippet_3\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n    ReadResumes[Read Resumes] --> TriggerEvaluations[Map: Trigger Evaluations]\n    TriggerEvaluations -->|evaluate_resume| EvaluateOneResume[Process: Evaluate One Resume]\n    EvaluateOneResume -->|aggregate_results| AggregateResults[Reduce: Aggregate Results]\n```\n\n----------------------------------------\n\nTITLE: Exposing Brainyflow Classes Globally in Browsers (JavaScript/TypeScript)\nDESCRIPTION: This describes the logic for making Brainyflow classes available globally in a browser context. It checks for a browser environment (`typeof window !== 'undefined'`) and ensures `globalThis.brainyflow` hasn't been previously defined. If conditions are met, it assigns an object containing the main classes (`BaseNode`, `Node`, `Flow`, `ParallelFlow`) to `globalThis.brainyflow`, facilitating usage without explicit module imports.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/design.md#2025-04-22_snippet_12\n\nLANGUAGE: javascript\nCODE:\n```\n// If in a browser and globalThis.brainyflow is not already defined,\n// the main classes (BaseNode, Node, Flow, ParallelFlow) are attached\n// to globalThis.brainyflow for easy access without module loaders.\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies\nDESCRIPTION: Lists required Python packages with their minimum version constraints. Includes BrainyFlow, NumPy, FAISS CPU version, and OpenAI packages.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-rag/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nbrainyflow>=0.0.5\nnumpy>=1.20.0\nfaiss-cpu>=1.7.0\nopenai>=1.0.0\n```\n\n----------------------------------------\n\nTITLE: Initializing Weaviate Client in Python\nDESCRIPTION: Creates and configures a Weaviate client connection using environment variables for URL and optional API key authentication. Includes connection verification and error handling.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/vector.md#2025-04-22_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef init_weaviate_client() -> weaviate.Client | None:\n    \"\"\"Initializes Weaviate client.\"\"\"\n    weaviate_url = os.environ.get(\"WEAVIATE_URL\") # e.g., \"http://localhost:8080\" or cloud URL\n    api_key = os.environ.get(\"WEAVIATE_API_KEY\") # Optional, for cloud/auth\n    if not weaviate_url:\n        print(\"Error: WEAVIATE_URL not set.\")\n        return None\n    try:\n        auth_config = weaviate.AuthApiKey(api_key=api_key) if api_key else None\n        client = weaviate.connect_to_custom(\n            http_host=weaviate_url.replace(\"http://\", \"\").split(\":\")[0],\n            http_port=int(weaviate_url.split(\":\")[-1]),\n            http_secure=weaviate_url.startswith(\"https\"),\n            auth_credentials=auth_config\n        )\n        client.is_ready()\n        print(\"Weaviate client initialized and ready.\")\n        return client\n    except Exception as e:\n        print(f\"Error initializing Weaviate client: {e}\")\n        return None\n```\n\n----------------------------------------\n\nTITLE: Implementing Flow Class RunNodes Method in TypeScript\nDESCRIPTION: A private method `runNodes` that takes an array of `BaseNode` instances and a `Memory` object. It uses the `runTasks` method (which runs sequentially by default in `Flow`) to execute each node's `runNode` logic in order, returning an array of their results.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/api.md#2025-04-22_snippet_26\n\nLANGUAGE: typescript\nCODE:\n```\n`private async runNodes(nodes: BaseNode[], memory: Memory<GlobalStore, SharedStore>): Promise<NestedActions<AllowedActions>[]>`\n```\n\n----------------------------------------\n\nTITLE: Running Asynchronous Tasks Concurrently in ParallelFlow Python\nDESCRIPTION: Overrides the `run_tasks` method from `Flow`. This implementation executes the list of awaitable `tasks` concurrently using `asyncio.gather`, allowing parallel execution of different branches in the flow.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nasync run_tasks(self, tasks)\n```\n\n----------------------------------------\n\nTITLE: Python Package Requirements\nDESCRIPTION: Specifies required Python packages and their version constraints for the Brainyflow project. Requires Pillow image processing library version 10.0.0 or higher.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-batch-flow/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nbrainyflow\nPillow>=10.0.0\n```\n\n----------------------------------------\n\nTITLE: Synthesizing Speech with ElevenLabs API in TypeScript\nDESCRIPTION: This asynchronous TypeScript function `synthesizeElevenlabs` uses the native `fetch` API to call the ElevenLabs Text-to-Speech REST endpoint. It requires `ELEVENLABS_API_KEY` and optionally `ELEVENLABS_VOICE_ID` environment variables. The function accepts input text and an optional output filename (defaults to `elevenlabs_output.mp3`). It constructs and sends a POST request with the text, model ID, voice settings, and API key. The response body is received as an ArrayBuffer, converted to a Node.js Buffer, and written to the specified file using `fs.writeFile`. Error handling for fetch and HTTP status codes is included.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/text_to_speech.md#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\n// Uses fetch API, no specific SDK needed unless desired\nimport * as fs from 'fs'\nimport { promisify } from 'util'\n\nconst writeFileAsync = promisify(fs.writeFile)\n\nasync function synthesizeElevenlabs(\n  text: string,\n  outputFilename: string = 'elevenlabs_output.mp3',\n): Promise<void> {\n  /** Synthesizes speech using ElevenLabs API. */\n  const apiKey = process.env.ELEVENLABS_API_KEY\n  // Find voice IDs via ElevenLabs website or API\n  const voiceId = process.env.ELEVENLABS_VOICE_ID || '21m00Tcm4TlvDq8ikWAM' // Example: Rachel\n\n  if (!apiKey) {\n    console.error('Error: ELEVENLABS_API_KEY not set.')\n    return\n  }\n  if (!voiceId) {\n    console.error('Error: ELEVENLABS_VOICE_ID not set.')\n    return\n  }\n\n  const url = `https://api.elevenlabs.io/v1/text-to-speech/${voiceId}`\n  const headers: HeadersInit = {\n    Accept: 'audio/mpeg',\n    'Content-Type': 'application/json',\n    'xi-api-key': apiKey,\n  }\n  const body = JSON.stringify({\n    text: text,\n    model_id: 'eleven_monolingual_v1', // Or other models\n    voice_settings: {\n      stability: 0.5,\n      similarity_boost: 0.75,\n    },\n  })\n\n  try {\n    const response = await fetch(url, {\n      method: 'POST',\n      headers: headers,\n      body: body,\n    })\n\n    if (!response.ok) {\n      throw new Error(`HTTP error! status: ${response.status}, message: ${await response.text()}`)\n    }\n\n    // Get response body as ArrayBuffer\n    const audioArrayBuffer = await response.arrayBuffer()\n    // Convert ArrayBuffer to Buffer for writing to file\n    const audioBuffer = Buffer.from(audioArrayBuffer)\n\n    await writeFileAsync(outputFilename, audioBuffer)\n    console.log(`Audio saved to ${outputFilename}`)\n  } catch (error) {\n    console.error('Error calling ElevenLabs API:', error)\n  }\n}\n\n// Example:\n// synthesizeElevenlabs(\"Hello from ElevenLabs Text-to-Speech!\");\n```\n\n----------------------------------------\n\nTITLE: Retrieving Successor Nodes for Action in BrainyFlow Python\nDESCRIPTION: Returns a list of successor nodes associated with the specified `action`. If no action is provided, it uses the `DEFAULT_ACTION`.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nget_next_nodes(self, action=DEFAULT_ACTION)\n```\n\n----------------------------------------\n\nTITLE: BrainyFlow Workflow Diagram\nDESCRIPTION: Mermaid flowchart showing the workflow of the agent, from getting tools to deciding and executing operations.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-mcp/README.md#2025-04-22_snippet_2\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n    tools[GetToolsNode] -->|decide| decide[DecideToolNode]\n    decide -->|execute| execute[ExecuteToolNode]\n```\n\n----------------------------------------\n\nTITLE: Running Nodes Sequentially within Flow in BrainyFlow Python\nDESCRIPTION: Asynchronously runs a list of `nodes` sequentially using the provided `memory` state. It leverages the `run_tasks` method to execute the `run_node` method for each node in the list.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nasync run_nodes(self, nodes, memory)\n```\n\n----------------------------------------\n\nTITLE: Initializing Pinecone Client TypeScript\nDESCRIPTION: Initializes a Pinecone client in TypeScript using an API key acquired from the environment variables. Checks if the API key exists and logs initialization status.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/vector.md#2025-04-22_snippet_3\n\nLANGUAGE: TypeScript\nCODE:\n```\n// Requires: npm install @pinecone-database/pinecone\nimport { Pinecone, PodSpec } from '@pinecone-database/pinecone'\n\nasync function initPinecone(): Promise<Pinecone | null> {\n  /** Initializes Pinecone client. */\n  const apiKey = process.env.PINECONE_API_KEY\n  if (!apiKey) {\n    console.error('Error: PINECONE_API_KEY not set.')\n    return null\n  }\n  try {\n    const pc = new Pinecone({ apiKey }) // Use new init style\n    console.log('Pinecone initialized.')\n    return pc\n  } catch (error) {\n    console.error('Error initializing Pinecone:', error)\n    return null\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing BaseNode Class Trigger Method in TypeScript\nDESCRIPTION: The `trigger` method is designed to be called within the `post` phase of a node's execution. It queues a specified `action` to be initiated after the current node finishes. Optional `forkingData` can be provided, which will be merged into a cloned local memory scope for the triggered branch.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/api.md#2025-04-22_snippet_15\n\nLANGUAGE: typescript\nCODE:\n```\n`trigger(action: AllowedActions[number], forkingData?: SharedStore): void`\n```\n\n----------------------------------------\n\nTITLE: Creating Weaviate Collection in TypeScript\nDESCRIPTION: Asynchronous function to create a Weaviate collection in TypeScript with error handling. Checks for existing collection and configures properties for title and content.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/vector.md#2025-04-22_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\nasync function createWeaviateCollectionIfNotExists(\n  client: WeaviateClient,\n  className: string,\n  dimension: number,\n): Promise<void> {\n  /** Creates a Weaviate collection (class) if it doesn't exist. */\n  try {\n    const schema = await client.schema.getter().do()\n    const exists = schema.classes?.some((c) => c.class === className)\n\n    if (!exists) {\n      console.log(`Creating collection '${className}'...`)\n      await client.schema\n        .classCreator()\n        .withClass({\n          class: className,\n          vectorizer: 'none',\n          properties: [\n            { name: 'title', dataType: ['text'] },\n            { name: 'content', dataType: ['text'] },\n          ],\n        })\n        .do()\n      console.log(`Collection '${className}' created.`)\n    } else {\n      console.log(`Collection '${className}' already exists.`)\n    }\n  } catch (error) {\n    console.error('Error creating or checking Weaviate collection:', error)\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Synthesizing Speech with IBM Watson TTS in TypeScript\nDESCRIPTION: This asynchronous TypeScript function `synthesizeIbmTts` uses the `ibm-watson` library to perform text-to-speech synthesis with IBM Watson. It requires IBM API Key and Service URL environment variables (`IBM_API_KEY`, `IBM_SERVICE_URL`) for authentication using `IamAuthenticator`. The function takes the input text and an optional output filename (defaulting to `ibm_tts_output.mp3`). It calls the `synthesize` method, receives the audio as a Node.js ReadableStream, converts it to a Buffer using a helper function `streamToBuffer`, and saves the buffer to the specified file using `fs.writeFile`. Error handling is included.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/text_to_speech.md#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\n// Requires: npm install ibm-watson\nimport * as fs from 'fs'\nimport { promisify } from 'util'\nimport { IamAuthenticator } from 'ibm-watson/auth'\nimport TextToSpeechV1 from 'ibm-watson/text-to-speech/v1'\n\nconst writeFileAsync = promisify(fs.writeFile)\n\nasync function synthesizeIbmTts(\n  text: string,\n  outputFilename: string = 'ibm_tts_output.mp3',\n): Promise<void> {\n  /** Synthesizes speech using IBM Watson TTS. */\n  const apiKey = process.env.IBM_API_KEY\n  const serviceUrl = process.env.IBM_SERVICE_URL\n\n  if (!apiKey || !serviceUrl) {\n    console.error('Error: IBM_API_KEY or IBM_SERVICE_URL not set.')\n    return\n  }\n\n  try {\n    const textToSpeech = new TextToSpeechV1({\n      authenticator: new IamAuthenticator({ apikey: apiKey }),\n      serviceUrl: serviceUrl,\n    })\n\n    const synthesizeParams = {\n      text: text,\n      voice: 'en-US_AllisonV3Voice', // Example voice\n      accept: 'audio/mp3', // Specify desired format\n    }\n\n    const response = await textToSpeech.synthesize(synthesizeParams)\n    // The response body is a ReadableStream in Node.js\n    const audioBuffer = await streamToBuffer(response.result as NodeJS.ReadableStream)\n\n    await writeFileAsync(outputFilename, audioBuffer)\n    console.log(`Audio saved to ${outputFilename}`)\n  } catch (error) {\n    console.error('Error calling IBM Watson TTS:', error)\n  }\n}\n\n// Helper function to convert a ReadableStream to a Buffer\nfunction streamToBuffer(stream: NodeJS.ReadableStream): Promise<Buffer> {\n  return new Promise((resolve, reject) => {\n    const chunks: Buffer[] = []\n    stream.on('data', (chunk) => chunks.push(chunk))\n    stream.on('error', reject)\n    stream.on('end', () => resolve(Buffer.concat(chunks)))\n  })\n}\n\n// Example:\n// synthesizeIbmTts(\"Hello from IBM Watson Text-to-Speech!\");\n```\n\n----------------------------------------\n\nTITLE: Applying Rate Limiting with RateLimiter (limiter) in TypeScript\nDESCRIPTION: Illustrates using the `RateLimiter` class from the `limiter` library to implement a window-based rate limit (e.g., 30 calls per minute). It requires explicitly consuming a token using `limiter.removeTokens(1)` before executing the rate-limited operation (`callApi`). Depends on the `limiter` library.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/throttling.md#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { RateLimiter } from 'limiter'\n\n// 30 calls per minute\nconst limiter = new RateLimiter({ tokensPerInterval: 30, interval: 'minute' })\n\nasync function callApi() {\n  await limiter.removeTokens(1)\n  // Your API call here\n}\n\n```\n\n----------------------------------------\n\nTITLE: Initializing ChromaDB Client and Collection in Python\nDESCRIPTION: Functions to initialize a persistent ChromaDB client and manage collections. Includes error handling and configuration options for both persistent and in-memory storage.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/vector.md#2025-04-22_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef init_chroma_client(persist_directory: str = \"./chroma_data_py\") -> chromadb.Client | None:\n    \"\"\"Initializes a persistent Chroma client.\"\"\"\n    try:\n        # Persistent client stores data on disk\n        client = chromadb.PersistentClient(path=persist_directory)\n        # Or use in-memory client: client = chromadb.Client()\n        print(f\"Chroma client initialized (persistent path: {persist_directory}).\")\n        return client\n    except Exception as e:\n        print(f\"Error initializing Chroma client: {e}\")\n        return None\n\ndef get_or_create_chroma_collection(client: chromadb.Client, collection_name: str) -> chromadb.Collection | None:\n    \"\"\"Gets or creates a Chroma collection.\"\"\"\n    try:\n        collection = client.get_or_create_collection(collection_name)\n        print(f\"Using Chroma collection '{collection_name}'.\")\n        return collection\n    except Exception as e:\n        print(f\"Error getting/creating Chroma collection: {e}\")\n        return None\n```\n\n----------------------------------------\n\nTITLE: Creating Qdrant Collection if Not Exists TypeScript\nDESCRIPTION: Validates the presence of a Qdrant collection and creates it as needed based on provided dimensions and distance metrics. Handles collection management through client methods.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/vector.md#2025-04-22_snippet_8\n\nLANGUAGE: TypeScript\nCODE:\n```\nasync function createQdrantCollectionIfNotExists(\n  client: QdrantClient,\n  collectionName: string,\n  dimension: number,\n  distanceMetric: Distance = Distance.Cosine,\n): Promise<void> {\n  /** Creates a Qdrant collection if it doesn't exist. */\n  try {\n    const collectionsResponse = await client.getCollections()\n    const collectionExists = collectionsResponse.collections.some((c) => c.name === collectionName)\n\n    if (!collectionExists) {\n      console.log(`Creating collection '${collectionName}'...`)\n      await client.recreateCollection(collectionName, {\n        // Use recreateCollection or check/create\n        vectors: { size: dimension, distance: distanceMetric },\n      })\n      console.log(`Collection '${collectionName}' created.`)\n    } else {\n      console.log(`Collection '${collectionName}' already exists.`)\n    }\n  } catch (error) {\n    console.error('Error creating or checking Qdrant collection:', error)\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing BaseNode Class Next Method in TypeScript\nDESCRIPTION: The `next` method is a convenience wrapper around the `on` method. It links a successor `node` using the `DEFAULT_ACTION` identifier. It returns the added successor node.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/api.md#2025-04-22_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\n`next<T extends BaseNode>(node: T, action?: Action): T`\n```\n\n----------------------------------------\n\nTITLE: Inserting Vector Data into Redis using HSET in Python\nDESCRIPTION: This code snippet shows how to insert vector data into Redis using the HSET command in Python. It demonstrates inserting two documents with embeddings and tags.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/vector.md#2025-04-22_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\nprint(\"Inserting data using HSET...\")\nclient.hset(doc_id1, mapping={\"embedding\": vector1, \"tag\": \"A\"})\nclient.hset(doc_id2, mapping={\"embedding\": vector2, \"tag\": \"B\"})\nprint(\"Data inserted.\")\n```\n\n----------------------------------------\n\nTITLE: Designing Node Flow Graph (QA Example, Mermaid Diagram)\nDESCRIPTION: Visualizes the simple directed graph structure of a QA flow, showing GetQuestionNode feeding into AnswerNode. Used to communicate flow topology; not executable code.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/getting_started.md#2025-04-22_snippet_2\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph LR\n    A[GetQuestionNode] --> B[AnswerNode]\n```\n\n----------------------------------------\n\nTITLE: Implementing BaseNode Class Clone Method in TypeScript\nDESCRIPTION: The `clone` method provides deep copying functionality for a `BaseNode` instance and its successor nodes. It handles cyclical references using the optional `seen` map to prevent infinite recursion during cloning.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/api.md#2025-04-22_snippet_8\n\nLANGUAGE: typescript\nCODE:\n```\n`clone<T extends this>(this: T, seen?: Map<BaseNode, BaseNode>): T`\n```\n\n----------------------------------------\n\nTITLE: Example Problem Statement for Requirements Definition\nDESCRIPTION: Provides a sample text illustrating how to define the 'Problem Statement', 'User Needs', and 'Success Criteria' within the Requirements Definition section of the AI Implementation Brief. This example focuses on a document processing system for a legal team.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/agentic_coding.md#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nWe need a document processing system that extracts key information from legal contracts,\nsummarizes them, and stores the results for easy retrieval. This will help our legal\nteam review contracts 70% faster.\n```\n\n----------------------------------------\n\nTITLE: Implementing Flow Class Constructor in TypeScript\nDESCRIPTION: The constructor for the `Flow` class, which orchestrates node execution. It requires a `start` node (an instance of `BaseNode`) to begin the flow and accepts optional `options`, such as `maxVisits` to limit potential infinite loops in cyclical graphs.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/api.md#2025-04-22_snippet_22\n\nLANGUAGE: typescript\nCODE:\n```\n`constructor(public start: BaseNode<GlobalStore>, private options?: { maxVisits: number })`\n```\n\n----------------------------------------\n\nTITLE: Implementing Token Bucket Rate Limiting (limiter) in TypeScript\nDESCRIPTION: Shows how to use the `TokenBucket` class from the `limiter` library to enforce token bucket rate limiting (10 tokens per minute, bucket size 10). Similar to `RateLimiter`, it requires explicitly removing a token via `limiter.removeTokens(1)` before the rate-limited action (`callApi`). Depends on the `limiter` library.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/throttling.md#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nimport { TokenBucket } from 'limiter'\n\n// 10 requests per minute\nconst limiter = new TokenBucket({\n  bucketSize: 10,\n  tokensPerInterval: 10,\n  interval: 'minute',\n})\n\nasync function callApi() {\n  await limiter.removeTokens(1)\n  // Your API call here\n}\n\n```\n\n----------------------------------------\n\nTITLE: Processing Node Triggers within Flow in BrainyFlow Python\nDESCRIPTION: An internal asynchronous helper method that processes a single trigger generated by a node. It takes the `action`, the list of `next_nodes` associated with that action, and the specific `node_memory` (potentially cloned and updated) for that branch. It then runs the `next_nodes` using this memory.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nasync _process_trigger(self, action, next_nodes, node_memory)\n```\n\n----------------------------------------\n\nTITLE: Implementing BaseNode Class GetNextNodes Method in TypeScript\nDESCRIPTION: The `getNextNodes` method retrieves an array of successor `BaseNode` instances associated with the specified `action`. If no action is provided, it defaults to retrieving nodes associated with the `DEFAULT_ACTION`.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/api.md#2025-04-22_snippet_11\n\nLANGUAGE: typescript\nCODE:\n```\n`getNextNodes(action?: Action): BaseNode[]`\n```\n\n----------------------------------------\n\nTITLE: Running BrainyFlow with asyncio in Python\nDESCRIPTION: This Python snippet demonstrates how to execute a BrainyFlow using asyncio. It includes an asynchronous main function set up with asyncio.run(), which initializes and runs a flow, manipulating a shared memory object. Dependencies include the asyncio library and the BrainyFlow framework.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/migrating_from_pocketflow.md#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nimport asyncio\n\nasync def main():\n    # ... setup your BrainyFlow nodes/flows ...\n    memory = {}\n    result = await my_flow.run(memory)\n    print(result)\n    print(memory)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Initializing Qdrant Client Python\nDESCRIPTION: Initializes a Qdrant client in Python using the URL and API key from environment variables. Ensures that the URL is set and logs the initialization status.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/vector.md#2025-04-22_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n# Requires: pip install qdrant-client\nimport os\nimport qdrant_client\nfrom qdrant_client.http.models import Distance, VectorParams, PointStruct, CollectionStatus\n\ndef init_qdrant_client() -> qdrant_client.QdrantClient | None:\n    \"\"\"Initializes Qdrant client.\"\"\"\n    qdrant_url = os.environ.get(\"QDRANT_URL\") # e.g., \"http://localhost:6333\" or cloud URL\n    api_key = os.environ.get(\"QDRANT_API_KEY\") # Optional, for cloud\n    if not qdrant_url:\n        print(\"Error: QDRANT_URL not set.\")\n        return None\n    try:\n        client = qdrant_client.QdrantClient(url=qdrant_url, api_key=api_key)\n        print(\"Qdrant client initialized.\")\n        return client\n    except Exception as e:\n        print(f\"Error initializing Qdrant client: {e}\")\n        return None\n```\n\n----------------------------------------\n\nTITLE: Defining Complex TypeScript Project Structure for BrainyFlow (haskell tree syntax)\nDESCRIPTION: This snippet illustrates an advanced structure for TypeScript-based BrainyFlow projects, promoting separation of source code, nodes, flows, utilities, type definitions, configuration, and documentation. It describes where compiled outputs, dependencies, and tests should be housed. This structure supports scalable, production-ready applications, and is presented for guidance rather than functional execution.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/best_practices.md#2025-04-22_snippet_3\n\nLANGUAGE: haskell\nCODE:\n```\nmy_complex_project/\\n├── src/                      # Source code\\n│   ├── index.ts              # Entry point\\n│   ├── nodes/                # Node implementations\\n│   │   ├── index.ts          # Exports all nodes\\n│   │   ├── inputNodes.ts\\n│   │   ├── processingNodes.ts\\n│   │   └── outputNodes.ts\\n│   ├── flows/                # Flow definitions\\n│   │   ├── index.ts          # Exports all flows\\n│   │   └── mainFlow.ts\\n│   ├── utils/                # Utility functions\\n│   │   ├── index.ts          # Exports all utilities\\n│   │   ├── llm.ts\\n│   │   ├── database.ts\\n│   │   └── webSearch.ts\\n│   ├── types/                # Type definitions\\n│   │   ├── index.ts          # Exports all types\\n│   │   ├── node.types.ts\\n│   │   └── flow.types.ts\\n│   └── config/               # Configuration\\n│       └── settings.ts\\n├── dist/                     # Compiled JavaScript\\n├── tests/                    # Test cases\\n│   ├── nodes.test.ts\\n│   └── flows.test.ts\\n├── package.json              # Dependencies and scripts\\n└── docs/                     # Documentation\\n    ├── design.md             # High-level design\\n    └── api.md                # API documentation\n```\n\n----------------------------------------\n\nTITLE: Getting Text Embeddings with AWS Bedrock in TypeScript\nDESCRIPTION: TypeScript implementation for retrieving text embeddings from AWS Bedrock. Requires AWS credentials configuration and handles integration with AWS SDK. Includes region and model selection options with proper error handling.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/embedding.md#2025-04-22_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\n// Requires: npm install @aws-sdk/client-bedrock-runtime\nimport { BedrockRuntimeClient, InvokeModelCommand } from '@aws-sdk/client-bedrock-runtime'\n\nasync function getBedrockEmbedding(\n  text: string,\n  region: string = process.env.AWS_REGION ?? 'us-east-1',\n  modelId: string = 'amazon.titan-embed-text-v2:0',\n): Promise<number[] | null> {\n  /** Gets embedding from AWS Bedrock. */\n  // Ensure AWS credentials are configured (e.g., via env vars, instance profile)\n  const client = new BedrockRuntimeClient({ region })\n  const body = JSON.stringify({ inputText: text })\n\n  const command = new InvokeModelCommand({\n    modelId,\n    contentType: 'application/json',\n    accept: 'application/json',\n    body,\n  })\n\n  try {\n    const response = await client.send(command)\n    // Decode the Uint8Array response body\n    const responseBodyString = new TextDecoder().decode(response.body)\n    const responseBody = JSON.parse(responseBodyString)\n    return responseBody.embedding ?? null\n  } catch (error) {\n    console.error('Error calling AWS Bedrock embedding API:', error)\n    return null\n  }\n}\n\n// Example:\n// const textToEmbed = \"Hello world\";\n// getBedrockEmbedding(textToEmbed).then(embedding => {\n//   if (embedding) {\n//     console.log(embedding);\n//   }\n// });\n```\n\n----------------------------------------\n\nTITLE: Implementing Sliding Window Rate Limiting (slidingwindow) in Python\nDESCRIPTION: Demonstrates using the `SlidingWindowRateLimiter` from the `slidingwindow` library to implement a sliding window rate limit (100 requests per 60 seconds). The code checks `limiter.allow_request()` and, if false, waits for `limiter.time_to_next_request()` seconds before proceeding with the API call. Depends on `slidingwindow` and `asyncio`.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/throttling.md#2025-04-22_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom slidingwindow import SlidingWindowRateLimiter\n\nlimiter = SlidingWindowRateLimiter(\n    max_requests=100,\n    window_size=60  # 60 seconds\n)\n\nasync def call_api():\n    if not limiter.allow_request():\n        await asyncio.sleep(limiter.time_to_next_request()) #  or raise RateLimitExceeded()\n    # Your API call here\n    return \"API response\"\n\n```\n\n----------------------------------------\n\nTITLE: Accessing Local Memory Store Property in BrainyFlow Python\nDESCRIPTION: Provides direct access to the local store dictionary of the `Memory` instance via a property.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlocal (property)\n```\n\n----------------------------------------\n\nTITLE: Implementing BaseNode Class Prep Method Stub in TypeScript\nDESCRIPTION: Defines the asynchronous `prep` method signature for the `BaseNode` abstract class. Subclasses should override this method to perform any necessary setup or data preparation before the main execution logic, using the provided `memory` instance. It can optionally return a result (`PrepResult`) to be passed to the `exec` phase.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/api.md#2025-04-22_snippet_12\n\nLANGUAGE: typescript\nCODE:\n```\n`async prep(memory: Memory<GlobalStore, LocalStore>): Promise<PrepResult | void>`\n```\n\n----------------------------------------\n\nTITLE: Exposing BrainyFlow Classes in Browser Environment\nDESCRIPTION: Conditionally checks if the code is running in a browser environment (by checking `typeof window !== 'undefined'`). If it is, it exposes the main BrainyFlow classes (`BaseNode`, `Node`, `Flow`, `ParallelFlow`) on the global `window` object under the `brainyflow` namespace (`globalThis.brainyflow`).\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/api.md#2025-04-22_snippet_29\n\nLANGUAGE: typescript\nCODE:\n```\n// Browser Compatibility\nif (typeof window !== 'undefined') {\n  (globalThis as any).brainyflow = {\n    BaseNode,\n    Node,\n    Flow,\n    ParallelFlow,\n  };\n}\n```\n\n----------------------------------------\n\nTITLE: Agent Communication Flow Diagram using Mermaid\nDESCRIPTION: A flowchart visualizing the asynchronous communication pattern between the AsyncHinter and AsyncGuesser nodes through a message queue.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-multi-agent/README.md#2025-04-22_snippet_3\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n    AsyncHinter[AsyncHinter Node] <--> MessageQueue{Message Queue}\n    MessageQueue <--> AsyncGuesser[AsyncGuesser Node]\n```\n\n----------------------------------------\n\nTITLE: Example Utility Function Definition\nDESCRIPTION: Demonstrates how to define an external utility function needed by the BrainyFlow application. This example specifies the purpose, inputs, outputs, and dependencies (spaCy library) for an `extract_entities` function.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/agentic_coding.md#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nextract_entities(text: str) -> dict:\n- Purpose: Uses NER to identify entities in text\n- Input: Document text string\n- Output: Dictionary of entity lists by type\n- Dependencies: spaCy NLP library with legal model\n```\n\n----------------------------------------\n\nTITLE: Dictionary-Style Read Access for Memory in BrainyFlow Python\nDESCRIPTION: Enables dictionary-style read access (e.g., `memory['key']`). Similar to `__getattr__`, it checks the local store first, then the global store.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n__getitem__(self, key)\n```\n\n----------------------------------------\n\nTITLE: Installing BrainyFlow using pip (Bash)\nDESCRIPTION: Installs the BrainyFlow Python package using the pip package manager. This command downloads and installs the latest version of the `brainyflow` package from the Python Package Index (PyPI). Requires Python and pip to be installed on the system.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/installation.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install brainyflow\n```\n\n----------------------------------------\n\nTITLE: Structuring Utility Functions in TypeScript Projects\nDESCRIPTION: Presents a recommended directory structure for a TypeScript project integrated with BrainyFlow. This structure advises organizing utility functions, such as those for calling LLMs (`callLlm.ts`), performing web searches (`searchWeb.ts`), and embedding text (`embedText.ts`), into distinct files within a `utils/` directory to enhance code organization and ease of maintenance.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/index.md#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nmy_project/\n├── utils/\n│   ├── callLlm.ts\n│   ├── searchWeb.ts\n│   └── embedText.ts\n└── ...\n```\n\n----------------------------------------\n\nTITLE: Implementing Retry Logic with Exponential Backoff (tenacity) in Python\nDESCRIPTION: Shows how to use the `@retry` decorator from the `tenacity` library to add automatic retry logic with exponential backoff to a function (`call_api_with_retry`). The example configuration retries up to 5 times with wait times between 4 and 10 seconds. Depends on the `tenacity` library.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/throttling.md#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom tenacity import retry, wait_exponential, stop_after_attempt\n\n@retry(\n    wait=wait_exponential(multiplier=1, min=4, max=10),\n    stop=stop_after_attempt(5)\n)\ndef call_api_with_retry():\n    # Your API call here\n    pass\n\n```\n\n----------------------------------------\n\nTITLE: Splitting Text by Fixed Size in TypeScript\nDESCRIPTION: This TypeScript function performs a similar operation to its Python counterpart by dividing text into fixed-size chunks based on character count. The method accepts a string and an optional chunk size. Outputs are segments of the text that might disrupt sentence flow.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/chunking.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nfunction fixedSizeChunk(text: string, chunkSize: number = 100): string[] {\n  /** Splits text into fixed-size chunks based on character count. */\n  const chunks: string[] = []\n  for (let i = 0; i < text.length; i += chunkSize) {\n    chunks.push(text.slice(i, i + chunkSize))\n  }\n  return chunks\n}\n\n// Example:\n// const text = \"This is a sample text to demonstrate fixed-size chunking.\";\n// const chunks = fixedSizeChunk(text, 20);\n// console.log(chunks);\n// Output: [ 'This is a sample tex', 't to demonstrate fix', 'ed-size chunking.' ]\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating a Virtual Environment in Bash\nDESCRIPTION: This shell snippet demonstrates how to initialize and activate a Python virtual environment. It uses the 'python -m venv' command to create an isolated environment called 'venv', and includes activation commands for both Unix-based ('source venv/bin/activate') and Windows ('venv\\Scripts\\activate') systems. No external dependencies are required beyond Python itself. Input: none; Output: activated virtual environment. Limitation: Users must ensure Python and venv are installed.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-tool-embeddings/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv venv\\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n```\n\n----------------------------------------\n\nTITLE: Executing Tasks Concurrently in ParallelFlow (TypeScript)\nDESCRIPTION: This snippet shows the core logic within the overridden `runTasks` method of the `ParallelFlow` class. It utilizes `Promise.all` combined with mapping tasks to function calls (`task => task()`) to execute an array of asynchronous task functions concurrently. This enables the parallel execution of different branches that might originate from a single node trigger within the flow graph.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/design.md#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nPromise.all(tasks.map(task => task()))\n```\n\n----------------------------------------\n\nTITLE: Defining Asynchronous Post-Processing Phase for BaseNode in BrainyFlow Python\nDESCRIPTION: Defines the asynchronous post-processing phase method (`post`). Subclasses can override this to perform actions after the main execution, such as cleanup or triggering subsequent actions. It receives the `memory`, `prep` result (`prep_res`), and `exec` result (`exec_res`).\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nasync post(self, memory, prep_res, exec_res)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Parallel Image Processing Flow with Mermaid\nDESCRIPTION: Flowchart diagram showing the architecture of parallel image processing system, from trigger through parallel processing to optional aggregation.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-parallel-batch-flow/README.md#2025-04-22_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph TD\n    A[Trigger Image Processing] --> B(ParallelFlow)\n    subgraph B [Parallel Image Processing Flow]\n        subgraph C [Per Image-Filter Flow]\n            D[Load Image] --> E[Apply Filter]\n            E --> F[Save Image]\n        end\n    end\n    B --> G[Aggregation (Optional)]\n```\n\n----------------------------------------\n\nTITLE: Checking Key Existence in Memory in BrainyFlow Python\nDESCRIPTION: Implements the `in` operator (e.g., `key in memory`). Checks if the specified `key` exists in either the local or the global store.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n__contains__(self, key)\n```\n\n----------------------------------------\n\nTITLE: Example Output: Invalid Non-Travel Query\nDESCRIPTION: Example conversation showing how the guardrail handles a non-travel related query. The system detects that the question about language models is not related to travel and rejects it.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-chat-guardrail/README.md#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nWelcome to the Travel Advisor Chat! Type 'exit' to end the conversation.\n\nYou: How to study large language models?\n\nTravel Advisor: The query is not related to travel advice, destinations, planning, or other travel topics. It is about studying large language models, which is a topic related to artificial intelligence and machine learning.\n\nYou: exit\n\nGoodbye! Safe travels!\n```\n\n----------------------------------------\n\nTITLE: Grouping Sentences into Chunks in Python\nDESCRIPTION: A Python function that chunks text by a set number of sentences, necessitating the NLTK sentence tokenizer. It takes a text input and an optional max number of sentences per chunk, outputting a list of sentence groups. The approach relies on the NLTK 'punkt' tokenizer and may require installing and downloading this data.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/chunking.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport nltk # Requires: pip install nltk\n\n# Ensure NLTK data is downloaded (run once)\n# try:\n#     nltk.data.find('tokenizers/punkt')\n# except nltk.downloader.DownloadError:\n#     nltk.download('punkt')\n\ndef sentence_based_chunk(text: str, max_sentences: int = 2) -> list[str]:\n    \"\"\"Chunks text by grouping a maximum number of sentences.\"\"\"\n    try:\n        sentences = nltk.sent_tokenize(text)\n    except LookupError:\n        print(\"NLTK 'punkt' tokenizer not found. Please run nltk.download('punkt')\")\n        return [] # Or handle error appropriately\n\n    chunks = []\n    for i in range(0, len(sentences), max_sentences):\n        chunks.append(\" \".join(sentences[i : i + max_sentences]))\n    return chunks\n\n# Example:\n# text = \"Mr. Smith went to Washington. He visited the White House. Then he went home.\"\n# chunks = sentence_based_chunk(text, 2)\n# print(chunks)\n# Output: ['Mr. Smith went to Washington. He visited the White House.', 'Then he went home.']\n```\n\n----------------------------------------\n\nTITLE: Generating Mermaid Diagram from Graph Structure in TypeScript\nDESCRIPTION: This TypeScript function mirrors the Python implementation for generating a Mermaid diagram from a given graph structure. Key features include unique ID assignment, edge linking, and subgraph processing for `Flow` instances. Dependencies include having node-like objects with `successors` properties and constructors recognizable through their class names.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/visualization_logging.md#2025-04-22_snippet_1\n\nLANGUAGE: TypeScript\nCODE:\n```\nfunction buildMermaid(start: any): string {\n  const ids: Record = {}\n  const visited = new Set()\n  const lines: string[] = ['graph LR']\n  let ctr = 1\n\n  function getId(n: any): string {\n    const key = n.toString()\n    if (key in ids) return ids[key]\n    ids[key] = `N${ctr++}`\n    return ids[key]\n  }\n\n  function link(a: string, b: string): void {\n    lines.push(`    ${a} --> ${b}`)\n  }\n\n  function walk(node: any, parent: string | null = null): void {\n    const nodeKey = node.toString()\n    if (visited.has(nodeKey)) {\n      if (parent) link(parent, getId(node))\n      return\n    }\n\n    visited.add(nodeKey)\n\n    if (node instanceof Flow) {\n      if (node.start && parent) {\n        link(parent, getId(node.start))\n      }\n\n      lines.push(`\\n    subgraph sub_flow_${getId(node)}[${node.constructor.name}]`)\n\n      if (node.start) {\n        walk(node.start)\n      }\n\n      for (const nxt of Object.values(node.successors)) {\n        if (node.start) {\n          walk(nxt, getId(node.start))\n        } else if (parent) {\n          link(parent, getId(nxt))\n        } else {\n          walk(nxt)\n        }\n      }\n\n      lines.push('    end\\n')\n    } else {\n      const nid = getId(node)\n      lines.push(`    ${nid}['${node.constructor.name}']`)\n\n      if (parent) {\n        link(parent, nid)\n      }\n\n      for (const nxt of Object.values(node.successors)) {\n        walk(nxt, nid)\n      }\n    }\n  }\n\n  walk(start)\n  return lines.join('\\n')\n}\n```\n\n----------------------------------------\n\nTITLE: Installing BrainyFlow and Dependencies using Pip\nDESCRIPTION: Installs the required Python packages `brainyflow`, `faiss-cpu`, and `openai` using the pip package manager. These are necessary prerequisites for running the subsequent Python code involving BrainyFlow and OpenAI API calls.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python_demo.ipynb#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n! pip install brainyflow\n! pip install faiss-cpu\n! pip install openai\n```\n\n----------------------------------------\n\nTITLE: Defining the Action Type Alias (TypeScript)\nDESCRIPTION: This type alias defines `Action` as a union type, allowing values to be either a standard `string` or the specific `DEFAULT_ACTION` constant. This type is used to represent the actions that trigger transitions between nodes in the flow graph.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/design.md#2025-04-22_snippet_7\n\nLANGUAGE: typescript\nCODE:\n```\nAction: string | typeof DEFAULT_ACTION\n```\n\n----------------------------------------\n\nTITLE: Running Full Node Lifecycle in BrainyFlow Python\nDESCRIPTION: Executes the complete lifecycle of a node asynchronously: `prep`, then `exec_runner`, then `post`. Takes the `memory` state as input. If `propagate` is `True`, it returns the list of triggers generated during the `post` phase; otherwise, it returns the result of `exec_runner`.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nasync run(self, memory, propagate=False)\n```\n\n----------------------------------------\n\nTITLE: Viewing Project Structure in BrainyFlow Communication Example\nDESCRIPTION: The directory structure of the BrainyFlow communication example project, showing the main files including README.md, requirements.txt, main.py, flow.py, and nodes.py. This structure organizes the implementation of the word counter application that demonstrates shared store usage.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-communication/README.md#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\npython-communication/\n├── README.md\n├── requirements.txt\n├── main.py\n├── flow.py\n└── nodes.py\n```\n\n----------------------------------------\n\nTITLE: Printing Node Summaries in Python\nDESCRIPTION: Code to print filename and summary information from a shared dictionary.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python_demo.ipynb#2025-04-22_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Summaries:\")\nfor filename, summary in shared[\"summaries\"].items():\n    print(f\"\\n{filename}:\")\n    print(summary)\n```\n\n----------------------------------------\n\nTITLE: Displaying Project Structure in BrainyFlow Hello World Example\nDESCRIPTION: Illustrates the directory structure for a basic BrainyFlow application, including documentation, utilities, implementation files, and the main entry point.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-hello-world/README.md#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n.\n├── docs/          # Documentation files\n├── utils/         # Utility functions\n├── flow.py        # BrainyFlow implementation\n├── main.py        # Main application entry point\n└── README.md      # Project documentation\n```\n\n----------------------------------------\n\nTITLE: Getting Text Embeddings with Cohere API in Python\nDESCRIPTION: Retrieves text embeddings using Cohere's embedding models. Requires Cohere API key and supports model selection. Returns embeddings as a numpy array with error handling.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/embedding.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Requires: pip install cohere numpy\nimport cohere\nimport os\nimport numpy as np\n\ndef get_cohere_embedding(text: str, model: str = \"embed-english-v3.0\") -> np.ndarray | None:\n    \"\"\"Gets embedding from Cohere API.\"\"\"\n    api_key = os.environ.get(\"COHERE_API_KEY\")\n    if not api_key:\n        print(\"Error: COHERE_API_KEY not set.\")\n        return None\n    try:\n        co = cohere.Client(api_key)\n        # Cohere API expects a list of texts\n        response = co.embed(texts=[text], model=model, input_type=\"search_document\") # Adjust input_type as needed\n        embedding = response.embeddings[0]\n        return np.array(embedding, dtype=np.float32)\n    except Exception as e:\n        print(f\"Error calling Cohere embedding API: {e}\")\n        return None\n\n# Example:\n# text_to_embed = \"Hello world\"\n# embedding_vector = get_cohere_embedding(text_to_embed)\n# if embedding_vector is not None:\n#     print(embedding_vector)\n```\n\n----------------------------------------\n\nTITLE: Defining the Trigger Interface Structure (TypeScript)\nDESCRIPTION: Describes the structure of the `Trigger<Action, L>` interface. Objects conforming to this interface are stored in a node's `triggers` array and typically contain an `action` (of type `Action`) and optional `forkingData` (of generic type `L`), representing the data needed to initiate a transition.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/design.md#2025-04-22_snippet_10\n\nLANGUAGE: typescript\nCODE:\n```\nTrigger<Action, L>: Interface for objects stored in the triggers array ({ action, forkingData })\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Dependencies with Minimum Versions\nDESCRIPTION: This requirements file lists the Python package dependencies required for the 'brainyflow' project. It uses the standard `requirements.txt` format, specifying minimum versions for `openai`, `numpy`, `faiss-cpu`, `python-dotenv`, and `brainyflow` itself using the '>=' operator. This file is intended to be used with `pip install -r <filename>` to install the necessary libraries.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-tool-embeddings/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nopenai>=1.0.0\nnumpy>=1.24.0\nfaiss-cpu>=1.7.0\npython-dotenv>=1.0.0\nbrainyflow>=0.1.0 \n```\n\n----------------------------------------\n\nTITLE: Defining Shared Data Store Structure in Python\nDESCRIPTION: This Python dictionary illustrates an example structure for a shared data store in the Brainyflow framework. It organizes data into categories like 'input', 'processing', 'output', and 'metadata', facilitating communication and data transfer between different nodes in the processing flow. Nested dictionaries allow for structured data within categories.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/agentic_coding.backup.md#2025-04-22_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Example shared store structure\n\nshared = {\n    \"input\": {\n        \"query\": \"How do neural networks learn?\",\n        \"context\": { # Another nested dict\n            \"persona\": \"The user is a computer science student.\",\n            \"location\": \"San Francisco\"\n        }\n    },\n    \"processing\": {\n        \"search_results\": [],\n        \"relevant_chunks\": []\n    },\n    \"output\": {\n        \"response\": None,\n        \"confidence\": None\n    },\n    \"metadata\": {\n        \"start_time\": \"2025-04-06T18:38:00Z\",\n        \"processing_steps\": []\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Initializing Memory State in BrainyFlow Python\nDESCRIPTION: Initializes a `Memory` instance, setting up the global state store (`_global`) and an optional local state store (`_local`). The global store is typically shared across related flow executions, while the local store is specific to a branch or node.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n__init__(self, _global, _local=None)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Chat Flow with Mermaid Diagram\nDESCRIPTION: A Mermaid flowchart showing the simple flow of the chat application. It illustrates how the ChatNode loops back to itself to maintain a continuous conversation.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-chat/README.md#2025-04-22_snippet_2\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n    chat[ChatNode] -->|continue| chat\n```\n\n----------------------------------------\n\nTITLE: Defining Default Action Constant in TypeScript\nDESCRIPTION: Defines a constant `DEFAULT_ACTION` with the string value 'default'. This constant is intended to be used as the default identifier for transitions between nodes in the BrainyFlow system.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/api.md#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n`DEFAULT_ACTION`: `'default'`\n```\n\n----------------------------------------\n\nTITLE: Adding Default Action Successor Node in BrainyFlow Python\nDESCRIPTION: A convenience method that calls `on` using the `DEFAULT_ACTION`. It links the provided `node` as the default successor. Returns the added successor node.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nnext(self, node, action=DEFAULT_ACTION)\n```\n\n----------------------------------------\n\nTITLE: Defining Shared Store Structure in Python\nDESCRIPTION: Python dictionary structure for maintaining the shared state including problem statement, thoughts list, counters and final solution.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-thinking/design.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nshared = {\n    \"problem\": \"The problem statement goes here\",\n    \"thoughts\": [],  # List of thought objects\n    \"current_thought_number\": 0,\n    \"total_thoughts_estimate\": 5,  # Initial estimate, can change\n    \"solution\": None  # Final solution when complete\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Dependencies for BrainyFlow Package in Requirements File\nDESCRIPTION: This requirements file specifies the necessary dependencies for the BrainyFlow package. It requires pandas version 2.0.0 or higher, indicating that the package relies on pandas for data manipulation and analysis capabilities.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-batch-node/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nbrainyflow\npandas>=2.0.0\n```\n\n----------------------------------------\n\nTITLE: Extracting Call Stack in Python\nDESCRIPTION: Extracts and returns the call stack in Python by examining current execution frames and filtering for specific BaseNode instances. Requires the 'inspect' module and a class derived from 'BaseNode' for correct execution.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/visualization_logging.md#2025-04-22_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nimport inspect\n\ndef get_node_call_stack():\n    stack = inspect.stack()\n    node_names = []\n    seen_ids = set()\n    for frame_info in stack[1:]:\n        local_vars = frame_info.frame.f_locals\n        if 'self' in local_vars:\n            caller_self = local_vars['self']\n            if isinstance(caller_self, BaseNode) and id(caller_self) not in seen_ids:\n                seen_ids.add(id(caller_self))\n                node_names.append(type(caller_self).__name__)\n    return node_names\n```\n\n----------------------------------------\n\nTITLE: Cloning BaseNode and Successors in BrainyFlow Python\nDESCRIPTION: Creates a deep copy of the node and recursively clones its successors. It uses a `seen` dictionary (memoization) to handle potential cycles in the node graph, ensuring nodes are not cloned multiple times.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclone(self, seen=None)\n```\n\n----------------------------------------\n\nTITLE: Installing and Running the Chat Application in Bash\nDESCRIPTION: Commands to install the required dependencies and run the chat application. The requirements.txt file contains all necessary packages, and main.py is the entry point for the application.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-chat/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Implementing a Data Science Flow Example in Python\nDESCRIPTION: An example in Python demonstrating creation of a data processing and modeling flow using hypothetical node classes. Establishes a linear process flow using node connections and Flow encapsulation to visualize data science processes structurally. This example uses the `build_mermaid` function to transform defined flows into a readable Mermaid graph.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/visualization_logging.md#2025-04-22_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nclass DataPrepBatchNode(BatchNode):\n    def prep(self,shared): return []\nclass ValidateDataNode(Node): pass\nclass FeatureExtractionNode(Node): pass\nclass TrainModelNode(Node): pass\nclass EvaluateModelNode(Node): pass\nclass ModelFlow(Flow): pass\nclass DataScienceFlow(Flow):pass\n\nfeature_node = FeatureExtractionNode()\ntrain_node = TrainModelNode()\nevaluate_node = EvaluateModelNode()\nfeature_node >> train_node >> evaluate_node\nmodel_flow = ModelFlow(start=feature_node)\ndata_prep_node = DataPrepBatchNode()\nvalidate_node = ValidateDataNode()\ndata_prep_node >> validate_node >> model_flow\ndata_science_flow = DataScienceFlow(start=data_prep_node)\nresult = build_mermaid(start=data_science_flow)\nprint(result) # Output the Mermaid string\n```\n\n----------------------------------------\n\nTITLE: Implementing OpenAI LLM Call Utility in Python\nDESCRIPTION: This Python script defines a utility function `call_llm` that interacts with the OpenAI API. It takes a text prompt, initializes the OpenAI client with an API key (placeholder used), sends the prompt to the 'gpt-4o' model via the chat completions endpoint, and returns the model's response content. A simple test case is included within the `if __name__ == \"__main__\":` block.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/agentic_coding.backup.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# utils/call_llm.py\nfrom openai import OpenAI\n\ndef call_llm(prompt):\n    client = OpenAI(api_key=\"YOUR_API_KEY_HERE\")\n    r = client.chat.completions.create(\n        model=\"gpt-4o\",\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return r.choices[0].message.content\n\nif __name__ == \"__main__\":\n\t# Simple test case\n    prompt = \"What is the meaning of life?\"\n    print(call_llm(prompt))\n```\n\n----------------------------------------\n\nTITLE: Disabling Direct Execution for Flow in BrainyFlow Python\nDESCRIPTION: Overrides the `exec` method from `BaseNode` to raise a `RuntimeError`. `Flow` instances are meant to orchestrate the execution of other nodes, not perform computational work themselves.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/python/api.md#2025-04-22_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nasync exec(self, prep_res)\n```\n\n----------------------------------------\n\nTITLE: Defining SharedStore Type Alias in TypeScript\nDESCRIPTION: Defines a type alias `SharedStore` as `Record<string, any>`. This represents a generic key-value store, typically used for managing global or local memory state within BrainyFlow.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/api.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\n`SharedStore`: `Record<string, any>`\n```\n\n----------------------------------------\n\nTITLE: Flow Diagram of Travel Advisor Chat Application\nDESCRIPTION: Mermaid flowchart illustrating the architecture of the application. It shows how user input flows through validation guardrails before being processed by the LLM, with retry paths for invalid queries.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-chat-guardrail/README.md#2025-04-22_snippet_2\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n    user[UserInputNode] -->|validate| guardrail[GuardrailNode]\n    guardrail -->|retry| user\n    guardrail -->|process| llm[LLMNode]\n    llm -->|continue| user\n```\n\n----------------------------------------\n\nTITLE: Using YAML Block Literals for Multi-line Strings without Escaping\nDESCRIPTION: Provides a YAML example using the block literal indicator (`|`) to represent a multi-line string. This syntax allows internal quotes and newlines to be included directly without requiring explicit escaping (like `\\\"` or `\\n` in JSON). This demonstrates YAML's potential advantage for LLMs generating text containing special characters or formatting.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/structure.md#2025-04-22_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\ndialogue: |\n  Alice said: \"Hello Bob.\n  How are you?\n  I am good.\"\n```\n\n----------------------------------------\n\nTITLE: Installing and Running BrainyFlow RAG System\nDESCRIPTION: Commands to install dependencies and run the main application with default settings.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-rag/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Defining Utility Function Metadata in YAML\nDESCRIPTION: This YAML snippet demonstrates how to document a utility function's metadata within the Brainyflow framework. It specifies the function's name (including its path), expected input type, output type, and the reason for its necessity within the application flow, aiding in design and understanding.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/agentic_coding.backup.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nname: get_embedding (utils/get_embedding.py)\ninput: string\noutput: a vector of 3072 floats\nnecessity: Used by the second node to embed text\n```\n\n----------------------------------------\n\nTITLE: Illustrating String Escaping Requirements in JSON\nDESCRIPTION: Shows a JSON example where internal double quotes (`\\\"`) and newline characters (`\\n`) within a string value must be explicitly escaped using backslashes. This highlights a potential difficulty for LLMs when generating complex strings in JSON format, as correct escaping is mandatory.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/structure.md#2025-04-22_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"dialogue\": \"Alice said: \\\"Hello Bob.\\\\nHow are you?\\\\nI am good.\\\"\"\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies for Research Agent\nDESCRIPTION: Command to install all necessary packages for the research agent from the requirements file.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-agent/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Implementing Flow Class Exec Method (Error) in TypeScript\nDESCRIPTION: The `exec` method for the `Flow` class. It intentionally throws an error because `Flow` instances are designed to orchestrate the execution of other nodes via their `run` method, not to perform computational logic themselves.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/api.md#2025-04-22_snippet_23\n\nLANGUAGE: typescript\nCODE:\n```\n`exec(): never`\n```\n\n----------------------------------------\n\nTITLE: Implementing BaseNode Class Exec Method Stub in TypeScript\nDESCRIPTION: Defines the asynchronous `exec` method signature for the `BaseNode` abstract class. Subclasses must override this method to implement the core computational logic of the node. It receives the result from the `prep` phase (`prepRes`) and can return a result (`ExecResult`).\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/api.md#2025-04-22_snippet_13\n\nLANGUAGE: typescript\nCODE:\n```\n`async exec(prepRes: PrepResult | void): Promise<ExecResult | void>`\n```\n\n----------------------------------------\n\nTITLE: Sample Program Output\nDESCRIPTION: Example output showing the processing of CSV chunks and final statistics.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-batch-node/README.md#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nTrigger: Triggering processing for 10 CSV chunks.\nProcessor: Processing chunk (Index 0)\nProcessor: Processing chunk (Index 1)\n...\nProcessor: Finished chunk (Index 9)\nProcessor: All chunks processed, triggering aggregate.\nReducer: Aggregating 10 chunk statistics.\nReducer: Statistics aggregation complete.\n\nFinal Statistics:\n- Total Sales: $1,234,567.89\n- Average Sale: $123.45\n- Total Transactions: 10,000\n```\n\n----------------------------------------\n\nTITLE: Example YAML Structure for Bullet Point Summary\nDESCRIPTION: Shows a target YAML structure for a document summary presented as a list of bullet points under the 'summary' key. This illustrates a common use case for structured LLM output, suitable for concisely presenting key takeaways.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/structure.md#2025-04-22_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nsummary:\n  - This product is easy to use.\n  - It is cost-effective.\n  - Suitable for all skill levels.\n```\n\n----------------------------------------\n\nTITLE: Chain of Thought Flow Diagram\nDESCRIPTION: Mermaid flowchart showing the self-looping Chain of Thought node implementation for step-by-step reasoning.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-thinking/README.md#2025-04-22_snippet_4\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n    cot[ChainOfThoughtNode] -->|\"continue\"| cot\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies and Running the Example in Bash\nDESCRIPTION: Commands to install required dependencies from requirements.txt and run the main Python script for the BrainyFlow nested flow example.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-nested-batch/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Example BrainyFlow Architecture using Mermaid\nDESCRIPTION: This Mermaid graph diagram illustrates an example flow design for a document processing system using BrainyFlow nodes. It shows the sequence and potential branching (validation) from DocumentLoader through TextExtractor, EntityExtractor, ValidationNode, SummaryGenerator, and DatabaseStorage.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/agentic_coding.md#2025-04-22_snippet_2\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph TD\n    A[DocumentLoader] --> B[TextExtractor]\n    B --> C[EntityExtractor]\n    C --> D[ValidationNode]\n    D -->|Valid| E[SummaryGenerator]\n    D -->|Invalid| C\n    E --> F[DatabaseStorage]\n```\n\n----------------------------------------\n\nTITLE: Writing to Memory in TypeScript\nDESCRIPTION: Shows how to write to global and local memory in a BrainyFlow node. The code writes data to the global store directly and uses the trigger method with forking data to populate local memory.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/core_abstraction/memory.md#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Memory, Node } from 'brainyflow'\n\ninterface MyGlobal {\n  fileList?: string[]\n  results?: Record<string, any>\n}\ninterface MyLocal {\n  processedCount?: number // Data passed via memory.local assignment\n  file?: string // Data passed via forkingData\n}\n\nclass DataWriterNode extends Node<MyGlobal, MyLocal, ['process_file']> {\n  async post(\n    memory: Memory<MyGlobal, MyLocal>,\n    prepRes: any,\n    execRes: { files: string[]; count: number }, // Assume exec returns this format\n  ): Promise<void> {\n    // --- Writing to Global Store ---\n    // Accessible to all nodes in the flow\n    memory.fileList = execRes.files\n    console.log(`Memory updated globally: fileList=${memory.fileList}`)\n\n    // --- Writing to Local Store ---\n    // Accessible to this node and all descendants\n    memory.local.processedCount = execRes.count\n    console.log(`Memory updated locally: processedCount=${memory.processedCount}`)\n\n    // --- Triggering with Local Data (Forking Data) ---\n    // 'file' will be added to the local store of the memory clone\n    // passed to the node(s) triggered by the 'process_file' action.\n    for (const file of execRes.files) {\n      this.trigger('process_file', { file: file })\n    }\n  }\n}\n\n// Example Processor Node (triggered by 'process_file')\nclass FileProcessorNode extends Node<MyGlobal, MyLocal> {\n  async prep(memory: Memory<MyGlobal, MyLocal>): Promise<string | undefined> {\n    // Reads 'file' from the local store first, then global\n    const fileToProcess = memory.file\n    console.log(`Processing file (from local memory): ${fileToProcess}`)\n    return fileToProcess\n  }\n  // ... exec, post ...\n}\n```\n\n----------------------------------------\n\nTITLE: Displaying Project Structure in Python\nDESCRIPTION: Shows the directory structure of the BrainyFlow batch processing example project, including main files and image directory.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-batch-flow/README.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npython-batch-flow/\n├── README.md\n├── requirements.txt\n├── images/\n│   ├── cat.jpg        # Sample image 1\n│   ├── dog.jpg        # Sample image 2\n│   └── bird.jpg       # Sample image 3\n├── main.py            # Entry point\n├── flow.py            # Main Flow and sub-flow definitions\n└── nodes.py           # Node implementations for image processing\n```\n\n----------------------------------------\n\nTITLE: Assessing Implementation Completion\nDESCRIPTION: This markdown snippet acts as a reflective prompt for AI agents during the implementation process. It ensures that specific steps and checks are completed, such as the removal of stubs, meeting all requirements, successful application runs, and identifying further implementation actions. The snippet serves as a guide to push AI agents to refine and complete tasks thoroughly according to design requirements.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/agentic_coding.md#2025-04-22_snippet_7\n\nLANGUAGE: markdown\nCODE:\n```\nReview the design document and the whole implemented code, then answer the following questions:\n\n- are all stubs and placeholders gone?\n- have all requirements and features been implemented?\n- have you successfully run the application and all tests?\n- what should be implemented next?\n\nIf the answer to any question is \\\"no\\\", or you can name something to be implemented next, then you are not done yet.\nGo back to working and finish it! Do not stop until you are done.\n```\n\n----------------------------------------\n\nTITLE: Installing BrainyFlow with npm (TypeScript Bash)\nDESCRIPTION: Installs BrainyFlow for TypeScript or JavaScript projects using npm, with alternatives for pnpm and yarn. This sets up the core BrainyFlow package in your project's dependencies. Requires Node.js and npm (or pnpm/yarn). Outputs install progress and any warnings/errors.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/getting_started.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install brainyflow # or pnpm/yarn\n```\n\n----------------------------------------\n\nTITLE: Defining Translation Node in TypeScript\nDESCRIPTION: This TypeScript snippet defines a conceptual BatchNode class named TranslateTextBatchNode for translation tasks. It includes methods for preparing translations with available languages, executing the translation, and posting results. Dependencies include a translation function and shared memory for input texts and target languages.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/migrating_from_pocketflow.md#2025-04-22_snippet_3\n\nLANGUAGE: TypeScript\nCODE:\n```\nclass TranslateTextBatchNode extends BatchNode<any, any, any, [string, string], string> {\n  async prep(shared: Record<string, any>): Promise<[string, string][]> {\n    const text = shared['text'] ?? '(No text provided)'\n    const languages = shared['languages'] ?? ['Chinese', 'Spanish', 'Japanese']\n    return languages.map((lang: string) => [text, lang])\n  }\n\n  async exec(item: [string, string]): Promise<string> {\n    const [text, lang] = item\n    // Assume translateText exists\n    return await translateText(text, lang)\n  }\n\n  async post(shared: Record<string, any>, prepRes: any, execResults: string[]): Promise<string> {\n    shared['translations'] = execResults\n    return 'default'\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with pip for the Taboo Game\nDESCRIPTION: Command to install the required Python dependencies for the Taboo game from the requirements.txt file.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-multi-agent/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Updating Imports and Adding Async/Await - Python\nDESCRIPTION: This snippet demonstrates how to update Python import statements when migrating from PocketFlow to BrainyFlow and how to convert synchronous methods to async using `async` and `await`.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/migrating_from_pocketflow.md#2025-04-22_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# Before\\nfrom pocketflow import Node, Flow, BatchNode # ... etc\\n\\n# After\\nimport asyncio\\nfrom brainyflow import Node, Flow, SequentialBatchNode # ... etc\n```\n\n----------------------------------------\n\nTITLE: Running the Article Workflow with Default or Custom Topic in Python\nDESCRIPTION: These Python command-line snippets demonstrate how to run the main workflow script. Running 'python main.py' starts the article workflow with a default topic ('AI Safety'). Optionally, users can specify any custom topic as a positional argument. The script initializes the processing pipeline, triggering sequential LLM-powered transformations to generate article content.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-workflow/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npython main.py\n```\n\nLANGUAGE: python\nCODE:\n```\npython main.py Climate Change\n```\n\n----------------------------------------\n\nTITLE: Running Research Agent with Custom Question\nDESCRIPTION: Command to run the research agent with a custom question by using the -- prefix followed by the query.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-agent/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython main.py --\"What is quantum computing?\"\n```\n\n----------------------------------------\n\nTITLE: Design Document Template in Markdown\nDESCRIPTION: A detailed template for creating design documentation, including sections for requirements, flow design, utility functions, and node design.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/agentic_coding.backup.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Design Doc: Your Project Name\n\n> Please DON'T remove notes for AI\n\n## Requirements\n\n> Notes for AI: Keep it simple and clear.\n> If the requirements are abstract, write concrete user stories\n\n### Problem Statement\n\n[Describe the problem you're trying to solve in 2-3 sentences]\n\n### Business Objectives\n\n- [list few objectives; keep it concise]\n\n### Success Criteria\n\n- [Measurable outcome 1]\n- [Measurable outcome 2]\n...\n```\n\n----------------------------------------\n\nTITLE: Running BrainyFlow with Custom Query\nDESCRIPTION: Command to execute the application with a specific query about Q-Mesh protocol.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-rag/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython main.py --\"How does the Q-Mesh protocol achieve high transaction speeds?\"\n```\n\n----------------------------------------\n\nTITLE: Running the BrainyFlow Communication Example\nDESCRIPTION: Command to execute the BrainyFlow communication example by running the main.py file. This launches the word counter application that demonstrates shared store usage for inter-node communication.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-communication/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Defining the Custom NodeError Type (TypeScript)\nDESCRIPTION: This type alias defines `NodeError` by intersecting the built-in `Error` type with an object containing a `retryCount` number property. This creates a custom error type specifically for node execution failures, carrying information about how many times a retry was attempted.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/design.md#2025-04-22_snippet_9\n\nLANGUAGE: typescript\nCODE:\n```\nNodeError: Error & { retryCount: number }\n```\n\n----------------------------------------\n\nTITLE: Sample Output Illustrating Article Workflow Result\nDESCRIPTION: This text output block demonstrates expected output from a successful run of the article workflow using the example topic 'AI Safety'. It contains structured YAML, parsed outlines, section contents, and the final styled article. While not executable code, this reference helps validate workflow implementation and set user expectations for the result.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-workflow/README.md#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\n=== Starting Article Workflow on Topic: AI Safety ===\\n\\n\\n===== OUTLINE (YAML) =====\\n\\nsections:\\n- Introduction to AI Safety\\n- Key Challenges in AI Safety\\n- Strategies for Ensuring AI Safety\\n\\n\\n===== PARSED OUTLINE =====\\n\\n1. Introduction to AI Safety\\n2. Key Challenges in AI Safety\\n3. Strategies for Ensuring AI Safety\\n\\n=========================\\n\\n\\n===== SECTION CONTENTS =====\\n\\n--- Introduction to AI Safety ---\\nAI Safety is about making sure that artificial intelligence (AI) systems are helpful and not harmful. Imagine teaching a robot to help with chores. AI Safety is like setting ground rules for the robot so it doesn't accidentally cause trouble, like mistaking a pet for a toy. By ensuring AI systems understand their tasks and limitations, we can trust them to act safely. It's about creating guidelines and checks to ensure AI assists us without unintended consequences.\\n\\n--- Key Challenges in AI Safety ---\\nAI safety is about ensuring that artificial intelligence systems operate in ways that are beneficial and not harmful. One key challenge is making sure AI makes decisions that align with human values. Imagine teaching a robot to fetch coffee, but it ends up knocking things over because it doesn't understand the mess it creates. Similarly, if AI systems don't fully grasp human intentions, they might act in unexpected ways. The task is to make AI smart enough to achieve goals without causing problems, much like training a puppy to follow rules without chewing on your shoes.\\n\\n--- Strategies for Ensuring AI Safety ---\\nEnsuring AI safety is about making sure artificial intelligence behaves as expected and doesn’t cause harm. Imagine AI as a new driver on the road; we need rules and safeguards to prevent accidents. By testing AI systems under different conditions, setting clear rules for their behavior, and keeping human oversight, we can manage risks. For instance, just as cars have brakes to ensure safety, AI systems need to have fail-safes. This helps in building trust and avoiding unexpected issues, keeping both humans and AI on the right track.\\n\\n===========================\\n\\n\\n===== FINAL ARTICLE =====\\n\\n# Welcome to the World of AI Safety\\n\\nHave you ever wondered what it would be like to have your very own robot helping you around the house? Sounds like a dream, right? But let’s hit pause for a moment. What if this robot mistook your fluffy cat for a toy? That’s exactly where AI Safety comes in. Think of AI Safety as setting some friendly ground rules for your household helper, ensuring that it knows the difference between doing chores and causing a bit of chaos. It’s all about making sure our AI allies play by the rules, making life easier without those pesky accidental hiccups.\\n\\n# Navigating the Maze of AI Challenges\\n\\nPicture this: you've asked your trusty robot to grab you a cup of coffee. But instead, it sends mugs flying and spills coffee because it doesn’t quite get the concept of a mess. Frustrating, isn’t it? One of the biggest hurdles in AI Safety is aligning AI decisions with our human values and intentions. It’s like training a puppy not to gnaw on your favorite pair of shoes. Our job is to teach AI how to reach its goals without stepping on our toes, all while being as reliable and lovable as a well-trained pup.\\n\\n# Steering AI Toward Safe Horizons\\n\\nNow, how do we keep our AI friends on the straight and narrow? Imagine AI as a new driver learning to navigate the roads of life. Just like we teach new drivers the rules of the road and equip cars with brakes for safety, we provide AI with guidelines and fail-safes to prevent any unintended mishaps. Testing AI systems in various scenarios and keeping a watchful human eye on them ensures they don’t veer off track. It’s all about building trust and creating a partnership where both humans and AI are cruising smoothly together.\\n\\n# Wrapping It Up\\n\\nAt the end of the day, AI Safety is about creating a harmonious relationship between humans and machines, where we trust our metal companions to support us without the fear of unexpected surprises. By setting boundaries and ensuring understanding, we’re not just building smarter machines—we’re crafting a future where AI and humanity can thrive together. So, next time you’re imagining that helpful robot assistant, rest easy knowing that AI Safety is making sure it's ready to lend a hand without dropping the ball—or your coffee mug!\\n\\n========================\\n\\n\\n=== Workflow Completed ===\\n\\nTopic: AI Safety\\nOutline Length: 96 characters\\nDraft Length: 1690 characters\\nFinal Article Length: 2266 characters\n```\n\n----------------------------------------\n\nTITLE: Defining the Default Action Constant (TypeScript)\nDESCRIPTION: Defines the `DEFAULT_ACTION` constant with the string value 'default'. This constant serves as a fallback or standard action identifier within the flow logic, used when no specific action is provided or applicable for a transition.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/design.md#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nDEFAULT_ACTION: 'default'\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies using pip\nDESCRIPTION: Installs the required Python packages listed in the `requirements.txt` file using the pip package installer. This step is necessary before running the application.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-tool-search/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Sample Output of Image Processing\nDESCRIPTION: Example console output showing the processing of multiple images with different filters and timing comparisons between sequential and parallel execution.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-parallel-batch-flow/README.md#2025-04-22_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n=== Processing Images in Parallel ===\nParallel Image Processor\n------------------------------\nFound 3 images:\n- images/bird.jpg\n- images/cat.jpg\n- images/dog.jpg\n\nRunning sequential flow...\nProcessing 3 images with 3 filters...\nTotal combinations: 9\nTrigger: Triggering summary for 9 files.\nProcessor: Summarizing images/bird.jpg (Index 0)...\nSaved: output/bird_grayscale.jpg\n...etc\n\nTiming Results:\nSequential processing: 13.76 seconds\nParallel processing: 1.71 seconds\nSpeedup: 8.04x\n\nProcessing complete! Check the output/ directory for results.\n```\n\n----------------------------------------\n\nTITLE: Installing and Running the Application\nDESCRIPTION: Commands to install required dependencies and execute the main application.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-structured-output/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Defining Action Type Alias in TypeScript\nDESCRIPTION: Defines a type alias `Action` as a union of `string` and the literal type `typeof DEFAULT_ACTION`. This type is used to identify specific transitions or triggers between nodes.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/api.md#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n`Action`: `string | typeof DEFAULT_ACTION`\n```\n\n----------------------------------------\n\nTITLE: Installing Project Dependencies using pip in Bash\nDESCRIPTION: This Bash snippet installs all required Python dependencies for the BrainyFlow article workflow project by referencing a requirements.txt file. Users must execute this command before running the main workflow application. The requirements.txt should list all necessary Python libraries and versions for successful execution.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-workflow/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Flow Visualization with Mermaid\nDESCRIPTION: A mermaid diagram showing the application flow with batch processing, error handling, and multiple steps.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/agentic_coding.backup.md#2025-04-22_snippet_1\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n    start[Start] --> batch[Batch]\n    batch --> check[Check]\n    check -->|OK| process\n    check -->|Error| fix[Fix]\n    fix --> check\n\n    subgraph process[Process]\n      step1[Step 1] --> step2[Step 2]\n    end\n\n    process --> endNode[End]\n```\n\n----------------------------------------\n\nTITLE: Visualizing Text Converter Workflow with Mermaid\nDESCRIPTION: This Mermaid diagram illustrates the workflow of the Text Converter application, showing the interaction between TextInput and TextTransform nodes.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-flow/README.md#2025-04-22_snippet_2\n\nLANGUAGE: mermaid\nCODE:\n```\ngraph TD\n    Input[TextInput Node] -->|transform| Transform[TextTransform Node]\n    Transform -->|input| Input\n    Transform -->|exit| End[End]\n    Input -->|exit| End\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for BrainyFlow Project\nDESCRIPTION: Specifies the required Python packages and their minimum versions for the BrainyFlow project. The dependencies include BrainyFlow itself (version 0.0.1 or higher), OpenAI's Python client (version 1.0.0 or higher), and PyYAML for YAML processing (version 6.0 or higher).\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-map-reduce/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nbrainyflow>=0.0.1\nopenai>=1.0.0\npyyaml>=6.0 \n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Text Converter Project\nDESCRIPTION: This command installs the required dependencies for the Text Converter project from the requirements.txt file.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-flow/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Visualizing a Nested Order Processing Pipeline with Mermaid Flowchart\nDESCRIPTION: Presents a Mermaid flowchart diagram using subgraphs to visualize the nested structure of the order processing pipeline example. It clearly shows the main `Order Pipeline` containing distinct sub-flows (`Payment Flow`, `Inventory Flow`, `Shipping Flow`), connected sequentially to represent the overall workflow.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/core_abstraction/flow.md#2025-04-22_snippet_8\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n    subgraph order_pipeline[Order Pipeline]\n        subgraph paymentFlow[\"Payment Flow\"]\n            A[Validate Payment] --> B[Process Payment] --> C[Payment Confirmation]\n        end\n\n        subgraph inventoryFlow[\"Inventory Flow\"]\n            D[Check Stock] --> E[Reserve Items] --> F[Update Inventory]\n        end\n\n        subgraph shippingFlow[\"Shipping Flow\"]\n            G[Create Label] --> H[Assign Carrier] --> I[Schedule Pickup]\n        end\n\n        paymentFlow --> inventoryFlow\n        inventoryFlow --> shippingFlow\n    end\n```\n\n----------------------------------------\n\nTITLE: Example Output: Valid Travel Query\nDESCRIPTION: Example conversation demonstrating a successful interaction with the travel advisor chatbot. The user asks about planning a trip to Thailand and receives detailed travel advice in response.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-chat-guardrail/README.md#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nWelcome to the Travel Advisor Chat! Type 'exit' to end the conversation.\n\nYou: Plan my trip to Thailand\n\nTravel Advisor: Thailand offers a blend of vibrant cities, serene beaches, and rich culture. Begin in Bangkok to explore the Grand Palace and local markets. Head north to Chiang Mai for temples and elephant sanctuaries. Fly south to the islands; Phuket or Koh Samui for beaches, diving, and nightlife. Consider visiting during the cool season (November-February) for pleasant weather. Accommodation ranges from budget hostels to luxury resorts. Internal flights, trains, and buses connect major destinations. Don't miss local cuisine, from street food to fine dining. Ensure your passport is valid for six months and consider travel insurance for peace of mind. Enjoy!\n\nYou: exit\n\nGoodbye! Safe travels!\n```\n\n----------------------------------------\n\nTITLE: Completing Document Context Retrieval in Python\nDESCRIPTION: Completes the document retrieval process by storing the retrieved question, filename, and document context in shared data. Returns 'answer' to trigger the answer generation step.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python_demo.ipynb#2025-04-22_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nself.trigger(\"end\")\n            \nquestion, filename = exec_res\nshared[\"current_question\"] = question\nshared[\"relevant_file\"] = filename\nshared[\"context\"] = shared[\"data\"][filename]\nreturn \"answer\"\n```\n\n----------------------------------------\n\nTITLE: Running the Example Application\nDESCRIPTION: Command to execute the main application script.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-tool-database/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for BrainyFlow\nDESCRIPTION: Specifies required Python packages and their minimum versions for the BrainyFlow project. Includes dependencies for async HTTP requests (aiohttp), OpenAI API integration (openai), and web search capabilities (duckduckgo-search).\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-supervisor/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nbrainyflow>=0.0.1\naiohttp>=3.8.0  # For async HTTP requests\nopenai>=1.0.0   # For async LLM calls \nduckduckgo-search>=7.5.2    # For web search\n```\n\n----------------------------------------\n\nTITLE: Package Dependencies for Brainyflow\nDESCRIPTION: Lists required Python package dependencies with version constraints. Includes aiohttp for async HTTP requests and OpenAI's Python client library for async LLM API calls.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-async-basic/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nbrainyflow\naiohttp>=3.8.0  # For async HTTP requests\nopenai>=1.0.0   # For async LLM calls\n```\n\n----------------------------------------\n\nTITLE: Creating Virtual Environment for BrainyFlow Application\nDESCRIPTION: Commands for setting up a Python virtual environment to isolate dependencies for the BrainyFlow application, with instructions for both Unix/Linux and Windows systems.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-hello-world/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key in Bash\nDESCRIPTION: Command to set the OpenAI API key as an environment variable before running the application. This key is required for authenticating with the OpenAI API.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-chat/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=\"your-api-key-here\"\n```\n\n----------------------------------------\n\nTITLE: Defining and Executing Flows in TypeScript\nDESCRIPTION: Illustrates the setup and execution of flows using various Node subclasses, logging details about execution flow stages, and using the call stack function to facilitate debugging.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/visualization_logging.md#2025-04-22_snippet_7\n\nLANGUAGE: TypeScript\nCODE:\n```\n// Define dummy nodes and flows\nclass DataPrepNode extends Node {\n  async prep(memory: Memory): Promise<void> {\n    console.log('Prep: DataPrepNode')\n  }\n}\nclass ValidateDataNode extends Node {\n  async prep(memory: Memory): Promise<void> {\n    console.log('Prep: ValidateDataNode')\n  }\n}\nclass FeatureExtractionNode extends Node {\n  async prep(memory: Memory): Promise<void> {\n    console.log('Prep: FeatureExtractionNode')\n  }\n}\nclass TrainModelNode extends Node {\n  async prep(memory: Memory): Promise<void> {\n    console.log('Prep: TrainModelNode')\n  }\n}\nclass EvaluateModelNode extends Node {\n  async prep(memory: Memory): Promise<void> {\n    // Call the stack inspection function here\n    const stack = getNodeCallStack() // Assuming getNodeCallStack is defined\n    console.log('Call stack inside EvaluateModelNode:', stack)\n  }\n}\nclass ModelFlow extends Flow {}\nclass DataScienceFlow extends Flow {}\n\n// Instantiate and connect\nconst featureNode = new FeatureExtractionNode()\nconst trainNode = new TrainModelNode()\nconst evaluateNode = new EvaluateModelNode()\nfeatureNode.next(trainNode).next(evaluateNode)\n\nconst modelFlow = new ModelFlow(featureNode)\n\nconst dataPrepNode = new DataPrepNode()\nconst validateNode = new ValidateDataNode()\ndataPrepNode.next(validateNode).next(modelFlow)\n\nconst dataScienceFlow = new DataScienceFlow(dataPrepNode)\n\n// Run the flow\nconsole.log('Running data science flow...')\ndataScienceFlow.run({}) // Pass an empty initial memory\n```\n\n----------------------------------------\n\nTITLE: Installing and Running the Application\nDESCRIPTION: Commands to install project dependencies and start the main application.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-mcp/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Sample Structured Resume Output\nDESCRIPTION: Example of the YAML-formatted output produced by the resume parser showing extracted fields like name, email, experience, and skills.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-structured-output/README.md#2025-04-22_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nname: John Smith\nemail: johnsmtih1983@gnail.com\nexperience:\n- title: Sales Manager\n  company: ABC Corporation\n- title: Assistant Manager\n  company: XYZ Industries\n- title: Customer Service Representative\n  company: Fast Solutions Inc\nskills:\n- Microsoft Office: Excel, Word, PowerPoint (Advanced)\n- Customer relationship management (CRM) software\n- Team leadership & management\n- Project management\n- Public speaking\n- Time management\n```\n\n----------------------------------------\n\nTITLE: Example Database Output\nDESCRIPTION: Sample output showing database initialization status and task creation results.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-tool-database/README.md#2025-04-22_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nDatabase Status: Database initialized\nTask Status: Task created successfully\n\nAll Tasks:\n- ID: 1\n  Title: Example Task\n  Description: This is an example task created using BrainyFlow\n  Status: pending\n  Created: 2024-03-02 12:34:56\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for BrainyFlow\nDESCRIPTION: This snippet lists the required Python packages and their minimum versions for the BrainyFlow project. It includes BrainyFlow itself, the Anthropic library, and PyYAML.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-batch/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nbrainyflow>=0.0.1\nanthropic>=0.15.0\npyyaml>=6.0\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for BrainyFlow Project\nDESCRIPTION: A requirements.txt file specifying the minimum versions of required Python packages. It requires brainyflow version 0.0.1 or higher and the OpenAI Python SDK version 1.0.0 or higher.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-chat/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nbrainyflow>=0.0.1\nopenai>=1.0.0\n```\n\n----------------------------------------\n\nTITLE: Running the BrainyFlow Batch Processing Example\nDESCRIPTION: Command to execute the main script of the BrainyFlow batch processing example, which will process the images with different filters.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-batch-flow/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key in Bash\nDESCRIPTION: Command to set the OpenAI API key as an environment variable before running the application. The API key is required for authenticating requests to OpenAI's services.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-chat-guardrail/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=\"your-api-key-here\"\n```\n\n----------------------------------------\n\nTITLE: Defining Project Dependencies with requirements.txt - Python\nDESCRIPTION: This snippet specifies external Python packages and their minimum required versions using the requirements.txt format. Dependencies listed (brainyflow>=0.0.1, openai>=1.0.0, pyyaml>=6.0) are prerequisites for the BrainyFlow project and must be installed via pip before running the application. The file uses the standard requirements syntax, where each line defines a package and its version constraint.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-workflow/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: requirements\nCODE:\n```\nbrainyflow>=0.0.1\\nopenai>=1.0.0\\npyyaml>=6.0\n```\n\n----------------------------------------\n\nTITLE: Installing dependencies with pip\nDESCRIPTION: Install required Python dependencies from the requirements.txt file using pip.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-tool-pdf-vision/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Testing API Configuration\nDESCRIPTION: Command to verify API key setup and test LLM and web search functionality\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-supervisor/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython utils.py\n```\n\n----------------------------------------\n\nTITLE: Setting the OpenAI API Key as Environment Variable in Bash\nDESCRIPTION: This Bash command sets the OPENAI_API_KEY environment variable, which is required for the workflow to authenticate requests to the OpenAI API. Replace 'your_api_key_here' with a valid OpenAI API key before executing. This variable must be set in the user's shell environment prior to running the workflow scripts.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-workflow/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=your_api_key_here\n```\n\n----------------------------------------\n\nTITLE: Executing a Brainyflow MapReduce Workflow in Python\nDESCRIPTION: This snippet shows how to execute a MapReduce-style summarization pipeline in Python using an asyncio-compatible Brainyflow flow. It sets up a memory object with input files, triggers the flow's execution, and prints out both the intermediate and final results. Dependencies include asyncio and a compatible Brainyflow implementation. The memory parameter stores input files and collects the resulting summaries. The output consists of both individual file summaries and a final aggregate summary.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/design_pattern/mapreduce.md#2025-04-22_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# --- Execution ---\\nasync def main():\\n    memory = {\\n        \\\"files\\\": {\\n            \\\"file1.txt\\\": \\\"Alice was beginning to get very tired of sitting by her sister...\\\",\\n            \\\"file2.txt\\\": \\\"The quick brown fox jumps over the lazy dog.\\\",\\n            \\\"file3.txt\\\": \\\"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\\\",\\n        }\\n    }\\n    await map_reduce_flow.run(memory) # Pass memory object\\n    print('\\\\n--- MapReduce Complete ---')\\n    print(\\\"Individual Summaries:\\\", memory.get(\\\"file_summaries\\\"))\\n    print(\\\"\\\\nFinal Summary:\\n\\\", memory.get(\\\"final_summary\\\"))\\n\\nif __name__ == \\\"__main__\\\":\\n    asyncio.run(main())\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies\nDESCRIPTION: Lists required Python packages and their version constraints. Specifies brainyflow version 0.0.1 or higher, openai version 1.0.0 or higher, and fastmcp with no version constraint.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-mcp/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\nbrainyflow>=0.0.1\nopenai>=1.0.0\nfastmcp\n```\n\n----------------------------------------\n\nTITLE: Package Dependencies Definition\nDESCRIPTION: Specifies minimum version requirements for Python packages brainyflow and openai. Uses standard package version specification syntax with >= operator.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-structured-output/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nbrainyflow>=0.0.1\nopenai>=1.0.0\n```\n\n----------------------------------------\n\nTITLE: Setting API Key Environment Variables in Bash\nDESCRIPTION: Sets the necessary API keys for SerpAPI and OpenAI as environment variables using the `export` command in a bash shell. Replace `'your-serpapi-key'` and `'your-openai-key'` with actual keys.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-tool-search/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport SERPAPI_API_KEY='your-serpapi-key'\nexport OPENAI_API_KEY='your-openai-key'\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with pip\nDESCRIPTION: Command to install required Python packages from requirements.txt file.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-tool-crawler/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Global Memory Structure Definition in Python\nDESCRIPTION: Definition of the shared memory structure using a Python dictionary to store key-value pairs.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-hello-world/docs/design.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nshared = {\n    \"key\": \"value\"\n}\n```\n\n----------------------------------------\n\nTITLE: Exporting OPENAI_API_KEY as a System Environment Variable in Bash\nDESCRIPTION: This bash command sets the OPENAI_API_KEY as an environment variable in the user's shell session so that it can be accessed by subprocesses and Python code as needed. Input: none; Output: the OPENAI_API_KEY available in environment for current session. Limitation: Only persists for the session or scope it was exported in.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-tool-embeddings/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=your_api_key_here\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with pip\nDESCRIPTION: Command to install required Python packages from requirements.txt file\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-supervisor/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Testing LLM and Web Search Functionality\nDESCRIPTION: Command to run the utility script that verifies both the LLM call and web search features are working properly.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-agent/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython utils.py\n```\n\n----------------------------------------\n\nTITLE: Package Dependencies Definition\nDESCRIPTION: Specifies the minimum required versions for the brainyflow and openai Python packages. The configuration ensures compatibility with BrainyFlow version 0.0.1 or higher and OpenAI's Python client library version 1.0.0 or higher.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-chat-guardrail/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: txt\nCODE:\n```\nbrainyflow>=0.0.1\nopenai>=1.0.0\n```\n\n----------------------------------------\n\nTITLE: Importing BrainyFlow in Browser via Module Script (HTML)\nDESCRIPTION: Demonstrates how to import the BrainyFlow library directly into an HTML file for browser usage using an ES module script tag. It imports the library from the unpkg CDN and shows a basic instantiation of `brainyflow.Node`. Requires a browser that supports ES modules.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/installation.md#2025-04-22_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<script type=\"module\">\n  import * as brainyflow from 'https://unpkg.com/brainyflow@latest/dist/brainyflow.js'\n\n  new brainyflow.Node(...)\n</script>\n```\n\n----------------------------------------\n\nTITLE: Running the Taboo Game Application\nDESCRIPTION: Command to execute the main Python script that starts the Taboo game with the asynchronous agents.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-multi-agent/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key\nDESCRIPTION: Environment variable configuration for OpenAI API authentication.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-tool-crawler/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY='your-api-key'\n```\n\n----------------------------------------\n\nTITLE: Detecting Browser Environment (JavaScript/TypeScript)\nDESCRIPTION: This code snippet provides a simple check to detect if the script is running in a browser environment. It does this by testing if the global `window` object is defined (`typeof window !== 'undefined'`).\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/design.md#2025-04-22_snippet_11\n\nLANGUAGE: javascript\nCODE:\n```\ntypeof window !== 'undefined'\n```\n\n----------------------------------------\n\nTITLE: Setting OPENAI_API_KEY in a .env File in Bash\nDESCRIPTION: This snippet provides an example of storing the OpenAI API key inside a .env file using simple assignment syntax. The .env file can be loaded by environment variable management tools in Python (such as python-dotenv) to securely provide credentials to the application. Input: none; Output: .env file containing OPENAI_API_KEY. Limitation: The .env file must be referenced/loaded by the application.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-tool-embeddings/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nOPENAI_API_KEY=your_api_key_here\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key in Bash\nDESCRIPTION: Command to set the OpenAI API key as an environment variable for authentication.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-rag/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=\"your-api-key-here\"\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Batch Translation Project\nDESCRIPTION: Command to install the necessary dependencies for the project using pip and the requirements.txt file.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-batch/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for BrainyFlow Application\nDESCRIPTION: Command to install all required packages for the BrainyFlow application from the requirements.txt file using pip.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-hello-world/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running the Web Crawler\nDESCRIPTION: Command to execute the main crawler script which starts the web crawling and analysis process.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-tool-crawler/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Processing Output Demonstration\nDESCRIPTION: Sample output showing the execution of sequential and parallel processing, including timing comparisons and results formatting.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-parallel-batch/README.md#2025-04-22_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n=== Running Sequential ===\nTrigger: Triggering summary for 3 files.\nProcessor: Summarizing file1.txt (Index 0)...\nProcessor: Summarizing file2.txt (Index 1)...\nProcessor: Summarizing file3.txt (Index 2)...\n\n=== Running Parallel ===\nTrigger: Triggering summary for 3 files.\nProcessor: Summarizing file1.txt (Index 0)...\nProcessor: Summarizing file2.txt (Index 1)...\nProcessor: Summarizing file3.txt (Index 2)...\n\n--- Results ---\nSequential Summaries: {'file1.txt': 'Summarized(13 chars)', 'file2.txt': 'Summarized(13 chars)', 'file3.txt': 'Summarized(13 chars)'}\nParallel Summaries:   {'file1.txt': 'Summarized(13 chars)', 'file2.txt': 'Summarized(13 chars)', 'file3.txt': 'Summarized(13 chars)'}\nSequential took: 3.00 seconds\nParallel took:   1.00 seconds\n```\n\n----------------------------------------\n\nTITLE: Installing BrainyFlow using pnpm/npm/yarn (Bash)\nDESCRIPTION: Installs the BrainyFlow TypeScript/JavaScript package using common Node.js package managers (pnpm, npm, or yarn). These commands download and install the `brainyflow` package from the npm registry into your project's dependencies. Requires Node.js and the respective package manager to be installed.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/installation.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npnpm add brainyflow\n# or\nnpm install brainyflow\n# or\nyarn add brainyflow\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies - Pip Command\nDESCRIPTION: Command to install required project dependencies from requirements.txt file.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-node/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key - Bash Configuration\nDESCRIPTION: Command to set the OpenAI API key as an environment variable for the application.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-chat-memory/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=\"your-api-key-here\"\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for BrainyFlow Communication Example\nDESCRIPTION: Command to install the required dependencies for the BrainyFlow communication example using pip and the requirements.txt file. This ensures all necessary packages are available before running the application.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-communication/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running the Default Test Problem\nDESCRIPTION: Command to execute the main script which demonstrates the majority voting system with a default problem.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-majority-vote/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Installing and Running the LLM Streaming Demo\nDESCRIPTION: Commands to install dependencies and run the main application. This sets up the required environment and launches the StreamNode demonstration.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-llm-streaming/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for BrainyFlow\nDESCRIPTION: Specifies the required Python package dependencies with minimum version requirements. Includes brainyflow itself (version 0.0.1 or higher), the OpenAI Python client (version 1.0.0 or higher), and PyYAML (version 6.0 or higher).\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-multi-agent/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nbrainyflow>=0.0.1\nopenai>=1.0.0\npyyaml>=6.0 \n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key for Environment Configuration\nDESCRIPTION: Command to set the OpenAI API key as an environment variable, which is required for the LLM-based agents to function.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-multi-agent/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=your_api_key_here\n```\n\n----------------------------------------\n\nTITLE: Specifying Minimum Version for Brainyflow Package in Requirements File\nDESCRIPTION: A requirements.txt entry that specifies the brainyflow package should be at least version 0.1.0. This is commonly used in Python projects to declare package dependencies.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-flow/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nbrainyflow>=0.1.0\n```\n\n----------------------------------------\n\nTITLE: Switching from Fake to Real OpenAI Streaming\nDESCRIPTION: Code modification needed to use real OpenAI streaming instead of the default fake streaming responses. This requires editing the main.py file to change the function call.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-llm-streaming/README.md#2025-04-22_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Change this line:\nchunks = fake_stream_llm(prompt)\n# To this:\nchunks = stream_llm(prompt)\n```\n\n----------------------------------------\n\nTITLE: Batch Processing Documentation HTML\nDESCRIPTION: HTML markup that documents the batch processing functionality including prep(), exec() and post() methods\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python_demo.ipynb#2025-04-22_snippet_5\n\nLANGUAGE: html\nCODE:\n```\n<!-- Title -->\n<p style=\"font-family: Arial, sans-serif; font-size: 24px; font-weight: bold; color: #333; \">\n  3. Batch\n</p>\n\n<!-- Description of Batch processing -->\n<p style=\"font-family: Arial, sans-serif; font-size: 16px; color: #333; margin-bottom: 16px;\">\n  <strong>Batch</strong> helps repeat the same work multiple items. \n  Instead of calling <code style=\"background: #f2f2f2; padding: 2px 4px; border-radius: 3px;\">exec()</code> once, a Batch Node calls \n  <code style=\"background: #f2f2f2; padding: 2px 4px; border-radius: 3px;\">exec()</code> \n  for each item in a list from <code style=\"background: #f2f2f2; padding: 2px 4px; border-radius: 3px;\">prep()</code>. \n</p>\n<p style=\"font-family: Arial, sans-serif; font-size: 16px; color: #333; margin-bottom: 16px;\">\n  Think of it as \"item-by-item\" processing:\n</p>\n\n<!-- Bullet points -->\n<ul style=\"font-family: Arial, sans-serif; font-size: 16px; color: #333; list-style-type: disc; padding-left: 20px;\">\n  <li style=\"margin-bottom: 16px;\">\n    <code>prep(shared)</code>: Return a list of items.\n  </li>\n  <li style=\"margin-bottom: 16px;\">\n    <code>exec(item)</code>: Called once per item.\n  </li>\n  <li style=\"margin-bottom: 16px;\">\n    <code>post(shared, item_list, results_list)</code>: Combines all results.\n  </li>\n</ul>\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies with Version Requirements\nDESCRIPTION: A requirements.txt style list of Python packages with their minimum version requirements. It specifies dependencies for BrainyFlow including numpy for numerical operations, faiss-cpu for vector search functionality, and OpenAI's API library.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-chat-memory/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nbrainyflow>=0.0.5\nnumpy>=1.20.0\nfaiss-cpu>=1.7.0\nopenai>=1.0.0\n```\n\n----------------------------------------\n\nTITLE: Installing and Running the Image Processor\nDESCRIPTION: Commands for setting up dependencies and executing the main program.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-parallel-batch-flow/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Installing and Running BrainyFlow Example\nDESCRIPTION: Commands to install BrainyFlow package and run the main demonstration script.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-parallel-batch/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install brainyflow\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for BrainyFlow\nDESCRIPTION: Specifies required Python packages and their minimum versions for the BrainyFlow project. Includes aiohttp for HTTP requests, OpenAI SDK for LLM integration, and DuckDuckGo search package for web search capabilities.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-agent/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nbrainyflow>=0.0.1\naiohttp>=3.8.0  # For HTTP requests\nopenai>=1.0.0   # For LLM calls \nduckduckgo-search>=7.5.2    # For web search\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key Environment Variable\nDESCRIPTION: Command to set the OpenAI API key as an environment variable, which is required for the LLM-based resume evaluation.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-map-reduce/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=your_api_key_here\n```\n\n----------------------------------------\n\nTITLE: Specifying OpenAI Library Dependency Version\nDESCRIPTION: Defines the minimum required version of the OpenAI library as 1.0.0 or higher.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-node/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nbrainyflow\nopenai>=1.0.0\n```\n\n----------------------------------------\n\nTITLE: Visualizing Project Directory Structure\nDESCRIPTION: Illustrates the directory and file layout of the `python-tool-search` project. Key components include `tools` for search/parsing, `utils` for LLM calls, `nodes.py` and `flow.py` for BrainyFlow configuration, and `main.py` as the entry point.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-tool-search/README.md#2025-04-22_snippet_3\n\nLANGUAGE: text\nCODE:\n```\npython-tool-search/\n├── tools/\n│   ├── search.py      # SerpAPI search functionality\n│   └── parser.py      # Result analysis using LLM\n├── utils/\n│   └── call_llm.py    # LLM API wrapper\n├── nodes.py           # BrainyFlow nodes\n├── flow.py           # Flow configuration\n├── main.py           # Main script\n└── requirements.txt   # Dependencies\n```\n\n----------------------------------------\n\nTITLE: Running Research Agent with Default Question\nDESCRIPTION: Command to execute the main script with the default question about Nobel Prize winners.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-agent/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Setting Up API Key for Anthropic LLM\nDESCRIPTION: Bash command to set the ANTHROPIC_API_KEY environment variable for authentication with the Anthropic API.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-batch/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport ANTHROPIC_API_KEY=\"your-api-key-here\"\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Command to install the necessary project dependencies from requirements.txt file.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-thinking/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies\nDESCRIPTION: Lists required Python packages with their minimum version requirements. Includes core dependencies like requests, beautifulsoup4, and openai for content analysis functionality.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-tool-crawler/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nbrainyflow>=0.1.0\nrequests>=2.31.0\nbeautifulsoup4>=4.12.0\nopenai>=1.0.0  # for content analysis\n```\n\n----------------------------------------\n\nTITLE: Project Directory Structure Definition\nDESCRIPTION: Shows the file and directory organization for the batch processing implementation.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-batch-node/README.md#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\npython-batch-node/\n├── README.md\n├── requirements.txt\n├── data/\n│   └── sales.csv      # Sample large CSV file\n├── main.py            # Entry point\n├── flow.py            # Flow definition\n└── nodes.py           # Node implementations (Trigger, Processor, Aggregator)\n```\n\n----------------------------------------\n\nTITLE: Running the Batch Translation Process\nDESCRIPTION: Command to execute the main Python script that initiates the batch translation process.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-batch/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key\nDESCRIPTION: Command to set the OpenAI API key as an environment variable\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-supervisor/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=\"your-api-key-here\"\n```\n\n----------------------------------------\n\nTITLE: Mermaid Flowchart for MajorityVoteNode\nDESCRIPTION: A simple Mermaid diagram showing the MajorityVoteNode component which is central to the implementation.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-majority-vote/README.md#2025-04-22_snippet_4\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n    mv[MajorityVoteNode] \n```\n\n----------------------------------------\n\nTITLE: Virtual Environment Setup Commands\nDESCRIPTION: Commands for creating and activating a Python virtual environment for the project.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-tool-database/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n```\n\n----------------------------------------\n\nTITLE: Running Custom Reasoning Problem\nDESCRIPTION: Command to execute the main script with a custom reasoning problem as an argument.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-thinking/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython main.py --\"Your complex reasoning problem here\"\n```\n\n----------------------------------------\n\nTITLE: Dependencies Installation Command\nDESCRIPTION: Command to install project dependencies from requirements.txt file.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-tool-database/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Python Execution Command\nDESCRIPTION: Command to run the main application script.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-batch-node/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Specifying BrainyFlow Package Version\nDESCRIPTION: This snippet defines the required version of the brainyflow package as 0.1.0. It follows the standard pip requirements format for specifying exact package versions.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-communication/requirements.txt#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nbrainyflow==0.1.0\n```\n\n----------------------------------------\n\nTITLE: Flow Diagram Using Mermaid\nDESCRIPTION: Visual representation of the flow between three nodes using a mermaid flowchart.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-hello-world/docs/design.md#2025-04-22_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart TD\n    firstNode[First Node] --> secondNode[Second Node]\n    secondNode --> thirdNode[Third Node]\n```\n\n----------------------------------------\n\nTITLE: Running the Text Converter Application\nDESCRIPTION: This command executes the main Python script to start the Text Converter application.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-flow/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Print Summary Python Code\nDESCRIPTION: Simple print statement that outputs a summary from a shared dictionary\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python_demo.ipynb#2025-04-22_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Summary:\", shared[\"summary\"])\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for BrainyFlow Resume Processor\nDESCRIPTION: Command to install the required Python dependencies for the resume qualification application using pip package manager.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-map-reduce/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key in Bash\nDESCRIPTION: Command to set the OpenAI API key as an environment variable for authentication.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-mcp/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=\"your-api-key-here\"\n```\n\n----------------------------------------\n\nTITLE: Testing Input Validation in Python with pytest\nDESCRIPTION: This snippet demonstrates how to test a BrainyFlow node's handling of invalid inputs using pytest. It uses parameterized tests to check various invalid input types and ensures the node handles them gracefully without unhandled exceptions.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/testing.md#2025-04-22_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Requires: pip install pytest pytest-asyncio\nimport pytest\n# from brainyflow import Node, Memory # Assuming imports\n# from my_nodes import MyNodeThatValidates # Your node\n\n@pytest.mark.parametrize(\"invalid_input\", [None, \"\", {}, [], {\"wrong_key\": 1}])\n@pytest.mark.asyncio\nasync def test_node_handles_invalid_input(invalid_input):\n    \"\"\"Tests if the node handles various invalid inputs gracefully.\"\"\"\n    node = MyNodeThatValidates() # Node that should validate memory.input_data\n    memory = {\"input_data\": invalid_input} # Pass invalid data\n\n    # Expect the node to run without unhandled exceptions\n    # and potentially set an error state or default output\n    await node.run(memory)\n\n    # Example assertions: Check for an error flag or a specific state\n    assert memory.get(\"error_message\") is not None or memory.get(\"status\") == \"validation_failed\"\n    # Or assert that a default value was set\n    # assert memory.get(\"output\") == \"default_value\"\n```\n\n----------------------------------------\n\nTITLE: Running the Resume Qualification Application\nDESCRIPTION: Command to execute the main Python script that starts the resume qualification workflow.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-map-reduce/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Splitting Text by Fixed Size in Python\nDESCRIPTION: This Python function splits a given text into chunks of a fixed size based on character count. It takes the entire text and an optional chunk size parameter, returning a list of substrings. This method does not account for semantic or sentence boundaries, which can result in awkward breaks in the text.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/utility_function/chunking.md#2025-04-22_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef fixed_size_chunk(text: str, chunk_size: int = 100) -> list[str]:\n    \"\"\"Splits text into fixed-size chunks based on character count.\"\"\"\n    chunks = []\n    for i in range(0, len(text), chunk_size):\n        chunks.append(text[i : i + chunk_size])\n    return chunks\n\n# Example:\n# text = \"This is a sample text to demonstrate fixed-size chunking.\"\n# chunks = fixed_size_chunk(text, 20)\n# print(chunks)\n# Output: ['This is a sample tex', 't to demonstrate fix', 'ed-size chunking.']\n```\n\n----------------------------------------\n\nTITLE: Running the BrainyFlow Hello World Example\nDESCRIPTION: Command to execute the main.py file which serves as the entry point for the BrainyFlow application example.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-hello-world/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Running Custom Agent Query\nDESCRIPTION: Command to run the agent with a custom question using the -- prefix\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-supervisor/README.md#2025-04-22_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython main.py --\"What is quantum computing?\"\n```\n\n----------------------------------------\n\nTITLE: Running Default Agent Query\nDESCRIPTION: Command to run the agent with the default Nobel Prize winners question\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-supervisor/README.md#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Example Output of Text Converter Application\nDESCRIPTION: This code block demonstrates the expected output and user interaction when running the Text Converter application, showcasing the menu options and text transformation.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-flow/README.md#2025-04-22_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\nWelcome to Text Converter!\n=========================\n\nEnter text to convert: BrainyFlow is a 100-line LLM framework\n\nChoose transformation:\n1. Convert to UPPERCASE\n2. Convert to lowercase\n3. Reverse text\n4. Remove extra spaces\n5. Exit\n\nYour choice (1-5): 1\n\nResult: BrainyFlow IS A 100-LINE LLM FRAMEWORK\n\nConvert another text? (y/n): n\n\nThank you for using Text Converter!\n```\n\n----------------------------------------\n\nTITLE: Executing the Main Python Script in Bash\nDESCRIPTION: Runs the main application script (`main.py`) using the Python interpreter from the command line. This command starts the web search and analysis tool.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-tool-search/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Implementing Token Bucket Rate Limiting (pyrate_limiter) in Python\nDESCRIPTION: Illustrates applying a token bucket rate limiting strategy using the `pyrate_limiter` library. It defines a `Rate` (10 requests per minute) and uses a `Limiter` with the `@limiter.ratelimit` decorator on an asynchronous function (`call_api`). Depends on the `pyrate_limiter` library.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/docs/guides/throttling.md#2025-04-22_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom pyrate_limiter import Duration, Rate, Limiter\n\n# 10 requests per minute\nrate = Rate(10, Duration.MINUTE)\nlimiter = Limiter(rate)\n\n@limiter.ratelimit(\"api_calls\")\nasync def call_api():\n    # Your API call here\n    pass\n\n```\n\n----------------------------------------\n\nTITLE: Setting Up API Key for Anthropic\nDESCRIPTION: Bash command to set the ANTHROPIC_API_KEY environment variable required for authenticating with Anthropic's API.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-majority-vote/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport ANTHROPIC_API_KEY=\"your-api-key-here\"\n```\n\n----------------------------------------\n\nTITLE: Setting API Key Environment Variable\nDESCRIPTION: Command to set up the Anthropic API key as an environment variable for authentication.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-thinking/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport ANTHROPIC_API_KEY=\"your-api-key-here\"\n```\n\n----------------------------------------\n\nTITLE: Setting OpenAI API Key in Environment\nDESCRIPTION: Command to set the OpenAI API key as an environment variable, which is required when using real OpenAI streaming functionality.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-llm-streaming/README.md#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport OPENAI_API_KEY=\"your-api-key-here\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Async User Approval Node in Python\nDESCRIPTION: This code defines the GetApproval node that asynchronously collects user confirmation and returns a decision action ('accept' or 'retry'). It demonstrates how to handle user interaction in an asynchronous workflow.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-async-basic/README.md#2025-04-22_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nasync def post_async(self, shared, prep_res, suggestion):\n    # Async user input\n    answer = await get_user_input(\n        f\"Accept {suggestion}? (y/n): \"\n    )\n    return \"accept\" if answer == \"y\" else \"retry\"\n```\n\n----------------------------------------\n\nTITLE: Defining the SharedStore Type Alias (TypeScript)\nDESCRIPTION: This type alias defines `SharedStore` as a generic `Record<string, any>`. It represents a basic key-value object structure intended for use as memory stores shared across different parts of the flow execution.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/typescript/design.md#2025-04-22_snippet_6\n\nLANGUAGE: typescript\nCODE:\n```\nSharedStore: Record<string, any>\n```\n\n----------------------------------------\n\nTITLE: Eric Raymond's Quote About Learning Lisp\nDESCRIPTION: A quote from Eric Raymond's essay \"How to Become a Hacker\" discussing the value of learning Lisp for its profound impact on programming skills, even if one doesn't use Lisp extensively afterward.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/data/PaulGrahamEssaysLarge/avg.txt#2025-04-22_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n  Lisp is worth learning for the profound enlightenment experience\n  you will have when you finally get it; that experience will make\n  you a better programmer for the rest of your days, even if you\n  never actually use Lisp itself a lot.\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies and Running the Application\nDESCRIPTION: Commands to install the required dependencies using pip and run the main application script. This prepares the environment and launches the travel advisor chat.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-chat-guardrail/README.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Customizing extraction prompts\nDESCRIPTION: Customize the text extraction process by modifying the prompt in the initial `memory` object.\nSOURCE: https://github.com/zvictor/brainyflow/blob/main/cookbook/python-tool-pdf-vision/README.md#2025-04-22_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nshared = {\n    \"pdf_path\": \"your_file.pdf\",\n    \"extraction_prompt\": \"Your custom prompt here\"\n}\n```"
  }
]
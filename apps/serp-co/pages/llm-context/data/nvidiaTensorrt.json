[
  {
    "owner": "nvidia",
    "repo": "tensorrt",
    "content": "TITLE: Setting Maximum Batch Size for TensorRT Inference\nDESCRIPTION: Defines the maximum batch size for TensorRT inference which determines the upper limit of samples that can be processed simultaneously during inference.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/BERT-TRT-FP16.ipynb#2025-04-06_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Maximum TensorRT inference batch size\nBATCH_SIZE = 128\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies and Setting Up Environment\nDESCRIPTION: This snippet imports necessary libraries, clears the Keras backend session, and sets up folders for saving models.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/notebooks/simple_network_quantize_partial.ipynb#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport tensorflow as tf\nfrom tensorflow_quantization import quantize_model, QuantizationSpec\nfrom tensorflow_quantization.custom_qdq_cases import ResNetV1QDQCase\nimport tiny_resnet\nimport os\nfrom tensorflow_quantization import utils\n\ntf.keras.backend.clear_session()\n\n# Create folders to save TF and ONNX models\nassets = utils.CreateAssetsFolders(os.path.join(os.getcwd(), \"tutorials\"))\nassets.add_folder(\"simple_network_quantize_partial\")\n```\n\n----------------------------------------\n\nTITLE: TensorRT FP16 Inference Function for BERT Q&A\nDESCRIPTION: This function performs inference using a TensorRT FP16 engine for BERT question answering. It handles input processing, GPU memory transfers, inference execution, and post-processing of results.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/Q-and-A.ipynb#2025-04-06_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndef inference_FP16(trt_context, d_inputs, h_output, d_output, features, tokens):\n    #global h_output\n    context = trt_context\n    \n    _NetworkOutput = collections.namedtuple(  # pylint: disable=invalid-name\n            \"NetworkOutput\",\n            [\"start_logits\", \"end_logits\", \"feature_index\"])\n    networkOutputs = []\n\n    eval_time_elapsed = 0\n    for feature_index, feature in enumerate(features):\n        # Copy inputs\n        input_ids_batch = np.repeat(np.expand_dims(feature.input_ids, 0), 1, axis=0)\n        segment_ids_batch = np.repeat(np.expand_dims(feature.segment_ids, 0), 1, axis=0)\n        input_mask_batch = np.repeat(np.expand_dims(feature.input_mask, 0), 1, axis=0)\n\n        input_ids = cuda.register_host_memory(np.ascontiguousarray(input_ids_batch.ravel()))\n        segment_ids = cuda.register_host_memory(np.ascontiguousarray(segment_ids_batch.ravel()))\n        input_mask = cuda.register_host_memory(np.ascontiguousarray(input_mask_batch.ravel()))\n\n        eval_start_time = time.time()\n        cuda.memcpy_htod_async(d_inputs[0], input_ids, stream)\n        cuda.memcpy_htod_async(d_inputs[1], segment_ids, stream)\n        cuda.memcpy_htod_async(d_inputs[2], input_mask, stream)\n\n        # Setup tensor address\n        bindings = [int(d_inputs[i]) for i in range(3)] + [int(d_output)]\n\n        for i in range(engine.num_io_tensors):\n            context.set_tensor_address(engine.get_tensor_name(i), bindings[i])\n\n        # Run inference\n        trt_context.execute_async_v3(stream_handle=stream.handle)\n        # Synchronize the stream\n        stream.synchronize()\n        eval_time_elapsed += (time.time() - eval_start_time)\n\n        # Transfer predictions back from GPU\n        cuda.memcpy_dtoh_async(h_output, d_output, stream)\n        stream.synchronize()\n\n        for index, batch in enumerate(h_output):\n            # Data Post-processing\n            networkOutputs.append(_NetworkOutput(\n                start_logits = np.array(batch.squeeze()[:, 0]),\n                end_logits = np.array(batch.squeeze()[:, 1]),\n                feature_index = feature_index\n                ))\n\n    eval_time_elapsed /= len(features)\n\n    # The total number of n-best predictions to generate in the nbest_predictions.json output file\n    n_best_size = 20\n\n    # The maximum length of an answer that can be generated. This is needed \n    #  because the start and end predictions are not conditioned on one another\n    max_answer_length = 30\n\n    prediction, nbest_json, scores_diff_json = dp.get_predictions(tokens, features,\n        networkOutputs, n_best_size, max_answer_length)\n\n    return eval_time_elapsed, prediction, nbest_json\n```\n\n----------------------------------------\n\nTITLE: Building a TensorRT Engine with Multiple Optimization Profiles Using Polygraphy\nDESCRIPTION: Command to convert an ONNX model to a TensorRT engine with three separate optimization profiles specifying different minimum, optimum, and maximum input shapes for a tensor named X.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/convert/03_dynamic_shapes_in_tensorrt/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy convert dynamic_identity.onnx -o dynamic_identity.engine \\\n    --trt-min-shapes X:[1,3,28,28] --trt-opt-shapes X:[1,3,28,28] --trt-max-shapes X:[1,3,28,28] \\\n    --trt-min-shapes X:[1,3,28,28] --trt-opt-shapes X:[4,3,28,28] --trt-max-shapes X:[32,3,28,28] \\\n    --trt-min-shapes X:[128,3,28,28] --trt-opt-shapes X:[128,3,28,28] --trt-max-shapes X:[128,3,28,28]\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for TensorRT BERT Q&A\nDESCRIPTION: This snippet imports necessary libraries and modules for running BERT question answering models with TensorRT and PyTorch. It sets up paths, imports custom helpers, and initializes TensorRT logger.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/Q-and-A.ipynb#2025-04-06_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport sys\nsys.path.append('/workspace/TensorRT/demo/BERT')\n\nimport ipywidgets as widgets\nimport tensorrt as trt;\nTRT_VERSION = trt.__version__\nprint(\"TensorRT version: \", TRT_VERSION)\n\nimport time\nimport json\nimport ctypes\nimport argparse\nimport collections\nimport numpy as np\nimport tensorrt as trt\nimport pycuda.driver as cuda\nimport pycuda.autoinit\n\nfrom helpers import tokenization as tokenization\nfrom helpers import data_processing as dp\n\nTRT_LOGGER = trt.Logger(trt.Logger.INFO)\n```\n\n----------------------------------------\n\nTITLE: Setting up PyTorch BERT Model for Q&A\nDESCRIPTION: This code sets up a PyTorch BERT model for question answering tasks. It loads a pre-trained SpanBERT large model and creates pipelines for both CPU and GPU inference using the Hugging Face Transformers library.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/Q-and-A.ipynb#2025-04-06_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nfrom transformers import BertForQuestionAnswering, AutoTokenizer\n\n#modelname = 'deepset/bert-base-cased-squad2'\nmodelname = 'mrm8488/spanbert-large-finetuned-squadv2'\nmodel = BertForQuestionAnswering.from_pretrained(modelname)\n\nfrom transformers import pipeline\nnlp = pipeline('question-answering', model=model, tokenizer=\"SpanBERT/spanbert-large-cased\")\n\n\nmodel_gpu = BertForQuestionAnswering.from_pretrained(modelname).cuda()\nnlp_gpu = pipeline('question-answering', model=model_gpu, tokenizer=\"SpanBERT/spanbert-large-cased\", device=0)\n```\n\n----------------------------------------\n\nTITLE: Debugging TensorRT Engine Builds\nDESCRIPTION: Command to repeatedly build TensorRT engines with FP16 precision, comparing results against golden outputs and saving tactic replay files for analysis.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/debug/01_debugging_flaky_trt_tactics/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy debug build identity.onnx --fp16 --save-tactics replay.json \\\n    --artifacts-dir replays --artifacts replay.json --until=10 \\\n    --check polygraphy run polygraphy_debug.engine --trt --load-outputs golden.json\n```\n\n----------------------------------------\n\nTITLE: Building BERT TensorRT FP16 Engine from NGC Checkpoint\nDESCRIPTION: Converts the BERT model checkpoint to an optimized TensorRT engine with FP16 precision, specifying batch sizes and sequence length for optimal inference performance.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/BERT-TRT-FP16.ipynb#2025-04-06_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Build BERT TensorRT FP16 model from NGC checkpoint\n!python3 ../builder.py -m models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/model.ckpt -w 40000 -o engines_$TRT_VERSION/bert_large_384.engine -b 1 -b $BATCH_SIZE -s 384 --fp16 -c models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1\n```\n\n----------------------------------------\n\nTITLE: Applying Quantization-Aware Training to ResNet50 V1 in TensorFlow\nDESCRIPTION: This snippet applies quantization-aware training to the ResNet50 V1 model using a custom quantization function. It uses a specific QDQ case for ResNet V1 architecture.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/example_resnet50v1.ipynb#2025-04-06_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nq_model = quantize_model(model, custom_qdq_cases=[ResNetV1QDQCase()])\n```\n\n----------------------------------------\n\nTITLE: Saving Quantized INT8 Model and Converting to ONNX\nDESCRIPTION: Saves the fine-tuned quantized model in TensorFlow's SavedModel format and converts it to ONNX format. The TensorFlow session is then cleared to free resources.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/getting_started.ipynb#2025-04-06_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# save TF INT8 original model\ntf.keras.models.save_model(quantized_model, assets.example.int8_saved_model)\n\n# Convert INT8 model to ONNX\nutils.convert_saved_model_to_onnx(saved_model_dir = assets.example.int8_saved_model, onnx_model_path = assets.example.int8_onnx_model)\n\ntf.keras.backend.clear_session()\n```\n\n----------------------------------------\n\nTITLE: TensorRT INT8 Inference Function for BERT Q&A\nDESCRIPTION: This function performs inference using a TensorRT INT8 engine for BERT question answering. It handles input processing, dynamic shape setting, GPU memory transfers, inference execution, and post-processing of results.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/Q-and-A.ipynb#2025-04-06_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef inference_INT8(trt_context, d_inputs, h_output, d_output, features, tokens):\n    #global h_output\n    context = trt_context\n    \n    _NetworkOutput = collections.namedtuple(  # pylint: disable=invalid-name\n            \"NetworkOutput\",\n            [\"start_logits\", \"end_logits\", \"feature_index\"])\n    networkOutputs = []\n\n    eval_time_elapsed = 0\n    for feature_index, feature in enumerate(features):\n        # Copy inputs\n        B = 1\n        S = np.sum(feature.input_mask)\n        input_ids = feature.input_ids[0:S]\n        segment_ids = feature.segment_ids[0:S]\n        cu_seq_lens = np.array([0, S], dtype=np.int32)\n\n        first_tensor_name = engine.get_tensor_name(0)\n        second_tensor_name = engine.get_tensor_name(1)\n        third_tensor_name = engine.get_tensor_name(2)\n        forth_tensor_name = engine.get_tensor_name(3)\n\n        if context.get_tensor_shape(first_tensor_name)[0] != S:\n            context.set_input_shape(first_tensor_name, (S,))\n        if context.get_tensor_shape(second_tensor_name)[0] != S:\n            context.set_input_shape(second_tensor_name, (S,))\n        if context.get_tensor_shape(third_tensor_name)[0] != 2:\n            context.set_input_shape(third_tensor_name, (2,))\n        if context.get_tensor_shape(forth_tensor_name)[0] != S:\n            context.set_input_shapee(forth_tensor_name, (S,))\n\n        h_input_ids = cuda.register_host_memory(np.ascontiguousarray(input_ids.ravel()))\n        h_segment_ids = cuda.register_host_memory(np.ascontiguousarray(segment_ids.ravel()))\n        h_cu_seq_lens = cuda.register_host_memory(np.ascontiguousarray(cu_seq_lens.ravel()))\n\n        eval_start_time = time.time()\n        cuda.memcpy_htod_async(d_inputs[0], h_input_ids, INT8_stream)\n        cuda.memcpy_htod_async(d_inputs[1], h_segment_ids, INT8_stream)\n        cuda.memcpy_htod_async(d_inputs[2], h_cu_seq_lens, INT8_stream)\n\n        # Setup tensor address\n        bindings = [int(d_inputs[i]) for i in range(3)] + [int(d_output)]\n\n        for i in range(engine.num_io_tensors):\n            context.set_tensor_address(engine.get_tensor_name(i), bindings[i])\n\n        # Run inference\n        trt_context.execute_async_v3(stream_handle=INT8_stream.handle)\n        # Synchronize the stream\n        INT8_stream.synchronize()\n        eval_time_elapsed += (time.time() - eval_start_time)\n\n        # Transfer predictions back from GPU\n        cuda.memcpy_dtoh_async(h_output, d_output, INT8_stream)\n        INT8_stream.synchronize()\n\n        # Only retrieve and post-process the first batch\n        networkOutputs.append(_NetworkOutput(\n            start_logits = np.array(h_output[0:S]),\n            end_logits = np.array(h_output[S:S*2]),\n            feature_index = feature_index\n            ))\n\n    eval_time_elapsed /= len(features)\n\n    # Total number of n-best predictions to generate in the nbest_predictions.json output file\n    n_best_size = 20\n\n    # The maximum length of an answer that can be generated. This is needed\n    # because the start and end predictions are not conditioned on one another\n    max_answer_length = 30\n\n    prediction, nbest_json, scores_diff_json = dp.get_predictions(tokens, features,\n            networkOutputs, n_best_size, max_answer_length)\n\n    return eval_time_elapsed, prediction, nbest_json\n```\n\n----------------------------------------\n\nTITLE: Running Inference and Comparing Results with TensorFlow\nDESCRIPTION: This snippet shows how to run inference using the TensorRT engine and compare the results with TensorFlow predictions and ground truth using the compare_tf.py script.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/EfficientDet/notebooks/EfficientDet-TensorRT8.ipynb#2025-04-06_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# Create directory for exported TensorRT engine\n![ ! -d \"output_imgs\" ] && mkdir output_imgs\n\n# Run inference and compare the outputs\n!python3 $TRT_OSSPATH/samples/python/efficientdet/compare_tf.py \\\n    --engine ./trt_engine/engine.trt \\\n    --saved_model ./tf_model/ \\\n    --input ./val2017 \\\n    --annotations ./annotations/instances_val2017.json \\\n    --labels $TRT_OSSPATH/samples/python/efficientdet/labels_coco.txt \\\n    --output ./output_imgs\n```\n\n----------------------------------------\n\nTITLE: Running BERT Inference with CUDA Graph Support\nDESCRIPTION: Executes inference using a Python script with C++ bindings for CUDA Graph support to improve performance.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npython3 inference_c.py -e engines/bert_large_128.engine --enable-graph -p \"TensorRT is a high performance deep learning inference platform that delivers low latency and high throughput for apps such as recommenders, speech and image/video on NVIDIA GPUs. It includes parsers to import models, and plugins to support novel ops and layers before applying optimizations for inference. Today NVIDIA is open-sourcing parsers and plugins in TensorRT so that the deep learning community can customize and extend these components to take advantage of powerful TensorRT optimizations for your apps.\" -q \"What is TensorRT?\" -v models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_128_v19.03.1/vocab.txt\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Quantized Model\nDESCRIPTION: This snippet fine-tunes the quantized model for a specified number of epochs and evaluates its accuracy after fine-tuning.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/notebooks/simple_network_quantize_full.ipynb#2025-04-06_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Fine-tune quantized model\nfine_tune_epochs = 2\n\nq_nn_model.fit(\n    train_images,\n    train_labels,\n    batch_size=32,\n    epochs=fine_tune_epochs,\n    validation_split=0.1,\n)\n\n_, q_model_accuracy = q_nn_model.evaluate(test_images, test_labels, verbose=0)\nq_model_accuracy = round(100 * q_model_accuracy, 2)\nprint(\n    \"Accuracy after fine-tuning for {} epochs: {}\".format(\n        fine_tune_epochs, q_model_accuracy\n    )\n)\nprint(\"Baseline accuracy (for reference): {}\".format(baseline_model_accuracy))\n```\n\n----------------------------------------\n\nTITLE: Initializing TensorRT INT8 BERT Model\nDESCRIPTION: Sets up a TensorRT INT8 engine for BERT inference with quantization-aware training and sparsity optimizations. The code handles runtime initialization, engine deserialization, and memory allocation specifically for INT8 precision with a maximum sequence length of 384.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/Q-and-A.ipynb#2025-04-06_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nengine_path = \"engines_%s/megatron_large_seqlen384_int8qat_sparse.engine\"%TRT_VERSION\nmax_seq_length = 384\n\nINT8_runtime = trt.Runtime(TRT_LOGGER)\nINT8_engine = INT8_runtime.deserialize_cuda_engine(open(engine_path, 'rb') .read()) \nINT8_context = INT8_engine.create_execution_context()\n\n# select engine profile\nINT8_context.set_optimization_profile_async(0, stream.handle)\n\ninput_nbytes = max_seq_length * trt.int32.itemsize\n\n# Allocate device memory for inputs.\nINT8_d_inputs = [cuda.mem_alloc(input_nbytes) for binding in range(4)]\n\n# Allocate output buffer by querying the size from the context. This may be different for different input shapes.\nINT8_h_output = cuda.pagelocked_empty((2 * max_seq_length), dtype=np.float32)\nINT8_d_output = cuda.mem_alloc(INT8_h_output.nbytes)\n\n# Create a stream in which to copy inputs/outputs and run inference.\nINT8_stream = cuda.Stream()\n```\n\n----------------------------------------\n\nTITLE: Implementing TensorRT Inference Pipeline\nDESCRIPTION: This function implements the complete TensorRT inference pipeline. It handles input processing, memory allocation, data transfers, inference execution, and output processing for semantic segmentation using a FCN-ResNet101 model.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/SemanticSegmentation/tutorial-runtime.ipynb#2025-04-06_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef infer(engine, input_file, output_file):\n    print(\"Reading input image from file {}\".format(input_file))\n    with Image.open(input_file) as img:\n        input_image = preprocess(img)\n        image_width = img.width\n        image_height = img.height\n\n    with engine.create_execution_context() as context:\n        # Allocate host and device buffers\n        tensor_names = [engine.get_tensor_name(i) for i in range(engine.num_io_tensors)]\n        for tensor in tensor_names:\n            size = trt.volume(context.get_tensor_shape(tensor))\n            dtype = trt.nptype(engine.get_tensor_dtype(tensor))\n            \n            if engine.get_tensor_mode(tensor) == trt.TensorIOMode.INPUT:\n                context.set_input_shape(tensor, (1, 3, image_height, image_width))\n                input_buffer = np.ascontiguousarray(input_image)\n                input_memory = cuda.mem_alloc(input_image.nbytes)\n                context.set_tensor_address(tensor, int(input_memory))\n            else:\n                output_buffer = cuda.pagelocked_empty(size, dtype)\n                output_memory = cuda.mem_alloc(output_buffer.nbytes)\n                context.set_tensor_address(tensor, int(output_memory))\n\n        stream = cuda.Stream()\n        \n        # Transfer input data to the GPU.\n        cuda.memcpy_htod_async(input_memory, input_buffer, stream)\n        \n        # Run inference\n        context.execute_async_v3(stream_handle=stream.handle)\n        \n        # Transfer prediction output from the GPU.\n        cuda.memcpy_dtoh_async(output_buffer, output_memory, stream)\n        \n        # Synchronize the stream\n        stream.synchronize()\n        output_d64 = np.array(output_buffer, dtype=np.int64)\n        np.savetxt('test.out', output_d64.astype(int), fmt='%i', delimiter=' ', newline=' ')\n\n    with postprocess(np.reshape(output_buffer, (image_height, image_width))) as img:\n        print(\"Writing output image to file {}\".format(output_file))\n        img.convert('RGB').save(output_file, \"PPM\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Executing BERT-Large Engine with TensorRT\nDESCRIPTION: Loads a serialized BERT TensorRT engine, sets up the execution context, allocates device memory, and runs inference on features. The code manages CUDA memory transfers, executes the model asynchronously, and collects output predictions.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/inference.ipynb#2025-04-06_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwith open(\"/workspace/TensorRT/demo/BERT/engines/bert_large_128.engine\", \"rb\") as f, \\\n    trt.Runtime(TRT_LOGGER) as runtime, \\\n    runtime.deserialize_cuda_engine(f.read()) as engine, \\\n    engine.create_execution_context() as context:\n\n     # We always use batch size 1.\n    input_shape = (1, max_seq_length)\n    input_nbytes = trt.volume(input_shape) * trt.int32.itemsize\n    \n    # Allocate device memory for inputs.\n    d_inputs = [cuda.mem_alloc(input_nbytes) for binding in range(3)]\n    # Create a stream in which to copy inputs/outputs and run inference.\n    stream = cuda.Stream()\n\n    # Specify input shapes. These must be within the min/max bounds of the active profile (0th profile in this case)\n    # Note that input shapes can be specified on a per-inference basis, but in this case, we only have a single shape.\n    for binding in range(3):\n        tensor_name = engine.get_tensor_name(binding)\n        context.set_input_shape(tensor_name, input_shape)\n    assert context.all_binding_shapes_specified\n\n    # Allocate output buffer by querying the size from the context. This may be different for different input shapes.\n    h_output = cuda.pagelocked_empty(tuple(context.get_tensor_shape(engine.get_tensor_name(3))), dtype=np.float32)\n    d_output = cuda.mem_alloc(h_output.nbytes)\n\n    print(\"\\nRunning Inference...\")\n\n    _NetworkOutput = collections.namedtuple(  # pylint: disable=invalid-name\n        \"NetworkOutput\",\n        [\"start_logits\", \"end_logits\", \"feature_index\"])\n    networkOutputs = []\n\n    eval_time_elapsed = 0\n    for feature_index, feature in enumerate(features):\n        # Copy inputs\n        input_ids = cuda.register_host_memory(np.ascontiguousarray(feature.input_ids.ravel()))\n        segment_ids = cuda.register_host_memory(np.ascontiguousarray(feature.segment_ids.ravel()))\n        input_mask = cuda.register_host_memory(np.ascontiguousarray(feature.input_mask.ravel()))\n\n        eval_start_time = time.time()\n        cuda.memcpy_htod_async(d_inputs[0], input_ids, stream)\n        cuda.memcpy_htod_async(d_inputs[1], segment_ids, stream)\n        cuda.memcpy_htod_async(d_inputs[2], input_mask, stream)\n\n        # Setup tensor address\n        bindings = [int(d_inputs[i]) for i in range(3)] + [int(d_output)]\n\n        for i in range(engine.num_io_tensors):\n            context.set_tensor_address(engine.get_tensor_name(i), bindings[i])\n\n        # Run inference\n        context.execute_async_v3(stream_handle=stream.handle)\n        # Synchronize the stream\n        stream.synchronize()\n        eval_time_elapsed += (time.time() - eval_start_time)\n\n        # Transfer predictions back from GPU\n        cuda.memcpy_dtoh_async(h_output, d_output, stream)\n        stream.synchronize()\n\n        for index, batch in enumerate(h_output):\n            # Data Post-processing\n            networkOutputs.append(_NetworkOutput(\n                start_logits = np.array(batch.squeeze()[:, 0]),\n                end_logits = np.array(batch.squeeze()[:, 1]),\n                feature_index = feature_index\n                ))\n\n    eval_time_elapsed /= len(features)\n    \n    print(\"-----------------------------\")\n    print(\"Running Inference at {:.3f} Sentences/Sec\".format(1.0/eval_time_elapsed))\n    print(\"-----------------------------\")\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for BERT with Variable Sequence Length (FP16)\nDESCRIPTION: This command builds a TensorRT engine for BERT with variable sequence length support using FP16 precision.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p engines && python3 builder_varseqlen.py -x models/fine-tuned/bert_pyt_onnx_large_qa_squad11_amp_fake_quant_v1/bert_large_v1_1_fake_quant.onnx -o engines/bert_varseq_fp16.engine -b 1 -s 64 --fp16 -c models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1 -v models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/vocab.txt\n```\n\n----------------------------------------\n\nTITLE: Benchmarking TensorRT Engine using trtexec\nDESCRIPTION: This snippet shows how to benchmark the TensorRT engine using the trtexec utility. It provides performance metrics such as GPU compute time.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/EfficientDet/notebooks/EfficientDet-TensorRT8.ipynb#2025-04-06_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n!/workspace/TensorRT/build/out/trtexec \\\n    --loadEngine=trt_engine/engine.trt \\\n    --useCudaGraph --noDataTransfers \\\n    --iterations=100 --avgRuns=100\n```\n\n----------------------------------------\n\nTITLE: Converting ONNX Model to TensorRT Engine\nDESCRIPTION: Uses trtexec command-line tool to convert the ONNX model to a TensorRT engine file, with options for FP16 or FP32 precision based on the USE_FP16 flag.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/2. Using PyTorch through ONNX.ipynb#2025-04-06_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# step out of Python for a moment to convert the ONNX model to a TRT engine using trtexec\nif USE_FP16:\n    !trtexec --onnx=resnet50_pytorch.onnx --saveEngine=resnet_engine_pytorch.trt   --inputIOFormats=fp16:chw --outputIOFormats=fp16:chw --fp16\nelse:\n    !trtexec --onnx=resnet50_pytorch.onnx --saveEngine=resnet_engine_pytorch.trt \n```\n\n----------------------------------------\n\nTITLE: Using Timing Cache for Deterministic Engine Build\nDESCRIPTION: Builds another TensorRT engine using the previously saved timing cache. The --error-on-timing-cache-miss flag ensures all layer timings come from the cache for complete determinism.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/convert/02_deterministic_engine_builds_in_tensorrt/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy convert identity.onnx \\\n    --load-timing-cache timing.cache --error-on-timing-cache-miss \\\n    -o 1.engine\n```\n\n----------------------------------------\n\nTITLE: Quantizing the Model with TensorFlow Quantization Toolkit\nDESCRIPTION: Uses NVIDIA's TensorFlow Quantization toolkit to apply full model quantization. This inserts Q/DQ (Quantize/Dequantize) nodes in all supported layers with a single function call.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/getting_started.ipynb#2025-04-06_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Quantize model\nquantized_model = quantize_model(model)\n```\n\n----------------------------------------\n\nTITLE: Converting ONNX to TensorRT Engine using Polygraphy Loaders\nDESCRIPTION: Demonstrates how to compose Polygraphy Loaders to convert from ONNX to TensorRT engine using both callable and functional approaches.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/polygraphy/README.md#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom polygraphy.backend.trt import EngineFromNetwork, NetworkFromOnnxPath\n\nbuild_engine = EngineFromNetwork(NetworkFromOnnxPath(\"/path/to/model.onnx\"))\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom polygraphy.backend.trt import engine_from_network, network_from_onnx_path\n\nengine = engine_from_network(network_from_onnx_path(\"/path/to/model.onnx\"))\n```\n\n----------------------------------------\n\nTITLE: Evaluating mAP Metric for TensorRT EfficientDet Engine\nDESCRIPTION: Command to evaluate the mean Average Precision (mAP) of a TensorRT engine against COCO validation data. Requires the COCO dataset, ground truth annotations, and the AutoML repository for metrics calculation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/efficientdet/README.md#2025-04-06_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\npython3 eval_coco.py \\\n    --engine /path/to/engine.trt \\\n    --input /path/to/coco/val2017 \\\n    --annotations /path/to/coco/annotations/instances_val2017.json \\\n    --automl_path /path/to/automl\n```\n\n----------------------------------------\n\nTITLE: Running Inference with ONNX Runtime and TensorRT Execution Provider\nDESCRIPTION: Demonstrates how to use ONNX Runtime with TensorRT Execution Provider as an alternative inference framework for DeBERTa models with and without plugin optimizations.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/DeBERTa/README.md#2025-04-06_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython deberta_ort_inference.py --onnx=./test/deberta_original.onnx --test fp16\n\npython deberta_ort_inference.py --onnx=./test/deberta_plugin.onnx --test fp16\n\npython deberta_ort_inference.py --onnx=./test/deberta --correctness-check fp16 # for correctness check\n```\n\n----------------------------------------\n\nTITLE: Benchmarking TensorRT Engine\nDESCRIPTION: Command to benchmark the TensorRT engine using trtexec utility with CUDA graphs enabled.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/efficientnet/README.md#2025-04-06_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ntrtexec \\\n    --loadEngine=/path/to/engine.trt \\\n    --useCudaGraph --noDataTransfers \\\n    --iterations=100 --avgRuns=100\n```\n\n----------------------------------------\n\nTITLE: Allocating GPU Memory and Setting Tensor Bindings for TensorRT Inference in Python\nDESCRIPTION: This snippet allocates device memory for input and output tensors, sets up tensor bindings, and prepares the CUDA stream for inference. It demonstrates how to interact with the TensorRT engine and context to prepare for GPU execution.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/2. Using PyTorch through ONNX.ipynb#2025-04-06_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# allocate device memory\nd_input = cuda.mem_alloc(1 * input_batch.nbytes)\nd_output = cuda.mem_alloc(1 * output.nbytes)\n\ntensor_names = [engine.get_tensor_name(i) for i in range(engine.num_io_tensors)]\nassert(len(tensor_names) == 2)\n\ncontext.set_tensor_address(tensor_names[0], int(d_input))\ncontext.set_tensor_address(tensor_names[1], int(d_output))\n\nbindings = [int(d_input), int(d_output)]\n\nstream = cuda.Stream()\n```\n\n----------------------------------------\n\nTITLE: Loading TensorRT Engine from File\nDESCRIPTION: This function loads a serialized TensorRT engine from a file. It uses the TensorRT Runtime to deserialize the engine, which is then used for inference.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/SemanticSegmentation/tutorial-runtime.ipynb#2025-04-06_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef load_engine(engine_file_path):\n    assert os.path.exists(engine_file_path)\n    print(\"Reading engine from file {}\".format(engine_file_path))\n    with open(engine_file_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n        return runtime.deserialize_cuda_engine(f.read())\n```\n\n----------------------------------------\n\nTITLE: Running Inference with TensorRT API\nDESCRIPTION: Builds and tests TensorRT engines from ONNX models with different precision options. Compares performance between the original DeBERTa model and the plugin-optimized version.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/DeBERTa/README.md#2025-04-06_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# build and test the original DeBERTa model (baseline)\npython deberta_tensorrt_inference.py --onnx=./test/deberta_original.onnx --build fp16 --test fp16\n\n# build and test the optimized DeBERTa model with plugin\npython deberta_tensorrt_inference.py --onnx=./test/deberta_plugin.onnx --build fp16 --test fp16\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Elementwise Addition Plugin\nDESCRIPTION: Implementation of a simple elementwise addition plugin using TensorRT's plugin registration decorator. The plugin accepts an input tensor and block size parameter, returning a tensor with identical characteristics.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/quickly_deployable_plugins/README.md#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport tensorrt.plugin as trtp\n\n@trtp.register(\"sample::elemwise_add_plugin\")\ndef add_plugin_desc(inp0: trtp.TensorDesc, block_size: int) -> trtp.TensorDesc:\n    return inp0.like()\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for TensorRT Conversion\nDESCRIPTION: This command installs the required dependencies for the example using pip.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/00_inference_with_tensorrt/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Building INT8 Precision TensorRT Engine with Calibration\nDESCRIPTION: Creates a TensorRT engine with INT8 precision, including calibration using a set of sample images to optimize quantization.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/detectron2/README.md#2025-04-06_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython3 build_engine.py \\\n    --onnx /path/to/converted.onnx \\\n    --engine /path/to/engine.trt \\\n    --precision int8 \\\n    --calib_input /path/to/calibration/images \\\n    --calib_cache /path/to/calibration.cache\n```\n\n----------------------------------------\n\nTITLE: Setting Upper Bounds for Unbounded DDS with Polygraphy\nDESCRIPTION: This command sets an upper bound of 1000 for all unbounded DDS tensors in the model, inserting min operators to limit tensor sizes.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/surgeon/04_setting_upper_bounds/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy surgeon sanitize folded.onnx --set-unbounded-dds-upper-bound 1000 -o modified.onnx\n```\n\n----------------------------------------\n\nTITLE: Comparing Per-Layer Outputs Between Frameworks in Polygraphy\nDESCRIPTION: Command to compare all intermediate layer outputs between TensorRT and ONNX-Runtime, useful for debugging where discrepancies originate in a model's execution.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/run/01_comparing_frameworks/README.md#2025-04-06_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run dynamic_identity.onnx --trt --onnxrt \\\n    --trt-outputs mark all \\\n    --onnx-outputs mark all\n```\n\n----------------------------------------\n\nTITLE: Running FP32 Model and Saving Results\nDESCRIPTION: Command to execute the original FP32 model in ONNX-Runtime and save inputs and outputs for comparison.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/convert/04_converting_models_to_fp16/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run --onnxrt identity.onnx \\\n   --save-inputs inputs.json --save-outputs outputs_fp32.json\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies\nDESCRIPTION: Commands to install required Python packages listed in requirements.txt and the onnx-graphsurgeon module.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/efficientdet/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip3 install -r requirements.txt\npip3 install onnx-graphsurgeon --index-url https://pypi.ngc.nvidia.com\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine with INT8 Precision and Calibration\nDESCRIPTION: Command to build and calibrate a TensorRT engine with INT8 precision. Requires a directory of calibration images and specifies a calibration cache file for storing or reusing calibration results.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/tensorflow_object_detection_api/README.md#2025-04-06_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython build_engine.py \\\n    --onnx /path/to/model.onnx \\\n    --engine /path/to/engine.trt \\\n    --precision int8 \\\n    --calib_input /path/to/calibration/images \\\n    --calib_cache /path/to/calibration.cache\n```\n\n----------------------------------------\n\nTITLE: Benchmarking TensorRT Engine Performance with trtexec\nDESCRIPTION: Command to benchmark a TensorRT engine using the trtexec utility. Measures execution timing information including min, max, mean, and median GPU compute times across multiple iterations.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/tensorflow_object_detection_api/README.md#2025-04-06_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ntrtexec \\\n    --loadEngine=/path/to/engine.trt \\\n    --useCudaGraph --noDataTransfers \\\n    --iterations=100 --avgRuns=100\n```\n\n----------------------------------------\n\nTITLE: Downloading Pre-trained BERT Large Model from NGC\nDESCRIPTION: Downloads a pre-trained, fine-tuned BERT large model from NVIDIA GPU Cloud (NGC). The model is optimized for NVIDIA Tensor Core GPUs and can be used for the SQuAD question answering task.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/BERT-TRT-INT8-QAT-sparse.ipynb#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!bash ../scripts/download_model.sh 384 # BERT-large model checkpoint\n!bash ../scripts/download_model.sh pyt megatron-large int8-qat sparse # Megatron-LM model weights\n```\n\n----------------------------------------\n\nTITLE: Parsing ONNX Model in TensorRT\nDESCRIPTION: Parses the ONNX model file using the initialized parser. It handles potential parsing failures and logs errors.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleOnnxMnistCoordConvAC/README.md#2025-04-06_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nif (!parser->parseFromFile(model_file, static_cast<int>(sample::gLogger.getReportableSeverity())))\n{\n\tstring msg(\"failed to parse onnx file\");\n\tsample::gLogger->log(nvinfer1::ILogger::Severity::kERROR, msg.c_str());\n\texit(EXIT_FAILURE);\n}\n```\n\n----------------------------------------\n\nTITLE: Running Triton Inference Server with TensorRT Model\nDESCRIPTION: Docker command to run NVIDIA's Triton Inference Server with the optimized TensorRT model. The command maps ports for HTTP/gRPC communication and mounts the model repository.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/deploy_to_triton/README.md#2025-04-06_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n# Make sure that the TensorRT version in the Triton container\n# and TensorRT version in the environment used to optimize the model\n# are the same. <xx.yy> as 24.07 will work in this example\n\n\ndocker run --gpus all --rm -p 8000:8000 -p 8001:8001 -p 8002:8002 -v /full/path/to/docs/examples/model_repository:/models nvcr.io/nvidia/tritonserver:<xx.yy>-py3 tritonserver --model-repository=/models\n```\n\n----------------------------------------\n\nTITLE: Creating a Simple Neural Network with TensorFlow\nDESCRIPTION: Defines a simple convolutional neural network using TensorFlow's Keras API. The network consists of multiple Conv2D layers, ReLU activations, and Dense layers, designed for the Fashion MNIST dataset.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/getting_started.ipynb#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport tensorflow as tf\nfrom tensorflow_quantization import quantize_model\nfrom tensorflow_quantization import utils\n\nassets = utils.CreateAssetsFolders(\"GettingStarted\")\nassets.add_folder(\"example\")\n\ndef simple_net():\n    \"\"\"\n    Return a simple neural network.\n    \"\"\"\n    input_img = tf.keras.layers.Input(shape=(28, 28), name=\"nn_input\")\n    x = tf.keras.layers.Reshape(target_shape=(28, 28, 1), name=\"reshape_0\")(input_img)\n    x = tf.keras.layers.Conv2D(filters=126, kernel_size=(3, 3), name=\"conv_0\")(x)\n    x = tf.keras.layers.ReLU(name=\"relu_0\")(x)\n    x = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), name=\"conv_1\")(x)\n    x = tf.keras.layers.ReLU(name=\"relu_1\")(x)\n    x = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), name=\"conv_2\")(x)\n    x = tf.keras.layers.ReLU(name=\"relu_2\")(x)\n    x = tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), name=\"conv_3\")(x)\n    x = tf.keras.layers.ReLU(name=\"relu_3\")(x)\n    x = tf.keras.layers.Conv2D(filters=8, kernel_size=(3, 3), name=\"conv_4\")(x)\n    x = tf.keras.layers.ReLU(name=\"relu_4\")(x)\n    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), name=\"max_pool_0\")(x)\n    x = tf.keras.layers.Flatten(name=\"flatten_0\")(x)\n    x = tf.keras.layers.Dense(100, name=\"dense_0\")(x)\n    x = tf.keras.layers.ReLU(name=\"relu_5\")(x)\n    x = tf.keras.layers.Dense(10, name=\"dense_1\")(x)\n    return tf.keras.Model(input_img, x, name=\"original\")\n\n# create model\nmodel = simple_net()\nmodel.summary()\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for EfficientDet in Python\nDESCRIPTION: This snippet demonstrates how to build a TensorRT engine from an ONNX model using the build_engine.py script. It includes options for different precision levels (FP32, INT8) and calibration for INT8.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/EfficientDet/notebooks/EfficientDet-TensorRT8.ipynb#2025-04-06_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Create directory for exported TensorRT engine\n![ ! -d \"trt_engine\" ] && mkdir trt_engine\n\n# Build engine with FP32 precision\n!python3 $TRT_OSSPATH/samples/python/efficientdet/build_engine.py \\\n    --onnx ./onnx_model/model.onnx \\\n    --engine ./trt_engine/engine.trt \\\n    --precision fp32\n\n## To build TensorRT engine with INT8 precision run the following after setting path to 'calib_input' and 'calib_cache':\n# python $TRT_OSSPATH/samples/python/efficientdet/build_engine.py \\\n#     --onnx ./onnx_model/model.onnx \\\n#     --engine ./trt_engine/engine.trt \\\n#     --precision int8 \\\n#     --calib_input /path/to/calibration/images \\\n#     --calib_cache /path/to/calibration.cache\n```\n\n----------------------------------------\n\nTITLE: Benchmarking TensorRT-Optimized BERT Model with Batch Size 1\nDESCRIPTION: This code snippet runs a performance benchmark on a TensorRT-optimized BERT model with a batch size of 1, sequence length of 384, and 1000 iterations. It's used to measure the inference latency for single-sample processing.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/BERT-TRT-INT8-QAT-sparse.ipynb#2025-04-06_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n!python3 ../perf_varseqlen.py -e ./engines_$TRT_VERSION/megatron_large_seqlen384_int8qat_sparse.engine -b 1 -s 384 -i 1000 -w 500\n```\n\n----------------------------------------\n\nTITLE: Inspecting TensorRT Engine with Polygraphy\nDESCRIPTION: This command uses Polygraphy's inspect tool to examine the built TensorRT engine.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/00_inference_with_tensorrt/README.md#2025-04-06_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy inspect model identity.engine\n```\n\n----------------------------------------\n\nTITLE: Running Inference and Evaluation on SQuAD Dataset with TensorRT\nDESCRIPTION: This snippet runs inference using a TensorRT-optimized BERT model on the SQuAD development set, generates predictions, and evaluates the model's performance using the F1 score metric. It aims to verify that the INT8 model maintains a 90% F1 score, comparable to FP16 and original models.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/BERT-TRT-INT8-QAT-sparse.ipynb#2025-04-06_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n!python3 ../inference_varseqlen.py -e engines_$TRT_VERSION/megatron_large_seqlen384_int8qat_sparse.engine -s 384 -sq ./squad/dev-v1.1.json -v models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/vocab.txt -o ./predictions.json\n!python3 ../squad/evaluate-v1.1.py  squad/dev-v1.1.json  ./predictions.json 90\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Quantized ResNet50 V1 Model in TensorFlow\nDESCRIPTION: This code compiles the quantized model and fine-tunes it using the training data. It uses the previously defined hyperparameters for batch size, steps per epoch, and number of epochs.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/example_resnet50v1.ipynb#2025-04-06_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncompile_model(q_model)\nq_model.fit(\n    train_batches,\n    validation_data=val_batches,\n    batch_size=HYPERPARAMS[\"batch_size\"],\n    steps_per_epoch=HYPERPARAMS[\"steps_per_epoch\"],\n    epochs=HYPERPARAMS[\"epochs\"]\n)\n```\n\n----------------------------------------\n\nTITLE: Processing ResNet Models\nDESCRIPTION: Shell command to process ResNet models and generate JSON files.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/examples/tensorflow/resnet/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n<path-to-trex>/examples/tensorflow/resnet/process_resnet.sh\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing PyTorch ResNet50 Model\nDESCRIPTION: Imports necessary libraries, loads a pre-trained ResNet50 model from torchvision, and sets it to evaluation mode for inference.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/2. Using PyTorch through ONNX.ipynb#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torchvision.models as models\nimport torch\nimport torch.onnx\n\n# load the pretrained model\nresnet50 = models.resnet50(pretrained=True, progress=False).eval()\n```\n\n----------------------------------------\n\nTITLE: Modifying ONNX Model for TensorRT Compatibility\nDESCRIPTION: Modifies the ONNX model to make it compatible with TensorRT by removing unsupported operations and optionally replacing disentangled attention subgraphs with optimized plugin nodes.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/DeBERTa/README.md#2025-04-06_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython deberta_onnx_modify.py ./test/deberta.onnx # generates original TRT-compatible model, `*_original.onnx`\n\npython deberta_onnx_modify.py ./test/deberta.onnx --plugin # generates TRT-compatible model with plugin nodes, `*_plugin.onnx`\n```\n\n----------------------------------------\n\nTITLE: Calculating Upper Bound for Size Tensor\nDESCRIPTION: Calculates the maximum possible number of non-zero elements by multiplying input dimensions using DimensionOperation.PROD.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/non_zero_plugin/README.md#2025-04-06_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nupper_bound = exprBuilder.operation(trt.DimensionOperation.PROD, inputs[0][0], inputs[0][1])\n```\n\n----------------------------------------\n\nTITLE: Creating a Network and ONNX Parser for MNIST Model in TensorRT\nDESCRIPTION: Creates an empty TensorRT network with full dimensions support and an ONNX parser to parse the MNIST model file.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleDynamicReshape/README.md#2025-04-06_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nauto network = makeUnique(builder->createNetworkV2(0));\nauto parser = nvonnxparser::createParser(*network, sample::gLogger.getTRTLogger());\n```\n\n----------------------------------------\n\nTITLE: Analyzing Memory and Compute Efficiency of Convolution Layers in Python\nDESCRIPTION: Visualizes memory and compute efficiency metrics for convolution layers. These plots show memory accesses per ms and compute operations per ms, providing insights into performance bottlenecks.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/tutorial.ipynb#2025-04-06_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n# Memory accesses per ms (assuming one time read/write penalty)\ntrex.plotting.plotly_bar2(\n    convs, \n    \"Convolution Memory Efficiency<BR>(bar color indicates latency)\", \n    \"attr.memory_efficiency\", \n    \"Name\", \n    color='latency.pct_time')\n\n# Compute operations per ms (assuming one time read/write penalty)\ntrex.plotting.plotly_bar2(\n    convs, \n    \"Convolution Compute Efficiency<BR>(bar color indicates latency)\",\n    \"attr.compute_efficiency\",\n    \"Name\",\n    color='latency.pct_time');\n```\n\n----------------------------------------\n\nTITLE: Verifying TensorRT Output Accuracy for Image Classification in Python\nDESCRIPTION: This snippet processes the TensorRT model output to display the top 5 predicted classes and their probabilities. It's used to verify the accuracy of the optimized TensorRT model compared to the original unoptimized version.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/2. Using PyTorch through ONNX.ipynb#2025-04-06_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nindices = (-pred[0]).argsort()[:5]\nprint(\"Class | Probability (out of 1)\")\nlist(zip(indices, pred[0][indices]))\n```\n\n----------------------------------------\n\nTITLE: Running the Int8 Calibration Example with Polygraphy\nDESCRIPTION: Executes the example Python script that demonstrates Int8 calibration using Polygraphy's calibrator with TensorRT.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/04_int8_calibration_in_tensorrt/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 example.py\n```\n\n----------------------------------------\n\nTITLE: Converting ONNX Model to TensorRT Engine using trtexec\nDESCRIPTION: Command to use TensorRT's trtexec tool for converting an ONNX model to an optimized TensorRT engine. The command enables CUDA graph for additional performance optimization.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/deploy_to_triton/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntrtexec --onnx=resnet50.onnx \\\n        --saveEngine=model.plan \\\n        --useCudaGraph\n```\n\n----------------------------------------\n\nTITLE: Building and Running TensorRT Engine\nDESCRIPTION: This command executes the Python script that builds a TensorRT engine from the ONNX model and runs inference.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/00_inference_with_tensorrt/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 build_and_run.py\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for BERT TensorRT Benchmarking in Python\nDESCRIPTION: Sets up the required imports for BERT TensorRT benchmarking, including TensorRT, CUDA libraries, numpy for data processing, and various utility modules. It also configures the environment by appending the BERT demo path and filtering warnings.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/benchmark.ipynb#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport sys\nsys.path.append('/workspace/TensorRT/demo/BERT')\n\nimport tensorrt as trt;\nTRT_VERSION = trt.__version__\n\nimport time\nimport argparse\nimport ctypes\nimport numpy as np\nimport tensorrt as trt\nimport pycuda.driver as cuda\nimport pycuda.autoinit\nimport numpy as np\nfrom tqdm import tqdm\n\nimport ipywidgets as widgets\nfrom ipywidgets import IntProgress\nfrom ipywidgets import Button, Layout\nfrom IPython.display import display\n\nimport helpers.tokenization as tokenization\nimport helpers.data_processing as dp\n\nTRT_LOGGER = trt.Logger(trt.Logger.ERROR)\n```\n\n----------------------------------------\n\nTITLE: Converting PyTorch DeBERTa Model to ONNX Format\nDESCRIPTION: Exports a DeBERTa model from HuggingFace implementation to ONNX format. Supports specifying model variants and sequence length configurations.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/DeBERTa/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython deberta_pytorch2onnx.py --filename ./test/deberta.onnx [--variant microsoft/deberta-v3-xsmall] [--seq-len 2048]\n```\n\n----------------------------------------\n\nTITLE: Generating Images with Flux Using TensorRT\nDESCRIPTION: Commands to generate images using Flux models with TensorRT optimization, including different precision options and ControlNet support.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/Diffusion/README.md#2025-04-06_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npython3 demo_txt2img_flux.py \"a beautiful photograph of Mt. Fuji during cherry blossom\" --hf-token=$HF_TOKEN\n```\n\nLANGUAGE: bash\nCODE:\n```\npython3 demo_txt2img_flux.py \"a beautiful photograph of Mt. Fuji during cherry blossom\" --hf-token=$HF_TOKEN --bf16 --download-onnx-models\n```\n\nLANGUAGE: bash\nCODE:\n```\npython3 demo_txt2img_flux.py \"a beautiful photograph of Mt. Fuji during cherry blossom\" --hf-token=$HF_TOKEN --fp8 --download-onnx-models\n```\n\nLANGUAGE: bash\nCODE:\n```\npython3 demo_txt2img_flux.py \"a beautiful photograph of Mt. Fuji during cherry blossom\" --hf-token=$HF_TOKEN --fp4 --download-onnx-models\n```\n\nLANGUAGE: bash\nCODE:\n```\nwget \"https://miro.medium.com/v2/resize:fit:640/format:webp/1*iD8mUonHMgnlP0qrSx3qPg.png\" -O yellow.png\n```\n\nLANGUAGE: bash\nCODE:\n```\npython3 demo_img2img_flux.py \"A home with 2 floors and windows. The front door is purple\" --hf-token=$HF_TOKEN --input-image yellow.png --image-strength 0.95 --bf16 --onnx-dir onnx-flux-dev/bf16 --engine-dir engine-flux-dev/\n```\n\nLANGUAGE: bash\nCODE:\n```\nwget https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/robot.png\n```\n\nLANGUAGE: bash\nCODE:\n```\npython3 demo_img2img_flux.py \"A robot made of exotic candies and chocolates of different kinds. The background is filled with confetti and celebratory gifts.\" --version=\"flux.1-dev-depth\" --hf-token=$HF_TOKEN --guidance-scale 10 --control-image robot.png --bf16 --denoising-steps 30  --download-onnx-models\n```\n\nLANGUAGE: bash\nCODE:\n```\npython3 demo_img2img_flux.py \"A robot made of exotic candies\" --version=\"flux.1-dev-depth\" --hf-token=$HF_TOKEN --guidance-scale 10 --control-image robot.png --fp8 --denoising-steps 30 --download-onnx-models --build-static-batch\n```\n\nLANGUAGE: bash\nCODE:\n```\nrm -rf onnx/* engine/* && python3 demo_img2img_flux.py \"A robot made of exotic candies\" --version=\"flux.1-dev-depth\" --hf-token=$HF_TOKEN --guidance-scale 10 --control-image robot.png --fp8 --denoising-steps 30\n```\n\nLANGUAGE: bash\nCODE:\n```\npython3 demo_img2img_flux.py \"A robot made of exotic candies\" --version=\"flux.1-dev-depth\" --hf-token=$HF_TOKEN --guidance-scale 10 --control-image robot.png --fp4 --denoising-steps 30 --download-onnx-models\n```\n\nLANGUAGE: bash\nCODE:\n```\npython3 demo_img2img_flux.py \"a robot made out of gold\" --version=\"flux.1-dev-canny\" --hf-token=$HF_TOKEN --guidance-scale 30 --control-image robot.png --bf16 --denoising-steps 30 --download-onnx-models\n```\n\nLANGUAGE: bash\nCODE:\n```\npython3 demo_img2img_flux.py \"a robot made out of gold\" --version=\"flux.1-dev-canny\" --hf-token=$HF_TOKEN --guidance-scale 30 --control-image robot.png --fp8 --denoising-steps 30 --download-onnx-models --build-static-batch\n```\n\nLANGUAGE: bash\nCODE:\n```\nrm -rf onnx/* engine/* && python3 demo_img2img_flux.py \"a robot made out of gold\" --version=\"flux.1-dev-canny\" --hf-token=$HF_TOKEN --guidance-scale 30 --control-image robot.png --fp8 --denoising-steps 30 --calibration-dataset {custom/dataset/path}\n```\n\nLANGUAGE: bash\nCODE:\n```\npython3 demo_img2img_flux.py \"a robot made out of gold\" --version=\"flux.1-dev-canny\" --hf-token=$HF_TOKEN --guidance-scale 30 --control-image robot.png --fp4 --denoising-steps 30 --download-onnx-models\n```\n\nLANGUAGE: bash\nCODE:\n```\npython3 demo_txt2img_flux.py \"a beautiful photograph of Mt. Fuji during cherry blossom\" --hf-token=$HF_TOKEN --onnx-export-only\n```\n\n----------------------------------------\n\nTITLE: Converting Model with Int8 Calibration\nDESCRIPTION: Command to convert an ONNX model to a TensorRT engine with Int8 precision using a custom data loader script for calibration data while saving the calibration cache for future use.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/convert/01_int8_calibration_in_tensorrt/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy convert identity.onnx --int8 \\\n    --data-loader-script ./data_loader.py \\\n    --calibration-cache identity_calib.cache \\\n    -o identity.engine\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT INT8 Engine\nDESCRIPTION: Command to build and calibrate TensorRT engine with INT8 precision from ONNX model.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/efficientnet/README.md#2025-04-06_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython3 build_engine.py \\\n    --onnx /path/to/model.onnx \\\n    --engine /path/to/engine.trt \\\n    --precision int8 \\\n    --calib_input /path/to/calibration/images \\\n    --calib_cache /path/to/calibration.cache \\\n    --calib_preprocessor V2\n```\n\n----------------------------------------\n\nTITLE: Validating TensorRT BERT Model on SQuAD Dev Set\nDESCRIPTION: Runs inference on the entire SQuAD development set using the optimized BERT TensorRT model and evaluates the F1 score to validate that it achieves state-of-the-art accuracy of at least 90%.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/BERT-TRT-FP16.ipynb#2025-04-06_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n!python3 ../inference.py -e engines_$TRT_VERSION/bert_large_384.engine -s 384 -sq ./squad/dev-v1.1.json -v models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/vocab.txt -o ./predictions-bert_large_384.json\n!python3 ../squad/evaluate-v1.1.py  squad/dev-v1.1.json  ./predictions-bert_large_384.json 90\n```\n\n----------------------------------------\n\nTITLE: Initial Model Reduction Using Bisect Mode\nDESCRIPTION: Commands for initial model reduction using Polygraphy's debug reduce tool in bisect mode, including both actual and simulated failure cases.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/debug/02_reducing_failing_onnx_models/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy debug reduce folded.onnx -o initial_reduced.onnx --mode=bisect --load-inputs layerwise_inputs.json \\\n    --check polygraphy run polygraphy_debug.onnx --trt \\\n            --load-inputs layerwise_inputs.json --load-outputs layerwise_golden.json\n```\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy debug reduce folded.onnx -o initial_reduced.onnx --mode=bisect \\\n    --fail-regex \"Op: Mul\" \\\n    --check polygraphy inspect model polygraphy_debug.onnx --show layers\n```\n\n----------------------------------------\n\nTITLE: Setting Dimension Ranges in Optimization Profile for TensorRT\nDESCRIPTION: Configures the optimization profile with minimum, optimal, and maximum input dimensions. TensorRT will optimize for the optimal dimensions while supporting any dimensions within the specified range.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleDynamicReshape/README.md#2025-04-06_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\nprofile->setDimensions(input->getName(), OptProfileSelector::kMIN, Dims4{1, 1, 1, 1});\nprofile->setDimensions(input->getName(), OptProfileSelector::kOPT, Dims4{1, 1, 28, 28});\nprofile->setDimensions(input->getName(), OptProfileSelector::kMAX, Dims4{1, 1, 56, 56});\npreprocessorConfig->addOptimizationProfile(profile);\n```\n\n----------------------------------------\n\nTITLE: Implementing Conv2DTransposeQuantizeWrapper with TensorFlow Quantization\nDESCRIPTION: Example implementation of a quantization wrapper for the Conv2DTranspose layer. This wrapper handles quantization of weights and inputs, supporting per-channel quantization for weights and per-tensor quantization for inputs. It manages the creation of min/max variables for quantization and properly integrates with the training process.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/bqw.rst#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom tensorflow.python.util import tf_inspect\nfrom tensorflow_quantization.quantize_wrapper_base import BaseQuantizeWrapper\n\nclass Conv2DTransposeQuantizeWrapper(BaseQuantizeWrapper):\n    def __init__(self, layer, kernel_type=\"kernel\", **kwargs):\n        \"\"\"\n        Create a quantize emulate wrapper for a keras layer.\n        This wrapper provides options to quantize inputs, outputs amd weights of a quantizable layer.\n        Args:\n        layer: The keras layer to be quantized.\n        kernel_type: Options=['kernel' for Conv2D/Dense, 'depthwise_kernel' for DepthwiseConv2D]\n        **kwargs: Additional keyword arguments to be passed to the keras layer.\n        \"\"\"\n        self.kernel_type = kernel_type\n        self.channel_axis = kwargs.get(\"axis\", -1)\n        super(Conv2DTransposeQuantizeWrapper, self).__init__(layer, **kwargs)\n\n    def build(self, input_shape):\n        super(Conv2DTransposeQuantizeWrapper, self).build(input_shape)\n\n        self._weight_vars = []\n        self.input_vars = {}\n        self.output_vars = {}\n        self.channel_axis = -1\n        if self.kernel_type == \"depthwise_kernel\":\n            self.channel_axis = 2\n        # quantize weights only applicable for weighted ops.\n        # By default weights is per channel quantization\n        if self.quantize_weights:\n            # get kernel weights dims.\n            kernel_weights = getattr(self.layer, self.kernel_type)\n            min_weight = self.layer.add_weight(\n                kernel_weights.name.split(\":\")[0] + \"_min\",\n                shape=(kernel_weights.shape[self.channel_axis]),\n                initializer=tf.keras.initializers.Constant(-6.0),\n                trainable=False,\n            )\n            max_weight = self.layer.add_weight(\n                kernel_weights.name.split(\":\")[0] + \"_max\",\n                shape=(kernel_weights.shape[self.channel_axis]),\n                initializer=tf.keras.initializers.Constant(6.0),\n                trainable=False,\n            )\n            quantizer_vars = {\"min_var\": min_weight, \"max_var\": max_weight}\n            self._weight_vars.append((kernel_weights, quantizer_vars))\n            # Needed to ensure unquantized weights get trained as part of the wrapper.\n            self._trainable_weights.append(kernel_weights)\n\n        # By default input is per tensor quantization\n        if self.quantize_inputs:\n            input_min_weight = self.layer.add_weight(\n                self.layer.name + \"_ip_min\",\n                shape=None,\n                initializer=tf.keras.initializers.Constant(-6.0),\n                trainable=False,\n            )\n            input_max_weight = self.layer.add_weight(\n                self.layer.name + \"_ip_max\",\n                shape=None,\n                initializer=tf.keras.initializers.Constant(6.0),\n                trainable=False,\n            )\n            self.input_vars[\"min_var\"] = input_min_weight\n            self.input_vars[\"max_var\"] = input_max_weight\n\n    def call(self, inputs, training=None):\n        if training is None:\n            training = tf.keras.backend.learning_phase()\n\n        # Quantize all weights, and replace them in the underlying layer.\n        if self.quantize_weights:\n            quantized_weights = []\n            quantized_weight = self._last_value_quantizer(\n                self._weight_vars[0][0],\n                training,\n                self._weight_vars[0][1],\n                per_channel=True,\n                channel_axis=self.channel_axis\n            )\n            quantized_weights.append(quantized_weight)\n            # Replace the original weights with QDQ weights\n            setattr(self.layer, self.kernel_type, quantized_weights[0])\n\n        # Quantize inputs to the conv layer\n        if self.quantize_inputs:\n            quantized_inputs = self._last_value_quantizer(\n                inputs, \n                training,\n                self.input_vars,\n                per_channel=False)\n        else:\n            quantized_inputs = inputs\n\n        args = tf_inspect.getfullargspec(self.layer.call).args\n        if \"training\" in args:\n            outputs = self.layer.call(quantized_inputs, training=training)\n        else:\n            outputs = self.layer.call(quantized_inputs)\n\n        return outputs\n```\n\n----------------------------------------\n\nTITLE: Saving and Converting Quantized Model to ONNX\nDESCRIPTION: This code saves the quantized INT8 model in TensorFlow format and converts it to ONNX format. It also clears the TensorFlow session.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/notebooks/simple_network_quantize_full.ipynb#2025-04-06_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Save TF INT8 original model\ntf.keras.models.save_model(q_nn_model, assets.simple_network_quantize_full.int8_saved_model)\n\n# Convert INT8 model to ONNX\nutils.convert_saved_model_to_onnx(saved_model_dir = assets.simple_network_quantize_full.int8_saved_model, onnx_model_path = assets.simple_network_quantize_full.int8_onnx_model)\n\ntf.keras.backend.clear_session()\n```\n\n----------------------------------------\n\nTITLE: Exporting PyTorch Model to ONNX Format\nDESCRIPTION: Exports the PyTorch ResNet50 model to ONNX format using the dummy input batch, creating a file named 'resnet50_pytorch.onnx'.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/2. Using PyTorch through ONNX.ipynb#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# export the model to ONNX\ntorch.onnx.export(resnet50, dummy_input, \"resnet50_pytorch.onnx\", verbose=False)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Memory Traffic and Layer Sizes\nDESCRIPTION: Code that creates multiple visualizations showing weights sizes, activations sizes, and their distributions across layers. Also provides statistical information about layer sizes.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/tutorial.ipynb#2025-04-06_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ntrex.plotting.plotly_bar2(\n    plan.df, \n    \"Weights Sizes Per Layer\", \n    \"weights_size\", \"Name\", \n    color='type', \n    colormap=trex.colors.layer_colormap)\n\ntrex.plotting.plotly_bar2(\n    plan.df, \n    \"Activations Sizes Per Layer\", \n    \"total_io_size_bytes\", \n    \"Name\", \n    color='type', \n    colormap=trex.colors.layer_colormap)\n\ntrex.plotting.plotly_hist(\n    plan.df, \n    \"Layer Activations Sizes Distribution\", \n    \"total_io_size_bytes\", \n    \"Size (bytes)\", \n    color='type', \n    colormap=trex.colors.layer_colormap)\n\nplan.df[\"total_io_size_bytes\"].describe()\n```\n\n----------------------------------------\n\nTITLE: Timing TensorRT Inference with IPython Magic Command in Python\nDESCRIPTION: This code uses the IPython %%timeit magic command to measure the execution time of the predict function. It provides performance metrics for the TensorRT inference process.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/2. Using PyTorch through ONNX.ipynb#2025-04-06_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n%%timeit\n\npred = predict(preprocessed_images)\n```\n\n----------------------------------------\n\nTITLE: Saving Quantized Model and Converting to ONNX\nDESCRIPTION: This snippet saves the fine-tuned quantized model in TensorFlow format and converts it to ONNX format.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/notebooks/simple_network_quantize_partial.ipynb#2025-04-06_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Save TF INT8 original model\ntf.keras.models.save_model(q_nn_model, assets.simple_network_quantize_partial.int8_saved_model)\n\n# Convert INT8 model to ONNX\nutils.convert_saved_model_to_onnx(saved_model_dir = assets.simple_network_quantize_partial.int8_saved_model, onnx_model_path = assets.simple_network_quantize_partial.int8_onnx_model)\n\ntf.keras.backend.clear_session()\n```\n\n----------------------------------------\n\nTITLE: Implementing Multiple Tactics for TensorRT Plugins with Autotuning\nDESCRIPTION: Example of creating a TensorRT plugin with multiple tactics (implementation backends) using the autotune decorator. This shows how to define different tactics (TORCH and TRITON) for a circular padding operation and let TensorRT select the best one.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/quickly_deployable_plugins/README.md#2025-04-06_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport tensorrt.plugin as trtp\nfrom enum import IntEnum\n\nclass Tactic(IntEnum):\n    TORCH = 1\n    TRITON = 2\n\n@trt.plugin.autotune(\"sample::circ_pad_plugin\")\ndef circ_pad_plugin_autotune(inp0: trtp.TensorDesc, pads: npt.NDArray[np.int32], outputs: Tuple[trtp.TensorDesc]) -> List[trtp.AutoTuneCombination]:\n    c = trtp.AutoTuneCombination()\n    c.pos([0, 1], \"FP32|FP16\")\n    c.tactics([int(Tactic.TORCH), int(Tactic.TRITON)])\n    return [c]\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine from ONNX Model\nDESCRIPTION: This command uses trtexec to build a TensorRT engine from an ONNX model file. It specifies the input ONNX file, output engine file, and optimal input shapes for the model.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/SemanticSegmentation/tutorial-runtime.ipynb#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n!trtexec --onnx=fcn-resnet101.onnx --saveEngine=fcn-resnet101.engine --optShapes=input:1x3x1026x1282\n```\n\n----------------------------------------\n\nTITLE: Building Serialized TensorRT Network\nDESCRIPTION: Code to build a serialized TensorRT engine from the network definition and configuration. This creates an optimized plan for inference.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleOnnxMNIST/README.md#2025-04-06_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nSampleUniquePtr<IHostMemory> plan{builder->buildSerializedNetwork(*network, *config)};\n```\n\n----------------------------------------\n\nTITLE: Setting Dynamic Range for Network Tensors\nDESCRIPTION: Code to set dynamic ranges for network input tensors and per-layer tensors using the tensor map values.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleINT8API/README.md#2025-04-06_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nstring input_name = network->getInput(i)->getName();\nnetwork->getInput(i)->setDynamicRange(-tensorMap.at(input_name), tensorMap.at(input_name));\n\nstring tensor_name = network->getLayer(i)->getOutput(j)->getName();\nnetwork->getLayer(i)->getOutput(j)->setDynamicRange(-tensorMap.at(name), tensorMap.at(name));\n```\n\n----------------------------------------\n\nTITLE: Generating ResNet ONNX Models\nDESCRIPTION: Command to run the Python script that generates various QAT ResNet18 ONNX model variations.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/examples/pytorch/resnet/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 <path-to-trex>/examples/pytorch/resnet/resnet_example.py\n```\n\n----------------------------------------\n\nTITLE: Running Object Detection Inference with TensorRT Engine\nDESCRIPTION: Python command for performing object detection on images using a TensorRT engine. Supports single image or directory input, with configurable NMS and IoU thresholds.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/detectron2/README.md#2025-04-06_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython infer.py \\\n    --engine /path/to/engine.trt \\\n    --input /path/to/images \\\n    --det2_config /detectron2/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml \\\n    --output /path/to/output \\\n```\n\n----------------------------------------\n\nTITLE: Quantizing Layers by Layer Class\nDESCRIPTION: Shows how to quantize all layers of a specific class using QuantizationSpec. This example targets all Conv2D layers in the model for quantization by specifying the class name rather than individual layer names.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/qspec.rst#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# 1. Create QuantizationSpec object and add layer class\nq_spec = QuantizationSpec()\nq_spec.add(name='Conv2D', is_keras_class=True)\n\n# 2. Quantize model\nq_model = quantize_model(model, quantization_mode='partial', quantization_spec=q_spec)\nq_model.summary()\n\ntf.keras.backend.clear_session()\n```\n\n----------------------------------------\n\nTITLE: Using TREx API to Get Layers by Type\nDESCRIPTION: Demonstrates the TREx API method for filtering layers by their type, as an alternative to DataFrame queries.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/api-examples.ipynb#2025-04-06_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nconvs2 = plan.get_layers_by_type('Convolution')\nprint(f\"There are {len(convs2)} convolutions\")\nprint(convs['latency.avg_time'].median())\n```\n\n----------------------------------------\n\nTITLE: Defining Calibration Functions for Quantized Model in Python\nDESCRIPTION: This snippet defines two functions: collect_stats for feeding data to the network and collecting statistics, and compute_amax for loading calibration results and computing activation max values.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/examples/calibrate_quant_resnet50.ipynb#2025-04-06_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef collect_stats(model, data_loader, num_batches):\n    \"\"\"Feed data to the network and collect statistic\"\"\"\n\n    # Enable calibrators\n    for name, module in model.named_modules():\n        if isinstance(module, quant_nn.TensorQuantizer):\n            if module._calibrator is not None:\n                module.disable_quant()\n                module.enable_calib()\n            else:\n                module.disable()\n\n    for i, (image, _) in tqdm(enumerate(data_loader), total=num_batches):\n        model(image.cuda())\n        if i >= num_batches:\n            break\n\n    # Disable calibrators\n    for name, module in model.named_modules():\n        if isinstance(module, quant_nn.TensorQuantizer):\n            if module._calibrator is not None:\n                module.enable_quant()\n                module.disable_calib()\n            else:\n                module.enable()\n            \ndef compute_amax(model, **kwargs):\n    # Load calib result\n    for name, module in model.named_modules():\n        if isinstance(module, quant_nn.TensorQuantizer):\n            if module._calibrator is not None:\n                if isinstance(module._calibrator, calib.MaxCalibrator):\n                    module.load_calib_amax()\n                else:\n                    module.load_calib_amax(**kwargs)\n#             print(F\"{name:40}: {module}\")\n    model.cuda()\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for BERT\nDESCRIPTION: Creates a TensorRT engine from a BERT checkpoint with specified parameters such as batch size, sequence length, and precision mode.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p engines && python3 builder.py -m models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_128_v19.03.1/model.ckpt -o engines/bert_large_128.engine -b 1 -s 128 --fp16 -c models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_128_v19.03.1\n```\n\n----------------------------------------\n\nTITLE: Creating ONNX Model\nDESCRIPTION: These commands create a directory for the ONNX model and convert the TensorFlow saved model to ONNX format using the create_onnx.py script.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/EfficientDet/notebooks/EfficientDet-TensorRT8.ipynb#2025-04-06_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n![ ! -d \"onnx_model\" ] && mkdir onnx_model\n\n!python3 $TRT_OSSPATH/samples/python/efficientdet/create_onnx.py \\\n    --saved_model ./tf_model/ \\\n    --onnx ./onnx_model/model.onnx \\\n    --input_size '512,512'\n```\n\n----------------------------------------\n\nTITLE: Using Functional API for TensorRT Engine Creation\nDESCRIPTION: Equivalent code using Polygraphy's functional API to parse an ONNX network, create a TensorRT config, and build a TensorRT engine.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/06_immediate_eval_api/README.md#2025-04-06_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nbuilder, network, parser = network_from_onnx_path(\"/path/to/model.onnx\")\nconfig = create_config(builder, network, fp16=True, tf32=True)\nengine = engine_from_network((builder, network, parser), config)\n```\n\n----------------------------------------\n\nTITLE: Loading TensorRT Engine for Inference\nDESCRIPTION: Loads the converted TensorRT engine file, creates a runtime environment, and prepares an execution context for running inference.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/2. Using PyTorch through ONNX.ipynb#2025-04-06_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n%%time\n\nimport tensorrt as trt\nimport pycuda.driver as cuda\nimport pycuda.autoinit\n\nf = open(\"resnet_engine_pytorch.trt\", \"rb\")\nruntime = trt.Runtime(trt.Logger(trt.Logger.WARNING)) \n\nengine = runtime.deserialize_cuda_engine(f.read())\ncontext = engine.create_execution_context()\n```\n\n----------------------------------------\n\nTITLE: Modified ResNet50 Constructor\nDESCRIPTION: Modified ResNet50 constructor with quantization support for TensorRT optimization.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/tutorials/quant_resnet50.rst#2025-04-06_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef resnet50(pretrained: bool = False, progress: bool = True, quantize: bool = False, **kwargs: Any) -> ResNet:\n    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress, quantize, **kwargs)\ndef _resnet(arch: str, block: Type[Union[BasicBlock, Bottleneck]], layers: List[int], pretrained: bool, progress: bool,\n            quantize: bool, **kwargs: Any) -> ResNet:\n    model = ResNet(block, layers, quantize, **kwargs)\nclass ResNet(nn.Module):\n    def __init__(self,\n                 block: Type[Union[BasicBlock, Bottleneck]],\n                 layers: List[int],\n                 quantize: bool = False,\n                 num_classes: int = 1000,\n                 zero_init_residual: bool = False,\n                 groups: int = 1,\n                 width_per_group: int = 64,\n                 replace_stride_with_dilation: Optional[List[bool]] = None,\n                 norm_layer: Optional[Callable[..., nn.Module]] = None) -> None:\n```\n\n----------------------------------------\n\nTITLE: Benchmarking BERT-large with Small Batch Size\nDESCRIPTION: This snippet runs a performance benchmark on the BERT-large model with sequence length 384 using a batch size of 1. It uses the perf.py script to measure inference latency.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/BERT-TRT-FP16.ipynb#2025-04-06_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nBATCH_SIZE=1\n!python3 ../perf.py -e ./engines_$TRT_VERSION/bert_large_384.engine -b $BATCH_SIZE -s 384 -i 100 -w 20\n```\n\n----------------------------------------\n\nTITLE: Building and Refitting TensorRT Engine with CPU Weights\nDESCRIPTION: Command to build a TensorRT engine from the modified ONNX model, refit it using CPU weights, and run inference with the refitted engine.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/engine_refit_onnx_bidaf/README.md#2025-04-06_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 build_and_refit_engine.py --weights-location CPU\n```\n\n----------------------------------------\n\nTITLE: Verifying TensorRT Version\nDESCRIPTION: This Python command imports TensorRT and prints its version number to confirm the installation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/EfficientDet/notebooks/EfficientDet-TensorRT8.ipynb#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!python3 -c 'import tensorrt; print(\"TensorRT version: {}\".format(tensorrt.__version__))'\n```\n\n----------------------------------------\n\nTITLE: Finding Top Time-Consuming Layers\nDESCRIPTION: Code that identifies and displays the three layers consuming the most execution time in the engine plan, sorted by percentage of total time.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/tutorial.ipynb#2025-04-06_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntop3 = plan.df.nlargest(3, 'latency.pct_time')\ntrex.notebook.display_df(top3)\n```\n\n----------------------------------------\n\nTITLE: Preparing ONNX Model for TensorRT Refitting\nDESCRIPTION: Command to run the preparation script that modifies the ONNX model by replacing unsupported nodes to make it compatible with TensorRT.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/engine_refit_onnx_bidaf/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 prepare_model.py\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine and Running Inference in Bash\nDESCRIPTION: Executes the Python script that builds a TensorRT engine from the ONNX model and performs inference on a sample image. This script handles the TensorRT optimization and the actual object detection process.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/yolov3_onnx/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 onnx_to_tensorrt.py\n```\n\n----------------------------------------\n\nTITLE: Converting Tensor to BFloat16\nDESCRIPTION: Creates a constant tensor with float32 values and sets export type to BFloat16. Shows two equivalent methods for setting the export data type.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/examples/12_using_numpy_unsupported_dtypes/README.md#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntensor = gs.Constant(name=\"weight\", values=np.ones(shape=(5, 3, 3, 3), dtype=np.float32), export_dtype=onnx.TensorProto.BFLOAT16)\n# or\ntensor = gs.Constant(name=\"weight\", values=np.ones(shape=(5, 3, 3, 3), dtype=np.float32))\ntensor.export_dtype = onnx.TensorProto.BFLOAT16\n```\n\n----------------------------------------\n\nTITLE: Benchmarking TensorRT-Optimized BERT Model with Batch Size 64\nDESCRIPTION: This code snippet runs a performance benchmark on a TensorRT-optimized BERT model with a batch size of 64, sequence length of 384, and 1000 iterations. It's used to measure the inference throughput for batch processing.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/BERT-TRT-INT8-QAT-sparse.ipynb#2025-04-06_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n!python3 ../perf_varseqlen.py -e ./engines_$TRT_VERSION/megatron_large_seqlen384_int8qat_sparse.engine -b 64 -s 384 -i 1000 -w 500\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Images for TensorRT\nDESCRIPTION: Defines a function to normalize images according to PyTorch's standard normalization for pre-trained models, converting the results to FP16 precision.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/2. Using PyTorch through ONNX.ipynb#2025-04-06_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torchvision.transforms import Normalize\n\ndef preprocess_image(img):\n    norm = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    result = norm(torch.from_numpy(img).transpose(0,2).transpose(1,2))\n    return np.array(result, dtype=np.float16)\n\npreprocessed_images = np.array([preprocess_image(image) for image in input_batch])\n```\n\n----------------------------------------\n\nTITLE: Accessing Layer Input Information\nDESCRIPTION: Shows how to access the input tensors for a specific layer using DataFrame indexing.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/api-examples.ipynb#2025-04-06_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprint(convs.iloc[0]['Inputs'])\n```\n\n----------------------------------------\n\nTITLE: Building INT8-QAT Sparse TensorRT Engine for BERT\nDESCRIPTION: Builds a TensorRT engine with INT8 precision, Quantization Aware Training (QAT) scales, and structured sparsity optimizations. Uses a pre-trained Megatron model with maximum sequence length of 384 and a batch size of 128.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/BERT-TRT-INT8-QAT-sparse.ipynb#2025-04-06_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n!export CKPT_PATH=models/fine-tuned/bert_pyt_statedict_megatron_sparse_int8qat_v21.03.0/bert_pyt_statedict_megatron_sparse_int8_qat\n!python3 ../builder_varseqlen.py -w 40000 -c models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1 -b $BATCH_SIZE -s 384 -o engines_$TRT_VERSION/megatron_large_seqlen384_int8qat_sparse.engine --fp16 --int8 --strict -il --megatron --pickle models/fine-tuned/bert_pyt_statedict_megatron_sparse_int8qat_v21.03.0/bert_pyt_statedict_megatron_sparse_int8_qat -v models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/vocab.txt -sp\n\n```\n\n----------------------------------------\n\nTITLE: Loading and Running Pre-built TensorRT Engine\nDESCRIPTION: This command executes the Python script that loads a previously built TensorRT engine and runs inference.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/00_inference_with_tensorrt/README.md#2025-04-06_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython3 load_and_run.py\n```\n\n----------------------------------------\n\nTITLE: Creating ONNX Custom Nodes with TensorRT Plugins using ONNX GraphSurgeon\nDESCRIPTION: Example of creating a custom ONNX node that maps to a TensorRT plugin using ONNX GraphSurgeon. The node is configured with the same name as the plugin and includes a plugin_namespace attribute.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/quickly_deployable_plugins/README.md#2025-04-06_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport onnx_graphsurgeon as gs\n\nvar_x = gs.Variable(name=\"x\", shape=inp_shape, dtype=np.float32)\nvar_y = gs.Variable(name=\"y\", dtype=np.float32)\n\ncirc_pad_node = gs.Node(\n    name=\"circ_pad_plugin\",\n    op=\"circ_pad_plugin\",\n    inputs=[var_x],\n    outputs=[var_y],\n    attrs={\"pads\": pads, \"plugin_namespace\": \"sample\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for BERT-base\nDESCRIPTION: This snippet builds an optimized TensorRT engine for the BERT-base model with sequence length 128. It uses the builder.py script with FP16 precision and supports both single and multi-batch inference.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/BERT-TRT-FP16.ipynb#2025-04-06_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n!python3 ../builder.py -m models/fine-tuned/bert_tf_ckpt_base_qa_squad2_amp_128_v19.03.1/model.ckpt -w 40000 -o engines_$TRT_VERSION/bert_base_128.engine -b 1 -b $BATCH_SIZE -s 128 --fp16 -c models/fine-tuned/bert_tf_ckpt_base_qa_squad2_amp_128_v19.03.1\n```\n\n----------------------------------------\n\nTITLE: Implementing TensorRT Prediction Function with CUDA in Python\nDESCRIPTION: This function handles the prediction process using TensorRT and CUDA. It copies input data to the GPU, executes the model, and transfers the results back to the CPU. The function is designed for asynchronous execution and includes stream synchronization.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/2. Using PyTorch through ONNX.ipynb#2025-04-06_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef predict(batch): # result gets copied into output\n    # transfer input data to device\n    cuda.memcpy_htod_async(d_input, batch, stream)\n    # execute model\n    context.execute_async_v3(stream.handle)\n    # transfer predictions back\n    cuda.memcpy_dtoh_async(output, d_output, stream)\n    # syncronize threads\n    stream.synchronize()\n    \n    return output\n```\n\n----------------------------------------\n\nTITLE: Benchmarking BERT-base Model Performance\nDESCRIPTION: This snippet runs a performance benchmark on the optimized BERT-base model with sequence length 128 using a batch size of 1. It uses the perf.py script to measure inference latency over 100 iterations with 20 warmup iterations.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/BERT-TRT-FP16.ipynb#2025-04-06_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n!python3 ../perf.py -e ./engines_$TRT_VERSION/bert_base_128.engine -b 1 -s 128 -i 100 -w 20\n```\n\n----------------------------------------\n\nTITLE: Running QAT Workflow for Inception Models in Python\nDESCRIPTION: This command executes the QAT workflow script, which quantizes the model, fine-tunes it, and saves the final graph in SavedModel format. It also handles the conversion to ONNX automatically.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/examples/inception/README.md#2025-04-06_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npython run_qat_workflow.py\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine from ONNX Model with INT8 Precision\nDESCRIPTION: Command to convert an ONNX model into a TensorRT engine with INT8 precision. This command also measures the engine's latency performance and outputs a verbose log for debugging.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/examples/README.md#2025-04-06_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\ntrtexec --onnx=model_qat.onnx --int8 --saveEngine=model_qat.engine --verbose\n```\n\n----------------------------------------\n\nTITLE: Benchmarking TensorRT Engine Performance\nDESCRIPTION: Uses trtexec to benchmark the performance of the built TensorRT engine, measuring GPU compute time across multiple iterations.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/detectron2/README.md#2025-04-06_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ntrtexec \\\n    --loadEngine=/path/to/engine.trt \\\n    --useCudaGraph --noDataTransfers \\\n    --iterations=100 --avgRuns=100\n```\n\n----------------------------------------\n\nTITLE: Quantizing Specific Layers of the Model\nDESCRIPTION: This snippet demonstrates how to quantize only specific layers ('conv2d_2' and 'dense') of the model using QuantizationSpec.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/notebooks/simple_network_quantize_partial.ipynb#2025-04-06_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Quantize model\n# 1.1 Create a dictionary to quantize only two layers named 'conv2d_2' and 'dense'\nqspec = QuantizationSpec()\nlayer_name = ['conv2d_2', 'dense']\nqspec.add(name=layer_name)\n# 1.2 Call quantize model function\nq_nn_model = quantize_model(\n    model=nn_model_original, quantization_mode=\"partial\", quantization_spec=qspec)\n\nq_nn_model.compile(\n    optimizer=tiny_resnet.optimizer(lr=1e-4),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[\"accuracy\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Extracting an ONNX Subgraph with Known Shapes and Types\nDESCRIPTION: Example command demonstrating how to extract a subgraph from an ONNX model with explicitly specified tensor shapes and data types.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/surgeon/01_isolating_subgraphs/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy surgeon extract model.onnx \\\n    --inputs x1:[1,3,224,224]:float32 \\\n    --outputs add_out:float32 \\\n    -o subgraph.onnx\n```\n\n----------------------------------------\n\nTITLE: Generating and Rendering DOT Graphs for ResNet18 Engine Plans\nDESCRIPTION: This snippet generates and renders DOT graphs for each ResNet18 engine plan. It uses the to_dot function to create the graph, configures it to display regions and expand layer details, and then renders the graph as an SVG using the render_dot function.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/examples/pytorch/resnet/example_qat_resnet18.ipynb#2025-04-06_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfor plan in plans:\n    graph = to_dot(plan, layer_type_formatter, display_regions=True, expand_layer_details=True)\n    render_dot(graph, plan.name, 'svg')\n```\n\n----------------------------------------\n\nTITLE: Running Model Comparison with JSON Input File\nDESCRIPTION: Command to run model comparison between TensorRT and ONNX-Runtime using pre-generated inputs from a JSON file. Includes configuration for TensorRT optimization profiles with dynamic shapes.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/run/05_comparing_with_custom_input_data/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run dynamic_identity.onnx --trt --onnxrt \\\n    --trt-min-shapes X:[1,2,28,28] --trt-opt-shapes X:[1,2,28,28] --trt-max-shapes X:[1,2,28,28] \\\n    --load-inputs custom_inputs.json\n```\n\n----------------------------------------\n\nTITLE: Adding Embedding Layer Normalization Plugin Sources in TensorRT CMake\nDESCRIPTION: This CMake command adds source files for TensorRT's Embedding Layer Normalization plugins to the build system. It includes CUDA kernel files (.cu), plugin implementation files (.cpp), and header files (.h) for both regular and variable sequence length versions of the plugin, as well as legacy implementations.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/embLayerNormPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_plugin_source(\n    embLayerNormKernel.cu\n    embLayerNormPlugin.cpp\n    embLayerNormPlugin.h\n    embLayerNormPluginLegacy.cpp\n    embLayerNormPluginLegacy.h\n    embLayerNormVarSeqlenKernelHFace.cu\n    embLayerNormVarSeqlenKernelMTron.cu\n    embLayerNormVarSeqlenPlugin.cpp\n    embLayerNormVarSeqlenPlugin.h\n    embLayerNormVarSeqlenPluginLegacy.cpp\n    embLayerNormVarSeqlenPluginLegacy.h\n)\n```\n\n----------------------------------------\n\nTITLE: Comparing TensorRT FP16 Precision with ONNX-Runtime using Polygraphy\nDESCRIPTION: Command to build a TensorRT engine with FP16 precision for comparison against ONNX-Runtime, demonstrating how to evaluate precision impact on model outputs.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/run/01_comparing_frameworks/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run dynamic_identity.onnx --trt --fp16 --onnxrt \\\n    --input-shapes X:[1,2,4,4]\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for BERT PTQ (Volta GPU)\nDESCRIPTION: This command builds a TensorRT engine for BERT PTQ on Volta GPU. It doesn't support QKVToContextPlugin or SkipLayerNormPlugin running with INT8 I/O.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p engines && python3 builder.py -m models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/model.ckpt -o engines/bert_large_384_int8mix.engine -b 1 -s 384 --int8 --fp16 --strict -c models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1 --squad-json ./squad/train-v1.1.json -v models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/vocab.txt --calib-num 100\n```\n\n----------------------------------------\n\nTITLE: Using Data Loader from API Example\nDESCRIPTION: Command showing how to use a data loader function from an external API example by specifying the exact function name when it's not the default 'load_data'.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/convert/01_int8_calibration_in_tensorrt/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy convert identity.onnx --int8 \\\n    --data-loader-script ../../../api/04_int8_calibration_in_tensorrt/example.py:calib_data \\\n    -o identity.engine\n```\n\n----------------------------------------\n\nTITLE: Constraining Precisions with Network Postprocessing Script in Polygraphy\nDESCRIPTION: This command uses a TensorRT network postprocessing script to apply precisions on the parsed network. It runs ONNX-Runtime and TensorRT with FP16 optimizations, obeying precision constraints.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/run/08_adding_precision_constraints/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run needs_constraints.onnx --onnxrt --trt --fp16 --precision-constraints obey \\\n    --val-range x:[1,2] --check-error-stat median \\\n    --trt-network-postprocess-script ./add_constraints.py\n```\n\n----------------------------------------\n\nTITLE: Loading Saved Inputs and Comparing Outputs\nDESCRIPTION: Runs the model again using previously saved inputs and compares the outputs against saved results from the first run.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/run/02_comparing_across_runs/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run identity.onnx --onnxrt \\\n    --load-inputs inputs.json --load-outputs run_0_outputs.json\n```\n\n----------------------------------------\n\nTITLE: Running Inference with TensorRT in Python\nDESCRIPTION: Command for classifying images using a TensorRT engine. The script takes an engine file, input images, and preprocessor type to run inference and outputs class predictions with confidence scores.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/efficientnet/README.md#2025-04-06_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npython3 infer.py \\\n    --engine /path/to/engine.trt \\\n    --input /path/to/images \\\n    --preprocessor V2\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning Quantized TensorFlow Model\nDESCRIPTION: This code fine-tunes the quantized model for a specified number of epochs and evaluates its accuracy after fine-tuning.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/notebooks/simple_network_quantize_specific_class.ipynb#2025-04-06_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Fine-tune quantized model\nfine_tune_epochs = 2\nq_nn_model.fit(\n    train_images,\n    train_labels,\n    batch_size=32,\n    epochs=fine_tune_epochs,\n    validation_split=0.1,\n)\n_, q_model_accuracy = q_nn_model.evaluate(test_images, test_labels, verbose=0)\nq_model_accuracy = round(100 * q_model_accuracy, 2)\nprint(\n    \"Accuracy after fine tuning for {} epochs :{}\".format(\n        fine_tune_epochs, q_model_accuracy\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Building and Deserializing TensorRT Prediction Engine\nDESCRIPTION: Builds a serialized network for the MNIST prediction model and deserializes it into a runtime engine. Performs error checking to ensure successful engine creation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleDynamicReshape/README.md#2025-04-06_snippet_10\n\nLANGUAGE: C++\nCODE:\n```\nSampleUniquePtr<nvinfer1::IHostMemory> predictionPlan = makeUnique(builder->buildSerializedNetwork(*network, *config));\nif (!predictionPlan)\n{\n    sample::gLogError << \"Prediction serialized engine build failed.\" << std::endl;\n    return false;\n}\n\nmPredictionEngine = makeUnique(\n    runtime->deserializeCudaEngine(predictionPlan->data(), predictionPlan->size()));\nif (!mPredictionEngine)\n{\n    sample::gLogError << \"Prediction engine deserialization failed.\" << std::endl;\n    return false;\n}\n```\n\n----------------------------------------\n\nTITLE: Building INT8 Precision TensorRT Engine with Calibration\nDESCRIPTION: Command to build and calibrate a TensorRT engine for INT8 precision, using calibration images and optional calibration cache.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/efficientdet/README.md#2025-04-06_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npython3 build_engine.py \\\n    --onnx /path/to/model.onnx \\\n    --engine /path/to/engine.trt \\\n    --precision int8 \\\n    --calib_input /path/to/calibration/images \\\n    --calib_cache /path/to/calibration.cache\n```\n\n----------------------------------------\n\nTITLE: Running Constant Folding with Polygraphy Surgeon\nDESCRIPTION: This command uses polygraphy surgeon to perform constant folding on the original model, which is a required step before identifying and handling unbounded DDS.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/surgeon/04_setting_upper_bounds/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy surgeon sanitize model.onnx -o folded.onnx --fold-constants\n```\n\n----------------------------------------\n\nTITLE: Calibrating Quantized ResNet50 Model in Python\nDESCRIPTION: This snippet performs the calibration of the quantized ResNet50 model using the collect_stats and compute_amax functions. It uses the percentile method with 99.99 percentile.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/examples/calibrate_quant_resnet50.ipynb#2025-04-06_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# It is a bit slow since we collect histograms on CPU\nwith torch.no_grad():\n    collect_stats(model, data_loader, num_batches=2)\n    compute_amax(model, method=\"percentile\", percentile=99.99)\n```\n\n----------------------------------------\n\nTITLE: Setting Dynamic Input Shape for TensorRT Execution Context\nDESCRIPTION: Specifies the actual shape of the current input to the preprocessor execution context before running inference. This is necessary for engines with dynamic input shapes.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleDynamicReshape/README.md#2025-04-06_snippet_12\n\nLANGUAGE: C++\nCODE:\n```\nmPreprocessorContext->setInputShape(inputTensorName, inputDims);\n```\n\n----------------------------------------\n\nTITLE: Sample Inference Output Display\nDESCRIPTION: Example output showing tensor statistics including shape, dtype, and statistical measures like mean, standard deviation, variance, median, min/max values, and percentiles.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/inspect/05_inspecting_inference_outputs/README.md#2025-04-06_snippet_2\n\nLANGUAGE: text\nCODE:\n```\n[I] ==== Run Results (1 runners) ====\n\n    ---- onnxrt-runner-N0-07/15/21-10:46:07 (1 iterations) ----\n\n    y [dtype=float32, shape=(1, 1, 2, 2)] | Stats: mean=0.35995, std-dev=0.25784, var=0.066482, median=0.35968, min=0.00011437 at (0, 0, 1, 0), max=0.72032 at (0, 0, 0, 1), avg-magnitude=0.35995, p90=0.62933, p95=0.67483, p99=0.71123\n        [[[[4.17021990e-01 7.20324516e-01]\n           [1.14374816e-04 3.02332580e-01]]]]\n```\n\n----------------------------------------\n\nTITLE: Creating Calibration Profile for INT8 Quantization in TensorRT\nDESCRIPTION: Creates an optimization profile specifically for calibration with fixed batch size. This profile is used during INT8 calibration to ensure consistent input dimensions.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleDynamicReshape/README.md#2025-04-06_snippet_6\n\nLANGUAGE: C++\nCODE:\n```\nauto profileCalib = builder->createOptimizationProfile();\nconst int calibBatchSize{256};\nprofileCalib->setDimensions(input->getName(), OptProfileSelector::kMIN, Dims4{calibBatchSize, 1, 28, 28});\nprofileCalib->setDimensions(input->getName(), OptProfileSelector::kOPT, Dims4{calibBatchSize, 1, 28, 28});\nprofileCalib->setDimensions(input->getName(), OptProfileSelector::kMAX, Dims4{calibBatchSize, 1, 28, 28});\npreprocessorConfig->setCalibrationProfile(profileCalib);\n```\n\n----------------------------------------\n\nTITLE: Parsing ONNX Model File in TensorRT\nDESCRIPTION: Parses the ONNX model file to populate the TensorRT network. Uses the file path locator and logger to handle parsing errors.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleDynamicReshape/README.md#2025-04-06_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nparser->parseFromFile(locateFile(mParams.onnxFileName, mParams.dataDirs).c_str(), static_cast<int>(sample::gLogger.getReportableSeverity()));\n```\n\n----------------------------------------\n\nTITLE: Running BERT QA Inference with TensorRT Optimized Engine\nDESCRIPTION: Executes inference on a sample passage and question using the optimized TensorRT engine. Demonstrates how to use the model to answer the question \"What is TensorRT?\" based on the provided passage.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/BERT-TRT-INT8-QAT-sparse.ipynb#2025-04-06_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nPASSAGE = 'TensorRT is a high performance deep learning inference platform that delivers low latency and high throughput for apps'\\\n'such as recommenders, speech and image/video on NVIDIA GPUs. It includes parsers to import models, and plugins to support novel ops'\\\n'and layers before applying optimizations for inference. Today NVIDIA is open-sourcing parsers and plugins in TensorRT so that the deep'\\\n'learning community can customize and extend these components to take advantage of powerful TensorRT optimizations for your apps.'\nQUESTION=\"What is TensorRT?\"\n\n!python3 ../inference_varseqlen.py -e engines_$TRT_VERSION/megatron_large_seqlen384_int8qat_sparse.engine  -p $PASSAGE -q $QUESTION -v models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/vocab.txt -s 256\n```\n\n----------------------------------------\n\nTITLE: Bounding Box Encoding for CodeTypeSSD::CORNER Method in TensorRT\nDESCRIPTION: Mathematical formulation for the CORNER coding method used in nmsPlugin. It shows how bounding boxes are encoded with and without variance encoding, representing the difference between ground truth and anchor box coordinates.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/nmsPlugin/README.md#2025-04-06_snippet_0\n\nLANGUAGE: math\nCODE:\n```\n[x_{min, gt} - x_{min, anchor}, y_{min, gt} - y_{min, anchor}, x_{max, gt} - x_{max, anchor}, y_{max, gt} - y_{max, anchor}]\n```\n\nLANGUAGE: math\nCODE:\n```\n[(x_{min, gt} - x_{min, anchor}) / variance_0, (y_{min, gt} - y_{min, anchor}) / variance_1, (x_{max, gt} - x_{max, anchor}) / variance_2, (y_{max, gt} - y_{max, anchor}) / variance_3]\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing Fashion MNIST Dataset\nDESCRIPTION: Loads the Fashion MNIST dataset from TensorFlow's keras.datasets and normalizes the pixel values to be between 0 and 1 for both training and test images.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/getting_started.ipynb#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Load Fashion MNIST dataset\nmnist = tf.keras.datasets.fashion_mnist\n(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n\n# Normalize the input image so that each pixel value is between 0 to 1.\ntrain_images = train_images / 255.0\ntest_images = test_images / 255.0   \n```\n\n----------------------------------------\n\nTITLE: Comparing TensorFlow and TensorRT EfficientDet Models\nDESCRIPTION: Command to compare detection results between TensorFlow and TensorRT implementations of EfficientDet. Processes images through both frameworks and generates visualization images showing inference results for qualitative comparison.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/efficientdet/README.md#2025-04-06_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\npython3 compare_tf.py \\\n    --engine /path/to/engine.trt \\\n    --saved_model /path/to/saved_model \\\n    --input /path/to/images \\\n    --nms_threshold 0.4 \\\n    --output /path/to/output\n```\n\n----------------------------------------\n\nTITLE: Running Inference with TensorRT Engine in Python\nDESCRIPTION: Command to perform object detection on images using a TensorRT engine with the infer.py script. Supports various preprocessing options and can output detection results as visualizations and tab-separated files.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/tensorflow_object_detection_api/README.md#2025-04-06_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npython infer.py \\\n    --engine /path/to/saved/engine.trt \\\n    --input /path/to/images \\\n    --output /path/to/output \\\n    --preprocessor fixed_shape_resizer \\\n    --labels /path/to/labels_coco.txt\n```\n\n----------------------------------------\n\nTITLE: Benchmarking FP16 Inference Performance\nDESCRIPTION: Measures the time taken for inference in FP16 precision using the %%timeit magic command.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/2. Using PyTorch through ONNX.ipynb#2025-04-06_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n%%timeit\n\nwith torch.no_grad():\n    preds = np.array(resnet50_gpu_half(input_half).cpu())\n```\n\n----------------------------------------\n\nTITLE: Running Stable Cascade with TensorRT\nDESCRIPTION: Commands to generate an image using Stable Cascade with TensorRT optimization, including support for lite versions of the models.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/Diffusion/README.md#2025-04-06_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython3 demo_stable_cascade.py --onnx-opset=16 \"Anthropomorphic cat dressed as a pilot\" --onnx-dir onnx-sc --engine-dir engine-sc\n```\n\nLANGUAGE: bash\nCODE:\n```\npython3 demo_stable_cascade.py --onnx-opset=16 \"Anthropomorphic cat dressed as a pilot\" --onnx-dir onnx-sc-lite --engine-dir engine-sc-lite --lite\n```\n\n----------------------------------------\n\nTITLE: Rendering TensorRT Engine Graph with TREX in Python\nDESCRIPTION: Converts an EnginePlan to a dot file and renders it as an SVG. This visualization shows the network structure with nodes colored by layer type or precision, providing a holistic view of the model architecture.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/tutorial.ipynb#2025-04-06_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nformatter = trex.graphing.layer_type_formatter if True else trex.graphing.precision_formatter\ngraph = trex.graphing.to_dot(plan, formatter)\nsvg_name = trex.graphing.render_dot(graph, engine_name, 'svg')\n```\n\n----------------------------------------\n\nTITLE: Creating and Training Original ResNet-like Model\nDESCRIPTION: This code creates the original ResNet-like model, compiles it, and trains it on the CIFAR10 dataset. It also evaluates the baseline model accuracy.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/notebooks/simple_network_quantize_full.ipynb#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nnn_model_original = tiny_resnet.model()\ntf.keras.utils.plot_model(nn_model_original, to_file = assets.simple_network_quantize_full.fp32 + \"/model.png\")\n\n# Train original classification model\nnn_model_original.compile(\n    optimizer=tiny_resnet.optimizer(lr=1e-4),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[\"accuracy\"],\n)\n\nnn_model_original.fit(\n    train_images, train_labels, batch_size=32, epochs=10, validation_split=0.1\n)\n\n# Get baseline model accuracy\n_, baseline_model_accuracy = nn_model_original.evaluate(\n    test_images, test_labels, verbose=0\n)\nbaseline_model_accuracy = round(100 * baseline_model_accuracy, 2)\nprint(\"Baseline FP32 model test accuracy: {}\".format(baseline_model_accuracy))\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for BERT with Variable Sequence Length (INT8)\nDESCRIPTION: This command builds a TensorRT engine for BERT with variable sequence length support using INT8 precision.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_25\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p engines && python3 builder_varseqlen.py -x models/fine-tuned/bert_pyt_onnx_large_qa_squad11_amp_fake_quant_v1/bert_large_v1_1_fake_quant.onnx -o engines/bert_varseq_int8.engine -b 1 -s 256 --int8 --fp16 -c models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1 -v models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/vocab.txt\n```\n\n----------------------------------------\n\nTITLE: Verifying Engine Determinism\nDESCRIPTION: Compares the layer attributes of both engines to verify they are identical, confirming the deterministic build process was successful.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/convert/02_deterministic_engine_builds_in_tensorrt/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndiff <(polygraphy inspect model 0.engine --show layers attrs) <(polygraphy inspect model 1.engine --show layers attrs)\n```\n\n----------------------------------------\n\nTITLE: Deploying QAT ResNet TensorRT Engine\nDESCRIPTION: Shell script for deploying a Quantization-Aware Trained (QAT) ResNet model as a TensorRT engine. Requires setting QAT_SUBDIR and ROOT_DIR variables to point to the ONNX file location.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/examples/resnet/scripts/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./scripts/deploy_engine_qat.sh\n```\n\n----------------------------------------\n\nTITLE: Running BERT Question Answering Inference\nDESCRIPTION: Command to run inference using the optimized TensorRT engine for question answering tasks with a sample question about TensorRT.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_30\n\nLANGUAGE: bash\nCODE:\n```\npython3 inference_varseqlen.py -e engines/megatron_large_seqlen384_int8qat_sparse.engine -p \"TensorRT is a high performance deep learning inference platform that delivers low latency and high throughput for apps such as recommenders, speech and image/video on NVIDIA GPUs. It includes parsers to import models, and plugins to support novel ops and layers before applying optimizations for inference. Today NVIDIA is open-sourcing parsers and plugins in TensorRT so that the deep learning community can customize and extend these components to take advantage of powerful TensorRT optimizations for your apps.\" -q \"What is TensorRT?\" -v models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/vocab.txt -s 256\n```\n\n----------------------------------------\n\nTITLE: Downloading Pre-exported ONNX Model for Stable Video Diffusion\nDESCRIPTION: Commands to download the pre-exported ONNX model for Stable Video Diffusion using Git LFS.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/Diffusion/README.md#2025-04-06_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ngit lfs install\ngit clone https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt-1-1-tensorrt onnx-svd-xt-1-1\ncd onnx-svd-xt-1-1 && git lfs pull && cd ..\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for Megatron-LM SQuAD Model\nDESCRIPTION: Commands to download model checkpoints and build a TensorRT engine for Megatron-LM with sparsity and INT8 quantization optimizations for SQuAD v2.0 task.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_29\n\nLANGUAGE: bash\nCODE:\n```\nbash ./scripts/download_model.sh 384 v1_1\nbash ./scripts/download_model.sh pyt megatron-large int8-qat sparse\nexport CKPT_PATH=models/fine-tuned/bert_pyt_statedict_megatron_sparse_int8qat_v21.03.0/bert_pyt_statedict_megatron_sparse_int8_qat\nmkdir -p engines && python3 builder_varseqlen.py -c models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1 -b 1 -s 384 -o engines/megatron_large_seqlen384_int8qat_sparse.engine --fp16 --int8 --strict -il --megatron --pickle $CKPT_PATH -v models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/vocab.txt -sp\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for TensorRT Validation\nDESCRIPTION: Commands to install TensorRT and other required dependencies for running the accuracy validation example.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/02_validating_on_a_dataset/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Analyzing Convolution Layers with TREX in Python\nDESCRIPTION: Retrieves and analyzes convolution layers using TREX's layer type-specific processing. Creates visualizations showing arithmetic intensity and data sizes of convolution layers with precision and latency color coding.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/tutorial.ipynb#2025-04-06_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nconvs = plan.get_layers_by_type('Convolution')\ntrex.notebook.display_df(convs)\n\ntrex.plotting.plotly_bar2(\n    convs, \n    \"Latency Per Layer (%)<BR>(bar color indicates precision)\",\n    \"attr.arithmetic_intensity\", \"Name\",\n    color='precision', \n    colormap=trex.colors.precision_colormap)\n\ntrex.plotting.plotly_bar2(\n    convs,\n    \"Convolution Data Sizes<BR>(bar color indicates latency)\",\n    \"total_io_size_bytes\", \n    \"Name\", \n    color='latency.pct_time');\n```\n\n----------------------------------------\n\nTITLE: Dynamic Shapes Usage Commands\nDESCRIPTION: Examples of running ONNX models with different shape configurations using trtexec, including static and dynamic input shapes.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/trtexec/README.md#2025-04-06_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./trtexec --onnx=model.onnx\n```\n\nLANGUAGE: bash\nCODE:\n```\n./trtexec --onnx=model.onnx --shapes=input:32x3x244x244\n```\n\nLANGUAGE: bash\nCODE:\n```\n./trtexec --onnx=model.onnx --minShapes=input:1x3x244x244 --optShapes=input:16x3x244x244 --maxShapes=input:32x3x244x244 --shapes=input:5x3x244x244\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine with Custom Batch Size\nDESCRIPTION: Commands to build TensorRT engines with either static or dynamic batch sizes for different inference scenarios.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/efficientdet/README.md#2025-04-06_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npython3 build_engine.py \\\n    --onnx /path/to/model.onnx \\\n    --engine /path/to/engine.trt \\\n    --batch_size 8\n```\n\n----------------------------------------\n\nTITLE: Counting Layers by Type Using TREx\nDESCRIPTION: Uses TREx's group_count function to count the number of layers of each type in the engine.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/api-examples.ipynb#2025-04-06_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Another trex convenience wrapper: group by 'type' and count the number of members in each group\ntrex.group_count(plan.df, \"type\")\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT FP16 Engine\nDESCRIPTION: Command to build TensorRT engine with FP16 precision from ONNX model.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/efficientnet/README.md#2025-04-06_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython3 build_engine.py \\\n    --onnx /path/to/model.onnx \\\n    --engine /path/to/engine.trt \\\n    --precision fp16\n```\n\n----------------------------------------\n\nTITLE: Implementing Quantized 1x1 Convolution in PyTorch\nDESCRIPTION: This function creates a 1x1 convolution layer, optionally using quantization. It supports parameters like stride and returns either a standard nn.Conv2d or a quant_nn.QuantConv2d based on the quantize flag.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/tutorials/quant_resnet50.rst#2025-04-06_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef conv1x1(in_planes: int, out_planes: int, stride: int = 1, quantize: bool = False) -> nn.Conv2d:\n    \"\"\"1x1 convolution\"\"\"\n    if quantize:\n        return quant_nn.QuantConv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n    else:\n        return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n```\n\n----------------------------------------\n\nTITLE: Running COCO Dataset Evaluation with TensorRT\nDESCRIPTION: Command for evaluating model performance using mAP metrics on COCO validation dataset. Includes options for configuring NMS and IoU thresholds.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/detectron2/README.md#2025-04-06_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npython eval_coco.py \\\n    --engine /path/to/engine.trt \\\n    --input /path/to/coco/val2017 \\\n    --det2_config /detectron2/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml \\\n    --det2_weights /model_final_f10217.pkl\n```\n\n----------------------------------------\n\nTITLE: Evaluating Calibrated ResNet50 Model in Python\nDESCRIPTION: This snippet evaluates the calibrated ResNet50 model using the test data loader and saves the model state dict.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/examples/calibrate_quant_resnet50.ipynb#2025-04-06_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncriterion = nn.CrossEntropyLoss()\nwith torch.no_grad():\n    evaluate(model, criterion, data_loader_test, device=\"cuda\", print_freq=20)\n    \n# Save the model\ntorch.save(model.state_dict(), \"/tmp/quant_resnet50-calibrated.pth\")\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for BERT PTQ (Turing and Ampere GPUs)\nDESCRIPTION: This command builds a TensorRT engine for BERT PTQ on Turing and Ampere GPUs. It uses INT8 mixed precision with QKVToContextPlugin and SkipLayerNormPlugin support.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p engines && python3 builder.py -m models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/model.ckpt -o engines/bert_large_384_int8mix.engine -b 1 -s 384 --int8 --fp16 --strict -c models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1 --squad-json ./squad/train-v1.1.json -v models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/vocab.txt --calib-num 100 -iln -imh\n```\n\n----------------------------------------\n\nTITLE: Saving Model Inputs and Outputs with Polygraphy\nDESCRIPTION: Executes an ONNX model using ONNX Runtime and saves both input and output values to JSON files for later comparison.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/run/02_comparing_across_runs/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run identity.onnx --onnxrt \\\n    --save-inputs inputs.json --save-outputs run_0_outputs.json\n```\n\n----------------------------------------\n\nTITLE: Filtering TensorRT Layers by Type Using Pandas in Python\nDESCRIPTION: Demonstrates two ways to filter layers by type using Pandas' query functionality. This allows focusing analysis on specific layer types by filtering the engine plan dataframe.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/tutorial.ipynb#2025-04-06_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nconvs1 = plan.df.query(\"type == 'Convolution'\")\nconvs2 = df[df.type == 'Convolution']\n```\n\n----------------------------------------\n\nTITLE: Layer Latency Comparison\nDESCRIPTION: Compares layer latencies between two engine plans with a 3% error threshold and exact matching\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/compare_engines.ipynb#2025-04-06_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncompare_engines_layer_latencies(\n    plan1, plan2,\n    # Allow for 3% error grace threshold when color highlighting performance differences\n    threshold=0.03,\n    # Inexact matching uses only the layer's first input and output to match to other layers.\n    exact_matching=True)\n```\n\n----------------------------------------\n\nTITLE: Adding SkipLayerNorm Plugin Source Files to TensorRT Build\nDESCRIPTION: This CMake snippet adds SkipLayerNorm plugin source files to the TensorRT build system. It includes CUDA kernel implementations, C++ plugin classes, and header files for both regular and Int8Interleaved versions of the SkipLayerNorm plugin, covering current and legacy implementations.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/skipLayerNormPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_plugin_source(\n    skipLayerNormInt8InterleavedKernelHFace.cu\n    skipLayerNormInt8InterleavedKernelMTron.cu\n    skipLayerNormInt8InterleavedPlugin.cpp\n    skipLayerNormInt8InterleavedPlugin.h\n    skipLayerNormInt8InterleavedPluginLegacy.cpp\n    skipLayerNormInt8InterleavedPluginLegacy.h\n    skipLayerNormKernel.cu\n    skipLayerNormPlugin.cpp\n    skipLayerNormPlugin.h\n    skipLayerNormPluginLegacy.cpp\n    skipLayerNormPluginLegacy.h\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluating mAP Metrics for TensorRT Models\nDESCRIPTION: Command to evaluate mean Average Precision (mAP) metrics for a TensorRT engine using validation data and ground truth annotations. Uses the TFOD research object detection repository's mAP metrics tools.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/tensorflow_object_detection_api/README.md#2025-04-06_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npython eval_coco.py \\\n    --engine /path/to/engine.trt \\\n    --input /path/to/coco/val2017 \\\n    --annotations /path/to/coco/annotations/instances_val2017.json \\\n    --preprocessor fixed_shape_resizer\n```\n\n----------------------------------------\n\nTITLE: Running INT8 Inference with Custom Dynamic Ranges in TensorRT\nDESCRIPTION: Command to run INT8 inference using custom dynamic ranges. It accepts parameters for model file, dynamic range file, input image, reference labels, and other options.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleINT8API/README.md#2025-04-06_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n./sample_int8_api [--model=model_file] [--ranges=per_tensor_dynamic_range_file] [--image=image_file] [--reference=reference_file] [--data=/path/to/data/dir] [--useDLACore=<int>] [-v or --verbose]\n```\n\n----------------------------------------\n\nTITLE: Setting CMake Version and Project Parameters for TensorRT Custom Plugin\nDESCRIPTION: Defines the minimum required CMake version (3.8) needed for CUDA support as a first-class language and sets up the project for a CustomHardMax plugin with C++ and CUDA support.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/onnx_custom_plugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\n# We need cmake >= 3.8, since 3.8 introduced CUDA as a first class language\ncmake_minimum_required(VERSION 3.8 FATAL_ERROR)\nproject(CustomHardMax LANGUAGES CXX CUDA)\n```\n\n----------------------------------------\n\nTITLE: Exporting Quantized Model to ONNX for TensorRT\nDESCRIPTION: Shows how to export a calibrated, quantized PyTorch model to ONNX format for TensorRT deployment. The example shows the process of loading a calibrated model, creating a dummy input, and using PyTorch's ONNX export with pytorch_quantization.enable_onnx_export context.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/userguide.rst#2025-04-06_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport pytorch_quantization\nfrom pytorch_quantization import nn as quant_nn\nfrom pytorch_quantization import quant_modules\n\nquant_modules.initialize()\nmodel = torchvision.models.resnet50()\n\n# load the calibrated model\nstate_dict = torch.load(\"quant_resnet50-entropy-1024.pth\", map_location=\"cpu\")\nmodel.load_state_dict(state_dict)\nmodel.cuda()\n\ndummy_input = torch.randn(128, 3, 224, 224, device='cuda')\n\ninput_names = [ \"actual_input_1\" ]\noutput_names = [ \"output1\" ]\n\nwith pytorch_quantization.enable_onnx_export():\n     # enable_onnx_checker needs to be disabled. See notes below.\n     torch.onnx.export(\n         model, dummy_input, \"quant_resnet50.onnx\", verbose=True, opset_version=10, enable_onnx_checker=False\n         )\n```\n\n----------------------------------------\n\nTITLE: Implementing Input/Output Processing Utilities for TensorRT\nDESCRIPTION: This snippet defines utility functions for preprocessing input images and postprocessing output data. It includes normalization, color palette creation, and image format conversion for segmentation visualization.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/SemanticSegmentation/tutorial-runtime.ipynb#2025-04-06_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef preprocess(image):\n    # Mean normalization\n    mean = np.array([0.485, 0.456, 0.406]).astype('float32')\n    stddev = np.array([0.229, 0.224, 0.225]).astype('float32')\n    data = (np.asarray(image).astype('float32') / float(255.0) - mean) / stddev\n    # Switch from HWC to to CHW order\n    return np.moveaxis(data, 2, 0)\n\ndef postprocess(data):\n    num_classes = 21\n    # create a color palette, selecting a color for each class\n    palette = np.array([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])\n    colors = np.array([palette*i%255 for i in range(num_classes)]).astype(\"uint8\")\n    # plot the segmentation predictions for 21 classes in different colors\n    img = Image.fromarray(data.astype('uint8'), mode='P')\n    img.putpalette(colors)\n    return img\n```\n\n----------------------------------------\n\nTITLE: Generating Convolution Layer Statistics with TREX in Python\nDESCRIPTION: Creates pie charts showing convolution layer statistics grouped by subtype, group size, and precision. The visualizations display both count and latency budget distribution across these categories to identify performance patterns.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/tutorial.ipynb#2025-04-06_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nconvs = plan.get_layers_by_type('Convolution')\n\ncharts = []\nconvs_count_by_type = trex.group_count(convs, 'subtype')\ncharts.append((convs_count_by_type, 'Count', 'count', 'subtype'))\n\nconvs_time_pct_by_type = trex.group_sum_attr(convs, grouping_attr='subtype', reduced_attr='latency.pct_time')\ncharts.append((convs_time_pct_by_type, '% Latency Budget', 'latency.pct_time', 'subtype'))\ntrex.plotting.plotly_pie2(\"Convolutions Statistics (Subtype)\", charts)\n\ncharts = []\nconvs_count_by_group_size = trex.group_count(convs, 'attr.groups')\ncharts.append((convs_count_by_group_size, 'Count', 'count', 'attr.groups'))\n\nconvs_time_pct_by_grp_size = trex.group_sum_attr(convs, grouping_attr='attr.groups', reduced_attr='latency.pct_time')\ncharts.append((convs_time_pct_by_grp_size, '% Latency Budget', 'latency.pct_time', 'attr.groups'))\ntrex.plotting.plotly_pie2(\"Convolutions Statistics (Number of Groups)\", charts)\n\ncharts = []\nconvs_count_by_precision = trex.group_count(convs, 'precision')\ncharts.append((convs_count_by_precision, 'Count', 'count', 'precision'))\n\nconvs_time_pct_by_precision = trex.group_sum_attr(convs, grouping_attr='precision', reduced_attr='latency.pct_time')\ncharts.append((convs_time_pct_by_precision, '% Latency Budget', 'latency.pct_time', 'precision'))\n\ntrex.plotting.plotly_pie2(\"Convolutions Statistics (Precision)\", charts, colormap=trex.colors.precision_colormap);\n```\n\n----------------------------------------\n\nTITLE: Running Inference and Evaluating F1 Score for BERT with Variable Sequence Length\nDESCRIPTION: These commands run inference using the SQuAD dataset and evaluate the F1 score and exact match score for the BERT model with variable sequence length support.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_26\n\nLANGUAGE: bash\nCODE:\n```\npython3 inference_varseqlen.py -e engines/bert_varseq_int8.engine -s 256 -sq ./squad/dev-v1.1.json -v models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/vocab.txt -o ./predictions.json\npython3 squad/evaluate-v1.1.py  squad/dev-v1.1.json  ./predictions.json 90\n```\n\n----------------------------------------\n\nTITLE: Comparing Custom Implementation with ONNX-Runtime\nDESCRIPTION: Command to run both the custom runner and ONNX-Runtime on the same model to verify functional correctness.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/dev/02_extending_polygraphy_run/README.md#2025-04-06_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run no_op_reshape.onnx --res-des --onnxrt\n```\n\n----------------------------------------\n\nTITLE: Comparing TensorRT Inference Results with TensorFlow\nDESCRIPTION: Command for comparing inference results between TensorRT and TensorFlow implementations. The script reports matching prediction percentage and RMSE between confidence scores.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/efficientnet/README.md#2025-04-06_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\npython3 compare_tf.py \\\n    --engine /path/to/engine.trt \\\n    --saved_model /path/to/saved_model \\\n    --input /path/to/images \\\n    --preprocessor V2\n```\n\n----------------------------------------\n\nTITLE: Running C++ Performance Benchmark with CUDA Graph\nDESCRIPTION: Executes a C++ benchmark program to measure inference performance with CUDA Graph enabled.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nbuild/perf -e engines/bert_large_128.engine -b 1 -s 128 -w 100 -i 1000 --enable_graph\n```\n\n----------------------------------------\n\nTITLE: Saving FP32 Model and Converting to ONNX\nDESCRIPTION: This code saves the trained FP32 model in TensorFlow format and converts it to ONNX format.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/notebooks/simple_network_quantize_partial.ipynb#2025-04-06_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# save TF FP32 original model\ntf.keras.models.save_model(nn_model_original, assets.simple_network_quantize_partial.fp32_saved_model)\n\n# Convert FP32 model to ONNX\nutils.convert_saved_model_to_onnx(saved_model_dir = assets.simple_network_quantize_partial.fp32_saved_model, onnx_model_path = assets.simple_network_quantize_partial.fp32_onnx_model)\n```\n\n----------------------------------------\n\nTITLE: Downloading Pre-trained BERT Large Model from NGC\nDESCRIPTION: Downloads a pretrained, fine-tuned BERT large model from NVIDIA GPU Cloud (NGC) that has been trained with automatic mixed precision for question answering tasks.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/BERT-TRT-FP16.ipynb#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!bash ../scripts/download_model.sh large 384 v2\n```\n\n----------------------------------------\n\nTITLE: Running ONNX Model Linter in Polygraphy\nDESCRIPTION: Command to run Polygraphy's check lint subtool on an ONNX model file to validate it and generate a JSON report of any issues found.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/check/01_linting_an_onnx_model/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy check lint bad_graph.onnx -o report.json\n```\n\n----------------------------------------\n\nTITLE: Comparing TensorFlow and TensorRT Model Results\nDESCRIPTION: Command to compare detection results between the original TensorFlow model and the TensorRT engine. Produces visualization images showing inference results from both frameworks for qualitative comparison.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/tensorflow_object_detection_api/README.md#2025-04-06_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\npython compare_tf.py \\\n    --engine /path/to/saved/engine.trt \\\n    --saved_model /path/to/exported/saved_model \\\n    --input /path/to/images \\\n    --annotations /path/to/coco/annotations/instances_val2017.json \\\n    --output /path/to/output \\\n    --preprocessor fixed_shape_resizer \\\n    --labels /path/to/labels_coco.txt\n```\n\n----------------------------------------\n\nTITLE: Engine Overview Comparison\nDESCRIPTION: Generates a high-level comparison overview of the engine plans\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/compare_engines.ipynb#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncompare_engines_overview(plans)\n```\n\n----------------------------------------\n\nTITLE: Installing Polygraphy Using Make on Linux\nDESCRIPTION: Command to build and install Polygraphy from source using the provided Makefile on Linux systems.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake install\n```\n\n----------------------------------------\n\nTITLE: Saving Quantized TensorFlow Model and Converting to ONNX\nDESCRIPTION: This snippet saves the quantized INT8 model in TensorFlow format and converts it to ONNX format.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/notebooks/simple_network_quantize_specific_class.ipynb#2025-04-06_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Save TF INT8 original model\ntf.keras.models.save_model(q_nn_model, assets.simple_network_quantize_specific_class.int8_saved_model)\n\n# Convert INT8 model to ONNX\nutils.convert_saved_model_to_onnx(saved_model_dir = assets.simple_network_quantize_specific_class.int8_saved_model, onnx_model_path = assets.simple_network_quantize_specific_class.int8_onnx_model)\n\ntf.keras.backend.clear_session()\n```\n\n----------------------------------------\n\nTITLE: Installing Polygraphy Using Make on Linux\nDESCRIPTION: Command to build and install Polygraphy from source using the provided Makefile on Linux systems.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake install\n```\n\n----------------------------------------\n\nTITLE: Setting Default QuantDescriptor for Histogram Calibration in Python\nDESCRIPTION: This snippet sets the default QuantDescriptor to use histogram-based calibration for activation in quantized convolutional and linear layers.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/examples/calibrate_quant_resnet50.ipynb#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nquant_desc_input = QuantDescriptor(calib_method='histogram')\nquant_nn.QuantConv2d.set_default_quant_desc_input(quant_desc_input)\nquant_nn.QuantLinear.set_default_quant_desc_input(quant_desc_input)\n```\n\n----------------------------------------\n\nTITLE: Downloading BERT Model Checkpoints\nDESCRIPTION: Downloads TensorFlow checkpoints for BERT large model with sequence length 128, fine-tuned for SQuAD v2.0.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbash scripts/download_model.sh\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Output Redirection and Custom Separator\nDESCRIPTION: Command for running inference and saving the results to a CSV file with a custom separator. Useful for batch processing of validation datasets.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/efficientnet/README.md#2025-04-06_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\npython3 infer.py \\\n    --engine /path/to/engine.trt \\\n    --input /path/to/ILSVRC2012_img_val \\\n    --preprocessor V2 \\\n    --separator ',' > results.csv\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine with Timing Cache\nDESCRIPTION: Converts an ONNX model to a TensorRT engine while saving timing information to a cache file. This creates a reference engine and timing cache for subsequent deterministic builds.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/convert/02_deterministic_engine_builds_in_tensorrt/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy convert identity.onnx \\\n    --save-timing-cache timing.cache \\\n    -o 0.engine\n```\n\n----------------------------------------\n\nTITLE: Sample Output of Polygraphy Inspect Capability Command\nDESCRIPTION: This snippet shows an example of the summary table output generated by the Polygraphy 'inspect capability' command. It displays unsupported operators, the reason for lack of support, their count in the graph, and the index range of affected nodes.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/inspect/08_inspecting_tensorrt_onnx_support/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n[I] ===== Summary =====\n    Operator | Count   | Reason                                                                                                                                                    | Nodes\n    -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    Fake     |       1 | In node 0 with name:  and operator: Fake (checkFallbackPluginImporter): INVALID_NODE: creator && \"Plugin not found, are the plugin name, version, and namespace correct?\" | [[2, 3]]\n```\n\n----------------------------------------\n\nTITLE: Visualizing Layer Types with Bar Chart\nDESCRIPTION: Code that creates a vertical bar chart showing the count of each layer type, with colors mapped to layer types using trex's color mapping.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/tutorial.ipynb#2025-04-06_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ntrex.plotting.plotly_bar2(\n    df=layer_types, \n    title='Layer Count By Type', \n    values_col='count', \n    names_col='type',\n    orientation='v',\n    color='type',\n    colormap=trex.colors.layer_colormap,\n    show_axis_ticks=(True, True));\n```\n\n----------------------------------------\n\nTITLE: Creating Layer Latency Bar Chart with Precision Color Coding in Python\nDESCRIPTION: Generates a bar chart showing latency budget per layer with color-coding by precision. This visualization helps identify which layers consume the most execution time and their precision types.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/tutorial.ipynb#2025-04-06_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ntrex.plotting.plotly_bar2(\n    plan.df, \n    \"% Latency Budget Per Layer<BR>(bar color indicates precision)\", \n    \"latency.pct_time\", \n    \"Name\",\n    color='precision',\n    colormap=trex.colors.precision_colormap);\n```\n\n----------------------------------------\n\nTITLE: Running DLA Network Commands\nDESCRIPTION: Command-line examples for running networks on NVIDIA DLA using trtexec with different precision modes (FP16 and INT8).\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/trtexec/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./trtexec --onnx=data/mnist/mnist.onnx --useDLACore=1 --fp16 --allowGPUFallback\n```\n\nLANGUAGE: bash\nCODE:\n```\n./trtexec --onnx=data/mnist/mnist.onnx --useDLACore=1 --int8 --allowGPUFallback\n```\n\nLANGUAGE: bash\nCODE:\n```\n./trtexec --onnx=data/mnist/mnist.onnx --useDLACore=0 --fp16 --allowGPUFallback\n```\n\n----------------------------------------\n\nTITLE: Running Question and Answer Mode for BERT with Variable Sequence Length\nDESCRIPTION: This command runs the question and answer mode for the BERT model with variable sequence length support.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\npython3 inference_varseqlen.py -e engines/bert_varseq_int8.engine -p \"TensorRT is a high performance deep learning inference platform that delivers low latency and high throughput for apps such as recommenders, speech and image/video on NVIDIA GPUs. It includes parsers to import models, and plugins to support novel ops and layers before applying optimizations for inference. Today NVIDIA is open-sourcing parsers and plugins in TensorRT so that the deep learning community can customize and extend these components to take advantage of powerful TensorRT optimizations for your apps.\" -q \"What is TensorRT?\" -v models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/vocab.txt -s 256\n```\n\n----------------------------------------\n\nTITLE: Creating Treemap of Layer Latencies\nDESCRIPTION: Code that creates an interactive treemap visualization showing layer latencies hierarchically organized by type and name, with both size and color indicating latency.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/tutorial.ipynb#2025-04-06_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport plotly.express as px\n\nfig = px.treemap(\n    plan.df,\n    path=['type', 'Name'],\n    values='latency.pct_time',\n    title='Treemap Of Layer Latencies (Size & Color Indicate Latency)',\n    color='latency.pct_time')\nfig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\nfig.show()\n```\n\n----------------------------------------\n\nTITLE: Inspecting a TensorRT Tactic Replay File with Polygraphy\nDESCRIPTION: This command uses Polygraphy's inspect tactics subtool to display information about the tactics stored in a replay file. The output shows which implementation and tactic was selected for each layer in the model.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/inspect/07_inspecting_tactic_replays/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy inspect tactics replay.json\n```\n\n----------------------------------------\n\nTITLE: Displaying Plan Summary Statistics\nDESCRIPTION: Shows summary statistics including layer latencies and engine timing measurements\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/engine_report_card.ipynb#2025-04-06_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nplan.summary()\n```\n\n----------------------------------------\n\nTITLE: Creating Pre-trained ResNet50 Model in Python\nDESCRIPTION: This snippet creates a pre-trained ResNet50 model and moves it to the GPU.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/examples/calibrate_quant_resnet50.ipynb#2025-04-06_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel = torchvision.models.resnet50(pretrained=True, progress=False)\nmodel.cuda()\n```\n\n----------------------------------------\n\nTITLE: Visualizing Latency Budget Per Layer\nDESCRIPTION: Code that creates a bar chart showing the percentage of total latency for each layer, with colors indicating layer types.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/tutorial.ipynb#2025-04-06_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ntrex.plotting.plotly_bar2(\n    df=plan.df, \n    title=\"% Latency Budget Per Layer\",\n    values_col=\"latency.pct_time\",\n    names_col=\"Name\",\n    color='type',\n    use_slider=False,\n    colormap=trex.colors.layer_colormap);\n```\n\n----------------------------------------\n\nTITLE: Finding k-Slowest Layers in TensorRT Engine\nDESCRIPTION: Identifies the top 3 slowest layers in the TensorRT engine and prints their names and types.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/api-examples.ipynb#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntop3 = plan.df.nlargest(3, 'latency.pct_time')\nfor i in range(len(top3)):\n    layer = top3.iloc[i]\n    print(\"%s: %s\" % (layer[\"Name\"], layer[\"type\"]))\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Quantized ResNet50 Model in Python\nDESCRIPTION: This snippet imports necessary libraries and modules for working with PyTorch, torchvision, and PyTorch Quantization. It also sets up logging and imports custom train and evaluation functions.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/examples/calibrate_quant_resnet50.ipynb#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\nimport os\nimport sys\nimport time\nimport collections\n\nimport torch\nimport torch.utils.data\nfrom torch import nn\n\nfrom tqdm import tqdm\n\nimport torchvision\nfrom torchvision import transforms\n\nfrom pytorch_quantization import nn as quant_nn\nfrom pytorch_quantization import calib\nfrom pytorch_quantization.tensor_quant import QuantDescriptor\n\nfrom absl import logging\nlogging.set_verbosity(logging.FATAL)  # Disable logging as they are too noisy in notebook\n\n# For simplicity, import train and eval functions from the train script from torchvision instead of copything them here\n# Download torchvision from https://github.com/pytorch/vision\nsys.path.append(\"/raid/skyw/models/torchvision/references/classification/\")\nfrom train import evaluate, train_one_epoch, load_data\n```\n\n----------------------------------------\n\nTITLE: Calculating Cumulative Latency of Top Layers\nDESCRIPTION: Computes the total latency and percentage of time consumed by the top 3 slowest layers in the engine.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/api-examples.ipynb#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntop3_latency = top3['latency.avg_time'].sum()\ntop3_percent = top3['latency.pct_time'].sum()\nprint(f\"top3 latency: {top3_latency:.6f} ms ({top3_percent:.2f}%)\")\n```\n\n----------------------------------------\n\nTITLE: Documenting PyTorch Quantization Module Structure\nDESCRIPTION: reStructuredText documentation structure defining the quantized neural network components including TensorQuantizer, quantized convolutions, linear layers, pooling layers and LSTM modules.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/nn.rst#2025-04-06_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. role:: hidden\n    :class: hidden-section\n\npytorch_quantization.nn\n===================================\n\n.. automodule:: pytorch_quantization.nn\n.. currentmodule:: pytorch_quantization.nn\n\n:hidden:`TensorQuantizer`\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. autoclass:: TensorQuantizer\n    :members:\n```\n\n----------------------------------------\n\nTITLE: Defining Documentation Structure with reStructuredText for TensorRT IR\nDESCRIPTION: A reStructuredText toctree directive that organizes documentation files for TensorRT's Intermediate Representation, linking to pages about graph, node, and tensor components.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/docs/ir/toc.rst#2025-04-06_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n    graph\n    node\n    tensor/toc\n```\n\n----------------------------------------\n\nTITLE: Instance Normalization Formula in TensorRT\nDESCRIPTION: The mathematical formula for instance normalization as implemented in the plugin. This normalizes input values by applying scale * (x-mean) / sqrt(variance + epsilon) + bias where mean and variance are computed per instance per channel.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/instanceNormalizationPlugin/README.md#2025-04-06_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\nscale * (x-mean) / sqrt(variance + epsilon) + bias\n```\n\n----------------------------------------\n\nTITLE: Post-Processing BERT Inference Results for Question Answering\nDESCRIPTION: Processes the inference output from the BERT model to extract answers to questions. It uses helper functions to generate predictions, specifies parameters like maximum answer length, and prints the final answer with its probability score.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/inference.ipynb#2025-04-06_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n    # The total number of n-best predictions to generate in the nbest_predictions.json output file\n    n_best_size = 20\n\n    # The maximum length of an answer that can be generated. This is needed \n    #  because the start and end predictions are not conditioned on one another\n    max_answer_length = 30\n\n    prediction, nbest_json, scores_diff_json = dp.get_predictions(doc_tokens, features,\n        networkOutputs, n_best_size, max_answer_length)\n    \n    for index, output in enumerate(networkOutputs):\n        print(\"Processing output\")\n        print(\"Answer: '{}'\".format(prediction))\n        print(\"with prob: {:.3f}%\".format(nbest_json[0]['probability'] * 100.0))\n```\n\n----------------------------------------\n\nTITLE: Creating EnginePlan with Multiple JSON Inputs in Python\nDESCRIPTION: This code shows how to construct an EnginePlan instance using three JSON files: a plan-graph file, a profiling file, and a metadata file. The profiling and metadata files are optional but provide additional information about the engine.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/trex/README.md#2025-04-06_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nplan = EnginePlan(\n    \"my-engine.graph.json\",\n    \"my-engine.profile.json\",\n    \"my-engine.profile.metadata.json\")\n```\n\n----------------------------------------\n\nTITLE: Running TensorRT Dynamic Shapes Example\nDESCRIPTION: Command to execute the main example script demonstrating dynamic shapes implementation\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/07_tensorrt_and_dynamic_shapes/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 example.py\n```\n\n----------------------------------------\n\nTITLE: Modifying Input Shape with Dynamic Batch Dimension using Polygraphy\nDESCRIPTION: This command uses the 'polygraphy surgeon sanitize' tool to modify the input shape of an ONNX model. It changes the input shape to have a dynamic batch dimension while keeping other dimensions the same.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/surgeon/03_modifying_input_shapes/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy surgeon sanitize identity.onnx \\\n    --override-input-shapes x:['batch',1,2,2] \\\n    -o dynamic_identity.onnx\n```\n\n----------------------------------------\n\nTITLE: Comparing Tactic Replays\nDESCRIPTION: Command to analyze and compare tactic replay files to identify potentially problematic tactics.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/debug/01_debugging_flaky_trt_tactics/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy inspect diff-tactics --dir replays\n```\n\n----------------------------------------\n\nTITLE: Visualizing Layer Latency Distribution\nDESCRIPTION: Code that creates a histogram showing the distribution of layer latencies, with colors indicating layer types. This helps identify latency patterns.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/tutorial.ipynb#2025-04-06_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ntrex.plotting.plotly_hist(\n    df=plan.df, \n    title=\"Layer Latency Distribution\", \n    values_col=\"latency.pct_time\",\n    xaxis_title=\"Latency (ms)\",\n    color='type',\n    colormap=trex.colors.layer_colormap);\n```\n\n----------------------------------------\n\nTITLE: Installing Required Python Packages for TensorRT Examples\nDESCRIPTION: A pip install command to set up the necessary dependencies for TensorRT examples, including pycuda for GPU access, onnx for model format handling, and scikit-image for image processing.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/0. Running This Guide.ipynb#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!pip install pycuda onnx scikit-image\n```\n\n----------------------------------------\n\nTITLE: Saving and Converting Original Model to ONNX\nDESCRIPTION: This snippet saves the original FP32 model in TensorFlow format and converts it to ONNX format.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/notebooks/simple_network_quantize_full.ipynb#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Save TF FP32 original model\ntf.keras.models.save_model(nn_model_original, assets.simple_network_quantize_full.fp32_saved_model)\n\n# Convert FP32 model to ONNX\nutils.convert_saved_model_to_onnx(saved_model_dir = assets.simple_network_quantize_full.fp32_saved_model, onnx_model_path = assets.simple_network_quantize_full.fp32_onnx_model)\n```\n\n----------------------------------------\n\nTITLE: Saving TensorFlow FP32 Model and Converting to ONNX\nDESCRIPTION: Saves the trained model in TensorFlow's SavedModel format and converts it to ONNX format for compatibility with other deep learning frameworks.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/getting_started.ipynb#2025-04-06_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# save TF FP32 original model\ntf.keras.models.save_model(model, assets.example.fp32_saved_model)\n\n# Convert FP32 model to ONNX\nutils.convert_saved_model_to_onnx(saved_model_dir = assets.example.fp32_saved_model, onnx_model_path = assets.example.fp32_onnx_model)\n```\n\n----------------------------------------\n\nTITLE: Setting up ImageNet Data Loaders\nDESCRIPTION: Configures data loaders for ImageNet training and validation datasets with specified batch sizes and workers.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/examples/finetune_quant_resnet50.ipynb#2025-04-06_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndata_path = \"/raid/data/imagenet/imagenet_pytorch\"\n\ntraindir = os.path.join(data_path, 'train')\nvaldir = os.path.join(data_path, 'val')\ndataset, dataset_test, train_sampler, test_sampler = load_data(traindir, valdir, False, False)\n\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=256,\n    sampler=train_sampler, num_workers=4, pin_memory=True)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test, batch_size=256,\n    sampler=test_sampler, num_workers=4, pin_memory=True)\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for ONNX TensorRT in Bash\nDESCRIPTION: Installs the required Python dependencies for running the YOLOv3 object detection sample with ONNX and TensorRT. This command installs all necessary packages defined in the requirements.txt file.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/yolov3_onnx/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip3 install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Creating a Simple TensorFlow Network with Keras\nDESCRIPTION: Defines a small convolutional neural network using TensorFlow Keras. It creates a model with multiple Conv2D layers, ReLU activations, a MaxPooling layer, and Dense layers, which will be used in subsequent quantization examples.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/qspec.rst#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport tensorflow as tf\n# Import necessary methods from the Quantization Toolkit\nfrom tensorflow_quantization.quantize import quantize_model, QuantizationSpec\n\n# 1. Create a small network\ninput_img = tf.keras.layers.Input(shape=(28, 28))\nx = tf.keras.layers.Reshape(target_shape=(28, 28, 1))(input_img)\nx = tf.keras.layers.Conv2D(filters=126, kernel_size=(3, 3))(x)\nx = tf.keras.layers.ReLU()(x)\nx = tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3))(x)\nx = tf.keras.layers.ReLU()(x)\nx = tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3))(x)\nx = tf.keras.layers.ReLU()(x)\nx = tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3))(x)\nx = tf.keras.layers.ReLU()(x)\nx = tf.keras.layers.Conv2D(filters=8, kernel_size=(3, 3))(x)\nx = tf.keras.layers.ReLU()(x)\nx = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\nx = tf.keras.layers.Flatten()(x)\nx = tf.keras.layers.Dense(100)(x)\nx = tf.keras.layers.ReLU()(x)\nx = tf.keras.layers.Dense(10)(x)\nmodel = tf.keras.Model(input_img, x)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Quantized ResNet50 V1 Model in TensorFlow\nDESCRIPTION: This snippet evaluates the fine-tuned quantized ResNet50 V1 model on the validation dataset and prints the accuracy.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/example_resnet50v1.ipynb#2025-04-06_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n_, qat_model_accuracy = q_model.evaluate(val_batches)\nprint(\"QAT val accuracy: {:.3f}%\".format(qat_model_accuracy*100))\n```\n\n----------------------------------------\n\nTITLE: Running the Circular Padding Plugin Sample with AOT Implementation\nDESCRIPTION: Command-line example for running the Circular Padding plugin sample with Ahead-of-Time implementation option. This allows the plugin computation to be compiled into the engine for runtime execution without requiring Python modules.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/quickly_deployable_plugins/README.md#2025-04-06_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython3 qdp_runner.py circ_pad [--save_engine] [--load_engine] --mode {onnx,inetdef} [--artifacts_dir ARTIFACTS_DIR]  [-v]\n\noptions:\n  --save_engine         Save engine to the artifacts_dir.\n  --load_engine         Load engine from the artifacts_dir. Ignores all other options.\n  --artifacts_dir ARTIFACTS_DIR\n                        Whether to store (or retrieve) artifacts.\n  --mode {onnx,inetdef} Whether to use ONNX parser or INetworkDefinition APIs to construct the network.\n  --aot                 Use the AOT implementation of the plugin.\n  -v, --verbose         Enable verbose log output.\n```\n\n----------------------------------------\n\nTITLE: Initializing Quantized ResNet Block in PyTorch\nDESCRIPTION: This snippet shows the __init__ method for a ResNet block (either BasicBlock or Bottleneck) with quantization support. It initializes the quantization flag and creates a residual quantizer when quantization is enabled.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/tutorials/quant_resnet50.rst#2025-04-06_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(self,\n               inplanes: int,\n               planes: int,\n               stride: int = 1,\n               downsample: Optional[nn.Module] = None,\n               groups: int = 1,\n               base_width: int = 64,\n               dilation: int = 1,\n               norm_layer: Optional[Callable[..., nn.Module]] = None,\n               quantize: bool = False) -> None:\n    # other code...\n    self._quantize = quantize\n    if self._quantize:\n        self.residual_quantizer = quant_nn.TensorQuantizer(quant_nn.QuantConv2d.default_quant_desc_input)\n```\n\n----------------------------------------\n\nTITLE: Adding Multi-scale Deformable Attention Plugin Sources\nDESCRIPTION: CMake command to add source files required for the Multi-scale Deformable Attention plugin. Includes CUDA implementation, header files, and plugin implementation files.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/multiscaleDeformableAttnPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_plugin_source(\n    multiscaleDeformableAttn.cu\n    multiscaleDeformableAttn.h\n    multiscaleDeformableAttnPlugin.cpp\n    multiscaleDeformableAttnPlugin.h\n    multiscaleDeformableIm2ColCuda.cuh\n)\n```\n\n----------------------------------------\n\nTITLE: Running the Non-Zero Plugin Sample with Verbose Logging\nDESCRIPTION: Command-line example for running the Non-Zero plugin sample with optional verbose logging to observe autotuning behavior. Autotuning is always enabled by default.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/quickly_deployable_plugins/README.md#2025-04-06_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython3 qdp_runner.py non_zero [-v]\n```\n\n----------------------------------------\n\nTITLE: Exporting Detectron 2 Model to ONNX\nDESCRIPTION: Uses Detectron 2's export_model.py script to convert the Mask R-CNN R50-FPN 3x model to ONNX format with specific configuration and input size.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/detectron2/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython detectron2/tools/deploy/export_model.py \\\n    --sample-image 1344x1344.jpg \\\n    --config-file detectron2/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml \\\n    --export-method tracing \\\n    --format onnx \\\n    --output ./ \\\n    MODEL.WEIGHTS path/to/model_final_f10217.pkl \\\n    MODEL.DEVICE cuda\n```\n\n----------------------------------------\n\nTITLE: Checking NVIDIA Driver and CUDA Version\nDESCRIPTION: This command displays information about the NVIDIA driver and CUDA version installed on the system.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/EfficientDet/notebooks/EfficientDet-TensorRT8.ipynb#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!nvidia-smi\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine with FP16 Precision\nDESCRIPTION: Command to convert an ONNX model to a TensorRT engine with FP16 precision using the build_engine.py script. The resulting engine file can be used for inference with TensorRT.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/tensorflow_object_detection_api/README.md#2025-04-06_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython build_engine.py \\\n    --onnx /path/to/saved/model.onnx \\\n    --engine /path/to/save/engine.trt \\\n    --precision fp16\n```\n\n----------------------------------------\n\nTITLE: Converting YOLOv3 to ONNX Format in Bash\nDESCRIPTION: Executes the Python script that converts the YOLOv3 model to ONNX format. This is the first step in the pipeline that creates an ONNX representation of the YOLOv3 network for subsequent TensorRT processing.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/yolov3_onnx/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 yolov3_to_onnx.py\n```\n\n----------------------------------------\n\nTITLE: Implementing Calibration Functions\nDESCRIPTION: Functions to collect statistics and compute activation ranges for quantization calibration.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/tutorials/quant_resnet50.rst#2025-04-06_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef collect_stats(model, data_loader, num_batches):\n    \"\"\"Feed data to the network and collect statistic\"\"\"\n\n    # Enable calibrators\n    for name, module in model.named_modules():\n        if isinstance(module, quant_nn.TensorQuantizer):\n            if module._calibrator is not None:\n                module.disable_quant()\n                module.enable_calib()\n            else:\n                module.disable()\n\n    for i, (image, _) in tqdm(enumerate(data_loader), total=num_batches):\n        model(image.cuda())\n        if i >= num_batches:\n            break\n\n    # Disable calibrators\n    for name, module in model.named_modules():\n        if isinstance(module, quant_nn.TensorQuantizer):\n            if module._calibrator is not None:\n                module.enable_quant()\n                module.disable_calib()\n            else:\n                module.enable()\n\ndef compute_amax(model, **kwargs):\n    # Load calib result\n    for name, module in model.named_modules():\n        if isinstance(module, quant_nn.TensorQuantizer):\n            if module._calibrator is not None:\n                if isinstance(module._calibrator, calib.MaxCalibrator):\n                    module.load_calib_amax()\n                else:\n                    module.load_calib_amax(**kwargs)\n            print(F\"{name:40}: {module}\")\n    model.cuda()\n```\n\n----------------------------------------\n\nTITLE: Generating a TensorRT Engine with Dynamic Shapes and Multiple Profiles\nDESCRIPTION: This command uses Polygraphy to convert an ONNX model to a TensorRT engine with dynamic shapes and two optimization profiles. The resulting engine file is saved as dynamic_identity.engine.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/inspect/02_inspecting_a_tensorrt_engine/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run dynamic_identity.onnx --trt \\\n    --trt-min-shapes X:[1,2,1,1] --trt-opt-shapes X:[1,2,3,3] --trt-max-shapes X:[1,2,5,5] \\\n    --trt-min-shapes X:[1,2,2,2] --trt-opt-shapes X:[1,2,4,4] --trt-max-shapes X:[1,2,6,6] \\\n    --save-engine dynamic_identity.engine\n```\n\n----------------------------------------\n\nTITLE: Grouping Layers by Type and Summarizing Latency\nDESCRIPTION: Groups layers by their type and computes the sum of latency metrics for each type using Pandas groupby functionality.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/api-examples.ipynb#2025-04-06_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Group by type, and perform a sum reduction on the latency\nplan.df.groupby([\"type\"])[[\"latency.avg_time\", \"latency.pct_time\"]].sum()\n```\n\n----------------------------------------\n\nTITLE: Running Inference with Python Script\nDESCRIPTION: Command to perform object detection inference using the built TensorRT engine on input images.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/efficientdet/README.md#2025-04-06_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\npython3 infer.py \\\n    --engine /path/to/engine.trt \\\n    --input /path/to/images \\\n    --output /path/to/output\n```\n\n----------------------------------------\n\nTITLE: Running QAT Workflow for ResNet Models in Python\nDESCRIPTION: This command executes the QAT workflow script, which quantizes the model, performs fine-tuning, and saves the final graph in SavedModel format. It also handles the conversion to ONNX automatically.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/examples/resnet/README.md#2025-04-06_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npython run_qat_workflow.py\n```\n\n----------------------------------------\n\nTITLE: Optimal Custom Quantization Using ResNetV1QDQCase\nDESCRIPTION: Implements optimal quantization for ResNet models using the ResNetV1QDQCase class, which properly handles residual connections.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/add_custom_qdq_cases.md#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# 1. Indicate one or more custom QDQ cases\ncustom_qdq_case = ResNetV1QDQCase()\n\n# 3. Quantize model\nq_nn_model = quantize_model(\n    model=nn_model_original, custom_qdq_cases=[custom_qdq_case]\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Quantization Descriptors and Model\nDESCRIPTION: Configure quantization descriptors for inputs and instantiate the pre-trained ResNet50 model.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/tutorials/quant_resnet50.rst#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nquant_desc_input = QuantDescriptor(calib_method='histogram')\nquant_nn.QuantConv2d.set_default_quant_desc_input(quant_desc_input)\nquant_nn.QuantLinear.set_default_quant_desc_input(quant_desc_input)\n\nmodel = models.resnet50(pretrained=True)\nmodel.cuda()\n```\n\n----------------------------------------\n\nTITLE: Implementing GlobalMaxPool2D Quantization Wrapper in Python\nDESCRIPTION: Example of how to create a new quantization wrapper for the GlobalMaxPool2D Keras layer. This wrapper inherits from NonWeightedBaseQuantizeWrapper and follows the toolkit's naming conventions.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/add_new_layer_support.md#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom tensorflow_quantization import NonWeightedBaseQuantizeWrapper\n\nclass GlobalMaxPool2DQuantizeWrapper(NonWeightedBaseQuantizeWrapper):\n    \n    def __init__(self, layer, **kwargs):\n        \"\"\"\n        Creates a wrapper to emulate quantization for the GlobalMaxPool2D keras layer.\n        Args:\n        layer: The keras layer to be quantized.\n        **kwargs: Additional keyword arguments to be passed to the keras layer.\n        \"\"\"\n        super().__init__(layer, **kwargs)\n\n    def build(self, input_shape):\n        super().build(input_shape)\n\n    def call(self, inputs, training=None):\n        return super().call(inputs, training=training)\n```\n\n----------------------------------------\n\nTITLE: Default Model Quantization in TensorRT\nDESCRIPTION: Basic quantization of a model using the default quantization behavior without any customization.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/add_custom_qdq_cases.md#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Quantize model\nq_nn_model = quantize_model(model=nn_model_original)\n```\n\n----------------------------------------\n\nTITLE: Initializing Quantization Modules\nDESCRIPTION: Initialize quantization modules for automatic layer substitution.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/tutorials/quant_resnet50.rst#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pytorch_quantization import quant_modules\nquant_modules.initialize()\n```\n\n----------------------------------------\n\nTITLE: Downloading Faster R-CNN ONNX Model\nDESCRIPTION: Command to download the pre-trained Faster R-CNN model in ONNX format from GitHub.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/dds_faster_rcnn/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nwget https://github.com/onnx/models/raw/refs/heads/main/validated/vision/object_detection_segmentation/faster-rcnn/model/FasterRCNN-12.onnx\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing TensorRT Engine Plans\nDESCRIPTION: Creates functions to load and prepare TensorRT engine plans from JSON files, including path handling and plan instantiation\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/compare_engines.ipynb#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nengine_name_1 = \"../tests/inputs/mobilenet.qat.onnx.engine\"\nengine_name_2 = \"../tests/inputs/mobilenet_v2_residuals.qat.onnx.engine\"\n\ndef extract_engine_name(engine_path):\n    from pathlib import Path \n    return Path(engine_path).name\n\ndef make_plan(engine_path, engine_name=None):\n    plan = EnginePlan(f'{engine_path}.graph.json', f'{engine_path}.profile.json', f\"{engine_path}.profile.metadata.json\", name=engine_name)\n    return plan\n    \nplan1 = make_plan(engine_name_1, \"mobilenet.qat\")\nplan2 = make_plan(engine_name_2, \"mobilenet_v2_residuals.qat\")\nplans = (plan1, plan2)\n```\n\n----------------------------------------\n\nTITLE: Defining Input Shape for Scores Tensor in Efficient NMS Plugin\nDESCRIPTION: Specifies the input shape for the scores tensor in the Efficient NMS plugin. The shape is [batch_size, number_boxes, number_classes], with float32 or float16 data types.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/efficientNMSPlugin/README.md#2025-04-06_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n> **Input Shape:** `[batch_size, number_boxes, number_classes]`\n>\n> **Data Type:** `float32` or `float16`\n```\n\n----------------------------------------\n\nTITLE: Accessing Layer Activations with TREx API\nDESCRIPTION: Creates Activation objects from layer information to access detailed properties of inputs and outputs, including shape, precision, format, and memory size.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/api-examples.ipynb#2025-04-06_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ninputs, outputs = trex.create_activations(convs.iloc[0])\nprint(inputs[0].name)\nprint(inputs[0].shape)\nprint(inputs[0].precision)\nprint(inputs[0].format)\nprint(inputs[0].size_bytes)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Layer Precision Distribution in TensorRT with TREX in Python\nDESCRIPTION: Creates pie charts showing layer count and latency budget distribution by precision. This code groups layer information by precision type, calculates percentage of time spent in each precision, and visualizes the results using TREX's plotting utilities.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/tutorial.ipynb#2025-04-06_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ncharts = []\nlayer_precisions = trex.group_count(plan.df, 'precision')\ncharts.append((layer_precisions, 'Layer Count By Precision', 'count', 'precision'))\n\nlayers_time_pct_by_precision = trex.group_sum_attr(plan.df, grouping_attr='precision', reduced_attr='latency.pct_time')\ndisplay(layers_time_pct_by_precision)\n\ncharts.append((layers_time_pct_by_precision, '% Latency Budget By Precision', 'latency.pct_time', 'precision'))\ntrex.plotting.plotly_pie2(\"Precision Statistics\", charts, colormap=trex.colors.precision_colormap);\n```\n\n----------------------------------------\n\nTITLE: Complete Quantized Linear Layer Implementation\nDESCRIPTION: Full implementation of quantized linear layer with both input and weight quantization.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/tutorials/creating_custom_quantized_modules.rst#2025-04-06_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass QuantLinear(nn.Linear, _utils.QuantMixin):\n\n    def __init__(self, in_features, out_features, bias=True, **kwargs):\n        super(QuantLinear, self).__init__(in_features, out_features, bias)\n        quant_desc_input, quant_desc_weight = _utils.pop_quant_desc_in_kwargs(self.__class__, **kwargs)\n\n        self.init_quantizer(quant_desc_input, quant_desc_weight)\n\n    def forward(self, input):\n        quant_input = self._input_quantizer(input)\n        quant_weight = self._weight_quantizer(self.weight)\n\n        output = F.linear(quant_input, quant_weight, bias=self.bias)\n\n        return output\n\n    @property\n    def input_quantizer(self):\n        return self._input_quantizer\n\n    @property\n    def weight_quantizer(self):\n        return self._weight_quantizer\n```\n\n----------------------------------------\n\nTITLE: Filtering Layers by Type Using DataFrame Query\nDESCRIPTION: Filters convolution layers from the engine plan using Pandas DataFrame query and calculates statistics.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/api-examples.ipynb#2025-04-06_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nltype = \"Convolution\"\nconvs = plan.df.query(f\"type == \\\"{ltype}\\\"\")\nprint(f\"There are {len(convs)} convolutions\")\nprint(convs['latency.avg_time'].median())\n```\n\n----------------------------------------\n\nTITLE: Selective Quantization of Layer Components (Weight Only)\nDESCRIPTION: Demonstrates how to selectively quantize only weights and not inputs for specific layers. This example quantizes the second and fourth Conv2D layers and first Dense layer, but for the second Conv2D layer, it only quantizes weights without quantizing inputs.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/qspec.rst#2025-04-06_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# 1. Find out layer names\nprint(model.summary())\n\n# 2. Create quantization spec and add layer names\nq_spec = QuantizationSpec()\nlayer_name = ['conv2d_1', 'conv2d_3', 'dense']\nlayer_q_input = [False, True, True]\n\n\"\"\"\n# Alternatively, each layer configuration can be added one at a time:\nq_spec.add('conv2d_1', quantize_input=False)\nq_spec.add('conv2d_3')\nq_spec.add('dense')\n\"\"\"\n\nq_spec.add(name=layer_name, quantize_input=layer_q_input)\n\n# 3. Quantize model\nq_model = quantize_model(model, quantization_mode='partial', quantization_spec=q_spec)\nprint(q_model.summary())\n\ntf.keras.backend.clear_session()\n```\n\n----------------------------------------\n\nTITLE: Creating ONNX Model\nDESCRIPTION: Command to convert TensorFlow SavedModel to ONNX format with specified batch size and input dimensions.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/efficientnet/README.md#2025-04-06_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython3 create_onnx.py \\\n    --saved_model /path/to/saved_model \\\n    --onnx /path/to/model.onnx \\\n    --batch_size 1 \\\n    --input_size 384\n```\n\n----------------------------------------\n\nTITLE: Building and Running C++ Circular Padding Plugin\nDESCRIPTION: Commands to build the C++ circular padding plugin shared library and run the Python script that uses this C++ plugin. It demonstrates the process of compiling a C++ plugin and integrating it with a Python TensorRT application.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/python_plugin/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd $TRT_OSSPATH/samples/python/trt_python_plugin\nmkdir build && pushd build\ncmake .. && make -j\npopd\npython3 circ_pad_plugin_cpp.py --plugin-lib build/libcirc_pad_plugin.so\n```\n\n----------------------------------------\n\nTITLE: Comparing TensorRT and ONNX-Runtime Outputs with Polygraphy\nDESCRIPTION: Basic command to run a model in both TensorRT and ONNX-Runtime frameworks and compare their outputs. This uses the default synthetic input data generation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/run/01_comparing_frameworks/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run dynamic_identity.onnx --trt --onnxrt\n```\n\n----------------------------------------\n\nTITLE: Running Constrained Network with TensorRT in Polygraphy\nDESCRIPTION: This command runs a network loader script that constrains precisions in the model, forcing TensorRT to obey constraints. It uses saved inputs and compares against saved golden outputs.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/run/08_adding_precision_constraints/README.md#2025-04-06_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run constrained_network.py --precision-constraints obey \\\n    --trt --fp16 --load-inputs inputs.json --load-outputs golden_outputs.json \\\n    --check-error-stat median\n```\n\n----------------------------------------\n\nTITLE: Local installation of TensorRT tensorflow-quantization\nDESCRIPTION: Instructions for installing tensorflow-quantization locally without using Docker. This involves cloning the TensorRT repository, running the installation script, and verifying the installation through tests.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/installation.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ cd ~/\n$ git clone https://github.com/NVIDIA/TensorRT.git\n$ cd TensorRT/tools/tensorflow-quantization\n$ ./install.sh\n$ cd tests\n$ python3 -m pytest quantize_test.py -rP\n```\n\n----------------------------------------\n\nTITLE: Creating Interactive Question Answering UI with Jupyter Widgets\nDESCRIPTION: Implements a Jupyter widget-based user interface for question answering that supports multiple inference backends including CPU PyTorch, GPU PyTorch, TensorRT FP16, and TensorRT INT8. The code includes input fields for text, device selection, and progress tracking during batch inference.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/Q-and-A.ipynb#2025-04-06_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndevice = widgets.RadioButtons(\n    options=['CPU - Framework (PyTorch)', \n             'GPU - Framework (PyTorch)', \n             'GPU - TensorRT FP16',\n             'GPU - TensorRT INT8'],\n    description='Device:',\n    disabled=False\n)\n\nparagraph_text = widgets.Textarea(\n    value='TensorRT is a high performance deep learning inference platform that delivers low latency and high throughput for apps'\\\n'such as recommenders, speech and image/video on NVIDIA GPUs. It includes parsers to import models, and plugins to support novel ops'\\\n'and layers before applying optimizations for inference. Today NVIDIA is open-sourcing parsers and plugins in TensorRT so that the deep'\\\n'learning community can customize and extend these components to take advantage of powerful TensorRT optimizations for your apps.',\n    placeholder='Type something',\n    description='Passage:',\n    disabled=False,\n    layout=widgets.Layout(width=\"auto\"),\n    rows=10,  \n)\n\nquestion_text = widgets.Textarea(\n    value='What is TensorRT?',\n    placeholder='Type something',\n    description='Question:',\n    disabled=False,\n    layout=widgets.Layout(width=\"auto\"),\n    rows=2,\n)\ndisplay(paragraph_text)\ndisplay(question_text)\n\nfrom IPython.display import display\nbox_layout = widgets.Layout(display='flex',\n                flex_flow='column',\n                align_items='center',\n                width='100%')\n\nbutton = widgets.Button(description=\"Answer Me!\")\noutput = widgets.Output()\nbox = widgets.HBox(children=[button],layout=box_layout)\n\nN_RUN = 10\n\ndef answer(b):\n    progress_bar.value = 0\n    inference_time_arr = []\n    with output:\n        if device.value == 'GPU - TensorRT FP16':\n            output.clear_output()\n            for _ in range(N_RUN):\n                doc_tokens = dp.convert_doc_tokens(paragraph_text.value)\n                features = question_features(doc_tokens, question_text.value)\n                eval_time_elapsed, prediction, nbest_json = inference_FP16(context, d_inputs, h_output, d_output, features, doc_tokens)\n                progress_bar.value += 1                \n                inference_time_arr.append(eval_time_elapsed)\n\n            print_single_query(eval_time_elapsed, prediction, nbest_json)\n            print(\"Average inference time (over {} runs): {:.2f} ms\".format(N_RUN, 1000*np.mean(inference_time_arr)))   \n        elif device.value == 'GPU - TensorRT INT8':\n            output.clear_output()\n            for _ in range(N_RUN):\n                doc_tokens = dp.convert_doc_tokens(paragraph_text.value)\n                features = question_features(doc_tokens, question_text.value)\n                eval_time_elapsed, prediction, nbest_json = inference_INT8(INT8_context, INT8_d_inputs, INT8_h_output, INT8_d_output, features, doc_tokens)\n                progress_bar.value += 1                \n                inference_time_arr.append(eval_time_elapsed)\n\n            print_single_query(eval_time_elapsed, prediction, nbest_json)\n            print(\"Average inference time (over {} runs): {:.2f} ms\".format(N_RUN, 1000*np.mean(inference_time_arr)))   \n \n        elif device.value == 'CPU - Framework (PyTorch)':\n            output.clear_output()\n            for _ in range(N_RUN):\n                inference_time = time.time()\n                answer = nlp({\n                        'question': question_text.value,\n                        'context': paragraph_text.value\n                        })\n                progress_bar.value += 1                \n                inference_time_arr.append(time.time() - inference_time)\n                \n            print(\"Answer: '{}'\".format(answer['answer']))\n            print(\"With probability: {:.2f}%\".format(answer['score']*100))\n            print(\"Average inference time (over {} runs): {:.2f} ms\".format(N_RUN, 1000*np.mean(inference_time_arr)))   \n        elif  device.value == 'GPU - Framework (PyTorch)':  \n            output.clear_output()\n            for _ in range(N_RUN):\n                inference_time = time.time()\n                answer = nlp_gpu({\n                        'question': question_text.value,\n                        'context': paragraph_text.value\n                        })\n                progress_bar.value += 1                \n                inference_time_arr.append(time.time() - inference_time)\n                \n            print(\"Answer: '{}'\".format(answer['answer']))\n            print(\"With probability: {:.2f}%\".format(answer['score']*100))\n            print(\"Average inference time (over {} runs): {:.2f} ms\".format(N_RUN, 1000*np.mean(inference_time_arr)))           \n            \nbutton.on_click(answer)\ndisplay(device, box, output)\n\nprogress_bar = widgets.IntProgress(\n    value=0,\n    min=0,\n    max=N_RUN,\n    description='Progress:',\n    bar_style='', # 'success', 'info', 'warning', 'danger' or ''\n    style={'bar_color': 'green'},\n    orientation='horizontal', \n    layout=widgets.Layout(width='100%', height='50px')\n)\ndisplay(progress_bar)\n```\n\n----------------------------------------\n\nTITLE: Quantizing TensorFlow Model for Specific Layer Class\nDESCRIPTION: This snippet demonstrates how to quantize only the Dense layers of the model using QuantizationSpec.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/notebooks/simple_network_quantize_specific_class.ipynb#2025-04-06_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Quantize model\n# 1.1 Create a list with keras layer classes to quantize\nqspec = QuantizationSpec()\nqspec.add(name=\"Dense\", is_keras_class=True)\n# 1.2 Call quantize model function\nq_nn_model = quantize_model(model=nn_model_original, quantization_mode='partial', quantization_spec=qspec)\n\nq_nn_model.compile(\n    optimizer=tiny_resnet.optimizer(lr=1e-4),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[\"accuracy\"],\n)\n```\n\n----------------------------------------\n\nTITLE: Quantizing Specific Layers by Layer Names\nDESCRIPTION: Demonstrates how to use QuantizationSpec to selectively quantize layers by their names. This example specifically targets the second and fourth Conv2D layers and the first Dense layer for quantization in a TensorFlow model.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/qspec.rst#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# 1. Find out layer names\nprint(model.summary())\n\n# 2. Create quantization spec and add layer names\nq_spec = QuantizationSpec()\nlayer_name = ['conv2d_1', 'conv2d_3', 'dense']\n\n\"\"\"\n# Alternatively, each layer configuration can be added one at a time:\nq_spec.add('conv2d_1')\nq_spec.add('conv2d_3')\nq_spec.add('dense')\n\"\"\"\n\nq_spec.add(name=layer_name)\n\n# 3. Quantize model\nq_model = quantize_model(model, quantization_mode='partial', quantization_spec=q_spec)\nprint(q_model.summary())\n\ntf.keras.backend.clear_session()\n```\n\n----------------------------------------\n\nTITLE: Calibrating a Quantized Model\nDESCRIPTION: Demonstrates the calibration workflow for a quantized model. The process involves enabling calibration on TensorQuantizer modules, feeding calibration data, computing optimal quantization parameters, and finalizing the calibration.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/userguide.rst#2025-04-06_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Find the TensorQuantizer and enable calibration\nfor name, module in model.named_modules():\n    if name.endswith('_quantizer'):\n        module.enable_calib()\n        module.disable_quant()  # Use full precision data to calibrate\n        \n# Feeding data samples\nmodel(x)\n# ...\n\n# Finalize calibration\nfor name, module in model.named_modules():\n    if name.endswith('_quantizer'):\n        module.load_calib_amax()\n        module.disable_calib()\n        module.enable_quant()\n        \n# If running on GPU, it needs to call .cuda() again because new tensors will be created by calibration process\nmodel.cuda()\n\n# Keep running the quantized model\n# ...\n```\n\n----------------------------------------\n\nTITLE: Running the TensorRT Network API Example\nDESCRIPTION: Command to execute the example Python script that demonstrates using the TensorRT Network API with Polygraphy.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/05_using_tensorrt_network_api/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 example.py\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT-OSS for Linux x86-64\nDESCRIPTION: Commands for building TensorRT-OSS on Linux x86-64 architecture with CUDA 12.8. Uses CMake for build configuration and make for compilation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/README.md#2025-04-06_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncd $TRT_OSSPATH\nmkdir -p build && cd build\ncmake .. -DTRT_LIB_DIR=$TRT_LIBPATH -DTRT_OUT_DIR=`pwd`/out\nmake -j$(nproc)\n```\n\n----------------------------------------\n\nTITLE: Downloading Alternative BERT Model (Base)\nDESCRIPTION: Downloads a BERT Base model checkpoint instead of the default BERT Large model.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nbash scripts/download_model.sh base\n```\n\n----------------------------------------\n\nTITLE: Installing TensorRT Python Wheel in Bash\nDESCRIPTION: This snippet demonstrates how to install the TensorRT Python wheel using pip. It installs the wheel file generated in the previous build step.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/python/README.md#2025-04-06_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pip install ./build/bindings_wheel/dist/tensorrt-*.whl\n```\n\n----------------------------------------\n\nTITLE: Training the Baseline Model on Fashion MNIST\nDESCRIPTION: Compiles and trains the neural network for 5 epochs using the Adam optimizer and sparse categorical crossentropy loss. The model's accuracy on the test set is evaluated and stored as the baseline.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/getting_started.ipynb#2025-04-06_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Train original classification model\nmodel.compile(\n    optimizer=\"adam\",\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[\"accuracy\"],\n)\n\nmodel.fit(\n    train_images, train_labels, batch_size=128, epochs=5, validation_split=0.1\n)\n\n# get baseline model accuracy\n_, baseline_model_accuracy = model.evaluate(\n    test_images, test_labels, verbose=0\n)\nprint(\"Baseline test accuracy:\", baseline_model_accuracy)\n```\n\n----------------------------------------\n\nTITLE: Running the CharRNN Sample\nDESCRIPTION: Command to execute the compiled CharRNN sample with the data directory parameter.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleCharRNN/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./sample_char_rnn --datadir=<path/to/data>\n```\n\n----------------------------------------\n\nTITLE: Compiling and Evaluating ResNet50 V1 Model in TensorFlow\nDESCRIPTION: This snippet defines a function to compile the model with SGD optimizer and sparse categorical crossentropy loss. It then compiles and evaluates the baseline model on the validation dataset.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/example_resnet50v1.ipynb#2025-04-06_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef compile_model(model, lr=0.001):\n    model.compile(\n        optimizer=tf.keras.optimizers.SGD(learning_rate=lr),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n        metrics=[\"accuracy\"],\n    )\n\ncompile_model(model)\n_, baseline_model_accuracy = model.evaluate(val_batches)\nprint(\"Baseline val accuracy: {:.3f}%\".format(baseline_model_accuracy*100))\n```\n\n----------------------------------------\n\nTITLE: Running the PyTorch Tensor Example with TensorRT\nDESCRIPTION: Command to execute the example script that demonstrates working with PyTorch tensors in Polygraphy and TensorRT.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/09_working_with_pytorch_tensors/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 example.py\n```\n\n----------------------------------------\n\nTITLE: Running Inference\nDESCRIPTION: Command to run the inference script using the built TensorRT engine on the test image, specifying input, output, and label files.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/dds_faster_rcnn/README.md#2025-04-06_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npython3 infer.py \\\n    --engine ./fasterrcnn12_trt.engine \\\n    --input ./demo.jpg \\\n    --output ./output_dir \\ \n    --labels labels_coco_80.txt\n```\n\n----------------------------------------\n\nTITLE: Using Lazy Loader API in Polygraphy\nDESCRIPTION: An example showing how to use Polygraphy's lazy loader pattern to create a loader for importing an ONNX model and generating a serialized TensorRT engine.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/06_immediate_eval_api/README.md#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nbuild_engine = EngineBytesFromNetwork(NetworkFromOnnxPath(\"/path/to/model.onnx\"))\n```\n\n----------------------------------------\n\nTITLE: Inspecting Polygraphy Inference Results\nDESCRIPTION: This command uses Polygraphy's inspect feature to examine the inference outputs stored in the 'inference_results.json' file. It provides a way to analyze the comparison results between ONNX-Runtime and TensorRT.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/01_comparing_frameworks/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy inspect data inference_results.json\n```\n\n----------------------------------------\n\nTITLE: Building FP16 Precision TensorRT Engine\nDESCRIPTION: Uses a custom build_engine.py script to create a TensorRT engine with FP16 precision from the converted ONNX model.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/detectron2/README.md#2025-04-06_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython3 build_engine.py \\\n    --onnx /path/to/converted.onnx \\\n    --engine /path/to/engine.trt \\\n    --precision fp16\n```\n\n----------------------------------------\n\nTITLE: Plotting Segmentation Output from TensorRT Inference\nDESCRIPTION: This snippet displays the output image after TensorRT inference, showing the results of semantic segmentation. It uses matplotlib to visualize the segmented image.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/SemanticSegmentation/tutorial-runtime.ipynb#2025-04-06_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nplt.imshow(Image.open(output_file))\n```\n\n----------------------------------------\n\nTITLE: Setting TensorRT Library Path on Linux\nDESCRIPTION: Bash commands to extract TensorRT GA build and set the library path environment variable on Linux.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncd ~/Downloads\ntar -xvzf TensorRT-10.9.0.34.Linux.x86_64-gnu.cuda-12.8.tar.gz\nexport TRT_LIBPATH=`pwd`/TensorRT-10.9.0.34\n```\n\n----------------------------------------\n\nTITLE: Generating ONNX Model via Command Line\nDESCRIPTION: Command to generate and save a complex ONNX model to model.onnx using the Python script.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/examples/07_creating_a_model_with_the_layer_api/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 generate.py\n```\n\n----------------------------------------\n\nTITLE: Running Inference with PyTorch Model\nDESCRIPTION: Performs inference using the GPU-based ResNet50 model with the input batch and converts predictions to numpy array.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/2. Using PyTorch through ONNX.ipynb#2025-04-06_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nwith torch.no_grad():\n    predictions = np.array(resnet50_gpu(input_batch_gpu).cpu())\n\npredictions.shape\n```\n\n----------------------------------------\n\nTITLE: Installing TensorFlow Quantization with Docker\nDESCRIPTION: Commands for setting up NVIDIA TensorFlow 2.x Quantization toolkit using Docker. The script clones the repository, pulls the recommended TensorFlow Docker image, runs the container with GPU support, and mounts the repository for installation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ cd ~/\n$ git clone https://github.com/NVIDIA/TensorRT.git\n$ docker pull nvcr.io/nvidia/tensorflow:22.03-tf2-py3\n$ docker run -it --runtime=nvidia --gpus all --net host -v ~/TensorRT/tools/tensorflow-quantization:/home/tensorflow-quantization nvcr.io/nvidia/tensorflow:22.03-tf2-py3 /bin/bash\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Quantization Fine-tuning\nDESCRIPTION: Sets up required Python imports for model training, quantization, and data handling. Includes PyTorch, TensorRT quantization modules, and utility libraries.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/examples/finetune_quant_resnet50.ipynb#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport datetime\nimport os\nimport sys\nimport time\n\nimport torch\nimport torch.utils.data\nfrom torch import nn\n\nfrom tqdm import tqdm\n\nimport torchvision\nfrom torchvision import transforms\n\nfrom pytorch_quantization import nn as quant_nn\nfrom pytorch_quantization import calib\nfrom pytorch_quantization.tensor_quant import QuantDescriptor\n\nfrom absl import logging\nlogging.set_verbosity(logging.FATAL)  # Disable logging as they are too noisy in notebook\n```\n\n----------------------------------------\n\nTITLE: Building Plugin on Linux\nDESCRIPTION: Commands to build the custom plugin and Python bindings on Linux systems using CMake\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/onnx_custom_plugin/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmkdir build && pushd build\ncmake .. && make -j\npopd\n```\n\n----------------------------------------\n\nTITLE: Downloading SQuAD Dataset for BERT QA\nDESCRIPTION: Downloads the Stanford Question Answering Dataset (SQuAD) for BERT question answering tasks. SQuAD consists of questions posed by crowdworkers on Wikipedia articles where answers are segments of text from the passage.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/BERT-TRT-INT8-QAT-sparse.ipynb#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!bash ../scripts/download_squad.sh\n```\n\n----------------------------------------\n\nTITLE: Installing TFOD API in NGC TF2 Docker\nDESCRIPTION: Series of commands to install the TensorFlow Object Detection API in an NGC TensorFlow 2 Docker container.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/tensorflow_object_detection_api/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd /workspace\nwget https://github.com/protocolbuffers/protobuf/releases/download/v3.15.4/protoc-3.15.4-linux-x86_64.zip\nunzip protoc*.zip bin/protoc -d /usr/local\ngit clone https://github.com/tensorflow/models.git\ncd /workspace/models/research\ngit checkout 66e22c4\nprotoc object_detection/protos/*.proto --python_out=.\ncp object_detection/packages/tf2/setup.py ./\n## Pin pyyaml==6.0.1 to avoid v5.4.1 with known CVEs\nsed -i '22i\\    '\"'\"'pyyaml==6.0.1'\"'\"',' setup.py\npip --use-deprecated=legacy-resolver install .\n```\n\n----------------------------------------\n\nTITLE: Converting PyTorch Model to ONNX\nDESCRIPTION: Command to convert the PackNet PyTorch model to ONNX format with custom layer handling\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/onnx_packnet/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 convert_to_onnx.py --output model.onnx\n```\n\n----------------------------------------\n\nTITLE: Native Build for Jetson aarch64\nDESCRIPTION: Native build commands for TensorRT-OSS on Jetson platforms using aarch64 architecture and CUDA 12.8. Requires explicit C compiler specification.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/README.md#2025-04-06_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncd $TRT_OSSPATH\nmkdir -p build && cd build\ncmake .. -DTRT_LIB_DIR=$TRT_LIBPATH -DTRT_OUT_DIR=`pwd`/out -DTRT_PLATFORM_ID=aarch64 -DCUDA_VERSION=12.8\nCC=/usr/bin/gcc make -j$(nproc)\n```\n\n----------------------------------------\n\nTITLE: Using Lazy Loader API for TensorRT Engine Creation\nDESCRIPTION: Example of using the lazy loader API in Polygraphy to parse an ONNX network, create a TensorRT config, and build a TensorRT engine.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/06_immediate_eval_api/README.md#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nparse_network = NetworkFromOnnxPath(\"/path/to/model.onnx\")\ncreate_config = CreateConfig(fp16=True, tf32=True)\nbuild_engine = EngineFromNetwork(parse_network, create_config)\nengine = build_engine()\n```\n\n----------------------------------------\n\nTITLE: Running TensorRT Inference Benchmark\nDESCRIPTION: Command to run inference benchmarks for BERT models on different GPU architectures.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_32\n\nLANGUAGE: bash\nCODE:\n```\nbash scripts/inference_benchmark.sh --gpu <arch>\n```\n\n----------------------------------------\n\nTITLE: Analyzing Convolution Layers and Tactic Counts in Python\nDESCRIPTION: This snippet demonstrates how to use the TREx API to filter convolution layers from the plan and count the occurrences of different tactics. It showcases the convenience functions provided by the package for data analysis.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/trex/README.md#2025-04-06_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nconvs = plan.get_layers_by_type('Convolution')\ntactic_cnt = group_count(convs, 'tactic')\n```\n\n----------------------------------------\n\nTITLE: Creating ONNX Graph from TensorFlow Saved Model\nDESCRIPTION: Command to convert a re-exported TensorFlow saved model to ONNX format using the create_onnx.py script.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/tensorflow_object_detection_api/README.md#2025-04-06_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython create_onnx.py \\\n    --pipeline_config /path/to/exported/pipeline.config \\\n    --saved_model /path/to/exported/saved_model \\\n    --onnx /path/to/save/model.onnx\n```\n\n----------------------------------------\n\nTITLE: Evaluating BERT F1 Score\nDESCRIPTION: Commands to evaluate the F1 score of the BERT model using the SQuAD v1.1 development dataset.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_31\n\nLANGUAGE: bash\nCODE:\n```\npython3 inference_varseqlen.py -e engines/megatron_large_seqlen384_int8qat_sparse.engine -s 384 -sq ./squad/dev-v1.1.json -v models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/vocab.txt -o ./predictions.json\npython3 squad/evaluate-v1.1.py  squad/dev-v1.1.json  ./predictions.json 90\n```\n\n----------------------------------------\n\nTITLE: Running QAT Workflow Script\nDESCRIPTION: Executes the quantization and fine-tuning workflow for EfficientNet models.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/examples/efficientnet/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython run_qat_workflow.py\n```\n\n----------------------------------------\n\nTITLE: Warming Up TensorRT Model for Inference in Python\nDESCRIPTION: This snippet demonstrates the warm-up process for the TensorRT model. It runs a single prediction to ensure the model is loaded and ready for efficient inference.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/2. Using PyTorch through ONNX.ipynb#2025-04-06_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Warming up...\")\n\npred = predict(preprocessed_images)\n\nprint(\"Done warming up!\")\n```\n\n----------------------------------------\n\nTITLE: Comparing Baseline and QAT ResNet50 V1 Models in TensorFlow\nDESCRIPTION: This snippet compares the accuracy of the baseline and quantized-aware trained (QAT) ResNet50 V1 models. It calculates and prints the accuracy difference, including the direction of change (positive or negative).\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/example_resnet50v1.ipynb#2025-04-06_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Baseline vs QAT: {:.3f}% vs {:.3f}%\".format(baseline_model_accuracy*100, qat_model_accuracy*100))\n\nacc_diff = (qat_model_accuracy - baseline_model_accuracy)*100\nacc_diff_sign = \"\" if acc_diff == 0 else (\"-\" if acc_diff < 0 else \"+\")\nprint(\"Accuracy difference of {}{:.3f}%\".format(acc_diff_sign, abs(acc_diff)))\n```\n\n----------------------------------------\n\nTITLE: Generating Excel Summary Reports\nDESCRIPTION: Creates Excel summary reports with default and custom configurations, including dataframes and images\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/engine_report_card.ipynb#2025-04-06_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom trex.excel_summary import *\n\nsummary = ExcelSummary(plan, path=\"default_summary.xlsx\")\nsummary.generate_default_summary()\n\nsummary = ExcelSummary(plan, path=\"customized_summary.xlsx\")\nsummary.add_dataframes({\"df\": plan.df})\nsummary.add_images({\"trex_logo\": \"../images/trex_logo.png\"})\nsummary.add_dataframes({\"clean_df\": clean_for_display(plan.df)})\nsummary.save()\n\nwith ExcelSummary(plan, path=\"customized_summary_with_manager.xlsx\") as summary:\n    summary.add_dataframes({\"df\": plan.df})\n    summary.add_images({\"trex_logo\": \"../images/trex_logo.png\"})\n    summary.add_dataframes({\"clean_df\": clean_for_display(plan.df)})\n```\n\n----------------------------------------\n\nTITLE: Running Model Processing Script\nDESCRIPTION: Command to download and preprocess the BiDAF model, replacing Hardmax layer with CustomHardmax\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/onnx_custom_plugin/README.md#2025-04-06_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython3 model.py\n```\n\n----------------------------------------\n\nTITLE: Implementing ReshapeDestroyer Polygraphy Extension for ONNX Models in Python\nDESCRIPTION: This code defines a ReshapeDestroyer class that extends Polygraphy's BaseRunner. It replaces no-op Reshape nodes with Identity nodes in an ONNX model and performs inference. The extension includes methods for loading and modifying the model, as well as running inference.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/dev/02_extending_polygraphy_run/extension_module/README.md#2025-04-06_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport copy\nimport onnx\nimport numpy as np\nfrom polygraphy.backend.onnx import onnx_util\nfrom polygraphy.backend.onnx import OnnxTensorRTRunner\nfrom polygraphy.common import TensorMetadata\nfrom polygraphy.comparator import IterationResult\nfrom polygraphy.extensions import Extension\n\nclass ReshapeDestroyer(Extension):\n    def __init__(self, model):\n        self.model = model\n\n    def load(self):\n        model = onnx.load(self.model)\n        graph = model.graph\n\n        for node in graph.node:\n            if node.op_type == \"Reshape\":\n                shape = onnx.numpy_helper.to_array(onnx_util.get_constant(graph, node.input[1]))\n                if shape.size == 0 or np.prod(shape) == 0:\n                    node.op_type = \"Identity\"\n                    del node.input[1]\n\n        self.trt_runner = OnnxTensorRTRunner(model)\n        self.trt_runner.load()\n\n    def activate(self):\n        return self\n\n    def deactivate(self):\n        self.trt_runner.deactivate()\n\n    def get_input_metadata(self):\n        return self.trt_runner.get_input_metadata()\n\n    def infer(self, feed_dict):\n        outputs = self.trt_runner.infer(feed_dict)\n        return outputs\n\n    def get_build_env(self):\n        return self.trt_runner.get_build_env()\n```\n\n----------------------------------------\n\nTITLE: Initializing TensorRT Analysis Dependencies in Python\nDESCRIPTION: Imports required libraries and configures display settings for TensorRT engine analysis including IPython, widgets, and custom trex modules\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/compare_engines.ipynb#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport IPython\nfrom ipywidgets import widgets\nfrom trex import *\nfrom trex.notebook import *\nfrom trex.report_card import *\nfrom trex.compare_engines import *\n\n# Configure a wider output (for the wide graphs)\nset_wide_display()\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for TensorRT\nDESCRIPTION: Specifies required Python packages with specific version constraints and platform/Python version conditions. Includes CUDA, deep learning, and utility packages from both PyPI and NVIDIA NGC repositories.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/python_plugin/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ncuda-python==12.2.0; python_version <= \"3.10\"\ncuda-python==12.6.0; python_version >= \"3.11\"\ncupy-cuda12x\nnumba\ntriton; platform_system != \"Windows\"\ntorch\n--extra-index-url https://pypi.ngc.nvidia.com\npolygraphy\ncolored\nnumpy==1.23.5; (platform_system != \"Windows\" and python_version <= \"3.10\")\nnumpy==1.26.4; (platform_system != \"Windows\" and python_version >= \"3.11\")\nonnx==1.16.0; platform_system == \"Windows\"\n--extra-index-url https://pypi.ngc.nvidia.com\nonnx-graphsurgeon\npywin32; platform_system == \"Windows\"\npyyaml==6.0.1\nrequests==2.32.2\ntqdm==4.66.4\n```\n\n----------------------------------------\n\nTITLE: Downloading Pre-trained TF2 Detection Model\nDESCRIPTION: Commands to download and extract a pre-trained TensorFlow 2 object detection model from the TF2 Detection Model Zoo.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/tensorflow_object_detection_api/README.md#2025-04-06_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nwget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz\ntar -xvf ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Folding Constants in ONNX Model\nDESCRIPTION: Folds constant expressions in the ONNX model, replacing '((a + b) + d)' with a single constant tensor. The resulting model computes 'output = input + e' where e is the pre-computed constant.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/examples/05_folding_constants/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 fold.py\n```\n\n----------------------------------------\n\nTITLE: Filtering Scores with EfficientNMSFilter CUDA Kernel\nDESCRIPTION: This CUDA kernel filters scores below a given threshold while maintaining indexing to cross-reference scores with their corresponding box coordinates.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/efficientNMSPlugin/README.md#2025-04-06_snippet_3\n\nLANGUAGE: CUDA\nCODE:\n```\nEfficientNMSFilter\n```\n\n----------------------------------------\n\nTITLE: Adding CropAndResize Plugin Source Files in CMake\nDESCRIPTION: CMake command to add the CropAndResize plugin source files (cpp and header) to the build system. This plugin is part of the TensorRT framework and handles crop and resize operations.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/cropAndResizePlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_plugin_source(\n    cropAndResizePlugin.cpp\n    cropAndResizePlugin.h\n)\n```\n\n----------------------------------------\n\nTITLE: Converting Tensor to Float8E4M3\nDESCRIPTION: Creates a constant tensor with float32 values and sets export type to Float8E4M3. Shows two equivalent methods for setting the export data type.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/examples/12_using_numpy_unsupported_dtypes/README.md#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntensor = gs.Constant(name=\"weight\", values=np.ones(shape=(5, 3, 3, 3), dtype=np.float32), export_dtype=onnx.TensorProto.FLOAT8E4M3FN)\n# or\ntensor = gs.Constant(name=\"weight\", values=np.ones(shape=(5, 3, 3, 3), dtype=np.float32))\ntensor.export_dtype = onnx.TensorProto.FLOAT8E4M3FN\n```\n\n----------------------------------------\n\nTITLE: Aggregating Latency by Layer Type\nDESCRIPTION: Code that groups and sums layer latencies by type, displaying results as a table and horizontal bar chart. This shows which types of layers consume most time.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/tutorial.ipynb#2025-04-06_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ntime_pct_by_type = plan.df.groupby([\"type\"])[[\"latency.pct_time\", \"latency.avg_time\"]].sum().reset_index()\ntrex.notebook.display_df(time_pct_by_type)\ntrex.plotting.plotly_bar2(\n    df=time_pct_by_type,\n    title=\"% Latency Budget Per Layer Type\",\n    values_col=\"latency.pct_time\",\n    names_col=\"type\",\n    orientation='h',\n    color='type',\n    colormap=trex.colors.layer_colormap);\n```\n\n----------------------------------------\n\nTITLE: Running Model Comparison with Data Loader Script\nDESCRIPTION: Command to run model comparison between TensorRT and ONNX-Runtime using a data loader script for custom input data. Includes configuration for TensorRT optimization profiles with dynamic shapes.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/run/05_comparing_with_custom_input_data/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run dynamic_identity.onnx --trt --onnxrt \\\n    --trt-min-shapes X:[1,2,28,28] --trt-opt-shapes X:[1,2,28,28] --trt-max-shapes X:[1,2,28,28] \\\n    --data-loader-script data_loader.py\n```\n\n----------------------------------------\n\nTITLE: Evaluating ResNet50 Model with Different Calibration Methods in Python\nDESCRIPTION: This snippet demonstrates evaluating the ResNet50 model with different calibration methods, including percentile (99.9%), MSE, and entropy. It computes amax values and evaluates the model for each method.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/examples/calibrate_quant_resnet50.ipynb#2025-04-06_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nwith torch.no_grad():\n    compute_amax(model, method=\"percentile\", percentile=99.9)\n    evaluate(model, criterion, data_loader_test, device=\"cuda\", print_freq=20)\n\nwith torch.no_grad():\n    for method in [\"mse\", \"entropy\"]:\n        print(F\"{method} calibration\")\n        compute_amax(model, method=method)\n        evaluate(model, criterion, data_loader_test, device=\"cuda\", print_freq=20)\n```\n\n----------------------------------------\n\nTITLE: Defining Input Shape for Boxes Tensor in Efficient NMS Plugin\nDESCRIPTION: Specifies the input shape for the boxes tensor in the Efficient NMS plugin. The shape can be either [batch_size, number_boxes, 4] or [batch_size, number_boxes, number_classes, 4], with float32 or float16 data types.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/efficientNMSPlugin/README.md#2025-04-06_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n> **Input Shape:** `[batch_size, number_boxes, 4]` or `[batch_size, number_boxes, number_classes, 4]`\n>\n> **Data Type:** `float32` or `float16`\n```\n\n----------------------------------------\n\nTITLE: Visualizing Arithmetic Intensity of Convolution Layers in Python\nDESCRIPTION: Creates bar charts showing arithmetic intensity of convolution layers with color coding by activation size and latency. Arithmetic intensity measures compute operations per data byte, with higher values indicating more computation-bound layers.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/tutorial.ipynb#2025-04-06_snippet_24\n\nLANGUAGE: python\nCODE:\n```\ntrex.plotting.plotly_bar2(\n    convs, \n    \"Convolution Arithmetic Intensity<BR>(bar color indicates activations size)\",\n    \"attr.arithmetic_intensity\", \n    \"Name\",\n    color='total_io_size_bytes')\n\ntrex.plotting.plotly_bar2(\n    convs, \n    \"Convolution Arithmetic Intensity<BR>(bar color indicates latency)\", \n    \"attr.arithmetic_intensity\", \n    \"Name\",\n    color='latency.pct_time');\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Dependencies for TensorRT\nDESCRIPTION: This snippet lists the required Python packages for the TensorRT project. It includes CUDA libraries, deep learning frameworks like PyTorch, and utility packages. Some dependencies have version constraints or platform-specific conditions.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/aliased_io_plugin/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\ncupy-cuda12x\ntriton; platform_system != \"Windows\"\ntorch\n--extra-index-url https://pypi.ngc.nvidia.com\npolygraphy\ncolored\nnumpy==1.23.5; (platform_system != \"Windows\" and python_version <= \"3.10\")\nnumpy==1.26.4; (platform_system != \"Windows\" and python_version >= \"3.11\")\npyyaml==6.0.1\nrequests==2.32.2\ntqdm==4.66.4\n```\n\n----------------------------------------\n\nTITLE: Verifying TFOD API Installation\nDESCRIPTION: Command to verify the successful installation of the TensorFlow Object Detection API.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/tensorflow_object_detection_api/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip show object-detection\n```\n\n----------------------------------------\n\nTITLE: DeBERTa TensorRT Integration Documentation\nDESCRIPTION: Markdown documentation detailing the background, setup process, and implementation steps for optimizing DeBERTa models with TensorRT's disentangled attention plugin. Includes information about performance benchmarking and various inference methods.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/DeBERTa/README.md#2025-04-06_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# DeBERTa Model Inference with TensorRT Disentangled Attention Optimizations\n\n- [DeBERTa Model Inference with TensorRT Disentangled Attention Optimizations](#deberta-model-inference-with-tensorrt-disentangled-attention-optimizations)\n  - [Background](#background)\n  - [Performance Benchmark](#performance-benchmark)\n  - [Environment Setup](#environment-setup)\n  - [Step 1: PyTorch model to ONNX model](#step-1-pytorch-model-to-onnx-model)\n  - [Step 2: Modify ONNX model for TensorRT engine building](#step-2-modify-onnx-model-for-tensorrt-engine-building)\n  - [Step 3: Model inference with TensorRT (using Python TensorRT API or `trtexec`)](#step-3-model-inference-with-tensorrt-using-python-tensorrt-api-or-trtexec)\n  - [Optional Step: Correctness check of model with and without plugin](#optional-step-correctness-check-of-model-with-and-without-plugin)\n  - [Optional Step: Model inference with ONNX Runtime and TensorRT Execution Provider (Python API)](#optional-step-model-inference-with-onnx-runtime-and-tensorrt-execution-provider-python-api)\n\n***\n\n## Background\nA performance gap has been observed between Google's [BERT](https://arxiv.org/abs/1810.04805) design and Microsoft's [DeBERTa](https://arxiv.org/abs/2006.03654) design. The main reason of the gap is the disentangled attention design in DeBERTa triples the attention computation over BERT's regular attention. In addition to the extra matrix multiplications, the disentangled attention design also involves indirect memory accesses during the gather operations. In this regard, a [TensorRT plugin](https://github.com/NVIDIA/TensorRT/tree/main/plugin/disentangledAttentionPlugin) has been implemented to optimize DeBERTa's disentangled attention module, which is built-in since TensorRT 8.4 GA Update 2 (8.4.3) release.\n\nThis DeBERTa demo includes code and scripts for (i) exporting ONNX model from PyTorch, (ii) modifying ONNX model by inserting the plugin nodes, (iii) model inference options with TensorRT `trtexec` executable, TensorRT Python API, or ONNX Runtime with TensorRT execution provider, and (iv) measuring the correctness and performance of the optimized model.\n\nThe demo works with the [HuggingFace implementation](https://github.com/huggingface/transformers/tree/main/src/transformers/models/deberta_v2) of DeBERTa.\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies List\nDESCRIPTION: List of required Python packages and their version constraints for the TensorRT environment. Includes testing frameworks like pytest, ML libraries like TensorFlow and PyTorch, and ONNX-related packages for model conversion and optimization.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/tests/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ncolored\npytest-rerunfailures\nnumpy<2\nonnx_graphsurgeon\nonnx==1.17.0\nonnxconverter_common==1.12.2\nonnxmltools==1.11.1\nonnxruntime\nprotobuf==3.20.3\npytest\npytest-console-scripts\npytest-virtualenv\nvirtualenv\npytest-xdist\nrequests==2.25.1\nsympy==1.9\ntensorflow<2.0; python_version<'3.8'\ntf2onnx\ntorch\nwheel\npyyaml\n```\n\n----------------------------------------\n\nTITLE: Converting ONNX Model to TensorRT Engine\nDESCRIPTION: Uses trtexec command-line tool to convert the ONNX model into an optimized TensorRT engine.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/1. Introduction.ipynb#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n!trtexec --onnx=resnet50/model.onnx --saveEngine=resnet_engine_intro.engine\n```\n\n----------------------------------------\n\nTITLE: Implementing Forward Pass for Quantized ResNet Block in PyTorch\nDESCRIPTION: This snippet shows the forward method for a ResNet block (either BasicBlock or Bottleneck) with quantization support. It applies quantization to the residual connection when the quantize flag is set.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/tutorials/quant_resnet50.rst#2025-04-06_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, x: Tensor) -> Tensor:\n    # other code...\n    if self._quantize:\n        out += self.residual_quantizer(identity)\n    else:\n        out += identity\n    out = self.relu(out)\n\n    return out\n```\n\n----------------------------------------\n\nTITLE: Referencing Node Class in ONNX GraphSurgeon\nDESCRIPTION: This directive generates automatic documentation for the Node class in the ONNX GraphSurgeon library. The autoclass directive instructs Sphinx to extract documentation from the actual Node class implementation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/docs/ir/node.rst#2025-04-06_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. autoclass:: onnx_graphsurgeon.Node\n```\n\n----------------------------------------\n\nTITLE: Running Inference with TensorRT Model\nDESCRIPTION: Performs inference on dummy input data using the optimized TensorRT model.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/1. Introduction.ipynb#2025-04-06_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ntrt_model.predict(dummy_input_batch)[:10]\n```\n\n----------------------------------------\n\nTITLE: Downloading SQuAD Dataset\nDESCRIPTION: Downloads the SQuAD v1.1 training and development datasets using a provided script.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbash ./scripts/download_squad.sh\n```\n\n----------------------------------------\n\nTITLE: Report Card Table Views\nDESCRIPTION: Generates detailed table views for each engine plan\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/compare_engines.ipynb#2025-04-06_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nprint(plan1.name)\nreport_card_table_view(plan1)\n```\n\nLANGUAGE: python\nCODE:\n```\nprint(plan2.name)\nreport_card_table_view(plan2)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking BERT-large with Large Batch Size\nDESCRIPTION: This snippet runs a performance benchmark on the BERT-large model with sequence length 384 using a larger batch size of 64. It uses the same perf.py script to measure throughput performance.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/BERT-TRT-FP16.ipynb#2025-04-06_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nBATCH_SIZE = 64\n!python3 ../perf.py -e ./engines_$TRT_VERSION/bert_large_384.engine -b $BATCH_SIZE -s 384 -i 100 -w 20\n```\n\n----------------------------------------\n\nTITLE: Rendering and Displaying TensorRT Engine Graph as PNG in Python\nDESCRIPTION: Renders the engine graph as a PNG file and displays it directly in the notebook. While PNG files can be rendered inside notebooks, they may suffer from resolution issues for large graphs.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/tutorial.ipynb#2025-04-06_snippet_21\n\nLANGUAGE: python\nCODE:\n```\npng_name = trex.graphing.render_dot(graph, engine_name, 'png')\nfrom IPython.display import Image\ndisplay(Image(filename=png_name))\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine with trtexec\nDESCRIPTION: Command to build a TensorRT engine directly from ONNX model using trtexec utility. Configures optimization shapes and workspace memory pool size.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/efficientdet/README.md#2025-04-06_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ntrtexec \\\n    --onnx=/path/to/model.onnx \\\n    --saveEngine=/path/to/engine.trt \\\n    --optShapes=input:$INPUT_SHAPE \\\n    --memPoolSize=workspace:1024\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorRT Inference Precision\nDESCRIPTION: Sets the target precision for TensorRT inference, choosing between FP16 and FP32 based on the USE_FP16 flag.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/2. Using PyTorch through ONNX.ipynb#2025-04-06_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\nUSE_FP16 = True\ntarget_dtype = np.float16 if USE_FP16 else np.float32\n```\n\n----------------------------------------\n\nTITLE: Example JSON Lint Report from Polygraphy\nDESCRIPTION: Sample JSON report generated by Polygraphy's check lint subtool, showing the summary of passing and failing nodes, as well as detailed lint entries with error levels, sources, and messages.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/check/01_linting_an_onnx_model/README.md#2025-04-06_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"summary\": {\n        \"passing\": [\n            \"MatMul_1\",\n            \"cast_to_int64\",\n            \"NonZero\"\n        ],\n        \"failing\": [\n            \"MatMul_0\",\n            \"MatMul_3\",\n            \"Add_0\"\n        ]\n    },\n    \"lint_entries\": [\n        {\n            \"level\": \"exception\",\n            \"source\": \"onnx_checker\",\n            \"message\": \"Field 'name' of 'graph' is required to be non-empty.\"\n        },\n        {\n            \"level\": \"exception\",\n            \"source\": \"onnxruntime\",\n            \"message\": \" Incompatible dimensions for matrix multiplication\",\n            \"nodes\": [\n                \"MatMul_3\"\n            ]\n        },\n        {\n            \"level\": \"exception\",\n            \"source\": \"onnxruntime\",\n            \"message\": \" Incompatible dimensions\",\n            \"nodes\": [\n                \"Add_0\"\n            ]\n        },\n        {\n            \"level\": \"exception\",\n            \"source\": \"onnxruntime\",\n            \"message\": \" Incompatible dimensions for matrix multiplication\",\n            \"nodes\": [\n                \"MatMul_0\"\n            ]\n        },\n        {\n            \"level\": \"warning\",\n            \"source\": \"onnx_graphsurgeon\",\n            \"message\": \"Input: 'A' does not affect outputs, can be removed.\"\n        },\n        {\n            \"level\": \"warning\",\n            \"source\": \"onnx_graphsurgeon\",\n            \"message\": \"Input: 'B' does not affect outputs, can be removed.\"\n        },\n        {\n            \"level\": \"warning\",\n            \"source\": \"onnx_graphsurgeon\",\n            \"message\": \"Does not affect outputs, can be removed.\",\n            \"nodes\": [\n                \"MatMul_0\"\n            ]\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Analyzing Top 5 Predictions\nDESCRIPTION: Identifies and displays the top 5 predictions from the model output, showing class indices and corresponding likelihood values.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/2. Using PyTorch through ONNX.ipynb#2025-04-06_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nindices = (-predictions[0]).argsort()[:5]\nprint(\"Class | Likelihood\")\nlist(zip(indices, predictions[0][indices]))\n```\n\n----------------------------------------\n\nTITLE: Defining Quantization Descriptors and TensorQuantizers\nDESCRIPTION: Shows how to define a QuantDescriptor to specify quantization parameters and create a TensorQuantizer module. This snippet demonstrates creating a 4-bit unsigned quantizer with axis-based quantization.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/userguide.rst#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pytorch_quantization.tensor_quant import QuantDescriptor\nfrom pytorch_quantization.nn.modules.tensor_quantizer import TensorQuantizer\n\nquant_desc = QuantDescriptor(num_bits=4, fake_quant=False, axis=(0), unsigned=True)\nquantizer = TensorQuantizer(quant_desc)\n\ntorch.manual_seed(12345)\nx = torch.rand(10, 9, 8, 7)\n\nquant_x = quantizer(x)\n```\n\n----------------------------------------\n\nTITLE: Downloading SQuAD Dataset\nDESCRIPTION: Downloads the SQuAD v1.1 training and development datasets using a provided script.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nbash ./scripts/download_squad.sh\n```\n\n----------------------------------------\n\nTITLE: Creating ONNX Model from TensorFlow Saved Model\nDESCRIPTION: Script command to convert a TensorFlow saved model to ONNX format with specified input size. Supports various preprocessing methods and NMS threshold configuration.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/efficientdet/README.md#2025-04-06_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython3 create_onnx.py \\\n    --input_size 512,512 \\\n    --saved_model /path/to/saved_model \\\n    --onnx /path/to/model.onnx\n```\n\n----------------------------------------\n\nTITLE: Building C++ Components for CUDA Graph Support\nDESCRIPTION: Compiles the C++ components needed for CUDA Graph support using CMake and Make.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p build; pushd build\ncmake .. -DPYTHON_EXECUTABLE=$(which python)\nmake -j\npopd\n```\n\n----------------------------------------\n\nTITLE: Creating Quantized Neural Network Modules\nDESCRIPTION: Demonstrates how to replace standard PyTorch nn.Linear and nn.Conv2d modules with their quantized equivalents. The quantized modules apply quantization to both weights and activations using specified descriptors.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/userguide.rst#2025-04-06_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom torch import nn\n\nfrom pytorch_quantization import tensor_quant\nimport pytorch_quantization.nn as quant_nn\n\n# pytorch's module\nfc1 = nn.Linear(in_features, out_features, bias=True)\nconv1 = nn.Conv2d(in_channels, out_channels, kernel_size)\n\n# quantized version\nquant_fc1 = quant_nn.Linear(\n    in_features, out_features, bias=True,\n    quant_desc_input=tensor_quant.QUANT_DESC_8BIT_PER_TENSOR,\n    quant_desc_weight=tensor_quant.QUANT_DESC_8BIT_LINEAR_WEIGHT_PER_ROW)\nquant_conv1 = quant_nn.Conv2d(\n    in_channels, out_channels, kernel_size,\n    quant_desc_input=tensor_quant.QUANT_DESC_8BIT_PER_TENSOR,\n    quant_desc_weight=tensor_quant.QUANT_DESC_8BIT_CONV2D_WEIGHT_PER_CHANNEL)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking TensorRT Engine Performance\nDESCRIPTION: Command to benchmark the TensorRT engine using trtexec utility with CUDA graphs and specific iteration settings.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/efficientdet/README.md#2025-04-06_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ntrtexec \\\n    --loadEngine=/path/to/engine.trt \\\n    --useCudaGraph --noDataTransfers \\\n    --iterations=100 --avgRuns=100\n```\n\n----------------------------------------\n\nTITLE: Installing trex Package\nDESCRIPTION: Commands for installing trex package in development mode, with options for core functionality or full notebook support.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ python3 -m pip install -e .\n\n# For full installation including notebook packages:\n$ python3 -m pip install -e .[notebook]\n```\n\n----------------------------------------\n\nTITLE: Converting SavedModel to ONNX\nDESCRIPTION: Converts the TensorFlow SavedModel to ONNX format using tf2onnx with opset 13.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/examples/efficientnet/README.md#2025-04-06_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m tf2onnx.convert --saved-model <path_to_saved_model> --output model_qat.onnx  --opset 13\n```\n\n----------------------------------------\n\nTITLE: Installing Graphviz Dependency for Engine Visualization\nDESCRIPTION: Command to install the Graphviz dependency required for graph drawing functionality in the utility scripts.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/utils/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ sudo apt-get --yes install graphviz\n```\n\n----------------------------------------\n\nTITLE: Running Polygraphy with Custom TensorRT Network and Config Scripts\nDESCRIPTION: Command to run a TensorRT network with a custom builder configuration, both defined in separate Python scripts.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/run/04_defining_a_tensorrt_network_or_config_manually/README.md#2025-04-06_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run --trt define_network.py --model-type=trt-network-script --trt-config-script=create_config.py\n```\n\n----------------------------------------\n\nTITLE: Exporting QAT SavedModel\nDESCRIPTION: Exports the quantized and fine-tuned model to SavedModel format with specified checkpoint path and model version.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/examples/efficientnet/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython export.py --ckpt <path_to_pretrained_ckpt> --output <saved_model_output_name> --model_version b0\n```\n\n----------------------------------------\n\nTITLE: Example Usage of process_engine.py for ONNX to JSON Pipeline\nDESCRIPTION: Demonstrates how to use process_engine.py to build and profile an engine from an ONNX model, generating engine files, logs, and visualization artifacts.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/utils/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ process_engine.py my_onnx.onnx outputs_dir int8\n```\n\n----------------------------------------\n\nTITLE: Counting Layer Types and Displaying Results\nDESCRIPTION: Code that counts the occurrences of each layer type in the engine plan and displays the results both as text and as an interactive table.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/tutorial.ipynb#2025-04-06_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nlayer_types = trex.group_count(plan.df, 'type')\n\n# Simple DF print\nprint(layer_types)\n\n# dtale DF display\ntrex.notebook.display_df(layer_types)\n```\n\n----------------------------------------\n\nTITLE: Modifying ONNX Model with Python\nDESCRIPTION: This command executes a Python script that modifies the previously generated ONNX model in various ways and saves the result as 'modified.onnx'. The modifications include removing inputs, changing node types, adding new nodes, and altering graph outputs.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/examples/04_modifying_a_model/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 modify.py\n```\n\n----------------------------------------\n\nTITLE: Creating and Visualizing Original TensorFlow ResNet Model\nDESCRIPTION: This snippet creates the original ResNet model and generates a visual representation of its architecture.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/notebooks/simple_network_quantize_specific_class.ipynb#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nnn_model_original = tiny_resnet.model()\ntf.keras.utils.plot_model(nn_model_original, to_file = assets.simple_network_quantize_specific_class.fp32 + \"/model.png\")\n```\n\n----------------------------------------\n\nTITLE: Creating Treemap of Latencies with Activations Size\nDESCRIPTION: Code that creates a treemap visualization where size indicates layer latency and color indicates activations size, revealing relationships between computational and memory demands.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/tutorial.ipynb#2025-04-06_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfig = px.treemap(\n    plan.df,\n    path=['type', 'Name'],\n    values='latency.pct_time',\n    title='Treemap Of Layer Latencies (Size Indicates Latency. Color Indicates Activations Size)',\n    color='total_io_size_bytes')\nfig.update_traces(root_color=\"white\")\nfig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\nfig.show()\n```\n\n----------------------------------------\n\nTITLE: Building and Refitting Version Compatible TensorRT Engine\nDESCRIPTION: Command to build and refit a version compatible TensorRT engine, which uses tensorrt_dispatch package instead of tensorrt package for compatibility across different TensorRT versions.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/engine_refit_onnx_bidaf/README.md#2025-04-06_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython3 build_and_refit_engine.py --version-compatible\n```\n\n----------------------------------------\n\nTITLE: Saving and Converting ResNet50 V1 Model to ONNX in TensorFlow\nDESCRIPTION: This code saves the baseline ResNet50 V1 model in TensorFlow's SavedModel format and then converts it to ONNX format using a custom utility function. It uses the previously defined save directory from hyperparameters.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/example_resnet50v1.ipynb#2025-04-06_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel_save_path = os.path.join(HYPERPARAMS[\"save_root_dir\"], \"saved_model_baseline\")\nmodel.save(model_save_path)\nconvert_saved_model_to_onnx(saved_model_dir=model_save_path,\n                            onnx_model_path=model_save_path + \".onnx\")\n```\n\n----------------------------------------\n\nTITLE: Loading ResNet50 Model to GPU\nDESCRIPTION: Loads the ResNet50 model to the GPU for accelerated inference, setting it to evaluation mode.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/2. Using PyTorch through ONNX.ipynb#2025-04-06_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nresnet50_gpu = models.resnet50(pretrained=True, progress=False).to(\"cuda\").eval()\n```\n\n----------------------------------------\n\nTITLE: Generating Capability Report with Polygraphy for ONNX Model in Bash\nDESCRIPTION: This command uses Polygraphy's 'inspect capability' subtool to generate a detailed report on TensorRT's ONNX operator support for a given ONNX model. The '--with-partitioning' flag enables partitioning and saving of supported and unsupported subgraphs.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/inspect/08_inspecting_tensorrt_onnx_support/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy inspect capability --with-partitioning model.onnx\n```\n\n----------------------------------------\n\nTITLE: Installing TensorRT\nDESCRIPTION: Commands for installing the latest TensorRT release via pip\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/Diffusion/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pip install --upgrade pip\npip install --pre tensorrt-cu12\n```\n\n----------------------------------------\n\nTITLE: Package Version Requirements\nDESCRIPTION: List of required package versions for the demo application\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/Diffusion/README.md#2025-04-06_snippet_4\n\nLANGUAGE: text\nCODE:\n```\ndiffusers           0.31.0\nonnx                1.15.0\nonnx-graphsurgeon   0.5.2\nonnxruntime         1.16.3\npolygraphy          0.49.9\ntensorrt            10.9.0.34\ntokenizers          0.13.3\ntorch               2.2.0\ntransformers        4.42.2\ncontrolnet-aux      0.0.6\nnvidia-modelopt     0.15.1\n```\n\n----------------------------------------\n\nTITLE: Generating TensorRT Config Script Template with Polygraphy\nDESCRIPTION: Command to generate a template script for creating a TensorRT builder configuration using Polygraphy's template feature.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/run/04_defining_a_tensorrt_network_or_config_manually/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy template trt-config -o my_create_config.py\n```\n\n----------------------------------------\n\nTITLE: Installing Backend-Specific Dependencies\nDESCRIPTION: Command to install dependencies for specific Polygraphy backends using the provided requirements files.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/README.md#2025-04-06_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install -r polygraphy/backend/<name>/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running QAT Workflow for MobileNet Models\nDESCRIPTION: This command executes the QAT workflow script that handles quantization, fine-tuning, and saving the model in SavedModel format, with automatic conversion to ONNX.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/examples/mobilenet/README.md#2025-04-06_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npython run_qat_workflow.py\n```\n\n----------------------------------------\n\nTITLE: Defining Output Structure for regionPlugin in TensorRT\nDESCRIPTION: Describes the output tensor structure for the regionPlugin, including shape and channel information for processed bounding box predictions, objectness scores, and class probabilities.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/regionPlugin/README.md#2025-04-06_snippet_1\n\nLANGUAGE: Markdown\nCODE:\n```\nThe output has the same shape as the input, in other words, `[N, C, H, W]`. The information order of the channels are:\n-  `[sigmoid(t_x), sigmoid(t_y), t_w, t_h, sigmoid(t_o), p_1, p_2, ..., p_classes]` for `bbox_1`\n-  `[sigmoid(t_x), sigmoid(t_y), t_w, t_h, sigmoid(t_o), p_1, p_2, ..., p_classes]` for `bbox_2`, and so on\n-  `[sigmoid(t_x), sigmoid(t_y), t_w, t_h, sigmoid(t_o), p_1, p_2, ..., p_classes]` for `bbox_num`, totalling `num * (coords + 1 + classes)` channels\n```\n\n----------------------------------------\n\nTITLE: Generating Comparison Script with Polygraphy CLI\nDESCRIPTION: This command uses Polygraphy's 'run' command to generate a Python script for comparing TensorRT and ONNX Runtime implementations of an ONNX model. It specifies the input ONNX file, enables TensorRT and ONNX Runtime backends, and sets the output script name.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/run/03_generating_a_comparison_script/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run identity.onnx --trt --onnxrt \\\n    --gen-script=compare_trt_onnxrt.py\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine with trtexec\nDESCRIPTION: Uses NVIDIA's trtexec tool to build a TensorRT engine from the converted ONNX graph, enabling CUDA graph optimization.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/detectron2/README.md#2025-04-06_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ntrtexec --onnx=/path/to/converted.onnx --saveEngine=/path/to/engine.trt --useCudaGraph\n```\n\n----------------------------------------\n\nTITLE: Executing TensorRT Prediction Engine for MNIST Inference\nDESCRIPTION: Runs the MNIST prediction engine using executeV2 with input and output bindings. The input comes from the preprocessor output and results are written to the output buffer.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleDynamicReshape/README.md#2025-04-06_snippet_14\n\nLANGUAGE: C++\nCODE:\n```\nstd::vector<void*> predicitonBindings = {mPredictionInput.data(), mOutput.deviceBuffer.data()};\nstatus = mPredictionContext->executeV2(predicitonBindings.data());\n```\n\n----------------------------------------\n\nTITLE: Exporting TensorFlow Saved Model\nDESCRIPTION: These commands create a directory for the TensorFlow saved model and export the checkpoint to the saved model format using the model_inspect.py script.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/EfficientDet/notebooks/EfficientDet-TensorRT8.ipynb#2025-04-06_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n![ ! -d \"tf_model\" ] && mkdir tf_model\n\n!python3 ./automl/efficientdet/model_inspect.py \\\n    --runmode saved_model \\\n    --model_name efficientdet-d0 \\\n    --ckpt_path ./tf_checkpoint/efficientdet-d0/ \\\n    --saved_model_dir ./tf_model\n```\n\n----------------------------------------\n\nTITLE: Inspecting TensorRT Network with Polygraphy\nDESCRIPTION: Command to inspect the TensorRT network generated by the create_network() function in example.py, showing layers, attributes, and weights.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/05_using_tensorrt_network_api/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy inspect model example.py --trt-network-func create_network --show layers attrs weights\n```\n\n----------------------------------------\n\nTITLE: Evaluating and Fine-tuning Quantized Model\nDESCRIPTION: This code evaluates the quantized model's accuracy, fine-tunes it for improved performance, and then re-evaluates.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/notebooks/simple_network_quantize_partial.ipynb#2025-04-06_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n_, q_model_accuracy = q_nn_model.evaluate(test_images, test_labels, verbose=0)\nq_model_accuracy = round(100 * q_model_accuracy, 2)\nprint(\n    \"Test accuracy immediately after quantization:{}, diff:{}\".format(\n        q_model_accuracy, (baseline_model_accuracy - q_model_accuracy)\n    )\n)\n\n# Fine-tune quantized model\nfine_tune_epochs = 2\nq_nn_model.fit(\n    train_images,\n    train_labels,\n    batch_size=32,\n    epochs=fine_tune_epochs,\n    validation_split=0.1,\n)\n_, q_model_accuracy = q_nn_model.evaluate(test_images, test_labels, verbose=0)\nq_model_accuracy = round(100 * q_model_accuracy, 2)\nprint(\n    \"Accuracy after fine tuning for {} epochs :{}\".format(\n        fine_tune_epochs, q_model_accuracy\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing TensorRT FP16 BERT Model\nDESCRIPTION: Initializes a TensorRT FP16 engine for BERT inference including runtime setup, engine deserialization, context creation, and memory allocation. The code configures input shapes and prepares buffers for inference with a maximum sequence length of 384.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/Q-and-A.ipynb#2025-04-06_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nengine_path = \"engines_{}/bert_large_384.engine\".format(TRT_VERSION)\nmax_seq_length = 384\nbatch_size = 1\n\nruntime = trt.Runtime(TRT_LOGGER)\nengine = runtime.deserialize_cuda_engine(open(engine_path, 'rb') .read()) \ncontext = engine.create_execution_context()\n\n # We always use batch size 1.\ninput_shape = (1, max_seq_length)\ninput_nbytes = trt.volume(input_shape) * trt.int32.itemsize\n\n# Allocate device memory for inputs.\nd_inputs = [cuda.mem_alloc(input_nbytes) for binding in range(3)]\n\n# Specify input shapes. These must be within the min/max bounds of the active profile (0th profile in this case)\n# Note that input shapes can be specified on a per-inference basis, but in this case, we only have a single shape.\nfor binding in range(3):\n    tensor_name = engine.get_tensor_name(binding)\n    context.set_input_shape(tensor_name, input_shape)\nassert context.all_binding_shapes_specified\n\n# Allocate output buffer by querying the size from the context. This may be different for different input shapes.\nh_output = cuda.pagelocked_empty(tuple(context.get_tensor_shape(engine.get_tensor_name(3))), dtype=np.float32)\nd_output = cuda.mem_alloc(h_output.nbytes)\n\n# Create a stream in which to copy inputs/outputs and run inference.\nstream = cuda.Stream()\n```\n\n----------------------------------------\n\nTITLE: Initializing TREx and Loading a TensorRT Engine Plan\nDESCRIPTION: Imports the TREx library and loads a TensorRT engine plan from JSON files including graph, profile, and metadata information.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/api-examples.ipynb#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport trex\nfrom trex.notebook import *\n\nengine_name = \"../tests/inputs/mobilenet.qat.onnx.engine\"\nplan = trex.EnginePlan(f\"{engine_name}.graph.json\", f\"{engine_name}.profile.json\", f\"{engine_name}.metadata.json\")\n```\n\n----------------------------------------\n\nTITLE: Auto-generated Polygraphy API Script for Model Comparison\nDESCRIPTION: An example of an auto-generated Python script that uses the Polygraphy API to compare an ONNX model between TensorRT and ONNX-Runtime. It includes imports, loaders configuration, runner execution, and accuracy comparison.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/README.md#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#!/usr/bin/env python3\n# Template auto-generated by polygraphy [v0.31.0] on 01/01/20 at 10:10:10\n# Generation Command: polygraphy run --gen - model.onnx --trt --onnxrt\n# This script compares model.onnx between TensorRT and ONNX-Runtime\n\nfrom polygraphy.logger import G_LOGGER\n\nfrom polygraphy.backend.onnxrt import OnnxrtRunner, SessionFromOnnx\nfrom polygraphy.backend.trt import EngineFromNetwork, NetworkFromOnnxPath, TrtRunner\nfrom polygraphy.comparator import Comparator\nimport sys\n\n# Loaders\nparse_network_from_onnx = NetworkFromOnnxPath('model.onnx')\nbuild_engine = EngineFromNetwork(parse_network_from_onnx)\nbuild_onnxrt_session = SessionFromOnnx('model.onnx')\n\n# Runners\nrunners = [\n    TrtRunner(build_engine),\n    OnnxrtRunner(build_onnxrt_session),\n]\n\n# Runner Execution\nresults = Comparator.run(runners)\n\nsuccess = True\n# Accuracy Comparison\nsuccess &= bool(Comparator.compare_accuracy(results))\n\n# Report Results\ncmd_run = ' '.join(sys.argv)\nif not success:\n    G_LOGGER.critical(f\"FAILED | Command: {cmd_run}\"))\nG_LOGGER.finish(f\"PASSED | Command: {cmd_run}\"))\n```\n\n----------------------------------------\n\nTITLE: Downloading Pre-trained TFHub EfficientDet Model\nDESCRIPTION: Wget command to download a pre-trained EfficientDet model from TensorFlow Hub.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/efficientdet/README.md#2025-04-06_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nwget https://storage.googleapis.com/tfhub-modules/tensorflow/efficientdet/d0/1.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Listing Unbounded DDS Tensors with Polygraphy\nDESCRIPTION: This command inspects the folded model to identify and list all tensors with unbounded Data-Dependent Shapes (DDS).\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/surgeon/04_setting_upper_bounds/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy inspect model folded.onnx --list-unbounded-dds\n```\n\n----------------------------------------\n\nTITLE: Running Polygraphy Validation on a Model with Infinity Values\nDESCRIPTION: This command runs Polygraphy's validation tool on an ONNX model that intentionally generates infinite values. The `--validate` flag checks for NaN and infinity values in the model's outputs, while `--onnx-outputs mark all` ensures all outputs are included in the validation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/run/07_checking_nan_inf/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run add_infinity.onnx --onnx-outputs mark all --onnxrt --validate\n```\n\n----------------------------------------\n\nTITLE: Running Inference with TensorRT Using trtexec Tool\nDESCRIPTION: Uses the trtexec command-line tool to benchmark DeBERTa model performance. Demonstrates setting workspace, batch size, and precision parameters for optimal performance.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/DeBERTa/README.md#2025-04-06_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ntrtexec --onnx=./test/deberta_plugin.onnx --workspace=4096 --explicitBatch --optShapes=input_ids:1x2048,attention_mask:1x2048 --iterations=10 --warmUp=10 --noDataTransfers --fp16\n```\n\n----------------------------------------\n\nTITLE: Loading Calibrated ResNet50 Model\nDESCRIPTION: Initializes quantization modules and loads a pre-calibrated ResNet50 model to GPU.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/examples/finetune_quant_resnet50.ipynb#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pytorch_quantization import quant_modules\nquant_modules.initialize()\n\n# Create and load the calibrated model\nmodel = torchvision.models.resnet50()\nmodel.load_state_dict(torch.load(\"/tmp/quant_resnet50-calibrated.pth\"))\nmodel.cuda()\n```\n\n----------------------------------------\n\nTITLE: BERT Data Preprocessing Implementation\nDESCRIPTION: Implements the preprocessing steps required for BERT input, including tokenization and feature extraction. Sets up parameters for maximum sequence length, query length, and document stride.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/inference.ipynb#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport helpers.data_processing as dp\nimport helpers.tokenization as tokenization\n\ntokenizer = tokenization.FullTokenizer(vocab_file=\"/workspace/TensorRT/demo/BERT/models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_128_v19.03.1/vocab.txt\", do_lower_case=True)\n\n# The maximum number of tokens for the question. Questions longer than this will be truncated to this length.\nmax_query_length = 64\n\n# When splitting up a long document into chunks, how much stride to take between chunks.\ndoc_stride = 128\n\n# The maximum total input sequence length after WordPiece tokenization. \n# Sequences longer than this will be truncated, and sequences shorter \nmax_seq_length = 128\n\n# Extract tokens from the paragraph\ndoc_tokens = dp.convert_doc_tokens(short_paragraph_text)\n\n# Extract features from the paragraph and question\nfeatures = dp.convert_example_to_features(doc_tokens, question_text, tokenizer, max_seq_length, doc_stride, max_query_length)\n```\n\n----------------------------------------\n\nTITLE: Using Custom Comparison Metrics and Tolerances in Polygraphy\nDESCRIPTION: Command to run a comparison with custom tolerance values and a median error statistic between TensorRT FP16 and ONNX-Runtime, useful for reduced precision comparisons.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/run/01_comparing_frameworks/README.md#2025-04-06_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run dynamic_identity.onnx --trt --fp16 --onnxrt \\\n    --input-shapes X:[1,2,4,4] \\\n    --atol 0.001 --rtol 0.001 --check-error-stat median\n```\n\n----------------------------------------\n\nTITLE: Running the Algorithm Selector Sample with Bash\nDESCRIPTION: Command to execute the sample_algorithm_selector sample with options for data directory location and precision mode (FP16).\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleAlgorithmSelector/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./sample_algorithm_selector --datadir $TRT_DATADIR/mnist --fp16\n```\n\n----------------------------------------\n\nTITLE: Defining PriorBox Layer Configuration in Caffe for SSD300\nDESCRIPTION: A typical Caffe configuration for a PriorBox layer in SSD300. It defines parameters like min_size, max_size, aspect ratios, and variance values needed to generate appropriate anchor boxes for a specific feature map.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/priorBoxPlugin/README.md#2025-04-06_snippet_0\n\nLANGUAGE: caffe\nCODE:\n```\nlayer {\n\tname: \"conv6_2_mbox_priorbox\"\n\ttype: \"PriorBox\"\n\tbottom: \"conv6_2\"\n\tbottom: \"data\"\n\ttop: \"conv6_2_mbox_priorbox\"\n\tprior_box_param {\n\t\tmin_size: 111.0\n\t\tmax_size: 162.0\n\t\taspect_ratio: 2\n\t\taspect_ratio: 3\n\t\tflip: true\n\t\tclip: false\n\t\tvariance: 0.1\n\t\tvariance: 0.1\n\t\tvariance: 0.2\n\t\tvariance: 0.2\n\t\tstep: 32\n\t\toffset: 0.5\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Running ONNX MNIST Sample with Command Line Options\nDESCRIPTION: Command line syntax for running the ONNX MNIST sample with various options including help, data directory path, DLA core selection, and precision modes.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleOnnxMNIST/README.md#2025-04-06_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\n./sample_onnx_mnist [-h or --help] [-d or --datadir=<path to data directory>] [--useDLACore=<int>] [--int8 or --fp16]\n```\n\n----------------------------------------\n\nTITLE: Generating Golden Outputs with ONNX-Runtime\nDESCRIPTION: Command to generate reference outputs from ONNX-Runtime for comparison, saving the results to a golden.json file.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/debug/01_debugging_flaky_trt_tactics/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run identity.onnx --onnxrt \\\n    --save-outputs golden.json\n```\n\n----------------------------------------\n\nTITLE: Defining DetectionLayer Input Tensor Shapes in CUDA\nDESCRIPTION: Specifies the shapes of input tensors for the DetectionLayer plugin. It defines the dimensions for delta_bbox, score, and roi tensors used in bounding box refinement and class prediction.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/detectionLayerPlugin/README.md#2025-04-06_snippet_0\n\nLANGUAGE: CUDA\nCODE:\n```\ndelta_bbox tensor's shape is [N, rois, num_classes*4, 1, 1]\nscore tensor's shape is [N, rois, num_classes, 1, 1]\nroi tensor's shape is [N, rois, 4]\n```\n\n----------------------------------------\n\nTITLE: Running the ONNX Functions Example\nDESCRIPTION: Command to generate a model with custom SelfAttention operators implemented using ONNX Functions.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/examples/11_creating_a_local_function/README.md#2025-04-06_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython3 generate.py\n```\n\n----------------------------------------\n\nTITLE: Executing TensorRT Preprocessor Engine with Dynamic Shapes\nDESCRIPTION: Runs the preprocessor engine using executeV2 with input and output bindings. The output of the preprocessor is written directly to the input buffer of the prediction engine.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleDynamicReshape/README.md#2025-04-06_snippet_13\n\nLANGUAGE: C++\nCODE:\n```\nstd::vector<void*> preprocessorBindings = {mInput.deviceBuffer.data(), mPredictionInput.data()};\nbool status = mPreprocessorContext->executeV2(preprocessorBindings.data());\n```\n\n----------------------------------------\n\nTITLE: Isolating Subgraph from ONNX Model with Python\nDESCRIPTION: This command executes a Python script to isolate a subgraph that computes (a * x1 + b) from the previously generated model and saves it as 'subgraph.onnx'. The resulting model computes add_out = (a * x1 + b).\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/examples/03_isolating_a_subgraph/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 isolate.py\n```\n\n----------------------------------------\n\nTITLE: Running Circular Padding Plugin Sample with Precision Selection\nDESCRIPTION: Command to run the circular padding plugin sample implemented with CUDA Python. It demonstrates how to specify the precision (FP32 or FP16) as a command-line argument.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/python_plugin/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 circ_pad_plugin_cuda_python.py --precision fp32 # fp32 or fp16\n```\n\n----------------------------------------\n\nTITLE: Quantizing and Evaluating the Model\nDESCRIPTION: This code quantizes the original model, compiles it, and evaluates its accuracy immediately after quantization.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/notebooks/simple_network_quantize_full.ipynb#2025-04-06_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Quantize model\nq_nn_model = quantize_model(model=nn_model_original)\nq_nn_model.compile(\n    optimizer=tiny_resnet.optimizer(lr=1e-4),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[\"accuracy\"],\n)\n\n_, q_model_accuracy = q_nn_model.evaluate(test_images, test_labels, verbose=0)\nq_model_accuracy = round(100 * q_model_accuracy, 2)\n\nprint(\n    \"Test accuracy immediately after quantization: {}, diff: {}\".format(\n        q_model_accuracy, (baseline_model_accuracy - q_model_accuracy)\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Reference Inputs and Outputs with ONNX-Runtime in Polygraphy\nDESCRIPTION: This command runs ONNX-Runtime on the model to generate reference inputs and golden outputs, saving them to JSON files for later use in comparison.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/run/08_adding_precision_constraints/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run needs_constraints.onnx --onnxrt --val-range x:[1,2] \\\n    --save-inputs inputs.json --save-outputs golden_outputs.json\n```\n\n----------------------------------------\n\nTITLE: Deploying Baseline ResNet TensorRT Engine\nDESCRIPTION: Shell script for deploying a baseline ResNet model as a TensorRT engine. Requires setting the ROOT_DIR variable to point to the ONNX file location.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/examples/resnet/scripts/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./scripts/deploy_engine_baseline.sh\n```\n\n----------------------------------------\n\nTITLE: Sample Help Options (Bash)\nDESCRIPTION: Displays the full list of available command-line options for the TensorRT ONNX MNIST CoordConv sample, including usage syntax and descriptions for each option.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleOnnxMnistCoordConvAC/README.md#2025-04-06_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nUsage: ./sample_onnx_mnist_coord_conv_ac [-h or --help] [-d or --datadir=<path to data directory>] [--useDLACore=<int>]\n--help Display help information\n--datadir Specify path to a data directory, overriding the default. This option can be used multiple times to add multiple directories. If no data directories are given, the default is to use (data/samples/mnist/, data/mnist/)\n--useDLACore=N Specify a DLA engine for layers that support DLA. Value can range from 0 to n-1, where n is the number of DLA engines on the platform.\n--int8 Run in Int8 mode.\n--fp16 Run in FP16 mode.\n```\n\n----------------------------------------\n\nTITLE: Input Tensor A Structure Example\nDESCRIPTION: Example showing the structure of first input tensor A with shape [2, 2, 2, 2]\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/batchTilePlugin/README.md#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n[[[[ 0 1]\n   [ 2 3]]\n\n  [[ 4 5]\n   [ 6 7]]]\n  \n\n [[[ 8 9]\n   [10 11]]\n\n  [[12 13]\n   [14 15]]]]\n```\n\n----------------------------------------\n\nTITLE: Exporting TensorRT Data Directory Path\nDESCRIPTION: Sets the environment variable TRT_DATADIR to point to the TensorRT sample data directory.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleCharRNN/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport TRT_DATADIR=/usr/src/tensorrt/data\n```\n\n----------------------------------------\n\nTITLE: Constraining Layer Precisions with --layer-precisions Option in Polygraphy\nDESCRIPTION: This command compares running the model with TensorRT using FP16 optimizations against ONNX-Runtime in FP32. It constrains the Add layer to float16 and the Sub layer to float32 precision.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/run/08_adding_precision_constraints/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run needs_constraints.onnx \\\n    --trt --fp16 --onnxrt --val-range x:[1,2] \\\n    --layer-precisions Add:float16 Sub:float32 --precision-constraints prefer \\\n    --check-error-stat median\n```\n\n----------------------------------------\n\nTITLE: Building Plugin on Windows\nDESCRIPTION: PowerShell commands to build the custom plugin and Python bindings on Windows using Visual Studio\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/onnx_custom_plugin/README.md#2025-04-06_snippet_3\n\nLANGUAGE: powershell\nCODE:\n```\nmkdir build; pushd build\ncmake .. -G \"Visual Studio 15 Win64\" /\n   -DTRT_LIB=C:\\path\\to\\tensorrt\\lib /\n   -DTRT_INCLUDE=C:\\path\\to\\tensorrt\\lib /\n   -DCUDA_INC_DIR=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v<CUDA_VERSION>\\include\" /\n   -DCUDA_LIB_DIR=\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v<CUDA_VERSION>\\lib\\x64\"\nmsbuild ALL_BUILD.vcxproj\npopd\n```\n\n----------------------------------------\n\nTITLE: Adding Fully Connected Plugin Source Files in TensorRT CMake\nDESCRIPTION: This CMake command adds the source files for a fully connected (FC) plugin to the TensorRT project. It specifies both the cpp and header files for the plugin.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/fcPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_plugin_source(\n    fcPlugin.cpp\n    fcPlugin.h\n)\n```\n\n----------------------------------------\n\nTITLE: Specifying Input Shapes for Dynamic Models with Polygraphy\nDESCRIPTION: Command to explicitly provide input shapes for a dynamic model when comparing TensorRT and ONNX-Runtime outputs, which prevents Polygraphy from using default shapes and suppresses warnings.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/run/01_comparing_frameworks/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run dynamic_identity.onnx --trt --onnxrt \\\n    --input-shapes X:[1,2,4,4]\n```\n\n----------------------------------------\n\nTITLE: Implementing INT8 BERT TensorRT Benchmarking Function in Python\nDESCRIPTION: Defines the run_benchmark_INT8 function that loads an INT8 QAT sparse BERT TensorRT engine, allocates GPU buffers, prepares synthetic input data, and executes benchmarking runs. It measures execution time statistics including average, 95th percentile, and 99th percentile latencies.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/benchmark.ipynb#2025-04-06_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n### INT8 TRT model\ndef run_benchmark_INT8(b):\n    engine_path = \"engines_%s/megatron_large_seqlen384_int8qat_sparse.engine\"%TRT_VERSION\n    with open(engine_path, 'rb') as f, trt.Runtime(TRT_LOGGER) as runtime, runtime.deserialize_cuda_engine(f.read()) as engine, engine.create_execution_context() as context:\n        with output:\n            #output.clear_output()\n            args.batch_size = [int(batchsize_selector.value)]\n\n            # Allocate buffers large enough to store the largest batch size\n            max_input_shape = (args.sequence_length * max(args.batch_size), )\n            max_output_shape = (args.sequence_length * max(args.batch_size), 2, 1, 1)\n            buffers = [\n                DeviceBuffer(max_input_shape),\n                DeviceBuffer(max_input_shape),\n                DeviceBuffer((max(args.batch_size) + 1, )),\n                DeviceBuffer((args.sequence_length, )),\n                DeviceBuffer(max_output_shape)\n            ]\n\n            # Prepare random input\n            pseudo_vocab_size = 30522\n            pseudo_type_vocab_size = 2\n            np.random.seed(args.random_seed)\n            test_word_ids = np.random.randint(0, pseudo_vocab_size, (args.sequence_length * max(args.batch_size)), dtype=np.int32)\n            test_segment_ids = np.random.randint(0, pseudo_type_vocab_size, (args.sequence_length * max(args.batch_size)), dtype=np.int32)\n            test_cu_seq_lens = np.arange(0, args.sequence_length * max(args.batch_size) + 1, args.sequence_length, dtype=np.int32)\n\n            # Copy input h2d\n            cuda.memcpy_htod(buffers[0].buf, test_word_ids.ravel())\n            cuda.memcpy_htod(buffers[1].buf, test_segment_ids.ravel())\n            cuda.memcpy_htod(buffers[2].buf, test_cu_seq_lens.ravel())\n\n            bench_times = {}\n            stream = cuda.Stream()\n\n            tensor_name = engine.get_tensor_name(engine.num_io_tensors-1)\n            for idx, batch_size in enumerate(sorted(args.batch_size)):\n                for idx in range(engine.num_optimization_profiles):\n                    profile_shape = engine.get_tensor_profile_shape(name = tensor_name, profile_index = idx)\n                    if profile_shape[0][0] <= batch_size and profile_shape[2][0] >= batch_size:\n                        context.set_optimization_profile_async(idx, stream.handle)\n                        binding_idx_offset = idx * engine.num_io_tensors\n                        break\n\n                # Each profile has unique bindings\n                bindings = [0] * binding_idx_offset + [buf.binding() for buf in buffers]\n                input_shape = (batch_size, args.sequence_length)\n                for binding in range(3):\n                    tensor_name = engine.get_tensor_name(binding)\n                    context.set_input_shape(tensor_name, input_shape)\n                assert context.all_binding_shapes_specified\n\n                for i in range(engine.num_io_tensors):\n                    context.set_tensor_address(engine.get_tensor_name(i), bindings[i + binding_idx_offset])\n\n                # Inference\n                total_time = 0\n                start = cuda.Event()\n                end = cuda.Event()\n\n                # Warmup\n                for _ in range(args.warm_up_runs):\n                    context.execute_async_v3(stream_handle=stream.handle)\n                    stream.synchronize()\n\n                # Timing loop\n                times = []\n                progress_bar.value = 0\n                for _ in range(iteration_selector.value):\n                    start.record(stream)\n                    context.execute_async_v3(stream_handle=stream.handle)\n                    end.record(stream)\n                    stream.synchronize()\n                    times.append(end.time_since(start))\n                    progress_bar.value +=1\n\n                # Compute average time, 95th percentile time and 99th percentile time.\n                bench_times[batch_size] = times\n\n            [b.free() for b in buffers]\n\n            for batch_size, times in bench_times.items():\n                total_time = sum(times)\n                avg_time = total_time / float(len(times))\n                times.sort()\n                percentile95 = times[int(len(times) * 0.95)]\n                percentile99 = times[int(len(times) * 0.99)]\n                print(\"BERT TRT INT8: Running {:} iterations with Batch Size: {:}\\n\\tTotal Time: {:.2f} ms \\tAverage Time: {:.2f} ms\\t95th Percentile Time: {:.2f} ms\\t99th Percentile Time: {:.2f}\".format(args.iterations, batch_size, total_time, avg_time, percentile95, percentile99))\n```\n\n----------------------------------------\n\nTITLE: Comparing Outputs with Custom Tolerance\nDESCRIPTION: Demonstrates how to compare model outputs with specific absolute and relative tolerance values, using median as the error statistic.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/run/02_comparing_across_runs/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run identity.onnx --onnxrt \\\n    --load-inputs inputs.json --load-outputs run_0_outputs.json \\\n    --atol 0.001 --rtol 0.001 --check-error-stat median\n```\n\n----------------------------------------\n\nTITLE: Running Inference and Evaluating F1 Score for BERT QAT\nDESCRIPTION: These commands run inference using the SQuAD dataset and evaluate the F1 score and exact match score for the BERT QAT model.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\npython3 inference.py -e engines/bert_large_384_int8mix.engine -s 384 -sq ./squad/dev-v1.1.json -v models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/vocab.txt -o ./predictions.json\npython3 squad/evaluate-v1.1.py  squad/dev-v1.1.json  ./predictions.json 90\n```\n\n----------------------------------------\n\nTITLE: Saving TensorFlow FP32 Model and Converting to ONNX\nDESCRIPTION: This code saves the original FP32 model in TensorFlow format and converts it to ONNX format.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/notebooks/simple_network_quantize_specific_class.ipynb#2025-04-06_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Save TF FP32 original model\ntf.keras.models.save_model(nn_model_original, assets.simple_network_quantize_specific_class.fp32_saved_model)\n\n# Convert FP32 model to ONNX\nutils.convert_saved_model_to_onnx(saved_model_dir = assets.simple_network_quantize_specific_class.fp32_saved_model, onnx_model_path = assets.simple_network_quantize_specific_class.fp32_onnx_model)\n```\n\n----------------------------------------\n\nTITLE: Utility Functions for BERT Q&A Output Processing\nDESCRIPTION: These utility functions handle the printing of single query results and the extraction of features from questions for BERT Q&A processing.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/Q-and-A.ipynb#2025-04-06_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndef print_single_query(eval_time_elapsed, prediction, nbest_json):\n    print(\"Answer: '{}'\".format(prediction))\n    print(\"With probability: {:.2f}%\".format(nbest_json[0]['probability'] * 100.0))\n    \ndef question_features(tokens, question):\n    # Extract features from the paragraph and question\n    return dp.convert_example_to_features(tokens, question, tokenizer, max_seq_length, doc_stride, max_query_length)\n\ndoc_stride = 128\nmax_query_length = 64\n\nvocab_file = \"models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/vocab.txt\"\ntokenizer = tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=True)\n```\n\n----------------------------------------\n\nTITLE: Running Inference Sample\nDESCRIPTION: Command to run inference using TensorRT with the custom Hardmax plugin\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/onnx_custom_plugin/README.md#2025-04-06_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython3 sample.py\n```\n\n----------------------------------------\n\nTITLE: Running TensorRT Sample for Named Dimensions\nDESCRIPTION: This command runs the compiled TensorRT sample that builds and executes an engine from the ONNX model with named dimensions. It demonstrates how TensorRT handles the named dimension constraints.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleNamedDimensions/README.md#2025-04-06_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\n./sample_named_dimensions [-h or --help] [-d or --datadir=<path to data directory>]\n```\n\n----------------------------------------\n\nTITLE: Summarizing TensorRT Tactics\nDESCRIPTION: Command to generate a tabulated summary of TensorRT tactics with profiling data, sorted by latency.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/bin/README.md#2025-04-06_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ntrex summary ./examples/pytorch/resnet/A100/fp32/resnet.onnx.engine.graph.json --profiling_json=./examples/pytorch/resnet/A100/fp32/resnet.onnx.engine.profile.json --sort_key=latency --group_tactics\n```\n\n----------------------------------------\n\nTITLE: Importing Training Functions\nDESCRIPTION: Imports training and evaluation functions from torchvision's classification reference implementation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/examples/finetune_quant_resnet50.ipynb#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsys.path.append(\"/raid/skyw/models/torchvision/references/classification/\")\nfrom train import evaluate, train_one_epoch, load_data\n```\n\n----------------------------------------\n\nTITLE: Installing NVIDIA TensorFlow Quantization Toolkit\nDESCRIPTION: Commands to clone and install NVIDIA's TensorFlow Quantization Toolkit from GitHub repository.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/examples/tensorflow/resnet/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/NVIDIA/TensorRT.git\ncd TensorRT/tools/tensorflow-quantization\n./install.sh\n```\n\n----------------------------------------\n\nTITLE: Displaying Test Image\nDESCRIPTION: Visualizes the first image in the input batch using matplotlib to verify the image was loaded correctly.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/2. Using PyTorch through ONNX.ipynb#2025-04-06_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nplt.imshow(input_batch[0].astype(np.float32))\n```\n\n----------------------------------------\n\nTITLE: Matching Plugin Subgraphs in ONNX Models with Polygraphy\nDESCRIPTION: Command for finding and saving matches of a plugin (toyPlugin) in an ONNX model. The command identifies subgraphs that match the plugin's pattern and generates a config.yaml file listing potential substitutions.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/plugin/01_match_and_replace_plugin/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy plugin match toy_subgraph.onnx \\\n    --plugin-dir ./plugins -o config.yaml\n```\n\n----------------------------------------\n\nTITLE: Rebuilding Engine Using Calibration Cache\nDESCRIPTION: Command to rebuild a TensorRT engine using a pre-existing calibration cache, which allows skipping the calibration process since the cache is already populated.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/convert/01_int8_calibration_in_tensorrt/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy convert identity.onnx --int8 \\\n    --calibration-cache identity_calib.cache \\\n    -o identity.engine\n```\n\n----------------------------------------\n\nTITLE: Running TensorRT Inference\nDESCRIPTION: Command to perform inference using TensorRT's trtexec tool on the converted ONNX model\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/onnx_packnet/README.md#2025-04-06_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ntrtexec --onnx=model.onnx\n```\n\n----------------------------------------\n\nTITLE: Initializing Quantized Modules in Python\nDESCRIPTION: This snippet initializes the quantized modules using PyTorch Quantization.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/examples/calibrate_quant_resnet50.ipynb#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom pytorch_quantization import quant_modules\nquant_modules.initialize()\n```\n\n----------------------------------------\n\nTITLE: Running TensorRT ONNX MNIST CoordConv Sample (Bash)\nDESCRIPTION: Command to run the TensorRT sample for MNIST using an ONNX model with CoordConv. Supports optional arguments for data directory, DLA core usage, and precision mode (INT8 or FP16).\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleOnnxMnistCoordConvAC/README.md#2025-04-06_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n./sample_onnx_mnist_coord_conv_ac [-h or --help] [-d or --datadir=<path to data directory>] [--useDLACore=<int>] [--int8 or --fp16]\n```\n\n----------------------------------------\n\nTITLE: Running BERT QA Inference with Sample Passage and Question\nDESCRIPTION: Executes inference on a sample passage about TensorRT with a question \"What is TensorRT?\" using the optimized TensorRT engine to demonstrate question answering capabilities.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/BERT-TRT-FP16.ipynb#2025-04-06_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nPASSAGE = 'TensorRT is a high performance deep learning inference platform that delivers low latency and high throughput for apps'\\\n'such as recommenders, speech and image/video on NVIDIA GPUs. It includes parsers to import models, and plugins to support novel ops'\\\n'and layers before applying optimizations for inference. Today NVIDIA is open-sourcing parsers and plugins in TensorRT so that the deep'\\\n'learning community can customize and extend these components to take advantage of powerful TensorRT optimizations for your apps.'\nQUESTION=\"What is TensorRT?\"\n\n!python3 ../inference.py -e engines_$TRT_VERSION/bert_large_384.engine -s 384 -p $PASSAGE -q $QUESTION -v models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/vocab.txt\n```\n\n----------------------------------------\n\nTITLE: Running Polygraphy Comparison Example Script\nDESCRIPTION: This command executes the Python script 'example.py' which contains the Polygraphy comparison logic for ONNX-Runtime and TensorRT.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/01_comparing_frameworks/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 example.py\n```\n\n----------------------------------------\n\nTITLE: Generating ONNX Model with Python\nDESCRIPTION: This command runs a Python script to generate an ONNX model with several nodes and save it as 'model.onnx'. The resulting model computes Y = x0 + (a * x1 + b).\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/examples/03_isolating_a_subgraph/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 generate.py\n```\n\n----------------------------------------\n\nTITLE: Inspecting ONNX Model with Polygraphy\nDESCRIPTION: This command uses Polygraphy's inspect tool to examine the ONNX model before conversion.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/00_inference_with_tensorrt/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy inspect model identity.onnx\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies from Requirements File\nDESCRIPTION: Command to install all dependencies listed in the sample's requirements.txt file.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/tensorflow_object_detection_api/README.md#2025-04-06_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Adding GroupNormalization Plugin Sources to TensorRT Build\nDESCRIPTION: Registers source files for the GroupNormalization plugin with the TensorRT build system. Includes a CUDA kernel file for GPU acceleration, a C++ implementation file, and a header file defining the plugin interface.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/groupNormalizationPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_plugin_source(\n    groupNormalizationKernel.cu\n    groupNormalizationPlugin.cpp\n    groupNormalizationPlugin.h\n)\n```\n\n----------------------------------------\n\nTITLE: Freezing Input Shapes and Folding Constants in ONNX Model\nDESCRIPTION: Command to sanitize an ONNX model by freezing input shapes and folding constant operations. Uses Polygraphy surgeon with specific input shape overrides.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/debug/02_reducing_failing_onnx_models/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy surgeon sanitize model.onnx -o folded.onnx --fold-constants \\\n    --override-input-shapes x0:[1,3,224,224] x1:[1,3,224,224]\n```\n\n----------------------------------------\n\nTITLE: Layer Details Comparison\nDESCRIPTION: Compares detailed layer information between two engine plans\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/compare_engines.ipynb#2025-04-06_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncompare_engines_layer_details(plans[0], plans[1])\n```\n\n----------------------------------------\n\nTITLE: Running INT8 Inference with User-Provided Dynamic Ranges in TensorRT\nDESCRIPTION: Command to run INT8 inference using user-provided dynamic ranges. It specifies paths for the model, dynamic range file, input image, and reference labels.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleINT8API/README.md#2025-04-06_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n./sample_int8_api [--model=model_file] [--ranges=per_tensor_dynamic_range_file] [--image=image_file] [--reference=reference_file] [--data=/path/to/data/dir] [--useDLACore=<int>] [-v or --verbose]\n```\n\n----------------------------------------\n\nTITLE: Evaluating Baseline TensorFlow Model Accuracy\nDESCRIPTION: This snippet evaluates the accuracy of the baseline FP32 model on the test dataset.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/notebooks/simple_network_quantize_specific_class.ipynb#2025-04-06_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Get baseline model accuracy\n_, baseline_model_accuracy = nn_model_original.evaluate(\n    test_images, test_labels, verbose=0\n)\nbaseline_model_accuracy = round(100 * baseline_model_accuracy, 2)\nprint(\"Baseline FP32 model test accuracy:\", baseline_model_accuracy)\n```\n\n----------------------------------------\n\nTITLE: Quantizing Specific Input Indices for Multi-Input Layers\nDESCRIPTION: Shows how to selectively quantize specific inputs of multi-input layers using input indices. This example targets index 1 of the first 'add' layer, index 0 of the 'add_1' layer, and the third Conv2D layer for quantization.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/qspec.rst#2025-04-06_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# 1. Create QuantizationSpec object and add layer information\nq_spec = QuantizationSpec()\n\nlayer_name = ['add', 'add_1', 'conv2d_2']\nlayer_q_indices = [[1], [0], None]\n\n\"\"\"\n# Alternatively, each layer configuration can be added one at a time:\nq_spec.add(name='add', quantization_index=[1])\nq_spec.add(name='add', quantization_index=[0])\nq_spec.add(name='conv2d_2')\n\"\"\"\n\nq_spec.add(name=layer_name, quantization_index=layer_q_indices)\n\n# 2. Quantize model\nq_model = quantize_model(model, quantization_mode='partial', quantization_spec=q_spec)\nq_model.summary()\n\ntf.keras.backend.clear_session()\n```\n\n----------------------------------------\n\nTITLE: Installing Polygraphy from NGC PyPI Repository\nDESCRIPTION: Command to install Polygraphy using pip with NVIDIA GPU Cloud (NGC) as an additional package index. This also installs the 'colored' package for improved terminal output.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install colored polygraphy --extra-index-url https://pypi.ngc.nvidia.com\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorRT Parser Options in CMake\nDESCRIPTION: Specifies the parser type to be used with TensorRT samples, setting ONNX as the supported parser format.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/trtexec/CMakeLists.txt#2025-04-06_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(SAMPLE_PARSERS \"onnx\")\n```\n\n----------------------------------------\n\nTITLE: Replacing Min-Max Subgraph with Clip Operation\nDESCRIPTION: Command to replace the Min-Max subgraph with a Clip operation using the replace.py script.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/examples/08_replacing_a_subgraph/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 replace.py\n```\n\n----------------------------------------\n\nTITLE: Setting Sample Target Configuration for TensorRT Samples with CMake\nDESCRIPTION: This CMake script configures the build environment for TensorRT samples. It handles dependency checks, sets up include paths, defines compilation options, and configures target properties. The script supports different parsers (like ONNX) and plugin dependencies based on the specific sample requirements.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/CMakeSamplesTemplate.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\n# ensure SAMPLE_SOURCES is set\nif (NOT SAMPLE_SOURCES)\n   message(FATAL_ERROR \"You must define non empty SAMPLE_SOURCES variable before including this template\")\nendif()\n\nset_ifndef(PLUGINS_NEEDED OFF)\nset_ifndef(SAMPLE_PARSERS \"none\")\nset_ifndef(CUDA_LIBS_REQUIRED False)\n\nset(TARGET_DIR ${CMAKE_CURRENT_SOURCE_DIR})\n\nget_filename_component(SAMPLES_DIR ../ ABSOLUTE)\nget_filename_component(SAMPLE_DIR_NAME ${TARGET_DIR} NAME)\n\nset_ifndef(CUDA_INSTALL_DIR /usr/local/cuda)\n\n# SAMPLES_COMMON_SOURCES\nset(SAMPLES_COMMON_SOURCES\n    ${SAMPLES_DIR}/common/logger.cpp\n    ${SAMPLES_DIR}/utils/timingCache.cpp\n    ${SAMPLES_DIR}/utils/fileLock.cpp\n)\n\nif (MSVC)\n    list(APPEND SAMPLES_COMMON_SOURCES ${SAMPLES_DIR}/common/getopt.c)\nendif()\n\n# add underscores (snake) to camelCase cases\nstring(REGEX REPLACE \"(.)([A-Z][a-z]+)\" \"\\\\1_\\\\2\" SAMPLE_NAME_SNAKE_MIXED ${SAMPLE_DIR_NAME})\nstring(REGEX REPLACE \"([a-z0-9])([A-Z])\" \"\\\\1_\\\\2\" SAMPLE_NAME_SNAKE_MIXED ${SAMPLE_NAME_SNAKE_MIXED})\nstring(TOLOWER ${SAMPLE_NAME_SNAKE_MIXED} SAMPLE_NAME_SNAKE)\n\n# fix a few sample names\nstring(REGEX REPLACE \"google_net\" \"googlenet\" SAMPLE_NAME_FIXED ${SAMPLE_NAME_SNAKE})\nstring(REGEX REPLACE \"([a-zA-Z0-0])api\" \"\\\\1_api\" SAMPLE_NAME_FIXED ${SAMPLE_NAME_FIXED})\nstring(REGEX REPLACE \"_rcnn\" \"RCNN\" SAMPLE_NAME_FIXED ${SAMPLE_NAME_FIXED})\n\nset(SAMPLE_NAME ${SAMPLE_NAME_FIXED})# CACHE STRING \"binary name of the sample\")\n\nset(TARGET_NAME ${SAMPLE_NAME})\n\nadd_executable(${TARGET_NAME}\n    ${SAMPLE_SOURCES}\n    ${SAMPLES_COMMON_SOURCES}\n)\nset(DEPS_LIST \"\")\n\nif(BUILD_PLUGINS)\n    list(APPEND DEPS_LIST ${nvinfer_plugin_lib_name})\nendif()\n\nif(BUILD_PARSERS)\n    list(APPEND DEPS_LIST ${nvonnxparser_lib_name})\nendif()\n\nif(BUILD_PLUGINS OR BUILD_PARSERS)\n    add_dependencies(${TARGET_NAME}\n        ${DEPS_LIST}\n    )\nendif()\n\nset(ONNX_INCLUDE_DIR ${PROJECT_SOURCE_DIR}/parsers/onnx CACHE STRING \"ONNX include directory\")\nmessage(ONNX_INCLUDE_DIR)\ntarget_include_directories(${TARGET_NAME}\n    PUBLIC ${PROJECT_SOURCE_DIR}/include\n    PUBLIC ${ONNX_INCLUDE_DIR}\n    PUBLIC ${CUDA_INSTALL_DIR}/include\n    PRIVATE ${SAMPLES_DIR}\n    PRIVATE ${SAMPLES_DIR}/common\n    PRIVATE ${SAMPLES_DIR}/common/windows\n    PRIVATE ${TARGET_DIR}\n)\n\ntarget_compile_options(${TARGET_NAME} PUBLIC\n             \"$<$<COMPILE_LANGUAGE:CUDA>:SHELL:-Xcompiler -fno-rtti>\"\n             \"$<$<COMPILE_LANGUAGE:CXX>:-fno-rtti>\")\n\nset(SAMPLE_DEP_LIBS\n    ${CUDART_LIB}\n    ${${nvinfer_lib_name}_LIB_PATH}\n    ${RT_LIB}\n    ${CMAKE_DL_LIBS}\n    ${CMAKE_THREAD_LIBS_INIT}\n)\n\nif (NOT MSVC)\n    list(APPEND SAMPLE_DEP_LIBS ${RT_LIB})\nendif()\n\nif(${PLUGINS_NEEDED})\n    list(APPEND SAMPLE_DEP_LIBS ${nvinfer_plugin_lib_name})\nendif()\n\nif(\"onnx\" IN_LIST SAMPLE_PARSERS)\n    list(APPEND SAMPLE_DEP_LIBS ${nvonnxparser_lib_name})\nendif()\n\n# Necessary to link nvinfer_plugin library. Add unresolved symbols flag for non-Windows platforms.\ntarget_link_libraries(${TARGET_NAME}\n    ${SAMPLE_DEP_LIBS}\n    $<$<NOT:$<CXX_COMPILER_ID:MSVC>>:-Wl,--unresolved-symbols=ignore-in-shared-libs>\n)\n\nset_target_properties(${TARGET_NAME} PROPERTIES LINK_FLAGS \"-Wl,--exclude-libs,ALL\")\n\nset_target_properties(${TARGET_NAME} PROPERTIES DEBUG_POSTFIX ${TRT_DEBUG_POSTFIX})\n\nset_target_properties(${TARGET_NAME}\n    PROPERTIES\n    ARCHIVE_OUTPUT_DIRECTORY \"${TRT_OUT_DIR}\"\n    LIBRARY_OUTPUT_DIRECTORY \"${TRT_OUT_DIR}\"\n    RUNTIME_OUTPUT_DIRECTORY \"${TRT_OUT_DIR}\"\n)\n\n# Add CUDA dependencies for samples that require them.\nif(${CUDA_LIBS_REQUIRED})\n    target_link_directories(${TARGET_NAME} PUBLIC ${CUDA_ROOT}/lib)\nendif()\n\nadd_dependencies(samples ${TARGET_NAME})\n\n################################### INSTALLATION ########################################\n\ninstall(TARGETS ${TARGET_NAME}\n        RUNTIME DESTINATION bin\n        LIBRARY DESTINATION lib\n        ARCHIVE DESTINATION lib\n)\n\n##################################### SUMMARY ###########################################\n\nget_filename_component(LICENSE_STATUS ../ ABSOLUTE)\nget_filename_component(LICENSE_STATUS \"${LICENSE_STATUS}\" NAME)\n\nmessage(STATUS \"Adding new sample: ${TARGET_NAME}\")\nmessage(STATUS \"    - Parsers Used: ${SAMPLE_PARSERS}\")\nmessage(STATUS \"    - InferPlugin Used: ${PLUGINS_NEEDED}\")\nmessage(STATUS \"    - Licensing: ${LICENSE_STATUS}\")\n```\n\n----------------------------------------\n\nTITLE: Generating ONNX Model for Shape Operations in Python\nDESCRIPTION: This Python script generates an ONNX model that implements shape operations on inputs with dynamic shapes. It saves the model as 'model.onnx'.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/examples/09_shape_operations_with_the_layer_api/README.md#2025-04-06_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\npython3 generate.py\n```\n\n----------------------------------------\n\nTITLE: Installing ONNX GraphSurgeon\nDESCRIPTION: This command installs the ONNX GraphSurgeon package from the NVIDIA GitHub repository, which is used for ONNX graph manipulation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/EfficientDet/notebooks/EfficientDet-TensorRT8.ipynb#2025-04-06_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n!pip3 install 'git+https://github.com/NVIDIA/TensorRT#subdirectory=tools/onnx-graphsurgeon'\n```\n\n----------------------------------------\n\nTITLE: Evaluating Initial Quantized Model Accuracy\nDESCRIPTION: Compiles the quantized model and evaluates its accuracy immediately after Q/DQ node insertion, before any fine-tuning. This helps to measure the initial accuracy drop due to quantization.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/getting_started.ipynb#2025-04-06_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Compile quantized model\nquantized_model.compile(\n    optimizer=tf.keras.optimizers.Adam(0.0001),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[\"accuracy\"],\n)\n# Get accuracy immediately after QDQ nodes are inserted.\n_, q_aware_model_accuracy = quantized_model.evaluate(test_images, test_labels, verbose=0)\nprint(\"Quantization test accuracy immediately after QDQ insertion:\", q_aware_model_accuracy)\n```\n\n----------------------------------------\n\nTITLE: Including TensorRT Kernel Subdirectory in CMake Build\nDESCRIPTION: Adds the 'kernels' subdirectory to the build process, which contains CUDA kernel implementations used by the plugins.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/common/CMakeLists.txt#2025-04-06_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(kernels)\n```\n\n----------------------------------------\n\nTITLE: Running Stable Video Diffusion with TensorRT\nDESCRIPTION: Commands to generate a video using Stable Video Diffusion with TensorRT optimization, including options for FP8 precision and custom input images.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/Diffusion/README.md#2025-04-06_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython3 demo_img2vid.py --version svd-xt-1.1 --onnx-dir onnx-svd-xt-1-1 --engine-dir engine-svd-xt-1-1 --hf-token=$HF_TOKEN\n```\n\nLANGUAGE: bash\nCODE:\n```\npython3 demo_img2vid.py --version svd-xt-1.1 --onnx-dir onnx-svd-xt-1-1 --engine-dir engine-svd-xt-1-1 --hf-token=$HF_TOKEN --fp8\n```\n\nLANGUAGE: bash\nCODE:\n```\npython3 demo_img2vid.py --version svd-xt-1.1 --onnx-dir onnx-svd-xt-1-1 --engine-dir engine-svd-xt-1-1 --input-image https://www.hdcarwallpapers.com/walls/2018_chevrolet_camaro_zl1_nascar_race_car_2-HD.jpg --hf-token=$HF_TOKEN\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorRT Plugin Build Properties\nDESCRIPTION: Sets up compilation and linking options for the plugin libraries, including platform-specific settings and dependency configurations.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/CMakeLists.txt#2025-04-06_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset(trt_plugin_dependencies\n    tensorrt\n    CUDA::cudart_static\n)\n\nset(trt_plugin_include_dirs\n    ${TensorRT_SOURCE_DIR}/externals\n    ${CMAKE_CURRENT_LIST_DIR}\n)\n\nset(trt_plugin_compile_options\n    $<$<COMPILE_LANGUAGE:CUDA>:--expt-relaxed-constexpr>\n)\n```\n\n----------------------------------------\n\nTITLE: Example Command for Running Sample with Default Data\nDESCRIPTION: Example showing how to run the sample using the environment variable for data directory.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleCharRNN/README.md#2025-04-06_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./sample_char_rnn --datadir $TRT_DATADIR/char-rnn\n```\n\n----------------------------------------\n\nTITLE: Inspecting an ONNX Model Layers with Polygraphy\nDESCRIPTION: Command to verify the extracted subgraph by displaying its layers using Polygraphy's inspect tool.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/surgeon/01_isolating_subgraphs/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy inspect model subgraph.onnx --show layers\n```\n\n----------------------------------------\n\nTITLE: Displaying Engine Plan Summary Information\nDESCRIPTION: Code that prints a high-level summary of the engine plan, including name and performance metrics such as latency and throughput information.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/tutorial.ipynb#2025-04-06_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprint(f\"Summary for {plan.name}:\\n\")\nplan.summary()\n```\n\n----------------------------------------\n\nTITLE: Configuring INT8 Builder Settings in TensorRT\nDESCRIPTION: Configuration code to enable INT8 mode and set builder flags without using a calibrator. Optionally enables strict type constraints for debugging.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleINT8API/README.md#2025-04-06_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nbuilder->setFlag(BuilderFlag::kINT8);\nbuilder->setInt8Calibrator(nullptr);\nbuilder->setStrictTypeConstraints(true);\n```\n\n----------------------------------------\n\nTITLE: Saving Minimum Good Models in debug reduce\nDESCRIPTION: Command to make debug reduce save minimum passing models, which can be compared against the minimum failing model for additional insights.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/how-to/use_debug_reduce_effectively.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n--min-good <path>\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with pip\nDESCRIPTION: Commands to upgrade pip and install required dependencies from requirements.txt\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/onnx_packnet/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip3 install --upgrade pip\npip3 install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Inspecting a TensorRT Engine with Polygraphy\nDESCRIPTION: Optional command to inspect the details of a TensorRT engine file after it has been created, allowing users to verify the engine configuration including the optimization profiles.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/convert/03_dynamic_shapes_in_tensorrt/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy inspect model dynamic_identity.engine\n```\n\n----------------------------------------\n\nTITLE: Generating Input Data with Polygraphy Run Command\nDESCRIPTION: This command runs inference on an ONNX model using ONNX Runtime and saves the input data to a JSON file.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/inspect/06_inspecting_input_data/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run identity.onnx --onnxrt --save-inputs inputs.json\n```\n\n----------------------------------------\n\nTITLE: Cloning AutoML Repository\nDESCRIPTION: Git commands to clone the Google AutoML repository and checkout a specific commit for compatibility.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/efficientdet/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/google/automl\ncd automl\ngit checkout 0b0ba5e\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Python Bindings in Bash\nDESCRIPTION: This snippet shows how to build the TensorRT Python bindings for Python 3.10 on an x86_64 architecture. It uses the build.sh script with specific environment variables to generate the installable wheel.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/python/README.md#2025-04-06_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd $TRT_OSSPATH/python\nTENSORRT_MODULE=tensorrt PYTHON_MAJOR_VERSION=3 PYTHON_MINOR_VERSION=10 TARGET_ARCHITECTURE=x86_64 ./build.sh\n```\n\n----------------------------------------\n\nTITLE: Creating Converted ONNX Graph for TensorRT\nDESCRIPTION: Runs a custom script to create a TensorRT-compatible ONNX graph from the exported Detectron 2 ONNX model, including anchor generation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/detectron2/README.md#2025-04-06_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython create_onnx.py \\\n    --exported_onnx /path/to/model.onnx \\\n    --onnx /path/to/converted.onnx \\\n    --det2_config /detectron2/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml \\\n    --det2_weights /model_final_f10217.pkl \\\n    --sample_image any_image.jpg\n```\n\n----------------------------------------\n\nTITLE: Running Polygraphy with Custom TensorRT Network Script\nDESCRIPTION: Command to run a TensorRT network defined in a custom Python script using Polygraphy's run command.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/run/04_defining_a_tensorrt_network_or_config_manually/README.md#2025-04-06_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run --trt define_network.py --model-type=trt-network-script\n```\n\n----------------------------------------\n\nTITLE: Modifying ONNX Input for Dynamic Batch Size\nDESCRIPTION: Basic code to convert a static ONNX model's input to support dynamic batch size by replacing the first dimension with 'N' symbol. This approach works for simple models but may need additional modifications for models with static internal layers.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/examples/10_dynamic_batch_size/README.md#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ngraph = gs.import_onnx(onnx_model)\nfor input in graph.inputs:\n   input.shape[0] = 'N'\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Data Loader Script\nDESCRIPTION: Method to create a Python script with a load_data function that returns an iterable or generator yielding input data. This approach allows for memory-efficient loading of large datasets.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/how-to/use_custom_input_data.md#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef load_data():\n    # Return an iterable or generator that yields Dict[str, np.ndarray]\n```\n\n----------------------------------------\n\nTITLE: Converting to Dynamic Batch Size Model\nDESCRIPTION: Command to run the modification script that converts the static model to support dynamic batch sizes by updating both input shapes and internal Reshape nodes.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/examples/10_dynamic_batch_size/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 modify.py\n```\n\n----------------------------------------\n\nTITLE: Adding Function to Graph and Creating Custom Op Node\nDESCRIPTION: Shows how to add a custom Function to a graph's function list and use it to create a node with that operation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/examples/11_creating_a_local_function/README.md#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ngraph = gs.Graph(inputs=[gs.Variable(\"model_input\")], functions=[custom_func])\ngraph.outputs = graph.CustomOp(inputs=[graph.inputs[0]])\n```\n\n----------------------------------------\n\nTITLE: Running the TensorRT Python Sample with Progress Monitor\nDESCRIPTION: Command to run the simple_progress_monitor.py script which creates a TensorRT inference engine and runs inference while displaying progress bars.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/simple_progress_monitor/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 simple_progress_monitor.py\n```\n\n----------------------------------------\n\nTITLE: Calibrator Context Manager Example\nDESCRIPTION: Implementation of __enter__/__exit__ methods for Calibrator class to enable context manager functionality for proper device buffer management during calibration.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/CHANGELOG.md#2025-04-06_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nclass Calibrator:\n    def __enter__(self):\n        return self\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        # Free device buffers after calibration\n```\n\n----------------------------------------\n\nTITLE: Calculating Group Normalization Output in Python\nDESCRIPTION: This snippet demonstrates the mathematical operation performed by the GroupNormalizationPlugin. It calculates the normalized output using group mean and variance, applying scale (gamma) and bias (beta) parameters.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/groupNormalizationPlugin/README.md#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ngroup_mean = mean(input, group)\ngroup_var = variance(input, group)\noutput = gamma (input - group_mean) / sqrt(group_var + epsilon) + beta\n```\n\n----------------------------------------\n\nTITLE: Running Constrained Network with Flexible Precision in TensorRT\nDESCRIPTION: This optional command runs the network script but allows TensorRT to ignore precision constraints if necessary, which may be required if TensorRT has no layer implementation satisfying the requested precision constraints.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/run/08_adding_precision_constraints/README.md#2025-04-06_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run constrained_network.py --precision-constraints prefer \\\n    --trt --fp16 --load-inputs inputs.json --load-outputs golden_outputs.json \\\n    --check-error-stat median\n```\n\n----------------------------------------\n\nTITLE: Running the Polygraphy Example\nDESCRIPTION: Bash commands to install dependencies, inspect an ONNX model, run the example scripts that build and run TensorRT engines, and inspect the generated engine.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/06_immediate_eval_api/README.md#2025-04-06_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy inspect model identity.onnx\n```\n\nLANGUAGE: bash\nCODE:\n```\npython3 build_and_run.py\n```\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy inspect model identity.engine\n```\n\nLANGUAGE: bash\nCODE:\n```\npython3 load_and_run.py\n```\n\n----------------------------------------\n\nTITLE: Displaying Engine Plan DataFrame\nDESCRIPTION: Code to display the full DataFrame containing all layers and properties of the engine plan, using trex's notebook display functionality.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/tutorial.ipynb#2025-04-06_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ntrex.notebook.display_df(plan.df)\n```\n\n----------------------------------------\n\nTITLE: Example Execution of INT8 Inference on ResNet50 with TensorRT\nDESCRIPTION: Specific example command for running INT8 inference on a ResNet50 model using TensorRT. It specifies paths for the ONNX model, input image, reference labels, and dynamic range file.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleINT8API/README.md#2025-04-06_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n./sample_int8_api --model=$TRT_DATADIR/resnet50/ResNet50.onnx --image=$TRT_DATADIR/int8_api/airliner.ppm --reference=$TRT_DATADIR/int8_api/reference_labels.txt --ranges=$TRT_DATADIR/int8_api/resnet50_per_tensor_dynamic_range.txt\n```\n\n----------------------------------------\n\nTITLE: Setting up Python Virtual Environment\nDESCRIPTION: Commands to create and activate a Python virtual environment for trex installation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ cd TensorRT/tools/experimental/trt-engine-explorer\n$ python3 -m virtualenv env_trex\n$ source env_trex/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Inspecting a TensorRT Engine File\nDESCRIPTION: This command uses Polygraphy's inspect model subtool to display information about the TensorRT engine, including its layers. Note that layer information is only available if the engine was built with a profiling_verbosity setting other than NONE.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/inspect/02_inspecting_a_tensorrt_engine/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy inspect model dynamic_identity.engine \\\n    --show layers\n```\n\n----------------------------------------\n\nTITLE: Replacing Subgraphs with Plugins Using Configuration\nDESCRIPTION: Command for replacing identified subgraphs in an ONNX model with plugin implementations. This uses the previously generated config.yaml file to create a new model with the replacements.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/plugin/01_match_and_replace_plugin/README.md#2025-04-06_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy plugin replace toy_subgraph.onnx \\\n    --plugin-dir ./plugins --config config.yaml -o replaced.onnx\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorRT Sample Build Settings in CMake\nDESCRIPTION: Defines the source files and ONNX parser dependencies for the Editable Timing Cache sample. Includes a common CMake template for standard TensorRT sample build configuration.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleEditableTimingCache/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(SAMPLE_SOURCES sampleEditableTimingCache.cpp)\nset(SAMPLE_PARSERS \"onnx\")\ninclude(../CMakeSamplesTemplate.txt)\n```\n\n----------------------------------------\n\nTITLE: Manually Creating a Node Using Custom Function\nDESCRIPTION: Alternative approach to create a node that uses a custom function by manually constructing it with the Node constructor.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/examples/11_creating_a_local_function/README.md#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nnode = gs.Node(op=custom_func.name, domain=custom_func.domain)\nnode.inputs = [graph.inputs[0]]\nnode.outputs = [gs.Variable(\"custom_op_output\")]\ngraph.nodes.append(node)\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorRT Named Dimensions Sample Build\nDESCRIPTION: Configures the build settings for the Named Dimensions sample application. Sets the source files to include sampleNamedDimensions.cpp and specifies ONNX as the required parser dependency. Includes a common CMake template for TensorRT samples.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleNamedDimensions/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(SAMPLE_SOURCES sampleNamedDimensions.cpp)\n\nset(SAMPLE_PARSERS \"onnx\")\n\ninclude(../CMakeSamplesTemplate.txt)\n```\n\n----------------------------------------\n\nTITLE: Using Interactive Mode with debug reduce in Polygraphy for ONNX Models\nDESCRIPTION: This command demonstrates how to use the interactive mode of Polygraphy's debug reduce tool by omitting the --check option. This approach is more user-friendly for beginners as it guides them through the process of model reduction.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/how-to/use_debug_subtools_effectively.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy debug reduce <model.onnx> -o <reduced.onnx>\n```\n\n----------------------------------------\n\nTITLE: Installing Prerequisites for PyTorch Tensor Example with TensorRT\nDESCRIPTION: Command to install the required dependencies for running the PyTorch tensor example with TensorRT. This assumes TensorRT is already installed on the system.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/09_working_with_pytorch_tensors/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Setting TensorRT Library Names Based on Platform\nDESCRIPTION: Configures the TensorRT library names differently for Windows and Linux environments, including versioning for Windows libraries and adding suffixes based on build configuration.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/python/CMakeLists.txt#2025-04-06_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif(MSVC)\n    set(nvinfer_lib_name \"nvinfer_${TENSORRT_MAJOR_VERSION}\")\n    set(nvinfer_plugin_lib_name \"nvinfer_plugin_${TENSORRT_MAJOR_VERSION}\")\n    set(nvonnxparser_lib_name \"nvonnxparser_${TENSORRT_MAJOR_VERSION}\")\n    set(nvinfer_lean_lib_name \"nvinfer_lean_${TENSORRT_MAJOR_VERSION}${vfc_suffix}\")\n    set(nvinfer_dispatch_lib_name \"nvinfer_dispatch_${TENSORRT_MAJOR_VERSION}${vfc_suffix}\")\nelse()\n    set(nvinfer_lib_name \"nvinfer\")\n    set(nvinfer_plugin_lib_name \"nvinfer_plugin\")\n    set(nvonnxparser_lib_name \"nvonnxparser\")\n    set(nvinfer_lean_lib_name \"nvinfer_lean${vfc_suffix}\")\n    set(nvinfer_dispatch_lib_name \"nvinfer_dispatch${vfc_suffix}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Inspecting Converted FP16 Model\nDESCRIPTION: Command to inspect the structure and properties of the converted FP16 ONNX model.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/convert/04_converting_models_to_fp16/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy inspect model identity_fp16.onnx\n```\n\n----------------------------------------\n\nTITLE: Adding Plugin Sources in CMake for NVIDIA TensorRT\nDESCRIPTION: Adds source files for various plugins including MHA runner, QKV to context converters, and zero padding. This function is likely part of a larger CMake build system for TensorRT plugins.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/bertQKVToContextPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_plugin_source(\n    mhaRunner.cu\n    mhaRunner.h\n    qkvToContextInt8InterleavedPlugin.cpp\n    qkvToContextInt8InterleavedPlugin.h\n    qkvToContextInt8InterleavedPluginLegacy.cpp\n    qkvToContextInt8InterleavedPluginLegacy.h\n    qkvToContextPlugin.cpp\n    qkvToContextPlugin.h\n    qkvToContextPluginLegacy.cpp\n    qkvToContextPluginLegacy.h\n    zeroPadding2d.cu\n    zeroPadding2d.h\n)\n```\n\n----------------------------------------\n\nTITLE: Creating Input Batch for TensorRT Inference\nDESCRIPTION: Initializes a dummy input batch with specified shape and precision for testing the TensorRT model.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/1. Introduction.ipynb#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nPRECISION = np.float32\n\n# The input tensor shape of the ONNX model.\ninput_shape = (1, 3, 224, 224)\n\ndummy_input_batch = np.zeros(input_shape, dtype=PRECISION)\n```\n\n----------------------------------------\n\nTITLE: Configuring NonZero Plugin Sample for TensorRT in CMake\nDESCRIPTION: Sets up the source files, parser requirements, and CUDA dependencies for the NonZero plugin sample. It includes a template CMake file for common sample configurations.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleNonZeroPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(SAMPLE_SOURCES sampleNonZeroPlugin.cpp nonZeroKernel.cu)\n\nset(SAMPLE_PARSERS \"onnx\")\n\nset(CUDA_LIBS_REQUIRED True)\n\ninclude(../CMakeSamplesTemplate.txt)\n```\n\n----------------------------------------\n\nTITLE: Generating Tabular View of Plan\nDESCRIPTION: Creates a detailed tabular view of the engine plan\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/engine_report_card.ipynb#2025-04-06_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nreport_card_table_view(plan)\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies and Setting Up Environment for TensorFlow Quantization\nDESCRIPTION: This snippet imports necessary libraries, clears the Keras backend session, and sets up folders for saving models.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/notebooks/simple_network_quantize_specific_class.ipynb#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport tensorflow as tf\nfrom tensorflow_quantization import quantize_model, QuantizationSpec\nimport tiny_resnet\nfrom tensorflow_quantization import utils\nimport os\n\ntf.keras.backend.clear_session()\n\n# Create folders to save TF and ONNX models\nassets = utils.CreateAssetsFolders(os.path.join(os.getcwd(), \"tutorials\"))\nassets.add_folder(\"simple_network_quantize_specific_class\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Python Dependencies for TensorRT\nDESCRIPTION: Requirements file specifying Python package dependencies for TensorRT with version constraints. Includes conditional dependencies based on Python version and platform system, particularly for CUDA, numpy, and Windows-specific packages.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/non_zero_plugin/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ncuda-python==12.2.0; python_version <= \"3.10\"\ncuda-python==12.6.0; python_version >= \"3.11\"\ncupy-cuda12x\ntorch\n--extra-index-url https://pypi.ngc.nvidia.com\npolygraphy\ncolored\nnumpy==1.23.5; (platform_system != \"Windows\" and python_version <= \"3.10\")\nnumpy==1.26.4; (platform_system != \"Windows\" and python_version >= \"3.11\")\n--extra-index-url https://pypi.ngc.nvidia.com\nonnx-graphsurgeon\npywin32; platform_system == \"Windows\"\npyyaml==6.0.1\nrequests==2.32.2\ntqdm==4.66.4\n```\n\n----------------------------------------\n\nTITLE: Sample Config YAML Output from Plugin Matching\nDESCRIPTION: Example of the generated config.yaml file that contains the matched subgraph information. It specifies the plugin name, inputs, outputs, and attributes required for the replacement.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/plugin/01_match_and_replace_plugin/README.md#2025-04-06_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nname: toyPlugin\ninstances:\n- inputs:\n- i1\n- i1\noutputs:\n- o1\n- o2\nattributes:\n    x: 1\n```\n\n----------------------------------------\n\nTITLE: Declaring Size Tensor\nDESCRIPTION: Declares a size tensor that specifies the number of non-zero elements, using the calculated optimum and upper bound values.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/non_zero_plugin/README.md#2025-04-06_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nnum_non_zero_size_tensor = exprBuilder.declare_size_tensor(1, opt_value, upper_bound)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies\nDESCRIPTION: Command to install all required dependencies listed in the requirements.txt file using pip.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/dds_faster_rcnn/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip3 install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Specifying TensorRT Python Dependencies with Version Requirements\nDESCRIPTION: This requirements file lists all the Python packages needed for TensorRT functionality. It includes conditional dependencies based on Python version (3.10 or lower vs 3.11+) for packages like cuda-python and numpy, as well as platform-specific dependencies like pywin32 for Windows systems.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/introductory_parser_samples/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nPillow>=10.0.0\ncuda-python==12.2.0; python_version <= \"3.10\"\ncuda-python==12.6.0; python_version >= \"3.11\"\npywin32; platform_system == \"Windows\"\npyyaml==6.0.1\nrequests==2.32.2\ntqdm==4.66.4\nnumpy==1.24.4; python_version <= \"3.10\"\nnumpy==1.26.4; python_version >= \"3.11\"\n```\n\n----------------------------------------\n\nTITLE: Running ONNX Capability Inspection Command\nDESCRIPTION: Command to generate a capability report for an ONNX model using Polygraphy's inspect capability tool.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/inspect/09_inspecting_tensorrt_static_onnx_support/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy inspect capability nested_local_function.onnx\n```\n\n----------------------------------------\n\nTITLE: Initializing Quantized MaxPool2d Module\nDESCRIPTION: Implementation of __init__ function for QuantMaxPool2d that initializes the base pooling layer and sets up input quantization.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/tutorials/creating_custom_quantized_modules.rst#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(self, kernel_size, stride=None, padding=0, dilation=1,\n                 return_indices=False, ceil_mode=False, **kwargs):\n        super(QuantMaxPool2d, self).__init__(kernel_size, stride, padding, dilation,\n                                             return_indices, ceil_mode)\n        quant_desc_input = _utils.pop_quant_desc_in_kwargs(self.__class__, input_only=True, **kwargs)\n        self.init_quantizer(quant_desc_input)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for TensorRT Dynamic Shapes\nDESCRIPTION: Command to install required dependencies for running the dynamic shapes example\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/07_tensorrt_and_dynamic_shapes/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Verifying TensorRT Installation with Python\nDESCRIPTION: A simple Python command to verify TensorRT installation by printing its version number.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 -c \"import tensorrt as trt; print(trt.__version__)\"\n```\n\n----------------------------------------\n\nTITLE: Inspecting Generated TensorRT Engine\nDESCRIPTION: Command to inspect the dynamic shapes engine using Polygraphy's inspect tool\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/07_tensorrt_and_dynamic_shapes/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy inspect model dynamic_identity.engine\n```\n\n----------------------------------------\n\nTITLE: Implementing Quantized 3x3 Convolution in PyTorch\nDESCRIPTION: This function creates a 3x3 convolution layer, optionally using quantization. It supports various parameters like stride, groups, and dilation, and returns either a standard nn.Conv2d or a quant_nn.QuantConv2d based on the quantize flag.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/tutorials/quant_resnet50.rst#2025-04-06_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef conv3x3(in_planes: int,\n                out_planes: int,\n                stride: int = 1,\n                groups: int = 1,\n                dilation: int = 1,\n                quantize: bool = False) -> nn.Conv2d:\n    \"\"\"3x3 convolution with padding\"\"\"\n    if quantize:\n        return quant_nn.QuantConv2d(in_planes,\n                                    out_planes,\n                                    kernel_size=3,\n                                    stride=stride,\n                                    padding=dilation,\n                                    groups=groups,\n                                    bias=False,\n                                    dilation=dilation)\n    else:\n        return nn.Conv2d(in_planes,\n                         out_planes,\n                         kernel_size=3,\n                         stride=stride,\n                         padding=dilation,\n                         groups=groups,\n                         bias=False,\n                         dilation=dilation)\n```\n\n----------------------------------------\n\nTITLE: Adding FasterRCNN Plugin Sources in CMake\nDESCRIPTION: CMake command to add FasterRCNN plugin source files to the build configuration. Includes both the implementation (.cpp) and header (.h) files for the plugin.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/nvFasterRCNN/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_plugin_source(\n    nvFasterRCNNPlugin.cpp\n    nvFasterRCNNPlugin.h\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Fine-tuned Model\nDESCRIPTION: Performs evaluation of the fine-tuned model on the test dataset using no gradients.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/examples/finetune_quant_resnet50.ipynb#2025-04-06_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nwith torch.no_grad():\n    evaluate(model, criterion, data_loader_test, device=\"cuda\")\n```\n\n----------------------------------------\n\nTITLE: Checking GPU and CUDA Configuration with nvidia-smi\nDESCRIPTION: A command to verify the GPU environment configuration and check which GPU and CUDA version are being used through nvidia-smi.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/0. Running This Guide.ipynb#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!nvidia-smi\n```\n\n----------------------------------------\n\nTITLE: Adding PyramidROIAlign Plugin Source Files\nDESCRIPTION: CMake directive to add the PyramidROIAlign plugin source files to the build system. Includes both the implementation (.cpp) and header (.h) files.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/pyramidROIAlignPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_plugin_source(\n    pyramidROIAlignPlugin.cpp\n    pyramidROIAlignPlugin.h\n)\n```\n\n----------------------------------------\n\nTITLE: Markdown Documentation for TensorRT Performance Resources\nDESCRIPTION: Structured documentation covering TensorRT performance optimization resources, including links to Tensor Core guides, performance documentation, and device capability references.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/RESOURCES.md#2025-04-06_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Performance Resources\n\n## Performance Recommendations\n* [Tensor Core Performance Guide](https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9926-tensor-core-performance-the-ultimate-guide.pdf)\n  * Arithmetic intensity\n  * Tensor Core (TC) refresher\n  * TC alignment requirements, tile quantization, wave quantization\n* [2-mins video on TensorCore Performance Guidance](https://youtu.be/i8-Jw48Cp8w)\n* [DL Performance Documentation](https://docs.nvidia.com/deeplearning/performance/index.html): recommendations that apply to most deep learning operations.\n* [Tips for Optimizing GPU Performance Using Tensor Cores](https://developer.nvidia.com/blog/optimizing-gpu-performance-tensor-cores/)\n\n## Device Capabilities\n\n* [GPU Compute Capability](https://developer.nvidia.com/cuda-gpus): Lists the SM version of various NVIDIA devices.\n* [Supported Precisions](https://docs.nvidia.com/deeplearning/tensorrt/support-matrix/index.html#hardware-precision-matrix):\nContains a table listing which precision modes various NVIDIA devices support.\n```\n\n----------------------------------------\n\nTITLE: NMS Processing with EfficientNMS CUDA Kernel\nDESCRIPTION: This CUDA kernel processes the top 4096 scores after sorting. It handles box decoding (if fused decoder is used) and applies an efficient filtering algorithm to reduce IOU overlap cross-checks between box pairs.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/efficientNMSPlugin/README.md#2025-04-06_snippet_5\n\nLANGUAGE: CUDA\nCODE:\n```\nEfficientNMS\n```\n\n----------------------------------------\n\nTITLE: Conditional Plugin Source Addition Function in CMake\nDESCRIPTION: Defines a function 'add_plugin_source_if_exists' that adds plugin source files if they exist in the current directory. This function is used to conditionally include BERT QKV kernel files for different SM architectures.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/bertQKVToContextPlugin/fused_multihead_attention_v2/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(add_plugin_source_if_exists)\n    foreach(SRC_FILE IN LISTS ARGN)\n        if(EXISTS ${CMAKE_CURRENT_LIST_DIR}/${SRC_FILE})\n            add_plugin_source(${SRC_FILE})\n        endif()\n    endforeach()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Custom Quantization Using QuantizationSpec\nDESCRIPTION: Enables quantization of Add layers using QuantizationSpec for custom behavior, though this approach is considered suboptimal for ResNet models.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/add_custom_qdq_cases.md#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# 1. Enable `Add` layer quantization\nqspec = QuantizationSpec()\nqspec.add(name='Add', is_keras_class=True)\n\n# 2. Quantize model\nq_nn_model = quantize_model(\n    model=nn_model_original, quantization_spec=qspec\n)\n```\n\n----------------------------------------\n\nTITLE: Engine Summaries Table Comparison\nDESCRIPTION: Creates a vertical comparison table of engine summaries\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/compare_engines.ipynb#2025-04-06_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncompare_engines_summaries_tbl(plans, orientation='vertical')\n```\n\n----------------------------------------\n\nTITLE: Checking TensorRT Python Bindings Installation\nDESCRIPTION: This command lists the installed TensorRT packages, including Python bindings, to verify their presence on the system.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/EfficientDet/notebooks/EfficientDet-TensorRT8.ipynb#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n!dpkg -l | grep TensorRT\n```\n\n----------------------------------------\n\nTITLE: Generating ONNX Model with Python\nDESCRIPTION: This command runs a Python script to generate an initial ONNX model with several nodes and save it as 'model.onnx'. The generated model computes Y = x0 + (a * x1 + b).\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/examples/04_modifying_a_model/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 generate.py\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for TensorRT Sample\nDESCRIPTION: Command to install the required Python packages for running the TensorRT sample. This should be executed before running the sample scripts.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/sample_weight_stripping/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip3 install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Computing Optimum Value for Size Tensor\nDESCRIPTION: Estimates the expected number of non-zero elements as half of the total elements using FLOOR_DIV operation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/non_zero_plugin/README.md#2025-04-06_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nopt_value = exprBuilder.operation(trt.DimensionOperation.FLOOR_DIV, upper_bound, exprBuilder.constant(2))\n```\n\n----------------------------------------\n\nTITLE: Printing Plan Layer Names using EnginePlan in Python\nDESCRIPTION: This snippet demonstrates how to create an EnginePlan instance from a plan file and print the names of layers in the plan using a Pandas dataframe.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/trex/README.md#2025-04-06_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# Example: print plan layer names.\nplan = EnginePlan(\"my-engine.graph.json\")\ndf = plan.df\nprint(df['Name'])\n```\n\n----------------------------------------\n\nTITLE: Adding TensorFlow-TensorRT Integration Subdirectory\nDESCRIPTION: Adds the TensorFlow-TensorRT integration subdirectory to the build process using CMake's add_subdirectory command.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/efficientNMSPlugin/CMakeLists.txt#2025-04-06_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(tftrt)\n```\n\n----------------------------------------\n\nTITLE: Defining Module Structure in ReStructuredText\nDESCRIPTION: Sets up the structure for documenting the polygraphy.tools.args module using ReStructuredText directives\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/tool/args/backend/pluginref/toc.rst#2025-04-06_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n================\nPlugin Reference\n================\n\nModule: ``polygraphy.tools.args``\n\n.. toctree::\n    runner\n```\n\n----------------------------------------\n\nTITLE: Adding Reorg Plugin Source Files to TensorRT Build\nDESCRIPTION: CMake directive that adds the Reorg plugin implementation files to the TensorRT build system. The directive specifies both the cpp implementation file and the header file for inclusion in the build process.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/reorgPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_plugin_source(\n    reorgPlugin.cpp\n    reorgPlugin.h\n)\n```\n\n----------------------------------------\n\nTITLE: Python Module Import Documentation - Polygraphy TensorRT Loader\nDESCRIPTION: Documentation for the Polygraphy TensorRT loader module that handles loading TensorRT models and engines.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/tool/args/backend/trt/loader.rst#2025-04-06_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\npolygraphy.tools.args.backend.trt.loader\n```\n\n----------------------------------------\n\nTITLE: Configuring BERT Parameters and Command Line Arguments in Python\nDESCRIPTION: Sets up the BERT model parameters including document stride, max query length, sequence length, and tokenizer configuration. It also defines command line arguments for the benchmarking script, allowing users to specify engine path, batch sizes, iterations, and other testing parameters.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/benchmark.ipynb#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndoc_stride = 128\nmax_query_length = 64\nmax_seq_length = 384\n\nvocab_file = \"models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/vocab.txt\"\ntokenizer = tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=True)\n\nparser = argparse.ArgumentParser(description='BERT Inference Benchmark')\nparser.add_argument(\"-e\", \"--engine\", help='Path to BERT TensorRT engine', default='')\nparser.add_argument('-b', '--batch-size', default=[], action=\"append\", help='Batch size(s) to benchmark. Can be specified multiple times for more than one batch size. This script assumes that the engine has been built with one optimization profile for each batch size, and that these profiles are in order of increasing batch size.', type=int)\nparser.add_argument('-s', '--sequence-length', default=384, help='Sequence length of the BERT model', type=int)\nparser.add_argument('-i', '--iterations', default=1000, help='Number of iterations to run when benchmarking each batch size.', type=int)\nparser.add_argument('-w', '--warm-up-runs', default=10, help='Number of iterations to run prior to benchmarking.', type=int)\nparser.add_argument('-r', '--random-seed', required=False, default=12345, help='Random seed.', type=int)\nargs, _ = parser.parse_known_args()\nargs.batch_size = args.batch_size or [1]\n\n# Import necessary plugins for BERT TensorRT\nctypes.CDLL(\"libnvinfer_plugin.so\", mode=ctypes.RTLD_GLOBAL)\n```\n\n----------------------------------------\n\nTITLE: Adding ResizeNearest Plugin Source Files in CMake\nDESCRIPTION: CMake command to add the resize nearest plugin source files to the build configuration. Includes both the implementation (.cpp) and header (.h) files for the plugin.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/resizeNearestPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_plugin_source(\n    resizeNearestPlugin.cpp\n    resizeNearestPlugin.h\n)\n```\n\n----------------------------------------\n\nTITLE: Disentangled Attention Plugin Input/Output Structure\nDESCRIPTION: Defines the input and output tensor specifications for the DisentangledAttention_TRT plugin node, including shape requirements and supported data types.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/disentangledAttentionPlugin/README.md#2025-04-06_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\ndata0: Content-to-content (\"c2c\") Attention Matrix\\n[batch_size*number_heads, sequence_length, sequence_length]\\nData Type: float32 or float16 or int8\\n\\ndata1: Content-to-position (\"c2p\") Attention Matrix\\n[batch_size*number_heads, sequence_length, relative_distance*2]\\nData Type: float32 or float16 or int8\\n\\ndata2: Position-to-content (\"p2c\") Attention Matrix\\n[batch_size*number_heads, sequence_length, relative_distance*2]\\nData Type: float32 or float16 or int8\\n\\nresult: Disentangled Attention Matrix\\n[batch_size*number_heads, sequence_length, sequence_length]\\nData Type: float32 or float16 or int8\n```\n\n----------------------------------------\n\nTITLE: Adding CLIP Plugin Source Files in CMake\nDESCRIPTION: CMake command to add source files for the CLIP plugin compilation. Includes CUDA implementation (clip.cu), header files (clip.h, clipPlugin.h), and C++ implementation (clipPlugin.cpp).\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/clipPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_plugin_source(\n    clip.cu\n    clip.h\n    clipPlugin.cpp\n    clipPlugin.h\n)\n```\n\n----------------------------------------\n\nTITLE: Comparing Layer Latencies between FP16 and QAT+Residual ResNet18 Models\nDESCRIPTION: This code compares the layer latencies between the FP16 and QAT+Residual versions of ResNet18. It uses a 3% error threshold for highlighting performance differences and performs exact matching of layers based on their first input and output.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/examples/pytorch/resnet/example_qat_resnet18.ipynb#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ncompare_engines_layer_latencies(\n    plans[1], plans[3],\n    # Allow for 3% error grace threshold when color highlighting performance differences\n    threshold=0.03,\n    # Inexact matching uses only the layer's first input and output to match to other layers.\n    exact_matching=True)\n```\n\n----------------------------------------\n\nTITLE: Exporting Model with External Data\nDESCRIPTION: Shows how to export an ONNX model with external data storage using ONNX GraphSurgeon and ONNX helpers.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/README.md#2025-04-06_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel = gs.export_onnx(graph)\nfrom onnx.external_data_helper import convert_model_to_external_data\n\nconvert_model_to_external_data(model, location=\"model.data\")\nonnx.save(model, \"model.onnx\")\n```\n\n----------------------------------------\n\nTITLE: Direct Input Quantization Example\nDESCRIPTION: Example showing how to directly quantize inputs in the model graph using TensorQuantizer.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/tutorials/creating_custom_quantized_modules.rst#2025-04-06_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntest_input = torch.randn(1, 5, 5, 5, dtype=torch.double)\n\nquantizer = TensorQuantizer(quant_nn.QuantLinear.default_quant_desc_input)\n\nquant_input = quantizer(test_input)\n\nout = F.adaptive_avg_pool2d(quant_input, 3)\n```\n\n----------------------------------------\n\nTITLE: Installing and testing tensorflow-quantization inside Docker container\nDESCRIPTION: Commands to run inside the Docker container to install tensorflow-quantization and verify the installation by running tests. These steps need to be executed after launching the Docker container.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/installation.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ cd /home/tensorflow-quantization\n$ ./install.sh\n$ cd tests\n$ python3 -m pytest quantize_test.py -rP\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Detectron 2 TensorRT Conversion\nDESCRIPTION: Installs required dependencies listed in requirements.txt file using pip.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/detectron2/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Int8 Calibration with Polygraphy\nDESCRIPTION: Installs the required Python dependencies for running the Int8 calibration example with Polygraphy and TensorRT.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/04_int8_calibration_in_tensorrt/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Disabling Shape Folding in Python\nDESCRIPTION: Shows how to disable folding of Shape nodes when using the fold_constants function. This allows more control over constant folding behavior.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/CHANGELOG.md#2025-04-06_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfold_constants(fold_shapes=False)\n```\n\n----------------------------------------\n\nTITLE: Generating CUDA Architecture Flags for Compilation\nDESCRIPTION: Creates the gencode flags for CUDA compilation based on supported architectures. This includes both specific SM targets and PTX generation for forward compatibility.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/python_plugin/CMakeLists.txt#2025-04-06_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT DEFINED GENCODES)\n    set(GENCODES \"\")\n\n    # Add -gencode flags for each SM in SAMPLE_SMS\n    foreach(sm ${SAMPLE_SMS})\n        list(APPEND GENCODES \"-gencode=arch=compute_${sm},code=sm_${sm}\")\n    endforeach()\n\n    # Filter out NON_HFC_SMS from SAMPLE_SMS to get HFC_SMS\n    set(HFC_SMS ${SAMPLE_SMS})\n    foreach(sm ${NON_HFC_SMS})\n        list(REMOVE_ITEM HFC_SMS \"${sm}\")\n    endforeach()\n\n    # Get the highest supported forward compatible SM\n    if(HFC_SMS)\n        list(SORT HFC_SMS)\n        list(GET HFC_SMS -1 GEN_PTX_SM)\n        # Add PTX generation flag\n        list(APPEND GENCODES \"-gencode=arch=compute_${GEN_PTX_SM},code=compute_${GEN_PTX_SM}\")\n    else()\n        message(WARNING \"No hardware forward compatible SMs found. PTX generation skipped.\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for NVIDIA TensorRT\nDESCRIPTION: This snippet lists the required Python packages and their versions for the NVIDIA TensorRT project. It includes conditional dependencies based on Python version and operating system.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/sample_weight_stripping/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nPillow>=10.0.0\ncuda-python==12.2.0; python_version <= \"3.10\"\ncuda-python==12.6.0; python_version >= \"3.11\"\npywin32; platform_system == \"Windows\"\npyyaml==6.0.1\nrequests==2.32.2\ntqdm==4.66.4\nnumpy==1.24.4; python_version <= \"3.10\"\nnumpy==1.26.4; python_version >= \"3.11\"\n```\n\n----------------------------------------\n\nTITLE: Installing and Running Pylint for Python Code Style\nDESCRIPTION: Commands for installing pylint and checking Python files against the project's style guide\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/CONTRIBUTING.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install pylint\npylint --rcfile=.pylintrc myfile.py\n```\n\n----------------------------------------\n\nTITLE: Marking Classes as Deprecated using Decorator\nDESCRIPTION: Example showing how to use the deprecate() decorator to mark a class as deprecated. The decorator specifies when the class will be removed and what to use instead.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/CONTRIBUTING.md#2025-04-06_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@mod.deprecate(remove_in=\"0.25.0\", use_instead=\"NewClass\")\nclass OldClass:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Installing Polygraphy Using PowerShell on Windows\nDESCRIPTION: Command to build and install Polygraphy from source using the provided PowerShell script on Windows systems.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/README.md#2025-04-06_snippet_2\n\nLANGUAGE: powershell\nCODE:\n```\n.\\install.ps1\n```\n\n----------------------------------------\n\nTITLE: Adding VoxelGenerator Plugin Source Files in CMake\nDESCRIPTION: CMake command to add VoxelGenerator plugin source files (cpp and header) to the build system. This is part of TensorRT's plugin architecture for extending functionality.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/voxelGeneratorPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_plugin_source(\n    voxelGenerator.cpp\n    voxelGenerator.h\n)\n```\n\n----------------------------------------\n\nTITLE: Preparing MNIST Sample Data with Bash\nDESCRIPTION: Commands to download TensorRT sample data and prepare the MNIST dataset. It exports the data directory path and installs the required Pillow dependency.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleAlgorithmSelector/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport TRT_DATADIR=/usr/src/tensorrt/data\npushd $TRT_DATADIR/mnist\npip3 install Pillow\npopd\n```\n\n----------------------------------------\n\nTITLE: Importing TensorRT Runner Module in Python\nDESCRIPTION: This code snippet demonstrates how to import the TensorRT runner module from Polygraphy. It uses Python's automodule directive to automatically generate documentation for the module and its inherited members.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/backend/trt/runner.rst#2025-04-06_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. automodule:: polygraphy.backend.trt.runner\n    :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Adding ROI Align VC Plugin Source Files in TensorRT\nDESCRIPTION: Adds the same ROI Align plugin source files specifically for the VC plugin build variant in TensorRT. This ensures the plugin is built for both standard and VC plugin configurations.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/roiAlignPlugin/CMakeLists.txt#2025-04-06_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nadd_vc_plugin_source(\n    roiAlignKernel.cu\n    roiAlignKernel.h\n    roiAlignPlugin.cpp\n    roiAlignPlugin.h\n    roiAlignPluginLegacy.cpp\n    roiAlignPluginLegacy.h\n)\n```\n\n----------------------------------------\n\nTITLE: Referencing TensorRT Backend Module in RST Documentation\nDESCRIPTION: This RST code defines the documentation structure for the TensorRT backend module in Polygraphy. It specifies the module name and lists all the submodules that are part of this backend.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/backend/trt/toc.rst#2025-04-06_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n=========\nTensorRT\n=========\n\nModule: ``polygraphy.backend.trt``\n\n.. toctree::\n    algorithm_selector\n    calibrator\n    config\n    loader\n    profile\n    runner\n    util\n```\n\n----------------------------------------\n\nTITLE: Adding Detection Layer Plugin Source Files\nDESCRIPTION: CMake command to add detection layer plugin source files (cpp and header) to the build configuration.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/detectionLayerPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_plugin_source(\n    detectionLayerPlugin.cpp\n    detectionLayerPlugin.h\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Subdirectories for Fused Multihead Attention in CMake\nDESCRIPTION: Includes additional CMake configuration files for fused multihead attention plugins. This extends the build system to include specialized attention mechanism implementations.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/bertQKVToContextPlugin/CMakeLists.txt#2025-04-06_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(fused_multihead_attention)\nadd_subdirectory(fused_multihead_attention_v2)\n```\n\n----------------------------------------\n\nTITLE: Displaying TensorRT Network from ONNX Model using Polygraphy\nDESCRIPTION: Command to convert an ONNX model to a TensorRT network and display its layers. The command uses the polygraphy tool's 'inspect model' subtool with options to show layers and display in TensorRT format.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/inspect/01_inspecting_a_tensorrt_network/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy inspect model identity.onnx \\\n    --show layers --display-as=trt\n```\n\n----------------------------------------\n\nTITLE: Inspecting ONNX Model Layers with Polygraphy\nDESCRIPTION: Command to inspect and display the layers of an ONNX model using Polygraphy's inspect model subtool. This shows basic model information including inputs, outputs, and node details.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/inspect/03_inspecting_an_onnx_model/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy inspect model identity.onnx --show layers\n```\n\n----------------------------------------\n\nTITLE: Plotting Engine Timings\nDESCRIPTION: Visualizes engine timing data from JSON file\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/engine_report_card.ipynb#2025-04-06_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nplot_engine_timings(timing_json_file= f\"{engine_name}.timing.json\")\n```\n\n----------------------------------------\n\nTITLE: Executing Generated Comparison Script\nDESCRIPTION: This command runs the generated Python script to perform the comparison between TensorRT and ONNX Runtime implementations. The script can be modified before execution to customize the comparison process if needed.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/run/03_generating_a_comparison_script/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 compare_trt_onnxrt.py\n```\n\n----------------------------------------\n\nTITLE: Post-Training Quantization with Initialization\nDESCRIPTION: Shows how to convert a standard model to a quantized model using post-training quantization. Simply calling quant_modules.initialize() transforms a model like ResNet50 to use quantized modules.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/userguide.rst#2025-04-06_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom pytorch_quantization import quant_modules\nmodel = torchvision.models.resnet50()\n```\n\n----------------------------------------\n\nTITLE: Installing TensorRT tensorflow-quantization using Docker\nDESCRIPTION: Instructions for installing tensorflow-quantization using Docker. This involves cloning the TensorRT repository, pulling the NVIDIA TensorFlow Docker image, and launching a container with the repository mounted.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/installation.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ cd ~/\n$ git clone https://github.com/NVIDIA/TensorRT.git\n$ docker pull nvcr.io/nvidia/tensorflow:22.03-tf2-py3\n$ docker run -it --runtime=nvidia --gpus all --net host -v ~/TensorRT/tools/tensorflow-quantization:/home/tensorflow-quantization nvcr.io/nvidia/tensorflow:22.03-tf2-py3 /bin/bash\n```\n\n----------------------------------------\n\nTITLE: Defining RPROIParams Structure for Faster R-CNN Plugin\nDESCRIPTION: C++ structure definition for RPROIParams that configures the RPROIPlugin instance with parameters for pooling dimensions, feature stride, NMS settings, anchor configurations, and various thresholds.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/nvFasterRCNN/README.md#2025-04-06_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nstruct RPROIParams\n{\n\tint poolingH, poolingW, featureStride, preNmsTop,\n\t\tnmsMaxOut, anchorsRatioCount, anchorsScaleCount;\n\tfloat iouThreshold, minBoxSize, spatialScale;\n};\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for BERT PTQ (Xavier GPU)\nDESCRIPTION: This command builds a TensorRT engine for BERT PTQ on Xavier GPU. It supports SkipLayerNormPlugin running with INT8 I/O.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p engines && python3 builder.py -m models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/model.ckpt -o engines/bert_large_384_int8mix.engine -b 1 -s 384 --int8 --fp16 --strict -c models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1 --squad-json ./squad/train-v1.1.json -v models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/vocab.txt --calib-num 100 -iln\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for ONNX Backend\nDESCRIPTION: Defines the documentation structure for the ONNX backend module in Polygraphy using reStructuredText format. Includes module path reference and toctree directive for loader submodule.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/backend/onnx/toc.rst#2025-04-06_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n====\nONNX\n====\n\nModule: ``polygraphy.backend.onnx``\n\n.. toctree::\n    loader\n```\n\n----------------------------------------\n\nTITLE: Conditional Kernel Compilation Function for CUDA SM Architectures in CMake\nDESCRIPTION: Defines a function to determine if a CUDA kernel should be compiled for a specific SM architecture. It handles special cases for SM 80, 86, and 89, which are considered binary compatible.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/bertQKVToContextPlugin/CMakeLists.txt#2025-04-06_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(should_compile_kernel SM OUT_VAR)\n    if((${SM} EQUAL 80) OR (${SM} EQUAL 86) OR (${SM} EQUAL 89))\n        list(FIND CMAKE_CUDA_ARCHITECTURES 80 SM80_INDEX)\n        list(FIND CMAKE_CUDA_ARCHITECTURES 86 SM86_INDEX)\n        list(FIND CMAKE_CUDA_ARCHITECTURES 89 SM89_INDEX)\n        if((NOT ${SM80_INDEX} EQUAL -1) OR\n           (NOT ${SM86_INDEX} EQUAL -1) OR\n           (NOT ${SM89_INDEX} EQUAL -1)\n        )\n            set(${OUT_VAR} TRUE PARENT_SCOPE)\n        else()\n            set(${OUT_VAR} FALSE PARENT_SCOPE)\n        endif()\n    else()\n        list(FIND CMAKE_CUDA_ARCHITECTURES ${SM} SM_INDEX)\n        if (NOT ${SM_INDEX} EQUAL -1)\n            set(${OUT_VAR} TRUE PARENT_SCOPE)\n        else()\n            set(${OUT_VAR} FALSE PARENT_SCOPE)\n        endif()\n    endif()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Importing CUDA Wrapper Module in Python\nDESCRIPTION: This code snippet shows how to import the CUDA wrapper module from the Polygraphy library. It demonstrates the correct import statement for accessing the CUDA functionality.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/cuda/toc.rst#2025-04-06_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom polygraphy.cuda import cuda\n```\n\n----------------------------------------\n\nTITLE: Setting Allowed Tensor Formats in TensorRT C++\nDESCRIPTION: This snippet demonstrates how to use the ITensor::setAllowedFormats API to specify which tensor format is expected to be supported for the input tensor.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleIOFormats/README.md#2025-04-06_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nbool SampleIOFormats::build(int dataWidth)\n{\n    ...\n\n    network->getInput(0)->setAllowedFormats(static_cast<TensorFormats>(1 << static_cast<int>(mTensorFormat)));\n    ...\n}\n```\n\n----------------------------------------\n\nTITLE: Adding VC Plugin Sources in TensorRT\nDESCRIPTION: CMake command to add VC-specific plugin source files including checkMacros and VFC common components to the build system.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/vc/CMakeLists.txt#2025-04-06_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nadd_vc_plugin_source(\n    checkMacrosPlugin.cpp\n    checkMacrosPlugin.h\n    vfcCommon.cpp\n    vfcCommon.h\n)\n```\n\n----------------------------------------\n\nTITLE: Exporting TensorFlow Saved Model from TFOD Checkpoint\nDESCRIPTION: Python command to export a TensorFlow saved model from a TFOD EfficientDet checkpoint using the exporter_main_v2.py script.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/efficientdet/README.md#2025-04-06_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython3 exporter_main_v2.py \\\n    --input_type image_tensor \\\n    --trained_checkpoint_dir /path/to/efficientdet_d0_coco17_tpu-32/checkpoint \\\n    --pipeline_config_path /path/to/efficientdet_d0_coco17_tpu-32/pipeline.config \\\n    --output_directory /path/to/export\n```\n\n----------------------------------------\n\nTITLE: Adding TensorRT Plugin Sources with CMake\nDESCRIPTION: CMake command that adds multiple CUDA and C++ source files to the plugin build system. The files implement various neural network operations including NMS, ROI pooling, mask RCNN, and other detection-related kernels.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/common/kernels/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_plugin_source(\n    allClassNMS.cu\n    bboxDeltas2Proposals.cu\n    common.cu\n    cropAndResizeKernel.cu\n    decodeBbox3DKernels.cu\n    decodeBBoxes.cu\n    detectionForward.cu\n    extractFgScores.cu\n    gatherTopDetections.cu\n    generateAnchors.cu\n    gridAnchorLayer.cu\n    kernel.cpp\n    kernel.h\n    lReLU.cu\n    maskRCNNKernels.cu\n    maskRCNNKernels.h\n    nmsLayer.cu\n    normalizeLayer.cu\n    permuteData.cu\n    pillarScatterKernels.cu\n    priorBoxLayer.cu\n    proposalKernel.cu\n    proposalsForward.cu\n    reducedMathPlugin.h\n    regionForward.cu\n    reorgForward.cu\n    roiPooling.cu\n    rproiInferenceFused.cu\n    saturate.h\n    sortScoresPerClass.cu\n    sortScoresPerImage.cu\n    voxelGeneratorKernels.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Comparing TensorRT Engine Overviews for ResNet18 Variants\nDESCRIPTION: This snippet compares the overviews of different TensorRT engine plans for ResNet18 variants. It uses the compare_engines_overview function from the TREx library to visualize and compare the performance characteristics of the different engine configurations.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/examples/pytorch/resnet/example_qat_resnet18.ipynb#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ncompare_engines_overview(plans)\n```\n\n----------------------------------------\n\nTITLE: Creating a Dummy Input Batch for ONNX Export\nDESCRIPTION: Creates a random tensor with appropriate dimensions to serve as a dummy input for ONNX export. The batch size is set to 32, with 3 channels and 224x224 image dimensions.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/2. Using PyTorch through ONNX.ipynb#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nBATCH_SIZE=32\n\ndummy_input=torch.randn(BATCH_SIZE, 3, 224, 224)\n```\n\n----------------------------------------\n\nTITLE: Adding DisentangledAttention Plugin Sources to Build\nDESCRIPTION: CMake command to add source files for the DisentangledAttention plugin to the build system. Includes the main plugin implementation, header file, and CUDA kernel implementation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/disentangledAttentionPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_plugin_source(\n    disentangledAttentionPlugin.cpp\n    disentangledAttentionPlugin.h\n    disentangledKernel.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Algorithm Selector Sample Command Line Options\nDESCRIPTION: The available command line options for the sample_algorithm_selector including help, data directory specification, DLA core selection, and precision modes.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleAlgorithmSelector/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./sample_algorithm_selector [-h] [--datadir=/path/to/data/dir/] [--useDLA=N] [--fp16 or --int8]\n```\n\n----------------------------------------\n\nTITLE: Creating a Dummy Input Batch for ONNX Export\nDESCRIPTION: Creates a random tensor with appropriate dimensions to serve as a dummy input for ONNX export. The batch size is set to 32, with 3 channels and 224x224 image dimensions.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/2. Using PyTorch through ONNX.ipynb#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nBATCH_SIZE=32\n\ndummy_input=torch.randn(BATCH_SIZE, 3, 224, 224)\n```\n\n----------------------------------------\n\nTITLE: Defining QA Text Input Data in Python\nDESCRIPTION: Defines the input paragraph text (both full and shortened versions) for BERT-based question answering. The shortened version is specifically for BERT models with max sequence length of 128.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/inference.ipynb#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nparagraph_text = \"The Apollo program, also known as Project Apollo, was the third United States human spaceflight program carried out by the National Aeronautics and Space Administration (NASA), which accomplished landing the first humans on the Moon from 1969 to 1972. First conceived during Dwight D. Eisenhower's administration as a three-man spacecraft to follow the one-man Project Mercury which put the first Americans in space, Apollo was later dedicated to President John F. Kennedy's national goal of landing a man on the Moon and returning him safely to the Earth by the end of the 1960s, which he proposed in a May 25, 1961, address to Congress. Project Mercury was followed by the two-man Project Gemini. The first manned flight of Apollo was in 1968. Apollo ran from 1961 to 1972, and was supported by the two-man Gemini program which ran concurrently with it from 1962 to 1966. Gemini missions developed some of the space travel techniques that were necessary for the success of the Apollo missions. Apollo used Saturn family rockets as launch vehicles. Apollo/Saturn vehicles were also used for an Apollo Applications Program, which consisted of Skylab, a space station that supported three manned missions in 1973-74, and the Apollo-Soyuz Test Project, a joint Earth orbit mission with the Soviet Union in 1975.\"\n\n# Short paragraph version for BERT models with max sequence length of 128\nshort_paragraph_text = \"The Apollo program was the third United States human spaceflight program. First conceived as a three-man spacecraft to follow the one-man Project Mercury which put the first Americans in space, Apollo was dedicated to President John F. Kennedy's national goal of landing a man on the Moon. The first manned flight of Apollo was in 1968. Apollo ran from 1961 to 1972 followed by the Apollo-Soyuz Test Project a joint Earth orbit mission with the Soviet Union in 1975.\"\n```\n\n----------------------------------------\n\nTITLE: Downloading EfficientNet V1 Model\nDESCRIPTION: Command to download pre-trained EfficientNet V1 model from TFHub.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/efficientnet/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nwget https://storage.googleapis.com/tfhub-modules/tensorflow/efficientnet/b0/classification/1.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Reinstalling After Changes\nDESCRIPTION: Command to reinstall the extension module after making changes, forcing reinstallation without updating dependencies.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/dev/02_extending_polygraphy_run/README.md#2025-04-06_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pip install extension_module/dist/polygraphy_reshape_destroyer-0.0.1-py3-none-any.whl \\\n    --force-reinstall --no-deps\n```\n\n----------------------------------------\n\nTITLE: Installing the Colored Package for Improved Output\nDESCRIPTION: Command to install the 'colored' Python package, which enhances Polygraphy's terminal output readability.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/README.md#2025-04-06_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install colored\n```\n\n----------------------------------------\n\nTITLE: Configuring CreateConfig in Python\nDESCRIPTION: Shows how to use the new strict_types argument when creating a TensorRT config.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/CHANGELOG.md#2025-04-06_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nCreateConfig(strict_types=True)\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Python Module and Performance Test\nDESCRIPTION: Configures build targets for the Python module using pybind11 and a performance testing executable. Links against TensorRT and CUDA libraries.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/CMakeLists.txt#2025-04-06_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\npybind11_add_module(infer_c\n    infer_c/infer_c.cpp\n    infer_c/logging.cpp\n)\ntarget_link_libraries(infer_c PRIVATE\n    ${CUDA_LIBRARIES}\n    nvinfer\n    nvinfer_plugin\n)\n\nadd_executable(perf\n    infer_c/perf.cpp\n    infer_c/logging.cpp\n)\n\ntarget_link_libraries(perf\n    ${CUDA_LIBRARIES}\n    nvinfer\n    nvinfer_plugin\n)\n```\n\n----------------------------------------\n\nTITLE: Python Dependencies Configuration for NVIDIA TensorRT\nDESCRIPTION: This requirements file specifies all Python package dependencies needed for the NVIDIA TensorRT project, including version constraints and conditional dependencies based on Python version and platform.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/onnx_custom_plugin/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nnltk==3.9.1\nonnx==1.16.0\n--extra-index-url https://pypi.ngc.nvidia.com\nonnx-graphsurgeon>=0.3.20\nwget>=3.2\ncuda-python==12.2.0; python_version <= \"3.10\"\ncuda-python==12.6.0; python_version >= \"3.11\"\npywin32; platform_system == \"Windows\"\npyyaml==6.0.1\nrequests==2.32.2\ntqdm==4.66.4\nnumpy==1.24.4; python_version <= \"3.10\"\nnumpy==1.26.4; python_version >= \"3.11\"\n```\n\n----------------------------------------\n\nTITLE: TensorRT Plugins Table Structure\nDESCRIPTION: Markdown table listing all TensorRT plugins with their names and supported versions. Includes deprecation status indicators for outdated plugins.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/README.md#2025-04-06_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Plugin | Name | Versions |\n|---|---|---|\n| [batchTilePlugin](batchTilePlugin) [DEPRECATED] | BatchTilePlugin_TRT | 1 |\n| [batchedNMSPlugin](batchedNMSPlugin) [DEPRECATED] | BatchedNMS_TRT | 1 |\n| [batchedNMSDynamicPlugin](batchedNMSPlugin) [DEPRECATED] | BatchedNMSDynamic_TRT | 1 |\n```\n\n----------------------------------------\n\nTITLE: Adding Regular Plugin Sources in TensorRT\nDESCRIPTION: CMake command to add standard plugin source files including checkMacros and VFC common components to the build system.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/vc/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_plugin_source(\n    checkMacrosPlugin.cpp\n    checkMacrosPlugin.h\n    vfcCommon.cpp\n    vfcCommon.h\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages for Python-based TRT Plugins\nDESCRIPTION: Command to install the necessary packages for running the Python-based TensorRT plugin samples. It uses pip to install the requirements listed in the requirements.txt file.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/python_plugin/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip3 install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Defining Documentation Structure in reStructuredText\nDESCRIPTION: This snippet defines the table of contents and structure for the pytorch-quantization documentation using reStructuredText directives. It includes sections for user guide, tutorials, and package reference.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/index.rst#2025-04-06_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n   :maxdepth: 1\n   :caption: User Guide\n\n   userguide\n\n.. toctree::\n   :caption: Tutorials\n   :titlesonly:\n   :maxdepth: 2\n\n   tutorials/quant_resnet50\n   tutorials/creating_custom_quantized_modules\n\n\n.. toctree::\n   :maxdepth: 1\n   :caption: Package Reference\n\n   calib\n   nn\n   functional\n   optim\n   tensor_quant\n   utils\n\nIndices\n==================\n\n* :ref:`genindex`\n```\n\n----------------------------------------\n\nTITLE: Creating Shared Library Target in CMake for TensorRT Plugins\nDESCRIPTION: Defines and configures a shared library target for TensorRT plugins. It sets include directories, compiler standards, output directories, and links against necessary libraries.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/CMakeLists.txt#2025-04-06_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(${SHARED_TARGET} SHARED ${PLUGIN_SOURCES})\n\ntarget_include_directories(\n    ${SHARED_TARGET}\n    PUBLIC ${PROJECT_SOURCE_DIR}/include\n    PRIVATE ${PROJECT_SOURCE_DIR}/common\n    PUBLIC ${CUDA_INSTALL_DIR}/include\n    PRIVATE ${TARGET_DIR})\n\nif(CUDA_VERSION VERSION_LESS 11.0)\n    target_include_directories(${SHARED_TARGET} PUBLIC ${CUB_ROOT_DIR})\nendif()\n\nset_target_properties(\n    ${SHARED_TARGET}\n    PROPERTIES CXX_STANDARD \"17\"\n               CXX_STANDARD_REQUIRED \"YES\"\n               CXX_EXTENSIONS \"NO\"\n               ARCHIVE_OUTPUT_DIRECTORY \"${TRT_OUT_DIR}\"\n               LIBRARY_OUTPUT_DIRECTORY \"${TRT_OUT_DIR}\"\n               RUNTIME_OUTPUT_DIRECTORY \"${TRT_OUT_DIR}\")\n\nif(MSVC)\n    set_target_properties(${SHARED_TARGET} PROPERTIES LINK_FLAGS \"/DEF:${PLUGIN_EXPORT_DEF}\")\nelse()\n    set_target_properties(\n        ${SHARED_TARGET}\n        PROPERTIES LINK_FLAGS\n                   \"-Wl,--exclude-libs,ALL -Wl,-Bsymbolic -Wl,--version-script=${PLUGIN_EXPORT_MAP} -Wl,--no-undefined\")\nendif()\n\nset_target_properties(${SHARED_TARGET} PROPERTIES DEBUG_POSTFIX ${TRT_DEBUG_POSTFIX})\n\nset_target_properties(${SHARED_TARGET} PROPERTIES VERSION ${TRT_VERSION} SOVERSION ${TRT_SOVERSION})\n\nset_property(TARGET ${SHARED_TARGET} PROPERTY CUDA_STANDARD 17)\n\ntarget_link_directories(${SHARED_TARGET} PUBLIC ${CUDA_ROOT}/lib)\n\ntarget_link_libraries(${SHARED_TARGET} ${CUDART_LIB} ${${nvinfer_lib_name}_LIB_PATH} ${CMAKE_DL_LIBS})\n\n# Needed when static linking CUDART\nif(NOT MSVC)\n    target_link_libraries(${SHARED_TARGET} Threads::Threads ${RT_LIB})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom QDQ Insertion for EfficientNet\nDESCRIPTION: Example implementation of CustomQDQInsertionCase for EfficientNet models that specifies quantization for Multiply operations. It quantizes inputs at indices 0 and 1 for all Multiply layers in the model.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/cqdq.rst#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass EfficientNetQDQCase(CustomQDQInsertionCase):\n    def __init__(self) -> None:\n        super().__init__()\n\n    def info(self):\n        return \"In Multiply operation quantize inputs at index 0 and 1.\"\n\n    def case(self, keras_model: 'tf.keras.Model', qspec: 'QuantizationSpec') -> 'QuantizationSpec':\n        se_block_qspec_object = QuantizationSpec()\n        for layer in keras_model.layers:\n            if isinstance(layer, tf.keras.layers.Multiply):\n                se_block_qspec_object.add(layer.name, quantize_input=True, quantize_weight=False, quantization_index=[0, 1])\n        return se_block_qspec_object\n```\n\n----------------------------------------\n\nTITLE: Creating TensorRT Builder with Logger\nDESCRIPTION: Code to create a TensorRT builder with a logger for reporting errors, warnings, and informational messages during network construction and optimization.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleOnnxMNIST/README.md#2025-04-06_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nIBuilder* builder = createInferBuilder(sample::gLogger);\n```\n\n----------------------------------------\n\nTITLE: Setting Source File Properties and Compiling Flags in CMake\nDESCRIPTION: Configures compile flags for CUDA source files, including BERT-specific sources if available. It also appends various source files to the PLUGIN_SOURCES list.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/CMakeLists.txt#2025-04-06_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nset_source_files_properties(${PLUGIN_CU_SOURCES} PROPERTIES COMPILE_FLAGS \"${GENCODES} ${ENABLED_SMS}\")\nlist(APPEND PLUGIN_SOURCES \"${PLUGIN_CU_SOURCES}\")\nif(BERT_CU_SOURCES)\n    set_source_files_properties(${BERT_CU_SOURCES} PROPERTIES COMPILE_FLAGS \"${BERT_GENCODES} ${ENABLED_SMS}\")\n    list(APPEND PLUGIN_SOURCES \"${BERT_CU_SOURCES}\")\nendif()\n\nlist(APPEND PLUGIN_SOURCES \"${CMAKE_CURRENT_SOURCE_DIR}/api/inferPlugin.cpp\")\nlist(APPEND PLUGIN_SOURCES \"${CMAKE_CURRENT_SOURCE_DIR}/../samples/common/logger.cpp\")\n```\n\n----------------------------------------\n\nTITLE: Running ONNX Model Generation Script\nDESCRIPTION: Command to execute the Python script that generates an ONNX model containing a GlobalLpPool node and saves it as 'test_globallppool.onnx'\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/examples/01_creating_a_model/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 example.py\n```\n\n----------------------------------------\n\nTITLE: Adding Grid Anchor Plugin Source Files in CMake\nDESCRIPTION: CMake command to add the grid anchor plugin source files (cpp and header) to the build configuration. This plugin is part of the NVIDIA TensorRT framework.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/gridAnchorPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_plugin_source(\n    gridAnchorPlugin.cpp\n    gridAnchorPlugin.h\n)\n```\n\n----------------------------------------\n\nTITLE: Loading ImageNet Data for ResNet50 V1 in TensorFlow\nDESCRIPTION: This snippet loads the ImageNet dataset for training and validation using a custom data loader function. It uses the previously defined hyperparameters to configure the data loading process.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/example_resnet50v1.ipynb#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom examples.data.data_loader import load_data\ntrain_batches, val_batches = load_data(HYPERPARAMS, model_name=\"resnet_v1\")\n```\n\n----------------------------------------\n\nTITLE: Processing TensorRT Engine\nDESCRIPTION: Command to build, profile and draw a TensorRT engine from an ONNX model.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/bin/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntrex process ./examples/pytorch/resnet/generated/resnet.onnx ./examples/pytorch/resnet/A100/fp32/\n```\n\n----------------------------------------\n\nTITLE: Installing ONNX GraphSurgeon Using Pip\nDESCRIPTION: Installs ONNX GraphSurgeon using prebuilt wheels from NVIDIA's PyPI repository.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pip install onnx_graphsurgeon --extra-index-url https://pypi.ngc.nvidia.com\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing CIFAR10 Dataset for TensorFlow Model Training\nDESCRIPTION: This code loads the CIFAR10 dataset and normalizes the input images for model training.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/notebooks/simple_network_quantize_specific_class.ipynb#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Load CIFAR10 dataset\ncifar10 = tf.keras.datasets.cifar10\n(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n\n# Normalize the input image so that each pixel value is between 0 and 1.\ntrain_images = train_images / 255.0\ntest_images = test_images / 255.0\n```\n\n----------------------------------------\n\nTITLE: Training Original TensorFlow Classification Model\nDESCRIPTION: This code compiles and trains the original ResNet model on the CIFAR10 dataset.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/notebooks/simple_network_quantize_specific_class.ipynb#2025-04-06_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Train original classification model\nnn_model_original.compile(\n    optimizer=tiny_resnet.optimizer(lr=1e-4),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[\"accuracy\"],\n)\n\n_ = nn_model_original.fit(\n    train_images, train_labels, batch_size=32, epochs=10, validation_split=0.1\n)\n```\n\n----------------------------------------\n\nTITLE: ONNX Model Inspection Output\nDESCRIPTION: Example output from Polygraphy's model inspection showing the model name, ONNX opset version, graph inputs/outputs, initializers, and node details for an Identity operation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/inspect/03_inspecting_an_onnx_model/README.md#2025-04-06_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n[I] ==== ONNX Model ====\n    Name: test_identity | ONNX Opset: 8\n\n    ---- 1 Graph Input(s) ----\n    {x [dtype=float32, shape=(1, 1, 2, 2)]}\n\n    ---- 1 Graph Output(s) ----\n    {y [dtype=float32, shape=(1, 1, 2, 2)]}\n\n    ---- 0 Initializer(s) ----\n    {}\n\n    ---- 1 Node(s) ----\n    Node 0    |  [Op: Identity]\n        {x [dtype=float32, shape=(1, 1, 2, 2)]}\n         -> {y [dtype=float32, shape=(1, 1, 2, 2)]}\n```\n\n----------------------------------------\n\nTITLE: Visualizing Quantized TensorFlow Model Architecture\nDESCRIPTION: This snippet generates a visual representation of the quantized model's architecture.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/notebooks/simple_network_quantize_specific_class.ipynb#2025-04-06_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntf.keras.utils.plot_model(q_nn_model, to_file = assets.simple_network_quantize_specific_class.int8 + \"/model.png\")\n```\n\n----------------------------------------\n\nTITLE: Configuring BERT Plugin Sources in CMake\nDESCRIPTION: Sets up BERT-specific plugin sources and include directories if BERT_GENCODES is defined. It also adds common include directories and iterates through a list of plugins to include their respective directories.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/CMakeLists.txt#2025-04-06_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif(BERT_GENCODES)\n    set(BERT_CU_SOURCES)\n    set(PLUGIN_LISTS ${PLUGIN_LISTS} bertQKVToContextPlugin embLayerNormPlugin fcPlugin geluPlugin skipLayerNormPlugin)\n    include_directories(bertQKVToContextPlugin/fused_multihead_attention\n                       /include bertQKVToContextPlugin/fused_multihead_attention_v2/include)\nendif()\n\ninclude_directories(common common/kernels ${CMAKE_SOURCE_DIR}/third_party)\n\nforeach(PLUGIN_ITER ${PLUGIN_LISTS})\n    include_directories(${PLUGIN_ITER})\n    add_subdirectory(${PLUGIN_ITER})\nendforeach(PLUGIN_ITER)\n\n# Add common\nadd_subdirectory(common)\nadd_subdirectory(vc)\n```\n\n----------------------------------------\n\nTITLE: Complete Quantized MaxPool2d Implementation\nDESCRIPTION: Full implementation of the quantized MaxPool2d module including initialization, forward pass, and property accessors.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/tutorials/creating_custom_quantized_modules.rst#2025-04-06_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass QuantMaxPool2d(pooling.MaxPool2d, _utils.QuantInputMixin):\n    \"\"\"Quantized 2D maxpool\"\"\"\n    def __init__(self, kernel_size, stride=None, padding=0, dilation=1,\n                return_indices=False, ceil_mode=False, **kwargs):\n        super(QuantMaxPool2d, self).__init__(kernel_size, stride, padding, dilation,\n                                            return_indices, ceil_mode)\n        quant_desc_input = _utils.pop_quant_desc_in_kwargs(self.__class__, input_only=True, **kwargs)\n        self.init_quantizer(quant_desc_input)\n\n    def forward(self, input):\n        quant_input = self._input_quantizer(input)\n        return super(QuantMaxPool2d, self).forward(quant_input)\n\n    @property\n    def input_quantizer(self):\n        return self._input_quantizer\n```\n\n----------------------------------------\n\nTITLE: Filtering and Displaying FP32 Convolutions\nDESCRIPTION: Filters 1x1 convolutions that have FP32 outputs and displays them using the display_df helper function.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/api-examples.ipynb#2025-04-06_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Display the dataframe of all INT8 convolutions with \nconvs_1x1_fp32 = convs_1x1[convs_1x1[\"Outputs\"].str.startswith(\"FP32\")]\ndisplay_df(convs_1x1_fp32)\n```\n\n----------------------------------------\n\nTITLE: Defining a CMake Macro for Default Variable Setting\nDESCRIPTION: Creates a macro called 'set_ifndef' that sets a variable to a default value only if the variable is currently unset. The macro also outputs a status message showing the final value.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/onnx_custom_plugin/CMakeLists.txt#2025-04-06_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\n# Sets variable to a value if variable is unset.\nmacro(set_ifndef var val)\n    if(NOT ${var})\n        set(${var} ${val})\n    endif()\n    message(STATUS \"Configurable variable ${var} set to ${${var}}\")\nendmacro()\n```\n\n----------------------------------------\n\nTITLE: Transferring Inference Results from GPU to CPU in TensorRT\nDESCRIPTION: Copies inference output data from device memory back to host memory using CUDA memcpy for further processing or display.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleDynamicReshape/README.md#2025-04-06_snippet_15\n\nLANGUAGE: C++\nCODE:\n```\nCHECK(cudaMemcpy(mOutput.hostBuffer.data(), mOutput.deviceBuffer.data(), mOutput.deviceBuffer.nbBytes(), cudaMemcpyDeviceToHost));\n```\n\n----------------------------------------\n\nTITLE: Modifying CUDA Kernel Calls for Increased Anchor Support\nDESCRIPTION: Suggests modifications to CUDA kernel calls in maskRCNNKernels.cu to support devices with higher memory capacity and increase the maximum number of anchors beyond 1024.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/detectionLayerPlugin/README.md#2025-04-06_snippet_2\n\nLANGUAGE: CUDA\nCODE:\n```\ncalls to sortPerClass, PerClassNMS and KeepTopKGather can be modified in RefineBatchClassNMS\n```\n\n----------------------------------------\n\nTITLE: Generating Golden Values for ONNX Model\nDESCRIPTION: Commands to generate and save reference outputs using ONNX-Runtime, then combine inputs and outputs into a single layerwise inputs file.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/debug/02_reducing_failing_onnx_models/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run folded.onnx --onnxrt \\\n    --save-inputs inputs.json \\\n    --onnx-outputs mark all --save-outputs layerwise_golden.json\n```\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy data to-input inputs.json layerwise_golden.json -o layerwise_inputs.json\n```\n\n----------------------------------------\n\nTITLE: Adding PriorBox Plugin Source Files in CMake\nDESCRIPTION: This CMake snippet adds the PriorBox plugin implementation files to the build system. It specifies both the implementation (.cpp) and header (.h) files required for the plugin to be compiled as part of the TensorRT plugin library.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/priorBoxPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_plugin_source(\n    priorBoxPlugin.cpp\n    priorBoxPlugin.h\n)\n```\n\n----------------------------------------\n\nTITLE: Calculating Upper Bound for NonZero Output Size\nDESCRIPTION: Defines the upper bound for the number of non-zero elements in the input tensor using dimension operations. This calculates the maximum possible output size as the product of input dimensions.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleNonZeroPlugin/README.md#2025-04-06_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nauto upperBound = exprBuilder.operation(DimensionOperation::kPROD, *inputs[0].d[0], *inputs[0].d[1]);\n```\n\n----------------------------------------\n\nTITLE: Running the TensorRT PyTorch MNIST Sample\nDESCRIPTION: Command to execute the sample application that creates a TensorRT inference engine from a PyTorch model and runs inference on MNIST data.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/network_api_pytorch_mnist/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 sample.py\n```\n\n----------------------------------------\n\nTITLE: Exporting ResNet to ONNX using NVIDIA PyTorch Container\nDESCRIPTION: Docker command to run NVIDIA's PyTorch container and export a ResNet model to ONNX format. This step is optional for users who don't have an ONNX model available.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/deploy_to_triton/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# <xx.xx> is the yy:mm for the publishing tag for NVIDIA's TensorRT \n# container; eg. 24.07\n# Check https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch for latest releases\n\ndocker run -it --gpus all -v /path/to/this/folder:/resnet50_eg nvcr.io/nvidia/pytorch:<xx.xx>-py3\n\npython export_resnet_to_onnx.py\nexit\n```\n\n----------------------------------------\n\nTITLE: Adding EfficientNMS Plugin Source Files to TensorRT Build\nDESCRIPTION: Adds the EfficientNMS plugin source files to the TensorRT build system using the add_plugin_source macro. The source files include CUDA implementation files, header files, and plugin definition files.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/efficientNMSPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_plugin_source(\n    efficientNMSInference.cu\n    efficientNMSInference.cuh\n    efficientNMSInference.h\n    efficientNMSParameters.h\n    efficientNMSPlugin.cpp\n    efficientNMSPlugin.h  \n)\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorRT Calibrator\nDESCRIPTION: Shows how to configure a TensorRT calibrator to inherit from a different base class.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/CHANGELOG.md#2025-04-06_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nCalibrator(base_class=trt.IInt8EntropyCalibrator2)\n```\n\n----------------------------------------\n\nTITLE: Specifying TensorRT Documentation Dependencies\nDESCRIPTION: Defines the required Python packages and their versions for building TensorRT documentation. The dependencies are split based on Python version, with different package versions specified for Python < 3.7 and >= 3.7.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ndocutils==0.16; python_version<\"3.7\"\ndocutils==0.18.1; python_version>=\"3.7\"\nsphinx==4.4.0; python_version<\"3.7\"\nsphinx==7.1.2; python_version>=\"3.7\"\nfuro==2022.4.7; python_version<\"3.7\"\nfuro==2024.8.6; python_version>=\"3.7\"\nmyst-parser==0.16.1; python_version<\"3.7\"\nmyst-parser==3.0.1; python_version>=\"3.7\"\n```\n\n----------------------------------------\n\nTITLE: Setting IPython Backend for Table Display in TREx (Python)\nDESCRIPTION: Demonstrates how to change the backend library used for rendering tables in TREx to IPython, as another alternative to the default dtale backend.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/KNOWN_ISSUES.md#2025-04-06_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntrex.set_table_display_backend(display)\n```\n\n----------------------------------------\n\nTITLE: Describing GeluPlugin Output Structure in Markdown\nDESCRIPTION: Specifies the output tensor shape for the GeluPlugin, which matches the input shape.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/geluPlugin/README.md#2025-04-06_snippet_1\n\nLANGUAGE: Markdown\nCODE:\n```\n`output`\noutput is a tensor with shape `[S, B, E]` where `B` is the batch size.\n```\n\n----------------------------------------\n\nTITLE: Adding EfficientNMS TensorRT Plugin Sources to Build System\nDESCRIPTION: This CMake command adds EfficientNMS plugin source files to the TensorRT build. It includes both explicit and implicit variants of the EfficientNMS implementation, with their corresponding header and implementation files.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/efficientNMSPlugin/tftrt/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_plugin_source(\n    efficientNMSExplicitTFTRTPlugin.cpp\n    efficientNMSExplicitTFTRTPlugin.h\n    efficientNMSImplicitTFTRTPlugin.cpp\n    efficientNMSImplicitTFTRTPlugin.h\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Sphinx Documentation Dependencies with Python Version Constraints\nDESCRIPTION: Requirements file defining documentation dependencies for TensorRT with conditional version specifications based on Python version. It includes Sphinx documentation generator, Furo theme, and MyST Markdown parser, with appropriate versions for Python <3.7 and >=3.7.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/docs/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\nsphinx==4.4.0; python_version<\"3.7\"\nsphinx==7.1.2; python_version>=\"3.7\"\nfuro==2022.4.7; python_version<\"3.7\"\nfuro==2024.8.6; python_version>=\"3.7\"\nmyst-parser==0.16.1; python_version<\"3.7\"\nmyst-parser==3.0.1; python_version>=\"3.7\"\n```\n\n----------------------------------------\n\nTITLE: Running in Fast Speed Mode\nDESCRIPTION: Command to run the custom runner in fast speed mode, which avoids injecting any time.sleep() during inference.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/dev/02_extending_polygraphy_run/README.md#2025-04-06_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run no_op_reshape.onnx --res-des --res-des-speed=fast\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies\nDESCRIPTION: Command to install required Python packages specified in requirements.txt\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/onnx_custom_plugin/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip3 install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Pulling NVIDIA TensorFlow Docker Image\nDESCRIPTION: Command to pull the NVIDIA TensorFlow Docker image that fulfills the TensorRT and TensorFlow version requirements.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/tensorflow_object_detection_api/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull nvcr.io/nvidia/tensorflow:23.07-tf2-py3\n```\n\n----------------------------------------\n\nTITLE: Installing ONNX GraphSurgeon\nDESCRIPTION: Command to manually install the onnx-graphsurgeon Python module if not already installed by TensorRT.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/tensorflow_object_detection_api/README.md#2025-04-06_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install onnx-graphsurgeon==0.3.10 --index-url https://pypi.ngc.nvidia.com\n```\n\n----------------------------------------\n\nTITLE: Generating Custom Polygraphy API Examples Using Command Line\nDESCRIPTION: Shows how to use the `polygraphy run --gen` command to generate Python scripts that use the Polygraphy API. The `-` argument outputs the generated script to stdout.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run --gen - <options...>\n```\n\n----------------------------------------\n\nTITLE: Cloning TensorRT Repository\nDESCRIPTION: Command to clone the TensorRT repository containing the trex tool.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ git clone https://github.com/NVIDIA/TensorRT.git\n```\n\n----------------------------------------\n\nTITLE: Affine Quantization Formula in Markdown\nDESCRIPTION: Mathematical formula explaining how real values are quantized to 8-bit integers using affine quantization with scale and zero point parameters.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/intro_to_quantization.md#2025-04-06_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nx<sub>q</sub>=clamp(round(x/scale)+zeroPt)\n```\n\n----------------------------------------\n\nTITLE: TensorRT Network Output Example\nDESCRIPTION: Example output from the polygraphy inspect command showing the TensorRT network structure. The output displays network name, inputs, outputs, and layers with their operations and shapes.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/inspect/01_inspecting_a_tensorrt_network/README.md#2025-04-06_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n[I] ==== TensorRT Network ====\n    Name: Unnamed Network 0 | Explicit Batch Network\n\n    ---- 1 Network Input(s) ----\n    {x [dtype=float32, shape=(1, 1, 2, 2)]}\n\n    ---- 1 Network Output(s) ----\n    {y [dtype=float32, shape=(1, 1, 2, 2)]}\n\n    ---- 1 Layer(s) ----\n    Layer 0    | node_of_y [Op: LayerType.IDENTITY]\n        {x [dtype=float32, shape=(1, 1, 2, 2)]}\n         -> {y [dtype=float32, shape=(1, 1, 2, 2)]}\n```\n\n----------------------------------------\n\nTITLE: Weight Matrix Transformation Description\nDESCRIPTION: Explanation of how the weight matrix W_qkv is transformed and structured internally within the plugin\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/bertQKVToContextPlugin/README.md#2025-04-06_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nW_tmp = [W_q', W_k', W_v']' -> reshape[E, 3, N, H] -> transpose[E, N, 3, H] -> reshape[E, 3 * E]\n```\n\n----------------------------------------\n\nTITLE: Loading Input Data from JSON File\nDESCRIPTION: Method to load input data using --load-inputs or --load-input-data flags. Data should be stored as a List of Dictionaries mapping strings to numpy arrays, saved using Polygraphy's JSON utilities.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/how-to/use_custom_input_data.md#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nDict[str, np.ndarray]\n```\n\n----------------------------------------\n\nTITLE: Accessing the Engine Plan DataFrame\nDESCRIPTION: Code that extracts the DataFrame from the EnginePlan object. This DataFrame contains all the detailed information about the engine layers and their properties.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/tutorial.ipynb#2025-04-06_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndf = plan.df\n```\n\n----------------------------------------\n\nTITLE: Installing AutoML Requirements\nDESCRIPTION: This command installs additional Python packages required for the AutoML repository, including matplotlib and TensorFlow model optimization tools.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/EfficientDet/notebooks/EfficientDet-TensorRT8.ipynb#2025-04-06_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n!pip3 install matplotlib>=3.0.3 PyYAML>=5.1 tensorflow-model-optimization>=0.5\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for TensorRT PyTorch MNIST Example\nDESCRIPTION: Commands to upgrade pip and install the required dependencies for running the TensorRT PyTorch MNIST sample.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/network_api_pytorch_mnist/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip3 install --upgrade pip\npip3 install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Bounding Box Encoding for CodeTypeSSD::TF_CENTER Method in TensorRT\nDESCRIPTION: Mathematical formulation for the TF_CENTER coding method used in nmsPlugin. This encoding method is similar to CENTER_SIZE but with a different coordinate order, used primarily for TensorFlow compatibility.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/nmsPlugin/README.md#2025-04-06_snippet_3\n\nLANGUAGE: math\nCODE:\n```\n[(y_{center, gt} - y_{center, anchor}) / h_{anchor} / variance_0, (x_{center, gt} - x_{center, anchor}) / w_{anchor} / variance_1, ln(h_{gt} / h_{anchor}) / variance_2, ln(w_{gt} / w_{anchor}) / variance_3]\n```\n\n----------------------------------------\n\nTITLE: Installing Hugging Face Transformers and PyTorch Dependencies\nDESCRIPTION: Installs the required dependencies including a specific version of Hugging Face Transformers library and PyTorch with CUDA support for running BERT models.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/BERT-TRT-FP16.ipynb#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n!cd /tmp && git clone https://github.com/vinhngx/transformers && cd transformers && pip install .\n!pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n```\n\n----------------------------------------\n\nTITLE: Affine Quantization Formula in Markdown\nDESCRIPTION: Mathematical formula explaining how real values are quantized to 8-bit integers using affine quantization with scale and zero point parameters.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/intro_to_quantization.md#2025-04-06_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nx<sub>q</sub>=clamp(round(x/scale)+zeroPt)\n```\n\n----------------------------------------\n\nTITLE: Exporting EfficientNet V1 SavedModel\nDESCRIPTION: Command to export a TensorFlow SavedModel from an EfficientNet V1 checkpoint.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/efficientnet/README.md#2025-04-06_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncd /path/to/tpu/models/official/efficientnet\npython3 export_model.py \\\n    --ckpt_dir /path/to/efficientnet-b0 \\\n    --image_size 224 \\\n    --model_name efficientnet-b0 \\\n    --output_tflite /dev/null \\\n    --noquantize \\\n    --output_saved_model_dir /path/to/saved_model\n```\n\n----------------------------------------\n\nTITLE: Downloading EfficientNet V2 Model\nDESCRIPTION: Command to download pre-trained EfficientNet V2 checkpoint.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/efficientnet/README.md#2025-04-06_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nwget https://storage.googleapis.com/cloud-tpu-checkpoints/efficientnet/v2/efficientnetv2-s.tgz\n```\n\n----------------------------------------\n\nTITLE: Inspecting TensorFlow Frozen Model with Polygraphy\nDESCRIPTION: Command to inspect a TensorFlow frozen model using Polygraphy's inspect model subtool. The command displays graph information including inputs, outputs, and node count.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/inspect/04_inspecting_a_tensorflow_graph/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy inspect model identity.pb --model-type=frozen\n```\n\nLANGUAGE: text\nCODE:\n```\n[I] ==== TensorFlow Graph ====\n    ---- 1 Graph Inputs ----\n    {Input:0 [dtype=float32, shape=(1, 15, 25, 30)]}\n\n    ---- 1 Graph Outputs ----\n    {Identity_2:0 [dtype=float32, shape=(1, 15, 25, 30)]}\n\n    ---- 4 Nodes ----\n```\n\n----------------------------------------\n\nTITLE: Defining RST Role for Hidden Sections\nDESCRIPTION: Creates a custom RST role 'hidden' for marking hidden documentation sections.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/calib.rst#2025-04-06_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. role:: hidden\n    :class: hidden-section\n```\n\n----------------------------------------\n\nTITLE: Adding FlattenConcat Plugin Source Files\nDESCRIPTION: CMake command to add the FlattenConcat plugin source files (cpp and header) to the build system. This plugin likely implements tensor flattening and concatenation operations in TensorRT.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/flattenConcat/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_plugin_source(\n    flattenConcat.cpp\n    flattenConcat.h\n)\n```\n\n----------------------------------------\n\nTITLE: TensorRT Benchmark UI Setup\nDESCRIPTION: Creates an interactive UI using IPython widgets to control benchmark parameters including engine type (FP16/INT8), batch size, and number of iterations. Includes a progress bar and output display area.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/benchmark.ipynb#2025-04-06_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# UI elements\nengine_selector = widgets.RadioButtons(\n    options=['GPU - TensorRT FP16',\n             'GPU - TensorRT INT8'],\n    description='Engine:',\n    disabled=False\n)\n\nbatchsize_selector = widgets.RadioButtons(\n    options=['1', '32', '64', '128'],\n    description='Batch size:',\n    disabled=False\n)\n\niteration_selector = widgets.IntSlider(\n    value=500,\n    min=100,\n    max=1000,\n    step=1,\n    description='Iterations:',\n    disabled=False,\n    continuous_update=False,\n    orientation='horizontal',\n    readout=True,\n    readout_format='d'\n)\n\nbutton = widgets.Button(description=\"Run benchmark\")\noutput = widgets.Output()\n#box = widgets.HBox(children=[button],layout=box_layout)\n\ndef run_benchmark(b):\n    args.iterations = iteration_selector.value\n    progress_bar.max = iteration_selector.value\n    with output:\n        if engine_selector.value=='GPU - TensorRT FP16':\n            run_benchmark_FP16(b)\n        elif engine_selector.value=='GPU - TensorRT INT8':\n            run_benchmark_INT8(b)\n        \nbutton.on_click(run_benchmark)\ndisplay(engine_selector, batchsize_selector, iteration_selector)\n\nfrom IPython.display import display\nbox_layout = widgets.Layout(display='flex',\n                flex_flow='column',\n                align_items='center',\n                width='100%')\nbox = widgets.HBox(children=[button],layout=box_layout)\ndisplay(box, output)\n\nprogress_bar = widgets.IntProgress(\n    value=0,\n    min=0,\n    max=1000,\n    description='Progress:',\n    bar_style='',\n    style={'bar_color': 'green'},\n    orientation='horizontal', \n    layout=Layout(width='100%', height='50px')\n)\ndisplay(progress_bar)\n```\n\n----------------------------------------\n\nTITLE: Analyzing Layer Tactics\nDESCRIPTION: Creates visualizations and statistics for layer tactics usage in the engine plan\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/engine_report_card.ipynb#2025-04-06_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nlatency_vs_prec_per_conv = partial(\n    plotting.plotly_bar2,\n    plan.df,\n    values_col='latency.pct_time',\n    names_col='Name',\n    color='tactic')\n\nlatency_vs_prec_per_conv(\"Latency per Layer (color=Tactics)\")\n\ntactic_cnt = group_count(plan.df, 'tactic')\ndisplay_df(tactic_cnt)\n```\n\n----------------------------------------\n\nTITLE: Running the Triton Client for Model Inference\nDESCRIPTION: Command to execute the Python client script that connects to the Triton server, sends the sample image, and receives inference results. This demonstrates the end-to-end workflow from client to server.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/deploy_to_triton/README.md#2025-04-06_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npython3 triton_client.py\n```\n\n----------------------------------------\n\nTITLE: Creating and Training Original FP32 Model\nDESCRIPTION: This snippet creates the original ResNet-like model, compiles it, and trains it on the CIFAR10 dataset.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/notebooks/simple_network_quantize_partial.ipynb#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nnn_model_original = tiny_resnet.model()\ntf.keras.utils.plot_model(nn_model_original, to_file = assets.simple_network_quantize_partial.fp32 + \"/model.png\")\n\n# Train original classification model\nnn_model_original.compile(\n    optimizer=tiny_resnet.optimizer(lr=1e-4),\n    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    metrics=[\"accuracy\"],\n)\n\nnn_model_original.fit(\n    train_images, train_labels, batch_size=32, epochs=10, validation_split=0.1\n)\n\n# Get baseline model accuracy\n_, baseline_model_accuracy = nn_model_original.evaluate(\n    test_images, test_labels, verbose=0\n)\nbaseline_model_accuracy = round(100 * baseline_model_accuracy, 2)\nprint(\"Baseline FP32 model test accuracy:\", baseline_model_accuracy)\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for BERT QAT (Xavier GPU)\nDESCRIPTION: This command builds a TensorRT engine for BERT QAT on Xavier GPU. It supports SkipLayerNormPlugin running with INT8 I/O.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p engines && python3 builder.py -o engines/bert_large_384_int8mix.engine -b 1 -s 384 --int8 --fp16 --strict -c models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1 -v models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/vocab.txt -x models/fine-tuned/bert_pyt_onnx_large_qa_squad11_amp_fake_quant_v1/bert_large_v1_1_fake_quant.onnx -iln\n```\n\n----------------------------------------\n\nTITLE: Displaying Polygraphy Inspect Help\nDESCRIPTION: Command to display usage information for the Polygraphy inspect tool.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/polygraphy/tools/inspect/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy inspect -h\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorRT ONNX MNIST Sample Build in CMake\nDESCRIPTION: Configures the build process for the ONNX MNIST sample application by setting the source files and parser requirements. This CMake snippet specifies sampleOnnxMNIST.cpp as the source file and sets the ONNX parser as a dependency, then includes a common template for TensorRT samples.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleOnnxMNIST/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(SAMPLE_SOURCES sampleOnnxMNIST.cpp)\n\nset(SAMPLE_PARSERS \"onnx\")\n\ninclude(../CMakeSamplesTemplate.txt)\n```\n\n----------------------------------------\n\nTITLE: Adding Scatter Layer Plugin Sources in CMake\nDESCRIPTION: This CMake instruction adds the necessary source files for the Scatter Layer plugin to the build process. It includes a CUDA source file, a C++ source file, and a header file.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/scatterPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_plugin_source(\n    scatterLayer.cu\n    scatterPlugin.cpp\n    scatterPlugin.h\n)\n```\n\n----------------------------------------\n\nTITLE: Adding Proposal Plugin Source Files in CMake for TensorRT\nDESCRIPTION: This CMake snippet adds the proposal plugin source files to the TensorRT build system. It specifies both the implementation file (proposalPlugin.cpp) and header file (proposalPlugin.h) to be included in the plugin compilation process.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/proposalPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_plugin_source(\n    proposalPlugin.cpp\n    proposalPlugin.h\n)\n```\n\n----------------------------------------\n\nTITLE: Conditional Source File Addition Function in CMake\nDESCRIPTION: CMake function that adds plugin source files only if they exist in the current directory. Takes a variable number of source file names as arguments and checks each one before adding it to the build.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/bertQKVToContextPlugin/fused_multihead_attention/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(add_plugin_source_if_exists)\n    foreach(SRC_FILE IN LISTS ARGN)\n        if(EXISTS ${CMAKE_CURRENT_LIST_DIR}/${SRC_FILE})\n            add_plugin_source(${SRC_FILE})\n        endif()\n    endforeach()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Conditional Message Function in CMake\nDESCRIPTION: A function that overrides the default message function to only output messages when VERBOSE is enabled, allowing for control of output verbosity.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/python/CMakeLists.txt#2025-04-06_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(message)\n    if(VERBOSE)\n        _message(${ARGN})\n    endif()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Specifying NumPy Dependency for TensorRT in Python\nDESCRIPTION: This snippet lists numpy as a required Python package for the TensorRT project. NumPy is likely used for numerical computations and array operations within the TensorRT framework.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/02_validating_on_a_dataset/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nnumpy\n```\n\n----------------------------------------\n\nTITLE: Running TensorFlow Weights Conversion Script\nDESCRIPTION: Command to extract weights from TensorFlow checkpoint files into a format TensorRT can use.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleCharRNN/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndumpTFWts.py -m /path/to/checkpoint -o /path/to/output\n```\n\n----------------------------------------\n\nTITLE: Setting Default Paths for TensorRT and CUDA Dependencies\nDESCRIPTION: Configures default paths for TensorRT libraries, TensorRT include files, and CUDA directories on non-Windows systems. These paths are used when explicit paths aren't provided.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/onnx_custom_plugin/CMakeLists.txt#2025-04-06_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\n# -------- CONFIGURATION --------\nif(NOT MSVC)\n    set_ifndef(TRT_LIB /usr/lib/x86_64-linux-gnu)\n    set_ifndef(TRT_INCLUDE /usr/include/x86_64-linux-gnu)\n    set_ifndef(CUDA_INC_DIR /usr/local/cuda/include)\n\n    set_ifndef(CUDA_LIB_DIR /usr/local/cuda)\n    set_ifndef(CUBLAS_LIB_SUFFIXES \"lib;lib64\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Dependencies for NVIDIA TensorRT\nDESCRIPTION: This snippet lists the required Python packages and their versions for the NVIDIA TensorRT project. It includes conditional dependencies for different Python versions and operating systems, ensuring compatibility across various environments.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/yolov3_onnx/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ncuda-python==12.2.0; python_version <= \"3.10\"\ncuda-python==12.6.0; python_version >= \"3.11\"\npywin32; platform_system == \"Windows\"\nnumpy==1.24.4; python_version <= \"3.10\"\nnumpy==1.26.4; python_version >= \"3.11\"\nonnx==1.16.0\nPillow>=10.0.0\nprotobuf==3.20.3\npyyaml==6.0.1\nrequests==2.32.2\ntqdm==4.66.4\n```\n\n----------------------------------------\n\nTITLE: Determining Supported CUDA Architectures Based on CUDA Version\nDESCRIPTION: Configures which CUDA Compute Capabilities (SM versions) are supported based on the detected CUDA version. Different CUDA versions support different GPU architectures.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/python_plugin/CMakeLists.txt#2025-04-06_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\n# Loop through minor versions from 0 to 9\nforeach(minor RANGE 0 9)\n    set(result_var \"CUDA_GE_11_${minor}\")\n    cuda_ge(11 ${minor} ${result_var})\nendforeach()\n\nset(SAMPLE_SMS \"75\")\n\nif(CUDA_GE_11_0)\n    list(APPEND SAMPLE_SMS \"80\")\nendif()\n\nif(CUDA_GE_11_1)\n    list(APPEND SAMPLE_SMS \"86\")\nendif()\n\nif(CUDA_GE_11_4)\n    list(APPEND SAMPLE_SMS \"87\")\nendif()\n\nif(CUDA_GE_11_8)\n    list(APPEND SAMPLE_SMS \"89\" \"90\")\nendif()\n\nset(NON_HFC_SMS \"89\" \"90\")\n```\n\n----------------------------------------\n\nTITLE: Displaying DataFrame Column Names\nDESCRIPTION: Code to print the names of all columns in the engine plan DataFrame, which shows what information is available for analysis.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/tutorial.ipynb#2025-04-06_snippet_6\n\nLANGUAGE: python\nCODE:\n```\navailable_cols = df.columns\nprint(f\"These are the column names in the plan\\n: {available_cols}\")\n```\n\n----------------------------------------\n\nTITLE: Final Model Reduction Using Linear Mode\nDESCRIPTION: Commands for further model reduction using linear mode, including both actual and simulated failure cases.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/debug/02_reducing_failing_onnx_models/README.md#2025-04-06_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy debug reduce initial_reduced.onnx -o final_reduced.onnx --mode=linear --load-inputs layerwise_inputs.json \\\n    --check polygraphy run polygraphy_debug.onnx --trt \\\n            --load-inputs layerwise_inputs.json --load-outputs layerwise_golden.json\n```\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy debug reduce initial_reduced.onnx -o final_reduced.onnx --mode=linear \\\n    --fail-regex \"Op: Mul\" \\\n    --check polygraphy inspect model polygraphy_debug.onnx --show layers\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for TensorRT\nDESCRIPTION: Lists required Python packages with specific versions for the TensorRT project. This includes basic utility packages like pyyaml, requests, and tqdm, along with conditional numpy requirements based on the Python version being used.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\npyyaml==6.0.1\nrequests==2.32.2\ntqdm==4.66.4\nnumpy==1.24.4; python_version <= \"3.10\"\nnumpy==1.26.4; python_version >= \"3.11\"\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for TensorRT Refitting Example\nDESCRIPTION: Command to install the required Python dependencies listed in requirements.txt file for the TensorRT Engine Refitting sample.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/engine_refit_onnx_bidaf/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip3 install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Output Tensor Structure Example\nDESCRIPTION: Example showing the structure of output tensor with shape [2, 3, 2, 2] after applying batchTilePlugin\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/batchTilePlugin/README.md#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n[[[[16 17]\n   [18 19]]\n\n  [[20 21]\n   [22 23]]\n \n  [[24 25]\n   [26 27]]]\n   \n   \n [[[16 17]\n   [18 19]]\n\n  [[20 21]\n   [22 23]]\n \n  [[24 25]\n   [26 27]]]]\n```\n\n----------------------------------------\n\nTITLE: Converting Model and Input to FP16 Precision\nDESCRIPTION: Converts both the ResNet50 model and input tensors to half precision (FP16) and performs a warm-up inference.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/2. Using PyTorch through ONNX.ipynb#2025-04-06_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nresnet50_gpu_half = resnet50_gpu.half()\ninput_half = input_batch_gpu.half()\n\nwith torch.no_grad():\n    preds = np.array(resnet50_gpu_half(input_half).cpu()) # Warm Up\n    \npreds.shape\n```\n\n----------------------------------------\n\nTITLE: Final Model Inspection\nDESCRIPTION: Command to verify the final reduced model contains only the failing node.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/debug/02_reducing_failing_onnx_models/README.md#2025-04-06_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy inspect model final_reduced.onnx --show layers\n```\n\n----------------------------------------\n\nTITLE: Creating EnginePlan Instance from JSON Files\nDESCRIPTION: Code that creates an EnginePlan instance by loading graph, profile, and metadata JSON files. This is the core object that will be used for engine analysis.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/tutorial.ipynb#2025-04-06_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nassert engine_name is not None\nplan = trex.EnginePlan(f'{engine_name}.graph.json', f'{engine_name}.profile.json', f'{engine_name}.profile.metadata.json')\n```\n\n----------------------------------------\n\nTITLE: Downloading BERT Large FP16 SQuAD v2 Model Checkpoints\nDESCRIPTION: This command downloads Tensorflow checkpoints for a BERT Large FP16 SQuAD v2 model with a sequence length of 384.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nbash scripts/download_model.sh large 384 v2\n```\n\n----------------------------------------\n\nTITLE: CUDA Version Extraction from nvcc\nDESCRIPTION: Executes the nvcc compiler to extract the CUDA version and parses the output using regex. This determines which CUDA architectures will be supported in the build.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/python_plugin/CMakeLists.txt#2025-04-06_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\n# extract CUDA version\nif(NVCC_EXECUTABLE)\n    execute_process(\n        COMMAND \"${NVCC_EXECUTABLE}\" --version\n        OUTPUT_VARIABLE NVCC_VERSION_OUTPUT\n        ERROR_VARIABLE NVCC_VERSION_ERROR\n        OUTPUT_STRIP_TRAILING_WHITESPACE)\n    # Parse the version number from the output\n    string(REGEX MATCH \"release ([0-9]+)\\\\.([0-9]+)\" CUDA_VERSION_MATCH \"${NVCC_VERSION_OUTPUT}\")\n    if(CUDA_VERSION_MATCH)\n        set(CUDA_VERSION_MAJOR \"${CMAKE_MATCH_1}\")\n        set(CUDA_VERSION_MINOR \"${CMAKE_MATCH_2}\")\n        set(CUDA_VER \"${CUDA_VERSION_MAJOR}.${CUDA_VERSION_MINOR}\")\n    else()\n        message(FATAL_ERROR \"Could not parse CUDA version from nvcc output.\")\n    endif()\nelse()\n    message(FATAL_ERROR \"nvcc not found in ${CUDA_INST_DIR}/bin\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Including CMake Template for TensorRT Samples\nDESCRIPTION: Includes the common CMake template file used across TensorRT samples for standardized build configuration.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleIOFormats/CMakeLists.txt#2025-04-06_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ninclude(../CMakeSamplesTemplate.txt)\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for TensorRT\nDESCRIPTION: A requirements file listing the Python packages needed for TensorRT development. It includes imaging libraries (Pillow), CUDA bindings, ONNX frameworks including NVIDIA's custom graphsurgeon package, and version-specific NumPy requirements that vary based on Python version.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/dds_faster_rcnn/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\npillow\ncuda-python\nonnx\nonnx-graphsurgeon --index-url https://pypi.ngc.nvidia.com\nnumpy==1.24.4; python_version <= \"3.10\"\nnumpy==1.26.4; python_version >= \"3.11\"\n```\n\n----------------------------------------\n\nTITLE: Running Additional BERT QA Inference Example\nDESCRIPTION: Demonstrates another inference example with a different question but using the same passage and optimized TensorRT engine. Shows how to query different information from the same context.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/BERT-TRT-INT8-QAT-sparse.ipynb#2025-04-06_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nQUESTION=\"What is included in TensorRT?\"\n!python3 ../inference_varseqlen.py -e engines_$TRT_VERSION/megatron_large_seqlen384_int8qat_sparse.engine  -p $PASSAGE -q $QUESTION -v models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/vocab.txt -s 256\n```\n\n----------------------------------------\n\nTITLE: Configuring Dependencies and Installation in CMake for TensorRT Plugins\nDESCRIPTION: Sets up dependencies between targets and configures installation rules for the shared libraries.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/CMakeLists.txt#2025-04-06_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nadd_dependencies(plugin ${SHARED_TARGET} ${STATIC_TARGET} ${VFC_SHARED_TARGET})\n\ninstall(\n    TARGETS ${SHARED_TARGET} ${VFC_SHARED_TARGET}\n    RUNTIME DESTINATION bin\n    LIBRARY DESTINATION lib\n    ARCHIVE DESTINATION lib)\n```\n\n----------------------------------------\n\nTITLE: Loading TensorRT Network Example\nDESCRIPTION: Example of defining a load_network function in a Python script to create TensorRT networks programmatically. This function should take no arguments and return a TensorRT builder, network and optionally parser.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/CHANGELOG.md#2025-04-06_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ndef load_network():\n    # Should return (builder, network, parser)\n    # See examples/cli/run/04_defining_a_tensorrt_network_or_config_manually/ for details\n```\n\n----------------------------------------\n\nTITLE: Demonstrating CLI Input Shape Syntax\nDESCRIPTION: Shows the old and new syntax for specifying input shapes and data types in Polygraphy CLI tools. The new syntax uses colons and brackets while the old style uses commas and 'x' notation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/CHANGELOG.md#2025-04-06_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n--inputs input0:[3,4]:int64 input1:[4,64,64]:float32\n```\n\nLANGUAGE: bash\nCODE:\n```\n--inputs input0,3x4,int64 input1,4x64x64,float32\n```\n\n----------------------------------------\n\nTITLE: Running BERT QA Inference with Alternative Question\nDESCRIPTION: Demonstrates the flexibility of the BERT QA model by running inference on the same passage with a different question: \"What is included in TensorRT?\"\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/BERT-TRT-FP16.ipynb#2025-04-06_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nQUESTION=\"What is included in TensorRT?\"\n\n!python3 ../inference.py -e engines_$TRT_VERSION/bert_large_384.engine -s 384 -p $PASSAGE -q $QUESTION -v models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/vocab.txt\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine\nDESCRIPTION: Command to build the TensorRT engine from the modified ONNX model using a Python script.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/dds_faster_rcnn/README.md#2025-04-06_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython3 build_engine.py \\\n    --onnx ./fasterrcnn12_trt.onnx \\\n    --engine ./fasterrcnn12_trt.engine\n```\n\n----------------------------------------\n\nTITLE: Loading TensorRT Engine Plan JSON Files\nDESCRIPTION: Creates an EnginePlan object by loading graph, profile, and metadata JSON files for analysis\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/engine_report_card.ipynb#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nplan = EnginePlan(f\"{engine_name}.graph.json\", f\"{engine_name}.profile.json\", f\"{engine_name}.profile.metadata.json\")\n```\n\n----------------------------------------\n\nTITLE: Creating TensorRT Samples Build Target\nDESCRIPTION: Creates a custom CMake target named 'samples' that will be used to build all TensorRT sample projects.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_target(samples)\n```\n\n----------------------------------------\n\nTITLE: Using Preprocessor Conditionals in C++\nDESCRIPTION: Demonstrates the preferred way to use preprocessor conditionals, using #if defined(...) instead of #ifdef for more complex conditions.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/CODING-GUIDELINES.md#2025-04-06_snippet_13\n\nLANGUAGE: cpp\nCODE:\n```\n#if defined(FOO) || defined(BAR)\nvoid foo();\n#endif // defined(FOO) || defined(BAR)\n```\n\n----------------------------------------\n\nTITLE: Adding LeakyReLU Plugin Source Files in CMake\nDESCRIPTION: A CMake command that adds the LeakyReLU plugin source files (lReluPlugin.cpp and lReluPlugin.h) to the TensorRT project build. This ensures these files are compiled and linked appropriately within the TensorRT plugin system.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/leakyReluPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_plugin_source(\n    lReluPlugin.cpp\n    lReluPlugin.h\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Data Loader Module in Python\nDESCRIPTION: This snippet demonstrates how to import the data_loader module from the polygraphy.comparator package. It uses Python's automodule directive to automatically generate documentation for the module.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/comparator/data_loader.rst#2025-04-06_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. automodule:: polygraphy.comparator.data_loader\n```\n\n----------------------------------------\n\nTITLE: Running Inference for Accuracy Evaluation\nDESCRIPTION: Shell script for running inference on a TensorRT engine to evaluate its accuracy without measuring performance metrics.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/examples/resnet/scripts/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./scripts/infer_engine.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring Build Settings for ONNX MNIST CoordConvAC Sample in CMake\nDESCRIPTION: This snippet defines the build configuration for the sampleOnnxMnistCoordConvAC application. It specifies the source file, sets ONNX as the required parser, enables plugins, and includes a common CMake template for TensorRT samples.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleOnnxMnistCoordConvAC/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(SAMPLE_SOURCES sampleOnnxMnistCoordConvAC.cpp)\n\nset(SAMPLE_PARSERS \"onnx\")\nset(PLUGINS_NEEDED ON)\n\ninclude(../CMakeSamplesTemplate.txt)\n```\n\n----------------------------------------\n\nTITLE: Setting TensorRT Sample Source Files in CMake\nDESCRIPTION: Defines the source files required for building TensorRT samples, including common utilities and the main trtexec executable. The sources include device handling, engine management, inference, options parsing, reporting, and utility functions.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/trtexec/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(SAMPLE_SOURCES\n    ../common/sampleDevice.cpp\n    ../common/sampleEngines.cpp\n    ../common/sampleInference.cpp\n    ../common/sampleOptions.cpp\n    ../common/sampleReporting.cpp\n    ../common/sampleUtils.cpp\n    ../common/bfloat16.cpp\n    trtexec.cpp)\n```\n\n----------------------------------------\n\nTITLE: Accessing Polygraphy Config as Top-Level Import\nDESCRIPTION: In v0.30.2, Polygraphy config was made available as a top-level import, making it no longer necessary to import it separately.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/CHANGELOG.md#2025-04-06_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom polygraphy import config\n```\n\n----------------------------------------\n\nTITLE: Importing ONNX Loader Module in Python\nDESCRIPTION: This snippet demonstrates how to import the ONNX loader module from the Polygraphy backend. It uses Python's automodule directive to include all inherited members of the module.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/backend/onnx/loader.rst#2025-04-06_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. automodule:: polygraphy.backend.onnx.loader\n    :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Symmetric Quantization Formula in Markdown\nDESCRIPTION: Formula for TensorRT's symmetric quantization where zero point is 0, simplifying the quantization process.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/intro_to_quantization.md#2025-04-06_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\nx<sub>q</sub>=clamp(round(x/scale))\n```\n\n----------------------------------------\n\nTITLE: Generating custom input and output data with Python\nDESCRIPTION: Command to run the Python script that generates custom input and output data files that will be used for comparison later.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/run/06_comparing_with_custom_output_data/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 generate_data.py\n```\n\n----------------------------------------\n\nTITLE: Setting Batch Size for TensorRT Engine\nDESCRIPTION: Sets the batch size to 32 for TensorRT engine, consistent with the ONNX export phase.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/2. Using PyTorch through ONNX.ipynb#2025-04-06_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nBATCH_SIZE = 32\n```\n\n----------------------------------------\n\nTITLE: Executing trtexec Command Parameters for JSON Export\nDESCRIPTION: Command line flags required for trtexec to export engine plan graph and profiling data to JSON files. These flags enable detailed profiling information export.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/tutorial.ipynb#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n --exportProfile=$profile_json\n --exportLayerInfo=$graph_json\n --profilingVerbosity=detailed\n```\n\n----------------------------------------\n\nTITLE: Benchmarking FP32 Inference Performance\nDESCRIPTION: Measures the time taken for inference in FP32 precision using the %%timeit magic command.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/2. Using PyTorch through ONNX.ipynb#2025-04-06_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n%%timeit\n\nwith torch.no_grad():\n    preds = np.array(resnet50_gpu(input_batch_gpu).cpu())\n```\n\n----------------------------------------\n\nTITLE: Modifying ONNX Model for TensorRT\nDESCRIPTION: Command to run a Python script that modifies the downloaded ONNX model to prepare it for TensorRT engine conversion.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/dds_faster_rcnn/README.md#2025-04-06_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 modify_onnx.py \\\n    --input ./FasterRCNN-12.onnx \\\n    --output ./fasterrcnn12_trt.onnx\n```\n\n----------------------------------------\n\nTITLE: Launching Jupyter Notebook for Interactive BERT Inference\nDESCRIPTION: Starts a Jupyter Notebook server to run interactive BERT inference with example passages and questions.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\njupyter notebook --ip 0.0.0.0 inference.ipynb\n```\n\n----------------------------------------\n\nTITLE: Importing trex Libraries and Configuring Environment\nDESCRIPTION: Setup code that imports necessary libraries for trex analysis and plotting. It configures display settings and sets paths to engine files for analysis.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/tutorial.ipynb#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport os\nimport pandas as pd\nimport trex\nimport trex.notebook\nimport trex.plotting\nimport trex.graphing\nimport trex.df_preprocessing\n\n# Configure a wider output (for the wide graphs)\ntrex.notebook.set_wide_display()\n\n# Choose an engine file to load.  This notebook assumes that you've saved the engine to the following paths.\nengine_name = \"../tests/inputs/mobilenet.qat.onnx.engine\"\nengine_name = \"../tests/inputs/mobilenet_v2_residuals.qat.onnx.engine\"\n```\n\n----------------------------------------\n\nTITLE: Importing TensorFlow Loader Module in Python\nDESCRIPTION: This code snippet demonstrates how to import the TensorFlow loader module from the Polygraphy backend. It uses the automodule directive to include inherited members.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/backend/tf/loader.rst#2025-04-06_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. automodule:: polygraphy.backend.tf.loader\n    :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Adding TensorRT Sample Subdirectories\nDESCRIPTION: Iterates through the list of sample projects and adds each one as a subdirectory to the CMake build system using a foreach loop.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/CMakeLists.txt#2025-04-06_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nforeach(SAMPLE_ITER ${OPENSOURCE_SAMPLES_LIST})\n    add_subdirectory(${SAMPLE_ITER})\nendforeach(SAMPLE_ITER)\n```\n\n----------------------------------------\n\nTITLE: Defining TensorRT Polygraphy Module Structure in reStructuredText\nDESCRIPTION: This snippet defines the structure of the polygraphy.tools.args module documentation using reStructuredText syntax. It includes a main heading and a table of contents directive for two submodules: loader and runner.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/tool/args/backend/trt/toc.rst#2025-04-06_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n=========\nTensorRT\n=========\n\nModule: ``polygraphy.tools.args``\n\n.. toctree::\n    loader\n    runner\n```\n\n----------------------------------------\n\nTITLE: Specifying Data Loader Dependency\nDESCRIPTION: Lists the Pillow library as a requirement for the data loader component, which is commonly used for image processing tasks.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/examples/requirements.txt#2025-04-06_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n# Data loader\nPillow\n```\n\n----------------------------------------\n\nTITLE: Documenting Polygraphy Backend Common Module in reStructuredText\nDESCRIPTION: This reStructuredText snippet defines documentation for the common module in Polygraphy's backend. It uses Sphinx directives to specify the module name and includes a toctree directive to link to a loader submodule.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/backend/common/toc.rst#2025-04-06_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\nModule: ``polygraphy.backend.common``\n\n.. toctree::\n    loader\n```\n\n----------------------------------------\n\nTITLE: Zero Point Calculation Formula in Markdown\nDESCRIPTION: Formula for calculating the zero point parameter in affine quantization, which shifts the quantized values to represent the original range correctly.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/intro_to_quantization.md#2025-04-06_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\nzeroPt = -round(&beta; * scale) - 2<sup>b-1</sup>\n```\n\n----------------------------------------\n\nTITLE: Defining RST Documentation Structure for Polygraphy Backend\nDESCRIPTION: ReStructuredText documentation defining the module structure and table of contents for the polygraphy.backend module, including various backend implementations.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/backend/toc.rst#2025-04-06_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n========\nBackends\n========\n\nModule: ``polygraphy.backend``\n\n.. toctree::\n    base/toc\n    common/toc\n    onnx/toc\n    onnxrt/toc\n    pluginref/toc\n    tf/toc\n    trt/toc\n```\n\n----------------------------------------\n\nTITLE: Generating a TensorRT Tactic Replay File with Polygraphy\nDESCRIPTION: This command uses Polygraphy to run an ONNX model with TensorRT while saving the tactics used to a JSON file. The saved tactics can be inspected later to understand optimization decisions.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/inspect/07_inspecting_tactic_replays/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run model.onnx --trt --save-tactics replay.json\n```\n\n----------------------------------------\n\nTITLE: Importing Lazy Module with Autoinstall Support\nDESCRIPTION: Example of how nested module imports work with POLYGRAPHY_AUTOINSTALL_DEPS feature, which had a bugfix in v0.30.1.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/CHANGELOG.md#2025-04-06_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nmod.lazy_import(\"onnx.shape_inference\")\n```\n\n----------------------------------------\n\nTITLE: Converting ONNX Model to FP16\nDESCRIPTION: Command to convert an ONNX model from FP32 to FP16 precision using Polygraphy's convert tool.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/convert/04_converting_models_to_fp16/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy convert --fp-to-fp16 -o identity_fp16.onnx identity.onnx\n```\n\n----------------------------------------\n\nTITLE: Installing EfficientDet Dependencies\nDESCRIPTION: This command installs the required Python packages for the EfficientDet sample using pip.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/EfficientDet/notebooks/EfficientDet-TensorRT8.ipynb#2025-04-06_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n!pip3 install -r $TRT_OSSPATH/samples/python/efficientdet/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Viewing Command-line Help with Extension\nDESCRIPTION: Command to display help information for polygraphy run, which should now include the custom options from the extension.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/dev/02_extending_polygraphy_run/README.md#2025-04-06_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run -h\n```\n\n----------------------------------------\n\nTITLE: RST Module Documentation for Maximum Reduction\nDESCRIPTION: Autodoc directive for documenting the reduce_amax utility module.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/utils.rst#2025-04-06_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: pytorch_quantization.utils.reduce_amax\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Specifying TensorFlow Version Constraint\nDESCRIPTION: Defines version constraint to ensure TensorFlow version compatibility is maintained below 2.0 for NVIDIA TensorRT integration.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/polygraphy/backend/tf/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntensorflow<2.0\n```\n\n----------------------------------------\n\nTITLE: Importing Polygraphy Data Type Module\nDESCRIPTION: Import statement for the polygraphy.datatype module which provides data type functionality for TensorRT operations.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/datatype/toc.rst#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom polygraphy.datatype import datatype\n```\n\n----------------------------------------\n\nTITLE: Importing Data Loader Module in Python\nDESCRIPTION: This code snippet shows how to import the data loader module from the Polygraphy tools argument parsing system. It is used to load and process data for model comparison tasks in TensorRT.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/tool/args/comparator/data_loader.rst#2025-04-06_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom polygraphy.tools.args.comparator import data_loader\n```\n\n----------------------------------------\n\nTITLE: Plugin Tool Command Example\nDESCRIPTION: Example command for accessing the plugin tool help documentation via Polygraphy CLI\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/polygraphy/tools/plugin/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy plugin -h\n```\n\n----------------------------------------\n\nTITLE: Marking Slow Tests in Python with pytest\nDESCRIPTION: This snippet shows how to use the pytest.mark.slow decorator to mark long-running tests. These tests will only be executed when the RUN_ALL_TESTS make option is enabled.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/tests/README.md#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@pytest.mark.slow\ndef my_long_running_test():\n    ...\n```\n\n----------------------------------------\n\nTITLE: Module Path Definition in RST\nDESCRIPTION: Defines the module path for the polygraphy.tools.args package and lists the available backend documentation sections using restructuredtext format.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/tool/args/backend/toc.rst#2025-04-06_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\nModule: ``polygraphy.tools.args``\n\n.. toctree::\n    onnx/toc\n    onnxrt/toc\n    pluginref/toc\n    tf/toc\n    trt/toc\n```\n\n----------------------------------------\n\nTITLE: Importing Polygraphy Exception Module in Python\nDESCRIPTION: This code snippet shows how to import the exception module from Polygraphy. It uses Python's automodule directive to automatically generate documentation for the exception module.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/exception/toc.rst#2025-04-06_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. automodule:: polygraphy.exception.exception\n```\n\n----------------------------------------\n\nTITLE: Inserting NVIDIA TensorRT License Header in Bash\nDESCRIPTION: This Bash script inserts the NVIDIA TensorRT license header into source files. It includes the copyright notice, Apache 2.0 license reference, and standard license text. The script is designed to be run on files within the TensorRT repository.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/tests/license_test_header_sh.txt#2025-04-06_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\n#!/bin/bash\n#\n# SPDX-FileCopyrightText: Copyright (c) 1993-2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n```\n\n----------------------------------------\n\nTITLE: Configuring Layer Precision Settings\nDESCRIPTION: Optional code for setting per-layer precision and output types for debugging mixed precision inference.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleINT8API/README.md#2025-04-06_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nauto layer = network->getLayer(i);\nlayer->setPrecision(nvinfer1::DataType::kINT8);\n\nfor (int j=0; j<layer->getNbOutputs(); ++j) {\n    layer->setOutputType(j, nvinfer1::DataType::kFLOAT);\n}\n```\n\n----------------------------------------\n\nTITLE: Importing TensorRT Utility Module\nDESCRIPTION: Python import statement for accessing the TensorRT utility functions from the Polygraphy backend\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/backend/trt/util.rst#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom polygraphy.backend.trt import util\n```\n\n----------------------------------------\n\nTITLE: Importing ONNX Model with ONNX GraphSurgeon in Python\nDESCRIPTION: Auto-generated documentation for the import_onnx function from ONNX GraphSurgeon. This function is used to import an ONNX model for manipulation with the GraphSurgeon tool.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/docs/importers/toc.rst#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n.. autofunction:: onnx_graphsurgeon.import_onnx\n```\n\n----------------------------------------\n\nTITLE: Symmetric Quantization Scale Calculation in Markdown\nDESCRIPTION: Formula for calculating the scale parameter in symmetric quantization used by TensorRT, based on the maximum absolute value in the range.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/intro_to_quantization.md#2025-04-06_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\nscale = &alpha;/(2<sup>b-1</sup>-1)\n```\n\n----------------------------------------\n\nTITLE: Generating Inference Outputs with ONNX-Runtime\nDESCRIPTION: Commands to run inference on an identity model using ONNX-Runtime and save the outputs to a JSON file.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/inspect/05_inspecting_inference_outputs/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run identity.onnx --onnxrt --save-outputs outputs.json\n```\n\n----------------------------------------\n\nTITLE: Dense Indexing with EfficientNMSDenseIndex CUDA Kernel\nDESCRIPTION: This fallback CUDA kernel is used when the score threshold is very low (less than 0.007). It passes all score elements densely packed and indexed to mitigate bottlenecks from atomic operations.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/efficientNMSPlugin/README.md#2025-04-06_snippet_4\n\nLANGUAGE: CUDA\nCODE:\n```\nEfficientNMSDenseIndex\n```\n\n----------------------------------------\n\nTITLE: Plugin Entry Points Implementation\nDESCRIPTION: C++ code showing the required entry point functions for custom TensorRT plugin shared libraries. Implements setLoggerFinder and getCreators functions for plugin registration.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/trtexec/README.md#2025-04-06_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nextern \"C\" void setLoggerFinder(ILoggerFinder* finder);\nextern \"C\" IPluginCreatorInterface* const* getCreators(int32_t& nbCreators)\n```\n\n----------------------------------------\n\nTITLE: Extracting an ONNX Subgraph with Auto Shape and Type Inference\nDESCRIPTION: Command to extract a subgraph from an ONNX model using Polygraphy's surgeon tool with automatic detection of shapes and data types for inputs and outputs.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/surgeon/01_isolating_subgraphs/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy surgeon extract model.onnx \\\n    --inputs x1:auto:auto \\\n    --outputs add_out:auto \\\n    -o subgraph.onnx\n```\n\n----------------------------------------\n\nTITLE: Executing Tests for TensorRT's 'trex' Component Using pytest\nDESCRIPTION: This command runs the pytest framework to execute all tests located in the 'tests' directory for the TensorRT project, specifically for the 'trex' component. It's a simple one-line command that initiates the testing process.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/tests/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ pytest tests\n```\n\n----------------------------------------\n\nTITLE: Environment Setup using Docker for TensorRT\nDESCRIPTION: Sets up the TensorRT OSS build environment using Docker, including cloning the repository, building the container, and installing dependencies for the DeBERTa demo.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/DeBERTa/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Download this TensorRT OSS repo\ngit clone -b main https://github.com/nvidia/TensorRT TensorRT\ncd TensorRT\ngit submodule update --init --recursive\n\n## at root of TensorRT OSS\n# build container\n./docker/build.sh --file docker/ubuntu-20.04.Dockerfile --tag tensorrt-ubuntu20.04-cuda12.8\n\n# launch container\n./docker/launch.sh --tag tensorrt-ubuntu20.04-cuda12.8 --gpus all\n\n## now inside container\n# build OSS (only required for pre-8.4.3 TensorRT versions)\ncd $TRT_OSSPATH\nmkdir -p build && cd build\ncmake .. -DTRT_LIB_DIR=$TRT_LIBPATH -DTRT_OUT_DIR=`pwd`/out\nmake -j$(nproc)\n\n# polygraphy bin location & trtexec bin location\nexport PATH=\"~/.local/bin\":\"${TRT_OSSPATH}/build/out\":$PATH \n\n# navigate to the demo folder install additional dependencies (note PyTorch 1.11.0 is recommended for onnx to export properly)\ncd $TRT_OSSPATH/demo/DeBERTa\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Creating Data Loaders for ImageNet Dataset in Python\nDESCRIPTION: This snippet sets up data loaders for the ImageNet dataset, including both training and validation sets. It uses custom load_data function and sets batch size and other parameters.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/examples/calibrate_quant_resnet50.ipynb#2025-04-06_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndata_path = \"/raid/data/imagenet/imagenet_pytorch\"\nbatch_size = 512\n\ntraindir = os.path.join(data_path, 'train')\nvaldir = os.path.join(data_path, 'val')\n_args = collections.namedtuple('mock_args', ['model', 'distributed', 'cache_dataset'])\ndataset, dataset_test, train_sampler, test_sampler = load_data(traindir, valdir, _args(model='resnet50', distributed=False, cache_dataset=False))\n\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=batch_size,\n    sampler=train_sampler, num_workers=4, pin_memory=True)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test, batch_size=batch_size,\n    sampler=test_sampler, num_workers=4, pin_memory=True)\n```\n\n----------------------------------------\n\nTITLE: Documenting ONNX Graph Surgeon Graph Class in RST\nDESCRIPTION: RestructuredText directive to auto-generate documentation for the onnx_graphsurgeon.Graph class from its docstrings.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/docs/ir/graph.rst#2025-04-06_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. autoclass:: onnx_graphsurgeon.Graph\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Sphinx Documentation in Bash\nDESCRIPTION: Runs the make command to clean any existing documentation and build new HTML documentation for the TensorRT project.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ make clean html\n```\n\n----------------------------------------\n\nTITLE: Formatting Git Changes with clang-format in Bash\nDESCRIPTION: Command to run clang-format on git changes using the project's style file. Optionally accepts a commit ID or reference to format specific changes.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/CONTRIBUTING.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit-clang-format --style file [commit ID/reference]\n```\n\n----------------------------------------\n\nTITLE: Importing TensorRT Calibrator Module\nDESCRIPTION: Module import statement for the polygraphy TensorRT calibrator functionality\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/backend/trt/calibrator.rst#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npolygraphy.backend.trt.calibrator\n```\n\n----------------------------------------\n\nTITLE: Symmetric DeQuantization Formula in Markdown\nDESCRIPTION: Mathematical formula showing how to convert symmetrically quantized values back to real values using just the scale parameter.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/intro_to_quantization.md#2025-04-06_snippet_6\n\nLANGUAGE: markdown\nCODE:\n```\nx=(x<sub>q</sub>)∗scale\n```\n\n----------------------------------------\n\nTITLE: Re-exporting TensorFlow Saved Model\nDESCRIPTION: Command to re-export the TensorFlow saved model with float_image_tensor as input type using the TFOD API exporter script.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/tensorflow_object_detection_api/README.md#2025-04-06_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncd /path/to/models/research/object_detection\npython exporter_main_v2.py \\\n    --input_type float_image_tensor \\\n    --trained_checkpoint_dir /path/to/ssd_mobilenet_v2_320x320_coco17_tpu-8/checkpoint \\\n    --pipeline_config_path /path/to/ssd_mobilenet_v2_320x320_coco17_tpu-8/pipeline.config \\\n    --output_directory /path/to/export\n```\n\n----------------------------------------\n\nTITLE: Importing Polygraphy Model Arguments Module in Python\nDESCRIPTION: This code snippet demonstrates how to import the model arguments module from the Polygraphy tools package. It's likely used in Polygraphy scripts or tools to handle model-related command-line arguments.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/tool/args/model.rst#2025-04-06_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom polygraphy.tools.args import model\n```\n\n----------------------------------------\n\nTITLE: Declaring Size Tensor for Data-Dependent Output\nDESCRIPTION: Creates a size tensor declaration that defines the dynamic dimension for the non-zero indices output, specifying the output index, optimal value, and upper bound.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleNonZeroPlugin/README.md#2025-04-06_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nauto numNonZeroSizeTensor = exprBuilder.declareSizeTensor(1, *optValue, *upperBound);\n```\n\n----------------------------------------\n\nTITLE: Inspecting TensorRT Network with Polygraphy CLI\nDESCRIPTION: This command uses Polygraphy's CLI to inspect the TensorRT network generated by the load_network() function, displaying layers, attributes, and weights.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/03_interoperating_with_tensorrt/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy inspect model example.py --trt-network-func load_network --show layers attrs weights\n```\n\n----------------------------------------\n\nTITLE: Running TensorRT I/O Formats Sample\nDESCRIPTION: This Bash command demonstrates how to run the compiled TensorRT I/O formats sample, specifying the data directory and optionally a DLA core for inference.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleIOFormats/README.md#2025-04-06_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\n./sample_io_formats --datadir=<path/to/data> --useDLACore=N\n```\n\n----------------------------------------\n\nTITLE: Finding TensorRT and CUDA Libraries for Plugin Development\nDESCRIPTION: Locates the required libraries for TensorRT plugin development including nvinfer (TensorRT), cudart (CUDA Runtime), cublas (CUDA BLAS), and cuda libraries using the find_library commands.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/onnx_custom_plugin/CMakeLists.txt#2025-04-06_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\n# Find dependencies:\nmessage(\"\\nThe following variables are derived from the values of the previous variables unless provided explicitly:\\n\")\n\n# TensorRT's nvinfer lib\nfind_library(\n    _NVINFER_LIB nvinfer\n    HINTS ${TRT_LIB}\n    PATH_SUFFIXES lib lib64)\nset_ifndef(NVINFER_LIB ${_NVINFER_LIB})\n\nfind_library(\n    _CUDART_LIB cudart\n    HINTS ${CUDA_LIB_DIR}\n    PATH_SUFFIXES lib lib64)\nset_ifndef(CUDART_LIB ${_CUDART_LIB})\n\nfind_library(\n    _CUBLAS_LIB cublas\n    HINTS ${CUDA_LIB_DIR}\n    PATH_SUFFIXES ${CUBLAS_LIB_SUFFIXES})\nset_ifndef(CUBLAS_LIB ${_CUBLAS_LIB})\n\nfind_library(\n    _CUDA_LIB cuda\n    HINTS ${CUDA_LIB_DIR}\n    PATH_SUFFIXES lib/stubs lib64/stubs)\nset_ifndef(CUDA_LIB ${_CUDA_LIB})\n```\n\n----------------------------------------\n\nTITLE: COCO Dataset Evaluation Error Message\nDESCRIPTION: Example error message when running evaluation without proper dataset setup in the correct directory structure.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/detectron2/README.md#2025-04-06_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nFileNotFoundError: [Errno 2] No such file or directory: 'datasets/coco/annotations/instances_val2017.json'\n```\n\n----------------------------------------\n\nTITLE: Defining Documentation Structure with toctree in reStructuredText\nDESCRIPTION: This snippet defines a toctree directive that links to three separate documentation files: tensor, variable, and constant. The toctree creates a hierarchical structure for documenting TensorRT's tensor components.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/docs/ir/tensor/toc.rst#2025-04-06_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n    tensor\n    variable\n    constant\n```\n\n----------------------------------------\n\nTITLE: Displaying Debug Tool Usage Information in Markdown\nDESCRIPTION: This code snippet shows how to display usage information for the 'debug' tool using the Polygraphy CLI.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/polygraphy/tools/debug/README.md#2025-04-06_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n```\npolygraphy debug -h\n```\n```\n\n----------------------------------------\n\nTITLE: Quantization Class and Function Naming Conventions\nDESCRIPTION: Example of naming conventions for quantized versions of classes and functions\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/CONTRIBUTING.md#2025-04-06_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass Linear(...)\nclass QuantLinear(...)\n\ndef linear(...)\ndef quant_linear(...)\n```\n\n----------------------------------------\n\nTITLE: Running Python Tests with PyTest\nDESCRIPTION: Commands for running all tests or specific test files using pytest\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/CONTRIBUTING.md#2025-04-06_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npytest --verbose\npytest --verbose mytestfile.py\n```\n\n----------------------------------------\n\nTITLE: Importing Postprocessing Module in Python\nDESCRIPTION: This snippet shows how to import the postprocessing module from the Polygraphy toolkit. The module is located in the 'polygraphy.tools.args.comparator' package and contains functions for postprocessing comparison results.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/tool/args/comparator/postprocess.rst#2025-04-06_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom polygraphy.tools.args.comparator import postprocess\n```\n\n----------------------------------------\n\nTITLE: Naming Conventions for Constants in C++\nDESCRIPTION: Example of the proper naming convention for constants in TensorRT, which uses uppercase snake_case with a 'k' prefix according to the guidelines.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/CODING-GUIDELINES.md#2025-04-06_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\nconst int kDIGIT_NUM = 10;\n```\n\n----------------------------------------\n\nTITLE: RST Note for TensorRT Model Zoo Results\nDESCRIPTION: A reStructuredText note block that provides additional context about the benchmark results, including hardware setup, accuracy metrics, and hyperparameters used for the quantization experiments.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/model_zoo.md#2025-04-06_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n```{eval-rst}\n\n.. NOTE::\n\n    The results here were obtained with NVIDIA's A100 GPU and TensorRT 8.4.\n    \n    Accuracy metric: Top-1 validation accuracy with the full ImageNet dataset.\n\n    Hyper-parameters\n\n    #. QAT fine-tuning: `bs=64`, `ep=10`, `lr=0.001` (unless otherwise stated).\n    #. PTQ calibration: `bs=64`.\n\n```\n```\n\n----------------------------------------\n\nTITLE: Checking for NaN/Infinity in FP16 Model\nDESCRIPTION: Command to validate the FP16 model by checking all intermediate outputs for NaN or infinity values.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/convert/04_converting_models_to_fp16/README.md#2025-04-06_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run --onnxrt identity_fp16.onnx --onnx-outputs mark all --validate\n```\n\n----------------------------------------\n\nTITLE: Inspecting ONNX Model Layers with Polygraphy\nDESCRIPTION: Optional command to verify the correctness of the folded ONNX model by displaying its layers using Polygraphy's inspect model functionality.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/surgeon/02_folding_constants/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy inspect model folded.onnx --show layers\n```\n\n----------------------------------------\n\nTITLE: Importing Polygraphy Script Module in Python\nDESCRIPTION: Shows the module import path for accessing the polygraphy script tools interface in TensorRT\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/tool/script.rst#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom polygraphy.tools.script import *\n```\n\n----------------------------------------\n\nTITLE: Creating Graph Layers in Python\nDESCRIPTION: Shows the usage of the new layer() function in Graph to simplify model creation from scratch. This provides a higher-level API for constructing ONNX graphs.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/CHANGELOG.md#2025-04-06_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ngraph.layer()\n```\n\n----------------------------------------\n\nTITLE: Implementing DeviceBuffer Class for GPU Memory Management in Python\nDESCRIPTION: Defines a DeviceBuffer class for managing CUDA memory allocations. It handles buffer creation, binding, and cleanup for TensorRT engine execution, providing a convenient abstraction over the low-level CUDA memory management.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/benchmark.ipynb#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass DeviceBuffer(object):\n    def __init__(self, shape, dtype=trt.int32):\n        self.buf = cuda.mem_alloc(trt.volume(shape) * dtype.itemsize)\n\n    def binding(self):\n        return int(self.buf)\n\n    def free(self):\n        self.buf.free()\n```\n\n----------------------------------------\n\nTITLE: Checking TensorRT Version and Creating Engine Directory\nDESCRIPTION: Displays the TensorRT version and creates a directory to store the optimized BERT engine files specific to the detected TensorRT version.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/BERT-TRT-FP16.ipynb#2025-04-06_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport tensorrt as trt;\nTRT_VERSION = trt.__version__\n\nprint(\"TensorRT version: {}\".format(TRT_VERSION))\n!mkdir -p engines_$TRT_VERSION\n```\n\n----------------------------------------\n\nTITLE: Viewing Available BERT Model Download Options\nDESCRIPTION: Displays help information for the model download script to see all available model options.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nbash scripts/download_model.sh -h\n```\n\n----------------------------------------\n\nTITLE: Naming Convention for Function-Scope Non-Magic Constants in C++\nDESCRIPTION: Example showing how function-scope constants that are not magic numbers should be named like regular variables, without special prefixes.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/CODING-GUIDELINES.md#2025-04-06_snippet_3\n\nLANGUAGE: cpp\nCODE:\n```\nconst bool pass = a && b;\n```\n\n----------------------------------------\n\nTITLE: Proper Ternary Operator Usage in C++\nDESCRIPTION: Examples demonstrating how to refactor nested ternary operators into separate statements for better readability and compliance with AUTOSAR C++ 2014 guidelines.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/CODING-GUIDELINES.md#2025-04-06_snippet_10\n\nLANGUAGE: cpp\nCODE:\n```\nconst auto var = (condition0 ? a : (condition1 ? b : c));\n```\n\nLANGUAGE: cpp\nCODE:\n```\nconst auto d = (condition1 ? b : c);\nconst auto var = (condition0 ? a : d);\n```\n\n----------------------------------------\n\nTITLE: Nesting Preprocessor Directives in C++\nDESCRIPTION: Illustrates the correct indentation and structure for nested preprocessor directives in C++.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/CODING-GUIDELINES.md#2025-04-06_snippet_14\n\nLANGUAGE: cpp\nCODE:\n```\n#if defined(FOO)\n# if FOO == 0\n#  define BAR 0\n# elif FOO == 1\n#  define BAR 5\n# else\n#  error \"invalid FOO value\"\n# endif\n#endif\n```\n\n----------------------------------------\n\nTITLE: Importing Algorithm Selector Module in Python\nDESCRIPTION: Import statement for the deprecated Algorithm Selector module from polygraphy's TensorRT backend. Note that this feature is deprecated in TensorRT 10.8 and users should use editable mode in ITimingCache instead.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/backend/trt/algorithm_selector.rst#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom polygraphy.backend.trt import algorithm_selector\n```\n\n----------------------------------------\n\nTITLE: Importing Polygraphy Logger Module\nDESCRIPTION: Module import statement for the polygraphy logger functionality, which is used for logging in TensorRT applications.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/logger/toc.rst#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npolygraphy.logger.logger\n```\n\n----------------------------------------\n\nTITLE: Using TREx Helper for Grouping and Summarizing\nDESCRIPTION: Demonstrates TREx's convenience wrapper for grouping layers by type and summing latency attributes.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/api-examples.ipynb#2025-04-06_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# trex provides another way to do the same thing\ntrex.group_sum_attr(plan.df,\"type\", \"latency.avg_time\")\n```\n\n----------------------------------------\n\nTITLE: Inspecting custom input data with Polygraphy CLI\nDESCRIPTION: Command to inspect the generated custom input data file using Polygraphy's inspect functionality.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/run/06_comparing_with_custom_output_data/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy inspect data custom_inputs.json\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for TensorRT Wheel Building\nDESCRIPTION: This snippet defines the required Python packages and their versions for building TensorRT wheel files. It includes wheel and setuptools, with different setuptools versions specified for various Python versions.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/python/packaging/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# Required for building wheel files\nwheel==0.37.1\nsetuptools~=59.6.0; python_version<\"3.8\"\nsetuptools~=69.0.3; python_version>=\"3.8\" and python_version < \"3.12\"\nsetuptools~=73.0.1; python_version>=\"3.12\"\n```\n\n----------------------------------------\n\nTITLE: Installing Test Prerequisites with pip\nDESCRIPTION: Commands for installing required packages for testing Polygraphy using pip. These commands install both test requirements and documentation requirements from specific package indexes.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/CONTRIBUTING.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pip install -r tests/requirements.txt --index-url https://pypi.ngc.nvidia.com --extra-index-url https://pypi.org/simple\n```\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pip install -r docs/requirements.txt --index-url https://pypi.ngc.nvidia.com --extra-index-url https://pypi.org/simple\n```\n\n----------------------------------------\n\nTITLE: Custom CMake Configuration with Dependencies\nDESCRIPTION: Example of CMake configuration with custom paths for CUDA, TensorRT, and other dependencies\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/onnx_custom_plugin/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncmake .. -DCMAKE_CUDA_COMPILER=/usr/local/cuda-x.x/bin/nvcc # (Or adding /path/to/nvcc into $PATH)\n         -DCUDA_INC_DIR=/usr/local/cuda-x.x/include/  # (Or adding /path/to/cuda/include into $CPLUS_INCLUDE_PATH)\n         -DTRT_LIB=/path/to/tensorrt/lib/\n         -DTRT_INCLUDE=/path/to/tensorrt/include/\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Polygraphy-TensorRT Interoperation\nDESCRIPTION: This command installs the required dependencies for the example using pip.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/03_interoperating_with_tensorrt/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running TensorRT Inference for Model Accuracy Evaluation\nDESCRIPTION: Python command to perform inference with a TensorRT engine and evaluate its accuracy on validation data. Supports different model architectures and allows configuration of batch size and top-k accuracy metrics.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/examples/README.md#2025-04-06_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npython infer_engine.py --engine=<path_to_trt_engine> --data_dir=<path_to_tfrecord_val_data> -b=<batch_size>\n```\n\n----------------------------------------\n\nTITLE: Apache 2.0 License Header for NVIDIA TensorRT\nDESCRIPTION: Standard license header template specifying the Apache 2.0 license terms for NVIDIA TensorRT project files. This header indicates copyright ownership by NVIDIA and outlines the permissions and limitations of using the software.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/tests/license_test_header_py.txt#2025-04-06_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n#\n# SPDX-FileCopyrightText: Copyright (c) 1993-2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: Apache-2.0\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n```\n\n----------------------------------------\n\nTITLE: Loading Default Profile with Profiling Data in TREx (Python)\nDESCRIPTION: Shows how to load the default profile (profile 0) in a multi-profile engine using TREx, which allows the use of profiling data.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/KNOWN_ISSUES.md#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nplan = EnginePlan(\n    graph_file=\"my_engine_name.graph.json\",\n    profiling_file=\"my_engine.profile.json\",\n    profile_id=0)\n```\n\n----------------------------------------\n\nTITLE: Using Signed Integers with std::vector in C++\nDESCRIPTION: Demonstrates the preferred way to use signed integers with std::vector::size() in loop conditions.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/CODING-GUIDELINES.md#2025-04-06_snippet_16\n\nLANGUAGE: cpp\nCODE:\n```\nfor (size_t i = 0; i < mTensors.size(); ++i) // preferred style\n```\n\n----------------------------------------\n\nTITLE: Apache 2.0 License Header for NVIDIA TensorRT\nDESCRIPTION: Standard license header for NVIDIA TensorRT source files indicating copyright ownership and licensing terms under Apache 2.0. This header provides the legal basis for using, modifying, and distributing the TensorRT codebase.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/tests/license_test_header_cpp.txt#2025-04-06_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\n/*\n * SPDX-FileCopyrightText: Copyright (c) 1993-2023 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n * SPDX-License-Identifier: Apache-2.0\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n```\n\n----------------------------------------\n\nTITLE: Implementing NVIDIA Copyright Header in C++\nDESCRIPTION: Shows the required NVIDIA copyright header to be included at the top of all TensorRT Open Source Software code files.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/CODING-GUIDELINES.md#2025-04-06_snippet_18\n\nLANGUAGE: cpp\nCODE:\n```\n/*\n * Copyright (c) 2021, NVIDIA CORPORATION. All rights reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n```\n\n----------------------------------------\n\nTITLE: Installing Required NumPy Version\nDESCRIPTION: Installs a compatible version of NumPy package for TensorRT execution.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/1. Introduction.ipynb#2025-04-06_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n!python3 -m pip install \"numpy<2.0\"\n```\n\n----------------------------------------\n\nTITLE: Importing TensorFlow 1.X Backend Module in Polygraphy\nDESCRIPTION: Reference to the TensorFlow 1.X backend module path in Polygraphy. This module provides functionality for working with TensorFlow 1.X models in the Polygraphy framework.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/backend/tf/toc.rst#2025-04-06_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n``polygraphy.backend.tf``\n```\n\n----------------------------------------\n\nTITLE: Inspecting Input Data with Polygraphy Inspect Command\nDESCRIPTION: This command inspects the previously generated input data file and displays detailed statistics and values of the input tensors.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/inspect/06_inspecting_input_data/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy inspect data inputs.json --show-values\n```\n\n----------------------------------------\n\nTITLE: Cloning AutoML Repository\nDESCRIPTION: This command clones the Google AutoML repository from GitHub, which contains the EfficientDet implementation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/EfficientDet/notebooks/EfficientDet-TensorRT8.ipynb#2025-04-06_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n!git clone https://github.com/google/automl\n```\n\n----------------------------------------\n\nTITLE: Creating Deprecated Aliases for Modules\nDESCRIPTION: Example showing how to manually apply the export_deprecated_alias decorator to create a deprecated alias for an entire module.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/CONTRIBUTING.md#2025-04-06_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmod.export_deprecated_alias(\"old_mod_name\", remove_in=\"0.25.0\")(sys.modules[__name__])\n```\n\n----------------------------------------\n\nTITLE: Generating TensorRT Network Script Template with Polygraphy\nDESCRIPTION: Command to generate a template script for creating a TensorRT network from scratch using Polygraphy's template feature.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/run/04_defining_a_tensorrt_network_or_config_manually/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy template trt-network -o my_define_network.py\n```\n\n----------------------------------------\n\nTITLE: Creating ONNX Model with Named Dimensions in Python\nDESCRIPTION: This code snippet creates a synthetic ONNX model with a single Concat layer and two 2D input tensors. The inputs have named dimensions 'n_rows', which TensorRT will recognize and enforce equality for.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleNamedDimensions/README.md#2025-04-06_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\npython3 create_model.py\n```\n\n----------------------------------------\n\nTITLE: Creating TensorRT Builder and Engine\nDESCRIPTION: Creates a TensorRT builder and uses it to build a CUDA engine from the parsed network. This step is crucial for preparing the model for inference.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleOnnxMnistCoordConvAC/README.md#2025-04-06_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nIBuilder* builder = createInferBuilder(sample::gLogger);\nnvinfer1::ICudaEngine* engine = builder->buildCudaEngine(*network);\n```\n\n----------------------------------------\n\nTITLE: Creating Debug Macros with Conditional Compilation in C++\nDESCRIPTION: Example demonstrating how to create a macro that conditionally includes debug code based on a compile-time flag, which helps prevent bitrot in debug features.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/CODING-GUIDELINES.md#2025-04-06_snippet_8\n\nLANGUAGE: cpp\nCODE:\n```\n// Alternative: use a macro which evaluates to a noop in release code.\n#if DEBUG_CONVOLUTION_INSTRUMENTATION\n# define DEBUG_CONV_CODE(x) x\n#else\n# define DEBUG_CONV_CODE(x)\n#endif\n```\n\n----------------------------------------\n\nTITLE: Loading TensorRT Plugin Library\nDESCRIPTION: Loads the NVIDIA TensorRT plugin library required for BERT inference.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/inference.ipynb#2025-04-06_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport ctypes\nimport os\n\nctypes.CDLL(\"libnvinfer_plugin.so\", mode=ctypes.RTLD_GLOBAL)\n```\n\n----------------------------------------\n\nTITLE: Creating Deprecated Aliases for Classes\nDESCRIPTION: Example showing how to use the export_deprecated_alias decorator to rename a class while maintaining backward compatibility with the old name.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/CONTRIBUTING.md#2025-04-06_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@mod.export_deprecated_alias(\"Old\", remove_in=\"0.25.0\")\nclass New:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Importing Polygraphy Tool Runners Module\nDESCRIPTION: This snippet shows the import statement for the runners module in the Polygraphy tool's backend plugin reference. It is likely used to provide functionality related to running or executing plugins within the Polygraphy framework.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/tool/args/backend/pluginref/runner.rst#2025-04-06_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. automodule:: polygraphy.tools.args.backend.pluginref.runner\n```\n\n----------------------------------------\n\nTITLE: Implementing Aliased Input for In-Place Scatter-Add Plugin in Python\nDESCRIPTION: This code snippet demonstrates how to implement the get_aliased_input method for the IPluginV3OneBuildV2 interface. It declares that the first output is aliased to the first input, allowing in-place operations.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/aliased_io_plugin/README.md#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef get_aliased_input(self, output_index: int):\n\tif output_index == 0:\n\t\treturn 0\n\t\n\treturn -1\n```\n\n----------------------------------------\n\nTITLE: Querying GPU Compute Capability with NVIDIA SMI\nDESCRIPTION: Command to check the compute capability of NVIDIA GPUs using the nvidia-smi utility. This helps determine if your GPU architecture is supported for the TensorRT BERT implementation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_33\n\nLANGUAGE: bash\nCODE:\n```\nnvidia-smi --query-gpu=compute_cap --format=csv\n```\n\n----------------------------------------\n\nTITLE: Defining Documentation Tree Structure\nDESCRIPTION: Specifies the documentation tree structure that includes loader and runner components.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/backend/base/toc.rst#2025-04-06_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n    loader\n    runner\n```\n\n----------------------------------------\n\nTITLE: Setting Question Input for BERT QA\nDESCRIPTION: Defines the question text to be answered by the BERT model based on the provided paragraph context.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/inference.ipynb#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nquestion_text = \"What project put the first Americans into space?\"\n#question_text =  \"What year did the first manned Apollo flight occur?\"\n#question_text =  \"What President is credited with the original notion of putting Americans in space?\"\n#question_text =  \"Who did the U.S. collaborate with on an Earth orbit mission in 1975?\"\n```\n\n----------------------------------------\n\nTITLE: Building and Installing the Extension Module\nDESCRIPTION: Commands to build the extension module using setup.py and install it with pip. Includes the extra index URL for NVIDIA packages.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/dev/02_extending_polygraphy_run/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 extension_module/setup.py bdist_wheel\n```\n\n----------------------------------------\n\nTITLE: Requirements File for TensorRT Dependencies\nDESCRIPTION: Lists required Python packages and their versions for TensorRT development environment. Includes Polygraphy for neural network inference, ONNX for model representation, ONNX Runtime for model execution, protobuf for serialization, and pytest for testing.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/polygraphy-extension-trtexec/tests/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ncolored\npolygraphy==0.49.0\nonnx==1.14.0\nonnxruntime==1.15.0\nprotobuf==3.20.2\npytest\nwheel\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for ResNet50 V1 QAT in TensorFlow\nDESCRIPTION: This snippet imports necessary libraries and modules for quantization-aware training of ResNet50 V1 model. It includes TensorFlow, custom quantization tools, and utility functions for ONNX conversion.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/example_resnet50v1.ipynb#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport tensorflow as tf\nfrom tensorflow_quantization.quantize import quantize_model\nfrom tensorflow_quantization.custom_qdq_cases import ResNetV1QDQCase\nfrom tensorflow_quantization.utils import convert_saved_model_to_onnx\n```\n\n----------------------------------------\n\nTITLE: Loading Multi-Profile Engine in TREx (Python)\nDESCRIPTION: Demonstrates how to load a specific profile (profile 3) in a multi-profile engine using TREx. This will load the engine plan but drop profiling data for non-default profiles.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/KNOWN_ISSUES.md#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nplan = EnginePlan(\n    graph_file=\"my_engine_name.graph.json\",\n    profiling_file=\"my_engine.profile.json\",\n    profile_id=3)\n```\n\n----------------------------------------\n\nTITLE: Checking TensorRT Version and Creating Engine Directory\nDESCRIPTION: Identifies the installed TensorRT version and creates a directory to store the optimized TensorRT engines. This ensures that engine files are properly organized by TensorRT version for compatibility.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/BERT-TRT-INT8-QAT-sparse.ipynb#2025-04-06_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport tensorrt as trt;\nTRT_VERSION = trt.__version__\n\nprint(\"TensorRT version: {}\".format(TRT_VERSION))\n!mkdir -p engines_$TRT_VERSION\n```\n\n----------------------------------------\n\nTITLE: HuggingFace Workaround for Stable Video Diffusion\nDESCRIPTION: Python code snippet to workaround a bug in HuggingFace for Stable Video Diffusion.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/Diffusion/README.md#2025-04-06_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nif torch.is_tensor(num_frames):\n    num_frames = num_frames.item()\nemb = emb.repeat_interleave(num_frames, dim=0)\n```\n\n----------------------------------------\n\nTITLE: Specifying Minimum Versions for TensorRT and PyTorch\nDESCRIPTION: This snippet defines the minimum required versions for TensorRT and PyTorch libraries. It ensures compatibility with a project that likely involves GPU-accelerated deep learning inference.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/09_working_with_pytorch_tensors/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: Plain Text\nCODE:\n```\ntensorrt>=8.5\ntorch>=1.13.0\n```\n\n----------------------------------------\n\nTITLE: Importing JSON Serialization Module in Polygraphy\nDESCRIPTION: Shows the import statement for accessing JSON serialization functionality in the Polygraphy framework.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/json/toc.rst#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom polygraphy.json import serde\n```\n\n----------------------------------------\n\nTITLE: Sample Capability Report Output\nDESCRIPTION: Example output showing unsupported operators in the ONNX model, including stack traces, operator names, node names, and detailed error messages for each incompatibility.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/inspect/09_inspecting_tensorrt_static_onnx_support/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n[I] ===== Summary =====\n    Stack trace                                                                             | Operator  | Node               | Reason\n    -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n    onnx_graphsurgeon_node_1 (OuterFunction) -> onnx_graphsurgeon_node_1 (NestedLocalFake2) | Fake_2    | nested_node_fake_2 | In node 0 with name: nested_node_fake_2 and operator: Fake_2 (checkFallbackPluginImporter): INVALID_NODE: creator && \"Plugin not found, are the plugin name, version, and namespace correct?\"\n    onnx_graphsurgeon_node_1 (OuterFunction)                                                | Fake_1    | nested_node_fake_1 | In node 0 with name: nested_node_fake_1 and operator: Fake_1 (checkFallbackPluginImporter): INVALID_NODE: creator && \"Plugin not found, are the plugin name, version, and namespace correct?\"\n```\n\n----------------------------------------\n\nTITLE: Running Tests with Make\nDESCRIPTION: Command for running the Polygraphy test suite using the make utility.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/CONTRIBUTING.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmake test\n```\n\n----------------------------------------\n\nTITLE: Running Custom Runner with ONNX Model\nDESCRIPTION: Command to run the custom Reshape Destroyer runner with an ONNX model containing a no-op Reshape node.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/dev/02_extending_polygraphy_run/README.md#2025-04-06_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run no_op_reshape.onnx --res-des\n```\n\n----------------------------------------\n\nTITLE: Running Polygraphy with TRTExec Backend\nDESCRIPTION: This command demonstrates how to use the newly installed trtexec backend with polygraphy run, specifying an ONNX model file.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/polygraphy-extension-trtexec/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run sample.onnx --trtexec\n```\n\n----------------------------------------\n\nTITLE: Building and Refitting TensorRT Engine with GPU Weights\nDESCRIPTION: Command to build a TensorRT engine from the modified ONNX model, refit it using GPU weights, and run inference with the refitted engine.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/engine_refit_onnx_bidaf/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 build_and_refit_engine.py --weights-location GPU\n```\n\n----------------------------------------\n\nTITLE: Allocating Output Memory for TensorRT Inference\nDESCRIPTION: Allocates memory for the output tensor of TensorRT inference, using the target precision (FP16 or FP32) determined earlier.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/2. Using PyTorch through ONNX.ipynb#2025-04-06_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\n\n# need to set input and output precisions to FP16 to fully enable it\noutput = np.empty([BATCH_SIZE, 1000], dtype = target_dtype) \n```\n\n----------------------------------------\n\nTITLE: Preparing Input Image and ONNX Model for TensorRT\nDESCRIPTION: This command runs a script named 'export.py' to prepare the input image and ONNX model file for TensorRT processing. The specific details of the export process are not shown in this snippet.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/SemanticSegmentation/tutorial-runtime.ipynb#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n!python3 export.py\n```\n\n----------------------------------------\n\nTITLE: Expressing Fixed Output Dimension with IPluginV3\nDESCRIPTION: Code snippet showing how to express a fixed dimension (second dimension) of the plugin output using the IExprBuilder interface in the getOutputShapes() method.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleNonZeroPlugin/README.md#2025-04-06_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\noutputs[0].d[1] = exprBuilder.constant(2);\n```\n\n----------------------------------------\n\nTITLE: Common Header Addition\nDESCRIPTION: Adds the common header file for fused multihead attention implementations to the plugin sources.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/bertQKVToContextPlugin/fused_multihead_attention/CMakeLists.txt#2025-04-06_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nadd_plugin_source(\n    fused_multihead_attention_common.h\n)\n```\n\n----------------------------------------\n\nTITLE: Folding Constants in ONNX Model using Polygraphy Surgeon\nDESCRIPTION: Command to fold constants in an ONNX model using the Polygraphy surgeon sanitize tool. This operation transforms expressions with multiple constants into simplified ones by pre-computing constant values.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/surgeon/02_folding_constants/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy surgeon sanitize model.onnx \\\n    --fold-constants \\\n    -o folded.onnx\n```\n\n----------------------------------------\n\nTITLE: Exporting TensorFlow Saved Model from AutoML Checkpoint\nDESCRIPTION: Python command to export a TensorFlow saved model from an AutoML EfficientDet checkpoint using the model_inspect.py script.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/efficientdet/README.md#2025-04-06_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython3 model_inspect.py \\\n    --runmode saved_model \\\n    --model_name efficientdet-d0 \\\n    --ckpt_path /path/to/efficientdet-d0 \\\n    --saved_model_dir /path/to/saved_model\n```\n\n----------------------------------------\n\nTITLE: Signing Git Commits for TensorRT Contributions in Bash\nDESCRIPTION: Command to commit changes with a sign-off, certifying the contribution as original work or properly licensed. This is required for all TensorRT contributions.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/CONTRIBUTING.md#2025-04-06_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit commit -s -m \"Add cool feature.\"\n```\n\n----------------------------------------\n\nTITLE: Verifying Removal of Unbounded DDS Tensors\nDESCRIPTION: This command inspects the modified model to confirm that there are no remaining tensors with unbounded DDS after applying the upper bounds.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/surgeon/04_setting_upper_bounds/README.md#2025-04-06_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy inspect model modified.onnx --list-unbounded-dds\n```\n\n----------------------------------------\n\nTITLE: Launching Jupyter Notebook Server\nDESCRIPTION: Commands to start Jupyter notebook or lab server with remote access configuration.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/README.md#2025-04-06_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ jupyter-notebook --ip=0.0.0.0 --no-browser\n\n# For JupyterLab:\n$ jupyter lab --ip=0.0.0.0 --port=8888\n```\n\n----------------------------------------\n\nTITLE: Displaying TensorRT Test Image\nDESCRIPTION: Visualizes the first image in the input batch prepared for TensorRT inference.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/2. Using PyTorch through ONNX.ipynb#2025-04-06_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nplt.imshow(input_batch[0].astype(np.float32))\n```\n\n----------------------------------------\n\nTITLE: TensorRT Python Dependencies Configuration\nDESCRIPTION: Comprehensive list of Python package dependencies with conditional version requirements based on Python version and operating system. Includes core ML frameworks like ONNX and CUDA support, along with utility packages for data handling and processing.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/tensorflow_object_detection_api/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nonnx==1.14.0; python_version <= \"3.10\"\nonnx==1.16.1; python_version >= \"3.11\"\nonnxruntime==1.15.1; python_version <= \"3.10\"\nonnxruntime==1.18.1; python_version >= \"3.11\"\nPillow>=10.0.0\ntf2onnx==1.15.0\npycocotools; platform_system != \"Windows\"\npycocotools-windows; platform_system == \"Windows\"\ncuda-python==12.2.0; python_version <= \"3.10\"\ncuda-python==12.6.0; python_version >= \"3.11\"\npywin32; platform_system == \"Windows\"\nCython<3.0\npyyaml==6.0.1\nrequests==2.32.2\ntqdm==4.66.4\nnumpy==1.24.4; python_version <= \"3.10\"\nnumpy==1.26.4; python_version >= \"3.11\"\n```\n\n----------------------------------------\n\nTITLE: Estimating Optimum Value for NonZero Output Size\nDESCRIPTION: Calculates an estimated optimal value for the number of non-zero elements (half of the total elements) using the floor division operation in the expression builder.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleNonZeroPlugin/README.md#2025-04-06_snippet_2\n\nLANGUAGE: C++\nCODE:\n```\nauto optValue = exprBuilder.operation(DimensionOperation::kFLOOR_DIV, *upperBound, *exprBuilder.constant(2));\n```\n\n----------------------------------------\n\nTITLE: Setting Data-Dependent Output Dimension\nDESCRIPTION: Assigns the size tensor to the first dimension of the output tensor, allowing for a data-dependent output shape in the TensorRT plugin.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleNonZeroPlugin/README.md#2025-04-06_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\noutputs[0].d[0] = numNonZeroSizeTensor;\n```\n\n----------------------------------------\n\nTITLE: Saving Input Data Using Python Script\nDESCRIPTION: Command to execute the data loader Python script that saves input data to disk for later use. This step is only necessary when using the JSON file approach.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/run/05_comparing_with_custom_input_data/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 data_loader.py\n```\n\n----------------------------------------\n\nTITLE: Input Tensor Structure Example\nDESCRIPTION: Example showing the input tensor shape and structure requirements for the bertQKVToContextPlugin. Input has shape [S, B, 3 * E, 1, 1] where B is batch size and E is hidden size.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/bertQKVToContextPlugin/README.md#2025-04-06_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n[S, B, 3 * E, 1, 1]\n```\n\n----------------------------------------\n\nTITLE: Collecting Performance Data for BERT with Variable Sequence Length\nDESCRIPTION: This command collects performance data for the BERT model with variable sequence length support.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\npython3 perf_varseqlen.py -e engines/bert_varseq_int8.engine -b 1 -s 256\n```\n\n----------------------------------------\n\nTITLE: Visualizing Post Training Quantization (PTQ) Flow with Mermaid\nDESCRIPTION: This Mermaid flowchart illustrates the process of Post Training Quantization (PTQ). It shows the steps from calibration data input to the final quantized model, including capturing layer distribution and computing scale values.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/qat.md#2025-04-06_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n    id1(Calibration data) --> id2(Pre-trained model) --> id3(Capture layer distribution) --> id4(Compute 'scale') --> id5(Quantize model)\n```\n\n----------------------------------------\n\nTITLE: Building and Installing PyTorch Quantization from Source\nDESCRIPTION: This command builds and installs the PyTorch Quantization package from source code, requiring Python 3.7+ and GCC 5.4+.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/README.md#2025-04-06_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# Python version >= 3.7, GCC version >= 5.4 required\npython setup.py install\n```\n\n----------------------------------------\n\nTITLE: Forward Pass Implementation for Quantized MaxPool2d\nDESCRIPTION: Forward function implementation that applies quantization to input before passing to base maxpool layer.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/tutorials/creating_custom_quantized_modules.rst#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, input):\n    quant_input = self._input_quantizer(input)\n    return super(QuantMaxPool2d, self).forward(quant_input)\n```\n\n----------------------------------------\n\nTITLE: Building and Deserializing TensorRT Preprocessor Engine\nDESCRIPTION: Builds a serialized network for the preprocessor and deserializes it into a runtime engine. Performs error checking to ensure successful engine creation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleDynamicReshape/README.md#2025-04-06_snippet_8\n\nLANGUAGE: C++\nCODE:\n```\nSampleUniquePtr<nvinfer1::IHostMemory> preprocessorPlan = makeUnique(\n        builder->buildSerializedNetwork(*preprocessorNetwork, *preprocessorConfig));\nif (!preprocessorPlan)\n{\n    sample::gLogError << \"Preprocessor serialized engine build failed.\" << std::endl;\n    return false;\n}\n\nmPreprocessorEngine = makeUnique(\n    runtime->deserializeCudaEngine(preprocessorPlan->data(), preprocessorPlan->size()));\nif (!mPreprocessorEngine)\n{\n    sample::gLogError << \"Preprocessor engine deserialization failed.\" << std::endl;\n    return false;\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Dynamic Output Dimension\nDESCRIPTION: Sets the first dimension of the output tensor using the declared size tensor to handle dynamic shapes.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/non_zero_plugin/README.md#2025-04-06_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\noutput_dims[0][0] = num_non_zero_size_tensor\n```\n\n----------------------------------------\n\nTITLE: Defining Minimum Package Requirements for TensorRT in Python\nDESCRIPTION: This requirements file specifies the minimal set of dependencies needed to use TensorRT in Python. It includes conditional numpy requirements based on Python version and platform, a Pillow dependency for older Python versions, and a TensorRT wheel installation command with template placeholders.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/python/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\n# This file should contain the minimum possible packages to be able to import tensorrt and use it correctly.\n# This must succeed during builds, so be VERY CAREFUL when you add packages here.\nnumpy==1.18.1; python_version < \"3.8\" and platform_system == \"Windows\"\nnumpy==1.19.4; python_version < \"3.8\" and platform_system != \"Windows\"\nnumpy==1.23.0; python_version >= \"3.8\" and python_version < \"3.10\"\nnumpy==1.23.1; python_version >= \"3.10\" and python_version < \"3.12\"\nnumpy==1.26.4; python_version >= \"3.12\"\nPillow; python_version<\"3.6\"\n##PYTHON_BUILDDIR##/tensorrt_bindings-py3.##PYTHON3_MINOR##/dist/tensorrt-##TENSORRT_PYTHON_VERSION##-cp3##PYTHON3_MINOR##-none-linux_##TARGET##.whl ; python_version==\"3.##PYTHON3_MINOR##\"\n```\n\n----------------------------------------\n\nTITLE: Using Custom Add Operation in Graph\nDESCRIPTION: Shows how to use the registered add operation to create a compound addition expression Y = (X + B) + C in the graph.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/examples/07_creating_a_model_with_the_layer_api/README.md#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n[Y] = graph.add(*graph.add(X, B), C)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies with pip\nDESCRIPTION: Command to install required Python dependencies using pip package manager.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/README.md#2025-04-06_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndownloader.py -d /path/to/data/dir -f /path/to/download.yml\n```\n\n----------------------------------------\n\nTITLE: Downloading Pre-trained EfficientDet Checkpoint\nDESCRIPTION: These commands create a directory, download the pre-trained EfficientDet-D0 checkpoint, and extract its contents.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/EfficientDet/notebooks/EfficientDet-TensorRT8.ipynb#2025-04-06_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n![ ! -d \"tf_checkpoint\" ] && mkdir tf_checkpoint\n!wget https://storage.googleapis.com/cloud-tpu-checkpoints/efficientdet/coco2/efficientdet-d0.tar.gz -P tf_checkpoint\n!tar -xvf tf_checkpoint/efficientdet-d0.tar.gz -C tf_checkpoint\n```\n\n----------------------------------------\n\nTITLE: Running TensorRT Dynamic Reshape Sample with FP16 Precision\nDESCRIPTION: Example command to run the TensorRT dynamic reshape sample using FP16 precision and specifying the data directory.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleDynamicReshape/README.md#2025-04-06_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\n./sample_dynamic_reshape --datadir $TRT_DATADIR/mnist --fp16\n```\n\n----------------------------------------\n\nTITLE: Setting TensorRT Library Path on Windows\nDESCRIPTION: PowerShell commands to extract TensorRT GA build and set the library path environment variable on Windows.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/README.md#2025-04-06_snippet_3\n\nLANGUAGE: powershell\nCODE:\n```\nExpand-Archive -Path TensorRT-10.9.0.34.Windows.win10.cuda-12.8.zip\n$env:TRT_LIBPATH=\"$pwd\\TensorRT-10.9.0.34\\lib\"\n```\n\n----------------------------------------\n\nTITLE: Adding Special Slice Plugin Source Files to TensorRT Build\nDESCRIPTION: Registers the Special Slice plugin source files with the TensorRT build system. It specifies both the implementation file (specialSlicePlugin.cpp) and header file (specialSlicePlugin.h) to be included in the plugin library compilation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/specialSlicePlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_plugin_source(\n    specialSlicePlugin.cpp\n    specialSlicePlugin.h\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Add Operation with Graph Register\nDESCRIPTION: Demonstrates how to create and register a custom add operation using the Graph.layer() API. The function creates an Add operation node that can take two inputs and produce one output.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/examples/07_creating_a_model_with_the_layer_api/README.md#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@gs.Graph.register()\ndef add(self, a, b):\n    return self.layer(op=\"Add\", inputs=[a, b], outputs=[\"add_out\"])\n```\n\n----------------------------------------\n\nTITLE: Inspecting Modified ONNX Model using Polygraphy\nDESCRIPTION: This optional command uses the 'polygraphy inspect model' tool to verify the changes made to the ONNX model. It displays the layers of the modified model to confirm if the changes were applied correctly.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/surgeon/03_modifying_input_shapes/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy inspect model dynamic_identity.onnx --show layers\n```\n\n----------------------------------------\n\nTITLE: Defining Hyperparameters for ResNet50 V1 Training in TensorFlow\nDESCRIPTION: This code block sets up a dictionary of hyperparameters for training the ResNet50 V1 model. It includes data directory, batch size, epochs, steps per epoch, and save directory for the model weights.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/example_resnet50v1.ipynb#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nHYPERPARAMS = {\n    \"tfrecord_data_dir\": \"/media/Data/ImageNet/train-val-tfrecord\",\n    \"batch_size\": 64,\n    \"epochs\": 2,\n    \"steps_per_epoch\": 500,\n    \"train_data_size\": None,\n    \"val_data_size\": None,\n    \"save_root_dir\": \"./weights/resnet_50v1_jupyter\"\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for NVIDIA TensorRT\nDESCRIPTION: Specifies required Python packages with version constraints for the NVIDIA TensorRT project. Includes conditional dependencies for different Python versions and operating systems, with packages for image processing, CUDA integration, Windows support, data handling, and numerical computation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/simple_progress_monitor/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\nPillow>=10.0.0\ncuda-python==12.2.0; python_version <= \"3.10\"\ncuda-python==12.6.0; python_version >= \"3.11\"\npywin32; platform_system == \"Windows\"\npyyaml==6.0.1\nrequests==2.32.2\ntqdm==4.66.4\nnumpy==1.24.4; python_version <= \"3.10\"\nnumpy==1.26.4; python_version >= \"3.11\"\n```\n\n----------------------------------------\n\nTITLE: COCO Dataset Object Labels\nDESCRIPTION: A comprehensive list of object categories from the COCO dataset, including common household items, vehicles, animals, and personal objects. These labels are typically used for training and evaluating object detection and image classification models.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/tensorflow_object_detection_api/labels_coco.txt#2025-04-06_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nperson\nbicycle\ncar\nmotorcycle\nairplane\nbus\ntrain\ntruck\nboat\ntraffic light\nfire hydrant\nstreet sign\nstop sign\nparking meter\nbench\nbird\ncat\ndog\nhorse\nsheep\ncow\nelephant\nbear\nzebra\ngiraffe\nhat\nbackpack\numbrella\nshoe\neye glasses\nhandbag\ntie\nsuitcase\nfrisbee\nskis\nsnowboard\nsports ball\nkite\nbaseball bat\nbaseball glove\nskateboard\nsurfboard\ntennis racket\nbottle\nplate\nwine glass\ncup\nfork\nknife\nspoon\nbowl\nbanana\napple\nsandwich\norange\nbroccoli\ncarrot\nhot dog\npizza\ndonut\ncake\nchair\ncouch\npotted plant\nbed\nmirror\ndining table\nwindow\ndesk\ntoilet\ndoor\ntv\nlaptop\nmouse\nremote\nkeyboard\ncell phone\nmicrowave\noven\ntoaster\nsink\nrefrigerator\nblender\nbook\nclock\nvase\nscissors\nteddy bear\nhair drier\ntoothbrush\nhair brush\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies\nDESCRIPTION: Command to install the necessary Python packages listed in requirements.txt file.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/efficientnet/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip3 install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Installing TensorRT Dependencies with Docker\nDESCRIPTION: Command to pull the NVIDIA TensorFlow Docker image, which simplifies TensorRT and TensorFlow installation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/efficientdet/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull nvcr.io/nvidia/tensorflow:23.07-tf2-py3\n```\n\n----------------------------------------\n\nTITLE: Cloning TensorRT Repository\nDESCRIPTION: Instructions for cloning the TensorRT OSS repository at version 10.9\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/Diffusion/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:NVIDIA/TensorRT.git -b release/10.9 --single-branch\ncd TensorRT\n```\n\n----------------------------------------\n\nTITLE: Running TensorRT ONNX ResNet-50 Inference Sample\nDESCRIPTION: This command runs the Python script onnx_resnet50.py, which creates a TensorRT inference engine from an ONNX model of ResNet-50 and performs inference. It demonstrates how to use the ONNX parser in TensorRT.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/introductory_parser_samples/README.md#2025-04-06_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\npython3 onnx_resnet50.py\n```\n\n----------------------------------------\n\nTITLE: Process Engine Script Execution for JSON Generation\nDESCRIPTION: Python command to execute a utility script that processes a TensorRT engine to generate JSON files. This command is commented out as it requires a specific environment with trtexec installed.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/tutorial.ipynb#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# !python3 ../utils/process_engine.py ../tests/inputs/mobilenet.qat.onnx ../tests/inputs best\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Polygraphy Comparison Example\nDESCRIPTION: This command installs the required dependencies for running the Polygraphy comparison example. It uses pip to install the packages listed in the requirements.txt file.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/01_comparing_frameworks/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Calculating Total Number of Anchors for ResNet101 with 1024x1024 Input\nDESCRIPTION: This snippet demonstrates how to calculate the total number of anchors for a ResNet101 network with a 1024x1024 input image. It breaks down the anchor count for each feature map level.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/proposalLayerPlugin/README.md#2025-04-06_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nAnchors in feature map P2: 256*256*3 \nAnchors in feature map P3: 128*128*3\nAnchors in feature map P4: 64*64*3\nAnchors in feature map P5: 32*32*3\nAnchors in feature map P6(maxpooling): 16*16*3  \n\ntotal number of anchors: 87296*3 = 261888\n```\n\n----------------------------------------\n\nTITLE: Installing Additional Required Packages\nDESCRIPTION: Command to install additional Python packages that may be required for specific Polygraphy functionality.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/README.md#2025-04-06_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install <package_name>\n```\n\n----------------------------------------\n\nTITLE: Using Downloader Helper Function\nDESCRIPTION: Example of using the getFilePath helper function from downloader.py to obtain the full path to downloaded data files.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/README.md#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom downloader import getFilePath\n\ncfg_file_path = getFilePath('samples/python/yolov3_onnx/yolov3.cfg')\n```\n\n----------------------------------------\n\nTITLE: Running Shape Operations Example in Bash\nDESCRIPTION: This Bash command executes the Python script to generate the ONNX model for shape operations and save it as 'model.onnx'.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/examples/09_shape_operations_with_the_layer_api/README.md#2025-04-06_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\npython3 generate.py\n```\n\n----------------------------------------\n\nTITLE: Calculating Step Values for CoordConv\nDESCRIPTION: Formulas for calculating the step values used to generate coordinate matrices for both height and width dimensions. These values determine the increments between -1 and 1 across the coordinate channels.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/coordConvACPlugin/README.md#2025-04-06_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nSTEP_VALUE_H = 2 / (H - 1)\nSTEP_VALUE_W = 2 / (W - 1)\n```\n\n----------------------------------------\n\nTITLE: Downloading SQuAD Dataset for BERT QA\nDESCRIPTION: Downloads the Stanford Question Answering Dataset (SQuAD) which consists of questions posed by crowdworkers on Wikipedia articles, used for testing reading comprehension models.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/BERT-TRT-FP16.ipynb#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!bash ../scripts/download_squad.sh\n```\n\n----------------------------------------\n\nTITLE: Displaying Directory Structure of Polygraphy Extension Module\nDESCRIPTION: Shows the directory structure of the custom extension module 'polygraphy_reshape_destroyer', which mirrors the structure of the Polygraphy repository for easier understanding of parallels between functionality.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/dev/02_extending_polygraphy_run/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n- extension_module/\n    - polygraphy_reshape_destroyer/\n        - backend/\n            - __init__.py   # Controls submodule-level exports\n            - loader.py     # Defines our custom loader.\n            - runner.py     # Defines our custom runner.\n        - args/\n            - __init__.py   # Controls submodule-level exports\n            - loader.py     # Defines command-line argument group for our custom loader.\n            - runner.py     # Defines command-line argument group for our custom runner.\n        - __init__.py       # Controls module-level exports\n        - export.py         # Defines the entry-point for `polygraphy run`.\n    - setup.py              # Builds our module\n```\n\n----------------------------------------\n\nTITLE: Initializing TensorFlow and Loading CIFAR10 Dataset\nDESCRIPTION: This snippet sets up the TensorFlow environment, creates asset folders, and loads the CIFAR10 dataset. It also normalizes the input images.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/notebooks/simple_network_quantize_full.ipynb#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport tensorflow as tf\nfrom tensorflow_quantization import quantize_model\nimport tiny_resnet\nfrom tensorflow_quantization import utils\nimport os\n\ntf.keras.backend.clear_session()\n\n# Create folders to save TF and ONNX models\nassets = utils.CreateAssetsFolders(os.path.join(os.getcwd(), \"tutorials\"))\nassets.add_folder(\"simple_network_quantize_full\")\n\n# Load CIFAR10 dataset\ncifar10 = tf.keras.datasets.cifar10\n(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n\n# Normalize the input image so that each pixel value is between 0 and 1.\ntrain_images = train_images / 255.0\ntest_images = test_images / 255.0\n```\n\n----------------------------------------\n\nTITLE: Using TensorRT Run Command\nDESCRIPTION: Command for displaying help information about the polygraphy run tool usage.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/polygraphy/tools/run/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run -h\n```\n\n----------------------------------------\n\nTITLE: Defining TensorRT Engine Build and Inference Dependency\nDESCRIPTION: Identifies PyCUDA as a necessary package for building and running inference with TensorRT engines, enabling GPU acceleration.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/examples/requirements.txt#2025-04-06_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n# TensorRT: build/infer engine\npycuda\n```\n\n----------------------------------------\n\nTITLE: Bounding Box Encoding for CodeTypeSSD::CORNER_SIZE Method in TensorRT\nDESCRIPTION: Mathematical formulation for the CORNER_SIZE coding method used in nmsPlugin. It shows how bounding boxes are encoded with and without variance encoding, using a combination of corner coordinates normalized by anchor box width and height.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/nmsPlugin/README.md#2025-04-06_snippet_2\n\nLANGUAGE: math\nCODE:\n```\n[(x_{min, gt} - x_{min, anchor}) / w_{anchor}, (y_{min, gt} - y_{min, anchor}) / h_{anchor}, (x_{max, gt} - x_{max, anchor}) / w_{anchor}, (y_{max, gt} - y_{max, anchor}) / h_{anchor}]\n```\n\nLANGUAGE: math\nCODE:\n```\n[(x_{min, gt} - x_{min, anchor}) / w_{anchor} / variance_0, (y_{min, gt} - y_{min, anchor}) / h_{anchor} / variance_1, (x_{max, gt} - x_{max, anchor}) / w_{anchor} / variance_2, (y_{max, gt} - y_{max, anchor}) / h_{anchor} / variance_3]\n```\n\n----------------------------------------\n\nTITLE: Running in Slow Speed Mode\nDESCRIPTION: Command to run the custom runner in slow speed mode, which injects a time.sleep() during inference.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/dev/02_extending_polygraphy_run/README.md#2025-04-06_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run no_op_reshape.onnx --res-des --res-des-speed=slow\n```\n\n----------------------------------------\n\nTITLE: Building Polygraphy Wheel Package\nDESCRIPTION: Command to build a wheel package for Polygraphy using Python's setup tools.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/README.md#2025-04-06_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython setup.py bdist_wheel\n```\n\n----------------------------------------\n\nTITLE: Using Inline Comments for Parameter Documentation in C++\nDESCRIPTION: Example demonstrating how to use inline C-style comments to clarify function parameters when their purpose is not obvious from inspection.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/CODING-GUIDELINES.md#2025-04-06_snippet_5\n\nLANGUAGE: cpp\nCODE:\n```\ndoSomeOperation(/* checkForErrors = */ false);\n```\n\n----------------------------------------\n\nTITLE: Running in Medium Speed Mode\nDESCRIPTION: Command to run the custom runner in medium speed mode, which injects a shorter time.sleep() during inference.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/dev/02_extending_polygraphy_run/README.md#2025-04-06_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run no_op_reshape.onnx --res-des --res-des-speed=medium\n```\n\n----------------------------------------\n\nTITLE: Downloading BERT Large FP16 SQuAD v1.1 Model Checkpoint for QAT\nDESCRIPTION: This command downloads the checkpoint for BERT Large FP16 SQuAD v1.1 model with sequence length of 384 for QAT evaluation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\nbash scripts/download_model.sh pyt v1_1\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing CIFAR10 Dataset\nDESCRIPTION: This code loads the CIFAR10 dataset and normalizes the input images for training.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/notebooks/simple_network_quantize_partial.ipynb#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Load CIFAR10 dataset\ncifar10 = tf.keras.datasets.cifar10\n(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n\n# Normalize the input image so that each pixel value is between 0 and 1.\ntrain_images = train_images / 255.0\ntest_images = test_images / 255.0\n```\n\n----------------------------------------\n\nTITLE: Parsing ONNX Model File in TensorRT\nDESCRIPTION: Code to parse an ONNX model file using the ONNX parser, with error handling if parsing fails. The parsed model creates a TensorRT network representation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleOnnxMNIST/README.md#2025-04-06_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nif (!parser->parseFromFile(model_file, static_cast<int>(sample::gLogger.getReportableSeverity())))\n{\n\t  string msg(\"failed to parse onnx file\");\n\t  sample::gLogger->log(nvinfer1::ILogger::Severity::kERROR, msg.c_str());\n\t  exit(EXIT_FAILURE);\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Doxygen Comments for Documentation in C++\nDESCRIPTION: Examples showing proper Doxygen comment syntax for documenting class interfaces and members in TensorRT codebase.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/CODING-GUIDELINES.md#2025-04-06_snippet_6\n\nLANGUAGE: cpp\nCODE:\n```\n//! This is a Doxygen comment\n//! in C++ style\n\nstruct Foo\n{\n    int x; //!< This is a Doxygen comment for members\n}\n```\n\n----------------------------------------\n\nTITLE: Exporting ONNX Graph with onnx_graphsurgeon in Python\nDESCRIPTION: This code snippet demonstrates the usage of the export_onnx function from the onnx_graphsurgeon library. The function is used to export an ONNX graph, likely after modifications or optimizations have been applied using the Graph Surgeon tool.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/docs/exporters/toc.rst#2025-04-06_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. autofunction:: onnx_graphsurgeon.export_onnx\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch Quantization via pip\nDESCRIPTION: This command installs the PyTorch Quantization toolkit using pip from NVIDIA's PyPI index.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install pytorch-quantization --extra-index-url https://pypi.ngc.nvidia.com\n```\n\n----------------------------------------\n\nTITLE: Formatting Input Batch for GPU Processing\nDESCRIPTION: Converts the input batch to PyTorch tensor, transposes it to the correct channel ordering (CHW format), and moves it to the GPU.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/2. Using PyTorch through ONNX.ipynb#2025-04-06_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ninput_batch_chw = torch.from_numpy(input_batch).transpose(1,3).transpose(2,3)\ninput_batch_gpu = input_batch_chw.to(\"cuda\")\n\ninput_batch_gpu.shape\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT-OSS for Windows x86\nDESCRIPTION: Build commands for TensorRT-OSS on Windows x86 platform using CUDA 12.8. Uses MSBuild for compilation with parallel processing.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/README.md#2025-04-06_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ncd $TRT_OSSPATH\nmkdir -p build\ncd -p build\ncmake .. -DTRT_LIB_DIR=\"$env:TRT_LIBPATH\" -DCUDNN_ROOT_DIR=\"$env:CUDNN_PATH\" -DTRT_OUT_DIR=\"$pwd\\\\out\"\nmsbuild TensorRT.sln /property:Configuration=Release -m:$env:NUMBER_OF_PROCESSORS\n```\n\n----------------------------------------\n\nTITLE: Using process_engine.py Command Line Interface\nDESCRIPTION: Command line interface for process_engine.py showing its usage, arguments, and parameters for building, profiling, and drawing TensorRT engines from ONNX files.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/utils/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nusage: process_engine.py [-h] [--print_only] [--build_engine] [--profile_engine] [--draw_engine] input outdir [trtexec [trtexec ...]]\n\nUtility to build and profile TensorRT engines\n\npositional arguments:\n  input                 input file (ONNX or engine)\n  outdir                directory to store output artifacts\n  trtexec               trtexec commands not including the preceding -- (e.g. int8 shapes=input_ids:32x512,attention_mask:32x512\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --print_only          print the command-line and exit\n  --build_engine, -b    build the engine\n  --profile_engine, -p  engine the engine\n  --draw_engine, -d     draw the engine\n```\n\n----------------------------------------\n\nTITLE: Cleaning Layer DataFrames in TREx\nDESCRIPTION: Demonstrates the clean_df utility function to prepare layer information for further processing.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/api-examples.ipynb#2025-04-06_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclean_convs = trex.clean_df(convs2.copy(), inplace=True)\nclean_convs.iloc[0]['Inputs']\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorRT Dynamic Reshape Sample in CMake\nDESCRIPTION: This CMake snippet sets the source files and parser dependencies for the TensorRT Dynamic Reshape sample. It specifies the main source file and the required ONNX parser, then includes a common CMake template for TensorRT samples.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleDynamicReshape/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(SAMPLE_SOURCES sampleDynamicReshape.cpp)\nset(SAMPLE_PARSERS \"onnx\")\ninclude(../CMakeSamplesTemplate.txt)\n```\n\n----------------------------------------\n\nTITLE: Exporting EfficientNet V2 SavedModel\nDESCRIPTION: Command to export a TensorFlow SavedModel from an EfficientNet V2 checkpoint.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/efficientnet/README.md#2025-04-06_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncd /path/to/automl/efficientnetv2\npython3 infer.py \\\n    --mode tf2bm \\\n    --model_name efficientnetv2-s \\\n    --model_dir ../../efficientnetv2-s/ \\\n    --export_dir ../../efficientnetv2-s/saved_model\n```\n\n----------------------------------------\n\nTITLE: Plotting Input Image for TensorRT Inference\nDESCRIPTION: This snippet uses matplotlib to display the input image before running TensorRT inference. It helps visualize the image that will be processed for semantic segmentation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/SemanticSegmentation/tutorial-runtime.ipynb#2025-04-06_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nplt.imshow(Image.open(input_file))\n```\n\n----------------------------------------\n\nTITLE: Installing Wheel Building Prerequisites\nDESCRIPTION: Command to install the Python wheel package, which is required for building Polygraphy from source.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/README.md#2025-04-06_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install wheel\n```\n\n----------------------------------------\n\nTITLE: Running TensorRT Validation Example\nDESCRIPTION: Command to execute the example script that demonstrates model validation using TrtRunner.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/02_validating_on_a_dataset/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 example.py\n```\n\n----------------------------------------\n\nTITLE: Using Polygraphy Comparator\nDESCRIPTION: Demonstrates how to use the Comparator class to compare accuracy with a custom comparison function.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/CHANGELOG.md#2025-04-06_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\ndef custom_compare(a, b):\n    return abs(a - b) < 1e-5\n\ncomparator = Comparator()\ncomparator.compare_accuracy(runners, compare_func=custom_compare)\n```\n\n----------------------------------------\n\nTITLE: Quantized Fine-tuning Training Loop\nDESCRIPTION: Implements the fine-tuning process with SGD optimizer, learning rate scheduler, and CrossEntropy loss for one epoch.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/examples/finetune_quant_resnet50.ipynb#2025-04-06_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=128,\n    sampler=train_sampler, num_workers=16, pin_memory=True)\n\n# Training takes about one and half hour per epoch on single V100\ntrain_one_epoch(model, criterion, optimizer, data_loader, \"cuda\", 0, 100)\n```\n\n----------------------------------------\n\nTITLE: Drawing Filtered TensorRT Graph\nDESCRIPTION: Command to draw a filtered graph with specific display options and query parameter to show only matching nodes, useful for large engine graphs.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/bin/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntrex draw mymodel.graph.json -pj=mymodel.profile.json --display_metadata --display_regions --display_constant --query=\"transformer_blocks.(12|37).*\"\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Input Mask Encoding for EmbLayerNormPlugin v1\nDESCRIPTION: Shows how contiguous input masks are encoded as a single number representing the count of valid elements. This encoding is only used in version 1 of the plugin. A valid mask must have contiguous '1's followed by '0's.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/embLayerNormPlugin/README.md#2025-04-06_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n111100 => 4\n110000 => 2\n110100: Invalid mask, because it is not contiguous\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch and Related Libraries for CUDA 12.4\nDESCRIPTION: A pip install command to set up PyTorch, torchvision, and torchaudio with CUDA 12.4 support, which is required for model creation and ONNX export to TensorRT.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/0. Running This Guide.ipynb#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n```\n\n----------------------------------------\n\nTITLE: Downloading Pre-trained TFOD EfficientDet Model\nDESCRIPTION: Wget command to download a pre-trained EfficientDet model from the TensorFlow Object Detection Model Zoo.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/efficientdet/README.md#2025-04-06_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nwget http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d0_coco17_tpu-32.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Generating ONNX Model with Python\nDESCRIPTION: Python command to generate an ONNX model with several nodes including identity layers and a fake node that will be removed later. The script saves the model to 'model.onnx'.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/examples/06_removing_nodes/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 generate.py\n```\n\n----------------------------------------\n\nTITLE: Setting Configurable CMake Variables with Default Values\nDESCRIPTION: A CMake macro that sets a variable to a default value if it hasn't been defined yet, with a status message. This is used throughout the build script for configuration parameters.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/python_plugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\n# Sets variable to a value if variable is unset.\nmacro(set_ifndef var val)\n    if(NOT ${var})\n        set(${var} ${val})\n    endif()\n    message(STATUS \"Configurable variable ${var} set to ${${var}}\")\nendmacro()\n```\n\n----------------------------------------\n\nTITLE: Inspecting Inference Results with Polygraphy\nDESCRIPTION: Command to inspect the saved inference outputs with detailed statistics and values.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/inspect/05_inspecting_inference_outputs/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy inspect data outputs.json --show-values\n```\n\n----------------------------------------\n\nTITLE: Cross-Validating Results with Polygraphy Tool\nDESCRIPTION: Uses the Polygraphy tool to compare inference results between TensorRT and ONNX Runtime implementations to ensure consistency across frameworks.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/DeBERTa/README.md#2025-04-06_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run ./test/deberta_original.onnx --trt --onnxrt --workspace=4000000000\n```\n\n----------------------------------------\n\nTITLE: Adding Normalize Plugin Source Files in CMake\nDESCRIPTION: CMake directive to add normalize plugin source files (cpp and header) to the build configuration. This makes the normalize plugin available as part of the TensorRT plugin library.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/normalizePlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_plugin_source(\n    normalizePlugin.cpp\n    normalizePlugin.h\n)\n```\n\n----------------------------------------\n\nTITLE: Device Array View Implementation\nDESCRIPTION: Example showing usage of DeviceArray view() method to create read-only DeviceViews over data\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/CHANGELOG.md#2025-04-06_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndevice_array.view()  # Creates read-only DeviceView over the data\n```\n\n----------------------------------------\n\nTITLE: Setting ONNX Parser Dependency in CMake\nDESCRIPTION: Specifies ONNX as the required parser for the sample project.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleIOFormats/CMakeLists.txt#2025-04-06_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(SAMPLE_PARSERS \"onnx\")\n```\n\n----------------------------------------\n\nTITLE: Comparing Lazy Loader Behavior in Polygraphy\nDESCRIPTION: A demonstration of how similar-looking lazy loader code in Polygraphy can produce different behavior. Shows examples of loader instances versus evaluated engines.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/06_immediate_eval_api/README.md#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Each line in this example looks almost the same, but has significantly\n# different behavior. Some of these lines even cause memory leaks!\nEngineBytesFromNetwork(NetworkFromOnnxPath(\"/path/to/model.onnx\")) # This is a loader instance, not an engine!\nEngineBytesFromNetwork(NetworkFromOnnxPath(\"/path/to/model.onnx\"))() # This is an engine.\nEngineBytesFromNetwork(NetworkFromOnnxPath(\"/path/to/model.onnx\")())) # And it's a loader instance again...\nEngineBytesFromNetwork(NetworkFromOnnxPath(\"/path/to/model.onnx\")())() # Back to an engine!\nEngineBytesFromNetwork(NetworkFromOnnxPath(\"/path/to/model.onnx\"))()() # This throws - can you see why?\n```\n\n----------------------------------------\n\nTITLE: Using Polygraphy Data Loader as Iterable\nDESCRIPTION: Demonstrates how to use the DataLoader class as an iterable with a specified number of iterations.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/CHANGELOG.md#2025-04-06_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\ndata_loader = DataLoader(iterations=100)\nfor data in data_loader:\n    # Process data\n```\n\n----------------------------------------\n\nTITLE: Running Inference and Evaluating F1 Score for BERT PTQ\nDESCRIPTION: These commands run inference using the SQuAD dataset and evaluate the F1 score and exact match score for the BERT PTQ model.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\npython3 inference.py -e engines/bert_large_384_int8mix.engine -s 384 -sq ./squad/dev-v1.1.json -v models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/vocab.txt -o ./predictions.json\npython3 squad/evaluate-v1.1.py  squad/dev-v1.1.json  ./predictions.json 90\n```\n\n----------------------------------------\n\nTITLE: Engine Graph Visualization\nDESCRIPTION: Creates visual graph representations of engine plans using DOT format and renders them as SVG\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/compare_engines.ipynb#2025-04-06_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfor plan in plans:\n    graph = graphing.to_dot(plan, graphing.layer_type_formatter, display_regions=True, expand_layer_details=True)\n    graphing.render_dot(graph, plan.name, 'svg')\n```\n\n----------------------------------------\n\nTITLE: Loading TensorRT Model for Inference\nDESCRIPTION: Initializes the TensorRT model using a custom wrapper class for inference execution.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/1. Introduction.ipynb#2025-04-06_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom onnx_helper import ONNXClassifierWrapper\ntrt_model = ONNXClassifierWrapper(\"resnet_engine_intro.engine\", target_dtype = PRECISION)\n```\n\n----------------------------------------\n\nTITLE: Running TensorRT Progress Monitor Sample\nDESCRIPTION: Command line example showing how to run the progress monitor sample with FP16 precision using the specified data directory.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleProgressMonitor/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./sample_progress_monitor --datadir $TRT_DATADIR/mnist --fp16\n```\n\n----------------------------------------\n\nTITLE: Evaluating TensorFlow Quantized Model Accuracy\nDESCRIPTION: This code evaluates the accuracy of the quantized model immediately after quantization and compares it to the baseline.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/notebooks/simple_network_quantize_specific_class.ipynb#2025-04-06_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n_, q_model_accuracy = q_nn_model.evaluate(test_images, test_labels, verbose=0)\nq_model_accuracy = round(100 * q_model_accuracy, 2)\nprint(\n    \"Test accuracy immediately after quantization:{}, diff:{}\".format(\n        q_model_accuracy, (baseline_model_accuracy - q_model_accuracy)\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Building the TensorRT Custom HardMax Plugin Library\nDESCRIPTION: Configures the build process for the custom HardMax plugin by setting up include directories, defining the library target with source files, enabling C++17 support, and linking necessary dependencies.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/onnx_custom_plugin/CMakeLists.txt#2025-04-06_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\n# -------- BUILDING --------\n\nadd_definitions(-DTENSORRT_BUILD_LIB)\n\n# Add include directories\nget_filename_component(SAMPLES_COMMON_DIR ${CMAKE_SOURCE_DIR}/../../common/ ABSOLUTE)\nget_filename_component(SAMPLES_DIR ${CMAKE_SOURCE_DIR}/../../ ABSOLUTE)\ninclude_directories(${CUDA_INC_DIR} ${TRT_INCLUDE} ${CMAKE_SOURCE_DIR}/plugin/\n                    ${SAMPLES_COMMON_DIR} ${SAMPLES_DIR})\n\n# Define Hardmax plugin library target\nadd_library(\n    customHardmaxPlugin MODULE\n    ${SAMPLES_COMMON_DIR}/logger.cpp ${SAMPLES_DIR}/utils/fileLock.cpp\n    ${CMAKE_SOURCE_DIR}/plugin/customHardmaxPlugin.cpp ${CMAKE_SOURCE_DIR}/plugin/customHardmaxPlugin.h)\n\n# Use C++11\ntarget_compile_features(customHardmaxPlugin PUBLIC cxx_std_17)\n\n# Link TensorRT's nvinfer lib\ntarget_link_libraries(customHardmaxPlugin PRIVATE ${NVINFER_LIB})\ntarget_link_libraries(customHardmaxPlugin PRIVATE ${CUDART_LIB})\ntarget_link_libraries(customHardmaxPlugin PRIVATE ${CUBLAS_LIB})\ntarget_link_libraries(customHardmaxPlugin PRIVATE ${CUDA_LIB})\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for BERT QAT (Turing and Ampere GPUs)\nDESCRIPTION: This command builds a TensorRT engine for BERT QAT on Turing and Ampere GPUs. It uses INT8 mixed precision with QKVToContextPlugin and SkipLayerNormPlugin support.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p engines && python3 builder.py -o engines/bert_large_384_int8mix.engine -b 1 -s 384 --int8 --fp16 --strict -c models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1 -v models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/vocab.txt -x models/fine-tuned/bert_pyt_onnx_large_qa_squad11_amp_fake_quant_v1/bert_large_v1_1_fake_quant.onnx -iln -imh\n```\n\n----------------------------------------\n\nTITLE: Quantizing Layers by Mixed Criteria (Names and Classes)\nDESCRIPTION: Demonstrates how to apply quantization using a combination of layer classes and specific layer names. This example quantizes all Dense layers and the third Conv2D layer by specifying both class names and individual layer names.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/qspec.rst#2025-04-06_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# 1. Create QuantizationSpec object and add layer information\nq_spec = QuantizationSpec()\n\nlayer_name = ['Dense', 'conv2d_2']\nlayer_is_keras_class = [True, False]\n\n\"\"\"\n# Alternatively, each layer configuration can be added one at a time:\nq_spec.add(name='Dense', is_keras_class=True)\nq_spec.add(name='conv2d_2')\n\"\"\"\n\nq_spec.add(name=layer_name, is_keras_class=layer_is_keras_class)\n\n# 2. Quantize model\nq_model = quantize_model(model, quantization_mode='partial', quantization_spec=q_spec)\nq_model.summary()\n\ntf.keras.backend.clear_session()\n```\n\n----------------------------------------\n\nTITLE: Setting G_NUM_BITS for 4-bit Quantization in TensorFlow\nDESCRIPTION: This snippet demonstrates how to use the G_NUM_BITS global variable to perform 4-bit quantization instead of the default 8-bit. It shows the process of getting a pretrained model, setting the quantization bits, and quantizing the model.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/globals.rst#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport tensorflow_quantization\n# get pretrained model\n.....\n\n# perform 4 bit quantization\ntensorflow_quantization.G_NUM_BITS = 4\nq_model = quantize_model(nn_model_original)\n\n# fine-tune model\n.....\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT-OSS Docker Container\nDESCRIPTION: Bash command to build a Docker container for TensorRT-OSS development on Ubuntu 20.04 with CUDA 12.8.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/README.md#2025-04-06_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n./docker/build.sh --file docker/ubuntu-20.04.Dockerfile --tag tensorrt-ubuntu20.04-cuda12.8\n```\n\n----------------------------------------\n\nTITLE: Installing TensorRT Python Dependencies\nDESCRIPTION: Requirements file listing Python packages required for TensorRT. Includes core dependencies like ONNX, PyTorch, torchvision, and support libraries with specific version constraints. Notable is the conditional numpy versioning based on Python version.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/onnx_packnet/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\nonnx==1.16.0\n--extra-index-url https://pypi.ngc.nvidia.com\nonnx-graphsurgeon>=0.3.20\ntorch\ntorchvision\npyyaml==6.0.1\nrequests==2.32.2\ntqdm==4.66.4\nnumpy==1.24.4; python_version <= \"3.10\"\nnumpy==1.26.4; python_version >= \"3.11\"\n```\n\n----------------------------------------\n\nTITLE: Running ONNX Model Generation Script\nDESCRIPTION: Command to execute the Python script that generates an ONNX model with a convolution layer and saves it as 'test_conv.onnx'\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/examples/02_creating_a_model_with_initializer/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 example.py\n```\n\n----------------------------------------\n\nTITLE: Creating GridAnchorGenerator for SSD Network in TensorRT\nDESCRIPTION: This code snippet demonstrates how to set up parameters for creating a GridAnchorGenerator plugin for an SSD network with 6 layers. It specifies the number of layers, minimum and maximum sizes, aspect ratios, variance, and feature map shapes.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/gridAnchorPlugin/README.md#2025-04-06_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nnumLayers=6,\nminSize=0.2,\nmaxSize=0.95,\naspectRatios=[1.0, 2.0, 0.5, 3.0, 0.33],\nvariance=[0.1, 0.1, 0.2, 0.2],\nfeatureMapShapes=[19, 10, 5, 3, 2, 1]\n```\n\n----------------------------------------\n\nTITLE: Adding ROI Align Plugin Source Files in TensorRT\nDESCRIPTION: Adds the source files needed for the ROI Align plugin to the TensorRT build system. The files include CUDA kernel implementations, plugin header and implementation files, and legacy plugin versions.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/roiAlignPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_plugin_source(\n    roiAlignKernel.cu\n    roiAlignKernel.h\n    roiAlignPlugin.cpp\n    roiAlignPlugin.h\n    roiAlignPluginLegacy.cpp\n    roiAlignPluginLegacy.h\n)\n```\n\n----------------------------------------\n\nTITLE: Selecting TensorRT Libraries Based on Module Type\nDESCRIPTION: Determines which TensorRT libraries to link against based on the module type (full, lean, or dispatch), ensuring the correct dependencies are included for each build variant.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/python/CMakeLists.txt#2025-04-06_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nif(${TENSORRT_MODULE} STREQUAL \"tensorrt\")\n    set(TRT_LIBS ${nvinfer_lib_name} ${nvonnxparser_lib_name} ${nvinfer_plugin_lib_name})\nelseif(${TENSORRT_MODULE} STREQUAL \"tensorrt_lean\")\n    set(TRT_LIBS ${nvinfer_lean_lib_name})\nelseif(${TENSORRT_MODULE} STREQUAL \"tensorrt_dispatch\")\n    set(TRT_LIBS ${nvinfer_dispatch_lib_name})\nelse()\n    message(FATAL_ERROR \"Unknown TensorRT module \" ${TENSORRT_MODULE})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Cloning and Setting Up PyTorch Quantization from Source\nDESCRIPTION: These commands clone the TensorRT repository and navigate to the pytorch-quantization directory.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/NVIDIA/TensorRT.git\ncd tools/pytorch-quantization\n```\n\n----------------------------------------\n\nTITLE: Running TensorRT inference and comparing against custom outputs\nDESCRIPTION: Command to run inference on an ONNX model using TensorRT with the custom inputs, then compare the results against the previously generated custom outputs.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/run/06_comparing_with_custom_output_data/README.md#2025-04-06_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run identity.onnx --trt \\\n    --load-inputs custom_inputs.json \\\n    --load-outputs custom_outputs.json\n```\n\n----------------------------------------\n\nTITLE: Generating Models with Custom Data Types\nDESCRIPTION: Command to generate the ONNX models with bfloat16 and float8 weights.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/examples/12_using_numpy_unsupported_dtypes/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 generate.py\n```\n\n----------------------------------------\n\nTITLE: Configuring Data Loaders\nDESCRIPTION: Set up data loaders for ImageNet dataset with specified batch size and workers.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/tutorials/quant_resnet50.rst#2025-04-06_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndata_path = \"PATH to imagenet\"\nbatch_size = 512\n\ntraindir = os.path.join(data_path, 'train')\nvaldir = os.path.join(data_path, 'val')\ndataset, dataset_test, train_sampler, test_sampler = load_data(traindir, valdir, False, False)\n\ndata_loader = torch.utils.data.DataLoader(\n    dataset, batch_size=batch_size,\n    sampler=train_sampler, num_workers=4, pin_memory=True)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test, batch_size=batch_size,\n    sampler=test_sampler, num_workers=4, pin_memory=True)\n```\n\n----------------------------------------\n\nTITLE: TensorRT Logger Initialization\nDESCRIPTION: Initializes TensorRT logger for inference setup with INFO level logging.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/inference.ipynb#2025-04-06_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport tensorrt as trt\nTRT_LOGGER = trt.Logger(trt.Logger.INFO)\n```\n\n----------------------------------------\n\nTITLE: Specifying Artifacts Directory in debug reduce\nDESCRIPTION: Command to specify the artifacts directory for debug reduce, which automatically sorts models from each iteration into good and bad directories.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/how-to/use_debug_reduce_effectively.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n--artifacts polygraphy_debug.onnx\n```\n\n----------------------------------------\n\nTITLE: Installing the Built Wheel Package\nDESCRIPTION: Command to install the built wheel package with pip, including the NVIDIA package index for dependencies.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/dev/02_extending_polygraphy_run/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pip install extension_module/dist/polygraphy_reshape_destroyer-0.0.1-py3-none-any.whl \\\n    --extra-index-url https://pypi.ngc.nvidia.com\n```\n\n----------------------------------------\n\nTITLE: Listing Software Version Requirements in Markdown\nDESCRIPTION: This markdown table displays the required software versions for running the BERT model, including Python, TensorRT, and CUDA versions.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Software | Version |\n| -------- | ------- |\n| Python   | >=3.8   |\n| TensorRT | 10.9    |\n| CUDA     | 12.8    |\n```\n\n----------------------------------------\n\nTITLE: Running TensorRT ONNX ResNet-50 Sample with Custom Data Directory\nDESCRIPTION: This command runs the onnx_resnet50.py script with a custom data directory specified using the -d flag. This is useful when the TensorRT sample data is not installed in the default location.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/introductory_parser_samples/README.md#2025-04-06_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\npython3 onnx_resnet50.py -d /path/to/my/data/\n```\n\n----------------------------------------\n\nTITLE: Initializing TensorRT Analysis Environment\nDESCRIPTION: Sets up the required imports and engine file paths for TensorRT analysis, including trex utilities and IPython widgets\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/engine_report_card.ipynb#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport IPython\nfrom ipywidgets import widgets\nfrom trex import *\nfrom trex.notebook import *\nfrom trex.report_card import *\n\n# Choose an engine file to load.\nengine_name = \"../tests/inputs/mobilenet.qat.onnx.engine\"\nengine_name = \"../tests/inputs/mobilenet_v2_residuals.qat.onnx.engine\"\nset_wide_display()\n```\n\n----------------------------------------\n\nTITLE: Listing Saved Model Files\nDESCRIPTION: This command displays the contents of the exported TensorFlow saved model directory.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/EfficientDet/notebooks/EfficientDet-TensorRT8.ipynb#2025-04-06_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n!ls tf_model/\n```\n\n----------------------------------------\n\nTITLE: Adding Split Plugin Source Files in CMake\nDESCRIPTION: CMake command to add CUDA split plugin source files (split.cu and split.h) to the TensorRT build system. This adds the implementation and header files for the split operation plugin.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/splitPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_plugin_source(\n    split.cu\n    split.h\n)\n```\n\n----------------------------------------\n\nTITLE: PyTorch Package Requirement\nDESCRIPTION: Package requirement specifying PyTorch as a dependency\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/polygraphy/backend/pyt/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntorch\n```\n\n----------------------------------------\n\nTITLE: Initializing TensorRT CMake Project\nDESCRIPTION: Sets up the CMake project for TensorRT, including version information, language requirements, and basic configuration options.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.13 FATAL_ERROR)\ninclude(cmake/modules/set_ifndef.cmake)\ninclude(cmake/modules/find_library_create_target.cmake)\n\nproject(TensorRT\n        LANGUAGES CXX CUDA\n        VERSION ${TRT_VERSION}\n        DESCRIPTION \"TensorRT is a C++ library that facilitates high-performance inference on NVIDIA GPUs and deep learning accelerators.\"\n        HOMEPAGE_URL \"https://github.com/NVIDIA/TensorRT\")\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch and Prerequisites for Quantization\nDESCRIPTION: This installs the required dependencies for PyTorch Quantization, with specific PyTorch versions for different CUDA versions.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n# for CUDA 10.2 users\npip install torch>=1.9.1\n# for CUDA 11.1 users\npip install torch>=1.9.1+cu111\n```\n\n----------------------------------------\n\nTITLE: Creating and Quantizing a Simple CNN Model in TensorFlow\nDESCRIPTION: Example demonstrating how to create a simple convolutional neural network using TensorFlow Keras API and quantize it using the quantize_model function. The model consists of two Conv2D layers with ReLU activations and processes 28x28 input images.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/qmodel.rst#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport tensorflow as tf\nfrom tensorflow_quantization.quantize import quantize_model\n\n# Simple full model quantization.\n# 1. Create a simple network\ninput_img = tf.keras.layers.Input(shape=(28, 28))\nr = tf.keras.layers.Reshape(target_shape=(28, 28, 1))(input_img)\nx = tf.keras.layers.Conv2D(filters=2, kernel_size=(3, 3))(r)\nx = tf.keras.layers.ReLU()(x)\nx = tf.keras.layers.Conv2D(filters=2, kernel_size=(3, 3))(x)\nx = tf.keras.layers.ReLU()(x)\nx = tf.keras.layers.Flatten()(x)\nmodel = tf.keras.Model(input_img, x)\n\nprint(model.summary())\n\n# 2. Quantize the network\nq_model = quantize_model(model)\nprint(q_model.summary())\n```\n\n----------------------------------------\n\nTITLE: Importing Required Modules for TensorRT Inference\nDESCRIPTION: This snippet imports necessary Python modules for TensorRT inference, including numpy for array operations, CUDA for GPU operations, TensorRT for inference, and matplotlib/PIL for image processing.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/SemanticSegmentation/tutorial-runtime.ipynb#2025-04-06_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport os\nimport pycuda.driver as cuda\nimport pycuda.autoinit\nimport tensorrt as trt\n\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nTRT_LOGGER = trt.Logger()\n\n# Filenames of TensorRT plan file and input/output images.\nengine_file = \"fcn-resnet101.engine\"\ninput_file  = \"input.ppm\"\noutput_file = \"output.ppm\"\n```\n\n----------------------------------------\n\nTITLE: Inspecting TensorRT Engine with Polygraphy\nDESCRIPTION: Command to use NVIDIA's Polygraphy tool to inspect the generated TensorRT engine. This provides information about input/output layers, shapes, and memory requirements of the model.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/deploy_to_triton/README.md#2025-04-06_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy inspect model model.plan\n```\n\n----------------------------------------\n\nTITLE: Downloading Test Image\nDESCRIPTION: Command to download a test image for inference demonstration.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/dds_faster_rcnn/README.md#2025-04-06_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nwget https://onnxruntime.ai/images/demo.jpg\n```\n\n----------------------------------------\n\nTITLE: Setting Up External Path and Downloading pybind11 in Bash\nDESCRIPTION: This snippet sets up an external path and clones the pybind11 repository. It demonstrates how to create a directory for external sources and download pybind11.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/python/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport EXT_PATH=~/external\n\nmkdir -p $EXT_PATH && cd $EXT_PATH\ngit clone https://github.com/pybind/pybind11.git\n```\n\n----------------------------------------\n\nTITLE: Preparing Sample Data for TensorRT MNIST Example\nDESCRIPTION: Shell commands for downloading and setting up the MNIST dataset for the TensorRT sample. Installs the Pillow Python package for image processing.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleDynamicReshape/README.md#2025-04-06_snippet_16\n\nLANGUAGE: Bash\nCODE:\n```\nexport TRT_DATADIR=/usr/src/tensorrt/data\npushd $TRT_DATADIR/mnist\npip3 install Pillow\npopd\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting ONNX ResNet50 Model\nDESCRIPTION: Downloads a pretrained ResNet50 model from ONNX model zoo and extracts it using wget and tar commands.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/1. Introduction.ipynb#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!wget https://download.onnxruntime.ai/onnx/models/resnet50.tar.gz -O resnet50.tar.gz\n!tar xzf resnet50.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Adding Softmax Layer to MNIST Network in TensorRT\nDESCRIPTION: Adds a Softmax layer to the end of the MNIST network, setting the softmax axis to 1 for the output shape [1, 10]. Replaces the existing network output with the Softmax output.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleDynamicReshape/README.md#2025-04-06_snippet_9\n\nLANGUAGE: C++\nCODE:\n```\nauto softmax = network->addSoftMax(*network->getOutput(0));\nsoftmax->setAxes(1 << 1);\nnetwork->unmarkOutput(*network->getOutput(0));\nnetwork->markOutput(*softmax->getOutput(0));\n```\n\n----------------------------------------\n\nTITLE: Importing ONNX Model with GraphSurgeon\nDESCRIPTION: Demonstrates how to import an ONNX model into the ONNX GraphSurgeon IR using the high-level API.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/README.md#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ngraph = gs.import_onnx(onnx.load(\"model.onnx\"))\n```\n\n----------------------------------------\n\nTITLE: Marking Non-Parallel Tests in Python with pytest\nDESCRIPTION: This snippet demonstrates how to use the pytest.mark.serial decorator to mark tests that should not be executed in parallel. These tests will be run serially by the build system.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/tests/README.md#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@pytest.mark.serial\ndef my_not_parallel_test():\n    ...\n```\n\n----------------------------------------\n\nTITLE: Visualizing Quantization Aware Training Workflow with Mermaid\nDESCRIPTION: A flowchart diagram showing the three-step workflow of Quantization Aware Training: starting with a pre-trained model, applying quantization, and then fine-tuning the model.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/getting_started.ipynb#2025-04-06_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n        id1(Pre-trained model) --> id2(Quantize) --> id3(Fine-tune)\n```\n\n----------------------------------------\n\nTITLE: Engine Execution Flow for INT8 Inference\nDESCRIPTION: Complete sequence for running inference including context creation, buffer management, and synchronization.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleINT8API/README.md#2025-04-06_snippet_3\n\nLANGUAGE: C++\nCODE:\n```\nauto context = mEngine->createExecutionContext();\ncudaStream_t stream;\ncudaStreamCreate(&stream);\n\nbuffers.copyInputToDeviceAsync(stream);\ncontext->enqueueV3(input_stream))\nbuffers.copyOutputToHostAsync(stream);\n\ncudaStreamSynchronize(stream);\ncudaStreamDestroy(stream);\n\noutputCorrect = verifyOutput(buffers);\n```\n\n----------------------------------------\n\nTITLE: Adding decodeBbox3D Plugin Source Files with CMake\nDESCRIPTION: CMake directive to include the decodeBbox3D plugin source files in the TensorRT build. This instructs the build system to compile and link the implementation (.cpp) and include the header (.h) for the 3D bounding box decoding functionality.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/decodeBbox3DPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_plugin_source(\n    decodeBbox3D.cpp\n    decodeBbox3D.h\n)\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Operations with Opsets in Python\nDESCRIPTION: Demonstrates how to register custom operations for specific ONNX opsets using the Graph.register() method. This allows extending GraphSurgeon's functionality for particular ONNX versions.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/CHANGELOG.md#2025-04-06_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nGraph.register(opsets=...)\n```\n\n----------------------------------------\n\nTITLE: Instantiating ResNet50 V1 Model in TensorFlow\nDESCRIPTION: This code creates an instance of the ResNet50 V1 model using TensorFlow's Keras API. It loads pre-trained ImageNet weights and configures the model for 1000 classes with softmax activation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/example_resnet50v1.ipynb#2025-04-06_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel = tf.keras.applications.ResNet50(\n    include_top=True,\n    weights=\"imagenet\",\n    classes=1000,\n    classifier_activation=\"softmax\",\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Parameters for regionPlugin in TensorRT\nDESCRIPTION: Lists and describes the parameters used to create an instance of the Region plugin, including number of bounding boxes, coordinates, classes, and input dimensions.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/regionPlugin/README.md#2025-04-06_snippet_2\n\nLANGUAGE: Markdown\nCODE:\n```\n| Type     | Parameter                | Description\n|----------|--------------------------|--------------------------------------------------------\n|`int`     |`num`                     |The number of predicted bounding box for each grid cell.\n|`int`     |`coords`                  |The number of coordinates for the bounding box. This value has to be `4`. Other values for `coords` are not supported currently.\n|`int`     |`classes`                 |The number of candidate classes to be predicted.\n|`smTree`  |`softmaxTree`             |When performing yolo9000, `softmaxTree` is helping to perform Softmax on confidence scores, for example, to get the precise candidate classes through the word-tree structured candidate classes definition. `softmaxTree` is not required for non-hierarchical classification. The definition of `softmaxTree` can be found in [NvInferPlugin.h](https://docs.nvidia.com/deeplearning/sdk/tensorrt-api/c_api/_nv_infer_plugin_8h_source.html) and [here](https://docs.nvidia.com/deeplearning/sdk/tensorrt-api/c_api/structnvinfer1_1_1plugin_1_1softmax_tree.html).\n|`bool`    |`hasSoftmaxTree`          |If `softmaxTree` is not `nullptr`, it is `true`; else it is `false`.\n|`int`     |`C`                       |The number of channels in the input tensor. `C = num * (coords + 1 + classes)` has to be satisfied.\n|`int`     |`H`                       |The height of the input tensor (feature map).\n|`int`     |`W`                       |The width of the input tensor (feature map).\n```\n\n----------------------------------------\n\nTITLE: AI Art Generation Text Prompts\nDESCRIPTION: Collection of text prompts used to generate images with AI art models. Each prompt specifies artistic style, composition, lighting, details, and referenced artists to influence the output.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/Diffusion/calibration_data/calibration-prompts.txt#2025-04-06_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nPortrait of Beautiful blonde Slavic woman in her early 30ï¿½s, league of legends, LOL, fantasy, d&d,  digital painting, artstation, concept art, sharp focus, illustration, art by greg rutkowski and alphonse mucha\n```\n\n----------------------------------------\n\nTITLE: Installing Polygraphy Wheel on Linux\nDESCRIPTION: Command to install the built Polygraphy wheel package manually on Linux systems.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/README.md#2025-04-06_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install Polygraphy/dist/polygraphy-*-py2.py3-none-any.whl\n```\n\n----------------------------------------\n\nTITLE: Performance Overview Widgets\nDESCRIPTION: Displays performance overview widgets for each engine plan\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/compare_engines.ipynb#2025-04-06_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(plan1.name)\nreport_card_perf_overview_widget(plan1)\n```\n\nLANGUAGE: python\nCODE:\n```\nprint(plan2.name)\nreport_card_perf_overview_widget(plan2)\n```\n\n----------------------------------------\n\nTITLE: Initializing ONNX Parser in TensorRT\nDESCRIPTION: Creates an ONNX parser to convert the ONNX model to a TensorRT network. It initializes the parser with the network definition and logger.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleOnnxMnistCoordConvAC/README.md#2025-04-06_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nauto parser = nvonnxparser::createParser(*network, sample::gLogger.getTRTLogger());\n```\n\n----------------------------------------\n\nTITLE: Adding GELU Plugin Source Files in CMake\nDESCRIPTION: This CMake snippet adds the source files necessary for the GELU (Gaussian Error Linear Unit) plugin in the TensorRT project. It includes a CUDA kernel file, a C++ implementation file, and a header file.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/geluPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_plugin_source(\n    geluKernel.cu\n    geluPlugin.cpp\n    geluPlugin.h\n)\n```\n\n----------------------------------------\n\nTITLE: Describing GeluPlugin Input Structure in Markdown\nDESCRIPTION: Specifies the input tensor shape for the GeluPlugin, where S and E are dimensions and B is the batch size.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/geluPlugin/README.md#2025-04-06_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n`input`\ninput is a tensor with shape `[S, B, E]` where `B` is the batch size.\n```\n\n----------------------------------------\n\nTITLE: Importing ONNX GraphSurgeon Tensor Class via reStructuredText Directive\nDESCRIPTION: This directive imports and auto-documents the Tensor class from the onnx_graphsurgeon module. The autoclass directive is used to automatically generate documentation based on the class's docstrings and methods.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/docs/ir/tensor/tensor.rst#2025-04-06_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autoclass:: onnx_graphsurgeon.Tensor\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT Engine for BERT QAT (Volta GPU)\nDESCRIPTION: This command builds a TensorRT engine for BERT QAT on Volta GPU. It doesn't support QKVToContextPlugin or SkipLayerNormPlugin running with INT8 I/O.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p engines && python3 builder.py -o engines/bert_large_384_int8mix.engine -b 1 -s 384 --int8 --fp16 --strict -c models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1 -v models/fine-tuned/bert_tf_ckpt_large_qa_squad2_amp_384_v19.03.1/vocab.txt -x models/fine-tuned/bert_pyt_onnx_large_qa_squad11_amp_fake_quant_v1/bert_large_v1_1_fake_quant.onnx\n```\n\n----------------------------------------\n\nTITLE: Listing EfficientDet Sample Location\nDESCRIPTION: This command displays the contents of the EfficientDet sample directory in the TensorRT installation path.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/EfficientDet/notebooks/EfficientDet-TensorRT8.ipynb#2025-04-06_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n!ls -ltr $TRT_OSSPATH/samples/python/efficientdet\n```\n\n----------------------------------------\n\nTITLE: Preprocessing Input Data for MNIST Model\nDESCRIPTION: Applies necessary preprocessing to the input image data, including normalization based on PyTorch's transform parameters.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleOnnxMnistCoordConvAC/README.md#2025-04-06_snippet_4\n\nLANGUAGE: C++\nCODE:\n```\nconst float PYTORCH_NORMALIZE_MEAN = 0.1307;\nconst float PYTORCH_NORMALIZE_STD = 0.3081;\nhostDataBuffer[i] = ((1.0 - float(fileData[i] / 255.0)) - PYTORCH_NORMALIZE_MEAN) / PYTORCH_NORMALIZE_STD;\n```\n\n----------------------------------------\n\nTITLE: Setting qgrid Backend for Table Display in TREx (Python)\nDESCRIPTION: Shows how to change the backend library used for rendering tables in TREx to qgrid, as an alternative to the default dtale backend.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/KNOWN_ISSUES.md#2025-04-06_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrex.set_table_display_backend(display_df_qgrid)\n```\n\n----------------------------------------\n\nTITLE: Promoting Plugin Sources to Parent Scope in CMake\nDESCRIPTION: Conditionally sets plugin source variables in the parent scope, allowing the parent CMake file to access the plugin sources defined in this file and its subdirectories.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/bertQKVToContextPlugin/CMakeLists.txt#2025-04-06_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif (NOT DEFINED ${TRT_BUILD_ENABLE_NEW_PLUGIN_FLOW})\n    set(PLUGIN_SOURCES ${PLUGIN_SOURCES} PARENT_SCOPE)\n    set(PLUGIN_CU_SOURCES ${PLUGIN_CU_SOURCES} PARENT_SCOPE)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Input Shape for Anchors Tensor in Efficient NMS Plugin\nDESCRIPTION: Specifies the optional input shape for the anchors tensor in the Efficient NMS plugin, used in Fused Box Decoder mode. The shape can be [1, number_boxes, 4] or [batch_size, number_boxes, 4], with float32 or float16 data types.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/efficientNMSPlugin/README.md#2025-04-06_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n> **Input Shape:** `[1, number_boxes, 4]` or `[batch_size, number_boxes, 4]`\n>\n> **Data Type:** `float32` or `float16`\n```\n\n----------------------------------------\n\nTITLE: Local Installation of TensorFlow Quantization\nDESCRIPTION: Commands for locally installing NVIDIA TensorFlow 2.x Quantization toolkit without Docker. This includes cloning the repository, running the installation script, and testing the installation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ cd ~/\n$ git clone https://github.com/NVIDIA/TensorRT.git\n$ cd TensorRT/tools/tensorflow-quantization\n$ ./install.sh\n$ cd tests\n$ python3 -m pytest quantize_test.py -rP\n```\n\n----------------------------------------\n\nTITLE: Generating Static ONNX Model\nDESCRIPTION: Command to generate an initial ONNX model with static input shape and internal nodes including static Reshape layers.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/examples/10_dynamic_batch_size/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 generate.py\n```\n\n----------------------------------------\n\nTITLE: Checking TensorRT Version in Python\nDESCRIPTION: This snippet prints the installed TensorRT version using Python. It's useful for verifying the TensorRT installation and version compatibility.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/SemanticSegmentation/tutorial-runtime.ipynb#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!python3 -c 'import tensorrt; print(\"TensorRT version: {}\".format(tensorrt.__version__))'\n```\n\n----------------------------------------\n\nTITLE: Inspecting Generated Data - Polygraphy Command\nDESCRIPTION: Command to inspect the generated data using Polygraphy's inspect data functionality.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/dev/01_writing_cli_tools/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy inspect data data.json -s\n```\n\n----------------------------------------\n\nTITLE: Platform-Specific Python Library Configuration\nDESCRIPTION: Sets up platform-specific paths for Python includes and libraries, handling differences between Windows (MSVC) and Linux build environments.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/python/CMakeLists.txt#2025-04-06_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif(MSVC)\n    find_path(\n        PY_INCLUDE Python.h\n        HINTS ${WIN_EXTERNALS}/${PYTHON} ${EXT_PATH}/${PYTHON}\n        PATH_SUFFIXES include)\n    find_path(\n        PY_LIB_DIR ${PYTHON_LIB_NAME}.lib\n        HINTS ${WIN_EXTERNALS}/${PYTHON} ${EXT_PATH}/${PYTHON}\n        PATH_SUFFIXES lib)\n    message(STATUS \"PY_LIB_DIR: ${PY_LIB_DIR}\")\nelse()\n    find_path(\n        PY_INCLUDE Python.h\n        HINTS ${EXT_PATH}/${PYTHON} /usr/include/${PYTHON}\n        PATH_SUFFIXES include)\nendif()\n```\n\n----------------------------------------\n\nTITLE: RST Module Documentation for AMP Wrapper\nDESCRIPTION: Autodoc directive for documenting the AMP wrapper utility module.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/utils.rst#2025-04-06_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: pytorch_quantization.utils.amp_wrapper\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: TensorRT Dynamic Reshape Sample Output\nDESCRIPTION: Expected output when running the TensorRT dynamic reshape sample successfully, showing input visualization and classification probabilities.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleDynamicReshape/README.md#2025-04-06_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\n&&&& RUNNING TensorRT.sample_dynamic_reshape # ./sample_dynamic_reshape\n----------------------------------------------------------------\nInput filename:   ../../../../../data/samples/mnist/mnist.onnx\nONNX IR version:  0.0.3\nOpset version:    8\nProducer name:    CNTK\nProducer version: 2.5.1\nDomain:           ai.cntk\nModel version:    1\nDoc string:  \n----------------------------------------------------------------\n[W] [TRT] onnx2trt_utils.cpp:214: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n[W] [TRT] onnx2trt_utils.cpp:214: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n[I] [TRT] Detected 1 inputs and 1 output network tensors.\n[I] [TRT] Detected 1 inputs and 1 output network tensors.\n[I] Profile dimensions in preprocessor engine:\n[I]     Minimum = (1, 1, 1, 1)\n[I]     Optimum = (1, 1, 28, 28)\n[I]     Maximum = (1, 1, 56, 56)\n[I] Input:\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@@@@@@@@@@@*.  .*@@@@@@@@@@@\n@@@@@@@@@@*.     +@@@@@@@@@@\n@@@@@@@@@@. :#+   %@@@@@@@@@\n@@@@@@@@@@.:@@@+  +@@@@@@@@@\n@@@@@@@@@@.:@@@@: +@@@@@@@@@\n@@@@@@@@@@=%@@@@: +@@@@@@@@@\n@@@@@@@@@@@@@@@@# +@@@@@@@@@\n@@@@@@@@@@@@@@@@* +@@@@@@@@@\n@@@@@@@@@@@@@@@@: +@@@@@@@@@\n@@@@@@@@@@@@@@@@: +@@@@@@@@@\n@@@@@@@@@@@@@@@* .@@@@@@@@@@\n@@@@@@@@@@%**%@. *@@@@@@@@@@\n@@@@@@@@%+.  .: .@@@@@@@@@@@\n@@@@@@@@=  ..   :@@@@@@@@@@@\n@@@@@@@@: *@@:  :@@@@@@@@@@@\n@@@@@@@%  %@*    *@@@@@@@@@@\n@@@@@@@%  ++  ++ .%@@@@@@@@@\n@@@@@@@@-    +@@- +@@@@@@@@@\n@@@@@@@@=  :*@@@# .%@@@@@@@@\n@@@@@@@@@+*@@@@@%.  %@@@@@@@\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n\n[I] Output:\n[I]  Prob 0  0.0000 Class 0: \n[I]  Prob 1  0.0000 Class 1: \n[I]  Prob 2  1.0000 Class 2: **********\n[I]  Prob 3  0.0000 Class 3: \n[I]  Prob 4  0.0000 Class 4: \n[I]  Prob 5  0.0000 Class 5: \n[I]  Prob 6  0.0000 Class 6: \n[I]  Prob 7  0.0000 Class 7: \n[I]  Prob 8  0.0000 Class 8: \n[I]  Prob 9  0.0000 Class 9: \n&&&& PASSED TensorRT.sample_dynamic_reshape # ./sample_dynamic_reshape\n```\n\n----------------------------------------\n\nTITLE: Output Tensor Structure Example\nDESCRIPTION: Example showing the output tensor shape and structure from the bertQKVToContextPlugin. Output has shape [S, B, E, 1, 1] where B is batch size and E is hidden size.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/bertQKVToContextPlugin/README.md#2025-04-06_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n[S, B, E, 1, 1]\n```\n\n----------------------------------------\n\nTITLE: Sample Help Command\nDESCRIPTION: Command to display available options and usage information for the progress monitor sample.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleProgressMonitor/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n./sample_progress_monitor [-h] [--datadir=/path/to/data/dir/] [--useDLA=N] [--fp16 or --int8]\n```\n\n----------------------------------------\n\nTITLE: Defining Size Tensor Dimensionality\nDESCRIPTION: Specifies that the size tensor output is a scalar (0-dimensional tensor) by setting the number of dimensions to zero.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleNonZeroPlugin/README.md#2025-04-06_snippet_5\n\nLANGUAGE: C++\nCODE:\n```\noutputs[1].nbDims = 0;\n```\n\n----------------------------------------\n\nTITLE: Listing Plugin Matches Without Saving Configuration\nDESCRIPTION: Command for listing matches of a plugin in an ONNX model without saving the configuration. This is useful for previewing potential substitutions before committing to changes.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/plugin/01_match_and_replace_plugin/README.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy plugin list toy_subgraph.onnx \\\n    --plugin-dir ./plugins\n```\n\n----------------------------------------\n\nTITLE: Installing ONNX GraphSurgeon\nDESCRIPTION: Command to install the ONNX GraphSurgeon package from NVIDIA's PyPI repository.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/efficientnet/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip3 install onnx-graphsurgeon --index-url https://pypi.ngc.nvidia.com\n```\n\n----------------------------------------\n\nTITLE: Benchmarking TensorRT Model Performance\nDESCRIPTION: Measures the inference time of the TensorRT model using IPython's timeit magic command.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/1. Introduction.ipynb#2025-04-06_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n%%timeit\ntrt_model.predict(dummy_input_batch)[:10]\n```\n\n----------------------------------------\n\nTITLE: Downloading and Extracting Python Headers in Bash\nDESCRIPTION: This snippet shows how to download and extract Python headers for version 3.10. It includes downloading the source code, extracting it, and copying the Include directory contents to the appropriate location.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/python/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nwget https://www.python.org/ftp/python/3.10.11/Python-3.10.11.tgz\ntar -xvf Python-3.10.11.tgz\nmkdir -p $EXT_PATH/python3.10/include\ncp -r Python-3.10.11/Include/* $EXT_PATH/python3.10/include\n```\n\n----------------------------------------\n\nTITLE: Pulling TensorRT Docker Image\nDESCRIPTION: Command to pull the NVIDIA TensorRT Docker image for simplifying TensorRT installation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/dds_faster_rcnn/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker pull nvcr.io/nvidia/tensorrt:25.01-py3\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for TensorRT ONNX Parsing\nDESCRIPTION: This command installs the necessary Python dependencies for the TensorRT ONNX parsing sample. It uses pip to install the requirements listed in the requirements.txt file.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/introductory_parser_samples/README.md#2025-04-06_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\npip3 install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Copying Input Data to GPU for TensorRT Inference\nDESCRIPTION: Transfers input data from host memory to device memory using CUDA memcpy before running inference with TensorRT engines.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleDynamicReshape/README.md#2025-04-06_snippet_11\n\nLANGUAGE: C++\nCODE:\n```\nCHECK(cudaMemcpy(mInput.deviceBuffer.data(), mInput.hostBuffer.data(), mInput.hostBuffer.nbBytes(), cudaMemcpyHostToDevice));\n```\n\n----------------------------------------\n\nTITLE: Accessing Tensor Inputs/Outputs in Python\nDESCRIPTION: Demonstrates the use of convenience functions i() and o() for Tensor objects to easily access connected input and output tensors. This simplifies graph traversal and manipulation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/CHANGELOG.md#2025-04-06_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ntensor.i()\ntensor.o()\n```\n\n----------------------------------------\n\nTITLE: Specifying Dependencies for TensorRT Notebooks\nDESCRIPTION: This snippet lists all the required Python packages and their versions for running TensorRT notebooks. It includes packages for interactive widgets (ipywidgets), data visualization tools (dtale, plotly), web frameworks (Flask, Werkzeug), and Jupyter environments.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/requirements-notebook.txt#2025-04-06_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# Packages required for using the notebooks.\nipywidgets==8.1.1\ndtale==3.8.1\nWerkzeug==2.3.7\nFlask==2.2.2\nplotly\njupyter\njupyterlab\nnetron\n```\n\n----------------------------------------\n\nTITLE: Launching NVIDIA PyTorch Container\nDESCRIPTION: Command to launch NVIDIA PyTorch container with GPU support and workspace mounting\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/Diffusion/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -it --gpus all -v $PWD:/workspace nvcr.io/nvidia/pytorch:25.01-py3 /bin/bash\n```\n\n----------------------------------------\n\nTITLE: Running TensorRT Inference for FCN-ResNet101\nDESCRIPTION: This snippet loads the TensorRT engine and runs the inference pipeline on the input image. It demonstrates how to use the previously defined functions to perform semantic segmentation using TensorRT.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/SemanticSegmentation/tutorial-runtime.ipynb#2025-04-06_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nprint(\"Running TensorRT inference for FCN-ResNet101\")\nwith load_engine(engine_file) as engine:\n    infer(engine, input_file, output_file)\n```\n\n----------------------------------------\n\nTITLE: Importing TensorRT\nDESCRIPTION: Imports the TensorRT module to verify it's installed correctly before proceeding with model conversion.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/2. Using PyTorch through ONNX.ipynb#2025-04-06_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport tensorrt\n```\n\n----------------------------------------\n\nTITLE: Preparing MNIST Sample Data for TensorRT\nDESCRIPTION: These Bash commands show how to download and set up the MNIST sample data for use with the TensorRT sample. It includes setting environment variables and installing required Python packages.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleIOFormats/README.md#2025-04-06_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\nexport TRT_DATADIR=/usr/src/tensorrt/data\npushd $TRT_DATADIR/mnist\npip3 install Pillow\npopd\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for TensorRT Sample\nDESCRIPTION: Command to install the required Python dependencies for running the TensorRT sample from the requirements.txt file.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/simple_progress_monitor/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip3 install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Rendering Extended Plan Graph\nDESCRIPTION: Visualizes the TensorRT engine plan graph with extended information\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/engine_report_card.ipynb#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nreport_card_draw_plan_graph_extended(plan, engine_name)\n```\n\n----------------------------------------\n\nTITLE: Installing Hugging Face Transformers and PyTorch Dependencies\nDESCRIPTION: Installs the necessary dependencies including a specific version of Hugging Face Transformers library and PyTorch 1.8.1 with CUDA 11.1 support required for running BERT with TensorRT.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/notebooks/BERT-TRT-INT8-QAT-sparse.ipynb#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n!cd /tmp && git clone https://github.com/vinhngx/transformers && cd transformers && pip install .\n!pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html\n```\n\n----------------------------------------\n\nTITLE: Downloading Sample Image for Inference Testing\nDESCRIPTION: Command to download a sample image for testing the inference capabilities of the deployed model. This provides input data for the Triton client to query the server.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/deploy_to_triton/README.md#2025-04-06_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nwget  -O img1.jpg \"https://www.hakaimagazine.com/wp-content/uploads/header-gulf-birds.jpg\"\n```\n\n----------------------------------------\n\nTITLE: Adding PillarScatter Plugin Source Files to TensorRT Build\nDESCRIPTION: CMake directive to include PillarScatter plugin source files (cpp and header) in the TensorRT build system. Uses the add_plugin_source function to register the plugin source files.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/pillarScatterPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_plugin_source(\n    pillarScatter.cpp\n    pillarScatter.h\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorRT Plugin Build Options\nDESCRIPTION: Defines build configuration and list of plugins to be included in the TensorRT build. Includes option for BERT QKV plugin and defines core plugin names.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (${TRT_BUILD_ENABLE_NEW_PLUGIN_FLOW})\n\noption(TRT_BUILD_INCLUDE_BERT_QKV_PLUGIN \"Build the BERT QKV to Context Plugin and related plugins.\" ON)\n\nset(TRT_PLUGIN_NAMES\n    batchedNMSPlugin\n    batchTilePlugin\n    clipPlugin\n    coordConvACPlugin\n    # ... other plugins ...\n    voxelGeneratorPlugin\n)\n```\n\n----------------------------------------\n\nTITLE: Adding MultilevelProposeROI Plugin Sources in CMake\nDESCRIPTION: CMake command to add source files for the MultilevelProposeROI plugin to the build system. Includes the main implementation file, header file, and configuration header.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/multilevelProposeROI/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_plugin_source(\n    multilevelProposeROIPlugin.cpp\n    multilevelProposeROIPlugin.h\n    tlt_mrcnn_config.h\n)\n```\n\n----------------------------------------\n\nTITLE: Adding ScatterElements Plugin Source Files in CMake for TensorRT\nDESCRIPTION: This CMake command adds multiple source files for the ScatterElements plugin to the TensorRT build. It includes CUDA headers, C++ implementation files, and legacy support files necessary for the plugin's functionality.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/scatterElementsPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_plugin_source(\n    atomics.cuh\n    reducer.cuh\n    scatterElementsCommon.h\n    scatterElementsPlugin.cpp\n    scatterElementsPlugin.h\n    scatterElementsPluginKernel.cu\n    scatterElementsPluginKernel.h\n    scatterElementsPluginLegacy.cpp\n    scatterElementsPluginLegacy.h\n    TensorInfo.cuh\n)\n```\n\n----------------------------------------\n\nTITLE: Cloning TensorRT Repository\nDESCRIPTION: Git commands to clone the TensorRT repository and update submodules.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone -b main https://github.com/nvidia/TensorRT TensorRT\ncd TensorRT\ngit submodule update --init --recursive\n```\n\n----------------------------------------\n\nTITLE: Adding CoordConvAC Plugin Source Files in CMake for NVIDIA TensorRT\nDESCRIPTION: This CMake command adds the source files necessary for the CoordConvAC (Coordinate Convolution with Attention Channel) plugin to the TensorRT project. It includes the main C++ implementation file, the header file, and the CUDA kernel file.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/coordConvACPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_plugin_source(\n    coordConvACPlugin.cpp\n    coordConvACPlugin.h\n    coordConvACPluginKernels.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Defining MultilevelCropAndResize Plugin Parameters in TensorRT\nDESCRIPTION: This snippet outlines the parameters used to create a MultilevelCropAndResize plugin instance. It specifies the pooled_size for the output feature area and the image_size for the input image dimensions.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/multilevelCropAndResizePlugin/README.md#2025-04-06_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Type             | Parameter                       | Description\n|------------------|---------------------------------|--------------------------------------------------------\n|`int`             |`pooled_size`                    | The spatial size of a feature area after ROIAlgin will be `[pooled_size, pooled_size]`  \n|`int[3]`          |`image_size`                     | The size of the input image in CHW. Defaults to [3, 832, 1344]\n```\n\n----------------------------------------\n\nTITLE: Creating VFC Shared Library Target in CMake for TensorRT Plugins\nDESCRIPTION: Defines and configures a shared library target for VFC (Video Frame Compression) TensorRT plugins. It sets compile flags, include directories, and links against necessary libraries.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/CMakeLists.txt#2025-04-06_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nset_source_files_properties(${VFC_PLUGIN_CU_SOURCES} PROPERTIES COMPILE_FLAGS \"${GENCODES} ${ENABLED_SMS}\")\nlist(APPEND VFC_PLUGIN_SOURCES \"${VFC_PLUGIN_CU_SOURCES}\")\n\nadd_library(${VFC_SHARED_TARGET} SHARED ${VFC_PLUGIN_SOURCES})\n\ntarget_include_directories(\n    ${VFC_SHARED_TARGET}\n    PUBLIC ${PROJECT_SOURCE_DIR}/include\n    PRIVATE ${PROJECT_SOURCE_DIR}/common\n    PUBLIC ${CUDA_INSTALL_DIR}/include\n    PRIVATE ${TARGET_DIR})\n\nset_target_properties(\n    ${VFC_SHARED_TARGET}\n    PROPERTIES CXX_STANDARD \"17\"\n               CXX_STANDARD_REQUIRED \"YES\"\n               CXX_EXTENSIONS \"NO\"\n               ARCHIVE_OUTPUT_DIRECTORY \"${TRT_OUT_DIR}\"\n               LIBRARY_OUTPUT_DIRECTORY \"${TRT_OUT_DIR}\"\n               RUNTIME_OUTPUT_DIRECTORY \"${TRT_OUT_DIR}\")\n\nif(MSVC)\n    set_target_properties(${VFC_SHARED_TARGET} PROPERTIES LINK_FLAGS \"/DEF:${VFC_PLUGIN_EXPORT_DEF}\")\nelse()\n    set_target_properties(\n        ${VFC_SHARED_TARGET}\n        PROPERTIES\n            LINK_FLAGS\n            \"-Wl,--exclude-libs,ALL -Wl,-Bsymbolic -Wl,--version-script=${VFC_PLUGIN_EXPORT_MAP} -Wl,--no-undefined\")\nendif()\n\nset_target_properties(${VFC_SHARED_TARGET} PROPERTIES DEBUG_POSTFIX ${TRT_DEBUG_POSTFIX})\n\nset_target_properties(${VFC_SHARED_TARGET} PROPERTIES VERSION ${TRT_VERSION} SOVERSION ${TRT_SOVERSION})\n\nset_property(TARGET ${VFC_SHARED_TARGET} PROPERTY CUDA_STANDARD 17)\n\ntarget_link_directories(${VFC_SHARED_TARGET} PUBLIC ${CUDA_ROOT}/lib)\n\ntarget_link_libraries(${VFC_SHARED_TARGET} ${CUDART_LIB} ${${nvinfer_lib_name}_LIB_PATH} ${CMAKE_DL_LIBS})\n\n# Needed when static linking CUDART\nif(NOT MSVC)\n    target_link_libraries(${VFC_SHARED_TARGET} Threads::Threads ${RT_LIB})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Generating TensorRT and ONNX-Runtime Comparison Script\nDESCRIPTION: Demonstrates generating a Python script that compares a model between TensorRT and ONNX-Runtime engines. This command creates a complete script with all the necessary Polygraphy API calls.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run --gen - model.onnx --trt --onnxrt\n```\n\n----------------------------------------\n\nTITLE: Installing and Testing TensorFlow Quantization Inside Docker Container\nDESCRIPTION: Commands to navigate to the mounted repository inside the Docker container, install the toolkit, and run tests to verify the installation was successful.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ cd /home/tensorflow-quantization\n$ ./install.sh\n$ cd tests\n$ python3 -m pytest quantize_test.py -rP\n```\n\n----------------------------------------\n\nTITLE: Installing Sphinx and Extensions for TensorRT Documentation in Bash\nDESCRIPTION: Executes the setup_sphinx.sh script to install Sphinx and all necessary extensions required for building the TensorRT project documentation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ ./setup_sphinx.sh\n```\n\n----------------------------------------\n\nTITLE: Building CircPadPlugin Shared Library for TensorRT\nDESCRIPTION: Configures the main target for the CircPadPlugin shared library, including source files, compile options, include directories, C++ standard, and required link libraries.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/python_plugin/CMakeLists.txt#2025-04-06_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(circ_pad_plugin SHARED ${CMAKE_SOURCE_DIR}/circ_plugin_cpp/circ_pad_plugin.cu)\ntarget_compile_options(circ_pad_plugin PRIVATE ${GENCODES})\n\ntarget_include_directories(\n    circ_pad_plugin\n    PUBLIC ${CUDA_INC_DIR}\n    PUBLIC ${TRT_INCLUDE})\n\nset_property(TARGET circ_pad_plugin PROPERTY CUDA_STANDARD 14)\n\ntarget_link_libraries(circ_pad_plugin PRIVATE ${NVINFER_LIB})\ntarget_link_libraries(circ_pad_plugin PRIVATE ${CUDA_LIB})\n```\n\n----------------------------------------\n\nTITLE: Input Tensor B Example for FlattenConcat Operation\nDESCRIPTION: Example showing the structure of second input tensor B with shape [2, 3, 2, 2] before flattening and concatenation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/flattenConcat/README.md#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n[[[[16 17]\n   [18 19]]\n\n  [[20 21]\n   [22 23]]\n \n  [[24 25]\n   [26 27]]]\n\n \n [[[28 29]\n   [30 31]]\n\n  [[32 33]\n   [34 35]]\n\n  [[36 37]\n   [38 39]]]]\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorRT Module Source Files Based on Build Type\nDESCRIPTION: Conditionally sets the source files to include based on which TensorRT module is being built. For the full tensorrt module, all sources are included, while for lean and dispatch modules, only specific files are included.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/python/CMakeLists.txt#2025-04-06_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif(${TENSORRT_MODULE} STREQUAL \"tensorrt\")\n    # tensorrt full dependencies\n    file(GLOB_RECURSE SOURCE_FILES src/*.cpp)\nelse()\n    # tensorrt_lean and tensorrt_dispatch dependencies\n    set(SOURCE_FILES src/pyTensorRT.cpp src/utils.cpp src/infer/pyCore.cpp src/infer/pyPlugin.cpp\n                     src/infer/pyFoundationalTypes.cpp)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building TensorRT-OSS for Linux aarch64\nDESCRIPTION: Commands for building TensorRT-OSS on Linux aarch64 architecture using custom toolchain file and CUDA 12.8.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/README.md#2025-04-06_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncd $TRT_OSSPATH\nmkdir -p build && cd build\ncmake .. -DTRT_LIB_DIR=$TRT_LIBPATH -DTRT_OUT_DIR=`pwd`/out -DCMAKE_TOOLCHAIN_FILE=$TRT_OSSPATH/cmake/toolchains/cmake_aarch64-native.toolchain\nmake -j$(nproc)\n```\n\n----------------------------------------\n\nTITLE: Building Polygraphy TRTExec Extension Module with Python\nDESCRIPTION: This command builds the extension module using setup.py, creating a wheel file for installation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/polygraphy-extension-trtexec/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 setup.py bdist_wheel\n```\n\n----------------------------------------\n\nTITLE: Defining Python Dependencies for TensorRT\nDESCRIPTION: Lists the required Python packages for TensorRT. The dependencies include numpy for numerical computations and onnx_graphsurgeon for manipulating ONNX models.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/polygraphy/backend/pluginref/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nnumpy\nonnx_graphsurgeon\n```\n\n----------------------------------------\n\nTITLE: Adding BatchedNMS Plugin Source Files in CMake\nDESCRIPTION: This CMake command specifies the source files required to build the BatchedNMS plugin for TensorRT. It includes CUDA (.cu) files for GPU operations and C++ (.cpp, .h) files for the plugin implementation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/batchedNMSPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_plugin_source(\n    batchedNMSInference.cu\n    batchedNMSPlugin.cpp\n    batchedNMSPlugin.h\n    gatherNMSOutputs.cu\n    gatherNMSOutputs.h\n)\n```\n\n----------------------------------------\n\nTITLE: Output Tensor Example After FlattenConcat Operation\nDESCRIPTION: Example showing the resulting output tensor with shape [2, 20, 1, 1] after applying flattenConcat operation on tensors A and B.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/flattenConcat/README.md#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n[[[[ 0]]\n\n  [[ 1]]\n\n  [[ 2]]\n\n  [[ 3]]\n\n  [[ 4]]\n\n  [[ 5]]\n\n  [[ 6]]\n\n  [[ 7]]\n\n  [[16]]\n\n  [[17]]\n\n  [[18]]\n\n  [[19]]\n\n  [[20]]\n\n  [[21]]\n\n  [[22]]\n\n  [[23]]\n\n  [[24]]\n\n  [[25]]\n\n  [[26]]\n\n  [[27]]]\n\n\n [[[ 8]]\n\n  [[ 9]]\n\n  [[10]]\n\n  [[11]]\n\n  [[12]]\n\n  [[13]]\n\n  [[14]]\n\n  [[15]]\n\n  [[28]]\n\n  [[29]]\n\n  [[30]]\n\n  [[31]]\n\n  [[32]]\n\n  [[33]]\n\n  [[34]]\n\n  [[35]]\n\n  [[36]]\n\n  [[37]]\n\n  [[38]]\n\n  [[39]]]]\n```\n\n----------------------------------------\n\nTITLE: Building the TensorRT Python Extension Module\nDESCRIPTION: Creates and configures the shared library for the Python extension module, setting up include directories, linked libraries, and ensuring the correct naming convention (removing 'lib' prefix) for Python extension modules.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/python/CMakeLists.txt#2025-04-06_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nset(LIB_NAME ${PY_MODULE_NAME})\n\n# Set up target\nadd_library(${LIB_NAME} SHARED ${SOURCE_FILES})\ntarget_include_directories(${LIB_NAME} BEFORE PUBLIC ${PY_CONFIG_INCLUDE} ${PY_INCLUDE})\nif(MSVC)\n    # For some reason, we must explicitly link against the Python library on Windows.\n    target_link_libraries(${LIB_NAME} PRIVATE ${TRT_LIBS} ${PYTHON_LIB_NAME})\nelse()\n    target_link_libraries(${LIB_NAME} PRIVATE ${TRT_LIBS})\nendif()\n\n# Note that we have to remove the `lib` prefix from the binding .so's\nset_target_properties(${LIB_NAME} PROPERTIES PREFIX \"\")\n```\n\n----------------------------------------\n\nTITLE: Running with Node Renaming Option\nDESCRIPTION: Command to run the custom runner with the option to rename nodes when replacing Reshape with Identity.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/dev/02_extending_polygraphy_run/README.md#2025-04-06_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run no_op_reshape.onnx --res-des --res-des-rename-nodes\n```\n\n----------------------------------------\n\nTITLE: Adding Proposal Layer Plugin Source Files to TensorRT Build\nDESCRIPTION: This CMake command adds the implementation and header files for the Proposal Layer Plugin to the TensorRT build system. The plugin likely implements the Region Proposal Network (RPN) functionality commonly used in object detection networks.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/proposalLayerPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_plugin_source(\n    proposalLayerPlugin.cpp\n    proposalLayerPlugin.h\n)\n```\n\n----------------------------------------\n\nTITLE: Cloning and Pushing Changes to Forked TensorRT Repository in Bash\nDESCRIPTION: Commands to clone a forked TensorRT repository, make changes, and push them to a remote branch on the fork.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/CONTRIBUTING.md#2025-04-06_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/YOUR_USERNAME/YOUR_FORK.git TensorRT\n# Checkout the targeted branch and commit changes\n# Push the commits to a branch on the fork (remote).\ngit push -u origin <local-branch>:<remote-branch>\n```\n\n----------------------------------------\n\nTITLE: Adding Common BERT QKV Plugin Header File\nDESCRIPTION: Adds the common header file for the BERT QKV plugin to the project. This file is included regardless of the SM architecture.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/bertQKVToContextPlugin/fused_multihead_attention_v2/CMakeLists.txt#2025-04-06_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nadd_plugin_source(\n    fused_multihead_attention_v2.h\n)\n```\n\n----------------------------------------\n\nTITLE: Generating TensorRT Network Script Template from Existing Model\nDESCRIPTION: Command to generate a template script for modifying a TensorRT network based on an existing model file.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/run/04_defining_a_tensorrt_network_or_config_manually/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy template trt-network identity.onnx -o my_define_network.py\n```\n\n----------------------------------------\n\nTITLE: Adding TensorRT Subcomponents\nDESCRIPTION: Conditionally adds subdirectories for TensorRT plugins, parsers, and samples based on build options.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/CMakeLists.txt#2025-04-06_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif(BUILD_PLUGINS)\n    add_subdirectory(plugin)\nelse()\n    find_library_create_target(nvinfer_plugin ${nvinfer_plugin_lib_name} SHARED ${TRT_OUT_DIR} ${TRT_LIB_DIR})\nendif()\n\nif(BUILD_PARSERS)\n    add_subdirectory(parsers)\nelse()\n    find_library_create_target(nvonnxparser ${nvonnxparser_lib_name} SHARED ${TRT_OUT_DIR} ${TRT_LIB_DIR})\nendif()\n\nif(BUILD_SAMPLES)\n    add_subdirectory(samples)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Specifying NumPy Dependency\nDESCRIPTION: This snippet specifies NumPy as a required dependency for the project. NumPy is a fundamental package for scientific computing with Python that provides support for arrays, matrices, and mathematical functions.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/03_interoperating_with_tensorrt/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nnumpy\n```\n\n----------------------------------------\n\nTITLE: Cleaning and Displaying Formatted DataFrame\nDESCRIPTION: Code to clean up the DataFrame for better display by reordering columns, reformatting inputs/outputs, and removing unnecessary information.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/tutorial.ipynb#2025-04-06_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndf = trex.df_preprocessing.clean_for_display(plan.df)\nprint(f\"These are the column names in the plan\\n: {df.columns}\")\ntrex.notebook.display_df(df)\n```\n\n----------------------------------------\n\nTITLE: Launching TensorRT-OSS Docker Container\nDESCRIPTION: Bash command to launch the TensorRT-OSS Docker container with GPU access.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/README.md#2025-04-06_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n./docker/launch.sh --tag tensorrt-ubuntu20.04-cuda12.8 --gpus all\n```\n\n----------------------------------------\n\nTITLE: Adding TensorRT Inference Plugin Source\nDESCRIPTION: CMake command to add the inference plugin source file to the build configuration. This adds inferPlugin.cpp to the plugin source files that will be compiled.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/api/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_plugin_source(inferPlugin.cpp)\n```\n\n----------------------------------------\n\nTITLE: Loading and Processing a Test Image\nDESCRIPTION: Downloads a test image from a URL, resizes it to 224x224, expands dimensions to create a batch, and repeats the image across the batch dimension for testing.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/IntroNotebooks/2. Using PyTorch through ONNX.ipynb#2025-04-06_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom skimage import io\nfrom skimage.transform import resize\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nurl='https://images.dog.ceo/breeds/retriever-golden/n02099601_3004.jpg'\nimg = resize(io.imread(url), (224, 224))\nimg = np.expand_dims(np.array(img, dtype=np.float32), axis=0) # Expand image to have a batch dimension\ninput_batch = np.array(np.repeat(img, BATCH_SIZE, axis=0), dtype=np.float32) # Repeat across the batch dimension\n\ninput_batch.shape\n```\n\n----------------------------------------\n\nTITLE: Defining TensorRT Sample Projects List\nDESCRIPTION: Defines a list of open source TensorRT sample projects that will be included in the build process. Each sample represents a different TensorRT feature or use case.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/CMakeLists.txt#2025-04-06_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(OPENSOURCE_SAMPLES_LIST\n    sampleCharRNN\n    sampleDynamicReshape\n    sampleEditableTimingCache\n    sampleINT8API\n    sampleNonZeroPlugin\n    sampleOnnxMNIST\n    sampleIOFormats\n    sampleOnnxMnistCoordConvAC\n    sampleNamedDimensions\n    sampleProgressMonitor\n    trtexec)\n```\n\n----------------------------------------\n\nTITLE: Configuring Python Parameters Example\nDESCRIPTION: Example of Python parameter usage from CreateConfig showing precision constraints and memory pool limits configuration\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/CHANGELOG.md#2025-04-06_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nCreateConfig(\n    precision_constraints=...,     # Added argument\n    memory_pool_limits=...,       # Added parameter\n    profiling_verbosity=...,     # Added parameter\n    obey_precision_constraints=... # Deprecated option\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Input Structure for regionPlugin in TensorRT\nDESCRIPTION: Describes the input tensor structure for the regionPlugin, including shape and channel information for bounding box predictions, objectness scores, and class probabilities.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/regionPlugin/README.md#2025-04-06_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\nThe input has a shape of `[N, C, H, W]`, where:\n-   `N` is the batch size\n-   `C` is the number of channels in the input tensor. For example, `C = num * (coords + 1 + classes)`.\n-   `H` is the height of feature map\n-   `W` is the width of feature map\n\nThe information order of the channels are:\n-   `[t_x, t_y, t_w, t_h, t_o, d_1, d_2, ..., d_classes] for bbox_1`\n-   `[t_x, t_y, t_w, t_h, t_o, d_1, d_2, ..., d_classes] for bbox_2`, and so on\n-   `[t_x, t_y, t_w, t_h, t_o, d_1, d_2, ..., d_classes]` for `bbox_num`, totalling `num * (coords + 1 + classes)` channels.\n```\n\n----------------------------------------\n\nTITLE: Running FP16 Model and Comparing Results\nDESCRIPTION: Command to run the FP16 model with saved inputs and compare against FP32 outputs using specified tolerance levels.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/convert/04_converting_models_to_fp16/README.md#2025-04-06_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy run --onnxrt identity_fp16.onnx \\\n   --load-inputs inputs.json --load-outputs outputs_fp32.json \\\n   --atol 0.001 --rtol 0.001\n```\n\n----------------------------------------\n\nTITLE: Data File Path Management Function\nDESCRIPTION: Python function to manage data file paths and handle command line arguments for data directory specification. Validates data paths and provides centralized access to downloaded files.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/README.md#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nTRT_DATA_DIR = None\n\ndef getFilePath(path):\n    global TRT_DATA_DIR\n    if not TRT_DATA_DIR:\n        parser = argparse.ArgumentParser(description=\"Convert YOLOv3 to ONNX model\")\n        parser.add_argument('-d', '--data', help=\"Specify the data directory where it is saved in. $TRT_DATA_DIR will be overwritten by this argument.\")\n        args, _ = parser.parse_known_args()\n        TRT_DATA_DIR = os.environ.get('TRT_DATA_DIR', None) if args.data is None else args.data\n    if TRT_DATA_DIR is None:\n        raise ValueError(\"Data directory must be specified by either `-d $DATA` or environment variable $TRT_DATA_DIR.\")\n\n    fullpath = os.path.join(TRT_DATA_DIR, path)\n    if not os.path.exists(fullpath):\n        raise ValueError(\"Data file %s doesn't exist!\" % fullpath)\n\n    return fullpath\n```\n\n----------------------------------------\n\nTITLE: Defining Parser Common Source Files in CMake for TensorRT\nDESCRIPTION: This CMake snippet defines a variable PARSER_COMMON_SRCS that lists common header files used in the TensorRT parser. The files include implementations for half-precision floating-point operations and parser helper functions.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/parsers/common/ParserCommonSrcs.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(PARSER_COMMON_SRCS\n    common/half.h\n    common/ieee_half.h\n    common/parserHelper.h\n)\n```\n\n----------------------------------------\n\nTITLE: Adding NMS Plugin Source Files in TensorRT\nDESCRIPTION: This CMake command adds the Non-Maximum Suppression (NMS) plugin source files to be compiled in the TensorRT project. It specifies both the implementation file (nmsPlugin.cpp) and header file (nmsPlugin.h) to be included in the build process.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/nmsPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_plugin_source(\n    nmsPlugin.cpp\n    nmsPlugin.h \n)\n```\n\n----------------------------------------\n\nTITLE: Cloning and Setting Up PackNet Repository\nDESCRIPTION: Commands to clone the PackNet repository, checkout a specific version, and set up the Python path\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/onnx_packnet/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/TRI-ML/packnet-sfm.git packnet-sfm\npushd packnet-sfm && git checkout tags/v0.1.2 && popd\nexport PYTHONPATH=$PWD/packnet-sfm\n```\n\n----------------------------------------\n\nTITLE: Visualizing Layer Latencies\nDESCRIPTION: Creates a sunburst visualization of layer latencies as percentages\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/notebooks/engine_report_card.ipynb#2025-04-06_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nlayer_latency_sunburst(plan.df, \"Layers Latencies (%)\")\n```\n\n----------------------------------------\n\nTITLE: Installing TensorRT Python Package\nDESCRIPTION: Command for installing the TensorRT Python package using pip, which is one of the options for setting up TensorRT for testing Polygraphy.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/CONTRIBUTING.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pip install tensorrt\n```\n\n----------------------------------------\n\nTITLE: Legacy Plugin Build Configuration\nDESCRIPTION: Defines the legacy build flow configuration used when new plugin flow is disabled. Sets up source collection and build flags for backward compatibility.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/CMakeLists.txt#2025-04-06_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nelse() # TRT_BUILD_ENABLE_NEW_PLUGIN_FLOW\nadd_custom_target(plugin)\n\nset(TARGET_NAME ${nvinfer_plugin_lib_name})\nset(SHARED_TARGET ${TARGET_NAME})\nset(STATIC_TARGET ${TARGET_NAME}_static)\nset(VFC_TARGET_NAME ${nvinfer_vc_plugin_lib_name})\nset(VFC_SHARED_TARGET ${VFC_TARGET_NAME})\n```\n\n----------------------------------------\n\nTITLE: Environment Variable Definition\nDESCRIPTION: Definition of POLYGRAPHY_INTERNAL_CORRECTNESS_CHECKS environment variable to enable runtime correctness checks. These checks are disabled by default and failures indicate potential bugs in Polygraphy.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/CHANGELOG.md#2025-04-06_snippet_7\n\nLANGUAGE: Shell\nCODE:\n```\nPOLYGRAPHY_INTERNAL_CORRECTNESS_CHECKS=1\n```\n\n----------------------------------------\n\nTITLE: Adding BatchTile Plugin Source Files to TensorRT Build\nDESCRIPTION: Adds the BatchTile plugin implementation (.cpp) and header (.h) files to the TensorRT plugin build system. The add_plugin_source CMake function registers these files for compilation and inclusion in the TensorRT plugin library.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/batchTilePlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_plugin_source(\n    batchTilePlugin.cpp\n    batchTilePlugin.h\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Project with CUDA and Pybind11\nDESCRIPTION: Sets up the base CMake project configuration, finds CUDA package, and fetches pybind11 dependency for Python bindings. Configures version requirements and project language settings.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.12 FATAL_ERROR)\nproject(infer_c LANGUAGES CXX)\nfind_package(CUDA)\n\ninclude(FetchContent)\nFetchContent_Declare(\n    pybind11\n    GIT_REPOSITORY https://github.com/pybind/pybind11\n    GIT_TAG        v2.2.3\n)\n\nFetchContent_GetProperties(pybind11)\nif(NOT pybind11_POPULATED)\n    FetchContent_Populate(pybind11)\n    add_subdirectory(${pybind11_SOURCE_DIR} ${pybind11_BINARY_DIR})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Using the Surgeon CLI Tool\nDESCRIPTION: Basic command for accessing Surgeon tool help documentation through the Polygraphy CLI.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/polygraphy/tools/surgeon/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy surgeon -h\n```\n\n----------------------------------------\n\nTITLE: Drawing TensorRT Engine Graph\nDESCRIPTION: Command to draw a graph diagram from a TensorRT engine JSON file with options to display regions and hide layer names.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/bin/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntrex draw ./examples/pytorch/resnet/A100/fp32/resnet.onnx.engine.graph.json --display_regions --no_layer_names\n```\n\n----------------------------------------\n\nTITLE: Propagating TensorRT Plugin Sources to Parent Scope in CMake\nDESCRIPTION: Conditionally propagates the PLUGIN_SOURCES and PLUGIN_CU_SOURCES variables to the parent scope if the new plugin flow is not enabled. This ensures the parent CMake file can access the plugin source files.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/common/CMakeLists.txt#2025-04-06_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\n# Promote PLUGIN_SOURCES and PLUGIN_CU_SOURCES added by `add_subdirectory` to this file's parent.\nif (NOT DEFINED ${TRT_BUILD_ENABLE_NEW_PLUGIN_FLOW})\n    set(PLUGIN_SOURCES ${PLUGIN_SOURCES} PARENT_SCOPE)\n    set(PLUGIN_CU_SOURCES ${PLUGIN_CU_SOURCES} PARENT_SCOPE)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Disabling Recursive Subgraph Processing in Python\nDESCRIPTION: Demonstrates how to disable recursive application of graph operations like cleanup, toposort, and fold_constants to subgraphs. This provides finer control over graph transformations.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/CHANGELOG.md#2025-04-06_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ncleanup(recurse_subgraphs=False)\ntoposort(recurse_subgraphs=False)\nfold_constants(recurse_subgraphs=False)\n```\n\n----------------------------------------\n\nTITLE: Adding Instance Normalization Plugin Source Files to TensorRT Compilation in CMake\nDESCRIPTION: This CMake command adds various source files needed for the instance normalization plugin implementation in TensorRT. It includes the main plugin files, legacy plugin versions, common headers, and forward pass implementation files.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/instanceNormalizationPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_plugin_source(\n    instanceNormalizationPlugin.cu\n    instanceNormalizationPlugin.h\n    instanceNormalizationPluginLegacy.cu\n    instanceNormalizationPluginLegacy.h\n    instanceNormCommon.h\n    instanceNormFwd.h\n    instanceNormFwdImpl.cu\n)\n```\n\n----------------------------------------\n\nTITLE: Including TensorRT CMake Template\nDESCRIPTION: Includes the common CMake template file for TensorRT samples, which contains shared build configurations and settings.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/trtexec/CMakeLists.txt#2025-04-06_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ninclude(../CMakeSamplesTemplate.txt)\n```\n\n----------------------------------------\n\nTITLE: Installing Prerequisite Packages for ONNX-GraphSurgeon Testing\nDESCRIPTION: Commands to install the required packages for running tests and building documentation for ONNX-GraphSurgeon.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/CONTRIBUTING.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pip install -r tests/requirements.txt\n```\n\nLANGUAGE: bash\nCODE:\n```\npython3 -m pip install -r docs/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Importing Polygraphy Function Module\nDESCRIPTION: Import statement for the polygraphy.func.func module which contains function helper utilities.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/func/toc.rst#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom polygraphy.func import func\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorRT INT8 API Sample Build in CMake\nDESCRIPTION: Sets up the source files and parser dependencies for the TensorRT INT8 API sample. It defines the main source file and specifies the ONNX parser as a dependency. The configuration is then completed by including a common CMake template for TensorRT samples.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleINT8API/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(SAMPLE_SOURCES sampleINT8API.cpp)\n\nset(SAMPLE_PARSERS \"onnx\")\n\ninclude(../CMakeSamplesTemplate.txt)\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Build Settings for TensorRT Parsers\nDESCRIPTION: Sets up CMake build configuration for TensorRT parsers with protobuf integration. Defines output directories, compiler options, and includes necessary protobuf dependencies. Creates a custom target for parsers and configures build paths.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/parsers/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_custom_target(parsers DEPENDS nvonnxparser)\n\nadd_definitions(\"-D_PROTOBUF_INSTALL_DIR=${Protobuf_INSTALL_DIR}\")\nadd_compile_options(\"-Dgoogle=google_trtrepack\")\nset(TENSORRT_ROOT ${PROJECT_SOURCE_DIR})\nset(TENSORRT_BUILD ${TRT_OUT_DIR})\n\n# Write the libs out to the top-level of the build directory.\nset(CMAKE_LIBRARY_OUTPUT_DIRECTORY ${TENSORRT_BUILD})\nmessage(NOTICE \"CMAKE_LIBRARY_OUTPUT_DIRECTORY ${CMAKE_LIBRARY_OUTPUT_DIRECTORY}\")\n\n# Write the libs out to the top-level of the build directory.\nset(CMAKE_ARCHIVE_OUTPUT_DIRECTORY ${TENSORRT_BUILD})\nmessage(NOTICE \"CMAKE_ARCHIVE_OUTPUT_DIRECTORY ${CMAKE_ARCHIVE_OUTPUT_DIRECTORY}\")\n\ninclude_directories(\n   ${Protobuf_INCLUDE_DIR}\n)\n\nadd_subdirectory(onnx)\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorRT Sample Sources in CMake\nDESCRIPTION: Defines the source files required for building the sampleIOFormats example. Includes main sample file and common utility sources for device handling, engine management, and bfloat16 operations.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleIOFormats/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(SAMPLE_SOURCES sampleIOFormats.cpp ../common/sampleDevice.cpp ../common/sampleEngines.cpp\n                   ../common/sampleOptions.cpp ../common/sampleUtils.cpp ../common/bfloat16.cpp)\n```\n\n----------------------------------------\n\nTITLE: Setting Variables Conditionally in CMake\nDESCRIPTION: A macro that sets a variable to a value only if the variable is currently undefined. This allows for default values that can be overridden.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/python/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nmacro(set_ifndef var val)\n    if(NOT DEFINED ${var})\n        set(${var} ${val})\n    endif()\nendmacro()\n```\n\n----------------------------------------\n\nTITLE: Listing Dependencies for ImageNet Conversion to TFRecord\nDESCRIPTION: Specifies the required packages for converting ImageNet data to TFRecord format, likely for use in a cloud environment.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/examples/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# ImageNet: conversion to tfrecord\ngcloud\ngoogle-cloud-storage\n```\n\n----------------------------------------\n\nTITLE: Generating Network Tensor Names for TensorRT INT8 Inference\nDESCRIPTION: Command to write network tensor names to a file. This is useful for creating custom dynamic range files for INT8 inference with TensorRT.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleINT8API/README.md#2025-04-06_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n./sample_int8_api [--model=model_file] [--write_tensors] [--network_tensors_file=network_tensors.txt] [-v or --verbose]\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Package Dependencies for TensorRT Project\nDESCRIPTION: This code snippet lists the required Python packages and their versions for the NVIDIA TensorRT project. It includes conditional version specifications based on the Python version being used. The dependencies cover image processing, ONNX, TensorRT, CUDA, and various utility libraries.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/efficientnet/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nPillow>=10.0.0\nonnx==1.14.0; python_version <= \"3.10\"\nonnx==1.16.1; python_version >= \"3.11\"\ntensorrt>=7.1.0.0\ntf2onnx==1.8.1; python_version <= \"3.10\"\ntf2onnx==1.16.0; python_version >= \"3.11\"\ncuda-python==12.2.0; python_version <= \"3.10\"\ncuda-python==12.6.0; python_version >= \"3.11\"\npywin32; platform_system == \"Windows\"\npyyaml==6.0.1\nrequests==2.32.2\ntqdm==4.66.4\nnumpy==1.24.4; python_version <= \"3.10\"\nnumpy==1.26.4; python_version >= \"3.11\"\n```\n\n----------------------------------------\n\nTITLE: BERT QKV Kernel Source Selection Loop\nDESCRIPTION: CMake foreach loop that iterates through supported SM architectures and conditionally adds FP16 and INT8 multihead attention kernel source files if they exist for each architecture. Handles multiple head sizes (64-512) for both precision types.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/bertQKVToContextPlugin/fused_multihead_attention/CMakeLists.txt#2025-04-06_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nforeach(SM IN LISTS BERT_QKV_SUPPORTED_SMS)\n    should_compile_kernel(${SM} SHOULD_COMPILE)\n    if (${SHOULD_COMPILE})\n        add_plugin_source_if_exists(\n            fused_multihead_attention_fp16_64_64_kernel.sm${SM}.cpp\n            fused_multihead_attention_fp16_96_64_kernel.sm${SM}.cpp\n            fused_multihead_attention_fp16_128_64_kernel.sm${SM}.cpp\n            fused_multihead_attention_fp16_256_64_kernel.sm${SM}.cpp\n            fused_multihead_attention_fp16_384_64_kernel.sm${SM}.cpp\n            fused_multihead_attention_fp16_512_64_kernel.sm${SM}.cpp\n\n            fused_multihead_attention_int8_64_64_kernel.sm${SM}.cpp\n            fused_multihead_attention_int8_96_64_kernel.sm${SM}.cpp\n            fused_multihead_attention_int8_128_64_kernel.sm${SM}.cpp\n            fused_multihead_attention_int8_192_64_kernel.sm${SM}.cpp\n            fused_multihead_attention_int8_256_64_kernel.sm${SM}.cpp\n            fused_multihead_attention_int8_384_64_kernel.sm${SM}.cpp\n            fused_multihead_attention_int8_512_64_kernel.sm${SM}.cpp\n        )\n    endif()\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Importing ONNX Runtime Runner Module in Python\nDESCRIPTION: This code snippet demonstrates how to import the ONNX Runtime runner module from the Polygraphy tools package. It's used to set up and configure ONNX Runtime-specific arguments for Polygraphy's command-line tools.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/tool/args/backend/onnxrt/runner.rst#2025-04-06_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom polygraphy.tools.args.backend.onnxrt.runner import OnnxrtRunnerArgs\n```\n\n----------------------------------------\n\nTITLE: Creating ONNX Custom Nodes with AOT TensorRT Plugins\nDESCRIPTION: Example of creating a custom ONNX node that uses an Ahead-of-Time (AOT) implementation of a TensorRT plugin. This is configured with an additional 'aot' attribute set to True to use the AOT implementation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/quickly_deployable_plugins/README.md#2025-04-06_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport onnx_graphsurgeon as gs\n\nvar_x = gs.Variable(name=\"x\", shape=inp_shape, dtype=np.float32)\nvar_y = gs.Variable(name=\"y\", dtype=np.float32)\n\ncirc_pad_aot_node = gs.Node(\n    name=\"circ_pad_plugin_aot\",\n    op=\"circ_pad_plugin\",\n    inputs=[var_x],\n    outputs=[var_y],\n    attrs={\"pads\": pads, \"plugin_namespace\": \"sample\", \"aot\": True},\n)\n```\n\n----------------------------------------\n\nTITLE: Formatting Individual Source Files with clang-format in Bash\nDESCRIPTION: Command to run clang-format on individual source files using the project's style file. Modifies files in-place and uses no fallback style.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/CONTRIBUTING.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nclang-format -style=file -i -fallback-style=none <file(s) to process>\n```\n\n----------------------------------------\n\nTITLE: Configuring Source Files and Parser for CharRNN Sample in TensorRT\nDESCRIPTION: This snippet defines the source files required for building the CharRNN sample in TensorRT. It also specifies ONNX as a required parser and includes a common CMake template for TensorRT samples.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleCharRNN/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(SAMPLE_SOURCES sampleCharRNN.cpp ../common/sampleDevice.cpp ../common/sampleEngines.cpp ../common/sampleOptions.cpp\n                   ../common/sampleUtils.cpp ../common/bfloat16.cpp)\n# Required due to inclusion of sampleEnines.h\nset(SAMPLE_PARSERS \"onnx\")\n\ninclude(../CMakeSamplesTemplate.txt)\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for NVIDIA TensorRT\nDESCRIPTION: Lists required Python packages with specific versions for the TensorRT project. Includes conditional dependencies that vary based on Python version (e.g., cuda-python and numpy) and platform (pywin32 for Windows only).\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/engine_refit_onnx_bidaf/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nonnx==1.16.0\nnltk==3.9.1\nwget==3.2\ncuda-python==12.2.0; python_version <= \"3.10\"\ncuda-python==12.6.0; python_version >= \"3.11\"\npywin32; platform_system == \"Windows\"\npyyaml==6.0.1\nrequests==2.32.2\ntqdm==4.66.4\nnumpy==1.24.4; python_version <= \"3.10\"\nnumpy==1.26.4; python_version >= \"3.11\"\n```\n\n----------------------------------------\n\nTITLE: Defining Hidden Role in Sphinx Documentation for PyTorch Quantization\nDESCRIPTION: This snippet defines a 'hidden' role in Sphinx documentation with a class name of 'hidden-section'. This is used to hide certain sections of documentation when rendered.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/optim.rst#2025-04-06_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. role:: hidden\n    :class: hidden-section\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure\nDESCRIPTION: ReStructuredText documentation showing the module structure and table of contents for the ONNX module.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/tool/args/backend/onnx/toc.rst#2025-04-06_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nModule: ``polygraphy.tools.args``\n\n.. toctree::\n    loader\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Quantization\nDESCRIPTION: Import necessary Python modules including PyTorch, quantization libraries, and vision models.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/tutorials/quant_resnet50.rst#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.utils.data\nfrom torch import nn\n\nfrom pytorch_quantization import nn as quant_nn\nfrom pytorch_quantization import calib\nfrom pytorch_quantization.tensor_quant import QuantDescriptor\n\nfrom torchvision import models\n\nsys.path.append(\"path to torchvision/references/classification/\")\nfrom train import evaluate, train_one_epoch, load_data\n```\n\n----------------------------------------\n\nTITLE: Listing Python Dependencies for NVIDIA TensorRT\nDESCRIPTION: This code snippet lists the Python package dependencies required for the NVIDIA TensorRT project. It includes numerical computing libraries like NumPy and SciPy, as well as utility packages for configuration, documentation, and data formatting.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nnumpy\nabsl-py>=0.7.0\nscipy\nsphinx_glpi_theme\nprettytable\npyyaml\n```\n\n----------------------------------------\n\nTITLE: Displaying BERT Model Configurations in Markdown\nDESCRIPTION: This code snippet shows a markdown table containing configurations for BERT-Base and BERT-Large models, including details on hidden layers, unit sizes, attention heads, and other parameters.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/BERT/README.md#2025-04-06_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| **Model**  | **Hidden layers** | **Hidden unit size** | **Attention heads** | **Feed-forward filter size** | **Max sequence length** | **Parameters** |\n| :--------: | :---------------: | :------------------: | :-----------------: | :--------------------------: | :---------------------: | :------------: |\n| BERT-Base  |    12 encoder     |         768          |         12          |           4 x 768            |           512           |      110M      |\n| BERT-Large |    24 encoder     |         1024         |         16          |           4 x 1024           |           512           |      330M      |\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for TensorRT\nDESCRIPTION: This snippet lists the required Python packages and their versions for the TensorRT project. It includes conditional dependencies based on Python version and platform. Key packages include Pillow, ONNX, ONNX Runtime, TensorFlow to ONNX converter, CUDA Python, and various utility libraries.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/efficientdet/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nPillow>=10.0.0\nonnx==1.14.0; python_version <= \"3.10\"\nonnx==1.16.1; python_version >= \"3.11\"\nonnxruntime==1.15.1; python_version <= \"3.10\"\nonnxruntime==1.18.1; python_version >= \"3.11\"\ntf2onnx==1.8.1; python_version <= \"3.10\"\ntf2onnx==1.16.0; python_version >= \"3.11\"\ncuda-python==12.2.0; python_version <= \"3.10\"\ncuda-python==12.6.0; python_version >= \"3.11\"\npywin32; platform_system == \"Windows\"\npyyaml==6.0.1\nrequests==2.32.2\ntqdm==4.66.4\nnumpy==1.24.4; python_version <= \"3.10\"\nnumpy==1.26.4; python_version >= \"3.11\"\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages\nDESCRIPTION: Commands for setting up the environment and installing dependencies\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/Diffusion/README.md#2025-04-06_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport TRT_OSSPATH=/workspace\ncd $TRT_OSSPATH/demo/Diffusion\npip3 install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Adding Plugin Source Files for Modulated Deformable Convolution in TensorRT\nDESCRIPTION: This CMake command adds source files for the modulated deformable convolution plugin to the TensorRT project. It includes header files, CUDA implementation files, and C++ source files necessary for the plugin's functionality.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/modulatedDeformConvPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_plugin_source(\n    commonCudaHelper.h\n    modulatedDeformConvCudaHelper.cu\n    modulatedDeformConvCudaHelper.h\n    modulatedDeformConvPlugin.cpp\n    modulatedDeformConvPlugin.h\n    modulatedDeformConvPluginKernel.cu\n    modulatedDeformConvPluginKernel.h\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Documentation Structure using reStructuredText\nDESCRIPTION: This snippet defines the table of contents (toctree) for the TensorFlow 2.x Quantization Toolkit documentation. It organizes the documentation into several sections, each with its own set of pages or notebooks.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/index.rst#2025-04-06_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n   :maxdepth: 1\n   :caption: Toolkit\n\n   docs/installation.md\n   docs/basics.md\n   docs/getting_started.ipynb\n\n.. toctree::\n   :maxdepth: 1\n   :caption: Tutorials\n\n   notebooks/simple_network_quantize_full.ipynb\n   notebooks/simple_network_quantize_partial.ipynb\n   notebooks/simple_network_quantize_specific_class.ipynb\n\n.. toctree::\n   :maxdepth: 1\n   :caption: Examples\n\n   docs/example_resnet50v1.ipynb\n   docs/model_zoo.md\n\n.. toctree::\n   :maxdepth: 1\n   :caption: Advanced Features\n\n   docs/add_new_layer_support.md\n   docs/add_custom_qdq_cases.md\n   \n.. toctree::\n   :maxdepth: 1\n   :caption: API Reference\n\n   globals.rst\n   qmodel.rst\n   qspec.rst\n   cqdq.rst\n   bqw.rst\n   utils.rst\n\n.. toctree::\n   :maxdepth: 1\n   :caption: Quantization Theory\n\n   docs/intro_to_quantization.md\n   docs/qat.md\n```\n\n----------------------------------------\n\nTITLE: Setting Up CUDA Compilation for TensorRT\nDESCRIPTION: Configures CUDA compilation settings, including target architectures and compilation flags for TensorRT.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/CMakeLists.txt#2025-04-06_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif (DEFINED GPU_ARCHS)\n  message(STATUS \"GPU_ARCHS defined as ${GPU_ARCHS}. Generating CUDA code for SM ${GPU_ARCHS}\")\n  separate_arguments(GPU_ARCHS)\n  foreach(SM IN LISTS GPU_ARCHS)\n    list(APPEND CMAKE_CUDA_ARCHITECTURES SM)\n  endforeach()\nelse()\n  list(APPEND CMAKE_CUDA_ARCHITECTURES 72 75 80 86 87 89 90)\n  \n  if(CUDA_VERSION VERSION_GREATER_EQUAL 12.8)\n      list(APPEND CMAKE_CUDA_ARCHITECTURES 100 120)\n  endif()\n\n  message(STATUS \"GPU_ARCHS is not defined. Generating CUDA code for default SMs: ${CMAKE_CUDA_ARCHITECTURES}\")\nendif()\n\nif(NOT MSVC)\n    set(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} --expt-relaxed-constexpr -Xcompiler -Wno-deprecated-declarations\")\nelse()\n    set(CMAKE_CUDA_SEPARABLE_COMPILATION ON)\n    set(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} --expt-relaxed-constexpr -Xcompiler\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Source Files for TensorRT Algorithm Selector Sample in CMake\nDESCRIPTION: This snippet defines the source files for the TensorRT Algorithm Selector sample. It specifies a single source file 'sampleAlgorithmSelector.cpp' and sets the parser dependency to 'onnx'.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleAlgorithmSelector/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nSET(SAMPLE_SOURCES\n     sampleAlgorithmSelector.cpp\n)\n\nSET(SAMPLE_PARSERS \"onnx\")\n\ninclude(../CMakeSamplesTemplate.txt)\n```\n\n----------------------------------------\n\nTITLE: Defining Python Dependencies for NVIDIA TensorRT\nDESCRIPTION: A requirements.txt file that specifies all the Python package dependencies for the TensorRT project. The dependencies are categorized into core packages (including protobuf, onnx, pandas, and numpy), utility packages (pynvml), and Excel reporting packages (xlsxwriter and openpyxl).\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# Packages required for core functionality.\nprotobuf\nonnx>=1.15.0\npandas==2.2.1\npytest\ngraphviz\npydot\nnumpy\nnetworkx\ntabulate\n\n# Packages required for utilities\npynvml\n\n# Packages required for excel reporting.\nxlsxwriter\nopenpyxl\n```\n\n----------------------------------------\n\nTITLE: Running Tests for ONNX-GraphSurgeon\nDESCRIPTION: Command to execute the test suite for ONNX-GraphSurgeon using Make.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/CONTRIBUTING.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake test\n```\n\n----------------------------------------\n\nTITLE: Defining OnnxGraphSurgeon Custom Exception in Python\nDESCRIPTION: This code snippet defines the OnnxGraphSurgeonException class, which is a custom exception used in the ONNX-GraphSurgeon library. It likely inherits from Python's built-in Exception class and is used to raise specific errors related to ONNX graph manipulation operations.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/docs/exception/toc.rst#2025-04-06_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: onnx_graphsurgeon.OnnxGraphSurgeonException\n```\n\n----------------------------------------\n\nTITLE: Input Tensor A Example for FlattenConcat Operation\nDESCRIPTION: Example showing the structure of first input tensor A with shape [2, 2, 2, 2] before flattening and concatenation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/flattenConcat/README.md#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n[[[[ 0 1]\n   [ 2 3]]\n\n  [[ 4 5]\n   [ 6 7]]]\n  \n\n [[[ 8 9]\n   [10 11]]\n\n  [[12 13]\n   [14 15]]]]\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorRT Build Options\nDESCRIPTION: Sets various build options for TensorRT, including output directories, version information, and compilation flags.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/CMakeLists.txt#2025-04-06_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(RUNTIME_OUTPUT_DIRECTORY ${TRT_OUT_DIR} CACHE PATH \"Output directory for runtime target files\")\nset(LIBRARY_OUTPUT_DIRECTORY ${TRT_OUT_DIR} CACHE PATH \"Output directory for library target files\")\nset(ARCHIVE_OUTPUT_DIRECTORY ${TRT_OUT_DIR} CACHE PATH \"Output directory for archive target files\")\n\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\nset(CMAKE_CXX_EXTENSIONS OFF)\n\nif(NOT MSVC)\n    set(CMAKE_CXX_FLAGS \"-Wno-deprecated-declarations ${CMAKE_CXX_FLAGS} -DBUILD_SYSTEM=cmake_oss\")\nelse()\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -DBUILD_SYSTEM=cmake_oss\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating a Network with Dynamic Input Dimensions in TensorRT\nDESCRIPTION: Creates a TensorRT network with full dimensions support for preprocessing dynamically shaped inputs. Adds an input layer that accepts dynamic shapes followed by a resize layer that will reshape the input to the expected model shape.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleDynamicReshape/README.md#2025-04-06_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nauto preprocessorNetwork = makeUnique(builder->createNetworkV2(0));\n```\n\n----------------------------------------\n\nTITLE: Displaying Polygraphy CLI Examples Directory Structure in Markdown\nDESCRIPTION: This Markdown snippet describes the contents of the Polygraphy CLI examples directory, pointing users to the API examples and the CLI User Guide for additional information.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/README.md#2025-04-06_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n# Polygraphy CLI Examples\n\nThis directory includes examples that use the Polygraphy CLI.\nFor examples of the Python API, see the [api](../api/) directory instead.\n\nYou may find the [CLI User Guide](../../polygraphy/tools/) useful to navigate the CLI examples.\n```\n\n----------------------------------------\n\nTITLE: Sample Polygraphy Lint Command Output\nDESCRIPTION: Example terminal output showing the results of running the Polygraphy lint command on a faulty ONNX model, displaying various errors and warnings detected by the linter.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/check/01_linting_an_onnx_model/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n[I] RUNNING | Command: polygraphy check lint bad_graph.onnx -o report.json\n[I] Loading model: bad_graph.onnx\n[E] LINT | Field 'name' of 'graph' is required to be non-empty.\n[I] Will generate inference input data according to provided TensorMetadata: {E [dtype=float32, shape=(1, 4)],\n     F [dtype=float32, shape=(4, 1)],\n     G [dtype=int64, shape=(4, 4)],\n     D [dtype=float32, shape=(4, 1)],\n     C [dtype=float32, shape=(3, 4)],\n     A [dtype=float32, shape=(1, 3)],\n     B [dtype=float32, shape=(4, 4)]}\n[E] LINT | Name: MatMul_3, Op: MatMul |  Incompatible dimensions for matrix multiplication\n[E] LINT | Name: Add_0, Op: Add |  Incompatible dimensions\n[E] LINT | Name: MatMul_0, Op: MatMul |  Incompatible dimensions for matrix multiplication\n[W] LINT | Input: 'A' does not affect outputs, can be removed.\n[W] LINT | Input: 'B' does not affect outputs, can be removed.\n[W] LINT | Name: MatMul_0, Op: MatMul | Does not affect outputs, can be removed.\n[I] Saving linting report to report.json\n[E] FAILED | Runtime: 1.006s | Command: polygraphy check lint bad_graph.onnx -o report.json\n```\n\n----------------------------------------\n\nTITLE: Setting Output Dimensions for NonZero Plugin\nDESCRIPTION: Sets the constant second dimension of the output tensor to 2, representing the row and column indices of non-zero elements.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/non_zero_plugin/README.md#2025-04-06_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\noutput_dims[0][1] = exprBuilder.constant(2)\n```\n\n----------------------------------------\n\nTITLE: Adding Region Plugin Source Files in TensorRT with CMake\nDESCRIPTION: This CMake directive adds the region plugin source files to the TensorRT plugin compilation. It specifies both the implementation file (regionPlugin.cpp) and header file (regionPlugin.h) to be included in the build process.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/regionPlugin/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_plugin_source(\n    regionPlugin.cpp\n    regionPlugin.h\n)\n```\n\n----------------------------------------\n\nTITLE: Converting ONNX to TensorRT Engine\nDESCRIPTION: Converts an ONNX model to a TensorRT engine and saves it to disk for later comparison.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/run/02_comparing_across_runs/README.md#2025-04-06_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy convert identity.onnx -o identity.engine\n```\n\n----------------------------------------\n\nTITLE: Building ONNX GraphSurgeon Manually\nDESCRIPTION: Steps to build and install ONNX GraphSurgeon from source code.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/README.md#2025-04-06_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmake build\npython3 -m pip install onnx_graphsurgeon/dist/onnx_graphsurgeon-*-py2.py3-none-any.whl\n```\n\n----------------------------------------\n\nTITLE: Specifying NumPy Dependency for TensorRT in Python\nDESCRIPTION: This snippet specifies numpy as a required dependency for the NVIDIA TensorRT project when using Python. NumPy is a fundamental package for scientific computing in Python and is likely used for numerical operations within TensorRT.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/api/05_using_tensorrt_network_api/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nnumpy\n```\n\n----------------------------------------\n\nTITLE: Implementing Quantized MaxPool2d Class Declaration\nDESCRIPTION: Base class declaration for quantized MaxPool2d module that inherits from pooling.MaxPool2d and _utils.QuantInputMixin.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/tutorials/creating_custom_quantized_modules.rst#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass QuantMaxPool2d(pooling.MaxPool2d, _utils.QuantInputMixin):\n```\n\n----------------------------------------\n\nTITLE: Defining Comparator Module Structure in reStructuredText\nDESCRIPTION: This snippet defines the structure of the Comparator module documentation using reStructuredText syntax. It specifies the module name and lists the submodules to be included in the documentation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/tool/args/comparator/toc.rst#2025-04-06_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n=============\nComparator\n=============\n\nModule: ``polygraphy.tools.args``\n\n.. toctree::\n    comparator\n    compare\n    data_loader\n    postprocess\n```\n\n----------------------------------------\n\nTITLE: Configuring dtale Host for display_df Function (Python)\nDESCRIPTION: Demonstrates how to explicitly configure the hostname in dtale to resolve issues with the display_df function. This should be done before using any table display functions.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/experimental/trt-engine-explorer/KNOWN_ISSUES.md#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndtale.app.ACTIVE_HOST = \"192.168.1.30\"\n```\n\n----------------------------------------\n\nTITLE: Generating TensorRT Config Script Template with FP16 Enabled\nDESCRIPTION: Command to generate a template script for creating a TensorRT builder configuration with FP16 mode pre-enabled.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/examples/cli/run/04_defining_a_tensorrt_network_or_config_manually/README.md#2025-04-06_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npolygraphy template trt-config --fp16 -o my_create_config.py\n```\n\n----------------------------------------\n\nTITLE: Installing TensorRT Package via pip from NVIDIA NGC\nDESCRIPTION: Command to install NVIDIA TensorRT package using pip by specifying NVIDIA's NGC PyPI repository as the package index source. Extra index URL is configured to point to pypi.ngc.nvidia.com.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/polygraphy/backend/trt/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n-i https://pypi.ngc.nvidia.com/\\nnvidia-tensorrt\n```\n\n----------------------------------------\n\nTITLE: Documenting MaxCalibrator Class\nDESCRIPTION: RST directive for auto-documenting the MaxCalibrator class with all its members and inherited members.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/calib.rst#2025-04-06_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n:hidden:`MaxCalibrator`\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. autoclass:: MaxCalibrator\n    :members:\n    :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Visualizing Quantization Aware Training (QAT) Flow with Mermaid\nDESCRIPTION: This Mermaid flowchart depicts the Quantization Aware Training (QAT) process. It shows the steps from the pre-trained model to the final quantized model, including adding Q/DQ nodes, fine-tuning, and storing scale values.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/qat.md#2025-04-06_snippet_1\n\nLANGUAGE: mermaid\nCODE:\n```\nflowchart LR\n    id1(Pre-trained model) --> id2(Add Q/DQ nodes) --> id3(Finetune model) --> id4(Store 'scale') --> id5(Quantize model)\n```\n\n----------------------------------------\n\nTITLE: Setting up TensorRT Data Directory\nDESCRIPTION: Commands to download and set up the required MNIST sample data directory and install dependencies.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleProgressMonitor/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport TRT_DATADIR=/usr/src/tensorrt/data\npushd $TRT_DATADIR/mnist\npip3 install Pillow\npopd\n```\n\n----------------------------------------\n\nTITLE: Defining RestructuredText Documentation Structure\nDESCRIPTION: Defines the documentation structure for the polygraphy.comparator module using RestructuredText format. It includes the module name and a table of contents listing all related submodules.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/comparator/toc.rst#2025-04-06_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n==================\nComparing Results\n==================\n\nModule: ``polygraphy.comparator``\n\n.. toctree::\n    comparator\n    data_structures\n    compare_func\n    postprocess_func\n    data_loader\n```\n\n----------------------------------------\n\nTITLE: Bounding Box Encoding for CodeTypeSSD::CENTER_SIZE Method in TensorRT\nDESCRIPTION: Mathematical formulation for the CENTER_SIZE coding method used in nmsPlugin. It shows how bounding boxes are encoded with and without variance encoding, representing the normalized difference between ground truth and anchor box centers and sizes.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/nmsPlugin/README.md#2025-04-06_snippet_1\n\nLANGUAGE: math\nCODE:\n```\n[(x_{center, gt} - x_{center, anchor}) / w_{anchor}, (y_{center, gt} - y_{center, anchor}) / h_{anchor}, ln(w_{gt} / w_{anchor}), ln(h_{gt} / h_{anchor})]\n```\n\nLANGUAGE: math\nCODE:\n```\n[(x_{center, gt} - x_{center, anchor}) / w_{anchor} / variance_0, (y_{center, gt} - y_{center, anchor}) / h_{anchor} / variance_1, ln(w_{gt} / w_{anchor}) / variance_2, ln(h_{gt} / h_{anchor}) / variance_3]\n```\n\n----------------------------------------\n\nTITLE: Input Tensor B Structure Example\nDESCRIPTION: Example showing the structure of second input tensor B with shape [1, 3, 2, 2]\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/batchTilePlugin/README.md#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n[[[[16 17]\n   [18 19]]\n\n  [[20 21]\n   [22 23]]\n \n  [[24 25]\n   [26 27]]]]\n```\n\n----------------------------------------\n\nTITLE: Input Shape Configuration\nDESCRIPTION: Examples showing old and new syntax for specifying input shapes in Polygraphy\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/CHANGELOG.md#2025-04-06_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n# Old syntax (deprecated)\n--input-shapes input0,xxyxz\n\n# New syntax\n--input-shapes input0:[x,y,z]\n```\n\n----------------------------------------\n\nTITLE: Defining DetectionLayer Output Tensor Shape in CUDA\nDESCRIPTION: Specifies the shape of the output tensor generated by the DetectionLayer plugin after performing NMS and filtering detections.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/detectionLayerPlugin/README.md#2025-04-06_snippet_1\n\nLANGUAGE: CUDA\nCODE:\n```\noutput shape is [N, keep_topk, 6]\n```\n\n----------------------------------------\n\nTITLE: Setting HuggingFace Token\nDESCRIPTION: Command to set HuggingFace access token for downloading model checkpoints\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/demo/Diffusion/README.md#2025-04-06_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport HF_TOKEN=<your access token>\n```\n\n----------------------------------------\n\nTITLE: Modifying Detectron 2 Export Script for Fixed Input Size\nDESCRIPTION: Changes the ResizeShortestEdge transformation in export_model.py to use a fixed input size of 1344x1344 for TensorRT compatibility.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/detectron2/README.md#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\naug = T.ResizeShortestEdge(\n    [1344, 1344], 1344\n)\n```\n\n----------------------------------------\n\nTITLE: Listing Python Dependencies for NVIDIA TensorRT\nDESCRIPTION: This snippet lists the Python package dependencies required for the NVIDIA TensorRT project. It includes packages for numerical computing (numpy), ONNX model handling (onnx, onnxruntime), protocol buffers (protobuf), testing (pytest), and machine learning data types (ml_dtypes).\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/tests/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nnumpy\nonnx\nonnxruntime==1.19.2\nprotobuf>=3.20.2\npytest\nml_dtypes\n```\n\n----------------------------------------\n\nTITLE: Calculating Anchor Distribution for ResNet50 with 832x1344 Input\nDESCRIPTION: Demonstrates the calculation of anchor counts across different feature map levels (P2-P6) for a ResNet50 backbone with 832x1344 input shape. Each level uses 3 anchors per position.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/multilevelProposeROI/README.md#2025-04-06_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nAnchors in feature map P2: 208*336*3\nAnchors in feature map P3: 104*168*3\nAnchors in feature map P4: 52*84*3\nAnchors in feature map P5: 26*42*3\nAnchors in feature map P6(maxpooling): 13*21*3\n```\n\n----------------------------------------\n\nTITLE: Importing TensorFlow Runner Module in Python\nDESCRIPTION: This code snippet demonstrates how to import the TensorFlow runner module from Polygraphy. It uses Python's automodule directive to automatically generate documentation for the module, including inherited members.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/backend/tf/runner.rst#2025-04-06_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. automodule:: polygraphy.backend.tf.runner\n    :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Configuring Compiler Warning Flags for TensorRT Plugin\nDESCRIPTION: Sets compiler flags for C++ and CUDA to enable warnings, excluding specific deprecation warnings. This configuration only applies to non-MSVC compilers.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/onnx_custom_plugin/CMakeLists.txt#2025-04-06_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT MSVC)\n    # Enable all compile warnings\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wall -Wno-long-long -pedantic -Wno-deprecated-declarations\")\n    set(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} -Xcompiler -Wno-deprecated-declarations\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Documenting Variable Class in ONNX-GraphSurgeon using Python Autodoc\nDESCRIPTION: This snippet uses Python's autodoc feature to automatically generate documentation for the Variable class in the onnx_graphsurgeon module. The autoclass directive is used to include the class documentation in the generated documentation.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/onnx-graphsurgeon/docs/ir/tensor/variable.rst#2025-04-06_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. autoclass:: onnx_graphsurgeon.Variable\n```\n\n----------------------------------------\n\nTITLE: Defining Python Package Dependencies for NVIDIA TensorRT\nDESCRIPTION: A requirements file that specifies the necessary Python packages for TensorRT. It includes platform-specific conditions for different packages and versions based on the operating system and Python version.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/python/quickly_deployable_plugins/requirements.txt#2025-04-06_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntriton; platform_system != \"Windows\"\ntorch\n--extra-index-url https://pypi.ngc.nvidia.com\npolygraphy\ncolored\nnumpy==1.23.5; (platform_system != \"Windows\" and python_version <= \"3.10\")\nnumpy==1.26.4; (platform_system != \"Windows\" and python_version >= \"3.11\")\nonnx==1.16.0; platform_system == \"Windows\"\n--extra-index-url https://pypi.ngc.nvidia.com\nonnx-graphsurgeon\npyyaml==6.0.1\nrequests==2.32.2\ntqdm==4.66.4\n```\n\n----------------------------------------\n\nTITLE: Input Tensor Example for reorgPlugin in Python\nDESCRIPTION: This code snippet shows an example input tensor of shape [2, 4, 6, 6] for the reorgPlugin. The tensor is represented as a 4D nested list in Python.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/reorgPlugin/README.md#2025-04-06_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n[[[[0, 1, 2, 3, 4, 5],\n[6, 7, 8, 9, 10, 11],\n[12, 13, 14, 15, 16, 17],\n[18, 19, 20, 21, 22, 23],\n[24, 25, 26, 27, 28, 29],\n[30, 31, 32, 33, 34, 35]],\n\n[[36, 37, 38, 39, 40, 41],\n[42, 43, 44, 45, 46, 47],\n[48, 49, 50, 51, 52, 53],\n[54, 55, 56, 57, 58, 59],\n[60, 61, 62, 63, 64, 65],\n[66, 67, 68, 69, 70, 71]],\n\n[[72, 73, 74, 75, 76, 77],\n[78, 79, 80, 81, 82, 83],\n[84, 85, 86, 87, 88, 89],\n[90, 91, 92, 93, 94, 95],\n[96, 97, 98, 99, 100, 101],\n[102, 103, 104, 105, 106, 107]],\n\n[[108, 109, 110, 111, 112, 113],\n[114, 115, 116, 117, 118, 119],\n[120, 121, 122, 123, 124, 125],\n[126, 127, 128, 129, 130, 131],\n[132, 133, 134, 135, 136, 137],\n[138, 139, 140, 141, 142, 143]]],\n\n[[[144, 145, 146, 147, 148, 149],\n[150, 151, 152, 153, 154, 155],\n[156, 157, 158, 159, 160, 161],\n[162, 163, 164, 165, 166, 167],\n[168, 169, 170, 171, 172, 173],\n[174, 175, 176, 177, 178, 179]],\n\n[[180, 181, 182, 183, 184, 185],\n[186, 187, 188, 189, 190, 191],\n[192, 193, 194, 195, 196, 197],\n[198, 199, 200, 201, 202, 203],\n[204, 205, 206, 207, 208, 209],\n[210, 211, 212, 213, 214, 215]],\n\n[[216, 217, 218, 219, 220, 221],\n[222, 223, 224, 225, 226, 227],\n[228, 229, 230, 231, 232, 233],\n[234, 235, 236, 237, 238, 239],\n[240, 241, 242, 243, 244, 245],\n[246, 247, 248, 249, 250, 251]],\n\n[[252, 253, 254, 255, 256, 257],\n[258, 259, 260, 261, 262, 263],\n[264, 265, 266, 267, 268, 269],\n[270, 271, 272, 273, 274, 275],\n[276, 277, 278, 279, 280, 281],\n[282, 283, 284, 285, 286, 287]]]]\n```\n\n----------------------------------------\n\nTITLE: Importing Polygraphy Tools Arguments Module\nDESCRIPTION: Module path for the Polygraphy tools argument system, which provides the foundation for command-line argument grouping functionality.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/tool/args/toc.rst#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npolygraphy.tools.args\n```\n\n----------------------------------------\n\nTITLE: Importing ONNX Runtime Runner Module in Python\nDESCRIPTION: This code snippet demonstrates how to import the ONNX Runtime runner module from Polygraphy. It uses Python's automodule directive to automatically generate documentation for the module and its inherited members.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/backend/onnxrt/runner.rst#2025-04-06_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n.. automodule:: polygraphy.backend.onnxrt.runner\n    :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Adding Input and Resize Layers for Dynamic Shape Handling in TensorRT\nDESCRIPTION: Adds an input layer with dynamic dimensions (indicated by -1) and a resize layer that reshapes the input to the fixed dimensions expected by the MNIST model.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleDynamicReshape/README.md#2025-04-06_snippet_1\n\nLANGUAGE: C++\nCODE:\n```\nauto input = preprocessorNetwork->addInput(\"input\", nvinfer1::DataType::kFLOAT, Dims4{-1, 1, -1, -1});\nauto resizeLayer = preprocessorNetwork->addResize(*input);\nresizeLayer->setOutputDimensions(mPredictionInputDims);\npreprocessorNetwork->markOutput(*resizeLayer->getOutput(0));\n```\n\n----------------------------------------\n\nTITLE: RST Role Definition for Hidden Sections\nDESCRIPTION: Defines a hidden role directive for RST documentation to create hidden sections.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/utils.rst#2025-04-06_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. role:: hidden\n    :class: hidden-section\n```\n\n----------------------------------------\n\nTITLE: Documenting HistogramCalibrator Class\nDESCRIPTION: RST directive for auto-documenting the HistogramCalibrator class with all its members and inherited members.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/calib.rst#2025-04-06_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n:hidden:`HistogramCalibrator`\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. autoclass:: HistogramCalibrator\n    :members:\n    :inherited-members:\n```\n\n----------------------------------------\n\nTITLE: Output Tensor Example for reorgPlugin in Python\nDESCRIPTION: This code snippet demonstrates the output tensor of shape [2, 16, 3, 3] after applying the reorgPlugin with a stride of 2. The reorganized tensor is represented as a 4D nested list in Python.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/reorgPlugin/README.md#2025-04-06_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n[[[[0, 2, 4],\n[6, 8, 10],\n[24, 26, 28]],\n\n[[30, 32, 34],\n[48, 50, 52],\n[54, 56, 58]],\n\n[[72, 74, 76],\n[78, 80, 82],\n[96, 98, 100]],\n\n[[102, 104, 106],\n[120, 122, 124],\n[126, 128, 130]],\n\n[[1, 3, 5],\n[7, 9, 11],\n[25, 27, 29]],\n\n[[31, 33, 35],\n[49, 51, 53],\n[55, 57, 59]],\n\n[[73, 75, 77],\n[79, 81, 83],\n[97, 99, 101]],\n\n[[103, 105, 107],\n[121, 123, 125],\n[127, 129, 131]],\n\n[[12, 14, 16],\n[18, 20, 22],\n[36, 38, 40]],\n\n[[42, 44, 46],\n[60, 62, 64],\n[66, 68, 70]],\n\n[[84, 86, 88],\n[90, 92, 94],\n[108, 110, 112]],\n\n[[114, 116, 118],\n[132, 134, 136],\n[138, 140, 142]],\n\n[[13, 15, 17],\n[19, 21, 23],\n[37, 39, 41]],\n\n[[43, 45, 47],\n[61, 63, 65],\n[67, 69, 71]],\n\n[[85, 87, 89],\n[91, 93, 95],\n[109, 111, 113]],\n\n[[115, 117, 119],\n[133, 135, 137],\n[139, 141, 143]]],\n\n[[[144, 146, 148],\n[150, 152, 154],\n[168, 170, 172]],\n\n[[174, 176, 178],\n[192, 194, 196],\n[198, 200, 202]],\n\n[[216, 218, 220],\n[222, 224, 226],\n[240, 242, 244]],\n\n[[246, 248, 250],\n[264, 266, 268],\n[270, 272, 274]],\n\n[[145, 147, 149],\n[151, 153, 155],\n[169, 171, 173]],\n\n[[175, 177, 179],\n[193, 195, 197],\n[199, 201, 203]],\n\n[[217, 219, 221],\n[223, 225, 227],\n[241, 243, 245]],\n\n[[247, 249, 251],\n[265, 267, 269],\n[271, 273, 275]],\n\n[[156, 158, 160],\n[162, 164, 166],\n[180, 182, 184]],\n\n[[186, 188, 190],\n[204, 206, 208],\n[210, 212, 214]],\n\n[[228, 230, 232],\n[234, 236, 238],\n[252, 254, 256]],\n\n[[258, 260, 262],\n[276, 278, 280],\n[282, 284, 286]],\n\n[[157, 159, 161],\n[163, 165, 167],\n[181, 183, 185]],\n\n[[187, 189, 191],\n[205, 207, 209],\n[211, 213, 215]],\n\n[[229, 231, 233],\n[235, 237, 239],\n[253, 255, 257]],\n\n[[259, 261, 263],\n[277, 279, 281],\n[283, 285, 287]]]]\n```\n\n----------------------------------------\n\nTITLE: Disallowed Code Disabling Pattern in Safety-Critical C++\nDESCRIPTION: Example of a disallowed pattern for disabling code in safety-critical software where using compile-time expressions and DCE (Dead Code Elimination) is forbidden.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/CODING-GUIDELINES.md#2025-04-06_snippet_9\n\nLANGUAGE: cpp\nCODE:\n```\n// Not allowed in safety-critical code.\nconst bool gDisabledFeature = false;\n\nvoid foo()\n{\n   if (gDisabledFeature)\n   {\n       doSomething();\n   }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining RestructuredText Documentation Structure\nDESCRIPTION: Sets up the documentation structure for the ONNX-Runtime backend module using RestructuredText directives. Includes module reference and table of contents entries for loader and runner components.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/backend/onnxrt/toc.rst#2025-04-06_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n============\nONNX-Runtime\n============\n\nModule: ``polygraphy.backend.onnxrt``\n\n.. toctree::\n    loader\n    runner\n```\n\n----------------------------------------\n\nTITLE: Example Usage of Tensor Quantization Functions\nDESCRIPTION: Demonstrates how to use tensor_quant.fake_tensor_quant and tensor_quant.tensor_quant to quantize a PyTorch tensor. The example shows both fake quantization (returning floating-point values) and actual quantization (returning integer values with scale).\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/userguide.rst#2025-04-06_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom pytorch_quantization import tensor_quant\n\n# Generate random input. With fixed seed 12345, x should be \n# tensor([0.9817, 0.8796, 0.9921, 0.4611, 0.0832, 0.1784, 0.3674, 0.5676, 0.3376, 0.2119])\ntorch.manual_seed(12345)\nx = torch.rand(10)\n\n# fake quantize tensor x. fake_quant_x will be \n# tensor([0.9843, 0.8828, 0.9921, 0.4609, 0.0859, 0.1797, 0.3672, 0.5703, 0.3359, 0.2109])\nfake_quant_x = tensor_quant.fake_tensor_quant(x, x.abs().max())\n\n# quantize tensor x. quant_x will be\n# tensor([126., 113., 127.,  59.,  11.,  23.,  47.,  73.,  43.,  27.])\n# with scale=128.0057\nquant_x, scale = tensor_quant.tensor_quant(x, x.abs().max())\n```\n\n----------------------------------------\n\nTITLE: Variable Naming for Quantization Functions\nDESCRIPTION: Example of naming conventions for quantized matrix multiplication functions\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/CONTRIBUTING.md#2025-04-06_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef matmul(a, b)\ndef quant_matmul(a, b)\n```\n\n----------------------------------------\n\nTITLE: Defining ClipFunction in PyTorch Quantization\nDESCRIPTION: Defines the ClipFunction class within the pytorch_quantization.nn.functional module. This class likely implements a clipping operation for tensor values during quantization.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/functional.rst#2025-04-06_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nclass ClipFunction:\n    # Members and methods are not shown in the provided snippet\n```\n\n----------------------------------------\n\nTITLE: Matrix Concatenation Operation\nDESCRIPTION: Formula showing how the input data is concatenated with the coordinate channels along the channel dimension to create the final tensor for convolution.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/plugin/coordConvACPlugin/README.md#2025-04-06_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nCONCAT([INPUT_DATA, 1ST_CHANNEL, 2ND_CHANNEL])\n```\n\n----------------------------------------\n\nTITLE: Defining Module Structure in reStructuredText\nDESCRIPTION: This snippet defines the structure of the documentation for the 'polygraphy.tools' module using reStructuredText syntax. It includes a section title and a table of contents directive.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/tool/toc.rst#2025-04-06_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n============================\nCommand-line Tool APIs\n============================\n\nModule: ``polygraphy.tools``\n\n.. toctree::\n    args/toc\n    script\n```\n\n----------------------------------------\n\nTITLE: Saving and Converting Quantized ResNet50 V1 Model to ONNX in TensorFlow\nDESCRIPTION: This code saves the quantized and fine-tuned ResNet50 V1 model in TensorFlow's SavedModel format and converts it to ONNX format. It uses the previously defined save directory from hyperparameters.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/example_resnet50v1.ipynb#2025-04-06_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nq_model_save_path = os.path.join(HYPERPARAMS[\"save_root_dir\"], \"saved_model_qat\")\nq_model.save(q_model_save_path)\nconvert_saved_model_to_onnx(saved_model_dir=q_model_save_path,\n                            onnx_model_path=q_model_save_path + \".onnx\")\n```\n\n----------------------------------------\n\nTITLE: Generating API Documentation for TensorFlow Quantization Utils\nDESCRIPTION: This reStructuredText code generates API documentation for the tensorflow_quantization.utils module. It uses the automodule directive to automatically include all members and undocumented members of the module.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/utils.rst#2025-04-06_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. _utils_api:\n\n**tensorflow_quantization.utils**\n============================================\n\n.. automodule:: tensorflow_quantization.utils\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Generating API Documentation for TensorFlow Quantization Utils\nDESCRIPTION: This reStructuredText code generates API documentation for the tensorflow_quantization.utils module. It uses the automodule directive to automatically include all members and undocumented members of the module.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/utils.rst#2025-04-06_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. _utils_api:\n\n**tensorflow_quantization.utils**\n============================================\n\n.. automodule:: tensorflow_quantization.utils\n   :members:\n   :undoc-members:\n```\n\n----------------------------------------\n\nTITLE: Markdown Links for TensorRT QuickStart Guide\nDESCRIPTION: Markdown formatted links to various TensorRT tutorials and notebooks, organized into introductory materials and practical implementations.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/quickstart/README.md#2025-04-06_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# QuickStartGuide\n\nIntroductory notebooks in TensorRT QuickStartGuide\n\n- [Running the QuickStartGuide](IntroNotebooks/0.%20Running%20This%20Guide.ipynb)\n- [Getting Started with TensorRT](IntroNotebooks/1.%20Introduction.ipynb)\n- [Using PyTorch through ONNX](IntroNotebooks/2.%20Using%20PyTorch%20through%20ONNX.ipynb)\n- [Understanding TensorRT Runtimes](IntroNotebooks/3.%20Understanding%20TensorRT%20Runtimes.ipynb)\n\nTutorials corresponding to TensorRT QuickStartGuide\n\n- Semantic Segmentation using TensorRT - [C++ sample and Python notebook](./SemanticSegmentation/)\n- Optimize with TensorRT, Deploy with Triton - [Walkthrough and Python code](./deploy_to_triton/)\n- Quantization with TensorRT Model Optimizer - [Stable Diffusion XL (Base/Turbo) and Stable Diffusion 1.5 Quantization with Model Optimizer](https://github.com/NVIDIA/TensorRT-Model-Optimizer/tree/main/diffusers/quantization)\n```\n\n----------------------------------------\n\nTITLE: Scale Calculation Formula in Markdown\nDESCRIPTION: Formula for calculating the scale parameter in affine quantization, based on the range of representable values and bit width.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/intro_to_quantization.md#2025-04-06_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\nscale = (&alpha; - &beta;) / (2<sup>b</sup>-1)\n```\n\n----------------------------------------\n\nTITLE: Loading TensorFlow Runner Python Module\nDESCRIPTION: Python module import statement showing the location of the TensorFlow runner backend arguments module in the Polygraphy tools package hierarchy\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/tool/args/backend/tf/runner.rst#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npolygraphy.tools.args.backend.tf.runner\n```\n\n----------------------------------------\n\nTITLE: Installing TensorFlow Models Repository\nDESCRIPTION: Clones the TensorFlow model garden repository, checks out version 2.8.0, and installs required dependencies.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/examples/efficientnet/README.md#2025-04-06_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/tensorflow/models.git\npushd models && git checkout tags/v2.8.0 && popd\nexport PYTHONPATH=$PWD/models:$PYTHONPATH\npip install -r models/official/requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Defining Module Path in reStructuredText\nDESCRIPTION: Specifies the Python module path for the base interface implementation within the Polygraphy backend system.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/backend/base/toc.rst#2025-04-06_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\nModule: ``polygraphy.backend.base``\n```\n\n----------------------------------------\n\nTITLE: Python Module Import Documentation - Polygraphy TensorRT Config\nDESCRIPTION: Documentation for the Polygraphy TensorRT configuration module that manages TensorRT engine and builder configurations.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/tool/args/backend/trt/loader.rst#2025-04-06_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\npolygraphy.tools.args.backend.trt.config\n```\n\n----------------------------------------\n\nTITLE: Affine DeQuantization Formula in Markdown\nDESCRIPTION: Mathematical formula showing how to convert quantized values back to real values using the scale and zero point parameters.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/intro_to_quantization.md#2025-04-06_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\nx=(x<sub>q</sub>−zeroPt)∗scale\n```\n\n----------------------------------------\n\nTITLE: Importing Base Interface Module in Python\nDESCRIPTION: Module import statement showing the package structure for the base interface of Polygraphy's argument handling system.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/tool/args/base.rst#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npolygraphy.tools.args.base\n```\n\n----------------------------------------\n\nTITLE: Basic Tensor Quantization Functions in PyTorch\nDESCRIPTION: Documentation of the tensor_quant and fake_tensor_quant functions used to quantize tensors. tensor_quant returns quantized tensors (integer values) with scale, while fake_tensor_quant returns fake quantized tensors (float values).\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/userguide.rst#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntensor_quant(inputs, amax, num_bits=8, output_dtype=torch.float, unsigned=False)\nfake_tensor_quant(inputs, amax, num_bits=8, output_dtype=torch.float, unsigned=False)\n```\n\n----------------------------------------\n\nTITLE: Importing TensorRT Backend Configuration\nDESCRIPTION: Python module import statement for the TensorRT backend configuration in Polygraphy framework\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/backend/trt/config.rst#2025-04-06_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npolygraphy.backend.trt.config\n```\n\n----------------------------------------\n\nTITLE: Implementing Namespace Closing Comments in C++\nDESCRIPTION: Example showing proper namespace declaration with a closing comment that indicates which namespace is being closed, following the TensorRT coding guidelines.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/CODING-GUIDELINES.md#2025-04-06_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nnamespace foo\n{\n...\n} // namespace foo\n```\n\n----------------------------------------\n\nTITLE: Formatting Entire TensorRT Codebase with clang-format in Bash\nDESCRIPTION: Command to run clang-format on the entire TensorRT codebase, including samples and plugins. This command is intended for project maintainers only.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/CONTRIBUTING.md#2025-04-06_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nfind samples plugin -iname *.h -o -iname *.c -o -iname *.cpp -o -iname *.hpp \\\n| xargs clang-format -style=file -i -fallback-style=none\n```\n\n----------------------------------------\n\nTITLE: Implementing Switch Clauses with Compound Statements in C++\nDESCRIPTION: Shows how to correctly implement a switch clause with a compound statement, ensuring the break statement is inside the braces.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/CODING-GUIDELINES.md#2025-04-06_snippet_12\n\nLANGUAGE: cpp\nCODE:\n```\nswitch (x)\n{\ncase 0:\ncase 1:\n{\n    y();\n    z();\n    break;\n}\n...other cases...\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring AutoModule Documentation for PyTorch Quantization Helper Module\nDESCRIPTION: This snippet uses Sphinx's automodule directive to automatically generate documentation for the pytorch_quantization.optim.helper module, including all members, undocumented members, and inheritance information.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/optim.rst#2025-04-06_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: pytorch_quantization.optim.helper\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Converting Magic Numbers to Constants in C++\nDESCRIPTION: Examples demonstrating how to convert literal values to named constants in conditional statements. This improves code readability and maintainability by giving meaning to numeric values.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/CODING-GUIDELINES.md#2025-04-06_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nif (nbInputs == 2U){/*...*/}\n```\n\nLANGUAGE: cpp\nCODE:\n```\nconstexpr size_t kNbInputsWBias = 2U;\nif (nbInputs == kNbInputsWBias) {/*...*/}\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorRT sampleProgressMonitor CMake Build\nDESCRIPTION: Defines the source files and ONNX parser requirement for the sampleProgressMonitor application, then includes a common template for TensorRT sample builds. This configuration enables the sample to be built with the correct dependencies and settings.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/samples/sampleProgressMonitor/CMakeLists.txt#2025-04-06_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(SAMPLE_SOURCES sampleProgressMonitor.cpp)\n\nset(SAMPLE_PARSERS \"onnx\")\n\ninclude(../CMakeSamplesTemplate.txt)\n```\n\n----------------------------------------\n\nTITLE: Initializing Non-POD Types in C++\nDESCRIPTION: Illustrates the correct way to initialize non-POD types in C++, avoiding potential corruption of the vtable.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/CODING-GUIDELINES.md#2025-04-06_snippet_17\n\nLANGUAGE: cpp\nCODE:\n```\nstruct Foo {\n    virtual int getX() { return x; }\n    int x;\n};\n...\n\n// Bad: use memset() to initialize Foo\n{\n    Foo foo;\n    memset(&foo, 0, sizeof(foo)); // Destroys hiddien virtual-function-table pointer!\n}\n// Good: use brace initialization to initialize Foo\n{\n    Foo foo = {};\n}\n```\n\n----------------------------------------\n\nTITLE: RST Module Documentation for Quantization Logging\nDESCRIPTION: Autodoc directive for documenting the quantization logging utility module.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/utils.rst#2025-04-06_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. automodule:: pytorch_quantization.utils.quant_logging\n    :members:\n    :undoc-members:\n    :show-inheritance:\n```\n\n----------------------------------------\n\nTITLE: Aliasing clip Function in PyTorch Quantization\nDESCRIPTION: Creates an alias 'clip' for the ClipFunction.apply method, allowing direct use of the clip function in the pytorch_quantization.nn.functional module.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/pytorch-quantization/docs/source/functional.rst#2025-04-06_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nclip = ClipFunction.apply\n```\n\n----------------------------------------\n\nTITLE: Layer Name Example\nDESCRIPTION: Example showing how to assign custom names to Keras layers\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/tensorflow-quantization/docs/source/docs/basics.md#2025-04-06_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport tensorflow as tf\nl = tf.keras.layers.Dense(units=100, name='my_dense')\n```\n\n----------------------------------------\n\nTITLE: Importing Polygraphy Comparator Module in Python\nDESCRIPTION: This snippet demonstrates how to import the comparator module from the Polygraphy library. The comparator module is likely used for comparing different aspects of neural network models or their outputs in the context of NVIDIA TensorRT.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/tools/Polygraphy/docs/comparator/comparator.rst#2025-04-06_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom polygraphy.comparator import comparator\n```\n\n----------------------------------------\n\nTITLE: Structuring Switch Statements in C++\nDESCRIPTION: Demonstrates the correct way to structure switch statements in C++, including handling fall-through cases and proper termination of case clauses.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/CODING-GUIDELINES.md#2025-04-06_snippet_11\n\nLANGUAGE: cpp\nCODE:\n```\nswitch (x)\n{\ncase 0:         // Fall-through allowed from case 0: to case 1: since case 0 is empty.\ncase 1:\n    a();\n    b();\n    break;\ncase 2:\ncase 4:\n{              // With optional braces\n    c();\n    d();\n    break;\n}\ncase 5:\n    c();\n    throw 42;  // Terminating with throw is okay\ndefault:\n    throw 42;\n}\n```\n\n----------------------------------------\n\nTITLE: Using Clang-Format Directive to Bypass Formatting in C++\nDESCRIPTION: Example showing how to bypass the automatic formatting rules for exceptional cases where standard formatting would be inappropriate.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/CODING-GUIDELINES.md#2025-04-06_snippet_4\n\nLANGUAGE: cpp\nCODE:\n```\n// clang-format off\n// .. Unformatted code ..\n// clang-format on\n```\n\n----------------------------------------\n\nTITLE: Disabling Code with Preprocessor Directives in C++\nDESCRIPTION: Example showing the recommended way to disable code blocks using preprocessor directives with mnemonic conditions for better readability.\nSOURCE: https://github.com/NVIDIA/TensorRT/blob/release/10.9/CODING-GUIDELINES.md#2025-04-06_snippet_7\n\nLANGUAGE: cpp\nCODE:\n```\n#if DEBUG_CONVOLUTION_INSTRUMENTATION\n// ...code to be disabled...\n#endif\n```"
  }
]
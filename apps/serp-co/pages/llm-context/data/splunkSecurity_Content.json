[
  {
    "owner": "splunk",
    "repo": "security_content",
    "content": "TITLE: Apply Pre-trained DGA Model for Prediction\nDESCRIPTION: The `apply` function loads various pre-trained artifacts (tokenizer, vectorizers, weights, and the model itself) and uses them to predict whether a given domain is DGA-generated. It preprocesses the input domain data using the loaded artifacts, computes additional features (entropy, length, Alexa ranking), feeds the processed data into the model, and returns a pandas DataFrame with the predicted DGA labels and probabilities.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/pretrained_dga_model_dsdl.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntokenizer = pickle.load(open(MODEL_DIRECTORY+\"pretrained_dga_model_dsdl/tokenizer\", 'rb'))\nvectorizer_words = pickle.load(open(MODEL_DIRECTORY+\"pretrained_dga_model_dsdl/vectorizer_words\", 'rb'))\nvectorizer_non_dga_domains = pickle.load(open(MODEL_DIRECTORY+\"pretrained_dga_model_dsdl/vectorizer_non_dga_domains\", 'rb'))\nweight_words = pickle.load(open(MODEL_DIRECTORY+\"pretrained_dga_model_dsdl/weight_words\", 'rb'))\nweight_non_dga_grams = pickle.load(open(MODEL_DIRECTORY+\"pretrained_dga_model_dsdl/weight_non_dga_grams\", 'rb'))\ndomains = pickle.load(open(MODEL_DIRECTORY+\"pretrained_dga_model_dsdl/domains\", 'rb'))\nmodel = tf.keras.models.load_model(MODEL_DIRECTORY + \"pretrained_dga_model_dsdl\")\n\ndef entropy(domain):\n        p, lns = Counter(domain), float(len(domain))\n        return -sum( count/lns * math.log(count/lns, 2) for count in p.values())\n    \nalexa_domains = domains['domain']\n\n    \ndef is_in_alexa1m(domain,domains):\n    return ((domain in domains))\n\n    \n    \ndef add_features(df):\n    print (\"1. Done adding ngram features\")\n    X_1= weight_words * vectorizer_words.transform(df['domain']).T \n    X_2= weight_non_dga_grams * vectorizer_non_dga_domains.transform(df['domain']).T\n    X_3 =   df['domain'].map(lambda x: entropy(x)) \n    print (\"2. Done adding entropy\")\n    X_4 = df['domain'].map(lambda x: len(x))\n    print (\"3. Done adding length of domain\")\n    in_alexa = set(alexa_domains) & set(df['domain'])\n    X_5 = df['domain'].map(lambda x: is_in_alexa1m(x, in_alexa))\n    print (\"4. Done adding domain present in alexa domains\")\n    X_5 = X_5.astype(int)\n    input2 = np.c_[X_1,X_2,X_3,X_4,X_5] #\n    print (\"appending done\")\n    return input2\n\ndef prep_text(texts):\n    text_sequences = tokenizer.texts_to_sequences(texts)\n    return sequence.pad_sequences(text_sequences, maxlen=MAX_VOCAB)\n\ndef apply(model,df,param):\n    input1 = prep_text(df['domain'])\n    input2 = add_features(df)\n    \n    yhat = model.predict([input1,input2])\n    y_ = (yhat > 0.5).astype(\"int32\")\n    output = pd.DataFrame()\n    #output['domain']  = df['domain']\n    #output['true_dga']  = df['is_dga'] \n    output['pred_dga']=y_.ravel()\n    output['pred_dga_proba']=yhat.ravel()\n    return output\n```\n\n----------------------------------------\n\nTITLE: Apply the Model to Predict DNS Exfiltration\nDESCRIPTION: This code defines the `apply` function that processes the input DataFrame, prepares features such as character indices, entropy, and aggregated statistics and then uses the pre-trained deep learning model to predict DNS exfiltration probability.  The function calculates features, converts the data into a PyTorch tensor, uses a dataloader to make the batch predictions using the model.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_dns_data_exfiltration_using_pretrained_model_in_dsdl.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntext_rows = []\nsize_avg = []\nentropy_avg = []\n\n# this method accepts a dataloader and makes predictions in batches\ndef predict(dataloader,model):\n        predict_label_proba = []\n        predict_label = []\n        for batch in (dataloader):\n            \n            #convert to 1d tensor\n            predictions = model(batch.to('cpu'))\n            output  = (predictions >= 0.5).int() \n            predict_label_proba.extend(predictions)\n            predict_label.extend(output)\n        predict_label = [x.cpu().detach().numpy().item() for x in predict_label]\n        predict_label_proba = [x.cpu().detach().numpy().item() for x in predict_label_proba]\n        return predict_label_proba,predict_label\n    \n# this method accepts a DNS request and converts into indexes based on printable characters\ndef index_chars(x):\n    request_chars = {}\n    for i in range(len(x)):\n        request_chars[keys.index(x[i])] = request_chars.get(keys.index(x[i]), 0) + 1\n    text_rows.append(request_chars)\n\n#  calculates entropy of a domain\ndef entropy(domain):\n    p, lns = Counter(domain), float(len(domain))\n    return -sum(count / lns * math.log(count / lns, 2) for count in p.values())\n\n\n# removes the subdomain/domain from the request\ndef replace_tld(x):\n    if x is not None:\n        return str(x).rsplit('.', 2)[0]\n    else:\n        return x\n    \n# get the subdomain/domain from the request\ndef get_tld(x):\n    without_tld = str(x).rsplit('.', 2)[0]\n    return str(x).replace(without_tld,'').lstrip(\".\")\n\n# compute aggregated features for the same src and subdomain/domain on a window of 10 events\ndef get_aggregated_features(row,df):\n    src = row['src']\n    tld = row['tld']\n    prev_events = df[(df['src']==src) & (df['tld']==tld)]\n    \n    size_avg.append(prev_events['len'].mean())\n    entropy_avg.append(prev_events['entropy'].mean())\n\n# prepare input df by calculating features\ndef prepare_input_df(df):\n    keys = list(string.printable.strip())\n\n    \n    \n    df['query'].apply(lambda x: index_chars(x))\n    text = pd.DataFrame(text_rows, columns=list(range(0, 94)))\n    text.reset_index(drop=True, inplace=True)\n    df.reset_index(drop=True, inplace=True)\n    text.fillna(0, inplace=True)\n    df = pd.concat([text, df], axis=1)\n\n\n    # request without tld\n    df['request_without_domain'] = df['query'].apply(lambda row: replace_tld(row))\n\n    # request without tld\n    df['tld'] = df['query'].apply(lambda row: get_tld(row))\n    \n    # length of domain\n    df['len'] = df['request_without_domain'].apply(len)\n    \n    # entropy\n    df['entropy'] = df['request_without_domain'].apply(lambda x: entropy(x))\n    \n    # take most-recent request\n    recent_df = df.loc[df['rank'] == 1]\n\n    # calculate feature by aggregating events\n\n    recent_df.apply(lambda x: get_aggregated_features(x,df),axis=1)\n    recent_df['size_avg'] = size_avg\n    recent_df['entropy_avg'] = entropy_avg\n    return recent_df\n\n    \n# apply model on processed dataframe to predict exfiltration\ndef apply(model,df,param):\n    df.drop(['_time'], axis=1,inplace=True, errors='ignore')\n    recent_df = prepare_input_df(df)\n    input_df = recent_df.drop(['src' ,'query','rank','request_without_domain','tld'], axis=1)\n    recent_df.drop(['request_without_domain','tld','len','entropy','size_avg','entropy_avg'], axis=1, inplace=True)\n    recent_df.drop(range(0, 94),axis=1,inplace=True)\n    input_tensor = torch.FloatTensor(input_df.values)\n    dataloader = DataLoader(input_tensor, shuffle=True, batch_size=256)\n    predict_is_exfiltration_proba, predict_is_exfiltration = predict(dataloader,model)\n    recent_df['pred_is_dns_data_exfiltration_proba'] = predict_is_exfiltration_proba\n    recent_df['pred_is_dns_data_exfiltration'] = predict_is_exfiltration\n    print(recent_df.columns)\n    print(df.columns)\n    text_rows.clear()\n    size_avg.clear()\n    entropy_avg.clear()\n    output = pd.merge(recent_df,df,on=['src','query','rank'],how='right')\n    return output\n```\n\n----------------------------------------\n\nTITLE: Define Apply Function with Preprocessing Python\nDESCRIPTION: Defines an `apply` function that applies the pre-trained model to the input DataFrame. It includes preprocessing steps (`preprocess_txt`) to clean the input text. It converts input lines to tensors using `lineToTensor`, evaluates them using the model, and calculates a probability score. Requires the model to be initialized and pre-trained.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_processnames_using_pretrained_model_in_dsdl.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# apply your model\n# returns the calculated results\n\n# Find letter index from all_letters\ndef letterToIndex(letter):\n    return all_letters.find(letter)\n\n\n# Turn a line into a <line_length x 1 x n_letters>,\n# or an array of one-hot letter vectors\ndef lineToTensor(line):\n    tensor = torch.zeros(len(line), 1, n_letters)\n    for li, letter in enumerate(line):\n        tensor[li][0][letterToIndex(letter)] = 1\n    return tensor\n\n\ndef evaluate(model, line_tensor):\n    hidden = model.initHidden()\n\n    for i in range(line_tensor.size()[0]):\n        output, hidden = model(line_tensor[i], hidden)\n\n    return output\n\n\ndef preprocess_txt(input_line):\n    input_line = input_line.split('/')[-1]\n    input_line = input_line.lower()\n    input_line = input_line.replace('.exe','')\n    input_line = ''.join(filter(str.islower, input_line))\n    \n    return input_line\n\n\ndef apply(model, df, param):\n    is_malicious_prob_lst = []\n    for idx, row in df.iterrows():\n        input_line = row['text']\n        input_line = preprocess_txt(input_line)\n        \n        if len(input_line) == 0:\n            is_mal_prob = 1\n        else:\n            with torch.no_grad():\n                output = evaluate(model, lineToTensor(input_line))\n\n            output = torch.exp(output)\n            is_mal_prob = round(output[0][1].item(), 4)\n\n        is_malicious_prob_lst.append(is_mal_prob)\n    \n    output = pd.DataFrame()\n    output['is_malicious_prob'] = is_malicious_prob_lst\n\n    return output\n```\n\n----------------------------------------\n\nTITLE: Load Pre-trained Model\nDESCRIPTION: The `load` function loads a pre-trained DGA detection model from the specified directory using TensorFlow/Keras. It returns the loaded model object for further use in prediction or analysis.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/pretrained_dga_model_dsdl.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef load(name):\n    model = tf.keras.models.load_model(MODEL_DIRECTORY + \"pretrained_dga_model_dsdl\")\n    return model\n```\n\n----------------------------------------\n\nTITLE: Model Application Function\nDESCRIPTION: Defines an `apply` function that uses the loaded model to predict the probability of a DNS TXT record being suspicious. It preprocesses the text, converts it to token indices, and feeds it to the model. The output is a Pandas DataFrame with the predicted probabilities.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_dns_txt_records_using_pretrained_model_in_dsdl.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# apply your model\n# returns the calculated results\n# apply your model\n# returns the calculated results\nvocab = pickle.load(open(MODEL_DIRECTORY+\"vocab.pkl\", 'rb'))\n\ndef remove_spaces_key_value(text):\n     return text.replace(' = ','=').replace(\" =\",\"=\").replace('= ','=').replace(' - ','-').replace(\" -\",\"-\").replace('- ','-').replace(' : ',\":\").replace(': ' ,\":\").replace(' :',':')\n\ndef get_token_index(text):\n    idxs = []\n    tokens = tokenize(text)\n    for token in tokens:\n        if token not in vocab:\n            idxs.append(vocab['<unk>'])\n        else:\n            idxs.append(vocab[token])\n    return idxs\n\ndef preprocess_text(text):\n    text = remove_spaces_key_value(text)\n    text = text.replace('\"','').replace(\"'\",\"\").lower()\n    return text\n\n\ndef apply(model,df,param):\n    is_unknown_probability_score = []\n    is_unknown = []\n    for idx, row in df.iterrows():\n        text = row['text']\n        text = preprocess_text(text)\n        indexed_text = get_token_index(text)\n        length = [len(indexed_text)]\n        token_tensor = torch.LongTensor(indexed_text).to(device)             \n        token_tensor = token_tensor.unsqueeze(1).T                            \n        length_tensor = torch.LongTensor(length) \n        pred_proba = model(token_tensor, length_tensor)[:, 0].item()\n        pred  = int(pred_proba >= 0.5)\n        is_unknown_probability_score.append(pred_proba)\n        is_unknown.append(pred)\n    output = pd.DataFrame()\n    output['predicted_is_unknown'] = is_unknown_probability_score\n    output['pred_is_unknown'] = is_unknown\n    return output\n```\n\n----------------------------------------\n\nTITLE: Import Libraries for DGA Detection\nDESCRIPTION: This snippet imports necessary Python libraries for data manipulation (pandas), deep learning (TensorFlow/Keras), and other utilities such as time, metrics, and serialization (pickle). It also sets up some configuration options and ignores warnings.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/pretrained_dga_model_dsdl.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import Input \nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding,LSTM,Dropout,Dense,Activation\nfrom tensorflow.keras.activations import sigmoid\nimport matplotlib.pyplot as plt\nimport math\nimport numpy as np\nimport pickle\nfrom sklearn.utils import shuffle\nfrom tensorflow.keras.preprocessing import sequence\nimport datetime\nfrom collections import Counter\nMODEL_DIRECTORY = \"/srv/app/model/data/\"\nMAX_VOCAB = 40\npd.options.mode.chained_assignment = None \nimport warnings\nwarnings.filterwarnings('ignore')\n```\n\n----------------------------------------\n\nTITLE: Initialize DNS Exfiltration Detection Model (PyTorch)\nDESCRIPTION: This snippet defines a PyTorch neural network model for detecting DNS exfiltration.  It consists of linear layers, ReLU activation functions, a sigmoid output, and dropout layers for regularization. The init function initializes the model, loads the pre-trained weights from a specified directory and sets the model to evaluation mode.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_dns_data_exfiltration_using_pretrained_model_in_dsdl.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndevice='cpu'\nclass DNSExfiltration(nn.Module):\n    def __init__(self,input_size):\n        super().__init__()\n        self.layer_1 = nn.Linear(input_size, 128) \n        self.layer_2 = nn.Linear(128, 128)\n        self.layer_out = nn.Linear(128, 1) \n        \n        \n        self.relu = nn.ReLU()\n        self.sigmoid =  nn.Sigmoid()\n        self.dropout = nn.Dropout(p=0.5)\n        \n\n    def forward(self, input):\n        x = self.relu(self.layer_1(input))\n        x = self.dropout(x)\n        x = self.relu(self.layer_2(x))\n        x = self.dropout(x)\n        x = self.sigmoid(self.layer_out(x))\n        return x\n    \n\nkeys = list(string.printable.strip())\nprint (len(list(string.printable.strip())))\n    \ndef init(df,param):\n    model = DNSExfiltration(98)\n    model.load_state_dict(torch.load(MODEL_DIRECTORY+'detect_dns_data_exfiltration_using_pretrained_model_in_dsdl.pt',map_location=torch.device('cpu')))\n    model = model.to('cpu')\n    model.eval()\n    return model\n```\n\n----------------------------------------\n\nTITLE: Define ProcessnameClassifier Model with PyTorch Python\nDESCRIPTION: Defines a `ProcessnameClassifier` class inheriting from `nn.Module`. This class implements a Recurrent Neural Network (RNN) for classifying processnames. Includes `__init__`, `forward`, and `initHidden` methods to initialize the model, perform the forward pass, and initialize the hidden state, respectively. Requires `torch` and `torch.nn`.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_processnames_using_pretrained_model_in_dsdl.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# initialize your model\n# available inputs: data and parameters\n# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n\nclass ProcessnameClassifier(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(ProcessnameClassifier, self).__init__()\n        self.hidden_size = hidden_size\n        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, input, hidden):\n        combined = torch.cat((input, hidden), 1)\n        hidden = self.i2h(combined)\n        output = self.i2o(combined)\n        output = self.softmax(output)\n        return output, hidden\n\n    def initHidden(self):\n        return torch.zeros(1, self.hidden_size)\n\n    \n```\n\n----------------------------------------\n\nTITLE: Load Pre-trained DNS Exfiltration Detection Model\nDESCRIPTION: This function loads the pre-trained DNS exfiltration detection model. It initializes the DNSExfiltration class, loads the state dictionary from the file, moves the model to CPU for inference, and sets the model to evaluation mode.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_dns_data_exfiltration_using_pretrained_model_in_dsdl.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef load(name):\n    model = DNSExfiltration(98)\n    model.load_state_dict(torch.load(MODEL_DIRECTORY+'detect_dns_data_exfiltration_using_pretrained_model_in_dsdl.pt',map_location=torch.device('cpu')))\n    model = model.to('cpu')\n    model.eval()\n    return model\n```\n\n----------------------------------------\n\nTITLE: Import Libraries and Define Constants Python\nDESCRIPTION: Imports necessary libraries (pandas, numpy, torch, etc.) and defines global constants such as the model directory, character set, hidden layer size, number of categories, and learning rate. These are critical for the functionality of the deep learning model.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_processnames_using_pretrained_model_in_dsdl.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# This definition exposes all python module imports that should be available in all subsequent commands\n\nimport pandas as pd\nimport numpy as np\nimport random\nimport string\nimport torch\nimport torch.nn as nn\n\n# global constants\nMODEL_DIRECTORY = \"/srv/app/model/data/detect_suspicious_processnames_using_pretrained_model_in_dsdl/detect_suspicious_processnames_using_pretrained_model_in_dsdl.pt\"\n\n# model parameters\nall_letters = string.ascii_lowercase\nn_letters = len(all_letters)\nn_hidden = 32\nn_categories = 2\nlearning_rate = 5e-4\n```\n\n----------------------------------------\n\nTITLE: Initialize the ProcessnameClassifier Model Python\nDESCRIPTION: Defines an `init` function that initializes the `ProcessnameClassifier` model, loads the pre-trained weights from a specified directory, and sets the model to evaluation mode. It takes a DataFrame and parameters as input, but the DataFrame is not used inside the function. Requires `torch` and the model definition.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_processnames_using_pretrained_model_in_dsdl.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef init(df, param):\n    model = ProcessnameClassifier(n_letters, n_hidden, n_categories)\n    model.load_state_dict(torch.load(MODEL_DIRECTORY, map_location=torch.device('cpu')))\n    model.eval()\n    \n    return model\n    \n```\n\n----------------------------------------\n\nTITLE: Model Loading Function\nDESCRIPTION: Defines a `load` function to load the model from a file.  It re-instantiates the model and loads the state dict from the specified path. This effectively restores the model's weights and architecture.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_dns_txt_records_using_pretrained_model_in_dsdl.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# load model from name in expected convention \"<algo_name>_<model_name>\"\ndef load(name):\n    model = DNSTxtClassifier(vocab_size, embedding_dim, hidden_size,fc_hidden_size,dropout = dropout)\n    model.load_state_dict(torch.load(MODEL_DIRECTORY+'/detect_suspicious_dns_txt_records_using_pretrained_model_in_dsdl.pt',map_location=torch.device('cpu')))\n    model = model.to(device)\n    model.eval()\n    return model \n```\n\n----------------------------------------\n\nTITLE: Model Initialization with Pre-trained Weights (PyTorch)\nDESCRIPTION: This code defines the model architecture, a custom `DataFrameDataset` class and an `init` function to initialize a deep learning model (DNSTxtClassifier) with pre-trained weights. It loads the weights from a specified directory and sets the model to evaluation mode.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_dns_txt_records_using_pretrained_model_in_dsdl.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# initialize your model\n# available inputs: data and parameters\n# returns the model object which will be used as a reference to call fit, apply and summary subsequently\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \nv = CountVectorizer(analyzer='char_wb', ngram_range=(2, 4), lowercase=True)\nanalyzer = v.build_analyzer()\ndef tokenize(s):\n    tokens = analyzer(s)\n    \n    try:\n        tokens.remove(' ')\n    except ValueError:\n        pass\n    tokens = list(map(str.strip, tokens))\n    return tokens\n\nclass DataFrameDataset(Dataset):\n\n    def __init__(self, df):\n        self.texts = []\n        self.labels = []\n        for i, row in df.iterrows():\n            label = row.is_unknown\n            text = row.text\n            self.texts.append(text)\n            self.labels.append(label)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return self.texts[idx],self.labels[idx]\n\n\nclass DNSTxtClassifier(nn.Module):\n        def __init__(self, vocab_size, embed_dim, hidden_size, fc_hidden_size,dropout=0.5,layers=1):\n            super().__init__()\n            self.embedding = nn.Embedding(vocab_size,embed_dim,padding_idx=0)\n            self.lstm = nn.LSTM(embed_dim, hidden_size,batch_first=True,dropout=dropout, num_layers=1)\n            self.fc2 = nn.Linear(hidden_size, 1)\n            self.sigmoid = nn.Sigmoid()\n#lengths.to(device)\n        def forward(self, text, lengths):\n            out = self.embedding(text)\n            out = nn.utils.rnn.pack_padded_sequence(out,lengths.detach().numpy() , enforce_sorted=False, batch_first=True )\n            out, (hidden, cell) = self.lstm(out)\n            out = hidden[-1, :, :]\n            out = self.fc2(out)\n            out = self.sigmoid(out)\n            return out\n        \n\n\ndef init(df,param):\n    model = DNSTxtClassifier(vocab_size, embedding_dim, hidden_size,fc_hidden_size,dropout = dropout)\n    model.load_state_dict(torch.load(MODEL_DIRECTORY+'/detect_suspicious_dns_txt_records_using_pretrained_model_in_dsdl.pt',map_location=torch.device('cpu')))\n    model = model.to(device)\n    model.eval()\n    return model\n```\n\n----------------------------------------\n\nTITLE: Pull Data from Splunk using SplunkSearch Python\nDESCRIPTION: Imports the `SplunkSearch` class from the `dsdlsupport` module. This class is used to interact with the Splunk REST API and retrieve data into the Jupyter environment. Requires the Splunk REST API to be accessible and a valid Splunk auth token.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_processnames_using_pretrained_model_in_dsdl.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom dsdlsupport import SplunkSearch as SplunkSearch\n```\n\n----------------------------------------\n\nTITLE: Stage Data from CSV and JSON files\nDESCRIPTION: This function reads data from a CSV file and parameters from a JSON file.  It is intended to be used for staging data into the notebook environment within Splunk MLTK. It reads the data into a pandas DataFrame and the parameters into a dictionary.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_dns_data_exfiltration_using_pretrained_model_in_dsdl.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef stage(name):\n    with open(\"data/\"+name+\".csv\", 'r') as f:\n        df = pd.read_csv(f)\n    with open(\"data/\"+name+\".json\", 'r') as f:\n        param = json.load(f)\n    return df, param\n```\n\n----------------------------------------\n\nTITLE: Splunk Search Dataframe Retrieval\nDESCRIPTION: Retrieves data from Splunk and stores it in a Pandas DataFrame.  The `as_df()` method is used which queries splunk and converts the results into a dataframe.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_dns_txt_records_using_pretrained_model_in_dsdl.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf = search.as_df()\ndf\n```\n\n----------------------------------------\n\nTITLE: Convert Splunk Search to Pandas DataFrame Python\nDESCRIPTION: Executes the pre-configured Splunk search and retrieves the results as a Pandas DataFrame. This DataFrame can then be used for further analysis and model training. Requires a `SplunkSearch` object to be initialized.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_processnames_using_pretrained_model_in_dsdl.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndf = search.as_df()\ndf\n```\n\n----------------------------------------\n\nTITLE: Stage Data from CSV and JSON Files\nDESCRIPTION: This function `stage` reads data from a CSV file and parameters from a JSON file. It opens the files using the provided name, loads the data into a pandas DataFrame and the parameters into a dictionary, and returns both.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/pretrained_dga_model_dsdl.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef stage(name):\n    with open(\"data/\"+name+\".csv\", 'r') as f:\n        df = pd.read_csv(f)\n    with open(\"data/\"+name+\".json\", 'r') as f:\n        param = json.load(f)\n    return df, param\n```\n\n----------------------------------------\n\nTITLE: Validating Content with contentctl\nDESCRIPTION: This command validates the YAML files in the project using 'contentctl'. It checks for adherence to defined specifications, required fields, correct data types, and overall consistency to maintain the integrity and quality of the content.\nSOURCE: https://github.com/splunk/security_content/blob/develop/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncontentctl validate\n```\n\n----------------------------------------\n\nTITLE: Initialize Pre-trained DGA Detection Model\nDESCRIPTION: This function `init` loads a pre-trained deep learning model for DGA detection using TensorFlow/Keras. It retrieves the model from a specified directory and returns the loaded model object.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/pretrained_dga_model_dsdl.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef init(df,param):\n    model = tf.keras.models.load_model(MODEL_DIRECTORY + \"pretrained_dga_model_dsdl\")\n    return model\n```\n\n----------------------------------------\n\nTITLE: Setting up Environment (Quick Start)\nDESCRIPTION: This script sets up the necessary environment for working with the Splunk Security Content project. It clones the repository, navigates to the directory, creates a Python virtual environment, activates the environment, and installs the contentctl tool. It requires git and python3.11 to be installed on the system.\nSOURCE: https://github.com/splunk/security_content/blob/develop/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/splunk/security_content.git\ncd security_content\npython3.11 -m venv .venv\nsource .venv/bin/activate\npip install contentctl\n```\n\n----------------------------------------\n\nTITLE: Stage Data Loading Function\nDESCRIPTION: Defines a function to load data and parameters from CSV and JSON files, respectively. This is designed for use with Splunk's `mode=stage` functionality, where search results are exported to these files.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_dns_txt_records_using_pretrained_model_in_dsdl.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\ndef stage(name):\n    with open(\"data/\"+name+\".csv\", 'r') as f:\n        df = pd.read_csv(f)\n    with open(\"data/\"+name+\".json\", 'r') as f:\n        param = json.load(f)\n    return df, param\n```\n\n----------------------------------------\n\nTITLE: Import Libraries\nDESCRIPTION: This code block imports necessary Python libraries for data manipulation, deep learning with PyTorch, and feature extraction. It also defines global constants used throughout the notebook, such as the model directory, vocabulary size, and embedding dimensions.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_dns_txt_records_using_pretrained_model_in_dsdl.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# this definition exposes all python module imports that should be available in all subsequent commands\nimport numpy as np \nimport pandas as pd \nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom torch.autograd import Variable\nfrom torch.optim import lr_scheduler\nfrom collections import Counter,OrderedDict\nimport pickle\n\n# global constants\nMODEL_DIRECTORY = \"/srv/app/model/data/detect_suspicious_dns_txt_records_using_pretrained_model_in_dsdl/\"\nvocab_size = 10002\nembedding_dim = 64\nhidden_size = 64\nfc_hidden_size = 64\nnum_output_nodes = 1\n\ndropout = 0.5\n```\n\n----------------------------------------\n\nTITLE: Import Python Libraries\nDESCRIPTION: This code snippet imports necessary Python libraries for data manipulation, numerical computation, deep learning with PyTorch, and machine learning tasks. It includes libraries such as pandas, numpy, torch, and scikit-learn.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_dns_data_exfiltration_using_pretrained_model_in_dsdl.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport json\nimport numpy as np\nimport pandas as pd\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader,TensorDataset,Dataset\nfrom torch.autograd import Variable as V\nimport time\nfrom sklearn.metrics import classification_report, confusion_matrix,roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nimport string\nimport torch.nn as nn\nimport collections\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import Normalizer, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom collections import Counter\nimport torch.optim as optim\nimport math\nMODEL_DIRECTORY = \"/srv/app/model/data/detect_dns_data_exfiltration_using_pretrained_model_in_dsdl/\"\n```\n\n----------------------------------------\n\nTITLE: Building ESCU App with contentctl\nDESCRIPTION: This command builds an ESCU (Enterprise Security Content Update) app using 'contentctl'. The '--enrichments' flag indicates that the build process should incorporate enrichments from Atomic Red Team and Mitre CTI repositories, enhancing the app's functionality.\nSOURCE: https://github.com/splunk/security_content/blob/develop/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncontentctl build --enrichments\n```\n\n----------------------------------------\n\nTITLE: Stage Data from CSV and JSON files Python\nDESCRIPTION: Defines a function `stage` that reads data from a CSV file and metadata from a JSON file, both named based on the input `name`. It returns a Pandas DataFrame and a dictionary containing the loaded parameters. This function simulates loading data pushed from Splunk via `mode=stage`.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_processnames_using_pretrained_model_in_dsdl.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# this cell is not executed from MLTK and should only be used for staging data into the notebook environment\ndef stage(name):\n    with open(\"data/\"+name+\".csv\", 'r') as f:\n        df = pd.read_csv(f)\n    with open(\"data/\"+name+\".json\", 'r') as f:\n        param = json.load(f)\n    return df, param\n```\n\n----------------------------------------\n\nTITLE: Installing contentctl with pip\nDESCRIPTION: This command installs the latest version of 'contentctl' using pip, the Python package installer. 'contentctl' is a tool used for validating, building, and testing Splunk security content, and it is a prerequisite for working with the project.\nSOURCE: https://github.com/splunk/security_content/blob/develop/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install contentctl\n```\n\n----------------------------------------\n\nTITLE: Sending JSON Object to Splunk HEC (Python)\nDESCRIPTION: This code snippet shows how to send a JSON object to a Splunk HEC endpoint using the `hec.send()` function. It constructs a dictionary containing an 'event' with a message and log level, and a 'time' field with the current timestamp. It requires the `datetime` module to be imported to generate the timestamp and uses the `hec` library.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_processnames_using_pretrained_model_in_dsdl.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom datetime import datetime\nresponse = hec.send({'event': {'message': 'operation done', 'log_level': 'INFO' }, 'time': datetime.now().timestamp()})\n```\n\n----------------------------------------\n\nTITLE: Define Load Function Python\nDESCRIPTION: Defines a `load` function that loads the pre-trained weights from a specified directory into a `ProcessnameClassifier` model. The `load` function initializes the model from the state dict saved at `MODEL_DIRECTORY`. Requires `torch` and the model definition.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_processnames_using_pretrained_model_in_dsdl.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# load model from name in expected convention \"<algo_name>_<model_name>\"\ndef load(name):\n    model = ProcessnameClassifier(n_letters, n_hidden, n_categories)\n    model.load_state_dict(torch.load(MODEL_DIRECTORY, map_location=torch.device('cpu')))\n    model.eval()\n    return model \n```\n\n----------------------------------------\n\nTITLE: Model Summary Function in Python\nDESCRIPTION: This Python function returns a summary of the model including the versions of numpy and pandas. It requires the numpy and pandas libraries to be imported as np and pd respectively.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_dns_data_exfiltration_using_pretrained_model_in_dsdl.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# return a model summary\ndef summary(model=None):\n    returns = {\"version\": {\"numpy\": np.__version__, \"pandas\": pd.__version__} }\n    return returns\n```\n\n----------------------------------------\n\nTITLE: Model Fitting Function\nDESCRIPTION: Defines a `fit` function that currently only returns a message indicating the model is trained.  This is a placeholder for actual training logic.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_dns_txt_records_using_pretrained_model_in_dsdl.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# train your model\n# returns a fit info json object and may modify the model object\ndef fit(model,df,param):\n    # model.fit()\n    info = {\"message\": \"model trained\"}\n    return info\n```\n\n----------------------------------------\n\nTITLE: Train Barebone Model using SPL\nDESCRIPTION: This Splunk Processing Language (SPL) code trains the barebone model using the 'fit' command. It generates sample data, creates features, and then trains the model 'barebone_model' using the 's' field as the target variable and 'feature_*' fields as input features.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_dns_data_exfiltration_using_pretrained_model_in_dsdl.ipynb#_snippet_8\n\nLANGUAGE: splunk\nCODE:\n```\n| makeresults count=10\n| streamstats c as i\n| eval s = i%3\n| eval feature_{s}=0\n| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]\n| fit MLTKContainer algo=barebone s from feature_* into app:barebone_model\n```\n\n----------------------------------------\n\nTITLE: Creating New Detection YAML with contentctl\nDESCRIPTION: This command initiates the creation of a new detection.yml file using the 'contentctl' tool. It likely prompts the user with a series of questions to gather the necessary information for defining the detection, ensuring it adheres to the required specifications.\nSOURCE: https://github.com/splunk/security_content/blob/develop/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ncontentctl new\n```\n\n----------------------------------------\n\nTITLE: Provide Model Summary (Version Info)\nDESCRIPTION: The `summary` function returns a dictionary containing version information for numpy and pandas. This is useful for tracking the environment in which the model was developed and used.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/pretrained_dga_model_dsdl.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef summary(model=None):\n    returns = {\"version\": {\"numpy\": np.__version__, \"pandas\": pd.__version__} }\n    return returns\n```\n\n----------------------------------------\n\nTITLE: Define Summary Function Python\nDESCRIPTION: Defines a `summary` function that returns a dictionary containing the versions of NumPy and Pandas. This function provides a basic summary of the environment in which the model is running. Requires `numpy` and `pandas`.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_processnames_using_pretrained_model_in_dsdl.ipynb#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# return a model summary\ndef summary(model=None):\n    returns = {\"version\": {\"numpy\": np.__version__, \"pandas\": pd.__version__} }\n    return returns\n```\n\n----------------------------------------\n\nTITLE: Splunk Search Library Import\nDESCRIPTION: Imports a custom library for interacting with Splunk.  This assumes a `SplunkSearch.py` file exists within a `libs` directory and provides functionalities for querying data from Splunk.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_dns_txt_records_using_pretrained_model_in_dsdl.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport libs.SplunkSearch as SplunkSearch\n```\n\n----------------------------------------\n\nTITLE: Model Summary Function in Python\nDESCRIPTION: This Python function returns a dictionary containing the versions of the NumPy and Pandas libraries.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_dns_txt_records_using_pretrained_model_in_dsdl.ipynb#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# return a model summary\ndef summary(model=None):\n    returns = {\"version\": {\"numpy\": np.__version__, \"pandas\": pd.__version__} }\n    return returns\n```\n\n----------------------------------------\n\nTITLE: Import SplunkHEC and Instantiate Python\nDESCRIPTION: Imports the `SplunkHEC` class from the `dsdlsupport` module and instantiates an object. This object is used to send data to Splunk's HTTP Event Collector (HEC). Requires the `dsdlsupport` library and configured Splunk HEC settings.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_processnames_using_pretrained_model_in_dsdl.ipynb#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom dsdlsupport import SplunkHEC as SplunkHEC\nhec = SplunkHEC.SplunkHEC()\n```\n\n----------------------------------------\n\nTITLE: Splunk Search Object Instantiation\nDESCRIPTION: Instantiates an object from the `SplunkSearch` class. This creates an instance of the Splunk search functionality, likely with pre-configured connection parameters.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_dns_txt_records_using_pretrained_model_in_dsdl.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsearch = SplunkSearch.SplunkSearch()\n```\n\n----------------------------------------\n\nTITLE: Print HEC Response for JSON in Python\nDESCRIPTION: This Python code prints the HEC endpoint, status code, and response message after sending a JSON object to Splunk. This is used to verify the successful transmission of the JSON data.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_dns_txt_records_using_pretrained_model_in_dsdl.ipynb#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nprint(\"HEC endpoint %s \\nreturned with status code %s \\nand response message: %s\" % (response.url, response.status_code, response.text))\n```\n\n----------------------------------------\n\nTITLE: Import SplunkHEC in Python\nDESCRIPTION: This Python code imports the `SplunkHEC` module and instantiates the SplunkHEC class, creating an object named `hec`. This object is used to interact with the Splunk HTTP Event Collector (HEC).\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_dns_txt_records_using_pretrained_model_in_dsdl.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport libs.SplunkHEC as SplunkHEC\nhec = SplunkHEC.SplunkHEC()\n```\n\n----------------------------------------\n\nTITLE: Printing HEC Response (Python)\nDESCRIPTION: This code snippet demonstrates how to print the details of an HTTP response received from a Splunk HEC endpoint. It extracts the URL, status code, and response message from the `response` object and prints them to the console. It assumes the `response` variable has been populated from a call to a `hec` function.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_processnames_using_pretrained_model_in_dsdl.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nprint(\"HEC endpoint %s \\nreturned with status code %s \\nand response message: %s\" % (response.url, response.status_code, response.text))\n```\n\n----------------------------------------\n\nTITLE: Print HEC Response in Python\nDESCRIPTION: This Python code prints the URL of the HEC endpoint, the HTTP status code of the response, and the response message from the HEC endpoint. This helps verify successful data submission.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_dns_txt_records_using_pretrained_model_in_dsdl.ipynb#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nprint(\"HEC endpoint %s \\nreturned with status code %s \\nand response message: %s\" % (response.url, response.status_code, response.text))\n```\n\n----------------------------------------\n\nTITLE: Sending Hello World Events to Splunk HEC (Python)\nDESCRIPTION: This code snippet demonstrates how to send a specified number of 'hello world' events to a Splunk HEC endpoint using the `hec.send_hello_world()` function. The number of events to send is passed as an argument. The example requires the `hec` library to be available.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_processnames_using_pretrained_model_in_dsdl.ipynb#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nresponse = hec.send_hello_world(10)\n```\n\n----------------------------------------\n\nTITLE: Send Hello World Events to Splunk HEC in Python\nDESCRIPTION: This Python code calls the `send_hello_world` method of the `SplunkHEC` object `hec` to send 10 \"hello world\" events to the Splunk HTTP Event Collector (HEC). The response from the HEC endpoint is stored in the `response` variable.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_dns_txt_records_using_pretrained_model_in_dsdl.ipynb#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# example to send 10 hello world events\nresponse = hec.send_hello_world(10)\n```\n\n----------------------------------------\n\nTITLE: Define Fit Function Python\nDESCRIPTION: Defines a placeholder `fit` function that simulates training the model. It takes a model, DataFrame, and parameters as input, but does not actually train the model. Instead, it returns a dictionary containing a success message. This function is a placeholder for actual model training logic.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_processnames_using_pretrained_model_in_dsdl.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# train your model\n# returns a fit info json object and may modify the model object\ndef fit(model,df,param):\n    # model.fit()\n    info = {\"message\": \"model trained\"}\n    return info\n```\n\n----------------------------------------\n\nTITLE: Send JSON object to Splunk HEC in Python\nDESCRIPTION: This Python code sends a JSON object to the Splunk HTTP Event Collector (HEC). The JSON object contains a message and log level, along with the current timestamp. It uses the `send` method of the SplunkHEC object.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_dns_txt_records_using_pretrained_model_in_dsdl.ipynb#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# example to send a JSON object, e.g. to log some data\nfrom datetime import datetime\nresponse = hec.send({'event': {'message': 'operation done', 'log_level': 'INFO' }, 'time': datetime.now().timestamp()})\n```\n\n----------------------------------------\n\nTITLE: Navigating to Repository Directory with CD\nDESCRIPTION: This command changes the current directory in the terminal to the 'security_content' directory, which is the location where the repository was cloned. It is necessary to navigate into this directory to execute further commands related to the project.\nSOURCE: https://github.com/splunk/security_content/blob/develop/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd security_content\n```\n\n----------------------------------------\n\nTITLE: Cloning Repository with Git\nDESCRIPTION: This command clones the Splunk Security Content repository from GitHub. It downloads the entire project, including all its files and history, to your local machine, enabling you to access and modify the content.\nSOURCE: https://github.com/splunk/security_content/blob/develop/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/splunk/security_content.git\n```\n\n----------------------------------------\n\nTITLE: Apply Barebone Model using SPL\nDESCRIPTION: This Splunk Processing Language (SPL) code applies the previously trained 'barebone_model'. It generates sample data, creates features, and then applies the model, storing the prediction in the 'the_meaning_of_life' field.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_dns_data_exfiltration_using_pretrained_model_in_dsdl.ipynb#_snippet_9\n\nLANGUAGE: splunk\nCODE:\n```\n| makeresults count=10\n| streamstats c as i\n| eval s = i%3\n| eval feature_{s}=0\n| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]\n| apply barebone_model as the_meaning_of_life\n```\n\n----------------------------------------\n\nTITLE: Applying the Barebone Model in SPL\nDESCRIPTION: This SPL code generates sample data, creates features, and then uses the `apply` command to apply the trained `barebone_model` to the data. The results are stored in the field `the_meaning_of_life`.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_dns_txt_records_using_pretrained_model_in_dsdl.ipynb#_snippet_12\n\nLANGUAGE: spl\nCODE:\n```\n| makeresults count=10\n| streamstats c as i\n| eval s = i%3\n| eval feature_{s}=0\n| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]\n| apply barebone_model as the_meaning_of_life\n```\n\n----------------------------------------\n\nTITLE: Training the Barebone Model in SPL\nDESCRIPTION: This SPL code generates sample data, creates features, and then uses the `fit` command to train a barebone MLTK model named `barebone_model` using the generated features. The model is stored in the `app:barebone_model`.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_dns_txt_records_using_pretrained_model_in_dsdl.ipynb#_snippet_11\n\nLANGUAGE: spl\nCODE:\n```\n| makeresults count=10\n| streamstats c as i\n| eval s = i%3\n| eval feature_{s}=0\n| foreach feature_* [eval &lt;&lt;FIELD&gt;&gt;=random()/pow(2,31)]\n| fit MLTKContainer algo=barebone s from feature_* into app:barebone_model\n```\n\n----------------------------------------\n\nTITLE: Fit the DGA Detection Model (No Active Training)\nDESCRIPTION: This function `fit` simulates the training of the model, but it doesn't actually train the model in this specific implementation. It returns a dictionary with a message indicating that the model has been 'trained'.  It takes the model, dataframe, and parameters as input but does not modify the model.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/pretrained_dga_model_dsdl.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef fit(model,df,param):\n    # model.fit()\n    info = {\"message\": \"model trained\"}\n    return info\n```\n\n----------------------------------------\n\nTITLE: Fit the Model (Dummy Implementation)\nDESCRIPTION: This is a placeholder function for fitting the model.  Currently, it returns a simple message indicating that the model is trained.  In a real-world scenario, this function would contain the actual model training logic.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_dns_data_exfiltration_using_pretrained_model_in_dsdl.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef fit(model,df,param):\n    # model.fit()\n    info = {\"message\": \"model trained\"}\n    return info\n```\n\n----------------------------------------\n\nTITLE: Save Model (JSON Placeholder)\nDESCRIPTION: This function `save` is intended to save the model to a JSON file, but in its current form, it attempts to serialize the entire model object as JSON, which is likely not feasible.  A more appropriate implementation would save the model's weights or other relevant state.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/pretrained_dga_model_dsdl.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef save(model,name):\n    with open(MODEL_DIRECTORY + name + \".json\", 'w') as file:\n        json.dump(model, file)\n    return model\n```\n\n----------------------------------------\n\nTITLE: Model Saving Function\nDESCRIPTION: Defines a `save` function to save the model to a JSON file. This function is a placeholder and currently saves a dictionary representation of the model. The filename follows the expected convention: `<algo_name>_<model_name>`.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_dns_txt_records_using_pretrained_model_in_dsdl.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# save model to name in expected convention \"<algo_name>_<model_name>\"\ndef save(model,name):\n    with open(MODEL_DIRECTORY + name + \".json\", 'w') as file:\n        json.dump(model, file)\n    return model\n```\n\n----------------------------------------\n\nTITLE: Save the Model (Dummy Implementation)\nDESCRIPTION: This is a placeholder function for saving the model to a JSON file. Currently, it serializes the model object as JSON and saves it to the specified directory. In the real implementation, this function should save the state_dict of the model rather than the entire model.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_dns_data_exfiltration_using_pretrained_model_in_dsdl.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef save(model,name):\n    with open(MODEL_DIRECTORY + name + \".json\", 'w') as file:\n        json.dump(model, file)\n    return model\n```\n\n----------------------------------------\n\nTITLE: Define Save Function Python\nDESCRIPTION: Defines a `save` function that saves the model to a JSON file. It takes the model object and a name as input, but actually saves the Python dictionary representation of the model, not the PyTorch state dictionary. Requires the `json` library.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_processnames_using_pretrained_model_in_dsdl.ipynb#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# save model to name in expected convention \"<algo_name>_<model_name>\"\ndef save(model,name):\n    with open(MODEL_DIRECTORY + name + \".json\", 'w') as file:\n        json.dump(model, file)\n    return model\n```\n\n----------------------------------------\n\nTITLE: Instantiate SplunkSearch object Python\nDESCRIPTION: Creates an instance of the `SplunkSearch` class. This object is used to execute SPL queries and retrieve the results as a Pandas DataFrame. Depends on the `SplunkSearch` class being available and configured.\nSOURCE: https://github.com/splunk/security_content/blob/develop/notebooks/detect_suspicious_processnames_using_pretrained_model_in_dsdl.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsearch = SplunkSearch.SplunkSearch()\n```"
  }
]
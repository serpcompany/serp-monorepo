[
  {
    "owner": "patrick-kidger",
    "repo": "diffrax",
    "content": "TITLE: Solving Controlled Differential Equations with Diffrax in Python\nDESCRIPTION: This example demonstrates solving a CDE using Diffrax. It defines a custom quadratic path as the control signal, sets up the vector field and term, and solves the equation dy = -y * dx(t) with x(t) = t^2 and initial condition y(0) = 1 over the interval [0, 3] using the Dormand-Prince method.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/usage/getting-started.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffrax import AbstractPath, ControlTerm, diffeqsolve, Dopri5\n\n\nclass QuadraticPath(AbstractPath):\n    @property\n    def t0(self):\n        return 0\n\n    @property\n    def t1(self):\n        return 3\n\n    def evaluate(self, t0, t1=None, left=True):\n        del left\n        if t1 is not None:\n            return self.evaluate(t1) - self.evaluate(t0)\n        return t0 ** 2\n\n\nvector_field = lambda t, y, args: -y\ncontrol = QuadraticPath()\nterm = ControlTerm(vector_field, control).to_ode()\nsolver = Dopri5()\nsol = diffeqsolve(term, solver, t0=0, t1=3, dt0=0.05, y0=1)\n\nprint(sol.ts)  # DeviceArray([3.])\nprint(sol.ys)  # DeviceArray([0.00012341])\n```\n\n----------------------------------------\n\nTITLE: Implementing Neural CDE Model\nDESCRIPTION: This class implements the full Neural CDE model. It includes an initial MLP to process the input, the Func class for the vector field, and a final linear layer with sigmoid activation for binary classification.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/neural_cde.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass NeuralCDE(eqx.Module):\n    initial: eqx.nn.MLP\n    func: Func\n    linear: eqx.nn.Linear\n\n    def __init__(self, data_size, hidden_size, width_size, depth, *, key, **kwargs):\n        super().__init__(**kwargs)\n        ikey, fkey, lkey = jr.split(key, 3)\n        self.initial = eqx.nn.MLP(data_size, hidden_size, width_size, depth, key=ikey)\n        self.func = Func(data_size, hidden_size, width_size, depth, key=fkey)\n        self.linear = eqx.nn.Linear(hidden_size, 1, key=lkey)\n\n    def __call__(self, ts, coeffs, evolving_out=False):\n        # Each sample of data consists of some timestamps `ts`, and some `coeffs`\n        # parameterising a control path. These are used to produce a continuous-time\n        # input path `control`.\n        control = diffrax.CubicInterpolation(ts, coeffs)\n        term = diffrax.ControlTerm(self.func, control).to_ode()\n        solver = diffrax.Tsit5()\n        dt0 = None\n        y0 = self.initial(control.evaluate(ts[0]))\n        if evolving_out:\n            saveat = diffrax.SaveAt(ts=ts)\n        else:\n            saveat = diffrax.SaveAt(t1=True)\n        solution = diffrax.diffeqsolve(\n            term,\n            solver,\n            ts[0],\n            ts[-1],\n            dt0,\n            y0,\n            stepsize_controller=diffrax.PIDController(rtol=1e-3, atol=1e-6),\n            saveat=saveat,\n        )\n        if evolving_out:\n            prediction = jax.vmap(lambda y: jnn.sigmoid(self.linear(y))[0])(solution.ys)\n        else:\n            (prediction,) = jnn.sigmoid(self.linear(solution.ys[-1]))\n        return prediction\n```\n\n----------------------------------------\n\nTITLE: Implementing Main Training Loop for CNF in Python\nDESCRIPTION: Defines the main function that sets up the CNF model, data processing, optimization, and training loop. Includes options for hyperparameters and model configuration.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/continuous_normalising_flow.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndef main(\n    in_path,\n    out_path=None,\n    batch_size=500,\n    virtual_batches=2,\n    lr=1e-3,\n    weight_decay=1e-5,\n    steps=10000,\n    exact_logp=True,\n    num_blocks=2,\n    width_size=64,\n    depth=3,\n    print_every=100,\n    seed=5678,\n):\n    if out_path is None:\n        out_path = here / pathlib.Path(in_path).name\n    else:\n        out_path = pathlib.Path(out_path)\n\n    key = jr.PRNGKey(seed)\n    model_key, loader_key, loss_key, sample_key = jr.split(key, 4)\n\n    dataset, weights, mean, std, img, width, height = get_data(in_path)\n    dataset_size, data_size = dataset.shape\n    dataloader = DataLoader((dataset, weights), batch_size, key=loader_key)\n\n    model = CNF(\n        data_size=data_size,\n        exact_logp=exact_logp,\n        num_blocks=num_blocks,\n        width_size=width_size,\n        depth=depth,\n        key=model_key,\n    )\n\n    optim = optax.adamw(lr, weight_decay=weight_decay)\n    opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n\n    @eqx.filter_value_and_grad\n    def loss(model, data, weight, loss_key):\n        batch_size, _ = data.shape\n        noise_key, train_key = jr.split(loss_key, 2)\n        train_key = jr.split(key, batch_size)\n        data = data + jr.normal(noise_key, data.shape) * 0.5 / std\n        log_likelihood = jax.vmap(model.train)(data, key=train_key)\n        return -jnp.mean(weight * log_likelihood)  # minimise negative log-likelihood\n\n    @eqx.filter_jit\n    def make_step(model, opt_state, step, loss_key):\n        # We only need gradients with respect to floating point JAX arrays, not any\n        # other part of our model. (e.g. the `exact_logp` flag. What would it even mean\n        # to differentiate that? Note that `eqx.filter_value_and_grad` does the same\n        # filtering by `eqx.is_inexact_array` by default.)\n        value = 0\n        grads = jax.tree_util.tree_map(\n            lambda leaf: jnp.zeros_like(leaf) if eqx.is_inexact_array(leaf) else None,\n            model,\n        )\n\n        # Get more accurate gradients by accumulating gradients over multiple batches.\n        # (Or equivalently, get lower memory requirements by splitting up a batch over\n        # multiple steps.)\n        def make_virtual_step(_, state):\n            value, grads, step, loss_key = state\n            data, weight = dataloader(step)\n            value_, grads_ = loss(model, data, weight, loss_key)\n            value = value + value_\n            grads = jax.tree_util.tree_map(lambda a, b: a + b, grads, grads_)\n            step = step + 1\n            loss_key = jr.split(loss_key, 1)[0]\n            return value, grads, step, loss_key\n\n        value, grads, step, loss_key = lax.fori_loop(\n            0, virtual_batches, make_virtual_step, (value, grads, step, loss_key)\n        )\n        value = value / virtual_batches\n        grads = jax.tree_util.tree_map(lambda a: a / virtual_batches, grads)\n        updates, opt_state = optim.update(\n            grads, opt_state, eqx.filter(model, eqx.is_inexact_array)\n        )\n```\n\n----------------------------------------\n\nTITLE: Solving ODE Step-by-Step with Diffrax Tsit5 Solver in Python\nDESCRIPTION: This code demonstrates how to solve an ODE step-by-step using the Tsit5 solver from Diffrax. It initializes the solver, sets up the ODE parameters, and then iteratively solves the equation, printing the result at each time step. The ODE being solved is dy/dt = -y.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/usage/manual-stepping.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax.numpy as jnp\nfrom diffrax import ODETerm, Tsit5\n\nvector_field = lambda t, y, args: -y\nterm = ODETerm(vector_field)\nsolver = Tsit5()\n\nt0 = 0\ndt0 = 0.05\nt1 = 1\ny0 = jnp.array(1.0)\nargs = None\n\ntprev = t0\ntnext = t0 + dt0\ny = y0\nstate = solver.init(term, tprev, tnext, y0, args)\n\nwhile tprev < t1:\n    y, _, _, state, _ = solver.step(term, tprev, tnext, y, args, state, made_jump=False)\n    print(f\"At time {tnext} obtained value {y}\")\n    tprev = tnext\n    tnext = min(tprev + dt0, t1)\n```\n\n----------------------------------------\n\nTITLE: Implementing Latent ODE Model\nDESCRIPTION: This class implements the complete Latent ODE model, including the encoder (VAE), decoder, loss function, and methods for training and sampling. It uses Diffrax for solving the differential equations.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/latent_ode.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass LatentODE(eqx.Module):\n    func: Func\n    rnn_cell: eqx.nn.GRUCell\n\n    hidden_to_latent: eqx.nn.Linear\n    latent_to_hidden: eqx.nn.MLP\n    hidden_to_data: eqx.nn.Linear\n\n    hidden_size: int\n    latent_size: int\n\n    def __init__(\n        self, *, data_size, hidden_size, latent_size, width_size, depth, key, **kwargs\n    ):\n        super().__init__(**kwargs)\n\n        mkey, gkey, hlkey, lhkey, hdkey = jr.split(key, 5)\n\n        scale = jnp.ones(())\n        mlp = eqx.nn.MLP(\n            in_size=hidden_size,\n            out_size=hidden_size,\n            width_size=width_size,\n            depth=depth,\n            activation=jnn.softplus,\n            final_activation=jnn.tanh,\n            key=mkey,\n        )\n        self.func = Func(scale, mlp)\n        self.rnn_cell = eqx.nn.GRUCell(data_size + 1, hidden_size, key=gkey)\n\n        self.hidden_to_latent = eqx.nn.Linear(hidden_size, 2 * latent_size, key=hlkey)\n        self.latent_to_hidden = eqx.nn.MLP(\n            latent_size, hidden_size, width_size=width_size, depth=depth, key=lhkey\n        )\n        self.hidden_to_data = eqx.nn.Linear(hidden_size, data_size, key=hdkey)\n\n        self.hidden_size = hidden_size\n        self.latent_size = latent_size\n\n    # Encoder of the VAE\n    def _latent(self, ts, ys, key):\n        data = jnp.concatenate([ts[:, None], ys], axis=1)\n        hidden = jnp.zeros((self.hidden_size,))\n        for data_i in reversed(data):\n            hidden = self.rnn_cell(data_i, hidden)\n        context = self.hidden_to_latent(hidden)\n        mean, logstd = context[: self.latent_size], context[self.latent_size :]\n        std = jnp.exp(logstd)\n        latent = mean + jr.normal(key, (self.latent_size,)) * std\n        return latent, mean, std\n\n    # Decoder of the VAE\n    def _sample(self, ts, latent):\n        dt0 = 0.4  # selected as a reasonable choice for this problem\n        y0 = self.latent_to_hidden(latent)\n        sol = diffrax.diffeqsolve(\n            diffrax.ODETerm(self.func),\n            diffrax.Tsit5(),\n            ts[0],\n            ts[-1],\n            dt0,\n            y0,\n            saveat=diffrax.SaveAt(ts=ts),\n        )\n        return jax.vmap(self.hidden_to_data)(sol.ys)\n\n    @staticmethod\n    def _loss(ys, pred_ys, mean, std):\n        # -log p_θ with Gaussian p_θ\n        reconstruction_loss = 0.5 * jnp.sum((ys - pred_ys) ** 2)\n        # KL(N(mean, std^2) || N(0, 1))\n        variational_loss = 0.5 * jnp.sum(mean**2 + std**2 - 2 * jnp.log(std) - 1)\n        return reconstruction_loss + variational_loss\n\n    # Run both encoder and decoder during training.\n    def train(self, ts, ys, *, key):\n        latent, mean, std = self._latent(ts, ys, key)\n        pred_ys = self._sample(ts, latent)\n        return self._loss(ys, pred_ys, mean, std)\n\n    # Run just the decoder during inference.\n    def sample(self, ts, *, key):\n        latent = jr.normal(key, (self.latent_size,))\n        return self._sample(ts, latent)\n```\n\n----------------------------------------\n\nTITLE: Main Training Loop and Visualization\nDESCRIPTION: Implements the main training function with model initialization, optimization setup, training loop, and result visualization using matplotlib.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/neural_sde.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef main(\n    initial_noise_size=5,\n    noise_size=3,\n    hidden_size=16,\n    width_size=16,\n    depth=1,\n    generator_lr=2e-5,\n    discriminator_lr=1e-4,\n    batch_size=1024,\n    steps=10000,\n    steps_per_print=200,\n    dataset_size=8192,\n    seed=5678,\n):\n    key = jr.PRNGKey(seed)\n    (\n        data_key,\n        generator_key,\n        discriminator_key,\n        dataloader_key,\n        train_key,\n        evaluate_key,\n        sample_key,\n    ) = jr.split(key, 7)\n    data_key = jr.split(data_key, dataset_size)\n\n    ts, ys = get_data(data_key)\n    _, _, data_size = ys.shape\n\n    generator = NeuralSDE(\n        data_size,\n        initial_noise_size,\n        noise_size,\n        hidden_size,\n        width_size,\n        depth,\n        key=generator_key,\n    )\n    discriminator = NeuralCDE(\n        data_size, hidden_size, width_size, depth, key=discriminator_key\n    )\n\n    g_optim = optax.rmsprop(generator_lr)\n    d_optim = optax.rmsprop(-discriminator_lr)\n    g_opt_state = g_optim.init(eqx.filter(generator, eqx.is_inexact_array))\n    d_opt_state = d_optim.init(eqx.filter(discriminator, eqx.is_inexact_array))\n\n    infinite_dataloader = dataloader(\n        (ts, ys), batch_size, loop=True, key=dataloader_key\n    )\n\n    for step, (ts_i, ys_i) in zip(range(steps), infinite_dataloader):\n        step = jnp.asarray(step)\n        generator, discriminator, g_opt_state, d_opt_state = make_step(\n            generator,\n            discriminator,\n            g_opt_state,\n            d_opt_state,\n            g_optim,\n            d_optim,\n            ts_i,\n            ys_i,\n            key,\n            step,\n        )\n        if (step % steps_per_print) == 0 or step == steps - 1:\n            total_score = 0\n            num_batches = 0\n            for ts_i, ys_i in dataloader(\n                (ts, ys), batch_size, loop=False, key=evaluate_key\n            ):\n                score = loss(generator, discriminator, ts_i, ys_i, sample_key)\n                total_score += score.item()\n                num_batches += 1\n            print(f\"Step: {step}, Loss: {total_score / num_batches}\")\n\n    # Plot samples\n    fig, ax = plt.subplots()\n    num_samples = min(50, dataset_size)\n    ts_to_plot = ts[:num_samples]\n    ys_to_plot = ys[:num_samples]\n\n    def _interp(ti, yi):\n        return diffrax.linear_interpolation(\n            ti, yi, replace_nans_at_start=0.0, fill_forward_nans_at_end=True\n        )\n\n    ys_to_plot = jax.vmap(_interp)(ts_to_plot, ys_to_plot)[..., 0]\n    ys_sampled = jax.vmap(generator)(ts_to_plot, key=jr.split(sample_key, num_samples))[\n        ..., 0\n    ]\n    kwargs = dict(label=\"Real\")\n    for ti, yi in zip(ts_to_plot, ys_to_plot):\n        ax.plot(ti, yi, c=\"dodgerblue\", linewidth=0.5, alpha=0.7, **kwargs)\n        kwargs = {}\n    kwargs = dict(label=\"Generated\")\n    for ti, yi in zip(ts_to_plot, ys_sampled):\n        ax.plot(ti, yi, c=\"crimson\", linewidth=0.5, alpha=0.7, **kwargs)\n        kwargs = {}\n    ax.set_title(f\"{num_samples} samples from both real and generated distributions.\")\n    fig.legend()\n    fig.tight_layout()\n    fig.savefig(\"neural_sde.png\")\n    plt.show()\n```\n\n----------------------------------------\n\nTITLE: Training and Evaluating Neural CDE Model\nDESCRIPTION: This main function sets up the Neural CDE model, defines the loss function, and implements the training loop. It also includes code for evaluating the model on test data and visualizing the results.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/neural_cde.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef main(\n    dataset_size=256,\n    add_noise=False,\n    batch_size=32,\n    lr=1e-2,\n    steps=20,\n    hidden_size=8,\n    width_size=128,\n    depth=1,\n    seed=5678,\n):\n    key = jr.PRNGKey(seed)\n    train_data_key, test_data_key, model_key, loader_key = jr.split(key, 4)\n\n    ts, coeffs, labels, data_size = get_data(\n        dataset_size, add_noise, key=train_data_key\n    )\n\n    model = NeuralCDE(data_size, hidden_size, width_size, depth, key=model_key)\n\n    # Training loop like normal.\n\n    @eqx.filter_jit\n    def loss(model, ti, label_i, coeff_i):\n        pred = jax.vmap(model)(ti, coeff_i)\n        # Binary cross-entropy\n        bxe = label_i * jnp.log(pred) + (1 - label_i) * jnp.log(1 - pred)\n        bxe = -jnp.mean(bxe)\n        acc = jnp.mean((pred > 0.5) == (label_i == 1))\n        return bxe, acc\n\n    grad_loss = eqx.filter_value_and_grad(loss, has_aux=True)\n\n    @eqx.filter_jit\n    def make_step(model, data_i, opt_state):\n        ti, label_i, *coeff_i = data_i\n        (bxe, acc), grads = grad_loss(model, ti, label_i, coeff_i)\n        updates, opt_state = optim.update(grads, opt_state)\n        model = eqx.apply_updates(model, updates)\n        return bxe, acc, model, opt_state\n\n    optim = optax.adam(lr)\n    opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n    for step, data_i in zip(\n        range(steps), dataloader((ts, labels) + coeffs, batch_size, key=loader_key)\n    ):\n        start = time.time()\n        bxe, acc, model, opt_state = make_step(model, data_i, opt_state)\n        end = time.time()\n        print(\n            f\"Step: {step}, Loss: {bxe}, Accuracy: {acc}, Computation time: \"\n            f\"{end - start}\"\n        )\n\n    ts, coeffs, labels, _ = get_data(dataset_size, add_noise, key=test_data_key)\n    bxe, acc = loss(model, ts, labels, coeffs)\n    print(f\"Test loss: {bxe}, Test Accuracy: {acc}\")\n\n    # Plot results\n    sample_ts = ts[-1]\n    sample_coeffs = tuple(c[-1] for c in coeffs)\n    pred = model(sample_ts, sample_coeffs, evolving_out=True)\n    interp = diffrax.CubicInterpolation(sample_ts, sample_coeffs)\n    values = jax.vmap(interp.evaluate)(sample_ts)\n    fig = plt.figure(figsize=(16, 8))\n    ax1 = fig.add_subplot(1, 2, 1)\n    ax2 = fig.add_subplot(1, 2, 2, projection=\"3d\")\n    ax1.plot(sample_ts, values[:, 1], c=\"dodgerblue\")\n    ax1.plot(sample_ts, values[:, 2], c=\"dodgerblue\", label=\"Data\")\n    ax1.plot(sample_ts, pred, c=\"crimson\", label=\"Classification\")\n    ax1.set_xticks([])\n    ax1.set_yticks([])\n    ax1.set_xlabel(\"t\")\n    ax1.legend()\n    ax2.plot(values[:, 1], values[:, 2], c=\"dodgerblue\", label=\"Data\")\n    ax2.plot(values[:, 1], values[:, 2], pred, c=\"crimson\", label=\"Classification\")\n    ax2.set_xticks([])\n    ax2.set_yticks([])\n    ax2.set_zticks([])\n    ax2.set_xlabel(\"x\")\n    ax2.set_ylabel(\"y\")\n    ax2.set_zlabel(\"Classification\")\n    plt.tight_layout()\n    plt.savefig(\"neural_cde.png\")\n    plt.show()\n```\n\n----------------------------------------\n\nTITLE: AbstractSolver Interface in Diffrax\nDESCRIPTION: The core abstract solver interface that defines the common behavior for all ODE and SDE solvers in Diffrax. It specifies methods for initialization, stepping, and accessing solver properties like order.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/solvers/abstract_solvers.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndiffrax.AbstractSolver\n    order\n    strong_order\n    error_order\n    init\n    step\n    func\n```\n\n----------------------------------------\n\nTITLE: Solving Ordinary Differential Equations with Diffrax in Python\nDESCRIPTION: This snippet demonstrates how to solve an ODE using Diffrax. It sets up the vector field, term, solver, and step size controller, then uses diffeqsolve to compute the solution. The example solves the equation dy/dt = -y with initial condition y(0) = 1 over the interval [0, 3].\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/usage/getting-started.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffrax import diffeqsolve, Dopri5, ODETerm, SaveAt, PIDController\n\nvector_field = lambda t, y, args: -y\nterm = ODETerm(vector_field)\nsolver = Dopri5()\nsaveat = SaveAt(ts=[0., 1., 2., 3.])\nstepsize_controller = PIDController(rtol=1e-5, atol=1e-5)\n\nsol = diffeqsolve(term, solver, t0=0, t1=3, dt0=0.1, y0=1, saveat=saveat,\n                  stepsize_controller=stepsize_controller)\n\nprint(sol.ts)  # DeviceArray([0.   , 1.   , 2.   , 3.    ])\nprint(sol.ys)  # DeviceArray([1.   , 0.368, 0.135, 0.0498])\n```\n\n----------------------------------------\n\nTITLE: Neural ODE Model Definition\nDESCRIPTION: Implements the complete Neural ODE model class that wraps the ODE solver with the vector field function.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/neural_ode.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass NeuralODE(eqx.Module):\n    func: Func\n\n    def __init__(self, data_size, width_size, depth, *, key, **kwargs):\n        super().__init__(**kwargs)\n        self.func = Func(data_size, width_size, depth, key=key)\n\n    def __call__(self, ts, y0):\n        solution = diffrax.diffeqsolve(\n            diffrax.ODETerm(self.func),\n            diffrax.Tsit5(),\n            t0=ts[0],\n            t1=ts[-1],\n            dt0=ts[1] - ts[0],\n            y0=y0,\n            stepsize_controller=diffrax.PIDController(rtol=1e-3, atol=1e-6),\n            saveat=diffrax.SaveAt(ts=ts),\n        )\n        return solution.ys\n```\n\n----------------------------------------\n\nTITLE: Solving ODE using Diffrax with Dormand-Prince 5(4) method\nDESCRIPTION: Demonstrates how to solve a simple ordinary differential equation using Diffrax. It defines the ODE, sets up the solver, and computes the solution using the Dormand-Prince 5(4) method.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/index.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffrax import diffeqsolve, ODETerm, Dopri5\nimport jax.numpy as jnp\n\ndef f(t, y, args):\n    return -y\n\nterm = ODETerm(f)\nsolver = Dopri5()\ny0 = jnp.array([2., 3.])\nsolution = diffeqsolve(term, solver, t0=0, t1=1, dt0=0.1, y0=y0)\n```\n\n----------------------------------------\n\nTITLE: Defining Continuous Normalizing Flow (CNF) Model in Python\nDESCRIPTION: Implements the main CNF model class, including methods for training, sampling, and visualizing the flow. Uses Diffrax for differential equation solving.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/continuous_normalising_flow.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndef normal_log_likelihood(y):\n    return -0.5 * (y.size * math.log(2 * math.pi) + jnp.sum(y**2))\n\n\nclass CNF(eqx.Module):\n    funcs: list[Func]\n    data_size: int\n    exact_logp: bool\n    t0: float\n    t1: float\n    dt0: float\n\n    def __init__(\n        self,\n        *,\n        data_size,\n        exact_logp,\n        num_blocks,\n        width_size,\n        depth,\n        key,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        keys = jr.split(key, num_blocks)\n        self.funcs = [\n            Func(\n                data_size=data_size,\n                width_size=width_size,\n                depth=depth,\n                key=k,\n            )\n            for k in keys\n        ]\n        self.data_size = data_size\n        self.exact_logp = exact_logp\n        self.t0 = 0.0\n        self.t1 = 0.5\n        self.dt0 = 0.05\n\n    # Runs backward-in-time to train the CNF.\n    def train(self, y, *, key):\n        if self.exact_logp:\n            term = diffrax.ODETerm(exact_logp_wrapper)\n        else:\n            term = diffrax.ODETerm(approx_logp_wrapper)\n        solver = diffrax.Tsit5()\n        eps = jr.normal(key, y.shape)\n        delta_log_likelihood = 0.0\n        for func in reversed(self.funcs):\n            y = (y, delta_log_likelihood)\n            sol = diffrax.diffeqsolve(\n                term, solver, self.t1, self.t0, -self.dt0, y, (eps, func)\n            )\n            (y,), (delta_log_likelihood,) = sol.ys\n        return delta_log_likelihood + normal_log_likelihood(y)\n\n    # Runs forward-in-time to draw samples from the CNF.\n    def sample(self, *, key):\n        y = jr.normal(key, (self.data_size,))\n        for func in self.funcs:\n            term = diffrax.ODETerm(func)\n            solver = diffrax.Tsit5()\n            sol = diffrax.diffeqsolve(term, solver, self.t0, self.t1, self.dt0, y)\n            (y,) = sol.ys\n        return y\n\n    # To make illustrations, we have a variant sample method we can query to see the\n    # evolution of the samples during the forward solve.\n    def sample_flow(self, *, key):\n        t_so_far = self.t0\n        t_end = self.t0 + (self.t1 - self.t0) * len(self.funcs)\n        save_times = jnp.linspace(self.t0, t_end, 6)\n        y = jr.normal(key, (self.data_size,))\n        out = []\n        for i, func in enumerate(self.funcs):\n            if i == len(self.funcs) - 1:\n                save_ts = save_times[t_so_far <= save_times] - t_so_far\n            else:\n                save_ts = (\n                    save_times[\n                        (t_so_far <= save_times)\n                        & (save_times < t_so_far + self.t1 - self.t0)\n                    ]\n                    - t_so_far\n                )\n                t_so_far = t_so_far + self.t1 - self.t0\n            term = diffrax.ODETerm(func)\n            solver = diffrax.Tsit5()\n            saveat = diffrax.SaveAt(ts=save_ts)\n            sol = diffrax.diffeqsolve(\n                term, solver, self.t0, self.t1, self.dt0, y, saveat=saveat\n            )\n            out.append(sol.ys)\n            y = sol.ys[-1]\n        out = jnp.concatenate(out)\n        assert len(out) == 6  # number of points we saved at\n        return out\n```\n\n----------------------------------------\n\nTITLE: Neural ODE Vector Field Definition\nDESCRIPTION: Implements the vector field function f_θ for the Neural ODE using an MLP (Multi-Layer Perceptron) through Equinox.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/neural_ode.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Func(eqx.Module):\n    mlp: eqx.nn.MLP\n\n    def __init__(self, data_size, width_size, depth, *, key, **kwargs):\n        super().__init__(**kwargs)\n        self.mlp = eqx.nn.MLP(\n            in_size=data_size,\n            out_size=data_size,\n            width_size=width_size,\n            depth=depth,\n            activation=jnn.softplus,\n            key=key,\n        )\n\n    def __call__(self, t, y, args):\n        return self.mlp(y)\n```\n\n----------------------------------------\n\nTITLE: Basic ODE Solver Example using Diffrax\nDESCRIPTION: Demonstrates solving a simple ordinary differential equation using the Dormand-Prince 5(4) solver (Dopri5). The example shows how to define an ODE term, initialize the solver, and solve the equation.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/README.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffrax import diffeqsolve, ODETerm, Dopri5\nimport jax.numpy as jnp\n\ndef f(t, y, args):\n    return -y\n\nterm = ODETerm(f)\nsolver = Dopri5()\ny0 = jnp.array([2., 3.])\nsolution = diffeqsolve(term, solver, t0=0, t1=1, dt0=0.1, y0=y0)\n```\n\n----------------------------------------\n\nTITLE: Neural SDE and CDE Classes Implementation\nDESCRIPTION: Implements the generator (Neural SDE) and discriminator (Neural CDE) classes with their respective forward passes and weight clipping functionality\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/neural_sde.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass NeuralSDE(eqx.Module):\n    initial: eqx.nn.MLP\n    vf: VectorField  # drift\n    cvf: ControlledVectorField  # diffusion\n    readout: eqx.nn.Linear\n    initial_noise_size: int\n    noise_size: int\n\n    def __init__(\n        self,\n        data_size,\n        initial_noise_size,\n        noise_size,\n        hidden_size,\n        width_size,\n        depth,\n        *,\n        key,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        initial_key, vf_key, cvf_key, readout_key = jr.split(key, 4)\n\n        self.initial = eqx.nn.MLP(\n            initial_noise_size, hidden_size, width_size, depth, key=initial_key\n        )\n        self.vf = VectorField(hidden_size, width_size, depth, scale=True, key=vf_key)\n        self.cvf = ControlledVectorField(\n            noise_size, hidden_size, width_size, depth, scale=True, key=cvf_key\n        )\n        self.readout = eqx.nn.Linear(hidden_size, data_size, key=readout_key)\n\n        self.initial_noise_size = initial_noise_size\n        self.noise_size = noise_size\n\n    def __call__(self, ts, *, key):\n        t0 = ts[0]\n        t1 = ts[-1]\n        # Very large dt0 for computational speed\n        dt0 = 1.0\n        init_key, bm_key = jr.split(key, 2)\n        init = jr.normal(init_key, (self.initial_noise_size,))\n        control = diffrax.VirtualBrownianTree(\n            t0=t0, t1=t1, tol=dt0 / 2, shape=(self.noise_size,), key=bm_key\n        )\n        vf = diffrax.ODETerm(self.vf)  # Drift term\n        cvf = diffrax.ControlTerm(self.cvf, control)  # Diffusion term\n        terms = diffrax.MultiTerm(vf, cvf)\n        # ReversibleHeun is a cheap choice of SDE solver. We could also use Euler etc.\n        solver = diffrax.ReversibleHeun()\n        y0 = self.initial(init)\n        saveat = diffrax.SaveAt(ts=ts)\n        sol = diffrax.diffeqsolve(terms, solver, t0, t1, dt0, y0, saveat=saveat)\n        return jax.vmap(self.readout)(sol.ys)\n\n\nclass NeuralCDE(eqx.Module):\n    initial: eqx.nn.MLP\n    vf: VectorField\n    cvf: ControlledVectorField\n    readout: eqx.nn.Linear\n\n    def __init__(self, data_size, hidden_size, width_size, depth, *, key, **kwargs):\n        super().__init__(**kwargs)\n        initial_key, vf_key, cvf_key, readout_key = jr.split(key, 4)\n\n        self.initial = eqx.nn.MLP(\n            data_size + 1, hidden_size, width_size, depth, key=initial_key\n        )\n        self.vf = VectorField(hidden_size, width_size, depth, scale=False, key=vf_key)\n        self.cvf = ControlledVectorField(\n            data_size, hidden_size, width_size, depth, scale=False, key=cvf_key\n        )\n        self.readout = eqx.nn.Linear(hidden_size, 1, key=readout_key)\n\n    def __call__(self, ts, ys):\n        # Interpolate data into a continuous path.\n        ys = diffrax.linear_interpolation(\n            ts, ys, replace_nans_at_start=0.0, fill_forward_nans_at_end=True\n        )\n        init = jnp.concatenate([ts[0, None], ys[0]])\n        control = diffrax.LinearInterpolation(ts, ys)\n        vf = diffrax.ODETerm(self.vf)\n        cvf = diffrax.ControlTerm(self.cvf, control)\n        terms = diffrax.MultiTerm(vf, cvf)\n        solver = diffrax.ReversibleHeun()\n        t0 = ts[0]\n        t1 = ts[-1]\n        dt0 = 1.0\n        y0 = self.initial(init)\n        saveat = diffrax.SaveAt(t0=True, t1=True)\n        sol = diffrax.diffeqsolve(terms, solver, t0, t1, dt0, y0, saveat=saveat)\n        return jax.vmap(self.readout)(sol.ys)\n\n    @eqx.filter_jit\n    def clip_weights(self):\n        leaves, treedef = jax.tree_util.tree_flatten(\n            self, is_leaf=lambda x: isinstance(x, eqx.nn.Linear)\n        )\n        new_leaves = []\n        for leaf in leaves:\n            if isinstance(leaf, eqx.nn.Linear):\n                lim = 1 / leaf.out_features\n                leaf = eqx.tree_at(\n                    lambda x: x.weight, leaf, leaf.weight.clip(-lim, lim)\n                )\n            new_leaves.append(leaf)\n        return jax.tree_util.tree_unflatten(treedef, new_leaves)\n```\n\n----------------------------------------\n\nTITLE: Solving Stochastic Differential Equations with Diffrax in Python\nDESCRIPTION: This code snippet shows how to solve an Itô SDE using Diffrax. It defines drift and diffusion terms, sets up a Brownian motion, and uses the Euler method to solve the equation dy = -y*dt + 0.1*t*dw(t) with initial condition y(0) = 1 over the interval [0, 3].\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/usage/getting-started.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport jax.random as jr\nfrom diffrax import diffeqsolve, ControlTerm, Euler, MultiTerm, ODETerm, SaveAt, VirtualBrownianTree\n\nt0, t1 = 0, 3\ndrift = lambda t, y, args: -y\ndiffusion = lambda t, y, args: 0.1 * t\nbrownian_motion = VirtualBrownianTree(t0, t1, tol=1e-3, shape=(), key=jr.PRNGKey(0))\nterms = MultiTerm(ODETerm(drift), ControlTerm(diffusion, brownian_motion))\nsolver = Euler()\nsaveat = SaveAt(dense=True)\n\nsol = diffeqsolve(terms, solver, t0, t1, dt0=0.05, y0=1.0, saveat=saveat)\nprint(sol.evaluate(1.1))  # DeviceArray(0.89436394)\n```\n\n----------------------------------------\n\nTITLE: Main Function for Kalman Filter Simulation and Optimization in Python\nDESCRIPTION: This function sets up the Kalman filter, simulates the system, and optionally performs gradient-based optimization of the filter parameters. It also includes plotting functionality for visualizing results.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/kalman_filter.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef main(\n    # evaluate at these timepoints\n    ts=jnp.arange(0, 5.0, 0.01),\n    # system that generates data\n    sys_true=harmonic_oscillator(0.3),\n    # initial state of our data generating system\n    sys_true_x0=jnp.array([1.0, 0.0]),\n    # standard deviation of measurement noise\n    sys_true_std_measurement_noise=1.0,\n    # our model for system `true`, it's not perfect\n    sys_model=harmonic_oscillator(0.7),\n    # initial state guess, it's not perfect\n    sys_model_x0=jnp.array([0.0, 0.0]),\n    # weighs how much we trust our model of the system\n    Q_root=jnp.diag(jnp.ones((2,))) * 0.1,\n    # weighs how much we trust in the measurements of the system\n    R_root=jnp.diag(jnp.ones((1,))),\n    # weighs how much we trust our initial guess\n    P0=jnp.diag(jnp.ones((2,))) * 10.0,\n    plot=True,\n    n_gradient_steps=0,\n    print_every=10,\n):\n    xs, ys = simulate_lti_system(\n        sys_true, sys_true_x0, ts, std_measurement_noise=sys_true_std_measurement_noise\n    )\n\n    kmf = KalmanFilter(sys_model, sys_model_x0, P0, Q_root, R_root)\n\n    initial_Q = kmf.Q_root.T @ kmf.Q_root\n    initial_R = kmf.R_root.T @ kmf.R_root\n    print(f\"Initial Q: \\n{initial_Q}\\n Initial R: \\n{initial_R}\")\n\n    # gradients should only be able to change Q/R parameters\n    # *not* the model (well at least not in this example :)\n    filter_spec = jtu.tree_map(lambda arr: False, kmf)\n    filter_spec = eqx.tree_at(\n        lambda tree: (tree.Q_root, tree.R_root), filter_spec, replace=(True, True)\n    )\n\n    opt = optax.adam(1e-2)\n    opt_state = opt.init(kmf)\n\n    @eqx.filter_value_and_grad\n    def loss_fn(dynamic_kmf, static_kmf, ts, ys, xs):\n        kmf = eqx.combine(dynamic_kmf, static_kmf)\n        xhats = kmf(ts, ys)\n        return jnp.mean((xs - xhats) ** 2)\n\n    @eqx.filter_jit\n    def make_step(kmf, opt_state, ts, ys, xs):\n        dynamic_kmf, static_kmf = eqx.partition(kmf, filter_spec)\n        value, grads = loss_fn(dynamic_kmf, static_kmf, ts, ys, xs)\n        updates, opt_state = opt.update(grads, opt_state)\n        kmf = eqx.apply_updates(kmf, updates)\n        return value, kmf, opt_state\n\n    for step in range(n_gradient_steps):\n        value, kmf, opt_state = make_step(kmf, opt_state, ts, ys, xs)\n        if step % print_every == 0:\n            print(\"Current MSE: \", value)\n\n    final_Q = kmf.Q_root.T @ kmf.Q_root\n    final_R = kmf.R_root.T @ kmf.R_root\n    print(f\"Final Q: \\n{final_Q}\\n Final R: \\n{final_R}\")\n\n    if plot:\n        xhats = kmf(ts, ys)\n        plt.plot(ts, xs[:, 0], label=\"true position\", color=\"orange\")\n        plt.plot(\n            ts,\n            xhats[:, 0],\n            label=\"estimated position\",\n            color=\"orange\",\n            linestyle=\"dashed\",\n        )\n        plt.plot(ts, xs[:, 1], label=\"true velocity\", color=\"blue\")\n        plt.plot(\n            ts,\n            xhats[:, 1],\n            label=\"estimated velocity\",\n            color=\"blue\",\n            linestyle=\"dashed\",\n        )\n        plt.xlabel(\"time\")\n        plt.ylabel(\"position / velocity\")\n        plt.grid()\n        plt.legend()\n        plt.title(\"Kalman-Filter optimization w.r.t Q/R\")\n```\n\n----------------------------------------\n\nTITLE: Spatial Discretization Implementation\nDESCRIPTION: Defines a SpatialDiscretisation class for handling the discretized spatial domain and implements the Laplacian operator for PDE calculations.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/nonlinear_heat_pde.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass SpatialDiscretisation(eqx.Module):\n    x0: float = eqx.field(static=True)\n    x_final: float = eqx.field(static=True)\n    vals: Float[Array, \"n\"]\n\n    @classmethod\n    def discretise_fn(cls, x0: float, x_final: float, n: int, fn: Callable):\n        if n < 2:\n            raise ValueError(\"Must discretise [x0, x_final] into at least two points\")\n        vals = jax.vmap(fn)(jnp.linspace(x0, x_final, n))\n        return cls(x0, x_final, vals)\n\n    @property\n    def δx(self):\n        return (self.x_final - self.x0) / (len(self.vals) - 1)\n\n    def binop(self, other, fn):\n        if isinstance(other, SpatialDiscretisation):\n            if self.x0 != other.x0 or self.x_final != other.x_final:\n                raise ValueError(\"Mismatched spatial discretisations\")\n            other = other.vals\n        return SpatialDiscretisation(self.x0, self.x_final, fn(self.vals, other))\n\n    def __add__(self, other):\n        return self.binop(other, lambda x, y: x + y)\n\n    def __mul__(self, other):\n        return self.binop(other, lambda x, y: x * y)\n\n    def __radd__(self, other):\n        return self.binop(other, lambda x, y: y + x)\n\n    def __rmul__(self, other):\n        return self.binop(other, lambda x, y: y * x)\n\n    def __sub__(self, other):\n        return self.binop(other, lambda x, y: x - y)\n\n    def __rsub__(self, other):\n        return self.binop(other, lambda x, y: y - x)\n\n\ndef laplacian(y: SpatialDiscretisation) -> SpatialDiscretisation:\n    y_next = jnp.roll(y.vals, shift=1)\n    y_prev = jnp.roll(y.vals, shift=-1)\n    Δy = (y_next - 2 * y.vals + y_prev) / (y.δx**2)\n    # Dirichlet boundary condition\n    Δy = Δy.at[0].set(0)\n    Δy = Δy.at[-1].set(0)\n    return SpatialDiscretisation(y.x0, y.x_final, Δy)\n```\n\n----------------------------------------\n\nTITLE: Defining Vector Field for Neural CDE\nDESCRIPTION: This class defines the vector field for the Controlled Differential Equation. It uses an MLP to compute the vector field, with a tanh activation in the final layer to prevent the model from blowing up.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/neural_cde.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Func(eqx.Module):\n    mlp: eqx.nn.MLP\n    data_size: int\n    hidden_size: int\n\n    def __init__(self, data_size, hidden_size, width_size, depth, *, key, **kwargs):\n        super().__init__(**kwargs)\n        self.data_size = data_size\n        self.hidden_size = hidden_size\n        self.mlp = eqx.nn.MLP(\n            in_size=hidden_size,\n            out_size=hidden_size * data_size,\n            width_size=width_size,\n            depth=depth,\n            activation=jnn.softplus,\n            # Note the use of a tanh final activation function. This is important to\n            # stop the model blowing up. (Just like how GRUs and LSTMs constrain the\n            # rate of change of their hidden states.)\n            final_activation=jnn.tanh,\n            key=key,\n        )\n\n    def __call__(self, t, y, args):\n        return self.mlp(y).reshape(self.hidden_size, self.data_size)\n```\n\n----------------------------------------\n\nTITLE: Using diffeqsolve function in Python\nDESCRIPTION: The diffeqsolve function is the main solver in the Diffrax library for solving differential equations. It provides access to various integration methods and configuration options.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/diffeqsolve.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n::: diffrax.diffeqsolve\n```\n\n----------------------------------------\n\nTITLE: Defining Vector Field for Continuous Normalising Flow\nDESCRIPTION: Implements the Func class, which defines the vector field on the right-hand side of the ODE used in the continuous normalising flow. It uses an MLP architecture with tanh activation and ConcatSquash layers.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/continuous_normalising_flow.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Func(eqx.Module):\n    layers: list[eqx.nn.Linear]\n\n    def __init__(self, *, data_size, width_size, depth, key, **kwargs):\n        super().__init__(**kwargs)\n        keys = jr.split(key, depth + 1)\n        layers = []\n        if depth == 0:\n            layers.append(\n                ConcatSquash(in_size=data_size, out_size=data_size, key=keys[0])\n            )\n        else:\n            layers.append(\n                ConcatSquash(in_size=data_size, out_size=width_size, key=keys[0])\n            )\n            for i in range(depth - 1):\n                layers.append(\n                    ConcatSquash(\n                        in_size=width_size, out_size=width_size, key=keys[i + 1]\n                    )\n                )\n            layers.append(\n                ConcatSquash(in_size=width_size, out_size=data_size, key=keys[-1])\n            )\n        self.layers = layers\n\n    def __call__(self, t, y, args):\n        t = jnp.asarray(t)[None]\n        for layer in self.layers[:-1]:\n            y = layer(t, y)\n            y = jnn.tanh(y)\n        y = self.layers[-1](t, y)\n        return y\n```\n\n----------------------------------------\n\nTITLE: Defining ConcatSquash Neural Network Layer in Python\nDESCRIPTION: Implements a custom neural network layer called ConcatSquash using equinox. This layer is used as part of the CNF model.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/continuous_normalising_flow.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nclass ConcatSquash(eqx.Module):\n    lin1: eqx.nn.Linear\n    lin2: eqx.nn.Linear\n    lin3: eqx.nn.Linear\n\n    def __init__(self, *, in_size, out_size, key, **kwargs):\n        super().__init__(**kwargs)\n        key1, key2, key3 = jr.split(key, 3)\n        self.lin1 = eqx.nn.Linear(in_size, out_size, key=key1)\n        self.lin2 = eqx.nn.Linear(1, out_size, key=key2)\n        self.lin3 = eqx.nn.Linear(1, out_size, use_bias=False, key=key3)\n\n    def __call__(self, t, y):\n        return self.lin1(y) * jnn.sigmoid(self.lin2(t)) + self.lin3(t)\n```\n\n----------------------------------------\n\nTITLE: Solving ODE with Interpolated Forcing Term and Gradient Computation\nDESCRIPTION: Implements a more complex ODE solution using cubic interpolation for the forcing term. Includes automatic differentiation with respect to the interpolation points using JAX's grad transform.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/forcing.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffrax import backward_hermite_coefficients, CubicInterpolation\n\n\ndef vector_field2(t, y, interp):\n    return -y + interp.evaluate(t)\n\n\n@jax.jit\n@jax.grad\ndef solve(points):\n    t0 = 0\n    t1 = 10\n    ts = jnp.linspace(t0, t1, len(points))\n    coeffs = backward_hermite_coefficients(ts, points)\n    interp = CubicInterpolation(ts, coeffs)\n    term = ODETerm(vector_field2)\n    solver = Tsit5()\n    dt0 = 0.1\n    y0 = 1.0\n    sol = diffeqsolve(term, solver, t0, t1, dt0, y0, args=interp)\n    (y1,) = sol.ys\n    return y1\n\n\npoints = jnp.array([3.0, 0.5, -0.8, 1.8])\ngrads = solve(points)\n```\n\n----------------------------------------\n\nTITLE: Main Solver Function Implementation\nDESCRIPTION: JIT-compiled main function that sets up and solves the Robertson system using Kvaerno5 implicit solver with PID controller for adaptive step sizes.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/stiff_ode.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\ndef main(k1, k2, k3):\n    robertson = Robertson(k1, k2, k3)\n    terms = diffrax.ODETerm(robertson)\n    t0 = 0.0\n    t1 = 100.0\n    y0 = jnp.array([1.0, 0.0, 0.0])\n    dt0 = 0.0002\n    solver = diffrax.Kvaerno5()\n    saveat = diffrax.SaveAt(ts=jnp.array([0.0, 1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2]))\n    stepsize_controller = diffrax.PIDController(rtol=1e-8, atol=1e-8)\n    sol = diffrax.diffeqsolve(\n        terms,\n        solver,\n        t0,\n        t1,\n        dt0,\n        y0,\n        saveat=saveat,\n        stepsize_controller=stepsize_controller,\n    )\n    return sol\n```\n\n----------------------------------------\n\nTITLE: Implementing Continuous-time Kalman Filter in Python\nDESCRIPTION: This class implements a continuous-time Kalman filter using Equinox, with methods for initialization and state estimation based on system dynamics and measurements.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/kalman_filter.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass KalmanFilter(eqx.Module):\n    \"\"\"Continuous-time Kalman Filter\n\n    Ref:\n        [1] Optimal and robust estimation. 2nd edition. Page 154.\n        https://lewisgroup.uta.edu/ee5322/lectures/CTKalmanFilterNew.pdf\n    \"\"\"\n\n    sys: LTISystem\n    x0: jnp.ndarray\n    P0: jnp.ndarray\n    Q_root: jnp.ndarray  # \"matrix roots\" to ensure that Q, R are positive definite\n    R_root: jnp.ndarray\n\n    def __call__(self, ts, ys, us: Optional[jnp.ndarray] = None):\n        A, B, C = self.sys.A, self.sys.B, self.sys.C\n\n        y_t = dfx.LinearInterpolation(ts=ts, ys=ys)\n        u_t = interpolate_us(ts, us, B)\n\n        y0 = (self.x0, self.P0)\n\n        def rhs(t, y, args):\n            x, P = y\n\n            # eq 3.22 of Ref [1]\n            R = self.R_root.T @ self.R_root\n            K = P @ C.transpose() @ jnp.linalg.inv(R)\n\n            # eq 3.21 of Ref [1]\n            Q = self.Q_root.T @ self.Q_root\n            dPdt = (\n                A @ P\n                + P @ A.transpose()\n                + Q\n                - P @ C.transpose() @ jnp.linalg.inv(R) @ C @ P\n            )\n\n            # eq 3.23 of Ref [1]\n            dxdt = A @ x + B @ u_t.evaluate(t) + K @ (y_t.evaluate(t) - C @ x)\n\n            return (dxdt, dPdt)\n\n        return diffeqsolve(rhs, ts, y0)[0]\n```\n\n----------------------------------------\n\nTITLE: Implementing Symbolic Regression with Neural ODEs\nDESCRIPTION: Defines the main function for performing symbolic regression. It trains a neural ODE, performs symbolic regression on the learned vector field, fine-tunes the resulting expressions, and quantizes the constants.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/symbolic_regression.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef main(\n    symbolic_dataset_size=2000,\n    symbolic_num_populations=100,\n    symbolic_population_size=20,\n    symbolic_migration_steps=4,\n    symbolic_mutation_steps=30,\n    symbolic_descent_steps=50,\n    pareto_coefficient=2,\n    fine_tuning_steps=500,\n    fine_tuning_lr=3e-3,\n    quantise_to=0.01,\n):\n    #\n    # First obtain a neural approximation to the dynamics.\n    # We begin by running the previous example.\n    #\n\n    # Runs the Neural ODE example.\n    # This defines the variables `ts`, `ys`, `model`.\n    print(\"Training neural differential equation.\")\n    %run neural_ode.ipynb\n\n    #\n    # Now symbolically regress across the learnt vector field, to obtain a Pareto\n    # frontier of symbolic equations, that trades loss against complexity of the\n    # equation. Select the \"best\" from this frontier.\n    #\n\n    print(\"Symbolically regressing across the vector field.\")\n    vector_field = model.func.mlp  # noqa: F821\n    dataset_size, length_size, data_size = ys.shape  # noqa: F821\n    in_ = ys.reshape(dataset_size * length_size, data_size)  # noqa: F821\n    in_ = in_[:symbolic_dataset_size]\n    out = jax.vmap(vector_field)(in_)\n    with tempfile.TemporaryDirectory() as tempdir:\n        symbolic_regressor = pysr.PySRRegressor(\n            niterations=symbolic_migration_steps,\n            ncycles_per_iteration=symbolic_mutation_steps,\n            populations=symbolic_num_populations,\n            population_size=symbolic_population_size,\n            optimizer_iterations=symbolic_descent_steps,\n            optimizer_nrestarts=1,\n            procs=1,\n            model_selection=\"score\",\n            progress=False,\n            tempdir=tempdir,\n            temp_equation_file=True,\n        )\n        symbolic_regressor.fit(in_, out)\n        best_expressions = [b.sympy_format for b in symbolic_regressor.get_best()]\n\n    #\n    # Now the constants in this expression have been optimised for regressing across\n    # the neural vector field. This was good enough to obtain the symbolic expression,\n    # but won't quite be perfect -- some of the constants will be slightly off.\n    #\n    # To fix this we now plug our symbolic function back into the original dataset\n    # and apply gradient descent.\n    #\n\n    print(\"\\nOptimising symbolic expression.\")\n\n    symbolic_fn = Stack([sympy2jax.SymbolicModule(expr) for expr in best_expressions])\n    symbolic_model = eqx.tree_at(lambda m: m.func.mlp, model, symbolic_fn)  # noqa: F821\n\n    @eqx.filter_grad\n    def grad_loss(symbolic_model):\n        vmap_model = jax.vmap(symbolic_model, in_axes=(None, 0))\n        pred_ys = vmap_model(ts, ys[:, 0])  # noqa: F821\n        return jnp.mean((ys - pred_ys) ** 2)  # noqa: F821\n\n    optim = optax.adam(fine_tuning_lr)\n    opt_state = optim.init(eqx.filter(symbolic_model, eqx.is_inexact_array))\n\n    @eqx.filter_jit\n    def make_step(symbolic_model, opt_state):\n        grads = grad_loss(symbolic_model)\n        updates, opt_state = optim.update(grads, opt_state)\n        symbolic_model = eqx.apply_updates(symbolic_model, updates)\n        return symbolic_model, opt_state\n\n    for _ in range(fine_tuning_steps):\n        symbolic_model, opt_state = make_step(symbolic_model, opt_state)\n\n    #\n    # Finally we round each constant to the nearest multiple of `quantise_to`.\n    #\n\n    trained_expressions = []\n    for symbolic_module in symbolic_model.func.mlp.modules:\n        expression = symbolic_module.sympy()\n        expression = quantise(expression, quantise_to)\n        trained_expressions.append(expression)\n\n    print(f\"Expressions found: {trained_expressions}\")\n```\n\n----------------------------------------\n\nTITLE: Custom Solvers Implementation - Python Inheritance\nDESCRIPTION: Custom solvers should inherit from diffrax.AbstractSolver. For Runge-Kutta methods, specialized base classes are available including AbstractERK, AbstractDIRK, AbstractSDIRK, and AbstractESDIRK. Additional abstract classes provide specific solver behaviors like AbstractImplicitSolver and AbstractAdaptiveSolver.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/usage/extending.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffrax import AbstractSolver, AbstractERK, AbstractDIRK, AbstractSDIRK, AbstractESDIRK\nfrom diffrax import AbstractImplicitSolver, AbstractAdaptiveSolver, AbstractItoSolver\nfrom diffrax import AbstractStratonovichSolver, AbstractWrappedSolver\n```\n\n----------------------------------------\n\nTITLE: GAN Training Functions Implementation\nDESCRIPTION: Implements core GAN training functions including loss calculation, gradient computation, and parameter updates with specialized handling for initial conditions of the Neural SDE.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/neural_sde.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@eqx.filter_jit\ndef loss(generator, discriminator, ts_i, ys_i, key, step=0):\n    batch_size, _ = ts_i.shape\n    key = jr.fold_in(key, step)\n    key = jr.split(key, batch_size)\n    fake_ys_i = jax.vmap(generator)(ts_i, key=key)\n    real_score = jax.vmap(discriminator)(ts_i, ys_i)\n    fake_score = jax.vmap(discriminator)(ts_i, fake_ys_i)\n    return jnp.mean(real_score - fake_score)\n\n@eqx.filter_grad\ndef grad_loss(g_d, ts_i, ys_i, key, step):\n    generator, discriminator = g_d\n    return loss(generator, discriminator, ts_i, ys_i, key, step)\n\ndef increase_update_initial(updates):\n    get_initial_leaves = lambda u: jax.tree_util.tree_leaves(u.initial)\n    return eqx.tree_at(get_initial_leaves, updates, replace_fn=lambda x: x * 10)\n\n@eqx.filter_jit\ndef make_step(\n    generator,\n    discriminator,\n    g_opt_state,\n    d_opt_state,\n    g_optim,\n    d_optim,\n    ts_i,\n    ys_i,\n    key,\n    step,\n):\n    g_grad, d_grad = grad_loss((generator, discriminator), ts_i, ys_i, key, step)\n    g_updates, g_opt_state = g_optim.update(g_grad, g_opt_state)\n    d_updates, d_opt_state = d_optim.update(d_grad, d_opt_state)\n    g_updates = increase_update_initial(g_updates)\n    d_updates = increase_update_initial(d_updates)\n    generator = eqx.apply_updates(generator, g_updates)\n    discriminator = eqx.apply_updates(discriminator, d_updates)\n    discriminator = discriminator.clip_weights()\n    return generator, discriminator, g_opt_state, d_opt_state\n```\n\n----------------------------------------\n\nTITLE: Custom Components Implementation - Python Classes\nDESCRIPTION: Implementation guidance for custom components including step size controllers (AbstractStepSizeController), Brownian motion simulations (AbstractBrownianPath), controls (AbstractPath), and terms (AbstractTerm). Each custom component should inherit from the appropriate abstract base class.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/usage/extending.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffrax import AbstractStepSizeController, AbstractBrownianPath, AbstractPath, AbstractTerm\nfrom diffrax import AbstractAdaptiveStepSizeController, CubicInterpolation, ControlTerm\n```\n\n----------------------------------------\n\nTITLE: Main Training and Visualization Loop for Latent ODE\nDESCRIPTION: This function is the main entry point for training the Latent ODE model. It sets up the model, optimizer, and training loop, and also handles visualization of the generated samples during training.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/latent_ode.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef main(\n    dataset_size=10000,\n    batch_size=256,\n    lr=1e-2,\n    steps=250,\n    save_every=50,\n    hidden_size=16,\n    latent_size=16,\n    width_size=16,\n    depth=2,\n    seed=5678,\n):\n    key = jr.PRNGKey(seed)\n    data_key, model_key, loader_key, train_key, sample_key = jr.split(key, 5)\n\n    ts, ys = get_data(dataset_size, key=data_key)\n\n    model = LatentODE(\n        data_size=ys.shape[-1],\n        hidden_size=hidden_size,\n        latent_size=latent_size,\n        width_size=width_size,\n        depth=depth,\n        key=model_key,\n    )\n\n    @eqx.filter_value_and_grad\n    def loss(model, ts_i, ys_i, key_i):\n        batch_size, _ = ts_i.shape\n        key_i = jr.split(key_i, batch_size)\n        loss = jax.vmap(model.train)(ts_i, ys_i, key=key_i)\n        return jnp.mean(loss)\n\n    @eqx.filter_jit\n    def make_step(model, opt_state, ts_i, ys_i, key_i):\n        value, grads = loss(model, ts_i, ys_i, key_i)\n        key_i = jr.split(key_i, 1)[0]\n        updates, opt_state = optim.update(grads, opt_state)\n        model = eqx.apply_updates(model, updates)\n        return value, model, opt_state, key_i\n\n    optim = optax.adam(lr)\n    opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n\n    # Plot results\n    num_plots = 1 + (steps - 1) // save_every\n    if ((steps - 1) % save_every) != 0:\n        num_plots += 1\n    fig, axs = plt.subplots(1, num_plots, figsize=(num_plots * 8, 8))\n    axs[0].set_ylabel(\"x\")\n    axs = iter(axs)\n    for step, (ts_i, ys_i) in zip(\n        range(steps), dataloader((ts, ys), batch_size, key=loader_key)\n    ):\n        start = time.time()\n        value, model, opt_state, train_key = make_step(\n            model, opt_state, ts_i, ys_i, train_key\n        )\n        end = time.time()\n        print(f\"Step: {step}, Loss: {value}, Computation time: {end - start}\")\n\n        if (step % save_every) == 0 or step == steps - 1:\n            ax = next(axs)\n            # Sample over a longer time interval than we trained on. The model will be\n            # sufficiently good that it will correctly extrapolate!\n            sample_t = jnp.linspace(0, 12, 300)\n            sample_y = model.sample(sample_t, key=sample_key)\n            sample_t = np.asarray(sample_t)\n            sample_y = np.asarray(sample_y)\n            ax.plot(sample_t, sample_y[:, 0])\n            ax.plot(sample_t, sample_y[:, 1])\n            ax.set_xticks([])\n```\n\n----------------------------------------\n\nTITLE: Defining AbstractPath Class in Python for Diffrax\nDESCRIPTION: The AbstractPath class represents piecewise continuous functions in differential equations. It includes properties for start and end times (t0 and t1) and methods for evaluation and derivative calculation.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/path.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass AbstractPath:\n    t0: float\n    t1: float\n    \n    def evaluate(self, t, args):\n        ...\n    \n    def derivative(self, t, args):\n        ...\n```\n\n----------------------------------------\n\nTITLE: Defining MultiTerm for SDE in Python using Diffrax\nDESCRIPTION: This snippet demonstrates how to create a MultiTerm object for a stochastic differential equation (SDE) using Diffrax's ODETerm and ControlTerm.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/terms.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nMultiTerm(ODETerm(...), ControlTerm(...))\n```\n\n----------------------------------------\n\nTITLE: Implementing Loss Function for Steady State Optimization\nDESCRIPTION: This function defines the loss for optimizing the steady state. It uses Diffrax to solve the ODE until a steady state is reached, employing event handling and implicit adjoint method for efficient backpropagation.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/steady_state.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef loss(model, target_steady_state):\n    term = diffrax.ODETerm(model)\n    solver = diffrax.Tsit5()\n    t0 = 0\n    t1 = jnp.inf\n    dt0 = None\n    y0 = 1.0\n    max_steps = None\n    controller = diffrax.PIDController(rtol=1e-3, atol=1e-6)\n    cond_fn = diffrax.steady_state_event()\n    event = diffrax.Event(cond_fn)\n    adjoint = diffrax.ImplicitAdjoint()\n    # This combination of event, t1, max_steps, adjoint is particularly\n    # natural: we keep integration forever until we hit the event, with\n    # no maximum time or number of steps. Backpropagation happens via\n    # the implicit function theorem.\n    sol = diffrax.diffeqsolve(\n        term,\n        solver,\n        t0,\n        t1,\n        dt0,\n        y0,\n        max_steps=max_steps,\n        stepsize_controller=controller,\n        event=event,\n        adjoint=adjoint,\n    )\n    (y1,) = sol.ys\n    return (y1 - target_steady_state) ** 2\n```\n\n----------------------------------------\n\nTITLE: Simulating LTI Systems with Diffrax in Python\nDESCRIPTION: These utility functions allow for simulating LTI systems using the Diffrax library, including input interpolation and differential equation solving.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/kalman_filter.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef interpolate_us(ts, us, B):\n    if us is None:\n        m = B.shape[-1]\n        u_t = SimpleNamespace(evaluate=lambda t: jnp.zeros((m,)))\n    else:\n        u_t = dfx.LinearInterpolation(ts=ts, ys=us)\n    return u_t\n\n\ndef diffeqsolve(\n    rhs,\n    ts: jnp.ndarray,\n    y0: jnp.ndarray,\n    solver: dfx.AbstractSolver = dfx.Dopri5(),\n    stepsize_controller: dfx.AbstractStepSizeController = dfx.ConstantStepSize(),\n    dt0: float = 0.01,\n) -> jnp.ndarray:\n    return dfx.diffeqsolve(\n        dfx.ODETerm(rhs),\n        solver=solver,\n        stepsize_controller=stepsize_controller,\n        t0=ts[0],\n        t1=ts[-1],\n        y0=y0,\n        dt0=dt0,\n        saveat=dfx.SaveAt(ts=ts),\n    ).ys\n\n\ndef simulate_lti_system(\n    sys: LTISystem,\n    y0: jnp.ndarray,\n    ts: jnp.ndarray,\n    us: Optional[jnp.ndarray] = None,\n    std_measurement_noise: float = 0.0,\n    key=jr.PRNGKey(\n        1,\n    ),\n):\n    u_t = interpolate_us(ts, us, sys.B)\n\n    def rhs(t, y, args):\n        return sys.A @ y + sys.B @ u_t.evaluate(t)\n\n    xs = diffeqsolve(rhs, ts, y0)\n    # noisy measurements\n    ys = xs @ sys.C.transpose()\n    ys = ys + jr.normal(key, shape=ys.shape) * std_measurement_noise\n    return xs, ys\n```\n\n----------------------------------------\n\nTITLE: Creating MultiTerm for Underdamped Langevin Diffusion in Python using Diffrax\nDESCRIPTION: This example demonstrates how to create a MultiTerm object for the Underdamped Langevin diffusion (ULD) using Diffrax's specialized terms.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/terms.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nMultiTerm(UnderdampedLangevinDriftTerm(gamma, u, grad_f), UnderdampedLangevinDiffusionTerm(gamma, u, bm))\n```\n\n----------------------------------------\n\nTITLE: Custom Crank-Nicolson Solver Implementation\nDESCRIPTION: Implements a custom Crank-Nicolson solver class for solving the PDE using an implicit scheme with fixed-point iteration.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/nonlinear_heat_pde.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass CrankNicolson(diffrax.AbstractSolver):\n    rtol: float\n    atol: float\n\n    term_structure = diffrax.ODETerm\n    interpolation_cls = diffrax.LocalLinearInterpolation\n\n    def order(self, terms):\n        return 2\n\n    def init(self, terms, t0, t1, y0, args):\n        return None\n\n    def step(self, terms, t0, t1, y0, args, solver_state, made_jump):\n        del solver_state, made_jump\n        δt = t1 - t0\n        f0 = terms.vf(t0, y0, args)\n\n        def keep_iterating(val):\n            _, not_converged = val\n            return not_converged\n\n        def fixed_point_iteration(val):\n            y1, _ = val\n            new_y1 = y0 + 0.5 * δt * (f0 + terms.vf(t1, y1, args))\n            diff = jnp.abs((new_y1 - y1).vals)\n            max_y1 = jnp.maximum(jnp.abs(y1.vals), jnp.abs(new_y1.vals))\n            scale = self.atol + self.rtol * max_y1\n            not_converged = jnp.any(diff > scale)\n            return new_y1, not_converged\n\n        euler_y1 = y0 + δt * f0\n        y1, _ = lax.while_loop(keep_iterating, fixed_point_iteration, (euler_y1, False))\n\n        y_error = y1 - euler_y1\n        dense_info = dict(y0=y0, y1=y1)\n\n        solver_state = None\n        result = diffrax.RESULTS.successful\n        return y1, y_error, dense_info, solver_state, result\n\n    def func(self, terms, t0, y0, args):\n        return terms.vf(t0, y0, args)\n```\n\n----------------------------------------\n\nTITLE: Vector Field Classes Implementation\nDESCRIPTION: Defines VectorField and ControlledVectorField classes for SDE and CDE implementations with MLP-based neural networks\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/neural_sde.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass VectorField(eqx.Module):\n    scale: Union[int, jnp.ndarray]\n    mlp: eqx.nn.MLP\n\n    def __init__(self, hidden_size, width_size, depth, scale, *, key, **kwargs):\n        super().__init__(**kwargs)\n        scale_key, mlp_key = jr.split(key)\n        if scale:\n            self.scale = jr.uniform(scale_key, (hidden_size,), minval=0.9, maxval=1.1)\n        else:\n            self.scale = 1\n        self.mlp = eqx.nn.MLP(\n            in_size=hidden_size + 1,\n            out_size=hidden_size,\n            width_size=width_size,\n            depth=depth,\n            activation=lipswish,\n            final_activation=jnn.tanh,\n            key=mlp_key,\n        )\n\n    def __call__(self, t, y, args):\n        t = jnp.asarray(t)\n        return self.scale * self.mlp(jnp.concatenate([t[None], y]))\n\n\nclass ControlledVectorField(eqx.Module):\n    scale: Union[int, jnp.ndarray]\n    mlp: eqx.nn.MLP\n    control_size: int\n    hidden_size: int\n\n    def __init__(\n        self, control_size, hidden_size, width_size, depth, scale, *, key, **kwargs\n    ):\n        super().__init__(**kwargs)\n        scale_key, mlp_key = jr.split(key)\n        if scale:\n            self.scale = jr.uniform(\n                scale_key, (hidden_size, control_size), minval=0.9, maxval=1.1\n            )\n        else:\n            self.scale = 1\n        self.mlp = eqx.nn.MLP(\n            in_size=hidden_size + 1,\n            out_size=hidden_size * control_size,\n            width_size=width_size,\n            depth=depth,\n            activation=lipswish,\n            final_activation=jnn.tanh,\n            key=mlp_key,\n        )\n        self.control_size = control_size\n        self.hidden_size = hidden_size\n\n    def __call__(self, t, y, args):\n        t = jnp.asarray(t)\n        return self.scale * self.mlp(jnp.concatenate([t[None], y])).reshape(\n            self.hidden_size, self.control_size\n        )\n```\n\n----------------------------------------\n\nTITLE: Converting CDE to ODE in Diffrax\nDESCRIPTION: This code snippet demonstrates how to convert a controlled differential equation (CDE) to an ordinary differential equation (ODE) using Diffrax. It sets up a vector field and control, creates a ControlTerm, and then converts it to an ODE term.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/usage/how-to-choose-a-solver.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nvector_field = ...\ncontrol = ...\nterm = ControlTerm(vector_field, control)\nterm = term.to_ode()\n```\n\n----------------------------------------\n\nTITLE: Implementing Log-Likelihood Wrappers for CNF in Python\nDESCRIPTION: Defines two wrapper functions for computing the change in log-density during CNF training. One uses Hutchinson's trace estimator (approximate), and the other computes the exact divergence.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/continuous_normalising_flow.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndef approx_logp_wrapper(t, y, args):\n    y, _ = y\n    *args, eps, func = args\n    fn = lambda y: func(t, y, args)\n    f, vjp_fn = jax.vjp(fn, y)\n    (eps_dfdy,) = vjp_fn(eps)\n    logp = jnp.sum(eps_dfdy * eps)\n    return f, logp\n\n\ndef exact_logp_wrapper(t, y, args):\n    y, _ = y\n    *args, _, func = args\n    fn = lambda y: func(t, y, args)\n    f, vjp_fn = jax.vjp(fn, y)\n    (size,) = y.shape  # this implementation only works for 1D input\n    eye = jnp.eye(size)\n    (dfdy,) = jax.vmap(vjp_fn)(eye)\n    logp = jnp.trace(dfdy)\n    return f, logp\n```\n\n----------------------------------------\n\nTITLE: Defining Solution Class in Python for Diffrax\nDESCRIPTION: This code snippet defines the Solution class in the Diffrax library. It lists the class attributes and methods, which are used to store and manipulate the results of solving a differential equation.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/solution.md#2025-04-21_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nclass Solution:\n    t0\n    t1\n    ts\n    ys\n    solver_state\n    controller_state\n    made_jump\n    stats\n    result\n    message\n    evaluate\n    derivative\n```\n\n----------------------------------------\n\nTITLE: Data Loader Implementation\nDESCRIPTION: Implements a data loader for batch processing during training with random permutation of indices.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/neural_ode.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef dataloader(arrays, batch_size, *, key):\n    dataset_size = arrays[0].shape[0]\n    assert all(array.shape[0] == dataset_size for array in arrays)\n    indices = jnp.arange(dataset_size)\n    while True:\n        perm = jr.permutation(key, indices)\n        (key,) = jr.split(key, 1)\n        start = 0\n        end = batch_size\n        while end < dataset_size:\n            batch_perm = perm[start:end]\n            yield tuple(array[batch_perm] for array in arrays)\n            start = end\n            end = start + batch_size\n```\n\n----------------------------------------\n\nTITLE: Configuring Newton Root Finder with Kvaerno5 Solver in Diffrax\nDESCRIPTION: Demonstrates how to set up a Newton root finder from Optimistix with specific tolerances and use it with Diffrax's Kvaerno5 solver. Shows the integration between Diffrax and Optimistix libraries for differential equation solving.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/nonlinear_solver.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport diffrax as dfx\nimport optimistix as optx\n\nroot_finder = optx.Newton(rtol=1e-8, atol=1e-8)\nsolver = dfx.Kvaerno5(root_finder=root_finder)\ndfx.diffeqsolve(..., solver, ...)\n```\n\n----------------------------------------\n\nTITLE: Implementing Dataloader for Neural CDE Training\nDESCRIPTION: This function creates a dataloader that yields batches of data for training the Neural CDE. It shuffles the data and provides batches in a cyclic manner.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/neural_cde.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef dataloader(arrays, batch_size, *, key):\n    dataset_size = arrays[0].shape[0]\n    assert all(array.shape[0] == dataset_size for array in arrays)\n    indices = jnp.arange(dataset_size)\n    while True:\n        perm = jr.permutation(key, indices)\n        (key,) = jr.split(key, 1)\n        start = 0\n        end = batch_size\n        while end < dataset_size:\n            batch_perm = perm[start:end]\n            yield tuple(array[batch_perm] for array in arrays)\n            start = end\n            end = start + batch_size\n```\n\n----------------------------------------\n\nTITLE: Initializing UnderdampedLangevinDriftTerm in Python using Diffrax\nDESCRIPTION: This code snippet demonstrates the initialization of an UnderdampedLangevinDriftTerm, which is part of the Underdamped Langevin diffusion (ULD) implementation in Diffrax.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/terms.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nUnderdampedLangevinDriftTerm(gamma, u, grad_f)\n```\n\n----------------------------------------\n\nTITLE: Creating In-Memory DataLoader for CNF in Python\nDESCRIPTION: Implements a custom DataLoader class for efficient in-memory data loading during CNF training. Designed to work within a JIT-compiled training loop.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/continuous_normalising_flow.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nclass DataLoader(eqx.Module):\n    arrays: tuple[jnp.ndarray, ...]\n    batch_size: int\n    key: jr.PRNGKey\n\n    def __check_init__(self):\n        dataset_size = self.arrays[0].shape[0]\n        assert all(array.shape[0] == dataset_size for array in self.arrays)\n\n    def __call__(self, step):\n        dataset_size = self.arrays[0].shape[0]\n        num_batches = dataset_size // self.batch_size\n        epoch = step // num_batches\n        key = jr.fold_in(self.key, epoch)\n        perm = jr.permutation(key, jnp.arange(dataset_size))\n        start = (step % num_batches) * self.batch_size\n        slice_size = self.batch_size\n        batch_indices = lax.dynamic_slice_in_dim(perm, start, slice_size)\n        return tuple(array[batch_indices] for array in self.arrays)\n```\n\n----------------------------------------\n\nTITLE: IMEX ODE Term Structure Example\nDESCRIPTION: Mathematical representation of IMEX (Implicit-Explicit) method formulation for ODEs. Shows the differential equation in the form dy/dt = f(t,y(t)) + g(t,y(t)) where f is non-stiff and g is stiff.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/solvers/ode_solvers.md#2025-04-21_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n\\frac{\\mathrm{d}y}{\\mathrm{d}t} = f(t, y(t)) + g(t, y(t))\n```\n\n----------------------------------------\n\nTITLE: Data Generation and Processing with Diffrax\nDESCRIPTION: Generates irregularly sampled time series data using Diffrax's differential equation solver with Euler integration. Includes random data dropping to create irregular sampling patterns.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/neural_sde.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nsolver = diffrax.Euler()\ndt0 = 0.1\ny0 = jr.uniform(y0_key, (1,), minval=-1, maxval=1)\nts = jnp.linspace(t0, t1, t_size)\nsaveat = diffrax.SaveAt(ts=ts)\nsol = diffrax.diffeqsolve(\n    terms, solver, t0, t1, dt0, y0, saveat=saveat, adjoint=diffrax.DirectAdjoint()\n)\n\n# Make the data irregularly sampled\nto_drop = jr.bernoulli(drop_key, 0.3, (t_size, 1))\nys = jnp.where(to_drop, jnp.nan, sol.ys)\n\nreturn ts, ys\n```\n\n----------------------------------------\n\nTITLE: Sampling and Flow Visualization with JAX and Matplotlib\nDESCRIPTION: This code generates samples from a trained model, computes sample flows, and visualizes the results using matplotlib. It creates scatter plots of samples and heatmaps of flow densities.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/continuous_normalising_flow.ipynb#2025-04-21_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n    num_samples = 5000\n    sample_key = jr.split(sample_key, num_samples)\n    samples = jax.vmap(model.sample)(key=sample_key)\n    sample_flows = jax.vmap(model.sample_flow, out_axes=-1)(key=sample_key)\n    fig, (*axs, ax, axtrue) = plt.subplots(\n        1,\n        2 + len(sample_flows),\n        figsize=((2 + len(sample_flows)) * 10 * height / width, 10),\n    )\n\n    samples = samples * std + mean\n    x = samples[:, 0]\n    y = samples[:, 1]\n    ax.scatter(x, y, c=\"black\", s=2)\n    ax.set_xlim(-0.5, width - 0.5)\n    ax.set_ylim(-0.5, height - 0.5)\n    ax.set_aspect(height / width)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    axtrue.imshow(img.T, origin=\"lower\", cmap=\"gray\")\n    axtrue.set_aspect(height / width)\n    axtrue.set_xticks([])\n    axtrue.set_yticks([])\n\n    x_resolution = 100\n    y_resolution = int(x_resolution * (height / width))\n    sample_flows = sample_flows * std[:, None] + mean[:, None]\n    x_pos, y_pos = jnp.broadcast_arrays(\n        jnp.linspace(-1, width + 1, x_resolution)[:, None],\n        jnp.linspace(-1, height + 1, y_resolution)[None, :],\n    )\n    positions = jnp.stack([jnp.ravel(x_pos), jnp.ravel(y_pos)])\n    densities = [stats.gaussian_kde(samples)(positions) for samples in sample_flows]\n    for i, (ax, density) in enumerate(zip(axs, densities)):\n        density = jnp.reshape(density, (x_resolution, y_resolution))\n        ax.imshow(density.T, origin=\"lower\", cmap=\"plasma\")\n        ax.set_aspect(height / width)\n        ax.set_xticks([])\n        ax.set_yticks([])\n    plt.savefig(out_path)\n    plt.show()\n```\n\n----------------------------------------\n\nTITLE: Initializing UnderdampedLangevinDiffusionTerm in Python using Diffrax\nDESCRIPTION: This code snippet shows the initialization of an UnderdampedLangevinDiffusionTerm, which is used in conjunction with the drift term for ULD in Diffrax.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/terms.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nUnderdampedLangevinDiffusionTerm(gamma, u, bm)\n```\n\n----------------------------------------\n\nTITLE: Generating Toy Dataset of Decaying Oscillators\nDESCRIPTION: This function generates a toy dataset of decaying oscillators using Diffrax to solve a differential equation. It creates irregularly sampled time series data for training the Latent ODE model.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/latent_ode.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef get_data(dataset_size, *, key):\n    ykey, tkey1, tkey2 = jr.split(key, 3)\n\n    y0 = jr.normal(ykey, (dataset_size, 2))\n\n    t0 = 0\n    t1 = 2 + jr.uniform(tkey1, (dataset_size,))\n    ts = jr.uniform(tkey2, (dataset_size, 20)) * (t1[:, None] - t0) + t0\n    ts = jnp.sort(ts)\n    dt0 = 0.1\n\n    def func(t, y, args):\n        return jnp.array([[-0.1, 1.3], [-1, -0.1]]) @ y\n\n    def solve(ts, y0):\n        sol = diffrax.diffeqsolve(\n            diffrax.ODETerm(func),\n            diffrax.Tsit5(),\n            ts[0],\n            ts[-1],\n            dt0,\n            y0,\n            saveat=diffrax.SaveAt(ts=ts),\n        )\n        return sol.ys\n\n    ys = jax.vmap(solve)(ts, y0)\n\n    return ts, ys\n```\n\n----------------------------------------\n\nTITLE: Creating PyTree Structure for Hamiltonian System in Python using Diffrax\nDESCRIPTION: This example shows how to structure terms for a Hamiltonian system using a tuple of ODETerm objects in Diffrax.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/terms.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n(ODETerm(...), ODETerm(...))\n```\n\n----------------------------------------\n\nTITLE: Configuring Diffrax Solver Equivalent to jax.experimental.ode.odeint\nDESCRIPTION: This snippet demonstrates how to configure Diffrax to be equivalent to jax.experimental.ode.odeint. It sets up the solver with specific parameters for step size control, adjoint method, and other solver options.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/further_details/faq.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndiffeqsolve(\n    ...,\n    dt0=None,\n    solver=Dopri5(),\n    stepsize_controller=PIDController(rtol=1.4e-8, atol=1.4e-8),\n    adjoint=BacksolveAdjoint(),\n    max_steps=None,\n)\n```\n\n----------------------------------------\n\nTITLE: Generating Spiral Dataset for Neural CDE\nDESCRIPTION: This function generates a toy dataset of spirals for training and testing the Neural CDE. It creates clockwise and counter-clockwise spirals, adds noise optionally, and prepares the data in the format required by the model.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/neural_cde.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef get_data(dataset_size, add_noise, *, key):\n    theta_key, noise_key = jr.split(key, 2)\n    length = 100\n    theta = jr.uniform(theta_key, (dataset_size,), minval=0, maxval=2 * math.pi)\n    y0 = jnp.stack([jnp.cos(theta), jnp.sin(theta)], axis=-1)\n    ts = jnp.broadcast_to(jnp.linspace(0, 4 * math.pi, length), (dataset_size, length))\n    matrix = jnp.array([[-0.3, 2], [-2, -0.3]])\n    ys = jax.vmap(\n        lambda y0i, ti: jax.vmap(lambda tij: jsp.linalg.expm(tij * matrix) @ y0i)(ti)\n    )(y0, ts)\n    ys = jnp.concatenate([ts[:, :, None], ys], axis=-1)  # time is a channel\n    ys = ys.at[: dataset_size // 2, :, 1].multiply(-1)\n    if add_noise:\n        ys = ys + jr.normal(noise_key, ys.shape) * 0.1\n    coeffs = jax.vmap(diffrax.backward_hermite_coefficients)(ts, ys)\n    labels = jnp.zeros((dataset_size,))\n    labels = labels.at[: dataset_size // 2].set(1.0)\n    _, _, data_size = ys.shape\n    return ts, coeffs, labels, data_size\n```\n\n----------------------------------------\n\nTITLE: RecursiveCheckpointAdjoint Class Definition\nDESCRIPTION: Implementation of the discretise-then-optimise approach for computing gradients through differential equations. This is the default adjoint method in Diffrax that directly applies autodifferentiation through the solver internals.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/adjoints.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n::: diffrax.RecursiveCheckpointAdjoint\n    selection:\n        members:\n            - __init__\n```\n\n----------------------------------------\n\nTITLE: Running Kalman Filter Simulation with Optimization in Python\nDESCRIPTION: This code snippet runs the main function with 100 gradient steps, optimizing the Kalman filter parameters to improve performance.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/kalman_filter.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmain(n_gradient_steps=100)\n```\n\n----------------------------------------\n\nTITLE: Implementing Data Processing for CNF in Python\nDESCRIPTION: Defines a function to process input images and prepare data for the CNF model. Converts images to grayscale and normalizes the data.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/continuous_normalising_flow.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ndef get_data(path):\n    # integer array of shape (height, width, channels) with values in {0, ..., 255}\n    img = jnp.asarray(imageio.imread(path))\n    if img.shape[-1] == 4:\n        img = img[..., :-1]  # ignore alpha channel\n    height, width, channels = img.shape\n    assert channels == 3\n    # Convert to greyscale for simplicity.\n    img = img @ jnp.array([0.2989, 0.5870, 0.1140])\n    img = jnp.transpose(img)[:, ::-1]  # (width, height)\n    x = jnp.arange(width, dtype=jnp.float32)\n    y = jnp.arange(height, dtype=jnp.float32)\n    x, y = jnp.broadcast_arrays(x[:, None], y[None, :])\n    weights = 1 - img.reshape(-1).astype(jnp.float32) / jnp.max(img)\n    dataset = jnp.stack(\n        [x.reshape(-1), y.reshape(-1)], axis=-1\n    )  # shape (dataset_size, 2)\n    # For efficiency we don't bother with the particles that will have weight zero.\n    cond = img.reshape(-1) < 254\n    dataset = dataset[cond]\n    weights = weights[cond]\n    mean = jnp.mean(dataset, axis=0)\n    std = jnp.std(dataset, axis=0) + 1e-6\n    dataset = (dataset - mean) / std\n\n    return dataset, weights, mean, std, img, width, height\n```\n\n----------------------------------------\n\nTITLE: Neural ODE Training and Visualization\nDESCRIPTION: Main training loop implementation with gradual training strategy and result visualization.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/neural_ode.ipynb#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef main(\n    dataset_size=256,\n    batch_size=32,\n    lr_strategy=(3e-3, 3e-3),\n    steps_strategy=(500, 500),\n    length_strategy=(0.1, 1),\n    width_size=64,\n    depth=2,\n    seed=5678,\n    plot=True,\n    print_every=100,\n):\n    key = jr.PRNGKey(seed)\n    data_key, model_key, loader_key = jr.split(key, 3)\n\n    ts, ys = get_data(dataset_size, key=data_key)\n    _, length_size, data_size = ys.shape\n\n    model = NeuralODE(data_size, width_size, depth, key=model_key)\n\n    @eqx.filter_value_and_grad\n    def grad_loss(model, ti, yi):\n        y_pred = jax.vmap(model, in_axes=(None, 0))(ti, yi[:, 0])\n        return jnp.mean((yi - y_pred) ** 2)\n\n    @eqx.filter_jit\n    def make_step(ti, yi, model, opt_state):\n        loss, grads = grad_loss(model, ti, yi)\n        updates, opt_state = optim.update(grads, opt_state)\n        model = eqx.apply_updates(model, updates)\n        return loss, model, opt_state\n\n    for lr, steps, length in zip(lr_strategy, steps_strategy, length_strategy):\n        optim = optax.adabelief(lr)\n        opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n        _ts = ts[: int(length_size * length)]\n        _ys = ys[:, : int(length_size * length)]\n        for step, (yi,) in zip(\n            range(steps), dataloader((_ys,), batch_size, key=loader_key)\n        ):\n            start = time.time()\n            loss, model, opt_state = make_step(_ts, yi, model, opt_state)\n            end = time.time()\n            if (step % print_every) == 0 or step == steps - 1:\n                print(f\"Step: {step}, Loss: {loss}, Computation time: {end - start}\")\n\n    if plot:\n        plt.plot(ts, ys[0, :, 0], c=\"dodgerblue\", label=\"Real\")\n        plt.plot(ts, ys[0, :, 1], c=\"dodgerblue\")\n        model_y = model(ts, ys[0, 0])\n        plt.plot(ts, model_y[:, 0], c=\"crimson\", label=\"Model\")\n        plt.plot(ts, model_y[:, 1], c=\"crimson\")\n        plt.legend()\n        plt.tight_layout()\n        plt.savefig(\"neural_ode.png\")\n        plt.show()\n\n    return ts, ys, model\n```\n\n----------------------------------------\n\nTITLE: ImplicitAdjoint Class Definition\nDESCRIPTION: Implementation of implicit adjoint method for gradient computation in differential equations\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/adjoints.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n::: diffrax.ImplicitAdjoint\n    selection:\n        members:\n            - __init__\n```\n\n----------------------------------------\n\nTITLE: Training Data Generation for Neural ODE\nDESCRIPTION: Functions to generate synthetic training data using nonlinear oscillators as a toy dataset.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/neural_ode.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef _get_data(ts, *, key):\n    y0 = jr.uniform(key, (2,), minval=-0.6, maxval=1)\n\n    def f(t, y, args):\n        x = y / (1 + y)\n        return jnp.stack([x[1], -x[0]], axis=-1)\n\n    solver = diffrax.Tsit5()\n    dt0 = 0.1\n    saveat = diffrax.SaveAt(ts=ts)\n    sol = diffrax.diffeqsolve(\n        diffrax.ODETerm(f), solver, ts[0], ts[-1], dt0, y0, saveat=saveat\n    )\n    ys = sol.ys\n    return ys\n\n\ndef get_data(dataset_size, *, key):\n    ts = jnp.linspace(0, 10, 100)\n    key = jr.split(key, dataset_size)\n    ys = jax.vmap(lambda key: _get_data(ts, key=key))(key)\n    return ts, ys\n```\n\n----------------------------------------\n\nTITLE: Installing Diffrax via pip\nDESCRIPTION: Command to install the Diffrax library using pip package manager. Requires Python 3.10+.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install diffrax\n```\n\n----------------------------------------\n\nTITLE: Robertson Problem Class Definition\nDESCRIPTION: Defines the Robertson system of ODEs as an Equinox Module with three rate constants (k1, k2, k3). Implements the system's dynamics through the call method.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/stiff_ode.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass Robertson(eqx.Module):\n    k1: float\n    k2: float\n    k3: float\n\n    def __call__(self, t, y, args):\n        f0 = -self.k1 * y[0] + self.k3 * y[1] * y[2]\n        f1 = self.k1 * y[0] - self.k2 * y[1] ** 2 - self.k3 * y[1] * y[2]\n        f2 = self.k2 * y[1] ** 2\n        return jnp.stack([f0, f1, f2])\n```\n\n----------------------------------------\n\nTITLE: Setting Up and Running Optimization Loop\nDESCRIPTION: This snippet initializes the model, target steady state, and optimizer. It then defines a step function and runs an optimization loop to find the target steady state using stochastic gradient descent with momentum.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/steady_state.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel = ExponentialDecayToSteadyState(\n    jnp.array(0.0)\n)  # initial steady state guess is 0.\n# target steady state is 0.76\ntarget_steady_state = jnp.array(0.76)\noptim = optax.sgd(1e-2, momentum=0.7, nesterov=True)\nopt_state = optim.init(model)\n\n\n@eqx.filter_jit\ndef make_step(model, opt_state, target_steady_state):\n    grads = eqx.filter_grad(loss)(model, target_steady_state)\n    updates, opt_state = optim.update(grads, opt_state)\n    model = eqx.apply_updates(model, updates)\n    return model, opt_state\n\n\nfor step in range(100):\n    model, opt_state = make_step(model, opt_state, target_steady_state)\n    print(f\"Step: {step} Steady State: {model.steady_state}\")\nprint(f\"Target: {target_steady_state}\")\n```\n\n----------------------------------------\n\nTITLE: Installing Diffrax using pip\nDESCRIPTION: Command to install Diffrax library using pip package manager. Requires Python 3.10 or higher.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/index.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install diffrax\n```\n\n----------------------------------------\n\nTITLE: Defining Helper Classes and Functions\nDESCRIPTION: Implements a Stack class for combining multiple modules and a quantise function for rounding symbolic expressions. These helpers are used in the main symbolic regression process.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/symbolic_regression.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Stack(eqx.Module):\n    modules: list[eqx.Module]\n\n    def __call__(self, x):\n        assert x.shape[-1] == 2\n        x0 = x[..., 0]\n        x1 = x[..., 1]\n        return jnp.stack([module(x0=x0, x1=x1) for module in self.modules], axis=-1)\n\n\ndef quantise(expr, quantise_to):\n    if isinstance(expr, sympy.Float):\n        return expr.func(round(float(expr) / quantise_to) * quantise_to)\n    elif isinstance(expr, (sympy.Symbol, sympy.Integer)):\n        return expr\n    else:\n        return expr.func(*[quantise(arg, quantise_to) for arg in expr.args])\n```\n\n----------------------------------------\n\nTITLE: AbstractRungeKutta Interface in Diffrax\nDESCRIPTION: An abstract class for Runge-Kutta solvers, which are a family of iterative ODE solvers.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/solvers/abstract_solvers.md#2025-04-21_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ndiffrax.AbstractRungeKutta\n```\n\n----------------------------------------\n\nTITLE: Defining ExponentialDecayToSteadyState Model\nDESCRIPTION: This class defines a simple ODE model that exponentially decays to a steady state. It inherits from Equinox's Module class and implements a callable method representing the ODE.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/steady_state.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass ExponentialDecayToSteadyState(eqx.Module):\n    steady_state: float\n\n    def __call__(self, t, y, args):\n        return self.steady_state - y\n```\n\n----------------------------------------\n\nTITLE: Symplectic ODE System Structure\nDESCRIPTION: Mathematical representation of symplectic ODEs showing the coupled system of equations for variables v and w, commonly used in Hamiltonian systems.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/solvers/ode_solvers.md#2025-04-21_snippet_1\n\nLANGUAGE: latex\nCODE:\n```\n\\frac{\\mathrm{d}v}{\\mathrm{d}t}(t) = f(t, w(t))\\n\\frac{\\mathrm{d}w}{\\mathrm{d}t}(t) = g(t, v(t))\n```\n\n----------------------------------------\n\nTITLE: AbstractERK Interface in Diffrax\nDESCRIPTION: An abstract class for Explicit Runge-Kutta methods, which are a specific category of Runge-Kutta solvers.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/solvers/abstract_solvers.md#2025-04-21_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndiffrax.AbstractERK\n```\n\n----------------------------------------\n\nTITLE: Creating Harmonic Oscillator LTI System in Python\nDESCRIPTION: This function creates an LTISystem instance representing a harmonic oscillator with optional damping and time scaling parameters.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/kalman_filter.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef harmonic_oscillator(damping: float = 0.0, time_scaling: float = 1.0) -> LTISystem:\n    A = jnp.array([[0.0, time_scaling], [-time_scaling, -2 * damping]])\n    B = jnp.array([[0.0], [1.0]])\n    C = jnp.array([[0.0, 1.0]])\n    return LTISystem(A, B, C)\n```\n\n----------------------------------------\n\nTITLE: AbstractSRK Interface in Diffrax\nDESCRIPTION: An abstract class for Stochastic Runge-Kutta methods, which are extensions of Runge-Kutta methods for stochastic differential equations.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/solvers/abstract_solvers.md#2025-04-21_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\ndiffrax.AbstractSRK\n```\n\n----------------------------------------\n\nTITLE: Implementing ULD Simulation for 2D Harmonic Oscillator\nDESCRIPTION: Sets up and solves an Underdamped Langevin Diffusion simulation for a 2D harmonic oscillator using Diffrax. Configures system parameters, Brownian motion, drift and diffusion terms, and uses the QUICSORT solver to compute the trajectory.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/underdamped_langevin_example.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom warnings import simplefilter\n\nsimplefilter(action=\"ignore\", category=FutureWarning)\nimport diffrax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib.pyplot as plt\n\nt0, t1 = 0.0, 20.0\ndt0 = 0.05\nsaveat = diffrax.SaveAt(steps=True)\n\n# Parameters\ngamma = jnp.array([2, 0.5], dtype=jnp.float32)\nu = jnp.array([0.5, 2], dtype=jnp.float32)\nx0 = jnp.zeros((2,), dtype=jnp.float32)\nv0 = jnp.zeros((2,), dtype=jnp.float32)\ny0 = (x0, v0)\n\n# Brownian motion\nbm = diffrax.VirtualBrownianTree(\n    t0, t1, tol=0.01, shape=(2,), key=jr.key(0), levy_area=diffrax.SpaceTimeTimeLevyArea\n)\n\ndrift_term = diffrax.UnderdampedLangevinDriftTerm(gamma, u, lambda x: 2 * x)\ndiffusion_term = diffrax.UnderdampedLangevinDiffusionTerm(gamma, u, bm)\nterms = diffrax.MultiTerm(drift_term, diffusion_term)\n\nsolver = diffrax.QUICSORT(100.0)\nsol = diffrax.diffeqsolve(\n    terms, solver, t0, t1, dt0=dt0, y0=y0, args=None, saveat=saveat\n)\nxs, vs = sol.ys\n```\n\n----------------------------------------\n\nTITLE: Initializing Event Class in Diffrax\nDESCRIPTION: Documents the constructor for the Event class in Diffrax, which is used to define conditions for interrupting differential equation solves.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/events.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n::: diffrax.Event\n    selection:\n        members:\n            - __init__\n```\n\n----------------------------------------\n\nTITLE: AbstractImplicitSolver Initialization in Diffrax\nDESCRIPTION: An abstract class for implicit solvers that extends AbstractSolver, providing initialization functionality specific to implicit methods.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/solvers/abstract_solvers.md#2025-04-21_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\ndiffrax.AbstractImplicitSolver\n    __init__\n```\n\n----------------------------------------\n\nTITLE: Solving ODE with Linear Forcing Term\nDESCRIPTION: Implements a solution for an ODE with a linear forcing term of the form m*t + c. Uses the Tsit5 solver and JIT compilation for efficiency. The vector field combines both the forcing term and the main ODE term (-y).\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/forcing.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nfrom diffrax import diffeqsolve, ODETerm, SaveAt, Tsit5\n\n\ndef force(t, args):\n    m, c = args\n    return m * t + c\n\n\ndef vector_field(t, y, args):\n    return -y + force(t, args)\n\n\n@jax.jit\ndef solve(y0, args):\n    term = ODETerm(vector_field)\n    solver = Tsit5()\n    t0 = 0\n    t1 = 10\n    dt0 = 0.1\n    saveat = SaveAt(ts=jnp.linspace(t0, t1, 1000))\n    sol = diffeqsolve(term, solver, t0, t1, dt0, y0, args=args, saveat=saveat)\n    return sol\n\n\ny0 = 1.0\nargs = (0.1, 0.02)\nsol = solve(y0, args)\n```\n\n----------------------------------------\n\nTITLE: AbstractAdaptiveSolver Interface in Diffrax\nDESCRIPTION: An abstract class for adaptive step size solvers, denoting solvers that can automatically adjust their step size based on error estimates.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/solvers/abstract_solvers.md#2025-04-21_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\ndiffrax.AbstractAdaptiveSolver\n```\n\n----------------------------------------\n\nTITLE: Solving Lotka-Volterra Equations with Diffrax in Python\nDESCRIPTION: This code snippet defines the vector field for the Lotka-Volterra equations, sets up the ODE solver using Diffrax, and solves the system. It uses the Tsit5 solver and saves the solution at specified time points.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/coupled_odes.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nfrom diffrax import diffeqsolve, ODETerm, SaveAt, Tsit5\n\n\ndef vector_field(t, y, args):\n    prey, predator = y\n    α, β, γ, δ = args\n    d_prey = α * prey - β * prey * predator\n    d_predator = -γ * predator + δ * prey * predator\n    d_y = d_prey, d_predator\n    return d_y\n\n\nterm = ODETerm(vector_field)\nsolver = Tsit5()\nt0 = 0\nt1 = 140\ndt0 = 0.1\ny0 = (10.0, 10.0)\nargs = (0.1, 0.02, 0.4, 0.02)\nsaveat = SaveAt(ts=jnp.linspace(t0, t1, 1000))\nsol = diffeqsolve(term, solver, t0, t1, dt0, y0, args=args, saveat=saveat)\n```\n\n----------------------------------------\n\nTITLE: AbstractStratonovichSolver Interface in Diffrax\nDESCRIPTION: An abstract class for solvers that specifically handle stochastic differential equations using Stratonovich calculus.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/solvers/abstract_solvers.md#2025-04-21_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\ndiffrax.AbstractStratonovichSolver\n```\n\n----------------------------------------\n\nTITLE: AbstractItoSolver Interface in Diffrax\nDESCRIPTION: An abstract class for solvers that specifically handle stochastic differential equations using Itô calculus.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/solvers/abstract_solvers.md#2025-04-21_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\ndiffrax.AbstractItoSolver\n```\n\n----------------------------------------\n\nTITLE: Computing Hessian of ODE Solution using JAX and Diffrax\nDESCRIPTION: Implements a Lotka-Volterra predator-prey model and computes its Hessian using JAX's automatic differentiation. The solution uses Tsit5 solver with specific settings for higher-order autodiff compatibility. The vector field defines the predator-prey dynamics with four parameters (α, β, γ, δ).\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/hessian.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport jax\nimport jax.numpy as jnp\nfrom diffrax import diffeqsolve, ODETerm, Tsit5\n\n\ndef vector_field(t, y, args):\n    prey, predator = y\n    α, β, γ, δ = args\n    d_prey = α * prey - β * prey * predator\n    d_predator = -γ * predator + δ * prey * predator\n    d_y = d_prey, d_predator\n    return d_y\n\n\n@jax.jit\n@jax.hessian\ndef run(y0):\n    term = ODETerm(vector_field)\n    solver = Tsit5(scan_kind=\"bounded\")\n    t0 = 0\n    t1 = 140\n    dt0 = 0.1\n    args = (0.1, 0.02, 0.4, 0.02)\n    sol = diffeqsolve(term, solver, t0, t1, dt0, y0, args=args)\n    ((prey,), _) = sol.ys\n    return prey\n\n\ny0 = (jnp.array(10.0), jnp.array(10.0))\nrun(y0)\n```\n\n----------------------------------------\n\nTITLE: LipSwish Activation Function Implementation\nDESCRIPTION: Defines a LipSwish activation function used in both generator and discriminator networks\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/neural_sde.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef lipswish(x):\n    return 0.909 * jnn.silu(x)\n```\n\n----------------------------------------\n\nTITLE: AbstractDIRK Interface in Diffrax\nDESCRIPTION: An abstract class for Diagonally Implicit Runge-Kutta methods, which are a specific category of Runge-Kutta solvers.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/solvers/abstract_solvers.md#2025-04-21_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ndiffrax.AbstractDIRK\n```\n\n----------------------------------------\n\nTITLE: ODE Problem Setup and Solution Configuration\nDESCRIPTION: Configures the ODE problem including vector field definition, initial conditions, and solver parameters for the direct ODE solution approach.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/nonlinear_heat_pde.ipynb#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef vector_field(t, y, args):\n    return (1 - y) * laplacian(y)\n\n\nterm = diffrax.ODETerm(vector_field)\nic = lambda x: x**2\n\n# Spatial discretisation\nx0 = -1\nx_final = 1\nn = 50\ny0 = SpatialDiscretisation.discretise_fn(x0, x_final, n, ic)\n\n# Temporal discretisation\nt0 = 0\nt_final = 1\nδt = 0.0001\nsaveat = diffrax.SaveAt(ts=jnp.linspace(t0, t_final, 50))\n\n# Tolerances\nrtol = 1e-10\natol = 1e-10\nstepsize_controller = diffrax.PIDController(\n    pcoeff=0.3, icoeff=0.4, rtol=rtol, atol=atol, dtmax=0.001\n)\n```\n\n----------------------------------------\n\nTITLE: Lévy Area Inheritance Hierarchy in Python\nDESCRIPTION: Displays the inheritance hierarchy of Lévy area classes used in Diffrax for SDE solvers. This structure shows how different types of Lévy areas relate to each other, from the base AbstractBrownianIncrement to more specific implementations like SpaceTimeTimeLevyArea.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/brownian.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nAbstractBrownianIncrement\n│   └── BrownianIncrement\n└── AbstractSpaceTimeLevyArea\n    │   └── SpaceTimeLevyArea\n    └── AbstractSpaceTimeTimeLevyArea\n            └── SpaceTimeTimeLevyArea\n```\n\n----------------------------------------\n\nTITLE: AbstractSDIRK Interface in Diffrax\nDESCRIPTION: An abstract class for Singly Diagonally Implicit Runge-Kutta methods, which are a specific category of Runge-Kutta solvers.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/solvers/abstract_solvers.md#2025-04-21_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ndiffrax.AbstractSDIRK\n```\n\n----------------------------------------\n\nTITLE: Defining Linear Time-Invariant System Class in Python\nDESCRIPTION: This snippet defines an LTISystem class using Equinox, representing a linear time-invariant system with matrices A, B, and C.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/kalman_filter.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass LTISystem(eqx.Module):\n    A: jnp.ndarray\n    B: jnp.ndarray\n    C: jnp.ndarray\n```\n\n----------------------------------------\n\nTITLE: AbstractESDIRK Interface in Diffrax\nDESCRIPTION: An abstract class for Explicit Singly Diagonally Implicit Runge-Kutta methods, which are a specific category of Runge-Kutta solvers.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/solvers/abstract_solvers.md#2025-04-21_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\ndiffrax.AbstractESDIRK\n```\n\n----------------------------------------\n\nTITLE: Solver Execution and Timing\nDESCRIPTION: Executes the solver with specific rate constants, first for JIT compilation and then for timing. Prints the results at specified time points.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/stiff_ode.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmain(0.04, 3e7, 1e4)\n\nstart = time.time()\nsol = main(0.04, 3e7, 1e4)\nend = time.time()\n\nprint(\"Results:\")\nfor ti, yi in zip(sol.ts, sol.ys):\n    print(f\"t={ti.item()}, y={yi.tolist()}\")\nprint(f\"Took {sol.stats['num_steps']} steps in {end - start} seconds.\")\n```\n\n----------------------------------------\n\nTITLE: ButcherTableau Initialization in Diffrax\nDESCRIPTION: A class representing Butcher tableaux, which are used to define Runge-Kutta methods by specifying their coefficients.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/solvers/abstract_solvers.md#2025-04-21_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\ndiffrax.ButcherTableau\n    __init__\n```\n\n----------------------------------------\n\nTITLE: CalculateJacobian Interface in Diffrax\nDESCRIPTION: A utility class for calculating Jacobian matrices, which are used in implicit solver methods.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/solvers/abstract_solvers.md#2025-04-21_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\ndiffrax.CalculateJacobian\n```\n\n----------------------------------------\n\nTITLE: Model Execution\nDESCRIPTION: Executes the main function to train and evaluate the Neural ODE model.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/neural_ode.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nts, ys, model = main()\n```\n\n----------------------------------------\n\nTITLE: Importing Diffrax Citation Module in Python\nDESCRIPTION: This code snippet demonstrates how to import the citation module from Diffrax. The citation module is used to generate automatic BibTeX citations for numerical methods used in the project.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/autocitation.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom diffrax import citation\n```\n\n----------------------------------------\n\nTITLE: Running Kalman Filter Simulation without Optimization in Python\nDESCRIPTION: This code snippet runs the main function without performing any gradient steps, effectively simulating the Kalman filter with initial parameters.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/kalman_filter.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmain(n_gradient_steps=0)\n```\n\n----------------------------------------\n\nTITLE: AbstractAdjoint Base Class\nDESCRIPTION: Abstract base class defining the interface for adjoint implementations in Diffrax\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/adjoints.md#2025-04-21_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n::: diffrax.AbstractAdjoint\n        selection:\n            members:\n                - loop\n```\n\n----------------------------------------\n\nTITLE: BacksolveAdjoint Implementation\nDESCRIPTION: Implementation of the optimise-then-discretise approach that computes gradients by solving a backwards-in-time ODE. This method is marked as not recommended.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/adjoints.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n::: diffrax.BacksolveAdjoint\n    selection:\n        members:\n            - __init__\n```\n\n----------------------------------------\n\nTITLE: Running the Neural CDE Example\nDESCRIPTION: This line calls the main function to execute the Neural CDE example, training the model and visualizing the results.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/neural_cde.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmain()\n```\n\n----------------------------------------\n\nTITLE: SDE Adaptive Step Size Citation in BibTeX\nDESCRIPTION: BibTeX citation for a research paper discussing the convergence of adaptive approximations for stochastic differential equations, which is relevant when using adaptive step controllers with SDEs.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/stepsize_controller.md#2025-04-21_snippet_0\n\nLANGUAGE: bibtex\nCODE:\n```\n@misc{foster2024convergenceadaptiveapproximationsstochastic,\n    title={On the convergence of adaptive approximations for stochastic differential equations}, \n    author={James Foster and Andraž Jelinčič},\n    year={2024},\n    eprint={2311.14201},\n    archivePrefix={arXiv},\n    primaryClass={math.NA},\n    url={https://arxiv.org/abs/2311.14201}, \n}\n```\n\n----------------------------------------\n\nTITLE: Visualizing ULD Simulation Results\nDESCRIPTION: Creates a visualization of the simulation results showing position and velocity trajectories over time using matplotlib. Plots two subplots: one for position components and one for velocity components.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/underdamped_langevin_example.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Plot the trajectory against time and velocity against time in a separate plot\nfig, axs = plt.subplots(2, 1, figsize=(10, 10))\naxs[0].plot(sol.ts, xs[:, 0], label=\"x1\")\naxs[0].plot(sol.ts, xs[:, 1], label=\"x2\")\naxs[0].set_xlabel(\"Time\")\naxs[0].set_ylabel(\"Position\")\naxs[0].legend()\naxs[0].grid()\n\naxs[1].plot(sol.ts, vs[:, 0], label=\"v1\")\naxs[1].plot(sol.ts, vs[:, 1], label=\"v2\")\naxs[1].set_xlabel(\"Time\")\naxs[1].set_ylabel(\"Velocity\")\naxs[1].legend()\naxs[1].grid()\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Defining Vector Field for Latent ODE\nDESCRIPTION: This class defines the vector field for the Latent ODE, using a structure of scalar * tanh(mlp(y)). It includes a scale parameter and an MLP for computing the vector field.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/latent_ode.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Func(eqx.Module):\n    scale: jnp.ndarray\n    mlp: eqx.nn.MLP\n\n    def __call__(self, t, y, args):\n        return self.scale * self.mlp(y)\n```\n\n----------------------------------------\n\nTITLE: Implementing Progress Meter Interface in Python\nDESCRIPTION: Abstract base class defining the interface for progress meters in Diffrax. Includes initialization, step updates, and cleanup methods.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/progress_meter.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass AbstractProgressMeter:\n    def init(self):\n        pass\n    \n    def step(self):\n        pass\n        \n    def close(self):\n        pass\n```\n\n----------------------------------------\n\nTITLE: Plotting ODE Solution Results\nDESCRIPTION: Visualizes the solution of the ODE using matplotlib, plotting the solution trajectory over time.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/forcing.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nplt.plot(sol.ts, sol.ys)\nplt.xlabel(\"t\")\nplt.ylabel(\"y\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Tqdm Progress Meter Implementation in Python\nDESCRIPTION: Implementation of a progress meter using the tqdm library for visual progress bars.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/progress_meter.md#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass TqdmProgressMeter:\n    def __init__(self):\n        pass\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Neural ODE Implementation\nDESCRIPTION: Imports required libraries including Diffrax for differential equations, Equinox for neural networks, JAX for numerical computing, and Optax for optimization.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/neural_ode.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nimport diffrax\nimport equinox as eqx\nimport jax\nimport jax.nn as jnn\nimport jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib.pyplot as plt\nimport optax\n```\n\n----------------------------------------\n\nTITLE: Referencing RESULTS Enum in Python for Diffrax\nDESCRIPTION: This code snippet references the RESULTS enum in the Diffrax library. The enum likely contains possible result statuses for the differential equation solver.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/solution.md#2025-04-21_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nRESULTS\n```\n\n----------------------------------------\n\nTITLE: StochasticButcherTableau Initialization in Diffrax\nDESCRIPTION: A class representing stochastic Butcher tableaux, which extend standard Butcher tableaux for use with stochastic differential equations.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/solvers/abstract_solvers.md#2025-04-21_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\ndiffrax.StochasticButcherTableau\n    __init__\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Neural CDE Implementation\nDESCRIPTION: This snippet imports necessary libraries for implementing the Neural CDE, including Diffrax for differential equation solving, Equinox for neural network components, JAX for numerical computing, and Matplotlib for visualization.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/neural_cde.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport math\nimport time\n\nimport diffrax\nimport equinox as eqx  # https://github.com/patrick-kidger/equinox\nimport jax\nimport jax.nn as jnn\nimport jax.numpy as jnp\nimport jax.random as jr\nimport jax.scipy as jsp\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport optax  # https://github.com/deepmind/optax\n\n\nmatplotlib.rcParams.update({\"font.size\": 30})\n```\n\n----------------------------------------\n\nTITLE: Executing Main Function with Cat Image\nDESCRIPTION: This code runs the main function with a cat image as input, demonstrating how to use the implemented image processing and visualization pipeline.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/continuous_normalising_flow.ipynb#2025-04-21_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmain(in_path=\"../imgs/cat.png\")\n```\n\n----------------------------------------\n\nTITLE: AbstractFosterLangevinSRK Interface in Diffrax\nDESCRIPTION: An abstract class for Foster-Langevin type Stochastic Runge-Kutta methods, which are specialized solvers for Langevin dynamics.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/solvers/abstract_solvers.md#2025-04-21_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\ndiffrax.AbstractFosterLangevinSRK\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Latent ODE Implementation\nDESCRIPTION: This snippet imports the necessary libraries and modules for implementing the Latent ODE model, including Diffrax, Equinox, JAX, and visualization tools.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/latent_ode.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nimport diffrax\nimport equinox as eqx\nimport jax\nimport jax.nn as jnn\nimport jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport optax\n\n\nmatplotlib.rcParams.update({\"font.size\": 30})\n```\n\n----------------------------------------\n\nTITLE: Steady State Event Detection in Diffrax\nDESCRIPTION: References the steady_state_event functionality in Diffrax, which is used to detect when a system reaches steady state and terminate the solve.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/events.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n::: diffrax.steady_state_event\n```\n\n----------------------------------------\n\nTITLE: Documentation Reference for SubSaveAt Class Init\nDESCRIPTION: Reference documentation marker for the SubSaveAt class initialization method in the Diffrax library. Used to specify subsample points for saving during differential equation solving.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/saveat.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n::: diffrax.SubSaveAt\n    selection:\n        members:\n            - __init__\n```\n\n----------------------------------------\n\nTITLE: Incorrect Usage of BacksolveAdjoint Leading to CustomVJPException\nDESCRIPTION: This example demonstrates incorrect usage of BacksolveAdjoint that will raise a CustomVJPException. The model is captured via closure and is not part of the terms PyTree, which causes issues with gradient computation.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/further_details/faq.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom diffrax import BacksolveAdjoint, diffeqsolve, Euler, ODETerm\nimport equinox as eqx\nimport jax.numpy as jnp\nimport jax.random as jr\n\nmlp = eqx.nn.MLP(1, 1, 8, 2, key=jr.PRNGKey(0))\n\n@eqx.filter_jit\n@eqx.filter_value_and_grad\ndef run(model):\n  def f(t, y, args):  # `model` captured via closure; is not part of the `terms` PyTree.\n    return model(y)\n  sol = diffeqsolve(ODETerm(f), Euler(), 0, 1, 0.1, jnp.array([1.0]),\n                    adjoint=BacksolveAdjoint())\n  return jnp.sum(sol.ys)\n\nrun(mlp)\n```\n\n----------------------------------------\n\nTITLE: Plotting Lotka-Volterra Solution with Matplotlib in Python\nDESCRIPTION: This code snippet plots the solution of the Lotka-Volterra equations obtained from the Diffrax solver. It creates a line plot showing the population dynamics of prey and predator over time.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/coupled_odes.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nplt.plot(sol.ts, sol.ys[0], label=\"Prey\")\nplt.plot(sol.ts, sol.ys[1], label=\"Predator\")\nplt.legend()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Kalman Filter Implementation in Python\nDESCRIPTION: This snippet imports necessary libraries for implementing a Kalman Filter. It includes Diffrax for differential equations, Equinox for building the filter, JAX for numerical operations, and Optax for optimization.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/kalman_filter.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom types import SimpleNamespace\nfrom typing import Optional\n\nimport diffrax as dfx\nimport equinox as eqx  # https://github.com/patrick-kidger/equinox\nimport jax.numpy as jnp\nimport jax.random as jr\nimport jax.tree_util as jtu\nimport matplotlib.pyplot as plt\nimport optax  # https://github.com/deepmind/optax\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Stiff ODE Solver\nDESCRIPTION: Imports necessary Python libraries including Diffrax for differential equation solving, Equinox for neural networks, and JAX for numerical computing.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/stiff_ode.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport time\n\nimport diffrax\nimport equinox as eqx  # https://github.com/patrick-kidger/equinox\nimport jax\nimport jax.numpy as jnp\n```\n\n----------------------------------------\n\nTITLE: Implementing Data Loader for Batch Processing\nDESCRIPTION: This function implements a data loader that yields batches of data for training. It shuffles the dataset and provides batches in a cyclic manner.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/latent_ode.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef dataloader(arrays, batch_size, *, key):\n    dataset_size = arrays[0].shape[0]\n    assert all(array.shape[0] == dataset_size for array in arrays)\n    indices = jnp.arange(dataset_size)\n    while True:\n        perm = jr.permutation(key, indices)\n        (key,) = jr.split(key, 1)\n        start = 0\n        end = batch_size\n        while start < dataset_size:\n            batch_perm = perm[start:end]\n            yield tuple(array[batch_perm] for array in arrays)\n            start = end\n            end = start + batch_size\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for ODE Solving and Optimization\nDESCRIPTION: This snippet imports necessary libraries including Diffrax for ODE solving, Equinox for neural networks, JAX NumPy for numerical operations, and Optax for optimization.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/steady_state.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport diffrax\nimport equinox as eqx  # https://github.com/patrick-kidger/equinox\nimport jax.numpy as jnp\nimport optax  # https://github.com/deepmind/optax\n```\n\n----------------------------------------\n\nTITLE: Cloning and Installing Diffrax for Development\nDESCRIPTION: Commands to clone the forked Diffrax repository and install it in development mode. This setup allows contributors to make changes to the codebase.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/CONTRIBUTING.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/your-username-here/diffrax.git\ncd diffrax\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Text Progress Meter Implementation in Python\nDESCRIPTION: Implementation of a text-based progress meter that outputs progress information as text.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/progress_meter.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass TextProgressMeter:\n    def __init__(self):\n        pass\n```\n\n----------------------------------------\n\nTITLE: Installing and Setting Up Pre-commit Hook\nDESCRIPTION: Commands to install the pre-commit package and set up the pre-commit hook. This hook uses ruff for linting and formatting, and pyright for type-checking the code.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/CONTRIBUTING.md#2025-04-21_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install pre-commit\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: Pushing Changes to Forked Repository\nDESCRIPTION: Command to push local changes back to the forked repository on GitHub. This is a necessary step before creating a pull request.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/CONTRIBUTING.md#2025-04-21_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit push\n```\n\n----------------------------------------\n\nTITLE: Documentation Reference for SaveAt Class Init\nDESCRIPTION: Reference documentation marker for the SaveAt class initialization method in the Diffrax library. Used to specify when during solving to save solution points.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/saveat.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n::: diffrax.SaveAt\n    selection:\n        members:\n            - __init__\n```\n\n----------------------------------------\n\nTITLE: Building and Serving Diffrax Documentation\nDESCRIPTION: Commands to install documentation requirements, build the documentation, and serve it locally. This process involves running mkdocs commands twice due to the project's documentation stack setup.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/CONTRIBUTING.md#2025-04-21_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install -r docs/requirements.txt\nmkdocs build\nmkdocs serve\n```\n\n----------------------------------------\n\nTITLE: DIRK Stage Nonlinear System\nDESCRIPTION: Mathematical formula showing the nonlinear system that needs to be solved at each stage of a diagonal implicit RK method.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/devdocs/predictor_dirk.md#2025-04-21_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\nf_i = f(y_0 + \\sum_{j=1}^i a_{ij} f_j Δt)\n```\n\n----------------------------------------\n\nTITLE: ForwardMode Adjoint Implementation\nDESCRIPTION: Forward mode differentiation implementation for computing gradients through differential equations\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/adjoints.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n::: diffrax.ForwardMode\n    selection: \n        members: false\n```\n\n----------------------------------------\n\nTITLE: Linear Combination Predictor Formula\nDESCRIPTION: Formula for initializing the prediction of f_i using a linear combination of previously computed values.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/devdocs/predictor_dirk.md#2025-04-21_snippet_1\n\nLANGUAGE: latex\nCODE:\n```\nf_i = \\sum_{j = 1}^{i-1} α_{ij} f_j\n```\n\n----------------------------------------\n\nTITLE: Defining Stratonovich SDE in LaTeX\nDESCRIPTION: Mathematical representation of a Stratonovich stochastic differential equation using LaTeX notation. It defines the drift term μ and diffusion term σ with the Stratonovich integral.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/devdocs/adjoint_commutative_noise.md#2025-04-21_snippet_1\n\nLANGUAGE: latex\nCODE:\n```\n\\mathrm{d}y(t) = μ(t, y(t))\\mathrm{d}t + σ(t, y(t))\\circ\\mathrm{d}w(t)\n```\n\n----------------------------------------\n\nTITLE: Data Generation Implementation\nDESCRIPTION: Implements data generation function that creates trajectories with positive drift, mean-reversion, and time-dependent diffusion\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/neural_sde.ipynb#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@jax.jit\n@jax.vmap\ndef get_data(key):\n    bm_key, y0_key, drop_key = jr.split(key, 3)\n\n    mu = 0.02\n    theta = 0.1\n    sigma = 0.4\n\n    t0 = 0\n    t1 = 63\n    t_size = 64\n\n    def drift(t, y, args):\n        return mu * t - theta * y\n\n    def diffusion(t, y, args):\n        return 2 * sigma * t / t1\n\n    bm = diffrax.UnsafeBrownianPath(shape=(), key=bm_key)\n    drift = diffrax.ODETerm(drift)\n    diffusion = diffrax.ControlTerm(diffusion, bm)\n    terms = diffrax.MultiTerm(drift, diffusion)\n```\n\n----------------------------------------\n\nTITLE: Commutativity Condition for SDE in LaTeX\nDESCRIPTION: Mathematical expression of the commutativity condition for the diffusion term σ in the SDE using LaTeX notation. This condition is crucial for higher-order SDE solvers.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/devdocs/adjoint_commutative_noise.md#2025-04-21_snippet_2\n\nLANGUAGE: latex\nCODE:\n```\nσ_{i\\, j_2} \\frac{\\partial σ_{k\\, j_1}}{\\partial y_i} = σ_{i\\, j_1} \\frac{\\partial σ_{k\\, j_2}}{\\partial y_i}\n```\n\n----------------------------------------\n\nTITLE: Higher-order Commutativity Condition in LaTeX\nDESCRIPTION: Mathematical expression of the newly introduced higher-order commutativity condition for the diffusion term σ using LaTeX notation. This condition ensures commutativity in the backward pass.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/devdocs/adjoint_commutative_noise.md#2025-04-21_snippet_3\n\nLANGUAGE: latex\nCODE:\n```\nσ_{i\\, j_2} \\frac{\\partial^2 σ_{k\\, j_1}}{\\partial y_i \\partial y_m} = σ_{i\\, j_1} \\frac{\\partial^2 σ_{k\\, j_2}}{\\partial y_i \\partial y_m}\n```\n\n----------------------------------------\n\nTITLE: Custom Dataloader Implementation\nDESCRIPTION: Implements a custom dataloader that creates batches from arrays with optional infinite looping. Includes random permutation for shuffling.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/neural_sde.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef dataloader(arrays, batch_size, loop, *, key):\n    dataset_size = arrays[0].shape[0]\n    assert all(array.shape[0] == dataset_size for array in arrays)\n    indices = jnp.arange(dataset_size)\n    while True:\n        perm = jr.permutation(key, indices)\n        key = jr.split(key, 1)[0]\n        start = 0\n        end = batch_size\n        while end < dataset_size:\n            batch_perm = perm[start:end]\n            yield tuple(array[batch_perm] for array in arrays)\n            start = end\n            end = start + batch_size\n        if not loop:\n            break\n```\n\n----------------------------------------\n\nTITLE: Reverse-time Adjoint SDE in LaTeX\nDESCRIPTION: Mathematical representation of the reverse-time adjoint SDE used in the proof, expressed using LaTeX notation. It shows the evolution of the adjoint variable a.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/devdocs/adjoint_commutative_noise.md#2025-04-21_snippet_4\n\nLANGUAGE: latex\nCODE:\n```\n\\mathrm{d}a_i(t) = -a_j(t) \\frac{\\partial μ_j}{\\partial y_i}(t, y(t))\\mathrm{d}t - a_j(t) \\frac{\\partial σ_{j\\, k}}{\\partial y_i}(t, y(t)) \\circ \\mathrm{d}w_k\n```\n\n----------------------------------------\n\nTITLE: BibTeX Citation for Neural Differential Equations Thesis\nDESCRIPTION: BibTeX entry for citing Patrick Kidger's PhD thesis 'On Neural Differential Equations' from the University of Oxford, which forms the theoretical foundation for the Diffrax library.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/further_details/.citation.md#2025-04-21_snippet_0\n\nLANGUAGE: bibtex\nCODE:\n```\n@phdthesis{kidger2021on,\n    title={{O}n {N}eural {D}ifferential {E}quations},\n    author={Patrick Kidger},\n    year={2021},\n    school={University of Oxford},\n}\n```\n\n----------------------------------------\n\nTITLE: Running Small Neural ODE Benchmark in Python\nDESCRIPTION: This command runs the small neural ODE benchmark script. It can be executed with optional --grad and --multiple flags to test differentiation and multiple point evaluation respectively.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/benchmarks/README.md#2025-04-21_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython small_neural_ode.py\n```\n\n----------------------------------------\n\nTITLE: Training Loop and Model Update in JAX\nDESCRIPTION: This snippet shows a training loop that updates a model iteratively. It prints the loss and computation time at regular intervals.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/continuous_normalising_flow.ipynb#2025-04-21_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n    step = 0\n    while step < steps:\n        start = time.time()\n        value, model, opt_state, step, loss_key = make_step(\n            model, opt_state, step, loss_key\n        )\n        end = time.time()\n        if (step % print_every) == 0 or step == steps - 1:\n            print(f\"Step: {step}, Loss: {value}, Computation time: {end - start}\")\n```\n\n----------------------------------------\n\nTITLE: No Progress Meter Implementation in Python\nDESCRIPTION: Null implementation of the progress meter interface that performs no output.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/progress_meter.md#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass NoProgressMeter:\n    def __init__(self):\n        pass\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies and Configuration\nDESCRIPTION: Sets up required libraries including Diffrax, Equinox, JAX, and matplotlib. Enables 64-bit floating point precision in JAX.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/nonlinear_heat_pde.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Callable\n\nimport diffrax\nimport equinox as eqx  # https://github.com/patrick-kidger/equinox\nimport jax\nimport jax.lax as lax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nfrom jaxtyping import Array, Float  # https://github.com/google/jaxtyping\n\n\njax.config.update(\"jax_enable_x64\", True)\n```\n\n----------------------------------------\n\nTITLE: AbstractWrappedSolver Initialization in Diffrax\nDESCRIPTION: An abstract class for solvers that wrap other solver implementations, providing a mechanism for extending or modifying existing solvers.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/solvers/abstract_solvers.md#2025-04-21_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ndiffrax.AbstractWrappedSolver\n    __init__\n```\n\n----------------------------------------\n\nTITLE: Specifying Documentation Dependencies and Versions\nDESCRIPTION: Defines specific package versions required for documentation generation, including MkDocs core, Material theme, extensions for LaTeX support, docstring processing, and Jupyter notebook conversion. Includes pinned versions to ensure compatibility, particularly for packages like Jinja2 which has known compatibility issues with mkdocstrings above version 3.0.3.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# Latest versions at time of writing.\nmkdocs==1.3.0            # Main documentation generator.\nmkdocs-material==7.3.6   # Theme\npymdown-extensions==9.4  # Markdown extensions e.g. to handle LaTeX.\nmkdocstrings==0.17.0     # Autogenerate documentation from docstrings.\nmknotebooks==0.7.1       # Turn Jupyter Lab notebooks into webpages.\npytkdocs_tweaks==0.0.8   # Tweaks mkdocstrings to improve various aspects\nmkdocs_include_exclude_files==0.0.1  # Allow for customising which files get included\njinja2==3.0.3            # Older version. After 3.1.0 seems to be incompatible with current versions of mkdocstrings.\nnbconvert==6.5.0         # | Older verson to avoid error\nnbformat==5.4.0          # |\npygments==2.14.0\nmkdocs-autorefs==1.0.1\nmkdocs-material-extensions==1.3.1\n\n# Install latest version of our dependencies\njax[cpu]\n```\n\n----------------------------------------\n\nTITLE: Configuring a MultiTerm SDE with Euler solver in Diffrax\nDESCRIPTION: Example of setting up a stochastic differential equation with drift and diffusion terms using MultiTerm and solving it with the Euler solver in Diffrax. The example defines a linear drift, a multiplicative diffusion, and a Brownian motion path.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/solvers/sde_solvers.md#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndrift = lambda t, y, args: -y\ndiffusion = lambda t, y, args: y[..., None]\nbm = UnsafeBrownianPath(shape=(1,), key=...)\nterms = MultiTerm(ODETerm(drift), ControlTerm(diffusion, bm))\ndiffeqsolve(terms, solver=Euler(), ...)\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Continuous Normalising Flow\nDESCRIPTION: Imports necessary libraries and modules for implementing a continuous normalising flow, including Diffrax, Equinox, JAX, and visualization tools.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/continuous_normalising_flow.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport math\nimport os\nimport pathlib\nimport time\n\nimport diffrax\nimport equinox as eqx  # https://github.com/patrick-kidger/equinox\nimport imageio\nimport jax\nimport jax.lax as lax\nimport jax.nn as jnn\nimport jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib.pyplot as plt\nimport optax  # https://github.com/deepmind/optax\nimport scipy.stats as stats\n\n\nhere = pathlib.Path(os.getcwd())\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Imports necessary Python libraries including Diffrax, Equinox, JAX, and visualization tools\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/neural_sde.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Union\n\nimport diffrax\nimport equinox as eqx  # https://github.com/patrick-kidger/equinox\nimport jax\nimport jax.nn as jnn\nimport jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib.pyplot as plt\nimport optax  # https://github.com/deepmind/optax\n```\n\n----------------------------------------\n\nTITLE: Linear Extrapolation for Third Stage\nDESCRIPTION: Formula showing the linear extrapolation used to compute the predictor for the third stage.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/devdocs/predictor_dirk.md#2025-04-21_snippet_2\n\nLANGUAGE: latex\nCODE:\n```\nF(c_3 Δt) = f_2 + (f_2 - f_1) (c_3 - c_2) / (c_2 - c_1)\n```\n\n----------------------------------------\n\nTITLE: Citing CDE Interpolation Scheme Paper in BibTeX\nDESCRIPTION: BibTeX citation for a paper investigating the choice of interpolation scheme for Controlled Differential Equations, which is relevant for understanding interpolation in this context.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/interpolation.md#2025-04-21_snippet_1\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{morrill2021cdeonline,\n        title={{N}eural {C}ontrolled {D}ifferential {E}quations for {O}nline {P}rediction {T}asks},\n        author={Morrill, James and Kidger, Patrick and Yang, Lingyi and Lyons, Terry},\n        journal={arXiv:2106.11028},\n        year={2021}\n}\n```\n\n----------------------------------------\n\nTITLE: Citing Neural CDE Paper in BibTeX\nDESCRIPTION: BibTeX citation for the original Neural Controlled Differential Equations paper, which is a key reference for using interpolation with controlled differential equations.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/interpolation.md#2025-04-21_snippet_0\n\nLANGUAGE: bibtex\nCODE:\n```\n@article{kidger2020neuralcde,\n        author={Kidger, Patrick and Morrill, James and Foster, James and Lyons, Terry},\n        title={{N}eural {C}ontrolled {D}ifferential {E}quations for {I}rregular {T}ime {S}eries},\n        journal={Neural Information Processing Systems},\n        year={2020},\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Ito SDE in LaTeX\nDESCRIPTION: Mathematical representation of an Ito stochastic differential equation using LaTeX notation. It defines the drift term μ and diffusion term σ.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/devdocs/adjoint_commutative_noise.md#2025-04-21_snippet_0\n\nLANGUAGE: latex\nCODE:\n```\n\\mathrm{d}y(t) = μ(t, y(t))\\mathrm{d}t + σ(t, y(t)) \\mathrm{d}w(t)\n```\n\n----------------------------------------\n\nTITLE: Correct Usage of BacksolveAdjoint to Avoid CustomVJPException\nDESCRIPTION: This example shows the correct way to use BacksolveAdjoint to avoid CustomVJPException. The model is properly included in the PyTree structure of terms, allowing for correct gradient computation.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/further_details/faq.md#2025-04-21_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom diffrax import BacksolveAdjoint, diffeqsolve, Euler, ODETerm\nimport equinox as eqx\nimport jax.numpy as jnp\nimport jax.random as jr\n\nmlp = eqx.nn.MLP(1, 1, 8, 2, key=jr.PRNGKey(0))\n\nclass VectorField(eqx.Module):\n    model: eqx.Module\n\n    def __call__(self, t, y, args):\n        return self.model(y)\n\n@eqx.filter_jit\n@eqx.filter_value_and_grad\ndef run(model):\n  f = VectorField(model)\n  sol = diffeqsolve(ODETerm(f), Euler(), 0, 1, 0.1, jnp.array([1.0]), adjoint=BacksolveAdjoint())\n  return jnp.sum(sol.ys)\n\nrun(mlp)\n```\n\n----------------------------------------\n\nTITLE: DirectAdjoint Implementation\nDESCRIPTION: Direct adjoint method implementation for gradient computation, marked as not recommended\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/api/adjoints.md#2025-04-21_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n::: diffrax.DirectAdjoint\n    selection:\n        members: false\n```\n\n----------------------------------------\n\nTITLE: Configuring Matplotlib Plot and Display\nDESCRIPTION: Sets up matplotlib plot axis properties, removes y-axis ticks, adds x-axis label, saves the figure to a file, and displays the plot\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/latent_ode.ipynb#2025-04-21_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nax.set_yticks([])\nax.set_xlabel(\"t\")\n\nplt.savefig(\"latent_ode.png\")\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Enabling 64-bit Precision in JAX\nDESCRIPTION: Configures JAX to use 64-bit floating-point precision, which is crucial for solving problems with small tolerances (1e-8 or smaller).\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/stiff_ode.ipynb#2025-04-21_snippet_1\n\nLANGUAGE: python\nCODE:\n```\njax.config.update(\"jax_enable_x64\", True)\n```\n\n----------------------------------------\n\nTITLE: Running Main Function with Different Parameters\nDESCRIPTION: This snippet demonstrates how to call the main function with different input images and parameters to reproduce the images displayed at the start of the script.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/continuous_normalising_flow.ipynb#2025-04-21_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nmain(in_path=\"../imgs/cat.png\")\nmain(in_path=\"../imgs/butterfly.png\", num_blocks=3)\nmain(in_path=\"../imgs/target.png\", width_size=128)\n```\n\n----------------------------------------\n\nTITLE: Dependency Requirements List\nDESCRIPTION: Lists the core Python packages required for the diffrax project including testing, optimization and scientific computing libraries.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/test/requirements.txt#2025-04-21_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nbeartype\njaxlib\noptax\npytest\nscipy\ntqdm\n```\n\n----------------------------------------\n\nTITLE: Main Function Execution\nDESCRIPTION: Simple call to execute the main function\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/latent_ode.ipynb#2025-04-21_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmain()\n```\n\n----------------------------------------\n\nTITLE: Referencing Benchmark Scripts Location in Markdown\nDESCRIPTION: This snippet provides a link to the GitHub folder containing the benchmark scripts for Diffrax.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/further_details/benchmarks.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n[benchmarks](https://github.com/patrick-kidger/diffrax/tree/main/benchmarks)\n```\n\n----------------------------------------\n\nTITLE: Running Tests for Diffrax\nDESCRIPTION: Commands to install test requirements and run the test suite using pytest. This ensures that all tests pass after making changes to the codebase.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/CONTRIBUTING.md#2025-04-21_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install -r test/requirements.txt\npytest\n```\n\n----------------------------------------\n\nTITLE: Running the Symbolic Regression Process\nDESCRIPTION: Executes the main function to perform symbolic regression and discover equations from data.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/symbolic_regression.ipynb#2025-04-21_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmain()\n```\n\n----------------------------------------\n\nTITLE: Markdown Citation Include Statement\nDESCRIPTION: MkDocs-style include statement for citation details stored in an external markdown file\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/docs/citation.md#2025-04-21_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n--8<-- \"further_details/.citation.md\"\n```\n\n----------------------------------------\n\nTITLE: Academic Citation for Diffrax\nDESCRIPTION: BibTeX citation entry for referencing the Diffrax library in academic research, pointing to a PhD thesis on Neural Differential Equations.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/README.md#2025-04-21_snippet_2\n\nLANGUAGE: bibtex\nCODE:\n```\n@phdthesis{kidger2021on,\n    title={{O}n {N}eural {D}ifferential {E}quations},\n    author={Patrick Kidger},\n    year={2021},\n    school={University of Oxford},\n}\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Symbolic Regression\nDESCRIPTION: Imports necessary libraries for implementing symbolic regression with neural differential equations. Key dependencies include equinox, JAX, optax, PySR, sympy, and sympy2jax.\nSOURCE: https://github.com/patrick-kidger/diffrax/blob/main/examples/symbolic_regression.ipynb#2025-04-21_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport tempfile\n\nimport equinox as eqx  # https://github.com/patrick-kidger/equinox\nimport jax\nimport jax.numpy as jnp\nimport optax  # https://github.com/deepmind/optax\nimport pysr  # https://github.com/MilesCranmer/PySR\nimport sympy\nimport sympy2jax  # https://github.com/google/sympy2jax\n```"
  }
]
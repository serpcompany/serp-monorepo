[
  {
    "owner": "onnx",
    "repo": "onnx",
    "content": "TITLE: ONNX Model Loading and Saving in Python\nDESCRIPTION: This snippet loads an ONNX model from a specified path and saves it to another path. It uses the `onnx` library's `load` and `save` functions. The paths are constructed using `os.path.join` to ensure cross-platform compatibility. It assumes that the input ONNX model exists at the specified path.\nSOURCE: https://github.com/onnx/onnx/blob/main/examples/save_model.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\nimport os\n\n\n# Preprocessing: load the old model\nold_model_path = os.path.join(\"resources\", \"single_relu.onnx\")\nonnx_model = onnx.load(old_model_path)\n\n# Preprocessing: get the path to the saved model\nnew_model_path = os.path.join(\"resources\", \"single_relu_new.onnx\")\n\n# Save the ONNX model\nonnx.save(onnx_model, new_model_path)\n\nprint(\"The model is saved.\")\n```\n\n----------------------------------------\n\nTITLE: Checking ONNX Model\nDESCRIPTION: This code demonstrates how to check the validity of an ONNX model using `onnx.checker.check_model()`. It first loads the model from a file and then calls the checker. It catches `onnx.checker.ValidationError` if the model is invalid. Requires the `onnx` library.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\n\n# Preprocessing: load the ONNX model\nmodel_path = \"path/to/the/model.onnx\"\nonnx_model = onnx.load(model_path)\n\nprint(f\"The model is:\\n{onnx_model}\")\n\n# Check the model\ntry:\n    onnx.checker.check_model(onnx_model)\nexcept onnx.checker.ValidationError as e:\n    print(f\"The model is invalid: {e}\")\nelse:\n    print(\"The model is valid!\")\n```\n\n----------------------------------------\n\nTITLE: Installing ONNX with pip\nDESCRIPTION: This command installs the ONNX package using pip, the Python package installer. The optional `[reference]` installs additional dependencies for the reference implementation.\nSOURCE: https://github.com/onnx/onnx/blob/main/INSTALL.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install onnx  # or pip install onnx[reference] for optional reference implementation dependencies\n```\n\n----------------------------------------\n\nTITLE: Evaluating Linear Regression Model - Python\nDESCRIPTION: This snippet demonstrates how to evaluate a simple linear regression model represented in ONNX using the ReferenceEvaluator. It defines the model using ONNX helper functions, creates input data using NumPy, and then runs the evaluation. The output of the model is printed to the console.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nimport numpy\nfrom onnx import numpy_helper, TensorProto\nfrom onnx.helper import (\n    make_model, make_node, set_model_props, make_tensor,\n    make_graph, make_tensor_value_info)\nfrom onnx.checker import check_model\nfrom onnx.reference import ReferenceEvaluator\n\nX = make_tensor_value_info('X', TensorProto.FLOAT, [None, None])\nA = make_tensor_value_info('A', TensorProto.FLOAT, [None, None])\nB = make_tensor_value_info('B', TensorProto.FLOAT, [None])\nY = make_tensor_value_info('Y', TensorProto.FLOAT, [None])\nnode1 = make_node('MatMul', ['X', 'A'], ['XA'])\nnode2 = make_node('Add', ['XA', 'B'], ['Y'])\ngraph = make_graph([node1, node2], 'lr', [X, A, B], [Y])\nonnx_model = make_model(graph)\ncheck_model(onnx_model)\n\nsess = ReferenceEvaluator(onnx_model)\n\nx = numpy.random.randn(4, 2).astype(numpy.float32)\na = numpy.random.randn(2, 1).astype(numpy.float32)\nb = numpy.random.randn(1, 1).astype(numpy.float32)\nfeeds = {'X': x, 'A': a, 'B': b}\n\nprint(sess.run(None, feeds))\n```\n\n----------------------------------------\n\nTITLE: Checking Large ONNX Model (>2GB)\nDESCRIPTION: This snippet shows how to check a large ONNX model (>2GB) using `onnx.checker.check_model()`.  It emphasizes the need to pass the model path directly to the checker and to ensure that the external data is located in the same directory.  Loading the model and then passing it to the checker will fail. Requires the `onnx` library.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\n\nonnx.checker.check_model(\"path/to/the/model.onnx\")\n# onnx.checker.check_model(loaded_onnx_model) will fail if given >2GB model\n```\n\n----------------------------------------\n\nTITLE: Loading ONNX Model\nDESCRIPTION: This code snippet demonstrates how to load an ONNX model from a file using the `onnx.load()` function. The model is loaded into memory as a `ModelProto` object. No external dependencies are required beyond the onnx library.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\n\n# onnx_model is an in-memory ModelProto\nonnx_model = onnx.load(\"path/to/the/model.onnx\")\n```\n\n----------------------------------------\n\nTITLE: Creating ONNX Model Using Helper Functions\nDESCRIPTION: This code demonstrates how to create an ONNX model from scratch using helper functions. It creates input/output value info, a Pad node, a graph, and a model. The model is then validated using `onnx.checker.check_model()`. Requires `onnx`, `onnx.helper`, `onnx.AttributeProto`, `onnx.TensorProto`, `onnx.GraphProto`.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\nfrom onnx import helper, AttributeProto, TensorProto, GraphProto\n\n# Create inputs and output value info\nX = helper.make_tensor_value_info(\"X\", TensorProto.FLOAT, [3, 2])\npads = helper.make_tensor_value_info(\"pads\", TensorProto.INT64, [8])  # pads is INT64\nY = helper.make_tensor_value_info(\"Y\", TensorProto.FLOAT, [5, 4])\n\n# Create Pad node with 'value' attribute (not input)\nnode_def = helper.make_node(\n    \"Pad\",\n    inputs=[\"X\", \"pads\"],  # Inputs: X and pads (INT64)\n    outputs=[\"Y\"],\n    mode=\"constant\",       # Attribute for padding mode\n    value=0.0              # Attribute for fill value\n)\n\n# Build graph and model\ngraph_def = helper.make_graph(\n    [node_def],\n    \"test-model\",\n    [X, pads],\n    [Y],\n)\nmodel_def = helper.make_model(\n    graph_def,\n    producer_name=\"onnx-example\",\n    opset_imports=[helper.make_opsetid(\"\", 11)]  # OPSET 11 required\n)\n\n# Validate the model\nonnx.checker.check_model(model_def)\nprint(\"Model is valid!\")\n```\n\n----------------------------------------\n\nTITLE: Loading ONNX Model from File in Python\nDESCRIPTION: This snippet loads an ONNX model from a specified file path using the `onnx.load()` function. It assumes the `onnx` package is installed and the file path is correct. The loaded model is then stored in the `onnx_model` variable.\nSOURCE: https://github.com/onnx/onnx/blob/main/examples/check_model.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\nimport os\n\n# Preprocessing: load the ONNX model\nmodel_path = os.path.join(\"resources\", \"single_relu.onnx\")\nonnx_model = onnx.load(model_path)\n```\n\n----------------------------------------\n\nTITLE: Loading an ONNX Model from a File\nDESCRIPTION: This snippet demonstrates how to load an ONNX model from a file named 'linear_regression.onnx' using the `load` function. The file is opened in binary read mode.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n    from onnx import load\n\n    with open(\"linear_regression.onnx\", \"rb\") as f:\n        onnx_model = load(f)\n\n    # display\n    print(onnx_model)\n```\n\n----------------------------------------\n\nTITLE: Building a Linear Regression ONNX Model\nDESCRIPTION: This code snippet constructs a simple linear regression model using ONNX operators. It defines inputs (X, A, B) and an output (Y), creates MatMul and Add nodes, and then combines them into an ONNX graph and model. Finally, it checks the model's consistency.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n    # imports\n\n    from onnx import TensorProto\n    from onnx.helper import (\n        make_model, make_node, make_graph,\n        make_tensor_value_info)\n    from onnx.checker import check_model\n\n    # inputs\n\n    # 'X' is the name, TensorProto.FLOAT the type, [None, None] the shape\n    X = make_tensor_value_info('X', TensorProto.FLOAT, [None, None])\n    A = make_tensor_value_info('A', TensorProto.FLOAT, [None, None])\n    B = make_tensor_value_info('B', TensorProto.FLOAT, [None, None])\n\n    # outputs, the shape is left undefined\n\n    Y = make_tensor_value_info('Y', TensorProto.FLOAT, [None])\n\n    # nodes\n\n    # It creates a node defined by the operator type MatMul,\n    # 'X', 'A' are the inputs of the node, 'XA' the output.\n    node1 = make_node('MatMul', ['X', 'A'], ['XA'])\n    node2 = make_node('Add', ['XA', 'B'], ['Y'])\n\n    # from nodes to graph\n    # the graph is built from the list of nodes, the list of inputs,\n    # the list of outputs and a name.\n\n    graph = make_graph([node1, node2],  # nodes\n                        'lr',  # a name\n                        [X, A, B],  # inputs\n                        [Y])  # outputs\n\n    # onnx graph\n    # there is no metadata in this case.\n\n    onnx_model = make_model(graph)\n\n    # Let's check the model is consistent,\n    # this function is described in section\n    # Checker and Shape Inference.\n    check_model(onnx_model)\n\n    # the work is done, let's display it...\n    print(onnx_model)\n```\n\n----------------------------------------\n\nTITLE: Loading ONNX Model from File\nDESCRIPTION: This code snippet loads an ONNX model from a specified file path using the `onnx.load()` function. It assumes that the `onnx` package is installed and the file 'single_relu.onnx' exists in the 'resources' directory. The loaded model is then printed to the console.\nSOURCE: https://github.com/onnx/onnx/blob/main/examples/load_model.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\nimport os\n\n\n# Load the ONNX model\nonnx_model = onnx.load(os.path.join(\"resources\", \"single_relu.onnx\"))\nprint(onnx_model)\n```\n\n----------------------------------------\n\nTITLE: Creating ONNX Models with Flexible Shapes in Python\nDESCRIPTION: This Python code snippet defines a function `create_model` that constructs an ONNX model with `MatMul` and `Add` operations. The shapes of the input tensors 'X' and 'A' are configurable using the `shapes` dictionary. The code then demonstrates several cases, including 2D x 2D, 2D x 1D, and 2D x 0D matrix operations, showcasing how ONNX Runtime handles models with varying tensor dimensions and partially unknown shapes during inference. It utilizes `numpy` for creating random input tensors and `onnx` and `onnxruntime` for model creation, validation and inference.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_32\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy\nfrom onnx import numpy_helper, TensorProto, FunctionProto\nfrom onnx.helper import (\n    make_model, make_node, set_model_props, make_tensor,\n    make_graph, make_tensor_value_info, make_opsetid,\n    make_function)\nfrom onnx.checker import check_model\nfrom onnxruntime import InferenceSession\n\ndef create_model(shapes):\n    new_domain = 'custom'\n    opset_imports = [make_opsetid(\"\", 14), make_opsetid(new_domain, 1)]\n\n    node1 = make_node('MatMul', ['X', 'A'], ['XA'])\n    node2 = make_node('Add', ['XA', 'A'], ['Y'])\n\n    X = make_tensor_value_info('X', TensorProto.FLOAT, shapes['X'])\n    A = make_tensor_value_info('A', TensorProto.FLOAT, shapes['A'])\n    Y = make_tensor_value_info('Y', TensorProto.FLOAT, shapes['Y'])\n\n    graph = make_graph([node1, node2], 'example', [X, A], [Y])\n\n    onnx_model = make_model(graph, opset_imports=opset_imports)\n    # Let models runnable by onnxruntime with a released ir_version\n    onnx_model.ir_version = 8\n\n    return onnx_model\n\nprint(\"----------- case 1: 2D x 2D -> 2D\")\nonnx_model = create_model({'X': [None, None], 'A': [None, None], 'Y': [None, None]})\ncheck_model(onnx_model)\nsess = InferenceSession(onnx_model.SerializeToString(),\n                        providers=[\"CPUExecutionProvider\"])\nres = sess.run(None, {\n    'X': numpy.random.randn(2, 2).astype(numpy.float32),\n    'A': numpy.random.randn(2, 2).astype(numpy.float32)})\nprint(res)\n\nprint(\"----------- case 2: 2D x 1D -> 1D\")\nonnx_model = create_model({'X': [None, None], 'A': [None], 'Y': [None]})\ncheck_model(onnx_model)\nsess = InferenceSession(onnx_model.SerializeToString(),\n                        providers=[\"CPUExecutionProvider\"])\nres = sess.run(None, {\n    'X': numpy.random.randn(2, 2).astype(numpy.float32),\n    'A': numpy.random.randn(2).astype(numpy.float32)})\nprint(res)\n\nprint(\"----------- case 3: 2D x 0D -> 0D\")\nonnx_model = create_model({'X': [None, None], 'A': [], 'Y': []})\ncheck_model(onnx_model)\ntry:\n    InferenceSession(onnx_model.SerializeToString(),\n                     providers=[\"CPUExecutionProvider\"])\nexcept Exception as e:\n    print(e)\n\nprint(\"----------- case 4: 2D x None -> None\")\nonnx_model = create_model({'X': [None, None], 'A': None, 'Y': None})\ntry:\n    check_model(onnx_model)\nexcept Exception as e:\n    print(type(e), e)\nsess = InferenceSession(onnx_model.SerializeToString(),\n                        providers=[\"CPUExecutionProvider\"])\nres = sess.run(None, {\n    'X': numpy.random.randn(2, 2).astype(numpy.float32),\n    'A': numpy.random.randn(2).astype(numpy.float32)})\nprint(res)\nprint(\"----------- end\")\n```\n\n----------------------------------------\n\nTITLE: Saving ONNX Model\nDESCRIPTION: This snippet shows how to save an ONNX model to a file using `onnx.save()`. The input is an in-memory `ModelProto` object. It requires the `onnx` library.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\n\n# onnx_model is an in-memory ModelProto\nonnx_model = ...\n\n# Save the ONNX model\nonnx.save(onnx_model, \"path/to/the/model.onnx\")\n```\n\n----------------------------------------\n\nTITLE: Updating ONNX Model Input/Output Dimensions\nDESCRIPTION: This snippet demonstrates how to update the input and output dimensions of an ONNX model using `onnx.tools.update_model_dims.update_inputs_outputs_dims`. It allows specifying both static and dynamic dimension sizes. Requires the `onnx` and `onnx.tools.update_model_dims` libraries. The function automatically runs a model checker after updating the dimensions.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\nfrom onnx.tools import update_model_dims\n\nmodel = onnx.load(\"path/to/the/model.onnx\")\n# Here both \"seq\", \"batch\" and -1 are dynamic using dim_param.\nvariable_length_model = update_model_dims.update_inputs_outputs_dims(model, {\"input_name\": [\"seq\", \"batch\", 3, -1]}, {\"output_name\": [\"seq\", \"batch\", 1, -1]})\n```\n\n----------------------------------------\n\nTITLE: ONNX If Operator Implementation in Python\nDESCRIPTION: This code snippet demonstrates how to implement the ONNX If operator using the onnx and onnxruntime libraries in Python. It builds an ONNX model that takes a matrix as input, calculates the sum of its elements, and then uses an If node to return either 1 or -1 based on whether the sum is positive or negative. The ONNX model is then saved to a file and executed using onnxruntime to verify its correctness. The initializers, input/output tensor value info are initialized and then the then and else bodies are created. After that, the final model is built and executed.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nimport numpy\nimport onnx\nfrom onnx.helper import (\n    make_node, make_graph, make_model, make_tensor_value_info)\nfrom onnx.numpy_helper import from_array\nfrom onnx.checker import check_model\nfrom onnxruntime import InferenceSession\n\n# initializers\nvalue = numpy.array([0], dtype=numpy.float32)\nzero = from_array(value, name='zero')\n\n# Same as before, X is the input, Y is the output.\nX = make_tensor_value_info('X', onnx.TensorProto.FLOAT, [None, None])\nY = make_tensor_value_info('Y', onnx.TensorProto.FLOAT, [None])\n\n# The node building the condition. The first one\n# sum over all axes.\nrsum = make_node('ReduceSum', ['X'], ['rsum'])\n# The second compares the result to 0.\ncond = make_node('Greater', ['rsum', 'zero'], ['cond'])\n\n# Builds the graph is the condition is True.\n# Input for then\nthen_out = make_tensor_value_info(\n    'then_out', onnx.TensorProto.FLOAT, None)\n# The constant to return.\nthen_cst = from_array(numpy.array([1]).astype(numpy.float32))\n\n# The only node.\nthen_const_node = make_node(\n    'Constant', inputs=[],\n    outputs=['then_out'],\n    value=then_cst, name='cst1')\n\n# And the graph wrapping these elements.\nthen_body = make_graph(\n    [then_const_node], 'then_body', [], [then_out])\n\n# Same process for the else branch.\nelse_out = make_tensor_value_info(\n    'else_out', onnx.TensorProto.FLOAT, [5])\nelse_cst = from_array(numpy.array([-1]).astype(numpy.float32))\n\nelse_const_node = make_node(\n    'Constant', inputs=[],\n    outputs=['else_out'],\n    value=else_cst, name='cst2')\n\nelse_body = make_graph(\n    [else_const_node], 'else_body',\n    [], [else_out])\n\n# Finally the node If taking both graphs as attributes.\nif_node = onnx.helper.make_node(\n    'If', ['cond'], ['Y'],\n    then_branch=then_body,\n    else_branch=else_body)\n\n# The final graph.\ngraph = make_graph([rsum, cond, if_node], 'if', [X], [Y], [zero])\nonnx_model = make_model(graph)\ncheck_model(onnx_model)\n\n# Let's freeze the opset.\ndel onnx_model.opset_import[:]\nopset = onnx_model.opset_import.add()\nopset.domain = ''\nopset.version = 15\nonnx_model.ir_version = 8\n\n# Save.\nwith open(\"onnx_if_sign.onnx\", \"wb\") as f:\n    f.write(onnx_model.SerializeToString())\n\n# Let's see the output.\nsess = InferenceSession(onnx_model.SerializeToString(),\n                        providers=[\"CPUExecutionProvider\"])\n\nx = numpy.ones((3, 2), dtype=numpy.float32)\nres = sess.run(None, {'X': x})\n\n# It works.\nprint(\"result\", res)\nprint()\n\n# Some display.\nprint(onnx_model)\n```\n\n----------------------------------------\n\nTITLE: Manipulating TensorProto and Numpy Array\nDESCRIPTION: This code demonstrates converting between NumPy arrays and TensorProto objects, as well as saving and loading TensorProto objects to/from files. It uses `numpy_helper.from_array()` and `numpy_helper.to_array()` for conversions. It also displays how to retrieve utility functions for mapping attributes in ONNX IR. Requires `numpy`, `onnx` and `onnx.numpy_helper`.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport numpy\nimport onnx\nfrom onnx import numpy_helper\n\n# Preprocessing: create a Numpy array\nnumpy_array = numpy.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], dtype=float)\nprint(f\"Original Numpy array:\\n{numpy_array}\\n\")\n\n# Convert the Numpy array to a TensorProto\ntensor = numpy_helper.from_array(numpy_array)\nprint(f\"TensorProto:\\n{tensor}\")\n\n# Convert the TensorProto to a Numpy array\nnew_array = numpy_helper.to_array(tensor)\nprint(f\"After round trip, Numpy array:\\n{new_array}\\n\")\n\n# Save the TensorProto\nwith open(\"tensor.pb\", \"wb\") as f:\n    f.write(tensor.SerializeToString())\n\n# Load a TensorProto\nnew_tensor = onnx.TensorProto()\nwith open(\"tensor.pb\", \"rb\") as f:\n    new_tensor.ParseFromString(f.read())\nprint(f\"After saving and loading, new TensorProto:\\n{new_tensor}\")\n\nfrom onnx import TensorProto, helper\n\n# Conversion utilities for mapping attributes in ONNX IR\n# The functions below are available after ONNX 1.13\nnp_dtype = helper.tensor_dtype_to_np_dtype(TensorProto.FLOAT)\nprint(f\"The converted numpy dtype for {helper.tensor_dtype_to_string(TensorProto.FLOAT)} is {np_dtype}.\")\nstorage_dtype = helper.tensor_dtype_to_storage_tensor_dtype(TensorProto.FLOAT)\nprint(f\"The storage dtype for {helper.tensor_dtype_to_string(TensorProto.FLOAT)} is {helper.tensor_dtype_to_string(storage_dtype)}.\")\nfield_name = helper.tensor_dtype_to_field(TensorProto.FLOAT)\nprint(f\"The field name for {helper.tensor_dtype_to_string(TensorProto.FLOAT)} is {field_name}.\")\ntensor_dtype = helper.np_dtype_to_tensor_dtype(np_dtype)\nprint(f\"The tensor data type for numpy dtype: {np_dtype} is {helper.tensor_dtype_to_string(tensor_dtype)}.\")\n\nfor tensor_dtype in helper.get_all_tensor_dtypes():\n    print(helper.tensor_dtype_to_string(tensor_dtype))\n```\n\n----------------------------------------\n\nTITLE: Inspecting Initializers in ONNX Model - Python\nDESCRIPTION: This code shows how to access and print the initializers of an ONNX model's graph.  It iterates through the `initializer` attribute of the graph and prints each initializer's information. This depends on the onnx, numpy packages.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport numpy\nfrom onnx import numpy_helper, TensorProto\nfrom onnx.helper import (\n    make_model, make_node, make_graph,\n    make_tensor_value_info)\nfrom onnx.checker import check_model\n\n# initializers\nvalue = numpy.array([0.5, -0.6], dtype=numpy.float32)\nA = numpy_helper.from_array(value, name='A')\n\nvalue = numpy.array([0.4], dtype=numpy.float32)\nC = numpy_helper.from_array(value, name='C')\n\n# the part which does not change\nX = make_tensor_value_info('X', TensorProto.FLOAT, [None, None])\nY = make_tensor_value_info('Y', TensorProto.FLOAT, [None])\nnode1 = make_node('MatMul', ['X', 'A'], ['AX'])\nnode2 = make_node('Add', ['AX', 'C'], ['Y'])\ngraph = make_graph([node1, node2], 'lr', [X], [Y], [A, C])\nonnx_model = make_model(graph)\ncheck_model(onnx_model)\n\nprint('** initializer **')\nfor init in onnx_model.graph.initializer:\n    print(init)\n```\n\n----------------------------------------\n\nTITLE: Inferring Shapes in ONNX Model\nDESCRIPTION: This snippet applies shape inference to the ONNX model created in the previous step. The shape_inference.infer_shapes function infers the shapes of all intermediate tensors in the model. After shape inference, the model is checked again, and the shape information of 'Y' is printed to show the inferred shape.\nSOURCE: https://github.com/onnx/onnx/blob/main/examples/shape_inference.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Apply shape inference on the model\ninferred_model = shape_inference.infer_shapes(original_model)\n\n# Check the model and print Y's shape information\nonnx.checker.check_model(inferred_model)\nprint(\n    \"After shape inference, the shape info of Y is:\\n{}\".format(\n        inferred_model.graph.value_info\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Extracting Sub-model in ONNX\nDESCRIPTION: This snippet shows how to extract a sub-model from an ONNX model based on specified input and output tensor names. It requires the `onnx` library and the `input_path`, `output_path`, `input_names`, and `output_names` variables. The extracted sub-model is saved to the specified output path.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\n\ninput_path = \"path/to/the/original/model.onnx\"\noutput_path = \"path/to/save/the/extracted/model.onnx\"\ninput_names = [\"input_0\", \"input_1\", \"input_2\"]\noutput_names = [\"output_0\", \"output_1\"]\n\nonnx.utils.extract_model(input_path, output_path, input_names, output_names)\n```\n\n----------------------------------------\n\nTITLE: Creating ONNX Model with Initializers - Python\nDESCRIPTION: This code demonstrates how to create an ONNX model where the coefficients of a linear regression are part of the model as initializers rather than inputs. It uses `numpy_helper.from_array` to convert numpy arrays into ONNX tensors and then constructs the graph with these initializers. It depends on the onnx, numpy packages.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport numpy\nfrom onnx import numpy_helper, TensorProto\nfrom onnx.helper import (\n    make_model, make_node, make_graph,\n    make_tensor_value_info)\nfrom onnx.checker import check_model\n\n# initializers\nvalue = numpy.array([0.5, -0.6], dtype=numpy.float32)\nA = numpy_helper.from_array(value, name='A')\n\nvalue = numpy.array([0.4], dtype=numpy.float32)\nC = numpy_helper.from_array(value, name='C')\n\n# the part which does not change\nX = make_tensor_value_info('X', TensorProto.FLOAT, [None, None])\nY = make_tensor_value_info('Y', TensorProto.FLOAT, [None])\nnode1 = make_node('MatMul', ['X', 'A'], ['AX'])\nnode2 = make_node('Add', ['AX', 'C'], ['Y'])\ngraph = make_graph([node1, node2], 'lr', [X], [Y], [A, C])\nonnx_model = make_model(graph)\ncheck_model(onnx_model)\n\nprint(onnx_model)\n```\n\n----------------------------------------\n\nTITLE: Drawing ONNX Graph with Pydot\nDESCRIPTION: This code snippet demonstrates how to use `GetPydotGraph` to create a visual representation of an ONNX model's graph. It takes an ONNX model (ModelProto instance), graph name, rank direction, and a node producer (here, `GetOpNodeProducer` which uses docstrings for node labels) as input. The resulting Pydot graph is then written to a DOT file.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/api/tools.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom onnx.tools.net_drawer import GetPydotGraph, GetOpNodeProducer\n\npydot_graph = GetPydotGraph(\n    model_onnx.graph,  # model_onnx is a ModelProto instance\n    name=model_onnx.graph.name,\n    rankdir=\"TP\",\n    node_producer=GetOpNodeProducer(\"docstring\"))\npydot_graph.write_dot(\"graph.dot\")\n```\n\n----------------------------------------\n\nTITLE: Loading ONNX Model with External Data (Same Directory)\nDESCRIPTION: This snippet demonstrates loading an ONNX model with external data located in the same directory as the model file. It uses `onnx.load()` to load the model. The ONNX library handles loading the external data automatically.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\n\nonnx_model = onnx.load(\"path/to/the/model.onnx\")\n```\n\n----------------------------------------\n\nTITLE: Converting and Saving an ONNX Model to External Data Directly\nDESCRIPTION: This snippet shows how to convert and save an ONNX model to external data in a single step using `onnx.save_model()` with the `save_as_external_data=True` argument. Requires the `onnx` library.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/ExternalData.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\n\nonnx_model = ... # Your model in memory as ModelProto\nonnx.save_model(onnx_model, \"path/to/save/the/model.onnx\", save_as_external_data=True, all_tensors_to_one_file=True, location=\"filename\", size_threshold=1024, convert_attribute=False)\n# Then the onnx_model has converted raw data as external data and saved to specific directory\n```\n\n----------------------------------------\n\nTITLE: Check ONNX Model\nDESCRIPTION: Checks the ONNX model for validity using `onnx.checker.check_model()`.  The code also prints the producer name and graph representation.\nSOURCE: https://github.com/onnx/onnx/blob/main/examples/make_model.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(\"The producer_name in model: {}\\n\".format(model_def.producer_name))\nprint(\"The graph in model:\\n{}\".format(model_def.graph))\nonnx.checker.check_model(model_def)\nprint(\"The model is checked!\")\n```\n\n----------------------------------------\n\nTITLE: Infer Shapes with C++ API in ONNX\nDESCRIPTION: This C++ code snippet demonstrates how to invoke the shape inference functionality on an ONNX model using the InferShapes function. It takes a ModelProto and an optional schema registry as input and annotates the model in-place with shape information. The schema registry can be null if no custom schemas are used.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/ShapeInference.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\nshape_inference::InferShapes(\n    ModelProto& m,\n    const ISchemaRegistry* schema_registry);\n```\n\n----------------------------------------\n\nTITLE: String to Int Label Encoding using LabelEncoder\nDESCRIPTION: This snippet demonstrates how to use the LabelEncoder to map string labels to integer values. It creates an ONNX node using `onnx.helper.make_node`, specifying string keys and integer values, and tests it with an input array of strings.  If a string is not in the `keys_strings`, the `default_int64` attribute is used, if available.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/Operators-ml.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nnode = onnx.helper.make_node(\n    \"LabelEncoder\",\n    inputs=[\"X\"],\n    outputs=[\"Y\"],\n    domain=\"ai.onnx.ml\",\n    keys_strings=[\"a\", \"b\", \"c\"],\n    values_int64s=[0, 1, 2],\n    default_int64=42,\n)\nx = np.array([\"a\", \"b\", \"d\", \"c\", \"g\"]).astype(object)\ny = np.array([0, 1, 42, 2, 42]).astype(np.int64)\nexpect(\n    node,\n    inputs=[x],\n    outputs=[y],\n    name=\"test_ai_onnx_ml_label_encoder_string_int\",\n)\n\nnode = onnx.helper.make_node(\n    \"LabelEncoder\",\n    inputs=[\"X\"],\n    outputs=[\"Y\"],\n    domain=\"ai.onnx.ml\",\n    keys_strings=[\"a\", \"b\", \"c\"],\n    values_int64s=[0, 1, 2],\n)\nx = np.array([\"a\", \"b\", \"d\", \"c\", \"g\"]).astype(object)\ny = np.array([0, 1, -1, 2, -1]).astype(np.int64)\nexpect(\n    node,\n    inputs=[x],\n    outputs=[y],\n    name=\"test_ai_onnx_ml_label_encoder_string_int_no_default\",\n)\n```\n\n----------------------------------------\n\nTITLE: Evaluating Custom Node Linear Regression - Python\nDESCRIPTION: This example demonstrates the evaluation of a linear regression model with a custom node (EyeLike) using the ReferenceEvaluator. It defines the model with the EyeLike node, generates random inputs, and then runs the evaluation. The custom node adds an identity matrix to the input 'A'. The model is also serialized to a file.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_28\n\nLANGUAGE: python\nCODE:\n```\nimport numpy\nfrom onnx import numpy_helper, TensorProto\nfrom onnx.helper import (\n    make_model, make_node, set_model_props, make_tensor,\n    make_graph, make_tensor_value_info)\nfrom onnx.checker import check_model\nfrom onnx.reference import ReferenceEvaluator\n\nX = make_tensor_value_info('X', TensorProto.FLOAT, [None, None])\nA = make_tensor_value_info('A', TensorProto.FLOAT, [None, None])\nB = make_tensor_value_info('B', TensorProto.FLOAT, [None])\nY = make_tensor_value_info('Y', TensorProto.FLOAT, [None])\nnode0 = make_node('EyeLike', ['A'], ['Eye'])\nnode1 = make_node('Add', ['A', 'Eye'], ['A1'])\nnode2 = make_node('MatMul', ['X', 'A1'], ['XA1'])\nnode3 = make_node('Add', ['XA1', 'B'], ['Y'])\ngraph = make_graph([node0, node1, node2, node3], 'lr', [X, A, B], [Y])\nonnx_model = make_model(graph)\ncheck_model(onnx_model)\nwith open(\"linear_regression.onnx\", \"wb\") as f:\n    f.write(onnx_model.SerializeToString())\n\nsess = ReferenceEvaluator(onnx_model, verbose=2)\n\nx = numpy.random.randn(4, 2).astype(numpy.float32)\na = numpy.random.randn(2, 2).astype(numpy.float32) / 10\nb = numpy.random.randn(1, 2).astype(numpy.float32)\nfeeds = {'X': x, 'A': a, 'B': b}\n\nprint(sess.run(None, feeds))\n```\n\n----------------------------------------\n\nTITLE: Merging ONNX Models with Explicit Outputs\nDESCRIPTION: This snippet showcases merging two ONNX models and specifying explicit outputs for the combined model. This allows dropping parts of the graph that do not contribute to the specified outputs. It uses `onnx.compose.merge_models` with the `outputs` parameter. `model1`, `model2`, and `io_map` must be defined before use.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\n\n# Default case. Include all outputs in the combined model\ncombined_model = onnx.compose.merge_models(\n    model1, model2,\n    io_map=[(\"C\", \"X\"), (\"C\", \"Y\")],\n)  # outputs: \"D\", \"Z\"\n\n# Explicit outputs. \"Y\" output and the Sub node are not present in the combined model\ncombined_model = onnx.compose.merge_models(\n    model1, model2,\n    io_map=[(\"C\", \"X\"), (\"C\", \"Y\")],\n    outputs=[\"Z\"],\n)  # outputs: \"Z\"\n```\n\n----------------------------------------\n\nTITLE: Infer Shapes of ONNX Model in Python\nDESCRIPTION: This code demonstrates how to infer the shapes of tensors in an ONNX model using `onnx.shape_inference.infer_shapes`. It parses an ONNX model from a string and then infers the shapes of the intermediate tensors. The inferred model includes shape information in the `value_info` attribute.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nimport onnx.parser\nfrom onnx import helper, shape_inference\n\ninput = '''\n    <\n        ir_version: 8,\n        opset_import: [ \"\" : 15]\n    >\n    agraph (float[I,4] X, float[4,2] A, float[4] B) => (float[I] Y) {\n        XA = MatMul(X, A)\n        Y = Add(XA, B)\n    }\n    '''\nonnx_model = onnx.parser.parse_model(input)\ninferred_model = shape_inference.infer_shapes(onnx_model)\n\nprint(inferred_model)\n```\n\n----------------------------------------\n\nTITLE: ArrayFeatureExtractor ONNX Node Creation and Expectation\nDESCRIPTION: This code snippet creates an ONNX node for the ArrayFeatureExtractor operator, defines sample input data (x and y), and calculates the expected output (z). It then uses the 'expect' function to verify the node's behavior with the given inputs and expected output. The node selects elements from the input tensor 'x' based on the indices provided in 'y'.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/Operators-ml.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nnode = onnx.helper.make_node(\n    \"ArrayFeatureExtractor\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n    domain=\"ai.onnx.ml\",\n)\n\nx = np.arange(12).reshape((3, 4)).astype(np.float32)\ny = np.array([0, 1], dtype=np.int64)\nz = np.array([[0, 4, 8], [1, 5, 9]], dtype=np.float32).T\nexpect(\n    node,\n    inputs=[x, y],\n    outputs=[z],\n    name=\"test_ai_onnx_ml_array_feature_extractor\",\n)\n```\n\n----------------------------------------\n\nTITLE: Printing ONNX Model in Python\nDESCRIPTION: This snippet prints the loaded ONNX model to the console, displaying its structure and attributes. It utilizes the `print()` function along with string formatting to present the model's content.\nSOURCE: https://github.com/onnx/onnx/blob/main/examples/check_model.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nprint(\"The model is:\\n{}\".format(onnx_model))\n```\n\n----------------------------------------\n\nTITLE: Verifying ONNX Installation\nDESCRIPTION: This command imports the ONNX package in Python to verify that it has been installed correctly.\nSOURCE: https://github.com/onnx/onnx/blob/main/INSTALL.md#_snippet_14\n\nLANGUAGE: sh\nCODE:\n```\npython -c \"import onnx\"\n```\n\n----------------------------------------\n\nTITLE: Converting and Saving ONNX Model to External Data\nDESCRIPTION: This code demonstrates converting an ONNX model to external data and saving it to a file in a single step using `onnx.save_model()`. It takes an in-memory `ModelProto` as input and specifies options for external data storage. Requires the `onnx` library.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\n\n# onnx_model is an in-memory ModelProto\nonnx_model = ...\nonnx.save_model(onnx_model, \"path/to/save/the/model.onnx\", save_as_external_data=True, all_tensors_to_one_file=True, location=\"filename\", size_threshold=1024, convert_attribute=False)\n# Then the onnx_model has converted raw data as external data and saved to specific directory\n```\n\n----------------------------------------\n\nTITLE: Building KNN regressor with ONNX Scan operator\nDESCRIPTION: This code constructs an ONNX model for a K-Nearest Neighbors (KNN) regressor using the Scan operator.  It defines a subgraph to compute pairwise distances and the main graph to find the top K nearest neighbors using operators like Scan, TopK, and ArrayFeatureExtractor. The final model is saved to a file.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nimport numpy\nfrom onnx import numpy_helper, TensorProto\nfrom onnx.helper import (\n    make_model, make_node, set_model_props, make_tensor, make_graph,\n    make_tensor_value_info)\nfrom onnx.checker import check_model\n\n# subgraph\ninitializers = []\nnodes = []\ninputs = []\noutputs = []\n\nvalue = make_tensor_value_info('next_in', 1, [None, 4])\ninputs.append(value)\nvalue = make_tensor_value_info('next', 1, [None])\ninputs.append(value)\n\nvalue = make_tensor_value_info('next_out', 1, [None, None])\noutputs.append(value)\nvalue = make_tensor_value_info('scan_out', 1, [None])\noutputs.append(value)\n\nnode = make_node(\n    'Identity', ['next_in'], ['next_out'],\n    name='cdistd_17_Identity', domain='')\nnodes.append(node)\n\nnode = make_node(\n    'Sub', ['next_in', 'next'], ['cdistdf_17_C0'],\n    name='cdistdf_17_Sub', domain='')\nnodes.append(node)\n\nnode = make_node(\n    'ReduceSumSquare', ['cdistdf_17_C0'], ['cdistdf_17_reduced0'],\n    name='cdistdf_17_ReduceSumSquare', axes=[1], keepdims=0, domain='')\nnodes.append(node)\n\nnode = make_node(\n    'Identity', ['cdistdf_17_reduced0'],\n    ['scan_out'], name='cdistdf_17_Identity', domain='')\nnodes.append(node)\n\ngraph = make_graph(nodes, 'OnnxIdentity',\n                   inputs, outputs, initializers)\n\n# main graph\n\ninitializers = []\nnodes = []\ninputs = []\noutputs = []\n\nopsets = {'': 15, 'ai.onnx.ml': 15}\ntarget_opset = 15  # subgraphs\n\n# initializers\nlist_value = [23.29599822460675, -120.86516699239603, -144.70495899914215, -260.08772982740413,\n              154.65272105889147, -122.23295157108991, 247.45232560871727, -182.83789715805776,\n              -132.92727431421793, 147.48710175784703, 88.27761768038069, -14.87785569894749,\n              111.71487894705504, 301.0518319089629, -29.64235742280055, -113.78493504731911,\n              -204.41218591022718, 112.26561056133608, 66.04032954135549,\n              -229.5428380626701, -33.549262642481615, -140.95737409864623, -87.8145187836131,\n              -90.61397011283958, 57.185488100413366, 56.864151796743855, 77.09054590340892,\n              -187.72501631246712, -42.779503579806025, -21.642642730674076, -44.58517761667535,\n              78.56025104939847, -23.92423223842056, 234.9166231927213, -73.73512816431007,\n              -10.150864499514297, -70.37105466673813, 65.5755688281476, 108.68676290979731, -78.36748960443065]\nvalue = numpy.array(list_value, dtype=numpy.float64).reshape((2, 20))\ntensor = numpy_helper.from_array(\n    value, name='knny_ArrayFeatureExtractorcst')\ninitializers.append(tensor)\n\nlist_value = [1.1394007205963135, -0.6848101019859314, -1.234825849533081, 0.4023416340351105,\n              0.17742614448070526, 0.46278226375579834, -0.4017809331417084, -1.630198359489441,\n              -0.5096521973609924, 0.7774903774261475, -0.4380742907524109, -1.2527953386306763,\n              -1.0485529899597168, 1.950775384902954, -1.420017957687378, -1.7062702178955078,\n              1.8675580024719238, -0.15135720372200012, -0.9772778749465942, 0.9500884413719177,\n              -2.5529897212982178, -0.7421650290489197, 0.653618574142456, 0.8644362092018127,\n              1.5327792167663574, 0.37816253304481506, 1.4693588018417358, 0.154947429895401,\n              -0.6724604368209839, -1.7262825965881348, -0.35955315828323364, -0.8131462931632996,\n              -0.8707971572875977, 0.056165341287851334, -0.5788496732711792, -0.3115525245666504,\n              1.2302906513214111, -0.302302747964859, 1.202379822731018, -0.38732680678367615,\n              2.269754648208618, -0.18718385696411133, -1.4543657302856445, 0.04575851559638977,\n              -0.9072983860969543, 0.12898291647434235, 0.05194539576768875, 0.7290905714035034,\n              1.4940791130065918, -0.8540957570075989, -0.2051582634449005, 0.3130677044391632,\n              1.764052391052246, 2.2408931255340576, 0.40015721321105957, 0.978738009929657,\n              0.06651721894741058, -0.3627411723136902, 0.30247190594673157, -0.6343221068382263,\n              -0.5108051300048828, 0.4283318817615509, -1.18063223361969, -0.02818222902715206,\n              -1.6138978004455566, 0.38690251111984253, -0.21274028718471527, -0.8954665660858154,\n              0.7610377073287964, 0.3336743414402008, 0.12167501449584961, 0.44386324286460876,\n              -0.10321885347366333, 1.4542734622955322, 0.4105985164642334, 0.14404356479644775,\n              -0.8877857327461243, 0.15634897351264954, -1.980796456336975, -0.34791216254234314]\nvalue = numpy.array(list_value, dtype=numpy.float32).reshape((20, 4))\ntensor = numpy_helper.from_array(value, name='Sc_Scancst')\ninitializers.append(tensor)\n\nvalue = numpy.array([2], dtype=numpy.int64)\ntensor = numpy_helper.from_array(value, name='To_TopKcst')\ninitializers.append(tensor)\n\nvalue = numpy.array([2, -1, 2], dtype=numpy.int64)\ntensor = numpy_helper.from_array(value, name='knny_Reshapecst')\ninitializers.append(tensor)\n\n# inputs\nvalue = make_tensor_value_info('input', 1, [None, 4])\ninputs.append(value)\n\n# outputs\nvalue = make_tensor_value_info('variable', 1, [None, 2])\noutputs.append(value)\n\n# nodes\n\nnode = make_node(\n    'Scan', ['input', 'Sc_Scancst'], ['UU032UU', 'UU033UU'],\n    name='Sc_Scan', body=graph, num_scan_inputs=1, domain='')\nnodes.append(node)\n\nnode = make_node(\n    'Transpose', ['UU033UU'], ['Tr_transposed0'],\n    name='Tr_Transpose', perm=[1, 0], domain='')\nnodes.append(node)\n\nnode = make_node(\n    'Sqrt', ['Tr_transposed0'], ['Sq_Y0'],\n    name='Sq_Sqrt', domain='')\nnodes.append(node)\n\nnode = make_node(\n    'TopK', ['Sq_Y0', 'To_TopKcst'], ['To_Values0', 'To_Indices1'],\n    name='To_TopK', largest=0, sorted=1, domain='')\nnodes.append(node)\n\nnode = make_node(\n    'Flatten', ['To_Indices1'], ['knny_output0'],\n    name='knny_Flatten', domain='')\nnodes.append(node)\n\nnode = make_node(\n    'ArrayFeatureExtractor',\n    ['knny_ArrayFeatureExtractorcst', 'knny_output0'], ['knny_Z0'],\n    name='knny_ArrayFeatureExtractor', domain='ai.onnx.ml')\nnodes.append(node)\n\nnode = make_node(\n    'Reshape', ['knny_Z0', 'knny_Reshapecst'], ['knny_reshaped0'],\n    name='knny_Reshape', allowzero=0, domain='')\nnodes.append(node)\n\nnode = make_node(\n    'Transpose', ['knny_reshaped0'], ['knny_transposed0'],\n    name='knny_Transpose', perm=[1, 0, 2], domain='')\nnodes.append(node)\n\nnode = make_node(\n    'Cast', ['knny_transposed0'], ['Ca_output0'],\n    name='Ca_Cast', to=TensorProto.FLOAT, domain='')\nnodes.append(node)\n\nnode = make_node(\n    'ReduceMean', ['Ca_output0'], ['variable'],\n    name='Re_ReduceMean', axes=[2], keepdims=0, domain='')\nnodes.append(node)\n\n# graph\ngraph = make_graph(nodes, 'KNN regressor', inputs, outputs, initializers)\n\n# model\nonnx_model = make_model(graph)\nonnx_model.ir_version = 8\nonnx_model.producer_name = 'skl2onnx'\nonnx_model.producer_version = ''\nonnx_model.domain = 'ai.onnx'\nonnx_model.model_version = 0\nonnx_model.doc_string = ''\nset_model_props(onnx_model, {})\n\n# opsets\ndel onnx_model.opset_import[:]\nfor dom, value in opsets.items():\n    op_set = onnx_model.opset_import.add()\n    op_set.domain = dom\n    op_set.version = value\n\ncheck_model(onnx_model)\nwith open(\"knnr.onnx\", \"wb\") as f:\n    f.write(onnx_model.SerializeToString())\n\nprint(onnx_model)\n```\n\n----------------------------------------\n\nTITLE: ONNX ReferenceEvaluator Usage\nDESCRIPTION: Demonstrates basic usage of the ReferenceEvaluator class to load an ONNX model and run it with input data. It imports necessary libraries, loads the model, and executes it.\nSOURCE: https://github.com/onnx/onnx/blob/main/onnx/reference/README.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom onnx.reference import ReferenceEvaluator\n\nX = np.array(...)\nsess = ReferenceEvaluator(\"model.onnx\")\nresults = sess.run(None, {\"X\": X})\nprint(results[0])\n```\n\n----------------------------------------\n\nTITLE: Printing ONNX IR Version and Opset - Python\nDESCRIPTION: This code loads an ONNX model from a file and then prints the IR version and the domain and version of each opset imported by the model. It depends on the `onnx` package and assumes a file named 'linear_regression.onnx' exists. It loops through the `opset_import` list and prints the domain and version for each opset.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom onnx import load\n\nwith open(\"linear_regression.onnx\", \"rb\") as f:\n    onnx_model = load(f)\n\nprint(\"ir_version:\", onnx_model.ir_version)\nfor opset in onnx_model.opset_import:\n    print(\"opset domain=%r version=%r\" % (opset.domain, opset.version))\n```\n\n----------------------------------------\n\nTITLE: Convert TensorProto to Numpy Array\nDESCRIPTION: Converts an ONNX TensorProto back to a NumPy array using `numpy_helper.to_array()`. This demonstrates a round trip conversion. The resulting NumPy array is then printed to the console.\nSOURCE: https://github.com/onnx/onnx/blob/main/examples/np_array_tensorproto.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Convert the TensorProto to a Numpy array\nnew_array = numpy_helper.to_array(tensor)\nprint(\"After round trip, Numpy array:\\n{}\\n\".format(numpy.array2string(numpy_array)))\n```\n\n----------------------------------------\n\nTITLE: Binarizer ONNX Node Creation and Expectation\nDESCRIPTION: This code snippet creates an ONNX node for the Binarizer operator with a specified threshold. It generates random input data (x), computes the binarized output (y) using the 'compute_binarizer' function (not shown), and uses the 'expect' function to verify the node's behavior with the input and output. The Binarizer operator maps values greater than the threshold to 1 and others to 0.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/Operators-ml.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nthreshold = 1.0\nnode = onnx.helper.make_node(\n    \"Binarizer\",\n    inputs=[\"X\"],\n    outputs=[\"Y\"],\n    threshold=threshold,\n    domain=\"ai.onnx.ml\",\n)\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = compute_binarizer(x, threshold)[0]\n\nexpect(node, inputs=[x], outputs=[y], name=\"test_ai_onnx_ml_binarizer\")\n```\n\n----------------------------------------\n\nTITLE: Checking an ONNX Model with External Data\nDESCRIPTION: This snippet demonstrates how to use `onnx.checker.check_model()` to check an ONNX model with external data. For models larger than 2GB, the model path should be passed, and the external data must be in the same directory. Requires the `onnx` library.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/ExternalData.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\n\nonnx.checker.check_model(\"path/to/the/model.onnx\")\n# onnx.checker.check_model(loaded_onnx_model) will fail if given >2GB model\n```\n\n----------------------------------------\n\nTITLE: Check ONNX Model Validity in Python\nDESCRIPTION: This code demonstrates how to check the validity of an ONNX model using `onnx.checker.check_model`. It creates a model with an incompatible input type, which results in an exception being raised by the checker.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\nimport onnx.parser\nimport onnx.checker\n\ninput = '''\n    <\n        ir_version: 8,\n        opset_import: [ \"\" : 15]\n    >\n    agraph (float[I,4] X, float[4,2] A, int[4] B) => (float[I] Y) {\n        XA = MatMul(X, A)\n        Y = Add(XA, B)\n    }\n    '''\ntry:\n    onnx_model = onnx.parser.parse_model(input)\n    onnx.checker.check_model(onnx_model)\nexcept Exception as e:\n    print(e)\n```\n\n----------------------------------------\n\nTITLE: Converting ONNX Model Version - C++\nDESCRIPTION: This C++ function converts an ONNX model from an initial opset version to a target opset version. It takes an input `ModelProto`, the initial opset version, and the target opset version as input.  It returns a new `ModelProto` after applying all relevant adapters between the initial and target versions. Header file convert.h provides a list of available passes.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/VersionConverter.md#_snippet_0\n\nLANGUAGE: C++\nCODE:\n```\nModelProto ConvertVersion(\n    const ModelProto& mp_in,\n    const OpSetID& initial_version,\n    const OpSetID& target_version);\n```\n\n----------------------------------------\n\nTITLE: Loading an ONNX Model with External Data\nDESCRIPTION: This snippet demonstrates how to load an ONNX model using `onnx.load()` when the external data is in the same directory as the model file. It imports the `onnx` library and loads the model from the specified path.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/ExternalData.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\n\nonnx_model = onnx.load(\"path/to/the/model.onnx\")\n```\n\n----------------------------------------\n\nTITLE: Converting ONNX Model Version\nDESCRIPTION: This code snippet demonstrates how to convert the version of an ONNX model using `onnx.version_converter.convert_version()`. It loads the model, converts it to a new opset version, and prints the converted model. Requires `onnx`, `onnx.version_converter`, and `onnx.helper`.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md#_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\nfrom onnx import version_converter, helper\n\n# Preprocessing: load the model to be converted.\nmodel_path = \"path/to/the/model.onnx\"\noriginal_model = onnx.load(model_path)\n\nprint(f\"The model before conversion:\\n{original_model}\")\n\n# A full list of supported adapters can be found here:\n# https://github.com/onnx/onnx/blob/main/onnx/version_converter.py#L21\n```\n\n----------------------------------------\n\nTITLE: Create Output ValueInfoProto\nDESCRIPTION: Creates an output `ValueInfoProto` object named 'Y'. 'Y' is a float tensor with shape [1, 4]. This object defines the output tensor of the model.\nSOURCE: https://github.com/onnx/onnx/blob/main/examples/make_model.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Create one output (ValueInfoProto)\nY = helper.make_tensor_value_info(\"Y\", TensorProto.FLOAT, [1, 4])\n```\n\n----------------------------------------\n\nTITLE: Running Shape Inference on ONNX Model\nDESCRIPTION: This code demonstrates how to run shape inference on an ONNX model using `onnx.shape_inference.infer_shapes()`. It creates a simple model with unknown shapes, performs shape inference, and then prints the shape information before and after inference. Requires `onnx`, `onnx.helper`, `onnx.shape_inference` and `onnx.TensorProto`.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\nfrom onnx import helper, shape_inference\nfrom onnx import TensorProto\n\n\n# Preprocessing: create a model with two nodes, Y\"s shape is unknown\nnode1 = helper.make_node(\"Transpose\", [\"X\"], [\"Y\"], perm=[1, 0, 2])\nnode2 = helper.make_node(\"Transpose\", [\"Y\"], [\"Z\"], perm=[1, 0, 2])\n\ngraph = helper.make_graph(\n    [node1, node2],\n    \"two-transposes\",\n    [helper.make_tensor_value_info(\"X\", TensorProto.FLOAT, (2, 3, 4))],\n    [helper.make_tensor_value_info(\"Z\", TensorProto.FLOAT, (2, 3, 4))],\n)\n\noriginal_model = helper.make_model(graph, producer_name=\"onnx-examples\")\n\n# Check the model and print Y\"s shape information\nonnx.checker.check_model(original_model)\nprint(f\"Before shape inference, the shape info of Y is:\\n{original_model.graph.value_info}\")\n\n# Apply shape inference on the model\ninferred_model = shape_inference.infer_shapes(original_model)\n\n# Check the model and print Y\"s shape information\nonnx.checker.check_model(inferred_model)\nprint(f\"After shape inference, the shape info of Y is:\\n{inferred_model.graph.value_info}\")\n```\n\n----------------------------------------\n\nTITLE: Creating ONNX Model with Attributes - Python\nDESCRIPTION: This code demonstrates how to create an ONNX model that includes an operator with an attribute. The example shows a Transpose node with the 'perm' attribute, which defines the permutation of axes. It utilizes the `make_node` function with named attributes. It depends on the onnx package.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom onnx import TensorProto\nfrom onnx.helper import (\n    make_model, make_node, make_graph,\n    make_tensor_value_info)\nfrom onnx.checker import check_model\n\n# unchanged\nX = make_tensor_value_info('X', TensorProto.FLOAT, [None, None])\nA = make_tensor_value_info('A', TensorProto.FLOAT, [None, None])\nB = make_tensor_value_info('B', TensorProto.FLOAT, [None, None])\nY = make_tensor_value_info('Y', TensorProto.FLOAT, [None])\n\n# added\nnode_transpose = make_node('Transpose', ['A'], ['tA'], perm=[1, 0])\n\n# unchanged except A is replaced by tA\nnode1 = make_node('MatMul', ['X', 'tA'], ['XA'])\nnode2 = make_node('Add', ['XA', 'B'], ['Y'])\n\n# node_transpose is added to the list\ngraph = make_graph([node_transpose, node1, node2],\n                   'lr', [X, A, B], [Y])\nonnx_model = make_model(graph)\ncheck_model(onnx_model)\n\n# the work is done, let's display it...\nprint(onnx_model)\n```\n\n----------------------------------------\n\nTITLE: Adding Prefix to ONNX Model Names\nDESCRIPTION: This snippet demonstrates how to add a prefix to all names (inputs, outputs, edges, nodes, initializers, sparse initializers and value infos) in an ONNX model using `onnx.compose.add_prefix`. It can be run in-place to modify the original model or create a new model with the prefixed names. Requires an ONNX model loaded from a file.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\n\nmodel = onnx.load(\"path/to/the/model.onnx\")\n# model - outputs: [\"out0\", \"out1\"], inputs: [\"in0\", \"in1\"]\n\nnew_model = onnx.compose.add_prefix(model, prefix=\"m1/\")\n# new_model - outputs: [\"m1/out0\", \"m1/out1\"], inputs: [\"m1/in0\", \"m1/in1\"]\n\n# Can also be run in-place\nonnx.compose.add_prefix(model, prefix=\"m1/\", inplace=True)\n```\n\n----------------------------------------\n\nTITLE: Building ONNX from Source (Linux)\nDESCRIPTION: These commands clone the ONNX repository, initialize submodules, optionally set `CMAKE_ARGS` to prefer lite proto, and install ONNX in editable mode using pip.\nSOURCE: https://github.com/onnx/onnx/blob/main/INSTALL.md#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://github.com/onnx/onnx.git\ncd onnx\ngit submodule update --init --recursive\n# Optional: prefer lite proto\nexport CMAKE_ARGS=-DONNX_USE_LITE_PROTO=ON\npip install -e . -v\n```\n\n----------------------------------------\n\nTITLE: Serializing a NumPy Array to an ONNX Tensor\nDESCRIPTION: This code snippet converts a NumPy array to an ONNX tensor using `from_array`, serializes the ONNX tensor to a string, and writes the serialized tensor to a file named 'saved_tensor.pb'.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n    import numpy\n    from onnx.numpy_helper import from_array\n\n    numpy_tensor = numpy.array([0, 1, 4, 5, 3], dtype=numpy.float32)\n    print(type(numpy_tensor))\n\n    onnx_tensor = from_array(numpy_tensor)\n    print(type(onnx_tensor))\n\n    serialized_tensor = onnx_tensor.SerializeToString()\n    print(type(serialized_tensor))\n\n    with open(\"saved_tensor.pb\", \"wb\") as f:\n        f.write(serialized_tensor)\n```\n\n----------------------------------------\n\nTITLE: Converting ONNX Model to External Data\nDESCRIPTION: This code snippet demonstrates converting an in-memory ONNX model to use external data storage. It uses `convert_model_to_external_data()` to convert the model, specifying options such as whether to store all tensors in a single file, the filename, a size threshold, and whether to convert attributes. Requires `onnx.external_data_helper`.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom onnx.external_data_helper import convert_model_to_external_data\n\n# onnx_model is an in-memory ModelProto\nonnx_model = ...\nconvert_model_to_external_data(onnx_model, all_tensors_to_one_file=True, location=\"filename\", size_threshold=1024, convert_attribute=False)\n# Then the onnx_model has converted raw data as external data\n# Must be followed by save\n```\n\n----------------------------------------\n\nTITLE: Verbose Evaluation of Linear Regression - Python\nDESCRIPTION: This code demonstrates step-by-step evaluation of a linear regression model with varying levels of verbosity.  It defines the model, creates a ReferenceEvaluator instance for different verbose levels, generates random inputs, and runs the model. The output and intermediate values are displayed according to the verbosity level, aiding in debugging and understanding the model's execution.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nimport numpy\nfrom onnx import numpy_helper, TensorProto\nfrom onnx.helper import (\n    make_model, make_node, set_model_props, make_tensor,\n    make_graph, make_tensor_value_info)\nfrom onnx.checker import check_model\nfrom onnx.reference import ReferenceEvaluator\n\nX = make_tensor_value_info('X', TensorProto.FLOAT, [None, None])\nA = make_tensor_value_info('A', TensorProto.FLOAT, [None, None])\nB = make_tensor_value_info('B', TensorProto.FLOAT, [None])\nY = make_tensor_value_info('Y', TensorProto.FLOAT, [None])\nnode1 = make_node('MatMul', ['X', 'A'], ['XA'])\nnode2 = make_node('Add', ['XA', 'B'], ['Y'])\ngraph = make_graph([node1, node2], 'lr', [X, A, B], [Y])\nonnx_model = make_model(graph)\ncheck_model(onnx_model)\n\nfor verbose in [1, 2, 3, 4]:\n    print()\n    print(f\"------ verbose={verbose}\")\n    print()\n    sess = ReferenceEvaluator(onnx_model, verbose=verbose)\n\n    x = numpy.random.randn(4, 2).astype(numpy.float32)\na = numpy.random.randn(2, 1).astype(numpy.float32)\nb = numpy.random.randn(1, 1).astype(numpy.float32)\nfeeds = {'X': x, 'A': a, 'B': b}\n\n    print(sess.run(None, feeds))\n```\n\n----------------------------------------\n\nTITLE: Inspecting ONNX Graph Details\nDESCRIPTION: This snippet demonstrates how to access and display the inputs, outputs, and nodes within an ONNX graph. It uses the `onnx_model.graph` object to iterate through these components and print their attributes, such as name, data type, and shape.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n    from onnx import TensorProto\n    from onnx.helper import (\n        make_model, make_node, make_graph,\n        make_tensor_value_info)\n    from onnx.checker import check_model\n\n    def shape2tuple(shape):\n        return tuple(getattr(d, 'dim_value', 0) for d in shape.dim)\n\n    X = make_tensor_value_info('X', TensorProto.FLOAT, [None, None])\n    A = make_tensor_value_info('A', TensorProto.FLOAT, [None, None])\n    B = make_tensor_value_info('B', TensorProto.FLOAT, [None, None])\n    Y = make_tensor_value_info('Y', TensorProto.FLOAT, [None])\n    node1 = make_node('MatMul', ['X', 'A'], ['XA'])\n    node2 = make_node('Add', ['XA', 'B'], ['Y'])\n    graph = make_graph([node1, node2], 'lr', [X, A, B], [Y])\n    onnx_model = make_model(graph)\n    check_model(onnx_model)\n\n    # the list of inputs\n    print('** inputs **')\n    print(onnx_model.graph.input)\n\n    # in a more nicely format\n    print('** inputs **')\n    for obj in onnx_model.graph.input:\n        print(\"name=%r dtype=%r shape=%r\" % (\n            obj.name, obj.type.tensor_type.elem_type,\n            shape2tuple(obj.type.tensor_type.shape)))\n\n    # the list of outputs\n    print('** outputs **')\n    print(onnx_model.graph.output)\n\n    # in a more nicely format\n    print('** outputs **')\n    for obj in onnx_model.graph.output:\n        print(\"name=%r dtype=%r shape=%r\" % (\n            obj.name, obj.type.tensor_type.elem_type,\n            shape2tuple(obj.type.tensor_type.shape)))\n\n    # the list of nodes\n    print('** nodes **')\n    print(onnx_model.graph.node)\n\n    # in a more nicely format\n    print('** nodes **')\n    for node in onnx_model.graph.node:\n        print(\"name=%r type=%r input=%r output=%r\" % (\n            node.name, node.op_type, node.input, node.output))\n```\n\n----------------------------------------\n\nTITLE: Creating a GraphProto in ONNX\nDESCRIPTION: This code creates a `GraphProto` representing a multi-layer perceptron (MLP) using `helper.make_graph`. It defines the nodes (FC and Relu), graph name, input/output tensor value information. The graph is printed in both raw and readable formats.\nSOURCE: https://github.com/onnx/onnx/blob/main/examples/Protobufs.ipynb#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# graph\ngraph_proto = helper.make_graph(\n    [\n        helper.make_node(\"FC\", [\"X\", \"W1\", \"B1\"], [\"H1\"]),\n        helper.make_node(\"Relu\", [\"H1\"], [\"R1\"]),\n        helper.make_node(\"FC\", [\"R1\", \"W2\", \"B2\"], [\"Y\"]),\n    ],\n    \"MLP\",\n    [\n        helper.make_tensor_value_info(\"X\" , TensorProto.FLOAT, [1]),\n        helper.make_tensor_value_info(\"W1\", TensorProto.FLOAT, [1]),\n        helper.make_tensor_value_info(\"B1\", TensorProto.FLOAT, [1]),\n        helper.make_tensor_value_info(\"W2\", TensorProto.FLOAT, [1]),\n        helper.make_tensor_value_info(\"B2\", TensorProto.FLOAT, [1]),\n    ],\n    [\n        helper.make_tensor_value_info(\"Y\", TensorProto.FLOAT, [1]),\n    ]\n)\n\nprint(\"\\ngraph proto:\\n\")\nprint(graph_proto)\n\nprint(\"\\nMore Readable GraphProto:\\n\")\nprint(helper.printable_graph(graph_proto))\n```\n\n----------------------------------------\n\nTITLE: Creating Int Attribute in ONNX\nDESCRIPTION: This code demonstrates how to create an integer attribute using `helper.make_attribute`. It defines the attribute name and its integer value. The resulting attribute is then printed to the console.\nSOURCE: https://github.com/onnx/onnx/blob/main/examples/Protobufs.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Int Attibute\narg = helper.make_attribute(\"this_is_an_int\", 1701)\nprint(\"\\nInt attribute:\\n\")\nprint(arg)\n```\n\n----------------------------------------\n\nTITLE: Load TensorProto from File\nDESCRIPTION: Loads an ONNX TensorProto from a file. The `ParseFromString()` method is used to deserialize the TensorProto from the binary file.  The file is read from the `resources` directory.\nSOURCE: https://github.com/onnx/onnx/blob/main/examples/np_array_tensorproto.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Load the TensorProto\nnew_tensor = onnx.TensorProto()\nwith open(os.path.join(\"resources\", \"tensor.pb\"), \"rb\") as f:\n    new_tensor.ParseFromString(f.read())\nprint(\"After saving and loading, new TensorProto:\\n{}\".format(new_tensor))\n```\n\n----------------------------------------\n\nTITLE: Create Pad Node (NodeProto)\nDESCRIPTION: Creates a `NodeProto` object representing a 'Pad' operation. It takes 'X' and 'Pads' as inputs and produces 'Y' as output. The 'mode' attribute is set to 'constant'.\nSOURCE: https://github.com/onnx/onnx/blob/main/examples/make_model.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Create a node (NodeProto)\nnode_def = helper.make_node(\n    \"Pad\", # node name\n    [\"X\", \"Pads\"], # inputs\n    [\"Y\"], # outputs\n    mode=\"constant\", # Attributes\n)\n```\n\n----------------------------------------\n\nTITLE: Loading and Inspecting ONNX Metadata - Python\nDESCRIPTION: This code loads an ONNX model from a file and then prints various metadata fields associated with the model, such as `doc_string`, `domain`, `ir_version`, `opset_import`, and others.  It depends on the `onnx` package and assumes a file named 'linear_regression.onnx' exists. It loops through a predefined list of field names and retrieves their values from the loaded model using `getattr`.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom onnx import load\n\nwith open(\"linear_regression.onnx\", \"rb\") as f:\n    onnx_model = load(f)\n\nfor field in ['doc_string', 'domain', 'functions',\n              'ir_version', 'metadata_props', 'model_version',\n              'opset_import', 'producer_name', 'producer_version',\n              'training_info']:\n    print(field, getattr(onnx_model, field))\n```\n\n----------------------------------------\n\nTITLE: Create Model (ModelProto)\nDESCRIPTION: Creates a `ModelProto` object from the `GraphProto` object. The producer name is set to 'onnx-example'. The model represents the complete ONNX model.\nSOURCE: https://github.com/onnx/onnx/blob/main/examples/make_model.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Create the model (ModelProto)\nmodel_def = helper.make_model(graph_def,\n                              producer_name=\"onnx-example\")\n```\n\n----------------------------------------\n\nTITLE: Create Graph (GraphProto)\nDESCRIPTION: Creates a `GraphProto` object named 'test-model'. It contains the 'Pad' node, input and output `ValueInfoProto` objects, and a `TensorProto` object for the 'Pads' input. The graph defines the computation to be performed by the model.\nSOURCE: https://github.com/onnx/onnx/blob/main/examples/make_model.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Create the graph (GraphProto)\ngraph_def = helper.make_graph(\n    [node_def],\n    \"test-model\",\n    [X, Pads],\n    [Y],\n    [helper.make_tensor(\"Pads\", TensorProto.INT64, [4,], [0, 0, 1, 1,])],\n)\n```\n\n----------------------------------------\n\nTITLE: Verifying ONNX Reshape Differentiability using PyTorch\nDESCRIPTION: This Python script demonstrates how to verify the differentiability of the ONNX Reshape operator using PyTorch. It defines a simple PyTorch model that performs a reshape operation and computes the backward pass. The script maps the PyTorch reshape operation to ONNX reshape, showing that the input and output of the ONNX Reshape operator are differentiable. It exports the PyTorch model to ONNX format and prints the gradients.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/DefineDifferentiability.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\n\n# A single-operator model. It's literally a Pytorch Reshape.\n# Note that Pytorch Reshape can be directly mapped to ONNX Reshape.\nclass MyModel(nn.Module):\n  def __init__(self):\n    super(MyModel, self).__init__()\n\n  def forward(self, x):\n    y = torch.reshape(x, (x.numel(),))\n    y.retain_grad()\n    return y\n\nmodel = MyModel()\n\nx = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\ny = model(x)\ndy = torch.tensor([1., 2., 3., 4.])\n\ntorch.autograd.backward([y],\n  grad_tensors=[dy],\n  retain_graph=True,\n  create_graph=True,\n  grad_variables=None)\n\n# This example shows the input and the output in Pytorch are differentiable.\n# From the exported ONNX model below, we also see that \"x\" is the first input\n# of ONNX Reshape and \"y\" the output of ONNX Reshape. Therefore, we can say\n# the first input and the output of ONNX Reshape are differentiable.\nprint(x.grad)\nprint(y.grad)\n\nwith open('model.onnx', 'wb') as f:\n  torch.onnx.export(model, x, f)\n```\n\n----------------------------------------\n\nTITLE: Creating Node with Attributes in ONNX\nDESCRIPTION: This code demonstrates creating a `NodeProto` with attributes using `helper.make_node`.  It specifies the operator type (`Conv`), input/output names, and attributes like `kernel`, `stride`, and `pad`. The attributes are then sorted, and the node is printed in both raw and readable formats.\nSOURCE: https://github.com/onnx/onnx/blob/main/examples/Protobufs.ipynb#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# node with args\nnode_proto = helper.make_node(\n    \"Conv\", [\"X\", \"W\", \"B\"], [\"Y\"],\n    kernel=3, stride=1, pad=1)\n\n# This is just for making the attributes to be printed in order\nnode_proto.attribute.sort(key=lambda attr: attr.name)\nprint(\"\\nNodeProto:\\n\")\nprint(node_proto)\n\nprint(\"\\nMore Readable NodeProto (no args yet):\\n\")\nprint(helper.printable_node(node_proto))\n```\n\n----------------------------------------\n\nTITLE: Validating ONNX Model Structure in Python\nDESCRIPTION: This snippet uses the `onnx.checker.check_model()` function to validate the structure of the loaded ONNX model. It includes a try-except block to catch potential `ValidationError` exceptions, providing informative messages based on the validation result.\nSOURCE: https://github.com/onnx/onnx/blob/main/examples/check_model.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    onnx.checker.check_model(onnx_model)\nexcept onnx.checker.ValidationError as e:\n    print(\"The model is invalid: %s\" % e)\nelse:\n    print(\"The model is valid!\")\n```\n\n----------------------------------------\n\nTITLE: Optimize Linear Regression with Custom Op - Python\nDESCRIPTION: This snippet optimizes a linear regression model by replacing the EyeLike and Add operations with a single custom operator, AddEyeLike, defined in the 'optimized' domain. It shows how to define and use custom operators in ONNX. The code constructs the optimized ONNX graph, serializes it to a file, and then prepares for evaluation with a custom operator implementation.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nimport numpy\nfrom onnx import numpy_helper, TensorProto\nfrom onnx.helper import (\n    make_model, make_node, set_model_props, make_tensor,\n    make_graph, make_tensor_value_info, make_opsetid)\nfrom onnx.checker import check_model\n\nX = make_tensor_value_info('X', TensorProto.FLOAT, [None, None])\nA = make_tensor_value_info('A', TensorProto.FLOAT, [None, None])\nB = make_tensor_value_info('B', TensorProto.FLOAT, [None])\nY = make_tensor_value_info('Y', TensorProto.FLOAT, [None])\n\nnode01 = make_node('AddEyeLike', ['A'], ['A1'], domain='optimized')\n\nnode2 = make_node('MatMul', ['X', 'A1'], ['XA1'])\nnode3 = make_node('Add', ['XA1', 'B'], ['Y'])\ngraph = make_graph([node01, node2, node3], 'lr', [X, A, B], [Y])\n\nonnx_model = make_model(graph, opset_imports=[\n    make_opsetid('', 18), make_opsetid('optimized', 1)\n])\n\ncheck_model(onnx_model)\nwith open(\"linear_regression_improved.onnx\", \"wb\") as f:\n    f.write(onnx_model.SerializeToString())\n```\n\n----------------------------------------\n\nTITLE: sklearn-onnx Operator Composition with Functions\nDESCRIPTION: This code shows how to define ONNX operators as composable functions in sklearn-onnx. This approach is used for implementing converters. It showcases the composition of operators like ReduceSumSquare, MatMul, Add, ArgMin, and Sqrt to create an ONNX graph.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/converters.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nrs = OnnxReduceSumSquare(\n    input_name, axes=[1], keepdims=1, op_version=opv)\n\ngemm_out = OnnxMatMul(\n    input_name, (C.T * (-2)).astype(dtype), op_version=opv)\n\nz = OnnxAdd(rs, gemm_out, op_version=opv)\ny2 = OnnxAdd(C2, z, op_version=opv)\nll = OnnxArgMin(y2, axis=1, keepdims=0, output_names=out[:1],\n                op_version=opv)\ny2s = OnnxSqrt(y2, output_names=out[1:], op_version=opv)\n```\n\n----------------------------------------\n\nTITLE: Add Shape Inference Function to OpSchema in ONNX\nDESCRIPTION: This code snippet demonstrates how to add a shape inference function to an operator's schema using the TypeAndShapeInferenceFunction method. The InferenceFunction defines the logic for inferring output shapes based on input shapes and operator attributes. InferenceContext provides access to input information and methods for writing inferred information.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/ShapeInference.md#_snippet_1\n\nLANGUAGE: cpp\nCODE:\n```\nOpSchema& Opschema::TypeAndShapeInferenceFunction(InferenceFunction inferenceFunction);\n```\n\n----------------------------------------\n\nTITLE: Conversion Utilities for ONNX IR Attributes\nDESCRIPTION: This code snippet provides conversion utilities for mapping attributes in ONNX IR to numpy data types and field names. It showcases the usage of helper functions like `tensor_dtype_to_np_dtype` and `tensor_dtype_to_field`. Requires `onnx`, `onnx.TensorProto` and `onnx.helper`.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom onnx import TensorProto, helper\n\nnp_dtype = helper.tensor_dtype_to_np_dtype(TensorProto.FLOAT)\nprint(f\"The converted numpy dtype for {helper.tensor_dtype_to_string(TensorProto.FLOAT)} is {np_dtype}.\")\n\nfield_name = helper.tensor_dtype_to_field(TensorProto.FLOAT)\nprint(f\"The field name for {helper.tensor_dtype_to_string(TensorProto.FLOAT)} is {field_name}.\")\n\n# There are other useful conversion utilities. Please checker onnx.helper\n```\n\n----------------------------------------\n\nTITLE: Evaluating Single ONNX Node - Python\nDESCRIPTION: This snippet shows how to evaluate a single ONNX node using the ReferenceEvaluator. It creates an 'EyeLike' node, initializes the evaluator with the node, provides input data, and then executes the node. This is useful for testing individual operators within a larger ONNX graph.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_26\n\nLANGUAGE: python\nCODE:\n```\nimport numpy\nfrom onnx import numpy_helper, TensorProto\nfrom onnx.helper import make_node\n\nfrom onnx.reference import ReferenceEvaluator\n\nnode = make_node('EyeLike', ['X'], ['Y'])\n\nsess = ReferenceEvaluator(node)\n\nx = numpy.random.randn(4, 2).astype(numpy.float32)\nfeeds = {'X': x}\n\nprint(sess.run(None, feeds))\n```\n\n----------------------------------------\n\nTITLE: Saving an ONNX model to a file - Python\nDESCRIPTION: This snippet demonstrates how to save an ONNX model to a file using the `SerializeToString` method. The model is first serialized into a byte string, which is then written to a file in binary write mode.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/api/serialization.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nwith open(\"model.onnx\", \"wb\") as f:\n    f.write(onnx_model.SerializeToString())\n```\n\n----------------------------------------\n\nTITLE: Implement and Evaluate Custom ONNX Operator - Python\nDESCRIPTION: This code demonstrates how to implement a custom operator (AddEyeLike) for ONNX and evaluate a model using it. It defines the AddEyeLike operator as a subclass of OpRun, implementing the _run method for its computation. The code then loads the optimized ONNX model, provides input data, and evaluates the model with the custom operator registered with the ReferenceEvaluator. It also compares the output with the original model.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_30\n\nLANGUAGE: python\nCODE:\n```\nimport numpy\nfrom onnx.reference import ReferenceEvaluator\nfrom onnx.reference.op_run import OpRun\n\nclass AddEyeLike(OpRun):\n\n    op_domain = \"optimized\"\n\n    def _run(self, X, alpha=1.):\n        assert len(X.shape) == 2\n        assert X.shape[0] == X.shape[1]\n        X = X.copy()\n        ind = numpy.diag_indices(X.shape[0])\n        X[ind] += alpha\n        return (X,)\n\nsess = ReferenceEvaluator(\"linear_regression_improved.onnx\", verbose=2, new_ops=[AddEyeLike])\n\nx = numpy.random.randn(4, 2).astype(numpy.float32)\na = numpy.random.randn(2, 2).astype(numpy.float32) / 10\nb = numpy.random.randn(1, 2).astype(numpy.float32)\nfeeds = {'X': x, 'A': a, 'B': b}\n\nprint(sess.run(None, feeds))\n\n# Let's check with the previous model.\n\nsess0 = ReferenceEvaluator(\"linear_regression.onnx\",)\nsess1 = ReferenceEvaluator(\"linear_regression_improved.onnx\", new_ops=[AddEyeLike])\n\ny0 = sess0.run(None, feeds)[0]\ny1 = sess1.run(None, feeds)[0]\nprint(y0)\nprint(y1)\nprint(f\"difference: {numpy.abs(y0 - y1).max()}\")\n```\n\n----------------------------------------\n\nTITLE: Parse ONNX Model from String in Python\nDESCRIPTION: This code demonstrates how to parse an ONNX model from a string representation using `onnx.parser.parse_model`. It defines a simple graph with MatMul and Add operations and then parses the string to create an ONNX model. The model is then checked for validity.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nimport onnx.parser\nfrom onnx.checker import check_model\n\ninput = '''\n    <\n        ir_version: 8,\n        opset_import: [ \"\" : 15]\n    >\n    agraph (float[I,J] X, float[I] A, float[I] B) => (float[I] Y) {\n        XA = MatMul(X, A)\n        Y = Add(XA, B)\n    }\n    '''\nonnx_model = onnx.parser.parse_model(input)\ncheck_model(onnx_model)\n\nprint(onnx_model)\n```\n\nLANGUAGE: text\nCODE:\n```\nir_version: 8\ngraph {\nnode {\n    input: \"X\"\n    input: \"A\"\n    output: \"XA\"\n    op_type: \"MatMul\"\n    domain: \"\"\n}\nnode {\n    input: \"XA\"\n    input: \"B\"\n    output: \"Y\"\n    op_type: \"Add\"\n    domain: \"\"\n}\nname: \"agraph\"\ninput {\n    name: \"X\"\n    type {\n    tensor_type {\n        elem_type: 1\n        shape {\n        dim {\n            dim_param: \"I\"\n        }\n        dim {\n            dim_param: \"J\"\n        }\n        }\n    }\n    }\n}\ninput {\n    name: \"A\"\n    type {\n    tensor_type {\n        elem_type: 1\n        shape {\n        dim {\n            dim_param: \"I\"\n        }\n        }\n    }\n    }\n}\ninput {\n    name: \"B\"\n    type {\n    tensor_type {\n        elem_type: 1\n        shape {\n        dim {\n            dim_param: \"I\"\n        }\n        }\n    }\n    }\n}\noutput {\n    name: \"Y\"\n    type {\n    tensor_type {\n        elem_type: 1\n        shape {\n        dim {\n            dim_param: \"I\"\n        }\n        }\n    }\n    }\n}\n}\nopset_import {\ndomain: \"\"\nversion: 15\n}\n```\n\n----------------------------------------\n\nTITLE: Creating ONNX Model with Unknown Shape\nDESCRIPTION: This code snippet creates an ONNX model with two transpose nodes. The shape of the intermediate tensor 'Y' is initially unknown. The model consists of two transpose operations applied sequentially. The input 'X' has a shape of (2, 3, 4) and the output 'Z' also has a shape of (2, 3, 4).\nSOURCE: https://github.com/onnx/onnx/blob/main/examples/shape_inference.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\nfrom onnx import helper, shape_inference\nfrom onnx import TensorProto\n\n\n# Preprocessing: create a model with two nodes, Y's shape is unknown\nnode1 = helper.make_node(\"Transpose\", [\"X\"], [\"Y\"], perm=[1, 0, 2])\nnode2 = helper.make_node(\"Transpose\", [\"Y\"], [\"Z\"], perm=[1, 0, 2])\n\ngraph = helper.make_graph(\n    [node1, node2],\n    \"two-transposes\",\n    [helper.make_tensor_value_info(\"X\", TensorProto.FLOAT, (2, 3, 4))],\n    [helper.make_tensor_value_info(\"Z\", TensorProto.FLOAT, (2, 3, 4))],\n)\n\noriginal_model = helper.make_model(graph, producer_name=\"onnx-examples\")\n\n# Check the model and print Y's shape information\nonnx.checker.check_model(original_model)\nprint(\n    \"Before shape inference, the shape info of Y is:\\n{}\".format(\n        original_model.graph.value_info\n    )\n)\n```\n\n----------------------------------------\n\nTITLE: Building ONNX from Source (Mac)\nDESCRIPTION: These commands update brew, install cmake, clone the ONNX repository, initialize submodules, optionally set `CMAKE_ARGS` to prefer lite proto, and install ONNX in editable mode using pip.\nSOURCE: https://github.com/onnx/onnx/blob/main/INSTALL.md#_snippet_12\n\nLANGUAGE: sh\nCODE:\n```\nbrew update\nbrew install cmake\ngit clone https://github.com/protocolbuffers/protobuf.git\ncd protobuf\ngit checkout v5.29.2\ngit submodule update --init --recursive\nmkdir build_source && cd build_source\ncmake -Dprotobuf_BUILD_SHARED_LIBS=OFF -Dprotobuf_BUILD_TESTS=OFF -DCMAKE_BUILD_TYPE=Release -DCMAKE_POSITION_INDEPENDENT_CODE=ON ..\ncmake --build . --target install\n```\n\n----------------------------------------\n\nTITLE: Deserializing an ONNX Tensor from a File\nDESCRIPTION: This snippet reads a serialized ONNX tensor from a file named 'saved_tensor.pb', parses it into an ONNX `TensorProto` object, and then converts it back to a NumPy array using `to_array`.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n    from onnx import TensorProto\n    from onnx.numpy_helper import to_array\n\n    with open(\"saved_tensor.pb\", \"rb\") as f:\n        serialized_tensor = f.read()\n    print(type(serialized_tensor))\n\n    onnx_tensor = TensorProto()\n    onnx_tensor.ParseFromString(serialized_tensor)\n    print(type(onnx_tensor))\n\n    numpy_tensor = to_array(onnx_tensor)\n    print(numpy_tensor)\n```\n\n----------------------------------------\n\nTITLE: Setting CMAKE_ARGS for Protobuf (Windows)\nDESCRIPTION: This command sets the `CMAKE_ARGS` environment variable to enable the use of shared Protobuf libraries. This is necessary when Protobuf is installed as a shared library rather than a static library.\nSOURCE: https://github.com/onnx/onnx/blob/main/INSTALL.md#_snippet_4\n\nLANGUAGE: bat\nCODE:\n```\nset CMAKE_ARGS=\"-DONNX_USE_PROTOBUF_SHARED_LIBS=ON\"\n```\n\n----------------------------------------\n\nTITLE: ONNX Linear Regression Example\nDESCRIPTION: This Python-like code represents an ONNX implementation of a linear regression model. It defines a function that takes an input `X`, multiplies it by `coefficients`, and adds a `bias` to produce the final output. This example showcases how mathematical operations are represented in ONNX.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/concepts.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef onnx_linear_regressor(X):\n    \"ONNX code for a linear regression\"\n    return onnx.Add(onnx.MatMul(X, coefficients), bias)\n```\n\n----------------------------------------\n\nTITLE: Loading an ONNX Model with External Data from a Separate Directory\nDESCRIPTION: This snippet shows how to load an ONNX model when the external data is stored in a different directory. It uses `onnx.load()` with `load_external_data=False` initially, and then uses `load_external_data_for_model()` from `onnx.external_data_helper` to specify the directory path for the external data. Requires the `onnx` library and `onnx.external_data_helper` module.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/ExternalData.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\nfrom onnx.external_data_helper import load_external_data_for_model\n\nonnx_model = onnx.load(\"path/to/the/model.onnx\", load_external_data=False)\nload_external_data_for_model(onnx_model, \"data/directory/path/\")\n# Then the onnx_model has loaded the external data from the specific directory\n```\n\n----------------------------------------\n\nTITLE: Parsing ONNX Graph from Text\nDESCRIPTION: This snippet shows how to create an ONNX graph from a textual representation using `onnx.parser.parse_graph`. The `input` variable contains the textual representation of the graph. Requires the `onnx` library and a valid textual representation of an ONNX graph.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md#_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ninput = \"\"\"\n   agraph (float[N, 128] X, float[128, 10] W, float[10] B) => (float[N, 10] C)\n   {\n        T = MatMul(X, W)\n        S = Add(T, B)\n        C = Softmax(S)\n   }\n\"\"\"\ngraph = onnx.parser.parse_graph(input)\n```\n\n----------------------------------------\n\nTITLE: Tensor-Based Label Encoding using LabelEncoder\nDESCRIPTION: This snippet demonstrates how to use the LabelEncoder with tensor inputs for keys, values, and the default value. It utilizes `make_tensor` to create ONNX tensors for the keys, values, and default value. The input is an array of strings, and the output is an array of int16 values.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/Operators-ml.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntensor_keys = make_tensor(\n    \"keys_tensor\", onnx.TensorProto.STRING, (3,), [\"a\", \"b\", \"c\"]\n)\nrepeated_string_keys = [\"a\", \"b\", \"c\"]\nx = np.array([\"a\", \"b\", \"d\", \"c\", \"g\"]).astype(object)\ny = np.array([0, 1, 42, 2, 42]).astype(np.int16)\n\nnode = onnx.helper.make_node(\n    \"LabelEncoder\",\n    inputs=[\"X\"],\n    outputs=[\"Y\"],\n    domain=\"ai.onnx.ml\",\n    keys_tensor=tensor_keys,\n    values_tensor=make_tensor(\n        \"values_tensor\", onnx.TensorProto.INT16, (3,), [0, 1, 2]\n    ),\n    default_tensor=make_tensor(\n        \"default_tensor\", onnx.TensorProto.INT16, (1,), [42]\n    ),\n)\n\nexpect(\n    node,\n    inputs=[x],\n    outputs=[y],\n    name=\"test_ai_onnx_ml_label_encoder_tensor_mapping\",\n)\n\nnode = onnx.helper.make_node(\n    \"LabelEncoder\",\n    inputs=[\"X\"],\n    outputs=[\"Y\"],\n    domain=\"ai.onnx.ml\",\n    keys_strings=repeated_string_keys,\n    values_tensor=make_tensor(\n        \"values_tensor\", onnx.TensorProto.INT16, (3,), [0, 1, 2]\n    ),\n    default_tensor=make_tensor(\n        \"default_tensor\", onnx.TensorProto.INT16, (1,), [42]\n    ),\n)\n\nexpect(\n    node,\n    inputs=[x],\n    outputs=[y],\n    name=\"test_ai_onnx_ml_label_encoder_tensor_value_only_mapping\",\n)\n```\n\n----------------------------------------\n\nTITLE: sklearn-onnx Initializer and Node Addition\nDESCRIPTION: This code snippet demonstrates how to add initializers (coefficients and intercepts) and nodes (MatMul and ArgMax) to an ONNX graph using sklearn-onnx. It involves getting unique variable names, reshaping model parameters, and adding the nodes with specified attributes.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/converters.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# initializer\n\ncoef = scope.get_unique_variable_name('coef')\nmodel_coef = np.array(\n    classifier_attrs['coefficients'], dtype=np.float64)\nmodel_coef = model_coef.reshape((number_of_classes, -1)).T\ncontainer.add_initializer(\n    coef, proto_dtype, model_coef.shape, model_coef.ravel().tolist())\n\nintercept = scope.get_unique_variable_name('intercept')\nmodel_intercept = np.array(\n    classifier_attrs['intercepts'], dtype=np.float64)\nmodel_intercept = model_intercept.reshape((number_of_classes, -1)).T\ncontainer.add_initializer(\n    intercept, proto_dtype, model_intercept.shape,\n    model_intercept.ravel().tolist())\n\n# add nodes\n\nmultiplied = scope.get_unique_variable_name('multiplied')\ncontainer.add_node(\n    'MatMul', [operator.inputs[0].full_name, coef], multiplied,\n    name=scope.get_unique_operator_name('MatMul'))\n\n# [...]\n\nargmax_output_name = scope.get_unique_variable_name('label')\ncontainer.add_node('ArgMax', raw_score_name, argmax_output_name,\n                   name=scope.get_unique_operator_name('ArgMax'),\n                   axis=1)\n```\n\n----------------------------------------\n\nTITLE: Creating a NodeProto in ONNX\nDESCRIPTION: This code snippet shows how to create a basic `NodeProto` using `helper.make_node`. It defines the operator type (`Relu`), input names (`X`), and output names (`Y`). The resulting node is then printed.\nSOURCE: https://github.com/onnx/onnx/blob/main/examples/Protobufs.ipynb#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# node\nnode_proto = helper.make_node(\"Relu\", [\"X\"], [\"Y\"])\n\nprint(\"\\nNodeProto:\\n\")\nprint(node_proto)\n```\n\n----------------------------------------\n\nTITLE: Installing Protobuf via apt-get (Ubuntu)\nDESCRIPTION: This command installs Protobuf and its dependencies using apt-get, the package manager for Ubuntu.\nSOURCE: https://github.com/onnx/onnx/blob/main/INSTALL.md#_snippet_10\n\nLANGUAGE: sh\nCODE:\n```\napt-get install python3-pip python3-dev libprotobuf-dev protobuf-compiler\n```\n\n----------------------------------------\n\nTITLE: Creating Conda Environment for ONNX Build (Windows)\nDESCRIPTION: This snippet creates a conda environment named 'py3.9' with Python 3.9 and installs necessary dependencies such as numpy and libprotobuf. The environment is specifically for building the ONNX project on Windows using Anaconda.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/converters.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nconda create --yes --quiet --name py3.9 python=3.9\nconda install -n py3.9 -y -c conda-forge numpy libprotobuf=3.16.0\n```\n\n----------------------------------------\n\nTITLE: Converting an ONNX Model to External Data\nDESCRIPTION: This snippet demonstrates how to convert an ONNX model to use external data storage. It uses the `convert_model_to_external_data()` function from `onnx.external_data_helper` to convert the model's data to external files.  The converted model needs to be saved using `onnx.save_model()`. Requires the `onnx` library and `onnx.external_data_helper` module.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/ExternalData.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\nfrom onnx.external_data_helper import convert_model_to_external_data\n\nonnx_model = ... # Your model in memory as ModelProto\nconvert_model_to_external_data(onnx_model, all_tensors_to_one_file=True, location=\"filename\", size_threshold=1024, convert_attribute=False)\n# Must be followed by save_model to save the converted model to a specific path\nonnx.save_model(onnx_model, \"path/to/save/the/model.onnx\")\n# Then the onnx_model has converted raw data as external data and saved to specific directory\n```\n\n----------------------------------------\n\nTITLE: ONNX Graph for Optional Tensor Input (Example 1)\nDESCRIPTION: This is the ONNX graph representation of the PyTorch model using an optional Tensor as input. It demonstrates how the optional type is handled in ONNX using the `OptionalHasElement` and `OptionalGetElement` operators within an `If` construct. The graph shows the flow of execution based on whether the optional tensor is present.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/ONNXTypes.md#_snippet_2\n\nLANGUAGE: onnx\nCODE:\n```\nGraph(\n    %x.1 : Float(2, 3),\n    %y.1 : Float(2, 3)\n):\n    %2 : Bool(1) = onnx::OptionalHasElement(%y.1)\n    %5 : Float(2, 3) = onnx::If(%2)\n        block0():\n            %3 : Float(2, 3) = onnx::OptionalGetElement(%y.1)\n            %4 : Float(2, 3) = onnx::Add(%x.1, %3)\n        -> (%4)\n        block1():\n            %x.2 : Float(2, 3) = onnx::Identity(%x.1)\n        -> (%x.2)\n    return (%5)\n```\n\n----------------------------------------\n\nTITLE: Create Input ValueInfoProto\nDESCRIPTION: Creates two input `ValueInfoProto` objects named 'X' and 'Pads'. 'X' is a float tensor with shape [1, 2], and 'Pads' is an int64 tensor with shape [4]. These objects define the input tensors of the model.\nSOURCE: https://github.com/onnx/onnx/blob/main/examples/make_model.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Create one input (ValueInfoProto)\nX = helper.make_tensor_value_info(\"X\", TensorProto.FLOAT, [1, 2])\n\n# Create second input (ValueInfoProto)\nPads = helper.make_tensor_value_info(\"Pads\", TensorProto.INT64, [4])\n```\n\n----------------------------------------\n\nTITLE: Creating Float Attribute in ONNX\nDESCRIPTION: This code creates a floating-point attribute using `helper.make_attribute`. It sets the attribute's name and its float value. The attribute is then printed to the console.\nSOURCE: https://github.com/onnx/onnx/blob/main/examples/Protobufs.ipynb#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Float Attribute\narg = helper.make_attribute(\"this_is_a_float\", 3.14)\nprint(\"\\nFloat attribute:\\n\")\nprint(arg)\n```\n\n----------------------------------------\n\nTITLE: Building ONNX Package (Linux)\nDESCRIPTION: This snippet builds the ONNX package on Linux after cloning the repository. It uses the `build` module to create a wheel package.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/converters.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\npython -m build --wheel\n```\n\n----------------------------------------\n\nTITLE: Numpy Array Initialization\nDESCRIPTION: Initializes a NumPy array with floating-point values. The `numpy` library is used to create the array with a specific data type and shape. The array is then printed to the console.\nSOURCE: https://github.com/onnx/onnx/blob/main/examples/np_array_tensorproto.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy\nimport onnx\nimport os\nfrom onnx import numpy_helper\n\n\n# Preprocessing: create a Numpy array\nnumpy_array = numpy.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], dtype=float)\nprint(\"Original Numpy array:\\n{}\\n\".format(numpy.array2string(numpy_array)))\n```\n\n----------------------------------------\n\nTITLE: TorchScript Graph for Optional Tensor Output (Example 2)\nDESCRIPTION: This is the TorchScript graph representing a PyTorch model that produces an optional Tensor as output. The graph uses `prim::If` to conditionally assign the `encoder_states` tensor. It highlights how the optional type is represented in the TorchScript graph using the `Tensor?` notation.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/ONNXTypes.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nGraph(\n    %src_tokens.1 : Float(3, 2, 4,),\n    %return_all_hiddens.1 : Bool(1)\n):\n    %3 : None = prim::Constant()\n    %encoder_states : Tensor? = prim::If(%return_all_hiddens.1)\n        block0():\n        -> (%src_tokens.1)\n        block1():\n        -> (%3)\n    return (%src_tokens.1, %encoder_states)\n```\n\n----------------------------------------\n\nTITLE: Shape Inference Optimization Example\nDESCRIPTION: This code shows an example of how shape inference can optimize memory usage in ONNX. If the inputs `x` and `y` to the `Add` operator have the same shape, the output `z` will also have the same shape.  The subsequent `Abs` operator's output `w` will share the same shape, enabling buffer reuse for increased efficiency.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/concepts.md#_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nAdd(x, y) -> z\nAbs(z) -> w\n```\n\n----------------------------------------\n\nTITLE: Creating TreeEnsemble node with set membership in ONNX (Python)\nDESCRIPTION: This code snippet demonstrates how to create a `TreeEnsemble` node in ONNX using the `onnx.helper.make_node` function.  It defines attributes like `n_targets`, `aggregate_function`, `membership_values`, and node-related attributes such as `nodes_modes`, `nodes_featureids`, `nodes_splits`, etc. The example includes input data (`x`), expected output (`expected`), and uses the `expect` function to test the node.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/Operators-ml.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nnode = onnx.helper.make_node(\n    \"TreeEnsemble\",\n    [\"X\"],\n    [\"Y\"],\n    domain=\"ai.onnx.ml\",\n    n_targets=4,\n    aggregate_function=1,\n    membership_values=make_tensor(\n        \"membership_values\",\n        onnx.TensorProto.FLOAT,\n        (8,),\n        [1.2, 3.7, 8, 9, np.nan, 12, 7, np.nan],\n    ),\n    nodes_missing_value_tracks_true=None,\n    nodes_hitrates=None,\n    post_transform=0,\n    tree_roots=[0],\n    nodes_modes=make_tensor(\n        \"nodes_modes\",\n        onnx.TensorProto.UINT8,\n        (3,),\n        np.array([0, 6, 6], dtype=np.uint8),\n    ),\n    nodes_featureids=[0, 0, 0],\n    nodes_splits=make_tensor(\n        \"nodes_splits\",\n        onnx.TensorProto.FLOAT,\n        (3,),\n        np.array([11, 232344.0, np.nan], dtype=np.float32),\n    ),\n    nodes_trueleafs=[0, 1, 1],\n    nodes_truenodeids=[1, 0, 1],\n    nodes_falseleafs=[1, 0, 1],\n    nodes_falsenodeids=[2, 2, 3],\n    leaf_targetids=[0, 1, 2, 3],\n    leaf_weights=make_tensor(\n        \"leaf_weights\", onnx.TensorProto.FLOAT, (4,), [1, 10, 1000, 100]\n    ),\n)\n\nx = np.array([1.2, 3.4, -0.12, np.nan, 12, 7], np.float32).reshape(-1, 1)\nexpected = np.array(\n    [\n        [1, 0, 0, 0],\n        [0, 0, 0, 100],\n        [0, 0, 0, 100],\n        [0, 0, 1000, 0],\n        [0, 0, 1000, 0],\n        [0, 10, 0, 0],\n    ],\n    dtype=np.float32,\n)\nexpect(\n    node,\n    inputs=[x],\n    outputs=[expected],\n    name=\"test_ai_onnx_ml_tree_ensemble_set_membership\",\n)\n\n```\n\n----------------------------------------\n\nTITLE: Define ONNX Function without Attributes in Python\nDESCRIPTION: This code defines an ONNX function named 'LinearRegression' within the custom domain. It creates a function that performs a matrix multiplication followed by an addition. The resulting ONNX model includes this function and uses it in a graph.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport numpy\nfrom onnx import numpy_helper, TensorProto\nfrom onnx.helper import (\n    make_model, make_node, set_model_props, make_tensor,\n    make_graph, make_tensor_value_info, make_opsetid,\n    make_function)\nfrom onnx.checker import check_model\n\nnew_domain = 'custom'\nopset_imports = [make_opsetid(\"\", 14), make_opsetid(new_domain, 1)]\n\n# Let's define a function for a linear regression\n\nnode1 = make_node('MatMul', ['X', 'A'], ['XA'])\nnode2 = make_node('Add', ['XA', 'B'], ['Y'])\n\nlinear_regression = make_function(\n    new_domain,            # domain name\n    'LinearRegression',     # function name\n    ['X', 'A', 'B'],        # input names\n    ['Y'],                  # output names\n    [node1, node2],         # nodes\n    opset_imports,          # opsets\n    [])                     # attribute names\n\n# Let's use it in a graph.\n\nX = make_tensor_value_info('X', TensorProto.FLOAT, [None, None])\nA = make_tensor_value_info('A', TensorProto.FLOAT, [None, None])\nB = make_tensor_value_info('B', TensorProto.FLOAT, [None, None])\nY = make_tensor_value_info('Y', TensorProto.FLOAT, [None])\n\ngraph = make_graph(\n    [make_node('LinearRegression', ['X', 'A', 'B'], ['Y1'], domain=new_domain),\n     make_node('Abs', ['Y1'], ['Y'])],\n    'example',\n    [X, A, B], [Y])\n\nonnx_model = make_model(\n    graph, opset_imports=opset_imports,\n    functions=[linear_regression])  # functions to add)\ncheck_model(onnx_model)\n\n# the work is done, let's display it...\nprint(onnx_model)\n```\n\n----------------------------------------\n\nTITLE: Setting ONNX Opset Version - Python\nDESCRIPTION: This code loads an ONNX model from a file and then modifies the opset version to 14. It first clears any existing opset imports and then adds a new opset with an empty domain and version 14. It depends on the `onnx` package and assumes a file named 'linear_regression.onnx' exists.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom onnx import load\n\nwith open(\"linear_regression.onnx\", \"rb\") as f:\n    onnx_model = load(f)\n\ndel onnx_model.opset_import[:]\nopset = onnx_model.opset_import.add()\nopset.domain = ''\nopset.version = 14\n\nfor opset in onnx_model.opset_import:\n    print(\"opset domain=%r version=%r\" % (opset.domain, opset.version))\n```\n\n----------------------------------------\n\nTITLE: ONNX If Operator Example\nDESCRIPTION: This code snippet illustrates the behavior of the ONNX If operator, which conditionally executes one of two ONNX graphs based on a condition's evaluation. The `then_branch` is executed if the condition is true, otherwise the `else_branch` is executed. Both branches must produce the same number of outputs, which become the outputs of the If operator.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/concepts.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nIf(condition) then\n    execute this ONNX graph (`then_branch`)\nelse\n    execute this ONNX graph (`else_branch`)\n```\n\n----------------------------------------\n\nTITLE: Convert Numpy Array to TensorProto\nDESCRIPTION: Converts a NumPy array to an ONNX TensorProto using `numpy_helper.from_array()`. This function is part of the `onnx` library. The resulting TensorProto is then printed to the console.\nSOURCE: https://github.com/onnx/onnx/blob/main/examples/np_array_tensorproto.ipynb#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Convert the Numpy array to a TensorProto\ntensor = numpy_helper.from_array(numpy_array)\nprint(\"TensorProto:\\n{}\".format(tensor))\n```\n\n----------------------------------------\n\nTITLE: Creating TreeEnsemble node with a single tree in ONNX (Python)\nDESCRIPTION: This Python code snippet demonstrates how to construct a `TreeEnsemble` node using `onnx.helper.make_node` for a single tree scenario. It specifies the necessary attributes, including `n_targets`, `aggregate_function`, `tree_roots`, `nodes_modes`, `nodes_featureids`, `nodes_splits`, `nodes_truenodeids`, `nodes_trueleafs`, `nodes_falsenodeids`, `nodes_falseleafs`, `leaf_targetids`, and `leaf_weights`. The `expect` function is used to validate the created node with input data (`x`) and expected output (`y`).\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/Operators-ml.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nnode = onnx.helper.make_node(\n    \"TreeEnsemble\",\n    [\"X\"],\n    [\"Y\"],\n    domain=\"ai.onnx.ml\",\n    n_targets=2,\n    membership_values=None,\n    nodes_missing_value_tracks_true=None,\n    nodes_hitrates=None,\n    aggregate_function=1,\n    post_transform=0,\n    tree_roots=[0],\n    nodes_modes=make_tensor(\n        \"nodes_modes\",\n        onnx.TensorProto.UINT8,\n        (3,),\n        np.array([0, 0, 0], dtype=np.uint8),\n    ),\n    nodes_featureids=[0, 0, 0],\n    nodes_splits=make_tensor(\n        \"nodes_splits\",\n        onnx.TensorProto.DOUBLE,\n        (3,),\n        np.array([3.14, 1.2, 4.2], dtype=np.float64),\n    ),\n    nodes_truenodeids=[1, 0, 1],\n    nodes_trueleafs=[0, 1, 1],\n    nodes_falsenodeids=[2, 2, 3],\n    nodes_falseleafs=[0, 1, 1],\n    leaf_targetids=[0, 1, 0, 1],\n    leaf_weights=make_tensor(\n        \"leaf_weights\",\n        onnx.TensorProto.DOUBLE,\n        (4,),\n        np.array([5.23, 12.12, -12.23, 7.21], dtype=np.float64),\n    ),\n)\n\nx = np.array([1.2, 3.4, -0.12, 1.66, 4.14, 1.77], np.float64).reshape(3, 2)\ny = np.array([[5.23, 0], [5.23, 0], [0, 12.12]], dtype=np.float64)\nexpect(\n    node,\n    inputs=[x],\n    outputs=[y],\n    name=\"test_ai_onnx_ml_tree_ensemble_single_tree\",\n)\n\n```\n\n----------------------------------------\n\nTITLE: Displaying ONNX Version and Opset\nDESCRIPTION: This Python code imports the `onnx` module and prints the current ONNX version and the default ONNX opset version. It's used to verify the installed ONNX version and the supported operator set. It requires the onnx module.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/concepts.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\nprint(onnx.__version__, \" opset=\", onnx.defs.onnx_opset_version())\n```\n\n----------------------------------------\n\nTITLE: Enabling Address Sanitizer (ASAN)\nDESCRIPTION: This snippet enables the Address Sanitizer (ASAN) and Undefined Behavior Sanitizer (UBSAN) for the 'onnx' target if `ONNX_USE_ASAN` is enabled and the compiler is not MSVC. It uses the `find_package(Sanitizer REQUIRED)` command to locate the sanitizers and links them to the target. Messages are printed to indicate whether ASAN and UBSAN are in use.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_38\n\nLANGUAGE: cmake\nCODE:\n```\nif(ONNX_USE_ASAN AND NOT MSVC)\n  find_package(Sanitizer REQUIRED)\n  if(TARGET Sanitizer::address)\n    target_link_libraries(onnx PRIVATE Sanitizer::address)\n    message(STATUS \"Use ASAN\")\n  endif()\n  if(TARGET Sanitizer::undefined)\n    target_link_libraries(onnx PRIVATE Sanitizer::undefined)\n    message(STATUS \"Use UBSAN\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Listing ONNX Helper Functions - Python\nDESCRIPTION: This snippet displays a list of functions within the `onnx.helper` module that begin with the prefix 'make'. It relies on the `onnx` and `pprint` modules to accomplish this. It demonstrates how to discover available helper functions for constructing ONNX models.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\nimport pprint\npprint.pprint([k for k in dir(onnx.helper)\n               if k.startswith('make')])\n```\n\n----------------------------------------\n\nTITLE: Creating Node with Graph Attribute in ONNX\nDESCRIPTION: This code demonstrates creating a `NodeProto` that contains a `GraphProto` as an attribute. It first defines a graph using `helper.make_graph` (similar to the previous example). Then, it creates a node of type \"graph\" and sets the graph as an attribute. This is useful for representing subgraphs or conditional execution within an ONNX model.\nSOURCE: https://github.com/onnx/onnx/blob/main/examples/Protobufs.ipynb#_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# An node that is also a graph\ngraph_proto = helper.make_graph(\n    [\n        helper.make_node(\"FC\", [\"X\", \"W1\", \"B1\"], [\"H1\"]),\n        helper.make_node(\"Relu\", [\"H1\"], [\"R1\"]),\n        helper.make_node(\"FC\", [\"R1\", \"W2\", \"B2\"], [\"Y\"]),\n    ],\n    \"MLP\",\n    [\n        helper.make_tensor_value_info(\"X\" , TensorProto.FLOAT, [1]),\n        helper.make_tensor_value_info(\"W1\", TensorProto.FLOAT, [1]),\n        helper.make_tensor_value_info(\"B1\", TensorProto.FLOAT, [1]),\n        helper.make_tensor_value_info(\"W2\", TensorProto.FLOAT, [1]),\n        helper.make_tensor_value_info(\"B2\", TensorProto.FLOAT, [1]),\n    ],\n    [\n        helper.make_tensor_value_info(\"Y\", TensorProto.FLOAT, [1]),\n    ]\n)\n\n# output = ThisSpecificgraph([input, w1, b1, w2, b2])\nnode_proto = helper.make_node(\n    \"graph\",\n    [\"Input\", \"W1\", \"B1\", \"W2\", \"B2\"],\n    [\"Output\"],\n    graph=[graph_proto],\n)\n\nprint(\"\\nNodeProto that contains a graph:\\n\")\nprint(node_proto)\n```\n\n----------------------------------------\n\nTITLE: Running Type Inference on ONNX Function\nDESCRIPTION: This code demonstrates how to run type inference on an ONNX function using `onnx.shape_inference.infer_function_output_types()`. It parses a function from text, creates input type and attribute value, and then infers the output types. Requires `onnx`, `onnx.helper`, `onnx.parser`, and `onnx.shape_inference`.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md#_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\nimport onnx.helper\nimport onnx.parser\nimport onnx.shape_inference\n\nfunction_text = \"\"\"\n    <opset_import: [ \\\"\\\" : 18 ], domain: \\\"local\\\">\n    CastTo <dtype> (x) => (y) {\n        y = Cast <to : int = @dtype> (x)\n    }\n\"\"\"\nfunction = onnx.parser.parse_function(function_text)\n\n# The function above has one input-parameter x, and one attribute-parameter dtype.\n# To apply type-and-shape-inference to this function, we must supply the type of\n# input-parameter and an attribute value for the attribute-parameter as below:\n\nfloat_type_ = onnx.helper.make_tensor_type_proto(1, None)\ndtype_6 = onnx.helper.make_attribute(\"dtype\", 6)\nresult = onnx.shape_inference.infer_function_output_types(\n    function, [float_type_], [dtype_6]\n)\nprint(result) # a list containing the (single) output type\n```\n\n----------------------------------------\n\nTITLE: Testing LabelEncoder ONNX-ML Operator with Tensors in Python\nDESCRIPTION: This snippet illustrates testing the LabelEncoder using tensor-based keys and values. It creates tensor objects for keys, values, and a default value using the `make_tensor` function.  The tests cover using both `keys_tensor` and `keys_strings` along with tensor values.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/TestCoverage-ml.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntensor_keys = make_tensor(\n    \"keys_tensor\", onnx.TensorProto.STRING, (3,), [\"a\", \"b\", \"c\"]\n)\nrepeated_string_keys = [\"a\", \"b\", \"c\"]\nx = np.array([\"a\", \"b\", \"d\", \"c\", \"g\"]).astype(object)\ny = np.array([0, 1, 42, 2, 42]).astype(np.int16)\n\nnode = onnx.helper.make_node(\n    \"LabelEncoder\",\n    inputs=[\"X\"],\n    outputs=[\"Y\"],\n    domain=\"ai.onnx.ml\",\n    keys_tensor=tensor_keys,\n    values_tensor=make_tensor(\n        \"values_tensor\", onnx.TensorProto.INT16, (3,), [0, 1, 2]\n    ),\n    default_tensor=make_tensor(\n        \"default_tensor\", onnx.TensorProto.INT16, (1,), [42]\n    ),\n)\n\nexpect(\n    node,\n    inputs=[x],\n    outputs=[y],\n    name=\"test_ai_onnx_ml_label_encoder_tensor_mapping\",\n)\n\nnode = onnx.helper.make_node(\n    \"LabelEncoder\",\n    inputs=[\"X\"],\n    outputs=[\"Y\"],\n    domain=\"ai.onnx.ml\",\n    keys_strings=repeated_string_keys,\n    values_tensor=make_tensor(\n        \"values_tensor\", onnx.TensorProto.INT16, (3,), [0, 1, 2]\n    ),\n    default_tensor=make_tensor(\n        \"default_tensor\", onnx.TensorProto.INT16, (1,), [42]\n    ),\n)\n\nexpect(\n    node,\n    inputs=[x],\n    outputs=[y],\n    name=\"test_ai_onnx_ml_label_encoder_tensor_value_only_mapping\",\n)\n```\n\n----------------------------------------\n\nTITLE: ONNX Expect Function Implementation in Python\nDESCRIPTION: This code snippet defines an `expect` function that takes an ONNX node, input NumPy arrays, expected output NumPy arrays, and a name as input. It constructs an ONNX model using the provided node and inputs, runs the model with `onnxruntime`, and compares the actual outputs with the expected outputs using an `assert_allclose` function (not defined in the snippet). The function handles optional input/output type protos and operator set version specifications.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/expect_onnxruntime.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom typing import Any, Sequence\nimport numpy as np\nimport onnx\nimport onnxruntime\n\n\ndef expect(\n    node: onnx.NodeProto,\n    inputs: Sequence[np.ndarray],\n    outputs: Sequence[np.ndarray],\n    name: str,\n    **kwargs: Any,\n) -> None:\n    # Builds the model\n    present_inputs = [x for x in node.input if (x != \"\")]\n    present_outputs = [x for x in node.output if (x != \"\")]\n    input_type_protos = [None] * len(inputs)\n    if \"input_type_protos\" in kwargs:\n        input_type_protos = kwargs[\"input_type_protos\"]\n        del kwargs[\"input_type_protos\"]\n    output_type_protos = [None] * len(outputs)\n    if \"output_type_protos\" in kwargs:\n        output_type_protos = kwargs[\"output_type_protos\"]\n        del kwargs[\"output_type_protos\"]\n    inputs_vi = [\n        _extract_value_info(arr, arr_name, input_type)\n        for arr, arr_name, input_type in zip(inputs, present_inputs, input_type_protos)\n    ]\n    outputs_vi = [\n        _extract_value_info(arr, arr_name, output_type)\n        for arr, arr_name, output_type in zip(\n            outputs, present_outputs, output_type_protos\n        )\n    ]\n    graph = onnx.helper.make_graph(\n        nodes=[node], name=name, inputs=inputs_vi, outputs=outputs_vi\n    )\n    kwargs[\"producer_name\"] = \"backend-test\"\n\n    if \"opset_imports\" not in kwargs:\n        # To make sure the model will be produced with the same opset_version after opset changes\n        # By default, it uses since_version as opset_version for produced models\n        produce_opset_version = onnx.defs.get_schema(\n            node.op_type, domain=node.domain\n        ).since_version\n        kwargs[\"opset_imports\"] = [\n            onnx.helper.make_operatorsetid(node.domain, produce_opset_version)\n        ]\n\n    model = onnx.helper.make_model_gen_version(graph, **kwargs)\n\n    # Checking the produces are the expected ones.\n    sess = onnxruntime.InferenceSession(model.SerializeToString(),\n                                        providers=[\"CPUExecutionProvider\"])\n    feeds = {name: value for name, value in zip(node.input, inputs)}\n    results = sess.run(None, feeds)\n    for expected, output in zip(outputs, results):\n        assert_allclose(expected, output)\n```\n\n----------------------------------------\n\nTITLE: Parsing ONNX Model from Textual Syntax in C++\nDESCRIPTION: This C++ snippet demonstrates how to parse an ONNX model from its textual representation using the `OnnxParser::Parse` method.  It initializes a model string, parses it into a `ModelProto` object, and then checks the model for validity.  It requires the ONNX library and its checker utilities.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/Syntax.md#_snippet_0\n\nLANGUAGE: cpp\nCODE:\n```\n  const char* code = R\"ONNX(\n<\n  ir_version: 7,\n  opset_import: [ \"\" : 10 ]\n>\nagraph (float[N, 128] X, float[128, 10] W, float[10] B) => (float[N, 10] C)\n{\n    T = MatMul(X, W)\n    S = Add(T, B)\n    C = Softmax(S)\n}\n)ONNX\";\n\n  ModelProto model;\n  OnnxParser::Parse(model, code);\n\n  checker::check_model(model);\n```\n\n----------------------------------------\n\nTITLE: Serializing an ONNX Model to a File\nDESCRIPTION: This code snippet creates a linear regression ONNX model and then serializes it to a file named 'linear_regression.onnx' using the `SerializeToString` method. The model is written in binary format.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n    from onnx import TensorProto\n    from onnx.helper import (\n        make_model, make_node, make_graph,\n        make_tensor_value_info)\n    from onnx.checker import check_model\n\n    def shape2tuple(shape):\n        return tuple(getattr(d, 'dim_value', 0) for d in shape.dim)\n\n    X = make_tensor_value_info('X', TensorProto.FLOAT, [None, None])\n    A = make_tensor_value_info('A', TensorProto.FLOAT, [None, None])\n    B = make_tensor_value_info('B', TensorProto.FLOAT, [None, None])\n    Y = make_tensor_value_info('Y', TensorProto.FLOAT, [None])\n    node1 = make_node('MatMul', ['X', 'A'], ['XA'])\n    node2 = make_node('Add', ['XA', 'B'], ['Y'])\n    graph = make_graph([node1, node2], 'lr', [X, A, B], [Y])\n    onnx_model = make_model(graph)\n    check_model(onnx_model)\n\n    # The serialization\n    with open(\"linear_regression.onnx\", \"wb\") as f:\n        f.write(onnx_model.SerializeToString())\n\n    # display\n    print(onnx_model)\n```\n\n----------------------------------------\n\nTITLE: Installing ONNX from TestPyPI (Wheel)\nDESCRIPTION: This command uninstalls ONNX and installs it from TestPyPI using a pre-built wheel. The `-i` flag specifies the index URL, and `--pre` allows installing pre-release versions. If a pre-built wheel isn't available, source install will be initiated.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/OnnxReleases.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\npip uninstall -y onnx && pip install -i https://test.pypi.org/simple/ --pre onnx\n```\n\n----------------------------------------\n\nTITLE: Define ONNX Function with Attributes in Python\nDESCRIPTION: This code defines an ONNX function named 'LinearRegression' with an attribute 'bias'. The input 'B' is replaced by a constant node whose value is defined by the 'bias' attribute. This demonstrates how to include attributes in ONNX function definitions.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nimport numpy\nfrom onnx import numpy_helper, TensorProto, AttributeProto\nfrom onnx.helper import (\n    make_model, make_node, set_model_props, make_tensor,\n    make_graph, make_tensor_value_info, make_opsetid,\n    make_function)\nfrom onnx.checker import check_model\n\nnew_domain = 'custom'\nopset_imports = [make_opsetid(\"\", 14), make_opsetid(new_domain, 1)]\n\n# Let's define a function for a linear regression\n# The first step consists in creating a constant\n# equal to the input parameter of the function.\ncst = make_node('Constant',  [], ['B'])\n\natt = AttributeProto()\natt.name = \"value\"\n\n# This line indicates the value comes from the argument\n# named 'bias' the function is given.\natt.ref_attr_name = \"bias\"\natt.type = AttributeProto.TENSOR\ncst.attribute.append(att)\n\nnode1 = make_node('MatMul', ['X', 'A'], ['XA'])\nnode2 = make_node('Add', ['XA', 'B'], ['Y'])\n\nlinear_regression = make_function(\n    new_domain,            # domain name\n    'LinearRegression',     # function name\n    ['X', 'A'],             # input names\n    ['Y'],                  # output names\n    [cst, node1, node2],    # nodes\n    opset_imports,          # opsets\n    [\"bias\"])               # attribute names\n\n# Let's use it in a graph.\n\nX = make_tensor_value_info('X', TensorProto.FLOAT, [None, None])\nA = make_tensor_value_info('A', TensorProto.FLOAT, [None, None])\nB = make_tensor_value_info('B', TensorProto.FLOAT, [None, None])\nY = make_tensor_value_info('Y', TensorProto.FLOAT, [None])\n\ngraph = make_graph(\n    [make_node('LinearRegression', ['X', 'A'], ['Y1'], domain=new_domain,\n               # bias is now an argument of the function and is defined as a tensor\n               bias=make_tensor('former_B', TensorProto.FLOAT, [1], [0.67])),\n     make_node('Abs', ['Y1'], ['Y'])],\n    'example',\n    [X, A], [Y])\n\nonnx_model = make_model(\n    graph, opset_imports=opset_imports,\n    functions=[linear_regression])  # functions to add)\ncheck_model(onnx_model)\n\n# the work is done, let's display it...\nprint(onnx_model)\n```\n\n----------------------------------------\n\nTITLE: Inspecting Model Metadata (Python)\nDESCRIPTION: This code snippet shows how to retrieve metadata of a model before downloading it, using `hub.get_model_info`. This provides information like model SHA, size, tags, and input/output ports.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/Hub.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nprint(hub.get_model_info(model=\"mnist\", opset=8))\n```\n\n----------------------------------------\n\nTITLE: Installing ONNX with Conda\nDESCRIPTION: This command installs the ONNX package from the conda-forge channel using the conda package manager.\nSOURCE: https://github.com/onnx/onnx/blob/main/INSTALL.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\nconda install -c conda-forge onnx\n```\n\n----------------------------------------\n\nTITLE: Extending OpSchema for Data Propagation in C++\nDESCRIPTION: This code snippet demonstrates how to extend the OpSchema class to include a PartialDataPropagationFunction, enabling data computation for operators during shape inference. It shows the addition of a new function pointer to the OpSchema and an example of its usage within the Shape operator's schema definition.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/proposals/SymbolicShapeInfProposal.md#_snippet_0\n\nLANGUAGE: c++\nCODE:\n```\nusing DataPropagationFunction = std::function<void(DataPropagationContext&)>  \n\nclass OpSchema final {  \n\n public:  \n  .  \n  .  \n  .  \n\n  OpSchema&PartialDataPropagationFunction(DataPropagationFunctiondataPropagationFunction) {  \n  partial_data_propagation_function_=std::move(dataPropagationFunction);  \n  return*this;  \n  }  \n\n  DataPropagationFunctionGetDataPropagationFunction()const{\nreturnpartial_data_propagation_function_? partial_data_propagation_function_:dummyDataPropagator;  \n  }  \n} \n\n// Operator schema example \nONNX_OPERATOR_SET_SCHEMA(  \nShape,  \n13,  \nOpSchema()  \n.SetDoc()  \n.Input(0, \"data\", \"Aninputtensor.\", \"T\", . . .)  \n.Output(0, \"shape\", \"Shapeoftheinputtensor\", \"T1\", . . .)  \n.TypeConstraint(\"T\", OpSchema::all_tensor_types())  \n.TypeConstraint(\"T1\", {\"tensor(int64)\"})  \n.TypeAndShapeInferenceFunction([](InferenceContext&ctx){  \n        . . .  \n})\n\n.PartialDataPropagationFunction([](DataPropagationContext&ctx){  \n          TensorShapePrototp; \n          // compute output data for shape operator \n          // add computed data to DataPropagationContext for propagating it downstream  \nctx.addOutputData(0,std::move(tp));  \n}));\n```\n\n----------------------------------------\n\nTITLE: Testing LabelEncoder ONNX-ML Operator with Strings and Integers in Python\nDESCRIPTION: This snippet demonstrates testing the LabelEncoder node using string keys and integer values. It shows two cases: one with a default value and one without. The `expect` function is used to verify the output after encoding string inputs to integer outputs.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/TestCoverage-ml.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nnode = onnx.helper.make_node(\n    \"LabelEncoder\",\n    inputs=[\"X\"],\n    outputs=[\"Y\"],\n    domain=\"ai.onnx.ml\",\n    keys_strings=[\"a\", \"b\", \"c\"],\n    values_int64s=[0, 1, 2],\n    default_int64=42,\n)\nx = np.array([\"a\", \"b\", \"d\", \"c\", \"g\"]).astype(object)\ny = np.array([0, 1, 42, 2, 42]).astype(np.int64)\nexpect(\n    node,\n    inputs=[x],\n    outputs=[y],\n    name=\"test_ai_onnx_ml_label_encoder_string_int\",\n)\n\nnode = onnx.helper.make_node(\n    \"LabelEncoder\",\n    inputs=[\"X\"],\n    outputs=[\"Y\"],\n    domain=\"ai.onnx.ml\",\n    keys_strings=[\"a\", \"b\", \"c\"],\n    values_int64s=[0, 1, 2],\n)\nx = np.array([\"a\", \"b\", \"d\", \"c\", \"g\"]).astype(object)\ny = np.array([0, 1, -1, 2, -1]).astype(np.int64)\nexpect(\n    node,\n    inputs=[x],\n    outputs=[y],\n    name=\"test_ai_onnx_ml_label_encoder_string_int_no_default\",\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Python Build Package\nDESCRIPTION: This command installs the `build` package using pip. The `build` package is used to generate source distributions.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/OnnxReleases.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\npip install build\n```\n\n----------------------------------------\n\nTITLE: Generate Protobuf Files with Custom Function\nDESCRIPTION: Calls the `RELATIVE_PROTOBUF_GENERATE_CPP` function for the main ONNX protobuf file (onnx.in.proto), operators protobuf file (onnx-operators.in.proto), and data protobuf file (onnx-data.in.proto).  The generated source and header files are appended to ONNX_PROTO_SRCS and ONNX_PROTO_HDRS lists respectively.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_30\n\nLANGUAGE: cmake\nCODE:\n```\nrelative_protobuf_generate_cpp(gen_onnx_proto\n                               __tmp_srcs\n                               __tmp_hdrs\n                               ${ONNX_ROOT}\n                               \"\"\n                               onnx/onnx.in.proto)\nlist(APPEND ONNX_PROTO_SRCS ${__tmp_srcs})\nlist(APPEND ONNX_PROTO_HDRS ${__tmp_hdrs})\n\nrelative_protobuf_generate_cpp(gen_onnx_operators_proto\n                               __tmp_srcs\n                               __tmp_hdrs\n                               ${ONNX_ROOT}\n                               gen_onnx_proto\n                               onnx/onnx-operators.in.proto)\nlist(APPEND ONNX_PROTO_SRCS ${__tmp_srcs})\nlist(APPEND ONNX_PROTO_HDRS ${__tmp_hdrs})\n\nrelative_protobuf_generate_cpp(gen_onnx_data_proto\n                               __tmp_srcs\n                               __tmp_hdrs\n                               ${ONNX_ROOT}\n                               gen_onnx_proto\n                               onnx/onnx-data.in.proto)\nlist(APPEND ONNX_PROTO_SRCS ${__tmp_srcs})\nlist(APPEND ONNX_PROTO_HDRS ${__tmp_hdrs})\n```\n\n----------------------------------------\n\nTITLE: Installing ONNX package using pip\nDESCRIPTION: This command demonstrates how to install the ONNX Python package using pip. It includes an option to install dependencies for the reference implementation. This allows users to easily integrate ONNX into their Python projects.\nSOURCE: https://github.com/onnx/onnx/blob/main/README.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install onnx # or pip install onnx[reference] for optional reference implementation dependencies\n```\n\n----------------------------------------\n\nTITLE: Downloading from Custom Repositories (Python)\nDESCRIPTION: This code snippet demonstrates how to download a model from a custom repository or a specific branch/commit of the main model hub. The `repo` parameter is used to specify the repository details.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/Hub.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel = hub.load(\"resnet50\", repo=\"onnx/models:771185265efbdc049fb223bd68ab1aeb1aecde76\")\n```\n\n----------------------------------------\n\nTITLE: ONNX ReferenceEvaluator Debugging\nDESCRIPTION: Shows how to use the verbose mode of the ReferenceEvaluator for debugging ONNX models. This mode likely prints intermediate results during the model execution. It initializes the evaluator with verbose=1.\nSOURCE: https://github.com/onnx/onnx/blob/main/onnx/reference/README.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom onnx.reference import ReferenceEvaluator\n\nX = np.array(...)\nsess = ReferenceEvaluator(\"model.onnx\", verbose=1)\nresults = sess.run(None, {\"X\": X})\nprint(results[0])\n```\n\n----------------------------------------\n\nTITLE: Define PyTorch Model with Optional Tensor Input (Example 1)\nDESCRIPTION: This example demonstrates a PyTorch model that accepts an optional Tensor as input. The model adds the optional tensor to the input tensor if it is not None, otherwise, it returns the input tensor. The TorchScript and ONNX graphs are also provided for context.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/ONNXTypes.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass Model(torch.nn.Module):\n    def forward(self, x, y:Optional[Tensor]=None):\n        if y is not None:\n            return x + y\n        return x\n```\n\n----------------------------------------\n\nTITLE: Define PyTorch Model with Optional Tensor Output (Example 2)\nDESCRIPTION: This example defines a PyTorch model that produces an optional Tensor as output. The `encoder_states` Tensor is conditionally assigned based on the `return_all_hiddens` flag. The model returns both the `src_tokens` and the potentially optional `encoder_states` tensor. The TorchScript and ONNX graphs are also provided.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/ONNXTypes.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass Model(torch.nn.Module):\n    def forward(\n            self,\n            src_tokens,\n            return_all_hiddens=torch.tensor([False]),\n    ):\n        encoder_states: Optional[Tensor] = None\n        if return_all_hiddens:\n            encoder_states = src_tokens\n\n        return src_tokens, encoder_states\n```\n\n----------------------------------------\n\nTITLE: Defining TensorShapeProto message in ONNX\nDESCRIPTION: This code snippet shows the structure of the TensorShapeProto message in ONNX, which defines the shape of a tensor. It includes a nested Dimension message that can hold either an integer value or a dimension parameter as a string. This allows for static or dynamic shape definitions.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/IR.md#_snippet_1\n\nLANGUAGE: protobuf\nCODE:\n```\nmessage TensorShapeProto {\n  message Dimension {\n    oneof value {\n      int64 dim_value = 1;\n      string dim_param = 2;\n    };\n  };\n  repeated Dimension dim = 1;\n}\n```\n\n----------------------------------------\n\nTITLE: Testing ArrayFeatureExtractor ONNX-ML Operator in Python\nDESCRIPTION: This snippet demonstrates how to create and test an ArrayFeatureExtractor node in ONNX-ML using the onnx and numpy libraries. It defines the node, creates input data, performs the operation, and uses the `expect` function to verify the output against the expected result.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/TestCoverage-ml.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nnode = onnx.helper.make_node(\n    \"ArrayFeatureExtractor\",\n    inputs=[\"x\", \"y\"],\n    outputs=[\"z\"],\n    domain=\"ai.onnx.ml\",\n)\n\nx = np.arange(12).reshape((3, 4)).astype(np.float32)\ny = np.array([0, 1], dtype=np.int64)\nz = np.array([[0, 4, 8], [1, 5, 9]], dtype=np.float32).T\nexpect(\n    node,\n    inputs=[x, y],\n    outputs=[z],\n    name=\"test_ai_onnx_ml_array_feature_extractor\",\n)\n```\n\n----------------------------------------\n\nTITLE: Installing ONNX from a Local PyPI Server\nDESCRIPTION: This command uninstalls any existing ONNX installation and installs ONNX from a local PyPI server.  The `--index-url` flag specifies the base URL of the server's simple index, and `--pre` includes pre-release versions.  The `-y` flag automatically confirms the uninstallation.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/OnnxReleases.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npip uninstall -y onnx && pip install --index-url http://127.0.0.1:80/simple/ --pre onnx\n```\n\n----------------------------------------\n\nTITLE: Building ONNX from Source (Windows)\nDESCRIPTION: These commands clone the ONNX repository, initialize submodules, set `CMAKE_ARGS` to prefer lite proto and use shared protobuf, and install ONNX in editable mode using pip.\nSOURCE: https://github.com/onnx/onnx/blob/main/INSTALL.md#_snippet_5\n\nLANGUAGE: text\nCODE:\n```\ngit clone https://github.com/onnx/onnx.git\ncd onnx\ngit submodule update --init --recursive\n# prefer lite proto\nset CMAKE_ARGS='-DONNX_USE_LITE_PROTO=ON -DONNX_USE_PROTOBUF_SHARED_LIBS=ON'\npip install -e . -v\n```\n\n----------------------------------------\n\nTITLE: Merging ONNX Models\nDESCRIPTION: This snippet demonstrates how to merge two ONNX models using `onnx.compose.merge_models`. It requires two ONNX models (`model1` and `model2`) and an `io_map` that specifies how the outputs of the first model are connected to the inputs of the second model. The resulting model combines the graphs of the two models.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md#_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\n\nmodel1 = onnx.load(\"path/to/model1.onnx\")\n# agraph (float[N] A, float[N] B) => (float[N] C, float[N] D)\n#   {\n#      C = Add(A, B)\n#      D = Sub(A, B)\n#   }\n\nmodel2 = onnx.load(\"path/to/model2.onnx\")\n#   agraph (float[N] X, float[N] Y) => (float[N] Z)\n#   {\n#      Z = Mul(X, Y)\n#   }\n\ncombined_model = onnx.compose.merge_models(\n    model1, model2,\n    io_map=[(\"C\", \"X\"), (\"D\", \"Y\")]\n)\n```\n\n----------------------------------------\n\nTITLE: Shape Inference for Large ONNX Model (>2GB)\nDESCRIPTION: This code demonstrates how to perform shape inference on a large ONNX model (>2GB) using `onnx.shape_inference.infer_shapes_path()`. It requires providing the path to the model file and ensuring that external data is in the same directory. The inferred model can be saved to a specified output path. Requires `onnx`.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md#_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\n\n# output the inferred model to the original model path\nonnx.shape_inference.infer_shapes_path(\"path/to/the/model.onnx\")\n\n# output the inferred model to the specified model path\nonnx.shape_inference.infer_shapes_path(\"path/to/the/model.onnx\", \"output/inferred/model.onnx\")\n\n# inferred_model = onnx.shape_inference.infer_shapes(loaded_onnx_model) will fail if given >2GB model\n```\n\n----------------------------------------\n\nTITLE: Building Protobuf from Source\nDESCRIPTION: This snippet downloads and builds Protobuf from source using FetchContent if ONNX_PROTOC_EXECUTABLE is not already set. It configures the build with specified URL and SHA1 hash.  It also handles patching the Protobuf CMakeLists.txt if needed. Finally, it resets some ONNX related variables after building protobuf.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_27\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT ONNX_PROTOC_EXECUTABLE)\n    set(Build_Protobuf ON)\n    set(protobuf_MSVC_STATIC_RUNTIME ${ONNX_USE_MSVC_STATIC_RUNTIME})\n\n    include(FetchContent)\n    set(ABSL_PROPAGATE_CXX_STD 1)\n    set(ONNX_BUILD_SHARED_LIBS ${BUILD_SHARED_LIBS})\n    set(ONNX_CMAKE_CXX_FLAGS ${CMAKE_CXX_FLAGS})\n    # Use this setting to build third-party libs.\n    set(BUILD_SHARED_LIBS ${ONNX_USE_PROTOBUF_SHARED_LIBS})\n    set(ProtobufURL https://github.com/protocolbuffers/protobuf/releases/download/v30.2/protobuf-30.2.tar.gz)\n    set(ProtobufSHA1 f846d1f047255d09142361ba6d1985336db5e407)\n    FetchContent_Declare(\n      Protobuf\n      URL ${ProtobufURL}\n      URL_HASH SHA1=${ProtobufSHA1}\n      PATCH_COMMAND \"${CMAKE_COMMAND}\" -Dprotobuf_MSVC_STATIC_RUNTIME=${protobuf_MSVC_STATIC_RUNTIME} -DFILENAME=CMakeLists.txt -P ${CMAKE_CURRENT_LIST_DIR}/cmake/ProtoBufPatch.cmake\n    )\n    set(protobuf_BUILD_TESTS OFF CACHE BOOL \"Build protobuf tests\" FORCE)\n    message(STATUS \"Download and build Protobuf from ${ProtobufURL}\")\n    FetchContent_MakeAvailable(Protobuf)\n    set(ONNX_PROTOC_EXECUTABLE $<TARGET_FILE:protobuf::protoc>)\n    set(Protobuf_VERSION \"6.30.2\")\n    # Change back the BUILD_SHARED_LIBS to control the onnx project.\n    set(BUILD_SHARED_LIBS ${ONNX_BUILD_SHARED_LIBS})\n    set(PROTOBUF_DIR \"${protobuf_BINARY_DIR}\")\n    set(CMAKE_CXX_FLAGS ${ONNX_CMAKE_CXX_FLAGS})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Building Protobuf from Source (Linux)\nDESCRIPTION: These commands clone the Protobuf repository, checkout a specific version, initialize submodules, configure the build with CMake to produce a static library with position independent code, build the library, and install it to /usr.\nSOURCE: https://github.com/onnx/onnx/blob/main/INSTALL.md#_snippet_11\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://github.com/protocolbuffers/protobuf.git\ncd protobuf\ngit checkout v5.29.2\ngit submodule update --init --recursive\nmkdir build_source && cd build_source\ncmake -Dprotobuf_BUILD_SHARED_LIBS=OFF -DCMAKE_INSTALL_PREFIX=/usr -Dprotobuf_BUILD_TESTS=OFF -DCMAKE_BUILD_TYPE=Release -DCMAKE_POSITION_INDEPENDENT_CODE=ON ..\ncmake --build . --target install\n```\n\n----------------------------------------\n\nTITLE: Define MyBackend struct with magic number and reference count in C\nDESCRIPTION: This code snippet defines a structure named `MyBackend` which includes a magic number and a reference count. This is a suggested implementation detail for vendors to manage backend objects. The magic number is used to verify that the backend object passed to `onnxInitGraph` was created by the same library, enhancing safety and preventing misuse.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/proposals/ONNXIFIproposal.md#_snippet_0\n\nLANGUAGE: c\nCODE:\n```\nstruct MyBackend {\n  uint32_t magic;\n  uint64_t referenceCount;\n  ...\n};\n\n/* This line won't compile, but gives you an idea of relation between MyBackend structure and onnxBackend type. */\ntypedef MyBackend* onnxBackend;\n```\n\n----------------------------------------\n\nTITLE: Installing ONNX from TestPyPI (Source)\nDESCRIPTION: This command uninstalls ONNX and installs it from source from TestPyPI.  The `--no-binary onnx` flag forces a source install. `-i` specifies the index URL, and `--pre` includes pre-release packages. \nSOURCE: https://github.com/onnx/onnx/blob/main/docs/OnnxReleases.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\npip uninstall -y onnx && pip install -i https://test.pypi.org/simple --no-binary onnx --pre onnx\n```\n\n----------------------------------------\n\nTITLE: Loading ONNX Model with External Data (Different Directory)\nDESCRIPTION: This code shows how to load an ONNX model with external data stored in a different directory. It first loads the model without external data using `onnx.load(..., load_external_data=False)`, then loads the external data using `load_external_data_for_model()`. Requires `onnx` and `onnx.external_data_helper`.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\nfrom onnx.external_data_helper import load_external_data_for_model\n\nonnx_model = onnx.load(\"path/to/the/model.onnx\", load_external_data=False)\nload_external_data_for_model(onnx_model, \"data/directory/path/\")\n# Then the onnx_model has loaded the external data from the specific directory\n```\n\n----------------------------------------\n\nTITLE: Parsing ONNX Model from Text\nDESCRIPTION: This snippet shows how to create an ONNX model from a textual representation using `onnx.parser.parse_model`. The `input` variable contains the textual representation of the model, including the IR version and opset imports. Requires the `onnx` library and a valid textual representation of an ONNX model.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md#_snippet_23\n\nLANGUAGE: python\nCODE:\n```\ninput = \"\"\"\n   <\n     ir_version: 7,\n     opset_import: [\"\" : 10]\n   >\n   agraph (float[N, 128] X, float[128, 10] W, float[10] B) => (float[N, 10] C)\n   {\n      T = MatMul(X, W)\n      S = Add(T, B)\n      C = Softmax(S)\n   }\n\"\"\"\nmodel = onnx.parser.parse_model(input)\n```\n\n----------------------------------------\n\nTITLE: Importing ONNX Hub in Python\nDESCRIPTION: This code snippet demonstrates how to import the ONNX Hub module in Python. This is the first step to using the ONNX Model Hub functionality.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/Hub.md#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom onnx import hub\n```\n\n----------------------------------------\n\nTITLE: Define and Configure onnx_proto Library\nDESCRIPTION: Defines the `onnx_proto` library, adds dependencies on the protobuf generation targets, and sets include directories. It also sets compiler definitions for ONNX_API based on the platform and build configuration, and links the appropriate protobuf library.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_31\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(onnx_proto ${ONNX_PROTO_SRCS} ${ONNX_PROTO_HDRS})\nadd_dependencies(onnx_proto gen_onnx_operators_proto gen_onnx_data_proto)\ntarget_include_directories(onnx_proto PUBLIC\n  $<BUILD_INTERFACE:${CMAKE_CURRENT_BINARY_DIR}>\n  $<INSTALL_INTERFACE:include>)\n\nif(MSVC)\n  if(BUILD_SHARED_LIBS OR ONNX_BUILD_MAIN_LIB)\n    set(ONNX_API_DEFINE \"-DONNX_API=__declspec(dllexport)\")\n  else()\n    set(ONNX_API_DEFINE \"-DONNX_API=\")\n  endif()\nelse()\n  # On non-Windows, hide all symbols we don't need\n  set(ONNX_API_DEFINE \"-DONNX_API=__attribute__((__visibility__(\\\"default\\\")))\")\n  set_target_properties(onnx_proto PROPERTIES CXX_VISIBILITY_PRESET hidden)\n  set_target_properties(onnx_proto PROPERTIES VISIBILITY_INLINES_HIDDEN 1)\nendif()\ntarget_compile_definitions(onnx_proto PRIVATE ${ONNX_API_DEFINE})\ntarget_compile_features(onnx_proto PUBLIC cxx_std_${CMAKE_CXX_STANDARD})\n\nset(LINKED_PROTOBUF_TARGET protobuf::libprotobuf)\nif(ONNX_USE_LITE_PROTO)\n  if(TARGET protobuf::libprotobuf-lite)\n    set(LINKED_PROTOBUF_TARGET protobuf::libprotobuf-lite)\n  endif()\nendif()\nif((NOT ONNX_USE_PROTOBUF_SHARED_LIBS) AND Build_Protobuf)\n  target_link_libraries(onnx_proto PRIVATE ${LINKED_PROTOBUF_TARGET})\n  target_include_directories(onnx_proto PUBLIC\n    $<BUILD_INTERFACE:$<TARGET_PROPERTY:${LINKED_PROTOBUF_TARGET},INCLUDE_DIRECTORIES>>\n  )\nelse()\n  target_link_libraries(onnx_proto PUBLIC ${LINKED_PROTOBUF_TARGET})\nendif()\nforeach(ABSL_USED_TARGET IN LISTS protobuf_ABSL_USED_TARGETS)\n  if(TARGET ${ABSL_USED_TARGET})\n    target_link_libraries(onnx_proto PRIVATE ${ABSL_USED_TARGET})\n  endif()\nendforeach()\nadd_onnx_global_defines(onnx_proto)\n```\n\n----------------------------------------\n\nTITLE: Building Protobuf from Source (Mac)\nDESCRIPTION: These commands clone the Protobuf repository, checkout a specific version, initialize submodules, configure the build with CMake to produce a static library with position independent code, build the library, and install it to /usr.\nSOURCE: https://github.com/onnx/onnx/blob/main/INSTALL.md#_snippet_13\n\nLANGUAGE: sh\nCODE:\n```\ngit clone --recursive https://github.com/onnx/onnx.git\ncd onnx\n# Optional: prefer lite proto\nset CMAKE_ARGS=-DONNX_USE_LITE_PROTO=ON\npip install -e . -v\n```\n\n----------------------------------------\n\nTITLE: Setting Compile Options for GCC\nDESCRIPTION: This snippet sets compile options for 'onnx' and 'onnx_proto' targets when not using MSVC. It enables `-Wall` and `-Wextra` warnings. If using GCC version 13 or greater, it disables the `-Wno-stringop-overflow` warning.  If `ONNX_WERROR` is enabled, it treats warnings as errors.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_37\n\nLANGUAGE: cmake\nCODE:\n```\nelse()\n  target_compile_options(onnx PRIVATE -Wall -Wextra)\n  target_compile_options(onnx_proto PRIVATE -Wall -Wextra)\n  if(CMAKE_COMPILER_IS_GNUCXX AND CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL 13)\n    target_compile_options(onnx PRIVATE \"-Wno-stringop-overflow\")\n    target_compile_options(onnx_proto PRIVATE \"-Wno-stringop-overflow\")\n  endif()\n  if(ONNX_WERROR)\n    target_compile_options(onnx PRIVATE \"-Werror\")\n    target_compile_options(onnx_proto PRIVATE \"-Werror\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Unity Builds\nDESCRIPTION: This section configures unity builds for the 'onnx' target, grouping source files to reduce compilation time. It excludes specific files that have issues with template explicit specialization. The `UNITY_BUILD_MODE` is set to `GROUP`, and the `UNITY_GROUP` property is set for the relevant source files. On MSVC, the `/bigobj` compiler option is added to handle potentially large object files.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_43\n\nLANGUAGE: cmake\nCODE:\n```\nif(ONNX_USE_UNITY_BUILD)\n  # If ONNX_USE_UNITY_BUILD is set to ON, set ONNX target to use Unity builds.\n  #  We set Unity build to use groups, it allows us to set some specific files to be compiled individually\n  set_target_properties(onnx\n    PROPERTIES\n      UNITY_BUILD ON\n      UNITY_BUILD_MODE GROUP\n  )\n\n  set(NEW_LIST __unity_src_files)\n  list(APPEND __unity_src_files ${ONNX_SRCS})\n  # These files have an issue with template explicit specialization after instantiation:\n  #   We take them out of the unity group so they are compiled individually.\n  list(REMOVE_ITEM __unity_src_files \"${ONNX_ROOT}/onnx/defs/schema.cc\")\n  list(REMOVE_ITEM __unity_src_files \"${ONNX_ROOT}/onnx/defs/tensor_proto_util.cc\")\n  set_source_files_properties(${__unity_src_files} PROPERTIES UNITY_GROUP \"Unity_Group\" )\n\n  # With unity build object file could get big, need this switch in MSVC.\n  if(MSVC)\n    target_compile_options(onnx PRIVATE /bigobj)\n  endif()\n\n# should be enabled for onnx_proto when protobuf can support Unity builds\n\nendif()\n```\n\n----------------------------------------\n\nTITLE: Listing ONNX Models (Python)\nDESCRIPTION: These code snippets demonstrate how to list available models in the ONNX Model Zoo. The `hub.list_models` function is used to query models based on different criteria such as repo, model name, and tags. This doesn't download the actual models.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/Hub.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# List all models in the onnx/models:main repo\nall_models = hub.list_models()\n\n# List all versions/opsets of a specific model\nmnist_models = hub.list_models(model=\"mnist\")\n\n# List all models matching a given \"tag\"\nvision_models = hub.list_models(tags=[\"vision\"])\n```\n\n----------------------------------------\n\nTITLE: Testing Binarizer ONNX-ML Operator in Python\nDESCRIPTION: This snippet shows how to create and test a Binarizer node in ONNX-ML with a specified threshold. It initializes the node, generates random input data, computes the expected binarized output using a `compute_binarizer` function (not shown), and asserts that the node produces the correct output using the `expect` function.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/TestCoverage-ml.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nthreshold = 1.0\nnode = onnx.helper.make_node(\n    \"Binarizer\",\n    inputs=[\"X\"],\n    outputs=[\"Y\"],\n    threshold=threshold,\n    domain=\"ai.onnx.ml\",\n)\nx = np.random.randn(3, 4, 5).astype(np.float32)\ny = compute_binarizer(x, threshold)[0]\n\nexpect(node, inputs=[x], outputs=[y], name=\"test_ai_onnx_ml_binarizer\")\n```\n\n----------------------------------------\n\nTITLE: Listing ONNX Proto Classes\nDESCRIPTION: This snippet prints a list of ONNX proto classes ending with 'Proto' (excluding those starting with '_') using the `dir` function to inspect the `onnx` module and `pprint` for formatted output.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n    import onnx\n    import pprint\n    pprint.pprint([p for p in dir(onnx)\n                   if p.endswith('Proto') and p[0] != '_'])\n```\n\n----------------------------------------\n\nTITLE: Configuring and Installing CMake Configuration Files\nDESCRIPTION: This snippet configures the `ONNXConfigVersion.cmake.in` and `ONNXConfig.cmake.in` files, replacing variables using `@ONLY`, and installs them to the `${CMAKE_INSTALL_LIBDIR}/cmake/ONNX` directory. It also installs the `ONNXTargets` export set.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_41\n\nLANGUAGE: cmake\nCODE:\n```\nconfigure_file(\n  ${PROJECT_SOURCE_DIR}/cmake/ONNXConfigVersion.cmake.in\n  ${PROJECT_BINARY_DIR}/ONNXConfigVersion.cmake\n  @ONLY)\nconfigure_file(\n  ${PROJECT_SOURCE_DIR}/cmake/ONNXConfig.cmake.in\n  ${PROJECT_BINARY_DIR}/ONNXConfig.cmake\n  @ONLY)\ninstall(FILES\n  ${PROJECT_BINARY_DIR}/ONNXConfigVersion.cmake\n  ${PROJECT_BINARY_DIR}/ONNXConfig.cmake\n  DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/ONNX\n  COMPONENT dev)\ninstall(EXPORT ONNXTargets\n  DESTINATION \"${CMAKE_INSTALL_LIBDIR}/cmake/ONNX\"\n  NAMESPACE ONNX::\n)\n```\n\n----------------------------------------\n\nTITLE: Transpose Operation Denotation Propagation (Pseudo-code)\nDESCRIPTION: This pseudo-code demonstrates how dimension denotations are propagated through a Transpose operation. The output dimension denotation is inferred based on the input dimension denotation and the permutation indices.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/DimensionDenotation.md#_snippet_0\n\nLANGUAGE: Pseudocode\nCODE:\n```\nfor i, j in enumerate(perm):\n    out_dim_denotaion[i] = in_dim_denotation[j]\n```\n\n----------------------------------------\n\nTITLE: Setting CMAKE_POSITION_INDEPENDENT_CODE\nDESCRIPTION: This snippet ensures that the CMAKE_POSITION_INDEPENDENT_CODE flag is set to ON, which is necessary for building shared libraries with position-independent code.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_23\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT DEFINED CMAKE_POSITION_INDEPENDENT_CODE)\n  set(CMAKE_POSITION_INDEPENDENT_CODE ON)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Uploading Source Distribution to TestPyPI\nDESCRIPTION: This command uploads the generated source distribution file to TestPyPI using `twine`. It requires TestPyPI credentials and the `dist/*` wildcard specifies all files in the dist directory should be uploaded.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/OnnxReleases.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\ntwine upload --repository testpypi --verbose -u <YOUR_TESTPYPI_USER> dist/*\n```\n\n----------------------------------------\n\nTITLE: Tensor Sharding split implementation - Python\nDESCRIPTION: This python code shows how the sharding is performed when multiple sharding axes are provided. It iterates through the number of shards for each sharded axis, calculates the indices, and splits the input tensor accordingly.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/proposals/ONNXMultiDeviceProposal.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nsplit_tensors = []\nfor a in range(num_shards_a):\n    a_width = input.shape[axis0] / num_shards_a\n    a_index = a * a_width\n    for b in range(num_shards_b):\n        b_width = input.shape[axis1] / num_shards_b\n        b_index =  b * b_width\n        split = input[a_index : a_index + a_width, b_index : b_index + b_width]\n        split_tensors.append(split)\n```\n\n----------------------------------------\n\nTITLE: Testing TreeEnsemble ONNX-ML Operator with Set Membership in Python\nDESCRIPTION: This code tests the TreeEnsemble operator's set membership functionality.  It defines a TreeEnsemble node with specific attributes for set membership evaluation, provides input data, and validates the output against the expected result using `expect`. Key parameters include membership_values, nodes_modes, nodes_featureids, nodes_splits, and leaf_weights which define the tree structure and how input features are evaluated.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/TestCoverage-ml.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nnode = onnx.helper.make_node(\n    \"TreeEnsemble\",\n    [\"X\"],\n    [\"Y\"],\n    domain=\"ai.onnx.ml\",\n    n_targets=4,\n    aggregate_function=1,\n    membership_values=make_tensor(\n        \"membership_values\",\n        onnx.TensorProto.FLOAT,\n        (8,),\n        [1.2, 3.7, 8, 9, np.nan, 12, 7, np.nan],\n    ),\n    nodes_missing_value_tracks_true=None,\n    nodes_hitrates=None,\n    post_transform=0,\n    tree_roots=[0],\n    nodes_modes=make_tensor(\n        \"nodes_modes\",\n        onnx.TensorProto.UINT8,\n        (3,),\n        np.array([0, 6, 6], dtype=np.uint8),\n    ),\n    nodes_featureids=[0, 0, 0],\n    nodes_splits=make_tensor(\n        \"nodes_splits\",\n        onnx.TensorProto.FLOAT,\n        (3,),\n        np.array([11, 232344.0, np.nan], dtype=np.float32),\n    ),\n    nodes_trueleafs=[0, 1, 1],\n    nodes_truenodeids=[1, 0, 1],\n    nodes_falseleafs=[1, 0, 1],\n    nodes_falsenodeids=[2, 2, 3],\n    leaf_targetids=[0, 1, 2, 3],\n    leaf_weights=make_tensor(\n        \"leaf_weights\", onnx.TensorProto.FLOAT, (4,), [1, 10, 1000, 100]\n    ),\n)\n\nx = np.array([1.2, 3.4, -0.12, np.nan, 12, 7], np.float32).reshape(-1, 1)\nexpected = np.array(\n    [\n        [1, 0, 0, 0],\n        [0, 0, 0, 100],\n        [0, 0, 0, 100],\n        [0, 0, 1000, 0],\n        [0, 0, 1000, 0],\n        [0, 10, 0, 0],\n    ],\n    dtype=np.float32,\n)\nexpect(\n    node,\n    inputs=[x],\n    outputs=[expected],\n    name=\"test_ai_onnx_ml_tree_ensemble_set_membership\",\n)\n```\n\n----------------------------------------\n\nTITLE: Import ONNX Modules\nDESCRIPTION: Imports the necessary modules from the `onnx` library. These modules provide functionalities for creating and manipulating ONNX models.\nSOURCE: https://github.com/onnx/onnx/blob/main/examples/make_model.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\nfrom onnx import helper\nfrom onnx import AttributeProto, TensorProto, GraphProto\n```\n\n----------------------------------------\n\nTITLE: Set LD_LIBRARY_PATH for C++ tests (Linux/MacOS)\nDESCRIPTION: Sets the LD_LIBRARY_PATH environment variable to include the location of the built ONNX libraries.  This is required for running the C++ tests (googletest) because they dynamically link to these libraries. The export ensures the path is available for the current session.\nSOURCE: https://github.com/onnx/onnx/blob/main/CONTRIBUTING.md#_snippet_9\n\nLANGUAGE: sh\nCODE:\n```\nexport LD_LIBRARY_PATH=\"./.setuptools-cmake-build/:$LD_LIBRARY_PATH\"\n.setuptools-cmake-build/onnx_gtests\n```\n\n----------------------------------------\n\nTITLE: Getting the Cache Location (Python)\nDESCRIPTION: This code snippet shows how to inspect the current cache location using the `hub.get_dir` function. This is useful for verifying where the models are being stored.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/Hub.md#_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nprint(hub.get_dir())\n```\n\n----------------------------------------\n\nTITLE: Configuring Protobuf Executable Path\nDESCRIPTION: This snippet configures the path to the protoc executable. It first checks if a custom protoc executable is defined. If not, it checks if the protobuf::protoc target exists and sets the ONNX_PROTOC_EXECUTABLE variable accordingly.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_26\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT ONNX_BUILD_CUSTOM_PROTOBUF)\n  if((ONNX_USE_LITE_PROTO AND TARGET protobuf::libprotobuf-lite) OR ((NOT ONNX_USE_LITE_PROTO) AND TARGET protobuf::libprotobuf))\n    # Sometimes we need to use protoc compiled for host architecture while linking\n    # libprotobuf against target architecture. See https://github.com/caffe2/caffe\n    # 2/blob/96f35ad75480b25c1a23d6e9e97bccae9f7a7f9c/cmake/ProtoBuf.cmake#L92-L99\n    if(EXISTS \"${ONNX_CUSTOM_PROTOC_EXECUTABLE}\")\n      message(STATUS \"Using custom protoc executable\")\n      set(ONNX_PROTOC_EXECUTABLE ${ONNX_CUSTOM_PROTOC_EXECUTABLE})\n    else()\n      if(TARGET protobuf::protoc)\n        set(ONNX_PROTOC_EXECUTABLE $<TARGET_FILE:protobuf::protoc>)\n      endif()\n    endif()\n  else()\n    # Customized version of find Protobuf. We need to avoid situations mentioned\n    # in https://github.com/caffe2/caffe2/blob/b7d983f255ef5496474f1ea188edb5e0ac4\n    # 42761/cmake/ProtoBuf.cmake#L82-L92 The following section is stolen from\n    # cmake/ProtoBuf.cmake in Caffe2\n    find_program(Protobuf_PROTOC_EXECUTABLE\n                NAMES protoc\n                DOC \"The Google Protocol Buffers Compiler\")\n\n    # Only if protoc was found, seed the include directories and libraries. We\n    # assume that protoc is installed at PREFIX/bin. We use get_filename_component\n    # to resolve PREFIX.\n    if(Protobuf_PROTOC_EXECUTABLE)\n      set(ONNX_PROTOC_EXECUTABLE ${Protobuf_PROTOC_EXECUTABLE})\n      get_filename_component(_PROTOBUF_INSTALL_PREFIX\n                            ${Protobuf_PROTOC_EXECUTABLE} DIRECTORY)\n      get_filename_component(_PROTOBUF_INSTALL_PREFIX\n                            ${_PROTOBUF_INSTALL_PREFIX}/.. REALPATH)\n      find_library(Protobuf_PROTOC_LIBRARY\n                  NAMES protoc\n                  PATHS ${_PROTOBUF_INSTALL_PREFIX}/${CMAKE_INSTALL_LIBDIR}\n                  NO_DEFAULT_PATH)\n      if(ONNX_USE_LITE_PROTO)\n        find_library(Protobuf_LITE_LIBRARY\n          NAMES protobuf-lite\n          PATHS ${_PROTOBUF_INSTALL_PREFIX}/${CMAKE_INSTALL_LIBDIR}\n          NO_DEFAULT_PATH)\n      else()\n        find_library(Protobuf_LIBRARY\n          NAMES protobuf\n          PATHS ${_PROTOBUF_INSTALL_PREFIX}/${CMAKE_INSTALL_LIBDIR}\n          NO_DEFAULT_PATH)\n      endif(ONNX_USE_LITE_PROTO)\n      find_path(Protobuf_INCLUDE_DIR google/protobuf/service.h\n                PATHS ${_PROTOBUF_INSTALL_PREFIX}/include\n                NO_DEFAULT_PATH)\n      if(ONNX_USE_PROTOBUF_SHARED_LIBS)\n        set(Protobuf_USE_STATIC_LIBS OFF)\n      else()\n        set(Protobuf_USE_STATIC_LIBS ON)\n      endif()\n      find_package(Protobuf)\n      if(Protobuf_FOUND)\n        set(PROTOBUF_DIR \"${_PROTOBUF_INSTALL_PREFIX}\")\n        set(Build_Protobuf OFF)\n        if(\"${Protobuf_VERSION}\" VERSION_GREATER_EQUAL \"4.22.0\")\n          # There are extra dependencies for protobuf.\n          find_package(absl REQUIRED)\n          find_package(utf8_range)\n          message(STATUS \"absl_VERSION: ${absl_VERSION}\")\n          set(protobuf_ABSL_USED_TARGETS\n            absl::absl_check\n            absl::absl_log\n            absl::algorithm\n            absl::base\n            absl::bind_front\n            absl::bits\n            absl::btree\n            absl::cleanup\n            absl::cord\n            absl::core_headers\n            absl::debugging\n            absl::die_if_null\n            absl::dynamic_annotations\n            absl::flags\n            absl::flat_hash_map\n            absl::flat_hash_set\n            absl::function_ref\n            absl::hash\n            absl::layout\n            absl::log_initialize\n            absl::log_severity\n            absl::memory\n            absl::node_hash_map\n            absl::node_hash_set\n            absl::optional\n            absl::span\n            absl::status\n            absl::statusor\n            absl::strings\n            absl::synchronization\n            absl::time\n            absl::type_traits\n            absl::utility\n            absl::variant\n            utf8_range::utf8_range\n            utf8_range::utf8_validity\n          )\n        endif()\n      endif()\n    endif()\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Uploading Wheels to a Local PyPI Server\nDESCRIPTION: This command uploads wheel files to a local PyPI server using `twine`.  It uses fake credentials as the local server doesn't require authentication. `--repository-url` specifies the address of the server.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/OnnxReleases.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ntwine upload --repository-url http://127.0.0.1:80 --verbose -u fake -p fake *.whl\n```\n\n----------------------------------------\n\nTITLE: Sharding Spec Example for Add Operation\nDESCRIPTION: This example demonstrates a sharding specification for an Add operation with two sharded dimensions (axis 0 and axis 1), each divided into two shards across four devices. It aims to distribute the output shards such that each resides on a single device.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/proposals/ShardingFormalism.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"device\" = [0, 1, 2, 3]\n    \"sharded_dim\" =[\n        {\n            \"axis\" = 0\n            \"simple_sharding\" =\n            [\n                {\n                    \"num_shards\" = 2\n                }\n            ]\n        }\n        {\n            \"axis\" = 1\n            \"simple_sharding\" =\n            [\n                {\n                    \"num_shards\" = 2\n                }\n            ]\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Testing TreeEnsemble ONNX-ML Operator with a Single Tree in Python\nDESCRIPTION: This snippet creates and tests a TreeEnsemble node representing a single decision tree. It defines the structure of the tree using attributes like `nodes_modes`, `nodes_featureids`, `nodes_splits`, `leaf_targetids`, and `leaf_weights`. The snippet then provides sample input and asserts that the node's output matches the expected output.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/TestCoverage-ml.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nnode = onnx.helper.make_node(\n    \"TreeEnsemble\",\n    [\"X\"],\n    [\"Y\"],\n    domain=\"ai.onnx.ml\",\n    n_targets=2,\n    membership_values=None,\n    nodes_missing_value_tracks_true=None,\n    nodes_hitrates=None,\n    aggregate_function=1,\n    post_transform=0,\n    tree_roots=[0],\n    nodes_modes=make_tensor(\n        \"nodes_modes\",\n        onnx.TensorProto.UINT8,\n        (3,),\n        np.array([0, 0, 0], dtype=np.uint8),\n    ),\n    nodes_featureids=[0, 0, 0],\n    nodes_splits=make_tensor(\n        \"nodes_splits\",\n        onnx.TensorProto.DOUBLE,\n        (3,),\n        np.array([3.14, 1.2, 4.2], dtype=np.float64),\n    ),\n    nodes_truenodeids=[1, 0, 1],\n    nodes_trueleafs=[0, 1, 1],\n    nodes_falsenodeids=[2, 2, 3],\n    nodes_falseleafs=[0, 1, 1],\n    leaf_targetids=[0, 1, 0, 1],\n    leaf_weights=make_tensor(\n        \"leaf_weights\",\n        onnx.TensorProto.DOUBLE,\n        (4,),\n        np.array([5.23, 12.12, -12.23, 7.21], dtype=np.float64),\n    ),\n)\n\nx = np.array([1.2, 3.4, -0.12, 1.66, 4.14, 1.77], np.float64).reshape(3, 2)\ny = np.array([[5.23, 0], [5.23, 0], [0, 12.12]], dtype=np.float64)\nexpect(\n    node,\n    inputs=[x],\n    outputs=[y],\n    name=\"test_ai_onnx_ml_tree_ensemble_single_tree\",\n)\n```\n\n----------------------------------------\n\nTITLE: Converting ONNX Tensor Type to NumPy Data Type\nDESCRIPTION: This snippet demonstrates how to convert an ONNX tensor data type (represented by `TensorProto.FLOAT`) to its corresponding NumPy data type using `tensor_dtype_to_np_dtype`. It also shows how to convert the tensor type to a string representation.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n    from onnx import TensorProto\n    from onnx.helper import tensor_dtype_to_np_dtype, tensor_dtype_to_string\n\n    np_dtype = tensor_dtype_to_np_dtype(TensorProto.FLOAT)\n    print(f\"The converted numpy dtype for {tensor_dtype_to_string(TensorProto.FLOAT)} is {np_dtype}.\")\n```\n\n----------------------------------------\n\nTITLE: Configuring FindPython3\nDESCRIPTION: This section configures FindPython3.cmake to find Python 3 in a virtual environment first, and use location based strategy.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_20\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT DEFINED Python3_FIND_VIRTUALENV)\n  set(Python3_FIND_VIRTUALENV FIRST)\nendif()\n\nif(NOT DEFINED Python3_FIND_STRATEGY)\n  set(Python3_FIND_STRATEGY LOCATION)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Uploading Source Distribution to a Local PyPI Server\nDESCRIPTION: This command uploads the source distribution to a local PyPI server for testing purposes. The `--repository-url` flag specifies the URL of the local server, and fake credentials are used as authentication is not required.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/OnnxReleases.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ntwine upload --repository-url http://127.0.0.1:80 --verbose -u fake -p fake dist/*\n```\n\n----------------------------------------\n\nTITLE: Setting ONNX Metadata Properties - Python\nDESCRIPTION: This code loads an ONNX model from a file and then sets various metadata properties such as `model_version`, `producer_name`, `producer_version`, and `doc_string`. It also adds custom metadata properties using `helper.set_model_props`. It depends on the `onnx` and `onnx.helper` packages and assumes a file named 'linear_regression.onnx' exists.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nfrom onnx import load, helper\n\nwith open(\"linear_regression.onnx\", \"rb\") as f:\n    onnx_model = load(f)\n\nonnx_model.model_version = 15\nonnx_model.producer_name = \"something\"\nonnx_model.producer_version = \"some other thing\"\nonnx_model.doc_string = \"documentation about this model\"\nprop = onnx_model.metadata_props\n\ndata = dict(key1=\"value1\", key2=\"value2\")\nhelper.set_model_props(onnx_model, data)\n\nprint(onnx_model)\n```\n\n----------------------------------------\n\nTITLE: Setting MSVC Compiler Flags\nDESCRIPTION: This conditional block sets MSVC specific compiler flags. If exceptions are not disabled, it appends `/EHsc /wd26812` to the compiler flags.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_17\n\nLANGUAGE: cmake\nCODE:\n```\nif(MSVC)\n  if(NOT ONNX_DISABLE_EXCEPTIONS)\n    string(APPEND CMAKE_CXX_FLAGS \" /EHsc /wd26812\")\n    string(APPEND CMAKE_C_FLAGS \" /EHsc /wd26812\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Inlining Local Functions in ONNX Model\nDESCRIPTION: This snippet demonstrates how to inline local functions in an ONNX model using `onnx.inliner.inline_local_functions`. It loads an ONNX model, inlines the functions, and saves the modified model to a new file. Requires the `onnx` and `onnx.inliner` libraries and a model with functions that can be inlined.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md#_snippet_24\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\nimport onnx.inliner\n\nmodel = onnx.load(\"path/to/the/model.onnx\")\ninlined = onnx.inliner.inline_local_functions(model)\nonnx.save(\"path/to/the/inlinedmodel.onnx\")\n```\n\n----------------------------------------\n\nTITLE: Save TensorProto to File\nDESCRIPTION: Saves an ONNX TensorProto to a file in binary format. The `SerializeToString()` method is used to serialize the TensorProto. The file is saved to the `resources` directory.\nSOURCE: https://github.com/onnx/onnx/blob/main/examples/np_array_tensorproto.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Save the TensorProto\nwith open(os.path.join(\"resources\", \"tensor.pb\"), \"wb\") as f:\n    f.write(tensor.SerializeToString())\n```\n\n----------------------------------------\n\nTITLE: Retrieving ONNX Versions and Opset - Python\nDESCRIPTION: This snippet demonstrates how to retrieve the ONNX library version, the current ONNX opset version, and the IR version. It imports the necessary modules from the onnx package and prints the values to the console. This is useful for determining the compatibility of ONNX models and operators.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/api/index.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom onnx import __version__, IR_VERSION\nfrom onnx.defs import onnx_opset_version\nprint(f\"onnx.__version__={__version__!r}, opset={onnx_opset_version()}, IR_VERSION={IR_VERSION}\")\n```\n\n----------------------------------------\n\nTITLE: Setting ONNX_ML Option\nDESCRIPTION: This block sets the ONNX_ML option, which enables the traditional ML API. It first checks if the environment variable ONNX_ML is defined, and if so, uses its value. Otherwise, it defaults to ON. The value of the option determines whether the traditional ML API is enabled.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT DEFINED ONNX_ML)\n  if(DEFINED ENV{ONNX_ML})\n    set(DEFAULT_ONNX_ML $ENV{ONNX_ML})\n  else()\n    set(DEFAULT_ONNX_ML ON)\n  endif()\n  option(ONNX_ML \"Enable traditional ML API.\" ${DEFAULT_ONNX_ML})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting ONNX_VERIFY_PROTO3 Option\nDESCRIPTION: This block sets the ONNX_VERIFY_PROTO3 option, which controls whether code is generated by proto3. It first checks if the environment variable ONNX_VERIFY_PROTO3 is defined, and if so, uses its value. Otherwise, it defaults to OFF. The value of the option determines whether code is generated by proto3.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_10\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT DEFINED ONNX_VERIFY_PROTO3)\n  if(DEFINED ENV{ONNX_VERIFY_PROTO3})\n    set(PROTO3_ENABLED $ENV{ONNX_VERIFY_PROTO3})\n  else()\n    set(PROTO3_ENABLED OFF)\n  endif()\n  option(ONNX_VERIFY_PROTO3 \"Generate code by proto3\" ${PROTO3_ENABLED})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Packing and Unpacking Float4 Values in C-like Code\nDESCRIPTION: This snippet illustrates how two consecutive Float4 elements, 'x' and 'y', are packed into a single byte 'z', and how a packed byte 'z' is unpacked to retrieve the original Float4 values 'x' and 'y'. It demonstrates bitwise operations for efficient storage and retrieval of 4-bit values.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/technical/float4.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\npack(x,y): y << 4 | x & 0x0F\nunpack(z): x = z & 0x0F, y = z >> 4\n```\n\n----------------------------------------\n\nTITLE: Defining Build Options\nDESCRIPTION: This section defines various build options using the `option` command. These options control different aspects of the build, such as building Python binaries, using a custom Protobuf build, enabling Werror, and more. Each option has a default value.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\noption(ONNX_BUILD_PYTHON \"Build Python binaries\" ${ONNX_BUILD_PYTHON_DEFAULT})\noption(ONNX_BUILD_CUSTOM_PROTOBUF \"Build and use ONNX's own protobuf\" OFF)\noption(ONNX_USE_PROTOBUF_SHARED_LIBS \"Build ONNX using protobuf shared library.\" OFF)\noption(ONNX_GEN_PB_TYPE_STUBS \"Generate protobuf python type stubs\" ON)\noption(ONNX_WERROR \"Build with Werror\" OFF)\noption(ONNX_COVERAGE \"Build with coverage instrumentation\" OFF)\noption(ONNX_BUILD_TESTS \"Build ONNX C++ APIs Tests\" OFF)\noption(ONNX_USE_ASAN \"Build ONNX with ASAN\" OFF)\noption(ONNX_USE_LITE_PROTO \"Use lite protobuf instead of full.\" OFF)\noption(ONNX_DISABLE_EXCEPTIONS \"Disable exception handling.\" OFF)\noption(ONNX_DISABLE_STATIC_REGISTRATION \"Disable static registration for ONNX operator schemas.\" OFF)\noption(ONNX_USE_UNITY_BUILD \"Enable Unity (Jumbo) build for\" OFF)\nif(WIN32)\n  option(ONNX_USE_MSVC_STATIC_RUNTIME \"Build with MSVC static runtime\" OFF)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Expanding ONNX Model Output Dimensions\nDESCRIPTION: This snippet shows how to expand the output dimensions of an ONNX model using `onnx.compose.expand_out_dims`. This is useful for connecting models with different dimension expectations. The `dim_idx` parameter specifies the index where the new dimension is inserted. Requires two ONNX models, `model1` and `model2` and knowing which dimension to expand on `model1`.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md#_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport onnx\n\n# outputs: \"out0\", shape=[200, 200, 3]\nmodel1 = onnx.load(\"path/to/the/model1.onnx\")\n\n# outputs: \"in0\", shape=[N, 200, 200, 3]\nmodel2 = onnx.load(\"path/to/the/model2.onnx\")\n\n# outputs: \"out0\", shape=[1, 200, 200, 3]\nnew_model1 = onnx.compose.expand_out_dims(model1, dim_idx=0)\n\n# Models can now be merged\ncombined_model = onnx.compose.merge_models(\n    new_model1, model2, io_map=[(\"out0\", \"in0\")]\n)\n\n# Can also be run in-place\nonnx.compose.expand_out_dims(model1, dim_idx=0, inplace=True)\n```\n\n----------------------------------------\n\nTITLE: Mapping TensorProto Attributes to Values\nDESCRIPTION: This Python code iterates through the attributes of `onnx.TensorProto`, filters them based on a regular expression, and prints the mapping between the integer value and the attribute name. It requires the `re` and `onnx` modules.  The code helps to understand the different tensor types supported by ONNX.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/concepts.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport re\nfrom onnx import TensorProto\n\nreg = re.compile('^[0-9A-Z_]+$')\n\nvalues = {}\nfor att in sorted(dir(TensorProto)):\n    if att in {'DESCRIPTOR'}:\n        continue\n    if reg.match(att):\n        values[getattr(TensorProto, att)] = att\nfor i, att in sorted(values.items()):\n    si = str(i)\n    if len(si) == 1:\n        si = \" \" + si\n    print(\"%s: onnx.TensorProto.%s\" % (si, att))\n```\n\n----------------------------------------\n\nTITLE: Running ONNX tests using pytest\nDESCRIPTION: This command executes the ONNX tests using pytest. It assumes that pytest has already been installed. This command validates the functionality and correctness of the ONNX implementation.\nSOURCE: https://github.com/onnx/onnx/blob/main/README.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\npytest\n```\n\n----------------------------------------\n\nTITLE: Displaying Protobuf Executable Path\nDESCRIPTION: This snippet displays the value of the ONNX_PROTOC_EXECUTABLE variable, which should contain the full path to the protoc executable.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_28\n\nLANGUAGE: cmake\nCODE:\n```\nmessage(STATUS \"ONNX_PROTOC_EXECUTABLE: ${ONNX_PROTOC_EXECUTABLE}\")\n```\n\n----------------------------------------\n\nTITLE: Setting ONNX_ML environment variable\nDESCRIPTION: Sets the ONNX_ML environment variable to 1 (true) in a PowerShell (pwsh) environment. This setting influences the generation of operator and changelog documentation, including machine learning related components.\nSOURCE: https://github.com/onnx/onnx/blob/main/CONTRIBUTING.md#_snippet_1\n\nLANGUAGE: pwsh\nCODE:\n```\n# Windows\nset ONNX_ML=1\n```\n\n----------------------------------------\n\nTITLE: Setting ONNX Root Directory\nDESCRIPTION: This line sets the ONNX_ROOT variable to the source directory of the ONNX project. This variable is used to refer to the root directory in other parts of the CMake configuration.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_13\n\nLANGUAGE: cmake\nCODE:\n```\nset(ONNX_ROOT ${onnx_SOURCE_DIR})\n```\n\n----------------------------------------\n\nTITLE: Required Field Comment Example - IR Version\nDESCRIPTION: This code snippet shows an example comment that MUST be included when marking a field as required for a particular IR version. The `ModelProto.ir_version` property, for example, must be present in every model.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/Versioning.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n// This field MUST be present for this version of the IR.\n```\n\n----------------------------------------\n\nTITLE: Forcing Model Reload (Python)\nDESCRIPTION: This code snippet demonstrates how to force a reload of a model from the remote repository, bypassing the cache. This can be done using the `force_reload` parameter in the `hub.load` function.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/Hub.md#_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmodel = hub.load(\"resnet50\", force_reload=True)\n```\n\n----------------------------------------\n\nTITLE: Setting the Cache Location (Python)\nDESCRIPTION: This code snippet demonstrates how to manually set the cache directory for downloaded models using the `hub.set_dir` function. This allows users to control where the models are stored locally.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/Hub.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nhub.set_dir(\"my/cache/directory\")\n```\n\n----------------------------------------\n\nTITLE: ONNX to_text Function\nDESCRIPTION: This entry documents the `onnx.printer.to_text` function. The function converts an ONNX model into a human-readable text format. The exact parameters, inputs, and outputs are determined by the `autofunction` directive.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/api/printer.md#_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. autofunction:: onnx.printer.to_text\n```\n\n----------------------------------------\n\nTITLE: Define MyGraph struct with magic number and backend pointer in C\nDESCRIPTION: This code snippet defines a structure named `MyGraph` which includes a magic number and a pointer to the backend object.  It provides guidance for vendors to represent ONNX ModelProto messages. The magic number is used for verification purposes and the pointer links the graph to its associated backend.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/proposals/ONNXIFIproposal.md#_snippet_1\n\nLANGUAGE: c\nCODE:\n```\nstruct MyGraph {\n  uint32_t magic;\n  struct MyBackend* backend;\n  ...\n};\n\n/* This line won't compile, but gives you an idea of relation between MyGraph structure and onnxGraph type. */\ntypedef MyGraph* onnxGraph;\n```\n\n----------------------------------------\n\nTITLE: Setting ONNX_PROTOC_EXECUTABLE (Windows)\nDESCRIPTION: This command sets the `CMAKE_ARGS` environment variable to specify the full path to the `protoc.exe` executable. This allows CMake to use a specific Protobuf compiler for building ONNX.\nSOURCE: https://github.com/onnx/onnx/blob/main/INSTALL.md#_snippet_8\n\nLANGUAGE: bat\nCODE:\n```\nset CMAKE_ARGS=-DONNX_PROTOC_EXECUTABLE=<full_path_to_protoc.exe>\n```\n\n----------------------------------------\n\nTITLE: Packing and Unpacking INT4 Values\nDESCRIPTION: Demonstrates how to pack two 4-bit integer values (x, y) into a single byte and how to unpack a byte (z) to retrieve the original 4-bit integer values.  The first element is stored in the 4 LSB and the second element is stored in the 4 MSB. If the total number of elements is odd, padding of 4 bits will be appended.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/technical/int4.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\npack(x,y): y << 4 | x & 0x0F\nunpack(z): x = z & 0x0F, y = z >> 4\n```\n\n----------------------------------------\n\nTITLE: Setting ONNX Namespace\nDESCRIPTION: This line sets the ONNX namespace to 'onnx' if it's not already defined. This namespace is used to avoid naming conflicts with other libraries.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_16\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT ONNX_NAMESPACE)\n  set(ONNX_NAMESPACE \"onnx\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Finding and Including GTest\nDESCRIPTION: This snippet checks if ONNX_BUILD_TESTS is enabled. If so, it attempts to find the GTest library. If GTest is not found, it includes the googletest module to download and build it.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_25\n\nLANGUAGE: cmake\nCODE:\n```\nif(ONNX_BUILD_TESTS)\n  find_package(GTest)\n  if(NOT GTest_FOUND)\n    include(googletest)\n  endif()\n  set(googletest_STATIC_LIBRARIES GTest::gtest)\nendif()\n```\n\n----------------------------------------\n\nTITLE: ONNX Graph for Optional Tensor Output (Example 2)\nDESCRIPTION: This is the ONNX graph for the PyTorch model producing an optional Tensor as output. It utilizes the `onnx::Optional[type=tensor(float)]()` to create an optional tensor. The `If` operator determines whether to assign `src_tokens.1` or the empty optional tensor to the output. The graph showcases how optional outputs are handled in ONNX.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/ONNXTypes.md#_snippet_5\n\nLANGUAGE: onnx\nCODE:\n```\nGraph(\n    %src_tokens.1 : Float(3, 2, 4),\n    %return_all_hiddens.1 : Bool(1)\n):\n    %2 : Float(3, 2, 4) = onnx::Optional[type=tensor(float)]()\n    %3 : Float(3, 2, 4) = onnx::If(%return_all_hiddens.1)\n        block0():\n        -> (%src_tokens.1)\n        block1():\n        -> (%2)\n    return (%3)\n```\n\n----------------------------------------\n\nTITLE: Installing ONNX with vcpkg\nDESCRIPTION: These commands clone the vcpkg repository, navigate into it, bootstrap vcpkg, and then install the ONNX package using vcpkg.\nSOURCE: https://github.com/onnx/onnx/blob/main/INSTALL.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\ngit clone https://github.com/microsoft/vcpkg.git\ncd vcpkg\n./bootstrap-vcpkg.bat # For powershell\n./bootstrap-vcpkg.sh # For bash\n./vcpkg install onnx\n```\n\n----------------------------------------\n\nTITLE: Setting Default Build Type\nDESCRIPTION: This snippet sets the default build type if one isn't already specified. It defaults to 'Release' and provides a user-configurable option to select different build types. Available options are 'Debug', 'Release', 'RelWithDebInfo', 'MinSizeRel', and 'Coverage'.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT CMAKE_BUILD_TYPE)\n  message(STATUS \"Build type not set - defaulting to Release\")\n  set(\n    CMAKE_BUILD_TYPE \"Release\"\n    CACHE\n      STRING\n      \"Choose the type of build from: Debug Release RelWithDebInfo MinSizeRel Coverage.\"\n    FORCE)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Including Utility Functions\nDESCRIPTION: This line includes a custom CMake utility file. It likely contains helper functions and macros used throughout the ONNX build process. The utility file is located in the cmake/Utils.cmake path relative to the source directory.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ninclude(cmake/Utils.cmake)\n```\n\n----------------------------------------\n\nTITLE: Setting Compiler Flags\nDESCRIPTION: This block sets compiler flags. It adds `-Wnon-virtual-dtor` flag when not using MSVC. It removes optimization flags in Debug and sets coverage flags if the ONNX_COVERAGE option is enabled.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_15\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT MSVC)\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wnon-virtual-dtor\")\n  set(CMAKE_C_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} -O0\")\n  set(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} -O0\")\n  if(ONNX_COVERAGE)\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fprofile-arcs -ftest-coverage\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fprofile-arcs -ftest-coverage\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Loading an ONNX model from a file object - Python\nDESCRIPTION: This example demonstrates loading an ONNX model from a file object opened in binary read mode ('rb'). The `load` function can accept either a file path string or a file object.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/api/serialization.md#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom onnx import load\n\nwith open(\"model.onnx\", \"rb\") as f:\n    onnx_model = load(f)\n```\n\n----------------------------------------\n\nTITLE: Loading ONNX Tensor from String\nDESCRIPTION: This snippet loads an ONNX tensor from a serialized string read from a file named 'saved_tensor.pb' using `load_tensor_from_string`. It demonstrates a simplified method for deserializing tensors.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n    from onnx import load_tensor_from_string\n\n    with open(\"saved_tensor.pb\", \"rb\") as f:\n        serialized = f.read()\n    proto = load_tensor_from_string(serialized)\n    print(type(proto))\n```\n\n----------------------------------------\n\nTITLE: Setting Minimum CMake Version\nDESCRIPTION: This snippet specifies the minimum required CMake version for the project. It's a prerequisite for using modern CMake features. CMake version 3.18 or higher is required.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.18)\n```\n\n----------------------------------------\n\nTITLE: Setting Python Component\nDESCRIPTION: This snippet sets the python_dev_component variable if `ONNX_BUILD_PYTHON` is enabled. It is used later to specify the Python component to be found.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_19\n\nLANGUAGE: cmake\nCODE:\n```\nif(ONNX_BUILD_PYTHON)\n  set(python_dev_component Development.Module)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Creating String Attribute in ONNX\nDESCRIPTION: This snippet shows how to create a string attribute using `helper.make_attribute`. It defines the attribute name and its string content, which are then used to create the attribute and print it.\nSOURCE: https://github.com/onnx/onnx/blob/main/examples/Protobufs.ipynb#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# String Attribute\narg = helper.make_attribute(\"this_is_a_string\", \"string_content\")\nprint(\"\\nString attribute:\\n\")\nprint(arg)\n```\n\n----------------------------------------\n\nTITLE: Define and Configure onnx Library\nDESCRIPTION: Defines the `onnx` library, which contains the main ONNX implementation. It handles AIX-specific object library creation, sets visibility properties, includes directories, links to `onnx_proto`, and adds ONNX global definitions.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_32\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_SYSTEM_NAME STREQUAL \"AIX\")\n  # whole-archive linker option not available on AIX.\n  # So, create a object library\n  add_library(onnx OBJECT ${ONNX_SRCS})\nelse()\n  add_library(onnx ${ONNX_SRCS})\nendif()\nset_target_properties(onnx PROPERTIES CXX_VISIBILITY_PRESET hidden)\nset_target_properties(onnx PROPERTIES VISIBILITY_INLINES_HIDDEN ON)\n\ntarget_include_directories(onnx PUBLIC\n  $<BUILD_INTERFACE:${ONNX_ROOT}>\n  $<BUILD_INTERFACE:${CMAKE_CURRENT_BINARY_DIR}>\n  $<INSTALL_INTERFACE:include>)\ntarget_link_libraries(onnx PUBLIC onnx_proto)\nadd_onnx_global_defines(onnx)\n```\n\n----------------------------------------\n\nTITLE: ShardingSpecProto for tensor split across axis 0 and 1 - JSON\nDESCRIPTION: This JSON represents a ShardingSpecProto where a 2x2 tensor is sharded across both axis 0 and axis 1 over four devices. Each device receives a 1x1 tensor.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/proposals/ONNXMultiDeviceProposal.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"device\" : [0, 1, 2, 3],\n    \"sharded_dim\" :[\n        {\n            \"axis\" : 0,\n            \"simple_sharding\" :\n            [\n                {\n                    \"num_shards\" : 2\n                }\n            ]\n        }\n        {\n            \"axis\" : 1,\n            \"simple_sharding\" :\n            [\n                {\n                    \"num_shards\" : 2\n                }\n            ]\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Saving a NodeProto to a file - Python\nDESCRIPTION: This example demonstrates how to create a `NodeProto` object, populate its fields (name, op_type, inputs, outputs), and then serialize it to a file using `SerializeToString`. The file is opened in binary write mode.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/api/serialization.md#_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom onnx import NodeProto\n\nnode = NodeProto()\nnode.name = \"example-type-proto\"\nnode.op_type = \"Add\"\nnode.input.extend([\"X\", \"Y\"])\nnode.output.extend([\"Z\"])\n\nwith open(\"node.pb\", \"wb\") as f:\n    f.write(node.SerializeToString())\n```\n\n----------------------------------------\n\nTITLE: Finding Python3 Package\nDESCRIPTION: This line uses the `find_package` command to locate the Python 3 package. It requires the Interpreter component and optionally the Development.Module component (if python_dev_component is set).\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_21\n\nLANGUAGE: cmake\nCODE:\n```\nfind_package(Python3 REQUIRED COMPONENTS Interpreter ${python_dev_component})\n```\n\n----------------------------------------\n\nTITLE: TorchScript Graph for Optional Tensor Input (Example 1)\nDESCRIPTION: This is the TorchScript graph corresponding to the PyTorch model with an optional tensor input.  It showcases how the optional type is represented in the TorchScript graph using the `Tensor?` notation.  The graph uses primitive operations like `aten::__isnot__` and `prim::If` to handle the optional input.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/ONNXTypes.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nGraph(\n    %self : __torch__.Model,\n    %x.1 : Tensor,\n    %y.1 : Tensor?\n):\n    %11 : int = prim::Constant[value=1]()\n    %4 : None = prim::Constant()\n    %5 : bool = aten::__isnot__(%y.1, %4)\n    %6 : Tensor = prim::If(%5)\n        block0():\n            %y.4 : Tensor = prim::unchecked_cast(%y.1)\n            %12 : Tensor = aten::add(%x.1, %y.4, %11)\n        -> (%12)\n        block1():\n        -> (%x.1)\n    return (%6)\n```\n\n----------------------------------------\n\nTITLE: ONNX Matrix Multiplication Shape Inference Example in C++\nDESCRIPTION: This C++ code snippet demonstrates a shape inference implementation for a simple matrix multiplication operator, where inputs have shapes [M,K] and [K,N], and the output has shape [M,N]. It includes checks for input ranks and safely handles missing dimensions or shapes using utility functions like `checkInputRank`, `unifyInputDim`, and `updateOutputShape`.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/ShapeInference.md#_snippet_2\n\nLANGUAGE: cpp\nCODE:\n```\n   // Check that input 0 has rank 2 (if its rank is known).\n   checkInputRank(ctx, 0, 2);\n   // Check that input 1 has rank 2 (if its rank is known).\n   checkInputRank(ctx, 1, 2);\n   Dim M, K, N;\n   // Check various dimensions, handling missing dimensions/shapes safely.\n   unifyInputDim(ctx, 0, 0, M);\n   unifyInputDim(ctx, 0, 1, K);\n   unifyInputDim(ctx, 1, 0, K);\n   unifyInputDim(ctx, 1, 1, N);\n   updateOutputShape(ctx, 0, {M. N});\n```\n\n----------------------------------------\n\nTITLE: Converting ONNX Model Version\nDESCRIPTION: This snippet demonstrates how to convert an ONNX model to a specific target version using the `version_converter` module. It takes the original model and the target version as input and returns the converted model. No specific dependencies beyond the onnx library and the version converter are required.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/PythonAPIOverview.md#_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Apply the version conversion on the original model\nconverted_model = version_converter.convert_version(original_model, <int target_version>)\n\nprint(f\"The model after conversion:\\n{converted_model}\")\n```\n\n----------------------------------------\n\nTITLE: Disabling Exceptions\nDESCRIPTION: This section disables exception handling if the ONNX_DISABLE_EXCEPTIONS option is enabled. It adds the ONNX_NO_EXCEPTIONS definition, and modifies compiler flags to disable C++ exceptions, or sets `-D_HAS_EXCEPTIONS=0` and uses regex to replace `/EHsc` with `/EHs-c-`.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_18\n\nLANGUAGE: cmake\nCODE:\n```\nif(ONNX_DISABLE_EXCEPTIONS)\n  add_compile_definitions(\"ONNX_NO_EXCEPTIONS\")\n  # Disable C++ exceptions.\n  if(MSVC)\n    string(REGEX REPLACE \"/EHsc\" \"/EHs-c-\" CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS}\")\n    add_definitions(-D_HAS_EXCEPTIONS=0)\n  else()\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fno-exceptions -fno-unwind-tables -fno-asynchronous-unwind-tables\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Printing ONNX Domain Constants\nDESCRIPTION: This code snippet imports and prints the values of the ONNX domain constants: ONNX_DOMAIN, ONNX_ML_DOMAIN, and AI_ONNX_PREVIEW_TRAINING_DOMAIN. These constants represent the different domains within the ONNX ecosystem.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/api/defs.md#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom onnx.defs import (\n        ONNX_DOMAIN,\n        ONNX_ML_DOMAIN,\n        AI_ONNX_PREVIEW_TRAINING_DOMAIN,\n    )\n    print(f\"ONNX_DOMAIN={ONNX_DOMAIN!r}\")\n    print(f\"ONNX_ML_DOMAIN={ONNX_ML_DOMAIN!r}\")\n    print(f\"AI_ONNX_PREVIEW_TRAINING_DOMAIN={AI_ONNX_PREVIEW_TRAINING_DOMAIN!r}\")\n```\n\n----------------------------------------\n\nTITLE: ShardingSpecProto for tensor split across axis 0 - JSON\nDESCRIPTION: This JSON represents a ShardingSpecProto where a 2x2 tensor is sharded across axis 0 over two devices. Device 0 receives the first row, and Device 1 receives the second row.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/proposals/ONNXMultiDeviceProposal.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"device\" : [0, 1],\n    \"sharded_dim\" :[\n        {\n            \"axis\" : 0,\n            \"simple_sharding\" :\n            [\n                {\n                    \"num_shards\" : 2\n                }\n            ]\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Importing ONNX\nDESCRIPTION: Imports the onnx library, providing access to its modules and functions.\nSOURCE: https://github.com/onnx/onnx/blob/main/examples/Protobufs.ipynb#_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom onnx import *\n```\n\n----------------------------------------\n\nTITLE: Defining Tensor message in ONNX\nDESCRIPTION: This code defines the Tensor message in ONNX, which incorporates the TensorShapeProto to specify the tensor's shape. It includes the element type (elem_type) which refers to a data type from TensorProto.DataType. This structure allows ONNX to represent tensors with specific data types and shapes.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/IR.md#_snippet_2\n\nLANGUAGE: protobuf\nCODE:\n```\nmessage Tensor {\n    optional TensorProto.DataType elem_type = 1;\n    optional TensorShapeProto shape = 2;\n  }\n```\n\n----------------------------------------\n\nTITLE: Generating ONNX Hub Manifest (Shell Script)\nDESCRIPTION: This script downloads the onnx/models repository, pulls Large File Storage (LFS) files and then runs a Python script `generate_onnx_hub_manifest.py` to create the ONNX Hub manifest file.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/Hub.md#_snippet_9\n\nLANGUAGE: shell script\nCODE:\n```\ngit clone https://github.com/onnx/models.git\ngit lfs pull --include=\"*\" --exclude=\"\"\ncd models/workflow_scripts\npython generate_onnx_hub_manifest.py\n```\n\n----------------------------------------\n\nTITLE: ShardingSpecProto for tensor split across axis 1 - JSON\nDESCRIPTION: This JSON represents a ShardingSpecProto where a 2x2 tensor is sharded across axis 1 over two devices. Device 0 receives the first column, and Device 1 receives the second column.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/proposals/ONNXMultiDeviceProposal.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"device\" : [0, 1],\n    \"sharded_dim\" :[\n        {\n            \"axis\" : 1,\n            \"simple_sharding\" :\n            [\n                {\n                    \"num_shards\" : 2\n                }\n            ]\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Repeated Int Attribute in ONNX\nDESCRIPTION: This code demonstrates creating an attribute that holds a list of integers using `helper.make_attribute`.  The attribute name and the list of integers are specified, creating a repeated integer attribute that is then printed.\nSOURCE: https://github.com/onnx/onnx/blob/main/examples/Protobufs.ipynb#_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Repeated Attribute\narg = helper.make_attribute(\"this_is_a_repeated_int\", [1, 2, 3, 4])\nprint(\"\\nRepeated int attribute:\\n\")\nprint(arg)\n```\n\n----------------------------------------\n\nTITLE: Downloading ONNX Model by Name (Python)\nDESCRIPTION: This code snippet shows how to download a pre-trained ONNX model from the official ONNX Model Zoo using the `hub.load` function. The model is downloaded to a local cache and loaded as a `ModelProto` object.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/Hub.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel = hub.load(\"resnet50\")\n```\n\n----------------------------------------\n\nTITLE: Including GNUInstallDirs\nDESCRIPTION: This line includes the GNUInstallDirs module, which provides standard installation directories (e.g., bin, lib, include). It helps ensure consistent installation locations across different platforms.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_12\n\nLANGUAGE: cmake\nCODE:\n```\ninclude(GNUInstallDirs)\n```\n\n----------------------------------------\n\nTITLE: Saving a Proto class to a file - Python\nDESCRIPTION: This snippet shows how to save any ONNX Proto class (e.g., a `NodeProto`) to a file using `SerializeToString`. Similar to saving a model, the Proto object is serialized to a byte string and written to a file in binary write mode.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/api/serialization.md#_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nwith open(\"proto.pb\", \"wb\") as f:\n    f.write(proto.SerializeToString())\n```\n\n----------------------------------------\n\nTITLE: Installing Headers\nDESCRIPTION: This snippet installs the ONNX header files located in the `${ONNX_ROOT}/onnx` and `${CMAKE_CURRENT_BINARY_DIR}/onnx` directories to the `${CMAKE_INSTALL_INCLUDEDIR}` directory. It excludes test case and data directories from the installation.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_40\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(DIRECTORY ${ONNX_ROOT}/onnx\n        DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}\n        FILES_MATCHING\n        PATTERN \"*.h\"\n        PATTERN \"backend/test/case\" EXCLUDE\n        PATTERN \"backend/test/data\" EXCLUDE)\ninstall(DIRECTORY ${CMAKE_CURRENT_BINARY_DIR}/onnx\n        DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}\n        FILES_MATCHING\n        PATTERN \"*.h\")\n```\n\n----------------------------------------\n\nTITLE: Loading an ONNX model from a file - Python\nDESCRIPTION: This snippet shows how to load an ONNX model from a file using the `load` function. The function reads the file content and deserializes it into an ONNX model object.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/api/serialization.md#_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom onnx import load\n\nonnx_model = load(\"model.onnx\")\n```\n\n----------------------------------------\n\nTITLE: Generating Source Distribution with Python Build\nDESCRIPTION: This command uses the `build` module to generate a source distribution file for the ONNX package.  This file is then uploaded to PyPI. Ensure `build` is installed.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/OnnxReleases.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\npython -m build --sdist\n```\n\n----------------------------------------\n\nTITLE: Handling Deprecated Option\nDESCRIPTION: This snippet handles the deprecated `BUILD_ONNX_PYTHON` option and sets `ONNX_BUILD_PYTHON` accordingly. It issues a warning message if `BUILD_ONNX_PYTHON` is defined and sets the default value for `ONNX_BUILD_PYTHON` based on `BUILD_ONNX_PYTHON`.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nif(DEFINED BUILD_ONNX_PYTHON)\n  message(WARNING \"'BUILD_ONNX_PYTHON' is deprecated. Please, use 'ONNX_BUILD_PYTHON' instead\")\n  set(ONNX_BUILD_PYTHON_DEFAULT ${BUILD_ONNX_PYTHON})\nelse()\n  set(ONNX_BUILD_PYTHON_DEFAULT OFF)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Decoding ONNX ModelProto using protoc\nDESCRIPTION: This snippet demonstrates how to use the `protoc` tool to decode the contents of an ONNX file.  `protoc` is part of the Protocol Buffers distribution. The command decodes the binary ONNX file into a human-readable format based on the specified `onnx.proto` definition.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/IR.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ protoc --decode=onnx.ModelProto onnx.proto < yourfile.onnx\n```\n\n----------------------------------------\n\nTITLE: Sharding Spec for Input1 in Add Operation\nDESCRIPTION: This JSON snippet defines the sharding specification for Input1 in an Add operation. It specifies that the input is sharded along axis 0 into two shards, with each shard existing on two devices to facilitate the computation of output shards across multiple devices.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/proposals/ShardingFormalism.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"device\" = [-1, -2] // keys into device_map\n    \"device_map\" = {-1: [0, 1], -2: [2, 3]}\n    \"sharded_dim\" =[\n        {\n            \"axis\" = 0\n            \"simple_sharding\" =\n            [\n                {\n                    \"num_shards\" = 2\n                }\n            ]\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Install ONNX in editable mode using pip\nDESCRIPTION: Installs the ONNX package in editable mode with verbose output. This allows changes to Python and C++ files to be reflected without reinstalling the package completely.  Specifically useful after building from source to trigger the native extension build for C++ changes. Requires pip.\nSOURCE: https://github.com/onnx/onnx/blob/main/CONTRIBUTING.md#_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install -e . -v\n```\n\n----------------------------------------\n\nTITLE: Install pytest\nDESCRIPTION: Installs the pytest testing framework using pip. Pytest is used for running unit tests in the ONNX project. Requires pip.\nSOURCE: https://github.com/onnx/onnx/blob/main/CONTRIBUTING.md#_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\npip install pytest\n```\n\n----------------------------------------\n\nTITLE: Reading ONNX Version\nDESCRIPTION: This snippet reads the ONNX version number from the VERSION_NUMBER file in the ONNX root directory. It then strips any leading or trailing whitespace from the version string.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_14\n\nLANGUAGE: cmake\nCODE:\n```\nfile(READ \"${ONNX_ROOT}/VERSION_NUMBER\" ONNX_VERSION)\nstring(STRIP \"${ONNX_VERSION}\" ONNX_VERSION)\n```\n\n----------------------------------------\n\nTITLE: Install lintrunner and linters\nDESCRIPTION: Installs lintrunner and lintrunner-adapters using pip.  Lintrunner is used to enforce coding style guidelines and code quality within the ONNX project. Requires pip.\nSOURCE: https://github.com/onnx/onnx/blob/main/CONTRIBUTING.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\npip install lintrunner lintrunner-adapters\nlintrunner init\n```\n\n----------------------------------------\n\nTITLE: Format code with lintrunner\nDESCRIPTION: Runs lintrunner to automatically fix code style and quality issues. The `-a` flag displays all lints and applies fixes, while `f` applies fixes only for faster execution. Requires lintrunner to be installed and configured.\nSOURCE: https://github.com/onnx/onnx/blob/main/CONTRIBUTING.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\n# Display all lints and apply the fixes\nlintrunner -a\n# Or apply fixes only (faster)\nlintrunner f\n```\n\n----------------------------------------\n\nTITLE: ShardingSpecProto for tensor broadcast - JSON\nDESCRIPTION: This JSON represents a ShardingSpecProto where a 2x2 tensor is broadcasted across two devices. Both devices receive the same 2x2 tensor.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/proposals/ONNXMultiDeviceProposal.md#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"device\" = [-1] // keys into device_map\n    \"device_map\" = {-1: [0, 1]}\n    \"sharded_dim\" =[]\n}\n```\n\n----------------------------------------\n\nTITLE: Configure Python Bindings (onnx_cpp2py_export)\nDESCRIPTION: Configures the Python bindings using `Python3_add_library`, setting properties for the module, include directories, and linking to pybind11. It also addresses a linking issue on macOS by setting the `LINK_FLAGS` property.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_33\n\nLANGUAGE: cmake\nCODE:\n```\nif(ONNX_BUILD_PYTHON)\n  Python3_add_library(onnx_cpp2py_export MODULE WITH_SOABI \"${ONNX_ROOT}/onnx/cpp2py_export.cc\")\n  set_target_properties(onnx_cpp2py_export PROPERTIES PREFIX \"\")\n  set_target_properties(onnx_cpp2py_export PROPERTIES CXX_VISIBILITY_PRESET hidden)\n  set_target_properties(onnx_cpp2py_export PROPERTIES VISIBILITY_INLINES_HIDDEN ON)\n  set_target_properties(onnx_cpp2py_export\n                        PROPERTIES LIBRARY_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR})\n  target_include_directories(onnx_cpp2py_export PRIVATE\n                             $<BUILD_INTERFACE:${ONNX_ROOT}>\n                             $<BUILD_INTERFACE:${CMAKE_CURRENT_BINARY_DIR}>\n                             $<INSTALL_INTERFACE:include>)\n\n  # search for FindPython3.cmake instead of legacy modules\n  set(PYBIND11_FINDPYTHON ON)\n\n  # pybind11 is a header only lib\n  find_package(pybind11 2.12 CONFIG)\n  if(NOT pybind11_FOUND)\n    if(EXISTS \"${ONNX_ROOT}/third_party/pybind11/include/pybind11/pybind11.h\")\n      add_subdirectory(\"${ONNX_ROOT}/third_party/pybind11\")\n    else()\n      message(FATAL_ERROR \"cannot find pybind at '${ONNX_ROOT}/third_party/pybind11/include/pybind11/pybind11.h'\")\n    endif()\n  endif()\n\n  target_include_directories(onnx_cpp2py_export PUBLIC \"${pybind11_INCLUDE_DIRS}\")\n\n  if(APPLE)\n    set_target_properties(onnx_cpp2py_export\n                          PROPERTIES LINK_FLAGS \"-undefined dynamic_lookup\")\n    # Only put double quotes around $<TARGET_FILE:onnx> for Mac\n    # Other platforms like Windows and Ubuntu originally work fine without double quotes\n```\n\n----------------------------------------\n\nTITLE: ONNX Hub Manifest Example (JSON)\nDESCRIPTION: This JSON snippet shows an example of a well-formed ONNX Hub manifest entry. It lists the important fields such as model name, path, ONNX version, opset version, metadata including SHA hashes, tags and IO ports information.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/Hub.md#_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"model\": \"BERT-Squad\",\n    \"model_path\": \"text/machine_comprehension/bert-squad/model/bertsquad-8.onnx\",\n    \"onnx_version\": \"1.3\",\n    \"opset_version\": 8,\n    \"metadata\": {\n        \"model_sha\": \"cad65b9807a5e0393e4f84331f9a0c5c844d9cc736e39781a80f9c48ca39447c\",\n        \"model_bytes\": 435882893,\n        \"tags\": [\"text\", \"machine comprehension\", \"bert-squad\"],\n        \"io_ports\": {\n            \"inputs\": [\n                {\n                    \"name\": \"unique_ids_raw_output___9:0\",\n                    \"shape\": [\"unk__475\"],\n                    \"type\": \"tensor(int64)\"\n                },\n                {\n                    \"name\": \"segment_ids:0\",\n                    \"shape\": [\"unk__476\", 256],\n                    \"type\": \"tensor(int64)\"\n                },\n                {\n                    \"name\": \"input_mask:0\",\n                    \"shape\": [\"unk__477\", 256],\n                    \"type\": \"tensor(int64)\"\n                },\n                {\n                    \"name\": \"input_ids:0\",\n                    \"shape\": [\"unk__478\", 256],\n                    \"type\": \"tensor(int64)\"\n                }\n            ],\n            \"outputs\": [\n                {\n                    \"name\": \"unstack:1\",\n                    \"shape\": [\"unk__479\", 256],\n                    \"type\": \"tensor(float)\"\n                },\n                {\n                    \"name\": \"unstack:0\",\n                    \"shape\": [\"unk__480\", 256],\n                    \"type\": \"tensor(float)\"\n                },\n                {\n                    \"name\": \"unique_ids:0\",\n                    \"shape\": [\"unk__481\"],\n                    \"type\": \"tensor(int64)\"\n                }\n            ]\n        },\n        \"model_with_data_path\": \"text/machine_comprehension/bert-squad/model/bertsquad-8.tar.gz\",\n        \"model_with_data_sha\": \"c8c6c7e0ab9e1333b86e8415a9d990b2570f9374f80be1c1cb72f182d266f666\",\n        \"model_with_data_bytes\": 403400046\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Twine for PyPI Uploads\nDESCRIPTION: This command installs the `twine` package, which is used to upload packages to TestPyPI and PyPI. Twine securely uploads package distributions to the Python Package Index (PyPI) and other PyPI compatible indexes.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/OnnxReleases.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install twine\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries with Target Link Libraries\nDESCRIPTION: This code snippet handles linking the 'onnx_cpp2py_export' target with the 'onnx' library, applying platform-specific linker flags. It uses different approaches for MSVC, AIX, and other systems (assumed to be GCC-like), including whole-archive linking and excluding standard library filesystems when needed.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_34\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(onnx_cpp2py_export\n                          PRIVATE -Wl,-force_load,\"$<TARGET_FILE:onnx>\")\n  elseif(MSVC)\n    # In MSVC, we will add whole archive in default\n    target_link_libraries(onnx_cpp2py_export\n                          PRIVATE -WHOLEARCHIVE:$<TARGET_FILE:onnx>)\n  elseif(CMAKE_SYSTEM_NAME STREQUAL \"AIX\")\n    # whole-archive linker option not available on AIX\n    target_sources(onnx_cpp2py_export\n                          PRIVATE $<TARGET_OBJECTS:onnx>)\n  else()\n    # Assume everything else is like gcc\n    target_link_libraries(onnx_cpp2py_export\n                          PRIVATE \"-Wl,--whole-archive\" $<TARGET_FILE:onnx>\n                                  \"-Wl,--no-whole-archive\")\n    # Prevent \"undefined symbol: _ZNSt10filesystem7__cxx114path14_M_split_cmptsEv\"\n    # (std::filesystem::__cxx11::path::_M_split_cmpts()) on gcc 8\n    if(CMAKE_CXX_STANDARD EQUAL 17 AND CMAKE_COMPILER_IS_GNUCXX AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 9.0)\n      target_link_libraries(onnx_cpp2py_export PRIVATE \"-lstdc++fs\")\n    endif()\n    set_target_properties(onnx_cpp2py_export\n                          PROPERTIES LINK_FLAGS \"-Wl,--exclude-libs,ALL\")\n  endif()\n\n  target_link_libraries(onnx_cpp2py_export PRIVATE onnx)\n\n  if(MSVC)\n    target_link_libraries(onnx_cpp2py_export PRIVATE ${Python3_LIBRARIES})\n    target_compile_options(onnx_cpp2py_export\n                           PRIVATE /MP\n                                   /wd4146 # unary minus operator applied to unsigned type,\n                                           # result still unsigned\n                                   /wd4244 # 'argument': conversion from 'google::\n                                           # protobuf::uint64' to 'int', possible\n                                           # loss of data\n                                   /wd4267 # Conversion from 'size_t' to 'int',\n                                           # possible loss of data\n                                   )\n    add_msvc_runtime_flag(onnx_cpp2py_export)\n    add_onnx_global_defines(onnx_cpp2py_export)\n  endif()\n```\n\n----------------------------------------\n\nTITLE: Setting CMAKE_PREFIX_PATH (Windows)\nDESCRIPTION: This command sets the `CMAKE_PREFIX_PATH` environment variable to the Protobuf installation directory. This allows CMake to find the Protobuf library when building ONNX.\nSOURCE: https://github.com/onnx/onnx/blob/main/INSTALL.md#_snippet_7\n\nLANGUAGE: bat\nCODE:\n```\nset CMAKE_PREFIX_PATH=<protobuf_install_dir>;%CMAKE_PREFIX_PATH%\n```\n\n----------------------------------------\n\nTITLE: Run pytest\nDESCRIPTION: Executes the pytest testing framework from the root of the repository to run unit tests for the ONNX project. Requires pytest to be installed.\nSOURCE: https://github.com/onnx/onnx/blob/main/CONTRIBUTING.md#_snippet_7\n\nLANGUAGE: sh\nCODE:\n```\npytest\n```\n\n----------------------------------------\n\nTITLE: Cleaning Untracked Files from Git Repository\nDESCRIPTION: This command cleans untracked files from the Git repository. `git clean -nxd` performs a dry run showing which files would be removed, while `git clean -ixd` interactively removes the files, including ignored ones.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/OnnxReleases.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ngit clean -nxd\n```\n\nLANGUAGE: shell\nCODE:\n```\ngit clean -ixd\n```\n\n----------------------------------------\n\nTITLE: Building Protobuf from Source (Windows)\nDESCRIPTION: These commands clone the Protobuf repository, checkout a specific version, initialize submodules, configure the build with CMake to produce a static library, build the library, and install it to a specified directory.\nSOURCE: https://github.com/onnx/onnx/blob/main/INSTALL.md#_snippet_6\n\nLANGUAGE: bat\nCODE:\n```\ngit clone https://github.com/protocolbuffers/protobuf.git\ncd protobuf\ngit checkout v5.29.2\ngit submodule update --init --recursive\ncmake -G \"Visual Studio 16 2019\" -A x64 -DCMAKE_INSTALL_PREFIX=<protobuf_install_dir> -Dprotobuf_MSVC_STATIC_RUNTIME=OFF -Dprotobuf_BUILD_SHARED_LIBS=OFF -Dprotobuf_BUILD_TESTS=OFF -Dprotobuf_BUILD_EXAMPLES=OFF\ncmake --build . --config Release --target install\n```\n\n----------------------------------------\n\nTITLE: Run lintrunner\nDESCRIPTION: Executes lintrunner to identify and report code style and quality issues. Requires lintrunner to be installed and configured.\nSOURCE: https://github.com/onnx/onnx/blob/main/CONTRIBUTING.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nlintrunner\n```\n\n----------------------------------------\n\nTITLE: Updating Git Submodules\nDESCRIPTION: This command updates the Git submodules within the ONNX repository, ensuring that all dependencies are up-to-date. It initializes any submodules that have not yet been initialized.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/OnnxReleases.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ngit submodule update --init\n```\n\n----------------------------------------\n\nTITLE: Setting CMAKE_ARGS for Protobuf (Linux/Mac)\nDESCRIPTION: This command sets the `CMAKE_ARGS` environment variable to enable the use of shared Protobuf libraries.  This is necessary when Protobuf is installed as a shared library rather than a static library.\nSOURCE: https://github.com/onnx/onnx/blob/main/INSTALL.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nexport CMAKE_ARGS=\"-DONNX_USE_PROTOBUF_SHARED_LIBS=ON\"\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories\nDESCRIPTION: This code sets the include directories for ONNX, including the ONNX root directory and the current binary directory.  If the current directory has a parent directory (meaning it's a subdirectory), the include directories are also set in the parent scope, making them available to parent CMakeLists.txt files.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_35\n\nLANGUAGE: cmake\nCODE:\n```\n# Export include directories\nset(ONNX_INCLUDE_DIRS \"${ONNX_ROOT}\" \"${CMAKE_CURRENT_BINARY_DIR}\")\nget_directory_property(hasParent PARENT_DIRECTORY)\nif(hasParent)\n  set(ONNX_INCLUDE_DIRS ${ONNX_INCLUDE_DIRS} PARENT_SCOPE)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding External CMake Modules\nDESCRIPTION: This snippet adds the ONNX_ROOT/cmake/external directory to the CMAKE_MODULE_PATH, allowing CMake to find custom modules in that directory.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_24\n\nLANGUAGE: cmake\nCODE:\n```\nlist(APPEND CMAKE_MODULE_PATH ${ONNX_ROOT}/cmake/external)\n```\n\n----------------------------------------\n\nTITLE: Installing pytest package using pip\nDESCRIPTION: This command shows how to install the pytest package, which is used as the test driver for ONNX.  Installing pytest is a prerequisite for running the ONNX tests.\nSOURCE: https://github.com/onnx/onnx/blob/main/README.md#_snippet_1\n\nLANGUAGE: sh\nCODE:\n```\npip install pytest\n```\n\n----------------------------------------\n\nTITLE: Setting Compile Options for MSVC\nDESCRIPTION: This snippet configures compile options for the 'onnx_proto' and 'onnx' targets specifically when using the MSVC compiler. It disables specific warnings related to data type conversions and signed/unsigned operations. It also configures the runtime library and adds additional defines.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_36\n\nLANGUAGE: cmake\nCODE:\n```\nif(MSVC)\n  target_compile_options(onnx_proto\n                         PRIVATE /MP\n                                 /wd4146 # unary minus operator applied to unsigned type,\n                                         # result still unsigned\n                                 /wd4244 #'argument': conversion from 'google::\n                                         #protobuf::uint64' to 'int', possible\n                                         # loss of data\n                                 /wd4267 # Conversion from 'size_t' to 'int',\n                                         # possible loss of data\n                                 /wd4141 # 'inline': used more than once\n                                   )\n  target_compile_options(onnx\n                         PRIVATE /MP\n                                 /wd4146 # unary minus operator applied to unsigned type,\n                                         # result still unsigned\n                                 /wd4244 # 'argument': conversion from 'google::\n                                         # protobuf::uint64' to 'int', possible\n                                         # loss of data\n                                 /wd4267 # Conversion from 'size_t' to 'int',\n                                         # possible loss of data\n                                 /wd4141 # 'inline': used more than once\n                                   )\n  add_msvc_runtime_flag(onnx_proto)\n  add_msvc_runtime_flag(onnx)\n  set(onnx_static_library_flags\n      -IGNORE:4221 # LNK4221: This object file does not define any previously\n                   # undefined public symbols, so it will not be used by any\n                   # link operation that consumes this library\n      )\n  set_target_properties(onnx\n                        PROPERTIES STATIC_LIBRARY_FLAGS\n                                   \"${onnx_static_library_flags}\")\n  if(ONNX_WERROR)\n    target_compile_options(onnx PRIVATE \"/WX\")\n    target_compile_options(onnx_proto PRIVATE \"/WX\")\n  endif()\nelse()\n  target_compile_options(onnx PRIVATE -Wall -Wextra)\n  target_compile_options(onnx_proto PRIVATE -Wall -Wextra)\n  if(CMAKE_COMPILER_IS_GNUCXX AND CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL 13)\n    target_compile_options(onnx PRIVATE \"-Wno-stringop-overflow\")\n    target_compile_options(onnx_proto PRIVATE \"-Wno-stringop-overflow\")\n  endif()\n  if(ONNX_WERROR)\n    target_compile_options(onnx PRIVATE \"-Werror\")\n    target_compile_options(onnx_proto PRIVATE \"-Werror\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: AIX System Name Configuration\nDESCRIPTION: This line sets the CMAKE_NO_SYSTEM_FROM_IMPORTED variable to 1 when the system is AIX. It helps to avoid problems with system libraries.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_22\n\nLANGUAGE: cmake\nCODE:\n```\nif(CMAKE_SYSTEM_NAME STREQUAL \"AIX\")\n  set(CMAKE_NO_SYSTEM_FROM_IMPORTED 1)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Run stat_coverage.py\nDESCRIPTION: Executes the stat_coverage.py script to regenerate test coverage information. This script is located in the onnx/backend/test directory and provides insights into the extent of test coverage for the ONNX backend.\nSOURCE: https://github.com/onnx/onnx/blob/main/CONTRIBUTING.md#_snippet_8\n\nLANGUAGE: sh\nCODE:\n```\npython onnx/backend/test/stat_coverage.py\n```\n\n----------------------------------------\n\nTITLE: Loading a NodeProto from a file - Python\nDESCRIPTION: This snippet shows how to load a `NodeProto` from a file. It reads the file content as bytes and then uses `ParseFromString` to deserialize it into a `NodeProto` object. Requires the `onnx` package to be installed.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/api/serialization.md#_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom onnx import NodeProto\n\ntp2 = NodeProto()\nwith open(\"node.pb\", \"rb\") as f:\n    content = f.read()\n\ntp2.ParseFromString(content)\n\nprint(tp2)\n```\n\n----------------------------------------\n\nTITLE: Setting CMake Policies\nDESCRIPTION: These lines set CMake policies to control CMake's behavior. They ensure compatibility and consistent behavior across different CMake versions. CMP0063 and CMP0074 are set to NEW.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_policy(SET CMP0063 NEW)\ncmake_policy(SET CMP0074 NEW)\n```\n\n----------------------------------------\n\nTITLE: Define Custom Protobuf Generation Function\nDESCRIPTION: Defines a CMake function `RELATIVE_PROTOBUF_GENERATE_CPP` to generate C++ protobuf source files from .proto files. This function manages namespaces, output directories, and calls the protobuf compiler.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_29\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(RELATIVE_PROTOBUF_GENERATE_CPP NAME SRCS HDRS ROOT_DIR DEPEND)\n  if(NOT ARGN)\n    message(\n      SEND_ERROR\n        \"Error: RELATIVE_PROTOBUF_GENERATE_CPP() called without any proto files\"\n      )\n    return()\n  endif()\n\n  # Add ONNX_API prefix to protobuf classes and methods in all cases\n  set(ONNX_DLLEXPORT_STR \"dllexport_decl=ONNX_API:\")\n\n  set(${SRCS})\n  set(${HDRS})\n\n  set(GEN_PROTO_PY \"${ROOT_DIR}/onnx/gen_proto.py\")\n  foreach(INFILE ${ARGN})\n    set(ABS_FILE \"${ROOT_DIR}/${INFILE}\")\n    get_filename_component(FILE_DIR ${ABS_FILE} DIRECTORY)\n    get_filename_component(FILE_WE ${INFILE} NAME_WE)\n    # \"onnx-data\" check is because we do not want to create/compile an \"onnx-data-ml.proto\" file\n    if(ONNX_ML AND NOT(FILE_WE STREQUAL \"onnx-data\"))\n      if(ONNX_NAMESPACE STREQUAL \"onnx\")\n        set(GENERATED_FILE_WE \"${FILE_WE}-ml\")\n      else()\n        set(GENERATED_FILE_WE \"${FILE_WE}_${ONNX_NAMESPACE}-ml\")\n      endif()\n    else()\n      if(ONNX_NAMESPACE STREQUAL \"onnx\")\n        set(GENERATED_FILE_WE \"${FILE_WE}\")\n      else()\n        set(GENERATED_FILE_WE \"${FILE_WE}_${ONNX_NAMESPACE}\")\n      endif()\n    endif()\n    file(RELATIVE_PATH REL_DIR \"${ROOT_DIR}\" \"${FILE_DIR}\")\n    set(OUTPUT_PROTO_DIR \"${CMAKE_CURRENT_BINARY_DIR}/${REL_DIR}\")\n\n    set(OUTPUT_PB_HEADER \"${OUTPUT_PROTO_DIR}/${GENERATED_FILE_WE}.pb.h\")\n    set(OUTPUT_PB_SRC \"${OUTPUT_PROTO_DIR}/${GENERATED_FILE_WE}.pb.cc\")\n    set(GENERATED_PROTO \"${OUTPUT_PROTO_DIR}/${GENERATED_FILE_WE}.proto\")\n    if(NOT (ONNX_NAMESPACE STREQUAL \"onnx\"))\n      # We need this dummy header generated by gen_proto.py when ONNX_NAMESPACE\n      # is not onnx\n      list(APPEND ${HDRS} \"${OUTPUT_PROTO_DIR}/${GENERATED_FILE_WE}.pb.h\")\n    endif()\n    list(APPEND ${SRCS} \"${OUTPUT_PB_SRC}\")\n    list(APPEND ${HDRS} \"${OUTPUT_PB_HEADER}\")\n\n    if(NOT EXISTS \"${OUTPUT_PROTO_DIR}\")\n      file(MAKE_DIRECTORY \"${OUTPUT_PROTO_DIR}\")\n    endif()\n\n    set(GEN_PROTO_ARGS\n        -p\n        \"${ONNX_NAMESPACE}\"\n        -o\n        \"${OUTPUT_PROTO_DIR}\"\n        \"${FILE_WE}\")\n    if(ONNX_ML)\n      list(APPEND GEN_PROTO_ARGS -m)\n    endif()\n    if(ONNX_USE_LITE_PROTO)\n      list(APPEND GEN_PROTO_ARGS -l)\n    endif()\n    if(ONNX_VERIFY_PROTO3)\n        if(NOT ONNX_PROTOC_EXECUTABLE)\n          message(FATAL_ERROR \"Protobuf compiler not found\")\n        endif()\n        list(APPEND GEN_PROTO_ARGS --protoc_path)\n        list(APPEND GEN_PROTO_ARGS \"${ONNX_PROTOC_EXECUTABLE}\")\n    endif()\n\n    add_custom_command(OUTPUT \"${GENERATED_PROTO}\"\n                       COMMAND Python3::Interpreter \"${GEN_PROTO_PY}\"\n                               ARGS ${GEN_PROTO_ARGS}\n                       DEPENDS ${INFILE}\n                       COMMENT \"Running gen_proto.py on ${INFILE}\"\n                       VERBATIM)\n    message(\"Generated: ${GENERATED_PROTO}\")\n    set(PROTOC_ARGS\n        ${GENERATED_PROTO}\n        -I\n        ${CMAKE_CURRENT_BINARY_DIR}\n        --cpp_out\n        ${ONNX_DLLEXPORT_STR}${CMAKE_CURRENT_BINARY_DIR})\n    if(ONNX_BUILD_PYTHON)\n      list(APPEND PROTOC_ARGS --python_out)\n      if(ONNX_GEN_PB_TYPE_STUBS)\n        list(APPEND PROTOC_ARGS pyi_out:${CMAKE_CURRENT_BINARY_DIR})\n      else()\n        list(APPEND PROTOC_ARGS ${CMAKE_CURRENT_BINARY_DIR})\n      endif()\n    endif()\n    if(NOT ONNX_PROTOC_EXECUTABLE)\n      message(FATAL_ERROR \"Protobuf compiler not found\")\n    endif()\n    if(ONNX_PROTO_POST_BUILD_SCRIPT)\n      add_custom_command(\n        OUTPUT \"${OUTPUT_PB_SRC}\" \"${OUTPUT_PB_HEADER}\"\n        COMMAND \"${ONNX_PROTOC_EXECUTABLE}\" ARGS ${PROTOC_ARGS}\n        COMMAND \"${CMAKE_COMMAND}\" -DFILENAME=${OUTPUT_PB_HEADER}\n                -DNAMESPACES=${ONNX_NAMESPACE} -P\n                ${ONNX_PROTO_POST_BUILD_SCRIPT}\n        COMMAND \"${CMAKE_COMMAND}\" -DFILENAME=${OUTPUT_PB_SRC}\n                -DNAMESPACES=${ONNX_NAMESPACE} -P\n                ${ONNX_PROTO_POST_BUILD_SCRIPT}\n        DEPENDS ${GENERATED_PROTO} ${DEPEND}\n        COMMENT \"Running C++ protocol buffer compiler on ${GENERATED_PROTO}\"\n        VERBATIM)\n    else()\n      add_custom_command(\n        OUTPUT \"${OUTPUT_PB_SRC}\" \"${OUTPUT_PB_HEADER}\"\n        COMMAND \"${ONNX_PROTOC_EXECUTABLE}\" ARGS ${PROTOC_ARGS}\n        DEPENDS ${GENERATED_PROTO} ${DEPEND}\n        COMMENT \"Running C++ protocol buffer compiler on ${GENERATED_PROTO}\"\n        VERBATIM)\n    endif()\n    add_custom_target(${NAME} DEPENDS ${OUTPUT_PB_SRC} ${OUTPUT_PB_HEADER})\n  endforeach()\n\n  set_source_files_properties(${${SRCS}} ${${HDRS}} PROPERTIES GENERATED TRUE)\n  set(${SRCS} ${${SRCS}} PARENT_SCOPE)\n  set(${HDRS} ${${HDRS}} PARENT_SCOPE)\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: ShardingSpecProto for mixed split and broadcast - JSON\nDESCRIPTION: This JSON represents a ShardingSpecProto where a tensor is split across axis 0 over devices 0 and 1, and duplicated on devices 2 and 3.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/proposals/ONNXMultiDeviceProposal.md#_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"device\" = [-1, -2] // keys into device_map\n    \"device_map\" = {-1: [0, 1], -2: [2, 3]}\n    \"sharded_dim\" =[\n        {\n            \"axis\" = 0\n            \"simple_sharding\" =\n            [\n                {\n                    \"num_shards\" = 2\n                }\n            ]\n        }\n    ]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining the Project\nDESCRIPTION: This command defines the ONNX project and specifies CXX as the language.  This is necessary for CMake to properly configure the build environment.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nproject(onnx LANGUAGES CXX)\n```\n\n----------------------------------------\n\nTITLE: Squash and signoff old commits in a PR\nDESCRIPTION: Commands to squash an old PR (original branch) into a single commit and sign it off. This is useful for adding a DCO sign-off to old commits that are missing it.\nSOURCE: https://github.com/onnx/onnx/blob/main/CONTRIBUTING.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout main\ngit checkout -b temporary_patch              # create a new branch as temporary\ngit merge --squash original_patch            # copy from old branch\ngit branch -d original_patch                 # remove old branch\ngit checkout -b original_patch               # create a new branch with the same name (override)\ngit commit -m 'type your own commit msg' -s  # signoff that single commit\ngit push origin original_patch -f            # forcibly override the old branch`\n```\n\n----------------------------------------\n\nTITLE: Building ONNX Markdown Documentation\nDESCRIPTION: This snippet builds the markdown documentation for ONNX. It sets environment variables, generates protobuf files, installs the package in editable mode, generates test data, calculates coverage statistics, and generates documentation using python scripts.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/converters.md#_snippet_5\n\nLANGUAGE: sh\nCODE:\n```\nset ONNX_BUILD_TESTS=1\nset ONNX_ML=$(onnx_ml)\nset CMAKE_ARGS=-DONNX_USE_PROTOBUF_SHARED_LIBS=ON -DONNX_USE_LITE_PROTO=ON -DONNX_WERROR=ON\n\npython onnx\\gen_proto.py -l\npython onnx\\gen_proto.py -l --ml\npip install -e .\npython onnx\\backend\\test\\cmd_tools.py generate-data\npython onnx\\backend\\test\\stat_coverage.py\npython onnx\\defs\\gen_doc.py\nset ONNX_ML=0\npython onnx\\defs\\gen_doc.py\nset ONNX_ML=1\n```\n\n----------------------------------------\n\nTITLE: Setting Library Type\nDESCRIPTION: This conditional statement sets the build type to static if `BUILD_SHARED_LIBS` is not enabled. By default, CMake builds static libraries.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT BUILD_SHARED_LIBS)\n  # by default, cmake builds static libraries\n  set(BUILD_SHARED_LIBS OFF)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting C++ Standard\nDESCRIPTION: This section enforces the use of at least C++17. It sets the CMAKE_CXX_STANDARD variable to 17 if it's not already defined, and issues a fatal error if the defined standard is less than C++17.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_11\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT DEFINED CMAKE_CXX_STANDARD)\n  set(CMAKE_CXX_STANDARD 17)\nelse()\n  if(CMAKE_CXX_STANDARD VERSION_LESS 17)\n    message(FATAL_ERROR \"At least C++17 is required.\")\n  endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: ONNX Scan operator node definition\nDESCRIPTION: This code snippet illustrates the structure of an ONNX Scan operator node. The Scan operator iterates over one or more input tensors, applying a subgraph to each slice and producing one or more output tensors. `num_scan_inputs` defines how many inputs are scanned.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nnode = make_node(\n    'Scan', ['X1', 'X2'], ['Y1', 'Y2'],\n    name='Sc_Scan', body=graph, num_scan_inputs=1, domain='')\n```\n\n----------------------------------------\n\nTITLE: Enabling Compile Commands Export\nDESCRIPTION: This line enables the generation of a compile_commands.json file. This file is used by language servers and other tools to provide better code completion and diagnostics.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nset(CMAKE_EXPORT_COMPILE_COMMANDS ON)\n```\n\n----------------------------------------\n\nTITLE: Setting Link Flags for Apple\nDESCRIPTION: This snippet sets the `-undefined dynamic_lookup` link flag for the 'onnx' target on Apple platforms. This allows the program to dynamically look up symbols at runtime.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_39\n\nLANGUAGE: cmake\nCODE:\n```\nif(APPLE)\n  set_target_properties(onnx PROPERTIES LINK_FLAGS \"-undefined dynamic_lookup\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Starting a Local PyPI Server with Docker\nDESCRIPTION: This Docker command starts a local PyPI server for testing purposes. It does not require authentication and allows pushing the same version multiple times, useful for repeated testing.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/OnnxReleases.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ndocker run --rm -it --platform linux/amd64 -p 80:8080 pypiserver/pypiserver:latest run -a . -P .\n```\n\n----------------------------------------\n\nTITLE: Installing Targets\nDESCRIPTION: This snippet installs the 'onnx' and 'onnx_proto' targets, exporting the 'ONNXTargets' to the specified destination directory.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_44\n\nLANGUAGE: cmake\nCODE:\n```\ninstall(TARGETS\n  onnx onnx_proto\n  EXPORT ONNXTargets DESTINATION ${CMAKE_INSTALL_LIBDIR})\n```\n\n----------------------------------------\n\nTITLE: Setting ONNX_ML environment variable (UNIX)\nDESCRIPTION: Sets the ONNX_ML environment variable to 1 (true) in a UNIX-like environment using the export command. This affects documentation generation related to machine learning components within ONNX.\nSOURCE: https://github.com/onnx/onnx/blob/main/CONTRIBUTING.md#_snippet_2\n\nLANGUAGE: sh\nCODE:\n```\n# UNIX\nexport ONNX_ML=1\npip install -e . -v\npython onnx/defs/gen_doc.py\n```\n\n----------------------------------------\n\nTITLE: Including Summary Configuration\nDESCRIPTION: This includes cmake/summary.cmake, which provides functions for printing a summary of the build configuration.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_46\n\nLANGUAGE: cmake\nCODE:\n```\ninclude(cmake/summary.cmake)\nonnx_print_configuration_summary()\n```\n\n----------------------------------------\n\nTITLE: Including Test Configuration\nDESCRIPTION: This code includes the unittest.cmake file for building and running tests if the ONNX_BUILD_TESTS flag is set.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_45\n\nLANGUAGE: cmake\nCODE:\n```\nif(ONNX_BUILD_TESTS)\n  include(${ONNX_ROOT}/cmake/unittest.cmake)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Run C++ tests (Windows)\nDESCRIPTION: Executes the C++ tests (googletest) executable on Windows. The path to the executable depends on the build configuration (Debug or Release).\nSOURCE: https://github.com/onnx/onnx/blob/main/CONTRIBUTING.md#_snippet_10\n\nLANGUAGE: pwsh\nCODE:\n```\n# If you set DEBUG=1, use `.setuptools-cmake-build\\Debug\\onnx_gtests.exe` instead\n.setuptools-cmake-build\\Release\\onnx_gtests.exe\n```\n\n----------------------------------------\n\nTITLE: Performance Comparison with Fused Operator - Python\nDESCRIPTION: This code performs a performance comparison between the original linear regression model and the optimized model with the fused AddEyeLike operator. It uses the timeit module to measure the execution time of both models with a larger input size. The difference in output and execution time are printed, showing the performance gain from operator fusion.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/python.md#_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nimport timeit\nimport numpy\nfrom onnx.reference import ReferenceEvaluator\nfrom onnx.reference.op_run import OpRun\n\nclass AddEyeLike(OpRun):\n\n    op_domain = \"optimized\"\n\n    def _run(self, X, alpha=1.):\n        assert len(X.shape) == 2\n        assert X.shape[0] == X.shape[1]\n        X = X.copy()\n        ind = numpy.diag_indices(X.shape[0])\n        X[ind] += alpha\n        return (X,)\n\nsess = ReferenceEvaluator(\"linear_regression_improved.onnx\", verbose=2, new_ops=[AddEyeLike])\n\nx = numpy.random.randn(4, 100).astype(numpy.float32)\na = numpy.random.randn(100, 100).astype(numpy.float32) / 10\nb = numpy.random.randn(1, 100).astype(numpy.float32)\nfeeds = {'X': x, 'A': a, 'B': b}\n\nsess0 = ReferenceEvaluator(\"linear_regression.onnx\")\nsess1 = ReferenceEvaluator(\"linear_regression_improved.onnx\", new_ops=[AddEyeLike])\n\ny0 = sess0.run(None, feeds)[0]\ny1 = sess1.run(None, feeds)[0]\nprint(f\"difference: {numpy.abs(y0 - y1).max()}\")\nprint(f\"time with EyeLike+Add: {timeit.timeit(lambda: sess0.run(None, feeds), number=1000)}\")\nprint(f\"time with AddEyeLike: {timeit.timeit(lambda: sess1.run(None, feeds), number=1000)}\")\n```\n\n----------------------------------------\n\nTITLE: Building ONNX Package (Windows)\nDESCRIPTION: This snippet builds the ONNX package on Windows. It initializes git submodules, sets environment variables for building tests and ONNX-ML, defines cmake arguments, and then uses the `build` module to create a wheel package.\nSOURCE: https://github.com/onnx/onnx/blob/main/docs/docsgen/source/intro/converters.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\ngit submodule update --init --recursive\nset ONNX_BUILD_TESTS=1\nset ONNX_ML=$(onnx_ml)\nset CMAKE_ARGS=-DONNX_USE_PROTOBUF_SHARED_LIBS=ON -DONNX_USE_LITE_PROTO=ON -DONNX_WERROR=ON\n\npython -m build --wheel\n```\n\n----------------------------------------\n\nTITLE: Exporting ONNXTargets\nDESCRIPTION: This snippet handles exporting the ONNXTargets if Protobuf is not built as part of this build. If Protobuf is fetched, it's assumed that the export sets are not available and exporting is skipped.\nSOURCE: https://github.com/onnx/onnx/blob/main/CMakeLists.txt#_snippet_42\n\nLANGUAGE: cmake\nCODE:\n```\nif(NOT Build_Protobuf)\n  # If we fetched a Protobuf release (and didn't run its install step), then we\n  # don't have the export sets for Protobuf and its dependencies, which\n  # prevents us from creating an export set for ONNXTargets.\n  export(EXPORT ONNXTargets\n    FILE \"${PROJECT_BINARY_DIR}/ONNXTargets.cmake\"\n    NAMESPACE ONNX::\n  )\nendif()\n```"
  }
]
[
  {
    "owner": "duckdb",
    "repo": "dbt-duckdb",
    "content": "TITLE: Configuring Minimal In-Memory DuckDB Profile for dbt-duckdb in YAML\nDESCRIPTION: This snippet demonstrates a minimal dbt profile configuration that runs dbt-duckdb against an in-memory DuckDB database, which does not persist data across runs. It uses the `default` profile with a `dev` target specifying the type as `duckdb`. This setup is useful for testing and pipelines working purely with external files without persistent storage.\nSOURCE: https://github.com/duckdb/dbt-duckdb/blob/master/README.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\ndefault:\n  outputs:\n    dev:\n      type: duckdb\n  target: dev\n```\n\n----------------------------------------\n\nTITLE: Configuring DuckDB Profile with File Persistence, Extensions, and Secrets in YAML\nDESCRIPTION: This YAML snippet configures a dbt-duckdb profile that persists the DuckDB database on disk at `/tmp/dbt.duckdb`. It loads multiple DuckDB extensions including core, community, and nightly core extensions, specified by string names and name/repo pairs. It also demonstrates setting S3 secrets with environment variable interpolation for the AWS access key ID and secret access key, necessary for accessing cloud storage. This configuration enables enhanced functionality and secure secret management.\nSOURCE: https://github.com/duckdb/dbt-duckdb/blob/master/README.md#_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\ndefault:\n  outputs:\n    dev:\n      type: duckdb\n      path: /tmp/dbt.duckdb\n      extensions:\n        - httpfs\n        - parquet\n        - name: h3\n          repo: community\n        - name: uc_catalog\n          repo: core_nightly\n  target: dev\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt-duckdb Profile with Secrets for S3 Access in YAML\nDESCRIPTION: This snippet shows how to configure the `secrets` key in a dbt-duckdb profile to use DuckDB's Secrets Manager for secure AWS S3 access. It specifies S3 credentials with region, key ID, and secret, using environment variables inline. Extensions necessary for HTTP filesystem and Parquet support are also loaded. This setup facilitates secure reading and writing of Parquet files on S3.\nSOURCE: https://github.com/duckdb/dbt-duckdb/blob/master/README.md#_snippet_2\n\nLANGUAGE: YAML\nCODE:\n```\ndefault:\n  outputs:\n    dev:\n      type: duckdb\n      path: /tmp/dbt.duckdb\n      extensions:\n        - httpfs\n        - parquet\n      secrets:\n        - type: s3\n          region: my-aws-region\n          key_id: \"{{ env_var('S3_ACCESS_KEY_ID') }}\"\n          secret: \"{{ env_var('S3_SECRET_ACCESS_KEY') }}\"\n  target: dev\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt-duckdb Plugins in Project Profile - YAML\nDESCRIPTION: This YAML snippet demonstrates how to configure dbt-duckdb plugins within a dbt profile file. Each plugin is specified as an entry under the 'plugins' key with a required 'module' property, an optional 'alias', and a 'config' dictionary for plugin-specific options. This configuration supports built-in modules (by short name) and custom plugins (using full import path). Required dependencies must be installed for each plugin used. The snippet highlights configuring Google Sheets and SQLAlchemy plugins, showing how to use environment variables for sensitive credentials.\nSOURCE: https://github.com/duckdb/dbt-duckdb/blob/master/README.md#_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\ndefault:\n  outputs:\n    dev:\n      type: duckdb\n      path: /tmp/dbt.duckdb\n      plugins:\n        - module: gsheet\n          config:\n            method: oauth\n        - module: sqlalchemy\n          alias: sql\n          config:\n            connection_url: \"{{ env_var('DBT_ENV_SECRET_SQLALCHEMY_URI') }}\"\n        - module: path.to.custom_udf_module\n\n```\n\n----------------------------------------\n\nTITLE: Configuring External Sources with Meta Option - YAML\nDESCRIPTION: This YAML snippet configures an external source in dbt using the 'meta' option, enabling dynamic reference to external files (e.g., Parquet files on S3) via an f-string pattern. Each table under the defined source will have its location resolved according to the pattern using the table's name. The configuration impacts both execution and auto-generated documentation. No additional dependencies are required beyond dbt-duckdb's reading capabilities. The expected output is the correct file path used in compiled SQL queries.\nSOURCE: https://github.com/duckdb/dbt-duckdb/blob/master/README.md#_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nsources:\n  - name: external_source\n    meta:\n      external_location: \"s3://my-bucket/my-sources/{name}.parquet\"\n    tables:\n      - name: source1\n      - name: source2\n\n```\n\n----------------------------------------\n\nTITLE: Attaching Additional Databases to dbt-duckdb Profile in YAML\nDESCRIPTION: This snippet configures additional databases attached to the primary DuckDB database using the `attach` key. It supports local DuckDB files, read-only attachments, SQLite files, and Postgres connections, allowing multi-database querying within dbt-duckdb. Each attached database can include an optional alias and a type to specify the backend database engine.\nSOURCE: https://github.com/duckdb/dbt-duckdb/blob/master/README.md#_snippet_6\n\nLANGUAGE: YAML\nCODE:\n```\ndefault:\n  outputs:\n    dev:\n      type: duckdb\n      path: /tmp/dbt.duckdb\n      attach:\n        - path: /tmp/other.duckdb\n        - path: ./yet/another.duckdb\n          alias: yet_another\n        - path: s3://yep/even/this/works.duckdb\n          read_only: true\n        - path: sqlite.db\n          type: sqlite\n        - path: postgresql://username@hostname/dbname\n          type: postgres\n```\n\n----------------------------------------\n\nTITLE: Referencing External Sources in dbt Model - SQL\nDESCRIPTION: This SQL snippet demonstrates how to select all records from an external source table in a dbt model using the dbt Jinja syntax. The '{{ source() }}' macro dynamically resolves to the external file path or connector as specified in the source configuration. This pattern enables dynamic table referencing within dbt DAGs. No special dependencies are needed beyond dbt-duckdb and the correct external source configuration. Provided input is the source name and table name; output is the resolved external data as a relational table.\nSOURCE: https://github.com/duckdb/dbt-duckdb/blob/master/README.md#_snippet_9\n\nLANGUAGE: sql\nCODE:\n```\nSELECT *\nFROM {{ source('external_source', 'source1') }}\n\n```\n\n----------------------------------------\n\nTITLE: Creating an External Materialized dbt Model - SQL-with-Jinja\nDESCRIPTION: This snippet configures a dbt model to write its results directly to an external file, using the 'external' materialization, and sets the output file location via config. The model selects from another model and source, demonstrating join logic and adding a new computed column. Supported external formats are Parquet, CSV, or JSON, and the 'location' can reference local paths or S3 buckets. No dependencies are needed beyond dbt-duckdb and any remote storage connectors.\nSOURCE: https://github.com/duckdb/dbt-duckdb/blob/master/README.md#_snippet_14\n\nLANGUAGE: sql\nCODE:\n```\n{{ config(materialized='external', location='local/directory/file.parquet') }}\nSELECT m.*, s.id IS NOT NULL as has_source_id\nFROM {{ ref('upstream_model') }} m\nLEFT JOIN {{ source('upstream', 'source') }} s USING (id)\n\n```\n\n----------------------------------------\n\nTITLE: Starting the dbt-duckdb Interactive Shell via Python CLI\nDESCRIPTION: Shows how to start the dbt-duckdb interactive command-line shell that integrates dbt commands and DuckDB UI for visual data exploration. Illustrates launching the shell normally and specifying a dbt profile using the '--profile' flag. The shell supports standard dbt commands, autocompletion with an optional package, and automates common startup tasks like connection debugging and project parsing. This facilitates seamless iterative development and testing of dbt models with immediate visual feedback.\nSOURCE: https://github.com/duckdb/dbt-duckdb/blob/master/README.md#_snippet_18\n\nLANGUAGE: Shell\nCODE:\n```\npython -m dbt.adapters.duckdb.cli\n```\n\nLANGUAGE: Shell\nCODE:\n```\npython -m dbt.adapters.duckdb.cli --profile my_profile\n```\n\n----------------------------------------\n\nTITLE: Using Credential Chain Provider for Automatic AWS Credential Fetching in dbt-duckdb Profile YAML\nDESCRIPTION: This snippet defines how to leverage the `CREDENTIAL_CHAIN` secret provider in DuckDB's Secrets Manager within a dbt-duckdb profile. Instead of hardcoding credentials, the credential chain provider automatically fetches AWS credentials (e.g., from web identity tokens, environment, or EC2 metadata). It also loads necessary extensions for HTTP and Parquet support. This enhances security by relying on AWS credential discovery mechanisms.\nSOURCE: https://github.com/duckdb/dbt-duckdb/blob/master/README.md#_snippet_4\n\nLANGUAGE: YAML\nCODE:\n```\ndefault:\n  outputs:\n    dev:\n      type: duckdb\n      path: /tmp/dbt.duckdb\n      extensions:\n        - httpfs\n        - parquet\n      secrets:\n        - type: s3\n          provider: credential_chain\n  target: dev\n```\n\n----------------------------------------\n\nTITLE: Scoping S3 Credentials by Storage Prefix in dbt-duckdb Profile YAML\nDESCRIPTION: This snippet demonstrates scoping multiple S3 secrets to different storage path prefixes using the `scope` parameter. Different AWS regions and credential sources can be applied per scoped storage URI prefix. When querying a path, DuckDB selects the best matching secret with the longest prefix. This is useful when accessing data from multiple S3 buckets or regions requiring separate credentials.\nSOURCE: https://github.com/duckdb/dbt-duckdb/blob/master/README.md#_snippet_5\n\nLANGUAGE: YAML\nCODE:\n```\ndefault:\n  outputs:\n    dev:\n      type: duckdb\n      path: /tmp/dbt.duckdb\n      extensions:\n        - httpfs\n        - parquet\n      secrets:\n        - type: s3\n          provider: credential_chain\n          scope: [ \"s3://bucket-in-eu-region\", \"s3://bucket-2-in-eu-region\" ]\n          region: \"eu-central-1\"\n        - type: s3\n          region: us-west-2\n          scope: \"s3://bucket-in-us-region\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Table-level External Location Override - YAML\nDESCRIPTION: This YAML snippet demonstrates overriding the external location for a specific table in a dbt source configuration. The 'config' option under the table allows more granular control, such as specifying multiple Parquet file inputs via a DuckDB function call. The more specific table-level configuration takes precedence over the source-level pattern and is not included in documentation by default. The approach requires dbt-duckdb and correct path specification; no extra dependencies are needed beyond database connectors for S3.\nSOURCE: https://github.com/duckdb/dbt-duckdb/blob/master/README.md#_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nsources:\n  - name: external_source\n    meta:\n      external_location: \"s3://my-bucket/my-sources/{name}.parquet\"\n    tables:\n      - name: source1\n      - name: source2\n        config:\n          external_location: \"read_parquet(['s3://my-bucket/my-sources/source2a.parquet', 's3://my-bucket/my-sources/source2b.parquet'])\"\n\n```\n\n----------------------------------------\n\nTITLE: Creating and Invoking Custom Table Functions in dbt-duckdb SQL\nDESCRIPTION: Demonstrates how to define a dbt model with the 'table_function' materialization in SQL, enabling parameterized views with late binding to underlying tables. Shows examples without parameters, with parameters, and how to invoke these table functions. Requires dbt-duckdb with support for table_function materializations. This feature reduces the need to recreate views when underlying tables change and enables filter pushdown via parameters. The example includes configuration blocks and SQL query templates that use Jinja macros like 'config' and 'ref'.\nSOURCE: https://github.com/duckdb/dbt-duckdb/blob/master/README.md#_snippet_16\n\nLANGUAGE: SQL\nCODE:\n```\n{{\n    config(\n        materialized='table_function'\n    )\n}}\nselect * from {{ ref(\"example_table\") }}\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect * from {{ ref(\"my_table_function\") }}()\n```\n\nLANGUAGE: SQL\nCODE:\n```\n{{\n    config(\n        materialized='table_function',\n        parameters=['where_a', 'where_b']\n    )\n}}\nselect *\nfrom {{ ref(\"example_table\") }}\nwhere 1=1\n    and a = where_a\n    and b = where_b\n```\n\nLANGUAGE: SQL\nCODE:\n```\nselect * from {{ ref(\"my_table_function_with_parameters\") }}(1, 2)\n```\n\n----------------------------------------\n\nTITLE: Configuring dbt-duckdb Profile to Use fsspec-Compatible Filesystems for Cloud Storage in YAML\nDESCRIPTION: This snippet illustrates configuring the `filesystems` attribute in the dbt-duckdb profile to enable access to cloud storage using fsspec implementations. Here, an S3 filesystem is configured with necessary credentials and client parameters, including an endpoint URL for local S3-compatible services (e.g., Localstack). This allows dbt-duckdb to read/write data files directly from cloud storage backends.\nSOURCE: https://github.com/duckdb/dbt-duckdb/blob/master/README.md#_snippet_3\n\nLANGUAGE: YAML\nCODE:\n```\ndefault:\n  outputs:\n    dev:\n      type: duckdb\n      path: /tmp/dbt.duckdb\n      filesystems:\n        - fs: s3\n          anon: false\n          key: \"{{ env_var('S3_ACCESS_KEY_ID') }}\"\n          secret: \"{{ env_var('S3_SECRET_ACCESS_KEY') }}\"\n          client_kwargs:\n            endpoint_url: \"http://localhost:4566\"\n  target: dev\n```\n\n----------------------------------------\n\nTITLE: Configuring Table-level External Source with Function Call and Formatter - YAML\nDESCRIPTION: This YAML configuration sets up a dbt source table to use a DuckDB function call, such as 'read_csv()', for loading external CSV data, and demonstrates using the 'formatter' config option to avoid conflicts between DBT string formatting and function parameters. The example passes column types and headers explicitly to 'read_csv'. This setup is especially important when complex arguments contain curly braces, which can confuse f-string or str.format processing. The main dependency is dbt-duckdb support for function-call locations.\nSOURCE: https://github.com/duckdb/dbt-duckdb/blob/master/README.md#_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\nsources:\n  - name: flights_source\n    tables:\n      - name: flights\n        config:\n          external_location: \"read_csv('flights.csv', types={'FlightDate': 'DATE'}, names=['FlightDate', 'UniqueCarrier'])\"\n          formatter: oldstyle\n\n```\n\n----------------------------------------\n\nTITLE: Batch Processing in Python Models for dbt-duckdb Using PyArrow\nDESCRIPTION: Provides a Python model example for dbt-duckdb demonstrating batch processing with PyArrow to handle datasets larger than memory. The snippet defines a 'batcher' generator function processing record batches into Pandas DataFrames, performing operations, and yielding transformed batches. The main 'model' function fetches a referenced model as a DuckDB Relation, obtains batches, processes them through 'batcher', and returns a RecordBatchReader compatible with DuckDB. Requires dbt version 1.6.1 or higher and PyArrow installed. This enables efficient memory use during model execution.\nSOURCE: https://github.com/duckdb/dbt-duckdb/blob/master/README.md#_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nimport pyarrow as pa\n\ndef batcher(batch_reader: pa.RecordBatchReader):\n    for batch in batch_reader:\n        df = batch.to_pandas()\n        # Do some operations on the DF...\n        # ...then yield back a new batch\n        yield pa.RecordBatch.from_pandas(df)\n\ndef model(dbt, session):\n    big_model = dbt.ref(\"big_model\")\n    batch_reader = big_model.record_batch(100_000)\n    batch_iter = batcher(batch_reader)\n    return pa.RecordBatchReader.from_batches(batch_reader.schema, batch_iter)\n```\n\n----------------------------------------\n\nTITLE: Referencing Source with Table-level External Location - SQL\nDESCRIPTION: This SQL example illustrates selecting from a dbt source that has a customized external location at the table level. dbt will compile this source reference to a function call (e.g., 'read_parquet([...])') based on the configuration, enabling more flexible data ingestion strategies like querying multiple files as a single table. The result is a compiled SQL statement that DuckDB executes against external resources.\nSOURCE: https://github.com/duckdb/dbt-duckdb/blob/master/README.md#_snippet_12\n\nLANGUAGE: sql\nCODE:\n```\nSELECT *\nFROM {{ source('external_source', 'source2') }}\n\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT *\nFROM read_parquet(['s3://my-bucket/my-sources/source2a.parquet', 's3://my-bucket/my-sources/source2b.parquet'])\n\n```\n\n----------------------------------------\n\nTITLE: Registering Upstream External Models Macro in dbt - YAML\nDESCRIPTION: This YAML snippet shows how to configure the 'on-run-start' setting in 'dbt_project.yml' to call the 'register_upstream_external_models' macro. This is necessary for ensuring that external models/tables are registered as DuckDB views when using an in-memory database across multiple dbt runs. Without this, downstream dependencies on external models may fail due to missing registrations. No extra dependencies are required besides dbt-duckdb.\nSOURCE: https://github.com/duckdb/dbt-duckdb/blob/master/README.md#_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\non-run-start:\n  - \"{{ register_upstream_external_models() }}\"\n\n```\n\n----------------------------------------\n\nTITLE: Compiled SQL for External Source Reference - SQL\nDESCRIPTION: This SQL snippet shows the result of compiling a dbt model referencing an external source where the source is backed by a Parquet file on S3. The final SQL used by DuckDB directly accesses the file, bypassing any import phase. This is the direct output of dbt's compilation process for such models. It assumes the correct 'external_location' pattern was specified in the source configuration.\nSOURCE: https://github.com/duckdb/dbt-duckdb/blob/master/README.md#_snippet_10\n\nLANGUAGE: sql\nCODE:\n```\nSELECT *\nFROM 's3://my-bucket/my-sources/source1.parquet'\n\n```"
  }
]
[
  {
    "owner": "ggml-org",
    "repo": "whisper.cpp",
    "content": "TITLE: Whisper CLI Usage and Options\nDESCRIPTION: Comprehensive list of command-line arguments and options for the whisper-cli program, including audio processing parameters, output formats, and model configuration settings. The CLI supports features like multi-threading, language detection, translation, diarization, and various output formats including TXT, VTT, SRT, LRC, CSV, and JSON.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/cli/README.md#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n./build/bin/whisper-cli -h\n\nusage: ./build-pkg/bin/whisper-cli [options] file0.wav file1.wav ...\n\noptions:\n  -h,        --help              [default] show this help message and exit\n  -t N,      --threads N         [4      ] number of threads to use during computation\n  -p N,      --processors N      [1      ] number of processors to use during computation\n  -ot N,     --offset-t N        [0      ] time offset in milliseconds\n  -on N,     --offset-n N        [0      ] segment index offset\n  -d  N,     --duration N        [0      ] duration of audio to process in milliseconds\n  -mc N,     --max-context N     [-1     ] maximum number of text context tokens to store\n  -ml N,     --max-len N         [0      ] maximum segment length in characters\n  -sow,      --split-on-word     [false  ] split on word rather than on token\n  -bo N,     --best-of N         [5      ] number of best candidates to keep\n  -bs N,     --beam-size N       [5      ] beam size for beam search\n  -ac N,     --audio-ctx N       [0      ] audio context size (0 - all)\n  -wt N,     --word-thold N      [0.01   ] word timestamp probability threshold\n  -et N,     --entropy-thold N   [2.40   ] entropy threshold for decoder fail\n  -lpt N,    --logprob-thold N   [-1.00  ] log probability threshold for decoder fail\n  -tp,       --temperature N     [0.00   ] The sampling temperature, between 0 and 1\n  -tpi,      --temperature-inc N [0.20   ] The increment of temperature, between 0 and 1\n  -debug,    --debug-mode        [false  ] enable debug mode (eg. dump log_mel)\n  -tr,       --translate         [false  ] translate from source language to english\n  -di,       --diarize           [false  ] stereo audio diarization\n  -tdrz,     --tinydiarize       [false  ] enable tinydiarize (requires a tdrz model)\n  -nf,       --no-fallback       [false  ] do not use temperature fallback while decoding\n  -otxt,     --output-txt        [false  ] output result in a text file\n  -ovtt,     --output-vtt        [false  ] output result in a vtt file\n  -osrt,     --output-srt        [false  ] output result in a srt file\n  -olrc,     --output-lrc        [false  ] output result in a lrc file\n  -owts,     --output-words      [false  ] output script for generating karaoke video\n  -fp,       --font-path         [/System/Library/Fonts/Supplemental/Courier New Bold.ttf] path to a monospace font for karaoke video\n  -ocsv,     --output-csv        [false  ] output result in a CSV file\n  -oj,       --output-json       [false  ] output result in a JSON file\n  -ojf,      --output-json-full  [false  ] include more information in the JSON file\n  -of FNAME, --output-file FNAME [       ] output file path (without file extension)\n  -np,       --no-prints         [false  ] do not print anything other than the results\n  -ps,       --print-special     [false  ] print special tokens\n  -pc,       --print-colors      [false  ] print colors\n  -pp,       --print-progress    [false  ] print progress\n  -nt,       --no-timestamps     [false  ] do not print timestamps\n  -l LANG,   --language LANG     [en     ] spoken language ('auto' for auto-detect)\n  -dl,       --detect-language   [false  ] exit after automatically detecting language\n             --prompt PROMPT     [       ] initial prompt (max n_text_ctx/2 tokens)\n  -m FNAME,  --model FNAME       [models/ggml-base.en.bin] model path\n  -f FNAME,  --file FNAME        [       ] input WAV file path\n  -oved D,   --ov-e-device DNAME [CPU    ] the OpenVINO device used for encode inference\n  -dtw MODEL --dtw MODEL         [       ] compute token-level timestamps\n  -ls,       --log-score         [false  ] log best decoder scores of tokens\n  -ng,       --no-gpu            [false  ] disable GPU\n  -fa,       --flash-attn        [false  ] flash attention\n  --suppress-regex REGEX         [       ] regular expression matching tokens to suppress\n  --grammar GRAMMAR              [       ] GBNF grammar to guide decoding\n  --grammar-rule RULE            [       ] top-level GBNF grammar rule name\n  --grammar-penalty N            [100.0  ] scales down logits of nongrammar tokens\n```\n\n----------------------------------------\n\nTITLE: Building whisper.cpp with BLAS CPU Support\nDESCRIPTION: Configures and builds the project with CMake, enabling OpenBLAS support for CPU acceleration.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -DGGML_BLAS=1\ncmake --build build -j --config Release\n```\n\n----------------------------------------\n\nTITLE: Cloning the Whisper.cpp Repository\nDESCRIPTION: This command clones the Whisper.cpp repository from GitHub to the local machine.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/ggml-org/whisper.cpp.git\n```\n\n----------------------------------------\n\nTITLE: Running whisper-stream with Sliding Window and VAD\nDESCRIPTION: Command to run whisper-stream in sliding window mode with Voice Activity Detection. Setting step to 0 enables the sliding window, while the -vth parameter controls the VAD threshold for speech detection.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/stream/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./build/bin/whisper-stream -m ./models/ggml-base.en.bin -t 6 --step 0 --length 30000 -vth 0.6\n```\n\n----------------------------------------\n\nTITLE: Generating Word-level Timestamps with whisper-cli\nDESCRIPTION: Command-line example showing how to use the -ml 1 flag to generate word-level timestamps with whisper.cpp. The output shows a detailed breakdown of timestamps for each word in the transcription.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_25\n\nLANGUAGE: text\nCODE:\n```\n$ ./build/bin/whisper-cli -m ./models/ggml-base.en.bin -f ./samples/jfk.wav -ml 1\n\nwhisper_model_load: loading model from './models/ggml-base.en.bin'\n...\nsystem_info: n_threads = 4 / 10 | AVX2 = 0 | AVX512 = 0 | NEON = 1 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 |\n\nmain: processing './samples/jfk.wav' (176000 samples, 11.0 sec), 4 threads, 1 processors, lang = en, task = transcribe, timestamps = 1 ...\n\n[00:00:00.000 --> 00:00:00.320]\n[00:00:00.320 --> 00:00:00.370]   And\n[00:00:00.370 --> 00:00:00.690]   so\n[00:00:00.690 --> 00:00:00.850]   my\n[00:00:00.850 --> 00:00:01.590]   fellow\n[00:00:01.590 --> 00:00:02.850]   Americans\n[00:00:02.850 --> 00:00:03.300]  ,\n[00:00:03.300 --> 00:00:04.140]   ask\n[00:00:04.140 --> 00:00:04.990]   not\n[00:00:04.990 --> 00:00:05.410]   what\n[00:00:05.410 --> 00:00:05.660]   your\n[00:00:05.660 --> 00:00:06.260]   country\n[00:00:06.260 --> 00:00:06.600]   can\n[00:00:06.600 --> 00:00:06.840]   do\n[00:00:06.840 --> 00:00:07.010]   for\n[00:00:07.010 --> 00:00:08.170]   you\n[00:00:08.170 --> 00:00:08.190]  ,\n[00:00:08.190 --> 00:00:08.430]   ask\n[00:00:08.430 --> 00:00:08.910]   what\n[00:00:08.910 --> 00:00:09.040]   you\n[00:00:09.040 --> 00:00:09.320]   can\n[00:00:09.320 --> 00:00:09.440]   do\n[00:00:09.440 --> 00:00:09.760]   for\n[00:00:09.760 --> 00:00:10.020]   your\n[00:00:10.020 --> 00:00:10.510]   country\n[00:00:10.510 --> 00:00:11.000]  .\n```\n\n----------------------------------------\n\nTITLE: Basic Transcription with Whisper in Ruby\nDESCRIPTION: Demonstrates how to initialize a Whisper context with a model, configure transcription parameters, and transcribe an audio file. Parameters include language specification, time offsets, token limits, translation options, and formatting preferences.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/bindings/ruby/README.md#2025-04-11_snippet_0\n\nLANGUAGE: ruby\nCODE:\n```\nrequire \"whisper\"\n\nwhisper = Whisper::Context.new(\"base\")\n\nparams = Whisper::Params.new(\n  language: \"en\",\n  offset: 10_000,\n  duration: 60_000,\n  max_text_tokens: 300,\n  translate: true,\n  print_timestamps: false,\n  initial_prompt: \"Initial prompt here.\"\n)\n\nwhisper.transcribe(\"path/to/audio.wav\", params) do |whole_text|\n  puts whole_text\nend\n```\n\n----------------------------------------\n\nTITLE: Building whisper-stream with SDL2 Support\nDESCRIPTION: Instructions for building the whisper-stream tool with SDL2 support for microphone capture. Includes package installation commands for different platforms and the necessary CMake commands.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/stream/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Install SDL2\n# On Debian based linux distributions:\nsudo apt-get install libsdl2-dev\n\n# On Fedora Linux:\nsudo dnf install SDL2 SDL2-devel\n\n# Install SDL2 on Mac OS\nbrew install sdl2\n\ncmake -B build -DWHISPER_SDL2=ON\ncmake --build build --config Release\n\n./build/bin/whisper-stream\n```\n\n----------------------------------------\n\nTITLE: Generating Karaoke-style Videos with whisper.cpp\nDESCRIPTION: Commands to generate karaoke-style videos where the currently pronounced word is highlighted. The example shows how to use the -owts flag to create a bash script that uses ffmpeg to produce the video.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_27\n\nLANGUAGE: bash\nCODE:\n```\n./build/bin/whisper-cli -m ./models/ggml-base.en.bin -f ./samples/jfk.wav -owts\nsource ./samples/jfk.wav.wts\nffplay ./samples/jfk.wav.mp4\n```\n\nLANGUAGE: bash\nCODE:\n```\n./build/bin/whisper-cli -m ./models/ggml-base.en.bin -f ./samples/mm0.wav -owts\nsource ./samples/mm0.wav.wts\nffplay ./samples/mm0.wav.mp4\n```\n\nLANGUAGE: bash\nCODE:\n```\n./build/bin/whisper-cli -m ./models/ggml-base.en.bin -f ./samples/gb0.wav -owts\nsource ./samples/gb0.wav.wts\nffplay ./samples/gb0.wav.mp4\n```\n\n----------------------------------------\n\nTITLE: Transcribing Audio with Whisper-CLI\nDESCRIPTION: This command uses the built whisper-cli tool to transcribe an audio file (jfk.wav) using the Whisper model.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n./build/bin/whisper-cli -f samples/jfk.wav\n```\n\n----------------------------------------\n\nTITLE: Downloading Whisper Model in GGML Format\nDESCRIPTION: This script downloads a Whisper model converted to GGML format. The example uses the 'base.en' model.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsh ./models/download-ggml-model.sh base.en\n```\n\n----------------------------------------\n\nTITLE: Setting up Python Environment for OpenVINO (Linux/macOS)\nDESCRIPTION: Creates a Python virtual environment and installs required dependencies for OpenVINO conversion on Linux and macOS.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ncd models\npython3 -m venv openvino_conv_env\nsource openvino_conv_env/bin/activate\npython -m pip install --upgrade pip\npip install -r requirements-openvino.txt\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using WhisperCpp in Java\nDESCRIPTION: This snippet demonstrates how to initialize WhisperCpp, load a model, transcribe audio, and retrieve the transcribed text segments. It includes error handling and resource cleanup.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/bindings/java/README.md#2025-04-11_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nimport io.github.ggerganov.whispercpp.WhisperCpp;\n\npublic class Example {\n\n    public static void main(String[] args) {\n        WhisperCpp whisper = new WhisperCpp();\n        // By default, models are loaded from ~/.cache/whisper/ and are usually named \"ggml-${name}.bin\"\n        // or you can provide the absolute path to the model file.\n        long context = whisper.initContext(\"base.en\");\n        try {\n            var whisperParams = whisper.getFullDefaultParams(WhisperSamplingStrategy.WHISPER_SAMPLING_GREEDY);\n            // custom configuration if required\n            whisperParams.temperature_inc = 0f;\n\n            var samples = readAudio(); // divide each value by 32767.0f\n            whisper.fullTranscribe(whisperParams, samples);\n\n            int segmentCount = whisper.getTextSegmentCount(context);\n            for (int i = 0; i < segmentCount; i++) {\n                String text = whisper.getTextSegment(context, i);\n                System.out.println(segment.getText());\n            }\n        } finally {\n             whisper.freeContext(context);\n        }\n     }\n}\n```\n\n----------------------------------------\n\nTITLE: Using Docker for Model Download and Transcription\nDESCRIPTION: Demonstrates how to use Docker to download a model and transcribe an audio file using whisper.cpp.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\ndocker run -it --rm \\\n  -v path/to/models:/models \\\n  whisper.cpp:main \"./models/download-ggml-model.sh base /models\"\n\ndocker run -it --rm \\\n  -v path/to/models:/models \\\n  -v path/to/audios:/audios \\\n  whisper.cpp:main \"./main -m /models/ggml-base.bin -f /audios/jfk.wav\"\n```\n\n----------------------------------------\n\nTITLE: Using Whisper XCFramework in Swift Projects\nDESCRIPTION: Example of how to integrate the pre-built Whisper XCFramework into Swift projects using Swift Package Manager. This allows using the whisper.cpp library in iOS, visionOS, tvOS, and macOS applications without compiling from source.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_30\n\nLANGUAGE: swift\nCODE:\n```\n// swift-tools-version: 5.10\n// The swift-tools-version declares the minimum version of Swift required to build this package.\n\nimport PackageDescription\n\nlet package = Package(\n    name: \"Whisper\",\n    targets: [\n        .executableTarget(\n            name: \"Whisper\",\n            dependencies: [\n                \"WhisperFramework\"\n            ]),\n        .binaryTarget(\n            name: \"WhisperFramework\",\n            url: \"https://github.com/ggml-org/whisper.cpp/releases/download/v1.7.5/whisper-v1.7.5-xcframework.zip\",\n            checksum: \"c7faeb328620d6012e130f3d705c51a6ea6c995605f2df50f6e1ad68c59c6c4a\"\n        )\n    ]\n)\n```\n\n----------------------------------------\n\nTITLE: Building Whisper.cpp Project with CMake\nDESCRIPTION: These commands build the Whisper.cpp project using CMake, creating a build directory and compiling the project in Release configuration.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build\ncmake --build build --config Release\n```\n\n----------------------------------------\n\nTITLE: Converting Audio to 16-bit WAV Format\nDESCRIPTION: This ffmpeg command converts an input audio file to a 16-bit WAV format compatible with the whisper-cli tool.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nffmpeg -i input.mp3 -ar 16000 -ac 1 -c:a pcm_s16le output.wav\n```\n\n----------------------------------------\n\nTITLE: Using Pre-converted Whisper Models in Ruby\nDESCRIPTION: Shows how to use pre-converted Whisper models, which are downloaded automatically on first use and then cached. Includes methods for accessing cached models, clearing the cache, and listing available pre-converted models.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/bindings/ruby/README.md#2025-04-11_snippet_1\n\nLANGUAGE: ruby\nCODE:\n```\nbase_en = Whisper::Model.pre_converted_models[\"base.en\"]\nwhisper = Whisper::Context.new(base_en)\n```\n\nLANGUAGE: ruby\nCODE:\n```\nWhisper::Model.pre_converted_models[\"base\"].clear_cache\n```\n\nLANGUAGE: ruby\nCODE:\n```\nwhisper = Whisper::Context.new(\"base.en\")\n```\n\nLANGUAGE: ruby\nCODE:\n```\nputs Whisper::Model.pre_converted_models.keys\n# tiny\n# tiny.en\n# tiny-q5_1\n# tiny.en-q5_1\n# tiny-q8_0\n# base\n# base.en\n# base-q5_1\n# base.en-q5_1\n# base-q8_0\n#   :\n#   :\n```\n\n----------------------------------------\n\nTITLE: Using Segment Callbacks in Whisper Ruby\nDESCRIPTION: Demonstrates how to register a callback that is triggered on each new segment during transcription. The callback receives segment information including text, timestamps, and speaker change indicators.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/bindings/ruby/README.md#2025-04-11_snippet_4\n\nLANGUAGE: ruby\nCODE:\n```\n# Add hook before calling #transcribe\nparams.on_new_segment do |segment|\n  line = \"[%{st} --> %{ed}] %{text}\" % {\n    st: format_time(segment.start_time),\n    ed: format_time(segment.end_time),\n    text: segment.text\n  }\n  line << \" (speaker turned)\" if segment.speaker_next_turn?\n  puts line\nend\n\nwhisper.transcribe(\"path/to/audio.wav\", params)\n```\n\n----------------------------------------\n\nTITLE: Running Basic Real-Time Transcription with whisper-stream\nDESCRIPTION: Command to run the whisper-stream tool for real-time audio transcription from the microphone with a fixed step interval. This samples audio every 500ms and processes 5-second chunks using 8 threads.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/stream/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./build/bin/whisper-stream -m ./models/ggml-base.en.bin -t 8 --step 500 --length 5000\n```\n\n----------------------------------------\n\nTITLE: Downloading the Whisper Base Model\nDESCRIPTION: Command to download the English base model for Whisper, which is required for transcription.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/whisper.nvim/README.md#2025-04-11_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n./models/download-ggml-model.sh base.en\n```\n\n----------------------------------------\n\nTITLE: Converting PyTorch Models to ggml Format\nDESCRIPTION: Bash script demonstrating how to convert a PyTorch Whisper model to ggml format using the convert-pt-to-ggml.py script. The process includes creating a directory, running the conversion script, and moving the output file.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/models/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmkdir models/whisper-medium\npython models/convert-pt-to-ggml.py ~/.cache/whisper/medium.pt ~/path/to/repo/whisper/ ./models/whisper-medium\nmv ./models/whisper-medium/ggml-model.bin models/ggml-medium.bin\nrmdir models/whisper-medium\n```\n\n----------------------------------------\n\nTITLE: Downloading ggml Models with download-ggml-model.sh\nDESCRIPTION: Example of downloading a pre-converted ggml model using the download-ggml-model.sh script. The command downloads the base.en model and outputs information about how to use it.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/models/README.md#2025-04-11_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n$ ./download-ggml-model.sh base.en\nDownloading ggml model base.en ...\nmodels/ggml-base.en.bin          100%[=============================================>] 141.11M  5.41MB/s    in 22s\nDone! Model 'base.en' saved in 'models/ggml-base.en.bin'\nYou can now use it like this:\n\n  $ ./build/bin/whisper-cli -m models/ggml-base.en.bin -f samples/jfk.wav\n```\n\n----------------------------------------\n\nTITLE: Defining Build Options for whisper.cpp\nDESCRIPTION: Defines the available build options for the project, including general settings, debug options, sanitizers, and third-party library integrations. These options control the build behavior and feature set of whisper.cpp.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/CMakeLists.txt#2025-04-11_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\n#\n# option list\n#\n\n# general\noption(WHISPER_CCACHE \"whisper: use ccache if available\" ON)\n\n# debug\noption(WHISPER_ALL_WARNINGS           \"whisper: enable all compiler warnings\"                   ON)\noption(WHISPER_ALL_WARNINGS_3RD_PARTY \"whisper: enable all compiler warnings in 3rd party libs\" OFF)\n\n# build\noption(WHISPER_FATAL_WARNINGS  \"whisper: enable -Werror flag\"               OFF)\noption(WHISPER_USE_SYSTEM_GGML \"whisper: use system-installed GGML library\" OFF)\n\n# sanitizers\noption(WHISPER_SANITIZE_THREAD    \"whisper: enable thread sanitizer\"    OFF)\noption(WHISPER_SANITIZE_ADDRESS   \"whisper: enable address sanitizer\"   OFF)\noption(WHISPER_SANITIZE_UNDEFINED \"whisper: enable undefined sanitizer\" OFF)\n\n# extra artifacts\noption(WHISPER_BUILD_TESTS    \"whisper: build tests\"          ${WHISPER_STANDALONE})\noption(WHISPER_BUILD_EXAMPLES \"whisper: build examples\"       ${WHISPER_STANDALONE})\noption(WHISPER_BUILD_SERVER   \"whisper: build server example\" ${WHISPER_STANDALONE})\n\n# 3rd party libs\noption(WHISPER_CURL \"whisper: use libcurl to download model from an URL\" OFF)\noption(WHISPER_SDL2 \"whisper: support for libSDL2\" OFF)\n\nif (CMAKE_SYSTEM_NAME MATCHES \"Linux\")\n    option(WHISPER_FFMPEG \"whisper: support building and linking with ffmpeg libs (avcodec, swresample, ...)\" OFF)\nendif()\n\noption(WHISPER_COREML                \"whisper: enable Core ML framework\"  OFF)\noption(WHISPER_COREML_ALLOW_FALLBACK \"whisper: allow non-CoreML fallback\" OFF)\noption(WHISPER_OPENVINO              \"whisper: support for OpenVINO\"      OFF)\n```\n\n----------------------------------------\n\nTITLE: Building and Running Voice-Controlled Chess in Bash\nDESCRIPTION: Commands for building the wchess project from source using CMake and running it with a Whisper model. The resulting program displays a chess board interface in the terminal and accepts voice commands for moves.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/wchess/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir build && cd build\ncmake -DWHISPER_SDL2=1 ..\nmake -j\n\n./bin/wchess -m ../models/ggml-base.en.bin\n\nMove: start\n\na b c d e f g h\nr n b q k b n r 8\np p p p p p p p 7\n. * . * . * . * 6\n* . * . * . * . 5\n. * . * . * . * 4\n* . * . * . * . 3\nP P P P P P P P 2\nR N B Q K B N R 1\n\nWhite's turn\n[(l)isten/(p)ause/(q)uit]: \n```\n\n----------------------------------------\n\nTITLE: Loading Custom Whisper Models in Ruby\nDESCRIPTION: Demonstrates how to use local model files and remotely hosted models in the Whisper context. Supports loading from local paths or remote URLs.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/bindings/ruby/README.md#2025-04-11_snippet_2\n\nLANGUAGE: ruby\nCODE:\n```\nwhisper = Whisper::Context.new(\"path/to/your/model.bin\")\n```\n\nLANGUAGE: ruby\nCODE:\n```\nwhisper = Whisper::Context.new(\"https://example.net/uri/of/your/model.bin\")\n# Or\nwhisper = Whisper::Context.new(URI(\"https://example.net/uri/of/your/model.bin\"))\n```\n\n----------------------------------------\n\nTITLE: Building Whisper.wasm with Emscripten\nDESCRIPTION: This snippet shows how to clone the whisper.cpp repository, create a build directory, and compile the project using Emscripten for WebAssembly output. It uses CMake for build configuration.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/whisper.wasm/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# build using Emscripten\ngit clone https://github.com/ggml-org/whisper.cpp\ncd whisper.cpp\nmkdir build-em && cd build-em\nemcmake cmake ..\nmake -j\n```\n\n----------------------------------------\n\nTITLE: Running Whisper.cpp Node.js Addon Example\nDESCRIPTION: Command to run the example Node.js script that uses the compiled Whisper.cpp addon. It demonstrates how to pass language, model path, and input file path as command-line arguments.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/addon.node/README.md#2025-04-11_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncd examples/addon.node\n\nnode index.js --language='language' --model='model-path' --fname_inp='file-path'\n```\n\n----------------------------------------\n\nTITLE: Accessing Whisper Model Information in Ruby\nDESCRIPTION: Shows how to retrieve detailed information about the loaded Whisper model, including vocabulary size, context dimensions, network architecture parameters, and model type.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/bindings/ruby/README.md#2025-04-11_snippet_5\n\nLANGUAGE: ruby\nCODE:\n```\nwhisper = Whisper::Context.new(\"base\")\nmodel = whisper.model\n\nmodel.n_vocab # => 51864\nmodel.n_audio_ctx # => 1500\nmodel.n_audio_state # => 512\nmodel.n_audio_head # => 8\nmodel.n_audio_layer # => 6\nmodel.n_text_ctx # => 448\nmodel.n_text_state # => 512\nmodel.n_text_head # => 8\nmodel.n_text_layer # => 6\nmodel.n_mels # => 80\nmodel.ftype # => 1\nmodel.type # => \"base\"\n```\n\n----------------------------------------\n\nTITLE: Configuring WebAssembly Build Target for Whisper.cpp using CMake\nDESCRIPTION: This CMake script configures the compilation of the Whisper speech recognition library to WebAssembly. It defines the target executable, links the Whisper library, and sets up Emscripten-specific link flags for thread support, memory management, and module export. The script also includes an optional configuration to embed the WASM binary directly in the JavaScript file for simpler deployment.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/bindings/javascript/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET libwhisper)\n\nadd_executable(${TARGET}\n    emscripten.cpp\n    )\n\ntarget_link_libraries(${TARGET} PRIVATE\n    whisper\n    )\n\nunset(EXTRA_FLAGS)\n\nif (WHISPER_WASM_SINGLE_FILE)\n    set(EXTRA_FLAGS \"-s SINGLE_FILE=1\")\n    message(STATUS \"Embedding WASM inside whisper.js\")\n\n    add_custom_command(\n        TARGET ${TARGET} POST_BUILD\n        COMMAND ${CMAKE_COMMAND} -E copy\n        ${CMAKE_BINARY_DIR}/bin/libwhisper.js\n        ${CMAKE_CURRENT_SOURCE_DIR}/whisper.js\n        )\n\n    add_custom_command(\n        TARGET ${TARGET} POST_BUILD\n        COMMAND ${CMAKE_COMMAND} -E copy\n        ${CMAKE_BINARY_DIR}/bin/libwhisper.worker.js\n        ${CMAKE_CURRENT_SOURCE_DIR}/libwhisper.worker.js\n        )\nendif()\n\nset_target_properties(${TARGET} PROPERTIES LINK_FLAGS \" \\\n    --bind \\\n    -s MODULARIZE=1 \\\n    -s EXPORT_NAME=\\\"'whisper_factory'\\\" \\\n    -s FORCE_FILESYSTEM=1 \\\n    -s USE_PTHREADS=1 \\\n    -s PTHREAD_POOL_SIZE=8 \\\n    -s ALLOW_MEMORY_GROWTH=1 \\\n    ${EXTRA_FLAGS} \\\n    \")\n```\n\n----------------------------------------\n\nTITLE: Generating Core ML Model for Whisper\nDESCRIPTION: This script generates a Core ML model for the 'base.en' Whisper model, which can be used for accelerated inference on Apple Silicon devices.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\n./models/generate-coreml-model.sh base.en\n```\n\n----------------------------------------\n\nTITLE: Building and Running Real-time Audio Input Example\nDESCRIPTION: Configures, builds, and runs the real-time audio input example using SDL2.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -DWHISPER_SDL2=ON\ncmake --build build --config Release\n./build/bin/whisper-stream -m ./models/ggml-base.en.bin -t 8 --step 500 --length 5000\n```\n\n----------------------------------------\n\nTITLE: Speaker Segmentation using tinydiarize in whisper.cpp\nDESCRIPTION: Example of using tinydiarize for speaker segmentation in whisper.cpp. It demonstrates how to download a compatible model and run it with the -tdrz flag, which adds speaker turn annotations to the transcription.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n# download a tinydiarize compatible model\n./models/download-ggml-model.sh small.en-tdrz\n\n# run as usual, adding the \"-tdrz\" command-line argument\n./build/bin/whisper-cli -f ./samples/a13.wav -m ./models/ggml-small.en-tdrz.bin -tdrz\n...\nmain: processing './samples/a13.wav' (480000 samples, 30.0 sec), 4 threads, 1 processors, lang = en, task = transcribe, tdrz = 1, timestamps = 1 ...\n...\n[00:00:00.000 --> 00:00:03.800]   Okay Houston, we've had a problem here. [SPEAKER_TURN]\n[00:00:03.800 --> 00:00:06.200]   This is Houston. Say again please. [SPEAKER_TURN]\n[00:00:06.200 --> 00:00:08.260]   Uh Houston we've had a problem.\n[00:00:08.260 --> 00:00:11.320]   We've had a main beam up on a volt. [SPEAKER_TURN]\n[00:00:11.320 --> 00:00:13.820]   Roger main beam interval. [SPEAKER_TURN]\n[00:00:13.820 --> 00:00:15.100]   Uh uh [SPEAKER_TURN]\n[00:00:15.100 --> 00:00:18.020]   So okay stand, by thirteen we're looking at it. [SPEAKER_TURN]\n[00:00:18.020 --> 00:00:25.740]   Okay uh right now uh Houston the uh voltage is uh is looking good um.\n[00:00:27.620 --> 00:00:29.940]   And we had a a pretty large bank or so.\n```\n\n----------------------------------------\n\nTITLE: Compiling Whisper.cpp Node.js Addon with cmake-js\nDESCRIPTION: Command to compile the Whisper.cpp Node.js addon using cmake-js. It specifies the target name as 'addon.node' and sets the build type to 'Release'.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/addon.node/README.md#2025-04-11_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nnpx cmake-js compile -T addon.node -B Release\n```\n\n----------------------------------------\n\nTITLE: Running the Whisper Benchmarking Tool with a Small English Model\nDESCRIPTION: This command demonstrates how to run the whisper-bench tool on the small.en model using 4 threads. The output shows detailed model information and performance metrics, including load time, encoding time per layer, and total execution time. The benchmark results can be submitted to a GitHub issue for comparison.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/bench/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# run the bench too on the small.en model using 4 threads\n$ ./build/bin/whisper-bench -m ./models/ggml-small.en.bin -t 4\n\nwhisper_model_load: loading model from './models/ggml-small.en.bin'\nwhisper_model_load: n_vocab       = 51864\nwhisper_model_load: n_audio_ctx   = 1500\nwhisper_model_load: n_audio_state = 768\nwhisper_model_load: n_audio_head  = 12\nwhisper_model_load: n_audio_layer = 12\nwhisper_model_load: n_text_ctx    = 448\nwhisper_model_load: n_text_state  = 768\nwhisper_model_load: n_text_head   = 12\nwhisper_model_load: n_text_layer  = 12\nwhisper_model_load: n_mels        = 80\nwhisper_model_load: f16           = 1\nwhisper_model_load: type          = 3\nwhisper_model_load: mem_required  = 1048.00 MB\nwhisper_model_load: adding 1607 extra tokens\nwhisper_model_load: ggml ctx size = 533.05 MB\nwhisper_model_load: memory size =    68.48 MB \nwhisper_model_load: model size  =   464.44 MB\n\nwhisper_print_timings:     load time =   240.82 ms\nwhisper_print_timings:      mel time =     0.00 ms\nwhisper_print_timings:   sample time =     0.00 ms\nwhisper_print_timings:   encode time =  1062.21 ms / 88.52 ms per layer\nwhisper_print_timings:   decode time =     0.00 ms / 0.00 ms per layer\nwhisper_print_timings:    total time =  1303.04 ms\n\nsystem_info: n_threads = 4 | AVX2 = 0 | AVX512 = 0 | NEON = 1 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | \n\nIf you wish, you can submit these results here:\n\n  https://github.com/ggml-org/whisper.cpp/issues/89\n\nPlease include the following information:\n\n  - CPU model\n  - Operating system\n  - Compiler\n```\n\n----------------------------------------\n\nTITLE: Building whisper.cpp with NVIDIA GPU Support\nDESCRIPTION: Configures and builds the project with CMake, enabling CUDA support for NVIDIA GPUs.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -DGGML_CUDA=1\ncmake --build build -j --config Release\n```\n\n----------------------------------------\n\nTITLE: Converting Hugging Face Fine-tuned Models to ggml Format\nDESCRIPTION: Bash script showing how to convert Hugging Face fine-tuned Whisper models to ggml format. The process includes cloning necessary repositories, downloading the model, and running the conversion script.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/models/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/openai/whisper\ngit clone https://github.com/ggml-org/whisper.cpp\n\n# clone HF fine-tuned model (this is just an example)\ngit clone https://huggingface.co/openai/whisper-medium\n\n# convert the model to ggml\npython3 ./whisper.cpp/models/convert-h5-to-ggml.py ./whisper-medium/ ./whisper .\n```\n\n----------------------------------------\n\nTITLE: Controlling Text Segment Length in Transcription\nDESCRIPTION: Demonstrates how to limit the length of generated text segments during transcription.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\n./build/bin/whisper-cli -m ./models/ggml-base.en.bin -f ./samples/jfk.wav -ml 16\n```\n\n----------------------------------------\n\nTITLE: Sample Output of Whisper.cpp Node.js Test Run\nDESCRIPTION: Example output showing the model loading process, system information, and transcription results from running the test script. Includes detailed timing metrics for different processing stages.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/bindings/javascript/README.md#2025-04-11_snippet_1\n\nLANGUAGE: text\nCODE:\n```\n$ node --experimental-wasm-threads --experimental-wasm-simd ../tests/test-whisper.js\n\nwhisper_model_load: loading model from 'whisper.bin'\nwhisper_model_load: n_vocab       = 51864\nwhisper_model_load: n_audio_ctx   = 1500\nwhisper_model_load: n_audio_state = 512\nwhisper_model_load: n_audio_head  = 8\nwhisper_model_load: n_audio_layer = 6\nwhisper_model_load: n_text_ctx    = 448\nwhisper_model_load: n_text_state  = 512\nwhisper_model_load: n_text_head   = 8\nwhisper_model_load: n_text_layer  = 6\nwhisper_model_load: n_mels        = 80\nwhisper_model_load: f16           = 1\nwhisper_model_load: type          = 2\nwhisper_model_load: adding 1607 extra tokens\nwhisper_model_load: mem_required  =  506.00 MB\nwhisper_model_load: ggml ctx size =  140.60 MB\nwhisper_model_load: memory size   =   22.83 MB\nwhisper_model_load: model size    =  140.54 MB\n\nsystem_info: n_threads = 8 / 10 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | NEON = 0 | F16C = 0 | FP16_VA = 0 | WASM_SIMD = 1 | BLAS = 0 |\n\noperator(): processing 176000 samples, 11.0 sec, 8 threads, 1 processors, lang = en, task = transcribe ...\n\n[00:00:00.000 --> 00:00:11.000]   And so my fellow Americans, ask not what your country can do for you, ask what you can do for your country.\n\nwhisper_print_timings:     load time =   162.37 ms\nwhisper_print_timings:      mel time =   183.70 ms\nwhisper_print_timings:   sample time =     4.27 ms\nwhisper_print_timings:   encode time =  8582.63 ms / 1430.44 ms per layer\nwhisper_print_timings:   decode time =   436.16 ms / 72.69 ms per layer\nwhisper_print_timings:    total time =  9370.90 ms\n```\n\n----------------------------------------\n\nTITLE: Building whisper.cpp with FFmpeg Support (Linux)\nDESCRIPTION: Configures and builds the project with CMake, enabling FFmpeg integration for additional audio format support on Linux.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_20\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -D WHISPER_FFMPEG=yes\ncmake --build build\n```\n\n----------------------------------------\n\nTITLE: Building whisper.cpp with Vulkan GPU Support\nDESCRIPTION: Configures and builds the project with CMake, enabling Vulkan support for cross-vendor GPU acceleration.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -DGGML_VULKAN=1\ncmake --build build -j --config Release\n```\n\n----------------------------------------\n\nTITLE: Configuring and Building Whisper Server with CMake\nDESCRIPTION: Sets up the whisper-server target by defining source files, including dependencies, linking libraries, and handling platform-specific requirements. The configuration links the common, json_cpp, and whisper libraries, adds threading support, and includes Windows-specific socket libraries when building on Windows.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/server/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET whisper-server)\nadd_executable(${TARGET} server.cpp httplib.h)\n\ninclude(DefaultTargetOptions)\n\ntarget_link_libraries(${TARGET} PRIVATE common json_cpp whisper ${CMAKE_THREAD_LIBS_INIT})\n\nif (WIN32)\n    target_link_libraries(${TARGET} PRIVATE ws2_32)\nendif()\n\ninstall(TARGETS ${TARGET} RUNTIME)\n```\n\n----------------------------------------\n\nTITLE: Running Transcription with Confidence Color-coding\nDESCRIPTION: Executes the whisper-cli with color-coded confidence output for transcribed text.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\n./build/bin/whisper-cli -m models/ggml-base.en.bin -f samples/gb0.wav --print-colors\n```\n\n----------------------------------------\n\nTITLE: Building whisper.cpp with OpenVINO Support\nDESCRIPTION: Configures and builds the project with CMake, enabling OpenVINO support.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -DWHISPER_OPENVINO=1\ncmake --build build -j --config Release\n```\n\n----------------------------------------\n\nTITLE: Whisper Server Model Loading API Request\nDESCRIPTION: cURL command example for loading a new model file into the server.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/server/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncurl 127.0.0.1:8080/load \\\n-H \"Content-Type: multipart/form-data\" \\\n-F model=\"<path-to-model-file>\"\n```\n\n----------------------------------------\n\nTITLE: Building Whisper Command with SDL2 Dependencies\nDESCRIPTION: Instructions for building the whisper-command tool with SDL2 library dependencies across different operating systems including Debian, Fedora, and MacOS.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/command/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Install SDL2\n# On Debian based linux distributions:\nsudo apt-get install libsdl2-dev\n\n# On Fedora Linux:\nsudo dnf install SDL2 SDL2-devel\n\n# Install SDL2 on Mac OS\nbrew install sdl2\n\ncmake -B build -DWHISPER_SDL2=ON\ncmake --build build --config Release\n```\n\n----------------------------------------\n\nTITLE: Building Whisper.cpp with Core ML Support\nDESCRIPTION: These commands build Whisper.cpp with Core ML support enabled, allowing for accelerated inference on Apple Silicon devices.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -DWHISPER_COREML=1\ncmake --build build -j --config Release\n```\n\n----------------------------------------\n\nTITLE: Creating Video Comparisons of Different Whisper Models\nDESCRIPTION: Commands to generate a video that compares the transcription quality of different whisper.cpp models on the same audio input. Uses a dedicated script and ffmpeg to produce a side-by-side comparison.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_28\n\nLANGUAGE: bash\nCODE:\n```\n./scripts/bench-wts.sh samples/jfk.wav\nffplay ./samples/jfk.wav.all.mp4\n```\n\n----------------------------------------\n\nTITLE: Building and Running whisper-talk-llama with SDL2\nDESCRIPTION: Instructions for installing SDL2 dependencies, building the whisper-talk-llama executable, and running it with specific command line arguments for Whisper and LLaMA models.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/talk-llama/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install SDL2\n# On Debian based linux distributions:\nsudo apt-get install libsdl2-dev\n\n# On Fedora Linux:\nsudo dnf install SDL2 SDL2-devel\n\n# Install SDL2 on Mac OS\nbrew install sdl2\n\n# Build the \"whisper-talk-llama\" executable\ncmake -B build -S . -DWHISPER_SDL2=ON\ncmake --build build --config Release\n\n# Run it\n./build/bin/whisper-talk-llama -mw ./models/ggml-small.en.bin -ml ../llama.cpp/models/llama-13b/ggml-model-q4_0.gguf -p \"Georgi\" -t 8\n```\n\n----------------------------------------\n\nTITLE: Running Whisper Command with Default Settings\nDESCRIPTION: Commands for running the voice assistant with default arguments and small model. Includes specific optimization parameters for Raspberry Pi deployment.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/command/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Run with default arguments and small model\n./whisper-command -m ./models/ggml-small.en.bin -t 8\n\n# On Raspberry Pi, use tiny or base models + \"-ac 768\" for better performance\n./whisper-command -m ./models/ggml-tiny.en.bin -ac 768 -t 3 -c 0\n```\n\n----------------------------------------\n\nTITLE: Whisper Server Command Line Options\nDESCRIPTION: Comprehensive list of command-line arguments for configuring the whisper.cpp server, including threading, processing, model, and server options.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/server/README.md#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n./build/bin/whisper-server -h\n\nusage: ./build/bin/whisper-server [options]\n\noptions:\n  -h,        --help              [default] show this help message and exit\n  -t N,      --threads N         [4      ] number of threads to use during computation\n  -p N,      --processors N      [1      ] number of processors to use during computation\n  -ot N,     --offset-t N        [0      ] time offset in milliseconds\n  -on N,     --offset-n N        [0      ] segment index offset\n  -d  N,     --duration N        [0      ] duration of audio to process in milliseconds\n  -mc N,     --max-context N     [-1     ] maximum number of text context tokens to store\n  -ml N,     --max-len N         [0      ] maximum segment length in characters\n  -sow,      --split-on-word     [false  ] split on word rather than on token\n  -bo N,     --best-of N         [2      ] number of best candidates to keep\n  -bs N,     --beam-size N       [-1     ] beam size for beam search\n  -wt N,     --word-thold N      [0.01   ] word timestamp probability threshold\n  -et N,     --entropy-thold N   [2.40   ] entropy threshold for decoder fail\n  -lpt N,    --logprob-thold N   [-1.00  ] log probability threshold for decoder fail\n  -debug,    --debug-mode        [false  ] enable debug mode (eg. dump log_mel)\n  -tr,       --translate         [false  ] translate from source language to english\n  -di,       --diarize           [false  ] stereo audio diarization\n  -tdrz,     --tinydiarize       [false  ] enable tinydiarize (requires a tdrz model)\n  -nf,       --no-fallback       [false  ] do not use temperature fallback while decoding\n  -ps,       --print-special     [false  ] print special tokens\n  -pc,       --print-colors      [false  ] print colors\n  -pr,       --print-realtime    [false  ] print output in realtime\n  -pp,       --print-progress    [false  ] print progress\n  -nt,       --no-timestamps     [false  ] do not print timestamps\n  -l LANG,   --language LANG     [en     ] spoken language ('auto' for auto-detect)\n  -dl,       --detect-language   [false  ] exit after automatically detecting language\n             --prompt PROMPT     [       ] initial prompt\n  -m FNAME,  --model FNAME       [models/ggml-base.en.bin] model path\n  -oved D,   --ov-e-device DNAME [CPU    ] the OpenVINO device used for encode inference\n  --host HOST,                   [127.0.0.1] Hostname/ip-adress for the server\n  --port PORT,                   [8080   ] Port number for the server\n  --convert,                     [false  ] Convert audio to WAV, requires ffmpeg on the server\n```\n\n----------------------------------------\n\nTITLE: Building and Testing Whisper.cpp Java Bindings\nDESCRIPTION: This bash script demonstrates how to clone the whisper.cpp repository, navigate to the Java bindings directory, and run the Gradle build process for testing the Java bindings.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/bindings/java/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/ggml-org/whisper.cpp.git\ncd whisper.cpp/bindings/java\n\n./gradlew build\n```\n\n----------------------------------------\n\nTITLE: Quantizing Whisper Model\nDESCRIPTION: This command quantizes a Whisper model using the Q5_0 method, which reduces model size and can improve inference speed.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n./build/bin/quantize models/ggml-base.en.bin models/ggml-base.en-q5_0.bin q5_0\n```\n\n----------------------------------------\n\nTITLE: GGML Backend Configuration Options\nDESCRIPTION: Defines CMake options for various hardware acceleration backends and their settings, including CUDA, Metal, Vulkan, OpenCL, and others. Each option controls specific features and behaviors of the respective backend.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/CMakeLists.txt#2025-04-11_snippet_7\n\nLANGUAGE: cmake\nCODE:\n```\noption(GGML_ACCELERATE \"ggml: enable Accelerate framework\" ON)\noption(GGML_BLAS \"ggml: use BLAS\" ${GGML_BLAS_DEFAULT})\nset(GGML_BLAS_VENDOR ${GGML_BLAS_VENDOR_DEFAULT} CACHE STRING \"ggml: BLAS library vendor\")\noption(GGML_LLAMAFILE \"ggml: use LLAMAFILE\" ${GGML_LLAMAFILE_DEFAULT})\n```\n\n----------------------------------------\n\nTITLE: Downloading LibriSpeech Audio Files\nDESCRIPTION: Command to download the necessary audio files from the LibriSpeech project for testing purposes.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/tests/librispeech/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ make get-audio\n```\n\n----------------------------------------\n\nTITLE: Running whisper-talk-llama with Session Support\nDESCRIPTION: Example command for running whisper-talk-llama with session management enabled, which allows for maintaining conversation context across multiple interactions by saving and loading model state.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/talk-llama/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./build/bin/whisper-talk-llama --session ./my-session-file -mw ./models/ggml-small.en.bin -ml ../llama.cpp/models/llama-13b/ggml-model-q4_0.gguf -p \"Georgi\" -t 8\n```\n\n----------------------------------------\n\nTITLE: Building whisper.cpp from the Project Root Directory\nDESCRIPTION: Commands to compile the whisper-cli executable and download a model in ggml format. These steps are prerequisites for running the LibriSpeech tests.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/tests/librispeech/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ # Execute the commands below in the project root dir.\n$ cmake -B build\n$ cmake --build build --config Release\n$ ./models/download-ggml-model.sh tiny\n```\n\n----------------------------------------\n\nTITLE: Starting a Local HTTP Server for Whisper.wasm\nDESCRIPTION: This snippet demonstrates how to start a local HTTP server using Python to serve the Whisper.wasm example. It also provides the URL to access the example in a web browser.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/whisper.wasm/README.md#2025-04-11_snippet_1\n\nLANGUAGE: console\nCODE:\n```\npython3 examples/server.py\n```\n\n----------------------------------------\n\nTITLE: Checking GPU Device Information\nDESCRIPTION: Command to verify GPU device installation and configuration using clinfo\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README_sycl.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo apt install clinfo\nsudo clinfo -l\n```\n\n----------------------------------------\n\nTITLE: Running CI Locally for whisper.cpp\nDESCRIPTION: Commands to execute the full CI process locally on your machine, with options for CPU-only build and CUDA support. Creates temporary directories for results and mounts, then runs the CI script.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ci/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir tmp\n\n# CPU-only build\nbash ./ci/run.sh ./tmp/results ./tmp/mnt\n\n# with CUDA support\nGG_BUILD_CUDA=1 bash ./ci/run.sh ./tmp/results ./tmp/mnt\n```\n\n----------------------------------------\n\nTITLE: Building Whisper.cpp with BLAS Support for POWER Architectures\nDESCRIPTION: These commands build Whisper.cpp with BLAS support, which is optimized for POWER architectures running Linux.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -DGGML_BLAS=1\ncmake --build build --config Release\n```\n\n----------------------------------------\n\nTITLE: Building whisper.cpp with SYCL Support\nDESCRIPTION: Commands for building whisper.cpp with SYCL support using CMake\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README_sycl.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nmkdir -p build\ncd build\nsource /opt/intel/oneapi/setvars.sh\n\n#for FP16\n#cmake .. -DWHISPER_SYCL=ON -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx -DWHISPER_SYCL_F16=ON \n\n#for FP32\ncmake .. -DWHISPER_SYCL=ON -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=icpx\n\n#build example/main only\n#cmake --build . --config Release --target main\n\n#build all binary\ncmake --build . --config Release -v\n```\n\n----------------------------------------\n\nTITLE: Building whisper.cpp with Ascend NPU Support\nDESCRIPTION: Configures and builds the project with CMake, enabling CANN support for Ascend NPU acceleration.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_19\n\nLANGUAGE: bash\nCODE:\n```\ncmake -B build -DGGML_CANN=1\ncmake --build build -j --config Release\n```\n\n----------------------------------------\n\nTITLE: Running Benchmarks with Python Script in whisper.cpp\nDESCRIPTION: Command to run the bench.py script for benchmarking whisper.cpp performance across different models and configurations. The script outputs a CSV file with benchmark results.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_29\n\nLANGUAGE: bash\nCODE:\n```\npython3 scripts/bench.py -f samples/jfk.wav -t 2,4,8 -p 1,2\n```\n\n----------------------------------------\n\nTITLE: Building the Whisper XCFramework for iOS\nDESCRIPTION: Command to build the whisper.xcframework which is required for the Objective-C application to function. This framework needs to be built before using the application.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/whisper.objc/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./build-xcframework.sh\n```\n\n----------------------------------------\n\nTITLE: Building whisper.cpp WebAssembly Benchmark with Emscripten\nDESCRIPTION: This snippet shows the commands to clone the whisper.cpp repository, create a build directory, and compile the project using Emscripten for WebAssembly. It requires Emscripten v3.1.2 to be installed.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/bench.wasm/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# build using Emscripten (v3.1.2)\ngit clone https://github.com/ggerganov/whisper.cpp\ncd whisper.cpp\nmkdir build-em && cd build-em\nemcmake cmake ..\nmake -j\n```\n\n----------------------------------------\n\nTITLE: Installing Python Dependencies for Core ML Model Creation\nDESCRIPTION: This command installs the necessary Python packages to generate a Core ML model for use with Whisper.cpp on Apple Silicon devices.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\npip install ane_transformers\npip install openai-whisper\npip install coremltools\n```\n\n----------------------------------------\n\nTITLE: Starting a Local HTTP Server for Testing\nDESCRIPTION: Command to start a Python-based HTTP server that hosts the WebAssembly application for local testing. This makes the stream.wasm demo accessible via a browser.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/stream.wasm/README.md#2025-04-11_snippet_1\n\nLANGUAGE: console\nCODE:\n```\npython3 examples/server.py\n```\n\n----------------------------------------\n\nTITLE: Whisper Server Inference API Request\nDESCRIPTION: cURL command example for making an inference request to the server with a WAV file upload and processing parameters.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/server/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl 127.0.0.1:8080/inference \\\n-H \"Content-Type: multipart/form-data\" \\\n-F file=\"@<file-path>\" \\\n-F temperature=\"0.0\" \\\n-F temperature_inc=\"0.2\" \\\n-F response_format=\"json\"\n```\n\n----------------------------------------\n\nTITLE: Building the command.wasm Voice Assistant with Emscripten\nDESCRIPTION: Build instructions for compiling the WebAssembly version of the command voice assistant using Emscripten v3.1.2. This process creates the necessary WebAssembly files that will run in the browser.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/command.wasm/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# build using Emscripten (v3.1.2)\ngit clone https://github.com/ggerganov/whisper.cpp\ncd whisper.cpp\nmkdir build-em && cd build-em\nemcmake cmake ..\nmake -j libcommand\n```\n\n----------------------------------------\n\nTITLE: Building stream.wasm with Emscripten\nDESCRIPTION: Commands to clone the whisper.cpp repository, create a build directory, and compile the project using Emscripten v3.1.2. This builds the WebAssembly version of the real-time transcription tool.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/stream.wasm/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# build using Emscripten (v3.1.2)\ngit clone https://github.com/ggerganov/whisper.cpp\ncd whisper.cpp\nmkdir build-em && cd build-em\nemcmake cmake ..\nmake -j\n```\n\n----------------------------------------\n\nTITLE: Building and Testing Whisper.cpp Go Bindings\nDESCRIPTION: Commands for cloning the repository and running tests for the Go bindings. This process compiles a static library and downloads a model file for testing.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/bindings/go/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/ggml-org/whisper.cpp.git\ncd whisper.cpp/bindings/go\nmake test\n```\n\n----------------------------------------\n\nTITLE: Building whisper.cpp Stream Tool\nDESCRIPTION: Commands to clone the whisper.cpp repository and build the stream tool needed for the plugin.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/whisper.nvim/README.md#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/ggerganov/whisper.cpp\ncd whisper.cpp\nmake stream\n```\n\n----------------------------------------\n\nTITLE: Configuring CPU Backend Variants with Architecture-Specific Flags in CMake\nDESCRIPTION: This CMake function configures CPU backend variants for the GGML library. It manages architecture-specific source files, compilation flags, and feature detection. The function handles setting up dynamic loading capabilities, manages special cases for Emscripten compilation, and includes error handling for incompatible build options.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-cpu/CMakeLists.txt#2025-04-11_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nlist(APPEND GGML_KLEIDIAI_SOURCES ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4vlx4_1x4vl_sme2_sdot.c)\n            set(PRIVATE_ARCH_FLAGS \"${PRIVATE_ARCH_FLAGS}+sve+sve2\")\n        endif()\n\n        set_source_files_properties(${GGML_KLEIDIAI_SOURCES} PROPERTIES COMPILE_OPTIONS \"${PRIVATE_ARCH_FLAGS}\")\n        list(APPEND GGML_CPU_SOURCES ${GGML_KLEIDIAI_SOURCES})\n    endif()\n\n    message(STATUS \"Adding CPU backend variant ${GGML_CPU_NAME}: ${ARCH_FLAGS} ${ARCH_DEFINITIONS}\")\n    target_sources(${GGML_CPU_NAME} PRIVATE ${GGML_CPU_SOURCES})\n    target_compile_options(${GGML_CPU_NAME} PRIVATE ${ARCH_FLAGS})\n    target_compile_definitions(${GGML_CPU_NAME} PRIVATE ${ARCH_DEFINITIONS})\n\n    if (GGML_BACKEND_DL)\n        if (GGML_NATIVE)\n            # the feature check relies on ARCH_DEFINITIONS, but it is not set with GGML_NATIVE\n            message(FATAL_ERROR \"GGML_NATIVE is not compatible with GGML_BACKEND_DL, consider using GGML_CPU_ALL_VARIANTS\")\n        endif()\n\n        # The feature detection code is compiled as a separate target so that\n        # it can be built without the architecture flags\n        # Since multiple variants of the CPU backend may be included in the same\n        # build, using set_source_files_properties() to set the arch flags is not possible\n        set(GGML_CPU_FEATS_NAME ${GGML_CPU_NAME}-feats)\n        add_library(${GGML_CPU_FEATS_NAME} OBJECT ggml-cpu/cpu-feats-x86.cpp)\n        target_include_directories(${GGML_CPU_FEATS_NAME} PRIVATE . .. ../include)\n        target_compile_definitions(${GGML_CPU_FEATS_NAME} PRIVATE ${ARCH_DEFINITIONS})\n        target_compile_definitions(${GGML_CPU_FEATS_NAME} PRIVATE GGML_BACKEND_DL GGML_BACKEND_BUILD GGML_BACKEND_SHARED)\n        set_target_properties(${GGML_CPU_FEATS_NAME} PROPERTIES POSITION_INDEPENDENT_CODE ON)\n        target_link_libraries(${GGML_CPU_NAME} PRIVATE ${GGML_CPU_FEATS_NAME})\n    endif()\n\n    if (EMSCRIPTEN)\n        set_target_properties(${GGML_CPU_NAME} PROPERTIES COMPILE_FLAGS \"-msimd128\")\n    endif()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Downloading a Whisper Model\nDESCRIPTION: Command to download the base English model required for speech recognition. The model is necessary for the application to perform transcription.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/whisper.objc/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./models/download-ggml-model.sh base.en\n```\n\n----------------------------------------\n\nTITLE: Sample GPU Device Output\nDESCRIPTION: Example output showing detected Intel GPU devices\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README_sycl.md#2025-04-11_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nPlatform #0: Intel(R) OpenCL Graphics\n `-- Device #0: Intel(R) Arc(TM) A770 Graphics\n\n\nPlatform #0: Intel(R) OpenCL HD Graphics\n `-- Device #0: Intel(R) Iris(R) Xe Graphics [0x9a49]\n```\n\n----------------------------------------\n\nTITLE: Converting Distilled Whisper Models to ggml Format\nDESCRIPTION: Bash script demonstrating how to convert distilled Whisper models from Hugging Face to ggml format. The process includes cloning repositories, downloading the models, and running conversion scripts for two different model sizes.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/models/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# clone OpenAI whisper and whisper.cpp\ngit clone https://github.com/openai/whisper\ngit clone https://github.com/ggml-org/whisper.cpp\n\n# get the models\ncd whisper.cpp/models\ngit clone https://huggingface.co/distil-whisper/distil-medium.en\ngit clone https://huggingface.co/distil-whisper/distil-large-v2\n\n# convert to ggml\npython3 ./convert-h5-to-ggml.py ./distil-medium.en/ ../../whisper .\nmv ggml-model.bin ggml-medium.en-distil.bin\n\npython3 ./convert-h5-to-ggml.py ./distil-large-v2/ ../../whisper .\nmv ggml-model.bin ggml-large-v2-distil.bin\n```\n\n----------------------------------------\n\nTITLE: Generating OpenVINO Encoder Model\nDESCRIPTION: Converts a Whisper model to OpenVINO format using a Python script.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\npython convert-whisper-to-openvino.py --model base.en\n```\n\n----------------------------------------\n\nTITLE: Running the Whisper Stream Tool\nDESCRIPTION: Command to run the stream tool with suggested parameters for real-time transcription. The step parameter can be increased on slower machines.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/whisper.nvim/README.md#2025-04-11_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\n./stream -t 8 -m models/ggml-base.en.bin --step 350 --length 10000 -f /tmp/whisper.nvim 2> /dev/null\n```\n\n----------------------------------------\n\nTITLE: Building Main whisper Library\nDESCRIPTION: Defines and configures the main whisper library target with version properties, includes, dependencies, and platform-specific settings. Integrates with optional components like CoreML, OpenVINO, and MKL based on build configuration.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/src/CMakeLists.txt#2025-04-11_snippet_6\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(whisper\n            ../include/whisper.h\n            whisper-arch.h\n            whisper.cpp\n            )\n\n# Set the version numbers\nset_target_properties(whisper PROPERTIES\n    VERSION ${PROJECT_VERSION}\n    SOVERSION ${SOVERSION}\n)\n\ntarget_include_directories(whisper PUBLIC . ../include)\ntarget_compile_features   (whisper PUBLIC cxx_std_11) # don't bump\n\nif (CMAKE_CXX_BYTE_ORDER STREQUAL \"BIG_ENDIAN\")\n    set(WHISPER_EXTRA_FLAGS ${WHISPER_EXTRA_FLAGS} -DWHISPER_BIG_ENDIAN)\nendif()\n\nif (WHISPER_EXTRA_FLAGS)\n    target_compile_options(whisper PRIVATE ${WHISPER_EXTRA_FLAGS})\nendif()\n\ntarget_link_libraries(whisper PUBLIC ggml)\n\nif (WHISPER_COREML)\n    target_link_libraries(whisper PRIVATE whisper.coreml)\nendif()\n\nif (WHISPER_OPENVINO)\n    target_link_libraries(whisper PRIVATE whisper.openvino)\nendif()\n\nif (WHISPER_MKL)\n    target_link_libraries(whisper PRIVATE MKL::MKL)\nendif()\n\nif (BUILD_SHARED_LIBS)\n    set_target_properties(whisper PROPERTIES POSITION_INDEPENDENT_CODE ON)\n    target_compile_definitions(whisper PRIVATE WHISPER_SHARED WHISPER_BUILD)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Model and Sample Paths in WhisperService.java\nDESCRIPTION: Modify the modelFilePath and sampleFilePath variables in the WhisperService.java file to specify the locations of the chosen whisper model and audio sample file.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/whisper.android.java/README.md#2025-04-11_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\n// In WhisperService.java\nString modelFilePath = \"path/to/your/model/file\";\nString sampleFilePath = \"path/to/your/sample/audio/file\";\n```\n\n----------------------------------------\n\nTITLE: Basic Usage of Whisper.cpp Go Bindings\nDESCRIPTION: Demonstrates the most basic usage pattern for the Go bindings, including loading a model, processing audio samples, and retrieving transcription segments.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/bindings/go/README.md#2025-04-11_snippet_0\n\nLANGUAGE: go\nCODE:\n```\nimport (\n\t\"github.com/ggerganov/whisper.cpp/bindings/go/pkg/whisper\"\n)\n\nfunc main() {\n\tvar modelpath string // Path to the model\n\tvar samples []float32 // Samples to process\n\n\t// Load the model\n\tmodel, err := whisper.New(modelpath)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tdefer model.Close()\n\n\t// Process samples\n\tcontext, err := model.NewContext()\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tif err := context.Process(samples, nil, nil, nil); err != nil {\n\t\treturn err\n\t}\n\n\t// Print out the results\n\tfor {\n\t\tsegment, err := context.NextSegment()\n\t\tif err != nil {\n\t\t\tbreak\n\t\t}\n\t\tfmt.Printf(\"[%6s->%6s] %s\\n\", segment.Start, segment.End, segment.Text)\n\t}\n}\n```\n\n----------------------------------------\n\nTITLE: Using Low-level API with Ruby Arrays in Whisper\nDESCRIPTION: Shows how to use the low-level API (#full and #full_parallel) with Ruby arrays as audio samples. This gives more flexibility but may be slower than the recommended file-based transcription method.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/bindings/ruby/README.md#2025-04-11_snippet_7\n\nLANGUAGE: ruby\nCODE:\n```\nrequire \"whisper\"\nrequire \"wavefile\"\n\nreader = WaveFile::Reader.new(\"path/to/audio.wav\", WaveFile::Format.new(:mono, :float, 16000))\nsamples = reader.enum_for(:each_buffer).map(&:samples).flatten\n\nwhisper = Whisper::Context.new(\"base\")\nwhisper\n  .full(Whisper::Params.new, samples)\n  .each_segment do |segment|\n    puts segment.text\n  end\n```\n\n----------------------------------------\n\nTITLE: Setting up OpenVINO Environment (Linux)\nDESCRIPTION: Sources the OpenVINO setup script to configure the environment on Linux.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nsource /path/to/l_openvino_toolkit_ubuntu22_2023.0.0.10926.b4452d56304_x86_64/setupvars.sh\n```\n\n----------------------------------------\n\nTITLE: Compiling Whisper.cpp Node.js Addon with Custom CMake Path\nDESCRIPTION: Example command showing how to compile the addon while specifying a custom CMake path using the '-c' option in cmake-js.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/addon.node/README.md#2025-04-11_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnpx cmake-js compile -c 'xxx/cmake' -T addon.node -B Release\n```\n\n----------------------------------------\n\nTITLE: Configuring Whisper-Talk-Llama Build with SDL2 in CMake\nDESCRIPTION: Sets up the build configuration for the whisper-talk-llama executable. It includes necessary source files, links required libraries, and sets compiler options. The build is conditional on WHISPER_SDL2 being enabled and includes special handling for Windows platforms.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/talk-llama/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif (WHISPER_SDL2)\n    set(CMAKE_CXX_STANDARD 17)\n    set(CMAKE_CXX_STANDARD_REQUIRED ON)\n\n    set(TARGET whisper-talk-llama)\n    add_executable(${TARGET} talk-llama.cpp\n        llama.cpp\n        llama-adapter.cpp\n        llama-arch.cpp\n        llama-batch.cpp\n        llama-chat.cpp\n        llama-context.cpp\n        llama-cparams.cpp\n        llama-grammar.cpp\n        llama-hparams.cpp\n        llama-impl.cpp\n        llama-kv-cache.cpp\n        llama-mmap.cpp\n        llama-model-loader.cpp\n        llama-model.cpp\n        llama-quant.cpp\n        llama-sampling.cpp\n        llama-vocab.cpp\n        unicode.cpp\n        unicode-data.cpp)\n    target_include_directories(${TARGET} PRIVATE ${SDL2_INCLUDE_DIRS})\n\n    target_link_libraries(${TARGET} PRIVATE common common-sdl whisper ${SDL2_LIBRARIES} ${CMAKE_THREAD_LIBS_INIT})\n\n    if(WIN32)\n        # It requires Windows 8.1 or later for PrefetchVirtualMemory\n        target_compile_definitions(${TARGET} PRIVATE -D_WIN32_WINNT=0x0602)\n    endif()\n\n    include(DefaultTargetOptions)\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Running the command.wasm Example with a Local HTTP Server\nDESCRIPTION: Commands to start a Python-based HTTP server that hosts the WebAssembly example. This allows testing the voice assistant in a browser on the local machine.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/command.wasm/README.md#2025-04-11_snippet_1\n\nLANGUAGE: console\nCODE:\n```\npython3 examples/server.py\n```\n\n----------------------------------------\n\nTITLE: Downloading and Converting Audio Samples in Makefile for whisper.cpp\nDESCRIPTION: This Makefile snippet defines a 'samples' target that downloads various public audio files and converts them to 16-bit WAV format using ffmpeg. It includes samples from different sources such as OSR, Common Voice, and specific YouTube videos.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/samples/README.md#2025-04-11_snippet_0\n\nLANGUAGE: Makefile\nCODE:\n```\nsamples:\n\t@echo \"Downloading samples...\"\n\t@mkdir -p samples\n\twhisper.cpp/wget -q --show-progress https://github.com/ggerganov/whisper.cpp/raw/master/samples/jfk.wav -O samples/jfk.wav\n\twhisper.cpp/wget -q --show-progress https://github.com/ggerganov/whisper.cpp/raw/master/samples/gb0.wav -O samples/gb0.wav\n\twhisper.cpp/wget -q --show-progress https://github.com/ggerganov/whisper.cpp/raw/master/samples/gb1.wav -O samples/gb1.wav\n\twhisper.cpp/wget -q --show-progress https://github.com/ggerganov/whisper.cpp/raw/master/samples/hp0.wav -O samples/hp0.wav\n\twhisper.cpp/wget -q --show-progress https://github.com/ggerganov/whisper.cpp/raw/master/samples/output-001.wav -O samples/output-001.wav\n\tffmpeg -i https://www.youtube.com/watch?v=HYuCa0Regx8 -ar 16000 -ac 1 -c:a pcm_s16le samples/test-001.wav\n\tffmpeg -i https://www.youtube.com/watch?v=PXi3Mv6KMzY -ar 16000 -ac 1 -c:a pcm_s16le samples/test-002.wav\n\tffmpeg -i https://www.youtube.com/watch?v=b-r5BM0lJpQ -ar 16000 -ac 1 -c:a pcm_s16le samples/test-003.wav\n\t@echo \"Done\"\n```\n\n----------------------------------------\n\nTITLE: Compiling Language Server for Whisper.cpp\nDESCRIPTION: Command to compile the language server for Whisper.cpp transcription integration.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/lsp/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake lsp\n```\n\n----------------------------------------\n\nTITLE: Copying Whisper.wasm Files to a Web Server\nDESCRIPTION: This snippet shows the commands to copy the necessary Whisper.wasm files to a web server's HTTP path. It includes the WebAssembly file and the worker JavaScript file.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/whisper.wasm/README.md#2025-04-11_snippet_2\n\nLANGUAGE: console\nCODE:\n```\n# copy the produced page to your HTTP path\ncp bin/whisper.wasm/*    /path/to/html/\ncp bin/libmain.worker.js /path/to/html/\n```\n\n----------------------------------------\n\nTITLE: Running whisper.cpp with SYCL\nDESCRIPTION: Command to run whisper.cpp with SYCL using specific GPU device\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README_sycl.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nGGML_SYCL_DEVICE=0 ./build/bin/main -m models/ggml-base.en.bin -f samples/jfk.wav\n```\n\n----------------------------------------\n\nTITLE: Configuring GGML CPU Backend with CMake\nDESCRIPTION: CMake function to configure GGML CPU backend variants with architecture-specific optimizations. Handles platform detection, compiler flags, dependencies like OpenMP and Accelerate framework, and sets up CPU-specific source files.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-cpu/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(ggml_add_cpu_backend_variant_impl tag_name)\n    if (tag_name)\n        set(GGML_CPU_NAME ggml-cpu-${tag_name})\n    else()\n        set(GGML_CPU_NAME ggml-cpu)\n    endif()\n\n    ggml_add_backend_library(${GGML_CPU_NAME})\n\n    list (APPEND GGML_CPU_SOURCES\n        ggml-cpu/ggml-cpu.c\n        ggml-cpu/ggml-cpu.cpp\n        ggml-cpu/ggml-cpu-aarch64.cpp\n        ggml-cpu/ggml-cpu-aarch64.h\n        ggml-cpu/ggml-cpu-hbm.cpp\n        ggml-cpu/ggml-cpu-hbm.h\n        ggml-cpu/ggml-cpu-quants.c\n        ggml-cpu/ggml-cpu-quants.h\n        ggml-cpu/ggml-cpu-traits.cpp\n        ggml-cpu/ggml-cpu-traits.h\n        ggml-cpu/amx/amx.cpp\n        ggml-cpu/amx/amx.h\n        ggml-cpu/amx/mmq.cpp\n        ggml-cpu/amx/mmq.h\n        ggml-cpu/ggml-cpu-impl.h\n        ggml-cpu/common.h\n        ggml-cpu/binary-ops.h\n        ggml-cpu/binary-ops.cpp\n        ggml-cpu/unary-ops.h\n        ggml-cpu/unary-ops.cpp\n        ggml-cpu/simd-mappings.h\n        ggml-cpu/vec.h\n        ggml-cpu/vec.cpp\n        ggml-cpu/ops.h\n        ggml-cpu/ops.cpp\n        )\n\n    target_compile_features(${GGML_CPU_NAME} PRIVATE c_std_11 cxx_std_17)\n    target_include_directories(${GGML_CPU_NAME} PRIVATE . ggml-cpu)\n\n    if (APPLE AND GGML_ACCELERATE)\n        find_library(ACCELERATE_FRAMEWORK Accelerate)\n        if (ACCELERATE_FRAMEWORK)\n            message(STATUS \"Accelerate framework found\")\n\n            target_compile_definitions(${GGML_CPU_NAME} PRIVATE GGML_USE_ACCELERATE)\n            target_compile_definitions(${GGML_CPU_NAME} PRIVATE ACCELERATE_NEW_LAPACK)\n            target_compile_definitions(${GGML_CPU_NAME} PRIVATE ACCELERATE_LAPACK_ILP64)\n\n            target_link_libraries(${GGML_CPU_NAME} PRIVATE ${ACCELERATE_FRAMEWORK})\n        else()\n            message(WARNING \"Accelerate framework not found\")\n        endif()\n    endif()\n\n    if (GGML_OPENMP)\n        find_package(OpenMP)\n        if (OpenMP_FOUND)\n            target_compile_definitions(${GGML_CPU_NAME} PRIVATE GGML_USE_OPENMP)\n\n            target_link_libraries(${GGML_CPU_NAME} PRIVATE OpenMP::OpenMP_C OpenMP::OpenMP_CXX)\n        else()\n            message(WARNING \"OpenMP not found\")\n        endif()\n    endif()\n\n    if (GGML_LLAMAFILE)\n        target_compile_definitions(${GGML_CPU_NAME} PRIVATE GGML_USE_LLAMAFILE)\n\n        list(APPEND GGML_CPU_SOURCES\n                    ggml-cpu/llamafile/sgemm.cpp\n                    ggml-cpu/llamafile/sgemm.h)\n    endif()\n\n    if (GGML_CPU_HBM)\n        find_library(memkind memkind REQUIRED)\n\n        message(STATUS \"Using memkind for CPU HBM\")\n\n        target_compile_definitions(${GGML_CPU_NAME} PRIVATE GGML_USE_CPU_HBM)\n\n        target_link_libraries(${GGML_CPU_NAME} PUBLIC memkind)\n    endif()\n\n    if (CMAKE_OSX_ARCHITECTURES      STREQUAL \"arm64\" OR\n        CMAKE_GENERATOR_PLATFORM_LWR STREQUAL \"arm64\" OR\n        (NOT CMAKE_OSX_ARCHITECTURES AND NOT CMAKE_GENERATOR_PLATFORM_LWR AND\n            CMAKE_SYSTEM_PROCESSOR MATCHES \"^(aarch64|arm.*|ARM64)$\"))\n\n        message(STATUS \"ARM detected\")\n\n        if (MSVC AND NOT CMAKE_C_COMPILER_ID STREQUAL \"Clang\")\n            message(FATAL_ERROR \"MSVC is not supported for ARM, use clang\")\n        else()\n            check_cxx_compiler_flag(-mfp16-format=ieee GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E)\n            if (NOT \"${GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E}\" STREQUAL \"\")\n                list(APPEND ARCH_FLAGS -mfp16-format=ieee)\n            endif()\n\n            if (GGML_NATIVE)\n                execute_process(\n                    COMMAND ${CMAKE_C_COMPILER} -mcpu=native -E -v -\n                    INPUT_FILE \"/dev/null\"\n                    OUTPUT_QUIET\n                    ERROR_VARIABLE ARM_MCPU\n                    RESULT_VARIABLE ARM_MCPU_RESULT\n                )\n                if (NOT ARM_MCPU_RESULT)\n                    string(REGEX MATCH \"-mcpu=[^ ']+\" ARM_MCPU_FLAG \"${ARM_MCPU}\")\n                endif()\n                if (\"${ARM_MCPU_FLAG}\" STREQUAL \"\")\n                    set(ARM_MCPU_FLAG -mcpu=native)\n                    message(STATUS \"ARM -mcpu not found, -mcpu=native will be used\")\n                endif()\n\n                include(CheckCXXSourceRuns)\n\n                function(check_arm_feature tag code)\n                    set(CMAKE_REQUIRED_FLAGS_SAVE ${CMAKE_REQUIRED_FLAGS})\n                    set(CMAKE_REQUIRED_FLAGS \"${ARM_MCPU_FLAG}+${tag}\")\n                    check_cxx_source_runs(\"${code}\" GGML_MACHINE_SUPPORTS_${tag})\n                    if (GGML_MACHINE_SUPPORTS_${tag})\n                        set(ARM_MCPU_FLAG_FIX \"${ARM_MCPU_FLAG_FIX}+${tag}\" PARENT_SCOPE)\n                    else()\n                        set(CMAKE_REQUIRED_FLAGS \"${ARM_MCPU_FLAG}+no${tag}\")\n                        check_cxx_source_compiles(\"int main() { return 0; }\" GGML_MACHINE_SUPPORTS_no${tag})\n                        if (GGML_MACHINE_SUPPORTS_no${tag})\n                            set(ARM_MCPU_FLAG_FIX \"${ARM_MCPU_FLAG_FIX}+no${tag}\" PARENT_SCOPE)\n                        endif()\n                    endif()\n                    set(CMAKE_REQUIRED_FLAGS ${CMAKE_REQUIRED_FLAGS_SAVE})\n                endfunction()\n\n                check_arm_feature(dotprod \"#include <arm_neon.h>\\nint main() { int8x16_t _a, _b; volatile int32x4_t _s = vdotq_s32(_s, _a, _b); return 0; }\")\n                check_arm_feature(i8mm    \"#include <arm_neon.h>\\nint main() { int8x16_t _a, _b; volatile int32x4_t _s = vmmlaq_s32(_s, _a, _b); return 0; }\")\n                check_arm_feature(sve     \"#include <arm_sve.h>\\nint main()  { svfloat32_t _a, _b; volatile svfloat32_t _c = svadd_f32_z(svptrue_b8(), _a, _b); return 0; }\")\n                check_arm_feature(sme     \"#include <arm_sme.h>\\n__arm_locally_streaming int main() { __asm__ volatile(\\\"smstart; smstop;\\\"); return 0; }\")\n\n                list(APPEND ARCH_FLAGS \"${ARM_MCPU_FLAG}${ARM_MCPU_FLAG_FIX}\")\n            else()\n                if (GGML_CPU_ARM_ARCH)\n                    list(APPEND ARCH_FLAGS -march=${GGML_CPU_ARM_ARCH})\n                endif()\n            endif()\n\n            if (CMAKE_HOST_SYSTEM_NAME STREQUAL \"Windows\")\n                set(FEAT_INPUT_FILE \"NUL\")\n            else()\n                set(FEAT_INPUT_FILE \"/dev/null\")\n            endif()\n\n            execute_process(\n                COMMAND ${CMAKE_C_COMPILER} ${ARCH_FLAGS} -dM -E -\n                INPUT_FILE ${FEAT_INPUT_FILE}\n                OUTPUT_VARIABLE ARM_FEATURE\n                RESULT_VARIABLE ARM_FEATURE_RESULT\n            )\n            if (ARM_FEATURE_RESULT)\n                message(WARNING \"Failed to get ARM features\")\n            else()\n                foreach(feature DOTPROD SVE MATMUL_INT8 FMA FP16_VECTOR_ARITHMETIC SME)\n                    string(FIND \"${ARM_FEATURE}\" \"__ARM_FEATURE_${feature} 1\" feature_pos)\n                    if (NOT ${feature_pos} EQUAL -1)\n                        message(STATUS \"ARM feature ${feature} enabled\")\n                    endif()\n                endforeach()\n            endif()\n        endif()\n    elseif (CMAKE_OSX_ARCHITECTURES STREQUAL \"x86_64\" OR CMAKE_GENERATOR_PLATFORM_LWR MATCHES \"^(x86_64|i686|amd64|x64|win32)$\" OR\n            (NOT CMAKE_OSX_ARCHITECTURES AND NOT CMAKE_GENERATOR_PLATFORM_LWR AND\n            CMAKE_SYSTEM_PROCESSOR MATCHES \"^(x86_64|i686|AMD64|amd64)$\"))\n\n        message(STATUS \"x86 detected\")\n\n        if (MSVC)\n            if (GGML_NATIVE)\n                include(ggml-cpu/cmake/FindSIMD.cmake)\n            endif ()\n            if (GGML_AVX512)\n                list(APPEND ARCH_FLAGS /arch:AVX512)\n                list(APPEND ARCH_DEFINITIONS GGML_AVX512)\n                if (GGML_AVX512_VBMI)\n                    list(APPEND ARCH_DEFINITIONS __AVX512VBMI__)\n                    if (CMAKE_C_COMPILER_ID STREQUAL \"Clang\")\n                        list(APPEND ARCH_FLAGS -mavx512vbmi)\n                    endif()\n                endif()\n                if (GGML_AVX512_VNNI)\n                    list(APPEND ARCH_DEFINITIONS __AVX512VNNI__ GGML_AVX512_VNNI)\n                    if (CMAKE_C_COMPILER_ID STREQUAL \"Clang\")\n                        list(APPEND ARCH_FLAGS -mavx512vnni)\n                    endif()\n                endif()\n                if (GGML_AVX512_BF16)\n                    list(APPEND ARCH_DEFINITIONS __AVX512BF16__ GGML_AVX512_BF16)\n                    if (CMAKE_C_COMPILER_ID STREQUAL \"Clang\")\n                        list(APPEND ARCH_FLAGS -mavx512bf16)\n                    endif()\n                endif()\n                if (GGML_AMX_TILE)\n                    list(APPEND ARCH_DEFINITIONS __AMX_TILE__ GGML_AMX_TILE)\n                endif()\n                if (GGML_AMX_INT8)\n                    list(APPEND ARCH_DEFINITIONS __AMX_INT8__ GGML_AMX_INT8)\n                endif()\n                if (GGML_AMX_BF16)\n                    list(APPEND ARCH_DEFINITIONS __AMX_BF16__ GGML_AMX_BF16)\n                endif()\n            elseif (GGML_AVX2)\n                list(APPEND ARCH_FLAGS /arch:AVX2)\n                list(APPEND ARCH_DEFINITIONS GGML_AVX2 GGML_FMA GGML_F16C)\n            elseif (GGML_AVX)\n                list(APPEND ARCH_FLAGS /arch:AVX)\n                list(APPEND ARCH_DEFINITIONS GGML_AVX)\n            else ()\n                list(APPEND ARCH_FLAGS /arch:SSE4.2)\n                list(APPEND ARCH_DEFINITIONS GGML_SSE42)\n            endif()\n            if (GGML_AVX_VNNI)\n                list(APPEND ARCH_DEFINITIONS __AVXVNNI__ GGML_AVX_VNNI)\n            endif()\n            if (GGML_BMI2)\n```\n\n----------------------------------------\n\nTITLE: Building Whisper.cpp XCFramework in Bash\nDESCRIPTION: Command to build the whisper.cpp XCFramework, which is required for the SwiftUI app. This should be run from the whisper.cpp project root.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/whisper.swiftui/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ ./build-xcframework.sh\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Whisper Models on V100 GPU with Flash Attention Enabled\nDESCRIPTION: Runs performance tests on various Whisper models (tiny through large-v3-turbo) on V100 GPU with CUDA, using 8 threads and with flash attention enabled. Shows encoding, decoding, batch processing, and prompt processing times.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/scripts/bench-all-gg.txt#2025-04-11_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nWHISPER_CUDA=1 make -j && ./scripts/bench-all.sh 8 1 1\n```\n\n----------------------------------------\n\nTITLE: Creating a Dummy Core ML Model Directory\nDESCRIPTION: Command to create a dummy model directory if you don't want to convert a Core ML model. This allows the application to run without requiring Core ML conversion.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/whisper.objc/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmkdir models/ggml-base.en-encoder.mlmodelc\n```\n\n----------------------------------------\n\nTITLE: Setting Up Python Environment for WER Calculation\nDESCRIPTION: Commands to install the required Python dependencies for computing Word Error Rate (WER) scores when evaluating speech recognition accuracy.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/tests/librispeech/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ pip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Running the Benchmark Test\nDESCRIPTION: Command to execute the benchmark evaluation of whisper.cpp against the LibriSpeech dataset.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/tests/librispeech/README.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ make\n```\n\n----------------------------------------\n\nTITLE: Configuring Whisper Stream Target with SDL2 in CMake\nDESCRIPTION: This CMake snippet conditionally builds the whisper-stream executable when SDL2 support is enabled. It sets up the target using stream.cpp as the source file, includes default target options, links necessary libraries including SDL components, and configures installation parameters.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/stream/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (WHISPER_SDL2)\n    set(TARGET whisper-stream)\n    add_executable(${TARGET} stream.cpp)\n\n    include(DefaultTargetOptions)\n\n    target_link_libraries(${TARGET} PRIVATE common common-sdl whisper ${CMAKE_THREAD_LIBS_INIT})\n\n    install(TARGETS ${TARGET} RUNTIME)\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Configuring Instruction Set Specific Options\nDESCRIPTION: Sets options for CPU-specific instruction set optimizations including AVX, AVX2, BMI2, AVX512, and various other architecture-specific features.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/CMakeLists.txt#2025-04-11_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\n# instruction set specific\nif (GGML_NATIVE OR NOT GGML_NATIVE_DEFAULT)\n    set(INS_ENB OFF)\nelse()\n    set(INS_ENB ON)\nendif()\n\nmessage(DEBUG \"GGML_NATIVE         : ${GGML_NATIVE}\")\nmessage(DEBUG \"GGML_NATIVE_DEFAULT : ${GGML_NATIVE_DEFAULT}\")\nmessage(DEBUG \"INS_ENB             : ${INS_ENB}\")\n\noption(GGML_CPU_HBM          \"ggml: use memkind for CPU HBM\" OFF)\noption(GGML_CPU_AARCH64      \"ggml: use runtime weight conversion of Q4_0 to Q4_X_X\" ON)\noption(GGML_CPU_KLEIDIAI     \"ggml: use KleidiAI optimized kernels if applicable\" OFF)\noption(GGML_AVX              \"ggml: enable AVX\"              ${INS_ENB})\noption(GGML_AVX_VNNI         \"ggml: enable AVX-VNNI\"         OFF)\noption(GGML_AVX2             \"ggml: enable AVX2\"             ${INS_ENB})\noption(GGML_BMI2             \"ggml: enable BMI2\"             ${INS_ENB})\noption(GGML_AVX512           \"ggml: enable AVX512F\"          OFF)\noption(GGML_AVX512_VBMI      \"ggml: enable AVX512-VBMI\"      OFF)\noption(GGML_AVX512_VNNI      \"ggml: enable AVX512-VNNI\"      OFF)\noption(GGML_AVX512_BF16      \"ggml: enable AVX512-BF16\"      OFF)\nif (NOT MSVC)\n    # in MSVC F16C and FMA is implied with AVX2/AVX512\n    option(GGML_FMA          \"ggml: enable FMA\"              ${INS_ENB})\n    option(GGML_F16C         \"ggml: enable F16C\"             ${INS_ENB})\n    # MSVC does not seem to support AMX\n    option(GGML_AMX_TILE     \"ggml: enable AMX-TILE\"         OFF)\n    option(GGML_AMX_INT8     \"ggml: enable AMX-INT8\"         OFF)\n    option(GGML_AMX_BF16     \"ggml: enable AMX-BF16\"         OFF)\nendif()\noption(GGML_LASX             \"ggml: enable lasx\"             ON)\noption(GGML_LSX              \"ggml: enable lsx\"              ON)\noption(GGML_RVV              \"ggml: enable rvv\"              ON)\noption(GGML_RV_ZFH           \"ggml: enable riscv zfh\"        OFF)\noption(GGML_VXE              \"ggml: enable vxe\"              ON)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Virtual Environment for Dependencies\nDESCRIPTION: Optional commands for setting up a Python virtual environment before installing the required dependencies.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/tests/librispeech/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n$ python3 -m venv venv\n$ . venv/bin/activate\n$ pip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Copying WebAssembly Files for Deployment\nDESCRIPTION: These commands copy the necessary WebAssembly and JavaScript files to a specified HTTP path for deployment on a different server. This step is required when not using the local Python server.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/bench.wasm/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# copy the produced page to your HTTP path\ncp bin/bench.wasm/*       /path/to/html/\ncp bin/libbench.worker.js /path/to/html/\n```\n\n----------------------------------------\n\nTITLE: Building Examples for Whisper.cpp Go Bindings\nDESCRIPTION: Command to build example applications that demonstrate the Go bindings functionality.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/bindings/go/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmake examples\n```\n\n----------------------------------------\n\nTITLE: Downloading Model Files for Whisper.cpp\nDESCRIPTION: Command to download all available models using the go-model-download utility, placing them in a models directory.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/bindings/go/README.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n./build/go-model-download -out models\n```\n\n----------------------------------------\n\nTITLE: Building and Testing Whisper.cpp Node.js Package\nDESCRIPTION: Commands for setting up, building, and testing the Whisper.cpp Node.js integration. Includes steps for loading emscripten, cloning the repo, downloading the model, preparing audio samples, and building/testing the package.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/bindings/javascript/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# load emscripten\nsource /path/to/emsdk/emsdk_env.sh\n\n# clone repo\ngit clone https://github.com/ggerganov/whisper.cpp\ncd whisper.cpp\n\n# grab base.en model\n./models/download-ggml-model.sh base.en\n\n# prepare PCM sample for testing\nffmpeg -i samples/jfk.wav -f f32le -acodec pcm_f32le samples/jfk.pcmf32\n\n# build\nmkdir build-em && cd build-em\nemcmake cmake .. && make -j\n\n# run test\nnode ../tests/test-whisper.js\n\n# For Node.js versions prior to v16.4.0, experimental features need to be enabled:\nnode --experimental-wasm-threads --experimental-wasm-simd ../tests/test-whisper.js\n\n# publish npm package\nmake publish-npm\n```\n\n----------------------------------------\n\nTITLE: Installation Configuration for whisper.cpp\nDESCRIPTION: Sets up the installation process for the whisper library, including package configuration files, version information, and pkg-config. Configures paths for header, library, and binary files.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/CMakeLists.txt#2025-04-11_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\n#\n# install\n#\n\ninclude(GNUInstallDirs)\ninclude(CMakePackageConfigHelpers)\n\nset(WHISPER_BUILD_NUMBER        ${BUILD_NUMBER})\nset(WHISPER_BUILD_COMMIT        ${BUILD_COMMIT})\nset(WHISPER_INSTALL_VERSION     ${CMAKE_PROJECT_VERSION})\n\nset(WHISPER_INCLUDE_INSTALL_DIR ${CMAKE_INSTALL_INCLUDEDIR} CACHE PATH \"Location of header  files\")\nset(WHISPER_LIB_INSTALL_DIR     ${CMAKE_INSTALL_LIBDIR}     CACHE PATH \"Location of library files\")\nset(WHISPER_BIN_INSTALL_DIR     ${CMAKE_INSTALL_BINDIR}     CACHE PATH \"Location of binary  files\")\n\nget_directory_property(WHISPER_TRANSIENT_DEFINES COMPILE_DEFINITIONS)\n\nset_target_properties(whisper PROPERTIES PUBLIC_HEADER ${CMAKE_CURRENT_SOURCE_DIR}/include/whisper.h)\ninstall(TARGETS whisper LIBRARY PUBLIC_HEADER)\n\nconfigure_package_config_file(\n        ${CMAKE_CURRENT_SOURCE_DIR}/cmake/whisper-config.cmake.in\n        ${CMAKE_CURRENT_BINARY_DIR}/whisper-config.cmake\n    INSTALL_DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/whisper\n    PATH_VARS\n    WHISPER_INCLUDE_INSTALL_DIR\n    WHISPER_LIB_INSTALL_DIR\n    WHISPER_BIN_INSTALL_DIR )\n\nwrite_basic_package_version_file(\n    ${CMAKE_CURRENT_BINARY_DIR}/whisper-version.cmake\n    VERSION ${WHISPER_INSTALL_VERSION}\n    COMPATIBILITY SameMajorVersion)\n\ninstall(FILES ${CMAKE_CURRENT_BINARY_DIR}/whisper-config.cmake\n              ${CMAKE_CURRENT_BINARY_DIR}/whisper-version.cmake\n        DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/whisper)\n\nconfigure_file(cmake/whisper.pc.in\n        \"${CMAKE_CURRENT_BINARY_DIR}/whisper.pc\"\n        @ONLY)\n\ninstall(FILES \"${CMAKE_CURRENT_BINARY_DIR}/whisper.pc\"\n        DESTINATION lib/pkgconfig)\n```\n\n----------------------------------------\n\nTITLE: Configuring Whisper.cpp Directory Path in Vim\nDESCRIPTION: Vim configuration snippet for setting the Whisper.cpp directory path using path expansion.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/lsp/README.md#2025-04-11_snippet_3\n\nLANGUAGE: vim\nCODE:\n```\nlet g:whisper_dir = expand(\"~/whisper.cpp/\")\n```\n\n----------------------------------------\n\nTITLE: Adding User to GPU Groups in Linux\nDESCRIPTION: Commands to add current user to render and video groups for GPU access permissions\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README_sycl.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsudo usermod -aG render username\nsudo usermod -aG video username\n```\n\n----------------------------------------\n\nTITLE: Building whisper.coreml Library\nDESCRIPTION: Defines and configures the whisper.coreml library when CoreML integration is enabled. Sets up source files, include directories, framework dependencies, and Objective-C configuration options.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/src/CMakeLists.txt#2025-04-11_snippet_4\n\nLANGUAGE: cmake\nCODE:\n```\nif (WHISPER_COREML)\n    set(TARGET whisper.coreml)\n\n    add_library(${TARGET}\n        coreml/whisper-encoder.h\n        coreml/whisper-encoder.mm\n        coreml/whisper-encoder-impl.h\n        coreml/whisper-encoder-impl.m\n        )\n\n    include(DefaultTargetOptions)\n\n    target_include_directories(${TARGET} PUBLIC\n        .\n        )\n\n    target_link_libraries(${TARGET} PRIVATE ${FOUNDATION_FRAMEWORK} ${COREML_FRAMEWORK})\n\n    set_target_properties(${TARGET} PROPERTIES\n        COMPILE_FLAGS \"-fobjc-arc\"\n        XCODE_ATTRIBUTE_CLANG_ENABLE_OBJC_ARC YES\n        )\n    set_target_properties(${TARGET} PROPERTIES FOLDER \"libs\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting up OpenVINO Environment (Windows)\nDESCRIPTION: Runs the OpenVINO setup script to configure the environment on Windows.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_14\n\nLANGUAGE: powershell\nCODE:\n```\nC:\\Path\\To\\w_openvino_toolkit_windows_2023.0.0.10926.b4452d56304_x86_64\\setupvars.bat\n```\n\n----------------------------------------\n\nTITLE: Creating and Configuring the ggml-hip Backend Library\nDESCRIPTION: Adds the ggml-hip backend library and configures compilation definitions based on various feature flags. Sets up language handling for source files based on whether hipcc is being used.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-hip/CMakeLists.txt#2025-04-11_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nggml_add_backend_library(ggml-hip\n                         ${GGML_HEADERS_ROCM}\n                         ${GGML_SOURCES_ROCM}\n                        )\n\n# TODO: do not use CUDA definitions for HIP\nif (NOT GGML_BACKEND_DL)\n    target_compile_definitions(ggml PUBLIC GGML_USE_CUDA)\nendif()\n\nadd_compile_definitions(GGML_USE_HIP)\n\nif (GGML_HIP_UMA)\n    add_compile_definitions(GGML_HIP_UMA)\nendif()\n\nif (GGML_CUDA_FORCE_MMQ)\n    add_compile_definitions(GGML_CUDA_FORCE_MMQ)\nendif()\n\nif (GGML_CUDA_FORCE_CUBLAS)\n    add_compile_definitions(GGML_CUDA_FORCE_CUBLAS)\nendif()\n\nif (GGML_CUDA_NO_PEER_COPY)\n    add_compile_definitions(GGML_CUDA_NO_PEER_COPY)\nendif()\n\nif (GGML_HIP_GRAPHS)\n    add_compile_definitions(GGML_HIP_GRAPHS)\nendif()\n\nif (GGML_HIP_NO_VMM)\n    add_compile_definitions(GGML_HIP_NO_VMM)\nendif()\n\nif (GGML_HIP_ROCWMMA_FATTN)\n    add_compile_definitions(GGML_HIP_ROCWMMA_FATTN)\nendif()\n\nif (NOT GGML_CUDA_FA)\n    add_compile_definitions(GGML_CUDA_NO_FA)\nendif()\n\nif (CXX_IS_HIPCC)\n    set_source_files_properties(${GGML_SOURCES_ROCM} PROPERTIES LANGUAGE CXX)\n    target_link_libraries(ggml-hip PRIVATE hip::device)\nelse()\n    set_source_files_properties(${GGML_SOURCES_ROCM} PROPERTIES LANGUAGE HIP)\nendif()\n\nif (GGML_STATIC)\n    message(FATAL_ERROR \"Static linking not supported for HIP/ROCm\")\nendif()\n\ntarget_link_libraries(ggml-hip PRIVATE ggml-base hip::host roc::rocblas roc::hipblas)\n```\n\n----------------------------------------\n\nTITLE: Setting up Python Environment for OpenVINO (Windows)\nDESCRIPTION: Creates a Python virtual environment and installs required dependencies for OpenVINO conversion on Windows.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README.md#2025-04-11_snippet_10\n\nLANGUAGE: powershell\nCODE:\n```\ncd models\npython -m venv openvino_conv_env\nopenvino_conv_env\\Scripts\\activate\npython -m pip install --upgrade pip\npip install -r requirements-openvino.txt\n```\n\n----------------------------------------\n\nTITLE: Accessing Whisper Log in Vim\nDESCRIPTION: Vim command to open the Whisper log buffer for viewing plugin activity and debugging information.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/lsp/README.md#2025-04-11_snippet_2\n\nLANGUAGE: vim\nCODE:\n```\n:e whisper_log\n```\n\n----------------------------------------\n\nTITLE: Displaying Binary Filename Changes Table in Markdown\nDESCRIPTION: A markdown table showing the mapping between old and new binary filenames for the whisper.cpp project. This table helps users update their scripts and workflows to use the new binary names.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/deprecation-warning/README.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Old Filename | New Filename |\n| ---- | ---- |\n| main | whisper-cli |\n| bench | whisper-bench |\n| stream | whisper-stream |\n| command | whisper-command |\n| server | whisper-server |\n| talk-llama | whisper-talk-llama |\n```\n\n----------------------------------------\n\nTITLE: Configuring CoreML Integration\nDESCRIPTION: Sets up CoreML framework integration when WHISPER_COREML is enabled. Finds necessary frameworks, configures compiler flags, and adds optional fallback support if requested.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/src/CMakeLists.txt#2025-04-11_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif (WHISPER_COREML)\n    find_library(FOUNDATION_FRAMEWORK Foundation)\n    find_library(COREML_FRAMEWORK CoreML)\n\n    if (COREML_FRAMEWORK)\n        message(STATUS \"CoreML framework found\")\n\n        set(WHISPER_EXTRA_FLAGS ${WHISPER_EXTRA_FLAGS} -DWHISPER_USE_COREML)\n    else()\n        message(FATAL_ERROR \"CoreML framework not found\")\n    endif()\n\n    if (WHISPER_COREML_ALLOW_FALLBACK)\n        set(WHISPER_EXTRA_FLAGS ${WHISPER_EXTRA_FLAGS} -DWHISPER_COREML_ALLOW_FALLBACK)\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting up oneAPI Environment for SYCL in llama.cpp\nDESCRIPTION: Commands to source the Intel oneAPI environment variables required for SYCL execution.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/sycl/README.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsource /opt/intel/oneapi/setvars.sh\n```\n\n----------------------------------------\n\nTITLE: Configuring GGML BLAS Backend with Located Dependencies\nDESCRIPTION: Sets up the ggml-blas target with the detected BLAS libraries and include directories. Also detects and configures Intel MKL if present in the include paths.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-blas/CMakeLists.txt#2025-04-11_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\n    message(STATUS \"BLAS found, Includes: ${BLAS_INCLUDE_DIRS}\")\n\n    target_compile_options(ggml-blas PRIVATE ${BLAS_LINKER_FLAGS})\n\n    if (${BLAS_INCLUDE_DIRS} MATCHES \"mkl\" AND (${GGML_BLAS_VENDOR} MATCHES \"Generic\" OR ${GGML_BLAS_VENDOR} MATCHES \"Intel\"))\n        add_compile_definitions(GGML_BLAS_USE_MKL)\n    endif()\n\n    target_link_libraries     (ggml-blas PRIVATE ${BLAS_LIBRARIES})\n    target_include_directories(ggml-blas PRIVATE ${BLAS_INCLUDE_DIRS})\n```\n\n----------------------------------------\n\nTITLE: Setting Platform-Specific Default Options\nDESCRIPTION: Configures platform-specific default options for Apple devices (enabling Metal and BLAS) versus other platforms. Also handles cross-compilation and reproducible build settings.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/CMakeLists.txt#2025-04-11_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif (APPLE)\n    set(GGML_METAL_DEFAULT ON)\n    set(GGML_BLAS_DEFAULT ON)\n    set(GGML_BLAS_VENDOR_DEFAULT \"Apple\")\nelse()\n    set(GGML_METAL_DEFAULT OFF)\n    set(GGML_BLAS_DEFAULT OFF)\n    set(GGML_BLAS_VENDOR_DEFAULT \"Generic\")\nendif()\n\nif (CMAKE_CROSSCOMPILING OR DEFINED ENV{SOURCE_DATE_EPOCH})\n    message(STATUS \"Setting GGML_NATIVE_DEFAULT to OFF\")\n    set(GGML_NATIVE_DEFAULT OFF)\nelse()\n    set(GGML_NATIVE_DEFAULT ON)\nendif()\n\n# defaults\nif (NOT GGML_LLAMAFILE_DEFAULT)\n    set(GGML_LLAMAFILE_DEFAULT OFF)\nendif()\n\nif (NOT GGML_CUDA_GRAPHS_DEFAULT)\n    set(GGML_CUDA_GRAPHS_DEFAULT OFF)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running memcpy Benchmark on M1 Pro\nDESCRIPTION: This snippet shows the command to run the memcpy benchmark and its results, demonstrating memory bandwidth scaling with multiple threads.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/scripts/bench-all-gg.txt#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nmake -j && ./scripts/bench-all.sh 8\n```\n\n----------------------------------------\n\nTITLE: Defining GGML Backend Library Addition Function in CMake\nDESCRIPTION: Creates a function 'ggml_add_backend_library' to add backend libraries to the GGML project. It handles both dynamic and static linking scenarios and sets up necessary compiler definitions and dependencies.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-11_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(ggml_add_backend_library backend)\n    if (GGML_BACKEND_DL)\n        add_library(${backend} MODULE ${ARGN})\n        # write the shared library to the output directory\n        set_target_properties(${backend} PROPERTIES LIBRARY_OUTPUT_DIRECTORY ${CMAKE_RUNTIME_OUTPUT_DIRECTORY})\n        target_compile_definitions(${backend} PRIVATE GGML_BACKEND_DL)\n        add_dependencies(ggml ${backend})\n    else()\n        add_library(${backend} ${ARGN})\n        target_link_libraries(ggml PUBLIC ${backend})\n        install(TARGETS ${backend} LIBRARY)\n    endif()\n\n    target_link_libraries(${backend} PRIVATE ggml-base)\n    target_include_directories(${backend} PRIVATE ..)\n\n    if (${BUILD_SHARED_LIBS})\n        target_compile_definitions(${backend} PRIVATE GGML_BACKEND_BUILD)\n        target_compile_definitions(${backend} PUBLIC  GGML_BACKEND_SHARED)\n    endif()\n\n    if(NOT GGML_AVAILABLE_BACKENDS)\n        set(GGML_AVAILABLE_BACKENDS \"${backend}\"\n            CACHE INTERNAL \"List of backends for cmake package\")\n    else()\n        list(FIND GGML_AVAILABLE_BACKENDS \"${backend}\" has_backend)\n        if(has_backend EQUAL -1)\n            set(GGML_AVAILABLE_BACKENDS \"${GGML_AVAILABLE_BACKENDS};${backend}\"\n                CACHE INTERNAL \"List of backends for cmake package\")\n        endif()\n    endif()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Processing Transcription Segments in Ruby Whisper\nDESCRIPTION: Shows how to iterate through transcription segments after calling transcribe, including formatting timestamps and handling segment metadata. Demonstrates how to extract transcript text, time markers, and speaker turn indicators.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/bindings/ruby/README.md#2025-04-11_snippet_3\n\nLANGUAGE: ruby\nCODE:\n```\ndef format_time(time_ms)\n  sec, decimal_part = time_ms.divmod(1000)\n  min, sec = sec.divmod(60)\n  hour, min = min.divmod(60)\n  \"%02d:%02d:%02d.%03d\" % [hour, min, sec, decimal_part]\nend\n\nwhisper\n  .transcribe(\"path/to/audio.wav\", params)\n  .each_segment.with_index do |segment, index|\n    line = \"[%{nth}: %{st} --> %{ed}] %{text}\" % {\n      nth: index + 1,\n      st: format_time(segment.start_time),\n      ed: format_time(segment.end_time),\n      text: segment.text\n    }\n    line << \" (speaker turned)\" if segment.speaker_next_turn?\n    puts line\n  end\n```\n\n----------------------------------------\n\nTITLE: AI Assistant Dialog Template\nDESCRIPTION: A structured dialog template where {0} represents the user, {1} represents the AI assistant name, {2} represents time, {3} represents year, and {4} represents dialog separator. The template includes basic conversation examples with short, concise responses.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/talk-llama/prompts/talk-alpaca.txt#2025-04-11_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n{0}{4} Hello, {1}!\n{1}{4} Hello {0}! How may I help you today?\n{0}{4} What time is it?\n{1}{4} It is {2} o'clock.\n{0}{4} What year is it?\n{1}{4} We are in {3}.\n{0}{4} What is a cat?\n{1}{4} A cat is a domestic species of small carnivorous mammal. It is the only domesticated species in the family Felidae.\n{0}{4} Name a color.\n{1}{4} Blue\n{0}{4}\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging for Whisper Ruby\nDESCRIPTION: Demonstrates how to set up custom logging callbacks for the Whisper library. Shows how to handle different log levels (none, info, warn, error, debug) and customize log output format.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/bindings/ruby/README.md#2025-04-11_snippet_6\n\nLANGUAGE: ruby\nCODE:\n```\nprefix = \"[MyApp] \"\nlog_callback = ->(level, buffer, user_data) {\n  case level\n  when Whisper::LOG_LEVEL_NONE\n    puts \"#{user_data}none: #{buffer}\"\n  when Whisper::LOG_LEVEL_INFO\n    puts \"#{user_data}info: #{buffer}\"\n  when Whisper::LOG_LEVEL_WARN\n    puts \"#{user_data}warn: #{buffer}\"\n  when Whisper::LOG_LEVEL_ERROR\n    puts \"#{user_data}error: #{buffer}\"\n  when Whisper::LOG_LEVEL_DEBUG\n    puts \"#{user_data}debug: #{buffer}\"\n  when Whisper::LOG_LEVEL_CONT\n    puts \"#{user_data}same to previous: #{buffer}\"\n  end\n}\nWhisper.log_set log_callback, prefix\n```\n\nLANGUAGE: ruby\nCODE:\n```\nWhisper.log_set ->(level, buffer, user_data) {\n  # do nothing\n}, nil\nWhisper::Context.new(\"base\")\n```\n\n----------------------------------------\n\nTITLE: Setting Emscripten Link Flags for WebAssembly Build\nDESCRIPTION: Configures the Emscripten linker flags for the whisper.wasm build. These settings enable threading, configure memory limits, enable filesystem access, and export specific runtime methods for JavaScript interoperability.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/whisper.wasm/CMakeLists.txt#2025-04-11_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET} PROPERTIES LINK_FLAGS \" \\\n    --bind \\\n    -s USE_PTHREADS=1 \\\n    -s PTHREAD_POOL_SIZE_STRICT=0 \\\n    -s INITIAL_MEMORY=512MB \\\n    -s MAXIMUM_MEMORY=2000MB \\\n    -s ALLOW_MEMORY_GROWTH=1 \\\n    -s FORCE_FILESYSTEM=1 \\\n    -s EXPORTED_RUNTIME_METHODS=\\\"['print', 'printErr', 'ccall', 'cwrap']\\\" \\\n    ${EXTRA_FLAGS} \\\n    \")\n```\n\n----------------------------------------\n\nTITLE: Configuring GGML Library Targets in CMake\nDESCRIPTION: Sets up include directories, compiler features, and linking options for the GGML library targets. It handles platform-specific settings and shared library configurations.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-11_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nforeach (target ggml-base ggml)\n    target_include_directories(${target} PUBLIC    $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/../include> $<INSTALL_INTERFACE:include>)\n    target_compile_features   (${target} PRIVATE c_std_11 cxx_std_17) # don't bump\nendforeach()\n\ntarget_link_libraries(ggml-base PRIVATE Threads::Threads)\n\nfind_library(MATH_LIBRARY m)\nif (MATH_LIBRARY)\n    if (NOT WIN32 OR NOT DEFINED ENV{ONEAPI_ROOT})\n        target_link_libraries(ggml-base PRIVATE m)\n    endif()\nendif()\n\nif (CMAKE_SYSTEM_NAME MATCHES \"Android\")\n    target_link_libraries(ggml-base PRIVATE dl)\nendif()\n\nif(CMAKE_SYSTEM_NAME MATCHES \"visionOS\")\n    target_compile_definitions(ggml-base PUBLIC _DARWIN_C_SOURCE)\nendif()\n\nif (BUILD_SHARED_LIBS)\n    foreach (target ggml-base ggml)\n        set_target_properties(${target} PROPERTIES POSITION_INDEPENDENT_CODE ON)\n        target_compile_definitions(${target} PRIVATE GGML_BUILD)\n        target_compile_definitions(${target} PUBLIC  GGML_SHARED)\n    endforeach()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining GGML Backend Addition Function in CMake\nDESCRIPTION: Creates a function 'ggml_add_backend' to add specific backends to the GGML project. It handles subdirectory inclusion and compiler definitions based on the backend type.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-11_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(ggml_add_backend backend)\n    string(TOUPPER \"GGML_${backend}\" backend_id)\n    if (${backend_id})\n        string(TOLOWER \"ggml-${backend}\" backend_target)\n        add_subdirectory(${backend_target})\n        message(STATUS \"Including ${backend} backend\")\n        if (NOT GGML_BACKEND_DL)\n            string(TOUPPER \"GGML_USE_${backend}\" backend_use)\n            target_compile_definitions(ggml PUBLIC ${backend_use})\n        endif()\n    endif()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Configuring FFmpeg Dependency in CMake\nDESCRIPTION: This block handles finding and configuring FFmpeg libraries when WHISPER_FFMPEG is enabled. It checks for required components, displays information about found libraries, and adds the necessary include directories and compile definitions.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/CMakeLists.txt#2025-04-11_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif (WHISPER_FFMPEG)\n    # As of cmake 3.27, there is no official cmake support for FindFFmpeg.\n    # Consequnelty we added a FindFFmpeg.cmake script the cmake subfolder:\n    # whisper.cpp does not need the full ffmpeg libs, just AVFORMAT AVCODEC AVUTIL SWRESAMPLE\n    # libswresample  performs highly optimized audio resampling, rematrixing and sample format conversion operations\n    # libavcodec provides a generic encoding/decoding framework and contains multiple decoders and encoders for audio, video and subtitle streams, and several bitstream filters.\n    # libavformat provides a generic framework for multiplexing and demultiplexing (muxing and demuxing) audio, video and subtitle streams.\n    find_package(FFmpeg REQUIRED)\n\n    if (NOT ${FFMPEG_FOUND})\n        message(FATAL_ERROR \"Cannot find ffmpeg libs/headers\")\n    endif()\n\n    message(STATUS \"Found ffmpeg libs:       ${FFMPEG_LIBRARIES}\")\n    message(STATUS \"Found ffmpeg headers in: ${FFMPEG_INCLUDE_DIRS}\")\n    message(STATUS \"ffmpeg definitions:      ${FFMPEG_DEFINITIONS}\")\n    message(STATUS \"Found avformat           ${AVFORMAT_VERSION}\")\n\n    include_directories(${FFMPEG_INCLUDE_DIRS})\n    add_compile_definitions(WHISPER_FFMPEG)\n\n    list(APPEND COMMON_EXTRA_LIBS ${FFMPEG_LIBRARIES})\n\n    set(COMMON_SOURCES_FFMPEG ffmpeg-transcode.cpp)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Deploying to a Custom HTTP Server\nDESCRIPTION: Instructions for copying the necessary WebAssembly and JavaScript worker files to a custom HTTP server path. This allows the stream.wasm application to be hosted on any web server.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/stream.wasm/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# copy the produced page to your HTTP path\ncp bin/stream.wasm/*       /path/to/html/\ncp bin/libstream.worker.js /path/to/html/\n```\n\n----------------------------------------\n\nTITLE: Running Whisper Command in Guided Mode\nDESCRIPTION: Commands for running the voice assistant in guided mode with predefined command lists. Includes performance optimization settings for Raspberry Pi.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/command/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Run in guided mode, the list of allowed commands is in commands.txt\n./whisper-command -m ./models/ggml-base.en.bin -cmd ./examples/command/commands.txt\n\n# On Raspberry Pi, in guided mode you can use \"-ac 128\" for extra performance\n./whisper-command -m ./models/ggml-tiny.en.bin -cmd ./examples/command/commands.txt -ac 128 -t 3 -c 0\n```\n\n----------------------------------------\n\nTITLE: Example SYCL Device Listing Output in llama.cpp\nDESCRIPTION: Sample output from the ls-sycl-device tool showing detected devices with their compute capabilities, work group sizes, and memory details.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/sycl/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nfound 4 SYCL devices:\n  Device 0: Intel(R) Arc(TM) A770 Graphics,\tcompute capability 1.3,\n    max compute_units 512,\tmax work group size 1024,\tmax sub group size 32,\tglobal mem size 16225243136\n  Device 1: Intel(R) FPGA Emulation Device,\tcompute capability 1.2,\n    max compute_units 24,\tmax work group size 67108864,\tmax sub group size 64,\tglobal mem size 67065057280\n  Device 2: 13th Gen Intel(R) Core(TM) i7-13700K,\tcompute capability 3.0,\n    max compute_units 24,\tmax work group size 8192,\tmax sub group size 64,\tglobal mem size 67065057280\n  Device 3: Intel(R) Arc(TM) A770 Graphics,\tcompute capability 3.0,\n    max compute_units 512,\tmax work group size 1024,\tmax sub group size 32,\tglobal mem size 16225243136\n```\n\n----------------------------------------\n\nTITLE: Defining CPU Backend Variant Addition Function in CMake\nDESCRIPTION: Creates a function 'ggml_add_cpu_backend_variant' to add CPU-specific backend variants with different instruction set optimizations. It sets up compiler flags for various CPU features.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-11_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(ggml_add_cpu_backend_variant tag_name)\n    set(GGML_CPU_TAG_NAME ${tag_name})\n    # other: OPENMP LLAMAFILE CPU_HBM\n    foreach (feat NATIVE\n                  AVX AVX2 BMI2 AVX_VNNI FMA F16C\n                  AVX512 AVX512_VBMI AVX512_VNNI AVX512_BF16\n                  AMX_TILE AMX_INT8 AMX_BF16)\n        set(GGML_${feat} OFF)\n    endforeach()\n\n    foreach (feat ${ARGN})\n        set(GGML_${feat} ON)\n    endforeach()\n\n    ggml_add_cpu_backend_variant_impl(${tag_name})\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Configuring MUSA Architectures and Source Files in CMake\nDESCRIPTION: Sets up MUSA architectures, gathers MUSA-specific header and source files, and configures compilation flags for MUSA sources. It also handles conditional compilation based on GGML_CUDA_FA_ALL_QUANTS.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-musa/CMakeLists.txt#2025-04-11_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif (MUSAToolkit_FOUND)\n    message(STATUS \"MUSA Toolkit found\")\n\n    if (NOT DEFINED MUSA_ARCHITECTURES)\n        set(MUSA_ARCHITECTURES \"21;22;31\")\n    endif()\n    message(STATUS \"Using MUSA architectures: ${MUSA_ARCHITECTURES}\")\n\n    file(GLOB   GGML_HEADERS_MUSA \"../ggml-cuda/*.cuh\")\n    list(APPEND GGML_HEADERS_MUSA \"../../include/ggml-cuda.h\")\n\n    file(GLOB   GGML_SOURCES_MUSA \"../ggml-cuda/*.cu\")\n    file(GLOB   SRCS \"../ggml-cuda/template-instances/fattn-mma*.cu\")\n    list(APPEND GGML_SOURCES_MUSA ${SRCS})\n    file(GLOB   SRCS \"../ggml-cuda/template-instances/mmq*.cu\")\n    list(APPEND GGML_SOURCES_MUSA ${SRCS})\n\n    if (GGML_CUDA_FA_ALL_QUANTS)\n        file(GLOB   SRCS \"../ggml-cuda/template-instances/fattn-vec*.cu\")\n        list(APPEND GGML_SOURCES_MUSA ${SRCS})\n        add_compile_definitions(GGML_CUDA_FA_ALL_QUANTS)\n    else()\n        file(GLOB   SRCS \"../ggml-cuda/template-instances/fattn-vec*q4_0-q4_0.cu\")\n        list(APPEND GGML_SOURCES_MUSA ${SRCS})\n        file(GLOB   SRCS \"../ggml-cuda/template-instances/fattn-vec*q8_0-q8_0.cu\")\n        list(APPEND GGML_SOURCES_MUSA ${SRCS})\n        file(GLOB   SRCS \"../ggml-cuda/template-instances/fattn-vec*f16-f16.cu\")\n        list(APPEND GGML_SOURCES_MUSA ${SRCS})\n    endif()\n\n    set_source_files_properties(${GGML_SOURCES_MUSA} PROPERTIES LANGUAGE CXX)\n    foreach(SOURCE ${GGML_SOURCES_MUSA})\n        set(COMPILE_FLAGS \"-fsigned-char -x musa -mtgpu\")\n        foreach(ARCH ${MUSA_ARCHITECTURES})\n            set(COMPILE_FLAGS \"${COMPILE_FLAGS} --cuda-gpu-arch=mp_${ARCH}\")\n        endforeach()\n        set_property(SOURCE ${SOURCE} PROPERTY COMPILE_FLAGS ${COMPILE_FLAGS})\n    endforeach()\n```\n\n----------------------------------------\n\nTITLE: Defining Common Library Target in CMake\nDESCRIPTION: This snippet defines the 'common' library target with its source files. It includes conditionally added FFmpeg source files and sets target properties and dependencies.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/CMakeLists.txt#2025-04-11_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(${TARGET} STATIC\n    common.h\n    common.cpp\n    common-ggml.h\n    common-ggml.cpp\n    common-whisper.h\n    common-whisper.cpp\n    grammar-parser.h\n    grammar-parser.cpp\n    ${COMMON_SOURCES_FFMPEG}\n    )\n\ninclude(DefaultTargetOptions)\n\ntarget_link_libraries(${TARGET} PRIVATE whisper ${COMMON_EXTRA_LIBS} ${CMAKE_DL_LIBS})\n\nset_target_properties(${TARGET} PROPERTIES POSITION_INDEPENDENT_CODE ON)\nset_target_properties(${TARGET} PROPERTIES FOLDER \"libs\")\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Whisper Models on M4 Max with Flash Attention Disabled\nDESCRIPTION: Runs performance tests on various Whisper models (tiny through large-v2) on M4 Max with METAL, using 1 thread and with flash attention disabled. Shows encoding, decoding, batch processing, and prompt processing times.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/scripts/bench-all-gg.txt#2025-04-11_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nmake -j && ./scripts/bench-all.sh 1 1 0\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenVINO Integration\nDESCRIPTION: Finds the OpenVINO package when WHISPER_OPENVINO is enabled. This integrates the OpenVINO runtime for potential acceleration of whisper models.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/src/CMakeLists.txt#2025-04-11_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif (WHISPER_OPENVINO)\n    find_package(OpenVINO REQUIRED COMPONENTS Runtime)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Adding Various GGML Backends in CMake\nDESCRIPTION: Adds multiple backend options to the GGML project, including BLAS, CUDA, HIP, Metal, and others. Each backend is conditionally added based on project configuration.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-11_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nggml_add_backend(BLAS)\nggml_add_backend(CANN)\nggml_add_backend(CUDA)\nggml_add_backend(HIP)\nggml_add_backend(Kompute)\nggml_add_backend(METAL)\nggml_add_backend(MUSA)\nggml_add_backend(RPC)\nggml_add_backend(SYCL)\nggml_add_backend(Vulkan)\nggml_add_backend(OpenCL)\n```\n\n----------------------------------------\n\nTITLE: Configuring Whisper CLI Build in CMake\nDESCRIPTION: Defines and configures the whisper-cli executable target with necessary dependencies including common libraries, whisper core, FFmpeg, and threading support. Sets up build options and installation rules for the CLI application.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/cli/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET whisper-cli)\nadd_executable(${TARGET} cli.cpp)\n\ninclude(DefaultTargetOptions)\n\ntarget_link_libraries(${TARGET} PRIVATE common whisper ${FFMPEG_LIBRARIES} ${CMAKE_THREAD_LIBS_INIT})\n\ninstall(TARGETS ${TARGET} RUNTIME)\n```\n\n----------------------------------------\n\nTITLE: Adding MUSA Backend Library and Setting Compile Definitions in CMake\nDESCRIPTION: Adds the MUSA backend library, sets various compile definitions for CUDA-like features, and links necessary MUSA libraries. It also handles different compilation options based on project settings.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-musa/CMakeLists.txt#2025-04-11_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nggml_add_backend_library(ggml-musa\n                         ${GGML_HEADERS_MUSA}\n                         ${GGML_SOURCES_MUSA}\n                        )\n\n# TODO: do not use CUDA definitions for MUSA\ntarget_compile_definitions(ggml PUBLIC GGML_USE_CUDA)\n\nadd_compile_definitions(GGML_USE_MUSA)\nadd_compile_definitions(GGML_CUDA_PEER_MAX_BATCH_SIZE=${GGML_CUDA_PEER_MAX_BATCH_SIZE})\n\nif (GGML_CUDA_FORCE_MMQ)\n    add_compile_definitions(GGML_CUDA_FORCE_MMQ)\nendif()\n\nif (GGML_CUDA_FORCE_CUBLAS)\n    add_compile_definitions(GGML_CUDA_FORCE_CUBLAS)\nendif()\n\nif (GGML_CUDA_NO_VMM)\n    add_compile_definitions(GGML_CUDA_NO_VMM)\nendif()\n\nif (NOT GGML_CUDA_FA)\n    add_compile_definitions(GGML_CUDA_NO_FA)\nendif()\n\nif (GGML_CUDA_F16 OR GGML_CUDA_DMMV_F16)\n    add_compile_definitions(GGML_CUDA_F16)\nendif()\n\nif (GGML_CUDA_NO_PEER_COPY)\n    add_compile_definitions(GGML_CUDA_NO_PEER_COPY)\nendif()\n\nif (GGML_STATIC)\n    target_link_libraries(ggml-musa PRIVATE MUSA::musart_static MUSA::mublas_static)\nelse()\n    target_link_libraries(ggml-musa PRIVATE MUSA::musart MUSA::mublas)\nendif()\n\nif (GGML_CUDA_NO_VMM)\n    # No VMM requested, no need to link directly with the musa driver lib (libmusa.so)\nelse()\n    target_link_libraries(ggml-musa PRIVATE MUSA::musa_driver)\nendif()\nelse()\n    message(FATAL_ERROR \"MUSA Toolkit not found\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Including CUDA Source Files in CMake\nDESCRIPTION: Gathers CUDA source files (.cu) and header files (.cuh) for inclusion in the GGML CUDA library build.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-cuda/CMakeLists.txt#2025-04-11_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB   GGML_HEADERS_CUDA \"*.cuh\")\nlist(APPEND GGML_HEADERS_CUDA \"../../include/ggml-cuda.h\")\n\nfile(GLOB   GGML_SOURCES_CUDA \"*.cu\")\nfile(GLOB   SRCS \"template-instances/fattn-mma*.cu\")\nlist(APPEND GGML_SOURCES_CUDA ${SRCS})\nfile(GLOB   SRCS \"template-instances/mmq*.cu\")\nlist(APPEND GGML_SOURCES_CUDA ${SRCS})\n```\n\n----------------------------------------\n\nTITLE: Whisper Model Inference Benchmarks (FA=0)\nDESCRIPTION: Inference performance measurements for different Whisper models with forced alignment disabled, showing encoding, decoding, batch processing and post-processing times\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/scripts/bench-all-gg.txt#2025-04-11_snippet_6\n\nLANGUAGE: text\nCODE:\n```\n|      CPU | Config |         Model |  Th |  FA |    Enc. |    Dec. |    Bch5 |      PP |  Commit |\n|      --- |    --- |           --- | --- | --- |     --- |     --- |     --- |     --- |     --- |\n| M2 ULTRA |  METAL |          tiny |   1 |   0 |    8.74 |    1.20 |    0.36 |    0.01 | ad4e350 |\n[...truncated for brevity...]\n```\n\n----------------------------------------\n\nTITLE: Collecting GGML Source and Header Files for HIP/ROCm\nDESCRIPTION: Gathers the necessary source and header files for the HIP backend implementation, including template instances for different quantization methods depending on build options.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-hip/CMakeLists.txt#2025-04-11_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB   GGML_HEADERS_ROCM \"../ggml-cuda/*.cuh\")\nlist(APPEND GGML_HEADERS_ROCM \"../../include/ggml-cuda.h\")\n\nfile(GLOB   GGML_SOURCES_ROCM \"../ggml-cuda/*.cu\")\nfile(GLOB   SRCS \"../ggml-cuda/template-instances/fattn-mma*.cu\")\nlist(APPEND GGML_SOURCES_ROCM ${SRCS})\nfile(GLOB   SRCS \"../ggml-cuda/template-instances/mmq*.cu\")\nlist(APPEND GGML_SOURCES_ROCM ${SRCS})\n\nif (GGML_CUDA_FA_ALL_QUANTS)\n    file(GLOB   SRCS \"../ggml-cuda/template-instances/fattn-vec*.cu\")\n    list(APPEND GGML_SOURCES_ROCM ${SRCS})\n    add_compile_definitions(GGML_CUDA_FA_ALL_QUANTS)\nelse()\n    file(GLOB   SRCS \"../ggml-cuda/template-instances/fattn-vec*q4_0-q4_0.cu\")\n    list(APPEND GGML_SOURCES_ROCM ${SRCS})\n    file(GLOB   SRCS \"../ggml-cuda/template-instances/fattn-vec*q8_0-q8_0.cu\")\n    list(APPEND GGML_SOURCES_ROCM ${SRCS})\n    file(GLOB   SRCS \"../ggml-cuda/template-instances/fattn-vec*f16-f16.cu\")\n    list(APPEND GGML_SOURCES_ROCM ${SRCS})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring GGML Base and Core Libraries in CMake\nDESCRIPTION: Sets up the main GGML library targets 'ggml-base' and 'ggml', including source files and compiler options. It also handles platform-specific linking and shared library settings.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-11_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(ggml-base\n            ../include/ggml.h\n            ../include/ggml-alloc.h\n            ../include/ggml-backend.h\n            ../include/ggml-cpp.h\n            ../include/ggml-opt.h\n            ../include/gguf.h\n            ggml.c\n            ggml-alloc.c\n            ggml-backend.cpp\n            ggml-opt.cpp\n            ggml-threading.cpp\n            ggml-threading.h\n            ggml-quants.c\n            ggml-quants.h\n            gguf.cpp)\n\ntarget_include_directories(ggml-base PRIVATE .)\nif (GGML_BACKEND_DL)\n    target_compile_definitions(ggml-base PUBLIC GGML_BACKEND_DL)\nendif()\n\nadd_library(ggml\n            ggml-backend-reg.cpp)\n\ntarget_link_libraries(ggml PUBLIC ggml-base)\n\nif (CMAKE_SYSTEM_NAME MATCHES \"Linux\")\n    target_link_libraries(ggml PRIVATE dl stdc++fs)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Emscripten and Shared Library Settings for whisper.cpp\nDESCRIPTION: Handles special configuration for Emscripten WebAssembly builds and sets the default behavior for shared libraries based on the platform. Includes pthread and stack size settings for WebAssembly.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/CMakeLists.txt#2025-04-11_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif (EMSCRIPTEN)\n    set(BUILD_SHARED_LIBS_DEFAULT OFF)\n\n    option(WHISPER_WASM_SINGLE_FILE \"whisper: embed WASM inside the generated whisper.js\" ON)\n\n    # TODO: without these, we get the following error:\n    #       wasm-ld: error: --shared-memory is disallowed by whisper.cpp.o because it was not compiled with 'atomics' or 'bulk-memory' features.\n    set(CMAKE_C_FLAGS   \"${CMAKE_C_FLAGS}   -pthread\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -pthread\")\n\n    set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -s TOTAL_STACK=5242880\")\n    set(CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} -s TOTAL_STACK=5242880\")\n\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wno-deprecated\")\nelse()\n    if (MINGW)\n        set(BUILD_SHARED_LIBS_DEFAULT OFF)\n    else()\n        set(BUILD_SHARED_LIBS_DEFAULT ON)\n    endif()\nendif()\n\noption(BUILD_SHARED_LIBS \"build shared libraries\" ${BUILD_SHARED_LIBS_DEFAULT})\n```\n\n----------------------------------------\n\nTITLE: Configuring SDL2-enabled Whisper CLI Build\nDESCRIPTION: Configures the build settings for the whisper command line interface executable when SDL2 support is enabled. Links required libraries including common, SDL components, and whisper core library, and sets up installation rules.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/command/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (WHISPER_SDL2)\n    set(TARGET whisper-command)\n    add_executable(${TARGET} command.cpp)\n\n    include(DefaultTargetOptions)\n\n    target_link_libraries(${TARGET} PRIVATE common common-sdl whisper ${CMAKE_THREAD_LIBS_INIT})\n\n    install(TARGETS ${TARGET} RUNTIME)\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Adding GGML Metal Backend Library in CMake\nDESCRIPTION: Adds the ggml-metal backend library to the project and links it with the required frameworks.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-metal/CMakeLists.txt#2025-04-11_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nggml_add_backend_library(ggml-metal\n                         ggml-metal.m\n                        )\n\ntarget_link_libraries(ggml-metal PRIVATE\n                      ${FOUNDATION_LIBRARY}\n                      ${METAL_FRAMEWORK}\n                      ${METALKIT_FRAMEWORK}\n                      )\n```\n\n----------------------------------------\n\nTITLE: Whisper Model Inference Benchmarks (FA=1)\nDESCRIPTION: Inference performance measurements for different Whisper models with forced alignment enabled, showing encoding, decoding, batch processing and post-processing times\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/scripts/bench-all-gg.txt#2025-04-11_snippet_7\n\nLANGUAGE: text\nCODE:\n```\n|      CPU | Config |         Model |  Th |  FA |    Enc. |    Dec. |    Bch5 |      PP |  Commit |\n|      --- |    --- |           --- | --- | --- |     --- |     --- |     --- |     --- |     --- |\n| M2 ULTRA |  METAL |          tiny |   1 |   1 |    7.82 |    1.31 |    0.35 |    0.01 | ad4e350 |\n[...truncated for brevity...]\n```\n\n----------------------------------------\n\nTITLE: Configuring SDL2-based Streaming Target in CMake\nDESCRIPTION: Sets up a conditional build target named 'lsp' when the WHISPER_SDL2 flag is enabled. The target is built from lsp.cpp and linked against several dependencies including common, json_cpp, common-sdl, whisper, and threading libraries.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/lsp/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif (WHISPER_SDL2)\n    # stream\n    set(TARGET lsp)\n    add_executable(${TARGET} lsp.cpp)\n\n    include(DefaultTargetOptions)\n\n    target_link_libraries(${TARGET} PRIVATE common json_cpp common-sdl whisper ${CMAKE_THREAD_LIBS_INIT})\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Configuring CMake Project for Whisper.cpp Android Library\nDESCRIPTION: Sets up the CMake project for Whisper.cpp, including C++ standard, source files, and library directories. It also finds the Android log library and defines a function to build the shared library with specific compiler options.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/whisper.android.java/app/src/main/jni/whisper/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.10)\n\nproject(whisper.cpp)\n\nset(CMAKE_CXX_STANDARD 17)\nset(WHISPER_LIB_DIR ${CMAKE_SOURCE_DIR}/../../../../../../../)\n\nset(SOURCE_FILES\n    ${WHISPER_LIB_DIR}/ggml/src/ggml.c\n    ${WHISPER_LIB_DIR}/ggml/src/ggml-cpu/ggml-cpu.c\n    ${WHISPER_LIB_DIR}/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp\n    ${WHISPER_LIB_DIR}/ggml/src/ggml-cpu/ggml-cpu-traits.cpp\n    ${WHISPER_LIB_DIR}/ggml/src/ggml-cpu/ggml-cpu-quants.c\n    ${WHISPER_LIB_DIR}/ggml/src/ggml-cpu/ggml-cpu.cpp\n    ${WHISPER_LIB_DIR}/ggml/src/ggml-cpu/unary-ops.cpp\n    ${WHISPER_LIB_DIR}/ggml/src/ggml-cpu/binary-ops.cpp\n    ${WHISPER_LIB_DIR}/ggml/src/ggml-cpu/vec.cpp\n    ${WHISPER_LIB_DIR}/ggml/src/ggml-cpu/ops.cpp\n    ${WHISPER_LIB_DIR}/ggml/src/ggml-alloc.c\n    ${WHISPER_LIB_DIR}/ggml/src/ggml-backend.cpp\n    ${WHISPER_LIB_DIR}/ggml/src/ggml-backend-reg.cpp\n    ${WHISPER_LIB_DIR}/ggml/src/ggml-quants.c\n    ${WHISPER_LIB_DIR}/ggml/src/ggml-threading.cpp\n    ${WHISPER_LIB_DIR}/src/whisper.cpp\n    ${CMAKE_SOURCE_DIR}/jni.c\n    )\n\nfind_library(LOG_LIB log)\n\nfunction(build_library target_name)\n    add_library(\n        ${target_name}\n        SHARED\n        ${SOURCE_FILES}\n    )\n\n    target_link_libraries(${target_name} ${LOG_LIB} android)\n    target_compile_definitions(${target_name} PUBLIC GGML_USE_CPU)\n\n    if (${target_name} STREQUAL \"whisper_v8fp16_va\")\n        target_compile_options(${target_name} PRIVATE -march=armv8.2-a+fp16)\n    elseif (${target_name} STREQUAL \"whisper_vfpv4\")\n        target_compile_options(${target_name} PRIVATE -mfpu=neon-vfpv4)\n    endif ()\n\n    if (NOT ${CMAKE_BUILD_TYPE} STREQUAL \"Debug\")\n\n        target_compile_options(${target_name} PRIVATE -O3)\n        target_compile_options(${target_name} PRIVATE -fvisibility=hidden -fvisibility-inlines-hidden)\n        target_compile_options(${target_name} PRIVATE -ffunction-sections -fdata-sections)\n\n        #target_link_options(${target_name} PRIVATE -Wl,--gc-sections)\n        #target_link_options(${target_name} PRIVATE -Wl,--exclude-libs,ALL)\n        #target_link_options(${target_name} PRIVATE -flto)\n    endif ()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Building Tests and Examples for whisper.cpp\nDESCRIPTION: Conditional configuration for building tests and examples based on the build options. This section includes CTest for testing and adds the tests and examples subdirectories if enabled.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/CMakeLists.txt#2025-04-11_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\n#\n# programs, examples and tests\n#\n\nif (WHISPER_BUILD_TESTS AND NOT CMAKE_JS_VERSION)\n    include(CTest)\n    add_subdirectory(tests)\nendif ()\n\nif (WHISPER_BUILD_EXAMPLES)\n    add_subdirectory(examples)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Up OpenCL Target Library in CMake\nDESCRIPTION: Creates and configures the ggml-opencl backend library, linking it with OpenCL libraries and including necessary directories.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-opencl/CMakeLists.txt#2025-04-11_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET_NAME ggml-opencl)\n\nggml_add_backend_library(${TARGET_NAME}\n                         ggml-opencl.cpp\n                         ../../include/ggml-opencl.h)\ntarget_link_libraries(${TARGET_NAME} PRIVATE ${OpenCL_LIBRARIES})\ntarget_include_directories(${TARGET_NAME} PRIVATE ${OpenCL_INCLUDE_DIRS})\n```\n\n----------------------------------------\n\nTITLE: Detecting ROCm Path in CMake\nDESCRIPTION: Sets up the ROCM_PATH variable by checking environment variables and common installation locations. This is essential for finding ROCm libraries and tools.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-hip/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif (NOT EXISTS $ENV{ROCM_PATH})\n    if (NOT EXISTS /opt/rocm)\n        set(ROCM_PATH /usr)\n    else()\n        set(ROCM_PATH /opt/rocm)\n    endif()\nelse()\n    set(ROCM_PATH $ENV{ROCM_PATH})\nendif()\n\nlist(APPEND CMAKE_PREFIX_PATH  ${ROCM_PATH})\nlist(APPEND CMAKE_PREFIX_PATH \"${ROCM_PATH}/lib64/cmake\")\n```\n\n----------------------------------------\n\nTITLE: Running Multi-threaded Memcpy Benchmark on M4 Max\nDESCRIPTION: Executes a benchmark to measure memory copy performance on M4 Max with 8 threads, showing memory bandwidth in GB/s for different thread counts.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/scripts/bench-all-gg.txt#2025-04-11_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nmake -j && ./scripts/bench-all.sh 8\n```\n\n----------------------------------------\n\nTITLE: Embedding Metal Library in GGML using Assembly in CMake\nDESCRIPTION: Creates an assembly file that embeds the Metal shader code directly into the library, allowing for standalone operation without external shader files.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-metal/CMakeLists.txt#2025-04-11_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nset(METALLIB_COMMON \"${CMAKE_CURRENT_SOURCE_DIR}/../ggml-common.h\")\nif (GGML_METAL_EMBED_LIBRARY)\n    enable_language(ASM)\n\n    add_compile_definitions(GGML_METAL_EMBED_LIBRARY)\n\n    set(METALLIB_SOURCE \"${CMAKE_CURRENT_SOURCE_DIR}/ggml-metal.metal\")\n    set(METALLIB_IMPL   \"${CMAKE_CURRENT_SOURCE_DIR}/ggml-metal-impl.h\")\n\n    file(MAKE_DIRECTORY \"${CMAKE_BINARY_DIR}/autogenerated\")\n\n    # merge ggml-common.h and ggml-metal.metal into a single file\n    set(METALLIB_EMBED_ASM        \"${CMAKE_BINARY_DIR}/autogenerated/ggml-metal-embed.s\")\n    set(METALLIB_SOURCE_EMBED     \"${CMAKE_BINARY_DIR}/autogenerated/ggml-metal-embed.metal\")\n    set(METALLIB_SOURCE_EMBED_TMP \"${CMAKE_BINARY_DIR}/autogenerated/ggml-metal-embed.metal.tmp\")\n\n    add_custom_command(\n        OUTPUT ${METALLIB_EMBED_ASM}\n        COMMAND echo \"Embedding Metal library\"\n        COMMAND sed -e '/__embed_ggml-common.h__/r         ${METALLIB_COMMON}' -e '/__embed_ggml-common.h__/d'         < ${METALLIB_SOURCE}           > ${METALLIB_SOURCE_EMBED_TMP}\n        COMMAND sed -e '/\\#include \\\"ggml-metal-impl.h\\\"/r ${METALLIB_IMPL}'   -e '/\\#include \\\"ggml-metal-impl.h\\\"/d' < ${METALLIB_SOURCE_EMBED_TMP} > ${METALLIB_SOURCE_EMBED}\n        COMMAND echo \".section __DATA,__ggml_metallib\"          >  ${METALLIB_EMBED_ASM}\n        COMMAND echo \".globl _ggml_metallib_start\"              >> ${METALLIB_EMBED_ASM}\n        COMMAND echo \"_ggml_metallib_start:\"                    >> ${METALLIB_EMBED_ASM}\n        COMMAND echo \".incbin \\\\\\\"${METALLIB_SOURCE_EMBED}\\\\\\\"\" >> ${METALLIB_EMBED_ASM}\n        COMMAND echo \".globl _ggml_metallib_end\"                >> ${METALLIB_EMBED_ASM}\n        COMMAND echo \"_ggml_metallib_end:\"                      >> ${METALLIB_EMBED_ASM}\n        DEPENDS ../ggml-common.h ggml-metal.metal ggml-metal-impl.h\n        COMMENT \"Generate assembly for embedded Metal library\"\n    )\n\n    target_sources(ggml-metal PRIVATE ${METALLIB_EMBED_ASM})\n```\n\n----------------------------------------\n\nTITLE: Configuring Platform-specific Subdirectories in CMake\nDESCRIPTION: This conditional block adds different subdirectories to the build based on the platform (Emscripten, Node.js, or native). It also handles optional dependencies like SDL2 and SYCL.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/CMakeLists.txt#2025-04-11_snippet_8\n\nLANGUAGE: CMake\nCODE:\n```\nif (EMSCRIPTEN)\n    add_subdirectory(whisper.wasm)\n    add_subdirectory(stream.wasm)\n    add_subdirectory(command.wasm)\n    add_subdirectory(bench.wasm)\nelseif(CMAKE_JS_VERSION)\n    add_subdirectory(addon.node)\nelse()\n    add_subdirectory(cli)\n    add_subdirectory(bench)\n    add_subdirectory(server)\n    add_subdirectory(quantize)\n    if (WHISPER_SDL2)\n        add_subdirectory(stream)\n        add_subdirectory(command)\n        add_subdirectory(talk-llama)\n        add_subdirectory(lsp)\n        if (GGML_SYCL)\n            add_subdirectory(sycl)\n        endif()\n    endif (WHISPER_SDL2)\n\n    add_subdirectory(deprecation-warning)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Compiler Cache Configuration in CMake\nDESCRIPTION: Sets up ccache or sccache for faster compilation by caching build artifacts. Includes special handling for SYCL on Windows.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-11_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif (GGML_CCACHE AND NOT CMAKE_C_COMPILER_LAUNCHER AND NOT CMAKE_CXX_COMPILER_LAUNCHER)\n    find_program(GGML_CCACHE_FOUND ccache)\n    find_program(GGML_SCCACHE_FOUND sccache)\n\n    if (GGML_CCACHE_FOUND OR GGML_SCCACHE_FOUND)\n        if(GGML_CCACHE_FOUND)\n            set(GGML_CCACHE_VARIANT ccache)\n        else()\n            set(GGML_CCACHE_VARIANT sccache)\n        endif()\n        if (GGML_SYCL AND GGML_CCACHE_FOUND AND WIN32)\n            set_property(GLOBAL PROPERTY RULE_LAUNCH_COMPILE \"ccache compiler_type=icl\")\n        else ()\n            set_property(GLOBAL PROPERTY RULE_LAUNCH_COMPILE \"${GGML_CCACHE_VARIANT}\")\n        endif ()\n        set(ENV{CCACHE_SLOPPINESS} time_macros)\n        message(STATUS \"${GGML_CCACHE_VARIANT} found, compilation results will be cached. Disable with GGML_CCACHE=OFF.\")\n    endif ()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Apple Accelerate BLAS\nDESCRIPTION: Sets compiler definitions for using Apple's Accelerate framework with LAPACK. This configures the GGML BLAS backend to use Apple's optimized BLAS implementation.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-blas/CMakeLists.txt#2025-04-11_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\n    if (${GGML_BLAS_VENDOR} MATCHES \"Apple\")\n        add_compile_definitions(ACCELERATE_NEW_LAPACK)\n        add_compile_definitions(ACCELERATE_LAPACK_ILP64)\n        add_compile_definitions(GGML_BLAS_USE_ACCELERATE)\n```\n\n----------------------------------------\n\nTITLE: Configuring Standard Model Tests\nDESCRIPTION: Defines test cases for various Whisper model sizes (tiny through large) with both general and English-specific variants. Each test uses the JFK sample WAV file and specifies appropriate test labels.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/tests/CMakeLists.txt#2025-04-11_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nset(TEST_TARGET test-whisper-cli-tiny)\nadd_test(NAME ${TEST_TARGET}\n    COMMAND $<TARGET_FILE:whisper-cli>\n    -m ${PROJECT_SOURCE_DIR}/models/for-tests-ggml-tiny.bin -l fr\n    -f ${PROJECT_SOURCE_DIR}/samples/jfk.wav)\nset_tests_properties(${TEST_TARGET} PROPERTIES LABELS \"tiny;gh\")\n\nset(TEST_TARGET test-whisper-cli-tiny.en)\nadd_test(NAME ${TEST_TARGET}\n    COMMAND $<TARGET_FILE:whisper-cli>\n    -m ${PROJECT_SOURCE_DIR}/models/for-tests-ggml-tiny.en.bin\n    -f ${PROJECT_SOURCE_DIR}/samples/jfk.wav)\nset_tests_properties(${TEST_TARGET} PROPERTIES LABELS \"tiny;en\")\n```\n\n----------------------------------------\n\nTITLE: Running Whisper Model Inference Benchmark on M1 Pro (With Flash Attention)\nDESCRIPTION: This snippet shows the command to run the Whisper model inference benchmark with flash attention enabled and its results for different model sizes.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/scripts/bench-all-gg.txt#2025-04-11_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nmake -j && ./scripts/bench-all.sh 1 1 1\n```\n\n----------------------------------------\n\nTITLE: Configuring CUDA Architecture Selection in CMake\nDESCRIPTION: Sets up CUDA architecture selection based on various conditions, including native GPU support and specific CUDA features.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-cuda/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif (NOT DEFINED CMAKE_CUDA_ARCHITECTURES)\n    if (GGML_NATIVE AND CUDAToolkit_VERSION VERSION_GREATER_EQUAL \"11.6\" AND CMAKE_VERSION VERSION_GREATER_EQUAL \"3.24\")\n        set(CMAKE_CUDA_ARCHITECTURES \"native\")\n    elseif(GGML_CUDA_F16 OR GGML_CUDA_DMMV_F16)\n        set(CMAKE_CUDA_ARCHITECTURES \"60;61;70;75;80\")\n    else()\n        set(CMAKE_CUDA_ARCHITECTURES \"50;61;70;75;80\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Neovim Key Mappings\nDESCRIPTION: Vim configuration for adding keyboard shortcuts that activate speech-to-text in different editor modes (insert, normal, and visual). The mappings run the whisper.nvim script and insert the transcribed text at the cursor.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/whisper.nvim/README.md#2025-04-11_snippet_4\n\nLANGUAGE: vim\nCODE:\n```\ninoremap <C-G>  <C-O>:!whisper.nvim<CR><C-O>:let @a = system(\"cat /tmp/whisper.nvim \\| tail -n 1 \\| xargs -0 \\| tr -d '\\\\n' \\| sed -e 's/^[[:space:]]*//'\"><CR><C-R>a\nnnoremap <C-G>       :!whisper.nvim<CR>:let @a = system(\"cat /tmp/whisper.nvim \\| tail -n 1 \\| xargs -0 \\| tr -d '\\\\n' \\| sed -e 's/^[[:space:]]*//'\"><CR>\"ap\nvnoremap <C-G> c<C-O>:!whisper.nvim<CR><C-O>:let @a = system(\"cat /tmp/whisper.nvim \\| tail -n 1 \\| xargs -0 \\| tr -d '\\\\n' \\| sed -e 's/^[[:space:]]*//'\"><CR><C-R>a\n```\n\n----------------------------------------\n\nTITLE: Configuring Vulkan Shader Generation in CMake\nDESCRIPTION: Sets up custom commands to generate Vulkan shaders using a custom tool (vulkan-shaders-gen). Handles both native and cross-compilation scenarios.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-vulkan/CMakeLists.txt#2025-04-11_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset (_ggml_vk_host_suffix $<IF:$<STREQUAL:${CMAKE_HOST_SYSTEM_NAME},Windows>,.exe,>)\nset (_ggml_vk_genshaders_cmd ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/vulkan-shaders-gen${_ggml_vk_host_suffix})\nset (_ggml_vk_header     ${CMAKE_CURRENT_BINARY_DIR}/ggml-vulkan-shaders.hpp)\nset (_ggml_vk_source     ${CMAKE_CURRENT_BINARY_DIR}/ggml-vulkan-shaders.cpp)\nset (_ggml_vk_input_dir  ${CMAKE_CURRENT_SOURCE_DIR}/vulkan-shaders)\nset (_ggml_vk_output_dir ${CMAKE_CURRENT_BINARY_DIR}/vulkan-shaders.spv)\n\nfile(GLOB _ggml_vk_shader_deps \"${_ggml_vk_input_dir}/*.comp\")\nset (_ggml_vk_shader_deps ${_ggml_vk_shader_deps} vulkan-shaders-gen)\n\nif (CMAKE_CROSSCOMPILING)\n    set(_ggml_vk_shader_deps ${_ggml_vk_shader_deps} vulkan-shaders-gen-build vulkan-shaders-gen-install)\nendif()\n\nadd_custom_command(\n    OUTPUT ${_ggml_vk_header}\n            ${_ggml_vk_source}\n\n    COMMAND ${_ggml_vk_genshaders_cmd}\n        --glslc      ${Vulkan_GLSLC_EXECUTABLE}\n        --input-dir  ${_ggml_vk_input_dir}\n        --output-dir ${_ggml_vk_output_dir}\n        --target-hpp ${_ggml_vk_header}\n        --target-cpp ${_ggml_vk_source}\n        --no-clean\n\n    DEPENDS ${_ggml_vk_shader_deps}\n    COMMENT \"Generate vulkan shaders\"\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring GGML BLAS Backend Library\nDESCRIPTION: Creates a BLAS backend library for GGML when BLAS is found. This adds the ggml-blas.cpp source file to compile as a backend library for GGML.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-blas/CMakeLists.txt#2025-04-11_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif (BLAS_FOUND)\n    message(STATUS \"BLAS found, Libraries: ${BLAS_LIBRARIES}\")\n\n    ggml_add_backend_library(ggml-blas\n                             ggml-blas.cpp\n                            )\n```\n\n----------------------------------------\n\nTITLE: Configuring FFmpeg MP3 Support Test\nDESCRIPTION: Adds an additional test case for MP3 format support when FFmpeg is enabled, using the tiny English model and JFK MP3 sample file.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/tests/CMakeLists.txt#2025-04-11_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif (WHISPER_FFMPEG)\n    set(TEST_TARGET test-whisper-cli-tiny-mp3)\n    # Check with reviewers: any way to check the output transcription via ctest (diff, ...)?\n    add_test(NAME ${TEST_TARGET}\n      COMMAND $<TARGET_FILE:whisper-cli>\n      -m ${PROJECT_SOURCE_DIR}/models/for-tests-ggml-tiny.en.bin\n      -f ${PROJECT_SOURCE_DIR}/samples/jfk.mp3)\n    set_tests_properties(${TEST_TARGET} PROPERTIES LABELS \"tiny;mp3\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running Whisper.cpp Speech Recognition on Audio Sample\nDESCRIPTION: Command to run speech recognition on an audio file using a specified model, demonstrating basic usage of the compiled example.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/bindings/go/README.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n./build/go-whisper -model models/ggml-tiny.en.bin samples/jfk.wav\n```\n\n----------------------------------------\n\nTITLE: Running Matrix Multiplication Benchmark on M1 Pro\nDESCRIPTION: This snippet shows the command to run the ggml_mul_mat benchmark with 1 thread and its results for various matrix sizes and quantization types.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/scripts/bench-all-gg.txt#2025-04-11_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nmake -j && ./scripts/bench-all.sh 1 0 0\n```\n\n----------------------------------------\n\nTITLE: Initializing CMake Project Configuration\nDESCRIPTION: Sets up basic CMake project configuration including minimum version, project name, and C++ standard. Defines the base project structure and source file paths.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/whisper.android/lib/src/main/jni/whisper/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\ncmake_minimum_required(VERSION 3.10)\n\nproject(whisper.cpp)\n\nset(CMAKE_CXX_STANDARD 17)\nset(WHISPER_LIB_DIR ${CMAKE_SOURCE_DIR}/../../../../../../..)\n\n# Path to external GGML, otherwise uses the copy in whisper.cpp.\noption(GGML_HOME \"whisper: Path to external GGML source\" OFF)\n\nset(\n    SOURCE_FILES\n    ${WHISPER_LIB_DIR}/src/whisper.cpp\n    ${CMAKE_SOURCE_DIR}/jni.c\n    )\n```\n\n----------------------------------------\n\nTITLE: Finding BLAS Include Directories with PkgConfig\nDESCRIPTION: Uses pkg-config to locate BLAS include directories when they're not provided by FindBLAS.cmake. This is a workaround for a known issue in CMake's BLAS detection.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-blas/CMakeLists.txt#2025-04-11_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\n    elseif (\"${BLAS_INCLUDE_DIRS}\" STREQUAL \"\")\n        # BLAS_INCLUDE_DIRS is missing in FindBLAS.cmake.\n        # see https://gitlab.kitware.com/cmake/cmake/-/issues/20268\n        find_package(PkgConfig REQUIRED)\n        if (${GGML_BLAS_VENDOR} MATCHES \"Generic\")\n            pkg_check_modules(DepBLAS blas)\n        elseif (${GGML_BLAS_VENDOR} MATCHES \"OpenBLAS\")\n            # As of openblas v0.3.22, the 64-bit is named openblas64.pc\n            pkg_check_modules(DepBLAS openblas64)\n            if (NOT DepBLAS_FOUND)\n                pkg_check_modules(DepBLAS openblas)\n            endif()\n        elseif (${GGML_BLAS_VENDOR} MATCHES \"FLAME\")\n            add_compile_definitions(GGML_BLAS_USE_BLIS)\n            pkg_check_modules(DepBLAS blis)\n        elseif (${GGML_BLAS_VENDOR} MATCHES \"ATLAS\")\n            pkg_check_modules(DepBLAS blas-atlas)\n        elseif (${GGML_BLAS_VENDOR} MATCHES \"FlexiBLAS\")\n            pkg_check_modules(DepBLAS flexiblas_api)\n        elseif (${GGML_BLAS_VENDOR} MATCHES \"Intel\")\n            add_compile_definitions(GGML_BLAS_USE_MKL)\n            # all Intel* libraries share the same include path\n            pkg_check_modules(DepBLAS mkl-sdl)\n        elseif (${GGML_BLAS_VENDOR} MATCHES \"NVHPC\")\n            # this doesn't provide pkg-config\n            # suggest to assign BLAS_INCLUDE_DIRS on your own\n            if (\"${NVHPC_VERSION}\" STREQUAL \"\")\n                message(WARNING \"Better to set NVHPC_VERSION\")\n            else()\n                set(DepBLAS_FOUND ON)\n                set(DepBLAS_INCLUDE_DIRS \"/opt/nvidia/hpc_sdk/${CMAKE_SYSTEM_NAME}_${CMAKE_SYSTEM_PROCESSOR}/${NVHPC_VERSION}/math_libs/include\")\n            endif()\n        endif()\n```\n\n----------------------------------------\n\nTITLE: Finding BLAS Package with Vendor Specification\nDESCRIPTION: Sets the BLAS vendor from GGML_BLAS_VENDOR variable and invokes CMake's find_package to locate the BLAS implementation.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-blas/CMakeLists.txt#2025-04-11_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(BLA_VENDOR ${GGML_BLAS_VENDOR})\nfind_package(BLAS)\n```\n\n----------------------------------------\n\nTITLE: GGML Core Build Configuration\nDESCRIPTION: Sets up core build requirements including C/C++ standards, threading support, and basic project structure. Configures compiler standards and finds required system dependencies.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/CMakeLists.txt#2025-04-11_snippet_8\n\nLANGUAGE: cmake\nCODE:\n```\nset(CMAKE_C_STANDARD 11)\nset(CMAKE_C_STANDARD_REQUIRED true)\n\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_STANDARD_REQUIRED true)\n\nset(THREADS_PREFER_PTHREAD_FLAG ON)\n\nfind_package(Threads REQUIRED)\n\ninclude(GNUInstallDirs)\n```\n\n----------------------------------------\n\nTITLE: Configuring SDL2-dependent wchess Executable in CMake\nDESCRIPTION: Conditional CMake block that creates and configures the wchess executable when SDL2 is enabled. Sets up target options and links necessary libraries including wchess-core, SDL components, and threading libraries.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/wchess/wchess.cmd/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (WHISPER_SDL2)\n    set(TARGET wchess)\n    add_executable(${TARGET} wchess.cmd.cpp)\n\n    include(DefaultTargetOptions)\n\n    target_link_libraries(${TARGET} PRIVATE wchess-core common-sdl ${CMAKE_THREAD_LIBS_INIT})\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Linking CUDA Libraries in CMake\nDESCRIPTION: Configures library linking for CUDA, including static and dynamic linking options for various CUDA libraries.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-cuda/CMakeLists.txt#2025-04-11_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nif (GGML_STATIC)\n    if (WIN32)\n        target_link_libraries(ggml-cuda PRIVATE CUDA::cudart_static CUDA::cublas CUDA::cublasLt)\n    else ()\n        target_link_libraries(ggml-cuda PRIVATE  CUDA::cudart_static CUDA::cublas_static CUDA::cublasLt_static)\n    endif()\nelse()\n    target_link_libraries(ggml-cuda PRIVATE CUDA::cudart CUDA::cublas CUDA::cublasLt)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring GGML Source Files\nDESCRIPTION: Conditionally adds GGML source files to the build if no external GGML path is specified. Includes core GGML functionality and CPU-specific implementations.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/whisper.android/lib/src/main/jni/whisper/CMakeLists.txt#2025-04-11_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif (NOT GGML_HOME)\n    set(\n        SOURCE_FILES\n        ${SOURCE_FILES}\n        ${WHISPER_LIB_DIR}/ggml/src/ggml.c\n        ${WHISPER_LIB_DIR}/ggml/src/ggml-alloc.c\n        ${WHISPER_LIB_DIR}/ggml/src/ggml-backend.cpp\n        ${WHISPER_LIB_DIR}/ggml/src/ggml-backend-reg.cpp\n        ${WHISPER_LIB_DIR}/ggml/src/ggml-quants.c\n        ${WHISPER_LIB_DIR}/ggml/src/ggml-threading.cpp\n        ${WHISPER_LIB_DIR}/ggml/src/ggml-cpu/ggml-cpu.c\n        ${WHISPER_LIB_DIR}/ggml/src/ggml-cpu/ggml-cpu.cpp\n        ${WHISPER_LIB_DIR}/ggml/src/ggml-cpu/ggml-cpu-aarch64.cpp\n        ${WHISPER_LIB_DIR}/ggml/src/ggml-cpu/ggml-cpu-hbm.cpp\n        ${WHISPER_LIB_DIR}/ggml/src/ggml-cpu/ggml-cpu-quants.c\n        ${WHISPER_LIB_DIR}/ggml/src/ggml-cpu/ggml-cpu-traits.cpp\n        ${WHISPER_LIB_DIR}/ggml/src/ggml-cpu/unary-ops.cpp\n        ${WHISPER_LIB_DIR}/ggml/src/ggml-cpu/binary-ops.cpp\n        ${WHISPER_LIB_DIR}/ggml/src/ggml-cpu/vec.cpp\n        ${WHISPER_LIB_DIR}/ggml/src/ggml-cpu/ops.cpp\n        )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Finding Required HIP Dependencies and Version Check\nDESCRIPTION: Locates necessary HIP packages and checks for ROCwmma if enabled. Verifies that the installed ROCm/HIP version is at least 5.5, which is required for proper functionality.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-hip/CMakeLists.txt#2025-04-11_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package(hip     REQUIRED)\nfind_package(hipblas REQUIRED)\nfind_package(rocblas REQUIRED)\nif (GGML_HIP_ROCWMMA_FATTN)\n    CHECK_INCLUDE_FILE_CXX(\"rocwmma/rocwmma.hpp\" FOUND_ROCWMMA)\n    if (NOT ${FOUND_ROCWMMA})\n        message(FATAL_ERROR \"rocwmma has not been found\")\n    endif()\nendif()\n\nif (${hip_VERSION} VERSION_LESS 5.5)\n    message(FATAL_ERROR \"At least ROCM/HIP V5.5 is required\")\nendif()\n\nmessage(STATUS \"HIP and hipBLAS found\")\n\n# Workaround old compilers\nset(CMAKE_HIP_FLAGS \"${CMAKE_HIP_FLAGS} --gpu-max-threads-per-block=1024\")\n```\n\n----------------------------------------\n\nTITLE: Handling BLAS Not Found Error\nDESCRIPTION: Displays an error message when BLAS is not found, providing guidance on setting the correct BLAS vendor. This helps users troubleshoot BLAS configuration issues.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-blas/CMakeLists.txt#2025-04-11_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nelse()\n    message(ERROR \"BLAS not found, please refer to \"\n                  \"https://cmake.org/cmake/help/latest/module/FindBLAS.html#blas-lapack-vendors\"\n                  \" to set correct GGML_BLAS_VENDOR\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Build Defaults for Different Platforms\nDESCRIPTION: Sets up default build options for different platforms, including Emscripten (WASM) and MinGW. Configures library prefix settings for Windows environments.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/CMakeLists.txt#2025-04-11_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif (EMSCRIPTEN)\n    set(BUILD_SHARED_LIBS_DEFAULT OFF)\n\n    option(GGML_WASM_SINGLE_FILE \"ggml: embed WASM inside the generated ggml.js\" ON)\nelse()\n    if (MINGW)\n        set(BUILD_SHARED_LIBS_DEFAULT OFF)\n    else()\n        set(BUILD_SHARED_LIBS_DEFAULT ON)\n    endif()\nendif()\n\n# remove the lib prefix on win32 mingw\nif (WIN32)\n    set(CMAKE_STATIC_LIBRARY_PREFIX \"\")\n    set(CMAKE_SHARED_LIBRARY_PREFIX \"\")\n    set(CMAKE_SHARED_MODULE_PREFIX  \"\")\nendif()\n\noption(BUILD_SHARED_LIBS \"ggml: build shared libraries\" ${BUILD_SHARED_LIBS_DEFAULT})\noption(GGML_BACKEND_DL   \"ggml: build backends as dynamic libraries (requires BUILD_SHARED_LIBS)\" OFF)\n```\n\n----------------------------------------\n\nTITLE: Initializing Vulkan Dependencies\nDESCRIPTION: Sets up required Vulkan components and validates the presence of the GLSL compiler (glslc)\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-kompute/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nfind_package(Vulkan COMPONENTS glslc REQUIRED)\nfind_program(glslc_executable NAMES glslc HINTS Vulkan::glslc)\n\nif (NOT glslc_executable)\n    message(FATAL_ERROR \"glslc not found\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring whisper.wasm Web Interface Files\nDESCRIPTION: Sets up the whisper.wasm target and configures the HTML and JavaScript files needed for the web interface. The template files are copied to the output directory with variables replaced.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/whisper.wasm/CMakeLists.txt#2025-04-11_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET whisper.wasm)\n\nconfigure_file(${CMAKE_CURRENT_SOURCE_DIR}/index-tmpl.html  ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/${TARGET}/index.html @ONLY)\nconfigure_file(${CMAKE_CURRENT_SOURCE_DIR}/../helpers.js    ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/${TARGET}/helpers.js @ONLY)\n```\n\n----------------------------------------\n\nTITLE: Configuring GGML Kompute Backend Library\nDESCRIPTION: Sets up the GGML Kompute backend library with necessary source files and dependencies\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-kompute/CMakeLists.txt#2025-04-11_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nggml_add_backend_library(ggml-kompute\n                         ggml-kompute.cpp\n                         ../../include/ggml-kompute.h\n                        )\n\ntarget_link_libraries(ggml-kompute PRIVATE ggml-base kompute)\ntarget_include_directories(ggml-kompute PRIVATE ${CMAKE_CURRENT_BINARY_DIR})\n\nadd_compile_definitions(VULKAN_HPP_DISPATCH_LOADER_DYNAMIC=1)\n```\n\n----------------------------------------\n\nTITLE: Shader Compilation Configuration\nDESCRIPTION: Configures the compilation of multiple GLSL compute shaders used for various operations in whisper.cpp\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-kompute/CMakeLists.txt#2025-04-11_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\ncompile_shader(SOURCES\n    kompute-shaders/op_scale.comp\n    kompute-shaders/op_scale_8.comp\n    kompute-shaders/op_add.comp\n    kompute-shaders/op_addrow.comp\n    kompute-shaders/op_mul.comp\n    kompute-shaders/op_silu.comp\n    kompute-shaders/op_relu.comp\n    kompute-shaders/op_gelu.comp\n    kompute-shaders/op_softmax.comp\n    kompute-shaders/op_norm.comp\n    kompute-shaders/op_rmsnorm.comp\n    kompute-shaders/op_diagmask.comp\n    kompute-shaders/op_mul_mat_mat_f32.comp\n    kompute-shaders/op_mul_mat_f16.comp\n    kompute-shaders/op_mul_mat_q8_0.comp\n    kompute-shaders/op_mul_mat_q4_0.comp\n    kompute-shaders/op_mul_mat_q4_1.comp\n    kompute-shaders/op_mul_mat_q4_k.comp\n    kompute-shaders/op_mul_mat_q6_k.comp\n    kompute-shaders/op_getrows_f32.comp\n    kompute-shaders/op_getrows_f16.comp\n    kompute-shaders/op_getrows_q4_0.comp\n    kompute-shaders/op_getrows_q4_1.comp\n    kompute-shaders/op_getrows_q6_k.comp\n    kompute-shaders/op_rope_norm_f16.comp\n    kompute-shaders/op_rope_norm_f32.comp\n    kompute-shaders/op_rope_neox_f16.comp\n    kompute-shaders/op_rope_neox_f32.comp\n    kompute-shaders/op_cpy_f16_f16.comp\n    kompute-shaders/op_cpy_f16_f32.comp\n    kompute-shaders/op_cpy_f32_f16.comp\n    kompute-shaders/op_cpy_f32_f32.comp\n)\n```\n\n----------------------------------------\n\nTITLE: Listing and Adding OpenCL Kernels in CMake\nDESCRIPTION: Defines the list of OpenCL kernel files to be included in the build and applies the kernel addition function to each one.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-opencl/CMakeLists.txt#2025-04-11_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nset(GGML_OPENCL_KERNELS\n    ggml-opencl\n    ggml-opencl_mm\n    ggml-opencl_cvt\n    ggml-opencl_gemv_noshuffle\n    ggml-opencl_gemv_noshuffle_general\n    ggml-opencl_mul_mat_Ab_Bi_8x4\n    ggml-opencl_transpose_16\n    ggml-opencl_transpose_32\n    ggml-opencl_transpose_32_16\n    ggml-opencl_im2col\n)\n\nforeach (K ${GGML_OPENCL_KERNELS})\n    ggml_opencl_add_kernel(${K})\nendforeach()\n```\n\n----------------------------------------\n\nTITLE: Setting Project Configuration in CMake for whisper.cpp\nDESCRIPTION: Configures the core project settings including version, build type, and compiler options for whisper.cpp. Sets the required CMake version and the default build type to Release if not specified.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.5) # for add_link_options and implicit target directories.\nproject(\"whisper.cpp\" C CXX)\nproject(\"whisper.cpp\" VERSION 1.7.5)\ninclude(CheckIncludeFileCXX)\n\nset(SOVERSION 1)\n\n#set(CMAKE_WARN_DEPRECATED YES)\nset(CMAKE_WARN_UNUSED_CLI YES)\n\nset(CMAKE_EXPORT_COMPILE_COMMANDS ON)\n\nif (NOT XCODE AND NOT MSVC AND NOT CMAKE_BUILD_TYPE)\n    set(CMAKE_BUILD_TYPE Release CACHE STRING \"Build type\" FORCE)\n    set_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS \"Debug\" \"Release\" \"MinSizeRel\" \"RelWithDebInfo\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting up CANN Package Path and Run Mode in CMake\nDESCRIPTION: This snippet sets up the CANN package path and defines a cache variable for the run mode. It allows switching between NPU and simulation modes.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-cann/kernels/CMakeLists.txt#2025-04-11_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nset(ASCEND_CANN_PACKAGE_PATH ${CANN_INSTALL_DIR})\nset(RUN_MODE \"npu\" CACHE STRING \"run mode: npu/sim\")\n```\n\n----------------------------------------\n\nTITLE: Copying Metal Files to Build Directory in CMake\nDESCRIPTION: Copies necessary Metal shader files and headers to the build output directory so they can be accessed at runtime.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-metal/CMakeLists.txt#2025-04-11_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\n# copy metal files to bin directory\nconfigure_file(../ggml-common.h  ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/ggml-common.h     COPYONLY)\nconfigure_file(ggml-metal.metal  ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/ggml-metal.metal  COPYONLY)\nconfigure_file(ggml-metal-impl.h ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/ggml-metal-impl.h COPYONLY)\n```\n\n----------------------------------------\n\nTITLE: Finding Required Dependencies for OpenCL Backend in CMake\nDESCRIPTION: Finds the required packages OpenCL and Python3 for building the OpenCL backend of ggml.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-opencl/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package(OpenCL REQUIRED)\nfind_package(Python3 REQUIRED)\n```\n\n----------------------------------------\n\nTITLE: Ascend SOC Type Detection Function\nDESCRIPTION: Function to automatically detect Ascend SOC type using npu-smi tool. Fails if detection is unsuccessful.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-cann/CMakeLists.txt#2025-04-11_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(detect_ascend_soc_type SOC_VERSION)\n    execute_process(\n        COMMAND bash -c \"npu-smi info|awk -F' ' 'NF > 0 && NR==7 {print $3}'\"\n        OUTPUT_VARIABLE npu_info\n        RESULT_VARIABLE npu_result\n        OUTPUT_STRIP_TRAILING_WHITESPACE\n    )\n    if(\"${npu_info}\" STREQUAL \"\" OR ${npu_result})\n        message(FATAL_ERROR \"Auto-detech ascend soc type failed, please specify manually or check ascend device working normally.\")\n    endif()\n    set(${SOC_VERSION} \"Ascend${npu_info}\" PARENT_SCOPE)\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Setting Up OpenCL Kernel Embedding in CMake\nDESCRIPTION: Configures the build to embed OpenCL kernels into the binary rather than loading them at runtime when GGML_OPENCL_EMBED_KERNELS is set.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-opencl/CMakeLists.txt#2025-04-11_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nif (GGML_OPENCL_EMBED_KERNELS)\n    add_compile_definitions(GGML_OPENCL_EMBED_KERNELS)\n\n    set(EMBED_KERNEL_SCRIPT \"${CMAKE_CURRENT_SOURCE_DIR}/kernels/embed_kernel.py\")\n    file(MAKE_DIRECTORY     \"${CMAKE_CURRENT_BINARY_DIR}/autogenerated\")\n\n    target_include_directories(${TARGET_NAME} PRIVATE \"${CMAKE_CURRENT_BINARY_DIR}/autogenerated\")\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Configuring Module Paths and Project Structure for whisper.cpp\nDESCRIPTION: Sets up the module paths and output directories for the build. Determines if this is a standalone build and configures JavaScript bindings if needed. This section handles setup for different build environments.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/CMakeLists.txt#2025-04-11_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\n# Add path to modules\nlist(APPEND CMAKE_MODULE_PATH \"${CMAKE_CURRENT_SOURCE_DIR}/cmake/\")\n\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/bin)\n\nif (CMAKE_SOURCE_DIR STREQUAL CMAKE_CURRENT_SOURCE_DIR)\n    set(WHISPER_STANDALONE ON)\n\n    include(git-vars)\n\n    # configure project version\n    configure_file(${CMAKE_SOURCE_DIR}/bindings/javascript/package-tmpl.json ${CMAKE_SOURCE_DIR}/bindings/javascript/package.json @ONLY)\nelse()\n    set(WHISPER_STANDALONE OFF)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Library Building Configuration for whisper.cpp\nDESCRIPTION: Configures the build of the whisper library, either using a system-installed GGML library or building it from source. Adds the src directory to the build process.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/CMakeLists.txt#2025-04-11_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\n#\n# build the library\n#\n\nif (NOT TARGET ggml)\n    if (WHISPER_USE_SYSTEM_GGML)\n        find_package(ggml REQUIRED)\n        if (NOT ggml_FOUND)\n            message(FATAL_ERROR \"System-installed GGML library not found.\")\n        endif()\n        add_library(ggml ALIAS ggml::ggml)\n    else()\n        add_subdirectory(ggml)\n    endif()\n    # ... otherwise assume ggml is added by a parent CMakeLists.txt\nendif()\nadd_subdirectory(src)\n```\n\n----------------------------------------\n\nTITLE: Emscripten Link Flags Configuration\nDESCRIPTION: Sets up Emscripten-specific linking flags including thread pool size, memory limits, filesystem support, and exported runtime methods.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/stream.wasm/CMakeLists.txt#2025-04-11_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET} PROPERTIES LINK_FLAGS \" \\\n    --bind \\\n    -s USE_PTHREADS=1 \\\n    -s PTHREAD_POOL_SIZE=8 \\\n    -s INITIAL_MEMORY=1024MB \\\n    -s TOTAL_MEMORY=1024MB \\\n    -s FORCE_FILESYSTEM=1 \\\n    -s EXPORTED_RUNTIME_METHODS=\\\"['print', 'printErr', 'ccall', 'cwrap']\\\" \\\n    ${EXTRA_FLAGS} \\\n    \")\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCL Profiling in CMake\nDESCRIPTION: Conditionally enables OpenCL profiling by adding a compile definition if GGML_OPENCL_PROFILING is set.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-opencl/CMakeLists.txt#2025-04-11_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif (GGML_OPENCL_PROFILING)\n    message(STATUS \"OpenCL profiling enabled (increases CPU overhead)\")\n    add_compile_definitions(GGML_OPENCL_PROFILING)\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Finding Required Metal Libraries and Frameworks in CMake\nDESCRIPTION: Finds the required Foundation, Metal, and MetalKit frameworks needed for Metal GPU acceleration in the GGML library.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-metal/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfind_library(FOUNDATION_LIBRARY Foundation REQUIRED)\nfind_library(METAL_FRAMEWORK    Metal      REQUIRED)\nfind_library(METALKIT_FRAMEWORK MetalKit   REQUIRED)\n\nmessage(STATUS \"Metal framework found\")\n```\n\n----------------------------------------\n\nTITLE: Configuring OpenCL Compile Definitions in CMake\nDESCRIPTION: Sets up standard compile definitions for the OpenCL backend, including SOA_Q and target version configurations.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-opencl/CMakeLists.txt#2025-04-11_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nadd_compile_definitions(GGML_OPENCL_SOA_Q)\nadd_compile_definitions(GGML_OPENCL_TARGET_VERSION=${GGML_OPENCL_TARGET_VERSION})\n```\n\n----------------------------------------\n\nTITLE: GGML Dependency and Options Configuration for whisper.cpp\nDESCRIPTION: Configures the GGML library dependency and overrides its options with whisper-specific ones. Includes a function to handle deprecated options and their transitions to new names.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/CMakeLists.txt#2025-04-11_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\n# Required for relocatable CMake package\ninclude(${CMAKE_CURRENT_SOURCE_DIR}/cmake/build-info.cmake)\n\n# override ggml options\nset(GGML_CCACHE             ${WHISPER_CCACHE})\nset(GGML_SANITIZE_THREAD    ${WHISPER_SANITIZE_THREAD})\nset(GGML_SANITIZE_ADDRESS   ${WHISPER_SANITIZE_ADDRESS})\nset(GGML_SANITIZE_UNDEFINED ${WHISPER_SANITIZE_UNDEFINED})\nset(GGML_ALL_WARNINGS       ${WHISPER_ALL_WARNINGS})\nset(GGML_FATAL_WARNINGS     ${WHISPER_FATAL_WARNINGS})\n\n# transition helpers\nfunction (whisper_option_depr TYPE OLD NEW)\n    if (${OLD})\n        message(${TYPE} \"${OLD} is deprecated and will be removed in the future.\\nUse ${NEW} instead\\n\")\n        set(${NEW} ON)\n    endif()\nendfunction()\n\nwhisper_option_depr(FATAL_ERROR WHISPER_CUBLAS              GGML_CUDA)\nwhisper_option_depr(WARNING     WHISPER_CUDA                GGML_CUDA)\nwhisper_option_depr(WARNING     WHISPER_KOMPUTE             GGML_KOMPUTE)\nwhisper_option_depr(WARNING     WHISPER_METAL               GGML_METAL)\nwhisper_option_depr(WARNING     WHISPER_METAL_EMBED_LIBRARY GGML_METAL_EMBED_LIBRARY)\nwhisper_option_depr(WARNING     WHISPER_NATIVE              GGML_NATIVE)\nwhisper_option_depr(WARNING     WHISPER_OPENMP              GGML_OPENMP)\nwhisper_option_depr(WARNING     WHISPER_RPC                 GGML_RPC)\nwhisper_option_depr(WARNING     WHISPER_SYCL                GGML_SYCL)\nwhisper_option_depr(WARNING     WHISPER_SYCL_F16            GGML_SYCL_F16)\n```\n\n----------------------------------------\n\nTITLE: Running Whisper Model Inference Benchmark on M1 Pro (No Flash Attention)\nDESCRIPTION: This snippet shows the command to run the Whisper model inference benchmark without flash attention and its results for different model sizes.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/scripts/bench-all-gg.txt#2025-04-11_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nmake -j && ./scripts/bench-all.sh 1 0 0\n```\n\n----------------------------------------\n\nTITLE: Building Whisper.cpp Libraries for Different Android ABIs\nDESCRIPTION: Creates multiple library targets based on the Android ABI. It builds the default 'whisper' target and additional targets for specific ARM architectures (arm64-v8a and armeabi-v7a) with optimized compiler options.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/whisper.android.java/app/src/main/jni/whisper/CMakeLists.txt#2025-04-11_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nbuild_library(\"whisper\") # Default target\n\nif (${ANDROID_ABI} STREQUAL \"arm64-v8a\")\n    build_library(\"whisper_v8fp16_va\")\nelseif (${ANDROID_ABI} STREQUAL \"armeabi-v7a\")\n    build_library(\"whisper_vfpv4\")\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Configuring Metal Compilation Options in CMake\nDESCRIPTION: Sets compile definitions for Metal configurations including debug mode and BF16 support.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-metal/CMakeLists.txt#2025-04-11_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif (GGML_METAL_NDEBUG)\n    add_compile_definitions(GGML_METAL_NDEBUG)\nendif()\n\nif (GGML_METAL_USE_BF16)\n    add_compile_definitions(GGML_METAL_USE_BF16)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring General Build Options\nDESCRIPTION: Defines general build options for GGML, including static linking, native optimization, link-time optimization, and ccache usage.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/CMakeLists.txt#2025-04-11_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\n# general\noption(GGML_STATIC \"ggml: static link libraries\"                     OFF)\noption(GGML_NATIVE \"ggml: optimize the build for the current system\" ${GGML_NATIVE_DEFAULT})\noption(GGML_LTO    \"ggml: enable link time optimization\"             OFF)\noption(GGML_CCACHE \"ggml: use ccache if available\"                   ON)\n```\n\n----------------------------------------\n\nTITLE: Compiling Metal Shaders with Custom Configuration in CMake\nDESCRIPTION: Compiles Metal shaders with custom flags for debugging, optimization, and macOS version compatibility when not embedding the Metal library.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-metal/CMakeLists.txt#2025-04-11_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nelse()\n    if (GGML_METAL_SHADER_DEBUG)\n        # custom command to do the following:\n        #   xcrun -sdk macosx metal    -fno-fast-math -c ggml-metal.metal -o ggml-metal.air\n        #   xcrun -sdk macosx metallib                   ggml-metal.air   -o default.metallib\n        #\n        # note: this is the only way I found to disable fast-math in Metal. it's ugly, but at least it works\n        #       disabling fast math is needed in order to pass tests/test-backend-ops\n        # note: adding -fno-inline fixes the tests when using MTL_SHADER_VALIDATION=1\n        # note: unfortunately, we have to call it default.metallib instead of ggml.metallib\n        #       ref: https://github.com/ggerganov/whisper.cpp/issues/1720\n        set(XC_FLAGS -fno-fast-math -fno-inline -g)\n    else()\n        set(XC_FLAGS -O3)\n    endif()\n\n    # Append macOS metal versioning flags\n    if (GGML_METAL_MACOSX_VERSION_MIN)\n        message(STATUS \"Adding  -mmacosx-version-min=${GGML_METAL_MACOSX_VERSION_MIN} flag to metal compilation\")\n        list   (APPEND XC_FLAGS -mmacosx-version-min=${GGML_METAL_MACOSX_VERSION_MIN})\n    endif()\n\n    if (GGML_METAL_STD)\n        message(STATUS \"Adding  -std=${GGML_METAL_STD} flag to metal compilation\")\n        list   (APPEND XC_FLAGS -std=${GGML_METAL_STD})\n    endif()\n\n    add_custom_command(\n        OUTPUT ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/default.metallib\n        COMMAND xcrun -sdk macosx metal ${XC_FLAGS} -c ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/ggml-metal.metal -o - |\n            xcrun -sdk macosx metallib - -o ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/default.metallib\n        COMMAND rm -f ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/ggml-common.h\n        COMMAND rm -f ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/ggml-metal.metal\n        DEPENDS ggml-metal.metal ${METALLIB_COMMON}\n        COMMENT \"Compiling Metal kernels\"\n        )\n\n    # FIXME: only add to the ggml-metal target?\n    add_custom_target(\n        ggml-metal-lib ALL\n        DEPENDS ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/default.metallib\n        )\nendif() # GGML_METAL_EMBED_LIBRARY\n```\n\n----------------------------------------\n\nTITLE: Configuring Windows-specific Build Settings in CMake\nDESCRIPTION: Sets up Windows-specific compiler definitions and export settings. Disables secure warnings and configures symbol exports for shared libraries on Windows platforms.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/src/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\n# TODO: should not use this\nif (WIN32)\n    add_compile_definitions(_CRT_SECURE_NO_WARNINGS)\n\n    if (BUILD_SHARED_LIBS)\n        set(CMAKE_WINDOWS_EXPORT_ALL_SYMBOLS ON)\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Node.js Addon Target\nDESCRIPTION: Sets up the basic configuration for the Node.js addon target, including NAPI version and compiler settings.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/addon.node/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET addon.node)\n\n# Base settings\n#==================================================================\n# env var supported by cmake-js\nadd_definitions(-DNAPI_VERSION=4)\ninclude_directories(${CMAKE_JS_INC})\n#==================================================================\n\nadd_library(${TARGET} SHARED ${CMAKE_JS_SRC} addon.cpp)\nset_target_properties(${TARGET} PROPERTIES PREFIX \"\" SUFFIX \".node\")\n\ninclude(DefaultTargetOptions)\n```\n\n----------------------------------------\n\nTITLE: Defining Library Build Function\nDESCRIPTION: Creates a function to build the shared library with specific compiler and linker options. Handles different target architectures and optimization levels.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/whisper.android/lib/src/main/jni/whisper/CMakeLists.txt#2025-04-11_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(build_library target_name)\n    add_library(\n        ${target_name}\n        SHARED\n        ${SOURCE_FILES}\n    )\n\n    target_compile_definitions(${target_name} PUBLIC GGML_USE_CPU)\n\n    if (${target_name} STREQUAL \"whisper_v8fp16_va\")\n        target_compile_options(${target_name} PRIVATE -march=armv8.2-a+fp16)\n        set(GGML_COMPILE_OPTIONS                      -march=armv8.2-a+fp16)\n    elseif (${target_name} STREQUAL \"whisper_vfpv4\")\n        target_compile_options(${target_name} PRIVATE -mfpu=neon-vfpv4)\n        set(GGML_COMPILE_OPTIONS                      -mfpu=neon-vfpv4)\n    endif ()\n\n    if (NOT ${CMAKE_BUILD_TYPE} STREQUAL \"Debug\")\n        target_compile_options(${target_name} PRIVATE -O3)\n        target_compile_options(${target_name} PRIVATE -fvisibility=hidden -fvisibility-inlines-hidden)\n        target_compile_options(${target_name} PRIVATE -ffunction-sections -fdata-sections)\n\n        target_link_options(${target_name} PRIVATE -Wl,--gc-sections)\n        target_link_options(${target_name} PRIVATE -Wl,--exclude-libs,ALL)\n        target_link_options(${target_name} PRIVATE -flto)\n    endif ()\n\n    if (GGML_HOME)\n        include(FetchContent)\n        FetchContent_Declare(ggml SOURCE_DIR ${GGML_HOME})\n        FetchContent_MakeAvailable(ggml)\n\n        target_compile_options(ggml PRIVATE ${GGML_COMPILE_OPTIONS})\n        target_link_libraries(${target_name} ${LOG_LIB} android ggml)\n    else()\n        target_link_libraries(${target_name} ${LOG_LIB} android)\n    endif()\n\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Configuring x86 Architecture Optimizations in CMake for GGML\nDESCRIPTION: This snippet configures compiler flags and definitions for x86 architecture. It conditionally adds appropriate flags for various instruction set extensions like SSE4.2, F16C, FMA, BMI2, AVX, AVX2, AVX-VNNI, AVX512 and its variants, and AMX instructions based on build configuration.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-cpu/CMakeLists.txt#2025-04-11_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif (GGML_NATIVE)\n    list(APPEND ARCH_FLAGS -march=native)\nelse ()\n    list(APPEND ARCH_FLAGS -msse4.2)\n    list(APPEND ARCH_DEFINITIONS GGML_SSE42)\n    if (GGML_F16C)\n        list(APPEND ARCH_FLAGS -mf16c)\n        list(APPEND ARCH_DEFINITIONS GGML_F16C)\n    endif()\n    if (GGML_FMA)\n        list(APPEND ARCH_FLAGS -mfma)\n        list(APPEND ARCH_DEFINITIONS GGML_FMA)\n    endif()\n    if (GGML_BMI2)\n        list(APPEND ARCH_FLAGS -mbmi2)\n        list(APPEND ARCH_DEFINITIONS GGML_BMI2)\n    endif()\n    if (GGML_AVX)\n        list(APPEND ARCH_FLAGS -mavx)\n        list(APPEND ARCH_DEFINITIONS GGML_AVX)\n    endif()\n    if (GGML_AVX2)\n        list(APPEND ARCH_FLAGS -mavx2)\n        list(APPEND ARCH_DEFINITIONS GGML_AVX2)\n    endif()\n    if (GGML_AVX_VNNI)\n        list(APPEND ARCH_FLAGS -mavxvnni)\n        list(APPEND ARCH_DEFINITIONS GGML_AVX_VNNI)\n    endif()\n    if (GGML_AVX512)\n        list(APPEND ARCH_FLAGS -mavx512f)\n        list(APPEND ARCH_FLAGS -mavx512cd)\n        list(APPEND ARCH_FLAGS -mavx512vl)\n        list(APPEND ARCH_FLAGS -mavx512dq)\n        list(APPEND ARCH_FLAGS -mavx512bw)\n        list(APPEND ARCH_DEFINITIONS GGML_AVX512)\n    endif()\n    if (GGML_AVX512_VBMI)\n        list(APPEND ARCH_FLAGS -mavx512vbmi)\n        list(APPEND ARCH_DEFINITIONS GGML_AVX512_VBMI)\n    endif()\n    if (GGML_AVX512_VNNI)\n        list(APPEND ARCH_FLAGS -mavx512vnni)\n        list(APPEND ARCH_DEFINITIONS GGML_AVX512_VNNI)\n    endif()\n    if (GGML_AVX512_BF16)\n        list(APPEND ARCH_FLAGS -mavx512bf16)\n        list(APPEND ARCH_DEFINITIONS GGML_AVX512_BF16)\n    endif()\n    if (GGML_AMX_TILE)\n        list(APPEND ARCH_FLAGS -mamx-tile)\n        list(APPEND ARCH_DEFINITIONS GGML_AMX_TILE)\n    endif()\n    if (GGML_AMX_INT8)\n        list(APPEND ARCH_FLAGS -mamx-int8)\n        list(APPEND ARCH_DEFINITIONS GGML_AMX_INT8)\n    endif()\n    if (GGML_AMX_BF16)\n        list(APPEND ARCH_FLAGS -mamx-bf16)\n        list(APPEND ARCH_DEFINITIONS GGML_AMX_BF16)\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring SDL2 Dependency in CMake\nDESCRIPTION: This code block handles finding and configuring the SDL2 library when WHISPER_SDL2 is enabled. It outputs the found include directories and libraries to the build log.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/CMakeLists.txt#2025-04-11_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif (WHISPER_SDL2)\n    # SDL2\n    find_package(SDL2 REQUIRED)\n\n    string(STRIP \"${SDL2_LIBRARIES}\" SDL2_LIBRARIES)\n\n    message(STATUS \"SDL2_INCLUDE_DIRS = ${SDL2_INCLUDE_DIRS}\")\n    message(STATUS \"SDL2_LIBRARIES    = ${SDL2_LIBRARIES}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Whisper Models on V100 GPU with Flash Attention Disabled\nDESCRIPTION: Runs performance tests on various Whisper models (tiny through large-v3-turbo) on V100 GPU with CUDA, using 8 threads and with flash attention disabled. Shows encoding, decoding, batch processing, and prompt processing times.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/scripts/bench-all-gg.txt#2025-04-11_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nWHISPER_CUDA=1 make -j && ./scripts/bench-all.sh 8 1 0\n```\n\n----------------------------------------\n\nTITLE: Setting Up Compiler Warning Flags for Multiple Languages\nDESCRIPTION: Configures compiler warning flags for C and C++ separately when WHISPER_ALL_WARNINGS is enabled. Includes different sets of flags for each language and handles platform-specific differences between MSVC and other compilers.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/src/CMakeLists.txt#2025-04-11_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif (WHISPER_ALL_WARNINGS)\n    if (NOT MSVC)\n        list(APPEND WARNING_FLAGS -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function)\n        list(APPEND C_FLAGS       -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes\n                                  -Werror=implicit-int -Werror=implicit-function-declaration)\n        list(APPEND CXX_FLAGS     -Wmissing-declarations -Wmissing-noreturn)\n\n        list(APPEND C_FLAGS   ${WARNING_FLAGS})\n        list(APPEND CXX_FLAGS ${WARNING_FLAGS})\n\n        add_compile_options(\"$<$<COMPILE_LANGUAGE:C>:${C_FLAGS}>\"\n                            \"$<$<COMPILE_LANGUAGE:CXX>:${CXX_FLAGS}>\")\n    else()\n        # todo : msvc\n        set(C_FLAGS   \"\")\n        set(CXX_FLAGS \"\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Including Node Addon API Headers\nDESCRIPTION: Executes Node.js to find the node-addon-api include directory and configures it for the target. Handles string manipulation to clean up the path output.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/addon.node/CMakeLists.txt#2025-04-11_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\n# Include N-API wrappers\n#==================================================================\nexecute_process(COMMAND node -p \"require('node-addon-api').include\"\n        WORKING_DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}\n        OUTPUT_VARIABLE NODE_ADDON_API_DIR\n        )\nstring(REPLACE \"\\n\" \"\" NODE_ADDON_API_DIR ${NODE_ADDON_API_DIR})\nstring(REPLACE \"\\\"\" \"\" NODE_ADDON_API_DIR ${NODE_ADDON_API_DIR})\ntarget_include_directories(${TARGET} PRIVATE ${NODE_ADDON_API_DIR})\n#==================================================================\n```\n\n----------------------------------------\n\nTITLE: Configuring CPU and Platform-Specific Options\nDESCRIPTION: Sets options for CPU architecture variants, ARM architecture settings, PowerPC CPU type, and Windows version.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/CMakeLists.txt#2025-04-11_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\noption(GGML_CPU_ALL_VARIANTS \"ggml: build all variants of the CPU backend (requires GGML_BACKEND_DL)\" OFF)\nset(GGML_CPU_ARM_ARCH        \"\" CACHE STRING \"ggml: CPU architecture for ARM\")\nset(GGML_CPU_POWERPC_CPUTYPE \"\" CACHE STRING \"ggml: CPU type for PowerPC\")\n\n\nif (WIN32)\n    set(GGML_WIN_VER \"0x602\" CACHE STRING   \"ggml: Windows version\")\nendif()\n\n# ggml core\nset(GGML_SCHED_MAX_COPIES  \"4\" CACHE STRING \"ggml: max input copies for pipeline parallelism\")\noption(GGML_CPU                             \"ggml: enable CPU backend\"                        ON)\n```\n\n----------------------------------------\n\nTITLE: Configuring PowerPC Architecture Optimizations in CMake for GGML\nDESCRIPTION: This snippet detects PowerPC processors and configures appropriate compiler flags. It determines the PowerPC generation (POWER9, POWER10) by reading system information and sets specific optimization flags based on the detected processor.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-cpu/CMakeLists.txt#2025-04-11_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nelseif (\"${CMAKE_SYSTEM_PROCESSOR} \" STREQUAL \"ppc64le \" OR \"${CMAKE_SYSTEM_PROCESSOR} \" STREQUAL \"powerpc \")\n    message(STATUS \"PowerPC detected\")\n    if (GGML_NATIVE)\n        if (${CMAKE_SYSTEM_PROCESSOR} MATCHES \"ppc64\")\n            file(READ \"/proc/cpuinfo\" POWER10_M)\n        elseif (${CMAKE_SYSTEM_PROCESSOR} MATCHES \"powerpc\")\n            execute_process(COMMAND bash -c \"prtconf |grep 'Implementation' | head -n 1\" OUTPUT_VARIABLE POWER10_M)\n        endif()\n\n        string(REGEX MATCHALL \"POWER *([0-9]+)\" MATCHED_STRING \"${POWER10_M}\")\n        string(REGEX REPLACE \"POWER *([0-9]+)\" \"\\\\1\" EXTRACTED_NUMBER \"${MATCHED_STRING}\")\n\n        if (EXTRACTED_NUMBER GREATER_EQUAL 10)\n            list(APPEND ARCH_FLAGS -mcpu=power10 -mpowerpc64)\n        elseif (EXTRACTED_NUMBER EQUAL 9)\n            list(APPEND ARCH_FLAGS -mcpu=power9 -mpowerpc64)\n        elseif (${CMAKE_SYSTEM_PROCESSOR} MATCHES \"ppc64le\")\n            list(APPEND ARCH_FLAGS -mcpu=powerpc64le -mtune=native)\n        else()\n            list(APPEND ARCH_FLAGS -mcpu=native -mtune=native -mpowerpc64)\n        endif()\n    else()\n        if (GGML_CPU_POWERPC_CPUTYPE)\n            list(APPEND ARCH_FLAGS -mcpu=${GGML_CPU_POWERPC_CPUTYPE})\n        endif()\n    endif()\n```\n\n----------------------------------------\n\nTITLE: Configuring RISC-V Architecture Optimizations in CMake for GGML\nDESCRIPTION: This snippet sets up compiler flags for RISC-V processors. It specifically configures the vector extension (RVV) and half-precision floating-point extension (Zfh) flags when enabled in the build.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-cpu/CMakeLists.txt#2025-04-11_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nelseif (${CMAKE_SYSTEM_PROCESSOR} MATCHES \"riscv64\")\n    message(STATUS \"RISC-V detected\")\n    if (GGML_RVV)\n        if (GGML_RV_ZFH)\n            list(APPEND ARCH_FLAGS -march=rv64gcv_zfhmin -DGGML_RV_ZFH -mabi=lp64d)\n        else()\n            list(APPEND ARCH_FLAGS -march=rv64gcv -mabi=lp64d)\n        endif()\n    endif()\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Whisper Models on M4 Max with Flash Attention Enabled\nDESCRIPTION: Runs performance tests on various Whisper models (tiny through large-v2) on M4 Max with METAL, using 1 thread and with flash attention enabled. Shows encoding, decoding, batch processing, and prompt processing times.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/scripts/bench-all-gg.txt#2025-04-11_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nmake -j && ./scripts/bench-all.sh 1 1 1\n```\n\n----------------------------------------\n\nTITLE: Building whisper.openvino Library\nDESCRIPTION: Defines and configures the whisper.openvino library component when OpenVINO integration is enabled. Sets up source files, include directories, and necessary compiler flags and dependencies.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/src/CMakeLists.txt#2025-04-11_snippet_5\n\nLANGUAGE: cmake\nCODE:\n```\nif (WHISPER_OPENVINO)\n    set(TARGET whisper.openvino)\n\n    add_library(${TARGET} OBJECT\n        openvino/whisper-openvino-encoder.h\n        openvino/whisper-openvino-encoder.cpp\n        )\n\n    target_include_directories(${TARGET} PUBLIC\n        .\n        )\n\n    set_property(TARGET ${TARGET} PROPERTY POSITION_INDEPENDENT_CODE ON)\n    set(WHISPER_EXTRA_FLAGS ${WHISPER_EXTRA_FLAGS} -DWHISPER_USE_OPENVINO)\n\n    target_link_libraries(${TARGET} PRIVATE ggml openvino::runtime)\n    set_target_properties(${TARGET} PROPERTIES FOLDER \"libs\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Linking Libraries and MSVC Configuration\nDESCRIPTION: Links required libraries to the target and handles MSVC-specific configuration for generating node.lib when needed.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/addon.node/CMakeLists.txt#2025-04-11_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(${TARGET} ${CMAKE_JS_LIB} common whisper ${CMAKE_THREAD_LIBS_INIT})\n\nif(MSVC AND CMAKE_JS_NODELIB_DEF AND CMAKE_JS_NODELIB_TARGET)\n    # Generate node.lib\n    execute_process(COMMAND ${CMAKE_AR} /def:${CMAKE_JS_NODELIB_DEF} /out:${CMAKE_JS_NODELIB_TARGET} ${CMAKE_STATIC_LINKER_FLAGS})\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Emscripten Link Flags for WebAssembly in CMake\nDESCRIPTION: Configures the Emscripten linker flags for the WebAssembly target. Enables threading, sets memory limits, configures filesystem support, and specifies exported JavaScript methods.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/command.wasm/CMakeLists.txt#2025-04-11_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET} PROPERTIES LINK_FLAGS \" \\\n    --bind \\\n    -s USE_PTHREADS=1 \\\n    -s PTHREAD_POOL_SIZE=8 \\\n    -s INITIAL_MEMORY=1024MB \\\n    -s TOTAL_MEMORY=1024MB \\\n    -s FORCE_FILESYSTEM=1 \\\n    -s EXPORTED_RUNTIME_METHODS=\\\"['print', 'printErr', 'ccall', 'cwrap']\\\" \\\n    ${EXTRA_FLAGS} \\\n    \")\n```\n\n----------------------------------------\n\nTITLE: CANN Backend Library Configuration\nDESCRIPTION: Configures CANN backend including platform checks, setting include directories, libraries, and compile definitions. Supports Linux on x86-64 and arm64 platforms.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-cann/CMakeLists.txt#2025-04-11_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nif (CANN_INSTALL_DIR)\n    # Only Support Linux.\n    if (NOT UNIX)\n        message(FATAL_ERROR \"CANN: CANN toolkit supports unix but not ${CMAKE_SYSTEM_NAME}\")\n    endif()\n\n    # Supported platforms: x86-64, arm64\n    if (CMAKE_SYSTEM_PROCESSOR STREQUAL \"aarch64\")\n    elseif (CMAKE_SYSTEM_PROCESSOR STREQUAL \"x86_64\" OR CMAKE_SYSTEM_PROCESSOR STREQUAL \"amd64\")\n    else()\n        message(FATAL_ERROR \"CANN: CANN toolkit supports x86-64 and arm64 but not ${CMAKE_SYSTEM_PROCESSOR}\")\n    endif()\n\n    # Set header and libs\n    set(CANN_INCLUDE_DIRS\n        ${CANN_INSTALL_DIR}/include\n        ${CANN_INSTALL_DIR}/include/aclnn\n        ${CANN_INSTALL_DIR}/acllib/include\n    )\n\n    list(APPEND CANN_LIBRARIES\n        ascendcl\n        nnopbase\n        opapi\n        acl_op_compiler\n    )\n\n    file(GLOB GGML_SOURCES_CANN \"*.cpp\")\n\n    ggml_add_backend_library(ggml-cann ${GGML_SOURCES_CANN})\n    target_link_libraries(ggml-cann PRIVATE ${CANN_LIBRARIES})\n    target_include_directories(ggml-cann PRIVATE ${CANN_INCLUDE_DIRS})\n    target_link_directories(ggml-cann PRIVATE ${CANN_INSTALL_DIR}/lib64)\n\n    target_compile_definitions(ggml-cann PRIVATE \"-D${SOC_TYPE_COMPILE_OPTION}\")\n\n    message(STATUS \"CANN: CANN_INCLUDE_DIRS =  ${CANN_INCLUDE_DIRS}\")\n    message(STATUS \"CANN: CANN_LIBRARIES =  ${CANN_LIBRARIES}\")\nelse()\n    message(FATAL_ERROR \"CANN: Can't find CANN_INSTALL_DIR, did you forget to source set_var.sh?\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining and Configuring libbench Executable in CMake for WebAssembly Compilation\nDESCRIPTION: Sets up the libbench executable target with its source file and links it to the whisper library. Configures build options and default target options.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/bench.wasm/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET libbench)\n\nadd_executable(${TARGET}\n    emscripten.cpp\n    )\n\ninclude(DefaultTargetOptions)\n\ntarget_link_libraries(${TARGET} PRIVATE\n    whisper\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring RPC Backend in CMake for GGML\nDESCRIPTION: This CMake script configures the RPC backend for GGML. It adds the ggml-rpc library and links the ws2_32 library on Windows platforms.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-rpc/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nmessage(STATUS \"Using RPC backend\")\n\nggml_add_backend_library(ggml-rpc\n                         ggml-rpc.cpp\n                        )\n\nif (WIN32)\n    target_link_libraries(ggml-rpc PRIVATE ws2_32)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Single-File WASM Option for Emscripten\nDESCRIPTION: Adds an option to embed the WASM binary inside the JavaScript file as a single file. When enabled, it copies the built JavaScript file to the output directory and adds the necessary compile flags.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/whisper.wasm/CMakeLists.txt#2025-04-11_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nunset(EXTRA_FLAGS)\n\nif (WHISPER_WASM_SINGLE_FILE)\n    set(EXTRA_FLAGS \"-s SINGLE_FILE=1\")\n    message(STATUS \"Embedding WASM inside main.js\")\n\n    add_custom_command(\n        TARGET ${TARGET} POST_BUILD\n        COMMAND ${CMAKE_COMMAND} -E copy\n        ${CMAKE_BINARY_DIR}/bin/libmain.js\n        ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/whisper.wasm/main.js\n        )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Whisper.cpp Node.js Addon\nDESCRIPTION: Command to install the necessary dependencies for the Whisper.cpp Node.js addon using npm.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/addon.node/README.md#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpm install\n```\n\n----------------------------------------\n\nTITLE: Configuring and Building the whisper-bench Executable with CMake\nDESCRIPTION: This CMake script defines the whisper-bench target, creates an executable from bench.cpp, applies default target options, links necessary libraries including whisper and thread libraries, and sets up installation rules for the target.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/bench/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET whisper-bench)\nadd_executable(${TARGET} bench.cpp)\n\ninclude(DefaultTargetOptions)\n\ntarget_link_libraries(${TARGET} PRIVATE whisper ${CMAKE_THREAD_LIBS_INIT})\n\ninstall(TARGETS ${TARGET} RUNTIME)\n```\n\n----------------------------------------\n\nTITLE: Configuring Sanitizer Options in CMake\nDESCRIPTION: Sets up compiler and linker flags for thread, address and undefined behavior sanitizers when not using MSVC compiler.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (NOT MSVC)\n    if (GGML_SANITIZE_THREAD)\n        add_compile_options(-fsanitize=thread)\n        link_libraries     (-fsanitize=thread)\n    endif()\n\n    if (GGML_SANITIZE_ADDRESS)\n        add_compile_options(-fsanitize=address -fno-omit-frame-pointer)\n        link_libraries     (-fsanitize=address)\n    endif()\n\n    if (GGML_SANITIZE_UNDEFINED)\n        add_compile_options(-fsanitize=undefined)\n        link_libraries     (-fsanitize=undefined)\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring HIP Compiler Settings in CMake\nDESCRIPTION: Detects whether hipcc is being used as the C++ compiler and sets up appropriate HIP language support. Handles platform-specific differences between Windows and Linux.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-hip/CMakeLists.txt#2025-04-11_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\n# CMake on Windows doesn't support the HIP language yet\nif (WIN32)\n    set(CXX_IS_HIPCC TRUE)\nelse()\n    string(REGEX MATCH \"hipcc(\\.bat)?$\" CXX_IS_HIPCC \"${CMAKE_CXX_COMPILER}\")\nendif()\n\nif (CXX_IS_HIPCC)\n    if (LINUX)\n        if (NOT ${CMAKE_CXX_COMPILER_ID} MATCHES \"Clang\")\n            message(WARNING \"Only LLVM is supported for HIP, hint: CXX=/opt/rocm/llvm/bin/clang++\")\n        endif()\n\n        message(WARNING \"Setting hipcc as the C++ compiler is legacy behavior.\"\n                \" Prefer setting the HIP compiler directly. See README for details.\")\n    endif()\nelse()\n    # Forward AMDGPU_TARGETS to CMAKE_HIP_ARCHITECTURES.\n    if (AMDGPU_TARGETS AND NOT CMAKE_HIP_ARCHITECTURES)\n        set(CMAKE_HIP_ARCHITECTURES ${AMDGPU_TARGETS})\n    endif()\n    cmake_minimum_required(VERSION 3.21)\n    enable_language(HIP)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring Emscripten JavaScript Build and NPM Publishing in CMake\nDESCRIPTION: Configures the JavaScript build process for Emscripten compilation, adding the javascript subdirectory and setting up NPM package publishing. Creates a custom command to publish the package to NPM when specified dependencies are updated.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/bindings/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (EMSCRIPTEN)\n    add_subdirectory(javascript)\n\n    add_custom_command(\n        OUTPUT ${CMAKE_CURRENT_SOURCE_DIR}/javascript/publish.log\n        DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/javascript/whisper.js\n        DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/javascript/libwhisper.worker.js\n        DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/javascript/package.json\n        WORKING_DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}/javascript\n        COMMAND npm publish\n        COMMAND touch publish.log\n        COMMENT \"Publishing npm module v${PROJECT_VERSION}\"\n        VERBATIM\n        )\n\n    add_custom_target(publish-npm\n        DEPENDS javascript/publish.log\n        )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Warning Configuration in CMake\nDESCRIPTION: Configures compiler warning flags for different compilers including GCC, Clang and MSVC. Enables various warning levels and treats warnings as errors when specified.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-11_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nif (GGML_ALL_WARNINGS)\n    if (NOT MSVC)\n        list(APPEND WARNING_FLAGS -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function)\n        list(APPEND C_FLAGS       -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes\n                                  -Werror=implicit-int -Werror=implicit-function-declaration)\n        list(APPEND CXX_FLAGS     -Wmissing-declarations -Wmissing-noreturn)\n\n        list(APPEND C_FLAGS   ${WARNING_FLAGS})\n        list(APPEND CXX_FLAGS ${WARNING_FLAGS})\n\n        ggml_get_flags(${CMAKE_CXX_COMPILER_ID} ${CMAKE_CXX_COMPILER_VERSION})\n\n        add_compile_options(\"$<$<COMPILE_LANGUAGE:C>:${C_FLAGS};${GF_C_FLAGS}>\"\n                            \"$<$<COMPILE_LANGUAGE:CXX>:${CXX_FLAGS};${GF_CXX_FLAGS}>\")\n    else()\n        # todo : msvc\n        set(C_FLAGS   \"\")\n        set(CXX_FLAGS \"\")\n    endif()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring GGML CMake Project\nDESCRIPTION: Sets up the basic CMake project configuration for GGML, including minimum CMake version, project name, build type settings, and output directories.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\ncmake_minimum_required(VERSION 3.14) # for add_link_options and implicit target directories.\nproject(\"ggml\" C CXX)\ninclude(CheckIncludeFileCXX)\n\nset(CMAKE_EXPORT_COMPILE_COMMANDS ON)\n\nif (NOT XCODE AND NOT MSVC AND NOT CMAKE_BUILD_TYPE)\n    set(CMAKE_BUILD_TYPE Release CACHE STRING \"Build type\" FORCE)\n    set_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS \"Debug\" \"Release\" \"MinSizeRel\" \"RelWithDebInfo\")\nendif()\n\nif (CMAKE_SOURCE_DIR STREQUAL CMAKE_CURRENT_SOURCE_DIR)\n    set(GGML_STANDALONE ON)\n\n    set(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/bin)\n\n    # configure project version\n    # TODO\nelse()\n    set(GGML_STANDALONE OFF)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring LoongArch64 Architecture Optimizations in CMake for GGML\nDESCRIPTION: This snippet configures compiler flags for LoongArch64 processors. It sets the base architecture flag and conditionally adds LASX and LSX instruction set extensions if they are enabled in the build configuration.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-cpu/CMakeLists.txt#2025-04-11_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nelseif (${CMAKE_SYSTEM_PROCESSOR} MATCHES \"loongarch64\")\n    message(STATUS \"loongarch64 detected\")\n\n    list(APPEND ARCH_FLAGS -march=loongarch64)\n    if (GGML_LASX)\n        list(APPEND ARCH_FLAGS -mlasx)\n    endif()\n    if (GGML_LSX)\n        list(APPEND ARCH_FLAGS -mlsx)\n    endif()\n```\n\n----------------------------------------\n\nTITLE: Fallback BLAS Include Directory Detection\nDESCRIPTION: Attempts to find cblas.h from common locations when pkg-config fails to locate BLAS include directories. This is a last resort approach to find the necessary headers for compilation.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-blas/CMakeLists.txt#2025-04-11_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\n        if (DepBLAS_FOUND)\n            set(BLAS_INCLUDE_DIRS ${DepBLAS_INCLUDE_DIRS})\n        else()\n            message(WARNING \"BLAS_INCLUDE_DIRS neither been provided nor been automatically\"\n            \" detected by pkgconfig, trying to find cblas.h from possible paths...\")\n            find_path(BLAS_INCLUDE_DIRS\n                NAMES cblas.h\n                HINTS\n                    /usr/include\n                    /usr/local/include\n                    /usr/include/openblas\n                    /opt/homebrew/opt/openblas/include\n                    /usr/local/opt/openblas/include\n                    /usr/include/x86_64-linux-gnu/openblas/include\n            )\n        endif()\n```\n\n----------------------------------------\n\nTITLE: Defining SDL2-dependent Common-SDL Library in CMake\nDESCRIPTION: This conditional block creates a 'common-sdl' library target when SDL2 is enabled. It links with SDL2 libraries and sets appropriate target properties.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/CMakeLists.txt#2025-04-11_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nif (WHISPER_SDL2)\n    # common-sdl\n\n    set(TARGET common-sdl)\n\n    add_library(${TARGET} STATIC\n        common-sdl.h\n        common-sdl.cpp\n        )\n\n    include(DefaultTargetOptions)\n\n    target_include_directories(${TARGET} PUBLIC  ${SDL2_INCLUDE_DIRS})\n    target_link_libraries     (${TARGET} PRIVATE ${SDL2_LIBRARIES})\n\n    set_target_properties(${TARGET} PROPERTIES POSITION_INDEPENDENT_CODE ON)\n    set_target_properties(${TARGET} PROPERTIES FOLDER \"libs\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Detecting Host Compiler in CMake\nDESCRIPTION: Function to detect the host C and C++ compilers, considering different system names and compiler options.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-vulkan/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(detect_host_compiler)\n    if (CMAKE_HOST_SYSTEM_NAME STREQUAL \"Windows\")\n        find_program(HOST_C_COMPILER NAMES cl gcc clang NO_CMAKE_FIND_ROOT_PATH)\n        find_program(HOST_CXX_COMPILER NAMES cl g++ clang++ NO_CMAKE_FIND_ROOT_PATH)\n    else()\n        find_program(HOST_C_COMPILER NAMES gcc clang NO_CMAKE_FIND_ROOT_PATH)\n        find_program(HOST_CXX_COMPILER NAMES g++ clang++ NO_CMAKE_FIND_ROOT_PATH)\n    endif()\n    set(HOST_C_COMPILER \"${HOST_C_COMPILER}\" PARENT_SCOPE)\n    set(HOST_CXX_COMPILER \"${HOST_CXX_COMPILER}\" PARENT_SCOPE)\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Configuring libcommand Target for WebAssembly in CMake\nDESCRIPTION: Sets up the libcommand executable target that compiles the Whisper speech recognition library to WebAssembly. It defines source files, links required libraries, and configures compiler options.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/command.wasm/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET libcommand)\n\nadd_executable(${TARGET}\n    emscripten.cpp\n    )\n\ninclude(DefaultTargetOptions)\n\ntarget_link_libraries(${TARGET} PRIVATE\n    common\n    whisper\n    )\n```\n\n----------------------------------------\n\nTITLE: Building with CUDA Support for Whisper.cpp Go Bindings\nDESCRIPTION: Command to build example applications with CUDA GPU acceleration support enabled.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/bindings/go/README.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nGGML_CUDA=1 make examples\n```\n\n----------------------------------------\n\nTITLE: Configuring Adreno-Optimized Kernels in CMake\nDESCRIPTION: Conditionally enables optimized matrix multiplication kernels for Adreno GPUs if GGML_OPENCL_USE_ADRENO_KERNELS is set.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-opencl/CMakeLists.txt#2025-04-11_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nif (GGML_OPENCL_USE_ADRENO_KERNELS)\n    message(STATUS \"OpenCL will use matmul kernels optimized for Adreno\")\n    add_compile_definitions(GGML_OPENCL_USE_ADRENO_KERNELS)\nendif ()\n```\n\n----------------------------------------\n\nTITLE: Configuring WebAssembly Single File Option for libbench\nDESCRIPTION: Handles the WHISPER_WASM_SINGLE_FILE option which embeds the WebAssembly code directly in the JavaScript file. Adds a post-build command to copy the compiled JavaScript file to the output directory.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/bench.wasm/CMakeLists.txt#2025-04-11_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nunset(EXTRA_FLAGS)\n\nif (WHISPER_WASM_SINGLE_FILE)\n    set(EXTRA_FLAGS \"-s SINGLE_FILE=1\")\n    message(STATUS \"Embedding WASM inside bench.js\")\n\n    add_custom_command(\n        TARGET ${TARGET} POST_BUILD\n        COMMAND ${CMAKE_COMMAND} -E copy\n        ${CMAKE_BINARY_DIR}/bin/libbench.js\n        ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/bench.wasm/bench.js\n        )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring CUDA Compilation Flags in CMake\nDESCRIPTION: Sets up CUDA-specific compilation flags, including fast math, compression mode, and warning levels.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-cuda/CMakeLists.txt#2025-04-11_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset(CUDA_FLAGS -use_fast_math)\n\nif (CUDAToolkit_VERSION VERSION_GREATER_EQUAL \"12.8\")\n    list(APPEND CUDA_FLAGS -compress-mode=${GGML_CUDA_COMPRESSION_MODE})\nendif()\n\nif (GGML_FATAL_WARNINGS)\n    list(APPEND CUDA_FLAGS -Werror all-warnings)\nendif()\n\ntarget_compile_options(ggml-cuda PRIVATE \"$<$<COMPILE_LANGUAGE:CUDA>:${CUDA_FLAGS}>\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Vulkan Shader Generator in CMake for whisper.cpp\nDESCRIPTION: This CMake snippet configures the build for a Vulkan shader generator. It includes thread support, adds conditional compilation definitions for Vulkan cooperative matrix features, sets up the build target, and specifies installation and linking options.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-vulkan/vulkan-shaders/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package (Threads REQUIRED)\n\nif (GGML_VULKAN_COOPMAT_GLSLC_SUPPORT)\n    add_compile_definitions(GGML_VULKAN_COOPMAT_GLSLC_SUPPORT)\nendif()\nif (GGML_VULKAN_COOPMAT2_GLSLC_SUPPORT)\n    add_compile_definitions(GGML_VULKAN_COOPMAT2_GLSLC_SUPPORT)\nendif()\nset(TARGET vulkan-shaders-gen)\nadd_executable(${TARGET} vulkan-shaders-gen.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\ntarget_link_libraries(vulkan-shaders-gen PUBLIC Threads::Threads)\n```\n\n----------------------------------------\n\nTITLE: Configuring libmain Executable Target for Emscripten Compilation\nDESCRIPTION: Sets up the 'libmain' target as an executable that compiles emscripten.cpp and links it with the whisper library. Includes default target options and defines the build specifications.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/whisper.wasm/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET libmain)\n\nadd_executable(${TARGET}\n    emscripten.cpp\n    )\n\ninclude(DefaultTargetOptions)\n\ntarget_link_libraries(${TARGET} PRIVATE\n    whisper\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Inference Parameters\nDESCRIPTION: Example of creating an evaluation configuration file to customize the Whisper model and inference parameters used during testing.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/tests/librispeech/README.md#2025-04-11_snippet_5\n\nLANGUAGE: conf\nCODE:\n```\nWHISPER_MODEL = large-v3-turbo\nWHISPER_FLAGS = --no-prints --threads 8 --language en --output-txt\n```\n\n----------------------------------------\n\nTITLE: Configuring Quantize Executable in CMake for Whisper.cpp\nDESCRIPTION: This CMake snippet defines and configures the 'quantize' executable. It sets the target name, adds the executable, includes default target options, and links the necessary libraries including 'common', 'whisper', and threading libraries.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/quantize/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET quantize)\nadd_executable(${TARGET} quantize.cpp)\n\ninclude(DefaultTargetOptions)\n\ntarget_link_libraries(${TARGET} PRIVATE common whisper ${CMAKE_THREAD_LIBS_INIT})\n```\n\n----------------------------------------\n\nTITLE: Configuring AMX Support for GGML Library in CMake\nDESCRIPTION: This snippet checks for x86_64 architecture and GCC compiler version to enable AMX support. It sets up the necessary files, libraries, and compiler flags for AMX functionality.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-amx/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif (CMAKE_OSX_ARCHITECTURES STREQUAL \"x86_64\" OR CMAKE_GENERATOR_PLATFORM_LWR MATCHES \"^(x86_64|i686|amd64|x64|win32)$\" OR\n        (NOT CMAKE_OSX_ARCHITECTURES AND NOT CMAKE_GENERATOR_PLATFORM_LWR AND\n         CMAKE_SYSTEM_PROCESSOR MATCHES \"^(x86_64|i686|AMD64)$\") AND\n        CMAKE_COMPILER_IS_GNUCC AND CMAKE_CXX_COMPILER_VERSION VERSION_GREATER 11.0)\n    message(STATUS \"Using AMX\")\n\n    file(GLOB   GGML_HEADERS_AMX \"*.h\")\n    list(APPEND GGML_HEADERS_AMX \"../../include/ggml-amx.h\")\n\n    file(GLOB   GGML_SOURCES_AMX \"*.cpp\")\n\n    add_library(ggml-amx\n                ${GGML_HEADERS_AMX}\n                ${GGML_SOURCES_AMX})\n\n    target_link_libraries(ggml-amx PRIVATE ggml-base)\n    target_include_directories(ggml-amx PRIVATE . ..)\n\n    # this is duplicated from the CPU backend, since the AMX backend also depends on the architecture flags\n    # TODO: integrate AMX backend into the CPU backend\n    if (MSVC)\n        # instruction set detection for MSVC only\n        if (GGML_NATIVE)\n            # TODO: improve, should not reference files from the parent folder\n            include(../ggml-cpu/cmake/FindSIMD.cmake)\n        endif ()\n        if (GGML_AVX512)\n            list(APPEND ARCH_FLAGS /arch:AVX512)\n            # MSVC has no compile-time flags enabling specific\n            # AVX512 extensions, neither it defines the\n            # macros corresponding to the extensions.\n            # Do it manually.\n            if (GGML_AVX512_VBMI)\n                add_compile_definitions($<$<COMPILE_LANGUAGE:C>:__AVX512VBMI__>)\n                add_compile_definitions($<$<COMPILE_LANGUAGE:CXX>:__AVX512VBMI__>)\n            endif()\n            if (GGML_AVX512_VNNI)\n                add_compile_definitions($<$<COMPILE_LANGUAGE:C>:__AVX512VNNI__>)\n                add_compile_definitions($<$<COMPILE_LANGUAGE:CXX>:__AVX512VNNI__>)\n            endif()\n            if (GGML_AVX512_BF16)\n                add_compile_definitions($<$<COMPILE_LANGUAGE:C>:__AVX512BF16__>)\n                add_compile_definitions($<$<COMPILE_LANGUAGE:CXX>:__AVX512BF16__>)\n            endif()\n            if (GGML_AMX_TILE)\n                add_compile_definitions($<$<COMPILE_LANGUAGE:C>:__AMX_TILE__>)\n                add_compile_definitions($<$<COMPILE_LANGUAGE:CXX>:__AMX_TILE__>)\n            endif()\n            if (GGML_AMX_INT8)\n                add_compile_definitions($<$<COMPILE_LANGUAGE:C>:__AMX_INT8__>)\n                add_compile_definitions($<$<COMPILE_LANGUAGE:CXX>:__AMX_INT8__>)\n            endif()\n            if (GGML_AMX_BF16)\n                add_compile_definitions($<$<COMPILE_LANGUAGE:C>:__AMX_BF16__>)\n                add_compile_definitions($<$<COMPILE_LANGUAGE:CXX>:__AMX_BF16__>)\n            endif()\n        elseif (GGML_AVX2)\n            list(APPEND ARCH_FLAGS /arch:AVX2)\n        elseif (GGML_AVX)\n            list(APPEND ARCH_FLAGS /arch:AVX)\n        endif()\n    else()\n        if (GGML_NATIVE)\n            list(APPEND ARCH_FLAGS -march=native)\n        endif()\n        if (GGML_F16C)\n            list(APPEND ARCH_FLAGS -mf16c)\n        endif()\n        if (GGML_FMA)\n            list(APPEND ARCH_FLAGS -mfma)\n        endif()\n        if (GGML_AVX)\n            list(APPEND ARCH_FLAGS -mavx)\n        endif()\n        if (GGML_AVX2)\n            list(APPEND ARCH_FLAGS -mavx2)\n        endif()\n        if (GGML_AVX512)\n            list(APPEND ARCH_FLAGS -mavx512f)\n            list(APPEND ARCH_FLAGS -mavx512dq)\n            list(APPEND ARCH_FLAGS -mavx512bw)\n        endif()\n        if (GGML_AVX512_VBMI)\n            list(APPEND ARCH_FLAGS -mavx512vbmi)\n        endif()\n        if (GGML_AVX512_VNNI)\n            list(APPEND ARCH_FLAGS -mavx512vnni)\n        endif()\n        if (GGML_AVX512_BF16)\n            list(APPEND ARCH_FLAGS -mavx512bf16)\n        endif()\n        if (GGML_AMX_TILE)\n            list(APPEND ARCH_FLAGS -mamx-tile)\n        endif()\n        if (GGML_AMX_INT8)\n            list(APPEND ARCH_FLAGS -mamx-int8)\n        endif()\n        if (GGML_AMX_BF16)\n            list(APPEND ARCH_FLAGS -mamx-bf16)\n        endif()\n    endif()\n\n    target_compile_options(ggml-amx PRIVATE ${ARCH_FLAGS})\nelse()\n    set(GGML_AMX OFF PARENT_SCOPE)\n    message(WARNING \"AMX requires x86 and gcc version > 11.0. Turning off GGML_AMX.\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories for Whisper.cpp Build\nDESCRIPTION: Specifies the include directories for the Whisper.cpp library build, including the main library directory, source files, and GGML-related headers.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/whisper.android.java/app/src/main/jni/whisper/CMakeLists.txt#2025-04-11_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\ninclude_directories(${WHISPER_LIB_DIR})\ninclude_directories(${WHISPER_LIB_DIR}/src)\ninclude_directories(${WHISPER_LIB_DIR}/include)\ninclude_directories(${WHISPER_LIB_DIR}/ggml/include)\ninclude_directories(${WHISPER_LIB_DIR}/ggml/src)\ninclude_directories(${WHISPER_LIB_DIR}/ggml/src/ggml-cpu)\n```\n\n----------------------------------------\n\nTITLE: Creating Ascend NPU Kernel Library in CMake\nDESCRIPTION: This snippet uses the ascendc_library command to create a static library named 'ascendc_kernels' from the previously collected source files.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-cann/kernels/CMakeLists.txt#2025-04-11_snippet_3\n\nLANGUAGE: CMake\nCODE:\n```\nascendc_library(ascendc_kernels STATIC\n    ${SRC_FILES}\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring libstream Target\nDESCRIPTION: Sets up the libstream executable target with Whisper library dependency and basic build configuration.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/stream.wasm/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET libstream)\n\nadd_executable(${TARGET}\n    emscripten.cpp\n    )\n\ninclude(DefaultTargetOptions)\n\ntarget_link_libraries(${TARGET} PRIVATE\n    whisper\n    )\n```\n\n----------------------------------------\n\nTITLE: Configuring Vim Plugin for Whisper.cpp\nDESCRIPTION: Vim configuration snippet for setting up the Whisper.cpp plugin, including path configuration and keybindings for transcription commands.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/lsp/README.md#2025-04-11_snippet_1\n\nLANGUAGE: vim\nCODE:\n```\nlet g:whisper_dir = \"~/whisper.cpp\"\n\" Start listening for commands when Ctrl - g is pressed in normal mode\nnnoremap <C-G> call whisper#requestCommands()<CR>\n\" Start unguided transcription when Ctrl - g is pressed in insert mode\ninoremap <C-G> <Cmd>call whisper#doTranscription()<CR>\n```\n\n----------------------------------------\n\nTITLE: Installing the Whisper.nvim Script\nDESCRIPTION: Commands to copy the whisper.nvim script to a directory in your PATH and make it executable.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/whisper.nvim/README.md#2025-04-11_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncp examples/whisper.nvim/whisper.nvim ~/bin/\nchmod u+x ~/bin/whisper.nvim\n```\n\n----------------------------------------\n\nTITLE: Defining CMake Executables for Whisper.cpp\nDESCRIPTION: This CMake snippet defines multiple executable targets for the Whisper.cpp project. It includes mandatory targets 'main' and 'bench', and conditional targets 'stream' and 'command' that are only built if SDL2 is enabled.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/deprecation-warning/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_executable(main ./deprecation-warning.cpp)\nadd_executable(bench ./deprecation-warning.cpp)\nif (WHISPER_SDL2)\n    add_executable(stream ./deprecation-warning.cpp)\n    add_executable(command ./deprecation-warning.cpp)\nendif()\n```\n\n----------------------------------------\n\nTITLE: CANN Installation Directory Detection\nDESCRIPTION: Checks for CANN installation directory using environment variables CANN_INSTALL_DIR or ASCEND_TOOLKIT_HOME.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-cann/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (\"cann${CANN_INSTALL_DIR}\" STREQUAL \"cann\" AND DEFINED ENV{ASCEND_TOOLKIT_HOME})\n    set(CANN_INSTALL_DIR $ENV{ASCEND_TOOLKIT_HOME})\n    message(STATUS \"CANN: updated CANN_INSTALL_DIR from ASCEND_TOOLKIT_HOME=$ENV{ASCEND_TOOLKIT_HOME}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Setting Compilation Definitions for Ascend NPU Kernels in CMake\nDESCRIPTION: This snippet sets compilation definitions for the Ascend NPU kernels. It includes a status message about the compilation settings and uses ascendc_compile_definitions to set private compile definitions.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-cann/kernels/CMakeLists.txt#2025-04-11_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\nmessage(STATUS \"CANN: compile ascend kernels witch SOC_TYPE:${SOC_TYPE}, SOC_VERSION:${SOC_VERSION}, compile macro:-D${SOC_TYPE_COMPILE_OPTION}.\")\nascendc_compile_definitions(ascendc_kernels PRIVATE \"-D${SOC_TYPE_COMPILE_OPTION}\")\n# ascendc_compile_definitions(ascendc_kernels PRIVATE -DASCENDC_DUMP)\n```\n\n----------------------------------------\n\nTITLE: Configuring SYCL Device Lister Executable in CMake\nDESCRIPTION: Defines CMake build instructions for the ls-sycl-device utility, which likely lists available SYCL-compatible devices. The executable requires the common and whisper libraries, threading support, and C++17 features.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/sycl/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\n#  MIT license\n#  Copyright (C) 2024 Intel Corporation\n#  SPDX-License-Identifier: MIT\n\nset(TARGET ls-sycl-device)\nadd_executable(${TARGET} ls-sycl-device.cpp)\ninstall(TARGETS ${TARGET} RUNTIME)\ntarget_link_libraries(${TARGET} PRIVATE common whisper ${CMAKE_THREAD_LIBS_INIT})\ntarget_compile_features(${TARGET} PRIVATE cxx_std_17)\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Whisper Commands in Vim\nDESCRIPTION: Example of defining custom spoken commands for the Whisper.cpp Vim plugin, demonstrating integration with other plugins.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/lsp/README.md#2025-04-11_snippet_4\n\nLANGUAGE: vim\nCODE:\n```\nlet g:whisper_user_commands = {\"gen\": \"llama#doLlamaGen\"}\n```\n\n----------------------------------------\n\nTITLE: Setting Up Project Include Directories in CMake\nDESCRIPTION: This code adds the current source directory to the include path for all targets.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/CMakeLists.txt#2025-04-11_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\ninclude_directories(${CMAKE_CURRENT_SOURCE_DIR})\n```\n\n----------------------------------------\n\nTITLE: Configuring Debug and Sanitizer Options\nDESCRIPTION: Sets options for debugging, warnings, and sanitizers including thread, address, and undefined behavior sanitizers.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/CMakeLists.txt#2025-04-11_snippet_4\n\nLANGUAGE: CMake\nCODE:\n```\n# debug\noption(GGML_ALL_WARNINGS           \"ggml: enable all compiler warnings\"                   ON)\noption(GGML_ALL_WARNINGS_3RD_PARTY \"ggml: enable all compiler warnings in 3rd party libs\" OFF)\noption(GGML_GPROF                  \"ggml: enable gprof\"                                   OFF)\n\n# build\noption(GGML_FATAL_WARNINGS    \"ggml: enable -Werror flag\"    OFF)\n\n# sanitizers\noption(GGML_SANITIZE_THREAD    \"ggml: enable thread sanitizer\"    OFF)\noption(GGML_SANITIZE_ADDRESS   \"ggml: enable address sanitizer\"   OFF)\noption(GGML_SANITIZE_UNDEFINED \"ggml: enable undefined sanitizer\" OFF)\n```\n\n----------------------------------------\n\nTITLE: Configuring Build Targets and Include Directories\nDESCRIPTION: Sets up architecture-specific build targets and configures include directories for the project. Handles different Android ABIs and includes necessary header paths.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/whisper.android/lib/src/main/jni/whisper/CMakeLists.txt#2025-04-11_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nif (${ANDROID_ABI} STREQUAL \"arm64-v8a\")\n    build_library(\"whisper_v8fp16_va\")\nelseif (${ANDROID_ABI} STREQUAL \"armeabi-v7a\")\n    build_library(\"whisper_vfpv4\")\nendif ()\n\nbuild_library(\"whisper\") # Default target\n\ninclude_directories(${WHISPER_LIB_DIR})\ninclude_directories(${WHISPER_LIB_DIR}/src)\ninclude_directories(${WHISPER_LIB_DIR}/include)\ninclude_directories(${WHISPER_LIB_DIR}/ggml/include)\ninclude_directories(${WHISPER_LIB_DIR}/ggml/src)\ninclude_directories(${WHISPER_LIB_DIR}/ggml/src/ggml-cpu)\n```\n\n----------------------------------------\n\nTITLE: GGML Installation and Package Configuration\nDESCRIPTION: Configures installation targets, public headers, and CMake package generation. Sets up version information based on git commit data and creates necessary package configuration files.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/CMakeLists.txt#2025-04-11_snippet_9\n\nLANGUAGE: cmake\nCODE:\n```\nset(GGML_PUBLIC_HEADERS\n    include/ggml.h\n    include/ggml-cpu.h\n    include/ggml-alloc.h\n    include/ggml-backend.h\n    include/ggml-blas.h\n    include/ggml-cann.h\n    include/ggml-cpp.h\n    include/ggml-cuda.h\n    include/ggml-kompute.h\n    include/ggml-opt.h\n    include/ggml-metal.h\n    include/ggml-rpc.h\n    include/ggml-sycl.h\n    include/ggml-vulkan.h\n    include/gguf.h)\n```\n\n----------------------------------------\n\nTITLE: Defining wchess-core Static Library in CMake\nDESCRIPTION: Creates a static library target called wchess-core with four source files: WChess.cpp, WChess.h, Chessboard.cpp, and Chessboard.h.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/wchess/libwchess/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nadd_library(wchess-core STATIC\n    WChess.cpp\n    WChess.h\n    Chessboard.cpp\n    Chessboard.h\n)\n```\n\n----------------------------------------\n\nTITLE: Adding JSON Library Target in CMake\nDESCRIPTION: This snippet adds a header-only 'json_cpp' interface library and sets up its include directories.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/CMakeLists.txt#2025-04-11_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nadd_library(json_cpp INTERFACE)\ntarget_include_directories(json_cpp INTERFACE ${CMAKE_CURRENT_SOURCE_DIR})\n```\n\n----------------------------------------\n\nTITLE: Configuring Single File WASM Output in CMake\nDESCRIPTION: Handles the optional embedding of WASM code into a single JavaScript file. When WHISPER_WASM_SINGLE_FILE is enabled, it sets appropriate flags and copies the output file to the correct destination.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/command.wasm/CMakeLists.txt#2025-04-11_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nunset(EXTRA_FLAGS)\n\nif (WHISPER_WASM_SINGLE_FILE)\n    set(EXTRA_FLAGS \"-s SINGLE_FILE=1\")\n    message(STATUS \"Embedding WASM inside command.js\")\n\n    add_custom_command(\n        TARGET ${TARGET} POST_BUILD\n        COMMAND ${CMAKE_COMMAND} -E copy\n        ${CMAKE_BINARY_DIR}/bin/libcommand.js\n        ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/command.wasm/command.js\n        )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Locating CANN Compiler CMake Directory in CMake\nDESCRIPTION: This snippet checks for the existence of the ascendc_kernel_cmake directory in different possible locations within the CANN package. It sets the ASCENDC_CMAKE_DIR variable accordingly.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-cann/kernels/CMakeLists.txt#2025-04-11_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nif(EXISTS ${ASCEND_CANN_PACKAGE_PATH}/compiler/tikcpp/ascendc_kernel_cmake)\n    set(ASCENDC_CMAKE_DIR ${ASCEND_CANN_PACKAGE_PATH}/compiler/tikcpp/ascendc_kernel_cmake)\nelseif(EXISTS ${ASCEND_CANN_PACKAGE_PATH}/ascendc_devkit/tikcpp/samples/cmake)\n    set(ASCENDC_CMAKE_DIR ${ASCEND_CANN_PACKAGE_PATH}/ascendc_devkit/tikcpp/samples/cmake)\nelse()\n    message(FATAL_ERROR \"ascendc_kernel_cmake does not exist, please check whether the compiler package is installed.\")\nendif()\ninclude(${ASCENDC_CMAKE_DIR}/ascendc.cmake)\n```\n\n----------------------------------------\n\nTITLE: Configuring command.wasm Target in CMake\nDESCRIPTION: Sets up the command.wasm target and copies necessary web interface files (HTML and JavaScript) to the output directory using the configure_file command.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/command.wasm/CMakeLists.txt#2025-04-11_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET command.wasm)\n\nconfigure_file(${CMAKE_CURRENT_SOURCE_DIR}/index-tmpl.html  ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/${TARGET}/index.html @ONLY)\nconfigure_file(${CMAKE_CURRENT_SOURCE_DIR}/../helpers.js    ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/${TARGET}/helpers.js @ONLY)\n```\n\n----------------------------------------\n\nTITLE: Finding Thread Dependencies in CMake\nDESCRIPTION: This snippet finds and includes the Threads package which is required for the whisper.cpp project.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfind_package(Threads REQUIRED)\n```\n\n----------------------------------------\n\nTITLE: Configuring IBM s390x Architecture Optimizations in CMake for GGML\nDESCRIPTION: This snippet detects IBM s390x processors and configures appropriate compiler flags. It reads the system information to identify the machine type (z15, z16) and sets specific optimization flags based on the detected processor model, including vector extension support.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-cpu/CMakeLists.txt#2025-04-11_snippet_5\n\nLANGUAGE: CMake\nCODE:\n```\nelseif (${CMAKE_SYSTEM_PROCESSOR} MATCHES \"s390x\")\n    message(STATUS \"s390x detected\")\n    file(READ \"/proc/cpuinfo\" CPUINFO_CONTENTS)\n    string(REGEX REPLACE \"machine[ \\t\\r\\n]*=[ \\t\\r\\n]*([0-9]+)\" \"\\\\1\" S390X_M ${CPUINFO_CONTENTS})\n\n    # TODO: Separation to determine activation of VX/VXE/VXE2\n    if (${S390X_M} MATCHES \"8561|8562\")\n        message(STATUS \"z15 target\")\n        list(APPEND ARCH_FLAGS -march=z15 -mtune=z15)\n    elseif (${S390X_M} MATCHES \"3931\")\n        message(STATUS \"z16 target\")\n        list(APPEND ARCH_FLAGS -march=z16 -mtune=z16)\n    else()\n        message(STATUS \"Unknown target\")\n        message(WARNING \"Unknown target. If you are compiling for z14 and earlier, you might have to add -DGGML_VXE=OFF.\")\n        list(APPEND ARCH_FLAGS -march=native -mtune=native)\n    endif()\n\n    if (GGML_VXE)\n        list(APPEND ARCH_FLAGS -mvx -mzvector)\n    endif()\n```\n\n----------------------------------------\n\nTITLE: Updating Windows DLL for Whisper.cpp Java Bindings\nDESCRIPTION: This Windows command copies the updated whisper.dll file to the appropriate location in the Java project's resources directory for Windows x86-64 platforms.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/bindings/java/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncopy /y ..\\..\\build\\bin\\Release\\whisper.dll build\\generated\\resources\\main\\win32-x86-64\\whisper.dll\n```\n\n----------------------------------------\n\nTITLE: Configuring Core Library and Platform-Specific Targets in CMake\nDESCRIPTION: This CMake snippet sets up the wchess project build. It adds the core library subdirectory, sets its folder property, and then conditionally includes either a WebAssembly or command-line target based on whether Emscripten is being used.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/wchess/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nadd_subdirectory(libwchess)\nset_target_properties(wchess-core PROPERTIES FOLDER \"libs\")\n\nif (EMSCRIPTEN)\n    add_subdirectory(wchess.wasm)\n    set_target_properties(wchess.wasm PROPERTIES FOLDER \"libs\")\nelse()\n    add_subdirectory(wchess.cmd)\n    set_target_properties(wchess PROPERTIES FOLDER \"libs\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring WebAssembly Chess Target in CMake\nDESCRIPTION: Sets up the main executable target for the WebAssembly chess application, including source files and linked libraries. It also configures compilation flags and post-build steps for file management.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/wchess/wchess.wasm/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET wchess.wasm)\n\nadd_executable(${TARGET}\n    wchess.wasm.cpp\n    )\n\ninclude(DefaultTargetOptions)\n\ntarget_link_libraries(${TARGET} PRIVATE\n    common\n    wchess-core\n    )\n\nunset(EXTRA_FLAGS)\n\nif (WHISPER_WASM_SINGLE_FILE)\n    set(EXTRA_FLAGS \"-s SINGLE_FILE=1\")\n    message(STATUS \"Embedding WASM inside chess.js\")\n\n    add_custom_command(\n        TARGET ${TARGET} POST_BUILD\n        COMMAND ${CMAKE_COMMAND} -E copy\n        ${CMAKE_BINARY_DIR}/bin/${TARGET}.js\n        ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/${TARGET}/js/chess.js\n        )\nendif()\n\nset_target_properties(${TARGET} PROPERTIES LINK_FLAGS \" \\\n    --bind \\\n    -s USE_PTHREADS=1 \\\n    -s PTHREAD_POOL_SIZE=8 \\\n    -s INITIAL_MEMORY=1024MB \\\n    -s TOTAL_MEMORY=1024MB \\\n    -s FORCE_FILESYSTEM=1 \\\n    -s EXPORTED_RUNTIME_METHODS=\\\"['print', 'printErr', 'ccall', 'cwrap']\\\" \\\n    ${EXTRA_FLAGS} \\\n    \")\n\n\nadd_custom_command(\n        TARGET ${TARGET} POST_BUILD\n        COMMAND ${CMAKE_COMMAND} -E copy_directory\n        ${CMAKE_CURRENT_SOURCE_DIR}/chessboardjs-1.0.0\n        ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/${TARGET}/\n        COMMAND ${CMAKE_COMMAND} -E copy\n        ${CMAKE_CURRENT_SOURCE_DIR}/jquery-3.7.1.min.js\n        ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/${TARGET}/js/\n    )\n\nconfigure_file(${CMAKE_CURRENT_SOURCE_DIR}/index-tmpl.html  ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/${TARGET}/index.html @ONLY)\nconfigure_file(${CMAKE_SOURCE_DIR}/examples/helpers.js    ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/${TARGET}/js/helpers.js @ONLY)\n```\n\n----------------------------------------\n\nTITLE: Setting MUSA Path and Compilers in CMake\nDESCRIPTION: Determines the MUSA installation path and sets the C and C++ compilers to use MUSA's clang. It also appends the MUSA cmake modules path.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-musa/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif (NOT EXISTS $ENV{MUSA_PATH})\n    if (NOT EXISTS /opt/musa)\n        set(MUSA_PATH /usr/local/musa)\n    else()\n        set(MUSA_PATH /opt/musa)\n    endif()\nelse()\n    set(MUSA_PATH $ENV{MUSA_PATH})\nendif()\n\nset(CMAKE_C_COMPILER \"${MUSA_PATH}/bin/clang\")\nset(CMAKE_C_EXTENSIONS OFF)\nset(CMAKE_CXX_COMPILER \"${MUSA_PATH}/bin/clang++\")\nset(CMAKE_CXX_EXTENSIONS OFF)\n\nlist(APPEND CMAKE_MODULE_PATH \"${MUSA_PATH}/cmake\")\n\nfind_package(MUSAToolkit)\n```\n\n----------------------------------------\n\nTITLE: Configuring KleidiAI Integration for ARM Acceleration in CMake for GGML\nDESCRIPTION: This snippet sets up KleidiAI integration for ARM architecture acceleration. It fetches the KleidiAI library from GitHub, configures include paths, and selectively includes optimized kernels based on the available ARM instruction set extensions like dotprod, i8mm, and SME.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-cpu/CMakeLists.txt#2025-04-11_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nif (GGML_CPU_AARCH64)\n    target_compile_definitions(${GGML_CPU_NAME} PRIVATE GGML_USE_CPU_AARCH64)\nendif()\n\nif (GGML_CPU_KLEIDIAI)\n    message(STATUS \"Using KleidiAI optimized kernels if applicable\")\n\n    # Disable the KleidiAI tests\n    set(KLEIDIAI_BUILD_TESTS  OFF)\n\n    # Fetch KleidiAI sources:\n    include(FetchContent)\n    set(KLEIDIAI_COMMIT_TAG \"v1.5.0\")\n    set(KLEIDIAI_DOWNLOAD_URL \"https://github.com/ARM-software/kleidiai/archive/refs/tags/${KLEIDIAI_COMMIT_TAG}.tar.gz\")\n    set(KLEIDIAI_ARCHIVE_MD5  \"ea22e1aefb800e9bc8c74d91633cc58e\")\n\n    if (POLICY CMP0135)\n        cmake_policy(SET CMP0135 NEW)\n    endif()\n\n    FetchContent_Declare(KleidiAI_Download\n        URL ${KLEIDIAI_DOWNLOAD_URL}\n        DOWNLOAD_EXTRACT_TIMESTAMP NEW\n        URL_HASH MD5=${KLEIDIAI_ARCHIVE_MD5})\n\n    FetchContent_MakeAvailable(KleidiAI_Download)\n    FetchContent_GetProperties(KleidiAI_Download\n        SOURCE_DIR  KLEIDIAI_SRC\n        POPULATED   KLEIDIAI_POPULATED)\n\n    if (NOT KLEIDIAI_POPULATED)\n        message(FATAL_ERROR \"KleidiAI source downloaded failed.\")\n    endif()\n\n    add_compile_definitions(GGML_USE_CPU_KLEIDIAI)\n\n    # Remove kleidiai target after fetching it\n    if (TARGET kleidiai)\n        set_target_properties(kleidiai PROPERTIES EXCLUDE_FROM_ALL TRUE)\n    endif()\n\n    list(APPEND GGML_CPU_SOURCES\n        ggml-cpu/kleidiai/kleidiai.cpp\n        ggml-cpu/kleidiai/kernels.cpp\n        ggml-cpu/kleidiai/kleidiai.h\n        ggml-cpu/kleidiai/kernels.h\n        )\n\n    # KleidiAI\n    include_directories(\n        ${KLEIDIAI_SRC}/\n        ${KLEIDIAI_SRC}/kai/\n        ${KLEIDIAI_SRC}/kai/ukernels/\n        ${KLEIDIAI_SRC}/kai/ukernels/matmul/\n        ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/\n        ${KLEIDIAI_SRC}/kai/ukernels/matmul/pack/)\n\n    set(ARCH_FLAGS_TEMP \"${ARCH_FLAGS}\")\n    if (NOT ARCH_FLAGS_TEMP)\n        string(REGEX MATCH \"-march=[^ ]+\" ARCH_FLAGS_TEMP \"${CMAKE_C_FLAGS}\")\n    endif()\n    string(FIND \"${ARCH_FLAGS_TEMP}\" \"+dotprod\" DOTPROD_ENABLED)\n    string(FIND \"${ARCH_FLAGS_TEMP}\" \"+i8mm\" I8MM_ENABLED)\n    string(FIND \"${ARCH_FLAGS_TEMP}\" \"+sme\" SME_ENABLED)\n\n    set(PRIVATE_ARCH_FLAGS ${ARCH_FLAGS})\n\n    list(APPEND GGML_KLEIDIAI_SOURCES ${KLEIDIAI_SRC}/kai/ukernels/matmul/pack/kai_lhs_quant_pack_qsi8d32p_f32.c)\n    list(APPEND GGML_KLEIDIAI_SOURCES ${KLEIDIAI_SRC}/kai/ukernels/matmul/pack/kai_rhs_pack_nxk_qsi4c32ps1s0scalef16_qsu4c32s16s0_neon.c)\n    list(APPEND GGML_KLEIDIAI_SOURCES ${KLEIDIAI_SRC}/kai/ukernels/matmul/pack/kai_lhs_quant_pack_qsi8d32p_f32_neon.c)\n    list(APPEND GGML_KLEIDIAI_SOURCES ${KLEIDIAI_SRC}/kai/ukernels/matmul/pack/kai_rhs_pack_nxk_qsi4c32pscalef16_qsu4c32s16s0.c)\n\n    if (NOT DOTPROD_ENABLED MATCHES -1)\n        list(APPEND GGML_KLEIDIAI_SOURCES ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p1x8_qsi4c32p4x8_1x4x32_neon_dotprod.c)\n        list(APPEND GGML_KLEIDIAI_SOURCES ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p1x4_qsi4c32p4x4_1x4_neon_dotprod.c)\n        list(APPEND GGML_KLEIDIAI_SOURCES ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p4x4_qsi4c32p4x4_16x4_neon_dotprod.c)\n    endif()\n\n    if (NOT I8MM_ENABLED MATCHES -1)\n        list(APPEND GGML_KLEIDIAI_SOURCES ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p4x8_qsi4c32p4x8_16x4_neon_i8mm.c)\n    endif()\n\n    if (NOT SME_ENABLED MATCHES -1)\n        list(APPEND GGML_KLEIDIAI_SOURCES ${KLEIDIAI_SRC}/kai/ukernels/matmul/matmul_clamp_f32_qsi8d32p_qsi4c32p/kai_matmul_clamp_f32_qsi8d32p1vlx4_qsi4c32p4vlx4_1vlx4vl_sme2_mopa.c)\n    endif()\n```\n\n----------------------------------------\n\nTITLE: Stream WASM Resource Configuration\nDESCRIPTION: Configures the stream.wasm target and sets up file copying for HTML template and helper JavaScript files.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/stream.wasm/CMakeLists.txt#2025-04-11_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET stream.wasm)\n\nconfigure_file(${CMAKE_CURRENT_SOURCE_DIR}/index-tmpl.html  ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/${TARGET}/index.html @ONLY)\nconfigure_file(${CMAKE_CURRENT_SOURCE_DIR}/../helpers.js    ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/${TARGET}/helpers.js @ONLY)\n```\n\n----------------------------------------\n\nTITLE: Linking Dependencies to wchess-core in CMake\nDESCRIPTION: Links the wchess-core library with the whisper and common libraries as PUBLIC dependencies, making them available to targets that link with wchess-core.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/wchess/libwchess/CMakeLists.txt#2025-04-11_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_link_libraries(wchess-core\n    PUBLIC\n    whisper\n    common\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Include Directories for wchess-core in CMake\nDESCRIPTION: Configures the include directories for the wchess-core library, making the current source directory available to targets that use this library.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/wchess/libwchess/CMakeLists.txt#2025-04-11_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\ntarget_include_directories(wchess-core\n    PUBLIC\n    \"$<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}>\"\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Metal Shader Files in CMake\nDESCRIPTION: Configures installation rules for Metal shader files when not embedding the Metal library, ensuring they're available at the installation location.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-metal/CMakeLists.txt#2025-04-11_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nif (NOT GGML_METAL_EMBED_LIBRARY)\n    install(\n        FILES src/ggml-metal/ggml-metal.metal\n        PERMISSIONS\n            OWNER_READ\n            OWNER_WRITE\n            GROUP_READ\n            WORLD_READ\n        DESTINATION ${CMAKE_INSTALL_BINDIR})\n\n        install(\n            FILES ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/default.metallib\n            DESTINATION ${CMAKE_INSTALL_BINDIR}\n        )\nendif()\n```\n\n----------------------------------------\n\nTITLE: WASM Single File Configuration\nDESCRIPTION: Configures optional single-file WASM build mode and sets up post-build file copying.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/stream.wasm/CMakeLists.txt#2025-04-11_snippet_1\n\nLANGUAGE: cmake\nCODE:\n```\nunset(EXTRA_FLAGS)\n\nif (WHISPER_WASM_SINGLE_FILE)\n    set(EXTRA_FLAGS \"-s SINGLE_FILE=1\")\n    message(STATUS \"Embedding WASM inside stream.js\")\n\n    add_custom_command(\n        TARGET ${TARGET} POST_BUILD\n        COMMAND ${CMAKE_COMMAND} -E copy\n        ${CMAKE_BINARY_DIR}/bin/libstream.js\n        ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/stream.wasm/stream.js\n        )\nendif()\n```\n\n----------------------------------------\n\nTITLE: Configuring the bench.wasm Web Interface\nDESCRIPTION: Sets up the bench.wasm target and copies template HTML and helper JavaScript files to the runtime output directory. These files provide the web interface for the WebAssembly benchmark tool.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/bench.wasm/CMakeLists.txt#2025-04-11_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\nset(TARGET bench.wasm)\n\nconfigure_file(${CMAKE_CURRENT_SOURCE_DIR}/index-tmpl.html  ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/${TARGET}/index.html @ONLY)\nconfigure_file(${CMAKE_CURRENT_SOURCE_DIR}/../helpers.js    ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/${TARGET}/helpers.js @ONLY)\n```\n\n----------------------------------------\n\nTITLE: Python Package Dependencies\nDESCRIPTION: Lists the required Python packages needed to work with whisper.cpp, including PyTorch, Core ML Tools, OpenAI Whisper, and ANE Transformers for Apple Neural Engine support.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/models/requirements-coreml.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntorch\ncoremltools\nopenai-whisper\nane_transformers\n```\n\n----------------------------------------\n\nTITLE: Defining Function for Adding OpenCL Kernels in CMake\nDESCRIPTION: Creates a function that either embeds an OpenCL kernel into the binary or copies it to the output directory depending on configuration.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-opencl/CMakeLists.txt#2025-04-11_snippet_6\n\nLANGUAGE: CMake\nCODE:\n```\nfunction(ggml_opencl_add_kernel KNAME)\n    set(KERN_HDR ${CMAKE_CURRENT_BINARY_DIR}/autogenerated/${KNAME}.cl.h)\n    set(KERN_SRC ${CMAKE_CURRENT_SOURCE_DIR}/kernels/${KNAME}.cl)\n\n    if (GGML_OPENCL_EMBED_KERNELS)\n        message(STATUS \"opencl: embedding kernel ${KNAME}\")\n\n        # Python must be accessible from command line\n        add_custom_command(\n            OUTPUT ${KERN_HDR}\n            COMMAND ${Python3_EXECUTABLE} ${EMBED_KERNEL_SCRIPT} ${KERN_SRC} ${KERN_HDR}\n            DEPENDS ${KERN_SRC} ${EMBED_KERNEL_SCRIPT}\n            COMMENT \"Generate ${KERN_HDR}\"\n        )\n\n        target_sources(${TARGET_NAME} PRIVATE ${KERN_HDR})\n    else ()\n        message(STATUS \"opencl: adding kernel ${KNAME}\")\n        configure_file(${KERN_SRC} ${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/${KNAME}.cl COPYONLY)\n    endif ()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Setting Emscripten Link Flags for WebAssembly Compilation\nDESCRIPTION: Configures the Emscripten linker flags for the libbench target. Enables threading support, sets memory limits, enables filesystem support, and exports necessary runtime methods for JavaScript interaction.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/bench.wasm/CMakeLists.txt#2025-04-11_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nset_target_properties(${TARGET} PROPERTIES LINK_FLAGS \" \\\n    --bind \\\n    -s USE_PTHREADS=1 \\\n    -s PTHREAD_POOL_SIZE_STRICT=0 \\\n    -s INITIAL_MEMORY=2000MB \\\n    -s TOTAL_MEMORY=2000MB \\\n    -s FORCE_FILESYSTEM=1 \\\n    -s EXPORTED_RUNTIME_METHODS=\\\"['print', 'printErr', 'ccall', 'cwrap']\\\" \\\n    ${EXTRA_FLAGS} \\\n    \")\n```\n\n----------------------------------------\n\nTITLE: Adding Optional Chess Application in CMake\nDESCRIPTION: This conditional block adds the 'wchess' subdirectory to the build when SDL2 is enabled.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/CMakeLists.txt#2025-04-11_snippet_9\n\nLANGUAGE: CMake\nCODE:\n```\nif (WHISPER_SDL2)\n    add_subdirectory(wchess)\nendif (WHISPER_SDL2)\n```\n\n----------------------------------------\n\nTITLE: Specifying Python Dependencies for OpenVINO and Whisper\nDESCRIPTION: This requirements file lists the Python packages needed for a project using OpenVINO with PyTorch and ONNX capabilities along with OpenAI's Whisper model. It's used with pip to install the necessary dependencies in a Python environment.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/models/requirements-openvino.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nopenvino-dev[pytorch,onnx]\nopenai-whisper\n```\n\n----------------------------------------\n\nTITLE: Gathering Source Files for Ascend NPU Kernels in CMake\nDESCRIPTION: This snippet uses the file(GLOB) command to collect source files for various kernel operations. It includes files for different data types and quantization operations.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-cann/kernels/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nfile(GLOB SRC_FILES\n    get_row_f32.cpp\n    get_row_f16.cpp\n    get_row_q4_0.cpp\n    get_row_q8_0.cpp\n    quantize_f32_q8_0.cpp\n    quantize_f16_q8_0.cpp\n    quantize_float_to_q4_0.cpp\n    dup.cpp\n)\n```\n\n----------------------------------------\n\nTITLE: Deploying the command.wasm Files to a Custom Web Server\nDESCRIPTION: Instructions for copying the necessary WebAssembly files to a custom web server. This allows hosting the voice assistant example on a different server than the local development environment.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/command.wasm/README.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ncp bin/command.wasm/*       /path/to/html/\ncp bin/libcommand.worker.js /path/to/html/\n```\n\n----------------------------------------\n\nTITLE: Configuring BLAS Static Linking in CMake\nDESCRIPTION: Sets BLA_STATIC flag for static linking when GGML_STATIC is enabled. This configures the BLAS library to be statically linked into the application.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-blas/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: CMake\nCODE:\n```\nif (GGML_STATIC)\n    set(BLA_STATIC ON)\nendif()\n```\n\n----------------------------------------\n\nTITLE: Shader Compilation Function Definition\nDESCRIPTION: Defines a CMake function for compiling GLSL shaders to SPIR-V and generating corresponding C++ headers\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-kompute/CMakeLists.txt#2025-04-11_snippet_2\n\nLANGUAGE: cmake\nCODE:\n```\nfunction(compile_shader)\n    set(options)\n    set(oneValueArgs)\n    set(multiValueArgs SOURCES)\n    cmake_parse_arguments(compile_shader \"${options}\" \"${oneValueArgs}\" \"${multiValueArgs}\" ${ARGN})\n    foreach(source ${compile_shader_SOURCES})\n        get_filename_component(filename ${source} NAME)\n        set(spv_file ${filename}.spv)\n        add_custom_command(\n            OUTPUT ${spv_file}\n            DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/${source}\n            ${CMAKE_CURRENT_SOURCE_DIR}/kompute-shaders/common.comp\n            ${CMAKE_CURRENT_SOURCE_DIR}/kompute-shaders/op_getrows.comp\n            ${CMAKE_CURRENT_SOURCE_DIR}/kompute-shaders/op_mul_mv_q_n_pre.comp\n            ${CMAKE_CURRENT_SOURCE_DIR}/kompute-shaders/op_mul_mv_q_n.comp\n            COMMAND ${glslc_executable} --target-env=vulkan1.2 -o ${spv_file} ${CMAKE_CURRENT_SOURCE_DIR}/${source}\n            COMMENT \"Compiling ${source} to ${spv_file}\"\n            )\n        # ... header generation code ...\n    endforeach()\nendfunction()\n```\n\n----------------------------------------\n\nTITLE: Running Local HTTP Server for WebAssembly Benchmark\nDESCRIPTION: This command starts a local Python HTTP server to host the WebAssembly benchmark. After running this, the benchmark can be accessed in a web browser at http://localhost:8000/bench.wasm.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/bench.wasm/README.md#2025-04-11_snippet_1\n\nLANGUAGE: console\nCODE:\n```\npython3 examples/server.py\n```\n\n----------------------------------------\n\nTITLE: Building chessboard.js Using npm Scripts\nDESCRIPTION: Shell commands for building chessboard.js and regenerating the documentation website. These commands help developers work with the codebase.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/wchess/wchess.wasm/chessboardjs-1.0.0/js/chessboard-1.0.0/README.md#2025-04-11_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\n# create a build in the build/ directory\nnpm run build\n\n# re-build the website\nnpm run website\n```\n\n----------------------------------------\n\nTITLE: Checking Vulkan Cooperative Matrix Support in CMake\nDESCRIPTION: Executes a test shader compilation to determine if GL_KHR_cooperative_matrix is supported by the Vulkan compiler (glslc). Sets a cache variable based on the result.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/ggml-vulkan/CMakeLists.txt#2025-04-11_snippet_1\n\nLANGUAGE: CMake\nCODE:\n```\nif(NOT DEFINED GGML_VULKAN_COOPMAT_GLSLC_SUPPORT)\n    execute_process(COMMAND ${Vulkan_GLSLC_EXECUTABLE} -o - -fshader-stage=compute --target-env=vulkan1.3 \"${CMAKE_CURRENT_SOURCE_DIR}/vulkan-shaders/test_coopmat_support.comp\"\n                    OUTPUT_VARIABLE glslc_output\n                    ERROR_VARIABLE glslc_error)\n\n    if (${glslc_error} MATCHES \".*extension not supported: GL_KHR_cooperative_matrix.*\")\n        message(STATUS \"GL_KHR_cooperative_matrix not supported by glslc\")\n        set(GGML_VULKAN_COOPMAT_GLSLC_SUPPORT OFF CACHE INTERNAL \"Whether coopmat is supported by glslc\")\n    else()\n        message(STATUS \"GL_KHR_cooperative_matrix supported by glslc\")\n        set(GGML_VULKAN_COOPMAT_GLSLC_SUPPORT ON CACHE INTERNAL \"Whether coopmat is supported by glslc\")\n    endif()\nelse()\n    message(STATUS \"GL_KHR_cooperative_matrix support already defined: ${GGML_VULKAN_COOPMAT_GLSLC_SUPPORT}\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Matrix Multiplication Benchmark Results\nDESCRIPTION: GGML matrix multiplication performance across different quantization methods (Q4_0, Q4_1, Q5_0, Q5_1, Q8_0, F16, F32) and matrix sizes from 64x64 to 4096x4096\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/scripts/bench-all-gg.txt#2025-04-11_snippet_5\n\nLANGUAGE: text\nCODE:\n```\nRunning ggml_mul_mat benchmark with 1 threads\n\n  64 x   64: Q4_0    37.7 GFLOPS (128 runs) | Q4_1    36.0 GFLOPS (128 runs)\n  64 x   64: Q5_0    20.1 GFLOPS (128 runs) | Q5_1    19.8 GFLOPS (128 runs) | Q8_0    39.5 GFLOPS (128 runs)\n[...truncated for brevity...]\n```\n\n----------------------------------------\n\nTITLE: Configuring Emscripten JavaScript Test\nDESCRIPTION: Sets up a test configuration for JavaScript/Emscripten builds using Node.js with WebAssembly threading support.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/tests/CMakeLists.txt#2025-04-11_snippet_0\n\nLANGUAGE: cmake\nCODE:\n```\nif (EMSCRIPTEN)\n    #\n    # test-whisper-js\n\n    set(TEST_TARGET test-whisper-js)\n\n    add_test(NAME ${TEST_TARGET}\n        COMMAND node test-whisper.js --experimental-wasm-threads\n        WORKING_DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}\n        )\n\n    return()\nendif()\n```\n\n----------------------------------------\n\nTITLE: Running Single-threaded Matrix Multiplication Benchmark on M4 Max\nDESCRIPTION: Executes a benchmark to measure matrix multiplication performance with different quantization methods (Q4_0, Q4_1, Q5_0, Q5_1, Q8_0, F16, F32) and matrix sizes using a single thread.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/scripts/bench-all-gg.txt#2025-04-11_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nmake -j && ./scripts/bench-all.sh 1\n```\n\n----------------------------------------\n\nTITLE: Memory Copy Benchmark Results\nDESCRIPTION: Memory bandwidth test results showing scaling from 1-8 threads, reaching peak performance of 198.10 GB/s with 8 threads\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/scripts/bench-all-gg.txt#2025-04-11_snippet_4\n\nLANGUAGE: text\nCODE:\n```\nmemcpy:   48.01 GB/s (heat-up)\nmemcpy:   56.00 GB/s ( 1 thread)\nmemcpy:   56.20 GB/s ( 1 thread)\nmemcpy:  102.69 GB/s ( 2 thread)\nmemcpy:  140.32 GB/s ( 3 thread)\nmemcpy:  179.04 GB/s ( 4 thread)\nmemcpy:  159.61 GB/s ( 5 thread)\nmemcpy:  159.02 GB/s ( 6 thread)\nmemcpy:  180.29 GB/s ( 7 thread)\nmemcpy:  198.10 GB/s ( 8 thread)\n```\n\n----------------------------------------\n\nTITLE: Executing the SYCL Device Listing Tool in llama.cpp\nDESCRIPTION: Command to run the ls-sycl-device tool that lists all available SYCL devices and their capabilities.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/sycl/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./build/bin/ls-sycl-device\n```\n\n----------------------------------------\n\nTITLE: SYCL Device List Output\nDESCRIPTION: Example output showing detected SYCL devices and their capabilities\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/README_sycl.md#2025-04-11_snippet_3\n\nLANGUAGE: plaintext\nCODE:\n```\n[opencl:acc:0] Intel(R) FPGA Emulation Platform for OpenCL(TM), Intel(R) FPGA Emulation Device OpenCL 1.2  [2023.16.10.0.17_160000]\n[opencl:cpu:1] Intel(R) OpenCL, 13th Gen Intel(R) Core(TM) i7-13700K OpenCL 3.0 (Build 0) [2023.16.10.0.17_160000]\n[opencl:gpu:2] Intel(R) OpenCL Graphics, Intel(R) Arc(TM) A770 Graphics OpenCL 3.0 NEO  [23.30.26918.50]\n[ext_oneapi_level_zero:gpu:0] Intel(R) Level-Zero, Intel(R) Arc(TM) A770 Graphics 1.3 [1.3.26918]\n```\n\n----------------------------------------\n\nTITLE: Fixing Xcode SDK Selection in Bash\nDESCRIPTION: Command to resolve the \"iphoneos is not an iOS SDK\" error by switching to the correct Xcode developer directory.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/whisper.swiftui/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsudo xcode-select -switch /Applications/Xcode.app/Contents/Developer\n```\n\n----------------------------------------\n\nTITLE: Setting up Common Library Variables in CMake\nDESCRIPTION: This snippet sets up the 'common' target name and initializes an empty list for extra libraries that might be needed.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/CMakeLists.txt#2025-04-11_snippet_2\n\nLANGUAGE: CMake\nCODE:\n```\nset(TARGET common)\n\nunset(COMMON_EXTRA_LIBS)\n```\n\n----------------------------------------\n\nTITLE: Commented-out Executable Target in CMake\nDESCRIPTION: A commented-out command that would create an executable target called test-chessboard using test-chessboard.cpp and Chessboard.cpp source files.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/examples/wchess/libwchess/CMakeLists.txt#2025-04-11_snippet_3\n\nLANGUAGE: cmake\nCODE:\n```\n# add_executable(test-chessboard test-chessboard.cpp Chessboard.cpp)\n```\n\n----------------------------------------\n\nTITLE: Adding CPU Backend Variants in CMake\nDESCRIPTION: Configures different CPU backend variants with specific instruction set optimizations. It adds variants for different CPU architectures when GGML_CPU_ALL_VARIANTS is enabled.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/ggml/src/CMakeLists.txt#2025-04-11_snippet_7\n\nLANGUAGE: CMake\nCODE:\n```\nggml_add_backend(CPU)\n\nif (GGML_CPU_ALL_VARIANTS)\n    if (NOT GGML_BACKEND_DL)\n        message(FATAL_ERROR \"GGML_CPU_ALL_VARIANTS requires GGML_BACKEND_DL\")\n    endif()\n    ggml_add_cpu_backend_variant(sandybridge    AVX)\n    ggml_add_cpu_backend_variant(haswell        AVX F16C AVX2 BMI2 FMA)\n    ggml_add_cpu_backend_variant(skylakex       AVX F16C AVX2 BMI2 FMA AVX512)\n    ggml_add_cpu_backend_variant(icelake        AVX F16C AVX2 BMI2 FMA AVX512 AVX512_VBMI AVX512_VNNI)\n    ggml_add_cpu_backend_variant(alderlake      AVX F16C AVX2 BMI2 FMA AVX_VNNI)\n    if (NOT MSVC)\n        # MSVC doesn't support AMX\n        ggml_add_cpu_backend_variant(sapphirerapids AVX F16C AVX2 BMI2 FMA AVX512 AVX512_VBMI AVX512_VNNI AVX512_BF16 AMX_TILE AMX_INT8)\n    endif()\nelseif (GGML_CPU)\n    ggml_add_cpu_backend_variant_impl(\"\")\nendif()\n```\n\n----------------------------------------\n\nTITLE: Defining Python Dependencies for WER Calculation in whisper.cpp\nDESCRIPTION: Lists the minimal set of Python package dependencies required to compute Word Error Rate (WER) scores, as referenced in Section 3.2 of the original Whisper paper. These packages include jiwer for WER calculation, regex for text processing, and more-itertools for advanced iteration utilities.\nSOURCE: https://github.com/ggml-org/whisper.cpp/blob/master/tests/librispeech/requirements.txt#2025-04-11_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\njiwer\nregex\nmore-itertools\n```"
  }
]
[
  {
    "owner": "apache",
    "repo": "nifi",
    "content": "TITLE: Handling Script Errors with Try/Catch in Groovy\nDESCRIPTION: Provides a basic structure for error handling within a Groovy script in NiFi's ExecuteScript processor. It uses a `try/catch` block to wrap potentially problematic code. If the code executes successfully, the FlowFile is transferred to `REL_SUCCESS`. If an exception occurs, it's caught, an error is logged, and the FlowFile is transferred to `REL_FAILURE`.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-scripting-bundle/nifi-scripting-processors/src/main/resources/docs/org.apache.nifi.processors.script.ExecuteScript/additionalDetails.md#_snippet_13\n\nLANGUAGE: groovy\nCODE:\n```\nflowFile = session.get()\nif (!flowFile) return\ntry {\n// Something that might throw an exception here\n\n// Last operation is transfer to success (failures handled in the catch block)\n    session.transfer(flowFile, REL_SUCCESS)\n} catch (e) {\n    log.error('Something went wrong', e)\n    session.transfer(flowFile, REL_FAILURE)\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring LookupRecord with Replace Existing Values Strategy in Apache NiFi (JSON)\nDESCRIPTION: Shows an example of input and output JSON with the LookupRecord processor set to 'Replace Existing Values' strategy. Here, nested 'locales' arrays contain 'region' and 'language' fields which are updated in-place via user-defined properties mapping to Lookup Service keys. Requires Apache NiFi LookupRecord processor, a Simple Key Value Lookup Service, and dynamic processor property configuration for record fields. Input: JSON arrays of objects with 'locales'; output: transformed 'region' and 'language' fields if mappings are found.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.LookupRecord/additionalDetails.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"locales\": [\n      {\n        \"region\": \"FR\",\n        \"language\": \"fr\"\n      },\n      {\n        \"region\": \"US\",\n        \"language\": \"en\"\n      }\n    ]\n  },\n  {\n    \"locales\": [\n      {\n        \"region\": \"CA\",\n        \"language\": \"fr\"\n      },\n      {\n        \"region\": \"JP\",\n        \"language\": \"ja\"\n      }\n    ]\n  }\n]\n```\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"locales\": [\n      {\n        \"region\": \"France\",\n        \"language\": \"French\"\n      },\n      {\n        \"region\": \"US\",\n        \"language\": \"English\"\n      }\n    ]\n  },\n  {\n    \"locales\": [\n      {\n        \"region\": \"Canada\",\n        \"language\": \"French\"\n      },\n      {\n        \"region\": \"JP\",\n        \"language\": \"ja\"\n      }\n    ]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Filtering NiFi FlowFiles with RPATH SQL\nDESCRIPTION: This SQL query filters FlowFiles based on the value found at a specific RPATH within the FlowFile's content. It selects all data from the 'FLOWFILE' treated as a table where the result of the RPATH_STRING function matches the 'Apache%' pattern.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.QueryRecord/additionalDetails.md#_snippet_6\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT * FROM FLOWFILE WHERE RPATH_STRING(project, '/maintainer/name') LIKE 'Apache%'\n```\n\n----------------------------------------\n\nTITLE: Running Apache NiFi Docker Container with OpenID Connect - Shell\nDESCRIPTION: This shell snippet demonstrates how to start an Apache NiFi Docker container configured to use OpenID Connect (OIDC) authentication. Required dependencies include Docker, an existing OpenID server with configured realm, client, and user matching the INITIAL_ADMIN_IDENTITY variable. Key environment variables specify keystore and truststore paths, OIDC discovery URL, client IDs and secrets, security parameters, and administrator identity. Input values are provided via environment variables and host-volume mappings; output is a running and OIDC-secured NiFi instance on port 8443. All certificate files and valid OIDC parameters must be set prior to use.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-docker/dockerhub/README.md#_snippet_4\n\nLANGUAGE: Shell\nCODE:\n```\ndocker run --name nifi \\\n  -v $(pwd)/certs/localhost:/opt/certs \\\n  -p 8443:8443 \\\n  -e AUTH=oidc \\\n  -e KEYSTORE_PATH=/opt/certs/keystore.jks \\\n  -e KEYSTORE_TYPE=JKS \\\n  -e KEYSTORE_PASSWORD=QKZv1hSWAFQYZ+WU1jjF5ank+l4igeOfQRp+OSbkkrs \\\n  -e TRUSTSTORE_PATH=/opt/certs/truststore.jks \\\n  -e TRUSTSTORE_PASSWORD=rHkWR1gDNW3R9hgbeRsT3OM3Ue0zwGtQqcFKJD2EXWE \\\n  -e TRUSTSTORE_TYPE=JKS \\\n  -e INITIAL_ADMIN_IDENTITY='test' \\\n  -e NIFI_SECURITY_USER_OIDC_DISCOVERY_URL=http://OPENID_SERVER_URL/auth/realms/OPENID_REALM/.well-known/openid-configuration \\\n  -e NIFI_SECURITY_USER_OIDC_CONNECT_TIMEOUT=10000 \\\n  -e NIFI_SECURITY_USER_OIDC_READ_TIMEOUT=10000 \\\n  -e NIFI_SECURITY_USER_OIDC_CLIENT_ID=nifi \\\n  -e NIFI_SECURITY_USER_OIDC_CLIENT_SECRET=tU47ugXO308WZqf5TtylyoMX3xH6W0kN \\\n  -e NIFI_SECURITY_USER_OIDC_PREFERRED_JWSALGORITHM=RS256 \\\n  -e NIFI_SECURITY_USER_OIDC_ADDITIONAL_SCOPES=email \\\n  -e NIFI_SECURITY_USER_OIDC_CLAIM_IDENTIFYING_USER=preferred_username \\\n  -e NIFI_SECURITY_USER_OIDC_CLAIM_GROUPS=admin \\\n  -e NIFI_SECURITY_USER_OIDC_FALLBACK_CLAIMS_IDENTIFYING_USER=email \\\n  -e NIFI_SECURITY_USER_OIDC_TRUSTSTORE_STRATEGY=PKIX \\\n  -e NIFI_SECURITY_USER_OIDC_TOKEN_REFRESH_WINDOW='60 secs' \\\n  -d \\\n  apache/nifi:latest\n```\n\n----------------------------------------\n\nTITLE: Running NiFi with Mutual TLS Authentication\nDESCRIPTION: Docker command to run NiFi with Mutual TLS Authentication, requiring user-provided certificates and configuration. This setup uses volume mounting to provide certificate files from the host system.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-docker/dockerhub/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --name nifi \\\n  -v /User/dreynolds/certs/localhost:/opt/certs \\\n  -p 8443:8443 \\\n  -e AUTH=tls \\\n  -e KEYSTORE_PATH=/opt/certs/keystore.jks \\\n  -e KEYSTORE_TYPE=JKS \\\n  -e KEYSTORE_PASSWORD=QKZv1hSWAFQYZ+WU1jjF5ank+l4igeOfQRp+OSbkkrs \\\n  -e TRUSTSTORE_PATH=/opt/certs/truststore.jks \\\n  -e TRUSTSTORE_PASSWORD=rHkWR1gDNW3R9hgbeRsT3OM3Ue0zwGtQqcFKJD2EXWE \\\n  -e TRUSTSTORE_TYPE=JKS \\\n  -e INITIAL_ADMIN_IDENTITY='CN=Random User, O=Apache, OU=NiFi, C=US' \\\n  -d \\\n  apache/nifi:latest\n```\n\n----------------------------------------\n\nTITLE: Configuring UpdateRecord Processor for Literal Value Replacement\nDESCRIPTION: This snippet demonstrates how to configure the UpdateRecord processor to replace specified record fields with literal values. The properties specify the replacement strategy and target fields. It updates fields such as 'name' and 'gender' with fixed strings, resulting in the modification of existing fields or addition of new ones if absent.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.UpdateRecord/additionalDetails.md#_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nreplacement_value_strategy: Literal Value\n# /name gets replaced with 'Jeremy'\n'name': Jeremy\n# /gender gets replaced with 'M'\ngender: M\n```\n\n----------------------------------------\n\nTITLE: SQL Join Query Example\nDESCRIPTION: This SQL query demonstrates how to join two tables, 'original' and 'enrichment', based on a common key 'id' and 'customer_id'. It selects all columns from both tables and combines the matching records using a standard JOIN clause.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.JoinEnrichment/additionalDetails.md#_snippet_6\n\nLANGUAGE: sql\nCODE:\n```\nSELECT o.*, e.*\nFROM original o\nJOIN enrichment e\nON o.id = e.customer_id\n```\n\n----------------------------------------\n\nTITLE: Configuring Warm-Redeploy Ingestor\nDESCRIPTION: This snippet demonstrates how to configure a PullHttpChangeIngestor in the bootstrap.conf file for automatic warm-redeploy. It specifies the ingestor implementation to be used.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-docs/src/main/markdown/System_Admin_Guide.md#_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\nnifi.minifi.notifier.ingestors=org.apache.nifi.minifi.bootstrap.configuration.ingestors.PullHttpChangeIngestor\n```\n\n----------------------------------------\n\nTITLE: Getting Multiple FlowFiles in Groovy for NiFi ExecuteScript\nDESCRIPTION: Retrieves up to a specified maximum number of FlowFiles (100 in this example) from the NiFi session's input queue using `session.get(maxResults)`. If no FlowFiles are available, it returns an empty list. The script then iterates through the retrieved list (`flowFileList`) to process each FlowFile individually. Depends on the implicit `session` object.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-scripting-bundle/nifi-scripting-processors/src/main/resources/docs/org.apache.nifi.processors.script.ExecuteScript/additionalDetails.md#_snippet_1\n\nLANGUAGE: groovy\nCODE:\n```\nflowFileList = session.get(100)\nif (!flowFileList.isEmpty()) {\n    flowFileList.each { flowFile ->\n// Process each FlowFile here\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring MiNiFi for C2 Protocol - INI Style Configuration Parameters\nDESCRIPTION: This snippet contains INI-style configuration entries to enable and configure MiNiFi's communication with a C2 server via the C2 protocol. It demonstrates enabling C2, specifying configuration directories, runtime identification, heartbeat URLs for REST endpoints, heartbeat period, and optional compression settings. These parameters must be placed in the MiNiFi configuration file to enable automatic dataflow updates and heartbeat reporting to the C2 server. Expected inputs are key-value pairs, and outputs affect MiNiFi's runtime behavior.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-docs/src/main/markdown/minifi-java-agent-quick-start.md#_snippet_2\n\nLANGUAGE: INI\nCODE:\n```\nc2.enable=true\nc2.config.directory=./conf\nc2.runtime.manifest.identifier=minifi\nc2.runtime.type=minifi-java\nc2.rest.url=http://localhost:10090/c2/config/heartbeat\nc2.rest.url.ack=http://localhost:10090/c2/config/acknowledge\nc2.agent.heartbeat.period=5000\n#(Optional) c2.rest.callTimeout=10 sec\n#(Optional) c2.agent.identifier=123-456-789\nc2.agent.class=agentClassName\n```\n\n----------------------------------------\n\nTITLE: Original JSON Payload Example for Insert Enrichment Fields Strategy\nDESCRIPTION: Demonstrates the structure of the original FlowFile JSON data used in the Insert Enrichment Fields join strategy. This example contains purchase records with nested customer information and item arrays. Highlights the record structure expected before enrichment insertion. No external dependencies; serves as input data.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.JoinEnrichment/additionalDetails.md#_snippet_3\n\nLANGUAGE: JSON\nCODE:\n```\n[\n  {\n    \"purchase\": {\n      \"customer\": {\n        \"loyaltyId\": 48202,\n        \"firstName\": \"John\",\n        \"lastName\": \"Doe\"\n      },\n      \"total\": 48.28,\n      \"items\": [\n        {\n          \"itemDescription\": \"book\",\n          \"price\": 24.14,\n          \"quantity\": 2\n        }\n      ]\n    }\n  },\n  {\n    \"purchase\": {\n      \"customer\": {\n        \"loyaltyId\": 5512,\n        \"firstName\": \"Jane\",\n        \"lastName\": \"Doe\"\n      },\n      \"total\": 121.44,\n      \"items\": [\n        {\n          \"itemDescription\": \"book\",\n          \"price\": 28.15,\n          \"quantity\": 4\n        },\n        {\n          \"itemDescription\": \"inkpen\",\n          \"price\": 4.42,\n          \"quantity\": 2\n        }\n      ]\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Defining CSV Validation Schema Using Super-CSV Cell Processors in Java\nDESCRIPTION: This snippet outlines the syntax and semantics for specifying schema properties which use super-csv Cell Processors to validate CSV columns. The provided schema is a comma-delimited string where each element corresponds to a column and describes the expected validation, such as parsing to specific data types, checking nullability, enforcing uniqueness, or applying string constraints. Dependencies include the super-csv library and its CellProcessor implementations. Input is a CSV file, and output is validated or rejected records according to the defined processors. Limitations include non-nestable cell processors except when wrapped with Optional.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ValidateCsv/additionalDetails.md#_snippet_0\n\nLANGUAGE: java\nCODE:\n```\n**Schema property:** Null, ParseDate(\"dd/MM/yyyy\"), Optional(ParseDouble())  \n**Meaning:** the input CSV has three columns, the first one can be null and has no specification, the second one must be\na date formatted as expected, and the third one must a double or null (no value).\n\n**Schema property:** ParseBigDecimal(), ParseBool(), ParseChar(), ParseInt(), ParseLong()  \n**Meaning:** the input CSV has five columns, the first one must be a big decimal, the second one must be a boolean, the\nthird one must be a char, the fourth one must be an integer and the fifth one must be a long.\n\n**Schema property:** Equals(), NotNull(), StrNotNullOrEmpty()  \n**Meaning:** the input CSV has three columns, all the values of the first column must be equal to each other, all the\nvalues of the second column must be not null, and all the values of the third column are not null/empty string values.\n\n**Schema property:** Strlen(4), StrMinMax(3,5), StrRegex(\"\\[a-z0-9\\\\\\.\\_\\]+@\\[a-z0-9\\\\\\.\\]+\")  \n**Meaning:** the input CSV has three columns, all the values of the first column must be 4-characters long, all the\nvalues of the second column must be between 3 and 5 characters (inclusive), and all the values of the last column must\nmatch the provided regular expression (email address).\n\n**Schema property:** Unique(), UniqueHashCode()  \n**Meaning:** the input CSV has two columns. All the values of the first column must be unique (all the values are stored\nin memory). All the values of the second column must be unique (only hash codes of the input values are stored to ensure\nuniqueness).\n\n**Schema property:** ForbidSubStr(\"test\", \"tset\"), RequireSubStr(\"test\")  \n**Meaning:** the input CSV has two columns. None of the values in the first column must contain one of the provided\nstrings. And all the values of the second column must contain the provided string.\n```\n\n----------------------------------------\n\nTITLE: Modifying FlowFile Content using StreamCallback in Groovy\nDESCRIPTION: Shows how to read, modify, and overwrite the content of an existing NiFi FlowFile using `session.write()` with a `StreamCallback`. This callback provides both an InputStream (to read original content) and an OutputStream (to write modified content). The example reads the FlowFile content as a string, reverses it, and writes the reversed string back, overwriting the original content. Finally, it transfers the modified FlowFile to the success relationship. Requires `org.apache.commons.io.IOUtils` and `java.nio.charset.StandardCharsets` imports.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-scripting-bundle/nifi-scripting-processors/src/main/resources/docs/org.apache.nifi.processors.script.ExecuteScript/additionalDetails.md#_snippet_12\n\nLANGUAGE: groovy\nCODE:\n```\nimport org.apache.commons.io.IOUtils\nimport java.nio.charset.StandardCharsets\n\nflowFile = session.get()\nif (!flowFile) return\ndef text = 'Hello world!'\n// Cast a closure with an inputStream and outputStream parameter to StreamCallback\nflowFile = session.write(flowFile, { inputStream, outputStream ->\n    text = IOUtils.toString(inputStream, StandardCharsets.UTF_8)\n    outputStream.write(text.reverse().getBytes(StandardCharsets.UTF_8))\n} as StreamCallback)\nsession.transfer(flowFile, REL_SUCCESS)\n```\n\n----------------------------------------\n\nTITLE: Example CSV Input Data for QueryRecord\nDESCRIPTION: Sample CSV data representing the content of an input FlowFile for the QueryRecord processor. This data includes columns for 'name', 'age', and 'title' and is used in subsequent SQL query examples.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.QueryRecord/additionalDetails.md#_snippet_0\n\nLANGUAGE: csv\nCODE:\n```\nname, age, title\nJohn Doe, 34, Software Engineer\nJane Doe, 30, Program Manager\nJacob Doe, 45, Vice President\nJanice Doe, 46, Vice President\n```\n\n----------------------------------------\n\nTITLE: Example JSON Payload for StartDocumentTextDetection in AWS Textract\nDESCRIPTION: This JSON object provides an example request payload for initiating an asynchronous text detection job (detecting lines and words) using the AWS Textract StartDocumentTextDetection API through the NiFi processor. Key parameters include the S3 document location, optional job tags, KMS key IDs, notification channels (SNS), and S3 output configuration.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-aws-bundle/nifi-aws-processors/src/main/resources/docs/org.apache.nifi.processors.aws.ml.textract.StartAwsTextractJob/additionalDetails.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"ClientRequestToken\": \"string\",\n  \"DocumentLocation\": {\n    \"S3Object\": {\n      \"Bucket\": \"string\",\n      \"Name\": \"string\",\n      \"Version\": \"string\"\n    }\n  },\n  \"JobTag\": \"string\",\n  \"KMSKeyId\": \"string\",\n  \"NotificationChannel\": {\n    \"RoleArn\": \"string\",\n    \"SNSTopicArn\": \"string\"\n  },\n  \"OutputConfig\": {\n    \"S3Bucket\": \"string\",\n    \"S3Prefix\": \"string\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Working with DBCP in Groovy\nDESCRIPTION: This example demonstrates interacting with a database using a DBCP connection pool in Groovy. It shows how to execute a query with a prepared statement, retrieve data, and update a database BLOB using the flow file content. A property `SQL.db` is expected to be configured as a DBCPConnectionPool controller service.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-groovyx-bundle/nifi-groovyx-processors/src/main/resources/docs/org.apache.nifi.processors.groovyx.ExecuteGroovyScript/additionalDetails.md#_snippet_7\n\nLANGUAGE: Groovy\nCODE:\n```\nimport groovy.sql.Sql\n\n// define property named `SQL.db` connected to a DBCPConnectionPool controller service\n// for this case it's an H2 database example\n\n// read value from the database with prepared statement\n// and assign into flowfile attribute `db.yesterday`\ndef daysAdd = -1\ndef row = SQL.db.firstRow(\"select dateadd('DAY', ${daysAdd}, sysdate) as DB_DATE from dual\")\nflowFile.'db.yesterday' = row.DB_DATE\n\n// to work with BLOBs and CLOBs in the database\n// use parameter casting using groovy.sql.Sql.BLOB(Stream) and groovy.sql.Sql.CLOB(Reader)\n\n// write content of the flow file into database blob\nflowFile.read { rawIn ->\n    def parms = [\n            p_id  : flowFile.ID as Long, // get flow file attribute named `ID`\n            p_data: Sql.BLOB(rawIn),   // use input stream as BLOB sql parameter\n    ]\n    SQL.db.executeUpdate(parms, \"update mytable set data = :p_data where id = :p_id\")\n}\n```\n\n----------------------------------------\n\nTITLE: Implementing Syslog5424Reader Controller Service in Apache NiFi\nDESCRIPTION: The Syslog5424Reader Controller Service enables parsing of syslog messages in RFC5424 and RFC3164 formats. It provides records with a common schema, facilitating data flow and processing within NiFi, with a required 'Character Set' property to specify encoding. This service converts raw syslog messages into structured data for further analysis or storage.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.syslog.SyslogReader/additionalDetails.md#_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nThe SyslogReader Controller Service provides a means to parse the contents of a Syslog message in accordance to RFC5424 and RFC3164 formats. This reader produces records with a set schema to match the common set of fields between the specifications. The Required Property of this service is named `Character Set` and specifies the Character Set of the incoming text.\n```\n\n----------------------------------------\n\nTITLE: Adding Multiple Attributes to a FlowFile in Groovy for NiFi ExecuteScript\nDESCRIPTION: Adds or updates multiple attributes on a FlowFile simultaneously using a Map and the `session.putAllAttributes(flowFile, attributeMap)` method. An incoming FlowFile is retrieved, and a Map containing the attributes (with String keys and String values) is created. This is more efficient than multiple single `putAttribute` calls. Depends on the implicit `session` object.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-scripting-bundle/nifi-scripting-processors/src/main/resources/docs/org.apache.nifi.processors.script.ExecuteScript/additionalDetails.md#_snippet_5\n\nLANGUAGE: groovy\nCODE:\n```\nattrMap = ['myAttr1': '1', 'myAttr2': Integer.toString(2)]\nflowFile = session.get()\nif (!flowFile) return\nflowFile = session.putAllAttributes(flowFile, attrMap)\n```\n\n----------------------------------------\n\nTITLE: Finding and configuring Box Folder ID in NiFi ListBoxFile processor\nDESCRIPTION: Instructions for retrieving a Box folder ID from the URL and configuring it in the NiFi ListBoxFile processor. The example shows how to extract a folder ID from a Box URL and set it in the processor's 'Folder ID' property.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-box-bundle/nifi-box-processors/src/main/resources/docs/org.apache.nifi.processors.box.ListBoxFile/additionalDetails.md#_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n1. **Find Folder ID**\n    * Navigate to the folder to be listed in Box and enter it. The URL in your browser will include the ID at the end of\n      the URL. For example, if the URL were `https://app.box.com/folder/191632099757`, the Folder ID would be\n      `191632099757`\n2. **Set Folder ID in 'Folder ID' property**\n```\n\n----------------------------------------\n\nTITLE: Using Dynamic Templates in an Elasticsearch Index Request in JSON\nDESCRIPTION: This snippet shows a record containing a message field and a \"dynamic_templates\" field encoded as a JSON string. Using PutElasticsearchRecord, the dynamic_templates field can be extracted and parsed to create custom mappings dynamically during index or create operations. The processor uses the record path to locate dynamic_templates and embed it into the bulk API index action. The second snippet demonstrates the partial document to be indexed as part of the request.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-elasticsearch-bundle/nifi-elasticsearch-restapi-processors/src/main/resources/docs/org.apache.nifi.processors.elasticsearch.PutElasticsearchRecord/additionalDetails.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"message\": \"Hello, world\",\n  \"dynamic_templates\": \"{\\\"message\\\": \\\"keyword_lower\\\"}\"\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"index\": {\n    \"_id\": \"1\",\n    \"_index\": \"test\",\n    \"dynamic_templates\": {\n      \"message\": \"keyword_lower\"\n    }\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"doc\": {\n    \"message\": \"Hello, world\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Replacing Field Values in Records Using Groovy\nDESCRIPTION: This script replaces field values in a record if they match a specified 'Value to Replace' attribute, substituting them with the 'Replacement Value'. It processes nested records recursively, ensuring all matching fields are updated while maintaining schema consistency.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-scripting-bundle/nifi-scripting-processors/src/main/resources/docs/org.apache.nifi.processors.script.ScriptedTransformRecord/additionalDetails.md#_snippet_1\n\nLANGUAGE: Groovy\nCODE:\n```\ndef replace(rec) {\n    rec.toMap().each { k, v ->\n        // If the field value is equal to the attribute 'Value to Replace', then set the         \n        // field value to the 'Replacement Value' attribute.         \n        if (v?.toString()?.equals(attributes['Value to Replace'])) {\n            rec.setValue(k, attributes['Replacement Value'])\n        }\n\n        // Call Recursively if the value is a Record         \n        if (v instanceof org.apache.nifi.serialization.record.Record) {\n            replace(v)\n        }\n    }\n}\n\nreplace(record)\nreturn record\n```\n\n----------------------------------------\n\nTITLE: Querying Max Value in NiFi FlowFile SQL\nDESCRIPTION: This SQL query demonstrates using an aggregate function (MAX) within a single NiFi FlowFile. It selects the 'name' from the FlowFile (treated as a table) where the 'age' column equals the maximum 'age' found within that same FlowFile.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.QueryRecord/additionalDetails.md#_snippet_7\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT name\nFROM FLOWFILE\nWHERE age = (SELECT MAX(age))\n```\n\n----------------------------------------\n\nTITLE: Configuring UpdateRecord Processor to Capitalize 'name' Fields Using Expression Language\nDESCRIPTION: This example shows how to modify all 'name' fields by applying the 'toUpper()' function through Expression Language, thus capitalizing the names across nested record structures. It leverages expression language variables to transform data during processing.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.UpdateRecord/additionalDetails.md#_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nreplacement_value_strategy: Literal Value\n# Capitalize the 'name' fields\n'name': ${field.value:toUpper()}\n```\n\n----------------------------------------\n\nTITLE: Defining Sample Hierarchical JSON Data - JSON\nDESCRIPTION: This snippet provides example hierarchical JSON data including nested arrays (addresses) and objects (project, maintainer) to be used as input for SQL-over-record processing in Apache NiFi. It features a personâ€™s profile with addresses for 'home' and 'work', as well as a nested project record. Data fields include both primitives and objects, which serve to illustrate complex data mapping and querying scenarios in NiFi.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.QueryRecord/additionalDetails.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"John Doe\",\n  \"title\": \"Software Engineer\",\n  \"age\": 40,\n  \"addresses\": [\n    {\n      \"streetNumber\": 4820,\n      \"street\": \"My Street\",\n      \"apartment\": null,\n      \"city\": \"New York\",\n      \"state\": \"NY\",\n      \"country\": \"USA\",\n      \"label\": \"work\"\n    },\n    {\n      \"streetNumber\": 327,\n      \"street\": \"Small Street\",\n      \"apartment\": 309,\n      \"city\": \"Los Angeles\",\n      \"state\": \"CA\",\n      \"country\": \"USA\",\n      \"label\": \"home\"\n    }\n  ],\n  \"project\": {\n    \"name\": \"Apache NiFi\",\n    \"maintainer\": {\n      \"id\": 28302873,\n      \"name\": \"Apache Software Foundation\"\n    },\n    \"debutYear\": 2014\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Transferring a FlowFile to a Relationship in Groovy for NiFi ExecuteScript\nDESCRIPTION: Transfers a processed FlowFile to a designated relationship (e.g., 'success' or 'failure') using `session.transfer(flowFile, relationship)`. It retrieves an incoming FlowFile and performs some processing. Based on the outcome (represented by `errorOccurred`), it transfers the latest reference of the FlowFile to either `REL_FAILURE` or `REL_SUCCESS`. Depends on the implicit `session` object and predefined relationship constants (like `REL_SUCCESS`, `REL_FAILURE`).\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-scripting-bundle/nifi-scripting-processors/src/main/resources/docs/org.apache.nifi.processors.script.ExecuteScript/additionalDetails.md#_snippet_8\n\nLANGUAGE: groovy\nCODE:\n```\nflowFile = session.get()\nif (!flowFile) return\n// Processing occurs here\nif (errorOccurred) {\n    session.transfer(flowFile, REL_FAILURE)\n} else {\n    session.transfer(flowFile, REL_SUCCESS)\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring RouteOnAttribute with NiFi Expression Language\nDESCRIPTION: This configuration example demonstrates how to add a user-defined property to the NiFi RouteOnAttribute processor. It uses the NiFi Expression Language (`${filename:startsWith('ABC')}`) to evaluate the 'filename' attribute of incoming FlowFiles. FlowFiles whose filename starts with 'ABC' will be routed to the relationship named 'ABC'.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.RouteOnAttribute/additionalDetails.md#_snippet_0\n\nLANGUAGE: Text\nCODE:\n```\nproperty name: ABC\nproperty value: ${filename:startsWith('ABC')}\n```\n\n----------------------------------------\n\nTITLE: Extracting Accounts With Parent Field Inclusion Using ForkRecord - JSON\nDESCRIPTION: This example shows the output when the ForkRecord processor is set to include parent fields while extracting 'accounts'. Each account record incorporates the parent's key fields such as name and address, with the child 'id' field taking precedence in cases of naming conflict. The output depends on setting the 'Include parent fields' dynamic property to true and configuring the Record Writer schema appropriately. Each output object includes all parent fields permitted by the schema alongside account fields.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ForkRecord/additionalDetails.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"name\": \"John Doe\",\n    \"address\": \"123 My Street\",\n    \"city\": \"My City\",\n    \"state\": \"MS\",\n    \"zipCode\": \"11111\",\n    \"country\": \"USA\",\n    \"id\": 42,\n    \"balance\": 4750.89\n  },\n  {\n    \"name\": \"John Doe\",\n    \"address\": \"123 My Street\",\n    \"city\": \"My City\",\n    \"state\": \"MS\",\n    \"zipCode\": \"11111\",\n    \"country\": \"USA\",\n    \"id\": 43,\n    \"balance\": 48212.38\n  },\n  {\n    \"name\": \"Jane Doe\",\n    \"address\": \"345 My Street\",\n    \"city\": \"Her City\",\n    \"state\": \"NY\",\n    \"zipCode\": \"22222\",\n    \"country\": \"USA\",\n    \"id\": 45,\n    \"balance\": 6578.45\n  },\n  {\n    \"name\": \"Jane Doe\",\n    \"address\": \"345 My Street\",\n    \"city\": \"Her City\",\n    \"state\": \"NY\",\n    \"zipCode\": \"22222\",\n    \"country\": \"USA\",\n    \"id\": 46,\n    \"balance\": 34567.21\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Query Example\nDESCRIPTION: This JSON snippet provides an example of a valid query for the SearchElasticsearch processor. It includes parameters for size, sort, and match within the 'query' object, as well as aggregations for weekly sales and product terms. The 'size' parameter determines the number of results per page, while 'sort' is required for Search After pagination.  The query targets Elasticsearch and uses its specific query DSL.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-elasticsearch-bundle/nifi-elasticsearch-restapi-processors/src/main/resources/docs/org.apache.nifi.processors.elasticsearch.SearchElasticsearch/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"query\": {\n    \"size\": 10000,\n    \"sort\": {\n      \"product\": \"desc\"\n    },\n    \"match\": {\n      \"restaurant.keyword\": \"Local Pizzaz FTW Inc\"\n    }\n  },\n  \"aggs\": {\n    \"weekly_sales\": {\n      \"date_histogram\": {\n        \"field\": \"date\",\n        \"interval\": \"week\"\n      },\n      \"aggs\": {\n        \"items\": {\n          \"terms\": {\n            \"field\": \"product\",\n            \"size\": 10\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Setting Transaction Thresholds using Properties\nDESCRIPTION: This snippet configures transaction thresholds to control the amount of data allowed into the flow per invocation. It sets the maximum number of FlowFiles to 10 and the maximum data size to 1 MB. This limits the source processors from bringing in more than those amounts of data into the flow each time it is triggered. It utilizes `nifi.stateless.transaction.thresholds.flowfiles` and `nifi.stateless.transaction.thresholds.bytes` properties.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-stateless/nifi-stateless-assembly/README.md#_snippet_4\n\nLANGUAGE: properties\nCODE:\n```\nnifi.stateless.transaction.thresholds.flowfiles=10\nnifi.stateless.transaction.thresholds.bytes=1 MB\n```\n\n----------------------------------------\n\nTITLE: Installing and Starting MiNiFi on Linux/Mac OS X - Shell Commands\nDESCRIPTION: This snippet illustrates shell commands for installing MiNiFi on Linux or Mac OS X systems. It covers extraction of the package, installation as a service optionally under a custom service name, and starting MiNiFi in foreground, background, or as a service. These commands require appropriate permissions and a Unix-like shell environment. Expected input: terminal commands typed after navigating to the MiNiFi installation directory. Output: running MiNiFi agent process in various modes.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-docs/src/main/markdown/minifi-java-agent-quick-start.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nbin/minifi.sh install\n```\n\nLANGUAGE: Shell\nCODE:\n```\nbin/minifi.sh install dataflow\n```\n\nLANGUAGE: Shell\nCODE:\n```\nbin/minifi.sh run\n```\n\nLANGUAGE: Shell\nCODE:\n```\nbin/minifi.sh start\n```\n\nLANGUAGE: Shell\nCODE:\n```\nsudo service minifi start\n```\n\n----------------------------------------\n\nTITLE: Configuring Schema for Column Filtering in GetWorkdayReport\nDESCRIPTION: JSON schema configuration example that demonstrates how to filter out specific columns (name2) from the Workday report output by explicitly defining the writer schema.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-workday-bundle/nifi-workday-processors/src/main/resources/docs/org.apache.nifi.processors.workday.GetWorkdayReport/additionalDetails.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{                   \"name\": \"test\",                   \"namespace\": \"nifi\",                   \"type\": \"record\",                   \"fields\": [                     { \"name\": \"name1\", \"type\": \"string\" }                 ]                 }\n```\n\n----------------------------------------\n\nTITLE: Parsing Log Messages with GrokReader Controller Service in JSON\nDESCRIPTION: The snippet provides example JSON outputs generated by the GrokReader Controller Service when parsing log messages using a Grok Expression. It demonstrates different 'No Match Behavior' options such as 'Append to Previous Message,' 'Skip Line,' and 'Raw Line' by showing how lines failing to match the Grok expression are handled and appear in the resulting JSON record output. The examples illustrate how fields like priority, timestamp, hostname, ident, message, stackTrace, and _raw are populated based on the configured behavior. This snippet requires a Grok pattern file, a Grok Expression property, and optionally a JSON Record Writer.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.grok.GrokReader/additionalDetails.md#_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n[\n  {\n    \"priority\": \"6\",\n    \"timestamp\": \"Feb 28 12:00:00\",\n    \"hostname\": \"192.168.0.1\",\n    \"ident\": \"aliyun\",\n    \"message\": \"[11111]: [error] Syslog test\\nThis is a bad message...\",\n    \"stackTrace\": null,\n    \"_raw\": \"<6>Feb 28 12:00:00 192.168.0.1 aliyun[11111]: [error] Syslog test\\nThis is a bad message...\"\n  }\n]\n```\n\nLANGUAGE: JSON\nCODE:\n```\n[\n  {\n    \"priority\": \"6\",\n    \"timestamp\": \"Feb 28 12:00:00\",\n    \"hostname\": \"192.168.0.1\",\n    \"ident\": \"aliyun\",\n    \"message\": \"[11111]: [error] Syslog test\",\n    \"stackTrace\": null,\n    \"_raw\": \"<6>Feb 28 12:00:00 192.168.0.1 aliyun[11111]: [error] Syslog test\"\n  }\n]\n```\n\nLANGUAGE: JSON\nCODE:\n```\n[\n  {\n    \"priority\": \"6\",\n    \"timestamp\": \"Feb 28 12:00:00\",\n    \"hostname\": \"192.168.0.1\",\n    \"ident\": \"aliyun\",\n    \"message\": \"[11111]: [error] Syslog test\",\n    \"stackTrace\": null,\n    \"_raw\": \"<6>Feb 28 12:00:00 192.168.0.1 aliyun[11111]: [error] Syslog test\"\n  },\n  {\n    \"priority\": null,\n    \"timestamp\": null,\n    \"hostname\": null,\n    \"ident\": null,\n    \"message\": null,\n    \"stackTrace\": null,\n    \"_raw\": \"This is a bad message...\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Defining Apache Log Grok Patterns\nDESCRIPTION: Defines Grok patterns for parsing Apache access logs, including the Common and Combined Log Formats. The COMBINEDAPACHELOG reuses the COMMONAPACHELOG pattern and adds referrer and agent information. The individual fields (clientip, user, timestamp, request, response, bytes) are extracted.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/default-grok-patterns.txt#_snippet_11\n\nLANGUAGE: Grok\nCODE:\n```\nSYSLOGBASE %{SYSLOGTIMESTAMP:timestamp} (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource} %{SYSLOGPROG}:\nCOMMONAPACHELOG %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-)\nCOMBINEDAPACHELOG %{COMMONAPACHELOG} %{QS:referrer} %{QS:agent}\n```\n\n----------------------------------------\n\nTITLE: Schema Definition for Parsed Syslog Records in JSON Schema\nDESCRIPTION: Defines the structure of the parsed syslog message record according to a generic schema with fields typically extracted from syslog messages, such as priority, severity, facility, version, timestamp, hostname, and body. This schema ensures consistent processing of syslog data within NiFi.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.syslog.SyslogReader/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"record\",\n  \"name\": \"nifiRecord\",\n  \"namespace\": \"org.apache.nifi\",\n  \"fields\": [\n    {\n      \"name\": \"priority\",\n      \"type\": [\"null\", \"string\"]\n    },\n    {\n      \"name\": \"severity\",\n      \"type\": [\"null\", \"string\"]\n    },\n    {\n      \"name\": \"facility\",\n      \"type\": [\"null\", \"string\"]\n    },\n    {\n      \"name\": \"version\",\n      \"type\": [\"null\", \"string\"]\n    },\n    {\n      \"name\": \"timestamp\",\n      \"type\": [\"null\", \"string\"]\n    },\n    {\n      \"name\": \"hostname\",\n      \"type\": [\"null\", \"string\"]\n    },\n    {\n      \"name\": \"body\",\n      \"type\": [\"null\", \"string\"]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Number Grok Patterns\nDESCRIPTION: Defines Grok patterns for different types of numbers, including integers, base-10 numbers, base-16 numbers, and floating-point numbers. It allows to capture different numerical values in logs.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/default-grok-patterns.txt#_snippet_7\n\nLANGUAGE: Grok\nCODE:\n```\nINT (?:[+-]?(?:[0-9]+))\nBASE10NUM (?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))\nNUMBER (?:%{BASE10NUM})\nBASE16NUM (?<![0-9A-Fa-f])(?:[+-]?(?:0x)?(?:[0-9A-Fa-f]+))\nBASE16FLOAT \\b(?<![0-9A-Fa-f.])(?:[+-]?(?:0x)?(?:(?:[0-9A-Fa-f]+(?:\\.[0-9A-Fa-f]*)?)|(?:\\.[0-9A-Fa-f]+)))\\b\n```\n\n----------------------------------------\n\nTITLE: XQuery: Return only the first fruit name\nDESCRIPTION: This XQuery expression selects the text content of the 'name' element within the first 'fruit' node. It returns only the name of the first fruit.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.EvaluateXQuery/additionalDetails.md#_snippet_6\n\nLANGUAGE: xquery\nCODE:\n```\n//fruit[1]/text()\n```\n\n----------------------------------------\n\nTITLE: JSON Input Example for Content-based Record Validation\nDESCRIPTION: Sample JSON input data showing railroad company records with company names and number of trains attributes that will be validated based on content values.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-scripting-bundle/nifi-scripting-processors/src/main/resources/docs/org.apache.nifi.processors.script.ScriptedValidateRecord/additionalDetails.md#_snippet_4\n\nLANGUAGE: JSON\nCODE:\n```\n[\n  {\n    \"company\": \"Boston & Maine Railroad\",\n    \"numberOfTrains\": 3\n  },\n  {\n    \"company\": \"Chesapeake & Ohio Railroad\",\n    \"numberOfTrains\": -1\n  },\n  {\n    \"company\": \"Pennsylvania Railroad\",\n    \"numberOfTrains\": 2\n  },\n  {\n    \"company\": \"Reading Railroad\",\n    \"numberOfTrains\": 4\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Update with Retry on Conflict - Elasticsearch JSON\nDESCRIPTION: This snippet shows how to perform an update operation in Elasticsearch using the PutElasticsearchJson processor with the 'retry_on_conflict' bulk header set dynamically. The bulk action includes the update header with a retry option and uses the supplied document for the update. Users must define the 'BULK:retry_on_conflict' dynamic property for the processor in NiFi.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-elasticsearch-bundle/nifi-elasticsearch-restapi-processors/src/main/resources/docs/org.apache.nifi.processors.elasticsearch.PutElasticsearchJson/additionalDetails.md#_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"message\": \"Hello, world\",\n  \"from\": \"john.smith\"\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"update\": {\n    \"_id\": \"1\",\n    \"_index\": \"test\",\n    \"retry_on_conflict\": \"3\"\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"doc\": {\n    \"message\": \"Hello, world\",\n    \"from\": \"john.smith\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: CSV Output Example for Valid Records (Position-based)\nDESCRIPTION: Expected CSV output for records routed to the 'valid' relationship when using position-based validation. Only the first two records (Boston & Maine and Chesapeake & Ohio) are included.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-scripting-bundle/nifi-scripting-processors/src/main/resources/docs/org.apache.nifi.processors.script.ScriptedValidateRecord/additionalDetails.md#_snippet_1\n\nLANGUAGE: CSV\nCODE:\n```\ncompany, numberOfTrains Boston & Maine Railroad, 3 Chesapeake & Ohio Railroad, 2\n```\n\n----------------------------------------\n\nTITLE: Example CEF Messages (CEF Format)\nDESCRIPTION: Illustrates various valid CEF message formats that the CEFReader is capable of parsing. Examples show messages with basic headers, with a Syslog prefix, and with a Syslog prefix followed by custom extension fields, demonstrating the expected input data structure.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.cef.CEFReader/additionalDetails.md#_snippet_0\n\nLANGUAGE: CEF\nCODE:\n```\nCEF:0|Company|Product|1.2.3|audit-login|Successful login|3|\nOct 12 04:16:11 localhost CEF:0|Company|Product|1.2.3|audit-login|Successful login|3|\nOct 12 04:16:11 localhost CEF:0|Company|Product|1.2.3|audit-login|Successful login|3|cn1Label=userid spt=46117 cn1=99999 cfp1=1.23  dst=127.0.0.1 c6a1=2345:0425:2CA1:0000:0000:0567:5673:23b5 dmac=00:0D:60:AF:1B:61 start=1479152665000 end=Jan 12 2017 12:23:45 dlat=456.789 loginsequence=123\n```\n\n----------------------------------------\n\nTITLE: Input JSON Data Structure with Nested Accounts Array\nDESCRIPTION: The original JSON data structure containing an array of user records where each user has a nested 'accounts' array containing multiple account objects.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ForkRecord/additionalDetails.md#_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"id\": 1,\n    \"name\": \"John Doe\",\n    \"address\": \"123 My Street\",\n    \"city\": \"My City\",\n    \"state\": \"MS\",\n    \"zipCode\": \"11111\",\n    \"country\": \"USA\",\n    \"accounts\": [\n      {\n        \"id\": 42,\n        \"balance\": 4750.89\n      },\n      {\n        \"id\": 43,\n        \"balance\": 48212.38\n      }\n    ]\n  },\n  {\n    \"id\": 2,\n    \"name\": \"Jane Doe\",\n    \"address\": \"345 My Street\",\n    \"city\": \"Her City\",\n    \"state\": \"NY\",\n    \"zipCode\": \"22222\",\n    \"country\": \"USA\",\n    \"accounts\": [\n      {\n        \"id\": 45,\n        \"balance\": 6578.45\n      },\n      {\n        \"id\": 46,\n        \"balance\": 34567.21\n      }\n    ]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Avro Schema for Root Node Strategy in Apache NiFi\nDESCRIPTION: An Avro schema definition for processing person records from JSON. This schema is used with the AvroSchemaRegistry Controller Service when applying the Root Node strategy to the JSON document.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.json.JsonTreeReader/additionalDetails.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"namespace\": \"nifi\",\n  \"name\": \"person\",\n  \"type\": \"record\",\n  \"fields\": [\n    {\n      \"name\": \"id\",\n      \"type\": \"int\"\n    },\n    {\n      \"name\": \"name\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"gender\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"dob\",\n      \"type\": {\n        \"type\": \"int\",\n        \"logicalType\": \"date\"\n      }\n    },\n    {\n      \"name\": \"siblings\",\n      \"type\": {\n        \"type\": \"array\",\n        \"items\": {\n          \"type\": \"record\",\n          \"fields\": [\n            {\n              \"name\": \"name\",\n              \"type\": \"string\"\n            }\n          ]\n        }\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Removing FlowFile Attribute in Groovy\nDESCRIPTION: This snippet illustrates how to remove a FlowFile attribute by setting its value to `null`. This is equivalent to calling the `session.removeAttribute` method. Removing an attribute sets it to `null` which is similar to removing the key entirely.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-groovyx-bundle/nifi-groovyx-processors/src/main/resources/docs/org.apache.nifi.processors.groovyx.ExecuteGroovyScript/additionalDetails.md#_snippet_2\n\nLANGUAGE: Groovy\nCODE:\n```\nflowFile.ATTRIBUTE_NAME = null\n// equals to\nflowFile = session.removeAttribute(flowFile, \"ATTRIBUTE_NAME\")\n```\n\n----------------------------------------\n\nTITLE: Defining Avro Schema for ListHDFS Record Output in JSON\nDESCRIPTION: This Avro schema defines the structure of records emitted by the NiFi ListHDFS processor when configured with a Record Writer for batch processing. Each record contains metadata about a listed HDFS file, including filename, path, size, modification time, permissions, owner, group, and HDFS-specific details like replication, encryption, and erasure coding status.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-hadoop-bundle/nifi-hdfs-processors/src/main/resources/docs/org.apache.nifi.processors.hadoop.ListHDFS/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"record\",\n  \"name\": \"nifiRecord\",\n  \"namespace\": \"org.apache.nifi\",\n  \"fields\": [\n    {\n      \"name\": \"filename\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"path\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"directory\",\n      \"type\": \"boolean\"\n    },\n    {\n      \"name\": \"size\",\n      \"type\": \"long\"\n    },\n    {\n      \"name\": \"lastModified\",\n      \"type\": {\n        \"type\": \"long\",\n        \"logicalType\": \"timestamp-millis\"\n      }\n    },\n    {\n      \"name\": \"permissions\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ]\n    },\n    {\n      \"name\": \"owner\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ]\n    },\n    {\n      \"name\": \"group\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ]\n    },\n    {\n      \"name\": \"replication\",\n      \"type\": [\n        \"null\",\n        \"int\"\n      ]\n    },\n    {\n      \"name\": \"symLink\",\n      \"type\": [\n        \"null\",\n        \"boolean\"\n      ]\n    },\n    {\n      \"name\": \"encrypted\",\n      \"type\": [\n        \"null\",\n        \"boolean\"\n      ]\n    },\n    {\n      \"name\": \"erasureCoded\",\n      \"type\": [\n        \"null\",\n        \"boolean\"\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining S3 Object Record Schema in Avro Format\nDESCRIPTION: This JSON snippet defines the schema for records emitted by the ListS3 processor when a Record Writer is configured. It specifies the fields, their data types, and potential null values. It's used to represent the metadata of S3 objects in a structured format for batch processing. The output is a single FlowFile containing the listed S3 object records.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-aws-bundle/nifi-aws-processors/src/main/resources/docs/org.apache.nifi.processors.aws.s3.ListS3/additionalDetails.md#_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"type\": \"record\",\n  \"name\": \"nifiRecord\",\n  \"namespace\": \"org.apache.nifi\",\n  \"fields\": [\n    {\n      \"name\": \"key\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"bucket\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"owner\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ]\n    },\n    {\n      \"name\": \"etag\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"lastModified\",\n      \"type\": {\n        \"type\": \"long\",\n        \"logicalType\": \"timestamp-millis\"\n      }\n    },\n    {\n      \"name\": \"size\",\n      \"type\": \"long\"\n    },\n    {\n      \"name\": \"storageClass\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"latest\",\n      \"type\": \"boolean\"\n    },\n    {\n      \"name\": \"versionId\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ]\n    },\n    {\n      \"name\": \"tags\",\n      \"type\": [\n        \"null\",\n        {\n          \"type\": \"map\",\n          \"values\": \"string\"\n        }\n      ]\n    },\n    {\n      \"name\": \"userMetadata\",\n      \"type\": [\n        \"null\",\n        {\n          \"type\": \"map\",\n          \"values\": \"string\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: JSON Output Example 2 with Force Types = false\nDESCRIPTION: This JSON snippet shows the output of the `ValidateRecord` processor when `Force Types From Reader's Schema = false` in Example 2.  The output is routed to the invalid relationship because the processor doesn't coerce the type of field1, which results in a single string value not complying with the schema.  The processor uses the data as is without coercing to the array type in the schema.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ValidateRecord/additionalDetails.md#_snippet_7\n\nLANGUAGE: JSON\nCODE:\n```\n[\n  {\n    \"field1\": \"content_1\",\n    \"field2\": [\n      \"content_2\",\n      \"content_3\"\n    ]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: XML Structure for Simple Fields in Apache NiFi XMLReader\nDESCRIPTION: Example of XML with simple fields that only contain content without attributes or embedded tags. This represents the simplest type of XML data that can be processed.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLReader/additionalDetails.md#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<root>\n    <record>\n        <simple_field>content</simple_field>\n    </record>\n</root>\n```\n\n----------------------------------------\n\nTITLE: Configuring UpdateRecord Processor to Replace Multiple 'id' Fields with Expression Language\nDESCRIPTION: This snippet configures the processor to replace all 'id' fields across the record hierarchy using an expression that references a FlowFile attribute 'replacement.id'. This allows bulk updates of multiple fields matching 'id' regardless of position in the record.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.UpdateRecord/additionalDetails.md#_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nreplacement_value_strategy: Literal Value\n# Replace all 'id' fields with the value of attribute 'replacement.id'\n'//id': ${replacement.id}\n```\n\n----------------------------------------\n\nTITLE: XQuery: Return only the last fruit name\nDESCRIPTION: This XQuery expression selects the text content of the 'name' element within the last 'fruit' node. It returns only the name of the last fruit.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.EvaluateXQuery/additionalDetails.md#_snippet_7\n\nLANGUAGE: xquery\nCODE:\n```\n//fruit[count(//fruit)]/text()\n```\n\n----------------------------------------\n\nTITLE: Position-based Validation Script in Groovy\nDESCRIPTION: Groovy script that validates records based on their position in the FlowFile. Only the first two records (recordIndex < 2) are considered valid.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-scripting-bundle/nifi-scripting-processors/src/main/resources/docs/org.apache.nifi.processors.script.ScriptedValidateRecord/additionalDetails.md#_snippet_3\n\nLANGUAGE: Groovy\nCODE:\n```\nreturn recordIndex < 2 ? true : false\n```\n\n----------------------------------------\n\nTITLE: MongoDB Bulk Write Operation Example - JSON\nDESCRIPTION: This JSON example demonstrates a flowfile content for the PutMongoBulkOperation processor in Apache NiFi. It contains two operations: inserting a new document with the field \"ho\" set to 42, and updating all documents where the \"hey\" field is greater than zero by incrementing it by 2.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-mongodb-bundle/nifi-mongodb-processors/src/main/resources/docs/org.apache.nifi.processors.mongodb.PutMongoBulkOperation/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"insertOne\": {\n      \"document\": {\n        \"ho\": 42\n      }\n    }\n  },\n  {\n    \"updateMany\": {\n      \"filter\": {\n        \"hey\": {\n          \"$gt\": 0\n        }\n      },\n      \"update\": {\n        \"$inc\": {\n          \"hey\": 2\n        }\n      }\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: CSV Output Example for Invalid Records (Position-based)\nDESCRIPTION: Expected CSV output for records routed to the 'invalid' relationship when using position-based validation. Contains the third and fourth records (Pennsylvania and Reading).\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-scripting-bundle/nifi-scripting-processors/src/main/resources/docs/org.apache.nifi.processors.script.ScriptedValidateRecord/additionalDetails.md#_snippet_2\n\nLANGUAGE: CSV\nCODE:\n```\ncompany, numberOfTrains Pennsylvania Railroad, 4 Reading Railroad, 2\n```\n\n----------------------------------------\n\nTITLE: Processing JSON Arrays with Root Node Strategy in Apache NiFi\nDESCRIPTION: An example JSON array containing person records with nested objects and arrays. This demonstrates the structure that would be processed using the Root Node strategy, where the reader begins at the root element.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.json.JsonTreeReader/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"id\": 17,\n    \"name\": \"John\",\n    \"child\": {\n      \"id\": \"1\"\n    },\n    \"dob\": \"10-29-1982\",\n    \"siblings\": [\n      {\n        \"name\": \"Jeremy\",\n        \"id\": 4\n      },\n      {\n        \"name\": \"Julia\",\n        \"id\": 8\n      }\n    ]\n  },\n  {\n    \"id\": 98,\n    \"name\": \"Jane\",\n    \"child\": {\n      \"id\": 2\n    },\n    \"dob\": \"08-30-1984\",\n    \"gender\": \"F\",\n    \"siblingIds\": [],\n    \"siblings\": []\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Avro Schema for Array of Records in NiFi\nDESCRIPTION: Defines an Avro schema for the 'Array of Records' XML example. It specifies an `array_field` of type `array`, containing records (`RecordInArray`) with string fields. This schema is used by NiFi's Record API.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLReader/additionalDetails.md#_snippet_17\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"namespace\": \"nifi\",\n  \"name\": \"test\",\n  \"type\": \"record\",\n  \"fields\": [\n    {\n      \"name\": \"array_field\",\n      \"type\": {\n        \"type\": \"array\",\n        \"items\": {\n          \"name\": \"RecordInArray\",\n          \"type\": \"record\",\n          \"fields\": [\n            {\n              \"name\": \"embedded_field\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"another_embedded_field\",\n              \"type\": \"string\"\n            }\n          ]\n        }\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Selecting Person Fields Based on Nested Address State Comparison - SQL\nDESCRIPTION: This SQL query, intended for use in the QueryRecord processor in Apache NiFi, retrieves the 'title' and 'name' fields from records where the 'addresses' array contains both a 'home' and 'work' entry with different values in their 'state' field. It uses the custom RPATH() function to extract nested data using Record Path expressions, supporting queries against complex JSON/Avro data structures. The query expects fields named 'title', 'name', and 'addresses' and is executed against a table named 'FLOWFILE'.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.QueryRecord/additionalDetails.md#_snippet_4\n\nLANGUAGE: sql\nCODE:\n```\nSELECT title, name\nFROM FLOWFILE\nWHERE RPATH(addresses, '/state[/label = ''home'']') <> RPATH(addresses, '/state[/label = ''work'']')\n```\n\n----------------------------------------\n\nTITLE: SQL Select with Aliases and Filtering\nDESCRIPTION: This SQL query shows how to select specific columns from two tables, 'original' and 'enrichment', using aliases to rename columns. It performs a LEFT OUTER JOIN and selects columns 'o.id', 'o.name', 'e.customer_name' aliased as 'preferred_name', 'o.age', and 'e.customer_email' aliased as 'email'.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.JoinEnrichment/additionalDetails.md#_snippet_8\n\nLANGUAGE: sql\nCODE:\n```\nSELECT o.id, o.name, e.customer_name AS preferred_name, o.age, e.customer_email AS email\nFROM original o\n         LEFT OUTER JOIN enrichment e\n                         ON o.id = e.customer_id\n```\n\n----------------------------------------\n\nTITLE: Defining Bulletin Record Schema - JSON\nDESCRIPTION: This JSON snippet defines the schema for bulletin records expected by the Site-to-Site Bulletin Reporting Task. The schema includes various fields such as `objectId`, `platform`, `bulletinId`, and many other bulletin-related attributes. The `bulletinTimestamp` field has a specified format of `yyyy-MM-dd'T'HH:mm:ss.SSS'Z'`. This schema is crucial for ensuring that the data conforms to the expected format.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-site-to-site-reporting-bundle/nifi-site-to-site-reporting-task/src/main/resources/docs/org.apache.nifi.reporting.SiteToSiteBulletinReportingTask/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"record\",\n  \"name\": \"bulletins\",\n  \"namespace\": \"bulletins\",\n  \"fields\": [\n    {\n      \"name\": \"objectId\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"platform\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"bulletinId\",\n      \"type\": \"long\"\n    },\n    {\n      \"name\": \"bulletinCategory\",\n      \"type\": [\n        \"string\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"bulletinGroupId\",\n      \"type\": [\n        \"string\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"bulletinGroupName\",\n      \"type\": [\n        \"string\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"bulletinGroupPath\",\n      \"type\": [\n        \"string\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"bulletinLevel\",\n      \"type\": [\n        \"string\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"bulletinMessage\",\n      \"type\": [\n        \"string\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"bulletinNodeAddress\",\n      \"type\": [\n        \"string\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"bulletinNodeId\",\n      \"type\": [\n        \"string\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"bulletinSourceId\",\n      \"type\": [\n        \"string\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"bulletinSourceName\",\n      \"type\": [\n        \"string\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"bulletinSourceType\",\n      \"type\": [\n        \"string\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"bulletinTimestamp\",\n      \"type\": [\n        \"string\",\n        \"null\"\n      ],\n      \"doc\": \"Format: yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"\n    },\n    {\n      \"name\": \"bulletinFlowFileUuid\",\n      \"type\": [\n        \"string\",\n        \"null\"\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Output JSON Data after Path Property Transformation\nDESCRIPTION: The transformed JSON result after applying the path property '/accounts', which splits each user record with multiple accounts into separate records, each containing only one account while maintaining the parent user information.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ForkRecord/additionalDetails.md#_snippet_8\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"id\": 1,\n    \"name\": \"John Doe\",\n    \"address\": \"123 My Street\",\n    \"city\": \"My City\",\n    \"state\": \"MS\",\n    \"zipCode\": \"11111\",\n    \"country\": \"USA\",\n    \"accounts\": [\n      {\n        \"id\": 42,\n        \"balance\": 4750.89\n      }\n    ]\n  },\n  {\n    \"id\": 1,\n    \"name\": \"John Doe\",\n    \"address\": \"123 My Street\",\n    \"city\": \"My City\",\n    \"state\": \"MS\",\n    \"zipCode\": \"11111\",\n    \"country\": \"USA\",\n    \"accounts\": [\n      {\n        \"id\": 43,\n        \"balance\": 48212.38\n      }\n    ]\n  },\n  {\n    \"id\": 2,\n    \"name\": \"Jane Doe\",\n    \"address\": \"345 My Street\",\n    \"city\": \"Her City\",\n    \"state\": \"NY\",\n    \"zipCode\": \"22222\",\n    \"country\": \"USA\",\n    \"accounts\": [\n      {\n        \"id\": 45,\n        \"balance\": 6578.45\n      }\n    ]\n  },\n  {\n    \"id\": 2,\n    \"name\": \"Jane Doe\",\n    \"address\": \"345 My Street\",\n    \"city\": \"Her City\",\n    \"state\": \"NY\",\n    \"zipCode\": \"22222\",\n    \"country\": \"USA\",\n    \"accounts\": [\n      {\n        \"id\": 46,\n        \"balance\": 34567.21\n      }\n    ]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Defining NiFi Provenance Events Avro Schema JSON\nDESCRIPTION: This JSON snippet defines an Avro schema used to represent provenance events within NiFi's Site-to-Site Provenance Reporting Task. It specifies the record name \"provenance\" within the \"provenance\" namespace and lists all event fields with their types, including primitives, nullable types, arrays, and maps. This schema enables structured serialization and deserialization of provenance event data, supporting extensible field details such as event identifiers, timestamps, component metadata, attributes, and relationships. It is essential for configuring Record Writers that output provenance data in formats other than JSON arrays. Dependencies include Avro schema compatibility and NiFi provenance event data conforming to these field definitions.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-site-to-site-reporting-bundle/nifi-site-to-site-reporting-task/src/main/resources/docs/org.apache.nifi.reporting.SiteToSiteProvenanceReportingTask/additionalDetails.md#_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"type\": \"record\",\n  \"name\": \"provenance\",\n  \"namespace\": \"provenance\",\n  \"fields\": [\n    {\n      \"name\": \"eventId\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"eventOrdinal\",\n      \"type\": \"long\"\n    },\n    {\n      \"name\": \"eventType\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"timestampMillis\",\n      \"type\": \"long\"\n    },\n    {\n      \"name\": \"durationMillis\",\n      \"type\": \"long\"\n    },\n    {\n      \"name\": \"lineageStart\",\n      \"type\": {\n        \"type\": \"long\",\n        \"logicalType\": \"timestamp-millis\"\n      }\n    },\n    {\n      \"name\": \"details\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ]\n    },\n    {\n      \"name\": \"componentId\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ]\n    },\n    {\n      \"name\": \"componentType\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ]\n    },\n    {\n      \"name\": \"componentName\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ]\n    },\n    {\n      \"name\": \"processGroupId\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ]\n    },\n    {\n      \"name\": \"processGroupName\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ]\n    },\n    {\n      \"name\": \"entityId\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ]\n    },\n    {\n      \"name\": \"entityType\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ]\n    },\n    {\n      \"name\": \"entitySize\",\n      \"type\": [\n        \"null\",\n        \"long\"\n      ]\n    },\n    {\n      \"name\": \"previousEntitySize\",\n      \"type\": [\n        \"null\",\n        \"long\"\n      ]\n    },\n    {\n      \"name\": \"updatedAttributes\",\n      \"type\": {\n        \"type\": \"map\",\n        \"values\": \"string\"\n      }\n    },\n    {\n      \"name\": \"previousAttributes\",\n      \"type\": {\n        \"type\": \"map\",\n        \"values\": \"string\"\n      }\n    },\n    {\n      \"name\": \"actorHostname\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ]\n    },\n    {\n      \"name\": \"contentURI\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ]\n    },\n    {\n      \"name\": \"previousContentURI\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ]\n    },\n    {\n      \"name\": \"parentIds\",\n      \"type\": {\n        \"type\": \"array\",\n        \"items\": \"string\"\n      }\n    },\n    {\n      \"name\": \"childIds\",\n      \"type\": {\n        \"type\": \"array\",\n        \"items\": \"string\"\n      }\n    },\n    {\n      \"name\": \"platform\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"application\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"remoteIdentifier\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ]\n    },\n    {\n      \"name\": \"alternateIdentifier\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ]\n    },\n    {\n      \"name\": \"transitUri\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: JSON Schema for Tags with Attributes in Apache NiFi XMLReader\nDESCRIPTION: JSON schema definition for XML fields with attributes. The field is defined as a record type with a field for the attribute and a field for the content as specified by the 'Field Name for Content' property.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLReader/additionalDetails.md#_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"test\",\n  \"namespace\": \"nifi\",\n  \"type\": \"record\",\n  \"fields\": [\n    {\n      \"name\": \"field_with_attribute\",\n      \"type\": {\n        \"name\": \"RecordForTag\",\n        \"type\": \"record\",\n        \"fields\": [\n          {\n            \"name\": \"attr\",\n            \"type\": \"string\"\n          },\n          {\n            \"name\": \"field_name_for_content\",\n            \"type\": \"string\"\n          }\n        ]\n      }\n      ]\n    }\n```\n\n----------------------------------------\n\nTITLE: Example MongoDB Aggregation Query\nDESCRIPTION: This JSON snippet demonstrates the structure of a MongoDB aggregation pipeline that the processor can execute. It includes a projection and grouping stage to count documents per domain.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-mongodb-bundle/nifi-mongodb-processors/src/main/resources/docs/org.apache.nifi.processors.mongodb.RunMongoAggregation/additionalDetails.md#_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n[\n  {\n    \"$project\": {\n      \"domain\": 1\n    },\n    \"$group\": {\n      \"_id\": {\n        \"domain\": \"$domain\"\n      },\n      \"total\": {\n        \"$sum\": 1\n      }\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: XML Structure for Nested Tags in Apache NiFi XMLReader\nDESCRIPTION: Example of XML with nested structure where a tag contains other tags. The enclosing field must be defined as a record type in the schema, even if it doesn't have attributes to parse.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLReader/additionalDetails.md#_snippet_8\n\nLANGUAGE: xml\nCODE:\n```\n<record>\n    <field_with_embedded_fields attr=\"attr_content\">\n        <embedded_field>embedded content</embedded_field>\n        <another_embedded_field>another embedded content</another_embedded_field>\n    </field_with_embedded_fields>\n</record>\n```\n\n----------------------------------------\n\nTITLE: Example Groovy Script for Record Partitioning\nDESCRIPTION: This snippet is a Groovy script demonstrating how to partition input records based on the 'stellarType' field. The script accesses the record variable via variable bindings and returns the value of 'stellarType' as the partition key, resulting in dynamic record grouping during flow execution.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-scripting-bundle/nifi-scripting-processors/src/main/resources/docs/org.apache.nifi.processors.script.ScriptedPartitionRecord/additionalDetails.md#_snippet_0\n\nLANGUAGE: Groovy\nCODE:\n```\nreturn record.getValue(\"stellarType\")\n```\n\n----------------------------------------\n\nTITLE: Adding a Single Attribute to a FlowFile in Groovy for NiFi ExecuteScript\nDESCRIPTION: Adds or updates a single attribute on a retrieved FlowFile using `session.putAttribute(flowFile, attributeKey, attributeValue)`. It first gets an incoming FlowFile. Since FlowFiles are immutable, `putAttribute` returns a new FlowFile reference which overwrites the original `flowFile` variable. The attribute value must be a String. Depends on the implicit `session` object.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-scripting-bundle/nifi-scripting-processors/src/main/resources/docs/org.apache.nifi.processors.script.ExecuteScript/additionalDetails.md#_snippet_4\n\nLANGUAGE: groovy\nCODE:\n```\nflowFile = session.get()\nif (!flowFile) return\nflowFile = session.putAttribute(flowFile, 'myAttr', 'myValue')\n```\n\n----------------------------------------\n\nTITLE: Removing Field from JSON Record with RemoveRecordField - JSON\nDESCRIPTION: This snippet demonstrates how the RemoveRecordField processor in Apache NiFi removes a simple field, 'dateOfBirth', from both a JSON record and its Avro schema using a RecordPath expression ('/dateOfBirth'). Requires Apache NiFi, the RemoveRecordField processor, JsonTreeReader, and JsonRecordSetWriter controller services. Inputs are in JSON and an Avro schema provided as a FlowFile attribute. Output is the record and schema with the specified field removed. Limitations include only removing fields explicitly specified in the RecordPath(s).\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.RemoveRecordField/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": 1,\n  \"name\": \"John Doe\",\n  \"dateOfBirth\": \"1980-01-01\"\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"record\",\n  \"name\": \"PersonRecord\",\n  \"namespace\": \"org.apache.nifi\",\n  \"fields\": [\n    {\n      \"name\": \"id\",\n      \"type\": \"int\"\n    },\n    {\n      \"name\": \"name\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"dateOfBirth\",\n      \"type\": \"string\"\n    }\n  ]\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": 1,\n  \"name\": \"John Doe\"\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"record\",\n  \"name\": \"PersonRecord\",\n  \"namespace\": \"org.apache.nifi\",\n  \"fields\": [\n    {\n      \"name\": \"id\",\n      \"type\": \"int\"\n    },\n    {\n      \"name\": \"name\",\n      \"type\": \"string\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Adding Filter Entries in Elasticsearch Query - JSON\nDESCRIPTION: This JSON array demonstrates how to add multiple filter clauses for use in a SearchElasticsearch query. Each object in the array represents a filter, such as matching a specific 'department' value or an exact 'title.keyword'. These filters are provided as a string in the processor's '_query filter_' property and combined with the main query to refine result selection. Dependencies include proper field mapping in Elasticsearch and awareness of potential keyword data types.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-elasticsearch-bundle/nifi-elasticsearch-restapi-processors/src/main/resources/docs/org.apache.nifi.processors.elasticsearch.ConsumeElasticsearch/additionalDetails.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"term\": {\n      \"department\": \"accounts\"\n    }\n  },\n  {\n    \"term\": {\n      \"title.keyword\": \"Annual Report\"\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Filtering Records by Index Position with Groovy Script\nDESCRIPTION: This Groovy script snippet filters records by their zero-based index within a FlowFile. It returns true for the first two records allowing them to pass through, and false for all subsequent records. The script runs for each record and relies on the 'recordIndex' variable. Input and output are in CSV format. This approach is suitable when filtering by record position regardless of content.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-scripting-bundle/nifi-scripting-processors/src/main/resources/docs/org.apache.nifi.processors.script.ScriptedFilterRecord/additionalDetails.md#_snippet_0\n\nLANGUAGE: Groovy\nCODE:\n```\nreturn recordIndex < 2 ? true : false\n```\n\n----------------------------------------\n\nTITLE: Delete All Documents Elasticsearch Query - JSON\nDESCRIPTION: This JSON snippet demonstrates an Elasticsearch Delete By Query DSL query that uses `match_all`. This query is used to select and delete all documents within the target index(es) specified for the NiFi DeleteByQueryElasticsearch processor.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-elasticsearch-bundle/nifi-elasticsearch-restapi-processors/src/main/resources/docs/org.apache.nifi.processors.elasticsearch.DeleteByQueryElasticsearch/additionalDetails.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"query\": {\n    \"match_all\": {}\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Constructing Elasticsearch Query JSON for Timestamp Tracking - JSON\nDESCRIPTION: This JSON object defines an Elasticsearch query that retrieves up to 10,000 documents sorted by the '@timestamp' field in descending order, and filters results to those with a timestamp greater than '2023-09-01'. It demonstrates the structure required for a NiFi SearchElasticsearch processor input, using the 'bool' query and Elastic's range filter. The main parameters are 'size', 'sort', and the filters in the 'bool' and 'range' objects; inputs should ensure the tracked field actually exists and matches one in the target index.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-elasticsearch-bundle/nifi-elasticsearch-restapi-processors/src/main/resources/docs/org.apache.nifi.processors.elasticsearch.ConsumeElasticsearch/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"query\": {\n    \"size\": 10000,\n    \"sort\": {\n      \"@timestamp\": \"desc\"\n    },\n    \"bool\": {\n      \"filter\": [\n        {\n          \"range\": {\n            \"@timestamp\": {\n              \"gt\": \"2023-09-01\"\n            }\n          }\n        }\n      ]\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Transactions with Parent Field Inclusion Using ForkRecord - JSON\nDESCRIPTION: This snippet shows the output of running ForkRecord in extract mode on a multi-nested JSON array, with parent field inclusion enabled and an appropriate Writer schema applied. The resulting flat records each contain both transaction fields and parent account/user fields. Dependencies include correctly configured Reader and Writer schemas and a RecordPath such as '/accounts[*]/transactions'. Input is the multi-nested JSON; output is a flat array of transactional records including parent context fields.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ForkRecord/additionalDetails.md#_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"id\": 5,\n    \"name\": \"John Doe\",\n    \"address\": \"123 My Street\",\n    \"city\": \"My City\",\n    \"state\": \"MS\",\n    \"zipCode\": \"11111\",\n    \"country\": \"USA\",\n    \"amount\": 150.31,\n    \"balance\": 4750.89\n  },\n  {\n    \"id\": 6,\n    \"name\": \"John Doe\",\n    \"address\": \"123 My Street\",\n    \"city\": \"My City\",\n    \"state\": \"MS\",\n    \"zipCode\": \"11111\",\n    \"country\": \"USA\",\n    \"amount\": -15.31,\n    \"balance\": 4750.89\n  },\n  {\n    \"id\": 7,\n    \"name\": \"John Doe\",\n    \"address\": \"123 My Street\",\n    \"city\": \"My City\",\n    \"state\": \"MS\",\n    \"zipCode\": \"11111\",\n    \"country\": \"USA\",\n    \"amount\": 36.78,\n    \"balance\": 48212.38\n  },\n  {\n    \"id\": 8,\n    \"name\": \"John Doe\",\n    \"address\": \"123 My Street\",\n    \"city\": \"My City\",\n    \"state\": \"MS\",\n    \"zipCode\": \"11111\",\n    \"country\": \"USA\",\n    \"amount\": -21.34,\n    \"balance\": 48212.38\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Handling Choice Datatype Fields in Apache NiFi JSON Records\nDESCRIPTION: This snippet demonstrates the behavior when removing a field from a record containing a choice datatype field in the schema. The example shows an input record where the \"name\" field is a CHOICE between a string and a record type. Removing a nested field (firstName) from the choice record does not affect the data because the field is stored as a string, but the schema is updated by removing the 'firstName' field from the record type within the choice. This operation requires Apache NiFi with support for schema-aware processing and demonstrates schema modification distinct from data modification for choice types.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.RemoveRecordField/additionalDetails.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": 12,\n  \"name\": \"John Doe\"\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"record\",\n  \"name\": \"nameRecord\",\n  \"namespace\": \"org.apache.nifi\",\n  \"fields\": [\n    {\n      \"name\": \"id\",\n      \"type\": \"int\"\n    },\n    {\n      \"name\": \"name\",\n      \"type\": [\n        \"string\",\n        {\n          \"type\": \"record\",\n          \"name\": \"nameType\",\n          \"fields\": [\n            {\n              \"name\": \"firstName\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"lastName\",\n              \"type\": \"string\"\n            }\n          ]\n        }\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Document Metadata for Elasticsearch Delete Operation in JSON\nDESCRIPTION: This JSON snippet illustrates how to define a record that specifies a delete operation on a document in Elasticsearch. The metadata block includes the document id, index, type, and the operation set to \"delete\". This structure can be processed by the PutElasticsearchRecord processor with record paths configured to extract these fields to perform deletion of the specified document.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-elasticsearch-bundle/nifi-elasticsearch-restapi-processors/src/main/resources/docs/org.apache.nifi.processors.elasticsearch.PutElasticsearchRecord/additionalDetails.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"metadata\": {\n    \"id\": \"12345\",\n    \"index\": \"test\",\n    \"type\": \"message\",\n    \"operation\": \"delete\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Analyzing Login Success and Failure Events with Python\nDESCRIPTION: This Python code analyzes log entries to detect successful and failed login attempts, useful for security monitoring. It processes log strings, looks for specific keywords, and categorizes events accordingly. Dependencies include the 're' library; inputs are log lines, and outputs are categorized event messages.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/test/resources/cef/single-row-with-extensions.txt#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport re\n\ndef analyze_login_event(log_line):\n    if re.search(r\"Successful login\", log_line):\n        return \"Login successful detected\"\n    elif re.search(r\"Failed login\", log_line):\n        return \"Failed login attempt\"\n    else:\n        return \"Unknown event\"\n```\n\n----------------------------------------\n\nTITLE: Example CSV Output for QueryRecord Relationships\nDESCRIPTION: Illustrates the potential CSV output FlowFiles generated by the QueryRecord processor for the defined SQL-based relationships ('Engineers', 'VP', 'Younger Than Average'). The format assumes a CSV Record Reader and Writer configured with header lines. Note how the schema differs based on the SELECT statement used.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.QueryRecord/additionalDetails.md#_snippet_2\n\nLANGUAGE: csv\nCODE:\n```\nname, age, title\nJohn Doe, 34, Software Engineer\n```\n\nLANGUAGE: csv\nCODE:\n```\nname\nJacob Doe\nJanice Doe\n```\n\nLANGUAGE: csv\nCODE:\n```\nname, age, title\nJohn Doe, 34, Software Engineer\nJane Doe, 30, Program Manager\n```\n\n----------------------------------------\n\nTITLE: Example: Reading Excel Data with Schema Inference and Cache Configuration\nDESCRIPTION: This example describes processing an Excel spreadsheet using NiFi's Record Reader configured to skip headers and interpret date fields with a specified format. It illustrates how the schema registry, schema inference cache, and Date Format property work together to produce structured records with accurate data types, including details on how each record's field values are parsed and represented.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-poi-bundle/nifi-poi-services/src/main/resources/docs/org.apache.nifi.excel.ExcelReader/additionalDetails.md#_snippet_1\n\n\n\n----------------------------------------\n\nTITLE: Defining an Elasticsearch Upsert Action with Script in JSON\nDESCRIPTION: This JSON snippet shows a record including a counter field and an embedded script object for an upsert operation using scripting. The PutElasticsearchRecord processor extracts the script and other fields to generate an update action with a script and upsert parameter, allowing incremental updates through Elasticsearch's painless scripting language. The script's source, language, and parameters must conform to Elasticsearch's scripting API requirements.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-elasticsearch-bundle/nifi-elasticsearch-restapi-processors/src/main/resources/docs/org.apache.nifi.processors.elasticsearch.PutElasticsearchRecord/additionalDetails.md#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"counter\": 1,\n  \"script\": {\n    \"source\": \"ctx._source.counter += params.param1\",\n    \"lang\": \"painless\",\n    \"params\": {\n      \"param1\": 1\n    }\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"update\": {\n    \"_id\": \"1\",\n    \"_index\": \"test\"\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"script\": {\n    \"source\": \"ctx._source.counter += params.param1\",\n    \"lang\": \"painless\",\n    \"params\": {\n      \"param1\": 1\n    }\n  },\n  \"upsert\": {\n    \"counter\": 1\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining a Document with Metadata for Elasticsearch Index Operation in JSON\nDESCRIPTION: This JSON snippet represents a record containing metadata fields including id, index, type, and operation, as well as message content and a timestamp. It demonstrates how to configure per-record indexing actions in the PutElasticsearchRecord processor. The metadata fields control the Elasticsearch document routing and operation type, while the timestamp field can be used for Data Stream ingestion. Dependencies include the PutElasticsearchRecord processor configured with record path operations that extract fields from this record structure.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-elasticsearch-bundle/nifi-elasticsearch-restapi-processors/src/main/resources/docs/org.apache.nifi.processors.elasticsearch.PutElasticsearchRecord/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"metadata\": {\n    \"id\": \"12345\",\n    \"index\": \"test\",\n    \"type\": \"message\",\n    \"operation\": \"index\"\n  },\n  \"message\": \"Hello, world\",\n  \"from\": \"john.smith\",\n  \"ts\": \"2021-12-03'T'14:00:00.000Z\"\n}\n```\n\n----------------------------------------\n\nTITLE: ExtractText Regex Configuration (With Named Groups)\nDESCRIPTION: Configuration examples for the NiFi ExtractText processor using regular expressions with named capture groups enabled (`Enable named group support` set to True). Each property defines a regex pattern (using the `(?s)` flag for DOTALL mode) with named groups (`?<NAME>...`) to extract data. The results are stored in FlowFile attributes named using the property key and the capture group name (e.g., `regex.result2.BAR1`). This allows for more descriptive attribute names based on the captured content.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ExtractText/additionalDetails.md#_snippet_2\n\nLANGUAGE: regex\nCODE:\n```\n(?s)(?<ALL>.*)\n```\n\nLANGUAGE: regex\nCODE:\n```\n(?s).*(?<BAR1>bar1).*\n```\n\nLANGUAGE: regex\nCODE:\n```\n(?s).*?(?<BAR1>bar\\d).*\n```\n\nLANGUAGE: regex\nCODE:\n```\n(?s).*?(?:bar\\d).*?(?<BAR2>bar\\d).*?(?<BAR3>bar3).*\n```\n\nLANGUAGE: regex\nCODE:\n```\n(?s).*(?<BAR3>bar\\d).*\n```\n\nLANGUAGE: regex\nCODE:\n```\n(?s)^(?<ALL>.*)$\n```\n\nLANGUAGE: regex\nCODE:\n```\n(?s)(?<MISS>XXX)\n```\n\n----------------------------------------\n\nTITLE: Content-based Validation Script in Groovy\nDESCRIPTION: Groovy script that validates records based on their content. Records with numberOfTrains greater than or equal to zero are considered valid.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-scripting-bundle/nifi-scripting-processors/src/main/resources/docs/org.apache.nifi.processors.script.ScriptedValidateRecord/additionalDetails.md#_snippet_7\n\nLANGUAGE: Groovy\nCODE:\n```\nif (record.getValue(\"numberOfTrains\").toInteger() >= 0) {\n    return true;\n} else {\n    return false;\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Avro Schema for Customer Data in NiFi (Example 1)\nDESCRIPTION: An Avro schema definition used in conjunction with the first example CSV data. It specifies the structure ('id', 'name', 'balance', 'join_date', 'notes') and data types (int, string, double, date, string) for the NiFi CSV Record Reader, including a logical date type for the 'join_date' field requiring a corresponding 'Date Format' configuration.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.csv.CSVReader/additionalDetails.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"namespace\": \"nifi\",\n  \"name\": \"balances\",\n  \"type\": \"record\",\n  \"fields\": [\n    {\n      \"name\": \"id\",\n      \"type\": \"int\"\n    },\n    {\n      \"name\": \"name\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"balance\",\n      \"type\": \"double\"\n    },\n    {\n      \"name\": \"join_date\",\n      \"type\": {\n        \"type\": \"int\",\n        \"logicalType\": \"date\"\n      }\n    },\n    {\n      \"name\": \"notes\",\n      \"type\": \"string\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Indexing Document with Dynamic Templates - Elasticsearch JSON\nDESCRIPTION: This snippet demonstrates the JSON format of a document ('message') to be indexed in Elasticsearch using the PutElasticsearchJson processor. No dependencies other than having the processor and Elasticsearch connection configured are needed. The input defines a 'message' field, which will be processed according to the dynamic templates associated with the index; the output is the indexed document appearing in Elasticsearch. The processor will use this structure as the core data payload for the bulk API call.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-elasticsearch-bundle/nifi-elasticsearch-restapi-processors/src/main/resources/docs/org.apache.nifi.processors.elasticsearch.PutElasticsearchJson/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"message\": \"Hello, world\"\n}\n```\n\n----------------------------------------\n\nTITLE: XML Input for ValidateRecord Example 1\nDESCRIPTION: This XML snippet represents the input data to the `ValidateRecord` processor, used with the schema defined in the first snippet. It contains nested elements for `field1`, and a single value for `field2`. The processor reads this XML data and uses the schema defined earlier to validate it. The output and routing depends on the `Force Types From Reader's Schema` property.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ValidateRecord/additionalDetails.md#_snippet_1\n\nLANGUAGE: XML\nCODE:\n```\n<test>\n    <field1>\n        <sub_field>content</sub_field>\n    </field1>\n    <field2>content_of_field_2</field2>\n</test>\n```\n\n----------------------------------------\n\nTITLE: Migrating Legacy MiNiFi Configuration Using MiNiFi Toolkit Converter (Shell)\nDESCRIPTION: This shell command migrates a legacy MiNiFi configuration from YAML and bootstrap.conf files into the new MiNiFi JSON raw flow format, updating the bootstrap configuration in the process. The command requires the MiNiFi Toolkit to be downloaded and extracted, then executed from the appropriate directory. Parameters include the source YAML and bootstrap.conf files, and the destination flow.json.raw and bootstrap.conf locations for the new MiNiFi instance. Output is a migrated JSON config file and a copied bootstrap configuration. The command must be run on Unix-like systems (Linux/OS X) with appropriate executable permissions.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-toolkit/minifi-toolkit-assembly/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n./config.sh transform-yml <legacy_minifi_directory>/conf/config.yml <legacy_minifi_directory>/conf/bootstrap.conf <latest_minifi_directory>/conf/flow.json.raw <latest_minifi_directory>/conf/bootstrap.conf\n```\n\n----------------------------------------\n\nTITLE: Setting FlowFile Attribute Value in Groovy\nDESCRIPTION: This code demonstrates how to set a FlowFile attribute's value using various Groovy syntax options. It shows direct assignment, using single quotes, and the `putAttribute` method.  These are equivalent to using `session.putAttribute` with the flowfile and attribute name/value.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-groovyx-bundle/nifi-groovyx-processors/src/main/resources/docs/org.apache.nifi.processors.groovyx.ExecuteGroovyScript/additionalDetails.md#_snippet_1\n\nLANGUAGE: Groovy\nCODE:\n```\nflowFile.ATTRIBUTE_NAME = ATTRIBUTE_VALUE\nflowFile.'mime.type' = 'text/xml'\nflowFile.putAttribute(\"ATTRIBUTE_NAME\", ATTRIBUTE_VALUE)\n// the same as\nflowFile = session.putAttribute(flowFile, \"ATTRIBUTE_NAME\", ATTRIBUTE_VALUE)\n```\n\n----------------------------------------\n\nTITLE: Executing Elasticsearch JSON Query with JsonQueryElasticsearch Processor in JSON\nDESCRIPTION: This JSON snippet illustrates an example Elasticsearch query using the Elasticsearch JSON DSL compatible with Elasticsearch 5.x and newer. It defines a match query filtering documents where the \"restaurant.keyword\" field equals \"Local Pizzaz FTW Inc\", and an aggregation named \"weekly_sales\" which performs a date histogram on the \"date\" field with weekly intervals. This aggregation further nests a terms aggregation \"items\" on the \"product\" field, limiting results to the top 10 terms. This query can be executed as-is by the JsonQueryElasticsearch NiFi processor to retrieve search and aggregation results. Important note: nested aggregations beyond the top level may lose context if split into multiple flowfiles.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-elasticsearch-bundle/nifi-elasticsearch-restapi-processors/src/main/resources/docs/org.apache.nifi.processors.elasticsearch.JsonQueryElasticsearch/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"query\": {\n    \"match\": {\n      \"restaurant.keyword\": \"Local Pizzaz FTW Inc\"\n    }\n  },\n  \"aggs\": {\n    \"weekly_sales\": {\n      \"date_histogram\": {\n        \"field\": \"date\",\n        \"interval\": \"week\"\n      },\n      \"aggs\": {\n        \"items\": {\n          \"terms\": {\n            \"field\": \"product\",\n            \"size\": 10\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Apache NiFi Stateless Engine Properties - Properties File - Plaintext\nDESCRIPTION: This snippet demonstrates a complete example of an Engine Configuration file for Apache NiFi Stateless, showing how to declare standard operational, security, and extension client properties. Dependencies include access to local directories for binaries, content repositories, and certificates for keystores/truststores. Properties cover paths for NARs, working directories, sensitive property encryption key, keystore/truststore file types and passwords, Kerberos kbr5 configuration, as well as extension client settings referencing the Maven Central repository. Each key-value pair configures a specific aspect of Stateless NiFi's runtime, security, or extension download behavior. The snippet requires the user to create appropriate directories and ensure referenced certificate stores exist and are accessible.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-stateless/nifi-stateless-assembly/README.md#_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\nnifi.stateless.nar.directory=/var/lib/nifi/lib\nnifi.stateless.working.directory=/var/lib/nifi/work/stateless\n\nnifi.stateless.security.keystore=/etc/certs/keystore.jks\nnifi.stateless.security.keystoreType=JKS\nnifi.stateless.security.keystorePasswd=my-keystore-password\nnifi.stateless.security.keyPasswd=\nnifi.stateless.security.truststore=/etc/certs/truststore.jks\nnifi.stateless.security.truststoreType=JKS\nnifi.stateless.security.truststorePasswd=my-truststore-password\nnifi.stateless.sensitive.props.key=nifi-stateless\n\n# Pull extensions from Maven Central\nnifi.stateless.extension.client.mvn-central.type=nexus\nnifi.stateless.extension.client.mvn-central.timeout=30 sec\nnifi.stateless.extension.client.mvn-central.baseUrl=https://repo1.maven.org/maven2/\nnifi.stateless.extension.client.mvn-central.useSslContext=false\n\nnifi.stateless.kerberos.krb5.file=/etc/krb5.conf\n```\n\n----------------------------------------\n\nTITLE: Avro Schema for Array Enclosed in a Field in NiFi\nDESCRIPTION: Provides the Avro schema for the 'Array in Record' XML example. The `field_enclosing_array` is defined as a record containing an `element` field of type `array` with string items, suitable for NiFi parsing.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLReader/additionalDetails.md#_snippet_19\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"namespace\": \"nifi\",\n  \"name\": \"test\",\n  \"type\": \"record\",\n  \"fields\": [\n    {\n      \"name\": \"field_enclosing_array\",\n      \"type\": {\n        \"name\": \"EmbeddedRecord\",\n        \"type\": \"record\",\n        \"fields\": [\n          {\n            \"name\": \"element\",\n            \"type\": {\n              \"type\": \"array\",\n              \"items\": \"string\"\n            }\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"field_without_array\",\n      \"type\": \"string\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Reading FlowFile Content using InputStreamCallback in Groovy\nDESCRIPTION: Demonstrates reading the content of an incoming NiFi FlowFile using the `session.read()` method and an `InputStreamCallback`. The example uses `IOUtils.toString()` to convert the InputStream content to a UTF-8 encoded string. Requires `org.apache.commons.io.IOUtils` and `java.nio.charset.StandardCharsets` imports.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-scripting-bundle/nifi-scripting-processors/src/main/resources/docs/org.apache.nifi.processors.script.ExecuteScript/additionalDetails.md#_snippet_10\n\nLANGUAGE: groovy\nCODE:\n```\nimport org.apache.commons.io.IOUtils\nimport java.nio.charset.StandardCharsets\n\nflowFile = session.get()\nif (!flowFile) return\ndef text = ''\n// Cast a closure with an inputStream parameter to InputStreamCallback\nsession.read(flowFile, { inputStream ->\n    text = IOUtils.toString(inputStream, StandardCharsets.UTF_8)\n// Do something with text here\n} as InputStreamCallback)\n```\n\n----------------------------------------\n\nTITLE: XML Structure for Tags with Attributes in Apache NiFi XMLReader\nDESCRIPTION: Example of XML with a field that contains both an attribute and content. This requires special handling with the 'Field Name for Content' property set in the XMLReader.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLReader/additionalDetails.md#_snippet_6\n\nLANGUAGE: xml\nCODE:\n```\n<record>\n    <field_with_attribute attr=\"attr_content\">content of field</field_with_attribute>\n</record>\n```\n\n----------------------------------------\n\nTITLE: Defining Update Operation with Bulk Action Header Field \"retry_on_conflict\" in JSON\nDESCRIPTION: This code demonstrates how to configure dynamic properties with the BULK: prefix to set additional Elasticsearch Bulk API action header fields from the record content. Here, the retry_on_conflict field is extracted from the /retry record path and added to the update action header, allowing conflict retries on updates. The resulting update action includes the header field and the partial document fields under \"doc\".\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-elasticsearch-bundle/nifi-elasticsearch-restapi-processors/src/main/resources/docs/org.apache.nifi.processors.elasticsearch.PutElasticsearchRecord/additionalDetails.md#_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"message\": \"Hello, world\",\n  \"from\": \"john.smith\",\n  \"retry\": 3\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"update\": {\n    \"_id\": \"1\",\n    \"_index\": \"test\",\n    \"retry_on_conflict\": 3\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"doc\": {\n    \"message\": \"Hello, world\",\n    \"from\": \"john.smith\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Fetching Gmail OAuth2 Tokens using Curl (Shell)\nDESCRIPTION: This command uses `curl` to exchange an authorization code for access and refresh tokens from Google's OAuth2 token endpoint (https://accounts.google.com/o/oauth2/token). It requires providing the Client ID, Client Secret, and the Authorization Code obtained from the Google consent screen as parameters. The `redirect_uri` is set to `urn:ietf:wg:oauth:2.0:oob` indicating an out-of-band flow typically used for desktop or command-line applications.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.PutEmail/additionalDetails.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl https://accounts.google.com/o/oauth2/token -d grant_type=authorization_code -d redirect_uri=\"urn:ietf:wg:oauth:2.0:oob\" -d client_id=_CLIENT_ID_ -d client_secret=_CLIENT_SECRET_ -d code=_AUTHORISATION_CODE_\n```\n\n----------------------------------------\n\nTITLE: Executing Paginated Elasticsearch JSON Query using Elasticsearch JSON DSL - JSON\nDESCRIPTION: This JSON snippet provides an example of a paginated Elasticsearch query accepted by the PaginatedJsonQueryElasticsearch processor. It includes a query section with parameters for pagination (size, sort), a match operation, and a top-level aggregation using date_histogram and nested terms aggregation. To execute this snippet, it must be passed as the query input to the processor, ensuring that the required Elasticsearch fields exist. The 'size' and 'sort' parameters are crucial for proper pagination, especially when using Search After. Inputs: valid Elasticsearch index with the referenced fields. Output: paginated query results and aggregation buckets. Dependent on Elasticsearch 5.X+ and the official Elastic client APIs.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-elasticsearch-bundle/nifi-elasticsearch-restapi-processors/src/main/resources/docs/org.apache.nifi.processors.elasticsearch.PaginatedJsonQueryElasticsearch/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"query\": {\n    \"size\": 10000,\n    \"sort\": {\n      \"product\": \"desc\"\n    },\n    \"match\": {\n      \"restaurant.keyword\": \"Local Pizzaz FTW Inc\"\n    }\n  },\n  \"aggs\": {\n    \"weekly_sales\": {\n      \"date_histogram\": {\n        \"field\": \"date\",\n        \"interval\": \"week\"\n      },\n      \"aggs\": {\n        \"items\": {\n          \"terms\": {\n            \"field\": \"product\",\n            \"size\": 10\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: JSON Example of Transformed Output\nDESCRIPTION: This JSON snippet illustrates the output format after applying a transformation using the TransformRecord processor and the provided Groovy script. Fields like 'ssn', 'phone', and 'email' are set to null based on the enrichment data.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.JoinEnrichment/additionalDetails.md#_snippet_11\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": 482028,\n  \"name\": \"John Doe\",\n  \"ssn\": null,\n  \"phone\": null,\n  \"email\": null\n}\n```\n\n----------------------------------------\n\nTITLE: SQL Left Outer Join Query Example\nDESCRIPTION: This SQL query demonstrates how to perform a LEFT OUTER JOIN between two tables, 'original' and 'enrichment'. It ensures that all records from the 'original' table are retained, even if there is no matching record in the 'enrichment' table. Missing values from the 'enrichment' table will be represented as NULL.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.JoinEnrichment/additionalDetails.md#_snippet_7\n\nLANGUAGE: sql\nCODE:\n```\nSELECT o.*, e.*\nFROM original o\nLEFT OUTER JOIN enrichment e\nON o.id = e.customer_id\n```\n\n----------------------------------------\n\nTITLE: Defining Avro Record Schema for ListFTP Results - Apache NiFi - JSON\nDESCRIPTION: This JSON code snippet provides the Avro schema definition used by the ListFTP processor in Apache NiFi when configured with a Record Writer. The schema specifies the structure of the records output for each file discovered in an FTP directory, including fields for file attributes such as filename, path, size, last modified timestamp, permissions, owner, and group. Key fields are: filename (string), path (string), directory (boolean), size (long), lastModified (long, logical type of timestamp-millis), with permissions, owner, and group as nullable strings. Input is not required for this schema; it is used internally for serialization and record-based processing. The schema must be compatible with the configured Record Writer and downstream processors.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ListFTP/additionalDetails.md#_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"type\": \"record\",\n  \"name\": \"nifiRecord\",\n  \"namespace\": \"org.apache.nifi\",\n  \"fields\": [\n    {\n      \"name\": \"filename\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"path\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"directory\",\n      \"type\": \"boolean\"\n    },\n    {\n      \"name\": \"size\",\n      \"type\": \"long\"\n    },\n    {\n      \"name\": \"lastModified\",\n      \"type\": {\n        \"type\": \"long\",\n        \"logicalType\": \"timestamp-millis\"\n      }\n    },\n    {\n      \"name\": \"permissions\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ]\n    },\n    {\n      \"name\": \"owner\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ]\n    },\n    {\n      \"name\": \"group\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Showing Enrichment CSV Payload Example for Wrapper Strategy in JSON\nDESCRIPTION: Sample CSV enrichment data with id and email fields aligned to original records by index for use in the Wrapper join strategy. Illustrates enrichment information corresponding to some original records with a missing entry for one of the original records, showcasing null handling in output.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.JoinEnrichment/additionalDetails.md#_snippet_1\n\nLANGUAGE: CSV\nCODE:\n```\nid, email\n28021, john.doe@nifi.apache.org\n832, jane.doe@nifi.apache.org\n29201, jake.doe@nifi.apache.org\n```\n\n----------------------------------------\n\nTITLE: Copying CA key/cert for key generation - Shell\nDESCRIPTION: This snippet copies the existing CA key and certificate to the working directory and renames them to default names for subsequent operations. These CA keys are used for signing new service or user certificates.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-core/nifi-registry-web-api/src/test/resources/keys/README.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\n# copy existing CA key/cert pair to working directory, rename to default names\ncp /path/to/nifi-registry/nifi-registry-core/nifi-registry-web-api/src/test/resources/keys/ca-key.pem ./nifi-key.key\ncp /path/to/nifi-registry/nifi-registry-core/nifi-registry-web-api/src/test/resources/keys/ca-cert.pem ./nifi-cert.pem\n```\n\n----------------------------------------\n\nTITLE: Example HashiCorp Vault Bootstrap Configuration (Properties)\nDESCRIPTION: Provides an example of the `bootstrap-hashicorp-vault.conf` file used to configure Vault connection details (URI), authentication (optional external file), timeouts, and TLS/SSL settings for the NiFi `StandardHashiCorpVaultClientService`. Properties defined here are utilized when the 'Properties Files' configuration strategy is selected in the controller service.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-hashicorp-vault-bundle/nifi-hashicorp-vault-client-service/src/main/resources/docs/org.apache.nifi.vault.hashicorp.StandardHashiCorpVaultClientService/additionalDetails.md#_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\n# HTTP or HTTPS URI for HashiCorp Vault is required to enable the Sensitive Properties Provider\nvault.uri=https://127.0.0.1:8200\n\n# Optional file supports authentication properties described in the Spring Vault Environment Configuration\n# https://docs.spring.io/spring-vault/docs/2.3.x/reference/html/#vault.core.environment-vault-configuration\n#\n# All authentication properties must be included in bootstrap-hashicorp-vault.conf when this property is not specified.\n# Properties in bootstrap-hashicorp-vault.conf take precedence when the same values are defined in both files.\n# Token Authentication is the default when the 'vault.authentication' property is not specified.\nvault.authentication.properties.file=[full/path/to/vault-auth.properties]\n\n# Optional Timeout properties\nvault.connection.timeout=5 secs\nvault.read.timeout=15 secs\n\n# Optional TLS properties\nvault.ssl.enabledCipherSuites=\nvault.ssl.enabledProtocols=TLSv1.3\nvault.ssl.key-store=[path/to/keystore.p12]\nvault.ssl.key-store-type=PKCS12\nvault.ssl.key-store-password=[keystore password]\nvault.ssl.trust-store=[path/to/truststore.p12]\nvault.ssl.trust-store-type=PKCS12\nvault.ssl.trust-store-password=[truststore password]\n```\n\n----------------------------------------\n\nTITLE: Removing the First Record with Apache NiFi ScriptedTransformRecord in Groovy\nDESCRIPTION: This Groovy script is to be used with the Apache NiFi ScriptedTransformRecord processor in order to remove the first record (index 0) from every incoming FlowFile. It requires NiFi's scripting capability (Groovy language support) and has access to the variable bindings 'record' (the current Record object) and 'recordIndex' (the record's position in the FlowFile). The script should return 'null' for the first record to drop it, or the unchanged record otherwise. The processor will write only non-null returned Records using its configured Record Writer, and any non-Record returns will route the FlowFile to failure. No additional dependencies are required beyond NiFi's standard scripting and record libraries.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-scripting-bundle/nifi-scripting-processors/src/main/resources/docs/org.apache.nifi.processors.script.ScriptedTransformRecord/additionalDetails.md#_snippet_0\n\nLANGUAGE: Groovy\nCODE:\n```\nreturn recordIndex == 0 ? null : record\n```\n\n----------------------------------------\n\nTITLE: Creating a New FlowFile in Groovy for NiFi ExecuteScript\nDESCRIPTION: Generates a new, empty FlowFile using the `session.create()` method. This new FlowFile object can then be populated with content and attributes before being transferred. Depends on the implicit `session` object.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-scripting-bundle/nifi-scripting-processors/src/main/resources/docs/org.apache.nifi.processors.script.ExecuteScript/additionalDetails.md#_snippet_2\n\nLANGUAGE: groovy\nCODE:\n```\nflowFile = session.create()\n// Additional processing here\n```\n\n----------------------------------------\n\nTITLE: Upsert Document With Script - Elasticsearch JSON\nDESCRIPTION: This example demonstrates upserting a document in Elasticsearch with a custom script using the PutElasticsearchJson processor. The script should be a valid JSON object with 'source', 'lang', and 'params'. The input document defines fields to be upserted, and the bulk action combines an 'update' operation with the script and upsert content. This requires Elasticsearch scripting enabled and appropriate permissions.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-elasticsearch-bundle/nifi-elasticsearch-restapi-processors/src/main/resources/docs/org.apache.nifi.processors.elasticsearch.PutElasticsearchJson/additionalDetails.md#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"counter\": 1\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"source\": \"ctx._source.counter += params.param1\",\n  \"lang\": \"painless\",\n  \"params\": {\n    \"param1\": 1\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"update\": {\n    \"_id\": \"1\",\n    \"_index\": \"test\"\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"script\": {\n    \"source\": \"ctx._source.counter += params.param1\",\n    \"lang\": \"painless\",\n    \"params\": {\n      \"param1\": 1\n    }\n  },\n  \"upsert\": {\n    \"counter\": 1\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: JSON Schema for Nested Tags in Apache NiFi XMLReader\nDESCRIPTION: JSON schema definition for nested XML structures. The parent field is defined as a record type with fields for attributes and each of the nested elements.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLReader/additionalDetails.md#_snippet_9\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"test\",\n  \"namespace\": \"nifi\",\n  \"type\": \"record\",\n  \"fields\": [\n    {\n      \"name\": \"field_with_embedded_fields\",\n      \"type\": {\n        \"name\": \"RecordForEmbedded\",\n        \"type\": \"record\",\n        \"fields\": [\n          {\n            \"name\": \"attr\",\n            \"type\": \"string\"\n          },\n          {\n            \"name\": \"embedded_field\",\n            \"type\": \"string\"\n          },\n          {\n            \"name\": \"another_embedded_field\",\n            \"type\": \"string\"\n          }\n        ]\n      }\n      ]\n    }\n```\n\n----------------------------------------\n\nTITLE: Linking Docker Unix Socket (Mac)\nDESCRIPTION: This command creates a symbolic link from the Docker Unix Socket in the user's home directory to the standard location.  It's necessary for Testcontainers to communicate with Docker on macOS.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-elasticsearch-bundle/README.md#_snippet_3\n\nLANGUAGE: sh\nCODE:\n```\nsudo ln -s $HOME/.docker/run/docker.sock /var/run/docker.sock\n```\n\n----------------------------------------\n\nTITLE: Parsing XML without Attributes using NiFi XMLReader (XML)\nDESCRIPTION: This snippet demonstrates how the NiFi XMLReader behaves when 'Parse XML Attributes' is set to false. The input XML includes an element with an attribute, but the parsed output discards the attribute entirely. No additional dependencies are required apart from the XMLReader itself. The expected input is a standard XML document; the output omits attributes, potentially leading to data loss if the element also had complex content or children.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLReader/additionalDetails.md#_snippet_10\n\nLANGUAGE: xml\nCODE:\n```\n<record>\n    <field_with_attribute attr=\"attr_content\">content of field</field_with_attribute>\n</record>\n```\n\nLANGUAGE: xml\nCODE:\n```\n<record>\n    <field_with_attribute>content of field</field_with_attribute>\n</record>\n```\n\n----------------------------------------\n\nTITLE: XML Output Example for Simple Records - XML\nDESCRIPTION: Demonstrates the XML output generated by the XMLRecordSetWriter when multiple records are wrapped with a configurable root node and record node names. Each field value is converted into its own XML tag nested inside record nodes, all under the root node.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLRecordSetWriter/additionalDetails.md#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<root_name>\n    <record_name>\n        <name1>value1</name1>\n        <name2>42</name2>\n    </record_name>\n    <record_name>\n        <name1>value2</name1>\n        <name2>84</name2>\n    </record_name>\n</root_name>\n```\n\n----------------------------------------\n\nTITLE: Removing Nested Fields from JSON Record with RemoveRecordField - JSON\nDESCRIPTION: This snippet illustrates the removal of all 'zip' fields from nested address records in a JSON object and its schema using the RemoveRecordField processor in Apache NiFi with a wildcard RecordPath ('*/zip'). It requires NiFi's RemoveRecordField processor, Reader/Writer controller services configured for JSON, and Avro schema as an attribute. Inputs feature nested fields; after processing, output data and schema both reflect the absence of the 'zip' fields from multiple levels. Limitations are that only fields matching the RecordPath are affected, and all matching fields are removed.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.RemoveRecordField/additionalDetails.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": 1,\n  \"name\": \"John Doe\",\n  \"homeAddress\": {\n    \"zip\": 1111,\n    \"street\": \"Main\",\n    \"number\": 24\n  },\n  \"mailingAddress\": {\n    \"zip\": 1121,\n    \"street\": \"Airport\",\n    \"number\": 12\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"PersonRecord\",\n  \"type\": \"record\",\n  \"namespace\": \"org.apache.nifi\",\n  \"fields\": [\n    {\n      \"name\": \"id\",\n      \"type\": \"int\"\n    },\n    {\n      \"name\": \"name\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"homeAddress\",\n      \"type\": {\n        \"name\": \"address\",\n        \"type\": \"record\",\n        \"fields\": [\n          {\n            \"name\": \"zip\",\n            \"type\": \"int\"\n          },\n          {\n            \"name\": \"street\",\n            \"type\": \"string\"\n          },\n          {\n            \"name\": \"number\",\n            \"type\": \"int\"\n          }\n        ]\n      }\n    },\n    {\n      \"name\": \"mailingAddress\",\n      \"type\": \"address\"\n    }\n  ]\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": 1,\n  \"name\": \"John Doe\",\n  \"homeAddress\": {\n    \"street\": \"Main\",\n    \"number\": 24\n  },\n  \"mailingAddress\": {\n    \"street\": \"Airport\",\n    \"number\": 12\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Logging Messages in Groovy for NiFi ExecuteScript\nDESCRIPTION: Sends messages to the NiFi application log using the implicit `log` object and its methods (`info`, `warn`, `error`, `debug`, `trace`). The example uses `log.info` with message placeholders `{}` and an Object array `['Hello', 1, true]` to dynamically insert values into the log message. Depends on the implicit `log` object provided by NiFi.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-scripting-bundle/nifi-scripting-processors/src/main/resources/docs/org.apache.nifi.processors.script.ExecuteScript/additionalDetails.md#_snippet_9\n\nLANGUAGE: groovy\nCODE:\n```\nlog.info('Found these things: {} {} {}', ['Hello', 1, true] as Object[])\n```\n\n----------------------------------------\n\nTITLE: Importing JKS Keystore to PKCS12 - Shell/Keytool\nDESCRIPTION: Imports a key and certificate entry from an existing JKS keystore (`keys/user1-ks.jks`) back into a new PKCS12 keystore (`keys/user1-ks.p12`). This step might be used to ensure a PKCS12 version is available after the initial JKS creation from an input PKCS12. Requires the `keytool` utility.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-core/nifi-registry-web-api/src/test/resources/keys/README.md#_snippet_8\n\nLANGUAGE: Shell\nCODE:\n```\nkeytool -importkeystore \\\n      -srckeystore keys/user1-ks.jks -srcstoretype jks -srcstorepass password \\\n      -destkeystore keys/user1-ks.p12 -deststoretype pkcs12 -deststorepass password\n```\n\n----------------------------------------\n\nTITLE: Accessing SQL Database in NiFi Groovy Script\nDESCRIPTION: Example showing how to access a database connection from a Groovy script in NiFi. It demonstrates how to execute a SQL query using a connection defined in a SQL.mydb property linked to a DBCPService.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-groovyx-bundle/nifi-groovyx-processors/src/main/resources/docs/org.apache.nifi.processors.groovyx.ExecuteGroovyScript/additionalDetails.md#_snippet_0\n\nLANGUAGE: Groovy\nCODE:\n```\nSQL.mydb.rows('select * from mytable')\n```\n\n----------------------------------------\n\nTITLE: Example DeleteByQuery Elasticsearch Query - JSON\nDESCRIPTION: This JSON snippet provides an example of an Elasticsearch Delete By Query DSL query. It uses a `match` query to target documents where the `username.keyword` field equals \"john.smith\". This query can be used with the NiFi DeleteByQueryElasticsearch processor to remove specific documents.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-elasticsearch-bundle/nifi-elasticsearch-restapi-processors/src/main/resources/docs/org.apache.nifi.processors.elasticsearch.DeleteByQueryElasticsearch/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"query\": {\n    \"match\": {\n      \"username.keyword\": \"john.smith\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring StatusLogger Properties in bootstrap.conf\nDESCRIPTION: Shows example configuration properties for the `StatusLogger` within the `bootstrap.conf` file. These properties define the specific FlowStatus query to execute (`instance:health,bulletins`), the log level (`INFO`) at which the results will be logged, and the reporting period in milliseconds (`60000`).\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-docs/src/main/markdown/System_Admin_Guide.md#_snippet_5\n\nLANGUAGE: properties\nCODE:\n```\n# The FlowStatus query to submit to the MiNiFi instance\nnifi.minifi.status.reporter.log.query=instance:health,bulletins\n# The log level at which the status will be logged\nnifi.minifi.status.reporter.log.level=INFO\n# The period (in milliseconds) at which to log the status\nnifi.minifi.status.reporter.log.period=60000\n```\n\n----------------------------------------\n\nTITLE: Example Output JSON for Wrapper JoinEnrichment Strategy\nDESCRIPTION: Output JSON representing the combined result of the Wrapper strategy where each original record is paired by index with the enrichment record. The result is an array of objects each containing an 'original' and 'enrichment' field containing corresponding records or null if missing. Useful for validating correct join operation.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.JoinEnrichment/additionalDetails.md#_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\n[\n  {\n    \"original\": {\n      \"id\": 28021,\n      \"name\": \"John Doe\",\n      \"age\": 55\n    },\n    \"enrichment\": {\n      \"id\": 28021,\n      \"email\": \"john.doe@nifi.apache.org\"\n    }\n  },\n  {\n    \"original\": {\n      \"id\": 832,\n      \"name\": \"Jane Doe\",\n      \"age\": 22\n    },\n    \"enrichment\": {\n      \"id\": 832,\n      \"email\": \"jane.doe@nifi.apache.org\"\n    }\n  },\n  {\n    \"original\": {\n      \"id\": 29201,\n      \"name\": \"Jake Doe\",\n      \"age\": 23\n    },\n    \"enrichment\": {\n      \"id\": 29201,\n      \"email\": \"jake.doe@nifi.apache.org\"\n    }\n  },\n  {\n    \"original\": {\n      \"id\": 555,\n      \"name\": \"Joseph Doe\",\n      \"age\": 2\n    },\n    \"enrichment\": null\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: ExtractText Regex Configuration (No Named Groups)\nDESCRIPTION: Configuration examples for the NiFi ExtractText processor using standard regular expressions without named capture groups. Each property defines a regex pattern (using the `(?s)` flag for DOTALL mode) to extract data from the input. The results are stored in FlowFile attributes named according to the property key (e.g., `regex.result1`). For multiple capture groups (like in `regex.result4`), numbered attributes are generated (e.g., `regex.result4.1`, `regex.result4.2`).\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ExtractText/additionalDetails.md#_snippet_1\n\nLANGUAGE: regex\nCODE:\n```\n(?s)(.*)\n```\n\nLANGUAGE: regex\nCODE:\n```\n(?s).*(bar1).*\n```\n\nLANGUAGE: regex\nCODE:\n```\n(?s).*?(bar\\d).*\n```\n\nLANGUAGE: regex\nCODE:\n```\n(?s).*?(?:bar\\d).*?(bar\\d).*?(bar3).*\n```\n\nLANGUAGE: regex\nCODE:\n```\n(?s).*(bar\\d).*\n```\n\nLANGUAGE: regex\nCODE:\n```\n(?s)^(.*)$\n```\n\nLANGUAGE: regex\nCODE:\n```\n(?s)(XXX)\n```\n\n----------------------------------------\n\nTITLE: Example Gmail OAuth2 Token Response (JSON)\nDESCRIPTION: This JSON object represents the typical successful response from Google's OAuth2 token endpoint after exchanging an authorization code. It includes the `access_token` for API calls, its `expires_in` duration (in seconds), the crucial `refresh_token` used by NiFi's StandardOauth2AccessTokenProvider to obtain new access tokens without user interaction, the granted `scope` (https://mail.google.com/), and the `token_type` (Bearer).\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.PutEmail/additionalDetails.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"access_token\": \"ACCESS_TOKEN\",\n  \"expires_in\": 3599,\n  \"refresh_token\": \"REFRESH_TOKEN\",\n  \"scope\": \"https://mail.google.com/\",\n  \"token_type\": \"Bearer\"\n}\n```\n\n----------------------------------------\n\nTITLE: Specifying Flow Location using Properties\nDESCRIPTION: This snippet demonstrates how to specify the location of the flow using properties. It shows an example of creating a parameter context named 'Kafka Parameter Context' and defining two parameters named 'Kafka Topic' and 'Kafka Brokers' within the context. This is accomplished by specifying the name of parameter context, and the parameters with associated key value pairs.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-stateless/nifi-stateless-assembly/README.md#_snippet_3\n\nLANGUAGE: properties\nCODE:\n```\nnifi.stateless.parameters.kafka=Kafa Parameter Context\nnifi.stateless.parameters.kafka.Kafka Topic=Sensor Data\nnifi.stateless.parameters.kafka.Kafka Brokers=kafka-01:9092,kafka-02:9092,kafka-03:9092\n```\n\n----------------------------------------\n\nTITLE: Filtering Records by Content Field with Groovy Script\nDESCRIPTION: This Groovy script filters records based on the value of the 'allyOf' field within each record. The script returns true only if the 'allyOf' attribute equals \"Athens\", which includes those records in the output FlowFile routed to the 'success' relationship. The input is JSON formatted records, with the output being a filtered JSON array. The script accesses record fields using 'record.getValue(\"allyOf\")' and must return a boolean per record to decide filtering.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-scripting-bundle/nifi-scripting-processors/src/main/resources/docs/org.apache.nifi.processors.script.ScriptedFilterRecord/additionalDetails.md#_snippet_1\n\nLANGUAGE: Groovy\nCODE:\n```\nif (record.getValue(\"allyOf\") == \"Athens\") {\n    return true;\n} else {\n    return false;\n}\n```\n\n----------------------------------------\n\nTITLE: Example Log Data for NiFi Grok\nDESCRIPTION: This snippet provides example log data that can be used with the NiFi Grok Controller Service. It shows multiple log entries with varying levels and stack traces, allowing users to understand how the Grok expression parses different types of log messages. The example log data is used in conjunction with the Grok expression to demonstrate the extraction of timestamp, level, thread, class, and message fields.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.grok.GrokReader/additionalDetails.md#_snippet_1\n\nLANGUAGE: Text\nCODE:\n```\n2016-08-04 13:26:32,473 INFO [Leader Election Notification Thread-1] o.a.n.c.l.e.CuratorLeaderElectionManager org.apache.nifi.controller.leader.election.CuratorLeaderElectionManager$ElectionListener@1fa27ea5 has been interrupted; no longer leader for role 'Cluster Coordinator'\n2016-08-04 13:26:32,474 ERROR [Leader Election Notification Thread-2] o.apache.nifi.controller.FlowController One\nTwo\nThree\norg.apache.nifi.exception.UnitTestException: Testing to ensure we are able to capture stack traces\n\tat org.apache.nifi.cluster.coordination.node.NodeClusterCoordinator.getElectedActiveCoordinatorAddress(NodeClusterCoordinator.java:185)\n\tat org.apache.nifi.cluster.coordination.node.NodeClusterCoordinator.getElectedActiveCoordinatorAddress(NodeClusterCoordinator.java:185)\n\tat org.apache.nifi.cluster.coordination.node.NodeClusterCoordinator.getElectedActiveCoordinatorAddress(NodeClusterCoordinator.java:185)\n\tat org.apache.nifi.cluster.coordination.node.NodeClusterCoordinator.getElectedActiveCoordinatorAddress(NodeClusterCoordinator.java:185)\n\tat org.apache.nifi.cluster.coordination.node.NodeClusterCoordinator.getElectedActiveCoordinatorAddress(NodeClusterCoordinator.java:185)\n\tat org.apache.nifi.cluster.coordination.node.NodeClusterCoordinator.getElectedActiveCoordinatorAddress(NodeClusterCoordinator.java:185)\n\tat org.apache.nifi.cluster.coordination.node.NodeClusterCoordinator.getElectedActiveCoordinatorAddress(NodeClusterCoordinator.java:185)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_45]\n\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [na:1.8.0_45]\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [na:1.8.0_45]\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [na:1.8.0_45]\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_45]\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_45]\n\tat java.lang.Thread.run(Thread.java:745) [na:1.8.0_45]\nCaused by: org.apache.nifi.exception.UnitTestException: Testing to ensure we are able to capture stack traces\n\tat org.apache.nifi.cluster.coordination.node.NodeClusterCoordinator.getElectedActiveCoordinatorAddress(NodeClusterCoordinator.java:185)\n\tat org.apache.nifi.cluster.coordination.node.NodeClusterCoordinator.getElectedActiveCoordinatorAddress(NodeClusterCoordinator.java:185)\n\tat org.apache.nifi.cluster.coordination.node.NodeClusterCoordinator.getElectedActiveCoordinatorAddress(NodeClusterCoordinator.java:185)\n\tat org.apache.nifi.cluster.coordination.node.NodeClusterCoordinator.getElectedActiveCoordinatorAddress(NodeClusterCoordinator.java:185)\n\tat org.apache.nifi.cluster.coordination.node.NodeClusterCoordinator.getElectedActiveCoordinatorAddress(NodeClusterCoordinator.java:185)\n\t... 12 common frames omitted\n2016-08-04 13:26:35,475 WARN [Curator-Framework-0] org.apache.curator.ConnectionState Connection attempt unsuccessful after 3008 (greater than max timeout of 3000). Resetting connection and trying again with a new connection.\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Stateless with Registry Flow Import in Properties File\nDESCRIPTION: A complete configuration example for a NiFi Stateless dataflow that imports a flow from NiFi Registry with parameter contexts and reporting tasks defined. The configuration specifies registry connection, flow identifiers, parameters, and a reporting task.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-stateless/nifi-stateless-assembly/README.md#_snippet_6\n\nLANGUAGE: properties\nCODE:\n```\nnifi.stateless.registry.url=https://nifi-registry/\nnifi.stateless.flow.bucketId=00000000-0000-0000-0000-000000000011\nnifi.stateless.flow.id=00000000-0000-0000-0000-000000000044\nnifi.stateless.flow.version=5\n\nnifi.stateless.parameters.kafkahdfs=Kafka to HDFS\nnifi.stateless.parameters.kafkahdfs.Kafka Topic=Sensor Data\nnifi.stateless.parameters.kafkahdfs.Kafka Brokers=kafka-01:9092,kafka-02:9092,kafka-03:9093\nnifi.stateless.parameters.kafkahdfs.HDFS Directory=/data/sensors\n\nnifi.stateless.reporting.task.stats.name=Stats\nnifi.stateless.reporting.task.stats.type=ControllerStatusReportingTask\nnifi.stateless.reporting.task.stats.bundle=org.apache.nifi:nifi-standard-nar:1.12.1\nnifi.stateless.reporting.task.stats.properties.Show Deltas=false\nnifi.stateless.reporting.task.stats.frequency=1 minute\nnifi.stateless.reporting.task.stats.properties.Reporting Granularity=One Second\n```\n\n----------------------------------------\n\nTITLE: Adding New Fields to Records in Apache NiFi\nDESCRIPTION: This script adds a 'favoriteColor' field with a fixed value to all records and an 'isOdd' boolean field to indicate odd-numbered records. It ensures all records share the same schema by adding fields with null values where appropriate, facilitating consistent schema evolution.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-scripting-bundle/nifi-scripting-processors/src/main/resources/docs/org.apache.nifi.processors.script.ScriptedTransformRecord/additionalDetails.md#_snippet_3\n\nLANGUAGE: Groovy\nCODE:\n```\nimport org.apache.nifi.serialization.record.RecordField;\nimport org.apache.nifi.serialization.record.RecordFieldType;\n\n// Always set favoriteColor to Blue. // Because we are calling #setValue with a String as the field name, the field type will be inferred. record.setValue(\"favoriteColor\", \"Blue\")  \n// Set the 'isOdd' field to true if the record index is odd. Otherwise, set the 'isOdd' field to `null`. \n// Because the value may be `null` for the first Record (in fact, it always will be for this particular case), \n// we need to ensure that the Record Writer's schema be given the correct type for the field. As a result, we will not call \n// #setValue with a String as the field name but rather will pass a RecordField as the first argument, as the RecordField \n// allows us to specify the type of the field. \n// Also note that `RecordField` and `RecordFieldType` are `import`ed above. record.setValue(new RecordField(\"isOdd\", RecordFieldType.BOOLEAN.getDataType()), recordIndex % 2 == 1 ? true : null)  return record\n```\n\n----------------------------------------\n\nTITLE: Defining Syslog Program Grok Pattern\nDESCRIPTION: Defines Grok patterns for matching program names and syslog program identifiers. SYSLOGPROG extracts the program name and process ID (pid) from syslog messages. It depends on the PROG and POSINT patterns. PROG matches a string consisting of word characters, dots, underscores, forward slashes, percentage signs, etc.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/default-grok-patterns.txt#_snippet_2\n\nLANGUAGE: Grok\nCODE:\n```\nPROG (?:[\\w._/%-]+)\nSYSLOGPROG %{PROG:program}(?:\\[%{POSINT:pid}\\])?\n```\n\n----------------------------------------\n\nTITLE: Writing Content to FlowFile in Groovy\nDESCRIPTION: This example demonstrates various ways to write content to a FlowFile, replacing its existing content. It includes writing a CharSequence with a specified encoding, using a Writer, and using an OutputStream or both an InputStream and OutputStream within a closure.  The `write` method requires a closure or a CharSequence and an encoding.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-groovyx-bundle/nifi-groovyx-processors/src/main/resources/docs/org.apache.nifi.processors.groovyx.ExecuteGroovyScript/additionalDetails.md#_snippet_4\n\nLANGUAGE: Groovy\nCODE:\n```\nflowFile.write(\"UTF-8\", \"THE CharSequence to write into flow file replacing current content\")\nflowFile.write(\"UTF-8\") { writer ->\n    // do something with java.io.Writer...\n}\nflowFile.write { outStream ->\n    // do something with output stream...\n}\nflowFile.write { inStream, outStream ->\n    // do something with input and output streams...\n}\n```\n\n----------------------------------------\n\nTITLE: Overriding NiFi Property Directories (YAML)\nDESCRIPTION: This YAML snippet demonstrates how to override default directory locations for NiFi repositories (flowfile, content, database) using the `NiFi Properties Overrides` configuration section in MiNiFi. This allows users to specify custom paths for storing data by directly setting corresponding `nifi.properties` values.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-docs/src/main/markdown/System_Admin_Guide.md#_snippet_14\n\nLANGUAGE: YAML\nCODE:\n```\nNiFi Properties Overrides:\n  nifi.flowfile.repository.directory: ./flowfile_repository_override\n  nifi.content.repository.directory.default: ./content_repository_override\n  nifi.database.directory: ./database_repository_override\n```\n\n----------------------------------------\n\nTITLE: Update Document Without Script - Elasticsearch JSON\nDESCRIPTION: This snippet demonstrates the JSON format for updating a document in Elasticsearch using the PutElasticsearchJson processor, without a script. The input document includes fields to be updated, and the output action specifies the 'update' operation in Elasticsearch. The processor expects an ID and index to be provided either via the FlowFile attributes or configuration. The document content is used for the actual update operation.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-elasticsearch-bundle/nifi-elasticsearch-restapi-processors/src/main/resources/docs/org.apache.nifi.processors.elasticsearch.PutElasticsearchJson/additionalDetails.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"message\": \"Hello, world\",\n  \"from\": \"john.smith\"\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"update\": {\n    \"_id\": \"1\",\n    \"_index\": \"test\"\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"doc\": {\n    \"message\": \"Hello, world\",\n    \"from\": \"john.smith\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: XQuery: Return fruit colors and names as comma separated list\nDESCRIPTION: This XQuery expression extracts the color and name of each fruit, joins them with a space, and then joins all the color-name pairs with a comma and a space. It returns a single string with all color-name pairs separated by commas.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.EvaluateXQuery/additionalDetails.md#_snippet_9\n\nLANGUAGE: xquery\nCODE:\n```\nstring-join((for $y in (for $x in //fruit return string-join(($x/color/text() , $x/name/text()), ' ')) return\n      $y), ', ')\n```\n\n----------------------------------------\n\nTITLE: Defining Jolt Transformation for Ambari Metrics Format in JSON\nDESCRIPTION: This JSON snippet defines a Jolt transformation spec used to reshape NiFi metrics data into the Ambari Metrics Collector API format. It dynamically shifts and maps metric timestamps and values into structured 'time' and 'value' keys within the 'metrics' array. The transformation is applied to JSON input containing nested metrics keyed by timestamp. This spec requires a Jolt library or compatible processor to perform the transformation during runtime.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-site-to-site-reporting-bundle/nifi-site-to-site-reporting-task/src/main/resources/docs/org.apache.nifi.reporting.SiteToSiteMetricsReportingTask/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"operation\": \"shift\",\n    \"spec\": {\n      \"metrics\": {\n        \"*\": {\n          \"metrics\": {\n            \"*\": {\n              \"$\": \"metrics.[#4].metrics.time\",\n              \"@\": \"metrics.[#4].metrics.value\"\n            }\n          },\n          \"*\": \"metrics.[&1].&\"\n        }\n      }\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Example JSON Payload for StartExpenseAnalysis in AWS Textract\nDESCRIPTION: This JSON object shows an example request payload for starting an asynchronous expense analysis job (e.g., for invoices or receipts) via the AWS Textract StartExpenseAnalysis API using the NiFi processor. It includes the S3 document location, optional job tags, KMS key IDs for encryption, notification channels (SNS), and output configuration for storing results.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-aws-bundle/nifi-aws-processors/src/main/resources/docs/org.apache.nifi.processors.aws.ml.textract.StartAwsTextractJob/additionalDetails.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"ClientRequestToken\": \"string\",\n  \"DocumentLocation\": {\n    \"S3Object\": {\n      \"Bucket\": \"string\",\n      \"Name\": \"string\",\n      \"Version\": \"string\"\n    }\n  },\n  \"JobTag\": \"string\",\n  \"KMSKeyId\": \"string\",\n  \"NotificationChannel\": {\n    \"RoleArn\": \"string\",\n    \"SNSTopicArn\": \"string\"\n  },\n  \"OutputConfig\": {\n    \"S3Bucket\": \"string\",\n    \"S3Prefix\": \"string\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example JSON Payload for Google Vision Batch Annotation\nDESCRIPTION: This JSON template defines the structure for sending batch annotation requests to the Google Vision API via NiFi. It includes input configuration, requested features, and output destination settings, with variables for GCS buckets, mime types, and features.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-gcp-bundle/nifi-gcp-processors/src/main/resources/docs/org.apache.nifi.processors.gcp.vision.StartGcpVisionAnnotateFilesOperation/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"requests\": [\n    {\n      \"inputConfig\": {\n        \"gcsSource\": {\n          \"uri\": \"gs://${gcs.bucket}/${filename}\"\n        },\n        \"mimeType\": \"application/pdf\"\n      },\n      \"features\": [\n        {\n          \"type\": \"${vision-feature-type}\",\n          \"maxResults\": 4\n        }\n      ],\n      \"outputConfig\": {\n        \"gcsDestination\": {\n          \"uri\": \"gs://${output-bucket}/${filename}/\"\n        },\n        \"batchSize\": 2\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Extracting Accounts Without Parent Fields Using ForkRecord - JSON\nDESCRIPTION: This snippet represents the expected output when using the ForkRecord processor to extract 'accounts' records without including parent fields. The resulting JSON is an array of account objects aggregated from all user records. Dependencies include a properly configured Record Writer schema capable of writing the output structure shown. The path to extract is '/accounts' set as a dynamic property; output is an array of items with 'id' and 'balance' only.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ForkRecord/additionalDetails.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"id\": 42,\n    \"balance\": 4750.89\n  },\n  {\n    \"id\": 43,\n    \"balance\": 48212.38\n  },\n  {\n    \"id\": 45,\n    \"balance\": 6578.45\n  },\n  {\n    \"id\": 46,\n    \"balance\": 34567.21\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Extracting Private Key to PEM - Shell/OpenSSL\nDESCRIPTION: Extracts the private key component from a PKCS12 file (`keys/user1-ks.p12`) and saves it in PEM format (`keys/user1-key.pem`). This is useful for tools that require keys and certificates in separate PEM files. Requires the `openssl` utility and specifies both input and output passwords.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-core/nifi-registry-web-api/src/test/resources/keys/README.md#_snippet_9\n\nLANGUAGE: Shell\nCODE:\n```\nopenssl pkcs12 -in keys/user1-ks.p12 -passin pass:password -out keys/user1-key.pem -passout pass:password\n```\n\n----------------------------------------\n\nTITLE: Defining Syslog Facility Grok Pattern\nDESCRIPTION: Defines the SYSLOGFACILITY pattern that matches the facility and priority codes in syslog messages. It extracts the facility and priority as integer values. It expects a format of <facility.priority> where facility and priority are non-negative integers. The facility and priority values are captured into the facility and priority fields respectively.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/default-grok-patterns.txt#_snippet_4\n\nLANGUAGE: Grok\nCODE:\n```\nSYSLOGFACILITY <%{NONNEGINT:facility}.%{NONNEGINT:priority}>\n```\n\n----------------------------------------\n\nTITLE: Sample NiFi Metrics JSON Input for Ambari Format\nDESCRIPTION: This JSON snippet is an example input representing NiFi metrics as collected, featuring an array of metric objects. Each metric object includes metadata such as 'metricname', 'appid', 'instanceid', 'hostname', timestamps, and a nested 'metrics' object with timestamp keys mapping to string values. This raw format is the input for the Jolt transformation to convert it into the Ambari format.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-site-to-site-reporting-bundle/nifi-site-to-site-reporting-task/src/main/resources/docs/org.apache.nifi.reporting.SiteToSiteMetricsReportingTask/additionalDetails.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"metrics\": [\n    {\n      \"metricname\": \"jvm.gc.time.G1OldGeneration\",\n      \"appid\": \"nifi\",\n      \"instanceid\": \"8927f4c0-0160-1000-597a-ea764ccd81a7\",\n      \"hostname\": \"localhost\",\n      \"timestamp\": \"1520456854361\",\n      \"starttime\": \"1520456854361\",\n      \"metrics\": {\n        \"1520456854361\": \"0\"\n      }\n    },\n    {\n      \"metricname\": \"jvm.thread_states.terminated\",\n      \"appid\": \"nifi\",\n      \"instanceid\": \"8927f4c0-0160-1000-597a-ea764ccd81a7\",\n      \"hostname\": \"localhost\",\n      \"timestamp\": \"1520456854361\",\n      \"starttime\": \"1520456854361\",\n      \"metrics\": {\n        \"1520456854361\": \"0\"\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Generated Zendesk Request Object (Dynamic Properties) - JSON\nDESCRIPTION: Shows the JSON structure of the Zendesk API request payload, demonstrating how additional nested properties can be added via NiFi dynamic properties using Json Pointer keys like /request/new\\_object and /request/new\\_array/0.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-zendesk-bundle/nifi-zendesk-services/src/main/resources/docs/org.apache.nifi.services.zendesk.ZendeskRecordSink/additionalDetails.md#_snippet_3\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"request\": {\n    \"new\\_object\": \"This is a new property\",\n    \"new\\_array\": [\n      \"This is a new array element\"\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: XQuery: Return fruit names as a comma separated list\nDESCRIPTION: This XQuery expression extracts all 'fruit' names and concatenates them into a single string, separated by commas. It uses the string-join function to achieve this.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.EvaluateXQuery/additionalDetails.md#_snippet_8\n\nLANGUAGE: xquery\nCODE:\n```\nstring-join((for $x in //fruit return $x/name/text()), ', ')\n```\n\n----------------------------------------\n\nTITLE: Defining Log Levels Grok Pattern\nDESCRIPTION: Defines a Grok pattern named LOGLEVEL for matching different log level strings such as 'Alert', 'Trace', 'Debug', 'Notice', 'Info', 'Warn', 'Error', 'Critical', 'Fatal', 'Severe', 'Emergency', 'FINE', 'FINER', 'FINEST', and 'CONFIG'. This pattern allows for case-insensitive matching of common log level abbreviations and variations.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/default-grok-patterns.txt#_snippet_0\n\nLANGUAGE: Grok\nCODE:\n```\nLOGLEVEL ([Aa]lert|ALERT|[Tt]race|TRACE|[Dd]ebug|DEBUG|[Nn]otice|NOTICE|[Ii]nfo|INFO|[Ww]arn?(?:ing)?|WARN?(?:ING)?|[Ee]rr?(?:or)?|ERR?(?:OR)?|[Cc]rit?(?:ical)?|CRIT?(?:ICAL)?|[Ff]atal|FATAL|[Ss]evere|SEVERE|EMERG(?:ENCY)?|[Ee]merg(?:ency)?)|FINE|FINER|FINEST|CONFIG\n```\n\n----------------------------------------\n\nTITLE: Credentials File Format for AWS Access Keys\nDESCRIPTION: This snippet provides the expected format for the Credentials File property in Apache NiFi. It demonstrates how to store AWS access key and secret key in a configuration file for the service. The file should have a \"default\" section containing \"accessKey\" and \"secretKey\" settings.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-aws-bundle/nifi-aws-processors/src/main/resources/docs/org.apache.nifi.processors.aws.s3.PutS3Object/additionalDetails.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n    [default]\n    accessKey=<access key>\n    secretKey=<security key>\n```\n\n----------------------------------------\n\nTITLE: Getting a Single Attribute from a FlowFile in Groovy for NiFi ExecuteScript\nDESCRIPTION: Retrieves the value of a specific attribute from a FlowFile using the `flowFile.getAttribute(attributeKey)` method. It first retrieves an incoming FlowFile. The method returns the attribute value as a String, or null if the attribute key does not exist on the FlowFile. Depends on the implicit `session` object and the `FlowFile` object.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-scripting-bundle/nifi-scripting-processors/src/main/resources/docs/org.apache.nifi.processors.script.ExecuteScript/additionalDetails.md#_snippet_6\n\nLANGUAGE: groovy\nCODE:\n```\nflowFile = session.get()\nif (!flowFile) return\nmyAttr = flowFile.getAttribute('filename')\n```\n\n----------------------------------------\n\nTITLE: Configuring Truststore in ssl-client.xml (XML)\nDESCRIPTION: This XML snippet configures the truststore settings for SSL client authentication. It specifies the truststore type, location, password, and reload interval. This configuration is used to establish secure communication with an SSL-secured HDFS file system. Dependencies include Hadoop and the necessary keystore/truststore files.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-hadoop-bundle/nifi-hdfs-processors/src/main/resources/docs/org.apache.nifi.processors.hadoop.PutHDFS/additionalDetails.md#_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<configuration>\n    <property>\n        <name>ssl.client.truststore.type</name>\n        <value>jks</value>\n    </property>\n    <property>\n        <name>ssl.client.truststore.location</name>\n        <value>/path/to/truststore.jks</value>\n    </property>\n    <property>\n        <name>ssl.client.truststore.password</name>\n        <value>clientfoo</value>\n    </property>\n    <property>\n        <name>ssl.client.truststore.reload.interval</name>\n        <value>10000</value>\n    </property>\n</configuration>\n```\n\n----------------------------------------\n\nTITLE: Build AWS Extensions for Existing NiFi Registry\nDESCRIPTION: This command builds only the AWS extensions for NiFi Registry, meant to be added to an existing installation. The `-f` option specifies the POM file for the AWS extension module. After the build, the generated zip file must be unzipped and placed in a directory accessible to NiFi Registry.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-extensions/nifi-registry-aws/nifi-registry-aws-assembly/README.md#_snippet_2\n\nLANGUAGE: Bash\nCODE:\n```\ncd nifi-registry\nmvn clean install -f nifi-registry-extensions/nifi-registry-aws\n```\n\n----------------------------------------\n\nTITLE: Querying Connection Status SQL\nDESCRIPTION: This SQL query retrieves all columns and rows from the CONNECTION_STATUS table. It provides real-time status information about all connections within the NiFi flow. Useful for monitoring the current state and metrics of dataflow connections.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-sql-reporting-bundle/nifi-sql-reporting-tasks/src/main/resources/docs/org.apache.nifi.reporting.sql.QueryNiFiReportingTask/additionalDetails.md#_snippet_0\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT * FROM CONNECTION_STATUS\n```\n\n----------------------------------------\n\nTITLE: Record Writer Schema for Forked Output - JSON\nDESCRIPTION: This schema snippet is designed for the Record Writer controller service and represents the target output structure after extracting nested 'transactions'. It includes both top-level user fields and fields from nested transaction and account objects. This schema must be configured in NiFi to ensure the processor outputs the expected flat records, merging parent and nested fields as appropriate. Key parameters include all output field names and types, with output records expected to match this layout exactly.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ForkRecord/additionalDetails.md#_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"record\",\n  \"name\": \"bank\",\n  \"fields\": [\n    {\n      \"name\": \"id\",\n      \"type\": \"int\"\n    },\n    {\n      \"name\": \"name\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"address\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"city\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"state\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"zipCode\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"country\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"amount\",\n      \"type\": \"double\"\n    },\n    {\n      \"name\": \"balance\",\n      \"type\": \"double\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: JSON Payload Example for Google Vision API\nDESCRIPTION: This JSON payload is a template for the request body used to call the Google Vision REST API for image annotation. It specifies the image source from a Google Cloud Storage (GCS) bucket, the desired vision features to be extracted, and the output configuration including the GCS destination and batch size.  The template includes placeholders for bucket names, filenames, and vision feature types, which should be replaced with actual values. The output will be stored in a GCS bucket at the specified URI, and the annotation process will process images in batches according to the configured batchSize.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-gcp-bundle/nifi-gcp-processors/src/main/resources/docs/org.apache.nifi.processors.gcp.vision.StartGcpVisionAnnotateImagesOperation/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"requests\": [\n    {\n      \"image\": {\n        \"source\": {\n          \"imageUri\": \"gs://${gcs.bucket}/${filename}\"\n        }\n      },\n      \"features\": [\n        {\n          \"type\": \"${vision-feature-type}\",\n          \"maxResults\": 4\n        }\n      ]\n    }\n  ],\n  \"outputConfig\": {\n    \"gcsDestination\": {\n      \"uri\": \"gs://${output-bucket}/${filename}/\"\n    },\n    \"batchSize\": 2\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example XSD Schema Definition\nDESCRIPTION: An XML Schema Definition (XSD) specifying the structure for XML documents. It defines a root element 'bundle' containing optional 'node' elements, which in turn contain optional 'subNode' elements, each holding a mandatory 'value' string element. This schema is intended for use with NiFi's ValidateXML processor.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ValidateXml/additionalDetails.md#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<xs:schema xmlns:xs=\"http://www.w3.org/2001/XMLSchema\" targetNamespace=\"http://namespace/1\"\n           xmlns:tns=\"http://namespace/1\" elementFormDefault=\"unqualified\">\n    <xs:element name=\"bundle\" type=\"tns:BundleType\"></xs:element>\n\n    <xs:complexType name=\"BundleType\">\n        <xs:sequence>\n            <xs:element name=\"node\" type=\"tns:NodeType\" maxOccurs=\"unbounded\" minOccurs=\"0\"></xs:element>\n        </xs:sequence>\n    </xs:complexType>\n    <xs:complexType name=\"NodeType\">\n        <xs:sequence>\n            <xs:element name=\"subNode\" type=\"tns:SubNodeType\" maxOccurs=\"unbounded\" minOccurs=\"0\"></xs:element>\n        </xs:sequence>\n    </xs:complexType>\n    <xs:complexType name=\"SubNodeType\">\n        <xs:sequence>\n            <xs:element name=\"value\" type=\"xs:string\"></xs:element>\n        </xs:sequence>\n    </xs:complexType>\n</xs:schema>\n```\n\n----------------------------------------\n\nTITLE: XQuery: Return all fruit nodes wrapped in a basket tag\nDESCRIPTION: This XQuery expression selects all 'fruit' nodes and wraps them within a 'basket' element. It returns a single result containing all 'fruit' nodes enclosed in the 'basket' tag.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.EvaluateXQuery/additionalDetails.md#_snippet_4\n\nLANGUAGE: xquery\nCODE:\n```\n<basket>{//fruit}</basket>\n```\n\n----------------------------------------\n\nTITLE: Reading Content from FlowFile in Groovy\nDESCRIPTION: This snippet shows different ways to read content from a FlowFile. It demonstrates reading the content as an InputStream, parsing JSON, and retrieving the content as a text string with a specified encoding. The `read` method opens a new InputStream for reading the flowfile content.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-groovyx-bundle/nifi-groovyx-processors/src/main/resources/docs/org.apache.nifi.processors.groovyx.ExecuteGroovyScript/additionalDetails.md#_snippet_5\n\nLANGUAGE: Groovy\nCODE:\n```\nInputStream i = flowFile.read()\ndef json = new groovy.json.JsonSlurper().parse(flowFile.read())\nString text = flowFile.read().getText(\"UTF-8\")\n```\n\n----------------------------------------\n\nTITLE: Defining ISO 8601 Timestamp Pattern in Grok for Syslog\nDESCRIPTION: This pattern defines a Grok expression to match ISO 8601 timestamp format in syslog messages. It captures the timestamp, optional syslog facility, the hostname of the log source, and the program name, enabling structured extraction of log metadata.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/test/resources/grok/grok_patterns.txt#_snippet_0\n\nLANGUAGE: Grok\nCODE:\n```\nSYSLOGBASE_ISO8601 %{TIMESTAMP_ISO8601:timestamp} (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource} %{SYSLOGPROG}:\n```\n\n----------------------------------------\n\nTITLE: Sample NiFi Release Script Execution Command\nDESCRIPTION: Example command showing how to run the release script for NiFi version 1.5.0, first release candidate, with build number 1.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-framework-bundle/nifi-framework/nifi-framework-cluster/src/test/resources/org/apache/nifi/cluster/firewall/impl/empty.txt#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n./nifi_release.sh 1.5.0 1.5.0 1 1\n```\n\n----------------------------------------\n\nTITLE: Querying Processor Status via minifi.sh Script\nDESCRIPTION: Demonstrates how to use the `minifi.sh` script in a Unix-like environment to query the health, statistics, and bulletins for a specific processor named 'TailFile'. This command outputs the requested status information directly to the command line.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-docs/src/main/markdown/System_Admin_Guide.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nminifi.sh flowStatus processor:TailFile:health,stats,bulletins\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi MiNiFi with Multiple Processors and Data Routing (YAML)\nDESCRIPTION: This YAML configuration defines a NiFi MiNiFi flow that tails a log file, splits it into individual lines, routes lines containing specific keywords, and writes the results to files. It demonstrates multi-processor orchestration, data routing based on content matching, and repository/security configurations. This flow enables selective data processing and storage while managing multiple auto-terminated relationships and connection pathways.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-docs/src/main/markdown/System_Admin_Guide.md#_snippet_16\n\nLANGUAGE: YAML\nCODE:\n```\n# <a id=\"example-config-file\" href=\"#example-config-file\">Example Config File</a>\n\nBelow are two example config YAML files. The first tails the *minifi-app.log* and sends the tailed log and provenance data back to a secure instance of NiFi. The second uses a series of processors to tail the app log, routes off only lines that contain \"WriteAheadFlowFileRepository\" and puts it as a file in the \"./\" directory.\n\n``` yaml\nFlow Controller:\n    name: MiNiFi Flow\n    comment:\n\nCore Properties:\n    flow controller graceful shutdown period: 10 sec\n    flow service write delay interval: 500 ms\n    administrative yield duration: 30 sec\n    bored yield duration: 10 millis\n    max concurrent threads: 1\n\nFlowFile Repository:\n    partitions: 256\n    checkpoint interval: 2 mins\n    always sync: false\n    Swap:\n        threshold: 20000\n        in period: 5 sec\n        in threads: 1\n        out period: 5 sec\n        out threads: 4\n\nContent Repository:\n    content claim max appendable size: 10 MB\n    content claim max flow files: 100\n    always sync: false\n\nComponent Status Repository:\n    buffer size: 1440\n    snapshot frequency: 1 min\n\nSecurity Properties:\n    keystore: /tmp/ssl/localhost-ks.jks\n    keystore type: JKS\n    keystore password: localtest\n    key password: localtest\n    truststore: /tmp/ssl/localhost-ts.jks\n    truststore type: JKS\n    truststore password: localtest\n    ssl protocol: TLS\n    Sensitive Props:\n        key:\n        algorithm: NIFI_PBKDF2_AES_GCM_256\n\nProcessors:\n    - name: TailAppLog\n      class: org.apache.nifi.processors.standard.TailFile\n      max concurrent tasks: 1\n      scheduling strategy: TIMER_DRIVEN\n      scheduling period: 10 sec\n      penalization period: 30 sec\n      yield period: 1 sec\n      run duration nanos: 0\n      auto-terminated relationships list:\n      Properties:\n          File to Tail: logs/minifi-app.log\n          Rolling Filename Pattern: minifi-app*\n          Initial Start Position: Beginning of File\n    - name: SplitIntoSingleLines\n      class: org.apache.nifi.processors.standard.SplitText\n      max concurrent tasks: 1\n      scheduling strategy: TIMER_DRIVEN\n      scheduling period: 0 sec\n      penalization period: 30 sec\n      yield period: 1 sec\n      run duration nanos: 0\n      auto-terminated relationships list:\n          - failure\n          - original\n      Properties:\n          Line Split Count: 1\n          Header Line Count: 0\n          Remove Trailing Newlines: true\n    - name: RouteErrors\n      class: org.apache.nifi.processors.standard.RouteText\n      max concurrent tasks: 1\n      scheduling strategy: TIMER_DRIVEN\n      scheduling period: 0 sec\n      penalization period: 30 sec\n      yield period: 1 sec\n      run duration nanos: 0\n      auto-terminated relationships list:\n          - unmatched\n          - original\n      Properties:\n          Routing Strategy: Route to 'matched' if line matches all conditions\n          Matching Strategy: Contains\n          Character Set: UTF-8\n          Ignore Leading/Trailing Whitespace: true\n          Ignore Case: true\n          Grouping Regular Expression:\n          WALFFR: WriteAheadFlowFileRepository\n    - name: PutFile\n      class: org.apache.nifi.processors.standard.PutFile\n      max concurrent tasks: 1\n      scheduling strategy: TIMER_DRIVEN\n      scheduling period: 0 sec\n      penalization period: 30 sec\n      yield period: 1 sec\n      run duration nanos: 0\n      auto-terminated relationships list:\n          - failure\n          - success\n      Properties:\n          Directory: ./\n          Conflict Resolution Strategy: replace\n          Create Missing Directories: true\n          Maximum File Count:\n          Last Modified Time:\n          Permissions:\n          Owner:\n          Group:\n\nConnections:\n    - name: TailToSplit\n      source name: TailAppLog\n      source relationship name: success\n      destination name: SplitIntoSingleLines\n      max work queue size: 0\n      max work queue data size: 1 MB\n      flowfile expiration: 60 sec\n      queue prioritizer class: org.apache.nifi.prioritizer.NewestFlowFileFirstPrioritizer\n    - name: SplitToRoute\n      source name: SplitIntoSingleLines\n      source relationship name: splits\n      destination name: RouteErrors\n      max work queue size: 0\n      max work queue data size: 1 MB\n      flowfile expiration: 60 sec\n      queue prioritizer class: org.apache.nifi.prioritizer.NewestFlowFileFirstPrioritizer\n    - name: RouteToS2S\n      source name: RouteErrors\n      source relationship name: matched\n      destination name: PutFile\n      max work queue size: 0\n      max work queue data size: 1 MB\n      flowfile expiration: 60 sec\n      queue prioritizer class: org.apache.nifi.prioritizer.NewestFlowFileFirstPrioritizer\n\nProvenance Reporting:\n    comment:\n    scheduling strategy: TIMER_DRIVEN\n    scheduling period: 30 sec\n    destination url: https://localhost:8080/\n    port name: provenance\n    originating url: http://${hostname(true)}:8081/nifi\n    use compression: true\n    timeout: 30 secs\n    batch size: 1000\n```\n\n```\n\n----------------------------------------\n\nTITLE: Defining an Elasticsearch Update Action without Script in JSON\nDESCRIPTION: This example shows a record with message and from fields intended for update operations. The resulting Elasticsearch bulk action separates the update header indicating _id and _index from the partial document to update under \"doc\". PutElasticsearchRecord uses record paths to extract the update metadata and content. No script is provided, so the partial document is used directly for the update.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-elasticsearch-bundle/nifi-elasticsearch-restapi-processors/src/main/resources/docs/org.apache.nifi.processors.elasticsearch.PutElasticsearchRecord/additionalDetails.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"message\": \"Hello, world\",\n  \"from\": \"john.smith\"\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"update\": {\n    \"_id\": \"1\",\n    \"_index\": \"test\"\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"doc\": {\n    \"message\": \"Hello, world\",\n    \"from\": \"john.smith\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Sample Input Record for Record Path (JSON)\nDESCRIPTION: This JSON snippet shows an example of an incoming record structure expected by the PutZendeskTicket processor when configured to use Record Path for extracting ticket attributes. It demonstrates nested objects and fields like description, issue type, issue name, and project name, which are referenced in the processor configuration.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-zendesk-bundle/nifi-zendesk-processors/src/main/resources/docs/org.apache.nifi.processors.zendesk.PutZendeskTicket/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"record\": {\n    \"description\": \"This is a sample description.\",\n    \"issue\\_type\": \"Immediate\",\n    \"issue\": {\n      \"name\": \"General error\",\n      \"type\": \"Immediate\"\n    },\n    \"project\": {\n      \"name\": \"Maintenance\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Removing Elements and Fields from Arrays Using Apache NiFi RecordPath in JSON\nDESCRIPTION: This snippet shows various ways to remove elements or fields from an array of addresses within a JSON record using RecordPath. It includes cases for removing a single element, all elements, fields within certain elements, and fields across all elements of the array. The examples illustrate how removal affects both the JSON data and the associated Avro-style schema, highlighting that schema updates occur only when all elements are affected. Nested arrays and multiple removals are also demonstrated. These examples require Apache NiFi processors that support RecordPath and structured record schemas. The inputs are JSON records with arrays of records; outputs show the JSON after removals with corresponding schema changes or null field values as per NiFi's rules.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.RemoveRecordField/additionalDetails.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": 1,\n  \"name\": \"John Doe\",\n  \"addresses\": [\n    {\n      \"zip\": 1111,\n      \"street\": \"Main\",\n      \"number\": 24\n    },\n    {\n      \"zip\": 1121,\n      \"street\": \"Airport\",\n      \"number\": 12\n    }\n  ]\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"PersonRecord\",\n  \"type\": \"record\",\n  \"namespace\": \"org.apache.nifi\",\n  \"fields\": [\n    {\n      \"name\": \"id\",\n      \"type\": \"int\"\n    },\n    {\n      \"name\": \"name\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"addresses\",\n      \"type\": {\n        \"type\": \"array\",\n        \"items\": {\n          \"name\": \"address\",\n          \"type\": \"record\",\n          \"fields\": [\n            {\n              \"name\": \"zip\",\n              \"type\": \"int\"\n            },\n            {\n              \"name\": \"street\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"number\",\n              \"type\": \"int\"\n            }\n          ]\n        }\n      }\n    }\n  ]\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": 1,\n  \"name\": \"John Doe\",\n  \"addresses\": [\n    {\n      \"zip\": 1121,\n      \"street\": \"Airport\",\n      \"number\": 12\n    }\n  ]\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"record\",\n  \"name\": \"nifiRecord\",\n  \"namespace\": \"org.apache.nifi\",\n  \"fields\": [\n    {\n      \"name\": \"id\",\n      \"type\": \"int\"\n    },\n    {\n      \"name\": \"name\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"addresses\",\n      \"type\": {\n        \"type\": \"array\",\n        \"items\": {\n          \"type\": \"record\",\n          \"name\": \"addressesType\",\n          \"fields\": [\n            {\n              \"name\": \"zip\",\n              \"type\": \"int\"\n            },\n            {\n              \"name\": \"street\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"number\",\n              \"type\": \"int\"\n            }\n          ]\n        }\n      }\n    }\n  ]\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": 1,\n  \"name\": \"John Doe\",\n  \"addresses\": []\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": 1,\n  \"name\": \"John Doe\",\n  \"addresses\": [\n    {\n      \"zip\": null,\n      \"street\": \"Main\",\n      \"number\": 24\n    },\n    {\n      \"zip\": 1121,\n      \"street\": \"Airport\",\n      \"number\": 12\n    }\n  ]\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": 1,\n  \"name\": \"John Doe\",\n  \"addresses\": [\n    {\n      \"street\": \"Main\",\n      \"number\": 24\n    },\n    {\n      \"street\": \"Airport\",\n      \"number\": 12\n    }\n  ]\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"record\",\n  \"name\": \"nifiRecord\",\n  \"namespace\": \"org.apache.nifi\",\n  \"fields\": [\n    {\n      \"name\": \"id\",\n      \"type\": \"int\"\n    },\n    {\n      \"name\": \"name\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"addresses\",\n      \"type\": {\n        \"type\": \"array\",\n        \"items\": {\n          \"type\": \"record\",\n          \"name\": \"addressesType\",\n          \"fields\": [\n            {\n              \"name\": \"street\",\n              \"type\": \"string\"\n            },\n            {\n              \"name\": \"number\",\n              \"type\": \"int\"\n            }\n          ]\n        }\n      }\n    }\n  ]\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": 1,\n  \"people\": [\n    {\n      \"id\": 11,\n      \"addresses\": [\n        {\n          \"zip\": 1111,\n          \"street\": \"Main\",\n          \"number\": 24\n        },\n        {\n          \"zip\": 1121,\n          \"street\": \"Airport\",\n          \"number\": 12\n        }\n      ]\n    },\n    {\n      \"id\": 22,\n      \"addresses\": [\n        {\n          \"zip\": 2222,\n          \"street\": \"Ocean\",\n          \"number\": 24\n        },\n        {\n          \"zip\": 2232,\n          \"street\": \"Sunset\",\n          \"number\": 12\n        }\n      ]\n    },\n    {\n      \"id\": 33,\n      \"addresses\": [\n        {\n          \"zip\": 3333,\n          \"street\": \"Dawn\",\n          \"number\": 24\n        },\n        {\n          \"zip\": 3323,\n          \"street\": \"Spring\",\n          \"number\": 12\n        }\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Granting Windows Event Log Access\nDESCRIPTION: This code snippet demonstrates how to use the `wevtutil` command-line tool in Windows to grant a specific user or group read access to the Security event log. It provides a concrete example of modifying the channel access attribute to include the SID of the desired user or group.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-windows-event-log-bundle/nifi-windows-event-log-processors/src/main/resources/docs/org.apache.nifi.processors.windows.event.log.ConsumeWindowsEventLog/additionalDetails.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nwevtutil sl Security /ca:O:BAG:SYD:(A;;0xf0005;;;SY)(A;;0x5;;;BA)(A;;0x1;;;S-1-5-32-573)(A;;0x1;;;S-1-5-21-3589080292-3448680409-2446571098-1001)\n```\n\n----------------------------------------\n\nTITLE: XQuery: Return all fruit nodes individually\nDESCRIPTION: This XQuery expression selects all 'fruit' nodes from the XML document. It returns each 'fruit' node as a separate result.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.EvaluateXQuery/additionalDetails.md#_snippet_1\n\nLANGUAGE: xquery\nCODE:\n```\n//fruit]\n```\n\n----------------------------------------\n\nTITLE: Getting All Attributes from a FlowFile in Groovy for NiFi ExecuteScript\nDESCRIPTION: Retrieves all attributes of a FlowFile as a Map<String, String> using the `flowFile.getAttributes()` method. An incoming FlowFile is retrieved first. The example then iterates over the key-value pairs in the returned map. Depends on the implicit `session` object and the `FlowFile` object.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-scripting-bundle/nifi-scripting-processors/src/main/resources/docs/org.apache.nifi.processors.script.ExecuteScript/additionalDetails.md#_snippet_7\n\nLANGUAGE: groovy\nCODE:\n```\nflowFile = session.get()\nif (!flowFile) return\nflowFile.getAttributes().each { key, value ->\n// Do something with the key/value pair\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Connection Status via minifi.sh Script\nDESCRIPTION: Shows a command example for querying the status of a specific connection named 'TailToS2S' using the `minifi.sh` script. This query retrieves the connection's health (queued bytes/FlowFiles) and statistics (input/output counts and bytes).\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-docs/src/main/markdown/System_Admin_Guide.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nminifi.sh flowStatus connection:TailToS2S:health,stats\n```\n\n----------------------------------------\n\nTITLE: Starting NiFi Application - NiFi Shell Script - Shell\nDESCRIPTION: Executes the provided NiFi shell script to start the service from within the distribution directory. Requires appropriate permissions. Writes logs and security credentials to the installation's logs directory. This command must be run after navigating to the unpacked distribution root.\nSOURCE: https://github.com/apache/nifi/blob/main/README.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n./bin/nifi.sh start\n```\n\n----------------------------------------\n\nTITLE: Querying Remote Process Groups with minifi.sh\nDESCRIPTION: This command retrieves the health, bulletins, input ports, and stats of all Remote Process Groups (RPGs) within the MiNiFi instance using the `flowStatus` command. It uses the `remoteprocessgroup` flag with the `all` identifier to target all RPGs and specifies the desired options (health, bulletins, inputports, stats) separated by commas.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-docs/src/main/markdown/System_Admin_Guide.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nminifi.sh flowStatus remoteprocessgroup:all:health,bulletins,inputports,stats\n```\n\n----------------------------------------\n\nTITLE: Configuring Amazon Polly Speech Synthesis Task - JSON\nDESCRIPTION: This JSON snippet serves as a template for the payload required by the AWS Polly StartSpeechSynthesisTask API, as used by the NiFi StartAwsPollyJob processor. Populate the fields to define language, voice, output S3 location, and other speech synthesis parameters. Required fields include at minimum 'Text', 'VoiceId', 'OutputFormat', and 'OutputS3BucketName', while others are optional or context-specific. Inputs are dynamically provided by NiFi flow file content or processor property, and outputs include the Polly job response written to an output flow file.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-aws-bundle/nifi-aws-processors/src/main/resources/docs/org.apache.nifi.processors.aws.ml.polly.StartAwsPollyJob/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"Engine\": \"string\",\n  \"LanguageCode\": \"string\",\n  \"LexiconNames\": [\n    \"string\"\n  ],\n  \"OutputFormat\": \"string\",\n  \"OutputS3BucketName\": \"string\",\n  \"OutputS3KeyPrefix\": \"string\",\n  \"SampleRate\": \"string\",\n  \"SnsTopicArn\": \"string\",\n  \"SpeechMarkTypes\": [\n    \"string\"\n  ],\n  \"Text\": \"string\",\n  \"TextType\": \"string\",\n  \"VoiceId\": \"string\"\n}\n```\n\n----------------------------------------\n\nTITLE: Building Project Modules with Parallel Execution - Apache Maven Wrapper - Shell\nDESCRIPTION: Runs the Maven Wrapper script to build all Apache NiFi project modules in parallel using all available CPU cores (1 thread per core). Requires execution permissions for ./mvnw and a Java 21 environment. Expects the user to be in the NiFi project root directory and have internet access for dependency resolution. Outputs built modules into the proper 'target' directories.\nSOURCE: https://github.com/apache/nifi/blob/main/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n./mvnw install -T1C\n```\n\n----------------------------------------\n\nTITLE: Fork Record in Apache NiFi\nDESCRIPTION: This script duplicates each input record, generating a second record where the 'num' field is decremented by one. It processes input records to produce an output list containing both original and transformed records, enabling complex data comparisons or augmentations.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-scripting-bundle/nifi-scripting-processors/src/main/resources/docs/org.apache.nifi.processors.script.ScriptedTransformRecord/additionalDetails.md#_snippet_4\n\nLANGUAGE: Groovy\nCODE:\n```\nimport org.apache.nifi.serialization.record.*\n\ndef derivedValues = new HashMap(record.toMap())\nderivedValues.put('num', derivedValues['num'] - 1)\nderived = new MapRecord(record.schema, derivedValues)\nreturn [record, derived]\n```\n\n----------------------------------------\n\nTITLE: Example CSV Data for NiFi Record Reader\nDESCRIPTION: Sample CSV data representing customer records with ID, name, balance, join date, and notes. This data serves as input for Example 1, demonstrating how the NiFi CSV Record Reader parses content based on a specified Avro schema and date format.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.csv.CSVReader/additionalDetails.md#_snippet_0\n\nLANGUAGE: csv\nCODE:\n```\nid, name, balance, join_date, notes\n1, John, 48.23, 04/03/2007, \"Our very   first customer!\"\n2, Jane, 1245.89, 08/22/2009,\t  \n3, Frank Franklin, \"48481.29\", 04/04/2016,  \n```\n\n----------------------------------------\n\nTITLE: XML Output Example 2 with either Force Type\nDESCRIPTION: This XML shows the output of the ValidateRecord processor in example 2 using XMLRecordSetWriter with either `Force Types From Reader's Schema = true` or `Force Types From Reader's Schema = false`. The output is the same in both cases, but the routing (valid or invalid) depends on the setting of `Force Types From Reader's Schema`.  This highlights how the schema affects the routing.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ValidateRecord/additionalDetails.md#_snippet_8\n\nLANGUAGE: XML\nCODE:\n```\n<test>\n    <field1>content_1</field1>\n    <field2>content_2</field2>\n    <field2>content_3</field2>\n</test>\n```\n\n----------------------------------------\n\nTITLE: Running a container with volume configuration\nDESCRIPTION: This command runs a Docker container in detached mode (`-d`), mapping host directories to container directories. It mounts `flow.json.raw` and `bootstrap.conf` from the `~/minifi-conf/` directory on the host to `/opt/minifi/minifi-current/conf/` within the container.  This allows the container to use the configuration files from the host machine.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-docker/dockerhub/README.md#_snippet_2\n\nLANGUAGE: Docker\nCODE:\n```\ndocker run -d \\\n        -v ~/minifi-conf/flow.json.raw:/opt/minifi/minifi-current/conf/flow.json.raw \\\n        -v ~/minifi-conf/bootstrap.conf:/opt/minifi/minifi-current/conf/bootstrap.conf \\\n        apache/nifi-minifi:latest\n```\n\n----------------------------------------\n\nTITLE: Importing keystore - Keytool\nDESCRIPTION: This snippet imports a keystore from a source to a destination. It's used for converting the keystore from one format to another, changing the alias, or moving certificates into new keystores. It utilizes keytool, a key and certificate management utility.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-core/nifi-registry-web-api/src/test/resources/keys/README.md#_snippet_3\n\nLANGUAGE: Java\nCODE:\n```\nkeytool -importkeystore \\\n      -srckeystore proxy/keystore.jks -srcstoretype jks -srcstorepass password -srcalias nifi-key \\\n      -destkeystore keys/proxy-ks.jks -deststoretype jks -deststorepass password -destalias proxy-key\n```\n\nLANGUAGE: Java\nCODE:\n```\nkeytool -importkeystore \\\n      -srckeystore keys/proxy-ks.jks -srcstoretype jks -srcstorepass password \\\n      -destkeystore keys/proxy-ks.p12 -deststoretype pkcs12 -deststorepass password\n```\n\nLANGUAGE: Java\nCODE:\n```\nkeytool -importkeystore \\\n      -srckeystore CN=user2_OU=nifi.p12 -srcstoretype PKCS12 -srcstorepass password -srcalias nifi-key \\\n      -destkeystore keys/user2-ks.jks -deststoretype JKS -deststorepass password -destalias user2-key\n```\n\nLANGUAGE: Java\nCODE:\n```\nkeytool -importkeystore \\\n      -srckeystore keys/user2-ks.jks -srcstoretype jks -srcstorepass password \\\n      -destkeystore keys/user2-ks.p12 -deststoretype pkcs12 -deststorepass password\n```\n\nLANGUAGE: Java\nCODE:\n```\nkeytool -importkeystore \\\n      -srckeystore registry/truststore.jks -srcstoretype jks -srcstorepass password -srcalias nifi-cert \\\n      -destkeystore keys/ca-ts.jks -deststoretype jks -deststorepass password -destalias ca-cert\n```\n\nLANGUAGE: Java\nCODE:\n```\nkeytool -importkeystore \\\n      -srckeystore keys/ca-ts.jks -srcstoretype jks -srcstorepass password \\\n      -destkeystore keys/ca-ts.p12 -deststoretype pkcs12 -deststorepass password\n```\n\nLANGUAGE: Java\nCODE:\n```\nkeytool -importkeystore \\\n      -srckeystore registry/keystore.jks -srcstoretype jks -srcstorepass password -srcalias nifi-key \\\n      -destkeystore keys/registry-ks.jks -deststoretype jks -deststorepass password -destalias registry-key\n```\n\nLANGUAGE: Java\nCODE:\n```\nkeytool -importkeystore \\\n      -srckeystore keys/registry-ks.jks -srcstoretype jks -srcstorepass password \\\n      -destkeystore keys/registry-ks.p12 -deststoretype pkcs12 -deststorepass password\n```\n\n----------------------------------------\n\nTITLE: Fetching Box Files in NiFi\nDESCRIPTION: This section explains how the FetchBoxFile processor operates within NiFi to retrieve files from Box. It details the typical workflow involving ListBoxFile and FetchBoxFile, including how to identify the File ID either from the 'box.id' attribute or directly via URL parsing. The instructions guide users on configuring the 'File ID' property to enable proper file retrieval.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-box-bundle/nifi-box-processors/src/main/resources/docs/org.apache.nifi.processors.box.FetchBoxFile/additionalDetails.md#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# Fetch Box files in NiFi\n\n1. Find File ID\n  Usually FetchBoxFile is used with ListBoxFile and 'box.id' is set.\n\n  In case 'box.id' is not available, you can find the ID of the file in the following way:\n    * Click on the file.\n    * The URL in the browser will include the File ID.\n      For example, if the URL were `https://app.box.com/file/1012106094023?s=ldiqjwuor2vwdxeeap2rtcz66dql89h3`,\n      the File ID would be `1012106094023`\n2. Set File ID in 'File ID' property\n```\n\n----------------------------------------\n\nTITLE: Initializing JNDI Context and Performing JMS ConnectionFactory Lookup in Java\nDESCRIPTION: This Java code snippet demonstrates how to set up the environment for a JNDI lookup to retrieve a JMS ConnectionFactory, mapping each configuration step to NiFi Controller Service properties (such as the initial context factory, provider URL, and additional environment variables). The code requires the appropriate JNDI and JMS client libraries to be available on the classpath, and all key parameters (context factory, URL, connection factory name) are supplied as input variables, typically configured via NiFi user interface. The expected input is a mapping of environment properties and the output is an instantiated ConnectionFactory object, with the limitation that all external classes must be available and accessible in the runtime environment.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-jms-bundle/nifi-jms-processors/src/main/resources/docs/org.apache.nifi.jms.cf.JndiJmsConnectionFactoryProvider/additionalDetails.md#_snippet_0\n\nLANGUAGE: java\nCODE:\n```\nHashtable env = new Hashtable();\nenv.put(Context.INITIAL_CONTEXT_FACTORY, JNDI_INITIAL_CONTEXT_FACTORY); // Value for this comes from the \"JNDI Initial Context Factory Class\" property.\nenv.put(Context.PROVIDER_URL, JNDI_PROVIDER_URL); // Value for this comes from the \"JNDI Provider URL\" property.\nenv.put(\"My-Environment-Variable\",\"Environment-Variable-Value\"); // This is accomplished by added a user-defined property with name \"My-Environment-Variable\" and value \"Environment-Variable-Value\"\n\nContext initialContext = new InitialContext(env);\nConnectionFactory connectionFactory = initialContext.lookup(JNDI_CONNECTION_FACTORY_NAME); // Value for this comes from the \"JNDI Name of the Connection Factory\" property\n```\n\n----------------------------------------\n\nTITLE: Transcribe Job Payload Example JSON\nDESCRIPTION: This is a template for the JSON payload used to configure and start an Amazon Transcribe job.  It defines parameters such as content redaction, language identification, media input, output settings, and other job-specific configurations. The parameters are passed to the AWS Transcribe API's StartTranscriptionJob method. Refer to the AWS documentation for a complete description of each parameter and their allowed values.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-aws-bundle/nifi-aws-processors/src/main/resources/docs/org.apache.nifi.processors.aws.ml.transcribe.StartAwsTranscribeJob/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"ContentRedaction\": {\n    \"PiiEntityTypes\": [\n      \"string\"\n    ],\n    \"RedactionOutput\": \"string\",\n    \"RedactionType\": \"string\"\n  },\n  \"IdentifyLanguage\": boolean,\n  \"IdentifyMultipleLanguages\": boolean,\n  \"JobExecutionSettings\": {\n    \"AllowDeferredExecution\": boolean,\n    \"DataAccessRoleArn\": \"string\"\n  },\n  \"KMSEncryptionContext\": {\n    \"string\": \"string\"\n  },\n  \"LanguageCode\": \"string\",\n  \"LanguageIdSettings\": {\n    \"string\": {\n      \"LanguageModelName\": \"string\",\n      \"VocabularyFilterName\": \"string\",\n      \"VocabularyName\": \"string\"\n    }\n  },\n  \"LanguageOptions\": [\n    \"string\"\n  ],\n  \"Media\": {\n    \"MediaFileUri\": \"string\",\n    \"RedactedMediaFileUri\": \"string\"\n  },\n  \"MediaFormat\": \"string\",\n  \"MediaSampleRateHertz\": number,\n  \"ModelSettings\": {\n    \"LanguageModelName\": \"string\"\n  },\n  \"OutputBucketName\": \"string\",\n  \"OutputEncryptionKMSKeyId\": \"string\",\n  \"OutputKey\": \"string\",\n  \"Settings\": {\n    \"ChannelIdentification\": boolean,\n    \"MaxAlternatives\": number,\n    \"MaxSpeakerLabels\": number,\n    \"ShowAlternatives\": boolean,\n    \"ShowSpeakerLabels\": boolean,\n    \"VocabularyFilterMethod\": \"string\",\n    \"VocabularyFilterName\": \"string\",\n    \"VocabularyName\": \"string\"\n  },\n  \"Subtitles\": {\n    \"Formats\": [\n      \"string\"\n    ],\n    \"OutputStartIndex\": number\n  },\n  \"Tags\": [\n    {\n      \"Key\": \"string\",\n      \"Value\": \"string\"\n    }\n  ],\n  \"TranscriptionJobName\": \"string\"\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Dynamic Properties via Json Pointer (Text)\nDESCRIPTION: This snippet illustrates how to add additional, non-standard ticket properties using NiFi's dynamic properties feature in the PutZendeskTicket processor. The property key uses Json Pointer syntax ('/path/to/element') to specify where in the final request JSON the value should be placed, allowing flexible construction of the Zendesk API request body.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-zendesk-bundle/nifi-zendesk-processors/src/main/resources/docs/org.apache.nifi.processors.zendesk.PutZendeskTicket/additionalDetails.md#_snippet_5\n\nLANGUAGE: text\nCODE:\n```\n/request/new_object : This is a new property\n/request/new_array/0 : This is a new array element\n```\n\n----------------------------------------\n\nTITLE: Example JSON Payload for StartDocumentAnalysis in AWS Textract\nDESCRIPTION: This JSON object represents an example request payload for initiating an asynchronous document analysis job using the AWS Textract StartDocumentAnalysis API via the NiFi processor. It specifies the document location in S3, desired feature types (e.g., FORMS, TABLES), optional notification channels (SNS), output configuration (S3 bucket/prefix), and specific query configurations.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-aws-bundle/nifi-aws-processors/src/main/resources/docs/org.apache.nifi.processors.aws.ml.textract.StartAwsTextractJob/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"ClientRequestToken\": \"string\",\n  \"DocumentLocation\": {\n    \"S3Object\": {\n      \"Bucket\": \"string\",\n      \"Name\": \"string\",\n      \"Version\": \"string\"\n    }\n  },\n  \"FeatureTypes\": [\n    \"string\"\n  ],\n  \"JobTag\": \"string\",\n  \"KMSKeyId\": \"string\",\n  \"NotificationChannel\": {\n    \"RoleArn\": \"string\",\n    \"SNSTopicArn\": \"string\"\n  },\n  \"OutputConfig\": {\n    \"S3Bucket\": \"string\",\n    \"S3Prefix\": \"string\"\n  },\n  \"QueriesConfig\": {\n    \"Queries\": [\n      {\n        \"Alias\": \"string\",\n        \"Pages\": [\n          \"string\"\n        ],\n        \"Text\": \"string\"\n      }\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Executing an Update with Script in Elasticsearch using JSON DSL\nDESCRIPTION: Example JSON DSL query with a painless script that increments a count field for documents matching a specific username. This demonstrates how to combine a query condition with a script action in the _update_by_query operation.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-elasticsearch-bundle/nifi-elasticsearch-restapi-processors/src/main/resources/docs/org.apache.nifi.processors.elasticsearch.UpdateByQueryElasticsearch/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"script\": {\n    \"source\": \"ctx._source.count++\",\n    \"lang\": \"painless\"\n  },\n  \"query\": {\n    \"match\": {\n      \"username.keyword\": \"john.smith\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Linting NiFi UI Codebase - Shell\nDESCRIPTION: Runs the linter against the NiFi UI codebase to identify stylistic issues, potential errors, and adherence to coding standards. It is recommended to run this command before submitting pull requests to ensure code quality.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-frontend/src/main/frontend/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nnpx nx lint\n```\n\n----------------------------------------\n\nTITLE: Converting Keystore to PEM using OpenSSL\nDESCRIPTION: This snippet converts a PKCS12 keystore (.p12) to PEM format using OpenSSL. This is done for both the key and the certificate separately.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-core/nifi-registry-web-api/src/test/resources/keys/README.md#_snippet_4\n\nLANGUAGE: OpenSSL\nCODE:\n```\nopenssl pkcs12 -in keys/proxy-ks.p12 -passin pass:password -out keys/proxy-key.pem -passout pass:password\nopenssl pkcs12 -in keys/proxy-ks.p12 -passin pass:password -out keys/proxy-cert.pem -nokeys\n```\n\nLANGUAGE: OpenSSL\nCODE:\n```\nopenssl pkcs12 -in keys/user2-ks.p12 -passin pass:password -out keys/user2-key.pem -passout pass:password\nopenssl pkcs12 -in keys/user2-ks.p12 -passin pass:password -out keys/user2-cert.pem -nokeys\n```\n\nLANGUAGE: OpenSSL\nCODE:\n```\nopenssl pkcs12 -in keys/registry-ks.p12 -passin pass:password -out keys/registry-key.pem -passout pass:password\nopenssl pkcs12 -in keys/registry-ks.p12 -passin pass:password -out keys/registry-cert.pem -nokeys\n```\n\n----------------------------------------\n\nTITLE: Monitoring MiNiFi Dataflow Status Using minifi.sh flowStatus Command\nDESCRIPTION: This section explains how to use the 'minifi.sh flowStatus' command-line tool to monitor various aspects of the MiNiFi dataflow, including processor, connection, remote process group, controller services, provenance reporting, instance, and system diagnostics. It details the flags and options for querying the health, status, bulletins, and metrics of different components.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-docs/src/main/markdown/minifi-java-agent-quick-start.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nminifi.sh flowStatus processor:TailFile:health,stats,bulletins\n```\n\n----------------------------------------\n\nTITLE: Querying Processor Status via flowstatus-minifi.bat Script\nDESCRIPTION: Provides the Windows batch equivalent of the `minifi.sh flowStatus` command. This snippet shows how to query the health, statistics, and bulletins for the 'TailFile' processor using the `flowstatus-minifi.bat` script on a Windows system.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-docs/src/main/markdown/System_Admin_Guide.md#_snippet_3\n\nLANGUAGE: batch\nCODE:\n```\nflowstatus-minifi.bat processor:TailFile:health,stats,bulletins\n```\n\n----------------------------------------\n\nTITLE: Java class for BigQuery data upload process in PutBigQuery processor\nDESCRIPTION: This Java class manages data transfer to BigQuery, supporting both streaming and batching modes via the gRPC Write API. It ensures data is appended to streams, handles stream lifecycle per FlowFile, and enforces delivery semantics such as exactly-once. Dependencies include the BigQuery API client libraries and protocol buffers. Critical parameters include configuration for streaming/batching, stream offsets, and table existence verification.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-gcp-bundle/nifi-gcp-processors/src/main/resources/docs/org.apache.nifi.processors.gcp.bigquery.PutBigQuery/additionalDetails.md#_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\npublic class PutBigQueryProcessor {\n    // Configurations for BigQuery write API\n    private boolean useStreaming;\n    private String tableName;\n    private BigQueryClient bigQueryClient;\n\n    public PutBigQueryProcessor(boolean useStreaming, String tableName, BigQueryClient client) {\n        this.useStreaming = useStreaming;\n        this.tableName = tableName;\n        this.bigQueryClient = client;\n    }\n\n    /**\n     * Appends data records to BigQuery, supporting both streaming and batching modes.\n     * Validates table existence and manages stream lifecycle per FlowFile.\n     * @param records List of data records to append\n     * @param flowFileId Identifier for the current FlowFile\n     */\n    public void appendData(List<Record> records, String flowFileId) {\n        if (!bigQueryClient.tableExists(tableName)) {\n            throw new RuntimeException(\"Target table does not exist in BigQuery.\");\n        }\n        if (useStreaming) {\n            Stream stream = bigQueryClient.openStream(flowFileId);\n            stream.append(records);\n            stream.close();\n        } else {\n            BatchStream batchStream = bigQueryClient.openBatchStream(flowFileId);\n            batchStream.append(records);\n            // Data is committed after FlowFile processing ends\n        }\n    }\n\n    /**\n     * Ensures exactly-once delivery semantics by tracking stream offsets.\n     * @param offset Last processed record offset\n     */\n    public void commitOffset(String offset) {\n        // Implementation for offset management to avoid duplicate data\n    }\n}\n\n```\n\n----------------------------------------\n\nTITLE: Calculating sum of flowfile values in NiFi\nDESCRIPTION: This snippet shows how to update a 'theSum' attribute by adding the current FlowFile's value to the stored sum using getStateValue and plus function. This helps accumulate total values over multiple FlowFiles.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-update-attribute-bundle/nifi-update-attribute-processor/src/main/resources/docs/org.apache.nifi.processors.attributes.UpdateAttribute/additionalDetails.md#_snippet_1\n\nLANGUAGE: NiFi Expression Language\nCODE:\n```\n${getStateValue(\"theSum\"):plus(${flowfileValue})}\n```\n\n----------------------------------------\n\nTITLE: Resulting Zendesk API Request via Dynamic Properties (JSON)\nDESCRIPTION: This JSON object shows the final structure of the Zendesk API request payload constructed by the PutZendeskTicket processor when dynamic properties are used. The keys, specified as Json Pointers ('/request/new_object', '/request/new_array/0'), dictate where the corresponding values are inserted into the request JSON, creating new fields and array elements in the payload sent to Zendesk.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-zendesk-bundle/nifi-zendesk-processors/src/main/resources/docs/org.apache.nifi.processors.zendesk.PutZendeskTicket/additionalDetails.md#_snippet_6\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"request\": {\n    \"new_object\": \"This is a new property\",\n    \"new_array\": [\n      \"This is a new array element\"\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Combined Log Line Matching Multiple Formats in Grok\nDESCRIPTION: This pattern matches syslog lines that conform to either the basic line format or the ISO 8601 timestamped format. It facilitates flexible parsing by attempting multiple pattern matches to extract log messages.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/test/resources/grok/grok_patterns.txt#_snippet_2\n\nLANGUAGE: Grok\nCODE:\n```\nLINE (?:%{LINE_1}|%{LINE_2})\n```\n\n----------------------------------------\n\nTITLE: Example CSV Data with Header Mismatching Schema\nDESCRIPTION: Sample CSV data including a header line ('id, name, balance, notes') used in Example 2. This data illustrates how the NiFi CSV Record Reader resolves conflicts between the header names ('notes') and field names defined in an associated Avro schema ('memo'), depending on the 'Ignore CSV Header Column Names' property.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.csv.CSVReader/additionalDetails.md#_snippet_3\n\nLANGUAGE: csv\nCODE:\n```\nid, name, balance, notes 1, John Doe, 123.45, First Customer\n```\n\n----------------------------------------\n\nTITLE: Updating All Documents in an Elasticsearch Index\nDESCRIPTION: Example JSON DSL query that matches all documents in an index, which can be used to update every document. This query uses the match_all operator to select all documents for the update operation.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-elasticsearch-bundle/nifi-elasticsearch-restapi-processors/src/main/resources/docs/org.apache.nifi.processors.elasticsearch.UpdateByQueryElasticsearch/additionalDetails.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"query\": {\n    \"match_all\": {}\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Secured NiFi Registry with OpenID Connect Authentication\nDESCRIPTION: This command runs the container configured for OIDC authentication, requiring volume mount for certificates and several environment variables to specify OIDC provider details, client credentials, and user identity claim settings.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-core/nifi-registry-docker/dockerhub/README.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --name nifi-registry \\\n  -v $(pwd)/certs/localhost:/opt/certs \\\n  -p 18443:18443 \\\n  -e AUTH=oidc \\\n  -e KEYSTORE_PATH=/opt/certs/keystore.p12 \\\n  -e KEYSTORE_TYPE=PKCS12 \\\n  -e KEYSTORE_PASSWORD=PLACEHOLDER \\\n  -e TRUSTSTORE_PATH=/opt/certs/truststore.p12 \\\n  -e TRUSTSTORE_PASSWORD=PLACEHOLDER \\\n  -e TRUSTSTORE_TYPE=PKCS12 \\\n  -e INITIAL_ADMIN_IDENTITY=PLACEHOLDER_USER \\\n  -e NIFI_REGISTRY_SECURITY_USER_OIDC_DISCOVERY_URL=http://OIDC_SERVER/.well-known/openid-configuration \\\n  -e NIFI_REGISTRY_SECURITY_USER_OIDC_CONNECT_TIMEOUT=10000 \\\n  -e NIFI_REGISTRY_SECURITY_USER_OIDC_READ_TIMEOUT=10000 \\\n  -e NIFI_REGISTRY_SECURITY_USER_OIDC_CLIENT_ID=CLIENT_ID \\\n  -e NIFI_REGISTRY_SECURITY_USER_OIDC_CLIENT_SECRET=CLIENT_SECRET \\\n  -e NIFI_REGISTRY_SECURITY_USER_OIDC_PREFERRED_JWSALGORITHM=RS256 \\\n  -e NIFI_REGISTRY_SECURITY_USER_OIDC_ADDITIONAL_SCOPES=profile \\\n  -e NIFI_REGISTRY_SECURITY_USER_OIDC_CLAIM_IDENTIFYING_USER=preferred_username \\\n  -e NIFI_REGISTRY_SECURITY_USER_OIDC_CLAIM_GROUPS=groups \\\n  -d \\\n  apache/nifi-registry:latest\n```\n\n----------------------------------------\n\nTITLE: Configuring HashiCorpVaultParameterValueProvider in NiFi (Properties)\nDESCRIPTION: Example configuration properties for registering and setting up the `HashiCorpVaultParameterValueProvider` within a NiFi Stateless dataflow configuration file. It specifies the provider's name (`Vault`), Java class (`org.apache.nifi.stateless.parameter.HashiCorpVaultParameterValueProvider`), and the location of the Vault configuration file (`./conf/bootstrap-hashicorp-vault.conf`).\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-stateless/nifi-stateless-assembly/README.md#_snippet_14\n\nLANGUAGE: properties\nCODE:\n```\nnifi.stateless.parameter.provider.Vault.name=HashiCorp Vault Provider\nnifi.stateless.parameter.provider.Vault.type=org.apache.nifi.stateless.parameter.HashiCorpVaultParameterValueProvider\nnifi.stateless.parameter.provider.Vault.properties.vault-configuration-file=./conf/bootstrap-hashicorp-vault.conf\n```\n\n----------------------------------------\n\nTITLE: JSON Example of Wrapper Strategy Output\nDESCRIPTION: This JSON snippet illustrates the output format produced by the Wrapper strategy, which encapsulates both the 'original' and 'enrichment' data within a single JSON object.  This format is designed for use with processors like TransformRecord.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.JoinEnrichment/additionalDetails.md#_snippet_10\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"original\": {\n    \"id\": 482028,\n    \"name\": \"John Doe\",\n    \"ssn\": \"555-55-5555\",\n    \"phone\": \"555-555-5555\",\n    \"email\": \"john.doe@nifi.apache.org\"\n  },\n  \"enrichment\": {\n    \"country\": \"UK\",\n    \"allowsPII\": false\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Error Bulletin Categories SQL\nDESCRIPTION: This SQL query retrieves the distinct bulletinCategory values from the BULLETINS table. It specifically filters for bulletins where the bulletinLevel is 'ERROR'. This is useful for getting an overview of the types of errors reported by the NiFi instance.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-sql-reporting-bundle/nifi-sql-reporting-tasks/src/main/resources/docs/org.apache.nifi.reporting.sql.QueryNiFiReportingTask/additionalDetails.md#_snippet_2\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT DISTINCT(bulletinCategory) FROM BULLETINS WHERE bulletinLevel = \"ERROR\"\n```\n\n----------------------------------------\n\nTITLE: Defining Avro Schema with Different Field Name (\"memo\")\nDESCRIPTION: An Avro schema definition used in Example 2 to demonstrate how the NiFi CSV Record Reader handles discrepancies between schema field names and CSV header names. This schema defines the last field as 'memo', while the corresponding CSV header uses 'notes'.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.csv.CSVReader/additionalDetails.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"namespace\": \"nifi\",\n  \"name\": \"balances\",\n  \"type\": \"record\",\n  \"fields\": [\n    {\n      \"name\": \"id\",\n      \"type\": \"int\"\n    },\n    {\n      \"name\": \"name\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"balance\",\n      \"type\": \"double\"\n    },\n    {\n      \"name\": \"memo\",\n      \"type\": \"string\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Elasticsearch Index Action with Dynamic Templates - Elasticsearch JSON\nDESCRIPTION: This code snippet shows the resulting Elasticsearch bulk API action for indexing a document using dynamic templates. The action defines the operation ('index'), document ID, index name, and the dynamic templates to be used. The payload specifies the actual document under the 'doc' key. Users must define the necessary fields for bulk ingestion, and ensure that all field mappings are appropriately configured in Elasticsearch.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-elasticsearch-bundle/nifi-elasticsearch-restapi-processors/src/main/resources/docs/org.apache.nifi.processors.elasticsearch.PutElasticsearchJson/additionalDetails.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"index\": {\n    \"_id\": \"1\",\n    \"_index\": \"test\",\n    \"dynamic_templates\": {\n      \"message\": \"keyword_lower\"\n    }\n  }\n}\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"doc\": {\n    \"message\": \"Hello, world\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Security Properties in bootstrap.conf for MiNiFi SSL Security\nDESCRIPTION: This snippet describes setting up security configurations such as keystore, truststore, passwords, and SSL protocols in the bootstrap.conf file to enable secure communication in a MiNiFi dataflow. It guides users on editing key-value properties necessary for SSL setup and specifies parameters like keystore paths, passwords, and protocol types.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-docs/src/main/markdown/minifi-java-agent-quick-start.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\nnifi.minifi.security.keystore=\n nifi.minifi.security.keystoreType=\n nifi.minifi.security.keystorePasswd=\n nifi.minifi.security.keyPasswd=\n nifi.minifi.security.truststore=\n nifi.minifi.security.truststoreType=\n nifi.minifi.security.truststorePasswd=\n nifi.minifi.security.ssl.protocol=\n\nnifi.minifi.flow.use.parent.ssl=false\n\nnifi.minifi.sensitive.props.key=\n nifi.minifi.sensitive.props.algorithm=\n```\n\n----------------------------------------\n\nTITLE: Example Log Output for Generated NiFi Credentials - Log Template - Shell\nDESCRIPTION: Represents the format of output lines in the NiFi startup log where credentials are generated. Real values for USERNAME (a UUID) and PASSWORD (a random string) will appear at runtime. These lines are informational; this snippet is used to identify them in logs.\nSOURCE: https://github.com/apache/nifi/blob/main/README.md#_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nGenerated Username [USERNAME]\nGenerated Password [PASSWORD]\n```\n\n----------------------------------------\n\nTITLE: Configuring AWSSecretsManagerParameterValueProvider in NiFi (Properties)\nDESCRIPTION: Example configuration properties for registering and setting up the `AwsSecretsManagerParameterValueProvider` in a NiFi Stateless dataflow configuration file. It defines the provider's name (`AWSSecretsManager`), Java class (`org.apache.nifi.stateless.parameter.AwsSecretsManagerParameterValueProvider`), the optional AWS credentials file (`./conf/bootstrap-aws.conf`), the default secret name (`Default`), and demonstrates mapping a specific Parameter Context (`MyContextName`) to a different secret name (`MappedSecretName`).\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-stateless/nifi-stateless-assembly/README.md#_snippet_16\n\nLANGUAGE: properties\nCODE:\n```\nnifi.stateless.parameter.provider.AWSSecretsManager.name=AWS SecretsManager Provider\nnifi.stateless.parameter.provider.AWSSecretsManager.type=org.apache.nifi.stateless.parameter.AwsSecretsManagerParameterValueProvider\nnifi.stateless.parameter.provider.AWSSecretsManager.properties.aws-credentials-file=./conf/bootstrap-aws.conf\nnifi.stateless.parameter.provider.AWSSecretsManager.properties.default-secret-name=Default\nnifi.stateless.parameter.provider.AWSSecretsManager.properties.MyContextName=MappedSecretName\n```\n\n----------------------------------------\n\nTITLE: Record Reader Schema for Nested Arrays in ForkRecord - JSON\nDESCRIPTION: This snippet defines an Avro-style JSON schema appropriate for the Record Reader controller service in NiFi. The schema supports users, their addresses, and an array of accounts, each of which includes a nested array of transactions. The key fields and nesting enable correct parsing for multi-nested record extraction. This schema is required in the Reader for proper operation, and must match the hierarchical structure of incoming JSON payloads to work with ForkRecord.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ForkRecord/additionalDetails.md#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"record\",\n  \"name\": \"bank\",\n  \"fields\": [\n    {\n      \"name\": \"id\",\n      \"type\": \"int\"\n    },\n    {\n      \"name\": \"name\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"address\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"city\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"state\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"zipCode\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"country\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"accounts\",\n      \"type\": {\n        \"type\": \"array\",\n        \"items\": {\n          \"type\": \"record\",\n          \"name\": \"accounts\",\n          \"fields\": [\n            {\n              \"name\": \"id\",\n              \"type\": \"int\"\n            },\n            {\n              \"name\": \"balance\",\n              \"type\": \"double\"\n            },\n            {\n              \"name\": \"transactions\",\n              \"type\": {\n                \"type\": \"array\",\n                \"items\": {\n                  \"type\": \"record\",\n                  \"name\": \"transactions\",\n                  \"fields\": [\n                    {\n                      \"name\": \"id\",\n                      \"type\": \"int\"\n                    },\n                    {\n                      \"name\": \"amount\",\n                      \"type\": \"double\"\n                    }\n                  ]\n                }\n              }\n            }\n          ]\n        }\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Setting up Google Drive Integration with Apache NiFi\nDESCRIPTION: Detailed instructions for configuring the PutGoogleDrive processor in Apache NiFi, including enabling the Google Drive API, granting folder access, and obtaining the folder ID needed for the processor configuration.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-gcp-bundle/nifi-gcp-processors/src/main/resources/docs/org.apache.nifi.processors.gcp.drive.PutGoogleDrive/additionalDetails.md#_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# PutGoogleDrive\n\n## Accessing Google Drive from NiFi\n\nThis processor uses Google Cloud credentials for authentication to access Google Drive. The following steps are required\nto prepare the Google Cloud and Google Drive accounts for the processors:\n\n1. **Enable Google Drive API in Google Cloud**\n    * Follow instructions\n      at [https://developers.google.com/workspace/guides/enable-apis](https://developers.google.com/workspace/guides/enable-apis)\n      and search for 'Google Drive API'.\n2. **Grant access to Google Drive folder**\n    * In Google Cloud Console navigate to IAM & Admin -> Service Accounts.\n    * Take a note of the email of the service account you are going to use.\n    * Navigate to the folder in Google Drive which will be used as the base folder.\n    * Right-click on the Folder -> Share.\n    * Enter the service account email.\n3. **Find Folder ID**\n    * Navigate to the folder to be listed in Google Drive and enter it. The URL in your browser will include the ID at\n      the end of the URL. For example, if the URL were\n      `https://drive.google.com/drive/folders/1trTraPVCnX5_TNwO8d9P_bz278xWOmGm`, the Folder ID would be\n      `1trTraPVCnX5_TNwO8d9P_bz278xWOmGm`\n4. **Set Folder ID in 'Folder ID' property**\n```\n\n----------------------------------------\n\nTITLE: Defining Syslog Timestamp Grok Pattern\nDESCRIPTION: Defines the SYSLOGTIMESTAMP pattern that matches the timestamp format commonly found in syslog messages (Month Day HH:MM:SS). It uses the MONTH and MONTHDAY patterns defined later, as well as a TIME pattern. The SYSLOGTIMESTAMP extracts the month, day and time components from the syslog timestamp.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/default-grok-patterns.txt#_snippet_1\n\nLANGUAGE: Grok\nCODE:\n```\nSYSLOGTIMESTAMP %{MONTH} +%{MONTHDAY} %{TIME}\n```\n\n----------------------------------------\n\nTITLE: Defining Dynamic Templates - Elasticsearch JSON\nDESCRIPTION: This snippet represents a dynamic template property for indexing documents using the PutElasticsearchJson processor. Users must supply a valid JSON object that matches the required template field structure. The template maps the 'message' field to a specific type (e.g., 'keyword_lower'), which Elasticsearch will use to process this field during indexing operations.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-elasticsearch-bundle/nifi-elasticsearch-restapi-processors/src/main/resources/docs/org.apache.nifi.processors.elasticsearch.PutElasticsearchJson/additionalDetails.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"message\": \"keyword_lower\"\n}\n```\n\n----------------------------------------\n\nTITLE: Parsing XML Attributes with Custom Field Name for Content (NiFi XMLReader, XML)\nDESCRIPTION: This snippet uses NiFi XMLReader with both 'Parse XML Attributes' enabled and 'Field Name for Content' set to 'original_content'. It safely maps the value of the parent element to 'original_content' and attribute to 'attr', preventing any naming clashes and data loss. The input XML must avoid elements named 'original_content' as children of the target node.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLReader/additionalDetails.md#_snippet_12\n\nLANGUAGE: xml\nCODE:\n```\n<record>\n    <field_with_attribute attr=\"attr_content\">content of field</field_with_attribute>\n</record>\n```\n\nLANGUAGE: xml\nCODE:\n```\n<record>\n    <field_with_attribute>\n        <attr>attr_content</attr>\n        <original_content>content of field</original_content>\n    </field_with_attribute>\n</record>\n```\n\n----------------------------------------\n\nTITLE: Copying Generated Keys to Test Directory - Shell\nDESCRIPTION: Copies all generated key and certificate files from a temporary directory (`$WD/keys/`) to the NiFi Registry test resources directory (`/path/to/nifi-registry/.../keys/`). The `-f` flag ensures existing files are overwritten. Requires the standard shell `cp` command.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-core/nifi-registry-web-api/src/test/resources/keys/README.md#_snippet_13\n\nLANGUAGE: Shell\nCODE:\n```\ncp -f \"$WD/keys/*\" /path/to/nifi-registry/nifi-registry-core/nifi-registry-web-api/src/test/resources/keys/\n```\n\n----------------------------------------\n\nTITLE: JSON Schema for Simple XML Fields in Apache NiFi XMLReader\nDESCRIPTION: JSON schema definition for describing XML data with simple fields. This schema tells the XMLReader to expect zero or one occurrences of 'simple_field' in each record.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLReader/additionalDetails.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"namespace\": \"nifi\",\n  \"name\": \"test\",\n  \"type\": \"record\",\n  \"fields\": [\n    {\n      \"name\": \"simple_field\",\n      \"type\": \"string\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configure S3 Bundle Persistence Provider\nDESCRIPTION: This XML configuration enables the S3 bundle persistence provider within the `providers.xml` file. The commented-out section needs to be uncommented and properly configured with the user's AWS credentials, region, bucket name, and other settings. It's important to disable the `FileSystemBundlePersistenceProvider` when using the S3 provider.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-extensions/nifi-registry-aws/nifi-registry-aws-assembly/README.md#_snippet_5\n\nLANGUAGE: XML\nCODE:\n```\n<!--\n<extensionBundlePersistenceProvider>\n    <class>org.apache.nifi.registry.aws.S3BundlePersistenceProvider</class>\n    <property name=\"Region\">us-east-1</property>\n    <property name=\"Bucket Name\">my-bundles</property>\n    <property name=\"Key Prefix\"></property>\n    <property name=\"Credentials Provider\">DEFAULT_CHAIN</property>\n    <property name=\"Access Key\"></property>\n    <property name=\"Secret Access Key\"></property>\n    <property name=\"Endpoint URL\"></property>\n</extensionBundlePersistenceProvider>\n-->\n```\n\n----------------------------------------\n\nTITLE: Transferring FlowFile in Groovy\nDESCRIPTION: This code demonstrates transferring a FlowFile to a specified relationship. It shows two equivalent ways to achieve this: using the `<<` operator and the `transfer` method of the `flowFile` object. This is equivalent to using session.transfer(flowFile, REL_SUCCESS)\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-groovyx-bundle/nifi-groovyx-processors/src/main/resources/docs/org.apache.nifi.processors.groovyx.ExecuteGroovyScript/additionalDetails.md#_snippet_6\n\nLANGUAGE: Groovy\nCODE:\n```\nREL_SUCCESS << flowFile\nflowFile.transfer(REL_SUCCESS)\n// the same as:\nsession.transfer(flowFile, REL_SUCCESS)\n```\n\n----------------------------------------\n\nTITLE: Creating a GCP Secret for NiFi Parameters via Command Line\nDESCRIPTION: This command creates a GCP Secret that will be recognized by the GcpSecretManagerParameterProvider. It assigns a group-name label to associate the secret with a specific Parameter Group in NiFi.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-gcp-bundle/nifi-gcp-parameter-providers/src/main/resources/docs/org.apache.nifi.parameter.gcp.GcpSecretManagerParameterProvider/additionalDetails.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nprintf \"[Parameter Value]\" | gcloud secrets create --labels=group-name=\"[Parameter Group Name]\" \"[Parameter Name]\" --data-file=-\n```\n\n----------------------------------------\n\nTITLE: Parsing XML Attributes with No Field Name for Content (NiFi XMLReader, XML)\nDESCRIPTION: This snippet shows NiFi XMLReader with 'Parse XML Attributes' enabled and no 'Field Name for Content' set. Attribute values are extracted into new fields (e.g., <attr>), but parent element content is lost and a 'value' field appears empty. This demonstrates schema inference behavior when attributes exist alongside content. Requires XMLReader configuration with attribute parsing enabled.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLReader/additionalDetails.md#_snippet_11\n\nLANGUAGE: xml\nCODE:\n```\n<record>\n    <field_with_attribute attr=\"attr_content\">content of field</field_with_attribute>\n</record>\n```\n\nLANGUAGE: xml\nCODE:\n```\n<record>\n    <field_with_attribute>\n        <attr>attr_content</attr>\n        <value></value>\n    </field_with_attribute>\n</record>\n```\n\n----------------------------------------\n\nTITLE: Valid XML Example 2\nDESCRIPTION: Another example of a valid XML document according to the provided XSD schema. This instance has a 'bundle' containing one 'node' with a single 'subNode' child.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ValidateXml/additionalDetails.md#_snippet_2\n\nLANGUAGE: xml\nCODE:\n```\n<ns:bundle xmlns:ns=\"http://namespace/1\">\n    <node>\n        <subNode>\n            <value>Hello World!</value>\n        </subNode>\n    </node>\n</ns:bundle>\n```\n\n----------------------------------------\n\nTITLE: Enrichment JSON Payload Example for Insert Enrichment Fields Strategy\nDESCRIPTION: Example enrichment JSON content that provides additional customerDetails fields to be merged into original records by the Insert Enrichment Fields join strategy. Contains nested information such as phone and email. Input data is used with the Record Path property to merge into original records.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.JoinEnrichment/additionalDetails.md#_snippet_4\n\nLANGUAGE: JSON\nCODE:\n```\n[\n  {\n    \"customerDetails\": {\n      \"id\": 48202,\n      \"phone\": \"555-555-5555\",\n      \"email\": \"john.doe@nifi.apache.org\"\n    }\n  },\n  {\n    \"customerDetails\": {\n      \"id\": 5512,\n      \"phone\": \"555-555-5511\",\n      \"email\": \"jane.doe@nifi.apache.org\"\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Maven Integration Tests Execution\nDESCRIPTION: This command executes the integration tests with Elasticsearch 7 support using Maven. It specifies the 'integration-tests' and 'elasticsearch7' profiles, fails the build at the end if any tests fail, and cleans and installs the project.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-elasticsearch-bundle/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmvn -Pintegration-tests,elasticsearch7 --fail-at-end clean install\n```\n\n----------------------------------------\n\nTITLE: JSON Schema for Arrays with Simple Fields in Apache NiFi XMLReader\nDESCRIPTION: JSON schema definition for describing XML data containing both array fields and simple fields. This schema defines 'array_field' as an array of strings and 'simple_field' as a string.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLReader/additionalDetails.md#_snippet_5\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"namespace\": \"nifi\",\n  \"name\": \"test\",\n  \"type\": \"record\",\n  \"fields\": [\n    {\n      \"name\": \"array_field\",\n      \"type\": {\n        \"type\": \"array\",\n        \"items\": \"string\"\n      }\n    },\n    {\n      \"name\": \"simple_field\",\n      \"type\": \"string\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configure NiFi Registry Extension Directory\nDESCRIPTION: This configuration sets the extension directory in the `nifi-registry.properties` file. This is required for the AWS extensions to be loaded when manually adding them to an existing NiFi Registry installation. It's already configured when building with the include-aws profile.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-extensions/nifi-registry-aws/nifi-registry-aws-assembly/README.md#_snippet_4\n\nLANGUAGE: Bash\nCODE:\n```\n# Specify AWS extension dir\nnifi.registry.extension.dir.aws=./ext/aws/lib\n```\n\n----------------------------------------\n\nTITLE: XQuery: Return only the last fruit node\nDESCRIPTION: This XQuery expression selects the last 'fruit' node from the XML document using the count function to determine its index. It returns only the last 'fruit' node.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.EvaluateXQuery/additionalDetails.md#_snippet_3\n\nLANGUAGE: xquery\nCODE:\n```\n//fruit[count(//fruit)]\n```\n\n----------------------------------------\n\nTITLE: XML Structure for Arrays with Simple Fields in Apache NiFi XMLReader\nDESCRIPTION: Example of XML data containing both array fields (repetitive tags) and simple fields. The 'array_field' appears multiple times, making it an array in the record object.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLReader/additionalDetails.md#_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<record>\n    <array_field>content</array_field>\n    <array_field>content</array_field>\n    <simple_field>content</simple_field>\n</record>\n```\n\n----------------------------------------\n\nTITLE: RecordSet Containing an Array Field - Pseudo-Code\nDESCRIPTION: Shows a record with a field containing an array of integer values, to demonstrate how arrays are represented in the record data before XML transformation.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLRecordSetWriter/additionalDetails.md#_snippet_5\n\nLANGUAGE: pseudo\nCODE:\n```\nRecordSet (\n  Record (\n    Field \"name1\" = \"value1\",\n    Field \"array_field\" = [ 1, 2, 3 ]\n  )\n)\n```\n\n----------------------------------------\n\nTITLE: Building Docker image for Apache NiFi MiNiFi\nDESCRIPTION: This command builds a Docker image for Apache NiFi MiNiFi, tagging it as `apache/nifi-minifi:latest`. The build process uses the Dockerfile in the current directory.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-docker/dockerhub/README.md#_snippet_0\n\nLANGUAGE: Docker\nCODE:\n```\ndocker build -t apache/nifi-minifi:latest .\n```\n\n----------------------------------------\n\nTITLE: Getting Amazon Translate Job Status in Java for Apache NiFi\nDESCRIPTION: This code snippet demonstrates how to implement a processor in Java within Apache NiFi to check the status of an Amazon Translate job. It interacts with AWS SDK to query the translation job status and retrieves the output location upon completion, facilitating integration with NiFi workflows.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-aws-bundle/nifi-aws-processors/src/main/resources/docs/org.apache.nifi.processors.aws.ml.translate.GetAwsTranslateJobStatus/additionalDetails.md#_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\n// Assumes dependencies on AWS SDK for Java are included\nimport com.amazonaws.services.translate.AmazonTranslate;\nimport com.amazonaws.services.translate.model.DescribeTextTranslationJobRequest;\nimport com.amazonaws.services.translate.model.DescribeTextTranslationJobResult;\n\n// Method to get translation job status\npublic String getTranslateJobStatus(AmazonTranslate translateClient, String jobId) {\n    DescribeTextTranslationJobRequest request = new DescribeTextTranslationJobRequest()\n        .withJobId(jobId);\n    DescribeTextTranslationJobResult response = translateClient.describeTextTranslationJob(request);\n    String jobStatus = response.getTextTranslationJobProperties().getJobStatus();\n    String outputLocation = response.getTextTranslationJobProperties().getOutputDataConfig().getOutputLocation();\n    // The method checks if the job is completed and retrieves output location\n    if (\"COMPLETED\".equals(jobStatus)) {\n        return \"Job completed. Output available at: \" + outputLocation;\n    } else {\n        return \"Job status: \" + jobStatus;\n    }\n}\n\n```\n\n----------------------------------------\n\nTITLE: JSON Payload Template for StartTextTranslationJob in Amazon Translate with Apache NiFi\nDESCRIPTION: Example JSON template for configuring a StartTextTranslationJob request through the StartAwsTranslateJob processor. This template shows all possible fields that can be included in the request to Amazon Translate service. The payload can be provided as a property or as flowfile content, with property taking precedence.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-aws-bundle/nifi-aws-processors/src/main/resources/docs/org.apache.nifi.processors.aws.ml.translate.StartAwsTranslateJob/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"ClientToken\": \"string\",\n  \"DataAccessRoleArn\": \"string\",\n  \"InputDataConfig\": {\n    \"ContentType\": \"string\",\n    \"S3Uri\": \"string\"\n  },\n  \"JobName\": \"string\",\n  \"OutputDataConfig\": {\n    \"EncryptionKey\": {\n      \"Id\": \"string\",\n      \"Type\": \"string\"\n    },\n    \"S3Uri\": \"string\"\n  },\n  \"ParallelDataNames\": [\n    \"string\"\n  ],\n  \"Settings\": {\n    \"Formality\": \"string\",\n    \"Profanity\": \"string\"\n  },\n  \"SourceLanguageCode\": \"string\",\n  \"TargetLanguageCodes\": [\n    \"string\"\n  ],\n  \"TerminologyNames\": [\n    \"string\"\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring a NiFi Data Ingestion Processor in JavaScript\nDESCRIPTION: This snippet configures a data ingestion processor within Apache NiFi using a JavaScript script. It sets up flow file handling, error handling, and data parsing. Dependencies include NiFi's scripting API environment; it processes incoming data streams and prepares them for subsequent processing stages.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/test/resources/TestReplaceTextLineByLine/ReplaceExceptLastLine.txt#_snippet_0\n\nLANGUAGE: JavaScript\nCODE:\n```\n// Initialize flow file processing\nvar flowFile = session.get();\nif (flowFile != null) {\n    try {\n        // Read data from flow file\n        var inputStream = session.read(flowFile);\n        var data = IOUtils.toByteArray(inputStream);\n        inputStream.close();\n        // Parse data or perform transformations here\n        // Transfer flow file to success relationship\n        session.transfer(flowFile, REL_SUCCESS);\n    } catch (e) {\n        // Transfer flow file to failure relationship on error\n        session.transfer(flowFile, REL_FAILURE);\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Valid XML Example 1\nDESCRIPTION: An example of an XML document that conforms to the provided XSD schema. It features a 'bundle' element containing one 'node' with two 'subNode' children.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ValidateXml/additionalDetails.md#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<ns:bundle xmlns:ns=\"http://namespace/1\">\n    <node>\n        <subNode>\n            <value>Hello</value>\n        </subNode>\n        <subNode>\n            <value>World!</value>\n        </subNode>\n    </node>\n</ns:bundle>\n```\n\n----------------------------------------\n\nTITLE: Starting MiNiFi on Windows - Batch Command\nDESCRIPTION: This snippet shows the method to launch the MiNiFi Java Agent on Windows platforms by running a batch file within the `/bin` directory of the MiNiFi installation. This keeps MiNiFi running in the foreground. MiNiFi can be terminated by focusing the launched window and pressing Ctrl+C. No additional parameters are required to run this batch file under Windows command prompt or double-clicking.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-docs/src/main/markdown/minifi-java-agent-quick-start.md#_snippet_1\n\nLANGUAGE: Batch\nCODE:\n```\nDouble-click the run-minifi.bat file located in the /bin directory of the MiNiFi installation.\n```\n\n----------------------------------------\n\nTITLE: XML Output Wrapping Arrays Using Field Name as Wrapper - XML\nDESCRIPTION: Shows the XML output when arrays are wrapped using the array field name as the container node and 'elem' as the tag name for each array element. This approach nests array elements inside a single parent node named after the array field.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLRecordSetWriter/additionalDetails.md#_snippet_7\n\nLANGUAGE: xml\nCODE:\n```\n<root_name>\n    <record_name>\n        <name1>value1</name1>\n        <array_field>\n            <elem>1</elem>\n            <elem>2</elem>\n            <elem>3</elem>\n        </array_field>\n    </record_name>\n</root_name>\n```\n\n----------------------------------------\n\nTITLE: Querying Provenance by Time Range SQL\nDESCRIPTION: This SQL query selects all columns from the PROVENANCE table within a specific time window. It filters for records where timestampMillis is greater than $provenanceStartTime and less than or equal to $provenanceEndTime. Requires replacing the variables with actual millisecond timestamp values. Essential for auditing, debugging, and tracking the lineage of data throughout the flow.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-sql-reporting-bundle/nifi-sql-reporting-tasks/src/main/resources/docs/org.apache.nifi.reporting.sql.QueryNiFiReportingTask/additionalDetails.md#_snippet_4\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT * from PROVENANCE where timestampMillis > $provenanceStartTime and timestampMillis <= $provenanceEndTime\n```\n\n----------------------------------------\n\nTITLE: Specifying charset-normalizer Dependency (Python)\nDESCRIPTION: Specifies the required version 3.4.0 for the `charset-normalizer` Python package. This line is typically found in a `requirements.txt` file, used by package managers like pip to install dependencies at a specific version, ensuring consistency across environments. It's required for components that handle character encoding detection.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-py4j-extension-bundle/nifi-python-test-extensions/src/main/resources/extensions/multi-processor/requirements.txt#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ncharset-normalizer==3.4.0\n```\n\n----------------------------------------\n\nTITLE: Avro Schema for Root Node YAML Data\nDESCRIPTION: Defines the expected structure of records derived from the sample YAML data when using the \"Root Node Strategy\" in NiFi's YamlTreeReader. This Avro schema includes complex types like records and arrays, mirroring the entire YAML document structure.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.yaml.YamlTreeReader/additionalDetails.md#_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"type\": \"record\",\n  \"name\": \"nifiRecord\",\n  \"namespace\": \"org.apache.nifi\",\n  \"fields\": [\n    {\n      \"name\": \"id\",\n      \"type\": [\n        \"int\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"name\",\n      \"type\": [\n        \"string\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"child\",\n      \"type\": [\n        {\n          \"type\": \"record\",\n          \"name\": \"childType\",\n          \"fields\": [\n            {\n              \"name\": \"id\",\n              \"type\": [\n                \"int\",\n                \"string\",\n                \"null\"\n              ]\n            }\n          ]\n        },\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"dob\",\n      \"type\": [\n        \"string\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"siblings\",\n      \"type\": [\n        {\n          \"type\": \"array\",\n          \"items\": {\n            \"type\": \"record\",\n            \"name\": \"siblingsType\",\n            \"fields\": [\n              {\n                \"name\": \"name\",\n                \"type\": [\n                  \"string\",\n                  \"null\"\n                ]\n              },\n              {\n                \"name\": \"id\",\n                \"type\": [\n                  \"int\",\n                  \"null\"\n                ]\n              }\n            ]\n          }\n        },\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"gender\",\n      \"type\": [\n        \"string\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"siblingIds\",\n      \"type\": [\n        {\n          \"type\": \"array\",\n          \"items\": \"string\"\n        },\n        \"null\"\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Invalid XML Example 1 with Error Message\nDESCRIPTION: An example of an XML document that fails validation against the provided XSD schema because the 'node' element contains text content instead of the required 'subNode' elements. The resulting validation error message is shown.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ValidateXml/additionalDetails.md#_snippet_3\n\nLANGUAGE: xml\nCODE:\n```\n<ns:bundle xmlns:ns=\"http://namespace/1\">\n    <node>Hello World!</node>\n</ns:bundle>\n```\n\nLANGUAGE: text\nCODE:\n```\nvalidatexml.invalid.error: cvc-complex-type.2.3: Element 'node' cannot have character [children], because the type's content type is element-only.\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi MiNiFi to Tail Logs and Forward Data (YAML)\nDESCRIPTION: This YAML configuration sets up a NiFi MiNiFi data flow that tails a log file, forwards log and provenance data to a remote NiFi instance, and manages various repositories, security, and provenance reporting settings. It includes a TailFile processor to monitor logs, remote group connections, and associated repository configurations. The flow facilitates secure and efficient data collection from local logs to a centralized NiFi cluster.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-docs/src/main/markdown/System_Admin_Guide.md#_snippet_15\n\nLANGUAGE: YAML\nCODE:\n```\n# <a id=\"example-config-file\" href=\"#example-config-file\">Example Config File</a>\n\nBelow are two example config YAML files. The first tails the *minifi-app.log* and sends the tailed log and provenance data back to a secure instance of NiFi. The second uses a series of processors to tail the app log, routes off only lines that contain \"WriteAheadFlowFileRepository\" and puts it as a file in the \"./\" directory.\n\n``` yaml\nMiNiFi Config Version: 1\nFlow Controller:\n    name: MiNiFi Flow\n    comment:\n\nCore Properties:\n    flow controller graceful shutdown period: 10 sec\n    flow service write delay interval: 500 ms\n    administrative yield duration: 30 sec\n    bored yield duration: 10 millis\n\nFlowFile Repository:\n    partitions: 256\n    checkpoint interval: 2 mins\n    always sync: false\n    Swap:\n        threshold: 20000\n        in period: 5 sec\n        in threads: 1\n        out period: 5 sec\n        out threads: 4\n\nProvenance Repository:\n    provenance rollover time: 1 min\n\nContent Repository:\n    content claim max appendable size: 10 MB\n    content claim max flow files: 100\n    always sync: false\n\nComponent Status Repository:\n    buffer size: 1440\n    snapshot frequency: 1 min\n\nSecurity Properties:\n    keystore: /tmp/ssl/localhost-ks.jks\n    keystore type: JKS\n    keystore password: localtest\n    key password: localtest\n    truststore: /tmp/ssl/localhost-ts.jks\n    truststore type: JKS\n    truststore password: localtest\n    ssl protocol: TLS\n    Sensitive Props:\n        key:\n        algorithm: NIFI_PBKDF2_AES_GCM_256\n\nProcessors:\n    - name: TailFile\n      class: org.apache.nifi.processors.standard.TailFile\n      max concurrent tasks: 1\n      scheduling strategy: TIMER_DRIVEN\n      scheduling period: 1 sec\n      penalization period: 30 sec\n      yield period: 1 sec\n      run duration nanos: 0\n      auto-terminated relationships list:\n      Properties:\n          File to Tail: logs/minifi-app.log\n          Rolling Filename Pattern: minifi-app*\n          Initial Start Position: Beginning of File\n\nConnections:\n    - name: TailToS2S\n      source name: TailFile\n      source relationship name: success\n      destination name: 8644cbcc-a45c-40e0-964d-5e536e2ada61\n      max work queue size: 0\n      max work queue data size: 1 MB\n      flowfile expiration: 60 sec\n      queue prioritizer class: org.apache.nifi.prioritizer.NewestFlowFileFirstPrioritizer\n\nRemote Processing Groups:\n    - name: NiFi Flow\n      comment:\n      url: https://localhost:8090/nifi\n      timeout: 30 secs\n      yield period: 10 sec\n      Input Ports:\n          - id: 8644cbcc-a45c-40e0-964d-5e536e2ada61\n            name: tailed log\n            comments:\n            max concurrent tasks: 1\n            use compression: false\n\nProvenance Reporting:\n    comment:\n    scheduling strategy: TIMER_DRIVEN\n    scheduling period: 30 sec\n    destination url: https://localhost:8090/\n    port name: provenance\n    originating url: http://${hostname(true)}:8081/nifi\n    use compression: true\n    timeout: 30 secs\n    batch size: 1000\n```\n\n```\n\n----------------------------------------\n\nTITLE: Defining Avro Schema for Status Records with Record Writer in Apache NiFi - JSON\nDESCRIPTION: This snippet provides the full Avro schema in JSON format expected by the SiteToSiteStatusReportingTask's Record Writer in Apache NiFi. The schema defines the structure for status event records, specifying required and optional fields such as statusId, timestampMillis, component metadata, flow file statistics, and additional process information. Users should ensure this schema matches the output of their Record Writer for compatibility, and should use Avro-compatible serialization/deserialization tools to process these records as expected by NiFi's Site To Site protocol.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-site-to-site-reporting-bundle/nifi-site-to-site-reporting-task/src/main/resources/docs/org.apache.nifi.reporting.SiteToSiteStatusReportingTask/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"record\",\n  \"name\": \"status\",\n  \"namespace\": \"status\",\n  \"fields\": [\n    {\n      \"name\": \"statusId\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"timestampMillis\",\n      \"type\": {\n        \"type\": \"long\",\n        \"logicalType\": \"timestamp-millis\"\n      }\n    },\n    {\n      \"name\": \"timestamp\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"actorHostname\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"componentType\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"componentName\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"parentId\",\n      \"type\": [\n        \"string\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"parentName\",\n      \"type\": [\n        \"string\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"parentPath\",\n      \"type\": [\n        \"string\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"platform\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"application\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"componentId\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"activeThreadCount\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"flowFilesReceived\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"flowFilesSent\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"bytesReceived\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"bytesSent\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"queuedCount\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"bytesRead\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"bytesWritten\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"terminatedThreadCount\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"runStatus\",\n      \"type\": [\n        \"string\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"bytesTransferred\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"flowFilesTransferred\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"inputContentSize\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"outputContentSize\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"queuedContentSize\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"versionedFlowState\",\n      \"type\": [\n        \"string\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"activeRemotePortCount\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"inactiveRemotePortCount\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"receivedContentSize\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"receivedCount\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"sentContentSize\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"sentCount\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"averageLineageDuration\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"transmissionStatus\",\n      \"type\": [\n        \"string\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"targetURI\",\n      \"type\": [\n        \"string\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"inputBytes\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"inputCount\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"outputBytes\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"outputCount\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"transmitting\",\n      \"type\": [\n        \"boolean\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"sourceId\",\n      \"type\": [\n        \"string\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"sourceName\",\n      \"type\": [\n        \"string\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"destinationId\",\n      \"type\": [\n        \"string\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"destinationName\",\n      \"type\": [\n        \"string\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"maxQueuedBytes\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"maxQueuedCount\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"queuedBytes\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"backPressureBytesThreshold\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"backPressureObjectThreshold\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"backPressureDataSizeThreshold\",\n      \"type\": [\n        \"string\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"isBackPressureEnabled\",\n      \"type\": [\n        \"string\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"processorType\",\n      \"type\": [\n        \"string\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"averageLineageDurationMS\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"flowFilesRemoved\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"invocations\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"processingNanos\",\n      \"type\": [\n        \"long\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"executionNode\",\n      \"type\": [\n        \"string\",\n        \"null\"\n      ]\n    },\n    {\n      \"name\": \"counters\",\n      \"type\": [\n        \"null\",\n        {\n          \"type\": \"map\",\n          \"values\": \"string\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Example JSON Input\nDESCRIPTION: This JSON snippet represents an example input array containing a single object with fields like 'type', 'subtype', 'class', and 'size'. It demonstrates the structure of the expected input data.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-aws-bundle/nifi-aws-processors/src/main/resources/docs/org.apache.nifi.processors.aws.dynamodb.PutDynamoDBRecord/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"type\": \"A\",\n    \"subtype\": 4,\n    \"class\": \"t\",\n    \"size\": 1\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: RecordSet with Null and Missing Values - Pseudo-Code\nDESCRIPTION: Illustrates records containing null and missing field values to explain how the XMLRecordSetWriter handles such cases. Depending on configuration, null or missing fields can be suppressed or included as empty XML nodes.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLRecordSetWriter/additionalDetails.md#_snippet_3\n\nLANGUAGE: pseudo\nCODE:\n```\nRecordSet (\n  Record (\n    Field \"name1\" = \"value1\",\n    Field \"name2\" = null\n  ),\n  Record (\n    Field \"name1\" = \"value2\",\n  )\n)\n```\n\n----------------------------------------\n\nTITLE: Avro Schema for Nested Field Strategy in Apache NiFi\nDESCRIPTION: An Avro schema definition for processing sibling records from a nested JSON structure. This schema is used when applying the Nested Field strategy with 'siblings' as the Starting Field Name.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.json.JsonTreeReader/additionalDetails.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"namespace\": \"nifi\",\n  \"name\": \"siblings\",\n  \"type\": \"record\",\n  \"fields\": [\n    {\n      \"name\": \"name\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"id\",\n      \"type\": \"int\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining the IPLookupService Enrichment Record Avro Schema (JSON)\nDESCRIPTION: Defines the Avro schema for the 'enrichmentRecord' returned by NiFi's IPLookupService. This complex record includes nullable fields for geographic data ('geo'), ISP information ('isp'), domain name, connection type, and anonymous IP details, sourced from a MaxMind database.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-lookup-services-bundle/nifi-lookup-services/src/main/resources/docs/org.apache.nifi.lookup.maxmind.IPLookupService/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"enrichmentRecord\",\n  \"namespace\": \"nifi\",\n  \"type\": \"record\",\n  \"fields\": [\n    {\n      \"name\": \"geo\",\n      \"type\": [\n        \"null\",\n        {\n          \"name\": \"cityGeo\",\n          \"type\": \"record\",\n          \"fields\": [\n            {\n              \"name\": \"city\",\n              \"type\": [\n                \"null\",\n                \"string\"\n              ]\n            },\n            {\n              \"name\": \"accuracy\",\n              \"type\": [\n                \"null\",\n                \"int\"\n              ],\n              \"doc\": \"The radius, in kilometers, around the given location, where the IP address is believed to be\"\n            },\n            {\n              \"name\": \"metroCode\",\n              \"type\": [\n                \"null\",\n                \"int\"\n              ]\n            },\n            {\n              \"name\": \"timeZone\",\n              \"type\": [\n                \"null\",\n                \"string\"\n              ]\n            },\n            {\n              \"name\": \"latitude\",\n              \"type\": [\n                \"null\",\n                \"double\"\n              ]\n            },\n            {\n              \"name\": \"longitude\",\n              \"type\": [\n                \"null\",\n                \"double\"\n              ]\n            },\n            {\n              \"name\": \"country\",\n              \"type\": [\n                \"null\",\n                {\n                  \"type\": \"record\",\n                  \"name\": \"country\",\n                  \"fields\": [\n                    {\n                      \"name\": \"name\",\n                      \"type\": \"string\"\n                    },\n                    {\n                      \"name\": \"isoCode\",\n                      \"type\": \"string\"\n                    }\n                  ]\n                }\n              ]\n            },\n            {\n              \"name\": \"subdivisions\",\n              \"type\": {\n                \"type\": \"array\",\n                \"items\": {\n                  \"type\": \"record\",\n                  \"name\": \"subdivision\",\n                  \"fields\": [\n                    {\n                      \"name\": \"name\",\n                      \"type\": \"string\"\n                    },\n                    {\n                      \"name\": \"isoCode\",\n                      \"type\": \"string\"\n                    }\n                  ]\n                }\n              }\n            },\n            {\n              \"name\": \"continent\",\n              \"type\": [\n                \"null\",\n                \"string\"\n              ]\n            },\n            {\n              \"name\": \"postalCode\",\n              \"type\": [\n                \"null\",\n                \"string\"\n              ]\n            }\n          ]\n        }\n      ]\n    },\n    {\n      \"name\": \"isp\",\n      \"type\": [\n        \"null\",\n        {\n          \"name\": \"ispEnrich\",\n          \"type\": \"record\",\n          \"fields\": [\n            {\n              \"name\": \"name\",\n              \"type\": [\n                \"null\",\n                \"string\"\n              ]\n            },\n            {\n              \"name\": \"organization\",\n              \"type\": [\n                \"null\",\n                \"string\"\n              ]\n            },\n            {\n              \"name\": \"asn\",\n              \"type\": [\n                \"null\",\n                \"int\"\n              ]\n            },\n            {\n              \"name\": \"asnOrganization\",\n              \"type\": [\n                \"null\",\n                \"string\"\n              ]\n            }\n          ]\n        }\n      ]\n    },\n    {\n      \"name\": \"domainName\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ]\n    },\n    {\n      \"name\": \"connectionType\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ],\n      \"doc\": \"One of 'Dialup', 'Cable/DSL', 'Corporate', 'Cellular'\"\n    },\n    {\n      \"name\": \"anonymousIp\",\n      \"type\": [\n        \"null\",\n        {\n          \"name\": \"anonymousIpType\",\n          \"type\": \"record\",\n          \"fields\": [\n            {\n              \"name\": \"anonymous\",\n              \"type\": \"boolean\"\n            },\n            {\n              \"name\": \"anonymousVpn\",\n              \"type\": \"boolean\"\n            },\n            {\n              \"name\": \"hostingProvider\",\n              \"type\": \"boolean\"\n            },\n            {\n              \"name\": \"publicProxy\",\n              \"type\": \"boolean\"\n            },\n            {\n              \"name\": \"torExitNode\",\n              \"type\": \"boolean\"\n            }\n          ]\n        }\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Handling Processor Start/Stop/Unscheduled Events in Groovy\nDESCRIPTION: This code illustrates how to define static methods `onStart`, `onStop`, and `onUnscheduled` within a Groovy processor to handle corresponding lifecycle events. It uses static variables stored in a Const class to retain values between executions. These methods will be automatically called when the processor's state changes.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-groovyx-bundle/nifi-groovyx-processors/src/main/resources/docs/org.apache.nifi.processors.groovyx.ExecuteGroovyScript/additionalDetails.md#_snippet_8\n\nLANGUAGE: Groovy\nCODE:\n```\nimport org.apache.nifi.processor.ProcessContext\nimport java.util.concurrent.atomic.AtomicLong\n\nclass Const {\n    static Date startTime = null;\n    static AtomicLong triggerCount = null;\n}\n\nstatic onStart(ProcessContext context) {\n    Const.startTime = new Date()\n    Const.triggerCount = new AtomicLong(0)\n    println \"onStart $context ${Const.startTime}\"\n}\n\nstatic onStop(ProcessContext context) {\n    def alive = (System.currentTimeMillis() - Const.startTime.getTime()) / 1000\n    println \"onStop $context executed ${Const.triggerCount} times during ${alive} seconds\"\n}\n\nstatic onUnscheduled(ProcessContext context) {\n    def alive = (System.currentTimeMillis() - Const.startTime.getTime()) / 1000\n    println \"onUnscheduled $context executed ${Const.triggerCount} times during ${alive} seconds\"\n}\n\nflowFile.'trigger.count' = Const.triggerCount.incrementAndGet()\nREL_SUCCESS << flowFile\n```\n\n----------------------------------------\n\nTITLE: Record Schema Including an Array Field - JSON\nDESCRIPTION: Defines a schema that includes an array field type, demonstrating how arrays of integers are described. This schema supports configuring the XML output for array serialization with different wrapping strategies.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLRecordSetWriter/additionalDetails.md#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"test\",\n  \"namespace\": \"nifi\",\n  \"type\": \"record\",\n  \"fields\": [\n    {\n      \"name\": \"array_field\",\n      \"type\": {\n        \"type\": \"array\",\n        \"items\": int\n      }\n    },\n    {\n      \"name\": \"name1\",\n      \"type\": \"string\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Calculating average from sum and count in NiFi\nDESCRIPTION: This code calculates the average by dividing 'theSum' by 'theCount' using getStateValue and divide functions, enabling real-time average computation based on accumulated sum and count.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-update-attribute-bundle/nifi-update-attribute-processor/src/main/resources/docs/org.apache.nifi.processors.attributes.UpdateAttribute/additionalDetails.md#_snippet_2\n\nLANGUAGE: NiFi Expression Language\nCODE:\n```\n${getStateValue(\"theSum\"):divide(getStateValue(\"theCount\"))}\n```\n\n----------------------------------------\n\nTITLE: Secured NiFi Registry with Two-Way TLS Certificates\nDESCRIPTION: This command runs the NiFi Registry container configured with TLS authentication, requiring volume mount for certificates and environment variables for keystore, truststore, and admin identity configuration.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-core/nifi-registry-docker/dockerhub/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --name nifi-registry \\\n  -v /path/to/tls/certs/localhost:/opt/certs \\\n  -p 18443:18443 \\\n  -e AUTH=tls \\\n  -e KEYSTORE_PATH=/opt/certs/keystore.jks \\\n  -e KEYSTORE_TYPE=JKS \\\n  -e KEYSTORE_PASSWORD=QKZv1hSWAFQYZ+WU1jjF5ank+l4igeOfQRp+OSbkkrs \\\n  -e TRUSTSTORE_PATH=/opt/certs/truststore.jks \\\n  -e TRUSTSTORE_PASSWORD=rHkWR1gDNW3R9hgbeRsT3OM3Ue0zwGtQqcFKJD2EXWE \\\n  -e TRUSTSTORE_TYPE=JKS \\\n  -e INITIAL_ADMIN_IDENTITY='CN=AdminUser, OU=nifi' \\\n  -d \\\n  apache/nifi-registry:latest\n```\n\n----------------------------------------\n\nTITLE: Transforming Data Payload with Python Script in NiFi\nDESCRIPTION: This Python script transforms incoming data payloads by applying business logic, such as data masking or format conversion. It is tailored for use within NiFi's ExecuteScript processor, requiring Python interpreter setup. It takes input data, processes it, and outputs the modified version for further flow steps.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/test/resources/TestReplaceTextLineByLine/ReplaceExceptLastLine.txt#_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport json\n# Read the flow file content\ndata = session.read(flowFile).decode('utf-8')\n# Parse JSON payload\npayload = json.loads(data)\n# Perform data transformation\npayload['masked'] = True\n# Write output back\ndataR = json.dumps(payload)\nsesssion.write(flowFile, lambda out: out.write(dataR.encode('utf-8')))\nsession.transfer(flowFile, REL_SUCCESS)\n```\n\n----------------------------------------\n\nTITLE: Specifying Projection for GetMongo Results (JSON)\nDESCRIPTION: Provides a JSON example for the 'Projection' configuration property in the NiFi GetMongo processor. This example demonstrates how to exclude the default '_id' field from the query results by setting its value to 0. Use 1 to include a field and 0 to exclude it.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-mongodb-bundle/nifi-mongodb-processors/src/main/resources/docs/org.apache.nifi.processors.mongodb.GetMongo/additionalDetails.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{ \"_id\": 0 }\n```\n\n----------------------------------------\n\nTITLE: Resulting Zendesk API Request via Record Path (JSON)\nDESCRIPTION: This JSON object represents the final request payload generated by the PutZendeskTicket processor and sent to the Zendesk API. It shows how the values extracted from the input record using the specified Record Paths (as configured in the processor attributes) are mapped to the expected structure of the Zendesk Create Ticket API request.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-zendesk-bundle/nifi-zendesk-processors/src/main/resources/docs/org.apache.nifi.processors.zendesk.PutZendeskTicket/additionalDetails.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"comment\": {\n    \"body\": \"This is a sample description.\"\n  },\n  \"subject\": \"General error\",\n  \"priority\": \"Immediate\",\n  \"type\": \"Maintenance\"\n}\n```\n\n----------------------------------------\n\nTITLE: XML Output Without Array Wrapping - XML\nDESCRIPTION: Illustrates the XML output when the array field elements are serialized as repeated tags with the same field name under the record node without a wrapping container. Each array element is directly represented as individual nodes.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLRecordSetWriter/additionalDetails.md#_snippet_6\n\nLANGUAGE: xml\nCODE:\n```\n<root_name>\n    <record_name>\n        <name1>value1</name1>\n        <array_field>1</array_field>\n        <array_field>2</array_field>\n        <array_field>3</array_field>\n    </record_name>\n</root_name>\n```\n\n----------------------------------------\n\nTITLE: Defining Avro Schema for NiFi Metrics Record Format in JSON\nDESCRIPTION: This JSON snippet defines an Avro schema for NiFi's metrics output when using the Record format in the Site-to-Site Metrics Reporting Task. The schema specifies a record named 'metrics' with fields covering application ID, instance ID, hostname, timestamps, and a wide variety of performance and JVM metrics. Each field's data type is clearly specified, including unions for nullable fields. This schema is used by a Record Writer controller service to serialize metrics data aligning with NiFi's record framework.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-site-to-site-reporting-bundle/nifi-site-to-site-reporting-task/src/main/resources/docs/org.apache.nifi.reporting.SiteToSiteMetricsReportingTask/additionalDetails.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"type\": \"record\",\n  \"name\": \"metrics\",\n  \"namespace\": \"metrics\",\n  \"fields\": [\n    { \"name\": \"appid\", \"type\": \"string\" },\n    { \"name\": \"instanceid\", \"type\": \"string\" },\n    { \"name\": \"hostname\", \"type\": \"string\" },\n    { \"name\": \"timestamp\", \"type\": \"long\" },\n    { \"name\": \"loadAverage1min\", \"type\": \"double\" },\n    { \"name\": \"availableCores\", \"type\": \"int\" },\n    { \"name\": \"FlowFilesReceivedLast5Minutes\", \"type\": \"int\" },\n    { \"name\": \"BytesReceivedLast5Minutes\", \"type\": \"long\" },\n    { \"name\": \"FlowFilesSentLast5Minutes\", \"type\": \"int\" },\n    { \"name\": \"BytesSentLast5Minutes\", \"type\": \"long\" },\n    { \"name\": \"FlowFilesQueued\", \"type\": \"int\" },\n    { \"name\": \"BytesQueued\", \"type\": \"long\" },\n    { \"name\": \"BytesReadLast5Minutes\", \"type\": \"long\" },\n    { \"name\": \"BytesWrittenLast5Minutes\", \"type\": \"long\" },\n    { \"name\": \"ActiveThreads\", \"type\": \"int\" },\n    { \"name\": \"TotalTaskDurationSeconds\", \"type\": \"long\" },\n    { \"name\": \"TotalTaskDurationNanoSeconds\", \"type\": \"long\" },\n    { \"name\": \"jvmuptime\", \"type\": \"long\" },\n    { \"name\": \"jvmheap_used\", \"type\": \"double\" },\n    { \"name\": \"jvmheap_usage\", \"type\": \"double\" },\n    { \"name\": \"jvmnon_heap_usage\", \"type\": \"double\" },\n    { \"name\": \"jvmthread_statesrunnable\", \"type\": [\"int\", \"null\"] },\n    { \"name\": \"jvmthread_statesblocked\", \"type\": [\"int\", \"null\"] },\n    { \"name\": \"jvmthread_statestimed_waiting\", \"type\": [\"int\", \"null\"] },\n    { \"name\": \"jvmthread_statesterminated\", \"type\": [\"int\", \"null\"] },\n    { \"name\": \"jvmthread_count\", \"type\": \"int\" },\n    { \"name\": \"jvmdaemon_thread_count\", \"type\": \"int\" },\n    { \"name\": \"jvmfile_descriptor_usage\", \"type\": \"double\" },\n    { \"name\": \"jvmgcruns\", \"type\": [\"long\", \"null\"] },\n    { \"name\": \"jvmgctime\", \"type\": [\"long\", \"null\"] }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Defining RFC 5424 Schema in JSON\nDESCRIPTION: This JSON snippet defines the schema used by the Syslog5424Reader to represent parsed RFC 5424 Syslog messages.  It specifies the structure and data types of each field in the record. The schema includes fields for various Syslog components like priority, timestamp, and structured data.  No external dependencies are needed.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.syslog.Syslog5424Reader/additionalDetails.md#_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"type\": \"record\",\n  \"name\": \"nifiRecord\",\n  \"namespace\": \"org.apache.nifi\",\n  \"fields\": [\n    {\n      \"name\": \"priority\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ]\n    },\n    {\n      \"name\": \"severity\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ]\n    },\n    {\n      \"name\": \"facility\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ]\n    },\n    {\n      \"name\": \"version\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ]\n    },\n    {\n      \"name\": \"timestamp\",\n      \"type\": [\n        \"null\",\n        {\n          \"type\": \"long\",\n          \"logicalType\": \"timestamp-millis\"\n        }\n      ]\n    },\n    {\n      \"name\": \"hostname\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ]\n    },\n    {\n      \"name\": \"body\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ]\n    },\n    \"name\"\n    :\n    \"appName\",\n    \"type\"\n    :\n    [\n      \"null\",\n      \"string\"\n    ]\n    },\n    {\n      \"name\": \"procid\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ]\n    },\n    {\n      \"name\": \"messageid\",\n      \"type\": [\n        \"null\",\n        \"string\"\n      ]\n    },\n    {\n      \"name\": \"structuredData\",\n      \"type\": [\n        \"null\",\n        {\n          \"type\": \"map\",\n          \"values\": {\n            \"type\": \"map\",\n            \"values\": \"string\"\n          }\n        }\n      ]\n    }\n  ]\n}\n\n```\n\n----------------------------------------\n\nTITLE: Netflowv5Parser JSON Output Schema Definition\nDESCRIPTION: Defines the structure of the JSON object produced by the Netflowv5Parser processor when configured for JSON output. It includes top-level fields for port and format, a 'header' object with metadata like version, count, timestamps, and engine info, and a 'record' object detailing flow information like source/destination addresses, ports, packet/octet counts, timestamps, protocol, and AS details.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-network-bundle/nifi-network-processors/src/main/resources/docs/org.apache.nifi.processors.network.ParseNetflowv5/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"port\": int,\n  \"format\": string,\n  \"header\": {\n    \"version\": int,\n    \"count\": int,\n    \"sys_uptime\": long,\n    \"unix_secs\": long,\n    \"unix_nsecs\": long,\n    \"flow_sequence\": long,\n    \"engine_type\": short,\n    \"engine_id\": short,\n    \"sampling_interval\": int\n  },\n  \"record\": {\n    \"srcaddr\": string,\n    \"dstaddr\": string,\n    \"nexthop\": string,\n    \"input\": int,\n    \"output\": int,\n    \"dPkts\": long,\n    \"dOctets\": long,\n    \"first\": long,\n    \"last\": long,\n    \"srcport\": int,\n    \"dstport\": int,\n    \"pad1\": short,\n    \"tcp_flags\": short,\n    \"prot\": short,\n    \"tos\": short,\n    \"src_as\": int,\n    \"dst_as\": int,\n    \"src_mask\": short,\n    \"dst_mask\": short,\n    \"pad2\": int\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Record Schema in JSON for XMLRecordSetWriter - JSON\nDESCRIPTION: Shows an example Avro-like schema in JSON format used to define the structure of incoming records for the XMLRecordSetWriter. This schema includes field names and data types (string and int), enabling the writer to understand how to serialize records to XML nodes. This schema is a prerequisite configuration for the Controller Service.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLRecordSetWriter/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"test\",\n  \"namespace\": \"nifi\",\n  \"type\": \"record\",\n  \"fields\": [\n    {\n      \"name\": \"name1\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"name2\",\n      \"type\": \"int\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Example RecordSet Structure in Pseudo-Code\nDESCRIPTION: Represents the structure of multiple records in a RecordSet format with fields and their corresponding values. This illustrates how records are logically grouped before transformation into XML. It also demonstrates examples of null or missing field values to showcase handling strategies.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLRecordSetWriter/additionalDetails.md#_snippet_1\n\nLANGUAGE: pseudo\nCODE:\n```\nRecordSet (\n  Record (\n    Field \"name1\" = \"value1\",\n    Field \"name2\" = 42\n  ),\n  Record (\n    Field \"name1\" = \"value2\",\n    Field \"name2\" = 84\n  )\n)\n```\n\n----------------------------------------\n\nTITLE: Python Script for Data Transformation in Apache NiFi\nDESCRIPTION: This Python script performs data transformation tasks within an Apache NiFi data flow. It depends on the 'nifi' Python SDK or API to interact with the NiFi framework, accepting data input, processing it, and outputting transformed data for further flow. It handles core data manipulation operations with specific focus on data enrichment or filtering.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/test/resources/TestUnpackContent/folder/date.txt#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport nifi\n\n# Data transformation logic\n\ndef transform_data(data):\n    # Process and modify data\n    transformed = data.upper()\n    return transformed\n\n# Main execution\nif __name__ == \"__main__\":\n    input_data = 'sample data'\n    output_data = transform_data(input_data)\n    print(output_data)\n```\n\n----------------------------------------\n\nTITLE: Output JSON for Insert Enrichment Fields Join Strategy Using '/purchase/customer' Record Path\nDESCRIPTION: Represents the result of applying the Insert Enrichment Fields strategy with a Record Path of '/purchase/customer'. Shows original records extended with nested 'customerDetails' enrichment data merged by index into the specified path within each record. Demonstrates successful enrichment field insertion.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.JoinEnrichment/additionalDetails.md#_snippet_5\n\nLANGUAGE: JSON\nCODE:\n```\n[\n  {\n    \"purchase\": {\n      \"customer\": {\n        \"loyaltyId\": 48202,\n        \"firstName\": \"John\",\n        \"lastName\": \"Doe\",\n        \"customerDetails\": {\n          \"id\": 48202,\n          \"phone\": \"555-555-5555\",\n          \"email\": \"john.doe@nifi.apache.org\"\n        }\n      },\n      \"total\": 48.28,\n      \"items\": [\n        {\n          \"itemDescription\": \"book\",\n          \"price\": 24.14,\n          \"quantity\": 2\n        }\n      ]\n    }\n  },\n  {\n    \"purchase\": {\n      \"customer\": {\n        \"loyaltyId\": 5512,\n        \"firstName\": \"Jane\",\n        \"lastName\": \"Doe\",\n        \"customerDetails\": {\n          \"id\": 5512,\n          \"phone\": \"555-555-5511\",\n          \"email\": \"jane.doe@nifi.apache.org\"\n        }\n      },\n      \"total\": 121.44,\n      \"items\": [\n        {\n          \"itemDescription\": \"book\",\n          \"price\": 28.15,\n          \"quantity\": 4\n        },\n        {\n          \"itemDescription\": \"inkpen\",\n          \"price\": 4.42,\n          \"quantity\": 2\n        }\n      ]\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Generated Zendesk Request Object (Record Path Mapping) - JSON\nDESCRIPTION: Illustrates the JSON structure of the Zendesk API request payload generated by the sink when processor attributes like Comment Body, Subject, Priority, and Type are mapped using the Record Path format from the incoming record.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-zendesk-bundle/nifi-zendesk-services/src/main/resources/docs/org.apache.nifi.services.zendesk.ZendeskRecordSink/additionalDetails.md#_snippet_1\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"comment\": {\n    \"body\": \"This is a sample description.\"\n  },\n  \"subject\": \"General error\",\n  \"priority\": \"Immediate\",\n  \"type\": \"Maintenance\"\n}\n```\n\n----------------------------------------\n\nTITLE: Minimal NiFi Stateless Configuration with Local JSON Flow File\nDESCRIPTION: A minimal configuration example for NiFi Stateless that only specifies a local JSON flow file path without any parameters or reporting tasks.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-stateless/nifi-stateless-assembly/README.md#_snippet_8\n\nLANGUAGE: properties\nCODE:\n```\nnifi.stateless.flow.snapshot.file=/var/lib/nifi/stateless-flows/kafka-to-hdfs.json\n```\n\n----------------------------------------\n\nTITLE: XML Input for ValidateRecord Example 2\nDESCRIPTION: This XML snippet is the input for the `ValidateRecord` processor using the schema defined in Example 2.  The input data for `field1` appears once and for `field2` appears twice. This highlights the array type and how the `Force Types From Reader's Schema` property affects the data transformation.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ValidateRecord/additionalDetails.md#_snippet_5\n\nLANGUAGE: XML\nCODE:\n```\n<test>\n    <field1>content_1</field1>\n    <field2>content_2</field2>\n    <field2>content_3</field2>\n</test>\n```\n\n----------------------------------------\n\nTITLE: Avro Schema for Map Structure in NiFi\nDESCRIPTION: Defines the Avro schema for the 'Map Example' XML. The `map_field` is declared with type `map` and string values (`items: string`), enabling NiFi to parse the corresponding XML structure.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLReader/additionalDetails.md#_snippet_21\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"namespace\": \"nifi\",\n  \"name\": \"test\",\n  \"type\": \"record\",\n  \"fields\": [\n    {\n      \"name\": \"map_field\",\n      \"type\": {\n        \"type\": \"map\",\n        \"items\": string\n      }\n    },\n    {\n      \"name\": \"simple_field\",\n      \"type\": \"string\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Shell Script for Automating NiFi Configuration Deployment\nDESCRIPTION: This shell script automates the deployment or updating of Apache NiFi configurations by copying configuration files to the proper directories and restarting services as needed. It requires default Unix shell environment and typical command-line tools, ensuring seamless configuration management for the NiFi server.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/test/resources/TestUnpackContent/folder/date.txt#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\n#!/bin/bash\n\n# Define NiFi configuration directory\nCONFIG_DIR=/opt/nifi/conf\n\n# Backup existing configuration\ncp $CONFIG_DIR/nifi.properties $CONFIG_DIR/nifi.properties.bak\n\n# Copy new configuration files\ncp ./nifi.properties $CONFIG_DIR/\n\n# Restart NiFi service to apply changes\nservice nifi restart\n```\n\n----------------------------------------\n\nTITLE: Avro Schema for ValidateRecord Example 2\nDESCRIPTION: This JSON snippet represents the schema for Example 2.  It defines a record with two fields, both of type array containing strings. This is the schema the `ValidateRecord` processor will use for validation, and how the output gets converted to JSON or XML format, depending on the configuration.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ValidateRecord/additionalDetails.md#_snippet_4\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"namespace\": \"nifi\",\n  \"name\": \"test\",\n  \"type\": \"record\",\n  \"fields\": [\n    {\n      \"name\": \"field1\",\n      \"type\": {\n        \"type\": \"array\",\n        \"items\": \"string\"\n      }\n    },\n    {\n      \"name\": \"field2\",\n      \"type\": {\n        \"type\": \"array\",\n        \"items\": \"string\"\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: JSON Output Example 1 with Force Types = false\nDESCRIPTION: This JSON snippet illustrates the output of the `ValidateRecord` processor when `Force Types From Reader's Schema` is set to false in Example 1.  Because the types are not forced the output contains all fields, even if they don't comply with the schema, this will be routed to the invalid relationship. This illustrates how the processor preserves all input fields and their original structure despite schema non-compliance.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ValidateRecord/additionalDetails.md#_snippet_3\n\nLANGUAGE: JSON\nCODE:\n```\n[\n  {\n    \"field1\": {\n      \"sub_field\": \"content\"\n    },\n    \"field2\": \"content_of_field_2\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Getting a Single FlowFile in Groovy for NiFi ExecuteScript\nDESCRIPTION: Retrieves a single FlowFile from the NiFi session's input queue using `session.get()`. The script checks if a FlowFile was successfully retrieved; if `session.get()` returns null (meaning no FlowFile is available), the script exits immediately. This approach is suitable when the script needs one FlowFile to perform its task. Depends on the implicit `session` object provided by NiFi.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-scripting-bundle/nifi-scripting-processors/src/main/resources/docs/org.apache.nifi.processors.script.ExecuteScript/additionalDetails.md#_snippet_0\n\nLANGUAGE: groovy\nCODE:\n```\nflowFile = session.get()\nif (!flowFile) return\n```\n\n----------------------------------------\n\nTITLE: XML Example of Windows Event Log Output\nDESCRIPTION: This XML snippet shows the structure of the event data emitted by the ConsumeWindowsEventLog processor, including system information, event data, and security details. It demonstrates how the processor represents Windows Event Log entries in an XML format for consumption by other NiFi processors.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-windows-event-log-bundle/nifi-windows-event-log-processors/src/main/resources/docs/org.apache.nifi.processors.windows.event.log.ConsumeWindowsEventLog/additionalDetails.md#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<Event xmlns=\"http://schemas.microsoft.com/win/2004/08/events/event\">\n    <System>\n        <Provider Name=\"Service Control Manager\" Guid=\"{555908d1-a6d7-4695-8e1e-26931d2012f4}\"\n                  EventSourceName=\"Service Control Manager\"/>\n        <EventID Qualifiers=\"16384\">7036</EventID>\n        <Version>0</Version>\n        <Level>4</Level>\n        <Task>0</Task>\n        <Opcode>0</Opcode>\n        <Keywords>0x8080000000000000</Keywords>\n        <TimeCreated SystemTime=\"2016-06-10T22:28:53.905233700Z\"/>\n        <EventRecordID>34153</EventRecordID>\n        <Correlation/>\n        <Execution ProcessID=\"684\" ThreadID=\"3504\"/>\n        <Channel>System</Channel>\n        <Computer>WIN-O05CNUCF16M.hdf.local</Computer>\n        <Security/>\n    </System>\n    <EventData>\n        <Data Name=\"param1\">Smart Card Device Enumeration Service</Data>\n        <Data Name=\"param2\">running</Data>\n        <Binary>5300630044006500760069006300650045006E0075006D002F0034000000</Binary>\n    </EventData>\n</Event>\n```\n\n----------------------------------------\n\nTITLE: Groovy Transformation for Conditional Data Masking\nDESCRIPTION: This Groovy script is used with the TransformRecord processor to conditionally mask fields in the 'original' record based on the 'allowsPII' field in the 'enrichment' record. It sets 'ssn', 'phone', and 'email' fields to null if 'allowsPII' is false or null.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.JoinEnrichment/additionalDetails.md#_snippet_9\n\nLANGUAGE: groovy\nCODE:\n```\nimport org.apache.nifi.serialization.record.Record\n\nRecord original = (Record) record.getValue(\"original\")\nRecord enrichment = (Record) record.getValue(\"enrichment\")\n\nif (Boolean.TRUE != enrichment?.getAsBoolean(\"allowsPII\")) {\n    original.setValue(\"ssn\", null)\n    original.setValue(\"phone\", null)\n    original.setValue(\"email\", null)\n}\n\nreturn original\n```\n\n----------------------------------------\n\nTITLE: Standard Syslog Message Example\nDESCRIPTION: A standard syslog message following RFC 5424 format, showing priority, version, timestamp, hostname, application name, process ID, message ID, and the message body.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/test/resources/syslog/syslog5424/log_mix_in_error.txt#_snippet_0\n\nLANGUAGE: syslog\nCODE:\n```\n<14>1 2014-06-20T09:14:07+00:00 loggregator d0602076-b14a-4c55-852a-981e7afeed38 DEA - - Removing instance\n```\n\n----------------------------------------\n\nTITLE: Querying Connection Backpressure Prediction SQL\nDESCRIPTION: This SQL query selects the connectionId from the CONNECTION_STATUS_PREDICTIONS table. It filters for connections where the predicted time to reach backpressure based on queue count is less than 300,000 milliseconds (5 minutes). This helps identify connections that may soon become bottlenecks.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-sql-reporting-bundle/nifi-sql-reporting-tasks/src/main/resources/docs/org.apache.nifi.reporting.sql.QueryNiFiReportingTask/additionalDetails.md#_snippet_1\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT connectionId FROM CONNECTION_STATUS_PREDICTIONS WHERE predictedTimeToCountBackpressureMillis < 300000\n```\n\n----------------------------------------\n\nTITLE: Parsing XML without Data Loss using Unique Field for Content (NiFi XMLReader, XML)\nDESCRIPTION: Shows safe extraction of both attribute and mixed content from an XML element by setting 'Field Name for Content' to a unique value ('original_content'). The attribute is preserved as <attr>, the child element <value> is preserved, and the parent element's text content is mapped to <original_content>, ensuring all source data survives parsing. Requires assigning a unique content field not present as a sub-element.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLReader/additionalDetails.md#_snippet_15\n\nLANGUAGE: xml\nCODE:\n```\n<record>\n    <field_with_attribute attr=\"attr_content\">content of field<value>123</value>\n    </field_with_attribute>\n</record>\n```\n\nLANGUAGE: xml\nCODE:\n```\n<record>\n    <field_with_attribute>\n        <attr>attr_content</attr>\n        <value>123</value>\n        <original_content>content of field</original_content>\n    </field_with_attribute>\n</record>\n```\n\n----------------------------------------\n\nTITLE: Running NiFi with HTTPS and Single User Authentication\nDESCRIPTION: Docker commands to run NiFi container with default HTTPS and Single User Authentication settings. This includes examples of configuring communication ports and specifying custom credentials.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-docker/dockerhub/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --name nifi \\\n  -p 8443:8443 \\\n  -d \\\n  apache/nifi:latest\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker logs nifi | grep Generated\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --name nifi \\\n  -p 9443:9443 \\\n  -d \\\n  -e NIFI_WEB_HTTPS_PORT='9443' \\\n  apache/nifi:latest\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --name nifi \\\n  -p 8443:8443 \\\n  -d \\\n  -e SINGLE_USER_CREDENTIALS_USERNAME=admin \\\n  -e SINGLE_USER_CREDENTIALS_PASSWORD=ctsBtRBKHRAx69EqUghvvgEvjnaLjFEB \\\n  apache/nifi:latest\n```\n\n----------------------------------------\n\nTITLE: Syslog Message with Structured Data (SD)\nDESCRIPTION: An RFC 5424 syslog message that includes Structured Data elements ([exampleSDID@...]). These elements provide additional machine-readable context like event source and ID, alongside the human-readable message.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/test/resources/syslog/syslog5424/log_mix_in_error.txt#_snippet_2\n\nLANGUAGE: syslog\nCODE:\n```\n<14>1 2014-06-20T09:14:07+00:00 loggregator d0602076-b14a-4c55-852a-981e7afeed38 DEA MSG-01 [exampleSDID@32473 iut=\"3\" eventSource=\"Application\" eventID=\"1011\"][exampleSDID@32480 iut=\"4\" eventSource=\"Other Application\" eventID=\"2022\"] Removing instance\n```\n\n----------------------------------------\n\nTITLE: Defining Common Log Format Patterns for Apache and Syslog (Grok Pattern)\nDESCRIPTION: Provides Grok patterns for extracting structured fields from common log formats including Apache 'common' and 'combined' log styles, and Syslog base logs. Patterns break down log lines into fields such as timestamp, IP, user, request, response, and more. Designed for incorporation into NiFi Grok processors, these enable field-by-field extraction and analytics. The underlying field patterns are referenced and must also be defined and loaded.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/default-grok-patterns.txt#_snippet_4\n\nLANGUAGE: Grok pattern\nCODE:\n```\nSYSLOGBASE %{SYSLOGTIMESTAMP:timestamp} (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource} %{SYSLOGPROG}:\nCOMMONAPACHELOG %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \\[%{HTTPDATE:timestamp}\\] \"(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})\" %{NUMBER:response} (?:%{NUMBER:bytes}|-)\nCOMBINEDAPACHELOG %{COMMONAPACHELOG} %{QS:referrer} %{QS:agent}\n```\n\n----------------------------------------\n\nTITLE: Defining the LOGLEVEL Pattern for Grok Parsing (Grok Pattern)\nDESCRIPTION: Defines the LOGLEVEL pattern to match various log severity levels (alert, trace, debug, info, warning, error, etc.) across multiple capitalizations and alternative forms. No external dependencies beyond Grok are required, and the pattern is intended for use as a named Grok field within larger log-parsing expressions. This enables extraction and normalization of log level for downstream filtering and analysis. No external parameters are required.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/default-grok-patterns.txt#_snippet_0\n\nLANGUAGE: Grok pattern\nCODE:\n```\nLOGLEVEL ([Aa]lert|ALERT|[Tt]race|TRACE|[Dd]ebug|DEBUG|[Nn]otice|NOTICE|[Ii]nfo|INFO|[Ww]arn?(?:ing)?|WARN?(?:ING)?|[Ee]rr?(?:or)?|ERR?(?:OR)?|[Cc]rit?(?:ical)?|CRIT?(?:ICAL)?|[Ff]atal|FATAL|[Ss]evere|SEVERE|EMERG(?:ENCY)?|[Ee]merg(?:ency)?)|FINE|FINER|FINEST|CONFIG\n```\n\n----------------------------------------\n\nTITLE: Parsing XML with Field Name for Content Clashing with Child (NiFi XMLReader, XML)\nDESCRIPTION: Demonstrates the outcome when 'Field Name for Content' is set to 'value' but a child element with that name is also present. During parsing, the child element's data is overwritten by the parent element's content, resulting in loss of the original child value. Best avoided by ensuring 'Field Name for Content' does not clash with sub-element names.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLReader/additionalDetails.md#_snippet_14\n\nLANGUAGE: xml\nCODE:\n```\n<record>\n    <field_with_attribute attr=\"attr_content\">content of field<value>123</value>\n    </field_with_attribute>\n</record>\n```\n\nLANGUAGE: xml\nCODE:\n```\n<record>\n    <field_with_attribute>\n        <attr>attr_content</attr>\n        <value>content of field</value>\n    </field_with_attribute>\n</record>\n```\n\n----------------------------------------\n\nTITLE: Enabling StatusLogger Periodic Reporter in bootstrap.conf\nDESCRIPTION: Illustrates how to configure the `bootstrap.conf` file to enable a periodic status reporter. This line adds the `StatusLogger` implementation to the list of components that will periodically report the dataflow's status.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-docs/src/main/markdown/System_Admin_Guide.md#_snippet_4\n\nLANGUAGE: properties\nCODE:\n```\nnifi.minifi.status.reporter.components=org.apache.nifi.minifi.bootstrap.status.reporters.StatusLogger\n```\n\n----------------------------------------\n\nTITLE: Multi-Line Log Message Example in TailFile Processor\nDESCRIPTION: Example log file content showing how TailFile processor would handle multi-line messages, including standard log entries and a multi-line warning message that should be processed as a single unit.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.TailFile/additionalDetails.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n2021-07-09 14:12:19,731 INFO [main] org.apache.nifi.NiFi Launching NiFi... \n2021-07-09 14:12:19,915 INFO [main] o.a.n.p.AbstractBootstrapPropertiesLoader Determined default application properties path to be '/Users/mpayne/devel/nifi/nifi-assembly/target/nifi-1.14.0-SNAPSHOT-bin/nifi-1.14.0-SNAPSHOT/./conf/nifi.properties' \n2021-07-09 14:12:19,919 INFO [main] o.a.nifi.properties.NiFiPropertiesLoader Loaded 199 properties from /Users/mpayne/devel/nifi/nifi-assembly/target/nifi-1.14.0-SNAPSHOT-bin/nifi-1.14.0-SNAPSHOT/./conf/nifi.properties \n2021-07-09 14:12:19,925 WARN Line 1 of Log Message \t\t\tLine 2: This is an important warning. \t\t\tLine 3: Please do not ignore this warning. \t\t\tLine 4: These lines of text make sense only in the context of the original message. \n2021-07-09 14:12:19,941 INFO [main] Final message in log file\n```\n\n----------------------------------------\n\nTITLE: Build NiFi Registry without AWS Extensions\nDESCRIPTION: This command builds NiFi Registry, skipping the AWS extensions. It is used when the AWS extensions are not required or are being managed separately. The `-DskipAws` flag instructs Maven to exclude the AWS-related modules during the build process.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-extensions/nifi-registry-aws/nifi-registry-aws-assembly/README.md#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\ncd nifi-registry\nmvn clean install -DskipAws\n```\n\n----------------------------------------\n\nTITLE: Syslog Message with POISONPILL\nDESCRIPTION: A log entry potentially indicating a special termination or error condition signaled by 'POISONPILL'. This might be used in specific systems to trigger shutdown or error handling.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/test/resources/syslog/syslog5424/log_mix_in_error.txt#_snippet_1\n\nLANGUAGE: syslog\nCODE:\n```\nPOISONPILL 30303030\n```\n\n----------------------------------------\n\nTITLE: Avro Schema for ValidateRecord Example 1\nDESCRIPTION: This JSON code snippet represents the schema definition for the first example, used to validate the incoming XML data. It defines a record with two string fields: `field1` and `field2`.  This schema is used by the `ValidateRecord` processor to validate the incoming XML, and then the output is converted to JSON format.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ValidateRecord/additionalDetails.md#_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"namespace\": \"nifi\",\n  \"name\": \"test\",\n  \"type\": \"record\",\n  \"fields\": [\n    {\n      \"name\": \"field1\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"field2\",\n      \"type\": \"string\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Running NiFi Stateless with Command Line Parameter Passing - Single Parameter\nDESCRIPTION: Demonstrates running NiFi Stateless with a parameter passed via command line using the -p flag with the Parameter Context specified. This approach allows passing parameters without including them in the properties file.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-stateless/nifi-stateless-assembly/README.md#_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nbin/nifi.sh stateless -c -p \"Kafka Parameter Context:Kafka Topic=Sensor Data\" /var/lib/nifi/stateless/config/stateless.properties /var/lib/nifi/stateless/flows/jms-to-kafka.properties\n```\n\n----------------------------------------\n\nTITLE: Running NiFi with LDAP Authentication\nDESCRIPTION: Docker command to run NiFi with LDAP Authentication, showing configuration for connecting to an LDAP server using SIMPLE authentication. Includes volume mounting for certificates and required environment variables.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-docker/dockerhub/README.md#_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --name nifi \\\n  -v /User/dreynolds/certs/localhost:/opt/certs \\\n  -p 8443:8443 \\\n  -e AUTH=ldap \\\n  -e KEYSTORE_PATH=/opt/certs/keystore.jks \\\n  -e KEYSTORE_TYPE=JKS \\\n  -e KEYSTORE_PASSWORD=QKZv1hSWAFQYZ+WU1jjF5ank+l4igeOfQRp+OSbkkrs \\\n  -e TRUSTSTORE_PATH=/opt/certs/truststore.jks \\\n  -e TRUSTSTORE_PASSWORD=rHkWR1gDNW3R9hgbeRsT3OM3Ue0zwGtQqcFKJD2EXWE \\\n  -e TRUSTSTORE_TYPE=JKS \\\n  -e INITIAL_ADMIN_IDENTITY='cn=admin,dc=example,dc=org' \\\n  -e LDAP_AUTHENTICATION_STRATEGY='SIMPLE' \\\n  -e LDAP_MANAGER_DN='cn=admin,dc=example,dc=org' \\\n  -e LDAP_MANAGER_PASSWORD='password' \\\n  -e LDAP_USER_SEARCH_BASE='dc=example,dc=org' \\\n  -e LDAP_USER_SEARCH_FILTER='cn={0}' \\\n  -e LDAP_IDENTITY_STRATEGY='USE_DN' \\\n  -e LDAP_URL='ldap://ldap:389' \\\n  -d \\\n  apache/nifi:latest\n```\n\nLANGUAGE: bash\nCODE:\n```\n-e LDAP_TLS_KEYSTORE: ''\n-e LDAP_TLS_KEYSTORE_PASSWORD: ''\n-e LDAP_TLS_KEYSTORE_TYPE: ''\n-e LDAP_TLS_TRUSTSTORE: ''\n-e LDAP_TLS_TRUSTSTORE_PASSWORD: ''\n-e LDAP_TLS_TRUSTSTORE_TYPE: ''\n```\n\n----------------------------------------\n\nTITLE: JSON Output Example 2 with Force Types = true\nDESCRIPTION: This JSON represents the output of the `ValidateRecord` processor when `Force Types From Reader's Schema = true` in Example 2. The output is routed to the valid relationship as the field1 value is coerced into an array.  Shows the processor's ability to transform data to comply with the specified schema.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ValidateRecord/additionalDetails.md#_snippet_6\n\nLANGUAGE: JSON\nCODE:\n```\n[\n  {\n    \"field1\": [\n      \"content_1\"\n    ],\n    \"field2\": [\n      \"content_2\",\n      \"content_3\"\n    ]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Defining Month Grok Pattern\nDESCRIPTION: Defines a Grok pattern named MONTH for matching month names. The pattern matches both full month names (e.g., January) and their abbreviations (e.g., Jan).  The pattern is case-insensitive and uses word boundary anchors to ensure accurate matching.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/default-grok-patterns.txt#_snippet_6\n\nLANGUAGE: Grok\nCODE:\n```\nMONTH \\b(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\\b\n```\n\n----------------------------------------\n\nTITLE: Transformed NiFi Metrics JSON Output after Jolt Shift Operation\nDESCRIPTION: This JSON snippet demonstrates the expected output after applying the described Jolt transformation to the sample input. The metrics array now includes 'time' and 'value' keys extracted from the original timestamped metrics map, aligning with the Ambari Metrics Collector format. This standardized output enables compatibility with Ambari monitoring tools.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-site-to-site-reporting-bundle/nifi-site-to-site-reporting-task/src/main/resources/docs/org.apache.nifi.reporting.SiteToSiteMetricsReportingTask/additionalDetails.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"metrics\": [\n    {\n      \"metricname\": \"jvm.gc.time.G1OldGeneration\",\n      \"appid\": \"nifi\",\n      \"instanceid\": \"8927f4c0-0160-1000-597a-ea764ccd81a7\",\n      \"hostname\": \"localhost\",\n      \"timestamp\": \"1520456854361\",\n      \"starttime\": \"1520456854361\",\n      \"metrics\": {\n        \"time\": \"1520456854361\",\n        \"value\": \"0\"\n      }\n    },\n    {\n      \"metricname\": \"jvm.thread_states.terminated\",\n      \"appid\": \"nifi\",\n      \"instanceid\": \"8927f4c0-0160-1000-597a-ea764ccd81a7\",\n      \"hostname\": \"localhost\",\n      \"timestamp\": \"1520456854361\",\n      \"starttime\": \"1520456854361\",\n      \"metrics\": {\n        \"time\": \"1520456854361\",\n        \"value\": \"0\"\n      }\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Running NiFi Stateless with Multiple Command Line Parameters\nDESCRIPTION: Example of running NiFi Stateless with multiple parameters passed via command line using multiple -p flags. Each parameter specifies both the Parameter Context and the parameter name and value.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-stateless/nifi-stateless-assembly/README.md#_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nbin/nifi.sh stateless -c -p \"Kafka Parameter Context:Kafka Brokers=kafka-01:9092,kafka-02:9092,kafka-03:9092\" -p \"Kafka Parameter Context:Kafka Topic=Sensor Data\" /var/lib/nifi/stateless/config /stateless.properties\n```\n\n----------------------------------------\n\nTITLE: Querying Bulletins by Time Range SQL\nDESCRIPTION: This SQL query selects all columns from the BULLETINS table within a specific time window. It filters for records where bulletinTimestamp is greater than $bulletinStartTime and less than or equal to $bulletinEndTime. Requires replacing the variables with actual timestamp values. Useful for historical analysis of system events and messages.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-sql-reporting-bundle/nifi-sql-reporting-tasks/src/main/resources/docs/org.apache.nifi.reporting.sql.QueryNiFiReportingTask/additionalDetails.md#_snippet_3\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT * from BULLETINS WHERE bulletinTimestamp > $bulletinStartTime AND bulletinTimestamp <= $bulletinEndTime\n```\n\n----------------------------------------\n\nTITLE: Creating a Child FlowFile from a Parent in Groovy for NiFi ExecuteScript\nDESCRIPTION: Creates a new FlowFile derived from an existing parent FlowFile using `session.create(parentFlowFile)`. First, an incoming FlowFile is retrieved using `session.get()`. The new child FlowFile inherits all attributes (except UUID) from the parent. This operation automatically generates provenance FORK or JOIN events. Depends on the implicit `session` object.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-scripting-bundle/nifi-scripting-processors/src/main/resources/docs/org.apache.nifi.processors.script.ExecuteScript/additionalDetails.md#_snippet_3\n\nLANGUAGE: groovy\nCODE:\n```\nflowFile = session.get()\nif (!flowFile) return\nnewFlowFile = session.create(flowFile)\n// Additional processing here\n```\n\n----------------------------------------\n\nTITLE: Defining Networking Grok Patterns\nDESCRIPTION: Defines Grok patterns for matching MAC addresses, IPv6 addresses, and IPv4 addresses. It also defines patterns for hostname, IP address, and IP or hostname, for identifying network information in log data. Different MAC address formats are supported (Cisco, Windows, Common).\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/default-grok-patterns.txt#_snippet_8\n\nLANGUAGE: Grok\nCODE:\n```\nMAC (?:%{CISCOMAC}|%{WINDOWSMAC}|%{COMMONMAC})\nCISCOMAC (?:(?:[A-Fa-f0-9]{4}\\.){2}[A-Fa-f0-9]{4})\nWINDOWSMAC (?:(?:[A-Fa-f0-9]{2}-){5}[A-Fa-f0-9]{2})\nCOMMONMAC (?:(?:[A-Fa-f0-9]{2}:){5}[A-Fa-f0-9]{2})\nIPV6 ((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:)))(%.+)?\nIPV4 (?<![0-9])(?:(?:25[0-5]|2[0-4][0-9]|[0-1]?[0-9]{1,2})[.](?:25[0-5]|2[0-4][0-9]|[0-1]?[0-9]{1,2})[.](?:25[0-5]|2[0-4][0-9]|[0-1]?[0-9]{1,2})[.](?:25[0-5]|2[0-4][0-9]|[0-1]?[0-9]{1,2}))(?![0-9])\nIP (?:%{IPV6}|%{IPV4})\nHOSTNAME \\b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\\.(?:[0-9A-Za-z][0-9A-Za-z-]{0,62}))*(\\.?|\\b)\nHOST %{HOSTNAME}\nIPORHOST (?:%{HOSTNAME}|%{IP})\nHOSTPORT %{IPORHOST}:%{POSINT}\n```\n\n----------------------------------------\n\nTITLE: Configuring Database Connectivity for NiFi Registry\nDESCRIPTION: This snippet lists environment variables for configuring database connection properties such as URL, driver class, credentials, and SQL debug options, which are set at container startup to customize database integration.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-core/nifi-registry-docker/dockerhub/README.md#_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# Set environment variables for database configuration:\n# - NIFI_REGISTRY_DB_URL: JDBC database URL\n# - NIFI_REGISTRY_DB_CLASS: JDBC driver class\n# - NIFI_REGISTRY_DB_DIR: Driver directory\n# - NIFI_REGISTRY_DB_USER: Database username\n# - NIFI_REGISTRY_DB_PASS: Database password\n# - NIFI_REGISTRY_DB_MAX_CONNS: Max connections\n# - NIFI_REGISTRY_DB_DEBUG_SQL: Enable SQL debugging\n\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Stateless with Local JSON Flow File in Properties File\nDESCRIPTION: A configuration example for NiFi Stateless using a locally stored JSON flow definition file instead of importing from registry. It includes parameter context and reporting task configuration identical to the registry example.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-stateless/nifi-stateless-assembly/README.md#_snippet_7\n\nLANGUAGE: properties\nCODE:\n```\nnifi.stateless.flow.snapshot.file=/var/lib/nifi/stateless-flows/kafka-to-hdfs.json\n\nnifi.stateless.parameters.kafkahdfs=Kafka to HDFS\nnifi.stateless.parameters.kafkahdfs.Kafka Topic=Sensor Data\nnifi.stateless.parameters.kafkahdfs.Kafka Brokers=kafka-01:9092,kafka-02:9092,kafka-03:9093\nnifi.stateless.parameters.kafkahdfs.HDFS Directory=/data/sensors\n\nnifi.stateless.reporting.task.stats.name=Stats\nnifi.stateless.reporting.task.stats.type=ControllerStatusReportingTask\nnifi.stateless.reporting.task.stats.bundle=org.apache.nifi:nifi-standard-nar:1.12.1\nnifi.stateless.reporting.task.stats.properties.Show Deltas=false\nnifi.stateless.reporting.task.stats.frequency=1 minute\nnifi.stateless.reporting.task.stats.properties.Reporting Granularity=One Second\n```\n\n----------------------------------------\n\nTITLE: Running Stateless NiFi from Command Line\nDESCRIPTION: This command demonstrates how to run Stateless NiFi from the command line using the `bin/nifi.sh` script. It specifies the engine configuration file and the dataflow configuration file. The `-c` option indicates that the flow should be continually triggered.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-stateless/nifi-stateless-assembly/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nbin/nifi.sh stateless -c -e /var/lib/nifi/stateless/config/stateless.properties -f /var/lib/nifi/stateless/flows/jms-to-kafka.properties\n```\n\n----------------------------------------\n\nTITLE: XML Output Wrapping Arrays Using Custom Wrapper with Field Name as Elements - XML\nDESCRIPTION: Demonstrates XML output wrapping arrays inside a custom wrapper node named 'wrap', with each array element tagged using the array field name. This configuration allows flexible naming of the wrapper and element nodes for arrays.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLRecordSetWriter/additionalDetails.md#_snippet_8\n\nLANGUAGE: xml\nCODE:\n```\n<root_name>\n    <record_name>\n        <name1>value1</name1>\n        <wrap>\n            <array_field>1</array_field>\n            <array_field>2</array_field>\n            <array_field>3</array_field>\n        </wrap>\n    </record_name>\n</root_name>\n```\n\n----------------------------------------\n\nTITLE: Regular Expressions for Dates, Times, and ISO8601 Timestamps (Grok Pattern)\nDESCRIPTION: Defines patterns for parsing various date and time formats such as ISO8601, RFC822, US/EU style dates, and log-specific timestamps. The patterns handle edge cases including leap seconds, optional time zone designations, variable delimiters, and support dynamic extraction for analytics workflows. These patterns serve as modular components for more complex timestamp-oriented Grok expressions.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/default-grok-patterns.txt#_snippet_2\n\nLANGUAGE: Grok pattern\nCODE:\n```\nYEAR (?>\\d\\d){1,2}\nHOUR (?:2[0123]|[01]?[0-9])\nMINUTE (?:[0-5][0-9])\nSECOND (?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)\nTIME (?!<[0-9])%{HOUR}:%{MINUTE}(?::%{SECOND})(?![0-9])\nDATE_US_MONTH_DAY_YEAR %{MONTHNUM}[/-]%{MONTHDAY}[/-]%{YEAR}\nDATE_US_YEAR_MONTH_DAY %{YEAR}[/-]%{MONTHNUM}[/-]%{MONTHDAY}\nDATE_US %{DATE_US_MONTH_DAY_YEAR}|%{DATE_US_YEAR_MONTH_DAY}\nDATE_EU %{MONTHDAY}[./-]%{MONTHNUM}[./-]%{YEAR}\nISO8601_TIMEZONE (?:Z|[+\\-]%{HOUR}(?::?%{MINUTE}))\nTIMESTAMP_ISO8601 %{YEAR}-%{MONTHNUM}-%{MONTHDAY}[T ]%{HOUR}:?%{MINUTE}(?::?%{SECOND})?%{ISO8601_TIMEZONE}?\n```\n\n----------------------------------------\n\nTITLE: Output of Grep Command on Log Input\nDESCRIPTION: Shows the resulting FlowFile content on the 'output stream' relationship after executing `grep POST` via ExecuteStreamCommand on the example Apache log input. This demonstrates piping FlowFile content as standard input (STDIN) to an external command and capturing its standard output (STDOUT). The processor was configured with Command Path 'grep', Command Arguments 'POST', and 'Ignore STDIN' set to false.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ExecuteStreamCommand/additionalDetails.md#_snippet_3\n\nLANGUAGE: log\nCODE:\n```\n127.0.0.1 - - [03/May/2023:14:05:32 +0000] \"POST /submit-form HTTP/1.1\" 302 0 \"http://localhost/example-page\" \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0\"\n```\n\n----------------------------------------\n\nTITLE: Defining Network, IP, MAC, and Host Patterns (Grok Pattern)\nDESCRIPTION: This snippet defines Grok patterns for extracting network-related information such as MAC addresses (Cisco, Windows, common), IPv4/IPv6 addresses, IP or host, and host/port fields from logs. These expressions are composed modularly for reliable extraction in logs from network equipment, webservers, and operating systems. The patterns support subnet and port detection and are used to populate network metadata fields for logs.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/default-grok-patterns.txt#_snippet_3\n\nLANGUAGE: Grok pattern\nCODE:\n```\nCISCOMAC (?:(?:[A-Fa-f0-9]{4}\\.){2}[A-Fa-f0-9]{4})\nWINDOWSMAC (?:(?:[A-Fa-f0-9]{2}-){5}[A-Fa-f0-9]{2})\nCOMMONMAC (?:(?:[A-Fa-f0-9]{2}:){5}[A-Fa-f0-9]{2})\nMAC (?:%{CISCOMAC}|%{WINDOWSMAC}|%{COMMONMAC})\nIPV6 ((((?:[0-9A-Fa-f]{1,4}:){7}(?:[0-9A-Fa-f]{1,4}|:))|((?:[0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|((?:[0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|((?:[0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|((?:[0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|((?:[0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|((?:[0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:)))(%.+)?\nIPV4 (?<![0-9])(?:(?:25[0-5]|2[0-4][0-9]|[0-1]?[0-9]{1,2})[.](?:25[0-5]|2[0-4][0-9]|[0-1]?[0-9]{1,2})[.](?:25[0-5]|2[0-4][0-9]|[0-1]?[0-9]{1,2})[.](?:25[0-5]|2[0-4][0-9]|[0-1]?[0-9]{1,2}))(?![0-9])\nIP (?:%{IPV6}|%{IPV4})\nHOSTNAME \\b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\\.(?:[0-9A-Za-z][0-9A-Za-z-]{0,62}))*(\\.?|\\b)\nIPORHOST (?:%{HOSTNAME}|%{IP})\nHOSTPORT %{IPORHOST}:%{POSINT}\n```\n\n----------------------------------------\n\nTITLE: Secured NiFi Registry with LDAP Authentication\nDESCRIPTION: This setup runs the container with LDAP authentication, requiring bind volume for certificates, environment variables for LDAP server connection details, admin identity, and optional TLS configuration for LDAPS or START_TLS.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-core/nifi-registry-docker/dockerhub/README.md#_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --name nifi-registry \\\n  -v /path/to/tls/certs/localhost:/opt/certs \\\n  -p 18443:18443 \\\n  -e AUTH=ldap \\\n  -e KEYSTORE_PATH=/opt/certs/keystore.jks \\\n  -e KEYSTORE_TYPE=JKS \\\n  -e KEYSTORE_PASSWORD=QKZv1hSWAFQYZ+WU1jjF5ank+l4igeOfQRp+OSbkkrs \\\n  -e TRUSTSTORE_PATH=/opt/certs/truststore.jks \\\n  -e TRUSTSTORE_PASSWORD=rHkWR1gDNW3R9hgbeRsT3OM3Ue0zwGtQqcFKJD2EXWE \\\n  -e TRUSTSTORE_TYPE=JKS \\\n  -e INITIAL_ADMIN_IDENTITY='cn=nifi-admin,dc=example,dc=org' \\\n  -e LDAP_AUTHENTICATION_STRATEGY='SIMPLE' \\\n  -e LDAP_MANAGER_DN='cn=ldap-admin,dc=example,dc=org' \\\n  -e LDAP_MANAGER_PASSWORD='password' \\\n  -e LDAP_USER_SEARCH_BASE='dc=example,dc=org' \\\n  -e LDAP_USER_SEARCH_FILTER='cn={0}' \\\n  -e LDAP_IDENTITY_STRATEGY='USE_DN' \\\n  -e LDAP_URL='ldap://ldap:389' \\\n  -d \\\n  apache/nifi-registry:latest\n```\n\n----------------------------------------\n\nTITLE: Starting Apache NiFi Registry (Linux/macOS) - Shell\nDESCRIPTION: Executes the startup script for Apache NiFi Registry on Unix-like operating systems (Linux, macOS). This command initiates the Registry service in the background, allowing users to access the web UI.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-assembly/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nbin/nifi-registry.sh start\n```\n\n----------------------------------------\n\nTITLE: Creating a Secret in HashiCorp Vault (Shell)\nDESCRIPTION: Demonstrates using the `vault kv put` command to store key/value pairs as a secret in HashiCorp Vault's Key/Value (unversioned) Secrets Engine, suitable for use with the `HashiCorpVaultParameterValueProvider` in NiFi Stateless. The example creates a secret named `Context` within the path defined by `vault.kv.path` (e.g., `nifi-kv`), containing parameters `param` and `param2`.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-stateless/nifi-stateless-assembly/README.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nvault kv put \"nifi-kv/Context\" param=value param2=value2\n```\n\n----------------------------------------\n\nTITLE: XQuery: Return all fruit names individually\nDESCRIPTION: This XQuery expression selects the text content of the 'name' element within each 'fruit' node. It returns each 'fruit' name as a separate result.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.EvaluateXQuery/additionalDetails.md#_snippet_5\n\nLANGUAGE: xquery\nCODE:\n```\n//fruit/text()\n```\n\n----------------------------------------\n\nTITLE: Defining Syslog Host Grok Pattern\nDESCRIPTION: Defines the SYSLOGHOST pattern that matches the hostname or IP address of the syslog sender. It relies on the IPORHOST pattern which can match either a hostname or an IP address. The matched hostname or IP is captured into the logsource field.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/default-grok-patterns.txt#_snippet_3\n\nLANGUAGE: Grok\nCODE:\n```\nSYSLOGHOST %{IPORHOST}\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom MIME Detection in NiFi - XML\nDESCRIPTION: This XML snippet provides an example configuration value for the \"Custom MIME Configuration\" property of the NiFi IdentifyMimeType processor. It demonstrates how to define a custom MIME type ('custom/abcd') that is detected if the file content starts with the string 'abcd' (offset 0) or if the file extension is '.abcd', following the Apache Tika 'mime-info' structure.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.IdentifyMimeType/additionalDetails.md#_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<mime-info>\n    <mime-type type=\"custom/abcd\">\n        <magic priority=\"50\">\n            <match value=\"abcd\" type=\"string\" offset=\"0\"/>\n        </magic>\n        <glob pattern=\"\\*.abcd\"/>\n    </mime-type>\n</mime-info>\n```\n\n----------------------------------------\n\nTITLE: Starting Apache NiFi Registry (Windows) - Batch\nDESCRIPTION: Executes the batch script to start Apache NiFi Registry on Windows systems. This command launches the Registry service, making it accessible for configuration and use.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-assembly/README.md#_snippet_1\n\nLANGUAGE: Batch\nCODE:\n```\nbin/run-nifi-registry.bat\n```\n\n----------------------------------------\n\nTITLE: JSON Representation After 'Any' Type Processing\nDESCRIPTION: Shows the final JSON output structure of a NiFi Record *after* the ProtobufReader has successfully processed the embedded message within an 'Any' field. The original 'Any' field structure ('type_url', 'value') is replaced by the actual fields and values ('field_1', 'field_2', 'field_3') of the deserialized embedded message ('NestedMessage' in this example).\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-protobuf-bundle/nifi-protobuf-services/src/main/resources/docs/org.apache.nifi.services.protobuf.ProtobufReader/additionalDetails.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  anyField: {\n    field_1: \"value 1\",\n    field_2: \"value 2\",\n    field_3: \"value 3\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Multi-nested Input Example for ForkRecord - JSON\nDESCRIPTION: This snippet illustrates a JSON record with multi-level nesting, containing users, their accounts, and account transactions. This structure demonstrates the application of a more complex RecordPath such as '/accounts[*]/transactions' for advanced record extraction. No dependencies are needed for the example itself, but corresponding Reader and Writer schema definitions should match this layout for ForkRecord. Input is a single array with nested arrays under 'accounts' and 'transactions'.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ForkRecord/additionalDetails.md#_snippet_3\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"id\": 1,\n    \"name\": \"John Doe\",\n    \"address\": \"123 My Street\",\n    \"city\": \"My City\",\n    \"state\": \"MS\",\n    \"zipCode\": \"11111\",\n    \"country\": \"USA\",\n    \"accounts\": [\n      {\n        \"id\": 42,\n        \"balance\": 4750.89,\n        \"transactions\": [\n          {\n            \"id\": 5,\n            \"amount\": 150.31\n          },\n          {\n            \"id\": 6,\n            \"amount\": -15.31\n          }\n        ]\n      },\n      {\n        \"id\": 43,\n        \"balance\": 48212.38,\n        \"transactions\": [\n          {\n            \"id\": 7,\n            \"amount\": 36.78\n          },\n          {\n            \"id\": 8,\n            \"amount\": -21.34\n          }\n        ]\n      }\n    ]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Running Flow Persistence Provider Migrator\nDESCRIPTION: This command runs the persistence toolkit shell script to migrate flow persistence providers. The `-t` parameter specifies the target providers configuration file.  Ensure the registry is shut down and a backup is created before running this command.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-toolkit/README.md#_snippet_1\n\nLANGUAGE: Shell\nCODE:\n```\npersistence-toolkit.sh -t providers-to.xml\n```\n\n----------------------------------------\n\nTITLE: XQuery: Return fruit colors and names as newline separated list\nDESCRIPTION: This XQuery expression extracts the color and name of each fruit, joins them with a space, and then joins all the color-name pairs with a newline character. It returns a single string with all color-name pairs separated by newlines.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.EvaluateXQuery/additionalDetails.md#_snippet_10\n\nLANGUAGE: xquery\nCODE:\n```\nstring-join((for $y in (for $x in //fruit return string-join(($x/color/text() , $x/name/text()), ' ')) return\n      $y), '\\n')\n```\n\n----------------------------------------\n\nTITLE: Displaying Maven Version with Custom JAVA_HOME\nDESCRIPTION: Illustrates the output of `mvn --version` when executed via ExecuteStreamCommand with the `JAVA_HOME` environment variable set using a Dynamic Property (e.g., Name: JAVA_HOME, Value: path/to/another/java/home). This demonstrates how dynamic properties not matching the 'command.argument.*' pattern are passed as environment variables, overriding system defaults for the executed command.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ExecuteStreamCommand/additionalDetails.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nApache Maven 3.8.6 (84538c9988a25aec085021c365c560670ad80f63) \nMaven home: /path/to/maven/home Java version: 11.0.18, vendor: Eclipse Adoptium, runtime: /path/to/another/java/home \nDefault locale: en_US, platform encoding: UTF-8 OS name: \"mac os x\", version: \"13.1\", arch: \"x86_64\", family: \"mac\"\n```\n\n----------------------------------------\n\nTITLE: Running NiFi Stateless with Parameters Without Context Names\nDESCRIPTION: Shows how to pass parameters to NiFi Stateless without specifying the Parameter Context name, which will apply the values to any Parameter Context containing those parameter names.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-stateless/nifi-stateless-assembly/README.md#_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nbin/nifi.sh stateless -c -p \"Kafka Brokers=kafka-01:9092,kafka-02:9092,kafka-03:9092\" -p \"Kafka Topic=Sensor Data\" /var/lib/nifi/stateless/config /stateless.properties\n```\n\n----------------------------------------\n\nTITLE: Transforming NiFi Flow JSON to MiNiFi Flow JSON (Shell)\nDESCRIPTION: This shell command executes the MiNiFi Toolkit Converter to transform a NiFi flow JSON file (NiFi 2 format) into a compatible MiNiFi flow JSON file. Users must provide the input NiFi flow JSON file and specify the desired output file location. The command assumes the MiNiFi Toolkit binary is already installed and the user is running from its directory. Outputs a MiNiFi-specific JSON configuration file suitable for deployment. Ensure that Java (JRE 21) and all toolkit dependencies are available.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-toolkit/minifi-toolkit-assembly/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n./config.sh transform-nifi <downloaded_nifi2_flow_json_file> <minifi_flow_json_file>\n```\n\n----------------------------------------\n\nTITLE: Selecting Entire Record Based on Nested Maintainer Name Condition - SQL\nDESCRIPTION: This SQL snippet demonstrates returning full records for persons whose associated 'project' contains a 'maintainer' named 'Apache Software Foundation', utilizing the RPATH() function with a Record Path criteria. The query is suited for NiFiâ€™s QueryRecord processor, operates over the 'project' field in nested data, and returns all top-level fields in 'FLOWFILE'. Required inputs include records with nested project.maintainer.name fields; output is subject to NiFiâ€™s handling of complex types.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.QueryRecord/additionalDetails.md#_snippet_5\n\nLANGUAGE: sql\nCODE:\n```\nSELECT *\nFROM FLOWFILE\nWHERE RPATH(project, '/maintainer/name') = 'Apache Software Foundation'\n```\n\n----------------------------------------\n\nTITLE: Displaying Default Maven Version Output\nDESCRIPTION: Shows the standard output of the `mvn --version` command executed via ExecuteStreamCommand using the system's default Java runtime. This output serves as a baseline before demonstrating environment variable configuration using dynamic properties.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ExecuteStreamCommand/additionalDetails.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nApache Maven 3.8.6 (84538c9988a25aec085021c365c560670ad80f63) \nMaven home: /path/to/maven/home Java version: 11.0.18, vendor: Eclipse Adoptium, runtime: /path/to/default/java/home \nDefault locale: en_US, platform encoding: UTF-8 OS name: \"mac os x\", version: \"13.1\", arch: \"x86_64\", family: \"mac\"\n```\n\n----------------------------------------\n\nTITLE: Setting Custom NiFi User Credentials - NiFi Shell Script - Shell\nDESCRIPTION: Updates the NiFi instance to use a specific username and password for authentication by executing the NiFi shell script with the set-single-user-credentials command and desired credentials as parameters. Requires that NiFi is stopped before running, and the user has permissions to update the configuration. Replace <username> and <password> with the intended values.\nSOURCE: https://github.com/apache/nifi/blob/main/README.md#_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\n./bin/nifi.sh set-single-user-credentials <username> <password>\n```\n\n----------------------------------------\n\nTITLE: Importing PKCS12 Keystore to JKS - Shell/Keytool\nDESCRIPTION: Imports a key and certificate entry from an existing PKCS12 keystore (`CN=user1_OU=nifi.p12`) into a new JKS keystore (`keys/user1-ks.jks`). This command facilitates converting a key/certificate pair between different keystore formats. Requires the `keytool` utility.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-core/nifi-registry-web-api/src/test/resources/keys/README.md#_snippet_7\n\nLANGUAGE: Shell\nCODE:\n```\nkeytool -importkeystore \\\n      -srckeystore CN=user1_OU=nifi.p12 -srcstoretype PKCS12 -srcstorepass password -srcalias nifi-key \\\n      -destkeystore keys/user1-ks.jks -deststoretype JKS -deststorepass password -destkeypass password -destalias user1-key\n```\n\n----------------------------------------\n\nTITLE: Configuring Reporting Tasks with Properties\nDESCRIPTION: This demonstrates configuring a reporting task named 'Stats' using properties. It sets the reporting task type to `ControllerStatusReportingTask`, specifies its bundle, and configures reporting properties such as `Show Deltas` and `Reporting Granularity`. Finally, it sets the frequency of execution to every 30 seconds. This configuration allows NiFi to log stats to the log file every 30 seconds.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-stateless/nifi-stateless-assembly/README.md#_snippet_5\n\nLANGUAGE: properties\nCODE:\n```\nnifi.stateless.reporting.task.stats.name=Stats\nnifi.stateless.reporting.task.stats.type=ControllerStatusReportingTask\nnifi.stateless.reporting.task.stats.bundle=org.apache.nifi:nifi-standard-nar:1.12.1\nnifi.stateless.reporting.task.stats.properties.Show Deltas=false\nnifi.stateless.reporting.task.stats.properties.Reporting Granularity=One Second  # Log 1-second metrics instead of 5-minute metrics\nnifi.stateless.reporting.task.stats.frequency=30 sec\n```\n\n----------------------------------------\n\nTITLE: Configuring LookupRecord with Use Property Strategy in Apache NiFi (JSON)\nDESCRIPTION: Demonstrates input and output JSON data when using the LookupRecord processor with the 'Use Property' update strategy. The input contains records with a null 'country' field and a 'code' field; the processor uses a user-defined property to map 'code' to 'country' via a Lookup Service. Requires Apache NiFi with the LookupRecord processor and a configured Simple Key Value Lookup Service. Input: JSON array of objects with 'country' and 'code'; output: updated 'country' fields if keys are found in the Lookup Service.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.LookupRecord/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"country\": null,\n    \"code\": \"FR\"\n  },\n  {\n    \"country\": null,\n    \"code\": \"CA\"\n  },\n  {\n    \"country\": null,\n    \"code\": \"JP\"\n  }\n]\n```\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"country\": \"France\",\n    \"code\": \"FR\"\n  },\n  {\n    \"country\": \"Canada\",\n    \"code\": \"CA\"\n  },\n  {\n    \"country\": null,\n    \"code\": \"JP\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Extracting Certificate to PEM - Shell/OpenSSL\nDESCRIPTION: Extracts the certificate component from a PKCS12 file (`keys/user1-ks.p12`) and saves it in PEM format (`keys/user1-cert.pem`). The `-nokeys` option ensures only the certificate is extracted. This is necessary for applications expecting the certificate separately from the key. Requires the `openssl` utility and the input password.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-core/nifi-registry-web-api/src/test/resources/keys/README.md#_snippet_10\n\nLANGUAGE: Shell\nCODE:\n```\nopenssl pkcs12 -in keys/user1-ks.p12 -passin pass:password -out keys/user1-cert.pem -nokeys\n```\n\n----------------------------------------\n\nTITLE: Creating working directory for key generation - Shell\nDESCRIPTION: This snippet creates a temporary working directory for generating test keys and certificates. It uses the current date and time to make the directory name unique, then navigates into it.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-core/nifi-registry-web-api/src/test/resources/keys/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\n# make working directory\nWD=\"/tmp/test-keys-$(date +\"%Y%m%d-%H%M%S\")\"\nmkdir \"$WD\"\ncd \"$WD\"\n```\n\n----------------------------------------\n\nTITLE: Example Apache HTTPD Log Input for Grep\nDESCRIPTION: Provides sample content for an incoming FlowFile, representing Apache HTTPD access logs. This data is intended to be streamed as standard input (STDIN) to an external command like `grep` using the ExecuteStreamCommand processor when the 'Ignore STDIN' property is set to false.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ExecuteStreamCommand/additionalDetails.md#_snippet_2\n\nLANGUAGE: log\nCODE:\n```\n127.0.0.1 - - [03/May/2023:13:54:26 +0000] \"GET /example-page HTTP/1.1\" 200 4825 \"-\" \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0\" \n127.0.0.1 - - [03/May/2023:14:05:32 +0000] \"POST /submit-form HTTP/1.1\" 302 0 \"http://localhost/example-page\" \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0\" \n127.0.0.1 - - [03/May/2023:14:10:48 +0000] \"GET /image.jpg HTTP/1.1\" 200 35785 \"http://localhost/example-page\" \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0\" \n127.0.0.1 - - [03/May/2023:14:20:15 +0000] \"GET /example-page HTTP/1.1\" 200 4825 \"-\" \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0\" \n127.0.0.1 - - [03/May/2023:14:30:42 +0000] \"GET /example-page HTTP/1.1\" 200 4825 \"-\" \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:88.0) Gecko/20100101 Firefox/88.0\"\n```\n\n----------------------------------------\n\nTITLE: Minimal Apache NiFi Stateless Engine Configuration - Properties File - Plaintext\nDESCRIPTION: This snippet shows a minimum viable Engine Configuration for Apache NiFi Stateless. It specifies only the essential properties required for the engine to load NiFi Archives and operate with a working directory. No security settings, repository directories, or extension clients are specified, placing the responsibility on the user to manage all other settings and ensure required NAR files are present in the specified directory. This is suitable for local development or simple deployments with no secured flow requirements.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-stateless/nifi-stateless-assembly/README.md#_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\nnifi.stateless.nar.directory=/var/lib/nifi/lib\nnifi.stateless.working.directory=/var/lib/nifi/work/stateless\n```\n\n----------------------------------------\n\nTITLE: Configuring StatusLogger Logging in logback.xml\nDESCRIPTION: Provides an example `logback.xml` configuration to direct the output of the `StatusLogger` to a dedicated rolling log file (`minifi-status.log`). It defines a new appender and associates the `StatusLogger` class with this appender, allowing status messages to be logged separately from the main bootstrap log.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-docs/src/main/markdown/System_Admin_Guide.md#_snippet_6\n\nLANGUAGE: xml\nCODE:\n```\n<appender name=\"STATUS_LOG_FILE\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\">\n    <file>logs/minifi-status.log</file>\n    <rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\">\n        <!--\n          For daily rollover, use 'user_%d.log'.\n          For hourly rollover, use 'user_%d{yyyy-MM-dd_HH}.log'.\n          To GZIP rolled files, replace '.log' with '.log.gz'.\n          To ZIP rolled files, replace '.log' with '.log.zip'.\n        -->\n        <fileNamePattern>./logs/minifi-status_%d.log</fileNamePattern>\n        <!-- keep 5 log files worth of history -->\n        <maxHistory>5</maxHistory>\n    </rollingPolicy>\n    <encoder class=\"ch.qos.logback.classic.encoder.PatternLayoutEncoder\">\n        <pattern>%date %level [%thread] %logger{40} %msg%n</pattern>\n    </encoder>\n</appender>\n\n<logger name=\"org.apache.nifi.minifi.bootstrap.status.reporters.StatusLogger\" level=\"INFO\" additivity=\"false\">\n    <appender-ref ref=\"STATUS_LOG_FILE\" />\n</logger>\n```\n\n----------------------------------------\n\nTITLE: Displaying Output Directory and Copy Command - Shell\nDESCRIPTION: Prints messages to the console indicating the location where the newly generated key and certificate files have been saved and provides a sample command for copying these files to a specific test directory within the NiFi Registry source code structure. Uses standard shell `echo` commands.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-core/nifi-registry-web-api/src/test/resources/keys/README.md#_snippet_11\n\nLANGUAGE: Shell\nCODE:\n```\necho\necho \"New keys written to ${WD}/keys\"\necho \"Copy to NiFi Registry test keys dir by running: \"\necho \"    cp -f \\\"$WD/keys/*\\\" /path/to/nifi-registry/nifi-registry-core/nifi-registry-web-api/src/test/resources/keys/\"\n```\n\n----------------------------------------\n\nTITLE: Starting MiNiFi on Linux/OS X\nDESCRIPTION: This snippet demonstrates how to start MiNiFi on Linux or macOS systems. It involves changing the directory to the MiNiFi installation location and executing the `minifi.sh` script with the `start` argument.  Ensure the correct path to the MiNiFi installation directory is used.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-assembly/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n$ cd ~/example-minifi-deploy/minifi-*\n$ ./bin/minifi.sh start\n```\n\n----------------------------------------\n\nTITLE: Building Docker image with version override\nDESCRIPTION: This command builds a Docker image for Apache NiFi MiNiFi, overriding the default MiNiFi version using the `MINIFI_VERSION` build argument.  The resulting image is tagged as `apache/nifi-minifi:latest`.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-docker/dockerhub/README.md#_snippet_1\n\nLANGUAGE: Docker\nCODE:\n```\ndocker build --build-arg=MINIFI_VERSION={Desired MiNiFi Version} -t apache/nifi-minifi:latest .\n```\n\n----------------------------------------\n\nTITLE: Parsing Windows EVTX Logs to XML Example Using XML\nDESCRIPTION: This snippet provides an example of the XML output generated by the ParseEvtx processor after successfully parsing Windows EVTX event log files. It illustrates the structure and namespace conventions used in the output, including event system metadata and event-specific data fields like Provider, EventID, TimeCreated, and EventData. This example is used to demonstrate the detailed XML formatting resulting from the binary EVTX parsing process.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-evtx-bundle/nifi-evtx-processors/src/main/resources/docs/org.apache.nifi.processors.evtx.ParseEvtx/additionalDetails.md#_snippet_0\n\nLANGUAGE: XML\nCODE:\n```\n<?xml version=\"1.0\"?>\n<Events>\n    <Event xmlns=\"http://schemas.microsoft.com/win/2004/08/events/event\">\n        <System>\n            <Provider Name=\"Service Control Manager\" Guid=\"{555908d1-a6d7-4695-8e1e-26931d2012f4}\" EventSourceName=\"Service Control Manager\"/>\n            <EventID Qualifiers=\"16384\">7036</EventID>\n            <Version>0</Version>\n            <Level>4</Level>\n            <Task>0</Task>\n            <Opcode>0</Opcode>\n            <Keywords>0x8080000000000000</Keywords>\n            <TimeCreated SystemTime=\"2016-01-08 16:49:47.518\"/>\n            <EventRecordID>780</EventRecordID>\n            <Correlation ActivityID=\"\" RelatedActivityID=\"\"/>\n            <Execution ProcessID=\"480\" ThreadID=\"596\"/>\n            <Channel>System</Channel>\n            <Computer>win7-pro-vm</Computer>\n            <Security UserID=\"\"/>\n        </System>\n        <EventData>\n            <Data Name=\"param1\">Workstation</Data>\n            <Data Name=\"param2\">running</Data>\n            <Binary>TABhAG4AbQBhAG4AVwBvAHIAawBzAHQAYQB0AGkAbwBuAC8ANAAAAA==</Binary>\n        </EventData>\n    </Event>\n    <Event xmlns=\"http://schemas.microsoft.com/win/2004/08/events/event\">\n        <System>\n            <Provider Name=\"Service Control Manager\" Guid=\"{555908d1-a6d7-4695-8e1e-26931d2012f4}\" EventSourceName=\"Service Control Manager\"/>\n            <EventID Qualifiers=\"16384\">7036</EventID>\n            <Version>0</Version>\n            <Level>4</Level>\n            <Task>0</Task>\n            <Opcode>0</Opcode>\n            <Keywords>0x8080000000000000</Keywords>\n            <TimeCreated SystemTime=\"2016-01-08 16:49:47.535\"/>\n            <EventRecordID>781</EventRecordID>\n            <Correlation ActivityID=\"\" RelatedActivityID=\"\"/>\n            <Execution ProcessID=\"480\" ThreadID=\"576\"/>\n            <Channel>System</Channel>\n            <Computer>win7-pro-vm</Computer>\n            <Security UserID=\"\"/>\n        </System>\n        <EventData>\n            <Data Name=\"param1\">Cryptographic Services</Data>\n            <Data Name=\"param2\">running</Data>\n            <Binary>QwByAHkAcAB0AFMAdgBjAC8ANAAAAA==</Binary>\n        </EventData>\n    </Event>\n</Events>\n```\n\n----------------------------------------\n\nTITLE: Viewing MiNiFi logs on Linux/OS X\nDESCRIPTION: This snippet provides the command to view the MiNiFi application logs on Linux or macOS systems. It uses the `tail -F` command to continuously follow the `minifi-app.log` file. The path to the log file may need adjustment based on the MiNiFi installation location.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-assembly/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n$ tail -F ~/example-minifi-deploy/logs/minifi-app.log\n```\n\n----------------------------------------\n\nTITLE: Parsing XML with Attribute and Child Element Name Clash (No Field Name for Content, NiFi XMLReader, XML)\nDESCRIPTION: Shows how schema inference works when an XML element has both attribute and child element named 'value', and 'Field Name for Content' is not set. The attribute is mapped to <attr>, the child node remains, but the original content of the parent is lost due to naming collision. Useful for understanding limitations in NiFi's default schema mapping.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLReader/additionalDetails.md#_snippet_13\n\nLANGUAGE: xml\nCODE:\n```\n<record>\n    <field_with_attribute attr=\"attr_content\">content of field<value>123</value>\n    </field_with_attribute>\n</record>\n```\n\nLANGUAGE: xml\nCODE:\n```\n<record>\n    <field_with_attribute>\n        <attr>attr_content</attr>\n        <value>123</value>\n    </field_with_attribute>\n</record>\n```\n\n----------------------------------------\n\nTITLE: CEF Login Audit Log Entries\nDESCRIPTION: Common Event Format (CEF) log entries representing successful login events. Each entry includes timestamp, source information, event type, severity level, and various extensions like user ID, source port, IP addresses, timestamps, and geographic data.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/test/resources/cef/multiple-rows-increasing-number-of-extensions.txt#_snippet_0\n\nLANGUAGE: log\nCODE:\n```\nOct 12 04:16:11 localhost CEF:0|Company|Product|1.2.3|audit-login|Successful login|3|cn1Label=userid spt=46117 cn1=99999 cfp1=1.23 dst=127.0.0.1 start=1479152665000 end=Jan 12 2017 12:23:45 dlat=456.789\nOct 12 04:16:11 localhost CEF:0|Company|Product|1.2.3|audit-login|Successful login|3|cn1Label=userid spt=46117 cn1=99999 cfp1=1.23 dst=127.0.0.1 c6a1=2345:0425:2CA1:0000:0000:0567:5673:23b5 dmac=00:0D:60:AF:1B:61 start=1479152665000 end=Jan 12 2017 12:23:45 dlat=456.789\n```\n\n----------------------------------------\n\nTITLE: Sending Config via REST POST Request\nDESCRIPTION: This snippet illustrates how to send a new configuration to the RestChangeIngestor via a POST request using curl. The body of the request contains the new configuration file (flow.json.raw).\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-docs/src/main/markdown/System_Admin_Guide.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl --request POST --data-binary \"@flow.json.raw\" http://localhost:8338/\n```\n\n----------------------------------------\n\nTITLE: Building Minifi Minifi-Assembly Module with Maven\nDESCRIPTION: This shell command compiles and installs the minifi-minifi-assembly module, skipping tests, and enabling parallel execution with 1 CPU core. It prepares the assembly for integration testing.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-integration-tests/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nmvn -pl minifi/minifi-assembly -am install -T1C -DskipTests\n```\n\n----------------------------------------\n\nTITLE: Starting MiNiFi on Windows\nDESCRIPTION: This snippet illustrates how to start MiNiFi on Windows systems. It involves executing the `run-minifi.bat` batch file located in the `bin` directory within the MiNiFi installation directory. The user should navigate to the MiNiFi installation directory via the command line prior to running the bat file.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-assembly/README.md#_snippet_1\n\nLANGUAGE: batch\nCODE:\n```\nexecute bin/run-minifi.bat\n```\n\n----------------------------------------\n\nTITLE: Verifying Keystore Contents - Shell/Keytool\nDESCRIPTION: Lists the detailed, verbose contents of a specified JKS keystore (`$WD/keys/registry-ks.jks`). This command is used to verify that keys and certificates were correctly imported into the keystore. Requires the keystore password and the `keytool` utility.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-core/nifi-registry-web-api/src/test/resources/keys/README.md#_snippet_12\n\nLANGUAGE: Shell\nCODE:\n```\nkeytool -list -v -keystore \"$WD/keys/registry-ks.jks\" -storepass password\n```\n\n----------------------------------------\n\nTITLE: Building with Static Analysis and License Compliance - Apache Maven Wrapper - Shell\nDESCRIPTION: Builds all project modules in parallel and performs code analysis for compliance with project contribution and licensing standards using the contrib-check Maven profile. Requires an internet connection and correct Java version. Reports compliance issues during build.\nSOURCE: https://github.com/apache/nifi/blob/main/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n./mvnw install -T1C -P contrib-check\n```\n\n----------------------------------------\n\nTITLE: Writing New Content to FlowFile using OutputStreamCallback in Groovy\nDESCRIPTION: Illustrates how to generate and write new content to a NiFi FlowFile using the `session.write()` method with an `OutputStreamCallback`. This example writes the string 'Hello world!' to the FlowFile, encoding it as UTF-8 bytes. Requires `org.apache.commons.io.IOUtils` and `java.nio.charset.StandardCharsets` imports (though IOUtils is not strictly used in this specific snippet but often used in conjunction).\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-scripting-bundle/nifi-scripting-processors/src/main/resources/docs/org.apache.nifi.processors.script.ExecuteScript/additionalDetails.md#_snippet_11\n\nLANGUAGE: groovy\nCODE:\n```\nimport org.apache.commons.io.IOUtils\nimport java.nio.charset.StandardCharsets\n\nflowFile = session.get()\nif (!flowFile) return\ndef text = 'Hello world!'\n// Cast a closure with an outputStream parameter to OutputStreamCallback\nflowFile = session.write(flowFile, { outputStream ->\n    outputStream.write(text.getBytes(StandardCharsets.UTF_8))\n} as OutputStreamCallback)\n```\n\n----------------------------------------\n\nTITLE: Defining Patterns for Syslog Timestamps and Program Extraction (Grok Pattern)\nDESCRIPTION: Creates the SYSLOGTIMESTAMP and SYSLOGPROG patterns for extracting timestamp and program information from Syslog-style logs. These patterns rely on previously defined patterns (MONTH, MONTHDAY, TIME, PROG, POSINT) to ensure modularity and reusability. The patterns are intended for use with Syslog log entries, capturing structured time and program fields.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/default-grok-patterns.txt#_snippet_1\n\nLANGUAGE: Grok pattern\nCODE:\n```\nSYSLOGTIMESTAMP %{MONTH} +%{MONTHDAY} %{TIME}\nPROG (?:[\\w._/%-]+)\nSYSLOGPROG %{PROG:program}(?:\\[%{POSINT:pid}\\])?\n```\n\n----------------------------------------\n\nTITLE: Defining URI Grok Pattern\nDESCRIPTION: Defines a Grok pattern named URI for matching URIs. It combines patterns for URI protocol, host, path, and parameters to match a complete URI string. It captures the protocol, host, path and parameters. It depends on URIPROTO, URIHOST, URIPATHPARAM patterns.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/default-grok-patterns.txt#_snippet_10\n\nLANGUAGE: Grok\nCODE:\n```\nURIPROTO [A-Za-z]+(\\+[A-Za-z+]+)?\nURIHOST %{IPORHOST}(?::%{POSINT:port})?\n# uripath comes loosely from RFC1738, but mostly from what Firefox\n# doesn't turn into %XX\nURIPATH (?:/[A-Za-z0-9$.+!*'(){},~:;=@#%_\\-]*)+\n#URIPARAM \\?(?:[A-Za-z0-9]+(?:=(?:[^&]*))?(?:&(?:[A-Za-z0-9]+(?:=(?:[^&]*))?)?)*)?\nURIPARAM \\?[A-Za-z0-9$.+!*'|(){},~@#%&/=:;_?\\-\\[\\]]*\nURIPATHPARAM %{URIPATH}(?:%{URIPARAM})?\nURI %{URIPROTO}://(?:%{USER}(?::[^@]*)?@)?(?:%{URIHOST})?(?:%{URIPATHPARAM})?\n```\n\n----------------------------------------\n\nTITLE: Displaying and Copying New Keys - Shell\nDESCRIPTION: This snippet outputs instructions for copying the generated keys to the NiFi Registry test keys directory after key generation.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-core/nifi-registry-web-api/src/test/resources/keys/README.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\necho\necho \"New keys written to ${WD}/keys\"\necho \"Copy to NiFi Registry test keys dir by running: \"\necho \"    cp \\\"$WD/keys/*\\\" /path/to/nifi-registry/nifi-registry-core/nifi-registry-web-api/src/test/resources/keys/\"\n```\n\n----------------------------------------\n\nTITLE: Finding Generated Credentials in NiFi Logs - grep Utility - Shell\nDESCRIPTION: Searches for occurrences of the word 'Generated' in NiFi application logs to locate system-generated usernames and passwords on startup. Requires the grep utility and access to logs/nifi-app*log files. Prints matching lines containing credentials.\nSOURCE: https://github.com/apache/nifi/blob/main/README.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ngrep Generated logs/nifi-app*log\n```\n\n----------------------------------------\n\nTITLE: Extracting User IDs from Log Messages with Bash\nDESCRIPTION: This Bash script searches for user ID patterns within log message files to facilitate audit and user activity tracking. It uses grep and sed for pattern matching and extraction, assuming the logs include a 'userid' field. The script outputs user IDs for further processing or alerting. Dependencies include standard Unix tools; inputs include log files, and outputs are lists of user IDs.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/test/resources/cef/single-row-with-extensions.txt#_snippet_1\n\nLANGUAGE: Bash\nCODE:\n```\ngrep 'userid=' /var/log/security.log | sed -n 's/.*userid=\\([^ \\\\]+\\).*/\\1/p'\n```\n\n----------------------------------------\n\nTITLE: Querying Provenance Reporting with minifi.sh\nDESCRIPTION: This command retrieves the health and bulletins of the Provenance Reporting component within the MiNiFi instance using the `flowStatus` command.  It uses the `provenancereporting` flag, specifying the desired options (health, bulletins) separated by commas.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-docs/src/main/markdown/System_Admin_Guide.md#_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nminifi.sh flowStatus provenancereporting:health,bulletins\n```\n\n----------------------------------------\n\nTITLE: XQuery: Return only the first fruit node\nDESCRIPTION: This XQuery expression selects the first 'fruit' node from the XML document using its index. It returns only the first 'fruit' node.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.EvaluateXQuery/additionalDetails.md#_snippet_2\n\nLANGUAGE: xquery\nCODE:\n```\n//fruit[1]\n```\n\n----------------------------------------\n\nTITLE: Sample XML for XQuery Examples\nDESCRIPTION: This XML document represents a fruit basket with various fruits, each having a name, color, and taste. It serves as the input for the XQuery examples provided in the document.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.EvaluateXQuery/additionalDetails.md#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"foo.xsl\"?>\n<ns:fruitbasket xmlns:ns=\"http://namespace/1\">\n    <fruit taste=\"crisp\">           <!-- Apples are my favorite-->\n        <name>apple</name>\n        <color>red</color>\n    </fruit>\n    <fruit>\n        <name>apple</name>\n        <color>green</color>\n    </fruit>\n    <fruit>\n        <name>banana</name>\n        <color>yellow</color>\n    </fruit>\n    <fruit taste=\"sweet\">\n        <name>orange</name>\n        <color>orange</color>\n    </fruit>\n    <fruit>\n        <name>blueberry</name>\n        <color>blue</color>\n    </fruit>\n    <fruit taste=\"tart\">\n        <name>raspberry</name>\n        <color>red</color>\n    </fruit>\n    <fruit>\n        <name>none</name>\n        <color/>\n    </fruit>\n</ns:fruitbasket>\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Parameter Value Provider in Properties File\nDESCRIPTION: Configuration example for a custom Parameter Value Provider in NiFi Stateless that can source parameter values from external systems. The example specifies the provider name, type, bundle, and custom properties.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-stateless/nifi-stateless-assembly/README.md#_snippet_12\n\nLANGUAGE: properties\nCODE:\n```\nnifi.stateless.parameter.provider.Props File Provider.name=My Custom Properties File Parameter Value Provider\nnifi.stateless.parameter.provider.Props File Provider.type=com.myorg.nifi.parameters.custom.MyCustomPropertiesFileParameterValueProvider\nnifi.stateless.parameter.provider.Props File Provider.bundle=com.myorg:nifi-custom-parameter-provider-nar:0.0.1\nnifi.stateless.parameter.provider.Props File Provider.properties.Filename=/tmp/parameters.properties\n```\n\n----------------------------------------\n\nTITLE: Querying Processor Health with minifi.sh\nDESCRIPTION: This command retrieves the health status of all processors in the MiNiFi flow. It uses the `flowStatus` command with the `processor` flag, specifying `all` to target all processors and requesting only the `health` status.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-docs/src/main/markdown/System_Admin_Guide.md#_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\nUser:minifi-0.0.1-SNAPSHOT user ./bin/minifi.sh flowStatus processor:all:health\n```\n\n----------------------------------------\n\nTITLE: Showing Original CSV Payload Example for Wrapper Strategy in JSON\nDESCRIPTION: Provides a sample CSV input representing original data records expected by the Wrapper join strategy. Demonstrates typical fields such as id, name, and age. This input is used to illustrate how records are matched with enrichment data by index. No dependencies or special parameters apply to this data snippet itself.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.JoinEnrichment/additionalDetails.md#_snippet_0\n\nLANGUAGE: CSV\nCODE:\n```\nid, name, age\n28021, John Doe, 55\n832, Jane Doe, 22\n29201, Jake Doe, 23\n555, Joseph Doe, 2\n```\n\n----------------------------------------\n\nTITLE: Avro Schema for Nested Siblings Field (Nested Field Strategy)\nDESCRIPTION: Specifies the expected structure of records extracted from the \"siblings\" array within the sample YAML data, as processed by the NiFi YamlTreeReader using the \"Nested Field Strategy\". This schema defines the fields (\"name\", \"id\") expected for each element in the 'siblings' array.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.yaml.YamlTreeReader/additionalDetails.md#_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"namespace\": \"nifi\",\n  \"name\": \"siblings\",\n  \"type\": \"record\",\n  \"fields\": [\n    {\n      \"name\": \"name\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"id\",\n      \"type\": \"int\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Creating custom Docker image with config\nDESCRIPTION: This Dockerfile extends the base Apache NiFi MiNiFi image and adds `flow.json.raw` and `bootstrap.conf` to the `/opt/minifi/minifi-current/conf/` directory within the image.  This ensures that the custom image will always use these specified configuration files.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-docker/dockerhub/README.md#_snippet_3\n\nLANGUAGE: Docker\nCODE:\n```\nFROM apache/nifi-minifi\n\nADD flow.json.raw /opt/minifi/minifi-current/conf/flow.json.raw\nADD bootstrap.conf /opt/minifi/minifi-current/conf/bootstrap.conf\n```\n\n----------------------------------------\n\nTITLE: Creating a Secret in AWS Secrets Manager (AWS CLI)\nDESCRIPTION: Shows how to use the AWS CLI (`aws secretsmanager create-secret`) to create a secret in AWS Secrets Manager. The secret name (`Context` in the example) maps to a NiFi Parameter Context, and the JSON string contains key/value pairs representing parameter names (`Param`, `Param2`) and their values. This operation requires the `secretsmanager:GetSecretValue` AWS permission.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-stateless/nifi-stateless-assembly/README.md#_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\naws secretsmanager create-secret --name \"Context\" --secret-string '{ \"Param\": \"secretValue\", \"Param2\": \"secretValue2\" }'\n```\n\n----------------------------------------\n\nTITLE: Using NiFi Toolkit CLI Inside Docker Container - Shell\nDESCRIPTION: This snippet shows how to execute a NiFi Toolkit CLI command interactively inside a running NiFi Docker container. It uses 'docker exec' with a terminal, running the 'current-user' command via the toolkit to verify user identity or obtain session information. Dependencies include a running NiFi container named 'nifi'; output is the printed user identity information. Toolkit usage requires container and toolkit to be properly initialized.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-docker/dockerhub/README.md#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\ndocker exec -ti nifi nifi-toolkit-current/bin/cli.sh nifi current-user\nanonymous\n```\n\n----------------------------------------\n\nTITLE: Listing Built Binary Distributions - UNIX ls - Shell\nDESCRIPTION: Lists NiFi binary distribution zip files in the nifi-assembly/target directory. This confirms that the Maven build produced the expected distribution archives. Assumes the current working directory is the project root, and the build has completed successfully.\nSOURCE: https://github.com/apache/nifi/blob/main/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nls nifi-assembly/target/nifi-*-bin.zip\n```\n\n----------------------------------------\n\nTITLE: Building Minifi Integration Tests Module with Maven\nDESCRIPTION: This command builds and installs the minifi-integration-tests module, skipping tests, with parallel jobs, preparing the environment for running integration tests.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-integration-tests/README.md#_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nmvn -pl minifi/minifi-integration-tests -am install -T1C -DskipTests\n```\n\n----------------------------------------\n\nTITLE: Copy CA Key and Cert - Shell\nDESCRIPTION: This snippet copies existing CA key and certificate pair to a final output directory.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-core/nifi-registry-web-api/src/test/resources/keys/README.md#_snippet_6\n\nLANGUAGE: Shell\nCODE:\n```\n# copy CA key/cert to final output dir in all formats\ncp nifi-key.key keys/ca-key.pem\ncp nifi-cert.pem keys/ca-cert.pem\n```\n\n----------------------------------------\n\nTITLE: JSON Representation Before 'Any' Type Processing\nDESCRIPTION: Illustrates the potential JSON output structure of a NiFi Record when the ProtobufReader encounters an 'Any' field but *before* it processes the embedded message. It shows the raw 'type_url' and the serialized 'value' (as a byte array) contained within the 'Any' field.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-protobuf-bundle/nifi-protobuf-services/src/main/resources/docs/org.apache.nifi.services.protobuf.ProtobufReader/additionalDetails.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  anyField: {\n    type_url: \"type.googleapis.com/NestedMessage\"\n    value: [\n      84,\n      101,\n      115,\n      116,\n      32,\n      98,\n      121,\n      116,\n      101,\n      115\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Example YAML Data for Root Node Strategy\nDESCRIPTION: Provides a sample YAML document with an array of objects, used to illustrate how the NiFi YamlTreeReader processes data when configured with the \"Root Node Strategy\". It contains nested structures and different data types.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.yaml.YamlTreeReader/additionalDetails.md#_snippet_0\n\nLANGUAGE: YAML\nCODE:\n```\n- id: 17\n  name: John\n  child:\n    id: \"1\"\n  dob: 10-29-1982\n  siblings:\n    - name: Jeremy\n      id: 4\n    - name: Julia\n      id: 8\n- id: 98\n  name: Jane\n  child:\n    id: 2\n  dob: 08-30-1984\n  gender: F\n  siblingIds: []\n  siblings: []\n```\n\n----------------------------------------\n\nTITLE: Configuring core-site.xml to use ssl-client.xml (XML)\nDESCRIPTION: This XML snippet configures the core-site.xml file to use the ssl-client.xml for SSL configurations. It sets the default file system (fs.defaultFS) and specifies the name of the SSL client configuration file (_hadoop.ssl.client.conf_).  This allows the HDFS processor to use the truststore settings defined in ssl-client.xml. Requires the core-site.xml file and ssl-client.xml file to be accessible.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-hadoop-bundle/nifi-hdfs-processors/src/main/resources/docs/org.apache.nifi.processors.hadoop.PutHDFS/additionalDetails.md#_snippet_1\n\nLANGUAGE: XML\nCODE:\n```\n<configuration>\n    <property>\n        <name>fs.defaultFS</name>\n        <value>swebhdfs://{namenode.hostname:port}</value>\n    </property>\n    <property>\n        <name>hadoop.ssl.client.conf</name>\n        <value>ssl-client.xml</value>\n    </property>\n</configuration>\n```\n\n----------------------------------------\n\nTITLE: Querying MiNiFi Instance with minifi.sh\nDESCRIPTION: This command retrieves the health, stats, and bulletins of the MiNiFi instance itself using the `flowStatus` command.  It uses the `instance` flag and specifies the desired options (health, stats, bulletins) separated by commas.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-docs/src/main/markdown/System_Admin_Guide.md#_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nminifi.sh flowStatus instance:health,stats,bulletins\n```\n\n----------------------------------------\n\nTITLE: Grok Expression for NiFi\nDESCRIPTION: This Grok expression is used for parsing log messages within NiFi. It defines patterns to extract key fields like timestamp, log level, thread, class, and message from the log data. This expression is intended for use with the NiFi Grok Controller Service to automatically parse and structure log data as it flows through the NiFi data pipelines.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.grok.GrokReader/additionalDetails.md#_snippet_2\n\nLANGUAGE: Grok\nCODE:\n```\n%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} \\[%{DATA:thread}\\] %{DATA:class} %{GREEDYDATA:message}\n```\n\n----------------------------------------\n\nTITLE: Defining Docker Image Tag for Apache NiFi MiNiFi 2.0.0\nDESCRIPTION: Specifies the Docker image repository path and version tag for Apache NiFi MiNiFi. This defines the image as 'apache/nifi-minifi' with the version tag '2.0.0'.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-docker/dockerhub/DockerImage.txt#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\napache/nifi-minifi:2.0.0\n```\n\n----------------------------------------\n\nTITLE: Defining Protobuf Messages including 'Any' Type\nDESCRIPTION: This snippet shows Protocol Buffers message definitions. It defines the standard 'Any' message type, a 'TestMessage' containing an 'Any' field, and a 'NestedMessage' which might be embedded within the 'Any' field. This illustrates the structure used in the Protobuf 'Any' field type example.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-protobuf-bundle/nifi-protobuf-services/src/main/resources/docs/org.apache.nifi.services.protobuf.ProtobufReader/additionalDetails.md#_snippet_0\n\nLANGUAGE: protobuf\nCODE:\n```\nmessage Any {\n    string type_url = 1;\n    bytes value = 2; \n}\n\nmessage TestMessage {\n    google.protobuf.Any anyField = 3; \n}\n\nmessage NestedMessage {\n    string field_1 = 1;\n    string field_2 = 2;     \n    string field_3 = 3;\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Processor Attributes via Record Path (Text)\nDESCRIPTION: This snippet illustrates how to configure standard PutZendeskTicket processor attributes like Comment Body, Subject, Priority, and Type using the Record Path syntax ('%{/path/to/field}') to dynamically extract values from the incoming record. This configuration is used in conjunction with a RecordReader to process structured data.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-zendesk-bundle/nifi-zendesk-processors/src/main/resources/docs/org.apache.nifi.processors.zendesk.PutZendeskTicket/additionalDetails.md#_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nComment Body : %{/record/description}\nSubject : %{/record/issue/name}\nPriority : %{/record/issue/type}\nType : %{/record/project/name}\n```\n\n----------------------------------------\n\nTITLE: JSON Output Example 1 with Force Types = true\nDESCRIPTION: This JSON represents the output of the `ValidateRecord` processor when `Force Types From Reader's Schema` is set to true in Example 1. Since the XML input data doesn't completely match the schema (nested field1 elements), the processor outputs only `field2` since it is the only field in the output that complies with the schema. This is routed to the invalid relationship. This showcases how the strict enforcement of the schema filters the output.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ValidateRecord/additionalDetails.md#_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\n[\n  {\n    \"field2\": \"content_of_field_2\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Running an Unsecured NiFi Registry Container\nDESCRIPTION: A minimal command to run an unsecured NiFi Registry container with port mapping, exposing the web UI on localhost:18080. It includes optional environment variable configuration for ports and hostnames.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-core/nifi-registry-docker/dockerhub/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --name nifi-registry \\\n  -p 18080:18080 \\\n  -d \\\n  apache/nifi-registry:latest\n```\n\n----------------------------------------\n\nTITLE: Configuring NiFi Processor Attributes via Constants (Text)\nDESCRIPTION: This snippet shows how to configure standard PutZendeskTicket processor attributes using constant string values. Unlike the Record Path method, these values are static and do not change based on the incoming record content, providing a fixed value for each ticket attribute in the generated Zendesk API request.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-zendesk-bundle/nifi-zendesk-processors/src/main/resources/docs/org.apache.nifi.processors.zendesk.PutZendeskTicket/additionalDetails.md#_snippet_3\n\nLANGUAGE: text\nCODE:\n```\nComment Body : Sample description\nSubject : Sample subject\nPriority : High\nType : Sample type\n```\n\n----------------------------------------\n\nTITLE: Creating Salesforce Account Records with JSON Input in PutSalesforceObject\nDESCRIPTION: This example demonstrates how to structure JSON input to create multiple Account records in Salesforce using the PutSalesforceObject processor. The input creates two Account records with different attributes including name, phone, website, employee count, and industry.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-salesforce-bundle/nifi-salesforce-processors/src/main/resources/docs/org.apache.nifi.processors.salesforce.PutSalesforceObject/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"name\": \"SampleAccount1\",\n    \"phone\": \"1111111111\",\n    \"website\": \"www.salesforce1.com\",\n    \"numberOfEmployees\": \"100\",\n    \"industry\": \"Banking\"\n  },\n  {\n    \"name\": \"SampleAccount2\",\n    \"phone\": \"22222222\",\n    \"website\": \"www.salesforce2.com\",\n    \"numberOfEmployees\": \"200\",\n    \"industry\": \"Banking\"\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Creating a Routine Data Filter Expression in Groovy\nDESCRIPTION: This Groovy script filters incoming data based on specific criteria, such as attribute values or content patterns. It is used within NiFi to route or process data conditionally. Dependencies include the NiFi scripting API and Groovy language support; it enables dynamic decision-making in flow configurations.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/test/resources/TestReplaceTextLineByLine/ReplaceExceptLastLine.txt#_snippet_1\n\nLANGUAGE: Groovy\nCODE:\n```\ndef attributeValue = flowFile.getAttribute('status')\nif (attributeValue == 'active') {\n    session.transfer(flowFile, REL_ACCEPT);\n} else {\n    session.transfer(flowFile, REL_REJECT);\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring UpdateRecord Processor for Relative RecordPath Replacement\nDESCRIPTION: This configuration illustrates how to use relative RecordPaths to update fields based on their parent context. The example replaces each siblingâ€™s 'name' with its 'id' by using a relative path starting with '..', reflecting updates within nested structures.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.UpdateRecord/additionalDetails.md#_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nreplacement_value_strategy: Record Path Value\n# Use relative path to set name to the siblingâ€™s id\n'siblings[].name': ../id\n```\n\n----------------------------------------\n\nTITLE: Schema Caching in NiFi Record Readers and Writers\nDESCRIPTION: This snippet explains how NiFi Record Readers and Writers utilize schema caches to optimize schema inference and propagation, reducing processing costs. It details flowfile attribute usage for schema identification and how schemas are shared via a Controller Service. No actual code is provided, but the configuration and behavior are thoroughly described.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-poi-bundle/nifi-poi-services/src/main/resources/docs/org.apache.nifi.excel.ExcelReader/additionalDetails.md#_snippet_0\n\n\n\n----------------------------------------\n\nTITLE: Integrating the Enrichment Schema into an Existing Avro Schema (JSON)\nDESCRIPTION: Demonstrates how to update an existing Avro schema ('ipRecord') to include the IP enrichment data. It adds a new nullable field named 'enrichment', where the previously defined 'enrichmentRecord' schema (from the first snippet) should be pasted.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-lookup-services-bundle/nifi-lookup-services/src/main/resources/docs/org.apache.nifi.lookup.maxmind.IPLookupService/additionalDetails.md#_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"ipRecord\",\n  \"namespace\": \"nifi\",\n  \"type\": \"record\",\n  \"fields\": [\n    {\n      \"name\": \"ip\",\n      \"type\": \"string\"\n    },\n    {\n      \"name\": \"enrichment\",\n      \"type\": [\n        \"null\",\n        <Paste Enrichment Schema Here>\n      ]\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Resulting Zendesk API Request via Constants (JSON)\nDESCRIPTION: This JSON object shows the structure of the Zendesk API request payload constructed by the PutZendeskTicket processor when standard attributes are configured with the specified constant values. The constant strings are directly used as the values for the corresponding ticket fields in the request body sent to Zendesk.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-zendesk-bundle/nifi-zendesk-processors/src/main/resources/docs/org.apache.nifi.processors.zendesk.PutZendeskTicket/additionalDetails.md#_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"comment\": {\n    \"body\": \"Sample description\"\n  },\n  \"subject\": \"Sample subject\",\n  \"priority\": \"High\",\n  \"type\": \"Sample type\"\n}\n```\n\n----------------------------------------\n\nTITLE: Generating Source Files with jasn1-compiler\nDESCRIPTION: This command uses the jasn1-compiler to generate Java source files from the example.asn ASN.1 definition file. The output directory is specified as the test source directory within the NiFi ASN.1 services project.  This command requires the jasn1-compiler to be installed and available in the system's PATH.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-asn1-bundle/nifi-asn1-services/src/test/resources/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ jasn1/bin/jasn1-compiler -f nifi-nar-bundles/nifi-asn1-bundle/nifi-asn1-services/src/test/resources/example.asn -o nifi-nar-bundles/nifi-asn1-bundle/nifi-asn1-services/src/test/java\n```\n\n----------------------------------------\n\nTITLE: Configuring UpdateRecord Processor for RecordPath-Based Replacement\nDESCRIPTION: This snippet shows how to set up the UpdateRecord processor to replace record field values with values obtained from other fields via RecordPath. The configuration directs the processor to set 'name' from the 'siblings[0]/name' field, enabling dynamic data referencing within records.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.UpdateRecord/additionalDetails.md#_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nreplacement_value_strategy: Record Path Value\n# /name is updated with the value of /siblings[0]/name\n'name': /siblings[0]/name\n```\n\n----------------------------------------\n\nTITLE: Generating Jacoco Code Coverage Report Using Maven in Apache NiFi\nDESCRIPTION: This snippet provides a terminal command to run Maven with specific profiles (`integration-tests` and `integration-tests-ci`) to execute tests and generate a code coverage report using JaCoCo. The output report is saved as an HTML file at a specified path within the NiFi project structure.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-kafka-bundle/nifi-kafka-code-coverage/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n/nifi/nifi-nar-bundles/nifi-kafka-bundle % mvn clean test verify -P integration-tests -P integration-tests-ci\n```\n\nLANGUAGE: bash\nCODE:\n```\nOutput written to\n  - `nifi/nifi-nar-bundles/nifi-kafka-bundle/nifi-kafka-code-coverage/target/site/jacoco-aggregate/index.html`\n```\n\n----------------------------------------\n\nTITLE: Starting Apache NiFi Docker Container (Non-OIDC) - Shell\nDESCRIPTION: This shell command illustrates starting a basic Apache NiFi Docker container without special authentication mechanisms. It assumes Docker is installed and uses the default NiFi image, running the service in detached mode. No additional certificates or environment variables are specified, so further configuration is likely required for production use.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-docker/dockerhub/README.md#_snippet_5\n\nLANGUAGE: Shell\nCODE:\n```\ndocker run -d --name nifi apache/nifi\n```\n\n----------------------------------------\n\nTITLE: Incrementing counter using NiFi Expression Language\nDESCRIPTION: This code snippet demonstrates how to increment a counter attribute 'theCount' that keeps track of the total number of FlowFiles processed. It uses NiFi's getStateValue function combined with the plus operator to increase the count by one each time the processor runs.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-update-attribute-bundle/nifi-update-attribute-processor/src/main/resources/docs/org.apache.nifi.processors.attributes.UpdateAttribute/additionalDetails.md#_snippet_0\n\nLANGUAGE: NiFi Expression Language\nCODE:\n```\n${getStateValue(\"theCount\"):plus(1)}\n```\n\n----------------------------------------\n\nTITLE: Configuring Flow Persistence Storage and Provider\nDESCRIPTION: This snippet shows environment variables for setting flow persistence directory and provider class like 'git' or 'file', as well as remote repository details for Git-based persistence.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-core/nifi-registry-docker/dockerhub/README.md#_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\n# Environment variables to configure flow persistence:\n# NIFI_REGISTRY_FLOW_STORAGE_DIR: Directory to store flow data\n# NIFI_REGISTRY_FLOW_PROVIDER: Persistence provider class ('git' or 'file')\n# NIFI_REGISTRY_GIT_REMOTE: Remote repository URL\n# NIFI_REGISTRY_GIT_USER: Remote repository access username\n# NIFI_REGISTRY_GIT_PASSWORD: Remote repository password\n# NIFI_REGISTRY_GIT_REPO: Clone repository URL\n\n```\n\n----------------------------------------\n\nTITLE: Querying System Diagnostics with minifi.sh\nDESCRIPTION: This command retrieves various system diagnostics information, including heap usage, processor stats, content repository usage, FlowFile repository usage, and garbage collection details, using the `flowStatus` command. It uses the `systemdiagnostics` flag and specifies the desired options separated by commas.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-docs/src/main/markdown/System_Admin_Guide.md#_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nminifi.sh flowStatus systemdiagnostics:heap,processorstats,contentrepositoryusage,flowfilerepositoryusage,garbagecollection\n```\n\n----------------------------------------\n\nTITLE: Example XML for Schema Inference Challenges\nDESCRIPTION: Illustrates an XML dataset with inconsistent data types across records (`age`, `values`). This example highlights complexities addressed by NiFi's schema inference logic when deriving schemas from data.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLReader/additionalDetails.md#_snippet_22\n\nLANGUAGE: xml\nCODE:\n```\n<root>\n    <record>\n        <name>John</name>\n        <age>8</age>\n        <values>N/A</values>\n    </record>\n    <record>\n        <name>Jane</name>\n        <age>Ten</age>\n        <values>8</values>\n        <values>Ten</values>\n    </record>\n</root>\n```\n\n----------------------------------------\n\nTITLE: Serving NiFi UI Development Server - Shell\nDESCRIPTION: Runs the NiFi UI application using the Nx development server for local development. The application is accessible via `http://localhost:4200/nifi` and automatically reloads upon source file changes. Note that direct login via the dev server does not work; authentication requires a cookie from a full NiFi instance.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-frontend/src/main/frontend/README.md#_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nnpx nx serve\n```\n\n----------------------------------------\n\nTITLE: Defining HTTP Date Grok Pattern\nDESCRIPTION: Defines the HTTPDATE pattern that matches the date and time format commonly found in Apache access logs. It captures the day, month, year, time and timezone offset. It relies on the MONTH, MONTHDAY, YEAR and TIME patterns defined elsewhere. The date and time are captured into the timestamp field.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/default-grok-patterns.txt#_snippet_5\n\nLANGUAGE: Grok\nCODE:\n```\nHTTPDATE %{MONTHDAY}/%{MONTH}/%{YEAR}:%{TIME} %{INT}\n```\n\n----------------------------------------\n\nTITLE: Pass-through Script for Records in Apache NiFi\nDESCRIPTION: This script allows each record to pass through unchanged, effectively acting as a no-operation, by returning the input record as is. Useful for templates or conditional processing where no modification is needed.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-scripting-bundle/nifi-scripting-processors/src/main/resources/docs/org.apache.nifi.processors.script.ScriptedTransformRecord/additionalDetails.md#_snippet_2\n\nLANGUAGE: Groovy\nCODE:\n```\nrecord\n```\n\n----------------------------------------\n\nTITLE: Setting Azure Key Vault Secret using Azure CLI\nDESCRIPTION: Command to create a secret in Azure Key Vault using the Azure CLI. The command sets a secret with a parameter name, value, and adds a group-name tag to associate it with a Parameter Group in NiFi.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-azure-bundle/nifi-azure-parameter-providers/src/main/resources/docs/org.apache.nifi.parameter.azure.AzureKeyVaultSecretsParameterProvider/additionalDetails.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\naz keyvault secret set --vault-name [Vault Name] --name [Parameter Name] --value [Parameter Value] --tags group-name=[Parameter Group Name]\n```\n\n----------------------------------------\n\nTITLE: Parsing Dropbox OAuth Token Response in JSON\nDESCRIPTION: This snippet shows the JSON response structure returned after successfully authenticating with Dropbox's OAuth service. It contains the access_token, refresh_token, token expiration time, scope permissions, and user identification information.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-dropbox-bundle/nifi-dropbox-services/src/main/resources/docs/org.apache.nifi.services.dropbox.StandardDropboxCredentialService/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"access_token\": \"sl.xxxxxxxxxxx\"\n  \"expires_in\": 14400,\n  \"refresh_token\": \"xxxxxx\",\n  \"scope\": \"files.content.read files.metadata.read\",\n  \"uid\": \"xxxxxx\",\n  \"account_id\": \"dbid:xxxx\"\n}\n```\n\n----------------------------------------\n\nTITLE: Example XML for Array of Records\nDESCRIPTION: Demonstrates an XML structure where an element (`array_field`) repeats, representing an array of records. Each `array_field` contains embedded fields, intended for parsing by a NiFi Record Reader.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLReader/additionalDetails.md#_snippet_16\n\nLANGUAGE: xml\nCODE:\n```\n<record>\n    <array_field>\n        <embedded_field>embedded content 1</embedded_field>\n        <another_embedded_field>another embedded content 1</another_embedded_field>\n    </array_field>\n    <array_field>\n        <embedded_field>embedded content 2</embedded_field>\n        <another_embedded_field>another embedded content 2</another_embedded_field>\n    </array_field>\n</record>\n```\n\n----------------------------------------\n\nTITLE: Running NiFi Registry with Custom Ports and Environment Variables\nDESCRIPTION: This snippet demonstrates passing environment variables to customize the web HTTP port. It shows how to run the container with different port mappings and environment variables for configuration.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-core/nifi-registry-docker/dockerhub/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --name nifi-registry \\\n  -p 19090:19090 \\\n  -d \\\n  -e NIFI_REGISTRY_WEB_HTTP_PORT='19090' \\\n  apache/nifi-registry:latest\n```\n\n----------------------------------------\n\nTITLE: CSV Input Example for Position-based Record Validation\nDESCRIPTION: Sample CSV input data showing four railroad company records with company names and number of trains attributes that will be validated based on position.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-scripting-bundle/nifi-scripting-processors/src/main/resources/docs/org.apache.nifi.processors.script.ScriptedValidateRecord/additionalDetails.md#_snippet_0\n\nLANGUAGE: CSV\nCODE:\n```\ncompany, numberOfTrains Boston & Maine Railroad, 3 Chesapeake & Ohio Railroad, 2 Pennsylvania Railroad, 4 Reading Railroad, 2\n```\n\n----------------------------------------\n\nTITLE: Bash Script for Running Integration Tests\nDESCRIPTION: This bash script iterates through different Elasticsearch versions and NiFi modules to run integration tests. It uses Maven to execute the tests and exits with an error code if any tests fail.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-elasticsearch-bundle/README.md#_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmvn --fail-at-end -Pcontrib-check clean install\n\n# blank entry to run the default integration-tests profile, i.e. Elasticsearch 8\nes_versions=(elasticsearch6 elasticsearch7 \" \")\nit_modules=(nifi-elasticsearch-client-service nifi-elasticsearch-restapi-processors)\nfor v in \"${es_versions[@]}\"; do\n    for m in \"${it_modules[@]}\"; do\n        pushd \"${m}\"\n        if ! mvn -P \"integration-tests,${v}\" --fail-at-end failsafe:integration-test failsafe:verify; then\n            echo; echo; echo \"Integration Tests failed for ${v} in ${m}, see Maven logs for details\"\n            exit 1\n        fi\n        popd\n    done\ndone\n```\n\n----------------------------------------\n\nTITLE: Querying Flow Config History Connections SQL\nDESCRIPTION: This SQL query selects specific connection-related fields (connectionSourceName, connectionDestinationName, connectionRelationship) from the FLOW_CONFIG_HISTORY table. This table records historical configuration changes. Useful for auditing and tracking changes made to connections over time.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-sql-reporting-bundle/nifi-sql-reporting-tasks/src/main/resources/docs/org.apache.nifi.reporting.sql.QueryNiFiReportingTask/additionalDetails.md#_snippet_5\n\nLANGUAGE: SQL\nCODE:\n```\nSELECT connectionSourceName, connectionDestinationName, connectionRelationship\nfrom FLOW_CONFIG_HISTORY\n```\n\n----------------------------------------\n\nTITLE: Defining Paths Grok Pattern\nDESCRIPTION: Defines Grok patterns for matching file paths, including UNIX paths and Windows paths. This allows for extracting file paths from log messages. It includes patterns for TTY devices as well.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/default-grok-patterns.txt#_snippet_9\n\nLANGUAGE: Grok\nCODE:\n```\nPATH (?:%{UNIXPATH}|%{WINPATH})\nUNIXPATH (?>/(?>[\\w_%!$@:.,-]+|\\\\.)*)+\nTTY (?:/dev/(pts|tty([pq])?)(\\w+)?/?(?:[0-9]+))\nWINPATH (?>[A-Za-z]+:|\\\\)(?:\\[^\\\\?*]*)+\n```\n\n----------------------------------------\n\nTITLE: Example XML for Map Structure\nDESCRIPTION: Presents an XML example where a `map_field` contains arbitrarily named child elements, representing a map structure. This illustrates data that can be processed using a map schema type in NiFi.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLReader/additionalDetails.md#_snippet_20\n\nLANGUAGE: xml\nCODE:\n```\n<record>\n    <map_field>\n        <field1>content</field1>\n        <field2>content</field2>                     ...\n    </map_field>\n    <simple_field>content</simple_field>\n</record>\n```\n\n----------------------------------------\n\nTITLE: Building Application Binaries Only - Apache Maven Wrapper - Shell\nDESCRIPTION: Builds only the main NiFi application binaries using the Maven Wrapper, avoiding optional modules. The '-am' flag includes required dependencies, and '-pl :nifi-assembly' specifies the nifi-assembly module. Outputs the distributable archives in the nifi-assembly/target directory.\nSOURCE: https://github.com/apache/nifi/blob/main/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\n./mvnw install -T1C -am -pl :nifi-assembly\n```\n\n----------------------------------------\n\nTITLE: ASCII Art Banner for Apache NiFi Registry\nDESCRIPTION: A decorative ASCII art banner displaying the Apache NiFi Registry name and version placeholder. This banner is likely shown during application startup or in logs to visually identify the application.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-core/nifi-registry-web-api/src/main/resources/banner.txt#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n  Apache NiFi   _     _\n _ __ ___  __ _(_)___| |_ _ __ _   _\n| '__/ _ \\/ _` | / __| __| '__| | | |\n| | |  __/ (_| | \\__ \\ |_| |  | |_| |\n|_|  \\___|\\__, |_|___/\\__|_|   \\__, |\n==========|___/================|___/=\n               v${application.version}\n```\n\n----------------------------------------\n\nTITLE: Example Record JSON for Deduplication\nDESCRIPTION: This JSON represents an example record used by the DeduplicateRecords processor. The processor can extract values from specific fields (e.g., firstName, lastName) to generate a unique identifier for deduplication purposes.  The dynamic properties configuration in the processor specifies the record path operations to use for extracting these values.  If any record path is missing, it results in an exception.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.DeduplicateRecords/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"firstName\": \"John\",\n  \"lastName\": \"Smith\"\n}\n```\n\n----------------------------------------\n\nTITLE: Example XML for Array Enclosed in a Field\nDESCRIPTION: Shows an XML structure where an array of simple elements (`element`) is nested within a parent tag (`field_enclosing_array`). This structure is used to demonstrate parsing nested arrays in NiFi.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLReader/additionalDetails.md#_snippet_18\n\nLANGUAGE: xml\nCODE:\n```\n<record>\n    <field_enclosing_array>\n        <element>content 1</element>\n        <element>content 2</element>\n    </field_enclosing_array>\n    <field_without_array>content 3</field_without_array>\n</record>\n```\n\n----------------------------------------\n\nTITLE: Defining Relationships with SQL Queries in QueryRecord\nDESCRIPTION: Demonstrates defining custom relationships in the QueryRecord processor using SQL queries as property values. Each property name becomes a relationship, and the corresponding SQL query filters or transforms the incoming FlowFile data. These examples query based on title and average age.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.QueryRecord/additionalDetails.md#_snippet_1\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM FLOWFILE WHERE title LIKE '%Engineer%'\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT name FROM FLOWFILE WHERE title = 'Vice President'\n```\n\nLANGUAGE: sql\nCODE:\n```\nSELECT * FROM FLOWFILE WHERE age < (SELECT AVG(age) FROM FLOWFILE)\n```\n\n----------------------------------------\n\nTITLE: Build NiFi Registry with AWS extensions\nDESCRIPTION: This command builds NiFi Registry, automatically including the AWS extensions. It assumes the user is in the root directory of the NiFi Registry project. This includes the dependencies specified in the `pom.xml` file to ensure all components are available.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-extensions/nifi-registry-aws/nifi-registry-aws-assembly/README.md#_snippet_0\n\nLANGUAGE: Bash\nCODE:\n```\ncd nifi-registry\nmvn clean install\n```\n\n----------------------------------------\n\nTITLE: Specifying Sort Order for GetMongo Results (JSON)\nDESCRIPTION: Provides a JSON example for the 'Sort' configuration property in the NiFi GetMongo processor. This example sorts results in descending order based on the 'someDate' field. It requires a valid JSON document specifying field names and sort order (1 for ascending, -1 for descending).\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-mongodb-bundle/nifi-mongodb-processors/src/main/resources/docs/org.apache.nifi.processors.mongodb.GetMongo/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{ \"someDate\": -1 }\n```\n\n----------------------------------------\n\nTITLE: Parsing a Single XML Record with XMLReader in Apache NiFi\nDESCRIPTION: Example of a single XML record format that can be processed by the XMLReader. This shows the basic structure of a single record with multiple fields.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLReader/additionalDetails.md#_snippet_0\n\nLANGUAGE: xml\nCODE:\n```\n<record>\n    <field1>content</field1>\n    <field2>content</field2>\n</record>\n```\n\n----------------------------------------\n\nTITLE: Finding Google Drive File ID from URL example\nDESCRIPTION: Example showing how to extract a file ID from a Google Drive sharing URL. The file ID appears in the URL path after '/d/' and is required for the 'File ID' property in the FetchGoogleDrive processor.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-gcp-bundle/nifi-gcp-processors/src/main/resources/docs/org.apache.nifi.processors.gcp.drive.FetchGoogleDrive/additionalDetails.md#_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\nhttps://drive.google.com/file/d/16ALV9KIU_KKeNG557zyctqy2Fmzyqtq/view?usp=share_link\n```\n\n----------------------------------------\n\nTITLE: Formatting NiFi UI Code with Prettier - Shell\nDESCRIPTION: Automatically formats the NiFi UI codebase using Prettier. This command applies the configured formatting rules to all relevant files, ensuring consistent code style across the project. It should be run before submitting pull requests.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-frontend/src/main/frontend/README.md#_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nnpx nx prettier:format\n```\n\n----------------------------------------\n\nTITLE: Pattern for Log Line with ISO 8601 Timestamp in Grok\nDESCRIPTION: This pattern matches a syslog message line that includes a full ISO 8601 timestamp followed by the message content. It helps to extract the timestamp and message for further processing or analysis, depending on the message format.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/test/resources/grok/grok_patterns.txt#_snippet_1\n\nLANGUAGE: Grok\nCODE:\n```\nLINE_2 %{SYSLOGBASE_ISO8601}%{GREEDYDATA:message}\n```\n\n----------------------------------------\n\nTITLE: Example Minimal Avro Schema for an IP Record (JSON)\nDESCRIPTION: Provides a basic example of an Avro schema named 'ipRecord' within the 'nifi' namespace. This simple record schema contains only a single string field named 'ip'.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-lookup-services-bundle/nifi-lookup-services/src/main/resources/docs/org.apache.nifi.lookup.maxmind.IPLookupService/additionalDetails.md#_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"name\": \"ipRecord\",\n  \"namespace\": \"nifi\",\n  \"type\": \"record\",\n  \"fields\": [\n    {\n      \"name\": \"ip\",\n      \"type\": \"string\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Invalid XML Example 2 with Error Message\nDESCRIPTION: An example of an invalid XML document where a 'value' element appears directly under 'node', violating the schema which expects a 'subNode' element. The associated validation error is provided.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ValidateXml/additionalDetails.md#_snippet_4\n\nLANGUAGE: xml\nCODE:\n```\n<ns:bundle xmlns:ns=\"http://namespace/1\">\n    <node>\n        <value>Hello World!</value>\n    </node>\n</ns:bundle>\n```\n\nLANGUAGE: text\nCODE:\n```\nvalidatexml.invalid.error: cvc-complex-type.2.4.a: Invalid content was found starting with element 'value'. One of '{subNode}' is expected.\n```\n\n----------------------------------------\n\nTITLE: Generated Zendesk Request Object (Constant Mapping) - JSON\nDESCRIPTION: Illustrates the JSON structure of the Zendesk API request payload generated by the sink when processor attributes are configured using constant string values.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-zendesk-bundle/nifi-zendesk-services/src/main/resources/docs/org.apache.nifi.services.zendesk.ZendeskRecordSink/additionalDetails.md#_snippet_2\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"comment\": {\n    \"body\": \"Sample description\"\n  },\n  \"subject\": \"Sample subject\",\n  \"priority\": \"High\",\n  \"type\": \"Sample type\"\n}\n```\n\n----------------------------------------\n\nTITLE: Building NiFi Toolkit Assembly with Maven\nDESCRIPTION: This Maven command compiles and installs the nifi-toolkit-assembly module, skipping tests, with parallel execution. It sets up the NiFi toolkit environment required for testing.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-integration-tests/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nmvn -pl nifi-toolkit/nifi-toolkit-assembly -am install -T1C -DskipTests\n```\n\n----------------------------------------\n\nTITLE: JSON Output Example for Valid Records (Content-based)\nDESCRIPTION: Expected JSON output for records routed to the 'valid' relationship when using content-based validation. Contains records with numberOfTrains >= 0.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-scripting-bundle/nifi-scripting-processors/src/main/resources/docs/org.apache.nifi.processors.script.ScriptedValidateRecord/additionalDetails.md#_snippet_5\n\nLANGUAGE: JSON\nCODE:\n```\n[\n  {\n    \"company\": \"Boston & Maine Railroad\",\n    \"numberOfTrains\": 3\n  },\n  {\n    \"company\": \"Pennsylvania Railroad\",\n    \"numberOfTrains\": 2\n  },\n  {\n    \"company\": \"Reading Railroad\",\n    \"numberOfTrains\": 4\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Example JSON Record Structure for CalculateRecordStats in Apache NiFi\nDESCRIPTION: A sample JSON record structure demonstrating the format that the CalculateRecordStats processor can analyze. This example shows a simple record with sport and name fields that could be counted using record paths.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.CalculateRecordStats/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"sport\": \"Soccer\",\n  \"name\": \"John Smith\"\n}\n```\n\n----------------------------------------\n\nTITLE: Building Minifi Docker Module with Maven\nDESCRIPTION: This command builds and installs the minifi-minifi-docker module, skipping tests, with the docker profile enabled. It prepares Docker-related components for testing.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-integration-tests/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nmvn -pl minifi/minifi-docker -am install -T1C -DskipTests -P docker\n```\n\n----------------------------------------\n\nTITLE: Building the NiFi Registry Docker Image\nDESCRIPTION: This snippet shows how to build the Docker image for Apache NiFi Registry using the docker build command, optionally specifying a particular version with a build argument. It labels the image as 'apache/nifi-registry:latest'.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-core/nifi-registry-docker/dockerhub/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n$ docker build -t apache/nifi-registry:latest .\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ docker build --build-arg NIFI_REGISTRY_VERSION={Desired NiFi Registry Version} -t apache/nifi-registry:latest .\n```\n\n----------------------------------------\n\nTITLE: Configure NiFi Python Controller Debugging Properties\nDESCRIPTION: Properties to be added to the nifi.properties file to enable and configure remote debugging for the main NiFi Python controller process that discovers and creates Processors. DebugPy is used for this purpose.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-framework-bundle/nifi-framework-extensions/nifi-py4j-framework-bundle/README.md#_snippet_0\n\nLANGUAGE: Properties\nCODE:\n```\nnifi.python.controller.debugpy.enabled=true\n# Indicates whether DebugPy should be used when launching the Controller.\n# Defaults to false.\n```\n\nLANGUAGE: Properties\nCODE:\n```\nnifi.python.controller.debugpy.port=5678\n# The local port for DebugPy to listen on.\n# Defaults to 5678.\n```\n\nLANGUAGE: Properties\nCODE:\n```\nnifi.python.controller.debugpy.host=localhost\n# The hostname for DebugPy to listen on.\n# Defaults to localhost.\n```\n\nLANGUAGE: Properties\nCODE:\n```\nnifi.python.controller.debugpy.logs.directory=./logs\n# The directory to write DebugPy logs to.\n# Defaults to ./logs.\n```\n\n----------------------------------------\n\nTITLE: Building NiFi UI Project - Shell\nDESCRIPTION: Builds the NiFi UI project for production or distribution. The resulting build artifacts, including compiled code and assets, are stored within the `dist/` directory. This command prepares the application for deployment.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-frontend/src/main/frontend/README.md#_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nnpx nx build\n```\n\n----------------------------------------\n\nTITLE: Running NiFi UI Unit Tests - Shell\nDESCRIPTION: Executes the unit tests for the NiFi UI project using the Jest test runner configured via Nx. This command is used to verify the correct functionality of individual code units and components.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-frontend/src/main/frontend/README.md#_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nnpx nx test\n```\n\n----------------------------------------\n\nTITLE: Executing Minifi Integration Tests with Maven\nDESCRIPTION: This command runs the integration tests for Minifi by verifying the build with the Docker profile, referencing the specific POM file. It performs the actual testing process.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-integration-tests/README.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nmvn verify -Pdocker -f minifi/minifi-integration-tests/pom.xml\n```\n\n----------------------------------------\n\nTITLE: Maven Integration Tests with Non-Docker Elasticsearch\nDESCRIPTION: This command executes the integration tests with a non-Docker version of Elasticsearch. It disables Testcontainers by setting 'elasticsearch.testcontainers.enabled' to false and sets the Elasticsearch 'elastic' user's password.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-elasticsearch-bundle/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nmvn -Pintegration-tests --fail-at-end -Delasticsearch.testcontainers.enabled=false -Delasticsearch.elastic_user.password=s3cret1234 clean install\n```\n\n----------------------------------------\n\nTITLE: JSON Output Example for Invalid Records (Content-based)\nDESCRIPTION: Expected JSON output for records routed to the 'invalid' relationship when using content-based validation. Contains the record with numberOfTrains < 0.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-scripting-bundle/nifi-scripting-processors/src/main/resources/docs/org.apache.nifi.processors.script.ScriptedValidateRecord/additionalDetails.md#_snippet_6\n\nLANGUAGE: JSON\nCODE:\n```\n[\n  {\n    \"company\": \"Chesapeake & Ohio Railroad\",\n    \"numberOfTrains\": -1\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Checking NiFi UI Formatting with Prettier - Shell\nDESCRIPTION: Executes Prettier to check the formatting of the NiFi UI codebase against the configured formatting rules. This command identifies files or code sections that do not conform to the standard style.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-frontend/src/main/frontend/README.md#_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nnpx nx prettier\n```\n\n----------------------------------------\n\nTITLE: Setting Testcontainers Docker Socket Override (Mac)\nDESCRIPTION: This command sets the TESTCONTAINERS_DOCKER_SOCKET_OVERRIDE environment variable to point to the Docker Unix Socket location. It's necessary for Testcontainers to find the Docker socket on macOS.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-elasticsearch-bundle/README.md#_snippet_4\n\nLANGUAGE: sh\nCODE:\n```\nexport TESTCONTAINERS_DOCKER_SOCKET_OVERRIDE=/var/run/docker.sock\n```\n\n----------------------------------------\n\nTITLE: Color to Fruit Price Mapping in Plain Text\nDESCRIPTION: A simple data mapping showing colors associated with fruits and their prices. Each line contains a color, followed by the price ($1), and the fruit name.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/test/resources/TestReplaceTextWithMapping/color-fruit-escaped-dollar-mapping.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nred \t\t\\$1 apple\norange \t\t\\$1 orange\nyellow \t\t\\$1 banana\ngreen\t\t\\$1 grape\nblue\t\t\\$1 blueberry\npurple\t\t\\$1 plum\n```\n\n----------------------------------------\n\nTITLE: Parsing CEF Log Entries with Python\nDESCRIPTION: This snippet demonstrates how to parse CEF (Common Event Format) log entries using Python. It extracts fields like hostname, device event class ID, and message details from raw log strings, which can be used for security event analysis and monitoring. Dependencies include the 're' module for regular expressions, and the input is a CEF log string; the output is a structured dictionary containing parsed data.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/test/resources/cef/single-row-with-extensions.txt#_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport re\n\ndef parse_cef(log_entry):\n    pattern = r\"CEF:0\\|(?P<device_vendor>[^\\|]+)\\|(?P<device_product>[^\\|]+)\\|(?P<device_version>[^\\|]+)\\|(?P<name>[^\\|]+)\\|(?P<severity>[^\\|]+)\\|(?P<extension>.+)\"\n    match = re.match(pattern, log_entry)\n    if match:\n        return match.groupdict()\n    else:\n        return None\n```\n\n----------------------------------------\n\nTITLE: Running the Apache NiFi Release Script in Bash\nDESCRIPTION: Command to execute the NiFi release script with sample parameters. The script automates the release process with specified version numbers, RC number, and build number.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-framework-bundle/nifi-framework/nifi-framework-cluster/src/test/resources/org/apache/nifi/cluster/firewall/impl/empty.txt#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n./nifi_release.sh <version-to-release> <version-to-release> <rc-number> <build-number>\n```\n\n----------------------------------------\n\nTITLE: Color-Fruit Mapping Table in Plain Text\nDESCRIPTION: A simple tab-separated table that maps colors to fruits. Each row contains a color in the first column and its corresponding fruit in the second column.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/test/resources/TestReplaceTextWithMapping/color-fruit-mapping.txt#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nred \t\tapple\norange \t\torange\nyellow \t\tbanana\ngreen\t\tgrape\nblue\t\tblueberry\npurple\t\tplum\n```\n\n----------------------------------------\n\nTITLE: Getting FlowFile Attribute Value in Groovy\nDESCRIPTION: This code shows how to retrieve a FlowFile attribute value using direct property access. The value will be returned as a String.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-groovyx-bundle/nifi-groovyx-processors/src/main/resources/docs/org.apache.nifi.processors.groovyx.ExecuteGroovyScript/additionalDetails.md#_snippet_3\n\nLANGUAGE: Groovy\nCODE:\n```\nString a = flowFile.ATTRIBUTE_NAME\n```\n\n----------------------------------------\n\nTITLE: Parsing an Array of XML Records with XMLReader in Apache NiFi\nDESCRIPTION: Example of an XML structure containing multiple records enclosed in a root tag. This format is used when the 'Expect Records as Array' property is set to 'true'.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-services/nifi-record-serialization-services-bundle/nifi-record-serialization-services/src/main/resources/docs/org.apache.nifi.xml.XMLReader/additionalDetails.md#_snippet_1\n\nLANGUAGE: xml\nCODE:\n```\n<root>\n    <record>\n        <field1>content</field1>\n        <field2>content</field2>\n    </record>\n    <record>\n        <field1>content</field1>\n        <field2>content</field2>\n    </record>\n</root>\n```\n\n----------------------------------------\n\nTITLE: Building Apache NiFi Registry Toolkit using Maven\nDESCRIPTION: This command builds the Apache NiFi Registry Toolkit using Maven.  It cleans the project and then installs the dependencies and builds the package.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-toolkit/README.md#_snippet_0\n\nLANGUAGE: Shell\nCODE:\n```\nmvn clean install\n```\n\n----------------------------------------\n\nTITLE: Example RecordSet Structure in GetWorkdayReport Processor\nDESCRIPTION: Sample record structure showing how data is represented in the GetWorkdayReport processor with two records containing name-value pairs.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-workday-bundle/nifi-workday-processors/src/main/resources/docs/org.apache.nifi.processors.workday.GetWorkdayReport/additionalDetails.md#_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nRecordSet (\n  Record (\n    Field \"name1\" = \"value1\",\n    Field \"name2\" = 42\n  ),\n  Record (\n    Field \"name1\" = \"value2\",\n    Field \"name2\" = 84\n  )\n)\n```\n\n----------------------------------------\n\nTITLE: Comparison of 'woodchuck' and 'wood' objects in Java\nDESCRIPTION: This snippet performs a series of conditional checks to compare 'woodchuck' and 'wood' objects, demonstrating repetitive if-else structures for object equality evaluation. It emphasizes the logic of nested comparisons and the use of 'if' statements to control flow based on object identity or values, with a focus on recurring patterns around 'woodâ€™ and 'woodchuck'. Dependencies include Java's object equality mechanisms. It operates on object references and outputs or processes based on these conditions.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/test/resources/CharacterSetConversionSamples/Converted2.txt#_snippet_0\n\nLANGUAGE: Java\nCODE:\n```\nif (a == woodchuck) {\n    // process code\n} else if (a == wood) {\n    // process code\n} else if (a == woodchuck) {\n    // process code\n} // ...additional nested if-else blocks for various comparisons\n\n```\n\n----------------------------------------\n\nTITLE: Sample Incoming Record Structure - JSON\nDESCRIPTION: Shows an example of the incoming record structure expected by the ZendeskRecordSink processor. Properties within this record can be mapped to Zendesk ticket attributes using Record Path expressions.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-zendesk-bundle/nifi-zendesk-services/src/main/resources/docs/org.apache.nifi.services.zendesk.ZendeskRecordSink/additionalDetails.md#_snippet_0\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"record\": {\n    \"description\": \"This is a sample description.\",\n    \"issue\\_type\": \"Immediate\",\n    \"issue\": {\n      \"name\": \"General error\",\n      \"type\": \"Immediate\"\n    },\n    \"project\": {\n      \"name\": \"Maintenance\"\n    }\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Querying Controller Services with minifi.sh\nDESCRIPTION: This command retrieves the health and bulletins of all Controller Services within the MiNiFi instance using the `flowStatus` command.  It leverages the `controllerservices` flag and specifies the desired options (health, bulletins) separated by commas.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-docs/src/main/markdown/System_Admin_Guide.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nminifi.sh flowStatus controllerservices:health,bulletins\n```\n\n----------------------------------------\n\nTITLE: Stopping MiNiFi Using Command-Line Scripts\nDESCRIPTION: This snippet provides instructions for stopping the MiNiFi agent either directly or as a service. It involves navigating to the installation directory and executing the stop command, or using system service commands to stop MiNiFi gracefully.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-docs/src/main/markdown/minifi-java-agent-quick-start.md#_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nbin/minifi.sh stop\n\n# To stop MiNiFi as a system service:\nsudo service minifi stop\n```\n\n----------------------------------------\n\nTITLE: Building Apache NiFi Docker Image\nDESCRIPTION: Commands for building the Apache NiFi Docker image from source, with options to specify different NiFi versions using build arguments.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-docker/dockerhub/README.md#_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ndocker build -t apache/nifi:latest .\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker build --build-arg=NIFI_VERSION={Desired NiFi Version} -t apache/nifi:latest .\n```\n\n----------------------------------------\n\nTITLE: Create Directory and Unzip AWS Extensions\nDESCRIPTION: This set of commands creates a directory and unzips the built AWS extensions into it, making them available to NiFi Registry. The `mkdir -p` command creates the directory, and the `unzip -d` command extracts the contents of the zip file into the specified directory.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-extensions/nifi-registry-aws/nifi-registry-aws-assembly/README.md#_snippet_3\n\nLANGUAGE: Bash\nCODE:\n```\nmkdir -p ${NIFI_REG_HOME}/ext/aws\nunzip -d ${NIFI_REG_HOME}/ext/aws nifi-registry-extensions/nifi-registry-aws/nifi-registry-aws-assembly/target/nifi-registry-aws-assembly-xxx-bin.zip\n```\n\n----------------------------------------\n\nTITLE: Fixing NiFi UI Lint Errors - Shell\nDESCRIPTION: Automatically attempts to fix linting errors detected in the NiFi UI codebase. This command helps in quickly resolving common code style and quality issues flagged by the linter, reducing manual correction effort.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-frontend/src/main/frontend/README.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nnpx nx lint:fix\n```\n\n----------------------------------------\n\nTITLE: Creating a JAR File with the jar command\nDESCRIPTION: This command creates a JAR file named jasn1-examples.jar containing the compiled test classes, essential for JASN1Reader testing in NiFi. It navigates into the target/test-classes directory and adds all files found there to the JAR. This command requires the jar command (part of the JDK) to be available in the system's PATH.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-asn1-bundle/nifi-asn1-services/src/test/resources/README.md#_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ jar -cvf /tmp/jasn1-examples.jar -C nifi-nar-bundles/nifi-asn1-bundle/nifi-asn1-services/target/test-classes/ .\n```\n\n----------------------------------------\n\nTITLE: Setting output directory for key generation - Shell\nDESCRIPTION: This snippet switches to an output directory and creates a final output directory named 'keys'. This separates the intermediate steps from the final key files.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-registry/nifi-registry-core/nifi-registry-web-api/src/test/resources/keys/README.md#_snippet_2\n\nLANGUAGE: Shell\nCODE:\n```\n# switch to output directory, create final output directory\ncd \"$WD\"\nmkdir keys\n```\n\n----------------------------------------\n\nTITLE: Navigating to the Local Binary Distribution Directory - Shell\nDESCRIPTION: Changes directory to the unpacked NiFi binary distribution for local development or testing. Uses wildcard expansion to select the latest build artifacts. Requires that binaries have been built and extracted by a previous step.\nSOURCE: https://github.com/apache/nifi/blob/main/README.md#_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ncd nifi-assembly/target/nifi-*-bin/nifi-*/\n```\n\n----------------------------------------\n\nTITLE: Providing Sample Input Data for ForkRecord Processor - JSON\nDESCRIPTION: This snippet supplies a representative JSON array of user records designed as an input example for the ForkRecord processor. Each object in the array contains user details as well as an 'accounts' array, which itself is an array of account objects. This structural pattern is crucial for demonstrating the extraction of nested records using a RecordPath. No dependencies are required, but NiFi's ForkRecord processor expects matching input and output schemas as set via its controller services. Inputs are the JSON objects as shown; outputs depend on processor configuration.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ForkRecord/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"id\": 1,\n    \"name\": \"John Doe\",\n    \"address\": \"123 My Street\",\n    \"city\": \"My City\",\n    \"state\": \"MS\",\n    \"zipCode\": \"11111\",\n    \"country\": \"USA\",\n    \"accounts\": [\n      {\n        \"id\": 42,\n        \"balance\": 4750.89\n      },\n      {\n        \"id\": 43,\n        \"balance\": 48212.38\n      }\n    ]\n  },\n  {\n    \"id\": 2,\n    \"name\": \"Jane Doe\",\n    \"address\": \"345 My Street\",\n    \"city\": \"Her City\",\n    \"state\": \"NY\",\n    \"zipCode\": \"22222\",\n    \"country\": \"USA\",\n    \"accounts\": [\n      {\n        \"id\": 45,\n        \"balance\": 6578.45\n      },\n      {\n        \"id\": 46,\n        \"balance\": 34567.21\n      }\n    ]\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Example Input JSON Data for PartitionRecord\nDESCRIPTION: This JSON snippet provides sample data used in the examples to demonstrate the functionality of the PartitionRecord processor. It represents a list of records, each containing nested structures and arrays, allowing illustration of RecordPath usage for partitioning.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.PartitionRecord/additionalDetails.md#_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n  {\n    \"name\": \"John Doe\",\n    \"dob\": \"11/30/1976\",\n    \"favorites\": [\n      \"spaghetti\",\n      \"basketball\",\n      \"blue\"\n    ],\n    \"locations\": {\n      \"home\": {\n        \"number\": 123,\n        \"street\": \"My Street\",\n        \"city\": \"New York\",\n        \"state\": \"NY\",\n        \"country\": \"US\"\n      },\n      \"work\": {\n        \"number\": 321,\n        \"street\": \"Your Street\",\n        \"city\": \"New York\",\n        \"state\": \"NY\",\n        \"country\": \"US\"\n      }\n    }\n  },\n  {\n    \"name\": \"Jane Doe\",\n    \"dob\": \"10/04/1979\",\n    \"favorites\": [\n      \"spaghetti\",\n      \"football\",\n      \"red\"\n    ],\n    \"locations\": {\n      \"home\": {\n        \"number\": 123,\n        \"street\": \"My Street\",\n        \"city\": \"New York\",\n        \"state\": \"NY\",\n        \"country\": \"US\"\n      },\n      \"work\": {\n        \"number\": 456,\n        \"street\": \"Our Street\",\n        \"city\": \"New York\",\n        \"state\": \"NY\",\n        \"country\": \"US\"\n      }\n    }\n  },\n  {\n    \"name\": \"Jacob Doe\",\n    \"dob\": \"04/02/2012\",\n    \"favorites\": [\n      \"chocolate\",\n      \"running\",\n      \"yellow\"\n    ],\n    \"locations\": {\n      \"home\": {\n        \"number\": 123,\n        \"street\": \"My Street\",\n        \"city\": \"New York\",\n        \"state\": \"NY\",\n        \"country\": \"US\"\n      },\n      \"work\": null\n    }\n  },\n  {\n    \"name\": \"Janet Doe\",\n    \"dob\": \"02/14/2007\",\n    \"favorites\": [\n      \"spaghetti\",\n      \"reading\",\n      \"white\"\n    ],\n    \"locations\": {\n      \"home\": {\n        \"number\": 1111,\n        \"street\": \"Far Away\",\n        \"city\": \"San Francisco\",\n        \"state\": \"CA\",\n        \"country\": \"US\"\n      },\n      \"work\": null\n    }\n  }\n]\n```\n\n----------------------------------------\n\nTITLE: Sample Input Data for ExtractText Processor\nDESCRIPTION: A sample multiline string used as input for the NiFi ExtractText processor examples. This data contains carriage return and newline characters (`\\r\\n`) separating words, which is relevant for regex matching behavior, especially with the `(?s)` flag.\nSOURCE: https://github.com/apache/nifi/blob/main/nifi-extension-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/resources/docs/org.apache.nifi.processors.standard.ExtractText/additionalDetails.md#_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n`foo\\r\\nbar1\\r\\nbar2\\r\\nbar3\\r\\nhello\\r\\nworld\\r\\n`\n```\n\n----------------------------------------\n\nTITLE: Accessing NiFi Web Interface - HTTPS URL Example\nDESCRIPTION: Provides the default HTTPS URL to access the NiFi web interface after startup. Assumes the application is running on localhost and has generated a self-signed certificate. Browsers may display security warnings until a trusted certificate is configured.\nSOURCE: https://github.com/apache/nifi/blob/main/README.md#_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nhttps://localhost:8443/nifi\n```\n\n----------------------------------------\n\nTITLE: Apache License Header Comment in HTML\nDESCRIPTION: Standard Apache License 2.0 header comment that indicates the licensing terms under which the software is distributed, including copyright notice and license reference.\nSOURCE: https://github.com/apache/nifi/blob/main/minifi/minifi-docker/README.md#_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!--\n  Licensed to the Apache Software Foundation (ASF) under one or more\n  contributor license agreements.  See the NOTICE file distributed with\n  this work for additional information regarding copyright ownership.\n  The ASF licenses this file to You under the Apache License, Version 2.0\n  (the \"License\"); you may not use this file except in compliance with\n  the License.  You may obtain a copy of the License at\n      http://www.apache.org/licenses/LICENSE-2.0\n  Unless required by applicable law or agreed to in writing, software\n  distributed under the License is distributed on an \"AS IS\" BASIS,\n  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n  See the License for the specific language governing permissions and\n  limitations under the License.\n-->\n```"
  }
]
[
  {
    "owner": "mattpocock",
    "repo": "evalite",
    "content": "TITLE: Creating Basic Evalite Test with Levenshtein Scorer in TypeScript\nDESCRIPTION: This example demonstrates creating a basic evaluation in Evalite using a .eval.ts file. It sets up test data, defines a task function that appends text, and uses the Levenshtein distance scorer to measure the similarity between the task output and the expected result.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/src/content/docs/what-is-evalite.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// my-eval.eval.ts\n\nimport { evalite } from \"evalite\";\nimport { Levenshtein } from \"autoevals\";\n\nevalite(\"My Eval\", {\n  // A set of data to test\n  data: async () => {\n    return [{ input: \"Hello\", expected: \"Hello World!\" }];\n  },\n  // The task to perform, usually to call a LLM.\n  task: async (input) => {\n    return input + \" World!\";\n  },\n  // Some methods to score the eval\n  scorers: [\n    // For instance, Levenshtein distance measures\n    // the similarity between two strings\n    Levenshtein,\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Stream Processing with Evalite and AI SDK\nDESCRIPTION: Example showing how to evaluate streaming responses using Evalite with the AI SDK's streamText function. The code demonstrates testing an AI model's response to a question about France's capital using the Factuality scorer.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/src/content/docs/guides/streams.md#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { evalite } from \"evalite\";\nimport { streamText } from \"ai\";\nimport { openai } from \"@ai-sdk/openai\";\nimport { Factuality } from \"autoevals\";\n\nevalite(\"My Eval\", {\n  data: async () => {\n    return [{ input: \"What is the capital of France?\", expected: \"Paris\" }];\n  },\n  task: async (input) => {\n    const result = await streamText({\n      model: openai(\"your-model\"),\n      system: `Answer the question concisely.`,\n      prompt: input,\n    });\n\n    return result.textStream;\n  },\n  scorers: [Factuality],\n});\n```\n\n----------------------------------------\n\nTITLE: Testing Capital Cities with Evalite and AI SDK\nDESCRIPTION: Implements an automated test using Evalite and Vercel's AI SDK to verify AI model responses for capital city queries. The code sets up a test framework that checks the accuracy of responses using Factuality and Levenshtein distance scorers. It includes configuration for the OpenAI model with specific system prompts for formatting responses.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/src/content/docs/examples/ai-sdk.md#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\n// my-eval.eval.ts\n\nimport { openai } from \"@ai-sdk/openai\";\nimport { streamText } from \"ai\";\nimport { Factuality, Levenshtein } from \"autoevals\";\nimport { evalite } from \"evalite\";\nimport { traceAISDKModel } from \"evalite/ai-sdk\";\n\nevalite(\"Test Capitals\", {\n  data: async () => [\n    {\n      input: `What's the capital of France?`,\n      expected: `Paris`,\n    },\n    {\n      input: `What's the capital of Germany?`,\n      expected: `Berlin`,\n    },\n  ],\n  task: async (input) => {\n    const result = await streamText({\n      model: traceAISDKModel(openai(\"gpt-4o-mini\")),\n      system: `\n        Answer the question concisely. Answer in as few words as possible.\n        Remove full stops from the end of the output.\n        If the country has no capital, return '<country> has no capital'.\n        If the country does not exist, return 'Unknown'.\n      `,\n      prompt: input,\n    });\n\n    return result.textStream;\n  },\n  scorers: [Factuality, Levenshtein],\n});\n```\n\n----------------------------------------\n\nTITLE: LLM-as-a-Judge Scorer Implementation\nDESCRIPTION: Complex implementation of a factuality scorer using OpenAI's GPT-4 model to evaluate answer accuracy against ground truth.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/src/content/docs/guides/scorers.mdx#2025-04-22_snippet_5\n\nLANGUAGE: typescript\nCODE:\n```\nimport { openai } from \"@ai-sdk/openai\";\nimport { generateObject } from \"ai\";\nimport { createScorer } from \"evalite\";\nimport { z } from \"zod\";\n\nexport const Factuality = createScorer<string, string, string>({\n  name: \"Factuality\",\n  scorer: async ({ input, expected, output }) => {\n    return checkFactuality({\n      question: input,\n      groundTruth: expected!,\n      submission: output,\n    });\n  },\n});\n\nconst checkFactuality = async (opts: {\n  question: string;\n  groundTruth: string;\n  submission: string;\n}) => {\n  const { object } = await generateObject({\n    model: openai(\"gpt-4o-2024-11-20\"),\n    prompt: `\n      You are comparing a submitted answer to an expert answer on a given question. Here is the data:\n      [BEGIN DATA]\n      ************\n      [Question]: ${opts.question}\n      ************\n      [Expert]: ${opts.groundTruth}\n      ************\n      [Submission]: ${opts.submission}\n      ************\n      [END DATA]\n\n      Compare the factual content of the submitted answer with the expert answer. Ignore any differences in style, grammar, or punctuation.\n      The submitted answer may either be a subset or superset of the expert answer, or it may conflict with it. Determine which case applies. Answer the question by selecting one of the following options:\n      (A) The submitted answer is a subset of the expert answer and is fully consistent with it.\n      (B) The submitted answer is a superset of the expert answer and is fully consistent with it.\n      (C) The submitted answer contains all the same details as the expert answer.\n      (D) There is a disagreement between the submitted answer and the expert answer.\n      (E) The answers differ, but these differences don't matter from the perspective of factuality.\n    `,\n    schema: z.object({\n      answer: z.enum([\"A\", \"B\", \"C\", \"D\", \"E\"]).describe(\"Your selection.\"),\n      rationale: z\n        .string()\n        .describe(\"Why you chose this answer. Be very detailed.\"),\n    }),\n  });\n\n  const scores = {\n    A: 0.4,\n    B: 0.6,\n    C: 1,\n    D: 0,\n    E: 1,\n  };\n\n  return {\n    score: scores[object.answer],\n    metadata: {\n      rationale: object.rationale,\n    },\n  };\n};\n\nevalite(\"Factuality\", {\n  data: async () => {\n    return [\n      {\n        input: \"What is the capital of France?\",\n        expected: \"Paris\",\n      },\n    ];\n  },\n  task: async (input) => {\n    return (\n      \"The capital of France is a city that starts \" +\n      \"with a letter P, and ends in 'aris'.\"\n    );\n  },\n  scorers: [Factuality],\n});\n```\n\n----------------------------------------\n\nTITLE: Customizing Columns in Evalite UI with TypeScript\nDESCRIPTION: This example demonstrates how to use the experimental_customColumns property to define custom columns in the Evalite UI. The function allows you to select specific properties from your input data and customize how results are displayed, replacing the default Input/Expected/Output columns with your own defined structure.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/src/content/docs/guides/customizing-the-ui.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { evalite } from \"evalite\";\n\nevalite(\"My Eval\", {\n  data: async () => {\n    return [\n      { input: { a: 1, b: 2, c: 3, theOnlyPropertyWeWantToShow: \"Hello\" } },\n    ];\n  },\n  task: async (input) => {\n    return input.theOnlyPropertyWeWantToShow + \" World!\";\n  },\n  scorers: [],\n  experimental_customColumns: async (result) => {\n    return [\n      {\n        label: \"Custom Input\",\n        value: result.input.theOnlyPropertyWeWantToShow, // \"Hello\"\n      },\n      {\n        label: \"Output\",\n        value: result.output, // \"Hello World!\"\n      },\n    ];\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Reporting LLM Traces Manually with reportTrace in Evalite\nDESCRIPTION: Example showing how to manually report traces for LLM calls in an Evalite evaluation. The code tracks start and end times of the LLM call, captures the input and output, and reports token usage metrics.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/src/content/docs/guides/traces.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { evalite, type Evalite } from \"evalite\";\nimport { reportTrace } from \"evalite/traces\";\n\nevalite(\"My Eval\", {\n  data: async () => {\n    return [{ input: \"Hello\", expected: \"Hello World!\" }];\n  },\n  task: async (input) => {\n    // Track the start time\n    const start = performance.now();\n\n    // Call our LLM\n    const result = await myLLMCall();\n\n    // Report the trace once it's finished\n    reportTrace({\n      start,\n      end: performance.now(),\n      output: result.output,\n      input: [\n        {\n          role: \"user\",\n          content: input,\n        },\n      ],\n      usage: {\n        completionTokens: result.completionTokens,\n        promptTokens: result.promptTokens,\n      },\n    });\n\n    // Return the output\n    return result.output;\n  },\n  scorers: [Levenshtein],\n});\n```\n\n----------------------------------------\n\nTITLE: Tracing Vercel AI SDK Models with traceAISDKModel in Evalite\nDESCRIPTION: Example demonstrating how to automatically trace all calls to a Vercel AI SDK model by wrapping it with the traceAISDKModel function. This approach simplifies the integration of tracing with the Vercel AI SDK.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/src/content/docs/guides/traces.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { traceAISDKModel } from \"evalite/ai-sdk\";\nimport { generateText } from \"ai\";\nimport { openai } from \"@ai-sdk/openai\";\n\n// All calls to this model will be recorded in evalite!\nconst tracedModel = traceAISDKModel(openai(\"gpt-4o-mini\"));\n\nconst result = await generateText({\n  model: tracedModel,\n  system: `Answer the question concisely.`,\n  prompt: `What is the capital of France?`,\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Inline Scorer in Evalite\nDESCRIPTION: Demonstrates how to create an inline scorer that checks for specific text content in the output. The scorer returns 1 if the word 'Paris' is found, 0 otherwise.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/src/content/docs/guides/scorers.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { evalite } from \"evalite\";\n\nevalite(\"My Eval\", {\n  data: async () => {\n    return [{ input: \"Hello\" }];\n  },\n  task: async (input) => {\n    return input + \" World!\";\n  },\n  scorers: [\n    {\n      name: \"Contains Paris\",\n      description: \"Checks if the output contains the word 'Paris'.\",\n      scorer: ({ output }) => {\n        return output.includes(\"Paris\") ? 1 : 0;\n      },\n    },\n  ],\n});\n```\n\n----------------------------------------\n\nTITLE: Creating Reusable Scorer in Evalite\nDESCRIPTION: Shows how to create a reusable scorer using createScorer function that can be used across multiple evaluations.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/src/content/docs/guides/scorers.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createScorer } from \"evalite\";\n\nconst containsParis = createScorer<string, string>({\n  name: \"Contains Paris\",\n  description: \"Checks if the output contains the word 'Paris'.\",\n  scorer: ({ output }) => {\n    return output.includes(\"Paris\") ? 1 : 0;\n  },\n});\n\nevalite(\"My Eval\", {\n  data: async () => {\n    return [{ input: \"Hello\" }];\n  },\n  task: async (input) => {\n    return input + \" World!\";\n  },\n  scorers: [containsParis],\n});\n```\n\n----------------------------------------\n\nTITLE: Using Uint8Array (Buffer) Objects in Evalite\nDESCRIPTION: Comprehensive example showing how to use Uint8Array/Buffer objects in various parts of an Evalite evaluation, including inputs, expected outputs, traces, task returns, and custom columns.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/src/content/docs/guides/multi-modal.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { evalite } from \"evalite\";\nimport { reportTrace } from \"evalite/traces\";\n\nevalite(\"My Eval\", {\n  data: async () => {\n    return [\n      {\n        // 1. In inputs...\n        input: readFileSync(\"path/to/file.jpg\"),\n        // 2. ...in expected...\n        expected: readFileSync(\"path/to/file.jpg\"),\n      },\n    ];\n  },\n  task: async (input) => {\n    reportTrace({\n      // 3. ...in traces...\n      input: readFileSync(\"path/to/file.jpg\"),\n      output: readFileSync(\"path/to/file.jpg\"),\n    });\n\n    // 4. ...returned from the task itself...\n    return readFileSync(\"path/to/new-file.jpg\", input);\n  },\n  experimental_customColumns: async () => {\n    return [\n      {\n        label: \"File\",\n        // 5. ...and returned from customColumns:\n        value: readFileSync(\"path/to/new-file.jpg\", input),\n      },\n    ];\n  },\n  scorers: [],\n});\n```\n\n----------------------------------------\n\nTITLE: Implementing Test Skipping in Evalite with TypeScript\nDESCRIPTION: This example demonstrates how to use the experimental_skip modifier in Evalite to define a test that will be skipped during execution. The test includes a data provider and task function but will not be executed when tests are run.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/src/content/docs/guides/skipping.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nevalite.experimental_skip(\"My Eval\", {\n  data: () => [],\n  task: () => {},\n});\n```\n\n----------------------------------------\n\nTITLE: Setting Score Threshold in Evalite\nDESCRIPTION: Shows how to set a minimum score threshold for evaluations. If evaluations score below this threshold, the process will exit with an error code, making it useful for CI environments.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/src/content/docs/guides/cli.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nevalite --threshold=50 # Score must be greater than or equal to 50\n\nevalite watch --threshold=70 # Also works in watch mode\n```\n\n----------------------------------------\n\nTITLE: Referencing Files on Disk Using EvaliteFile.fromPath\nDESCRIPTION: Shows how to reference files directly from disk without loading them into memory using the EvaliteFile.fromPath method in Evalite, which allows displaying media in the UI without memory overhead.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/src/content/docs/guides/multi-modal.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { EvaliteFile, evalite } from \"evalite\";\n\nevalite(\"My Eval\", {\n  data: async () => {\n    return [\n      {\n        input: EvaliteFile.fromPath(\"path/to/file.jpg\"),\n      },\n    ];\n  },\n  task: async (input) => {\n    console.log(input.path); // \"path/to/file.jpg\"\n  },\n  scorers: [],\n});\n```\n\n----------------------------------------\n\nTITLE: Creating First Evalite Test\nDESCRIPTION: Example of creating an evaluation test file using Evalite with Levenshtein scoring method. Demonstrates setup of test data, task execution, and scoring configuration.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/src/content/docs/quickstart.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\n// my-eval.eval.ts\n\nimport { evalite } from \"evalite\";\nimport { Levenshtein } from \"autoevals\";\n\nevalite(\"My Eval\", {\n  // A function that returns an array of test data\n  // - TODO: Replace with your test data\n  data: async () => {\n    return [{ input: \"Hello\", expected: \"Hello World!\" }];\n  },\n  // The task to perform\n  // - TODO: Replace with your LLM call\n  task: async (input) => {\n    return input + \" World!\";\n  },\n  // The scoring methods for the eval\n  scorers: [Levenshtein],\n});\n```\n\n----------------------------------------\n\nTITLE: Running Evalite in Watch Mode (Bash)\nDESCRIPTION: Demonstrates how to run Evalite in watch mode, which monitors changes to .eval.ts files and automatically re-runs evaluations. It's recommended to implement a caching layer for LLM calls to optimize performance and reduce API usage.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/packages/evalite/readme.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nevalite watch\n```\n\n----------------------------------------\n\nTITLE: Running Evalite in Watch Mode\nDESCRIPTION: Demonstrates how to run Evalite in watch mode, which monitors .eval.ts files for changes and automatically re-runs evaluations when they change.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/src/content/docs/guides/cli.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nevalite watch\n```\n\n----------------------------------------\n\nTITLE: Scorer with Type Arguments\nDESCRIPTION: Shows how to implement a scorer with explicit type arguments for input, output, and expected values.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/src/content/docs/guides/scorers.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createScorer } from \"evalite\";\n\nconst containsParis = createScorer<\n  string, // Type of 'input'\n  string, // Type of 'output'\n  string // Type of 'expected'\n>({\n  name: \"Contains Word\",\n  description: \"Checks if the output contains the specified word.\",\n  scorer: ({ output, input, expected }) => {\n    // output is typed as string!\n    return output.includes(expected) ? 1 : 0;\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Scorer Properties Implementation\nDESCRIPTION: Demonstrates the available properties in a scorer function and their typing.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/src/content/docs/guides/scorers.mdx#2025-04-22_snippet_2\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createScorer } from \"evalite\";\n\nconst containsParis = createScorer<string, string>({\n  name: \"Contains Paris\",\n  description: \"Checks if the output contains the word 'Paris'.\",\n  scorer: ({ input, output, expected }) => {\n    // input comes from `data`\n    // expected also comes from `data`\n    // output is the output of `task`\n    return output.includes(\"Paris\") ? 1 : 0;\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Scorer with Metadata Implementation\nDESCRIPTION: Demonstrates how to include additional metadata with scorer results for debugging and UI display.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/src/content/docs/guides/scorers.mdx#2025-04-22_snippet_4\n\nLANGUAGE: typescript\nCODE:\n```\nimport { createScorer } from \"evalite\";\n\nconst containsParis = createScorer<string>({\n  name: \"Contains Paris\",\n  description: \"Checks if the output contains the word 'Paris'.\",\n  scorer: (output) => {\n    return {\n      score: output.includes(\"Paris\") ? 1 : 0,\n      metadata: {\n        // Can be anything!\n      },\n    };\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Running Specific Evalite Files (Bash)\nDESCRIPTION: Shows how to run specific evaluation files by passing them as arguments to the Evalite command. This can be used for both standard and watch mode execution.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/packages/evalite/readme.md#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nevalite my-eval.eval.ts\n```\n\nLANGUAGE: bash\nCODE:\n```\nevalite watch my-eval.eval.ts\n```\n\n----------------------------------------\n\nTITLE: Running Evalite on Specific Files\nDESCRIPTION: Shows how to run Evalite on specific evaluation files by passing the filename as an argument to the command.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/src/content/docs/guides/cli.mdx#2025-04-22_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nevalite my-eval.eval.ts\n```\n\n----------------------------------------\n\nTITLE: Running Evalite Watch Mode on Specific Files\nDESCRIPTION: Demonstrates how to combine watch mode with specific file targeting, allowing automatic re-evaluation when a specific file changes.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/src/content/docs/guides/cli.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nevalite watch my-eval.eval.ts\n```\n\n----------------------------------------\n\nTITLE: Running Evalite Development Environment\nDESCRIPTION: Command to start the development environment, which runs the TypeScript type checker, tests, UI dev server, and watches for changes in the example packages.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/readme.md#2025-04-22_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npnpm run dev\n```\n\n----------------------------------------\n\nTITLE: Creating Environment Variables File\nDESCRIPTION: Creates a .env file in the project root to store API keys and sensitive information\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/src/content/docs/guides/environment-variables.mdx#2025-04-22_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n// .env\nOPENAI_API_KEY=your-api-key\n```\n\n----------------------------------------\n\nTITLE: Configuring GitIgnore for Environment Files\nDESCRIPTION: Adds .env file to .gitignore to prevent committing sensitive information to version control\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/src/content/docs/guides/environment-variables.mdx#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n// .gitignore\n.env\n```\n\n----------------------------------------\n\nTITLE: Configuring Vite for Environment Variables\nDESCRIPTION: Sets up Vite configuration to include dotenv setup files for testing environment\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/src/content/docs/guides/environment-variables.mdx#2025-04-22_snippet_3\n\nLANGUAGE: typescript\nCODE:\n```\n// vite.config.ts\n\nimport { defineConfig } from \"vite\";\n\nexport default defineConfig({\n  test: {\n    setupFiles: [\"dotenv/config\"],\n  },\n});\n```\n\n----------------------------------------\n\nTITLE: Installing Evalite Dependencies\nDESCRIPTION: Command to install Evalite, Vitest, and the autoevals scoring library using pnpm package manager.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/src/content/docs/quickstart.mdx#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npnpm add -D evalite vitest autoevals\n```\n\n----------------------------------------\n\nTITLE: Configuring Package.json Script\nDESCRIPTION: Adding an eval:dev script to package.json to enable running Evalite in watch mode.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/src/content/docs/quickstart.mdx#2025-04-22_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"scripts\": {\n    \"eval:dev\": \"evalite watch\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Running Evalite Dev Mode\nDESCRIPTION: Command to run Evalite in development mode using pnpm.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/src/content/docs/quickstart.mdx#2025-04-22_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npnpm run eval:dev\n```\n\n----------------------------------------\n\nTITLE: Setting Up Global Evalite Command\nDESCRIPTION: Commands for building the project and linking the Evalite package globally, necessary for using the 'evalite' command in the terminal during development.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/readme.md#2025-04-22_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npnpm build\nnpm link\n```\n\n----------------------------------------\n\nTITLE: Reading Files into Memory in Node.js\nDESCRIPTION: Demonstrates how to read a file into memory as a Buffer using Node.js fs module. The readFileSync function returns a Buffer containing all file information.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/src/content/docs/guides/multi-modal.mdx#2025-04-22_snippet_0\n\nLANGUAGE: typescript\nCODE:\n```\nimport { readFileSync } from \"fs\";\n\nconst fileContents = readFileSync(\"path/to/file.jpg\");\n//    ^ Buffer\n```\n\n----------------------------------------\n\nTITLE: Writing a Buffer to File in Node.js\nDESCRIPTION: Shows how to write a Buffer (file contents in memory) to a new file using the writeFileSync function from the fs module.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/src/content/docs/guides/multi-modal.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { writeFileSync } from \"fs\";\n\nwriteFileSync(\"path/to/new-file.jpg\", fileContents);\n```\n\n----------------------------------------\n\nTITLE: Installing Evalite with Version Threshold\nDESCRIPTION: Demonstrates how to use the --threshold flag when running Evalite to set a minimum score requirement. This is useful for continuous integration scenarios.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/packages/evalite/CHANGELOG.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nevalite --threshold=50 # Score must be greater than or equal to 50\n\nevalite watch --threshold=70 # Also works in watch mode\n```\n\n----------------------------------------\n\nTITLE: Installing DotEnv Package\nDESCRIPTION: Command to install dotenv package as a development dependency for environment variable management\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/src/content/docs/guides/environment-variables.mdx#2025-04-22_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npnpm add -D dotenv\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for Evalite Development\nDESCRIPTION: Instructions for creating a .env file in the packages/example directory with an OpenAI API key, which is required for local development of the Evalite project.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/readme.md#2025-04-22_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nOPENAI_API_KEY=your-api-key\n```\n\n----------------------------------------\n\nTITLE: Running Evalite UI Development Server\nDESCRIPTION: Command to start the development server for Evalite UI from the root of the monorepo.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-ui/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npnpm run dev\n```\n\n----------------------------------------\n\nTITLE: Installing Starlight with Astro CLI\nDESCRIPTION: Command to create a new Astro project using the Starlight template. This is the recommended way to start a new Starlight documentation site.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/README.md#2025-04-22_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nnpm create astro@latest -- --template starlight\n```\n\n----------------------------------------\n\nTITLE: Starlight Project Structure Overview\nDESCRIPTION: A representation of the typical file and directory structure in a Starlight project. Shows the organization of documentation content, assets, and configuration files.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/README.md#2025-04-22_snippet_1\n\nLANGUAGE: plaintext\nCODE:\n```\n.\n├── public/\n├── src/\n│   ├── assets/\n│   ├── content/\n│   │   ├── docs/\n│   │   └── config.ts\n│   └── env.d.ts\n├── astro.config.mjs\n├── package.json\n└── tsconfig.json\n```\n\n----------------------------------------\n\nTITLE: Markdown Configuration for Documentation Hero Section\nDESCRIPTION: YAML frontmatter and markdown configuration for the documentation landing page, defining the hero section with title, description, and action buttons.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/src/content/docs/index.mdx#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: Test GenAI-powered apps in TypeScript\ndescription: Evalite makes evals simple. Test your AI-powered apps with a local dev server. No API key required.\ntemplate: splash\nhero:\n  tagline: Evals are <b>hard</b>. Evalite makes them simple. Test your AI-powered apps with a local dev server. No API key required.\n  image:\n    html: <img src=\"/hero.webp\" style=\"width:min(100%,25rem);\" alt=\"\" aria-hidden=\"true\" loading=\"eager\" />\n  actions:\n    - text: Get Started\n      link: /quickstart/\n      icon: right-arrow\n---\n```\n\n----------------------------------------\n\nTITLE: Change Log Entries in Markdown\nDESCRIPTION: Structured changelog entries documenting version updates, dependency changes and patches for the evalite-tests project and related packages.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/packages/evalite-tests/CHANGELOG.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# evalite-tests\n\n## 0.0.11\n\n### Patch Changes\n\n- Updated dependencies [66e8dac]\n  - evalite@0.4.0\n\n## 0.0.10\n\n### Patch Changes\n\n- Updated dependencies [9769ab8]\n  - @evalite/core@0.2.0\n  - evalite@0.3.0\n\n## 0.0.9\n\n### Patch Changes\n\n- a520613: UI tweaks\n- Updated dependencies [a520613]\n  - @evalite/core@0.1.1\n  - evalite@0.2.1\n\n## 0.0.8\n\n### Patch Changes\n\n- Updated dependencies [099b198]\n- Updated dependencies [099b198]\n  - @evalite/core@0.1.0\n  - evalite@0.2.0\n\n## 0.0.7\n\n### Patch Changes\n\n- Updated dependencies [eb294a7]\n  - evalite@0.1.4\n  - @evalite/core@0.0.5\n\n## 0.0.6\n\n### Patch Changes\n\n- Updated dependencies [213211f]\n  - evalite@0.1.3\n  - @evalite/core@0.0.4\n\n## 0.0.5\n\n### Patch Changes\n\n- Updated dependencies [e43c7a4]\n  - evalite@0.1.2\n\n## 0.0.4\n\n### Patch Changes\n\n- Updated dependencies [a6a86f1]\n  - evalite@0.1.1\n\n## 0.0.3\n\n### Patch Changes\n\n- Updated dependencies [4ca6a7d]\n- Updated dependencies [28517ff]\n- Updated dependencies [28517ff]\n- Updated dependencies [e53a652]\n  - evalite@0.1.0\n```\n\n----------------------------------------\n\nTITLE: Changelog Document in Markdown\nDESCRIPTION: A markdown changelog documenting version history and dependency updates for the example package, showing incremental version changes from 0.0.3 to 0.0.11 with corresponding evalite dependency updates.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/packages/example/CHANGELOG.md#2025-04-22_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# example\n\n## 0.0.11\n\n### Patch Changes\n\n- Updated dependencies [66e8dac]\n  - evalite@0.4.0\n\n## 0.0.10\n\n### Patch Changes\n\n- Updated dependencies [9769ab8]\n  - evalite@0.3.0\n\n## 0.0.9\n\n### Patch Changes\n\n- evalite@0.2.1\n\n## 0.0.8\n\n### Patch Changes\n\n- Updated dependencies [099b198]\n- Updated dependencies [099b198]\n  - evalite@0.2.0\n\n## 0.0.7\n\n### Patch Changes\n\n- Updated dependencies [eb294a7]\n  - evalite@0.1.4\n\n## 0.0.6\n\n### Patch Changes\n\n- Updated dependencies [213211f]\n  - evalite@0.1.3\n\n## 0.0.5\n\n### Patch Changes\n\n- Updated dependencies [e43c7a4]\n  - evalite@0.1.2\n\n## 0.0.4\n\n### Patch Changes\n\n- Updated dependencies [a6a86f1]\n  - evalite@0.1.1\n\n## 0.0.3\n\n### Patch Changes\n\n- Updated dependencies [4ca6a7d]\n- Updated dependencies [28517ff]\n- Updated dependencies [28517ff]\n- Updated dependencies [e53a652]\n  - evalite@0.1.0\n```\n\n----------------------------------------\n\nTITLE: React Component Import Statement\nDESCRIPTION: Import statement for Card and CardGrid components from the Starlight Astro components library.\nSOURCE: https://github.com/mattpocock/evalite/blob/main/apps/evalite-docs/src/content/docs/index.mdx#2025-04-22_snippet_1\n\nLANGUAGE: typescript\nCODE:\n```\nimport { Card, CardGrid } from \"@astrojs/starlight/components\";\n```"
  }
]
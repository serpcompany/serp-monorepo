[
  {
    "owner": "nixtla",
    "repo": "neuralforecast",
    "content": "TITLE: Minimal Example of Using NeuralForecast with NBEATS Model\nDESCRIPTION: A minimal working example demonstrating the core functionality of NeuralForecast. It initializes an NBEATS model, fits it to the AirPassengers dataset, and generates forecasts. The model is configured with 24 periods input size and forecasts 12 periods ahead.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/README.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import NBEATS\nfrom neuralforecast.utils import AirPassengersDF\n\nnf = NeuralForecast(\n    models = [NBEATS(input_size=24, h=12, max_steps=100)],\n    freq = 'ME'\n)\n\nnf.fit(df=AirPassengersDF)\nnf.predict()\n```\n\n----------------------------------------\n\nTITLE: Training and Using NeuralForecast Models with Example Data\nDESCRIPTION: Complete example showing how to split data, fit NBEATS and NHITS models using the NeuralForecast interface, and plot predictions. The example uses the Air Passengers dataset to forecast 12 months ahead.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/getting-started/01_introduction.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Split data and declare panel dataset\nY_df = AirPassengersDF\nY_train_df = Y_df[Y_df.ds<='1959-12-31'] # 132 train\nY_test_df = Y_df[Y_df.ds>'1959-12-31'] # 12 test\n\n# Fit and predict with NBEATS and NHITS models\nhorizon = len(Y_test_df)\nmodels = [NBEATS(input_size=2 * horizon, h=horizon, max_steps=100, enable_progress_bar=False),\n          NHITS(input_size=2 * horizon, h=horizon, max_steps=100, enable_progress_bar=False)]\nnf = NeuralForecast(models=models, freq='ME')\nnf.fit(df=Y_train_df)\nY_hat_df = nf.predict()\n\n# Plot predictions\nplot_series(Y_train_df, Y_hat_df)\n```\n\n----------------------------------------\n\nTITLE: LSTM Usage Example with AirPassengers Dataset\nDESCRIPTION: Example demonstrating how to use the LSTM model for time series forecasting with the AirPassengers dataset, including model configuration, training, prediction, and visualization.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.lstm.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import LSTM\nfrom neuralforecast.losses.pytorch import DistributionLoss\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nnf = NeuralForecast(\n    models=[LSTM(h=12, \n                 input_size=24,\n                 loss=DistributionLoss(distribution=\"Normal\", level=[80, 90]),\n                 scaler_type='robust',\n                 encoder_n_layers=2,\n                 encoder_hidden_size=128,\n                 decoder_hidden_size=128,\n                 decoder_layers=2,\n                 max_steps=200,\n                 futr_exog_list=['y_[lag12]'],\n                 stat_exog_list=['airline1'],\n                 recurrent=False,\n                 )\n    ],\n    freq='ME'\n)\nnf.fit(df=Y_train_df, static_df=AirPassengersStatic)\nY_hat_df = nf.predict(futr_df=Y_test_df)\n\n# Plots\nY_hat_df = Y_hat_df.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['LSTM-median'], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'][-12:], \n                 y1=plot_df['LSTM-lo-90'][-12:].values,\n                 y2=plot_df['LSTM-hi-90'][-12:].values,\n                 alpha=0.4, label='level 90')\nplt.grid()\nplt.plot()\n```\n\n----------------------------------------\n\nTITLE: DeepAR Usage Example with AirPassengers Dataset\nDESCRIPTION: This example demonstrates how to use the DeepAR model with the AirPassengers dataset. It shows model configuration with distribution loss, exogenous variables, and plotting of prediction intervals.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.deepar.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n#| eval: false\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import DeepAR\nfrom neuralforecast.losses.pytorch import DistributionLoss, MQLoss\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nnf = NeuralForecast(\n    models=[DeepAR(h=12,\n                   input_size=24,\n                   lstm_n_layers=1,\n                   trajectory_samples=100,\n                   loss=DistributionLoss(distribution='StudentT', level=[80, 90], return_params=True),\n                   valid_loss=MQLoss(level=[80, 90]),\n                   learning_rate=0.005,\n                   stat_exog_list=['airline1'],\n                   futr_exog_list=['trend'],\n                   max_steps=100,\n                   val_check_steps=10,\n                   early_stop_patience_steps=-1,\n                   scaler_type='standard',\n                   enable_progress_bar=True,\n                   ),\n    ],\n    freq='ME'\n)\nnf.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nY_hat_df = nf.predict(futr_df=Y_test_df)\n\n# Plot quantile predictions\nY_hat_df = Y_hat_df.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['DeepAR-median'], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'][-12:], \n                 y1=plot_df['DeepAR-lo-90'][-12:].values, \n                 y2=plot_df['DeepAR-hi-90'][-12:].values,\n                 alpha=0.4, label='level 90')\nplt.legend()\nplt.grid()\nplt.plot()\n```\n\n----------------------------------------\n\nTITLE: Implementing TimeLLM Class in Python\nDESCRIPTION: A PyTorch-based implementation of the TimeLLM model that repurposes LLMs for time series forecasting. The class inherits from BaseModel and includes extensive configuration options for the model architecture, training parameters, and data processing settings. It implements a reprogramming layer to translate time series data into language tasks and back to numerical predictions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timellm.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass TimeLLM(BaseModel):\n    EXOGENOUS_FUTR = False\n    EXOGENOUS_HIST = False\n    EXOGENOUS_STAT = False\n    MULTIVARIATE = False    \n    RECURRENT = False       \n\n    def __init__(self,\n                 h,\n                 input_size,\n                 patch_len: int = 16,\n                 stride: int = 8,\n                 d_ff: int = 128,\n                 top_k: int = 5,\n                 d_llm: int = 768,\n                 d_model: int = 32,\n                 n_heads: int = 8,\n                 enc_in: int = 7,\n                 dec_in: int  = 7,\n                 llm = None,\n                 llm_config = None,\n                 llm_tokenizer = None,\n                 llm_num_hidden_layers = 32,\n                 llm_output_attention: bool = True,\n                 llm_output_hidden_states: bool = True,\n                 prompt_prefix: Optional[str] = None,\n                 dropout: float = 0.1,\n                 stat_exog_list = None,\n                 hist_exog_list = None,\n                 futr_exog_list = None,\n                 loss = MAE(),\n                 valid_loss = None,\n                 learning_rate: float = 1e-4,\n                 max_steps: int = 5,\n                 val_check_steps: int = 100,\n                 batch_size: int = 32,\n                 valid_batch_size: Optional[int] = None,\n                 windows_batch_size: int = 1024,\n                 inference_windows_batch_size: int = 1024,\n                 start_padding_enabled: bool = False,\n                 step_size: int = 1,\n                 num_lr_decays: int = 0,\n                 early_stop_patience_steps: int = -1,\n                 scaler_type: str = 'identity',\n                 random_seed: int = 1,\n                 drop_last_loader: bool = False,\n                 alias: Optional[str] = None,\n                 optimizer = None,\n                 optimizer_kwargs = None,\n                 lr_scheduler = None,\n                 lr_scheduler_kwargs = None,\n                 dataloader_kwargs = None,\n                 **trainer_kwargs):\n        super(TimeLLM, self).__init__(h=h,\n                                      input_size=input_size,\n                                      hist_exog_list=hist_exog_list,\n                                      stat_exog_list=stat_exog_list,\n                                      futr_exog_list = futr_exog_list,\n                                      loss=loss,\n                                      valid_loss=valid_loss,\n                                      max_steps=max_steps,\n                                      learning_rate=learning_rate,\n                                      num_lr_decays=num_lr_decays,\n                                      early_stop_patience_steps=early_stop_patience_steps,\n                                      val_check_steps=val_check_steps,\n                                      batch_size=batch_size,\n                                      valid_batch_size=valid_batch_size,\n                                      windows_batch_size=windows_batch_size,\n                                      inference_windows_batch_size=inference_windows_batch_size,\n                                      start_padding_enabled=start_padding_enabled,\n                                      step_size=step_size,\n                                      scaler_type=scaler_type,\n                                      drop_last_loader=drop_last_loader,\n                                      alias=alias,\n                                      random_seed=random_seed,\n                                      optimizer=optimizer,\n                                      optimizer_kwargs=optimizer_kwargs,\n                                      lr_scheduler=lr_scheduler,\n                                      lr_scheduler_kwargs=lr_scheduler_kwargs,\n                                      dataloader_kwargs=dataloader_kwargs,\n                                      **trainer_kwargs)\n```\n\n----------------------------------------\n\nTITLE: Cross-Validation for TSMixerx Model in Python\nDESCRIPTION: This example shows how to use cross-validation with the TSMixerx model to forecast multiple historic values. It demonstrates the use of the cross_validation method and visualizes the results.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tsmixerx.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfcst = NeuralForecast(models=[model], freq='M')\nforecasts = fcst.cross_validation(df=AirPassengersPanel, static_df=AirPassengersStatic, n_windows=2, step_size=12)\n\n# Plot predictions\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nY_hat_df = forecasts.loc['Airline1']\nY_df = AirPassengersPanel[AirPassengersPanel['unique_id']=='Airline1']\n\nplt.plot(Y_df['ds'], Y_df['y'], c='black', label='True')\nplt.plot(Y_hat_df['ds'], Y_hat_df['TSMixerx-median'], c='blue', label='Forecast')\nax.set_title('AirPassengers Forecast', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Year', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\n```\n\n----------------------------------------\n\nTITLE: Implementing AutoMLP Class for Time Series Forecasting with Hyperparameter Tuning in Python\nDESCRIPTION: Defines the AutoMLP class that extends BaseAuto for automatic hyperparameter tuning of MLP models for time series forecasting. The class includes default configuration with search spaces for parameters like input size, hidden size, and learning rate, supporting both Ray Tune and Optuna backends.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_34\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass AutoMLP(BaseAuto):\n\n    default_config = {\n        \"input_size_multiplier\": [1, 2, 3, 4, 5],\n        \"h\": None,\n        \"hidden_size\": tune.choice( [256, 512, 1024] ),\n        \"num_layers\": tune.randint(2, 6),\n        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n        \"scaler_type\": tune.choice([None, 'robust', 'standard']),\n        \"max_steps\": tune.choice([500, 1000]),\n        \"batch_size\": tune.choice([32, 64, 128, 256]),\n        \"windows_batch_size\": tune.choice([128, 256, 512, 1024]),\n        \"loss\": None,\n        \"random_seed\": tune.randint(1, 20),\n    }\n\n    def __init__(self,\n                 h,\n                 loss=MAE(),\n                 valid_loss=None,     \n                 config=None,\n                 search_alg=BasicVariantGenerator(random_state=1),\n                 num_samples=10,\n                 refit_with_val=False,\n                 cpus=cpu_count(),\n                 gpus=torch.cuda.device_count(),\n                 verbose=False,\n                 alias=None,\n                 backend='ray',\n                 callbacks=None):\n\n        # Define search space, input/output sizes       \n        if config is None:\n            config = self.get_default_config(h=h, backend=backend)          \n\n        super(AutoMLP, self).__init__(\n              cls_model=MLP,\n              h=h,\n              loss=loss,\n              valid_loss=valid_loss,\n              config=config, \n              search_alg=search_alg,\n              num_samples=num_samples, \n              refit_with_val=refit_with_val,\n              cpus=cpus,\n              gpus=gpus,\n              verbose=verbose,\n              alias=alias,\n              backend=backend,\n              callbacks=callbacks,            \n        )\n\n    @classmethod\n    def get_default_config(cls, h, backend, n_series=None):\n        config = cls.default_config.copy()\n        config['input_size'] = tune.choice([h*x \\\n                        for x in config['input_size_multiplier']])\n        config['step_size'] = tune.choice([1, h])        \n        del config['input_size_multiplier']\n        if backend == 'optuna':\n            config = cls._ray_config_to_optuna(config)         \n\n        return config \n```\n\n----------------------------------------\n\nTITLE: Defining PatchTST Class for Time Series Forecasting in Python\nDESCRIPTION: This code defines the PatchTST class, a Transformer-based model for multivariate time series forecasting. It includes a comprehensive set of parameters for customizing the model architecture, training process, and data handling. The class inherits from BaseModel and initializes various components such as encoder layers, attention mechanisms, and normalization techniques.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.patchtst.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass PatchTST(BaseModel):\n    \"\"\" PatchTST\n\n    The PatchTST model is an efficient Transformer-based model for multivariate time series forecasting.\n\n    It is based on two key components:\n    - segmentation of time series into windows (patches) which are served as input tokens to Transformer\n    - channel-independence, where each channel contains a single univariate time series.\n\n    **Parameters:**<br>\n    `h`: int, Forecast horizon. <br>\n    `input_size`: int, autorregresive inputs size, y=[1,2,3,4] input_size=2 -> y_[t-2:t]=[1,2].<br>\n    `stat_exog_list`: str list, static exogenous columns.<br>\n    `hist_exog_list`: str list, historic exogenous columns.<br>\n    `futr_exog_list`: str list, future exogenous columns.<br>\n    `exclude_insample_y`: bool=False, the model skips the autoregressive features y[t-input_size:t] if True.<br>\n    `encoder_layers`: int, number of layers for encoder.<br>\n    `n_heads`: int=16, number of multi-head's attention.<br>\n    `hidden_size`: int=128, units of embeddings and encoders.<br>\n    `linear_hidden_size`: int=256, units of linear layer.<br>\n    `dropout`: float=0.1, dropout rate for residual connection.<br>\n    `fc_dropout`: float=0.1, dropout rate for linear layer.<br>\n    `head_dropout`: float=0.1, dropout rate for Flatten head layer.<br>\n    `attn_dropout`: float=0.1, dropout rate for attention layer.<br>\n    `patch_len`: int=32, length of patch. Note: patch_len = min(patch_len, input_size + stride).<br>\n    `stride`: int=16, stride of patch.<br>\n    `revin`: bool=True, bool to use RevIn.<br>\n    `revin_affine`: bool=False, bool to use affine in RevIn.<br>\n    `revin_subtract_last`: bool=False, bool to use substract last in RevIn.<br>\n    `activation`: str='ReLU', activation from ['gelu','relu'].<br>\n    `res_attention`: bool=False, bool to use residual attention.<br>\n    `batch_normalization`: bool=False, bool to use batch normalization.<br>\n    `learn_pos_embed`: bool=True, bool to learn positional embedding.<br>\n    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `max_steps`: int=1000, maximum number of training steps.<br>\n    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n    `batch_size`: int=32, number of different series in each batch.<br>\n    `valid_batch_size`: int=None, number of different series in each validation and test batch, if None uses batch_size.<br>\n    `windows_batch_size`: int=1024, number of windows to sample in each training batch, default uses all.<br>\n    `inference_windows_batch_size`: int=1024, number of windows to sample in each inference batch.<br>\n    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n    `step_size`: int=1, step size between each window of temporal data.<br>\n    `scaler_type`: str='identity', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n    `random_seed`: int, random_seed for pytorch initializer and numpy generators.<br>\n    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n    `alias`: str, optional,  Custom name of the model.<br>\n    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>    \n    `dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n\n    **References:**<br>\n    -[Nie, Y., Nguyen, N. H., Sinthong, P., & Kalagnanam, J. (2022). \"A Time Series is Worth 64 Words: Long-term Forecasting with Transformers\"](https://arxiv.org/pdf/2211.14730.pdf)\n    \"\"\"\n    # Class attributes\n    EXOGENOUS_FUTR = False\n    EXOGENOUS_HIST = False\n    EXOGENOUS_STAT = False\n    MULTIVARIATE = False    # If the model produces multivariate forecasts (True) or univariate (False)\n    RECURRENT = False       # If the model produces forecasts recursively (True) or direct (False)\n\n    def __init__(self,\n                 h,\n                 input_size,\n                 stat_exog_list = None,\n                 hist_exog_list = None,\n                 futr_exog_list = None,\n                 exclude_insample_y = False,\n                 encoder_layers: int = 3,\n                 n_heads: int = 16,\n                 hidden_size: int = 128,\n                 linear_hidden_size: int = 256,\n                 dropout: float = 0.2,\n                 fc_dropout: float = 0.2,\n                 head_dropout: float = 0.0,\n                 attn_dropout: float = 0.,\n                 patch_len: int = 16,\n                 stride: int = 8,\n                 revin: bool = True,\n                 revin_affine: bool = False,\n                 revin_subtract_last: bool = True,\n                 activation: str = \"gelu\",\n                 res_attention: bool = True, \n                 batch_normalization: bool = False,\n                 learn_pos_embed: bool = True,\n                 loss = MAE(),\n                 valid_loss = None,\n                 max_steps: int = 5000,\n                 learning_rate: float = 1e-4,\n                 num_lr_decays: int = -1,\n                 early_stop_patience_steps: int =-1,\n                 val_check_steps: int = 100,\n                 batch_size: int = 32,\n                 valid_batch_size: Optional[int] = None,\n                 windows_batch_size = 1024,\n                 inference_windows_batch_size: int = 1024,\n                 start_padding_enabled = False,\n                 step_size: int = 1,\n                 scaler_type: str = 'identity',\n                 random_seed: int = 1,\n                 drop_last_loader: bool = False,\n                 alias: Optional[str] = None,\n                 optimizer = None,\n                 optimizer_kwargs = None,\n                 lr_scheduler = None,\n                 lr_scheduler_kwargs = None,\n                 dataloader_kwargs = None,\n                 **trainer_kwargs):\n        super(PatchTST, self).__init__(h=h,\n                                       input_size=input_size,\n                                       stat_exog_list=stat_exog_list,\n                                       hist_exog_list=hist_exog_list,\n                                       futr_exog_list = futr_exog_list,\n                                       exclude_insample_y = exclude_insample_y,\n                                       loss=loss,\n                                       valid_loss=valid_loss,\n                                       max_steps=max_steps,\n                                       learning_rate=learning_rate,\n                                       num_lr_decays=num_lr_decays,\n                                       early_stop_patience_steps=early_stop_patience_steps,\n                                       val_check_steps=val_check_steps,\n                                       batch_size=batch_size,\n                                       valid_batch_size=valid_batch_size,\n                                       windows_batch_size=windows_batch_size,\n                                       inference_windows_batch_size=inference_windows_batch_size,\n                                       start_padding_enabled=start_padding_enabled,\n                                       step_size=step_size,\n                                       scaler_type=scaler_type,\n                                       random_seed=random_seed,\n                                       drop_last_loader=drop_last_loader,\n                                       alias=alias,\n                                       optimizer=optimizer,\n                                       optimizer_kwargs=optimizer_kwargs,\n                                       lr_scheduler=lr_scheduler,\n                                       lr_scheduler_kwargs=lr_scheduler_kwargs,\n                                       dataloader_kwargs=dataloader_kwargs,\n                                       **trainer_kwargs) \n\n        # Enforce correct patch_len, regardless of user input\n        patch_len = min(input_size + stride, patch_len)\n\n        c_out = self.loss.outputsize_multiplier\n\n        # Fixed hyperparameters\n        c_in = 1                  # Always univariate\n        padding_patch='end'       # Padding at the end\n        pretrain_head = False     # No pretrained head\n        norm = 'BatchNorm'        # Use BatchNorm (if batch_normalization is True)\n        pe = 'zeros'              # Initial zeros for positional encoding \n        d_k = None                # Key dimension\n        d_v = None                # Value dimension\n        store_attn = False        # Store attention weights\n        head_type = 'flatten'     # Head type\n        individual = False        # Separate heads for each time series\n```\n\n----------------------------------------\n\nTITLE: NeuralForecast Class Implementation\nDESCRIPTION: Main class for neural forecasting implementing initialization, data preparation, and scaling functionality for time series forecasting models.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass NeuralForecast:\n    def __init__(\n        self, \n        models: List[Any],\n        freq: Union[str, int],\n        local_scaler_type: Optional[str] = None\n    ):\n```\n\n----------------------------------------\n\nTITLE: Defining the Temporal Fusion Decoder Module in Python\nDESCRIPTION: This snippet defines the `TemporalFusionDecoder` class, a PyTorch `nn.Module`, which constitutes the decoder part of the Temporal Fusion Transformer. It initializes components like Gated Residual Networks (GRN), Interpretable Multi-Head Attention, Gated Linear Units (GLU), and Layer Normalization. The `forward` method processes temporal features and static context (`ce`), applies enrichment, attention, and gating, returning the decoded features and attention vectors. Key parameters include `n_head`, `hidden_size`, `example_length`, `encoder_length`, dropout rates, and activation function type.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# | exporti\nclass TemporalFusionDecoder(nn.Module):\n    def __init__(\n        self,\n        n_head,\n        hidden_size,\n        example_length,\n        encoder_length,\n        attn_dropout,\n        dropout,\n        grn_activation,\n    ):\n        super(TemporalFusionDecoder, self).__init__()\n        self.encoder_length = encoder_length\n\n        # ------------- Encoder-Decoder Attention --------------#\n        self.enrichment_grn = GRN(\n            input_size=hidden_size,\n            hidden_size=hidden_size,\n            context_hidden_size=hidden_size,\n            dropout=dropout,\n            activation=grn_activation,\n        )\n        self.attention = InterpretableMultiHeadAttention(\n            n_head=n_head,\n            hidden_size=hidden_size,\n            example_length=example_length,\n            attn_dropout=attn_dropout,\n            dropout=dropout,\n        )\n        self.attention_gate = GLU(hidden_size, hidden_size)\n        self.attention_ln = LayerNorm(normalized_shape=hidden_size, eps=1e-3)\n\n        self.positionwise_grn = GRN(\n            input_size=hidden_size,\n            hidden_size=hidden_size,\n            dropout=dropout,\n            activation=grn_activation,\n        )\n\n        # ---------------------- Decoder -----------------------#\n        self.decoder_gate = GLU(hidden_size, hidden_size)\n        self.decoder_ln = LayerNorm(normalized_shape=hidden_size, eps=1e-3)\n\n    def forward(self, temporal_features, ce):\n        # ------------- Encoder-Decoder Attention --------------#\n        # Static enrichment\n        enriched = self.enrichment_grn(temporal_features, c=ce)\n\n        # Temporal self attention\n        x, atten_vect = self.attention(enriched, mask_future_timesteps=True)\n\n        # Don't compute historical quantiles\n        x = x[:, self.encoder_length :, :]\n        temporal_features = temporal_features[:, self.encoder_length :, :]\n        enriched = enriched[:, self.encoder_length :, :]\n\n        x = self.attention_gate(x)\n        x = x + enriched\n        x = self.attention_ln(x)\n\n        # Position-wise feed-forward\n        x = self.positionwise_grn(x)\n\n        # ---------------------- Decoder ----------------------#\n        # Final skip connection\n        x = self.decoder_gate(x)\n        x = x + temporal_features\n        x = self.decoder_ln(x)\n\n        return x, atten_vect\n```\n\n----------------------------------------\n\nTITLE: Time Series Prediction Method with PyTorch Lightning\nDESCRIPTION: Generates time series forecasts using a neural model. Handles multivariate data, supports quantile predictions, and includes GPU acceleration handling. Processes predictions through a PyTorch Lightning trainer and reshapes output for proper dimensionality.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_36\n\nLANGUAGE: Python\nCODE:\n```\nself._check_exog(dataset)\nself._restart_seed(random_seed)\nif \"quantile\" in data_module_kwargs:\n    warnings.warn(\"The 'quantile' argument will be deprecated, use 'quantiles' instead.\")\n    if quantiles is not None:\n        raise ValueError(\"You can't specify quantile and quantiles.\")\n    quantiles = [data_module_kwargs.pop(\"quantile\")]\nself._set_quantiles(quantiles)\n\nself.predict_step_size = step_size\nself.decompose_forecast = False\ndatamodule = TimeSeriesDataModule(dataset=dataset,\n                                  valid_batch_size=self.valid_batch_size,\n                                  **data_module_kwargs)\n\npred_trainer_kwargs = self.trainer_kwargs.copy()\nif (pred_trainer_kwargs.get('accelerator', None) == \"gpu\") and (torch.cuda.device_count() > 1):\n    pred_trainer_kwargs['devices'] = [0]\n\ntrainer = pl.Trainer(**pred_trainer_kwargs)\nfcsts = trainer.predict(self, datamodule=datamodule)        \nfcsts = torch.vstack(fcsts)\n\nif self.MULTIVARIATE:\n    fcsts = fcsts.swapaxes(0, 2)\n    fcsts = fcsts.swapaxes(1, 2)\n\nfcsts = tensor_to_numpy(fcsts).flatten()\nfcsts = fcsts.reshape(-1, len(self.loss.output_names))\nreturn fcsts\n```\n\n----------------------------------------\n\nTITLE: Defining Forward Pass and Interpretability for TFT Model - PyTorch - Python\nDESCRIPTION: Implements the core forward logic for the Temporal Fusion Transformer (TFT) model, including input handling, static and temporal encoding, decoding, and output adaptation. Incorporates methods for average computations, feature importances, attention weights, and correlations, providing comprehensive model interpretability. Dependencies include torch, pandas, the TFT model's encoder and decoder classes, and appropriate handling of model attributes; inputs are dictionaries of batched sequences, outputs include predictions and interpretation statistics.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n            hidden_size=hidden_size,\\n            stat_input_size=self.stat_exog_size,\\n            futr_input_size=futr_exog_size,\\n            hist_input_size=self.hist_exog_size,\\n            tgt_size=tgt_size,\\n        )\\n\\n        if self.stat_exog_size > 0:\\n            self.static_encoder = StaticCovariateEncoder(\\n                hidden_size=hidden_size,\\n                num_static_vars=self.stat_exog_size,\\n                dropout=dropout,\\n                grn_activation=self.grn_activation,\\n                rnn_type=self.rnn_type,\\n                n_rnn_layers=n_rnn_layers,\\n                one_rnn_initial_state=one_rnn_initial_state,\\n            )\\n\\n        self.temporal_encoder = TemporalCovariateEncoder(\\n            hidden_size=hidden_size,\\n            num_historic_vars=num_historic_vars,\\n            num_future_vars=futr_exog_size,\\n            dropout=dropout,\\n            grn_activation=self.grn_activation,\\n            n_rnn_layers=n_rnn_layers,\\n            rnn_type=self.rnn_type,\\n        )\\n\\n        # ------------------------------ Decoders -----------------------------#\\n        self.temporal_fusion_decoder = TemporalFusionDecoder(\\n            n_head=n_head,\\n            hidden_size=hidden_size,\\n            example_length=self.example_length,\\n            encoder_length=self.input_size,\\n            attn_dropout=attn_dropout,\\n            dropout=dropout,\\n            grn_activation=self.grn_activation,\\n        )\\n\\n        # Adapter with Loss dependent dimensions\\n        self.output_adapter = nn.Linear(\\n            in_features=hidden_size, out_features=self.loss.outputsize_multiplier\\n        )\\n\\n    def forward(self, windows_batch):\\n\\n        # Parsiw windows_batch\\n        y_insample = windows_batch[\"insample_y\"]  # <- [B,T,1]\\n        futr_exog = windows_batch[\"futr_exog\"]\\n        hist_exog = windows_batch[\"hist_exog\"]\\n        stat_exog = windows_batch[\"stat_exog\"]\\n\\n        if futr_exog is None:\\n            futr_exog = y_insample[:, [-1]]\\n            futr_exog = futr_exog.repeat(1, self.example_length, 1)\\n\\n        s_inp, k_inp, o_inp, t_observed_tgt = self.embedding(\\n            target_inp=y_insample,\\n            hist_exog=hist_exog,\\n            futr_exog=futr_exog,\\n            stat_exog=stat_exog,\\n        )\\n\\n        # -------------------------------- Inputs ------------------------------#\\n        # Static context\\n        if s_inp is not None:\\n            cs, ce, ch, cc, static_encoder_sparse_weights = self.static_encoder(s_inp)\\n            # ch, cc = ch.unsqueeze(0), cc.unsqueeze(0)  # LSTM initial states\\n        else:\\n            # If None add zeros\\n            batch_size, example_length, target_size, hidden_size = t_observed_tgt.shape\\n            cs = torch.zeros(size=(batch_size, hidden_size), device=y_insample.device)\\n            ce = torch.zeros(size=(batch_size, hidden_size), device=y_insample.device)\\n            ch = torch.zeros(\\n                size=(self.n_rnn_layers, batch_size, hidden_size),\\n                device=y_insample.device,\\n            )\\n            cc = torch.zeros(\\n                size=(self.n_rnn_layers, batch_size, hidden_size),\\n                device=y_insample.device,\\n            )\\n            static_encoder_sparse_weights = []\\n\\n        # Historical inputs\\n        _historical_inputs = [\\n            k_inp[:, : self.input_size, :],\\n            t_observed_tgt[:, : self.input_size, :],\\n        ]\\n        if o_inp is not None:\\n            _historical_inputs.insert(0, o_inp[:, : self.input_size, :])\\n        historical_inputs = torch.cat(_historical_inputs, dim=-2)\\n        # Future inputs\\n        future_inputs = k_inp[:, self.input_size :]\\n\\n        # ---------------------------- Encode/Decode ---------------------------#\\n        # Embeddings + VSN + LSTM encoders\\n        temporal_features, history_vsn_wgts, future_vsn_wgts = self.temporal_encoder(\\n            historical_inputs=historical_inputs,\\n            future_inputs=future_inputs,\\n            cs=cs,\\n            ch=ch,\\n            cc=cc,\\n        )\\n\\n        # Static enrichment, Attention and decoders\\n        temporal_features, attn_wts = self.temporal_fusion_decoder(\\n            temporal_features=temporal_features, ce=ce\\n        )\\n\\n        # Store params\\n        self.interpretability_params = {\\n            \"history_vsn_wgts\": history_vsn_wgts,\\n            \"future_vsn_wgts\": future_vsn_wgts,\\n            \"static_encoder_sparse_weights\": static_encoder_sparse_weights,\\n            \"attn_wts\": attn_wts,\\n        }\\n\\n        # Adapt output to loss\\n        y_hat = self.output_adapter(temporal_features)\\n\\n        return y_hat\\n\\n    def mean_on_batch(self, tensor):\\n        batch_size = tensor.size(0)\\n        if batch_size > 1:\\n            return tensor.mean(dim=0)\\n        else:\\n            return tensor.squeeze(0)\\n\\n    def feature_importances(self):\\n        \"\"\"\\n        Compute the feature importances for historical, future, and static features.\\n\\n        Returns:\\n            dict: A dictionary containing the feature importances for each feature type.\\n                The keys are 'hist_vsn', 'future_vsn', and 'static_vsn', and the values\\n                are pandas DataFrames with the corresponding feature importances.\\n        \"\"\"\\n        if not self.interpretability_params:\\n            raise ValueError(\\n                \"No interpretability_params. Make a prediction using the model to generate them.\"\\n            )\\n\\n        importances = {}\\n\\n        # Historical feature importances\\n        hist_vsn_wgts = self.interpretability_params.get(\"history_vsn_wgts\")\\n        hist_exog_list = list(self.hist_exog_list) + list(self.futr_exog_list)\\n        hist_exog_list += (\\n            [f\"observed_target_{i+1}\" for i in range(self.tgt_size)]\\n            if self.tgt_size > 1\\n            else [\"observed_target\"]\\n        )\\n        if len(self.futr_exog_list) < 1:\\n            hist_exog_list += [\"repeated_target\"]\\n        hist_vsn_imp = pd.DataFrame(\\n            self.mean_on_batch(hist_vsn_wgts).cpu().numpy(), columns=hist_exog_list\\n        )\\n        importances[\"Past variable importance over time\"] = hist_vsn_imp\\n        #  importances[\"Past variable importance\"] = hist_vsn_imp.mean(axis=0).sort_values()\\n\\n        # Future feature importances\\n        if self.futr_exog_size > 0:\\n            future_vsn_wgts = self.interpretability_params.get(\"future_vsn_wgts\")\\n            future_vsn_imp = pd.DataFrame(\\n                self.mean_on_batch(future_vsn_wgts).cpu().numpy(),\\n                columns=self.futr_exog_list,\\n            )\\n            importances[\"Future variable importance over time\"] = future_vsn_imp\\n        #   importances[\"Future variable importance\"] = future_vsn_imp.mean(axis=0).sort_values()\\n\\n        # Static feature importances\\n        if self.stat_exog_size > 0:\\n            static_encoder_sparse_weights = self.interpretability_params.get(\\n                \"static_encoder_sparse_weights\"\\n            )\\n\\n            static_vsn_imp = pd.DataFrame(\\n                self.mean_on_batch(static_encoder_sparse_weights).cpu().numpy(),\\n                index=self.stat_exog_list,\\n                columns=[\"importance\"],\\n            )\\n            importances[\"Static covariates\"] = static_vsn_imp.sort_values(\\n                by=\"importance\"\\n            )\\n\\n        return importances\\n\\n    def attention_weights(self):\\n        \"\"\"\\n        Batch average attention weights\\n\\n        Returns:\\n        np.ndarray: A 1D array containing the attention weights for each time step.\\n\\n        \"\"\"\\n\\n        attention = (\\n            self.mean_on_batch(self.interpretability_params[\"attn_wts\"])\\n            .mean(dim=0)\\n            .cpu()\\n            .numpy()\\n        )\\n\\n        return attention\\n\\n    def feature_importance_correlations(self) -> pd.DataFrame:\\n        \"\"\"\\n        Compute the correlation between the past and future feature importances and the mean attention weights.\\n\\n        Returns:\\n        pd.DataFrame: A DataFrame containing the correlation coefficients between the past feature importances and the mean attention weights.\\n        \"\"\"\\n        attention = self.attention_weights()[self.input_size :, :].mean(axis=0)\\n        p_c = self.feature_importances()[\"Past variable importance over time\"]\\n        p_c[\"Correlation with Mean Attention\"] = attention[: self.input_size]\\n        return p_c.corr(method=\"spearman\").round(2)\\n```\n```\n\n----------------------------------------\n\nTITLE: Initializing and Defining Forward Pass for FEDformer Model in Python\nDESCRIPTION: This snippet defines the initialization (`__init__`) and forward pass (`forward`) methods for the FEDformer time series forecasting model class. The `__init__` method sets up the model's layers, including embeddings, Fourier blocks for attention, an encoder, and a decoder, based on provided hyperparameters like hidden size, number of layers, modes, etc. The `forward` method defines the data flow: processing input windows, handling exogenous variables, performing decomposition, passing data through the encoder and decoder, and returning the final forecast. It utilizes components like `AutoCorrelationLayer`, `FourierBlock`, `MovingAvg`, and `LayerNorm` from the underlying library.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.fedformer.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n                                           exog_input_size=self.futr_exog_size,\n                                           hidden_size=hidden_size, \n                                           pos_embedding=False,\n                                           dropout=dropout)\n\n        encoder_self_att = FourierBlock(in_channels=hidden_size,\n                                        out_channels=hidden_size,\n                                        seq_len=input_size,\n                                        modes=modes,\n                                        mode_select_method=mode_select)\n        decoder_self_att = FourierBlock(in_channels=hidden_size,\n                                        out_channels=hidden_size,\n                                        seq_len=input_size//2+self.h,\n                                        modes=modes,\n                                        mode_select_method=mode_select)\n        decoder_cross_att = FourierCrossAttention(in_channels=hidden_size,\n                                                    out_channels=hidden_size,\n                                                    seq_len_q=input_size//2+self.h,\n                                                    seq_len_kv=input_size,\n                                                    modes=modes,\n                                                    mode_select_method=mode_select)\n\n        self.encoder = Encoder(\n            [\n                EncoderLayer(\n                    AutoCorrelationLayer(\n                        encoder_self_att,\n                        hidden_size, n_head),\n\n                    hidden_size=hidden_size,\n                    conv_hidden_size=conv_hidden_size,\n                    MovingAvg=MovingAvg_window,\n                    dropout=dropout,\n                    activation=activation\n                ) for l in range(encoder_layers)\n            ],\n            norm_layer=LayerNorm(hidden_size)\n        )\n        # Decoder\n        self.decoder = Decoder(\n            [\n                DecoderLayer(\n                    AutoCorrelationLayer(\n                        decoder_self_att,\n                        hidden_size, n_head),\n                    AutoCorrelationLayer(\n                        decoder_cross_att,\n                        hidden_size, n_head),\n                    hidden_size=hidden_size,\n                    c_out=self.c_out,\n                    conv_hidden_size=conv_hidden_size,\n                    MovingAvg=MovingAvg_window,\n                    dropout=dropout,\n                    activation=activation,\n                )\n                for l in range(decoder_layers)\n            ],\n            norm_layer=LayerNorm(hidden_size),\n            projection=nn.Linear(hidden_size, self.c_out, bias=True)\n        )\n\n    def forward(self, windows_batch):\n        # Parse windows_batch\n        insample_y    = windows_batch['insample_y']\n        futr_exog     = windows_batch['futr_exog']\n\n        # Parse inputs\n        if self.futr_exog_size > 0:\n            x_mark_enc = futr_exog[:,:self.input_size,:]\n            x_mark_dec = futr_exog[:,-(self.label_len+self.h):,:]\n        else:\n            x_mark_enc = None\n            x_mark_dec = None\n\n        x_dec = torch.zeros(size=(len(insample_y),self.h, self.dec_in), device=insample_y.device)\n        x_dec = torch.cat([insample_y[:,-self.label_len:,:], x_dec], dim=1)\n                \n        # decomp init\n        mean = torch.mean(insample_y, dim=1).unsqueeze(1).repeat(1, self.h, 1)\n        zeros = torch.zeros([x_dec.shape[0], self.h, x_dec.shape[2]], device=insample_y.device)\n        seasonal_init, trend_init = self.decomp(insample_y)\n        # decoder input\n        trend_init = torch.cat([trend_init[:, -self.label_len:, :], mean], dim=1)\n        seasonal_init = torch.cat([seasonal_init[:, -self.label_len:, :], zeros], dim=1)\n        # enc\n        enc_out = self.enc_embedding(insample_y, x_mark_enc)\n        enc_out, attns = self.encoder(enc_out, attn_mask=None)\n        # dec\n        dec_out = self.dec_embedding(seasonal_init, x_mark_dec)\n        seasonal_part, trend_part = self.decoder(dec_out, enc_out, x_mask=None, cross_mask=None,\n                                                 trend=trend_init)\n        # final\n        dec_out = trend_part + seasonal_part\n        forecast = dec_out[:, -self.h:]\n        \n        return forecast\n```\n\n----------------------------------------\n\nTITLE: Initializing the Temporal Fusion Transformer (TFT) Model in Python\nDESCRIPTION: This snippet defines the main `TFT` model class, inheriting from `BaseModel`. It orchestrates the components of the Temporal Fusion Transformer for multi-horizon time series forecasting, handling static, historic, and future exogenous variables. The `__init__` method takes numerous parameters to configure the model architecture (e.g., `h`, `input_size`, `hidden_size`, `n_head`, `n_rnn_layers`), data processing (e.g., exogenous variable lists, `scaler_type`), and training (e.g., `loss`, `learning_rate`, `max_steps`, `batch_size`). It sets up the model's structure, including embeddings and potentially recurrent layers, preparing it for use within the neuralforecast framework. Dependencies include PyTorch, `BaseModel`, loss functions like `MAE`, and standard Python types.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass TFT(BaseModel):\n    \"\"\"TFT\n\n    The Temporal Fusion Transformer architecture (TFT) is an Sequence-to-Sequence\n    model that combines static, historic and future available data to predict an\n    univariate target. The method combines gating layers, an LSTM recurrent encoder,\n    with and interpretable multi-head attention layer and a multi-step forecasting\n    strategy decoder.\n\n    **Parameters:**<br>\n    `h`: int, Forecast horizon. <br>\n    `input_size`: int, autorregresive inputs size, y=[1,2,3,4] input_size=2 -> y_[t-2:t]=[1,2].<br>\n    `tgt_size`: int=1, target size.<br>\n    `stat_exog_list`: str list, static continuous columns.<br>\n    `hist_exog_list`: str list, historic continuous columns.<br>\n    `futr_exog_list`: str list, future continuous columns.<br>\n    `hidden_size`: int, units of embeddings and encoders.<br>\n    `n_head`: int=4, number of attention heads in temporal fusion decoder.<br>\n    `attn_dropout`: float (0, 1), dropout of fusion decoder's attention layer.<br>\n    `grn_activation`: str, activation for the GRN module from ['ReLU', 'Softplus', 'Tanh', 'SELU', 'LeakyReLU', 'Sigmoid', 'ELU', 'GLU'].<br>\n    `n_rnn_layers`: int=1, number of RNN layers.<br>\n    `rnn_type`: str=\"lstm\", recurrent neural network (RNN) layer type from [\"lstm\",\"gru\"].<br>\n    `one_rnn_initial_state`:str=False, Initialize all rnn layers with the same initial states computed from static covariates.<br>\n    `dropout`: float (0, 1), dropout of inputs VSNs.<br>\n    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `max_steps`: int=1000, maximum number of training steps.<br>\n    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n    `batch_size`: int, number of different series in each batch.<br>\n    `valid_batch_size`: int=None, number of different series in each validation and test batch.<br>\n    `windows_batch_size`: int=None, windows sampled from rolled data, default uses all.<br>\n    `inference_windows_batch_size`: int=-1, number of windows to sample in each inference batch, -1 uses all.<br>\n    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n    `step_size`: int=1, step size between each window of temporal data.<br>\n    `scaler_type`: str='robust', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n    `random_seed`: int, random seed initialization for replicability.<br>\n    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n    `alias`: str, optional,  Custom name of the model.<br>\n    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>\n    `dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>\n\n    **References:**<br>\n    - [Bryan Lim, Sercan O. Arik, Nicolas Loeff, Tomas Pfister,\n    \"Temporal Fusion Transformers for interpretable multi-horizon time series forecasting\"](https://www.sciencedirect.com/science/article/pii/S0169207021000637)\n    \"\"\"\n\n    # Class attributes\n    EXOGENOUS_FUTR = True\n    EXOGENOUS_HIST = True\n    EXOGENOUS_STAT = True\n    MULTIVARIATE = False    # If the model produces multivariate forecasts (True) or univariate (False)\n    RECURRENT = False       # If the model produces forecasts recursively (True) or direct (False)\n\n    def __init__(\n        self,\n        h,\n        input_size,\n        tgt_size: int = 1,\n        stat_exog_list=None,\n        hist_exog_list=None,\n        futr_exog_list=None,\n        hidden_size: int = 128,\n        n_head: int = 4,\n        attn_dropout: float = 0.0,\n        grn_activation: str = \"ELU\",\n        n_rnn_layers: int = 1,\n        rnn_type: str = \"lstm\",\n        one_rnn_initial_state: bool = False,\n        dropout: float = 0.1,\n        loss=MAE(),\n        valid_loss=None,\n        max_steps: int = 1000,\n        learning_rate: float = 1e-3,\n        num_lr_decays: int = -1,\n        early_stop_patience_steps: int = -1,\n        val_check_steps: int = 100,\n        batch_size: int = 32,\n        valid_batch_size: Optional[int] = None,\n        windows_batch_size: int = 1024,\n        inference_windows_batch_size: int = 1024,\n        start_padding_enabled=False,\n        step_size: int = 1,\n        scaler_type: str = \"robust\",\n        random_seed: int = 1,\n        drop_last_loader=False,\n        alias: Optional[str] = None,\n        optimizer=None,\n        optimizer_kwargs=None,\n        lr_scheduler=None,\n        lr_scheduler_kwargs=None,\n        dataloader_kwargs=None,\n        **trainer_kwargs,\n    ):\n        # Inherit BaseWindows class\n        super(TFT, self).__init__(\n            h=h,\n            input_size=input_size,\n            stat_exog_list=stat_exog_list,\n            hist_exog_list=hist_exog_list,\n            futr_exog_list=futr_exog_list,\n            loss=loss,\n            valid_loss=valid_loss,\n            max_steps=max_steps,\n            learning_rate=learning_rate,\n            num_lr_decays=num_lr_decays,\n            early_stop_patience_steps=early_stop_patience_steps,\n            val_check_steps=val_check_steps,\n            batch_size=batch_size,\n            valid_batch_size=valid_batch_size,\n            windows_batch_size=windows_batch_size,\n            inference_windows_batch_size=inference_windows_batch_size,\n            start_padding_enabled=start_padding_enabled,\n            step_size=step_size,\n            scaler_type=scaler_type,\n            random_seed=random_seed,\n            drop_last_loader=drop_last_loader,\n            alias=alias,\n            optimizer=optimizer,\n            optimizer_kwargs=optimizer_kwargs,\n            lr_scheduler=lr_scheduler,\n            lr_scheduler_kwargs=lr_scheduler_kwargs,\n            dataloader_kwargs=dataloader_kwargs,\n            **trainer_kwargs,\n        )\n        self.example_length = input_size + h\n        self.interpretability_params = dict([])  # type: ignore\n        self.tgt_size = tgt_size\n        self.grn_activation = grn_activation\n        futr_exog_size = max(self.futr_exog_size, 1)\n        num_historic_vars = futr_exog_size + self.hist_exog_size + tgt_size\n        self.n_rnn_layers = n_rnn_layers\n        self.rnn_type = rnn_type.lower()\n        # ------------------------------- Encoders -----------------------------#\n        self.embedding = TFTEmbedding(\n```\n\n----------------------------------------\n\nTITLE: Defining DilatedRNN Forward Pass in Python\nDESCRIPTION: Defines the forward pass logic for the DilatedRNN model using PyTorch. It processes an input batch (`windows_batch`) containing time series data (`insample_y`) and exogenous variables (future, historical, static). The method concatenates relevant inputs, passes them through stacked dilated RNN layers with residual connections, adapts the context, incorporates future exogenous variables, and finally uses a decoder MLP to generate the forecast output. Dependencies include PyTorch (`torch`), an `MLP` class, and the model's pre-defined attributes like `encoder_hidden_size`, `futr_exog_size`, etc.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.dilated_rnn.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n                                         out_features=h)\n\n        # Decoder MLP\n        self.mlp_decoder = MLP(in_features=self.encoder_hidden_size + self.futr_exog_size,\n                               out_features=self.loss.outputsize_multiplier,\n                               hidden_size=self.decoder_hidden_size,\n                               num_layers=self.decoder_layers,\n                               activation='ReLU',\n                               dropout=0.0)\n\n    def forward(self, windows_batch):\n        \n        # Parse windows_batch\n        encoder_input = windows_batch['insample_y']                         # [B, L, 1]\n        futr_exog     = windows_batch['futr_exog']                          # [B, L + h, F]\n        hist_exog     = windows_batch['hist_exog']                          # [B, L, X]\n        stat_exog     = windows_batch['stat_exog']                          # [B, S]\n\n        # Concatenate y, historic and static inputs              \n        batch_size, seq_len = encoder_input.shape[:2]\n        if self.hist_exog_size > 0:\n            encoder_input = torch.cat((encoder_input, hist_exog), dim=2)    # [B, L, 1] + [B, L, X] -> [B, L, 1 + X]\n\n        if self.stat_exog_size > 0:\n            stat_exog = stat_exog.unsqueeze(1).repeat(1, seq_len, 1)        # [B, S] -> [B, L, S]\n            encoder_input = torch.cat((encoder_input, stat_exog), dim=2)    # [B, L, 1 + X] + [B, L, S] -> [B, L, 1 + X + S]\n\n        if self.futr_exog_size > 0:\n            encoder_input = torch.cat((encoder_input, \n                                       futr_exog[:, :seq_len]), dim=2)      # [B, L, 1 + X + S] + [B, L, F] -> [B, L, 1 + X + S + F]\n\n        # DilatedRNN forward\n        for layer_num in range(len(self.rnn_stack)):\n            residual = encoder_input\n            output, _ = self.rnn_stack[layer_num](encoder_input)\n            if layer_num > 0:\n                output += residual\n            encoder_input = output\n\n        # Context adapter\n        output = output.permute(0, 2, 1)                                    # [B, L, C] -> [B, C, L]\n        context = self.context_adapter(output)                              # [B, C, L] -> [B, C, h]\n\n        # Residual connection with futr_exog\n        if self.futr_exog_size > 0:\n            futr_exog_futr = futr_exog[:, seq_len:].permute(0, 2, 1)        # [B, h, F] -> [B, F, h]\n            context = torch.cat((context, futr_exog_futr), \n                                dim=1)                                      # [B, C, h] + [B, F, h] = [B, C + F, h]\n\n        # Final forecast\n        context = context.permute(0, 2, 1)                                  # [B, C + F, h] -> [B, h, C + F]\n        output = self.mlp_decoder(context)                                  # [B, h, C + F] -> [B, h, n_output]\n        \n        return output\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using AutoHINT Model in Python\nDESCRIPTION: This snippet demonstrates the initialization and usage of the AutoHINT model. It sets up the model with specific parameters, fits it to a dataset, and generates predictions. The model uses NHITS as the base forecasting model and incorporates hierarchical reconciliation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_103\n\nLANGUAGE: python\nCODE:\n```\nmodel = AutoHINT(h=4, S=S_df.values,\n                 cls_model=NHITS,\n                 config=nhits_config,\n                 loss=GMM(n_components=2, level=[80, 90]),\n                 valid_loss=sCRPS(level=[80, 90]),\n                 num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=hint_dataset)\n```\n\n----------------------------------------\n\nTITLE: Implementing NBEATSx Model for RUL Prediction\nDESCRIPTION: Configures and trains an NBEATSx model using sensor readings as future exogenous variables to predict RUL values.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/use-cases/predictive_maintenance.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n%%capture\nfutr_exog_list =['s_2', 's_3', 's_4', 's_7', 's_8', 's_9', 's_11',\n                 's_12', 's_13', 's_14', 's_15', 's_17', 's_20', 's_21']\n\nmodel = NBEATSx(h=1, \n                input_size=24,\n                loss=HuberLoss(),\n                scaler_type='robust',\n                stack_types=['identity', 'identity', 'identity'],\n                dropout_prob_theta=0.5,\n                futr_exog_list=futr_exog_list,\n                exclude_insample_y = True,\n                max_steps=1000)\nnf = NeuralForecast(models=[model], freq=1)\n\nnf.fit(df=Y_train_df)\nY_hat_df = nf.predict(futr_df=Y_test_df)\n```\n\n----------------------------------------\n\nTITLE: Implementing Autoformer Class for Time Series Forecasting in Python\nDESCRIPTION: This code defines the Autoformer class that inherits from BaseModel. It implements the Autoformer architecture which uses decomposition transformers with auto-correlation for long-term series forecasting. The class includes model configuration parameters, initialization logic, and documentation on the model's approach to time series forecasting.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.autoformer.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass Autoformer(BaseModel):\n    \"\"\" Autoformer\n\n    The Autoformer model tackles the challenge of finding reliable dependencies on intricate temporal patterns of long-horizon forecasting.\n\n    The architecture has the following distinctive features:\n    - In-built progressive decomposition in trend and seasonal compontents based on a moving average filter.\n    - Auto-Correlation mechanism that discovers the period-based dependencies by\n    calculating the autocorrelation and aggregating similar sub-series based on the periodicity.\n    - Classic encoder-decoder proposed by Vaswani et al. (2017) with a multi-head attention mechanism.\n\n    The Autoformer model utilizes a three-component approach to define its embedding:\n    - It employs encoded autoregressive features obtained from a convolution network.\n    - Absolute positional embeddings obtained from calendar features are utilized.\n\n    *Parameters:*<br>\n    `h`: int, forecast horizon.<br>\n    `input_size`: int, maximum sequence length for truncated train backpropagation. Default -1 uses all history.<br>\n    `futr_exog_list`: str list, future exogenous columns.<br>\n    `hist_exog_list`: str list, historic exogenous columns.<br>\n    `stat_exog_list`: str list, static exogenous columns.<br>\n    `exclude_insample_y`: bool=False, the model skips the autoregressive features y[t-input_size:t] if True.<br>\n\t`decoder_input_size_multiplier`: float = 0.5, .<br>\n    `hidden_size`: int=128, units of embeddings and encoders.<br>\n    `n_head`: int=4, controls number of multi-head's attention.<br>\n    `dropout`: float (0, 1), dropout throughout Autoformer architecture.<br>\n\t`factor`: int=3, Probsparse attention factor.<br>\n\t`conv_hidden_size`: int=32, channels of the convolutional encoder.<br>\n\t`activation`: str=`GELU`, activation from ['ReLU', 'Softplus', 'Tanh', 'SELU', 'LeakyReLU', 'PReLU', 'Sigmoid', 'GELU'].<br>\n    `encoder_layers`: int=2, number of layers for the TCN encoder.<br>\n    `decoder_layers`: int=1, number of layers for the MLP decoder.<br>\n    `MovingAvg_window`: int=25, window size for the moving average filter.<br>\n    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `valid_loss`: PyTorch module, instantiated validation loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `max_steps`: int=1000, maximum number of training steps.<br>\n    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n    `batch_size`: int=32, number of different series in each batch.<br>\n    `valid_batch_size`: int=None, number of different series in each validation and test batch, if None uses batch_size.<br>\n    `windows_batch_size`: int=1024, number of windows to sample in each training batch, default uses all.<br>\n    `inference_windows_batch_size`: int=1024, number of windows to sample in each inference batch.<br>\n    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n    `scaler_type`: str='robust', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n    `random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n    `alias`: str, optional, Custom name of the model.<br>\n    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>\n    `dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>\n\n\t*References*<br>\n\t- [Wu, Haixu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. \"Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting\"](https://proceedings.neurips.cc/paper/2021/hash/bcc0d400288793e8bdcd7c19a8ac0c2b-Abstract.html)<br>\n    \"\"\"\n    # Class attributes\n    EXOGENOUS_FUTR = True\n    EXOGENOUS_HIST = False\n    EXOGENOUS_STAT = False\n    MULTIVARIATE = False    # If the model produces multivariate forecasts (True) or univariate (False)\n    RECURRENT = False       # If the model produces forecasts recursively (True) or direct (False)\n\n    def __init__(self,\n                 h: int, \n                 input_size: int,\n                 stat_exog_list = None,\n                 hist_exog_list = None,\n                 futr_exog_list = None,\n                 exclude_insample_y = False,\n                 decoder_input_size_multiplier: float = 0.5,\n                 hidden_size: int = 128, \n                 dropout: float = 0.05,\n                 factor: int = 3,\n                 n_head: int = 4,\n                 conv_hidden_size: int = 32,\n                 activation: str = 'gelu',\n                 encoder_layers: int = 2, \n                 decoder_layers: int = 1,\n                 MovingAvg_window: int = 25,\n                 loss = MAE(),\n                 valid_loss = None,\n                 max_steps: int = 5000,\n                 learning_rate: float = 1e-4,\n                 num_lr_decays: int = -1,\n                 early_stop_patience_steps: int =-1,\n                 val_check_steps: int = 100,\n                 batch_size: int = 32,\n                 valid_batch_size: Optional[int] = None,\n                 windows_batch_size = 1024,\n                 inference_windows_batch_size = 1024,\n                 start_padding_enabled = False,\n                 step_size: int = 1,\n                 scaler_type: str = 'identity',\n                 random_seed: int = 1,\n                 drop_last_loader: bool = False,\n                 alias: Optional[str] = None,\n                 optimizer = None,\n                 optimizer_kwargs = None,\n                 lr_scheduler = None,\n                 lr_scheduler_kwargs = None,\n                 dataloader_kwargs=None,\n                 **trainer_kwargs):\n        super(Autoformer, self).__init__(h=h,\n                                       input_size=input_size,\n                                       stat_exog_list=stat_exog_list,\n                                       hist_exog_list=hist_exog_list,\n                                       futr_exog_list = futr_exog_list,\n                                       exclude_insample_y = exclude_insample_y,\n                                       loss=loss,\n                                       valid_loss=valid_loss,\n                                       max_steps=max_steps,\n                                       learning_rate=learning_rate,\n                                       num_lr_decays=num_lr_decays,\n                                       early_stop_patience_steps=early_stop_patience_steps,\n                                       val_check_steps=val_check_steps,\n                                       batch_size=batch_size,\n                                       valid_batch_size=valid_batch_size,\n                                       windows_batch_size=windows_batch_size,\n                                       inference_windows_batch_size=inference_windows_batch_size,\n                                       start_padding_enabled = start_padding_enabled,\n                                       step_size=step_size,\n                                       scaler_type=scaler_type,\n                                       random_seed=random_seed,\n                                       drop_last_loader=drop_last_loader,\n                                       alias=alias,\n                                       optimizer=optimizer,\n                                       optimizer_kwargs=optimizer_kwargs,\n                                       lr_scheduler=lr_scheduler,\n                                       lr_scheduler_kwargs=lr_scheduler_kwargs,\n                                       dataloader_kwargs=dataloader_kwargs,\n                                       **trainer_kwargs)\n\n        # Architecture\n        self.label_len = int(np.ceil(input_size * decoder_input_size_multiplier))\n        if (self.label_len >= input_size) or (self.label_len <= 0):\n            raise Exception(f'Check decoder_input_size_multiplier={decoder_input_size_multiplier}, range (0,1)')\n\n        if activation not in ['relu', 'gelu']:\n            raise Exception(f'Check activation={activation}')\n        \n        self.c_out = self.loss.outputsize_multiplier\n        self.output_attention = False\n        self.enc_in = 1 \n        self.dec_in = 1\n\n        # Decomposition\n        self.decomp = SeriesDecomp(MovingAvg_window)\n\n        # Embedding\n        self.enc_embedding = DataEmbedding(c_in=self.enc_in,\n                                           exog_input_size=self.futr_exog_size,\n                                           hidden_size=hidden_size, \n                                           pos_embedding=False,\n                                           dropout=dropout)\n        self.dec_embedding = DataEmbedding(self.dec_in,\n\n```\n\n----------------------------------------\n\nTITLE: Forward Pass Implementation for NHITS Model in PyTorch\nDESCRIPTION: This method implements the forward pass of the NHITS neural forecasting model. It processes input batches containing time series data, masks, and exogenous variables, computes residuals through iterative backcast refinement, and can return either the final forecast or decomposed forecasts from each block.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nhits.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, windows_batch):\n        \n        # Parse windows_batch\n        insample_y    = windows_batch['insample_y'].squeeze(-1).contiguous()\n        insample_mask = windows_batch['insample_mask'].squeeze(-1).contiguous()\n        futr_exog     = windows_batch['futr_exog']\n        hist_exog     = windows_batch['hist_exog']\n        stat_exog     = windows_batch['stat_exog']\n        \n        # insample\n        residuals = insample_y.flip(dims=(-1,)) #backcast init\n        insample_mask = insample_mask.flip(dims=(-1,))\n        \n        forecast = insample_y[:, -1:, None] # Level with Naive1\n        block_forecasts = [ forecast.repeat(1, self.h, 1) ]\n        for i, block in enumerate(self.blocks):\n            backcast, block_forecast = block(insample_y=residuals, futr_exog=futr_exog,\n                                             hist_exog=hist_exog, stat_exog=stat_exog)\n            residuals = (residuals - backcast) * insample_mask\n            forecast = forecast + block_forecast\n            \n            if self.decompose_forecast:\n                block_forecasts.append(block_forecast)\n        \n        if self.decompose_forecast:\n            # (n_batch, n_blocks, h, output_size)\n            block_forecasts = torch.stack(block_forecasts)\n            block_forecasts = block_forecasts.permute(1,0,2,3)\n            block_forecasts = block_forecasts.squeeze(-1) # univariate output\n            return block_forecasts\n        else:\n            return forecast\n```\n\n----------------------------------------\n\nTITLE: Usage Example: Training and Predicting with FEDformer in Python\nDESCRIPTION: This comprehensive example demonstrates how to use the FEDformer model for time series forecasting with the NeuralForecast library. It involves importing necessary libraries (pandas, matplotlib, neuralforecast), preparing the AirPassengers dataset (splitting into train/test, augmenting with calendar features), initializing the FEDformer model with specific hyperparameters (horizon, input size, modes, hidden sizes, loss function, exogenous features, etc.), wrapping it in a NeuralForecast object, training the model (`nf.fit`), generating predictions (`nf.predict`), and finally plotting the actual values against the forecasts. The example also includes logic to handle and plot quantile forecasts if the loss function produces distributional output.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.fedformer.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n#| eval: false\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import FEDformer\nfrom neuralforecast.utils import AirPassengersPanel, augment_calendar_df\n\nAirPassengersPanel, calendar_cols = augment_calendar_df(df=AirPassengersPanel, freq='M')\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = FEDformer(h=12,\n                 input_size=24,\n                 modes=64,\n                 hidden_size=64,\n                 conv_hidden_size=128,\n                 n_head=8,\n                 loss=MAE(),\n                 futr_exog_list=calendar_cols,\n                 scaler_type='robust',\n                 learning_rate=1e-3,\n                 max_steps=500,\n                 batch_size=2,\n                 windows_batch_size=32,\n                 val_check_steps=50,\n                 early_stop_patience_steps=2)\n\nnf = NeuralForecast(\n    models=[model],\n    freq='ME',\n)\nnf.fit(df=Y_train_df, static_df=None, val_size=12)\nforecasts = nf.predict(futr_df=Y_test_df)\n\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nif model.loss.is_distribution_output:\n    plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n    plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n    plt.plot(plot_df['ds'], plot_df['FEDformer-median'], c='blue', label='median')\n    plt.fill_between(x=plot_df['ds'][-12:], \n                    y1=plot_df['FEDformer-lo-90'][-12:].values, \n                    y2=plot_df['FEDformer-hi-90'][-12:].values,\n                    alpha=0.4, label='level 90')\n    plt.grid()\n    plt.legend()\n    plt.plot()\nelse:\n    plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n    plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n    plt.plot(plot_df['ds'], plot_df['FEDformer'], c='blue', label='Forecast')\n    plt.legend()\n    plt.grid()\n```\n\n----------------------------------------\n\nTITLE: Implementing SOFTS Time Series Forecasting with NeuralForecast in Python\nDESCRIPTION: Demonstrates complete workflow of time series forecasting using the SOFTS model from data loading to visualization. Uses the AirPassengers dataset to train a model with specified architecture parameters, performs predictions, and creates a visualization comparing actual vs forecasted values.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.softs.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import SOFTS\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\nfrom neuralforecast.losses.pytorch import MASE\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = SOFTS(h=12,\n              input_size=24,\n              n_series=2,\n              hidden_size=256,\n              d_core=256,\n              e_layers=2,\n              d_ff=64,\n              dropout=0.1,\n              use_norm=True,\n              loss=MASE(seasonality=4),\n              early_stop_patience_steps=3,\n              batch_size=32)\n\nfcst = NeuralForecast(models=[model], freq='ME')\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\n# Plot predictions\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['SOFTS'], c='blue', label='Forecast')\nax.set_title('AirPassengers Forecast', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Year', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\n```\n\n----------------------------------------\n\nTITLE: NHITS Model Usage Example with AirPassengers Dataset\nDESCRIPTION: A complete example demonstrating how to use the NHITS model for forecasting with the AirPassengers dataset. It shows model initialization with specific hyperparameters, training with quantile forecasting, and visualization of the results including prediction intervals.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nhits.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import NHITS\nfrom neuralforecast.losses.pytorch import DistributionLoss\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = NHITS(h=12,\n              input_size=24,\n              loss=DistributionLoss(distribution='StudentT', level=[80, 90], return_params=True),\n              stat_exog_list=['airline1'],\n              futr_exog_list=['trend'],\n              n_freq_downsample=[2, 1, 1],\n              scaler_type='robust',\n              max_steps=200,\n              early_stop_patience_steps=2,\n              inference_windows_batch_size=1,\n              val_check_steps=10,\n              learning_rate=1e-3)\n\nfcst = NeuralForecast(models=[model], freq='ME')\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\n# Plot quantile predictions\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['NHITS-median'], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'][-12:], \n                 y1=plot_df['NHITS-lo-90'][-12:].values, \n                 y2=plot_df['NHITS-hi-90'][-12:].values,\n                 alpha=0.4, label='level 90')\nplt.legend()\nplt.grid()\nplt.plot()\n```\n\n----------------------------------------\n\nTITLE: Implementing AutoNBEATSx Class for Time Series Forecasting with Hyperparameter Tuning in Python\nDESCRIPTION: Defines the AutoNBEATSx class extending BaseAuto for automatic hyperparameter tuning of NBEATSx models for time series forecasting. Provides a default configuration with search spaces for hyperparameters, supporting both Ray Tune and Optuna backends.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_38\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass AutoNBEATSx(BaseAuto):\n\n    default_config = {\n        \"input_size_multiplier\": [1, 2, 3, 4, 5],\n        \"h\": None,\n        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n        \"scaler_type\": tune.choice([None, 'robust', 'standard']),\n        \"max_steps\": tune.choice([500, 1000]),\n        \"batch_size\": tune.choice([32, 64, 128, 256]),\n        \"windows_batch_size\": tune.choice([128, 256, 512, 1024]),\n        \"loss\": None,\n        \"random_seed\": tune.randint(1, 20),\n    }\n\n    def __init__(self,\n                 h,\n                 loss=MAE(),\n                 valid_loss=None,\n                 config=None, \n                 search_alg=BasicVariantGenerator(random_state=1),\n                 num_samples=10,\n                 refit_with_val=False,\n                 cpus=cpu_count(),\n                 gpus=torch.cuda.device_count(),\n                 verbose=False,\n                 alias=None,\n                 backend='ray',\n                 callbacks=None):\n        \n        # Define search space, input/output sizes\n        if config is None:\n            config = self.get_default_config(h=h, backend=backend)          \n\n        super(AutoNBEATSx, self).__init__(\n              cls_model=NBEATSx,\n              h=h,\n              loss=loss,\n              valid_loss=valid_loss,\n              config=config,\n              search_alg=search_alg,\n              num_samples=num_samples, \n              refit_with_val=refit_with_val,\n              cpus=cpus,\n              gpus=gpus,\n              verbose=verbose,\n              alias=alias,\n              backend=backend,\n              callbacks=callbacks,            \n        )\n\n    @classmethod\n    def get_default_config(cls, h, backend, n_series=None):\n        config = cls.default_config.copy()\n        config['input_size'] = tune.choice([h*x \\\n                        for x in config['input_size_multiplier']])\n        config['step_size'] = tune.choice([1, h])        \n        del config['input_size_multiplier']\n        if backend == 'optuna':\n            config = cls._ray_config_to_optuna(config)         \n\n        return config           \n```\n\n----------------------------------------\n\nTITLE: Forward Pass Implementation for DeepAR Model\nDESCRIPTION: This code implements the forward pass of the DeepAR model. It processes input data through the LSTM encoder and MLP decoder, handling both future and static exogenous variables, and maintains RNN state when required.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.deepar.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n    def forward(self, windows_batch):\n\n        # Parse windows_batch\n        encoder_input = windows_batch['insample_y'] # <- [B, T, 1]\n        futr_exog  = windows_batch['futr_exog']\n        stat_exog  = windows_batch['stat_exog']\n\n        _, input_size = encoder_input.shape[:2]\n        if self.futr_exog_size > 0:\n            encoder_input = torch.cat((encoder_input, futr_exog), dim=2)\n\n        if self.stat_exog_size > 0:\n            stat_exog = stat_exog.unsqueeze(1).repeat(1, input_size, 1)     # [B, S] -> [B, input_size-1, S]\n            encoder_input = torch.cat((encoder_input, stat_exog), dim=2)\n\n        # RNN forward\n        if self.maintain_state:\n            rnn_state = self.rnn_state\n        else:\n            rnn_state = None\n\n        hidden_state, rnn_state = self.hist_encoder(encoder_input, \n                                                    rnn_state)              # [B, input_size-1, rnn_hidden_state]\n\n        if self.maintain_state:\n            self.rnn_state = rnn_state\n\n        # Decoder forward\n        output = self.decoder(hidden_state)                                 # [B, input_size-1, output_size]\n\n        # Return only horizon part\n        return output[:, -self.h:]\n```\n\n----------------------------------------\n\nTITLE: Implementing AutoAutoformer Class for Hyperparameter Tuning\nDESCRIPTION: This code defines the AutoAutoformer class that inherits from BaseAuto. It provides automated hyperparameter tuning for the Autoformer model with a customizable search space. The class includes default configurations, initialization parameters, and methods for adapting configurations between Ray and Optuna backends.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_76\n\nLANGUAGE: python\nCODE:\n```\nclass AutoAutoformer(BaseAuto):\n\n    default_config = {\n        \"input_size_multiplier\": [1, 2, 3, 4, 5],\n        \"h\": None,\n        \"hidden_size\": tune.choice([64, 128, 256]),\n        \"n_head\": tune.choice([4, 8]),\n        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n        \"scaler_type\": tune.choice([None, 'robust', 'standard']),\n        \"max_steps\": tune.choice([500, 1000, 2000]),\n        \"batch_size\": tune.choice([32, 64, 128, 256]),\n        \"windows_batch_size\": tune.choice([128, 256, 512, 1024]),\n        \"loss\": None,\n        \"random_seed\": tune.randint(1, 20),\n    }\n\n    def __init__(self,\n                 h,\n                 loss=MAE(),\n                 valid_loss=None,\n                 config=None, \n                 search_alg=BasicVariantGenerator(random_state=1),\n                 num_samples=10,\n                 refit_with_val=False,\n                 cpus=cpu_count(),\n                 gpus=torch.cuda.device_count(),\n                 verbose=False,\n                 alias=None,\n                 backend='ray',\n                 callbacks=None):\n        \n        # Define search space, input/output sizes\n        if config is None:\n            config = self.get_default_config(h=h, backend=backend)          \n\n        super(AutoAutoformer, self).__init__(\n              cls_model=Autoformer, \n              h=h,\n              loss=loss,\n              valid_loss=valid_loss,\n              config=config,\n              search_alg=search_alg,\n              num_samples=num_samples, \n              refit_with_val=refit_with_val,\n              cpus=cpus,\n              gpus=gpus,\n              verbose=verbose,\n              alias=alias,\n              backend=backend,\n              callbacks=callbacks,            \n        )\n\n    @classmethod\n    def get_default_config(cls, h, backend, n_series=None):\n        config = cls.default_config.copy()\n        config['input_size'] = tune.choice([h*x \\\n                        for x in config['input_size_multiplier']])\n        config['step_size'] = tune.choice([1, h])        \n        del config['input_size_multiplier']\n        if backend == 'optuna':\n            config = cls._ray_config_to_optuna(config)         \n\n        return config                \n```\n\n----------------------------------------\n\nTITLE: Implementing TemporalMixing Layer for TSMixerx in Python\nDESCRIPTION: Defines the TemporalMixing class, a part of the mixing layer that performs temporal mixing using a Multi-Layer Perceptron (MLP) and layer normalization.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tsmixerx.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass TemporalMixing(nn.Module):\n    \"\"\" \n    TemporalMixing\n    \"\"\"      \n    def __init__(self, num_features, h, dropout):\n        super().__init__()\n        self.temporal_norm = nn.LayerNorm(normalized_shape=(h, num_features))\n        self.temporal_lin = nn.Linear(h, h)\n        self.temporal_drop = nn.Dropout(dropout)\n\n    def forward(self, input):\n        x = input.permute(0, 2, 1)                                      # [B, h, C] -> [B, C, h]\n        x = F.relu(self.temporal_lin(x))                                # [B, C, h] -> [B, C, h]\n        x = x.permute(0, 2, 1)                                          # [B, C, h] -> [B, h, C]\n        x = self.temporal_drop(x)                                       # [B, h, C] -> [B, h, C]\n\n        return self.temporal_norm(x + input)\n```\n\n----------------------------------------\n\nTITLE: Time Series Forecasting with NeuralForecast and NLinear Model\nDESCRIPTION: Comprehensive example showing how to perform time series forecasting using NeuralForecast library with the NLinear model. The code demonstrates data preprocessing, model configuration with StudentT distribution loss, training, prediction, and visualization of results with confidence intervals. Uses the Air Passengers dataset as an example.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nlinear.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import NLinear\nfrom neuralforecast.losses.pytorch import DistributionLoss\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic, augment_calendar_df\n\nAirPassengersPanel, calendar_cols = augment_calendar_df(df=AirPassengersPanel, freq='M')\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = NLinear(h=12,\n                 input_size=24,\n                 loss=DistributionLoss(distribution='StudentT', level=[80, 90], return_params=True),\n                 scaler_type='robust',\n                 learning_rate=1e-3,\n                 max_steps=500,\n                 val_check_steps=50,\n                 early_stop_patience_steps=2)\n\nnf = NeuralForecast(\n    models=[model],\n    freq='ME'\n)\nnf.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nforecasts = nf.predict(futr_df=Y_test_df)\n\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nif model.loss.is_distribution_output:\n    plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n    plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n    plt.plot(plot_df['ds'], plot_df['NLinear-median'], c='blue', label='median')\n    plt.fill_between(x=plot_df['ds'][-12:], \n                    y1=plot_df['NLinear-lo-90'][-12:].values, \n                    y2=plot_df['NLinear-hi-90'][-12:].values,\n                    alpha=0.4, label='level 90')\n    plt.grid()\n    plt.legend()\n    plt.plot()\nelse:\n    plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n    plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n    plt.plot(plot_df['ds'], plot_df['NLinear'], c='blue', label='Forecast')\n    plt.legend()\n    plt.grid()\n```\n\n----------------------------------------\n\nTITLE: Initializing and Fitting AutoTimeMixer Model in Python\nDESCRIPTION: This snippet demonstrates how to initialize an AutoTimeMixer model with custom configuration and fit it to a dataset. It includes examples for both Ray and Optuna backends, and shows how to make predictions using the fitted model.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_112\n\nLANGUAGE: python\nCODE:\n```\n# Use your own config or AutoTimeMixer.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12, d_model=16)\nmodel = AutoTimeMixer(h=12, n_series=1, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n# Optuna\nmodel = AutoTimeMixer(h=12, n_series=1, config=None, backend='optuna')\n```\n\n----------------------------------------\n\nTITLE: Implementing TSMixer Neural Network Model Class in Python\nDESCRIPTION: Complete implementation of the TSMixer class which inherits from BaseModel. This class creates an MLP-based multivariate time-series forecasting model that combines temporal and feature information using stacked mixing layers. It includes initialization parameters, model architecture definition, and the forward pass implementation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tsmixer.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass TSMixer(BaseModel):\n    \"\"\" TSMixer\n\n    Time-Series Mixer (`TSMixer`) is a MLP-based multivariate time-series forecasting model. `TSMixer` jointly learns temporal and cross-sectional representations of the time-series by repeatedly combining time- and feature information using stacked mixing layers. A mixing layer consists of a sequential time- and feature Multi Layer Perceptron (`MLP`).\n\n    **Parameters:**<br>\n    `h`: int, forecast horizon.<br>\n    `input_size`: int, considered autorregresive inputs (lags), y=[1,2,3,4] input_size=2 -> lags=[1,2].<br>\n    `n_series`: int, number of time-series.<br>\n    `futr_exog_list`: str list, future exogenous columns.<br>\n    `hist_exog_list`: str list, historic exogenous columns.<br>\n    `stat_exog_list`: str list, static exogenous columns.<br>\n    `exclude_insample_y`: bool=False, if True excludes the target variable from the input features.<br>\n    `n_block`: int=2, number of mixing layers in the model.<br>\n    `ff_dim`: int=64, number of units for the second feed-forward layer in the feature MLP.<br>\n    `dropout`: float=0.9, dropout rate between (0, 1) .<br>\n    `revin`: bool=True, if True uses Reverse Instance Normalization to process inputs and outputs.<br>\n    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `max_steps`: int=1000, maximum number of training steps.<br>\n    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n    `batch_size`: int=32, number of different series in each batch.<br>\n    `valid_batch_size`: int=None, number of different series in each validation and test batch, if None uses batch_size.<br>\n    `windows_batch_size`: int=32, number of windows to sample in each training batch, default uses all.<br>\n    `inference_windows_batch_size`: int=32, number of windows to sample in each inference batch, -1 uses all.<br>\n    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n    `step_size`: int=1, step size between each window of temporal data.<br>\n    `scaler_type`: str='identity', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n    `random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n    `alias`: str, optional,  Custom name of the model.<br>\n    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>    \n    `dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n\n    **References:**<br>\n    - [Chen, Si-An, Chun-Liang Li, Nate Yoder, Sercan O. Arik, and Tomas Pfister (2023). \"TSMixer: An All-MLP Architecture for Time Series Forecasting.\"](http://arxiv.org/abs/2303.06053)\n\n    \"\"\"\n    # Class attributes\n    EXOGENOUS_FUTR = False\n    EXOGENOUS_HIST = False\n    EXOGENOUS_STAT = False\n    MULTIVARIATE = True    # If the model produces multivariate forecasts (True) or univariate (False)\n    RECURRENT = False       # If the model produces forecasts recursively (True) or direct (False)\n\n    def __init__(self,\n                 h,\n                 input_size,\n                 n_series,\n                 futr_exog_list = None,\n                 hist_exog_list = None,\n                 stat_exog_list = None,\n                 exclude_insample_y = False,\n                 n_block = 2,\n                 ff_dim = 64,\n                 dropout = 0.9,\n                 revin = True,\n                 loss = MAE(),\n                 valid_loss = None,\n                 max_steps: int = 1000,\n                 learning_rate: float = 1e-3,\n                 num_lr_decays: int = -1,\n                 early_stop_patience_steps: int =-1,\n                 val_check_steps: int = 100,\n                 batch_size: int = 32,\n                 valid_batch_size: Optional[int] = None,\n                 windows_batch_size = 32,\n                 inference_windows_batch_size = 32,\n                 start_padding_enabled = False,\n                 step_size: int = 1,\n                 scaler_type: str = 'identity',\n                 random_seed: int = 1,\n                 drop_last_loader: bool = False,\n                 alias: Optional[str] = None,\n                 optimizer = None,\n                 optimizer_kwargs = None,\n                 lr_scheduler = None,\n                 lr_scheduler_kwargs = None,\n                 dataloader_kwargs = None,\n                 **trainer_kwargs):\n\n        # Inherit BaseMultivariate class\n        super(TSMixer, self).__init__(h=h,\n                                    input_size=input_size,\n                                    n_series=n_series,\n                                    futr_exog_list=futr_exog_list,\n                                    hist_exog_list=hist_exog_list,\n                                    stat_exog_list=stat_exog_list,\n                                    exclude_insample_y = exclude_insample_y,\n                                    loss=loss,\n                                    valid_loss=valid_loss,\n                                    max_steps=max_steps,\n                                    learning_rate=learning_rate,\n                                    num_lr_decays=num_lr_decays,\n                                    early_stop_patience_steps=early_stop_patience_steps,\n                                    val_check_steps=val_check_steps,\n                                    batch_size=batch_size,\n                                    valid_batch_size=valid_batch_size,\n                                    windows_batch_size=windows_batch_size,\n                                    inference_windows_batch_size=inference_windows_batch_size,\n                                    start_padding_enabled=start_padding_enabled,\n                                    step_size=step_size,\n                                    scaler_type=scaler_type,\n                                    random_seed=random_seed,\n                                    drop_last_loader=drop_last_loader,\n                                    alias=alias,\n                                    optimizer=optimizer,\n                                    optimizer_kwargs=optimizer_kwargs,\n                                    lr_scheduler=lr_scheduler,\n                                    lr_scheduler_kwargs=lr_scheduler_kwargs,\n                                    dataloader_kwargs=dataloader_kwargs,\n                                    **trainer_kwargs)\n\n        # Reversible InstanceNormalization layer\n        self.revin = revin\n        if self.revin:\n            self.norm = RevINMultivariate(num_features = n_series, affine=True)\n\n        # Mixing layers\n        mixing_layers = [MixingLayer(n_series=n_series, \n                                     input_size=input_size, \n                                     dropout=dropout, \n                                     ff_dim=ff_dim) \n                                     for _ in range(n_block)]\n        self.mixing_layers = nn.Sequential(*mixing_layers)\n\n        # Linear output with Loss dependent dimensions\n        self.out = nn.Linear(in_features=input_size, \n                             out_features=h * self.loss.outputsize_multiplier)\n\n    def forward(self, windows_batch):\n        # Parse batch\n        x = windows_batch['insample_y']  # x: [batch_size, input_size, n_series]\n        batch_size = x.shape[0]\n\n        # TSMixer: InstanceNorm + Mixing layers + Dense output layer + ReverseInstanceNorm\n        if self.revin:\n            x = self.norm(x, 'norm')\n        x = self.mixing_layers(x)\n        x = x.permute(0, 2, 1)\n        x = self.out(x)\n        x = x.permute(0, 2, 1)\n        if self.revin:\n            x = self.norm(x, 'denorm')\n\n        x = x.reshape(batch_size, self.h, self.loss.outputsize_multiplier * self.n_series)\n\n        return x\n```\n\n----------------------------------------\n\nTITLE: Implementing TimeMixer Neural Network Class in Python\nDESCRIPTION: A PyTorch-based implementation of the TimeMixer model for time series forecasting. The class inherits from BaseModel and includes comprehensive configuration options for model architecture, training parameters, and optimization settings. It features capabilities for handling multivariate time series, exogenous variables, and various decomposition methods.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timemixer.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass TimeMixer(BaseModel):\n    \"\"\" TimeMixer\n    **Parameters**<br>\n    `h`: int, Forecast horizon. <br>\n    `input_size`: int, autorregresive inputs size, y=[1,2,3,4] input_size=2 -> y_[t-2:t]=[1,2].<br>\n    `n_series`: int, number of time-series.<br>\n    `stat_exog_list`: str list, static exogenous columns.<br>\n    `hist_exog_list`: str list, historic exogenous columns.<br>\n    `futr_exog_list`: str list, future exogenous columns.<br>\n    `d_model`: int, dimension of the model.<br>\n    `d_ff`: int, dimension of the fully-connected network.<br>\n    `dropout`: float, dropout rate.<br>\n    `e_layers`: int, number of encoder layers.<br>\n    `top_k`: int, number of selected frequencies.<br>\n    `decomp_method`: str, method of series decomposition [moving_avg, dft_decomp].<br>\n    `moving_avg`: int, window size of moving average.<br>\n    `channel_independence`: int, 0: channel dependence, 1: channel independence.<br>\n    `down_sampling_layers`: int, number of downsampling layers.<br>\n    `down_sampling_window`: int, size of downsampling window.<br>\n    `down_sampling_method`: str, down sampling method [avg, max, conv].<br>\n    `use_norm`: bool, whether to normalize or not.<br>\n\t`decoder_input_size_multiplier`: float = 0.5.<br>\n    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `max_steps`: int=1000, maximum number of training steps.<br>\n    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n    `batch_size`: int=32, number of different series in each batch.<br>\n    `valid_batch_size`: int=None, number of different series in each validation and test batch, if None uses batch_size.<br>\n    `windows_batch_size`: int=32, number of windows to sample in each training batch, default uses all.<br>\n    `inference_windows_batch_size`: int=32, number of windows to sample in each inference batch, -1 uses all.<br>\n    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n    `step_size`: int=1, step size between each window of temporal data.<br>\n    `scaler_type`: str='identity', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n    `random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n    `alias`: str, optional,  Custom name of the model.<br>\n    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>\n    `dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n    `**trainer_kwargs`: keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>\n\n    **References**<br>\n    [Shiyu Wang, Haixu Wu, Xiaoming Shi, Tengge Hu, Huakun Luo, Lintao Ma, James Y. Zhang, Jun Zhou.\"TimeMixer: Decomposable Multiscale Mixing For Time Series Forecasting\"](https://openreview.net/pdf?id=7oLshfEIC2)<br>\n    \"\"\"\n\n    # Class attributes\n    EXOGENOUS_FUTR = False\n    EXOGENOUS_HIST = False\n    EXOGENOUS_STAT = False\n    MULTIVARIATE = True    # If the model produces multivariate forecasts (True) or univariate (False)\n    RECURRENT = False       # If the model produces forecasts recursively (True) or direct (False)\n\n    def __init__(self,\n                 h,\n                 input_size,\n                 n_series,\n                 stat_exog_list = None,\n                 hist_exog_list = None,\n                 futr_exog_list = None,\n                 d_model: int = 32,\n                 d_ff: int = 32,\n                 dropout: float = 0.1,\n                 e_layers: int = 4,\n                 top_k: int = 5,\n                 decomp_method: str = 'moving_avg',\n                 moving_avg: int = 25,\n                 channel_independence: int = 0,\n                 down_sampling_layers: int = 1,\n                 down_sampling_window: int = 2,\n                 down_sampling_method: str = 'avg',\n                 use_norm: bool = True,\n                 decoder_input_size_multiplier: float = 0.5,\n                 loss = MAE(),\n                 valid_loss = None,\n                 max_steps: int = 1000,\n                 learning_rate: float = 1e-3,\n                 num_lr_decays: int = -1,\n                 early_stop_patience_steps: int =-1,\n                 val_check_steps: int = 100,\n                 batch_size: int = 32,\n                 valid_batch_size: Optional[int] = None,\n                 windows_batch_size = 32,\n                 inference_windows_batch_size = 32,\n                 start_padding_enabled = False,\n                 step_size: int = 1,\n                 scaler_type: str = 'identity',\n                 random_seed: int = 1,\n                 drop_last_loader: bool = False,\n                 alias: Optional[str] = None,\n                 optimizer = None,\n                 optimizer_kwargs = None,\n                 lr_scheduler = None,\n                 lr_scheduler_kwargs = None,    \n                 dataloader_kwargs = None,        \n                 **trainer_kwargs):\n        \n        super(TimeMixer, self).__init__(h=h,\n                                    input_size=input_size,\n                                    n_series=n_series,\n                                    stat_exog_list = stat_exog_list,\n                                    futr_exog_list = futr_exog_list,\n                                    hist_exog_list = hist_exog_list,\n                                    loss=loss,\n                                    valid_loss=valid_loss,\n                                    max_steps=max_steps,\n                                    learning_rate=learning_rate,\n                                    num_lr_decays=num_lr_decays,\n                                    early_stop_patience_steps=early_stop_patience_steps,\n                                    val_check_steps=val_check_steps,\n                                    batch_size=batch_size,\n                                    valid_batch_size=valid_batch_size,\n                                    windows_batch_size=windows_batch_size,\n                                    inference_windows_batch_size=inference_windows_batch_size,\n                                    start_padding_enabled=start_padding_enabled,\n                                    step_size=step_size,\n                                    scaler_type=scaler_type,\n                                    random_seed=random_seed,\n                                    drop_last_loader=drop_last_loader,\n                                    alias=alias,\n                                    optimizer=optimizer,\n                                    optimizer_kwargs=optimizer_kwargs,\n                                    lr_scheduler=lr_scheduler,\n                                    lr_scheduler_kwargs=lr_scheduler_kwargs,\n                                    dataloader_kwargs=dataloader_kwargs,\n                                    **trainer_kwargs)\n        \n        self.label_len = int(np.ceil(input_size * decoder_input_size_multiplier))\n        if (self.label_len >= input_size) or (self.label_len <= 0):\n            raise Exception(f'Check decoder_input_size_multiplier={decoder_input_size_multiplier}, range (0,1)')\n        \n        self.h = h\n        self.input_size = input_size\n        self.e_layers = e_layers\n        self.d_model = d_model\n        self.d_ff = d_ff\n        self.dropout = dropout\n        self.top_k = top_k\n\n        self.use_norm = use_norm\n\n        self.use_future_temporal_feature = 0\n        if futr_exog_list is not None:\n            self.use_future_temporal_feature = 1\n\n        self.decomp_method = decomp_method\n        self.moving_avg = moving_avg\n        self.channel_independence = channel_independence\n\n        self.down_sampling_layers = down_sampling_layers\n        self.down_sampling_window = down_sampling_window\n        self.down_sampling_method = down_sampling_method\n\n        self.pdm_blocks = nn.ModuleList([PastDecomposableMixing(self.input_size, self.h, self.down_sampling_window, self.down_sampling_layers, self.d_model, self.dropout, self.channel_independence, self.decomp_method, self.d_ff, self.moving_avg, self.top_k)\n                                         for _ in range(self.e_layers)])\n        \n        self.preprocess = SeriesDecomp(self.moving_avg)\n        self.enc_in = n_series\n        self.c_out = n_series\n\n        if self.channel_independence == 1:\n            self.enc_embedding = DataEmbedding_wo_pos(1, self.d_model, self.dropout)\n        else:\n            self.enc_embedding = DataEmbedding_wo_pos(self.enc_in, self.d_model, self.dropout)\n\n        self.normalize_layers = torch.nn.ModuleList(\n            [\n                RevIN(self.enc_in, affine=True, non_norm=False if self.use_norm else True)\n```\n\n----------------------------------------\n\nTITLE: Fitting NeuralForecast Models\nDESCRIPTION: The main fit method for NeuralForecast models. It handles various input data formats, prepares the data, and fits the models. It also supports distributed training and prediction interval calculation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ndef fit(\n    self,\n    df: Optional[Union[DataFrame, SparkDataFrame, Sequence[str]]] = None,\n    static_df: Optional[Union[DataFrame, SparkDataFrame]] = None,\n    val_size: Optional[int] = 0,\n    use_init_models: bool = False,\n    verbose: bool = False,\n    id_col: str = 'unique_id',\n    time_col: str = 'ds',\n    target_col: str = 'y',\n    distributed_config: Optional[DistributedConfig] = None,\n    prediction_intervals: Optional[PredictionIntervals] = None,\n) -> None:\n    \"\"\"Fit the core.NeuralForecast.\n\n    Fit `models` to a large set of time series from DataFrame `df`.\n    and store fitted models for later inspection.\n\n    Parameters\n    ----------\n    df : pandas, polars or spark DataFrame, or a list of parquet files containing the series, optional (default=None)\n        DataFrame with columns [`unique_id`, `ds`, `y`] and exogenous variables.\n        If None, a previously stored dataset is required.\n    static_df : pandas, polars or spark DataFrame, optional (default=None)\n        DataFrame with columns [`unique_id`] and static exogenous.\n    val_size : int, optional (default=0)\n        Size of validation set.\n    use_init_models : bool, optional (default=False)\n        Use initial model passed when NeuralForecast object was instantiated.\n    verbose : bool (default=False)\n        Print processing steps.\n    id_col : str (default='unique_id')\n        Column that identifies each serie.\n    time_col : str (default='ds')\n        Column that identifies each timestep, its values can be timestamps or integers.\n    target_col : str (default='y')\n        Column that contains the target.\n    distributed_config : neuralforecast.DistributedConfig\n        Configuration to use for DDP training. Currently only spark is supported.\n    prediction_intervals : PredictionIntervals, optional (default=None)\n        Configuration to calibrate prediction intervals (Conformal Prediction).            \n\n    Returns\n    -------\n    self : NeuralForecast\n        Returns `NeuralForecast` class with fitted `models`.\n    \"\"\"\n    if (df is None) and not (hasattr(self, 'dataset')):\n        raise Exception('You must pass a DataFrame or have one stored.')\n\n    # Model and datasets interactions protections\n    if (\n        any(model.early_stop_patience_steps > 0 for model in self.models)\n        and val_size == 0\n    ):\n        raise Exception('Set val_size>0 if early stopping is enabled.')\n    \n    if (val_size is not None) and (0 < val_size < self.h):\n        raise ValueError(f'val_size must be either 0 or greater than or equal to the horizon: {self.h}')\n    \n    self._cs_df: Optional[DataFrame] = None\n    self.prediction_intervals: Optional[PredictionIntervals] = None\n\n    # Process and save new dataset (in self)\n    if isinstance(df, (pd.DataFrame, pl_DataFrame)):\n        validate_freq(df[time_col], self.freq)\n        self.dataset, self.uids, self.last_dates, self.ds = self._prepare_fit(\n            df=df,\n            static_df=static_df,\n            predict_only=False,\n            id_col=id_col,\n            time_col=time_col,\n            target_col=target_col,\n        )\n        if prediction_intervals is not None:\n            self.prediction_intervals = prediction_intervals\n            self._cs_df = self._conformity_scores(\n                df=df,\n                id_col=id_col,\n                time_col=time_col,\n                target_col=target_col,\n                static_df=static_df,\n            )\n\n    elif isinstance(df, SparkDataFrame):\n        if static_df is not None and not isinstance(static_df, SparkDataFrame):\n            raise ValueError(\n                \"`static_df` must be a spark dataframe when `df` is a spark dataframe.\"\n            )\n        self.dataset = self._prepare_fit_distributed(\n            df=df,\n            static_df=static_df,\n            id_col=id_col,\n            time_col=time_col,\n            target_col=target_col,\n            distributed_config=distributed_config,\n        )\n\n        if prediction_intervals is not None:\n            raise NotImplementedError(\"Prediction intervals are not supported for distributed training.\")\n\n    elif isinstance(df, Sequence):\n        if not all(isinstance(val, str) for val in df):\n            raise ValueError(\"All entries in the list of files must be of type string\")        \n        self.dataset = self._prepare_fit_for_local_files(\n            files_list=df,\n            static_df=static_df,\n            id_col=id_col,\n            time_col=time_col,\n            target_col=target_col,\n        )\n        self.uids = self.dataset.indices\n        self.last_dates = self.dataset.last_times\n        \n        if prediction_intervals is not None:\n            raise NotImplementedError(\"Prediction intervals are not supported for local files.\")\n        \n    elif df is None:\n        if verbose:\n            print(\"Using stored dataset.\")\n    else:\n        raise ValueError(\n            f\"`df` must be a pandas, polars or spark DataFrame, or a list of parquet files containing the series, or `None`, got: {type(df)}\"\n        )\n\n    if val_size is not None:\n        if self.dataset.min_size < val_size:\n            warnings.warn('Validation set size is larger than the shorter time-series.')\n\n    # Recover initial model if use_init_models\n    if use_init_models:\n        self._reset_models()\n\n    for i, model in enumerate(self.models):\n        self.models[i] = model.fit(\n            self.dataset, val_size=val_size, distributed_config=distributed_config\n        )\n\n    self._fitted = True\n```\n\n----------------------------------------\n\nTITLE: Forward Pass Implementation for TCN Model\nDESCRIPTION: Implementation of the forward method for the TCN model, which processes input windows, applies temporal convolution, adapts context, and generates forecasts. The method handles historical, static, and future exogenous variables.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tcn.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, windows_batch):\n    \n    # Parse windows_batch\n    encoder_input = windows_batch['insample_y']                         # [B, L, 1]\n    futr_exog     = windows_batch['futr_exog']                          # [B, L + h, F]\n    hist_exog     = windows_batch['hist_exog']                          # [B, L, X]\n    stat_exog     = windows_batch['stat_exog']                          # [B, S]\n\n    # Concatenate y, historic and static inputs              \n    batch_size, input_size = encoder_input.shape[:2]\n    if self.hist_exog_size > 0:\n        encoder_input = torch.cat((encoder_input, hist_exog), dim=2)    # [B, L, 1] + [B, L, X] -> [B, L, 1 + X]\n\n    if self.stat_exog_size > 0:\n        # print(encoder_input.shape)\n        stat_exog = stat_exog.unsqueeze(1).repeat(1, input_size, 1)     # [B, S] -> [B, L, S]\n        encoder_input = torch.cat((encoder_input, stat_exog), dim=2)    # [B, L, 1 + X] + [B, L, S] -> [B, L, 1 + X + S]\n\n    if self.futr_exog_size > 0:\n        encoder_input = torch.cat((encoder_input, \n                                   futr_exog[:, :input_size]), dim=2)   # [B, L, 1 + X + S] + [B, L, F] -> [B, L, 1 + X + S + F]\n\n    # TCN forward       \n    hidden_state = self.hist_encoder(encoder_input)                     # [B, L, C]\n\n    # Context adapter\n    hidden_state = hidden_state.permute(0, 2, 1)                        # [B, L, C] -> [B, C, L]\n    context = self.context_adapter(hidden_state)                        # [B, C, L] -> [B, C, h]\n\n    # Residual connection with futr_exog\n    if self.futr_exog_size > 0:\n        futr_exog_futr = futr_exog[:, input_size:].swapaxes(1, 2)       # [B, L + h, F] -> [B, F, h] \n        context = torch.cat((context, futr_exog_futr), dim=1)           # [B, C, h] + [B, F, h] = [B, C + F, h]\n\n    context = context.swapaxes(1, 2)                                    # [B, C + F, h] -> [B, h, C + F]\n\n    # Final forecast\n    output = self.mlp_decoder(context)                                  # [B, h, C + F] -> [B, h, n_output]\n    \n    return output\n```\n\n----------------------------------------\n\nTITLE: Implementing AutoTiDE Class for Temporal Fusion Decoder Forecasting\nDESCRIPTION: Definition of the AutoTiDE class that inherits from BaseAuto to provide automated hyperparameter tuning for the TiDE (Temporal Fusion Decoder) model, with configurable network architecture parameters and training settings.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_53\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass AutoTiDE(BaseAuto):\n\n    default_config = {\n       \"input_size_multiplier\": [1, 2, 3, 4, 5],\n       \"h\": None,\n       \"hidden_size\": tune.choice([256, 512, 1024]),\n       \"decoder_output_dim\": tune.choice([8, 16, 32]),\n       \"temporal_decoder_dim\": tune.choice([32, 64, 128]),\n       \"num_encoder_layers\": tune.choice([1, 2, 3]),\n       \"num_decoder_layers\": tune.choice([1, 2, 3]),\n       \"temporal_width\": tune.choice([4, 8, 16]),\n       \"dropout\":tune.choice([0.0, 0.1, 0.2, 0.3, 0.5]),\n       \"layernorm\": tune.choice([True, False]),\n       \"learning_rate\": tune.loguniform(1e-5, 1e-2),\n       \"scaler_type\": tune.choice([None, 'robust', 'standard']),\n       \"max_steps\": tune.quniform(lower=500, upper=1500, q=100),\n       \"batch_size\": tune.choice([32, 64, 128, 256]),\n       \"windows_batch_size\": tune.choice([128, 256, 512, 1024]),\n       \"loss\": None,\n       \"random_seed\": tune.randint(lower=1, upper=20),\n    }\n\n    def __init__(self,\n                 h,\n                 loss=MAE(),\n                 valid_loss=None,\n                 config=None, \n                 search_alg=BasicVariantGenerator(random_state=1),\n                 num_samples=10,\n                 refit_with_val=False,\n                 cpus=cpu_count(),\n                 gpus=torch.cuda.device_count(),\n                 verbose=False,\n                 alias=None,\n                 backend='ray',\n                 callbacks=None,\n                ):\n\n        # Define search space, input/output sizes\n        if config is None:\n            config = self.get_default_config(h=h, backend=backend)                 \n\n        super(AutoTiDE, self).__init__(\n              cls_model=TiDE, \n              h=h,\n              loss=loss,\n              valid_loss=valid_loss,\n              config=config,\n              search_alg=search_alg,\n              num_samples=num_samples,\n              refit_with_val=refit_with_val,\n              cpus=cpus,\n              gpus=gpus,\n              verbose=verbose,\n              alias=alias,\n              backend=backend,\n              callbacks=callbacks,\n        )\n\n    @classmethod\n    def get_default_config(cls, h, backend, n_series=None):\n        config = cls.default_config.copy()\n        config['input_size'] = tune.choice([h*x \\\n                        for x in config['input_size_multiplier']])\n        config['step_size'] = tune.choice([1, h])        \n        del config['input_size_multiplier']\n        if backend == 'optuna':\n            config = cls._ray_config_to_optuna(config)         \n\n        return config    \n```\n\n----------------------------------------\n\nTITLE: Implementing AutoMLPMultivariate Class for Automated MLP-based Forecasting in Python\nDESCRIPTION: This class implements an automated machine learning model for multivariate time series forecasting using MLPMultivariate as the base model. It supports both Ray and Optuna backends for hyperparameter tuning and includes methods for configuration and initialization.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_108\n\nLANGUAGE: python\nCODE:\n```\nclass AutoMLPMultivariate(BaseAuto):\n\n    default_config = {\n        \"input_size_multiplier\": [1, 2, 3, 4, 5],\n        \"h\": None,\n        \"n_series\": None,\n        \"hidden_size\": tune.choice( [256, 512, 1024] ),\n        \"num_layers\": tune.randint(2, 6),\n        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n        \"scaler_type\": tune.choice([None, 'robust', 'standard']),\n        \"max_steps\": tune.choice([500, 1000]),\n        \"batch_size\": tune.choice([32, 64, 128, 256]),\n        \"loss\": None,\n        \"random_seed\": tune.randint(1, 20),\n    }\n\n    def __init__(self,\n                 h,\n                 n_series,\n                 loss=MAE(),\n                 valid_loss=None,     \n                 config=None,\n                 search_alg=BasicVariantGenerator(random_state=1),\n                 num_samples=10,\n                 refit_with_val=False,\n                 cpus=cpu_count(),\n                 gpus=torch.cuda.device_count(),\n                 verbose=False,\n                 alias=None,\n                 backend='ray',\n                 callbacks=None):\n\n        # Define search space, input/output sizes\n        if config is None:\n            config = self.get_default_config(h=h, backend=backend, n_series=n_series)         \n\n        # Always use n_series from parameters, raise exception with Optuna because we can't enforce it\n        if backend == 'ray':\n            config['n_series'] = n_series\n        elif backend == 'optuna':\n            mock_trial = MockTrial()\n            if ('n_series' in config(mock_trial) and config(mock_trial)['n_series'] != n_series) or ('n_series' not in config(mock_trial)):\n                raise Exception(f\"config needs 'n_series': {n_series}\")  \n\n        super(AutoMLPMultivariate, self).__init__(\n              cls_model=MLPMultivariate,\n              h=h,\n              loss=loss,\n              valid_loss=valid_loss,\n              config=config, \n              search_alg=search_alg,\n              num_samples=num_samples, \n              refit_with_val=refit_with_val,\n              cpus=cpus,\n              gpus=gpus,\n              verbose=verbose,\n              alias=alias,\n              backend=backend,\n              callbacks=callbacks,            \n        )\n\n    @classmethod\n    def get_default_config(cls, h, backend, n_series):\n        config = cls.default_config.copy()        \n        config['input_size'] = tune.choice([h * x \\\n                        for x in config[\"input_size_multiplier\"]])\n\n        # Rolling windows with step_size=1 or step_size=h\n        # See `BaseWindows` and `BaseRNN`'s create_windows\n        config['step_size'] = tune.choice([1, h])\n        del config[\"input_size_multiplier\"]\n        if backend == 'optuna':\n            # Always use n_series from parameters\n            config['n_series'] = n_series\n            config = cls._ray_config_to_optuna(config)           \n\n        return config\n```\n\n----------------------------------------\n\nTITLE: Training Step Implementation for Neural Network Forecasting Model in Python\nDESCRIPTION: This method implements the training step for the neural network forecasting model. It handles both distribution and non-distribution outputs, computes the loss, and logs training metrics. The method also includes error handling for NaN losses.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_31\n\nLANGUAGE: Python\nCODE:\n```\ndef training_step(self, batch, batch_idx):\n        # Create and normalize windows [Ws, L + h, C, n_series]\n        windows = self._create_windows(batch, step='train')\n        y_idx = batch['y_idx']\n        original_outsample_y = torch.clone(windows['temporal'][:, self.input_size:, y_idx])\n        windows = self._normalization(windows=windows, y_idx=y_idx)\n\n        # Parse windows\n        insample_y, insample_mask, outsample_y, outsample_mask, \\\n            hist_exog, futr_exog, stat_exog = self._parse_windows(batch, windows)\n\n        windows_batch = dict(insample_y=insample_y,                 # [Ws, L, n_series]\n                        insample_mask=insample_mask,                # [Ws, L, n_series]\n                        futr_exog=futr_exog,                        # univariate: [Ws, L, F]; multivariate: [Ws, F, L, n_series]\n                        hist_exog=hist_exog,                        # univariate: [Ws, L, X]; multivariate: [Ws, X, L, n_series]\n                        stat_exog=stat_exog)                        # univariate: [Ws, S]; multivariate: [n_series, S]\n\n        # Model Predictions\n        output = self(windows_batch)\n        output = self.loss.domain_map(output)\n        \n        if self.loss.is_distribution_output:\n            y_loc, y_scale = self._get_loc_scale(y_idx)\n            outsample_y = original_outsample_y\n            distr_args = self.loss.scale_decouple(output=output, loc=y_loc, scale=y_scale)\n            loss = self.loss(y=outsample_y, distr_args=distr_args, mask=outsample_mask)\n        else:\n            loss = self.loss(y=outsample_y, y_hat=output, y_insample=insample_y, mask=outsample_mask)\n\n        if torch.isnan(loss):\n            print('Model Parameters', self.hparams)\n            print('insample_y', torch.isnan(insample_y).sum())\n            print('outsample_y', torch.isnan(outsample_y).sum())\n            raise Exception('Loss is NaN, training stopped.')\n\n        train_loss_log = loss.detach().item()\n        self.log(\n            'train_loss',\n            train_loss_log,\n            batch_size=outsample_y.size(0),\n            prog_bar=True,\n            on_epoch=True,\n        )\n        self.train_trajectories.append((self.global_step, train_loss_log))\n\n        self.h = self.horizon_backup\n\n        return loss\n```\n\n----------------------------------------\n\nTITLE: Validation Step Implementation for Neural Network Forecasting Model in Python\nDESCRIPTION: This method implements the validation step for the neural network forecasting model. It handles batching of windows, computes the validation loss, and logs validation metrics. The method supports both recurrent and non-recurrent models.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_32\n\nLANGUAGE: Python\nCODE:\n```\ndef validation_step(self, batch, batch_idx):\n        if self.val_size == 0:\n            return np.nan\n\n        # TODO: Hack to compute number of windows\n        windows = self._create_windows(batch, step='val')\n        n_windows = len(windows['temporal'])\n        y_idx = batch['y_idx']\n\n        # Number of windows in batch\n        windows_batch_size = self.inference_windows_batch_size\n        if windows_batch_size < 0:\n            windows_batch_size = n_windows\n        n_batches = int(np.ceil(n_windows / windows_batch_size))\n\n        valid_losses = []\n        batch_sizes = []\n        for i in range(n_batches):\n            # Create and normalize windows [Ws, L + h, C, n_series]\n            w_idxs = np.arange(i*windows_batch_size, \n                               min((i+1)*windows_batch_size, n_windows))\n            windows = self._create_windows(batch, step='val', w_idxs=w_idxs)\n            original_outsample_y = torch.clone(windows['temporal'][:, self.input_size:, y_idx])\n\n            windows = self._normalization(windows=windows, y_idx=y_idx)\n\n            # Parse windows\n            insample_y, insample_mask, _, outsample_mask, \\\n                hist_exog, futr_exog, stat_exog = self._parse_windows(batch, windows)\n\n            if self.RECURRENT:\n                output_batch = self._validate_step_recurrent_batch(insample_y=insample_y,\n                                                           insample_mask=insample_mask,\n                                                           futr_exog=futr_exog,\n                                                           hist_exog=hist_exog,\n                                                           stat_exog=stat_exog,\n                                                           y_idx=y_idx)\n            else:       \n                windows_batch = dict(insample_y=insample_y,                 # [Ws, L, n_series]\n                                insample_mask=insample_mask,                # [Ws, L, n_series]\n                                futr_exog=futr_exog,                        # univariate: [Ws, L, F]; multivariate: [Ws, F, L, n_series]\n                                hist_exog=hist_exog,                        # univariate: [Ws, L, X]; multivariate: [Ws, X, L, n_series]\n                                stat_exog=stat_exog)                        # univariate: [Ws, S]; multivariate: [n_series, S]\n                \n                # Model Predictions\n                output_batch = self(windows_batch)   \n\n            output_batch = self.loss.domain_map(output_batch)\n            valid_loss_batch = self._compute_valid_loss(insample_y=insample_y,\n                                                        outsample_y=original_outsample_y,\n                                                output=output_batch, \n                                                outsample_mask=outsample_mask,\n                                                y_idx=batch['y_idx'])\n            valid_losses.append(valid_loss_batch)\n            batch_sizes.append(len(output_batch))\n        \n        valid_loss = torch.stack(valid_losses)\n        batch_sizes = torch.tensor(batch_sizes, device=valid_loss.device)\n        batch_size = torch.sum(batch_sizes)\n        valid_loss = torch.sum(valid_loss * batch_sizes) / batch_size\n\n        if torch.isnan(valid_loss):\n            raise Exception('Loss is NaN, training stopped.')\n\n        valid_loss_log = valid_loss.detach()\n        self.log(\n            'valid_loss',\n            valid_loss_log.item(),\n            batch_size=batch_size,\n            prog_bar=True,\n            on_epoch=True,\n        )\n        self.validation_step_outputs.append(valid_loss_log)\n        return valid_loss\n```\n\n----------------------------------------\n\nTITLE: Implementing FEDformer Neural Network Model Class in Python\nDESCRIPTION: The FEDformer class inherits from BaseModel and implements a frequency-enhanced deep transformer model for time series forecasting. It includes comprehensive initialization parameters for configuring model architecture, training behavior, and optimization strategies.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.fedformer.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass FEDformer(BaseModel):\n    \"\"\" FEDformer\n\n    The FEDformer model tackles the challenge of finding reliable dependencies on intricate temporal patterns of long-horizon forecasting.\n\n    The architecture has the following distinctive features:\n    - In-built progressive decomposition in trend and seasonal components based on a moving average filter.\n    - Frequency Enhanced Block and Frequency Enhanced Attention to perform attention in the sparse representation on basis such as Fourier transform.\n    - Classic encoder-decoder proposed by Vaswani et al. (2017) with a multi-head attention mechanism.\n\n    The FEDformer model utilizes a three-component approach to define its embedding:\n    - It employs encoded autoregressive features obtained from a convolution network.\n    - Absolute positional embeddings obtained from calendar features are utilized.\n\n    *Parameters:*<br>\n    `h`: int, forecast horizon.<br>\n    `input_size`: int, maximum sequence length for truncated train backpropagation. <br>\n    `stat_exog_list`: str list, static exogenous columns.<br>\n    `hist_exog_list`: str list, historic exogenous columns.<br>\n    `futr_exog_list`: str list, future exogenous columns.<br>\n\t`decoder_input_size_multiplier`: float = 0.5, .<br>\n    `version`: str = 'Fourier', version of the model.<br>\n    `modes`: int = 64, number of modes for the Fourier block.<br>\n    `mode_select`: str = 'random', method to select the modes for the Fourier block.<br>\n    `hidden_size`: int=128, units of embeddings and encoders.<br>\n    `dropout`: float (0, 1), dropout throughout Autoformer architecture.<br>\n    `n_head`: int=8, controls number of multi-head's attention.<br>\n\t`conv_hidden_size`: int=32, channels of the convolutional encoder.<br>\n\t`activation`: str=`GELU`, activation from ['ReLU', 'Softplus', 'Tanh', 'SELU', 'LeakyReLU', 'PReLU', 'Sigmoid', 'GELU'].<br>\n    `encoder_layers`: int=2, number of layers for the TCN encoder.<br>\n    `decoder_layers`: int=1, number of layers for the MLP decoder.<br>\n    `MovingAvg_window`: int=25, window size for the moving average filter.<br>\n    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `valid_loss`: PyTorch module, instantiated validation loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `max_steps`: int=1000, maximum number of training steps.<br>\n    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n    `batch_size`: int=32, number of different series in each batch.<br>\n    `valid_batch_size`: int=None, number of different series in each validation and test batch, if None uses batch_size.<br>\n    `windows_batch_size`: int=1024, number of windows to sample in each training batch, default uses all.<br>\n    `inference_windows_batch_size`: int=1024, number of windows to sample in each inference batch.<br>\n    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n    `step_size`: int=1, step size between each window of temporal data.<br>    \n    `scaler_type`: str='robust', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n    `random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n    `alias`: str, optional,  Custom name of the model.<br>\n    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>\n    `dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>\n\n    \"\"\"\n    # Class attributes\n    EXOGENOUS_FUTR = True\n    EXOGENOUS_HIST = False\n    EXOGENOUS_STAT = False\n    MULTIVARIATE = False    # If the model produces multivariate forecasts (True) or univariate (False)\n    RECURRENT = False       # If the model produces forecasts recursively (True) or direct (False)\n\n    def __init__(self,\n                 h: int, \n                 input_size: int,\n                 stat_exog_list = None,\n                 hist_exog_list = None,\n                 futr_exog_list = None,\n                 decoder_input_size_multiplier: float = 0.5,\n                 version: str = 'Fourier',\n                 modes: int = 64,\n                 mode_select: str = 'random',\n                 hidden_size: int = 128, \n                 dropout: float = 0.05,\n                 n_head: int = 8,\n                 conv_hidden_size: int = 32,\n                 activation: str = 'gelu',\n                 encoder_layers: int = 2, \n                 decoder_layers: int = 1,\n                 MovingAvg_window: int = 25,\n                 loss = MAE(),\n                 valid_loss = None,\n                 max_steps: int = 5000,\n                 learning_rate: float = 1e-4,\n                 num_lr_decays: int = -1,\n                 early_stop_patience_steps: int =-1,\n                 val_check_steps: int = 100,\n                 batch_size: int = 32,\n                 valid_batch_size: Optional[int] = None,\n                 windows_batch_size = 1024,\n                 inference_windows_batch_size = 1024,\n                 start_padding_enabled = False,\n                 step_size: int = 1,\n                 scaler_type: str = 'identity',\n                 random_seed: int = 1,\n                 drop_last_loader: bool = False,\n                 alias: Optional[str] = None,\n                 optimizer=None,\n                 optimizer_kwargs=None,\n                 lr_scheduler = None,\n                 lr_scheduler_kwargs = None,\n                 dataloader_kwargs = None,\n                 **trainer_kwargs):\n        super(FEDformer, self).__init__(h=h,\n                                       input_size=input_size,\n                                       stat_exog_list=stat_exog_list,\n                                       hist_exog_list=hist_exog_list,\n                                       futr_exog_list = futr_exog_list,\n                                       loss=loss,\n                                       valid_loss=valid_loss,\n                                       max_steps=max_steps,\n                                       learning_rate=learning_rate,\n                                       num_lr_decays=num_lr_decays,\n                                       early_stop_patience_steps=early_stop_patience_steps,\n                                       val_check_steps=val_check_steps,\n                                       batch_size=batch_size,\n                                       valid_batch_size=valid_batch_size,\n                                       windows_batch_size=windows_batch_size,\n                                       inference_windows_batch_size=inference_windows_batch_size,\n                                       start_padding_enabled=start_padding_enabled,\n                                       step_size=step_size,\n                                       scaler_type=scaler_type,\n                                       random_seed=random_seed,\n                                       drop_last_loader=drop_last_loader,\n                                       alias=alias,\n                                       optimizer=optimizer,\n                                       optimizer_kwargs=optimizer_kwargs,\n                                       lr_scheduler=lr_scheduler,\n                                       lr_scheduler_kwargs=lr_scheduler_kwargs,\n                                       dataloader_kwargs=dataloader_kwargs,                                    \n                                       **trainer_kwargs)\n        # Architecture\n        self.label_len = int(np.ceil(input_size * decoder_input_size_multiplier))\n        if (self.label_len >= input_size) or (self.label_len <= 0):\n            raise Exception(f'Check decoder_input_size_multiplier={decoder_input_size_multiplier}, range (0,1)')\n\n        if activation not in ['relu', 'gelu']:\n            raise Exception(f'Check activation={activation}')\n        \n        if n_head != 8:\n            raise Exception('n_head must be 8')\n        \n        if version not in ['Fourier']:\n            raise Exception('Only Fourier version is supported currently.')\n\n        self.c_out = self.loss.outputsize_multiplier\n        self.output_attention = False\n        self.enc_in = 1 \n        self.dec_in = 1\n        \n        self.decomp = SeriesDecomp(MovingAvg_window)\n\n        # Embedding\n        self.enc_embedding = DataEmbedding(c_in=self.enc_in,\n                                           exog_input_size=self.futr_exog_size,\n                                           hidden_size=hidden_size, \n                                           pos_embedding=False,\n                                           dropout=dropout)\n        self.dec_embedding = DataEmbedding(self.dec_in,\n\n```\n\n----------------------------------------\n\nTITLE: Implementing TimeXer Neural Network Class in Python\nDESCRIPTION: A PyTorch implementation of the TimeXer model class that extends BaseModel. The model uses transformer architecture with attention mechanisms for time series forecasting. It supports various parameters for customization including forecast horizon, input size, number of series, and exogenous variables handling.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timexer.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass TimeXer(BaseModel):\n    \"\"\"\n    TimeXer\n\n    **Parameters:**<br>\n    `h`: int, Forecast horizon. <br>\n    `input_size`: int, autorregresive inputs size, y=[1,2,3,4] input_size=2 -> y_[t-2:t]=[1,2].<br>\n    `n_series`: int, number of time-series.<br>\n    `futr_exog_list`: str list, future exogenous columns.<br>\n    `hist_exog_list`: str list, historic exogenous columns.<br>\n    `stat_exog_list`: str list, static exogenous columns.<br>\n    `patch_len`: int, length of patches.<br>\n    `hidden_size`: int, dimension of the model.<br>\n    `n_heads`: int, number of heads.<br>\n    `e_layers`: int, number of encoder layers.<br>\n    `d_ff`: int, dimension of fully-connected layer.<br>\n    `factor`: int, attention factor.<br>\n    `dropout`: float, dropout rate.<br>\n    `use_norm`: bool, whether to normalize or not.<br>\n    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `max_steps`: int=1000, maximum number of training steps.<br>\n    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n    `batch_size`: int=32, number of different series in each batch.<br>\n    `valid_batch_size`: int=None, number of different series in each validation and test batch, if None uses batch_size.<br>\n    `windows_batch_size`: int=32, number of windows in each batch.<br>    \n    `inference_windows_batch_size`: int=32, number of windows to sample in each inference batch, -1 uses all.<br>\n    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n    `step_size`: int=1, step size between each window of temporal data.<br>\n    `scaler_type`: str='identity', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n    `random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n    `alias`: str, optional,  Custom name of the model.<br>\n    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>\n    `dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>\n\n    **Parameters:**<br>\n\n    **References**\n    - [Yuxuan Wang, Haixu Wu, Jiaxiang Dong, Guo Qin, Haoran Zhang, Yong Liu, Yunzhong Qiu, Jianmin Wang, Mingsheng Long. \"TimeXer: Empowering Transformers for Time Series Forecasting with Exogenous Variables\"](https://arxiv.org/abs/2402.19072)\n    \"\"\"\n\n    # Class attributes\n    EXOGENOUS_FUTR = True\n    EXOGENOUS_HIST = False\n    EXOGENOUS_STAT = False\n    MULTIVARIATE = True    # If the model produces multivariate forecasts (True) or univariate (False)\n    RECURRENT = False       # If the model produces forecasts recursively (True) or direct (False)\n\n    def __init__(self,\n                 h,\n                 input_size,\n                 n_series,\n                 futr_exog_list = None,\n                 hist_exog_list = None,\n                 stat_exog_list = None,\n                 exclude_insample_y: bool = False,\n                 patch_len: int = 16,\n                 hidden_size: int = 512,\n                 n_heads: int = 8,\n                 e_layers: int = 2,\n                 d_ff: int = 2048,\n                 factor: int = 1,\n                 dropout: float = 0.1,\n                 use_norm: bool = True,\n                 loss = MAE(),\n                 valid_loss = None,\n                 max_steps: int = 1000,\n                 learning_rate: float = 1e-3,\n                 num_lr_decays: int = -1,\n                 early_stop_patience_steps: int =-1,\n                 val_check_steps: int = 100,\n                 batch_size: int = 32,\n                 valid_batch_size: Optional[int] = None,\n                 windows_batch_size = 32,\n                 inference_windows_batch_size = 32,\n                 start_padding_enabled = False,\n                 step_size: int = 1,\n                 scaler_type: str = 'identity',\n                 random_seed: int = 1,\n                 drop_last_loader: bool = False,\n                 alias: Optional[str] = None,\n                 optimizer = None,\n                 optimizer_kwargs = None,\n                 lr_scheduler = None,\n                 lr_scheduler_kwargs = None,\n                 dataloader_kwargs = None,\n                 **trainer_kwargs):\n        \n        super(TimeXer, self).__init__(h=h,\n                                    input_size=input_size,\n                                    n_series=n_series,\n                                    futr_exog_list=futr_exog_list,\n                                    hist_exog_list=hist_exog_list,\n                                    stat_exog_list=stat_exog_list,\n                                    exclude_insample_y=exclude_insample_y,\n                                    loss=loss,\n                                    valid_loss=valid_loss,\n                                    max_steps=max_steps,\n                                    learning_rate=learning_rate,\n                                    num_lr_decays=num_lr_decays,\n                                    early_stop_patience_steps=early_stop_patience_steps,\n                                    val_check_steps=val_check_steps,\n                                    batch_size=batch_size,\n                                    valid_batch_size=valid_batch_size,\n                                    windows_batch_size=windows_batch_size,\n                                    inference_windows_batch_size=inference_windows_batch_size,\n                                    start_padding_enabled=start_padding_enabled,\n                                    step_size=step_size,\n                                    scaler_type=scaler_type,\n                                    random_seed=random_seed,\n                                    drop_last_loader=drop_last_loader,\n                                    alias=alias,\n                                    optimizer=optimizer,\n                                    optimizer_kwargs=optimizer_kwargs,\n                                    lr_scheduler=lr_scheduler,\n                                    lr_scheduler_kwargs=lr_scheduler_kwargs,\n                                    dataloader_kwargs=dataloader_kwargs,\n                                    **trainer_kwargs)\n        \n        self.enc_in = n_series\n        self.hidden_size = hidden_size\n        self.n_heads = n_heads\n        self.e_layers = e_layers\n        self.d_ff = d_ff\n        self.dropout = dropout\n        self.factor = factor\n        self.patch_len = patch_len\n        self.use_norm = use_norm\n        self.patch_num = int(input_size // self.patch_len)\n\n        # Architecture\n        self.en_embedding = EnEmbedding(n_series, self.hidden_size, self.patch_len, self.dropout)\n        self.ex_embedding = DataEmbedding_inverted(input_size, self.hidden_size, self.dropout)\n\n        self.encoder = Encoder(\n            [\n                EncoderLayer(\n                    AttentionLayer(\n                        FullAttention(False, self.factor, attention_dropout=self.dropout,\n                                      output_attention=False),\n                        self.hidden_size, self.n_heads),\n                    AttentionLayer(\n                        FullAttention(False, self.factor, attention_dropout=self.dropout,\n                                      output_attention=False),\n                        self.hidden_size, self.n_heads),\n                    self.hidden_size,\n                    self.d_ff,\n                    dropout=self.dropout,\n                    activation='relu',\n                )\n                for l in range(self.e_layers)\n            ],\n            norm_layer=torch.nn.LayerNorm(self.hidden_size)\n        )\n        self.head_nf = self.hidden_size * (self.patch_num + 1)\n        self.head = FlattenHead(self.enc_in, self.head_nf, h * self.loss.outputsize_multiplier,\n                                head_dropout=self.dropout)\n        \n    def forecast(self, x_enc, x_mark_enc):\n        if self.use_norm:\n            # Normalization from Non-stationary Transformer\n            means = x_enc.mean(1, keepdim=True).detach()\n            x_enc = x_enc - means\n            stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)\n            x_enc /= stdev\n\n        _, _, N = x_enc.shape\n\n        en_embed, n_vars = self.en_embedding(x_enc.permute(0, 2, 1))\n        ex_embed = self.ex_embedding(x_enc, x_mark_enc)\n\n        enc_out = self.encoder(en_embed, ex_embed)\n        enc_out = torch.reshape(\n            enc_out, (-1, n_vars, enc_out.shape[-2], enc_out.shape[-1]))\n        # z: [bs x nvars x d_model x patch_num]\n```\n\n----------------------------------------\n\nTITLE: Model Prediction Method for Neural Network Forecasting Model in Python\nDESCRIPTION: This method implements the prediction process for the neural network forecasting model. It uses a PyTorch Lightning Trainer to execute the predict_step method. The method supports customization of test size, step size, and quantile predictions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_35\n\nLANGUAGE: Python\nCODE:\n```\ndef predict(self, dataset, test_size=None, step_size=1,\n                random_seed=None, quantiles=None, **data_module_kwargs):\n        \"\"\" Predict.\n\n        Neural network prediction with PL's `Trainer` execution of `predict_step`.\n\n        **Parameters:**<br>\n        `dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n        `test_size`: int=None, test size for temporal cross-validation.<br>\n        `step_size`: int=1, Step size between each window.<br>\n        `random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n        `quantiles`: list of floats, optional (default=None), target quantiles to predict. <br>\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing VanillaTransformer Class for Time Series Forecasting in Python\nDESCRIPTION: This code defines the VanillaTransformer class, a PyTorch model for time series forecasting based on the Transformer architecture. It features full-attention mechanism with O(L^2) complexity, MLP multi-step decoder, and three-component embedding approach. The implementation includes extensive configuration options for model architecture, training process, and optimization.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.vanillatransformer.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass VanillaTransformer(BaseModel):\n    \"\"\" VanillaTransformer\n\n    Vanilla Transformer, following implementation of the Informer paper, used as baseline.\n\n    The architecture has three distinctive features:\n    - Full-attention mechanism with O(L^2) time and memory complexity.\n    - An MLP multi-step decoder that predicts long time-series sequences in a single forward operation rather than step-by-step.\n\n    The Vanilla Transformer model utilizes a three-component approach to define its embedding:\n    - It employs encoded autoregressive features obtained from a convolution network.\n    - It uses window-relative positional embeddings derived from harmonic functions.\n    - Absolute positional embeddings obtained from calendar features are utilized.\n\n    *Parameters:*<br>\n    `h`: int, forecast horizon.<br>\n    `input_size`: int, maximum sequence length for truncated train backpropagation. <br>\n    `stat_exog_list`: str list, static exogenous columns.<br>\n    `hist_exog_list`: str list, historic exogenous columns.<br>\n    `futr_exog_list`: str list, future exogenous columns.<br>\n    `exclude_insample_y`: bool=False, whether to exclude the target variable from the input.<br>\n  \t`decoder_input_size_multiplier`: float = 0.5, .<br>\n    `hidden_size`: int=128, units of embeddings and encoders.<br>\n    `dropout`: float (0, 1), dropout throughout Informer architecture.<br>\n    `n_head`: int=4, controls number of multi-head's attention.<br>\n\t`conv_hidden_size`: int=32, channels of the convolutional encoder.<br>\n\t`activation`: str=`GELU`, activation from ['ReLU', 'Softplus', 'Tanh', 'SELU', 'LeakyReLU', 'PReLU', 'Sigmoid', 'GELU'].<br>\n    `encoder_layers`: int=2, number of layers for the TCN encoder.<br>\n    `decoder_layers`: int=1, number of layers for the MLP decoder.<br>\n    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>    \n    `max_steps`: int=1000, maximum number of training steps.<br>\n    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n    `batch_size`: int=32, number of different series in each batch.<br>\n    `valid_batch_size`: int=None, number of different series in each validation and test batch, if None uses batch_size.<br>\n    `windows_batch_size`: int=1024, number of windows to sample in each training batch, default uses all.<br>\n    `inference_windows_batch_size`: int=1024, number of windows to sample in each inference batch.<br>\n    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n    `step_size`: int=1, step size between each window of temporal data.<br>\n    `scaler_type`: str='robust', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n    `random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n    `alias`: str, optional,  Custom name of the model.<br>\n    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>\n    `dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>\n\n\t*References*<br>\n\t- [Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, Wancai Zhang. \"Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting\"](https://arxiv.org/abs/2012.07436)<br>\n    \"\"\"\n    # Class attributes\n    EXOGENOUS_FUTR = True\n    EXOGENOUS_HIST = False\n    EXOGENOUS_STAT = False\n    MULTIVARIATE = False    # If the model produces multivariate forecasts (True) or univariate (False)\n    RECURRENT = False       # If the model produces forecasts recursively (True) or direct (False)\n\n    def __init__(self,\n                 h: int, \n                 input_size: int,\n                 stat_exog_list = None,\n                 hist_exog_list = None,\n                 futr_exog_list = None,\n                 exclude_insample_y = False,\n                 decoder_input_size_multiplier: float = 0.5,\n                 hidden_size: int = 128, \n                 dropout: float = 0.05,\n                 n_head: int = 4,\n                 conv_hidden_size: int = 32,\n                 activation: str = 'gelu',\n                 encoder_layers: int = 2, \n                 decoder_layers: int = 1, \n                 loss = MAE(),\n                 valid_loss = None,\n                 max_steps: int = 5000,\n                 learning_rate: float = 1e-4,\n                 num_lr_decays: int = -1,\n                 early_stop_patience_steps: int =-1,\n                 val_check_steps: int = 100,\n                 batch_size: int = 32,\n                 valid_batch_size: Optional[int] = None,\n                 windows_batch_size = 1024,\n                 inference_windows_batch_size: int = 1024,\n                 start_padding_enabled = False,\n                 step_size: int = 1,\n                 scaler_type: str = 'identity',\n                 random_seed: int = 1,\n                 drop_last_loader: bool = False,\n                 alias: Optional[str] = None,\n                 optimizer = None,\n                 optimizer_kwargs = None,\n                 lr_scheduler = None,\n                 lr_scheduler_kwargs = None,\n                 dataloader_kwargs = None,\n                 **trainer_kwargs):\n        super(VanillaTransformer, self).__init__(h=h,\n                                       input_size=input_size,\n                                       stat_exog_list=stat_exog_list,\n                                       hist_exog_list=hist_exog_list,\n                                       futr_exog_list = futr_exog_list,\n                                       exclude_insample_y=exclude_insample_y,\n                                       loss=loss,\n                                       valid_loss=valid_loss,\n                                       max_steps=max_steps,\n                                       learning_rate=learning_rate,\n                                       num_lr_decays=num_lr_decays,\n                                       early_stop_patience_steps=early_stop_patience_steps,\n                                       val_check_steps=val_check_steps,\n                                       batch_size=batch_size,\n                                       valid_batch_size=valid_batch_size,\n                                       windows_batch_size=windows_batch_size,\n                                       inference_windows_batch_size=inference_windows_batch_size,\n                                       start_padding_enabled=start_padding_enabled,\n                                       step_size=step_size,\n                                       scaler_type=scaler_type,\n                                       drop_last_loader=drop_last_loader,\n                                       alias=alias,\n                                       random_seed=random_seed,\n                                       optimizer=optimizer,\n                                       optimizer_kwargs=optimizer_kwargs,\n                                       lr_scheduler=lr_scheduler,\n                                       lr_scheduler_kwargs=lr_scheduler_kwargs,\n                                       dataloader_kwargs=dataloader_kwargs,\n                                       **trainer_kwargs)\n\n        # Architecture\n        self.label_len = int(np.ceil(input_size * decoder_input_size_multiplier))\n        if (self.label_len >= input_size) or (self.label_len <= 0):\n            raise Exception(f'Check decoder_input_size_multiplier={decoder_input_size_multiplier}, range (0,1)')\n\n        if activation not in ['relu', 'gelu']:\n            raise Exception(f'Check activation={activation}')\n        \n        self.c_out = self.loss.outputsize_multiplier\n        self.output_attention = False\n        self.enc_in = 1 \n        self.dec_in = 1\n\n        # Embedding\n        self.enc_embedding = DataEmbedding(c_in=self.enc_in,\n                                           exog_input_size=self.futr_exog_size,\n                                           hidden_size=hidden_size, \n                                           pos_embedding=True,\n                                           dropout=dropout)\n        self.dec_embedding = DataEmbedding(self.dec_in,\n                                           exog_input_size=self.futr_exog_size,\n                                           hidden_size=hidden_size, \n                                           pos_embedding=True,\n                                           dropout=dropout)\n\n        # Encoder\n        self.encoder = TransEncoder(\n            [\n                TransEncoderLayer(\n```\n\n----------------------------------------\n\nTITLE: Training LSTM and NHITS Models\nDESCRIPTION: Initializes and trains LSTM and NHITS models with custom hyperparameters. The models are configured for a 12-step forecasting horizon and use the AirPassengers dataset with monthly frequency.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/getting-started/02_quickstart.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n%%capture\nhorizon = 12\n\n# Try different hyperparmeters to improve accuracy.\nmodels = [LSTM(input_size=2 * horizon,\n               h=horizon,                    # Forecast horizon\n               max_steps=500,                # Number of steps to train\n               scaler_type='standard',       # Type of scaler to normalize data\n               encoder_hidden_size=64,       # Defines the size of the hidden state of the LSTM\n               decoder_hidden_size=64,),     # Defines the number of hidden units of each layer of the MLP decoder\n          NHITS(h=horizon,                   # Forecast horizon\n                input_size=2 * horizon,      # Length of input sequence\n                max_steps=100,               # Number of steps to train\n                n_freq_downsample=[2, 1, 1]) # Downsampling factors for each stack output\n          ]\nnf = NeuralForecast(models=models, freq='ME')\nnf.fit(df=Y_df)\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for NHITS Implementation in Python\nDESCRIPTION: This snippet imports the necessary libraries and modules for implementing the NHITS model, including PyTorch for neural network operations and custom loss functions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nhits.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Tuple, Optional\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom neuralforecast.losses.pytorch import MAE\nfrom neuralforecast.common._base_model import BaseModel\n```\n\n----------------------------------------\n\nTITLE: Example Usage of AutoLSTM Model\nDESCRIPTION: Demonstrates initialization and usage of AutoLSTM model with custom configuration and different backends (Ray and Optuna). Shows basic fitting and prediction workflow.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nconfig = dict(max_steps=1, val_check_steps=1, input_size=-1, encoder_hidden_size=8)\nmodel = AutoLSTM(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n# Optuna\nmodel = AutoLSTM(h=12, config=None, backend='optuna')\n```\n\n----------------------------------------\n\nTITLE: Importing NeuralForecast Components\nDESCRIPTION: Imports required components from Ray Tune and NeuralForecast for model configuration and training.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/experiments/nbeats_basis/nbeats_basis_experiment.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import tune\nfrom neuralforecast.auto import AutoNBEATS\nfrom neuralforecast import NeuralForecast\n```\n\n----------------------------------------\n\nTITLE: GRU Model Initialization and Forward Pass Implementation\nDESCRIPTION: Core implementation of GRU model with configurable encoder and decoder components. Handles various types of input features including historical, static and future exogenous variables. The forward method processes batched time series data through RNN encoder and MLP decoder.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.gru.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# RNN\nself.encoder_n_layers = encoder_n_layers\nself.encoder_hidden_size = encoder_hidden_size\nself.encoder_bias = encoder_bias\nself.encoder_dropout = encoder_dropout\n\n# Context adapter\nif context_size is not None:\n    warnings.warn(\"context_size is deprecated and will be removed in future versions.\")\n\n# MLP decoder\nself.decoder_hidden_size = decoder_hidden_size\nself.decoder_layers = decoder_layers\n\n# RNN input size (1 for target variable y)\ninput_encoder = 1 + self.hist_exog_size + self.stat_exog_size + self.futr_exog_size\n\n# Instantiate model\nself.rnn_state = None\nself.maintain_state = False\nself.hist_encoder = nn.GRU(input_size=input_encoder,\n                            hidden_size=self.encoder_hidden_size,\n                            num_layers=self.encoder_n_layers,\n                            bias=self.encoder_bias,\n                            dropout=self.encoder_dropout,\n                            batch_first=True)\n\n# Decoder MLP\nif self.RECURRENT:\n    self.proj = nn.Linear(self.encoder_hidden_size, self.loss.outputsize_multiplier)\nelse:\n    self.mlp_decoder = MLP(in_features=self.encoder_hidden_size + self.futr_exog_size,\n                        out_features=self.loss.outputsize_multiplier,\n                        hidden_size=self.decoder_hidden_size,\n                        num_layers=self.decoder_layers,\n                        activation='ReLU',\n                        dropout=0.0)\n```\n\n----------------------------------------\n\nTITLE: Fitting and Predicting with AutoNHITS Model in Python\nDESCRIPTION: This snippet demonstrates how to configure an AutoNHITS model with hyperparameters, fit it to a dataset, and make predictions. It uses the MAE loss function and MSE for validation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_115\n\nLANGUAGE: python\nCODE:\n```\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n```\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n## TESTS\nnhits_config = {\n       \"learning_rate\": tune.choice([1e-3]),                                     # Initial Learning rate\n       \"max_steps\": tune.choice([1]),                                            # Number of SGD steps\n       \"val_check_steps\": tune.choice([1]),                                      # Number of steps between validation\n       \"input_size\": tune.choice([5 * 12]),                                      # input_size = multiplier * horizon\n       \"batch_size\": tune.choice([7]),                                           # Number of series in windows\n       \"windows_batch_size\": tune.choice([256]),                                 # Number of windows in batch\n       \"n_pool_kernel_size\": tune.choice([[2, 2, 2], [16, 8, 1]]),               # MaxPool's Kernelsize\n       \"n_freq_downsample\": tune.choice([[168, 24, 1], [24, 12, 1], [1, 1, 1]]), # Interpolation expressivity ratios\n       \"activation\": tune.choice(['ReLU']),                                      # Type of non-linear activation\n       \"n_blocks\":  tune.choice([[1, 1, 1]]),                                    # Blocks per each 3 stacks\n       \"mlp_units\":  tune.choice([[[512, 512], [512, 512], [512, 512]]]),        # 2 512-Layers per block for each stack\n       \"interpolation_mode\": tune.choice(['linear']),                            # Type of multi-step interpolation\n       \"random_seed\": tune.randint(1, 10),\n    }\n\nmodel = AutoNHITS(h=12, loss=MAE(), valid_loss=MSE(), config=nhits_config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n# Test equality\ntest_eq(str(type(model.valid_loss)), \"<class 'neuralforecast.losses.pytorch.MSE'>\")\n```\n\n----------------------------------------\n\nTITLE: Training and Predicting with TimeMixer in Python\nDESCRIPTION: This example demonstrates the practical usage of the `TimeMixer` model within the `NeuralForecast` framework. It involves importing required libraries (pandas, matplotlib, neuralforecast components), preparing the AirPassengers training and testing dataframes, configuring and instantiating the `TimeMixer` model with specific hyperparameters (horizon `h`, `input_size`, `n_series`, `scaler_type`, `max_steps`, `learning_rate`, `loss`, `batch_size`), training the model using `NeuralForecast.fit` with training and static data, and generating future forecasts using `NeuralForecast.predict` based on the test dataframe.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timemixer.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n#| eval: false\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import TimeMixer\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\nfrom neuralforecast.losses.pytorch import MAE\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = TimeMixer(h=12,\n                input_size=24,\n                n_series=2,\n                scaler_type='standard',\n                max_steps=500,\n                early_stop_patience_steps=-1,\n                val_check_steps=5,\n                learning_rate=1e-3,\n                loss = MAE(),\n                valid_loss=MAE(),\n                batch_size=32\n                )\n\nfcst = NeuralForecast(models=[model], freq='ME')\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nforecasts = fcst.predict(futr_df=Y_test_df)\n```\n\n----------------------------------------\n\nTITLE: GRU Model Class Implementation for Time Series Forecasting\nDESCRIPTION: Definition of the GRU class that implements a multi-layer recurrent network with Gated Recurrent Units for time series forecasting. The implementation includes configurable parameters for the encoder, decoder, training process, and handling of exogenous variables.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.gru.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass GRU(BaseModel):\n    \"\"\" GRU\n\n    Multi Layer Recurrent Network with Gated Units (GRU), and\n    MLP decoder. The network has non-linear activation functions, it is trained \n    using ADAM stochastic gradient descent. The network accepts static, historic \n    and future exogenous data, flattens the inputs.\n\n    **Parameters:**<br>\n    `h`: int, forecast horizon.<br>\n    `input_size`: int, maximum sequence length for truncated train backpropagation. Default -1 uses 3 * horizon <br>\n    `inference_input_size`: int, maximum sequence length for truncated inference. Default None uses input_size history.<br>\n    `encoder_n_layers`: int=2, number of layers for the GRU.<br>\n    `encoder_hidden_size`: int=200, units for the GRU's hidden state size.<br>\n    `encoder_activation`: Optional[str]=None, Deprecated. Activation function in GRU is frozen in PyTorch.<br>\n    `encoder_bias`: bool=True, whether or not to use biases b_ih, b_hh within GRU units.<br>\n    `encoder_dropout`: float=0., dropout regularization applied to GRU outputs.<br>\n    `context_size`: deprecated.<br>\n    `decoder_hidden_size`: int=200, size of hidden layer for the MLP decoder.<br>\n    `decoder_layers`: int=2, number of layers for the MLP decoder.<br>\n    `futr_exog_list`: str list, future exogenous columns.<br>\n    `hist_exog_list`: str list, historic exogenous columns.<br>\n    `stat_exog_list`: str list, static exogenous columns.<br>\n    `exclude_insample_y`: bool=False, whether to exclude the target variable from the input.<br>\n    `recurrent`: bool=False, whether to produce forecasts recursively (True) or direct (False).<br>\n    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `max_steps`: int=1000, maximum number of training steps.<br>\n    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n    `batch_size`: int=32, number of differentseries in each batch.<br>\n    `valid_batch_size`: int=None, number of different series in each validation and test batch.<br>\n    `windows_batch_size`: int=128, number of windows to sample in each training batch, default uses all.<br>\n    `inference_windows_batch_size`: int=1024, number of windows to sample in each inference batch, -1 uses all.<br>\n    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n    `step_size`: int=1, step size between each window of temporal data.<br>\n    `scaler_type`: str='robust', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n    `random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n    `alias`: str, optional,  Custom name of the model.<br>\n    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>\n    `dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n    \"\"\"\n    # Class attributes\n    EXOGENOUS_FUTR = True\n    EXOGENOUS_HIST = True\n    EXOGENOUS_STAT = True\n    MULTIVARIATE = False    # If the model produces multivariate forecasts (True) or univariate (False)\n    RECURRENT = True       # If the model produces forecasts recursively (True) or direct (False)\n\n    def __init__(self,\n                 h: int,\n                 input_size: int = -1,\n                 inference_input_size: Optional[int] = None,\n                 encoder_n_layers: int = 2,\n                 encoder_hidden_size: int = 200,\n                 encoder_activation: Optional[str] = None,\n                 encoder_bias: bool = True,\n                 encoder_dropout: float = 0.,\n                 context_size: Optional[int] = None,\n                 decoder_hidden_size: int = 128,\n                 decoder_layers: int = 2,\n                 futr_exog_list = None,\n                 hist_exog_list = None,\n                 stat_exog_list = None,\n                 exclude_insample_y = False,\n                 recurrent = False,\n                 loss = MAE(),\n                 valid_loss = None,\n                 max_steps: int = 1000,\n                 learning_rate: float = 1e-3,\n                 num_lr_decays: int = -1,\n                 early_stop_patience_steps: int =-1,\n                 val_check_steps: int = 100,\n                 batch_size=32,\n                 valid_batch_size: Optional[int] = None,\n                 windows_batch_size = 128,\n                 inference_windows_batch_size = 1024,\n                 start_padding_enabled = False,\n                 step_size: int = 1,\n                 scaler_type: str='robust',\n                 random_seed=1,\n                 drop_last_loader = False,\n                 alias: Optional[str] = None,\n                 optimizer = None,\n                 optimizer_kwargs = None,\n                 lr_scheduler = None,\n                 lr_scheduler_kwargs = None,\n                 dataloader_kwargs = None,\n                 **trainer_kwargs):\n        \n        self.RECURRENT = recurrent\n\n        super(GRU, self).__init__(\n            h=h,\n            input_size=input_size,\n            inference_input_size=inference_input_size,\n            futr_exog_list=futr_exog_list,\n            hist_exog_list=hist_exog_list,\n            stat_exog_list=stat_exog_list,\n            exclude_insample_y = exclude_insample_y,\n            loss=loss,\n            valid_loss=valid_loss,\n            max_steps=max_steps,\n            learning_rate=learning_rate,\n            num_lr_decays=num_lr_decays,\n            early_stop_patience_steps=early_stop_patience_steps,\n            val_check_steps=val_check_steps,\n            batch_size=batch_size,\n            valid_batch_size=valid_batch_size,\n            windows_batch_size=windows_batch_size,\n            inference_windows_batch_size=inference_windows_batch_size,\n            start_padding_enabled=start_padding_enabled,\n            step_size=step_size,\n            scaler_type=scaler_type,\n            random_seed=random_seed,\n            drop_last_loader=drop_last_loader,\n            alias=alias,\n            optimizer=optimizer,\n            optimizer_kwargs=optimizer_kwargs,\n            lr_scheduler=lr_scheduler,\n            lr_scheduler_kwargs=lr_scheduler_kwargs,\n            dataloader_kwargs=dataloader_kwargs,\n            **trainer_kwargs\n        )\n\n        if encoder_activation is not None:\n            warnings.warn(\n                \"The 'encoder_activation' argument is deprecated and will be removed in \"\n                \"future versions. The activation function in GRU is frozen in PyTorch and \"\n                \"it cannot be modified.\",\n                DeprecationWarning,\n            )\n\n\n```\n\n----------------------------------------\n\nTITLE: Predicting Insample with NeuralForecast in Python\nDESCRIPTION: This method generates model predictions on the insample (historical) time series used for training in NeuralForecast, optionally skipping held-out tails for each series by 'test_size' and stepping through time. It restructures per-series data into trimmed datasets as needed and uses helper functions to construct forecast DataFrames per split, aggregating results from all models. This requires models to already be fitted and uses numpy and custom TimeSeriesDataset/_insample_times utilities to orchestrate per-series rolling evaluation. Inputs include the insample dataset managed by the NeuralForecast instance, with configuration over step size and time structure; outputs are concatenated DataFrames of predictions. This method assumes the existence and fit of underlying models and raises an exception if this is not satisfied.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef predict_insample(self, step_size: int = 1):\n    \"\"\"Predict insample with core.NeuralForecast.\n\n    `core.NeuralForecast`'s `predict_insample` uses stored fitted `models`\n    to predict historic values of a time series from the stored dataframe.\n\n    Parameters\n    ----------\n    step_size : int (default=1)\n        Step size between each window.\n\n    Returns\n    -------\n    fcsts_df : pandas.DataFrame\n        DataFrame with insample predictions for all fitted `models`.    \n    \"\"\"\n    if not self._fitted:\n        raise Exception('The models must be fitted first with `fit` or `cross_validation`.')\n    test_size = self.models[0].get_test_size()\n    \n    # Process each series separately\n    fcsts_dfs = []\n    trimmed_datasets = []\n    \n    for i in range(self.dataset.n_groups):\n        # Calculate series-specific length and offset\n        series_length = self.dataset.indptr[i + 1] - self.dataset.indptr[i]\n        _, forefront_offset = np.divmod((series_length - test_size - self.h), step_size)\n        \n        if test_size > 0 or forefront_offset > 0:\n            # Create single-series dataset\n            series_dataset = TimeSeriesDataset(\n                temporal=self.dataset.temporal[self.dataset.indptr[i]:self.dataset.indptr[i + 1]],\n                temporal_cols=self.dataset.temporal_cols,\n                indptr=np.array([0, series_length]),\n                y_idx=self.dataset.y_idx\n            )\n            \n            # Trim the series\n            trimmed_series = TimeSeriesDataset.trim_dataset(\n                dataset=series_dataset,\n                right_trim=test_size,\n                left_trim=forefront_offset\n            )\n            \n            new_idxs = np.arange(\n                self.dataset.indptr[i] + forefront_offset,\n                self.dataset.indptr[i + 1] - test_size\n            )\n            times = self.ds[new_idxs]\n        else:\n            trimmed_series = TimeSeriesDataset(\n                temporal=self.dataset.temporal[self.dataset.indptr[i]:self.dataset.indptr[i + 1]],\n                temporal_cols=self.dataset.temporal_cols,\n                indptr=np.array([0, series_length]),\n                y_idx=self.dataset.y_idx\n            )\n            times = self.ds[self.dataset.indptr[i]:self.dataset.indptr[i + 1]]\n        \n        series_fcsts_df = _insample_times(\n            times=times,\n            uids=self.uids[i:i+1],\n            indptr=trimmed_series.indptr,\n            h=self.h,\n            freq=self.freq,\n            step_size=step_size,\n            id_col=self.id_col,\n            time_col=self.time_col,\n        )\n        \n        fcsts_dfs.append(series_fcsts_df)\n        trimmed_datasets.append(trimmed_series)\n\n    # Combine all series forecasts DataFrames\n    fcsts_df = ufp.vertical_concat(fcsts_dfs)\n    \n    # Generate predictions for each model\n    fcsts_list = []\n    for model in self.models:\n        model_series_preds = []\n```\n\n----------------------------------------\n\nTITLE: Implementing DLinear Neural Forecasting Model in Python\nDESCRIPTION: A PyTorch implementation of the DLinear model which decomposes time series into trend and seasonal components using moving averages, then applies separate linear projections to each component. The model follows the architecture described in the paper 'Are transformers effective for time series forecasting?' by Zeng et al.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.dlinear.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass DLinear(BaseModel):\n    \"\"\" DLinear\n\n    *Parameters:*<br>\n    `h`: int, forecast horizon.<br>\n    `input_size`: int, maximum sequence length for truncated train backpropagation. <br>\n    `stat_exog_list`: str list, static exogenous columns.<br>\n    `hist_exog_list`: str list, historic exogenous columns.<br>\n    `futr_exog_list`: str list, future exogenous columns.<br>\n    `exclude_insample_y`: bool=False, the model skips the autoregressive features y[t-input_size:t] if True.<br>\n    `moving_avg_window`: int=25, window size for trend-seasonality decomposition. Should be uneven.<br>\n    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `max_steps`: int=1000, maximum number of training steps.<br>\n    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n    `batch_size`: int=32, number of different series in each batch.<br>\n    `valid_batch_size`: int=None, number of different series in each validation and test batch, if None uses batch_size.<br>\n    `windows_batch_size`: int=1024, number of windows to sample in each training batch, default uses all.<br>\n    `inference_windows_batch_size`: int=1024, number of windows to sample in each inference batch.<br>\n    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n    `step_size`: int=1, step size between each window of temporal data.<br>\n    `scaler_type`: str='robust', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n    `random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n    `alias`: str, optional,  Custom name of the model.<br>\n    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>\n    `dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>\n\n\t*References*<br>\n\t- Zeng, Ailing, et al. \"Are transformers effective for time series forecasting?\" Proceedings of the AAAI conference on artificial intelligence. Vol. 37. No. 9. 2023.\"\n    \"\"\"\n    # Class attributes\n    EXOGENOUS_FUTR = False\n    EXOGENOUS_HIST = False\n    EXOGENOUS_STAT = False\n    MULTIVARIATE = False    # If the model produces multivariate forecasts (True) or univariate (False)\n    RECURRENT = False       # If the model produces forecasts recursively (True) or direct (False)\n\n    def __init__(self,\n                 h: int, \n                 input_size: int,\n                 stat_exog_list = None,\n                 hist_exog_list = None,\n                 futr_exog_list = None,\n                 exclude_insample_y = False,\n                 moving_avg_window: int = 25,\n                 loss = MAE(),\n                 valid_loss = None,\n                 max_steps: int = 5000,\n                 learning_rate: float = 1e-4,\n                 num_lr_decays: int = -1,\n                 early_stop_patience_steps: int =-1,\n                 val_check_steps: int = 100,\n                 batch_size: int = 32,\n                 valid_batch_size: Optional[int] = None,\n                 windows_batch_size = 1024,\n                 inference_windows_batch_size = 1024,\n                 start_padding_enabled = False,\n                 step_size: int = 1,\n                 scaler_type: str = 'identity',\n                 random_seed: int = 1,\n                 drop_last_loader: bool = False,\n                 alias: Optional[str] = None,\n                 optimizer = None,\n                 optimizer_kwargs = None,\n                 lr_scheduler = None,\n                 lr_scheduler_kwargs = None,\n                 dataloader_kwargs=None,\n                 **trainer_kwargs):\n        super(DLinear, self).__init__(h=h,\n                                       input_size=input_size,\n                                       hist_exog_list=hist_exog_list,\n                                       stat_exog_list=stat_exog_list,\n                                       futr_exog_list = futr_exog_list,\n                                       exclude_insample_y = exclude_insample_y,\n                                       loss=loss,\n                                       valid_loss=valid_loss,\n                                       max_steps=max_steps,\n                                       learning_rate=learning_rate,\n                                       num_lr_decays=num_lr_decays,\n                                       early_stop_patience_steps=early_stop_patience_steps,\n                                       val_check_steps=val_check_steps,\n                                       batch_size=batch_size,\n                                       windows_batch_size=windows_batch_size,\n                                       valid_batch_size=valid_batch_size,\n                                       inference_windows_batch_size=inference_windows_batch_size,\n                                       start_padding_enabled = start_padding_enabled,\n                                       step_size=step_size,\n                                       scaler_type=scaler_type,\n                                       drop_last_loader=drop_last_loader,\n                                       alias=alias,\n                                       random_seed=random_seed,\n                                       optimizer=optimizer,\n                                       optimizer_kwargs=optimizer_kwargs,\n                                       lr_scheduler=lr_scheduler,\n                                       lr_scheduler_kwargs=lr_scheduler_kwargs,\n                                       dataloader_kwargs=dataloader_kwargs,\n                                       **trainer_kwargs)\n                                                                \n        # Architecture\n        if moving_avg_window % 2 == 0:\n            raise Exception('moving_avg_window should be uneven')\n\n        self.c_out = self.loss.outputsize_multiplier\n        self.output_attention = False\n        self.enc_in = 1 \n        self.dec_in = 1\n\n        # Decomposition\n        self.decomp = SeriesDecomp(moving_avg_window)\n\n        self.linear_trend = nn.Linear(self.input_size, self.loss.outputsize_multiplier * h, bias=True)\n        self.linear_season = nn.Linear(self.input_size, self.loss.outputsize_multiplier * h, bias=True)\n\n    def forward(self, windows_batch):\n        # Parse windows_batch\n        insample_y    = windows_batch['insample_y'].squeeze(-1)\n\n        # Parse inputs\n        batch_size = len(insample_y)\n        seasonal_init, trend_init = self.decomp(insample_y)\n\n        trend_part = self.linear_trend(trend_init)\n        seasonal_part = self.linear_season(seasonal_init)\n        \n        # Final\n        forecast = trend_part + seasonal_part\n        forecast = forecast.reshape(batch_size, self.h, self.loss.outputsize_multiplier)\n        return forecast\n```\n\n----------------------------------------\n\nTITLE: Training NHITS and Performing Cross-Validation\nDESCRIPTION: Creates a NeuralForecast object, trains the AutoNHITS model, and performs cross-validation to generate forecasts for the test set.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/04_longhorizon_nhits.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n%%capture\nnf = NeuralForecast(\n    models=models,\n    freq='15min')\n\nY_hat_df = nf.cross_validation(df=Y_df, val_size=val_size,\n                               test_size=test_size, n_windows=None)\n```\n\n----------------------------------------\n\nTITLE: Using AutoMLP for Time Series Forecasting with Configuration Examples in Python\nDESCRIPTION: Demonstrates how to instantiate and use the AutoMLP class with custom configuration and default parameters. Shows examples of model fitting and prediction with both Ray and Optuna backends. The snippet includes capturing output to suppress verbose logging.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_35\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n# Use your own config or AutoMLP.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=8)\nmodel = AutoMLP(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n# Optuna\nmodel = AutoMLP(h=12, config=None, backend='optuna')\n```\n\n----------------------------------------\n\nTITLE: Implementing AutoNHITS Class for Neural Hierarchical Interpolation Time Series Models\nDESCRIPTION: Complete implementation of the AutoNHITS class which extends BaseAuto to facilitate hyperparameter tuning for NHITS models. It defines default hyperparameter search space and includes methods to adapt configurations for different optimization backends.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_42\n\nLANGUAGE: python\nCODE:\n```\nclass AutoNHITS(BaseAuto):\n\n    default_config = {\n       \"input_size_multiplier\": [1, 2, 3, 4, 5],\n       \"h\": None,\n       \"n_pool_kernel_size\": tune.choice([[2, 2, 1], 3*[1], 3*[2], 3*[4], \n                                         [8, 4, 1], [16, 8, 1]]),\n       \"n_freq_downsample\": tune.choice([[168, 24, 1], [24, 12, 1], \n                                         [180, 60, 1], [60, 8, 1], \n                                         [40, 20, 1], [1, 1, 1]]),\n       \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n       \"scaler_type\": tune.choice([None, 'robust', 'standard']),\n       \"max_steps\": tune.quniform(lower=500, upper=1500, q=100),\n       \"batch_size\": tune.choice([32, 64, 128, 256]),\n       \"windows_batch_size\": tune.choice([128, 256, 512, 1024]),\n       \"loss\": None,\n       \"random_seed\": tune.randint(lower=1, upper=20),\n    }\n\n    def __init__(self,\n                 h,\n                 loss=MAE(),\n                 valid_loss=None,\n                 config=None, \n                 search_alg=BasicVariantGenerator(random_state=1),\n                 num_samples=10,\n                 refit_with_val=False,\n                 cpus=cpu_count(),\n                 gpus=torch.cuda.device_count(),\n                 verbose=False,\n                 alias=None,\n                 backend='ray',\n                 callbacks=None,\n                ):\n\n        # Define search space, input/output sizes\n        if config is None:\n            config = self.get_default_config(h=h, backend=backend)                     \n\n        super(AutoNHITS, self).__init__(\n              cls_model=NHITS, \n              h=h,\n              loss=loss,\n              valid_loss=valid_loss,\n              config=config,\n              search_alg=search_alg,\n              num_samples=num_samples,\n              refit_with_val=refit_with_val,\n              cpus=cpus,\n              gpus=gpus,\n              verbose=verbose,\n              alias=alias,\n              backend=backend,\n              callbacks=callbacks,\n        )\n\n    @classmethod\n    def get_default_config(cls, h, backend, n_series=None):\n        config = cls.default_config.copy()\n        config['input_size'] = tune.choice([h*x \\\n                        for x in config['input_size_multiplier']])\n        config['step_size'] = tune.choice([1, h])        \n        del config['input_size_multiplier']\n        if backend == 'optuna':\n            config = cls._ray_config_to_optuna(config)         \n\n        return config        \n```\n\n----------------------------------------\n\nTITLE: Generating Forecasts with NeuralForecast Models\nDESCRIPTION: This method generates forecasts using NeuralForecast models. It handles different types of losses (Distribution, IQ, Point) and computes prediction intervals if specified. The method returns forecasts and column names for each model and quantile/level.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_26\n\nLANGUAGE: Python\nCODE:\n```\ndef _generate_forecasts(self, dataset: TimeSeriesDataset, uids: Series, quantiles_: Optional[List[float]] = None, level_: Optional[List[Union[int, float]]] = None, has_level: Optional[bool] = False, **data_kwargs) -> np.array:\n    fcsts_list: List = []\n    cols = []\n    count_names = {'model': 0}\n    for model in self.models:\n        old_test_size = model.get_test_size()\n        model.set_test_size(self.h) # To predict h steps ahead\n        \n        # Increment model name if the same model is used more than once\n        model_name = repr(model)\n        count_names[model_name] = count_names.get(model_name, -1) + 1\n        if count_names[model_name] > 0:\n            model_name += str(count_names[model_name])\n\n        # Predict for every quantile or level if requested and the loss function supports it\n        # case 1: DistributionLoss and MixtureLosses\n        if quantiles_ is not None and not isinstance(model.loss, (IQLoss, HuberIQLoss)) and hasattr(model.loss, 'update_quantile') and callable(model.loss.update_quantile):\n            model_fcsts = model.predict(dataset=dataset, quantiles = quantiles_, **data_kwargs)\n            fcsts_list.append(model_fcsts)      \n            col_names = []\n            for i, quantile in enumerate(quantiles_):\n                col_name = self._get_column_name(model_name, quantile, has_level)\n                if i == 0:\n                    col_names.extend([f\"{model_name}\", col_name])\n                else:\n                    col_names.extend([col_name])\n            if hasattr(model.loss, 'return_params') and model.loss.return_params:\n                cols.extend(col_names + [model_name + param_name for param_name in model.loss.param_names])\n            else:\n                cols.extend(col_names)\n        # case 2: IQLoss\n        elif quantiles_ is not None and isinstance(model.loss, (IQLoss, HuberIQLoss)):\n            # IQLoss does not give monotonically increasing quantiles, so we apply a hack: compute all quantiles, and take the quantile over the quantiles\n            quantiles_iqloss = np.linspace(0.01, 0.99, 20)\n            fcsts_list_iqloss = []\n            for i, quantile in enumerate(quantiles_iqloss):\n                model_fcsts = model.predict(dataset=dataset, quantiles = [quantile], **data_kwargs)               \n                fcsts_list_iqloss.append(model_fcsts)      \n            fcsts_iqloss = np.concatenate(fcsts_list_iqloss, axis=-1)\n\n            # Get the actual requested quantiles\n            model_fcsts = np.quantile(fcsts_iqloss, quantiles_, axis=-1).T\n            fcsts_list.append(model_fcsts)      \n\n            # Get the right column names\n            col_names = []\n            for i, quantile in enumerate(quantiles_):\n                col_name = self._get_column_name(model_name, quantile, has_level)\n                col_names.extend([col_name])                \n            cols.extend(col_names)\n        # case 3: PointLoss via prediction intervals\n        elif quantiles_ is not None and model.loss.outputsize_multiplier == 1:\n            if self.prediction_intervals is None:\n                raise AttributeError(\n                f\"You have trained {model_name} with loss={type(model.loss).__name__}(). \\n\"\n                \" You then must set `prediction_intervals` during fit to use level or quantiles during predict.\")  \n            model_fcsts = model.predict(dataset=dataset, quantiles = quantiles_, **data_kwargs)\n            prediction_interval_method = get_prediction_interval_method(self.prediction_intervals.method)\n            fcsts_with_intervals, out_cols = prediction_interval_method(\n                model_fcsts,\n                self._cs_df,\n                model=model_name,\n                level=level_ if has_level else None,\n                cs_n_windows=self.prediction_intervals.n_windows,\n                n_series=len(uids),\n                horizon=self.h,\n                quantiles=quantiles_ if not has_level else None,\n            )  \n            fcsts_list.append(fcsts_with_intervals)      \n            cols.extend([model_name] + out_cols)\n        # base case: quantiles or levels are not supported or provided as arguments\n        else:\n            model_fcsts = model.predict(dataset=dataset, **data_kwargs)\n            fcsts_list.append(model_fcsts)\n            cols.extend(model_name + n for n in model.loss.output_names)\n        model.set_test_size(old_test_size) # Set back to original value\n    fcsts = np.concatenate(fcsts_list, axis=-1)\n\n    return fcsts, cols\n```\n\n----------------------------------------\n\nTITLE: Implementing RMoK Class for Time Series Forecasting with PyTorch\nDESCRIPTION: Defines the Reversible Mixture of KAN (RMoK) class, which combines different KAN experts (Taylor, Jacobi, Wavelet) with a gating mechanism for time series forecasting. The model includes comprehensive parameter configuration options and RevIN normalization for improved performance.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.rmok.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n#| export\n\nclass RMoK(BaseModel):\n    \"\"\" Reversible Mixture of KAN\n    \n    \n    **Parameters:**<br>\n    `h`: int, Forecast horizon. <br>\n    `input_size`: int, autorregresive inputs size, y=[1,2,3,4] input_size=2 -> y_[t-2:t]=[1,2].<br>\n    `n_series`: int, number of time-series.<br>\n    `futr_exog_list`: str list, future exogenous columns.<br>\n    `hist_exog_list`: str list, historic exogenous columns.<br>\n    `stat_exog_list`: str list, static exogenous columns.<br>\n    `taylor_order`: int, order of the Taylor polynomial.<br>\n    `jacobi_degree`: int, degree of the Jacobi polynomial.<br>\n    `wavelet_function`: str, wavelet function to use in the WaveKAN. Choose from [\"mexican_hat\", \"morlet\", \"dog\", \"meyer\", \"shannon\"]<br>\n    `dropout`: float, dropout rate.<br>\n    `revin_affine`: bool=False, bool to use affine in RevIn.<br>\n    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `max_steps`: int=1000, maximum number of training steps.<br>\n    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n    `batch_size`: int=32, number of different series in each batch.<br>\n    `valid_batch_size`: int=None, number of different series in each validation and test batch, if None uses batch_size.<br>\n    `windows_batch_size`: int=32, number of windows to sample in each training batch, default uses all.<br>\n    `inference_windows_batch_size`: int=32, number of windows to sample in each inference batch, -1 uses all.<br>\n    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n    `step_size`: int=1, step size between each window of temporal data.<br>\n    `scaler_type`: str='identity', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n    `random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n    `alias`: str, optional,  Custom name of the model.<br>\n    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>\n    `dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>\n\n    **References**<br>\n    - [Xiao Han, Xinfeng Zhang, Yiling Wu, Zhenduo Zhang, Zhe Wu.\"KAN4TSF: Are KAN and KAN-based models Effective for Time Series Forecasting?\". arXiv.](https://arxiv.org/abs/2408.11306)<br>\n    \"\"\"\n\n    # Class attributes\n    EXOGENOUS_FUTR = False\n    EXOGENOUS_HIST = False\n    EXOGENOUS_STAT = False\n    MULTIVARIATE = True    # If the model produces multivariate forecasts (True) or univariate (False)\n    RECURRENT = False       # If the model produces forecasts recursively (True) or direct (False)\n\n    def __init__(self,\n                 h,\n                 input_size,\n                 n_series: int,\n                 futr_exog_list = None,\n                 hist_exog_list = None,\n                 stat_exog_list = None,\n                 taylor_order: int = 3,\n                 jacobi_degree: int = 6,\n                 wavelet_function: str = 'mexican_hat',\n                 dropout: float = 0.1,\n                 revin_affine: bool = True,\n                 loss = MAE(),\n                 valid_loss = None,\n                 max_steps: int = 1000,\n                 learning_rate: float = 1e-3,\n                 num_lr_decays: int = -1,\n                 early_stop_patience_steps: int =-1,\n                 val_check_steps: int = 100,\n                 batch_size: int = 32,\n                 valid_batch_size: Optional[int] = None,\n                 windows_batch_size = 32,\n                 inference_windows_batch_size = 32,\n                 start_padding_enabled = False,\n                 step_size: int = 1,\n                 scaler_type: str = 'identity',\n                 random_seed: int = 1,\n                 drop_last_loader: bool = False,\n                 alias: Optional[str] = None,\n                 optimizer = None,\n                 optimizer_kwargs = None,\n                 lr_scheduler = None,\n                 lr_scheduler_kwargs = None,\n                 dataloader_kwargs = None,            \n                 **trainer_kwargs):\n        \n        super(RMoK, self).__init__(h=h,\n                                   input_size=input_size,\n                                   n_series=n_series,\n                                   futr_exog_list = hist_exog_list,\n                                   hist_exog_list = stat_exog_list,\n                                   stat_exog_list = futr_exog_list,\n                                   loss=loss,\n                                   valid_loss=valid_loss,\n                                   max_steps=max_steps,\n                                   learning_rate=learning_rate,\n                                   num_lr_decays=num_lr_decays,\n                                   early_stop_patience_steps=early_stop_patience_steps,\n                                   val_check_steps=val_check_steps,\n                                   batch_size=batch_size,\n                                   valid_batch_size=valid_batch_size,\n                                   windows_batch_size=windows_batch_size,\n                                   inference_windows_batch_size=inference_windows_batch_size,\n                                   start_padding_enabled=start_padding_enabled,\n                                   step_size=step_size,\n                                   scaler_type=scaler_type,\n                                   random_seed=random_seed,\n                                   drop_last_loader=drop_last_loader,\n                                   alias=alias,\n                                   optimizer=optimizer,\n                                   optimizer_kwargs=optimizer_kwargs,\n                                   lr_scheduler=lr_scheduler,\n                                   lr_scheduler_kwargs=lr_scheduler_kwargs,\n                                   dataloader_kwargs=dataloader_kwargs,\n                                   **trainer_kwargs)\n        \n        self.input_size = input_size\n        self.h = h\n        self.n_series = n_series\n        self.dropout = nn.Dropout(dropout)\n        self.revin_affine = revin_affine\n\n        self.taylor_order = taylor_order\n        self.jacobi_degree = jacobi_degree\n        self.wavelet_function = wavelet_function\n\n        self.experts = nn.ModuleList([\n            TaylorKANLayer(self.input_size, self.h * self.loss.outputsize_multiplier, order=self.taylor_order, addbias=True),\n            JacobiKANLayer(self.input_size, self.h * self.loss.outputsize_multiplier, degree=self.jacobi_degree),\n            WaveKANLayer(self.input_size, self.h * self.loss.outputsize_multiplier, wavelet_type=self.wavelet_function),\n            nn.Linear(self.input_size, self.h * self.loss.outputsize_multiplier),\n        ])\n        \n        self.num_experts = len(self.experts)\n        self.gate = nn.Linear(self.input_size, self.num_experts)\n        self.softmax = nn.Softmax(dim=-1)\n        self.rev = RevINMultivariate(self.n_series, affine=self.revin_affine)\n\n    def forward(self, windows_batch):\n        insample_y = windows_batch['insample_y']\n        B, L, N = insample_y.shape\n        x = self.rev(insample_y, 'norm')\n        x = self.dropout(x).transpose(1, 2).reshape(B * N, L)\n\n        score = F.softmax(self.gate(x), dim=-1)\n        expert_outputs = torch.stack([self.experts[i](x) for i in range(self.num_experts)], dim=-1)\n\n        y_pred = torch.einsum(\"BLE, BE -> BL\", expert_outputs, score).reshape(B, N, self.h * self.loss.outputsize_multiplier).permute(0, 2, 1)\n        y_pred = self.rev(y_pred, 'denorm')\n        y_pred = y_pred.reshape(B, self.h, -1)\n\n        return y_pred\n```\n\n----------------------------------------\n\nTITLE: NBEATSx Model Class Definition\nDESCRIPTION: Defines the NBEATSx model class that inherits from BaseModel. Includes comprehensive parameter configuration for model architecture, training settings, and optimization options.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeatsx.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass NBEATSx(BaseModel):\n    \"\"\"NBEATSx\n\n    The Neural Basis Expansion Analysis with Exogenous variables (NBEATSx) is a simple\n    and effective deep learning architecture. It is built with a deep stack of MLPs with\n    doubly residual connections. The NBEATSx architecture includes additional exogenous\n    blocks, extending NBEATS capabilities and interpretability. With its interpretable\n    version, NBEATSx decomposes its predictions on seasonality, trend, and exogenous effects.\n    \"\"\"\n\n    # Class attributes\n    EXOGENOUS_FUTR = True\n    EXOGENOUS_HIST = True\n    EXOGENOUS_STAT = True\n    MULTIVARIATE = False\n    RECURRENT = False\n\n    def __init__(\n        self,\n        h,\n        input_size,\n        futr_exog_list=None,\n        hist_exog_list=None,\n        stat_exog_list=None,\n        exclude_insample_y=False,\n        n_harmonics=2,\n        n_polynomials=2,\n        stack_types: list = [\"identity\", \"trend\", \"seasonality\"],\n        n_blocks: list = [1, 1, 1],\n        mlp_units: list = 3 * [[512, 512]],\n        dropout_prob_theta=0.0,\n        activation=\"ReLU\",\n        shared_weights=False,\n        loss=MAE(),\n        valid_loss=None,\n        max_steps: int = 1000,\n        learning_rate: float = 1e-3,\n        num_lr_decays: int = 3,\n        early_stop_patience_steps: int = -1,\n        val_check_steps: int = 100,\n        batch_size=32,\n        valid_batch_size: Optional[int] = None,\n        windows_batch_size: int = 1024,\n        inference_windows_batch_size: int = -1,\n        start_padding_enabled: bool = False,\n        step_size: int = 1,\n        scaler_type: str = \"identity\",\n        random_seed: int = 1,\n        drop_last_loader: bool = False,\n        alias: Optional[str] = None,\n        optimizer = None,\n        optimizer_kwargs = None,\n        lr_scheduler = None,\n        lr_scheduler_kwargs = None,\n        dataloader_kwargs = None,\n        **trainer_kwargs\n    ):\n```\n\n----------------------------------------\n\nTITLE: Using AutoTimeXer with Custom Configuration and Multiple Backends\nDESCRIPTION: Example showing how to use the AutoTimeXer class with a custom configuration, fit/predict workflow, and initialization with the Optuna backend as an alternative to Ray.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_91\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n# Use your own config or AutoTimeXer.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12, patch_len=12)\nmodel = AutoTimeXer(h=12, n_series=1, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n# Optuna\nmodel = AutoTimeXer(h=12, n_series=1, config=None, backend='optuna')\n```\n\n----------------------------------------\n\nTITLE: Implementing PatchTST Backbone in PyTorch\nDESCRIPTION: Main backbone class for PatchTST that handles patching, RevIN normalization, encoding, and head operations. Supports various configurations for patch size, stride, attention heads, and normalization types.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.patchtst.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass PatchTST_backbone(nn.Module):\n    def __init__(self, c_in:int, c_out:int, input_size:int, h:int, patch_len:int, stride:int, max_seq_len:Optional[int]=1024, \n                 n_layers:int=3, hidden_size=128, n_heads=16, d_k:Optional[int]=None, d_v:Optional[int]=None,\n                 linear_hidden_size:int=256, norm:str='BatchNorm', attn_dropout:float=0., dropout:float=0., act:str=\"gelu\", key_padding_mask:str='auto',\n                 padding_var:Optional[int]=None, attn_mask:Optional[torch.Tensor]=None, res_attention:bool=True, pre_norm:bool=False, store_attn:bool=False,\n                 pe:str='zeros', learn_pe:bool=True, fc_dropout:float=0., head_dropout = 0, padding_patch = None,\n                 pretrain_head:bool=False, head_type = 'flatten', individual = False, revin = True, affine = True, subtract_last = False):\n        \n        super().__init__()\n        # RevIn\n        self.revin = revin\n        if self.revin: self.revin_layer = RevIN(c_in, affine=affine, subtract_last=subtract_last)\n\n        # Patching\n        self.patch_len = patch_len\n        self.stride = stride\n        self.padding_patch = padding_patch\n        patch_num = int((input_size - patch_len)/stride + 1)\n        if padding_patch == 'end':\n            self.padding_patch_layer = nn.ReplicationPad1d((0, stride)) \n            patch_num += 1\n\n        # Rest of implementation...\n```\n\n----------------------------------------\n\nTITLE: Defining the VanillaTransformer Model in Python\nDESCRIPTION: This snippet defines the `VanillaTransformer` class using PyTorch. It sets up an encoder with `FullAttention` layers and a decoder with masked and unmasked attention layers. The `forward` method outlines the data flow for generating forecasts, including embedding, encoding, and decoding steps. It handles optional future exogenous variables (`futr_exog`).\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.vanillatransformer.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n                    AttentionLayer(\n                        FullAttention(mask_flag=False,\n                                      attention_dropout=dropout,\n                                      output_attention=self.output_attention),\n                        hidden_size, n_head),\n                    hidden_size,\n                    conv_hidden_size,\n                    dropout=dropout,\n                    activation=activation\n                ) for l in range(encoder_layers)\n            ],\n            norm_layer=torch.nn.LayerNorm(hidden_size)\n        )\n        # Decoder\n        self.decoder = TransDecoder(\n            [\n                TransDecoderLayer(\n                    AttentionLayer(\n                        FullAttention(mask_flag=True, attention_dropout=dropout, output_attention=False),\n                        hidden_size, n_head),\n                    AttentionLayer(\n                        FullAttention(mask_flag=False, attention_dropout=dropout, output_attention=False),\n                        hidden_size, n_head),\n                    hidden_size,\n                    conv_hidden_size,\n                    dropout=dropout,\n                    activation=activation,\n                )\n                for l in range(decoder_layers)\n            ],\n            norm_layer=torch.nn.LayerNorm(hidden_size),\n            projection=nn.Linear(hidden_size, self.c_out, bias=True)\n        )\n\n    def forward(self, windows_batch):\n        # Parse windows_batch\n        insample_y    = windows_batch['insample_y']\n        futr_exog     = windows_batch['futr_exog']\n\n        if self.futr_exog_size > 0:\n            x_mark_enc = futr_exog[:,:self.input_size,:]\n            x_mark_dec = futr_exog[:,-(self.label_len+self.h):,:]\n        else:\n            x_mark_enc = None\n            x_mark_dec = None\n\n        x_dec = torch.zeros(size=(len(insample_y),self.h,1), device=insample_y.device)\n        x_dec = torch.cat([insample_y[:,-self.label_len:,:], x_dec], dim=1)\n\n        enc_out = self.enc_embedding(insample_y, x_mark_enc)\n        enc_out, _ = self.encoder(enc_out, attn_mask=None) # attns visualization\n\n        dec_out = self.dec_embedding(x_dec, x_mark_dec)\n        dec_out = self.decoder(dec_out, enc_out, x_mask=None, \n                               cross_mask=None)\n\n        forecast = dec_out[:, -self.h:]\n        return forecast\n```\n\n----------------------------------------\n\nTITLE: Endogenous Embedding Implementation\nDESCRIPTION: Implements the embedding layer for endogenous variables with patching and positional encoding capabilities.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timexer.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass EnEmbedding(nn.Module):\n    def __init__(self, n_vars, d_model, patch_len, dropout):\n        super(EnEmbedding, self).__init__()\n        # Patching\n        self.patch_len = patch_len\n\n        self.value_embedding = nn.Linear(patch_len, d_model, bias=False)\n        self.glb_token = nn.Parameter(torch.randn(1, n_vars, 1, d_model))\n        self.position_embedding = PositionalEmbedding(d_model)\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # do patching\n        n_vars = x.shape[1]\n        glb = self.glb_token.repeat((x.shape[0], 1, 1, 1))\n\n        x = x.unfold(dimension=-1, size=self.patch_len, step=self.patch_len)\n        x = torch.reshape(x, (x.shape[0] * x.shape[1], x.shape[2], x.shape[3]))\n        # Input encoding\n        x = self.value_embedding(x) + self.position_embedding(x)\n        x = torch.reshape(x, (-1, n_vars, x.shape[-2], x.shape[-1]))\n        x = torch.cat([x, glb], dim=2)\n        x = torch.reshape(x, (x.shape[0] * x.shape[1], x.shape[2], x.shape[3]))\n        return self.dropout(x), n_vars\n```\n\n----------------------------------------\n\nTITLE: Training Step Implementation for Neural Forecasting Models\nDESCRIPTION: Implements the training step for neural forecasting models, handling different modes of operation (recurrent vs. direct). The method creates time windows, performs normalization, and prepares the input data structure for model training including exogenous variables management.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_30\n\nLANGUAGE: Python\nCODE:\n```\ndef training_step(self, batch, batch_idx):\n        # Set horizon to h_train in case of recurrent model to speed up training\n        if self.RECURRENT:\n            self.h = self.h_train\n        \n        # windows: [Ws, L + h, C, n_series] or [Ws, L + h, C]\n        y_idx = batch['y_idx']\n\n        windows = self._create_windows(batch, step='train')\n        original_outsample_y = torch.clone(windows['temporal'][:, self.input_size:, y_idx])\n        windows = self._normalization(windows=windows, y_idx=y_idx)\n        \n        # Parse windows\n        insample_y, insample_mask, outsample_y, outsample_mask, \\\n               hist_exog, futr_exog, stat_exog = self._parse_windows(batch, windows)\n\n        windows_batch = dict(insample_y=insample_y,                 # [Ws, L, n_series]\n                        insample_mask=insample_mask,                # [Ws, L, n_series]\n                        futr_exog=futr_exog,                        # univariate: [Ws, L, F]; multivariate: [Ws, F, L, n_series]\n```\n\n----------------------------------------\n\nTITLE: Testing AutoTimeXer Configuration and Implementations\nDESCRIPTION: Comprehensive unit tests for the AutoTimeXer class, verifying correct behavior with both Optuna and Ray backends, configuration parameter handling, and model fitting functionality.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_92\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n# Check Optuna\nassert model.config(MockTrial())['h'] == 12\n\n# Unit test to test that Auto* model contains all required arguments from BaseAuto\ntest_args(AutoTimeXer, exclude_args=['cls_model']) \n\n# Unit test for situation: Optuna with updated default config\nmy_config = AutoTimeXer.get_default_config(h=12, n_series=1, backend='optuna')\ndef my_config_new(trial):\n    config = {**my_config(trial)}\n    config.update({'max_steps': 1, 'val_check_steps': 1, 'input_size': 12, 'patch_len': 12})\n    return config\n\nmodel = AutoTimeXer(h=12, n_series=1, config=my_config_new, backend='optuna', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n\n# Unit test for situation: Ray with updated default config\nmy_config = AutoTimeXer.get_default_config(h=12, n_series=1, backend='ray')\nmy_config['max_steps'] = 1\nmy_config['val_check_steps'] = 1\nmy_config['input_size'] = 12\nmy_config['patch_len'] = 12\nmodel = AutoTimeXer(h=12, n_series=1, config=my_config, backend='ray', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: Displaying TSMixer.fit Method Documentation in Python\nDESCRIPTION: Command to show the documentation specifically for the fit method of the TSMixer class, which contains information about training the model on time series data.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tsmixer.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(TSMixer.fit, name='TSMixer.fit')\n```\n\n----------------------------------------\n\nTITLE: Training and Testing NBEATS Model with TimeSeriesDataset - Python\nDESCRIPTION: Splits air passenger data into training and test sets, instantiates and trains an NBEATS model, predicts on the dataset, and visualizes results by plotting actual vs. forecasted values. Demonstrates an end-to-end workflow for time series regression with plotting. Depends on pandas, matplotlib, and neuralforecast modules.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeats.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n#| hide\nY_train_df = Y_df[Y_df.ds<Y_df['ds'].values[-12]] # 132 train\nY_test_df = Y_df[Y_df.ds>=Y_df['ds'].values[-12]]   # 12 test\n\ndataset, *_ = TimeSeriesDataset.from_df(df = Y_train_df)\nnbeats = NBEATS(h=12, input_size=24, windows_batch_size=None, \n                stack_types=['identity', 'trend', 'seasonality'], max_steps=1)\nnbeats.fit(dataset=dataset)\ny_hat = nbeats.predict(dataset=dataset)\nY_test_df['N-BEATS'] = y_hat\n\npd.concat([Y_train_df, Y_test_df]).drop('unique_id', axis=1).set_index('ds').plot()\n```\n\n----------------------------------------\n\nTITLE: Implementing TFT Embedding Layer in PyTorch\nDESCRIPTION: Neural network module that handles embedding of static, future, historical and target inputs. Processes four types of inputs: static continuous, temporal known a priori continuous, temporal observed continuous, and temporal observed targets.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nclass TFTEmbedding(nn.Module):\n    def __init__(\n        self, hidden_size, stat_input_size, futr_input_size, hist_input_size, tgt_size\n    ):\n        super().__init__()\n        # There are 4 types of input:\n        # 1. Static continuous\n        # 2. Temporal known a priori continuous\n        # 3. Temporal observed continuous\n        # 4. Temporal observed targets (time series obseved so far)\n\n        self.hidden_size = hidden_size\n\n        self.stat_input_size = stat_input_size\n        self.futr_input_size = futr_input_size\n        self.hist_input_size = hist_input_size\n        self.tgt_size = tgt_size\n\n        # Instantiate Continuous Embeddings if size is not None\n        for attr, size in [\n            (\"stat_exog_embedding\", stat_input_size),\n            (\"futr_exog_embedding\", futr_input_size),\n            (\"hist_exog_embedding\", hist_input_size),\n            (\"tgt_embedding\", tgt_size),\n        ]:\n            if size:\n                vectors = nn.Parameter(torch.Tensor(size, hidden_size))\n                bias = nn.Parameter(torch.zeros(size, hidden_size))\n                torch.nn.init.xavier_normal_(vectors)\n                setattr(self, attr + \"_vectors\", vectors)\n                setattr(self, attr + \"_bias\", bias)\n            else:\n                setattr(self, attr + \"_vectors\", None)\n                setattr(self, attr + \"_bias\", None)\n```\n\n----------------------------------------\n\nTITLE: TiDE Model Usage Example with Air Passengers Dataset\nDESCRIPTION: Example demonstrating how to use the TiDE model for forecasting using the Air Passengers dataset, including model configuration, training, prediction, and visualization of results with confidence intervals.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tide.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import TiDE\nfrom neuralforecast.losses.pytorch import GMM\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]]\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True)\n\nfcst = NeuralForecast(\n    models=[\n            TiDE(h=12,\n                input_size=24,\n                loss=GMM(n_components=7, return_params=True, level=[80,90], weighted=True),\n                max_steps=100,\n                scaler_type='standard',\n                futr_exog_list=['y_[lag12]'],\n                hist_exog_list=None,\n                stat_exog_list=['airline1'],\n                ),     \n    ],\n    freq='ME'\n)\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\n# Plot quantile predictions\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['TiDE-median'], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'][-12:], \n                 y1=plot_df['TiDE-lo-90'][-12:].values,\n                 y2=plot_df['TiDE-hi-90'][-12:].values,\n                 alpha=0.4, label='level 90')\nplt.legend()\nplt.grid()\n```\n\n----------------------------------------\n\nTITLE: Processing Model Outputs with Distribution-Based or Point-Based Loss in PyTorch\nDESCRIPTION: This snippet handles the processing of model outputs, dealing with both distribution-based and point-based losses. For distribution-based losses, it extracts location and scale parameters and samples from the distribution. For point-based losses, it handles special cases with multi-dimensional outputs.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_26\n\nLANGUAGE: Python\nCODE:\n```\noutput_batch = self.loss.domain_map(output_batch_unmapped)\n        \n        # Inverse normalization and sampling\n        if self.loss.is_distribution_output:\n            # Sample distribution\n            y_loc, y_scale = self._get_loc_scale(y_idx)\n            distr_args = self.loss.scale_decouple(output=output_batch, loc=y_loc, scale=y_scale)\n            # When validating, the output is the mean of the distribution which is an attribute\n            distr = self.loss.get_distribution(distr_args=distr_args)\n\n            # Scale back to feed back as input\n            insample_y = self.scaler.scaler(distr.mean, y_loc, y_scale)\n        else:\n            # Todo: for now, we assume that in case of a BasePointLoss with ndim==4, the last dimension\n            # contains a set of predictions for the target (e.g. MQLoss multiple quantiles), for which we use the \n            # mean as feedback signal for the recurrent predictions. A more precise way is to increase the\n            # insample input size of the recurrent network by the number of outputs so that each output\n            # can be fed back to a specific input channel. \n            if output_batch.ndim == 4:\n                output_batch = output_batch.mean(dim=-1)\n\n            insample_y = output_batch\n\n        # Remove horizon dim: [B, 1, N * n_outputs] -> [B, N * n_outputs]\n        y_hat = output_batch_unmapped.squeeze(1)\n        return y_hat, insample_y\n```\n\n----------------------------------------\n\nTITLE: Performing Temporal Cross-Validation with NeuralForecast in Python\nDESCRIPTION: This method performs rolling or chained cross-validation for NeuralForecast models on time series data. It manages windowed train/test splits, refitting logic, and supports probabilistic prediction intervals with requirements on 'level' or 'quantiles' parameters. The method requires pandas or polars DataFrames and expects columns like 'unique_id', 'ds', and 'y', as well as optional static and future exogenous features. The output is a DataFrame containing predictions aligned to the cross-validation folds; input validation ensures that only supported combinations of refitting, test sizing, and probabilistic interval computation are allowed. Limitations include constraints on refitting and interval calculation options, and strict requirements on input DataFrame structure.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nlevel: Optional[List[Union[int, float]]] = None,\nquantiles: Optional[List[float]] = None,\n**data_kwargs\n) -> DataFrame:\n    \"\"\"Temporal Cross-Validation with core.NeuralForecast.\n\n    `core.NeuralForecast`'s cross-validation efficiently fits a list of NeuralForecast \n    models through multiple windows, in either chained or rolled manner.\n\n    Parameters\n    ----------\n    df : pandas or polars DataFrame, optional (default=None)\n        DataFrame with columns [`unique_id`, `ds`, `y`] and exogenous variables.\n        If None, a previously stored dataset is required.\n    static_df : pandas or polars DataFrame, optional (default=None)\n        DataFrame with columns [`unique_id`] and static exogenous.\n    n_windows : int (default=1)\n        Number of windows used for cross validation.\n    step_size : int (default=1)\n        Step size between each window.\n    val_size : int, optional (default=None)\n        Length of validation size. If passed, set `n_windows=None`.\n    test_size : int, optional (default=None)\n        Length of test size. If passed, set `n_windows=None`.\n    use_init_models : bool, option (default=False)\n        Use initial model passed when object was instantiated.\n    verbose : bool (default=False)\n        Print processing steps.\n    refit : bool or int (default=False)\n        Retrain model for each cross validation window.\n        If False, the models are trained at the beginning and then used to predict each window.\n        If positive int, the models are retrained every `refit` windows.\n    id_col : str (default='unique_id')\n        Column that identifies each serie.\n    time_col : str (default='ds')\n        Column that identifies each timestep, its values can be timestamps or integers.\n    target_col : str (default='y')\n        Column that contains the target.            \n    prediction_intervals : PredictionIntervals, optional (default=None)\n        Configuration to calibrate prediction intervals (Conformal Prediction).            \n    level : list of ints or floats, optional (default=None)\n        Confidence levels between 0 and 100.\n    quantiles : list of floats, optional (default=None)\n        Alternative to level, target quantiles to predict.\n    data_kwargs : kwargs\n        Extra arguments to be passed to the dataset within each model.\n\n    Returns\n    -------\n    fcsts_df : pandas or polars DataFrame\n        DataFrame with insample `models` columns for point predictions and probabilistic\n        predictions for all fitted `models`.    \n    \"\"\"\n    h = self.h\n    if n_windows is None and test_size is None:\n        raise Exception('you must define `n_windows` or `test_size`.')            \n    if test_size is None:\n        test_size = h + step_size * (n_windows - 1)\n    elif n_windows is None:\n        if (test_size - h) % step_size:\n            raise Exception('`test_size - h` should be module `step_size`')\n        n_windows = int((test_size - h) / step_size) + 1\n    else:\n        raise Exception('you must define `n_windows` or `test_size` but not both')    \n\n    # Recover initial model if use_init_models.\n    if use_init_models:\n        self._reset_models()\n\n    # Checks for prediction intervals\n    if prediction_intervals is not None:\n        if level is None and quantiles is None:\n            raise Exception('When passing prediction_intervals you need to set the level or quantiles argument.')  \n        if not refit:\n            raise Exception('Passing prediction_intervals is only supported with refit=True.')  \n\n    if level is not None and quantiles is not None:\n        raise ValueError(\"You can't set both level and quantiles argument.\")\n    \n    if not refit:\n\n        return self._no_refit_cross_validation(\n            df=df,\n            static_df=static_df,\n            n_windows=n_windows,\n            step_size=step_size,\n            val_size=val_size,\n            test_size=test_size,\n            verbose=verbose,\n            id_col=id_col,\n            time_col=time_col,\n            target_col=target_col,\n            **data_kwargs\n        )\n    if df is None:\n        raise ValueError('Must specify `df` with `refit!=False`.')\n    validate_freq(df[time_col], self.freq)\n    splits = ufp.backtest_splits(\n        df,\n        n_windows=n_windows,\n        h=self.h,\n        id_col=id_col,\n        time_col=time_col,\n        freq=self.freq,\n        step_size=step_size,\n        input_size=None,\n    )\n    results = []\n    for i_window, (cutoffs, train, test) in enumerate(splits):\n        should_fit = i_window == 0 or (refit > 0 and i_window % refit == 0)\n        if should_fit:\n            self.fit(\n                df=train,\n                static_df=static_df,\n                val_size=val_size,\n                use_init_models=False,\n                verbose=verbose,\n                id_col=id_col,\n                time_col=time_col,\n                target_col=target_col,\n                prediction_intervals=prediction_intervals,                                     \n            )\n            predict_df: Optional[DataFrame] = None\n        else:\n            predict_df = train\n        needed_futr_exog = self._get_needed_futr_exog()\n        if needed_futr_exog:\n            futr_df: Optional[DataFrame] = test\n        else:\n            futr_df = None\n        preds = self.predict(\n            df=predict_df,\n            static_df=static_df,\n            futr_df=futr_df,\n            verbose=verbose,\n            level=level,\n            quantiles=quantiles,\n            **data_kwargs\n        )\n        preds = ufp.join(preds, cutoffs, on=id_col, how='left')\n        fold_result = ufp.join(\n            preds, test[[id_col, time_col, target_col]], on=[id_col, time_col]\n        )\n        results.append(fold_result)\n    out = ufp.vertical_concat(results, match_categories=False)\n    out = ufp.drop_index_if_pandas(out)\n    # match order of cv with no refit\n    first_out_cols = [id_col, time_col, \"cutoff\"]\n    remaining_cols = [\n        c for c in out.columns if c not in first_out_cols + [target_col]\n    ]\n    cols_order = first_out_cols + remaining_cols + [target_col]\n    return ufp.sort(out[cols_order], by=[id_col, 'cutoff', time_col])\n```\n\n----------------------------------------\n\nTITLE: Main Prediction Method for NeuralForecast\nDESCRIPTION: The primary forecasting method that handles various input formats (pandas, polars, and Spark) and supports both point forecasts and probabilistic predictions. Takes optional confidence levels or quantiles for probabilistic forecasts.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef predict(\n    self,\n    df: Optional[Union[DataFrame, SparkDataFrame]] = None,\n    static_df: Optional[Union[DataFrame, SparkDataFrame]] = None,\n    futr_df: Optional[Union[DataFrame, SparkDataFrame]] = None,\n    verbose: bool = False,\n    engine = None,\n    level: Optional[List[Union[int, float]]] = None,\n    quantiles: Optional[List[float]] = None,\n    **data_kwargs\n):\n    \"\"\"Predict with core.NeuralForecast.\n\n    Use stored fitted `models` to predict large set of time series from DataFrame `df`.        \n\n    Parameters\n    ----------\n    df : pandas, polars or spark DataFrame, optional (default=None)\n        DataFrame with columns [`unique_id`, `ds`, `y`] and exogenous variables.\n        If a DataFrame is passed, it is used to generate forecasts.\n    static_df : pandas, polars or spark DataFrame, optional (default=None)\n        DataFrame with columns [`unique_id`] and static exogenous.\n    futr_df : pandas, polars or spark DataFrame, optional (default=None)\n        DataFrame with [`unique_id`, `ds`] columns and `df`'s future exogenous.\n    verbose : bool (default=False)\n        Print processing steps.\n    engine : spark session\n        Distributed engine for inference. Only used if df is a spark dataframe or if fit was called on a spark dataframe.\n    level : list of ints or floats, optional (default=None)\n        Confidence levels between 0 and 100.\n    quantiles : list of floats, optional (default=None)\n        Alternative to level, target quantiles to predict.\n    data_kwargs : kwargs\n        Extra arguments to be passed to the dataset within each model.\n\n    Returns\n    -------\n    fcsts_df : pandas or polars DataFrame\n        DataFrame with insample `models` columns for point predictions and probabilistic\n        predictions for all fitted `models`.    \n    \"\"\"\n    if df is None and not hasattr(self, 'dataset'):\n        raise Exception('You must pass a DataFrame or have one stored.')\n```\n\n----------------------------------------\n\nTITLE: Testing AutoTFT Model with Various Configurations in Python\nDESCRIPTION: This snippet contains unit tests for the AutoTFT model, verifying its functionality with different configurations and backends (Optuna and Ray). It tests the model with default configurations, custom configurations, and ensures all required arguments are present.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_66\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n# Check Optuna\nassert model.config(MockTrial())['h'] == 12\n\n# Unit test to test that Auto* model contains all required arguments from BaseAuto\ntest_args(AutoTFT, exclude_args=['cls_model']) \n\n# Unit test for situation: Optuna with updated default config\nmy_config = AutoTFT.get_default_config(h=12, backend='optuna')\ndef my_config_new(trial):\n    config = {**my_config(trial)}\n    config.update({'max_steps': 1, 'val_check_steps': 1, 'input_size': 12, 'hidden_size': 8})\n    return config\n\nmodel = AutoTFT(h=12, config=my_config_new, backend='optuna', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n\n# Unit test for situation: Ray with updated default config\nmy_config = AutoTFT.get_default_config(h=12, backend='ray')\nmy_config['max_steps'] = 1\nmy_config['val_check_steps'] = 1\nmy_config['input_size'] = 12\nmy_config['hidden_size'] = 8\nmodel = AutoTFT(h=12, config=my_config, backend='ray', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: Displaying TSMixer Documentation in Python\nDESCRIPTION: Command to display the documentation for the TSMixer class using the show_doc function, which renders the class docstring with all parameters and functionality descriptions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tsmixer.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(TSMixer)\n```\n\n----------------------------------------\n\nTITLE: Implementing LSTM Encoder and Decoder for DeepAR in PyTorch\nDESCRIPTION: This code snippet shows the implementation of the LSTM encoder and decoder components of the DeepAR model. It initializes the LSTM encoder for processing time series data and a decoder MLP for output generation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.deepar.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n        # LSTM\n        self.encoder_n_layers = lstm_n_layers\n        self.encoder_hidden_size = lstm_hidden_size\n        self.encoder_dropout = lstm_dropout\n       \n        # LSTM input size (1 for target variable y)\n        input_encoder = 1 + self.futr_exog_size + self.stat_exog_size\n\n        # Instantiate model\n        self.rnn_state = None\n        self.maintain_state = False\n        self.hist_encoder = nn.LSTM(input_size=input_encoder,\n                                    hidden_size=self.encoder_hidden_size,\n                                    num_layers=self.encoder_n_layers,\n                                    dropout=self.encoder_dropout,\n                                    batch_first=True)\n\n        # Decoder MLP\n        self.decoder = Decoder(in_features=lstm_hidden_size,\n                               out_features=self.loss.outputsize_multiplier,\n                               hidden_size=decoder_hidden_size,\n                               hidden_layers=decoder_hidden_layers)\n```\n\n----------------------------------------\n\nTITLE: Defining LSTM Class for Time Series Forecasting\nDESCRIPTION: Implements the LSTM class, a subclass of BaseModel, with an LSTM encoder and MLP decoder for time series forecasting. It includes various hyperparameters and configuration options for model architecture and training.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.lstm.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass LSTM(BaseModel):\n    \"\"\" LSTM\n\n    LSTM encoder, with MLP decoder.\n    The network has `tanh` or `relu` non-linearities, it is trained using \n    ADAM stochastic gradient descent. The network accepts static, historic \n    and future exogenous data.\n\n    **Parameters:**<br>\n    `h`: int, forecast horizon.<br>\n    `input_size`: int, maximum sequence length for truncated train backpropagation. Default -1 uses 3 * horizon <br>\n    `inference_input_size`: int, maximum sequence length for truncated inference. Default None uses input_size history.<br>\n    `encoder_n_layers`: int=2, number of layers for the LSTM.<br>\n    `encoder_hidden_size`: int=200, units for the LSTM's hidden state size.<br>\n    `encoder_bias`: bool=True, whether or not to use biases b_ih, b_hh within LSTM units.<br>\n    `encoder_dropout`: float=0., dropout regularization applied to LSTM outputs.<br>\n    `context_size`: deprecated.<br>\n    `decoder_hidden_size`: int=200, size of hidden layer for the MLP decoder.<br>\n    `decoder_layers`: int=2, number of layers for the MLP decoder.<br>\n    `futr_exog_list`: str list, future exogenous columns.<br>\n    `hist_exog_list`: str list, historic exogenous columns.<br>\n    `stat_exog_list`: str list, static exogenous columns.<br>\n    `exclude_insample_y`: bool=False, whether to exclude the target variable from the input.<br>\n    `recurrent`: bool=False, whether to produce forecasts recursively (True) or direct (False).<br>\n    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `max_steps`: int=1000, maximum number of training steps.<br>\n    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n    `batch_size`: int=32, number of differentseries in each batch.<br>\n    `valid_batch_size`: int=None, number of different series in each validation and test batch.<br>\n    `windows_batch_size`: int=128, number of windows to sample in each training batch, default uses all.<br>\n    `inference_windows_batch_size`: int=1024, number of windows to sample in each inference batch, -1 uses all.<br>\n    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n    `step_size`: int=1, step size between each window of temporal data.<br>    \n    `scaler_type`: str='robust', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n    `random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n    `alias`: str, optional,  Custom name of the model.<br>\n    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>    \n    `dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n    \"\"\"\n    # Class attributes\n    EXOGENOUS_FUTR = True\n    EXOGENOUS_HIST = True\n    EXOGENOUS_STAT = True\n    MULTIVARIATE = False    # If the model produces multivariate forecasts (True) or univariate (False)\n    RECURRENT = True        # If the model produces forecasts recursively (True) or direct (False)\n\n    def __init__(self,\n                 h: int,\n                 input_size: int = -1,\n                 inference_input_size: Optional[int] = None,\n                 encoder_n_layers: int = 2,\n                 encoder_hidden_size: int = 128,\n                 encoder_bias: bool = True,\n                 encoder_dropout: float = 0.,\n                 context_size: Optional[int] = None,\n                 decoder_hidden_size: int = 128,\n                 decoder_layers: int = 2,\n                 futr_exog_list = None,\n                 hist_exog_list = None,\n                 stat_exog_list = None,\n                 exclude_insample_y = False,\n                 recurrent = False,\n                 loss = MAE(),\n                 valid_loss = None,\n                 max_steps: int = 1000,\n                 learning_rate: float = 1e-3,\n                 num_lr_decays: int = -1,\n                 early_stop_patience_steps: int =-1,\n                 val_check_steps: int = 100,\n                 batch_size = 32,\n                 valid_batch_size: Optional[int] = None,\n                 windows_batch_size = 128,\n                 inference_windows_batch_size = 1024,\n                 start_padding_enabled = False,\n                 step_size: int = 1,\n                 scaler_type: str = 'robust',\n                 random_seed = 1,\n                 drop_last_loader = False,\n                 alias: Optional[str] = None,\n                 optimizer = None,\n                 optimizer_kwargs = None,\n                 lr_scheduler = None,\n                 lr_scheduler_kwargs = None,\n                 dataloader_kwargs = None,\n                 **trainer_kwargs):\n        \n        self.RECURRENT = recurrent\n        \n        super(LSTM, self).__init__(\n            h=h,\n            input_size=input_size,\n            inference_input_size=inference_input_size,\n            futr_exog_list=futr_exog_list,\n            hist_exog_list=hist_exog_list,\n            stat_exog_list=stat_exog_list,\n            exclude_insample_y = exclude_insample_y,\n            loss=loss,\n            valid_loss=valid_loss,\n            max_steps=max_steps,\n            learning_rate=learning_rate,\n            num_lr_decays=num_lr_decays,\n            early_stop_patience_steps=early_stop_patience_steps,\n            val_check_steps=val_check_steps,\n            batch_size=batch_size,\n            valid_batch_size=valid_batch_size,\n            windows_batch_size=windows_batch_size,\n            inference_windows_batch_size=inference_windows_batch_size,\n            start_padding_enabled=start_padding_enabled,\n            step_size=step_size,\n            scaler_type=scaler_type,\n            random_seed=random_seed,\n            drop_last_loader=drop_last_loader,\n            alias=alias,\n            optimizer=optimizer,\n            optimizer_kwargs=optimizer_kwargs,\n            lr_scheduler=lr_scheduler,\n            lr_scheduler_kwargs=lr_scheduler_kwargs,\n            dataloader_kwargs=dataloader_kwargs,\n            **trainer_kwargs\n        )\n\n        # LSTM\n        self.encoder_n_layers = encoder_n_layers\n        self.encoder_hidden_size = encoder_hidden_size\n        self.encoder_bias = encoder_bias\n        self.encoder_dropout = encoder_dropout\n        \n        # Context adapter\n        if context_size is not None:\n            warnings.warn(\"context_size is deprecated and will be removed in future versions.\")\n\n        # MLP decoder\n        self.decoder_hidden_size = decoder_hidden_size\n        self.decoder_layers = decoder_layers\n```\n\n----------------------------------------\n\nTITLE: Time Series Forecasting with NeuralForecast and RMoK in Python\nDESCRIPTION: This code demonstrates the process of training a RMoK model using NeuralForecast for time series prediction on the AirPassengers dataset. It includes data preparation, model configuration, training, forecasting, and visualization of results. The example uses pandas for data manipulation and matplotlib for plotting.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.rmok.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import RMoK\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\nfrom neuralforecast.losses.pytorch import MSE\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = RMoK(h=12,\n             input_size=24,\n             n_series=2,\n             taylor_order=3,\n             jacobi_degree=6,\n             wavelet_function='mexican_hat',\n             dropout=0.1,\n             revin_affine=True,\n             loss=MSE(),\n             valid_loss=MAE(),\n             early_stop_patience_steps=3,\n             batch_size=32)\n\nfcst = NeuralForecast(models=[model], freq='ME')\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\n# Plot predictions\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['RMoK'], c='blue', label='Forecast')\nax.set_title('AirPassengers Forecast', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Year', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\n```\n\n----------------------------------------\n\nTITLE: Implementing NHITS Model Class in Python\nDESCRIPTION: This class defines the main NHITS model, inheriting from BaseModel. It sets up the model architecture and parameters for time series forecasting.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nhits.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass NHITS(BaseModel):\n    \"\"\" NHITS\n\n    The Neural Hierarchical Interpolation for Time Series (NHITS), is an MLP-based deep\n    neural architecture with backward and forward residual links. NHITS tackles volatility and\n    memory complexity challenges, by locally specializing its sequential predictions into\n    the signals frequencies with hierarchical interpolation and pooling.\n\n    **Parameters:**<br>\n    `h`: int, Forecast horizon. <br>\n    `input_size`: int, autorregresive inputs size, y=[1,2,3,4] input_size=2 -> y_[t-2:t]=[1,2].<br>\n    `futr_exog_list`: str list, future exogenous columns.<br>\n    `hist_exog_list`: str list, historic exogenous columns.<br>\n    `stat_exog_list`: str list, static exogenous columns.<br>\n    `exclude_insample_y`: bool=False, the model skips the autoregressive features y[t-input_size:t] if True.<br>\n    `stack_types`: List[str], stacks list in the form N * ['identity'], to be deprecated in favor of `n_stacks`. Note that len(stack_types)=len(n_freq_downsample)=len(n_pool_kernel_size).<br>\n    `n_blocks`: List[int], Number of blocks for each stack. Note that len(n_blocks) = len(stack_types).<br>\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: NBEATS Main Model Class\nDESCRIPTION: Main NBEATS model class inheriting from BaseModel, implementing the complete neural network architecture with detailed configuration options for time series forecasting.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeats.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass NBEATS(BaseModel):\n    \"\"\" NBEATS\n\n    The Neural Basis Expansion Analysis for Time Series (NBEATS), is a simple and yet\n    effective architecture, it is built with a deep stack of MLPs with the doubly \n    residual connections. It has a generic and interpretable architecture depending\n    on the blocks it uses. Its interpretable architecture is recommended for scarce\n    data settings, as it regularizes its predictions through projections unto harmonic\n    and trend basis well-suited for most forecasting tasks.\n\n    **Parameters:**<br>\n    `h`: int, forecast horizon.<br>\n    `input_size`: int, considered autorregresive inputs (lags), y=[1,2,3,4] input_size=2 -> lags=[1,2].<br>\n    `n_harmonics`: int, Number of harmonic terms for seasonality stack type. Note that len(n_harmonics) = len(stack_types). Note that it will only be used if a seasonality stack is used.<br>\n    `n_polynomials`: int, DEPRECATED - polynomial degree for trend stack. Note that len(n_polynomials) = len(stack_types). Note that it will only be used if a trend stack is used.<br>\n    `basis`: str, Type of basis function to use in the trend stack. Choose one from ['legendre', 'polynomial', 'changepoint', 'piecewise_linear', 'linear_hat', 'spline', 'chebyshev']<br>\n    `n_basis`: int, the degree of the basis function for the trend stack. Note that it will only be used if a trend stack is used.<br>\n    `stack_types`: List[str], List of stack types. Subset from ['seasonality', 'trend', 'identity'].<br>\n    `n_blocks`: List[int], Number of blocks for each stack. Note that len(n_blocks) = len(stack_types).<br>\n    `mlp_units`: List[List[int]], Structure of hidden layers for each stack type. Each internal list should contain the number of units of each hidden layer. Note that len(n_hidden) = len(stack_types).<br>\n    `dropout_prob_theta`: float, Float between (0, 1). Dropout for N-BEATS basis.<br>\n    `activation`: str, activation from ['ReLU', 'Softplus', 'Tanh', 'SELU', 'LeakyReLU', 'PReLU', 'Sigmoid'].<br>\n    `shared_weights`: bool, If True, all blocks within each stack will share parameters. <br>\n    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `max_steps`: int=1000, maximum number of training steps.<br>\n    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n    `num_lr_decays`: int=3, Number of learning rate decays, evenly distributed across max_steps.<br>\n    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n    `batch_size`: int=32, number of different series in each batch.<br>\n    `valid_batch_size`: int=None, number of different series in each validation and test batch, if None uses batch_size.<br>\n    `windows_batch_size`: int=1024, number of windows to sample in each training batch, default uses all.<br>\n    `inference_windows_batch_size`: int=-1, number of windows to sample in each inference batch, -1 uses all.<br>\n    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n    `step_size`: int=1, step size between each window of temporal data.<br>\n    `scaler_type`: str='identity', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n    `random_seed`: int, random_seed for pytorch initializer and numpy generators.<br>\n    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n    `alias`: str, optional,  Custom name of the model.<br>\n    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>\n    `dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>\n\n    **References:**<br>\n    -[Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, Yoshua Bengio (2019). \n    \"N-BEATS: Neural basis expansion analysis for interpretable time series forecasting\".](https://arxiv.org/abs/1905.10437)\n    \"\"\"\n    # Class attributes\n    EXOGENOUS_FUTR = False\n    EXOGENOUS_HIST = False\n    EXOGENOUS_STAT = False\n    MULTIVARIATE = False    # If the model produces multivariate forecasts (True) or univariate (False)\n    RECURRENT = False       # If the model produces forecasts recursively (True) or direct (False)\n    \n    def __init__(self,\n                 h,\n                 input_size,\n                 n_harmonics: int = 2,\n                 n_polynomials: Optional[int] = None,\n                 n_basis: int = 2,\n                 basis: str = 'polynomial',\n                 stack_types: list = ['identity', 'trend', 'seasonality'],\n                 n_blocks: list = [1, 1, 1],\n                 mlp_units: list = 3 * [[512, 512]],\n                 dropout_prob_theta: float = 0.,\n                 activation: str = 'ReLU',\n                 shared_weights: bool = False,                 \n                 loss = MAE(),\n                 valid_loss = None,\n                 max_steps: int = 1000,\n                 learning_rate: float = 1e-3,\n                 num_lr_decays: int = 3,\n                 early_stop_patience_steps: int =-1,\n                 val_check_steps: int = 100,\n                 batch_size: int = 32,\n                 valid_batch_size: Optional[int] = None,\n                 windows_batch_size: int = 1024,\n                 inference_windows_batch_size: int = -1,\n                 start_padding_enabled = False,\n                 step_size: int = 1,\n                 scaler_type: str ='identity',\n                 random_seed: int = 1,\n                 drop_last_loader: bool = False,\n                 alias: Optional[str] = None,\n                 optimizer = None,\n                 optimizer_kwargs = None,\n                 lr_scheduler = None,\n                 lr_scheduler_kwargs = None,\n                 dataloader_kwargs = None,\n                 **trainer_kwargs):\n        \n        # Protect horizon collapsed seasonality and trend NBEATSx-i basis\n        if h == 1 and ((\"seasonality\" in stack_types) or (\"trend\" in stack_types)):\n            raise Exception(\n                \"Horizon `h=1` incompatible with `seasonality` or `trend` in stacks\"\n            )\n\n        # Inherit BaseWindows class\n        super(NBEATS, self).__init__(h=h,\n                                     input_size=input_size,\n                                     loss=loss,\n                                     valid_loss=valid_loss,\n                                     max_steps=max_steps,\n                                     learning_rate=learning_rate,\n                                     num_lr_decays=num_lr_decays,\n                                     early_stop_patience_steps=early_stop_patience_steps,\n                                     val_check_steps=val_check_steps,\n                                     batch_size=batch_size,\n                                     windows_batch_size=windows_batch_size,\n                                     valid_batch_size=valid_batch_size,\n                                     inference_windows_batch_size=inference_windows_batch_size)\n```\n\n----------------------------------------\n\nTITLE: Implementing BaseModel Class for NeuralForecast in Python\nDESCRIPTION: Defines the BaseModel class, a PyTorch Lightning module that serves as the foundation for neural forecasting models. It includes initialization logic, attribute definitions, and various checks for model configuration.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass BaseModel(pl.LightningModule):\n    EXOGENOUS_FUTR = True   # If the model can handle future exogenous variables\n    EXOGENOUS_HIST = True   # If the model can handle historical exogenous variables\n    EXOGENOUS_STAT = True   # If the model can handle static exogenous variables\n    MULTIVARIATE = False    # If the model produces multivariate forecasts (True) or univariate (False)\n    RECURRENT = False       # If the model produces forecasts recursively (True) or direct (False)\n\n    def __init__(\n        self,\n        h: int,\n        input_size: int,\n        loss: Union[BasePointLoss, DistributionLoss, nn.Module],\n        valid_loss: Union[BasePointLoss, DistributionLoss, nn.Module],\n        learning_rate: float,\n        max_steps: int,\n        val_check_steps: int,\n        batch_size: int,\n        valid_batch_size: Union[int, None],\n        windows_batch_size: int,\n        inference_windows_batch_size: Union[int, None],\n        start_padding_enabled: bool,\n        n_series: Union[int, None] = None,\n        n_samples: Union[int, None] = 100,\n        h_train: int = 1,\n        inference_input_size: Union[int, None] = None,\n        step_size: int = 1,\n        num_lr_decays: int = 0,\n        early_stop_patience_steps: int = -1,\n        scaler_type: str = 'identity',\n        futr_exog_list: Union[List, None] = None,\n        hist_exog_list: Union[List, None] = None,\n        stat_exog_list: Union[List, None] = None,\n        exclude_insample_y: Union[bool, None] = False,\n        drop_last_loader: Union[bool, None] = False,\n        random_seed: Union[int, None] = 1,\n        alias: Union[str, None] = None,\n        optimizer: Union[torch.optim.Optimizer, None] = None,\n        optimizer_kwargs: Union[Dict, None] = None,\n        lr_scheduler: Union[torch.optim.lr_scheduler.LRScheduler, None] = None,\n        lr_scheduler_kwargs: Union[Dict, None] = None,\n        dataloader_kwargs=None,\n        **trainer_kwargs,\n    ):\n        super().__init__()\n\n        # Multivarariate checks\n        if self.MULTIVARIATE and n_series is None:\n            raise Exception(f'{type(self).__name__} is a multivariate model. Please set n_series to the number of unique time series in your dataset.')\n        if not self.MULTIVARIATE:\n            n_series = 1\n        self.n_series = n_series          \n\n        # Protections for previous recurrent models\n        if input_size < 1:\n            input_size = 3 * h\n            warnings.warn(\n                f'Input size too small. Automatically setting input size to 3 * horizon = {input_size}'\n            )\n\n        if inference_input_size is None:\n            inference_input_size = input_size            \n        elif inference_input_size is not None and inference_input_size < 1:\n            inference_input_size = input_size\n            warnings.warn(\n                f'Inference input size too small. Automatically setting inference input size to input_size = {input_size}'\n            )\n\n        # For recurrent models we need one additional input as we need to shift insample_y to use it as input\n        if self.RECURRENT:\n            input_size += 1\n            inference_input_size += 1\n\n        # Attributes needed for recurrent models\n        self.horizon_backup = h\n        self.input_size_backup = input_size\n        self.n_samples = n_samples\n        if self.RECURRENT:\n            if (\n                hasattr(loss, \"horizon_weight\")\n                and loss.horizon_weight is not None\n                and h_train != h\n            ):\n                warnings.warn(\n                    f\"Setting h_train={h} to match the horizon_weight length.\"\n                )\n                h_train = h\n            self.h_train = h_train\n            self.inference_input_size = inference_input_size\n            self.rnn_state = None\n            self.maintain_state = False\n\n        with warnings.catch_warnings(record=False):\n            warnings.filterwarnings('ignore')\n            # the following line issues a warning about the loss attribute being saved\n            # but we do want to save it\n            self.save_hyperparameters() # Allows instantiation from a checkpoint from class\n        self.random_seed = random_seed\n        pl.seed_everything(self.random_seed, workers=True)\n\n        # Loss\n        self.loss = loss\n        if valid_loss is None:\n            self.valid_loss = loss\n        else:\n            self.valid_loss = valid_loss\n        self.train_trajectories: List = []\n        self.valid_trajectories: List = []\n\n        # Optimization\n        if optimizer is not None and not issubclass(optimizer, torch.optim.Optimizer):\n            raise TypeError(\"optimizer is not a valid subclass of torch.optim.Optimizer\")\n        self.optimizer = optimizer\n        self.optimizer_kwargs = optimizer_kwargs if optimizer_kwargs is not None else {}\n\n        # lr scheduler\n        if lr_scheduler is not None and not issubclass(lr_scheduler, torch.optim.lr_scheduler.LRScheduler):\n            raise TypeError(\"lr_scheduler is not a valid subclass of torch.optim.lr_scheduler.LRScheduler\")\n        self.lr_scheduler = lr_scheduler\n        self.lr_scheduler_kwargs = lr_scheduler_kwargs if lr_scheduler_kwargs is not None else {}\n\n        # Variables\n        self.futr_exog_list = list(futr_exog_list) if futr_exog_list is not None else []\n        self.hist_exog_list = list(hist_exog_list) if hist_exog_list is not None else []\n        self.stat_exog_list = list(stat_exog_list) if stat_exog_list is not None else []\n\n        # Set data sizes\n        self.futr_exog_size = len(self.futr_exog_list)\n        self.hist_exog_size = len(self.hist_exog_list)\n        self.stat_exog_size = len(self.stat_exog_list)   \n\n        # Check if model supports exogenous, otherwise raise Exception\n        if not self.EXOGENOUS_FUTR and self.futr_exog_size > 0:\n            raise Exception(f'{type(self).__name__} does not support future exogenous variables.')\n        if not self.EXOGENOUS_HIST and self.hist_exog_size > 0:\n            raise Exception(f'{type(self).__name__} does not support historical exogenous variables.')\n        if not self.EXOGENOUS_STAT and self.stat_exog_size > 0:\n            raise Exception(f'{type(self).__name__} does not support static exogenous variables.')\n\n        # Protections for loss functions\n        if isinstance(self.loss, (losses.IQLoss, losses.HuberIQLoss)):\n            loss_type = type(self.loss)\n            if not isinstance(self.valid_loss, loss_type):\n                raise Exception(f'Please set valid_loss={type(self.loss).__name__}() when training with {type(self.loss).__name__}')\n        if isinstance(self.loss, (losses.MQLoss, losses.HuberMQLoss)):\n            if not isinstance(self.valid_loss, (losses.MQLoss, losses.HuberMQLoss)):\n                raise Exception(f'Please set valid_loss to MQLoss() or HuberMQLoss() when training with {type(self.loss).__name__}')\n        if isinstance(self.valid_loss, (losses.IQLoss, losses.HuberIQLoss)):\n            valid_loss_type = type(self.valid_loss)\n            if not isinstance(self.loss, valid_loss_type):\n                raise Exception(f'Please set loss={type(self.valid_loss).__name__}() when validating with {type(self.valid_loss).__name__}')        \n\n        # Deny impossible loss / valid_loss combinations\n        if isinstance(self.loss, losses.BasePointLoss) and self.valid_loss.is_distribution_output:\n            raise Exception(f'Validation with distribution loss {type(self.valid_loss).__name__} is not possible when using loss={type(self.loss).__name__}. Please use a point valid_loss (MAE, MSE, ...)')\n        elif self.valid_loss.is_distribution_output and self.valid_loss is not loss:\n```\n\n----------------------------------------\n\nTITLE: DeepNPTS Model Implementation\nDESCRIPTION: Defines the DeepNPTS class implementing the Deep Non-Parametric Time Series Forecaster with comprehensive configuration options and neural network architecture.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.deepnpts.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass DeepNPTS(BaseModel):\n    \"\"\" DeepNPTS\n\n    Deep Non-Parametric Time Series Forecaster (`DeepNPTS`) is a baseline model for time-series forecasting. This model generates predictions by (weighted) sampling from the empirical distribution according to a learnable strategy. The strategy is learned by exploiting the information across multiple related time series.\n\n    **Parameters:**<br>\n    `h`: int, Forecast horizon. <br>\n    `input_size`: int, autorregresive inputs size, y=[1,2,3,4] input_size=2 -> y_[t-2:t]=[1,2].<br>\n    `hidden_size`: int=32, hidden size of dense layers.<br>\n    `batch_norm`: bool=True, if True, applies Batch Normalization after each dense layer in the network.<br>\n    `dropout`: float=0.1, dropout.<br>\n    `n_layers`: int=2, number of dense layers.<br>\n    `stat_exog_list`: str list, static exogenous columns.<br>\n    `hist_exog_list`: str list, historic exogenous columns.<br>\n    `futr_exog_list`: str list, future exogenous columns.<br>\n    `exclude_insample_y`: bool=False, the model skips the autoregressive features y[t-input_size:t] if True.<br>\n    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `max_steps`: int=1000, maximum number of training steps.<br>\n    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n    `batch_size`: int=32, number of different series in each batch.<br>\n    `valid_batch_size`: int=None, number of different series in each validation and test batch, if None uses batch_size.<br>\n    `windows_batch_size`: int=1024, number of windows to sample in each training batch, default uses all.<br>\n    `inference_windows_batch_size`: int=-1, number of windows to sample in each inference batch, -1 uses all.<br>\n    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n    `step_size`: int=1, step size between each window of temporal data.<br>\n    `scaler_type`: str='identity', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n    `random_seed`: int, random_seed for pytorch initializer and numpy generators.<br>\n    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n    `alias`: str, optional,  Custom name of the model.<br>\n    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>\n    `dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n\n    **References**<br>\n    - [Rangapuram, Syama Sundar, Jan Gasthaus, Lorenzo Stella, Valentin Flunkert, David Salinas, Yuyang Wang, and Tim Januschowski (2023). \"Deep Non-Parametric Time Series Forecaster\". arXiv.](https://arxiv.org/abs/2312.14657)<br>\n\n    \"\"\"\n    # Class attributes\n    EXOGENOUS_FUTR = True\n    EXOGENOUS_HIST = True\n    EXOGENOUS_STAT = True\n    MULTIVARIATE = False    # If the model produces multivariate forecasts (True) or univariate (False)\n    RECURRENT = False       # If the model produces forecasts recursively (True) or direct (False)\n    \n    def __init__(self,\n                 h,\n                 input_size: int,\n                 hidden_size: int = 32,\n                 batch_norm: bool = True,\n                 dropout: float = 0.1,\n                 n_layers: int = 2,\n                 stat_exog_list = None,\n                 hist_exog_list = None,\n                 futr_exog_list = None,\n                 exclude_insample_y = False,\n                 loss = MAE(),\n                 valid_loss = MAE(),\n                 max_steps: int = 1000,\n                 learning_rate: float = 1e-3,\n                 num_lr_decays: int = 3,\n                 early_stop_patience_steps: int =-1,\n                 val_check_steps: int = 100,\n                 batch_size: int = 32,\n                 valid_batch_size: Optional[int] = None,\n                 windows_batch_size: int = 1024,\n                 inference_windows_batch_size: int = 1024,\n                 start_padding_enabled = False,\n                 step_size: int = 1,\n                 scaler_type: str = 'standard',\n                 random_seed: int = 1,\n                 drop_last_loader = False,\n                 alias: Optional[str] = None,\n                 optimizer = None,\n                 optimizer_kwargs = None,\n                 lr_scheduler = None,\n                 lr_scheduler_kwargs = None,\n                 dataloader_kwargs = None,\n                 **trainer_kwargs):\n\n        if exclude_insample_y:\n            raise Exception('DeepNPTS has no possibility for excluding y.')\n\n        if loss.outputsize_multiplier > 1:\n            raise Exception('DeepNPTS only supports point loss functions (MAE, MSE, etc) as loss function.')               \n    \n        if valid_loss is not None and not isinstance(valid_loss, losses.BasePointLoss):\n            raise Exception('DeepNPTS only supports point loss functions (MAE, MSE, etc) as valid loss function.')   \n            \n        # Inherit BaseWindows class\n        super(DeepNPTS, self).__init__(h=h,\n                                    input_size=input_size,\n                                    stat_exog_list=stat_exog_list,\n                                    hist_exog_list=hist_exog_list,\n                                    futr_exog_list=futr_exog_list,\n                                    exclude_insample_y = exclude_insample_y,\n                                    loss=loss,\n                                    valid_loss=valid_loss,\n                                    max_steps=max_steps,\n                                    learning_rate=learning_rate,\n                                    num_lr_decays=num_lr_decays,\n                                    early_stop_patience_steps=early_stop_patience_steps,\n                                    val_check_steps=val_check_steps,\n                                    batch_size=batch_size,\n                                    valid_batch_size=valid_batch_size,\n                                    windows_batch_size=windows_batch_size,\n                                    inference_windows_batch_size=inference_windows_batch_size,\n                                    start_padding_enabled=start_padding_enabled,\n                                    step_size=step_size,\n                                    scaler_type=scaler_type,\n                                    random_seed=random_seed,\n                                    drop_last_loader=drop_last_loader,\n                                    alias=alias,\n                                    optimizer=optimizer,\n                                    optimizer_kwargs=optimizer_kwargs,\n                                    lr_scheduler=lr_scheduler,\n                                    lr_scheduler_kwargs=lr_scheduler_kwargs,\n                                    dataloader_kwargs=dataloader_kwargs,\n                                    **trainer_kwargs)\n\n        self.h = h\n        self.hidden_size = hidden_size\n        self.dropout = dropout\n\n        input_dim = input_size * (1 + self.futr_exog_size + self.hist_exog_size) + self.stat_exog_size + self.h * self.futr_exog_size\n        \n        # Create DeepNPTSNetwork\n        modules = []       \n        for i in range(n_layers):\n            modules.append(nn.Linear(input_dim if i == 0 else hidden_size, hidden_size))\n            modules.append(nn.ReLU())\n            if batch_norm:\n                modules.append(nn.BatchNorm1d(hidden_size))\n            if dropout > 0.0:\n                modules.append(nn.Dropout(dropout))\n\n        modules.append(nn.Linear(hidden_size, input_size * self.h))\n        self.deepnptsnetwork = nn.Sequential(*modules)\n\n    def forward(self, windows_batch):\n        # Parse windows_batch\n        x             = windows_batch['insample_y']                     #   [B, L, 1]\n        hist_exog     = windows_batch['hist_exog']                      #   [B, L, X]\n```\n\n----------------------------------------\n\nTITLE: Full Model Training and Forecasting Example with TimesNet - Python\nDESCRIPTION: This comprehensive example demonstrates loading and partitioning the AirPassengers dataset, specifying TimesNet model hyperparameters (including loss distribution, optimizer, validation, and early stopping settings), training on the data, and making forecasts using NeuralForecast's API. It also includes plotting predicted intervals or values using matplotlib, depending on whether the loss is probabilistic. Required dependencies are pandas, matplotlib.pyplot, NeuralForecast, and neuralforecast.losses.pytorch. Inputs are time series DataFrames; outputs are forecasted values and plots. The code handles both point and distributional forecasts and assumes existence of AirPassengersPanel and AirPassengersStatic data.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timesnet.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n#| eval: false\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.losses.pytorch import DistributionLoss\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = TimesNet(h=12,\n                 input_size=24,\n                 hidden_size = 16,\n                 conv_hidden_size = 32,\n                 loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n                 scaler_type='standard',\n                 learning_rate=1e-3,\n                 max_steps=100,\n                 val_check_steps=50,\n                 early_stop_patience_steps=2)\n\nnf = NeuralForecast(\n    models=[model],\n    freq='ME'\n)\nnf.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nforecasts = nf.predict(futr_df=Y_test_df)\n\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nif model.loss.is_distribution_output:\n    plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n    plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n    plt.plot(plot_df['ds'], plot_df['TimesNet-median'], c='blue', label='median')\n    plt.fill_between(x=plot_df['ds'][-12:], \n                    y1=plot_df['TimesNet-lo-90'][-12:].values, \n                    y2=plot_df['TimesNet-hi-90'][-12:].values,\n                    alpha=0.4, label='level 90')\n    plt.grid()\n    plt.legend()\n    plt.plot()\nelse:\n    plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n    plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n    plt.plot(plot_df['ds'], plot_df['TimesNet'], c='blue', label='Forecast')\n    plt.legend()\n    plt.grid()\n```\n\n----------------------------------------\n\nTITLE: Implementing Transformer Encoder Layer and Encoder in PyTorch\nDESCRIPTION: Defines the transformer encoder layer and full encoder implementation with attention layers and optional convolutional layers. Includes layer normalization and dropout for regularization.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.modules.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass TransEncoderLayer(nn.Module):\n    def __init__(self, attention, hidden_size, conv_hidden_size=None, dropout=0.1, activation=\"relu\"):\n        super(TransEncoderLayer, self).__init__()\n        conv_hidden_size = conv_hidden_size or 4 * hidden_size\n        self.attention = attention\n        self.conv1 = nn.Conv1d(in_channels=hidden_size, out_channels=conv_hidden_size, kernel_size=1)\n        self.conv2 = nn.Conv1d(in_channels=conv_hidden_size, out_channels=hidden_size, kernel_size=1)\n        self.norm1 = nn.LayerNorm(hidden_size)\n        self.norm2 = nn.LayerNorm(hidden_size)\n        self.dropout = nn.Dropout(dropout)\n        self.activation = F.relu if activation == \"relu\" else F.gelu\n\n    def forward(self, x, attn_mask=None):\n        new_x, attn = self.attention(\n            x, x, x,\n            attn_mask=attn_mask\n        )\n        \n        x = x + self.dropout(new_x)\n\n        y = x = self.norm1(x)\n        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n        y = self.dropout(self.conv2(y).transpose(-1, 1))\n\n        return self.norm2(x + y), attn\n\n\nclass TransEncoder(nn.Module):\n    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n        super(TransEncoder, self).__init__()\n        self.attn_layers = nn.ModuleList(attn_layers)\n        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n        self.norm = norm_layer\n\n    def forward(self, x, attn_mask=None):\n        # x [B, L, D]\n        attns = []\n        if self.conv_layers is not None:\n            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):\n                x, attn = attn_layer(x, attn_mask=attn_mask)\n                x = conv_layer(x)\n                attns.append(attn)\n            x, attn = self.attn_layers[-1](x)\n            attns.append(attn)\n        else:\n            for attn_layer in self.attn_layers:\n                x, attn = attn_layer(x, attn_mask=attn_mask)\n                attns.append(attn)\n\n        if self.norm is not None:\n            x = self.norm(x)\n\n        return x, attns\n```\n\n----------------------------------------\n\nTITLE: TCN Model Class Implementation in PyTorch\nDESCRIPTION: Defines the TCN class that inherits from BaseModel. Implements a Temporal Convolution Network with MLP decoder for time series forecasting, supporting various exogenous variables and configuration options.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tcn.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass TCN(BaseModel):\n    \"\"\" TCN\n\n    Temporal Convolution Network (TCN), with MLP decoder.\n    The historical encoder uses dilated skip connections to obtain efficient long memory,\n    while the rest of the architecture allows for future exogenous alignment.\n\n    **Parameters:**<br>\n    `h`: int, forecast horizon.<br>\n    `input_size`: int, maximum sequence length for truncated train backpropagation. Default -1 uses 3 * horizon <br>\n    `inference_input_size`: int, maximum sequence length for truncated inference. Default None uses input_size history.<br>\n    `kernel_size`: int, size of the convolving kernel.<br>\n    `dilations`: int list, ontrols the temporal spacing between the kernel points; also known as the  trous algorithm.<br>\n    `encoder_hidden_size`: int=200, units for the TCN's hidden state size.<br>\n    `encoder_activation`: str=`tanh`, type of TCN activation from `tanh` or `relu`.<br>\n    `context_size`: int=10, size of context vector for each timestamp on the forecasting window.<br>\n    `decoder_hidden_size`: int=200, size of hidden layer for the MLP decoder.<br>\n    `decoder_layers`: int=2, number of layers for the MLP decoder.<br>\n    `futr_exog_list`: str list, future exogenous columns.<br>\n    `hist_exog_list`: str list, historic exogenous columns.<br>\n    `stat_exog_list`: str list, static exogenous columns.<br>\n    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `max_steps`: int=1000, maximum number of training steps.<br>\n    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>    `batch_size`: int=32, number of differentseries in each batch.<br>\n    `batch_size`: int=32, number of differentseries in each batch.<br>\n    `valid_batch_size`: int=None, number of different series in each validation and test batch.<br>\n    `windows_batch_size`: int=128, number of windows to sample in each training batch, default uses all.<br>\n    `inference_windows_batch_size`: int=1024, number of windows to sample in each inference batch, -1 uses all.<br>\n    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n    `step_size`: int=1, step size between each window of temporal data.<br>    \n    `scaler_type`: str='robust', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n    `random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n    `alias`: str, optional,  Custom name of the model.<br>\n    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>    \n    `dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n    \"\"\"\n    # Class attributes\n    EXOGENOUS_FUTR = True\n    EXOGENOUS_HIST = True\n    EXOGENOUS_STAT = True    \n    MULTIVARIATE = False    # If the model produces multivariate forecasts (True) or univariate (False)\n    RECURRENT = False       # If the model produces forecasts recursively (True) or direct (False)    \n\n    def __init__(self,\n                 h: int,\n                 input_size: int = -1,\n                 inference_input_size: Optional[int] = None,\n                 kernel_size: int = 2,\n                 dilations: List[int] = [1, 2, 4, 8, 16],\n                 encoder_hidden_size: int = 128,\n                 encoder_activation: str = 'ReLU',\n                 context_size: int = 10,\n                 decoder_hidden_size: int = 128,\n                 decoder_layers: int = 2,\n                 futr_exog_list = None,\n                 hist_exog_list = None,\n                 stat_exog_list = None,\n                 loss=MAE(),\n                 valid_loss=None,\n                 max_steps: int = 1000,\n                 learning_rate: float = 1e-3,\n                 num_lr_decays: int = -1,\n                 early_stop_patience_steps: int =-1,\n                 val_check_steps: int = 100,\n                 batch_size: int = 32,\n                 valid_batch_size: Optional[int] = None,\n                 windows_batch_size = 128,\n                 inference_windows_batch_size = 1024,\n                 start_padding_enabled = False,\n                 step_size: int = 1,                 \n                 scaler_type: str ='robust',\n                 random_seed: int = 1,\n                 drop_last_loader = False,\n                 alias: Optional[str] = None,\n                 optimizer = None,\n                 optimizer_kwargs = None,\n                 lr_scheduler = None,\n                 lr_scheduler_kwargs = None, \n                 dataloader_kwargs = None,                \n                 **trainer_kwargs):\n        super(TCN, self).__init__(\n            h=h,\n            input_size=input_size,\n            inference_input_size=inference_input_size,\n            futr_exog_list=futr_exog_list,\n            hist_exog_list=hist_exog_list,\n            stat_exog_list=stat_exog_list,\n            loss=loss,\n            valid_loss=valid_loss,\n            max_steps=max_steps,\n            learning_rate=learning_rate,\n            num_lr_decays=num_lr_decays,\n            early_stop_patience_steps=early_stop_patience_steps,\n            val_check_steps=val_check_steps,\n            batch_size=batch_size,\n            valid_batch_size=valid_batch_size,\n            windows_batch_size=windows_batch_size,\n            inference_windows_batch_size=inference_windows_batch_size,\n            start_padding_enabled=start_padding_enabled,\n            step_size=step_size,\n            scaler_type=scaler_type,\n            random_seed=random_seed,\n            drop_last_loader=drop_last_loader,\n            alias=alias,\n            optimizer=optimizer,\n            optimizer_kwargs=optimizer_kwargs,\n            lr_scheduler=lr_scheduler,\n            lr_scheduler_kwargs=lr_scheduler_kwargs,\n            dataloader_kwargs = dataloader_kwargs,\n            **trainer_kwargs\n        )\n\n        #----------------------------------- Parse dimensions -----------------------------------#\n        # TCN\n        self.kernel_size = kernel_size\n        self.dilations = dilations\n        self.encoder_hidden_size = encoder_hidden_size\n        self.encoder_activation = encoder_activation\n```\n\n----------------------------------------\n\nTITLE: Training and Predicting with DilatedRNN using NeuralForecast (Python)\nDESCRIPTION: Demonstrates a complete workflow for using the `DilatedRNN` model within the `NeuralForecast` framework. It involves importing necessary libraries (pandas, matplotlib, neuralforecast components), loading and splitting the AirPassengers dataset, configuring `DilatedRNN` with specific parameters (horizon, input size, loss function, exogenous features), training the model using `fcst.fit()`, generating predictions with `fcst.predict()`, and finally plotting the actual values alongside the predicted median and confidence intervals. Dependencies include `pandas`, `matplotlib.pyplot`, `NeuralForecast`, `DilatedRNN`, `DistributionLoss`, and utility datasets/functions (`AirPassengersPanel`, `AirPassengersStatic`).\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.dilated_rnn.ipynb#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n#| eval: false\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import DilatedRNN\nfrom neuralforecast.losses.pytorch import DistributionLoss\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nfcst = NeuralForecast(\n    models=[DilatedRNN(h=12,\n                       input_size=-1,\n                       loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n                       scaler_type='robust',\n                       encoder_hidden_size=100,\n                       max_steps=200,\n                       futr_exog_list=['y_[lag12]'],\n                       hist_exog_list=None,\n                       stat_exog_list=['airline1'],\n    )\n    ],\n    freq='ME'\n)\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['DilatedRNN-median'], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'][-12:], \n                 y1=plot_df['DilatedRNN-lo-90'][-12:].values, \n                 y2=plot_df['DilatedRNN-hi-90'][-12:].values,\n                 alpha=0.4, label='level 90')\nplt.legend()\nplt.grid()\nplt.plot()\n```\n\n----------------------------------------\n\nTITLE: Implementing Temporal Covariate Encoder in PyTorch for TFT\nDESCRIPTION: The TemporalCovariateEncoder processes historical and future time-dependent covariates using variable selection networks and LSTM/GRU encoders. It applies gated skip connections to combine the original features with the encoded representations.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass TemporalCovariateEncoder(nn.Module):\n    def __init__(\n        self,\n        hidden_size,\n        num_historic_vars,\n        num_future_vars,\n        dropout,\n        grn_activation,\n        rnn_type=\"lstm\",\n        n_rnn_layers=1,\n    ):\n        super(TemporalCovariateEncoder, self).__init__()\n        self.rnn_type = rnn_type.lower()\n        self.n_rnn_layers = n_rnn_layers\n\n        self.history_vsn = VariableSelectionNetwork(\n            hidden_size=hidden_size,\n            num_inputs=num_historic_vars,\n            dropout=dropout,\n            grn_activation=grn_activation,\n        )\n        if self.rnn_type == \"lstm\":\n            self.history_encoder = nn.LSTM(\n                input_size=hidden_size,\n                hidden_size=hidden_size,\n                batch_first=True,\n                num_layers=n_rnn_layers,\n            )\n\n            self.future_encoder = nn.LSTM(\n                input_size=hidden_size,\n                hidden_size=hidden_size,\n                batch_first=True,\n                num_layers=n_rnn_layers,\n            )\n\n        elif self.rnn_type == \"gru\":\n            self.history_encoder = nn.GRU(\n                input_size=hidden_size,\n                hidden_size=hidden_size,\n                batch_first=True,\n                num_layers=n_rnn_layers,\n            )\n            self.future_encoder = nn.GRU(\n                input_size=hidden_size,\n                hidden_size=hidden_size,\n                batch_first=True,\n                num_layers=n_rnn_layers,\n            )\n        else:\n            raise ValueError('RNN type should be in [\"lstm\",\"gru\"] !')\n\n        self.future_vsn = VariableSelectionNetwork(\n            hidden_size=hidden_size,\n            num_inputs=num_future_vars,\n            dropout=dropout,\n            grn_activation=grn_activation,\n        )\n\n        # Shared Gated-Skip Connection\n        self.input_gate = GLU(hidden_size, hidden_size)\n        self.input_gate_ln = LayerNorm(hidden_size, eps=1e-3)\n\n    def forward(self, historical_inputs, future_inputs, cs, ch, cc):\n        # [N,X_in,L] -> [N,hidden_size,L]\n        historical_features, history_vsn_sparse_weights = self.history_vsn(\n            historical_inputs, cs\n        )\n        if self.rnn_type == \"lstm\":\n            history, state = self.history_encoder(historical_features, (ch, cc))\n\n        elif self.rnn_type == \"gru\":\n            history, state = self.history_encoder(historical_features, ch)\n\n        future_features, future_vsn_sparse_weights = self.future_vsn(future_inputs, cs)\n        future, _ = self.future_encoder(future_features, state)\n        # torch.cuda.synchronize() # this call gives prf boost for unknown reasons\n\n        input_embedding = torch.cat([historical_features, future_features], dim=1)\n        temporal_features = torch.cat([history, future], dim=1)\n        temporal_features = self.input_gate(temporal_features)\n        temporal_features = temporal_features + input_embedding\n        temporal_features = self.input_gate_ln(temporal_features)\n        return temporal_features, history_vsn_sparse_weights, future_vsn_sparse_weights\n```\n\n----------------------------------------\n\nTITLE: Implementing Full Attention and Causal Mask in PyTorch\nDESCRIPTION: Implements full attention mechanism with optional masking and scaling. Includes triangular causal mask implementation for autoregressive attention.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.modules.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nclass TriangularCausalMask():\n    \"\"\"\n    TriangularCausalMask\n    \"\"\"      \n    def __init__(self, B, L, device=\"cpu\"):\n        mask_shape = [B, 1, L, L]\n        with torch.no_grad():\n            self._mask = torch.triu(torch.ones(mask_shape, dtype=torch.bool), diagonal=1).to(device)\n\n    @property\n    def mask(self):\n        return self._mask\n\nclass FullAttention(nn.Module):\n    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n        super(FullAttention, self).__init__()\n        self.scale = scale\n        self.mask_flag = mask_flag\n        self.output_attention = output_attention\n        self.dropout = nn.Dropout(attention_dropout)\n\n    def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):\n        B, L, H, E = queries.shape\n        _, S, _, D = values.shape\n        scale = self.scale or 1. / math.sqrt(E)\n\n        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n\n        if self.mask_flag:\n            if attn_mask is None:\n                attn_mask = TriangularCausalMask(B, L, device=queries.device)\n\n            scores.masked_fill_(attn_mask.mask, -np.inf)\n\n        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n        V = torch.einsum(\"bhls,bshd->blhd\", A, values)\n\n        if self.output_attention:\n            return V.contiguous(), A\n        else:\n            return V.contiguous(), None\n```\n\n----------------------------------------\n\nTITLE: Testing Model Prediction Consistency and No Data Leakage\nDESCRIPTION: Tests to verify that the NBEATSx model produces consistent forecasts across multiple predictions and that there's no data leakage when using test_size parameter. The tests compare forecast values for equality using numpy's testing functions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeatsx.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n#test we recover the same forecast\ny_hat2 = model.predict(dataset=dataset2)\ntest_eq(y_hat, y_hat2)\n\n#test no leakage with test_size\ndataset, *_ = TimeSeriesDataset.from_df(Y_df)\nmodel = NBEATSx(h=12,\n                input_size=24,\n                scaler_type='robust',\n                stack_types = [\"identity\", \"trend\", \"seasonality\", \"exogenous\"],\n                n_blocks = [1,1,1,1],\n                futr_exog_list=['month','year'],\n                windows_batch_size=None,\n                max_steps=1)\nmodel.fit(dataset=dataset, test_size=12)\ny_hat_test = model.predict(dataset=dataset, step_size=1)\nnp.testing.assert_almost_equal(y_hat, y_hat_test, decimal=4)\n#test we recover the same forecast\ny_hat_test2 = model.predict(dataset=dataset, step_size=1)\ntest_eq(y_hat_test, y_hat_test2)\n```\n\n----------------------------------------\n\nTITLE: Initializing TiDE Model Class in Python\nDESCRIPTION: Definition of the TiDE (Time-series Dense Encoder) class that inherits from BaseModel. Implements a MLP-based encoder-decoder architecture for time-series forecasting with support for exogenous variables, customizable architecture parameters, and training configurations.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tide.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass TiDE(BaseModel):\n    EXOGENOUS_FUTR = True\n    EXOGENOUS_HIST = True\n    EXOGENOUS_STAT = True \n    MULTIVARIATE = False    \n    RECURRENT = False       \n\n    def __init__(self,\n                 h,\n                 input_size,   \n                 hidden_size = 512,\n                 decoder_output_dim = 32,\n                 temporal_decoder_dim = 128,\n                 dropout = 0.3,\n                 layernorm=True,\n                 num_encoder_layers = 1,\n                 num_decoder_layers = 1,\n                 temporal_width = 4,\n                 futr_exog_list = None,\n                 hist_exog_list = None,\n                 stat_exog_list = None,\n                 exclude_insample_y = False,\n                 loss = MAE(),\n                 valid_loss = None,\n                 max_steps: int = 1000,\n                 learning_rate: float = 1e-3,\n                 num_lr_decays: int = -1,\n                 early_stop_patience_steps: int =-1,\n                 val_check_steps: int = 100,\n                 batch_size: int = 32,\n                 valid_batch_size: Optional[int] = None,\n                 windows_batch_size = 1024,\n                 inference_windows_batch_size = 1024,\n                 start_padding_enabled = False,\n                 step_size: int = 1,\n                 scaler_type: str = 'identity',\n                 random_seed: int = 1,\n                 drop_last_loader: bool = False,\n                 alias: Optional[str] = None,\n                 optimizer = None,\n                 optimizer_kwargs = None,\n                 lr_scheduler = None,\n                 lr_scheduler_kwargs = None,\n                 dataloader_kwargs = None,\n                 **trainer_kwargs):\n\n        super(TiDE, self).__init__(\n            h=h,\n            input_size=input_size,\n            futr_exog_list=futr_exog_list,\n            hist_exog_list=hist_exog_list,\n            stat_exog_list=stat_exog_list,\n            exclude_insample_y = exclude_insample_y,\n            loss=loss,\n            valid_loss=valid_loss,\n            max_steps=max_steps,\n            learning_rate=learning_rate,\n            num_lr_decays=num_lr_decays,\n            early_stop_patience_steps=early_stop_patience_steps,\n            val_check_steps=val_check_steps,\n            batch_size=batch_size,\n            valid_batch_size=valid_batch_size,\n            windows_batch_size=windows_batch_size,\n            inference_windows_batch_size=inference_windows_batch_size,\n            start_padding_enabled=start_padding_enabled,\n            step_size=step_size,\n            scaler_type=scaler_type,\n            random_seed=random_seed,\n            drop_last_loader=drop_last_loader,\n            alias=alias,\n            optimizer=optimizer,\n            optimizer_kwargs=optimizer_kwargs,\n            lr_scheduler=lr_scheduler,\n            lr_scheduler_kwargs=lr_scheduler_kwargs,\n            dataloader_kwargs=dataloader_kwargs,\n            **trainer_kwargs\n        )   \n        self.h = h\n\n        if self.hist_exog_size > 0 or self.futr_exog_size > 0:\n            self.hist_exog_projection = MLPResidual(input_dim = self.hist_exog_size,\n                                                    hidden_size=hidden_size,\n                                                        output_dim=temporal_width,\n                                                        dropout=dropout,\n                                                        layernorm=layernorm)  \n        if self.futr_exog_size > 0:\n            self.futr_exog_projection = MLPResidual(input_dim = self.futr_exog_size,\n                                                    hidden_size = hidden_size,\n                                                    output_dim=temporal_width,\n                                                    dropout=dropout,\n                                                    layernorm=layernorm)\n\n        dense_encoder_input_size = input_size + \\\n                                    input_size * (self.hist_exog_size > 0) * temporal_width + \\\n                                    (input_size + h) * (self.futr_exog_size > 0) * temporal_width + \\\n                                    (self.stat_exog_size > 0) * self.stat_exog_size\n\n        dense_encoder_layers = [MLPResidual(input_dim=dense_encoder_input_size if i == 0 else hidden_size,\n                                            hidden_size=hidden_size,\n                                          output_dim=hidden_size,\n                                          dropout=dropout,\n                                          layernorm=layernorm) for i in range(num_encoder_layers)]\n        self.dense_encoder = nn.Sequential(*dense_encoder_layers)\n\n        decoder_output_size = decoder_output_dim * h\n        dense_decoder_layers = [MLPResidual(input_dim=hidden_size,\n                                            hidden_size=hidden_size,\n                                          output_dim=decoder_output_size if i == num_decoder_layers - 1 else hidden_size,\n                                          dropout=dropout,\n                                          layernorm=layernorm) for i in range(num_decoder_layers)]\n        self.dense_decoder = nn.Sequential(*dense_decoder_layers)\n\n        self.temporal_decoder = MLPResidual(input_dim = decoder_output_dim + (self.futr_exog_size > 0) * temporal_width,\n                                            hidden_size = temporal_decoder_dim,\n                                          output_dim=self.loss.outputsize_multiplier)\n```\n\n----------------------------------------\n\nTITLE: LSTM Model Architecture Implementation - PyTorch\nDESCRIPTION: Core LSTM model implementation with encoder and optional MLP decoder. Handles input concatenation of target variable with historical, static and future exogenous variables. Supports both recurrent and non-recurrent prediction modes.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.lstm.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# LSTM input size (1 for target variable y)\ninput_encoder = 1 + self.hist_exog_size + self.stat_exog_size + self.futr_exog_size\n\n# Instantiate model\nself.rnn_state = None\nself.maintain_state = False\nself.hist_encoder = nn.LSTM(input_size=input_encoder,\n                            hidden_size=self.encoder_hidden_size,\n                            num_layers=self.encoder_n_layers,\n                            bias=self.encoder_bias,\n                            dropout=self.encoder_dropout,\n                            batch_first=True,\n                            proj_size=self.loss.outputsize_multiplier if self.RECURRENT else 0)\n\n# Decoder MLP\nif not self.RECURRENT:\n    self.mlp_decoder = MLP(in_features=self.encoder_hidden_size + self.futr_exog_size,\n                        out_features=self.loss.outputsize_multiplier,\n                        hidden_size=self.decoder_hidden_size,\n                        num_layers=self.decoder_layers,\n                        activation='ReLU',\n                        dropout=0.0)\n```\n\n----------------------------------------\n\nTITLE: Testing AutoNLinear Configuration and Usage\nDESCRIPTION: Example demonstrating how to configure, fit, and predict with the AutoNLinear model using both Ray and Optuna backends. The code uses a minimal configuration for quick testing.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_51\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n# Use your own config or AutoNLinear.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12)\nmodel = AutoNLinear(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n# Optuna\nmodel = AutoNLinear(h=12, config=None, backend='optuna')\n```\n\n----------------------------------------\n\nTITLE: Implementing Informer Transformer Model Architecture in PyTorch\nDESCRIPTION: Implementation of the Informer transformer model architecture with encoder and decoder components. The code shows the initialization of embedding layers, encoder with ProbAttention layers, and decoder structure that processes time series data.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.informer.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Encoder\nself.encoder = TransEncoder(\n    [\n        TransEncoderLayer(\n            AttentionLayer(\n                ProbAttention(False, factor,\n                              attention_dropout=dropout,\n                              output_attention=self.output_attention),\n                hidden_size, n_head),\n            hidden_size,\n            conv_hidden_size,\n            dropout=dropout,\n            activation=activation\n        ) for l in range(encoder_layers)\n    ],\n    [\n        ConvLayer(\n            hidden_size\n        ) for l in range(encoder_layers - 1)\n    ] if distil else None,\n    norm_layer=torch.nn.LayerNorm(hidden_size)\n)\n# Decoder\nself.decoder = TransDecoder(\n    [\n        TransDecoderLayer(\n            AttentionLayer(\n                ProbAttention(True, factor, attention_dropout=dropout, output_attention=False),\n                hidden_size, n_head),\n            AttentionLayer(\n                ProbAttention(False, factor, attention_dropout=dropout, output_attention=False),\n                hidden_size, n_head),\n            hidden_size,\n            conv_hidden_size,\n            dropout=dropout,\n            activation=activation,\n        )\n        for l in range(decoder_layers)\n    ],\n    norm_layer=torch.nn.LayerNorm(hidden_size),\n    projection=nn.Linear(hidden_size, self.c_out, bias=True)\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing KANLinear Layer for Kolmogorov-Arnold Networks in PyTorch\nDESCRIPTION: This class implements a custom linear layer for Kolmogorov-Arnold Networks. It includes methods for computing B-spline bases, curve coefficients, and forward pass calculations.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.kan.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass KANLinear(torch.nn.Module):\n    \"\"\"\n    KANLinear \n    \"\"\"\n    def __init__(\n        self,\n        in_features,\n        out_features,\n        grid_size=5,\n        spline_order=3,\n        scale_noise=0.1,\n        scale_base=1.0,\n        scale_spline=1.0,\n        enable_standalone_scale_spline=True,\n        base_activation=torch.nn.SiLU,\n        grid_eps=0.02,\n        grid_range=[-1, 1],\n    ):\n        super(KANLinear, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.grid_size = grid_size\n        self.spline_order = spline_order\n\n        h = (grid_range[1] - grid_range[0]) / grid_size\n        grid = (\n            (\n                torch.arange(-spline_order, grid_size + spline_order + 1) * h\n                + grid_range[0]\n            )\n            .expand(in_features, -1)\n            .contiguous()\n        )\n        self.register_buffer(\"grid\", grid)\n\n        self.base_weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n        self.spline_weight = torch.nn.Parameter(\n            torch.Tensor(out_features, in_features, grid_size + spline_order)\n        )\n        if enable_standalone_scale_spline:\n            self.spline_scaler = torch.nn.Parameter(\n                torch.Tensor(out_features, in_features)\n            )\n\n        self.scale_noise = scale_noise\n        self.scale_base = scale_base\n        self.scale_spline = scale_spline\n        self.enable_standalone_scale_spline = enable_standalone_scale_spline\n        self.base_activation = base_activation()\n        self.grid_eps = grid_eps\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        torch.nn.init.kaiming_uniform_(self.base_weight, a=math.sqrt(5) * self.scale_base)\n        with torch.no_grad():\n            noise = (\n                (\n                    torch.rand(self.grid_size + 1, self.in_features, self.out_features)\n                    - 1 / 2\n                )\n                * self.scale_noise\n                / self.grid_size\n            )\n            self.spline_weight.data.copy_(\n                (self.scale_spline if not self.enable_standalone_scale_spline else 1.0)\n                * self.curve2coeff(\n                    self.grid.T[self.spline_order : -self.spline_order],\n                    noise,\n                )\n            )\n            if self.enable_standalone_scale_spline:\n                # torch.nn.init.constant_(self.spline_scaler, self.scale_spline)\n                torch.nn.init.kaiming_uniform_(self.spline_scaler, a=math.sqrt(5) * self.scale_spline)\n\n    def b_splines(self, x: torch.Tensor):\n        \"\"\"\n        Compute the B-spline bases for the given input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: B-spline bases tensor of shape (batch_size, in_features, grid_size + spline_order).\n        \"\"\"\n        assert x.dim() == 2 and x.size(1) == self.in_features\n\n        grid: torch.Tensor = (\n            self.grid\n        )  # (in_features, grid_size + 2 * spline_order + 1)\n        x = x.unsqueeze(-1)\n        bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).to(x.dtype)\n        for k in range(1, self.spline_order + 1):\n            bases = (\n                (x - grid[:, : -(k + 1)])\n                / (grid[:, k:-1] - grid[:, : -(k + 1)])\n                * bases[:, :, :-1]\n            ) + (\n                (grid[:, k + 1 :] - x)\n                / (grid[:, k + 1 :] - grid[:, 1:(-k)])\n                * bases[:, :, 1:]\n            )\n\n        assert bases.size() == (\n            x.size(0),\n            self.in_features,\n            self.grid_size + self.spline_order,\n        )\n        return bases.contiguous()\n\n    def curve2coeff(self, x: torch.Tensor, y: torch.Tensor):\n        \"\"\"\n        Compute the coefficients of the curve that interpolates the given points.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n            y (torch.Tensor): Output tensor of shape (batch_size, in_features, out_features).\n\n        Returns:\n            torch.Tensor: Coefficients tensor of shape (out_features, in_features, grid_size + spline_order).\n        \"\"\"\n        assert x.dim() == 2 and x.size(1) == self.in_features\n        assert y.size() == (x.size(0), self.in_features, self.out_features)\n\n        A = self.b_splines(x).transpose(\n            0, 1\n        )  # (in_features, batch_size, grid_size + spline_order)\n        B = y.transpose(0, 1)  # (in_features, batch_size, out_features)\n        solution = torch.linalg.lstsq(\n            A, B\n        ).solution  # (in_features, grid_size + spline_order, out_features)\n        result = solution.permute(\n            2, 0, 1\n        )  # (out_features, in_features, grid_size + spline_order)\n\n        assert result.size() == (\n            self.out_features,\n            self.in_features,\n            self.grid_size + self.spline_order,\n        )\n        return result.contiguous()\n\n    @property\n    def scaled_spline_weight(self):\n        return self.spline_weight * (\n            self.spline_scaler.unsqueeze(-1)\n            if self.enable_standalone_scale_spline\n            else 1.0\n        )\n\n    def forward(self, x: torch.Tensor):\n        assert x.dim() == 2 and x.size(1) == self.in_features\n\n        base_output = F.linear(self.base_activation(x), self.base_weight)\n        spline_output = F.linear(\n            self.b_splines(x).view(x.size(0), -1),\n            self.scaled_spline_weight.view(self.out_features, -1),\n        )\n        return base_output + spline_output\n\n    @torch.no_grad()\n    def update_grid(self, x: torch.Tensor, margin=0.01):\n        assert x.dim() == 2 and x.size(1) == self.in_features\n        batch = x.size(0)\n\n        splines = self.b_splines(x)  # (batch, in, coeff)\n        splines = splines.permute(1, 0, 2)  # (in, batch, coeff)\n        orig_coeff = self.scaled_spline_weight  # (out, in, coeff)\n        orig_coeff = orig_coeff.permute(1, 2, 0)  # (in, coeff, out)\n        unreduced_spline_output = torch.bmm(splines, orig_coeff)  # (in, batch, out)\n        unreduced_spline_output = unreduced_spline_output.permute(\n            1, 0, 2\n        )  # (batch, in, out)\n\n        # sort each channel individually to collect data distribution\n        x_sorted = torch.sort(x, dim=0)[0]\n        grid_adaptive = x_sorted[\n            torch.linspace(\n                0, batch - 1, self.grid_size + 1, dtype=torch.int64, device=x.device\n            )\n        ]\n\n        uniform_step = (x_sorted[-1] - x_sorted[0] + 2 * margin) / self.grid_size\n        grid_uniform = (\n            torch.arange(\n                self.grid_size + 1, dtype=torch.float32, device=x.device\n            ).unsqueeze(1)\n            * uniform_step\n            + x_sorted[0]\n            - margin\n        )\n\n        grid = self.grid_eps * grid_uniform + (1 - self.grid_eps) * grid_adaptive\n        grid = torch.concatenate(\n            [\n                grid[:1]\n                - uniform_step\n                * torch.arange(self.spline_order, 0, -1, device=x.device).unsqueeze(1),\n                grid,\n                grid[-1:]\n                + uniform_step\n                * torch.arange(1, self.spline_order + 1, device=x.device).unsqueeze(1),\n            ],\n            dim=0,\n        )\n\n        self.grid.copy_(grid.T)\n        self.spline_weight.data.copy_(self.curve2coeff(x, unreduced_spline_output))\n\n    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n        l1_fake = self.spline_weight.abs().mean(-1)\n        regularization_loss_activation = l1_fake.sum()\n        p = l1_fake / regularization_loss_activation\n        regularization_loss_entropy = -torch.sum(p * p.log())\n        return (\n            regularize_activation * regularization_loss_activation\n            + regularize_entropy * regularization_loss_entropy\n        )\n```\n\n----------------------------------------\n\nTITLE: MLPMultivariate Usage Example with Air Passengers Dataset\nDESCRIPTION: Demonstrates how to use the MLPMultivariate model for time series forecasting using the Air Passengers dataset. Shows model initialization, training, prediction and visualization of results.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.mlpmultivariate.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import MLPMultivariate\nfrom neuralforecast.losses.pytorch import MAE\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = MLPMultivariate(h=12, \n            input_size=24,\n            n_series=2,\n            stat_exog_list=['airline1'],\n            futr_exog_list=['trend'],            \n            loss = MAE(),\n            scaler_type='robust',\n            learning_rate=1e-3,\n            stat_exog_list=['airline1'],\n            max_steps=200,\n            val_check_steps=10,\n            early_stop_patience_steps=2)\n\nfcst = NeuralForecast(\n    models=[model],\n    freq='ME'\n)\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\n# Plot predictions\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['MLPMultivariate'], c='blue', label='median')\nplt.grid()\nplt.legend()\nplt.plot()\n```\n\n----------------------------------------\n\nTITLE: Defining NHITS Block Class in Python\nDESCRIPTION: This class implements the core NHITS block, which includes MLP layers, pooling, and basis function application. It processes input data and produces backcast and forecast outputs.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nhits.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nACTIVATIONS = ['ReLU',\n               'Softplus',\n               'Tanh',\n               'SELU',\n               'LeakyReLU',\n               'PReLU',\n               'Sigmoid']\n\nPOOLING = ['MaxPool1d',\n           'AvgPool1d']\n\nclass NHITSBlock(nn.Module):\n    \"\"\"\n    NHITS block which takes a basis function as an argument.\n    \"\"\"\n    def __init__(self, \n                 input_size: int,\n                 h: int,\n                 n_theta: int,\n                 mlp_units: list,\n                 basis: nn.Module,\n                 futr_input_size: int,\n                 hist_input_size: int,\n                 stat_input_size: int,\n                 n_pool_kernel_size: int,\n                 pooling_mode: str,\n                 dropout_prob: float,\n                 activation: str):\n        super().__init__()\n\n        pooled_hist_size = int(np.ceil(input_size/n_pool_kernel_size))\n        pooled_futr_size = int(np.ceil((input_size+h)/n_pool_kernel_size))\n\n        input_size = pooled_hist_size + \\\n                     hist_input_size * pooled_hist_size + \\\n                     futr_input_size * pooled_futr_size + stat_input_size\n\n        self.dropout_prob = dropout_prob\n        self.futr_input_size = futr_input_size\n        self.hist_input_size = hist_input_size\n        self.stat_input_size = stat_input_size\n        \n        assert activation in ACTIVATIONS, f'{activation} is not in {ACTIVATIONS}'\n        assert pooling_mode in POOLING, f'{pooling_mode} is not in {POOLING}'\n\n        activ = getattr(nn, activation)()\n\n        self.pooling_layer = getattr(nn, pooling_mode)(kernel_size=n_pool_kernel_size,\n                                                       stride=n_pool_kernel_size, ceil_mode=True)\n\n        # Block MLPs\n        hidden_layers = [nn.Linear(in_features=input_size, \n                                   out_features=mlp_units[0][0])]\n        for layer in mlp_units:\n            hidden_layers.append(nn.Linear(in_features=layer[0], \n                                           out_features=layer[1]))\n            hidden_layers.append(activ)\n\n            if self.dropout_prob>0:\n                #raise NotImplementedError('dropout')\n                hidden_layers.append(nn.Dropout(p=self.dropout_prob))\n\n        output_layer = [nn.Linear(in_features=mlp_units[-1][1], out_features=n_theta)]\n        layers = hidden_layers + output_layer\n        self.layers = nn.Sequential(*layers)\n        self.basis = basis\n\n    def forward(self, insample_y: torch.Tensor, futr_exog: torch.Tensor,\n                hist_exog: torch.Tensor, stat_exog: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n\n        # Pooling\n        # Pool1d needs 3D input, (B,C,L), adding C dimension\n        insample_y = insample_y.unsqueeze(1)\n        insample_y = self.pooling_layer(insample_y)\n        insample_y = insample_y.squeeze(1)\n\n        # Flatten MLP inputs [B, L+H, C] -> [B, (L+H)*C]\n        # Contatenate [ Y_t, | X_{t-L},..., X_{t} | F_{t-L},..., F_{t+H} | S ]\n        batch_size = len(insample_y)\n        if self.hist_input_size > 0:\n            hist_exog = hist_exog.permute(0,2,1) # [B, L, C] -> [B, C, L]\n            hist_exog = self.pooling_layer(hist_exog)\n            hist_exog = hist_exog.permute(0,2,1) # [B, C, L] -> [B, L, C]\n            insample_y = torch.cat(( insample_y, hist_exog.reshape(batch_size,-1) ), dim=1)\n\n        if self.futr_input_size > 0:\n            futr_exog = futr_exog.permute(0,2,1) # [B, L, C] -> [B, C, L]\n            futr_exog = self.pooling_layer(futr_exog)\n            futr_exog = futr_exog.permute(0,2,1) # [B, C, L] -> [B, L, C]\n            insample_y = torch.cat(( insample_y, futr_exog.reshape(batch_size,-1) ), dim=1)\n\n        if self.stat_input_size > 0:\n            insample_y = torch.cat(( insample_y, stat_exog.reshape(batch_size,-1) ), dim=1)\n\n        # Compute local projection weights and projection\n        theta = self.layers(insample_y)\n        backcast, forecast = self.basis(theta)\n        return backcast, forecast\n```\n\n----------------------------------------\n\nTITLE: Implementing TimeXer Forward and Forecast Methods in Python\nDESCRIPTION: This snippet shows the forward and forecast methods of the TimeXer model. It processes input data, applies normalization if required, and generates predictions for time series forecasting.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timexer.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nenc_out = enc_out.permute(0, 1, 3, 2)\n\ndec_out = self.head(enc_out)  # z: [bs x nvars x h * n_outputs]\ndec_out = dec_out.permute(0, 2, 1)\n\nif self.use_norm:\n    # De-Normalization from Non-stationary Transformer\n    dec_out = dec_out * (stdev[:, 0, :].unsqueeze(1).repeat(1, self.h * self.loss.outputsize_multiplier, 1))\n    dec_out = dec_out + (means[:, 0, :].unsqueeze(1).repeat(1, self.h * self.loss.outputsize_multiplier, 1))\n\nreturn dec_out\n\ndef forward(self, windows_batch):\n    insample_y = windows_batch['insample_y']\n    futr_exog = windows_batch['futr_exog']\n    \n    if self.futr_exog_size > 0:\n        x_mark_enc = futr_exog[:, :, :self.input_size, :]\n        B, V, T, D = x_mark_enc.shape\n        x_mark_enc = x_mark_enc.reshape(B, T, V*D)\n    else:\n        x_mark_enc = None\n\n    y_pred = self.forecast(insample_y, x_mark_enc)\n    y_pred = y_pred.reshape(insample_y.shape[0],\n                            self.h,\n                            -1)\n    return y_pred\n```\n\n----------------------------------------\n\nTITLE: TimeXer Model Usage Example with AirPassengers Dataset in Python\nDESCRIPTION: This example demonstrates how to use the TimeXer model for forecasting using the AirPassengers dataset. It includes data preparation, model initialization, training, prediction, and visualization of results.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timexer.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import TimeXer\nfrom neuralforecast.losses.pytorch import MSE\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic, augment_calendar_df\n\nAirPassengersPanel, calendar_cols = augment_calendar_df(df=AirPassengersPanel, freq='M')\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = TimeXer(h=12,\n                input_size=24,\n                n_series=2,\n                futr_exog_list=[\"trend\", \"month\"],\n                patch_len=12,\n                hidden_size=128,\n                n_heads=16,\n                e_layers=2,\n                d_ff=256,\n                factor=1,\n                dropout=0.1,\n                use_norm=True,\n                loss=MSE(),\n                valid_loss=MAE(),\n                early_stop_patience_steps=3,\n                batch_size=32)\n\nfcst = NeuralForecast(models=[model], freq='ME')\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\n# Plot predictions\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['TimeXer'], c='blue', label='Forecast')\nax.set_title('AirPassengers Forecast', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Year', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\n```\n\n----------------------------------------\n\nTITLE: Example Usage of AutoHINT and HINT Classes in Python\nDESCRIPTION: Demonstrates how to use the AutoHINT and HINT classes for model fitting and prediction. It includes examples of creating synthetic datasets, performing hyperparameter optimization with NHITS, and using HINT for reconciliation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_101\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n# Perform a simple hyperparameter optimization with \n# NHITS and then reconcile with HINT\nfrom neuralforecast.losses.pytorch import GMM, sCRPS\n\nbase_config = dict(max_steps=1, val_check_steps=1, input_size=8)\nbase_model = AutoNHITS(h=4, loss=GMM(n_components=2, quantiles=quantiles), \n                       config=base_config, num_samples=1, cpus=1)\nmodel = HINT(h=4, S=S_df.values,\n             model=base_model,  reconciliation='MinTraceOLS')\n\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=hint_dataset)\n\n# Perform a conjunct hyperparameter optimization with \n```\n\n----------------------------------------\n\nTITLE: RNN Class Definition and Implementation\nDESCRIPTION: Defines the RNN class that implements a multi-layer Elman RNN with MLP decoder. The class inherits from BaseModel and includes comprehensive configuration options for the encoder and decoder architectures.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.rnn.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass RNN(BaseModel):\n    \"\"\" RNN\n\n    Multi Layer Elman RNN (RNN), with MLP decoder.\n    The network has `tanh` or `relu` non-linearities, it is trained using \n    ADAM stochastic gradient descent. The network accepts static, historic \n    and future exogenous data.\n\n    **Parameters:**<br>\n    `h`: int, forecast horizon.<br>\n    `input_size`: int, maximum sequence length for truncated train backpropagation. Default -1 uses 3 * horizon <br>\n    `inference_input_size`: int, maximum sequence length for truncated inference. Default None uses input_size history.<br>\n    `encoder_n_layers`: int=2, number of layers for the RNN.<br>\n    `encoder_hidden_size`: int=200, units for the RNN's hidden state size.<br>\n    `encoder_activation`: str=`tanh`, type of RNN activation from `tanh` or `relu`.<br>\n    `encoder_bias`: bool=True, whether or not to use biases b_ih, b_hh within RNN units.<br>\n    `encoder_dropout`: float=0., dropout regularization applied to RNN outputs.<br>\n    `context_size`: deprecated.<br>\n    `decoder_hidden_size`: int=200, size of hidden layer for the MLP decoder.<br>\n    `decoder_layers`: int=2, number of layers for the MLP decoder.<br>\n    `futr_exog_list`: str list, future exogenous columns.<br>\n    `hist_exog_list`: str list, historic exogenous columns.<br>\n    `stat_exog_list`: str list, static exogenous columns.<br>\n    `exclude_insample_y`: bool=False, whether to exclude the target variable from the historic exogenous data.<br>\n    `recurrent`: bool=False, whether to produce forecasts recursively (True) or direct (False).<br>\n    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `max_steps`: int=1000, maximum number of training steps.<br>\n    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n    `batch_size`: int=32, number of differentseries in each batch.<br>\n    `valid_batch_size`: int=None, number of different series in each validation and test batch.<br>\n    `windows_batch_size`: int=128, number of windows to sample in each training batch, default uses all.<br>\n    `inference_windows_batch_size`: int=1024, number of windows to sample in each inference batch, -1 uses all.<br>\n    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n    `step_size`: int=1, step size between each window of temporal data.<br>    \n    `scaler_type`: str='robust', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n    `random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n    `alias`: str, optional,  Custom name of the model.<br>\n    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>    \n    `dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n\n    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n    \"\"\"\n    # Class attributes\n    EXOGENOUS_FUTR = True\n    EXOGENOUS_HIST = True\n    EXOGENOUS_STAT = True\n    MULTIVARIATE = False    # If the model produces multivariate forecasts (True) or univariate (False)\n    RECURRENT = True        # If the model produces forecasts recursively (True) or direct (False)\n\n    def __init__(self,\n                 h: int,\n                 input_size: int = -1,\n                 inference_input_size: Optional[int] = None,\n                 encoder_n_layers: int = 2,\n                 encoder_hidden_size: int = 128,\n                 encoder_activation: str = 'tanh',\n                 encoder_bias: bool = True,\n                 encoder_dropout: float = 0.,\n                 context_size: Optional[int] = None,\n                 decoder_hidden_size: int = 128,\n                 decoder_layers: int = 2,\n                 futr_exog_list = None,\n                 hist_exog_list = None,\n                 stat_exog_list = None,\n                 exclude_insample_y = False,\n                 recurrent = False,\n                 loss = MAE(),\n                 valid_loss = None,\n                 max_steps: int = 1000,\n                 learning_rate: float = 1e-3,\n                 num_lr_decays: int = -1,\n                 early_stop_patience_steps: int =-1,\n                 val_check_steps: int = 100,\n                 batch_size=32,\n                 valid_batch_size: Optional[int] = None,\n                 windows_batch_size = 128,\n                 inference_windows_batch_size = 1024,\n                 start_padding_enabled = False,\n                 step_size: int = 1,\n                 scaler_type: str='robust',\n                 random_seed=1,\n                 drop_last_loader=False,\n                 alias: Optional[str] = None,\n                 optimizer=None,\n                 optimizer_kwargs=None,\n                 lr_scheduler = None,\n                 lr_scheduler_kwargs = None,  \n                 dataloader_kwargs = None,               \n                 **trainer_kwargs):\n        \n        self.RECURRENT = recurrent\n\n        super(RNN, self).__init__(\n            h=h,\n            input_size=input_size,\n            inference_input_size=inference_input_size,\n            futr_exog_list=futr_exog_list,\n            hist_exog_list=hist_exog_list,\n            stat_exog_list=stat_exog_list,\n            exclude_insample_y = exclude_insample_y,\n            loss=loss,\n            valid_loss=valid_loss,\n            max_steps=max_steps,\n            learning_rate=learning_rate,\n            num_lr_decays=num_lr_decays,\n            early_stop_patience_steps=early_stop_patience_steps,\n            val_check_steps=val_check_steps,\n            batch_size=batch_size,\n            valid_batch_size=valid_batch_size,\n            windows_batch_size=windows_batch_size,\n            inference_windows_batch_size=inference_windows_batch_size,\n            start_padding_enabled=start_padding_enabled,\n            step_size=step_size,\n            scaler_type=scaler_type,\n            random_seed=random_seed,\n            drop_last_loader=drop_last_loader,\n            alias=alias,\n            optimizer=optimizer,\n            optimizer_kwargs=optimizer_kwargs,\n            lr_scheduler=lr_scheduler,\n            lr_scheduler_kwargs=lr_scheduler_kwargs,\n            dataloader_kwargs=dataloader_kwargs,\n            **trainer_kwargs\n        )\n\n        # RNN\n        self.encoder_n_layers = encoder_n_layers\n        self.encoder_hidden_size = encoder_hidden_size\n        self.encoder_activation = encoder_activation\n        self.encoder_bias = encoder_bias\n        self.encoder_dropout = encoder_dropout\n```\n\n----------------------------------------\n\nTITLE: Example Usage of PatchTST for Time Series Forecasting\nDESCRIPTION: Comprehensive example demonstrating how to use PatchTST model with the NeuralForecast framework, including data preparation, model configuration, training, and visualization of forecasts with confidence intervals.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.patchtst.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import PatchTST\nfrom neuralforecast.losses.pytorch import DistributionLoss\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic, augment_calendar_df\n\nAirPassengersPanel, calendar_cols = augment_calendar_df(df=AirPassengersPanel, freq='M')\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = PatchTST(h=12,\n                 input_size=104,\n                 patch_len=24,\n                 stride=24,\n                 revin=False,\n                 hidden_size=16,\n                 n_heads=4,\n                 scaler_type='robust',\n                 loss=DistributionLoss(distribution='StudentT', level=[80, 90]),\n                 learning_rate=1e-3,\n                 max_steps=500,\n                 val_check_steps=50,\n                 early_stop_patience_steps=2)\n\nnf = NeuralForecast(\n    models=[model],\n    freq='ME'\n)\nnf.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nforecasts = nf.predict(futr_df=Y_test_df)\n\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nif model.loss.is_distribution_output:\n    plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n    plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n    plt.plot(plot_df['ds'], plot_df['PatchTST-median'], c='blue', label='median')\n    plt.fill_between(x=plot_df['ds'][-12:], \n                    y1=plot_df['PatchTST-lo-90'][-12:].values, \n                    y2=plot_df['PatchTST-hi-90'][-12:].values,\n                    alpha=0.4, label='level 90')\n    plt.grid()\n    plt.legend()\n    plt.plot()\nelse:\n    plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n    plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n    plt.plot(plot_df['ds'], plot_df['PatchTST'], c='blue', label='Forecast')\n    plt.legend()\n    plt.grid()\n```\n\n----------------------------------------\n\nTITLE: StemGNN Model Class Constructor and Core Architecture\nDESCRIPTION: Defines the core StemGNN model architecture including initialization of neural network layers, graph attention mechanisms, and model parameters. Inherits from BaseMultivariate class and sets up GRU, stock blocks, and fully connected layers.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.stemgnn.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass StemGNN(BaseMultivariate):\n    def __init__(self,\n                 start_padding_enabled = False,\n                 step_size: int = 1,\n                 scaler_type: str = 'robust',\n                 random_seed: int = 1,\n                 drop_last_loader = False,\n                 alias: Optional[str] = None,\n                 optimizer = None,\n                 optimizer_kwargs = None,\n                 lr_scheduler = None,\n                 lr_scheduler_kwargs = None,\n                 dataloader_kwargs = None,\n                 **trainer_kwargs):\n        # ... initialization code ...\n        self.fc = nn.Sequential(\n            nn.Linear(int(self.time_step), int(self.time_step)),\n            nn.LeakyReLU(),\n            nn.Linear(int(self.time_step), self.horizon * self.loss.outputsize_multiplier),\n        )\n```\n\n----------------------------------------\n\nTITLE: Autoformer Usage Example with AirPassengers Dataset\nDESCRIPTION: This code snippet demonstrates how to use the Autoformer model with the AirPassengers dataset. It includes data preparation, model initialization, training, prediction, and visualization of results using matplotlib.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.autoformer.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n#| eval: false\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import Autoformer\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic, augment_calendar_df\n\nAirPassengersPanel, calendar_cols = augment_calendar_df(df=AirPassengersPanel, freq='M')\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = Autoformer(h=12,\n                 input_size=24,\n                 hidden_size = 16,\n                 conv_hidden_size = 32,\n                 n_head=2,\n                 loss=MAE(),\n                 futr_exog_list=calendar_cols,\n                 scaler_type='robust',\n                 learning_rate=1e-3,\n                 max_steps=300,\n                 val_check_steps=50,\n                 early_stop_patience_steps=2)\n\nnf = NeuralForecast(\n    models=[model],\n    freq='ME'\n)\nnf.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nforecasts = nf.predict(futr_df=Y_test_df)\n\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nif model.loss.is_distribution_output:\n    plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n    plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n    plt.plot(plot_df['ds'], plot_df['Autoformer-median'], c='blue', label='median')\n    plt.fill_between(x=plot_df['ds'][-12:], \n                    y1=plot_df['Autoformer-lo-90'][-12:].values, \n                    y2=plot_df['Autoformer-hi-90'][-12:].values,\n                    alpha=0.4, label='level 90')\n    plt.grid()\n    plt.legend()\n    plt.plot()\nelse:\n    plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n    plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n    plt.plot(plot_df['ds'], plot_df['Autoformer'], c='blue', label='Forecast')\n    plt.legend()\n    plt.grid()\n```\n\n----------------------------------------\n\nTITLE: Implementing Invariant Scaler and Inverse Functions in Python\nDESCRIPTION: Provides functions for applying invariant scaling with arcsinh transformation and its inverse transformation using pre-computed statistics.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.scalers.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef invariant_scaler(x, x_median, x_mad):\n    return torch.arcsinh((x - x_median) / x_mad)\n\ndef inv_invariant_scaler(z, x_median, x_mad):\n    return torch.sinh(z) * x_mad + x_median\n```\n\n----------------------------------------\n\nTITLE: Creating Time Series Windows for Neural Forecasting\nDESCRIPTION: Creates sliding windows of time series data for training, validation, and prediction. Supports both univariate and multivariate forecasting with custom window sizes and optional sampling of windows.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef _create_windows(self, batch, step, w_idxs=None):\n    # Parse common data\n    window_size = self.input_size + self.h\n    temporal_cols = batch['temporal_cols']\n    temporal = batch['temporal']                \n\n    if step == 'train':\n        if self.val_size + self.test_size > 0:\n            cutoff = -self.val_size - self.test_size\n            temporal = temporal[:, :, :cutoff]\n\n        temporal = self.padder_train(temporal)\n        \n        if temporal.shape[-1] < window_size:\n            raise Exception('Time series is too short for training, consider setting a smaller input size or set start_padding_enabled=True')\n        \n        windows = temporal.unfold(dimension=-1, \n                                  size=window_size, \n                                  step=self.step_size)\n\n        if self.MULTIVARIATE:\n            # [n_series, C, Ws, L + h] -> [Ws, L + h, C, n_series]\n            windows = windows.permute(2, 3, 1, 0)\n        else:\n            # [n_series, C, Ws, L + h] -> [Ws * n_series, L + h, C, 1]\n            windows_per_serie = windows.shape[2]\n            windows = windows.permute(0, 2, 3, 1)\n            windows = windows.flatten(0, 1)\n            windows = windows.unsqueeze(-1)\n\n        # Sample and Available conditions\n        available_idx = temporal_cols.get_loc('available_mask')           \n        available_condition = windows[:, :self.input_size, available_idx]\n        available_condition = torch.sum(available_condition, axis=(1, -1)) # Sum over time & series dimension\n        final_condition = (available_condition > 0)\n        \n        if self.h > 0:\n            sample_condition = windows[:, self.input_size:, available_idx]\n            sample_condition = torch.sum(sample_condition, axis=(1, -1)) # Sum over time & series dimension\n            final_condition = (sample_condition > 0) & (available_condition > 0)\n        \n        windows = windows[final_condition]\n        \n        # Parse Static data to match windows\n        static = batch.get('static', None)\n        static_cols=batch.get('static_cols', None)\n\n        # Repeat static if univariate: [n_series, S] -> [Ws * n_series, S]\n        if static is not None and not self.MULTIVARIATE:\n            static = torch.repeat_interleave(static, \n                                repeats=windows_per_serie, dim=0)\n            static = static[final_condition]        \n\n        # Protection of empty windows\n        if final_condition.sum() == 0:\n            raise Exception('No windows available for training')\n\n        # Sample windows\n        if self.windows_batch_size is not None:\n            n_windows = windows.shape[0]\n            w_idxs = np.random.choice(n_windows, \n                                      size=self.windows_batch_size,\n                                      replace=(n_windows < self.windows_batch_size))\n            windows = windows[w_idxs]\n            \n            if static is not None and not self.MULTIVARIATE:\n                static = static[w_idxs]\n\n        windows_batch = dict(temporal=windows,\n                             temporal_cols=temporal_cols,\n                             static=static,\n                             static_cols=static_cols)\n        return windows_batch\n\n    elif step in ['predict', 'val']:\n\n        if step == 'predict':\n            initial_input = temporal.shape[-1] - self.test_size\n            if initial_input <= self.input_size: # There is not enough data to predict first timestamp\n                temporal = F.pad(temporal, pad=(self.input_size-initial_input, 0), mode=\"constant\", value=0.0)\n            predict_step_size = self.predict_step_size\n            cutoff = - self.input_size - self.test_size\n            temporal = temporal[:, :, cutoff:]\n\n        elif step == 'val':\n            predict_step_size = self.step_size\n            cutoff = -self.input_size - self.val_size - self.test_size\n            if self.test_size > 0:\n                temporal = batch['temporal'][:, :, cutoff:-self.test_size]\n            else:\n                temporal = batch['temporal'][:, :, cutoff:]\n            if temporal.shape[-1] < window_size:\n                initial_input = temporal.shape[-1] - self.val_size\n                temporal = F.pad(temporal, pad=(self.input_size-initial_input, 0), mode=\"constant\", value=0.0)\n\n        if (step=='predict') and (self.test_size==0) and (len(self.futr_exog_list)==0):\n            temporal = F.pad(temporal, pad=(0, self.h), mode=\"constant\", value=0.0)\n\n        windows = temporal.unfold(dimension=-1,\n                                  size=window_size,\n                                  step=predict_step_size)\n\n        static = batch.get('static', None)\n        static_cols=batch.get('static_cols', None)\n\n        if self.MULTIVARIATE:\n            # [n_series, C, Ws, L + h] -> [Ws, L + h, C, n_series]\n            windows = windows.permute(2, 3, 1, 0)\n        else:\n            # [n_series, C, Ws, L + h] -> [Ws * n_series, L + h, C, 1]\n            windows_per_serie = windows.shape[2]\n            windows = windows.permute(0, 2, 3, 1)\n            windows = windows.flatten(0, 1)\n            windows = windows.unsqueeze(-1)\n            if static is not None:\n                static = torch.repeat_interleave(static, \n                                repeats=windows_per_serie, dim=0)\n```\n\n----------------------------------------\n\nTITLE: Processing Time Series Batch Data with Exogenous Variables in PyTorch\nDESCRIPTION: Processes batched time series data with multiple types of exogenous variables (future, historic, static) and generates forecasts using a DeepNPTS network. Handles tensor reshaping and concatenation for different input features.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.deepnpts.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfutr_exog     = windows_batch['futr_exog']                      #   [B, L + h, F]\nstat_exog     = windows_batch['stat_exog']                      #   [B, S]\n\nbatch_size, seq_len = x.shape[:2]                               #   B = batch_size, L = seq_len\ninsample_y = windows_batch['insample_y'] \n\n# Concatenate x_t with future exogenous of input\nif self.futr_exog_size > 0:      \n    x = torch.cat((x, futr_exog[:, :seq_len]), dim=2)           #   [B, L, 1] + [B, L, F] -> [B, L, 1 + F]            \n\n# Concatenate x_t with historic exogenous\nif self.hist_exog_size > 0:      \n    x = torch.cat((x, hist_exog), dim=2)                        #   [B, L, 1 + F + X] + [B, L, X] -> [B, L, 1 + F + X]            \n\nx = x.reshape(batch_size, -1)                                   #   [B, L, 1 + F + X] -> [B, L * (1 + F + X)]\n\n# Concatenate x with static exogenous\nif self.stat_exog_size > 0:\n    x = torch.cat((x, stat_exog), dim=1)                        #   [B, L * (1 + F + X)] + [B, S] -> [B, L * (1 + F + X) + S]\n\n# Concatenate x_t with future exogenous of horizon\nif self.futr_exog_size > 0:\n    futr_exog = futr_exog[:, seq_len:]                          #   [B, L + h, F] -> [B, h, F]\n    futr_exog = futr_exog.reshape(batch_size, -1)               #   [B, L + h, F] -> [B, h * F]\n    x = torch.cat((x, futr_exog), dim=1)                        #   [B, L * (1 + F + X) + S] + [B, h * F] -> [B, L * (1 + F + X) + S + h * F]            \n\n# Run through DeepNPTSNetwork\nweights = self.deepnptsnetwork(x)                               #   [B, L * (1 + F + X) + S + h * F]  -> [B, L * h]\n\n# Apply softmax for weighted input predictions\nweights = weights.reshape(batch_size, seq_len, -1)              #   [B, L * h] -> [B, L, h]\nx = F.softmax(weights, dim=1) * insample_y                      #   [B, L, h] * [B, L, 1] = [B, L, h]\nforecast = torch.sum(x, dim=1).unsqueeze(-1)                      #   [B, L, h] -> [B, h, 1]\n\nreturn forecast\n```\n\n----------------------------------------\n\nTITLE: Implementing Autoformer Decoder Class\nDESCRIPTION: PyTorch module implementation of an Autoformer decoder that processes input through multiple layers with optional normalization and projection. Handles masked attention and trend components.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.autoformer.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nclass Decoder(nn.Module):\n    \"\"\"\n    Autoformer decoder\n    \"\"\"\n    def __init__(self, layers, norm_layer=None, projection=None):\n        super(Decoder, self).__init__()\n        self.layers = nn.ModuleList(layers)\n        self.norm = norm_layer\n        self.projection = projection\n\n    def forward(self, x, cross, x_mask=None, cross_mask=None, trend=None):\n        for layer in self.layers:\n            x, residual_trend = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask)\n            trend = trend + residual_trend\n\n        if self.norm is not None:\n            x = self.norm(x)\n\n        if self.projection is not None:\n            x = self.projection(x)\n        return x, trend\n```\n\n----------------------------------------\n\nTITLE: Implementing Flatten Head Layer\nDESCRIPTION: Head layer implementation that flattens and processes the encoded features. Supports both individual and shared processing across variables.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.patchtst.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass Flatten_Head(nn.Module):\n    def __init__(self, individual, n_vars, nf, h, c_out, head_dropout=0):\n        super().__init__()\n        \n        self.individual = individual\n        self.n_vars = n_vars\n        self.c_out = c_out\n        \n        if self.individual:\n            self.linears = nn.ModuleList()\n            self.dropouts = nn.ModuleList()\n            self.flattens = nn.ModuleList()\n            for i in range(self.n_vars):\n                self.flattens.append(nn.Flatten(start_dim=-2))\n                self.linears.append(nn.Linear(nf, h*c_out))\n                self.dropouts.append(nn.Dropout(head_dropout))\n        else:\n            self.flatten = nn.Flatten(start_dim=-2)\n            self.linear = nn.Linear(nf, h*c_out)\n            self.dropout = nn.Dropout(head_dropout)\n```\n\n----------------------------------------\n\nTITLE: Implementing PyTorch Multivariate Reversible Instance Normalization\nDESCRIPTION: Defines the `RevINMultivariate` PyTorch `nn.Module`, a Reversible Instance Normalization variant for multivariate time series. It normalizes data by subtracting the batch mean and dividing by the batch standard deviation calculated across the time dimension (axis 1). It can optionally apply learnable affine transformations. The `forward` method takes a `mode` argument ('norm' or 'denorm') to control whether to normalize or denormalize the input tensor `x`. Depends on `torch` and `torch.nn`.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.modules.ipynb#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass RevINMultivariate(nn.Module):\n    \"\"\" \n    ReversibleInstanceNorm1d for Multivariate models\n    \"\"\"  \n    def __init__(self, num_features: int, eps=1e-5, affine=False, subtract_last=False, non_norm=False):\n        super().__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.affine = affine\n        if self.affine:\n            self._init_params()\n\n    def forward(self, x, mode: str):\n        if mode == 'norm':\n            x = self._normalize(x)\n        elif mode == 'denorm':\n            x = self._denormalize(x)\n        else:\n            raise NotImplementedError\n        return x\n\n    def _init_params(self):\n        # initialize RevIN params: (C,)\n        self.affine_weight = nn.Parameter(torch.ones((1, 1, self.num_features)))\n        self.affine_bias = nn.Parameter(torch.zeros((1, 1, self.num_features)))\n\n    def _normalize(self, x):\n        # Batch statistics\n        self.batch_mean = torch.mean(x, axis=1, keepdim=True).detach()\n        self.batch_std = torch.sqrt(torch.var(x, axis=1, keepdim=True, unbiased=False) + self.eps).detach()\n    \n        # Instance normalization\n        x = x - self.batch_mean\n        x = x / self.batch_std\n        \n        if self.affine:\n            x = x * self.affine_weight\n            x = x + self.affine_bias\n\n        return x\n\n    def _denormalize(self, x):\n        # Reverse the normalization\n        if self.affine:\n            x = x - self.affine_bias\n            x = x / self.affine_weight       \n        \n        x = x * self.batch_std\n        x = x + self.batch_mean       \n\n        return x\n```\n\n----------------------------------------\n\nTITLE: Implementing ProbMask and ProbAttention for Informer Model in Python\nDESCRIPTION: Defines ProbMask and ProbAttention classes for the Informer model. ProbAttention implements the ProbSparse self-attention mechanism, which is a key feature of the Informer architecture for efficient long sequence processing.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.informer.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass ProbMask():\n    \"\"\"\n    ProbMask\n    \"\"\"    \n    def __init__(self, B, H, L, index, scores, device=\"cpu\"):\n        _mask = torch.ones(L, scores.shape[-1], dtype=torch.bool, device=device).triu(1)\n        _mask_ex = _mask[None, None, :].expand(B, H, L, scores.shape[-1])\n        indicator = _mask_ex[torch.arange(B)[:, None, None],\n                    torch.arange(H)[None, :, None],\n                    index, :].to(device)\n        self._mask = indicator.view(scores.shape).to(device)\n\n    @property\n    def mask(self):\n        return self._mask\n\n\nclass ProbAttention(nn.Module):\n    \"\"\"\n    ProbAttention\n    \"\"\"      \n    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n        super(ProbAttention, self).__init__()\n        self.factor = factor\n        self.scale = scale\n        self.mask_flag = mask_flag\n        self.output_attention = output_attention\n        self.dropout = nn.Dropout(attention_dropout)\n\n    def _prob_QK(self, Q, K, sample_k, n_top):  # n_top: c*ln(L_q)\n        # Q [B, H, L, D]\n        B, H, L_K, E = K.shape\n        _, _, L_Q, _ = Q.shape\n\n        # calculate the sampled Q_K\n        K_expand = K.unsqueeze(-3).expand(B, H, L_Q, L_K, E)\n\n        index_sample = torch.randint(L_K, (L_Q, sample_k))  # real U = U_part(factor*ln(L_k))*L_q\n        K_sample = K_expand[:, :, torch.arange(L_Q).unsqueeze(1), index_sample, :]\n        Q_K_sample = torch.matmul(Q.unsqueeze(-2), K_sample.transpose(-2, -1)).squeeze()\n\n        # find the Top_k query with sparisty measurement\n        M = Q_K_sample.max(-1)[0] - torch.div(Q_K_sample.sum(-1), L_K)\n        M_top = M.topk(n_top, sorted=False)[1]\n\n        # use the reduced Q to calculate Q_K\n        Q_reduce = Q[torch.arange(B)[:, None, None],\n                   torch.arange(H)[None, :, None],\n                   M_top, :]  # factor*ln(L_q)\n        Q_K = torch.matmul(Q_reduce, K.transpose(-2, -1))  # factor*ln(L_q)*L_k\n\n        return Q_K, M_top\n\n    def _get_initial_context(self, V, L_Q):\n        B, H, L_V, D = V.shape\n        if not self.mask_flag:\n            # V_sum = V.sum(dim=-2)\n            V_sum = V.mean(dim=-2)\n            contex = V_sum.unsqueeze(-2).expand(B, H, L_Q, V_sum.shape[-1]).clone()\n        else:  # use mask\n            assert (L_Q == L_V)  # requires that L_Q == L_V, i.e. for self-attention only\n            contex = V.cumsum(dim=-2)\n        return contex\n\n    def _update_context(self, context_in, V, scores, index, L_Q, attn_mask):\n        B, H, L_V, D = V.shape\n\n        if self.mask_flag:\n            attn_mask = ProbMask(B, H, L_Q, index, scores, device=V.device)\n            scores.masked_fill_(attn_mask.mask, -np.inf)\n\n        attn = torch.softmax(scores, dim=-1)  # nn.Softmax(dim=-1)(scores)\n\n        context_in[torch.arange(B)[:, None, None],\n        torch.arange(H)[None, :, None],\n        index, :] = torch.matmul(attn, V).type_as(context_in)\n        if self.output_attention:\n            attns = (torch.ones([B, H, L_V, L_V], device=attn.device) / L_V).type_as(attn)\n            attns[torch.arange(B)[:, None, None], torch.arange(H)[None, :, None], index, :] = attn\n            return (context_in, attns)\n        else:\n            return (context_in, None)\n\n    def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):\n        B, L_Q, H, D = queries.shape\n        _, L_K, _, _ = keys.shape\n\n        queries = queries.transpose(2, 1)\n        keys = keys.transpose(2, 1)\n        values = values.transpose(2, 1)\n\n        U_part = self.factor * np.ceil(np.log(L_K)).astype('int').item()  # c*ln(L_k)\n        u = self.factor * np.ceil(np.log(L_Q)).astype('int').item()  # c*ln(L_q)\n\n        U_part = U_part if U_part < L_K else L_K\n        u = u if u < L_Q else L_Q\n\n        scores_top, index = self._prob_QK(queries, keys, sample_k=U_part, n_top=u)\n\n        # add scale factor\n        scale = self.scale or 1. / math.sqrt(D)\n        if scale is not None:\n            scores_top = scores_top * scale\n        # get the context\n        context = self._get_initial_context(values, L_Q)\n        # update the context with selected top_k queries\n        context, attn = self._update_context(context, values, scores_top, index, L_Q, attn_mask)\n\n        return context.contiguous(), attn\n```\n\n----------------------------------------\n\nTITLE: Implementing ConvLayer for Informer Model in Python\nDESCRIPTION: Defines a ConvLayer class that applies convolutional, normalization, activation, and pooling operations on input data. This is used as part of the Informer model's architecture.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.informer.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass ConvLayer(nn.Module):\n    \"\"\"\n    ConvLayer\n    \"\"\"\n    def __init__(self, c_in):\n        super(ConvLayer, self).__init__()\n        self.downConv = nn.Conv1d(in_channels=c_in,\n                                  out_channels=c_in,\n                                  kernel_size=3,\n                                  padding=2,\n                                  padding_mode='circular')\n        self.norm = nn.BatchNorm1d(c_in)\n        self.activation = nn.ELU()\n        self.maxPool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n\n    def forward(self, x):\n        x = self.downConv(x.permute(0, 2, 1))\n        x = self.norm(x)\n        x = self.activation(x)\n        x = self.maxPool(x)\n        x = x.transpose(1, 2)\n        return x\n```\n\n----------------------------------------\n\nTITLE: TimeSeriesLoader Implementation\nDESCRIPTION: Custom DataLoader implementation for time series data that handles batch collation for temporal and static features. Extends PyTorch's DataLoader with specialized collate function for time series data structures.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/tsdataset.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass TimeSeriesLoader(DataLoader):\n    def __init__(self, dataset, **kwargs):\n        if 'collate_fn' in kwargs:\n            kwargs.pop('collate_fn')\n        kwargs_ = {**kwargs, **dict(collate_fn=self._collate_fn)}\n        DataLoader.__init__(self, dataset=dataset, **kwargs_)\n    \n    def _collate_fn(self, batch):\n        elem = batch[0]\n        elem_type = type(elem)\n\n        if isinstance(elem, torch.Tensor):\n            out = None\n            if torch.utils.data.get_worker_info() is not None:\n                numel = sum(x.numel() for x in batch)\n                storage = elem.storage()._new_shared(numel, device=elem.device)\n                out = elem.new(storage).resize_(len(batch), *list(elem.size()))\n            return torch.stack(batch, 0, out=out)\n\n        elif isinstance(elem, Mapping):\n            if elem['static'] is None:\n                return dict(temporal=self.collate_fn([d['temporal'] for d in batch]),\n                            temporal_cols = elem['temporal_cols'],\n                            y_idx=elem['y_idx'])\n            \n            return dict(static=self.collate_fn([d['static'] for d in batch]),\n                        static_cols = elem['static_cols'],\n                        temporal=self.collate_fn([d['temporal'] for d in batch]),\n                        temporal_cols = elem['temporal_cols'],\n                        y_idx=elem['y_idx'])\n\n        raise TypeError(f'Unknown {elem_type}')\n```\n\n----------------------------------------\n\nTITLE: Implementing Autoformer Model in PyTorch\nDESCRIPTION: This code snippet defines the Autoformer class, including its architecture with encoder and decoder layers, and the forward method for processing input data. It uses various components like AutoCorrelationLayer, EncoderLayer, and DecoderLayer to build the model structure.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.autoformer.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass Autoformer(BaseModelWithCovariates):\n    def __init__(self,\n                 h, \n                 input_size,\n                 hidden_size=128,\n                 conv_hidden_size=128,\n                 n_head=4,\n                 encoder_layers=2,\n                 decoder_layers=1,\n                 factor=3,\n                 activation='gelu',\n                 MovingAvg_window=25,\n                 dropout=0.05,\n                 **kwargs):\n        # ... (constructor code)\n\n    def forward(self, windows_batch):\n        # Parse windows_batch\n        insample_y    = windows_batch['insample_y']\n        futr_exog     = windows_batch['futr_exog']\n\n        # Parse inputs\n        if self.futr_exog_size > 0:\n            x_mark_enc = futr_exog[:,:self.input_size,:]\n            x_mark_dec = futr_exog[:,-(self.label_len+self.h):,:]\n        else:\n            x_mark_enc = None\n            x_mark_dec = None\n\n        x_dec = torch.zeros(size=(len(insample_y),self.h,1), device=insample_y.device)\n        x_dec = torch.cat([insample_y[:,-self.label_len:,:], x_dec], dim=1)\n\n        # ... (rest of the forward method)\n\n        return forecast\n```\n\n----------------------------------------\n\nTITLE: Implementing Huberized Quantile Loss in PyTorch\nDESCRIPTION: A PyTorch implementation of Huberized Quantile Loss that combines quantile loss and Huber loss for robust regression. This loss function handles outliers while maintaining sensitivity to the quantile parameter, making it suitable for probabilistic forecasting.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_68\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass HuberQLoss(BasePointLoss):\n    \"\"\" Huberized Quantile Loss\n\n    The Huberized quantile loss is a modified version of the quantile loss function that\n    combines the advantages of the quantile loss and the Huber loss. It is commonly used\n    in regression tasks, especially when dealing with data that contains outliers or heavy tails.\n\n    The Huberized quantile loss between `y` and `y_hat` measure the Huber Loss in a non-symmetric way.\n    The loss pays more attention to under/over-estimation depending on the quantile parameter $q$; \n    and controls the trade-off between robustness and accuracy in the predictions with the parameter $delta$.\n\n    $$ \\mathrm{HuberQL}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{(q)}_{\\\\tau}) = \n    (1-q)\\, L_{\\delta}(y_{\\\\tau},\\; \\hat{y}^{(q)}_{\\\\tau}) \\mathbb{1}\\{ \\hat{y}^{(q)}_{\\\\tau} \\geq y_{\\\\tau} \\} + \n    q\\, L_{\\delta}(y_{\\\\tau},\\; \\hat{y}^{(q)}_{\\\\tau}) \\mathbb{1}\\{ \\hat{y}^{(q)}_{\\\\tau} < y_{\\\\tau} \\} $$\n\n    **Parameters:**<br>\n    `delta`: float=1.0, Specifies the threshold at which to change between delta-scaled L1 and L2 loss.<br>\n    `q`: float, between 0 and 1. The slope of the quantile loss, in the context of quantile regression, the q determines the conditional quantile level.<br>\n    `horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n\n    **References:**<br>\n    [Huber Peter, J (1964). \"Robust Estimation of a Location Parameter\". Annals of Statistics](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-35/issue-1/Robust-Estimation-of-a-Location-Parameter/10.1214/aoms/1177703732.full)<br>\n    [Roger Koenker and Gilbert Bassett, Jr., \"Regression Quantiles\".](https://www.jstor.org/stable/1913643)\n    \"\"\"\n    def __init__(self, q, delta: float=1., horizon_weight=None):\n        super(HuberQLoss, self).__init__(horizon_weight=horizon_weight,\n                                           outputsize_multiplier=1,\n                                           output_names=[f'_q{q}_d{delta}'])\n        self.q = q\n        self.delta = delta\n\n    def __call__(self,\n                 y: torch.Tensor,\n                 y_hat: torch.Tensor,\n                 y_insample: torch.Tensor,\n                 mask: Union[torch.Tensor, None] = None,\n                 ) -> torch.Tensor:\n        \"\"\"\n        **Parameters:**<br>\n        `y`: tensor, Actual values.<br>\n        `y_hat`: tensor, Predicted values.<br>\n        `mask`: tensor, Specifies datapoints to consider in loss.<br>\n\n        **Returns:**<br>\n        `huber_qloss`: tensor (single value).\n        \"\"\"\n        \n        error  = y_hat - y\n        zero_error = torch.zeros_like(error)\n        sq     = torch.maximum(-error, zero_error)\n        s1_q   = torch.maximum(error, zero_error)\n        losses = self.q * F.huber_loss(sq, zero_error, \n                                       reduction='none', delta=self.delta) + \\\n                 (1 - self.q) * F.huber_loss(s1_q, zero_error, \n                                        reduction='none', delta=self.delta)\n\n        weights = self._compute_weights(y=y, mask=mask)\n        return _weighted_mean(losses=losses, weights=weights)\n```\n\n----------------------------------------\n\nTITLE: Model Fitting Method for Neural Network Forecasting Model in Python\nDESCRIPTION: This method implements the fitting process for the neural network forecasting model. It uses a PyTorch Lightning Trainer to optimize the model's weights based on the provided dataset and initialization parameters. The method supports customization of validation and test sizes.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_34\n\nLANGUAGE: Python\nCODE:\n```\ndef fit(self, dataset, val_size=0, test_size=0, random_seed=None, distributed_config=None):\n        \"\"\" Fit.\n\n        The `fit` method, optimizes the neural network's weights using the\n        initialization parameters (`learning_rate`, `windows_batch_size`, ...)\n        and the `loss` function as defined during the initialization. \n        Within `fit` we use a PyTorch Lightning `Trainer` that\n        inherits the initialization's `self.trainer_kwargs`, to customize\n        its inputs, see [PL's trainer arguments](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).\n\n        The method is designed to be compatible with SKLearn-like classes\n        and in particular to be compatible with the StatsForecast library.\n\n        By default the `model` is not saving training checkpoints to protect \n        disk memory, to get them change `enable_checkpointing=True` in `__init__`.\n\n        **Parameters:**<br>\n        `dataset`: NeuralForecast's `TimeSeriesDataset`, see [documentation](https://nixtla.github.io/neuralforecast/tsdataset.html).<br>\n        `val_size`: int, validation size for temporal cross-validation.<br>\n        `random_seed`: int=None, random_seed for pytorch initializer and numpy generators, overwrites model.__init__'s.<br>\n        `test_size`: int, test size for temporal cross-validation.<br>\n        \"\"\"\n        return self._fit(\n            dataset=dataset,\n            batch_size=self.batch_size,\n            valid_batch_size=self.valid_batch_size,\n            val_size=val_size,\n            test_size=test_size,\n            random_seed=random_seed,\n            distributed_config=distributed_config,\n        )\n```\n\n----------------------------------------\n\nTITLE: SOFTS Model Class Implementation in Python\nDESCRIPTION: Core implementation of the SOFTS model class that inherits from BaseModel. Includes initialization parameters, network architecture setup, and forecasting methods. The model uses transformer-based architecture with Series-Core Fusion for time series prediction.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.softs.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass SOFTS(BaseModel):\n    def __init__(self,\n                 h,\n                 input_size,\n                 n_series,\n                 futr_exog_list = None,\n                 hist_exog_list = None,\n                 stat_exog_list = None,\n                 exclude_insample_y = False,\n                 hidden_size: int = 512,\n                 d_core: int = 512,\n                 e_layers: int = 2,\n                 d_ff: int = 2048,\n                 dropout: float = 0.1,\n                 use_norm: bool = True,\n                 loss = MAE(),\n                 valid_loss = None,\n                 max_steps: int = 1000,\n                 learning_rate: float = 1e-3,\n                 num_lr_decays: int = -1,\n                 early_stop_patience_steps: int =-1,\n                 val_check_steps: int = 100,\n                 batch_size: int = 32,\n                 valid_batch_size: Optional[int] = None,\n                 windows_batch_size = 32,\n                 inference_windows_batch_size = 32,\n                 start_padding_enabled = False,\n                 step_size: int = 1,\n                 scaler_type: str = 'identity',\n                 random_seed: int = 1,\n                 drop_last_loader: bool = False,\n                 alias: Optional[str] = None,\n                 optimizer = None,\n                 optimizer_kwargs = None,\n                 lr_scheduler = None,\n                 lr_scheduler_kwargs = None, \n                 dataloader_kwargs = None,           \n                 **trainer_kwargs):\n```\n\n----------------------------------------\n\nTITLE: Defining the TimeMixer PyTorch Model in Python\nDESCRIPTION: Defines the `TimeMixer` class, a PyTorch neural network model for time series forecasting. It initializes various layers including down-sampling, prediction, projection, residual, and regression layers based on configuration parameters like `channel_independence`, `down_sampling_layers`, `input_size`, `h`, `d_model`, and `c_out`. It includes methods for output projection (`out_projection`), input preprocessing (`pre_enc`), multi-scale input processing (`__multi_scale_process_inputs`), the core forecasting logic (`forecast`), future mixing (`future_multi_mixing`), and the main forward pass (`forward`) which integrates these components.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timemixer.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n                for i in range(self.down_sampling_layers + 1)\n            ]\n        )\n\n        self.predict_layers = torch.nn.ModuleList(\n            [\n                torch.nn.Linear(\n                    math.ceil(self.input_size // (self.down_sampling_window ** i)),\n                    self.h,\n                )\n                for i in range(self.down_sampling_layers + 1)\n            ]\n        )\n\n        if self.channel_independence == 1:\n            self.projection_layer = nn.Linear(\n                self.d_model, 1, bias=True)\n        else:\n            self.projection_layer = nn.Linear(\n                self.d_model, self.c_out, bias=True)\n\n            self.out_res_layers = torch.nn.ModuleList([\n                torch.nn.Linear(\n                    self.input_size // (self.down_sampling_window ** i),\n                    self.input_size // (self.down_sampling_window ** i),\n                )\n                for i in range(self.down_sampling_layers + 1)\n            ])\n\n            self.regression_layers = torch.nn.ModuleList(\n                [\n                    torch.nn.Linear(\n                        self.input_size // (self.down_sampling_window ** i),\n                        self.h,\n                    )\n                    for i in range(self.down_sampling_layers + 1)\n                ]\n            )\n        \n        if self.loss.outputsize_multiplier > 1:\n            self.distr_output = nn.Linear(self.n_series, self.n_series * self.loss.outputsize_multiplier)\n\n    def out_projection(self, dec_out, i, out_res):\n        dec_out = self.projection_layer(dec_out)\n        out_res = out_res.permute(0, 2, 1)\n        out_res = self.out_res_layers[i](out_res)\n        out_res = self.regression_layers[i](out_res).permute(0, 2, 1)\n        dec_out = dec_out + out_res\n        return dec_out\n    \n    def pre_enc(self, x_list):\n        if self.channel_independence == 1:\n            return (x_list, None)\n        else:\n            out1_list = []\n            out2_list = []\n            for x in x_list:\n                x_1, x_2 = self.preprocess(x)\n                out1_list.append(x_1)\n                out2_list.append(x_2)\n            return (out1_list, out2_list)\n        \n    def __multi_scale_process_inputs(self, x_enc, x_mark_enc):\n        if self.down_sampling_method == 'max':\n            down_pool = torch.nn.MaxPool1d(self.down_sampling_window, return_indices=False)\n        elif self.down_sampling_method == 'avg':\n            down_pool = torch.nn.AvgPool1d(self.down_sampling_window)\n        elif self.down_sampling_method == 'conv':\n            padding = 1\n            down_pool = nn.Conv1d(in_channels=self.enc_in, out_channels=self.enc_in,\n                                  kernel_size=3, padding=padding,\n                                  stride=self.down_sampling_window,\n                                  padding_mode='circular',\n                                  bias=False)\n        else:\n            return x_enc, x_mark_enc\n        # B,T,C -> B,C,T\n        x_enc = x_enc.permute(0, 2, 1)\n\n        x_enc_ori = x_enc\n        x_mark_enc_mark_ori = x_mark_enc\n\n        x_enc_sampling_list = []\n        x_mark_sampling_list = []\n        x_enc_sampling_list.append(x_enc.permute(0, 2, 1))\n        x_mark_sampling_list.append(x_mark_enc)\n\n        for i in range(self.down_sampling_layers):\n            x_enc_sampling = down_pool(x_enc_ori)\n\n            x_enc_sampling_list.append(x_enc_sampling.permute(0, 2, 1))\n            x_enc_ori = x_enc_sampling\n\n            if x_mark_enc_mark_ori is not None:\n                x_mark_sampling_list.append(x_mark_enc_mark_ori[:, ::self.down_sampling_window, :])\n                x_mark_enc_mark_ori = x_mark_enc_mark_ori[:, ::self.down_sampling_window, :]\n\n        x_enc = x_enc_sampling_list\n        if x_mark_enc_mark_ori is not None:\n            x_mark_enc = x_mark_sampling_list\n        else:\n            x_mark_enc = x_mark_enc\n\n        return x_enc, x_mark_enc\n    \n    def forecast(self, x_enc, x_mark_enc, x_mark_dec):\n\n        if self.use_future_temporal_feature:\n            if self.channel_independence == 1:\n                B, T, N = x_enc.size()\n                x_mark_dec = x_mark_dec.repeat(N, 1, 1)\n                self.x_mark_dec = self.enc_embedding(None, x_mark_dec)\n            else:\n                self.x_mark_dec = self.enc_embedding(None, x_mark_dec)\n\n        x_enc, x_mark_enc = self.__multi_scale_process_inputs(x_enc, x_mark_enc)\n\n        x_list = []\n        x_mark_list = []\n        if x_mark_enc is not None:\n            for i, x, x_mark in zip(range(len(x_enc)), x_enc, x_mark_enc):\n                B, T, N = x.size()\n                x = self.normalize_layers[i](x, 'norm')\n                if self.channel_independence == 1:\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                    x_mark = x_mark.repeat(N, 1, 1)\n                x_list.append(x)\n                x_mark_list.append(x_mark)\n        else:\n            for i, x in zip(range(len(x_enc)), x_enc, ):\n                B, T, N = x.size()\n                x = self.normalize_layers[i](x, 'norm')\n                if self.channel_independence == 1:\n                    x = x.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n                x_list.append(x)\n\n        # embedding\n        enc_out_list = []\n        x_list = self.pre_enc(x_list)\n        if x_mark_enc is not None:\n            for i, x, x_mark in zip(range(len(x_list[0])), x_list[0], x_mark_list):\n                enc_out = self.enc_embedding(x, x_mark)  # [B,T,C]\n                enc_out_list.append(enc_out)\n        else:\n            for i, x in zip(range(len(x_list[0])), x_list[0]):\n                enc_out = self.enc_embedding(x, None)  # [B,T,C]\n                enc_out_list.append(enc_out)\n\n        # Past Decomposable Mixing as encoder for past\n        for i in range(self.e_layers):\n            enc_out_list = self.pdm_blocks[i](enc_out_list)\n\n        # Future Multipredictor Mixing as decoder for future\n        dec_out_list = self.future_multi_mixing(B, enc_out_list, x_list)\n\n        dec_out = torch.stack(dec_out_list, dim=-1).sum(-1)\n        dec_out = self.normalize_layers[0](dec_out, 'denorm')\n        return dec_out\n    \n    def future_multi_mixing(self, B, enc_out_list, x_list):\n        dec_out_list = []\n        if self.channel_independence == 1:\n            x_list = x_list[0]\n            for i, enc_out in zip(range(len(x_list)), enc_out_list):\n                dec_out = self.predict_layers[i](enc_out.permute(0, 2, 1)).permute(\n                    0, 2, 1)  # align temporal dimension\n                if self.use_future_temporal_feature:\n                    dec_out = dec_out + self.x_mark_dec\n                    dec_out = self.projection_layer(dec_out)\n                else:\n                    dec_out = self.projection_layer(dec_out)\n                dec_out = dec_out.reshape(B, self.c_out, self.h).permute(0, 2, 1).contiguous()\n                dec_out_list.append(dec_out)\n\n        else:\n            for i, enc_out, out_res in zip(range(len(x_list[0])), enc_out_list, x_list[1]):\n                dec_out = self.predict_layers[i](enc_out.permute(0, 2, 1)).permute(\n                    0, 2, 1)  # align temporal dimension\n                dec_out = self.out_projection(dec_out, i, out_res)\n                dec_out_list.append(dec_out)\n\n        return dec_out_list\n    \n    def forward(self, windows_batch):\n        insample_y = windows_batch['insample_y']\n        futr_exog = windows_batch['futr_exog']\n\n        if self.futr_exog_size > 0:\n            x_mark_enc = futr_exog[:, :, :self.input_size, :]\n            x_mark_dec = futr_exog[:, :, -(self.label_len + self.h):, :]\n        else:\n            x_mark_enc = None\n            x_mark_dec = None\n\n\n        y_pred = self.forecast(insample_y, x_mark_enc, x_mark_dec)\n        y_pred = y_pred[:, -self.h:, :]\n        if self.loss.outputsize_multiplier > 1:\n            y_pred = self.distr_output(y_pred)\n\n        return y_pred\n```\n\n----------------------------------------\n\nTITLE: Model Input/Output Batch Shapes Table\nDESCRIPTION: Table showing the tensor shapes for various inputs in BaseModel when MULTIVARIATE=False. Defines expected dimensions for insample_y, insample_mask, futr_exog, hist_exog, and stat_exog tensors.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/18_adding_models.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ninsample_y:    (w_bs, L, 1)\ninsample_mask: (w_bs, L)\nfutr_exog:     (w_bs, L+h, n_f)\nhist_exog:     (w_bs, L, n_h)\nstat_exog:     (w_bs, n_s)\n```\n\n----------------------------------------\n\nTITLE: Implementing BaseAuto Class for Automated Hyperparameter Optimization\nDESCRIPTION: Defines the BaseAuto class that extends PyTorch Lightning's LightningModule to provide automated hyperparameter optimization. It supports both Ray and Optuna backends and handles configuration of search spaces, validation metrics, and model training.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_auto.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass BaseAuto(pl.LightningModule):\n    \"\"\"\n    Class for Automatic Hyperparameter Optimization, it builds on top of `ray` to \n    give access to a wide variety of hyperparameter optimization tools ranging \n    from classic grid search, to Bayesian optimization and HyperBand algorithm.\n\n    The validation loss to be optimized is defined by the `config['loss']` dictionary\n    value, the config also contains the rest of the hyperparameter search space.\n\n    It is important to note that the success of this hyperparameter optimization\n    heavily relies on a strong correlation between the validation and test periods.\n\n    Parameters\n    ----------\n    cls_model : PyTorch/PyTorchLightning model\n        See `neuralforecast.models` [collection here](https://nixtla.github.io/neuralforecast/models.html).\n    h : int\n        Forecast horizon\n    loss : PyTorch module\n        Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).\n    valid_loss : PyTorch module\n        Instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).\n    config : dict or callable\n        Dictionary with ray.tune defined search space or function that takes an optuna trial and returns a configuration dict.\n    search_alg : ray.tune.search variant or optuna.sampler\n        For ray see https://docs.ray.io/en/latest/tune/api_docs/suggestion.html\n        For optuna see https://optuna.readthedocs.io/en/stable/reference/samplers/index.html.\n    num_samples : int\n        Number of hyperparameter optimization steps/samples.\n    cpus : int (default=os.cpu_count())\n        Number of cpus to use during optimization. Only used with ray tune.\n    gpus : int (default=torch.cuda.device_count())\n        Number of gpus to use during optimization, default all available. Only used with ray tune.\n    refit_with_val : bool\n        Refit of best model should preserve val_size.\n    verbose : bool\n        Track progress.\n    alias : str, optional (default=None)\n        Custom name of the model.\n    backend : str (default='ray')\n        Backend to use for searching the hyperparameter space, can be either 'ray' or 'optuna'.\n    callbacks : list of callable, optional (default=None)\n        List of functions to call during the optimization process.\n        ray reference: https://docs.ray.io/en/latest/tune/tutorials/tune-metrics.html\n        optuna reference: https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/007_optuna_callback.html\n    \"\"\"\n    def __init__(self, \n                 cls_model,\n                 h,\n                 loss,\n                 valid_loss,\n                 config, \n                 search_alg=BasicVariantGenerator(random_state=1),\n                 num_samples=10,\n                 cpus=cpu_count(),\n                 gpus=torch.cuda.device_count(),\n                 refit_with_val=False,\n                 verbose=False,\n                 alias=None,\n                 backend='ray',\n                 callbacks=None,\n                ):\n        super(BaseAuto, self).__init__()\n        with warnings.catch_warnings(record=False):\n            warnings.filterwarnings('ignore')\n            # the following line issues a warning about the loss attribute being saved\n            # but we do want to save it\n            self.save_hyperparameters() # Allows instantiation from a checkpoint from class\n\n        if backend == 'ray':\n            if not isinstance(config, dict):\n                raise ValueError(\n                    \"You have to provide a dict as `config` when using `backend='ray'`\"\n                )\n            config_base = deepcopy(config)\n        elif backend == 'optuna':\n            if not callable(config):\n                raise ValueError(\n                    \"You have to provide a function that takes a trial and returns a dict as `config` when using `backend='optuna'`\"\n                )\n            # extract constant values from the config fn for validations\n            config_base = config(MockTrial())\n        else:\n            raise ValueError(f\"Unknown backend {backend}. The supported backends are 'ray' and 'optuna'.\")\n        if config_base.get('h', None) is not None:\n            raise Exception(\"Please use `h` init argument instead of `config['h']`.\")\n        if config_base.get('loss', None) is not None:\n            raise Exception(\"Please use `loss` init argument instead of `config['loss']`.\")\n        if config_base.get('valid_loss', None) is not None:\n            raise Exception(\"Please use `valid_loss` init argument instead of `config['valid_loss']`.\")\n        # This attribute helps to protect \n        # model and datasets interactions protections\n        if 'early_stop_patience_steps' in config_base.keys():\n            self.early_stop_patience_steps = 1\n        else:\n            self.early_stop_patience_steps = -1\n\n        if callable(config):\n            # reset config_base here to save params to override in the config fn\n            config_base = {}\n\n        # Add losses to config and protect valid_loss default\n        config_base['h'] = h\n        config_base['loss'] = loss\n        if valid_loss is None:\n            valid_loss = loss\n        config_base['valid_loss'] = valid_loss\n\n        if isinstance(config, dict):\n            self.config = config_base            \n        else:\n            def config_f(trial):\n                return {**config(trial), **config_base}\n            self.config = config_f            \n        \n        self.h = h\n        self.cls_model = cls_model\n        self.loss = loss\n        self.valid_loss = valid_loss\n\n        self.num_samples = num_samples\n        self.search_alg = search_alg\n        self.cpus = cpus\n        self.gpus = gpus\n        self.refit_with_val = refit_with_val or self.early_stop_patience_steps > 0\n        self.verbose = verbose\n        self.alias = alias\n        self.backend = backend\n        self.callbacks = callbacks\n\n        # Base Class attributes\n        self.EXOGENOUS_FUTR = cls_model.EXOGENOUS_FUTR\n        self.EXOGENOUS_HIST = cls_model.EXOGENOUS_HIST\n        self.EXOGENOUS_STAT = cls_model.EXOGENOUS_STAT\n        self.MULTIVARIATE = cls_model.MULTIVARIATE    \n        self.RECURRENT = cls_model.RECURRENT    \n\n    def __repr__(self):\n        return type(self).__name__ if self.alias is None else self.alias\n    \n    def _train_tune(self, config_step, cls_model, dataset, val_size, test_size):\n        \"\"\" BaseAuto._train_tune\n\n        Internal function that instantiates a NF class model, then automatically\n        explores the validation loss (ptl/val_loss) on which the hyperparameter \n        exploration is based.\n\n        **Parameters:**<br>\n        `config_step`: Dict, initialization parameters of a NF model.<br>\n        `cls_model`: NeuralForecast model class, yet to be instantiated.<br>\n        `dataset`: NeuralForecast dataset, to fit the model.<br>\n        `val_size`: int, validation size for temporal cross-validation.<br>\n        `test_size`: int, test size for temporal cross-validation.<br>\n        \"\"\"\n        metrics = {\"loss\": \"ptl/val_loss\", \"train_loss\": \"train_loss\"}\n        callbacks = [TuneReportCallback(metrics, on=\"validation_end\")]\n        if 'callbacks' in config_step.keys():\n            callbacks.extend(config_step['callbacks'])\n        config_step = {**config_step, **{'callbacks': callbacks}}\n\n        # Protect dtypes from tune samplers\n        if 'batch_size' in config_step.keys():\n            config_step['batch_size'] = int(config_step['batch_size'])\n        if 'windows_batch_size' in config_step.keys():\n            config_step['windows_batch_size'] = int(config_step['windows_batch_size'])\n\n```\n\n----------------------------------------\n\nTITLE: Forecasting Function in TimeLLM\nDESCRIPTION: Core forecasting function that processes input time series data, creates prompts with statistical information, applies transformations through various layers, and generates forecasted values. Handles normalization, embedding, and output projection.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timellm.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef forecast(self, x_enc):\n\n    x_enc = self.normalize_layers(x_enc, 'norm')\n\n    B, T, N = x_enc.size()\n    x_enc = x_enc.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n\n    min_values = torch.min(x_enc, dim=1)[0]\n    max_values = torch.max(x_enc, dim=1)[0]\n    medians = torch.median(x_enc, dim=1).values\n    lags = self.calcute_lags(x_enc)\n    trends = x_enc.diff(dim=1).sum(dim=1)\n\n    prompt = []\n    for b in range(x_enc.shape[0]):\n        min_values_str = str(min_values[b].tolist()[0])\n        max_values_str = str(max_values[b].tolist()[0])\n        median_values_str = str(medians[b].tolist()[0])\n        lags_values_str = str(lags[b].tolist())\n        prompt_ = (\n            f\"<|start_prompt|>{self.prompt_prefix}\"\n            f\"Task description: forecast the next {str(self.h)} steps given the previous {str(self.input_size)} steps information; \"\n            \"Input statistics: \"\n            f\"min value {min_values_str}, \"\n            f\"max value {max_values_str}, \"\n            f\"median value {median_values_str}, \"\n            f\"the trend of input is {'upward' if trends[b] > 0 else 'downward'}, \"\n            f\"top 5 lags are : {lags_values_str}<|<end_prompt>|>\"\n        )\n\n        prompt.append(prompt_)\n\n    x_enc = x_enc.reshape(B, N, T).permute(0, 2, 1).contiguous()\n\n    prompt = self.llm_tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048).input_ids\n    prompt_embeddings = self.llm.get_input_embeddings()(prompt.to(x_enc.device))  # (batch, prompt_token, dim)\n\n    source_embeddings = self.mapping_layer(self.word_embeddings.permute(1, 0)).permute(1, 0)\n\n    x_enc = x_enc.permute(0, 2, 1).contiguous()\n    enc_out, n_vars = self.patch_embedding(x_enc.to(torch.float32))\n    enc_out = self.reprogramming_layer(enc_out, source_embeddings, source_embeddings)\n    llm_enc_out = torch.cat([prompt_embeddings, enc_out], dim=1)\n    dec_out = self.llm(inputs_embeds=llm_enc_out).last_hidden_state\n    dec_out = dec_out[:, :, :self.d_ff]\n\n    dec_out = torch.reshape(\n        dec_out, (-1, n_vars, dec_out.shape[-2], dec_out.shape[-1]))\n    dec_out = dec_out.permute(0, 1, 3, 2).contiguous()\n\n    dec_out = self.output_projection(dec_out[:, :, :, -self.patch_nums:])\n    dec_out = dec_out.permute(0, 2, 1).contiguous()\n\n    dec_out = self.normalize_layers(dec_out, 'denorm')\n\n    return dec_out\n```\n\n----------------------------------------\n\nTITLE: Implementing AutoPatchTST Class for Time Series Forecasting\nDESCRIPTION: Defines the AutoPatchTST class that extends BaseAuto for automated time series forecasting using PatchTST model. Includes default hyperparameter configurations and methods for model initialization and configuration generation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_84\n\nLANGUAGE: python\nCODE:\n```\nclass AutoPatchTST(BaseAuto):\n\n    default_config = {\n        \"input_size_multiplier\": [1, 2, 3],\n        \"h\": None,\n        \"hidden_size\": tune.choice([16, 128, 256]),\n        \"n_heads\": tune.choice([4, 16]),\n        \"patch_len\": tune.choice([16, 24]),\n        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n        \"scaler_type\": tune.choice([None, 'robust', 'standard']),\n        \"revin\": tune.choice([False, True]),\n        \"max_steps\": tune.choice([500, 1000, 5000]),\n        \"batch_size\": tune.choice([32, 64, 128, 256]),\n        \"windows_batch_size\": tune.choice([128, 256, 512, 1024]),\n        \"loss\": None,\n        \"random_seed\": tune.randint(1, 20),\n    }\n```\n\n----------------------------------------\n\nTITLE: Implementing LayerNorm and AutoCorrelation in Python\nDESCRIPTION: Defines custom LayerNorm and AutoCorrelation layer implementations for neural networks. LayerNorm is specially designed for seasonal components, while AutoCorrelation handles attention mechanisms with multi-head support.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.fedformer.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass LayerNorm(nn.Module):\n    \"\"\"\n    Special designed layernorm for the seasonal part\n    \"\"\"\n    def __init__(self, channels):\n        super(LayerNorm, self).__init__()\n        self.layernorm = nn.LayerNorm(channels)\n\n    def forward(self, x):\n        x_hat = self.layernorm(x)\n        bias = torch.mean(x_hat, dim=1).unsqueeze(1).repeat(1, x.shape[1], 1)\n        return x_hat - bias\n\n\nclass AutoCorrelationLayer(nn.Module):\n    \"\"\"\n    Auto Correlation Layer\n    \"\"\"\n    def __init__(self, correlation, hidden_size, n_head, d_keys=None,\n                 d_values=None):\n        super(AutoCorrelationLayer, self).__init__()\n\n        d_keys = d_keys or (hidden_size // n_head)\n        d_values = d_values or (hidden_size // n_head)\n\n        self.inner_correlation = correlation\n        self.query_projection = nn.Linear(hidden_size, d_keys * n_head)\n        self.key_projection = nn.Linear(hidden_size, d_keys * n_head)\n        self.value_projection = nn.Linear(hidden_size, d_values * n_head)\n        self.out_projection = nn.Linear(d_values * n_head, hidden_size)\n        self.n_head = n_head\n\n    def forward(self, queries, keys, values, attn_mask):\n        B, L, _ = queries.shape\n        _, S, _ = keys.shape\n        H = self.n_head\n\n        queries = self.query_projection(queries).view(B, L, H, -1)\n        keys = self.key_projection(keys).view(B, S, H, -1)\n        values = self.value_projection(values).view(B, S, H, -1)\n\n        out, attn = self.inner_correlation(\n            queries,\n            keys,\n            values,\n            attn_mask\n        )\n        out = out.view(B, L, -1)\n\n        return self.out_projection(out), attn\n```\n\n----------------------------------------\n\nTITLE: Configuring NeuralForecast Models\nDESCRIPTION: Initializes NHITS and BiTCN models with exogenous variables configuration for day-ahead forecasting.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/03_exogenous_variables.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nhorizon = 24 # day-ahead daily forecast\nmodels = [NHITS(h = horizon,\n                max_steps=100,\n                input_size = 5*horizon,\n                futr_exog_list = ['gen_forecast', 'week_day'], # <- Future exogenous variables\n                hist_exog_list = ['system_load'], # <- Historical exogenous variables\n                stat_exog_list = ['market_0', 'market_1'], # <- Static exogenous variables\n                scaler_type = 'robust'),\n          BiTCN(h = horizon,\n                input_size = 5*horizon,\n                max_steps=100,\n                futr_exog_list = ['gen_forecast', 'week_day'], # <- Future exogenous variables\n                hist_exog_list = ['system_load'], # <- Historical exogenous variables\n                stat_exog_list = ['market_0', 'market_1'], # <- Static exogenous variables\n                scaler_type = 'robust',\n                ),                \n                ]\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Usage of AutoTSMixerx in Python\nDESCRIPTION: This snippet shows how to use the AutoTSMixerx class for extended time series forecasting. It demonstrates initialization with custom and default configurations, fitting the model to a dataset, making predictions, and using different backends (Ray and Optuna).\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_107\n\nLANGUAGE: python\nCODE:\n```\n# Use your own config or AutoTSMixerx.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12)\nmodel = AutoTSMixerx(h=12, n_series=1, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n# Optuna\nmodel = AutoTSMixerx(h=12, n_series=1, config=None, backend='optuna')\n```\n\n----------------------------------------\n\nTITLE: Forward Pass Implementation\nDESCRIPTION: Implementation of the forward pass method that processes input batches through the RNN model. Handles concatenation of various input features and generates predictions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.rnn.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, windows_batch):\n    # Parse windows_batch\n    encoder_input = windows_batch['insample_y']                         # [B, seq_len, 1]\n    futr_exog     = windows_batch['futr_exog']                          # [B, seq_len, F]\n    hist_exog     = windows_batch['hist_exog']                          # [B, seq_len, X]\n    stat_exog     = windows_batch['stat_exog']                          # [B, S]\n\n    # Concatenate y, historic and static inputs              \n    batch_size, seq_len = encoder_input.shape[:2]\n    if self.hist_exog_size > 0:\n        encoder_input = torch.cat((encoder_input, hist_exog), dim=2)    # [B, seq_len, 1] + [B, seq_len, X] -> [B, seq_len, 1 + X]\n\n    if self.stat_exog_size > 0:\n        stat_exog = stat_exog.unsqueeze(1).repeat(1, seq_len, 1)        # [B, S] -> [B, seq_len, S]\n        encoder_input = torch.cat((encoder_input, stat_exog), dim=2)    # [B, seq_len, 1 + X] + [B, seq_len, S] -> [B, seq_len, 1 + X + S]\n\n    if self.futr_exog_size > 0:\n        encoder_input = torch.cat((encoder_input, \n                                   futr_exog[:, :seq_len]), dim=2)      # [B, seq_len, 1 + X + S] + [B, seq_len, F] -> [B, seq_len, 1 + X + S + F]\n```\n\n----------------------------------------\n\nTITLE: Implementing PyTorch Time Series Decomposition Modules\nDESCRIPTION: Defines two PyTorch `nn.Module` classes for time series decomposition. `MovingAvg` computes the moving average of a time series using `nn.AvgPool1d`, padding the ends to maintain sequence length. `SeriesDecomp` utilizes the `MovingAvg` module to decompose a time series into its trend (moving mean) and residual components by subtracting the trend from the original series. Depends on `torch` and `torch.nn`.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.modules.ipynb#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n#| export\n\nclass MovingAvg(nn.Module):\n    \"\"\"\n    Moving average block to highlight the trend of time series\n    \"\"\"\n    def __init__(self, kernel_size, stride):\n        super(MovingAvg, self).__init__()\n        self.kernel_size = kernel_size\n        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n\n    def forward(self, x):\n        # padding on the both ends of time series\n        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n        x = torch.cat([front, x, end], dim=1)\n        x = self.avg(x.permute(0, 2, 1))\n        x = x.permute(0, 2, 1)\n        return x\n    \nclass SeriesDecomp(nn.Module):\n    \"\"\"\n    Series decomposition block\n    \"\"\"\n    def __init__(self, kernel_size):\n        super(SeriesDecomp, self).__init__()\n        self.MovingAvg = MovingAvg(kernel_size, stride=1)\n\n    def forward(self, x):\n        moving_mean = self.MovingAvg(x)\n        res = x - moving_mean\n        return res, moving_mean\n```\n\n----------------------------------------\n\nTITLE: Parsing Time Series Windows for Model Input in Python using PyTorch\nDESCRIPTION: This method parses a batch of time series windows into distinct components required for model input: insample target (`insample_y`), insample mask, outsample target (`outsample_y`), outsample mask, historical exogenous features (`hist_exog`), future exogenous features (`futr_exog`), and static exogenous features (`stat_exog`). It handles specific adjustments for recurrent models (shifting inputs) and multivariate scenarios, potentially excluding insample targets based on `self.exclude_insample_y`. Dependencies include `torch`, `get_indexer_raise_missing`, and various model configuration attributes.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n    def _parse_windows(self, batch, windows):\n        # windows: [Ws, L + h, C, n_series]\n\n        # Filter insample lags from outsample horizon\n        y_idx = batch['y_idx']\n        mask_idx = batch['temporal_cols'].get_loc('available_mask')\n\n        insample_y = windows['temporal'][:, :self.input_size, y_idx]\n        insample_mask = windows['temporal'][:, :self.input_size, mask_idx]\n\n        # Declare additional information\n        outsample_y = None\n        outsample_mask = None\n        hist_exog = None\n        futr_exog = None\n        stat_exog = None\n\n        if self.h > 0:\n            outsample_y = windows['temporal'][:, self.input_size:, y_idx]\n            outsample_mask = windows['temporal'][:, self.input_size:, mask_idx]\n\n        # Recurrent models at t predict t+1, so we shift the input (insample_y) by one\n        if self.RECURRENT:\n            insample_y = torch.cat((insample_y, outsample_y[:, :-1]), dim=1)\n            insample_mask = torch.cat((insample_mask, outsample_mask[:, :-1]), dim=1)\n            self.maintain_state = False\n\n        if len(self.hist_exog_list):\n            hist_exog_idx = get_indexer_raise_missing(windows['temporal_cols'], self.hist_exog_list)\n            if self.RECURRENT:\n                hist_exog = windows['temporal'][:, :, hist_exog_idx]\n                hist_exog[:, self.input_size:] = 0.0\n                hist_exog = hist_exog[:, 1:]\n            else:\n                hist_exog = windows['temporal'][:, :self.input_size, hist_exog_idx]\n            if not self.MULTIVARIATE:\n                hist_exog = hist_exog.squeeze(-1)\n            else:\n                hist_exog = hist_exog.swapaxes(1, 2)\n\n        if len(self.futr_exog_list):\n            futr_exog_idx = get_indexer_raise_missing(windows['temporal_cols'], self.futr_exog_list)\n            futr_exog = windows['temporal'][:, :, futr_exog_idx]\n            if self.RECURRENT:\n                futr_exog = futr_exog[:, 1:]\n            if not self.MULTIVARIATE:\n                futr_exog = futr_exog.squeeze(-1)\n            else:\n                futr_exog = futr_exog.swapaxes(1, 2)                \n\n        if len(self.stat_exog_list):\n            static_idx = get_indexer_raise_missing(windows['static_cols'], self.stat_exog_list)\n            stat_exog = windows['static'][:, static_idx]\n\n        # TODO: think a better way of removing insample_y features\n        if self.exclude_insample_y:\n            insample_y = insample_y * 0\n\n        return insample_y, insample_mask, outsample_y, outsample_mask, \\\n               hist_exog, futr_exog, stat_exog\n```\n\n----------------------------------------\n\nTITLE: Predicting with NeuralForecast Model in Python\nDESCRIPTION: This method generates predictions using a fitted NeuralForecast model. It handles various input scenarios including distributed datasets, applies necessary transformations, and formats the output forecast dataframe.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\ndef predict(self, df=None, static_df=None, futr_df=None, engine=None, level=None, quantiles=None, verbose=False, **data_kwargs):\n    if not self._fitted:\n        raise Exception(\"You must fit the model before predicting.\")\n    \n    quantiles_ = None\n    level_ = None\n    has_level = False   \n    if level is not None:\n        has_level = True\n        if quantiles is not None:\n            raise ValueError(\"You can't set both level and quantiles.\")\n        level_ = sorted(list(set(level)))\n        quantiles_ = level_to_quantiles(level_)\n    \n    if quantiles is not None:\n        if level is not None:\n            raise ValueError(\"You can't set both level and quantiles.\")            \n        quantiles_ = sorted(list(set(quantiles)))\n        level_ = quantiles_to_level(quantiles_)\n\n    needed_futr_exog = self._get_needed_futr_exog()\n    if needed_futr_exog:\n        if futr_df is None:\n            raise ValueError(\n                f'Models require the following future exogenous features: {needed_futr_exog}. '\n                'Please provide them through the `futr_df` argument.'\n            )\n        else:\n            missing = needed_futr_exog - set(futr_df.columns)\n            if missing:\n                raise ValueError(f'The following features are missing from `futr_df`: {missing}')\n\n    # ... (rest of the method implementation)\n\n    return fcsts_df\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using AutoTFT Model for Time Series Forecasting in Python\nDESCRIPTION: This snippet demonstrates the basic usage of the AutoTFT model for time series forecasting. It shows how to configure, initialize, fit, and predict with the model, as well as how to use Optuna for hyperparameter optimization.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_65\n\nLANGUAGE: python\nCODE:\n```\n# Use your own config or AutoTFT.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=8)\nmodel = AutoTFT(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n# Optuna\nmodel = AutoTFT(h=12, config=None, backend='optuna')\n```\n\n----------------------------------------\n\nTITLE: DilatedRNN Class Definition in PyTorch\nDESCRIPTION: This snippet defines the DilatedRNN class, which inherits from BaseModel. It includes initialization of various parameters, setting up the RNN stack, and configuring the model architecture for time series forecasting.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.dilated_rnn.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\nclass DilatedRNN(BaseModel):\n    \"\"\" DilatedRNN\n\n    **Parameters:**<br>\n    `h`: int, forecast horizon.<br>\n    `input_size`: int, maximum sequence length for truncated train backpropagation. Default -1 uses 3 * horizon <br>\n    `inference_input_size`: int, maximum sequence length for truncated inference. Default None uses input_size history.<br>\n    `cell_type`: str, type of RNN cell to use. Options: 'GRU', 'RNN', 'LSTM', 'ResLSTM', 'AttentiveLSTM'.<br>\n    `dilations`: int list, dilations betweem layers.<br>\n    `encoder_hidden_size`: int=200, units for the RNN's hidden state size.<br>\n    `context_size`: int=10, size of context vector for each timestamp on the forecasting window.<br>\n    `decoder_hidden_size`: int=200, size of hidden layer for the MLP decoder.<br>\n    `decoder_layers`: int=2, number of layers for the MLP decoder.<br>\n    `futr_exog_list`: str list, future exogenous columns.<br>\n    `hist_exog_list`: str list, historic exogenous columns.<br>\n    `stat_exog_list`: str list, static exogenous columns.<br>\n    `exclude_insample_y`: bool=False, the model skips the autoregressive features y[t-input_size:t] if True.<br>\n    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `max_steps`: int, maximum number of training steps.<br>\n    `learning_rate`: float, Learning rate between (0, 1).<br>\n    `num_lr_decays`: int, Number of learning rate decays, evenly distributed across max_steps.<br>\n    `early_stop_patience_steps`: int, Number of validation iterations before early stopping.<br>\n    `val_check_steps`: int, Number of training steps between every validation loss check.<br>\n    `batch_size`: int=32, number of different series in each batch.<br>\n    `valid_batch_size`: int=None, number of different series in each validation and test batch.<br>\n    `windows_batch_size`: int=128, number of windows to sample in each training batch, default uses all.<br>\n    `inference_windows_batch_size`: int=1024, number of windows to sample in each inference batch, -1 uses all.<br>\n    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>    \n    `step_size`: int=1, step size between each window of temporal data.<br>\n    `scaler_type`: str='robust', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n    `random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n    `alias`: str, optional,  Custom name of the model.<br>\n    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br> \n    `dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n    \"\"\"\n    # Class attributes\n    EXOGENOUS_FUTR = True\n    EXOGENOUS_HIST = True\n    EXOGENOUS_STAT = True\n    MULTIVARIATE = False    # If the model produces multivariate forecasts (True) or univariate (False)\n    RECURRENT = False       # If the model produces forecasts recursively (True) or direct (False)\n\n    def __init__(self,\n                 h: int,\n                 input_size: int = -1,\n                 inference_input_size: Optional[int] = None,\n                 cell_type: str = 'LSTM',\n                 dilations: List[List[int]] = [[1, 2], [4, 8]],\n                 encoder_hidden_size: int = 128,\n                 context_size: int = 10,\n                 decoder_hidden_size: int = 128,\n                 decoder_layers: int = 2,\n                 futr_exog_list = None,\n                 hist_exog_list = None,\n                 stat_exog_list = None,\n                 exclude_insample_y = False,\n                 loss = MAE(),\n                 valid_loss = None,\n                 max_steps: int = 1000,\n                 learning_rate: float = 1e-3,\n                 num_lr_decays: int = 3,\n                 early_stop_patience_steps: int =-1,\n                 val_check_steps: int = 100,\n                 batch_size = 32,\n                 valid_batch_size: Optional[int] = None,\n                 windows_batch_size = 128,\n                 inference_windows_batch_size = 1024,\n                 start_padding_enabled = False,\n                 step_size: int = 1,\n                 scaler_type: str = 'robust',\n                 random_seed: int = 1,\n                 drop_last_loader: bool = False,\n                 alias: Optional[str] = None,\n                 optimizer = None,\n                 optimizer_kwargs = None,\n                 lr_scheduler = None,\n                 lr_scheduler_kwargs = None,\n                 dataloader_kwargs = None,\n                 **trainer_kwargs):\n        super(DilatedRNN, self).__init__(\n            h=h,\n            input_size=input_size,\n            inference_input_size=inference_input_size,\n            futr_exog_list=futr_exog_list,\n            hist_exog_list=hist_exog_list,\n            stat_exog_list=stat_exog_list,\n            exclude_insample_y = exclude_insample_y,\n            loss=loss,\n            valid_loss=valid_loss,\n            max_steps=max_steps,\n            learning_rate=learning_rate,\n            num_lr_decays=num_lr_decays,\n            early_stop_patience_steps=early_stop_patience_steps,\n            val_check_steps=val_check_steps,\n            batch_size=batch_size,\n            valid_batch_size=valid_batch_size,\n            windows_batch_size=windows_batch_size,\n            inference_windows_batch_size=inference_windows_batch_size,\n            start_padding_enabled=start_padding_enabled,\n            step_size=step_size,\n            scaler_type=scaler_type,\n            random_seed=random_seed,\n            drop_last_loader=drop_last_loader,\n            alias=alias,\n            optimizer=optimizer,\n            optimizer_kwargs=optimizer_kwargs,\n            lr_scheduler=lr_scheduler,\n            lr_scheduler_kwargs=lr_scheduler_kwargs,\n            dataloader_kwargs=dataloader_kwargs,\n            **trainer_kwargs\n        )\n\n        # Dilated RNN\n        self.cell_type = cell_type\n        self.dilations = dilations\n        self.encoder_hidden_size = encoder_hidden_size\n        \n        # Context adapter\n        self.context_size = context_size\n\n        # MLP decoder\n        self.decoder_hidden_size = decoder_hidden_size\n        self.decoder_layers = decoder_layers\n\n        # RNN input size (1 for target variable y)\n        input_encoder = 1 + self.hist_exog_size + self.stat_exog_size + self.futr_exog_size\n\n        # Instantiate model\n        layers = []\n        for grp_num in range(len(self.dilations)):\n            if grp_num > 0:\n                input_encoder = self.encoder_hidden_size\n            layer = DRNN(input_encoder,\n                         self.encoder_hidden_size,\n                         n_layers=len(self.dilations[grp_num]),\n                         dilations=self.dilations[grp_num],\n                         cell_type=self.cell_type)\n            layers.append(layer)\n\n        self.rnn_stack = nn.Sequential(*layers)\n\n        # Context adapter\n        self.context_adapter = nn.Linear(in_features=self.input_size,\n```\n\n----------------------------------------\n\nTITLE: NBEATSx Full Usage Example with Quantile Predictions\nDESCRIPTION: Complete example showing NBEATSx model initialization, training with quantile loss, prediction, and visualization of results with confidence intervals. Includes data preparation and plotting code.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeatsx.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import NBEATSx\nfrom neuralforecast.losses.pytorch import MQLoss\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = NBEATSx(h=12, input_size=24,\n                loss=MQLoss(level=[80, 90]),\n                scaler_type='robust',\n                dropout_prob_theta=0.5,\n                stat_exog_list=['airline1'],\n                futr_exog_list=['trend'],\n                max_steps=200,\n                val_check_steps=10,\n                early_stop_patience_steps=2)\n\nnf = NeuralForecast(\n    models=[model],\n    freq='ME'\n)\nnf.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nY_hat_df = nf.predict(futr_df=Y_test_df)\n\n# Plot quantile predictions\nY_hat_df = Y_hat_df.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['NBEATSx-median'], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'][-12:], \n                 y1=plot_df['NBEATSx-lo-90'][-12:].values, \n                 y2=plot_df['NBEATSx-hi-90'][-12:].values,\n                 alpha=0.4, label='level 90')\nplt.legend()\nplt.grid()\nplt.plot()\n```\n\n----------------------------------------\n\nTITLE: Implementing AutoKAN Class\nDESCRIPTION: Implementation of the AutoKAN class for Kolmogorov-Arnold Networks (KAN) forecasting model. Defines configuration space with KAN-specific hyperparameters, initialization method, and configuration generation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_61\n\nLANGUAGE: python\nCODE:\n```\nclass AutoKAN(BaseAuto):\n\n    default_config = {\n       \"input_size_multiplier\": [1, 2, 3, 4, 5],\n       \"h\": None,\n       \"grid_size\": tune.choice([5, 10, 15]),\n       \"spline_order\": tune.choice([2, 3, 4]),\n       \"hidden_size\": tune.choice([64, 128, 256, 512]),\n       \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n       \"scaler_type\": tune.choice([None, 'robust', 'standard']),\n       \"max_steps\": tune.quniform(lower=500, upper=1500, q=100),\n       \"batch_size\": tune.choice([32, 64, 128, 256]),\n       \"windows_batch_size\": tune.choice([128, 256, 512, 1024]),\n       \"loss\": None,\n       \"random_seed\": tune.randint(lower=1, upper=20),\n    }\n\n    def __init__(self,\n                 h,\n                 loss=MAE(),\n                 valid_loss=None,\n                 config=None, \n                 search_alg=BasicVariantGenerator(random_state=1),\n                 num_samples=10,\n                 refit_with_val=False,\n                 cpus=cpu_count(),\n                 gpus=torch.cuda.device_count(),\n                 verbose=False,\n                 alias=None,\n                 backend='ray',\n                 callbacks=None,\n                ):\n\n        # Define search space, input/output sizes\n        if config is None:\n            config = self.get_default_config(h=h, backend=backend)                 \n\n        super(AutoKAN, self).__init__(\n              cls_model=KAN, \n              h=h,\n              loss=loss,\n              valid_loss=valid_loss,\n              config=config,\n              search_alg=search_alg,\n              num_samples=num_samples,\n              refit_with_val=refit_with_val,\n              cpus=cpus,\n              gpus=gpus,\n              verbose=verbose,\n              alias=alias,\n              backend=backend,\n              callbacks=callbacks,\n        )\n\n    @classmethod\n    def get_default_config(cls, h, backend, n_series=None):\n        config = cls.default_config.copy()\n        config['input_size'] = tune.choice([h*x \\\n                        for x in config['input_size_multiplier']])\n        config['step_size'] = tune.choice([1, h])        \n        del config['input_size_multiplier']\n        if backend == 'optuna':\n            config = cls._ray_config_to_optuna(config)         \n\n        return config\n```\n\n----------------------------------------\n\nTITLE: Forecasting with KAN Model Using NeuralForecast Python - Full Example\nDESCRIPTION: This usage example demonstrates setting up, training, and evaluating the KAN model with the NeuralForecast library in Python. It utilizes pandas for data handling, matplotlib for visualization, and specific neuralforecast utilities for data loading and forecasting. Required libraries are pandas, matplotlib, and neuralforecast. Key parameters include the forecast horizon (h), input window size, and various exogenous variables. The code inputs daily time series airline data, produces quantile forecasts, and plots the results, showcasing KANs capabilities and output structure.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.kan.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n#| eval: false\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import KAN\nfrom neuralforecast.losses.pytorch import DistributionLoss\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nfcst = NeuralForecast(\n    models=[\n            KAN(h=12,\n                input_size=24,\n                loss = DistributionLoss(distribution=\"Normal\"),\n                max_steps=100,\n                scaler_type='standard',\n                futr_exog_list=['y_[lag12]'],\n                hist_exog_list=None,\n                stat_exog_list=['airline1'],\n                ),     \n    ],\n    freq='ME'\n)\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\n# Plot quantile predictions\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['KAN-median'], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'][-12:], \n                 y1=plot_df['KAN-lo-90'][-12:].values,\n                 y2=plot_df['KAN-hi-90'][-12:].values,\n                 alpha=0.4, label='level 90')\nplt.legend()\nplt.grid()\n```\n\n----------------------------------------\n\nTITLE: Preparing Data for Local File-based Training in NeuralForecast\nDESCRIPTION: This method prepares the input data for training when the dataset is split between local files. It handles the creation of a LocalFilesTimeSeriesDataset for processing.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ndef _prepare_fit_for_local_files(\n    self, \n    files_list: Sequence[str], \n    static_df: Optional[DataFrame], \n    id_col: str, \n    time_col: str, \n    target_col: str\n):\n    if self.local_scaler_type is not None:\n        raise ValueError(\n            \"Historic scaling isn't supported when the dataset is split between files. \"\n            \"Please open an issue if this would be valuable to you.\"\n        )\n    \n    self.id_col = id_col\n    self.time_col = time_col\n    self.target_col = target_col   \n    self.scalers_ = {}     \n\n    exogs = self._get_needed_exog() \n    return LocalFilesTimeSeriesDataset.from_data_directories(\n        directories=files_list,\n        static_df=static_df,\n        exogs=exogs,\n        id_col=id_col,\n        time_col=time_col,\n        target_col=target_col,\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining MLPMultivariate Class for Time Series Forecasting\nDESCRIPTION: Implements the MLPMultivariate class, a multi-layer perceptron model for multivariate time series forecasting. It includes initialization of model parameters, architecture definition, and forward pass implementation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.mlpmultivariate.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass MLPMultivariate(BaseModel):\n    \"\"\" MLPMultivariate\n\n    Simple Multi Layer Perceptron architecture (MLP) for multivariate forecasting. \n    This deep neural network has constant units through its layers, each with\n    ReLU non-linearities, it is trained using ADAM stochastic gradient descent.\n    The network accepts static, historic and future exogenous data, flattens \n    the inputs and learns fully connected relationships against the target variables.\n\n    **Parameters:**<br>\n    `h`: int, forecast horizon.<br>\n    `input_size`: int, considered autorregresive inputs (lags), y=[1,2,3,4] input_size=2 -> lags=[1,2].<br>\n    `n_series`: int, number of time-series.<br>\n    `stat_exog_list`: str list, static exogenous columns.<br>\n    `hist_exog_list`: str list, historic exogenous columns.<br>\n    `futr_exog_list`: str list, future exogenous columns.<br>\n    `num_layers`: int, number of layers for the MLP.<br>\n    `hidden_size`: int, number of units for each layer of the MLP.<br>\n    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `max_steps`: int=1000, maximum number of training steps.<br>\n    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n    `batch_size`: int=32, number of different series in each batch.<br>\n    `valid_batch_size`: int=None, number of different series in each validation and test batch, if None uses batch_size.<br>\n    `windows_batch_size`: int=32, number of windows to sample in each training batch, default uses all.<br>\n    `inference_windows_batch_size`: int=32, number of windows to sample in each inference batch, -1 uses all.<br>\n    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n    `step_size`: int=1, step size between each window of temporal data.<br>\n    `scaler_type`: str='identity', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n    `random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n    `alias`: str, optional,  Custom name of the model.<br>\n    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>\n    `dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n    \"\"\"\n    # Class attributes\n    EXOGENOUS_FUTR = True\n    EXOGENOUS_HIST = True\n    EXOGENOUS_STAT = True    \n    MULTIVARIATE = True    # If the model produces multivariate forecasts (True) or univariate (False)\n    RECURRENT = False       # If the model produces forecasts recursively (True) or direct (False)\n\n    def __init__(self,\n                 h,\n                 input_size,\n                 n_series,\n                 stat_exog_list = None,\n                 hist_exog_list = None,\n                 futr_exog_list = None,\n                 exclude_insample_y = False,\n                 num_layers = 2,\n                 hidden_size = 1024,\n                 loss = MAE(),\n                 valid_loss = None,\n                 max_steps: int = 1000,\n                 learning_rate: float = 1e-3,\n                 num_lr_decays: int = -1,\n                 early_stop_patience_steps: int =-1,\n                 val_check_steps: int = 100,\n                 batch_size: int = 32,\n                 valid_batch_size: Optional[int] = None,\n                 windows_batch_size = 32,\n                 inference_windows_batch_size = 32,\n                 start_padding_enabled = False,\n                 step_size: int = 1,\n                 scaler_type: str = 'identity',\n                 random_seed: int = 1,\n                 drop_last_loader: bool = False,\n                 alias: Optional[str] = None,\n                 optimizer = None,\n                 optimizer_kwargs = None,\n                 lr_scheduler = None,\n                 lr_scheduler_kwargs = None,\n                 dataloader_kwargs = None,\n                 **trainer_kwargs):\n\n        # Inherit BaseMultivariate class\n        super(MLPMultivariate, self).__init__(h=h,\n                                  input_size=input_size,\n                                  n_series=n_series,\n                                  stat_exog_list=stat_exog_list,\n                                  hist_exog_list=hist_exog_list,\n                                  futr_exog_list=futr_exog_list,\n                                  exclude_insample_y = exclude_insample_y,\n                                  loss=loss,\n                                  valid_loss=valid_loss,\n                                  max_steps=max_steps,\n                                  learning_rate=learning_rate,\n                                  num_lr_decays=num_lr_decays,\n                                  early_stop_patience_steps=early_stop_patience_steps,\n                                  val_check_steps=val_check_steps,\n                                  batch_size=batch_size,\n                                  valid_batch_size=valid_batch_size,\n                                  windows_batch_size=windows_batch_size,\n                                  inference_windows_batch_size=inference_windows_batch_size,\n                                  start_padding_enabled=start_padding_enabled,\n                                  step_size=step_size,\n                                  scaler_type=scaler_type,\n                                  random_seed=random_seed,\n                                  drop_last_loader=drop_last_loader,\n                                  alias=alias,\n                                  optimizer=optimizer,\n                                  optimizer_kwargs=optimizer_kwargs,\n                                  lr_scheduler=lr_scheduler,\n                                  lr_scheduler_kwargs=lr_scheduler_kwargs,\n                                  dataloader_kwargs=dataloader_kwargs,\n                                  **trainer_kwargs)\n\n        # Architecture\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n\n        input_size_first_layer = n_series * (input_size + self.hist_exog_size * input_size + \\\n                                 self.futr_exog_size*(input_size + h) + self.stat_exog_size)\n\n        # MultiLayer Perceptron\n        layers = [nn.Linear(in_features=input_size_first_layer, out_features=hidden_size)]\n        for i in range(num_layers - 1):\n            layers += [nn.Linear(in_features=hidden_size, out_features=hidden_size)]\n        self.mlp = nn.ModuleList(layers)\n\n        # Adapter with Loss dependent dimensions\n        self.out = nn.Linear(in_features=hidden_size, \n                             out_features=h * self.loss.outputsize_multiplier * n_series)\n\n    def forward(self, windows_batch):\n\n        # Parse windows_batch\n        x             = windows_batch['insample_y']             #   [batch_size (B), input_size (L), n_series (N)]\n```\n\n----------------------------------------\n\nTITLE: Using AutoNBEATS for Time Series Forecasting with Configuration Examples in Python\nDESCRIPTION: Demonstrates how to instantiate and use the AutoNBEATS class with custom configuration and default parameters. Shows examples of model fitting and prediction with both Ray and Optuna backends. Includes configuration for MLP units specific to NBEATS architecture.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_37\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n# Use your own config or AutoNBEATS.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12,\n              mlp_units=3*[[8, 8]])\nmodel = AutoNBEATS(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n# Optuna\nmodel = AutoNBEATS(h=12, config=None, backend='optuna')\n```\n\n----------------------------------------\n\nTITLE: Implementing KAN Model for Time Series Forecasting in PyTorch\nDESCRIPTION: This class implements the Kolmogorov-Arnold Network (KAN) model for time series forecasting. It inherits from a BaseModel and uses the KANLinear layer for its architecture.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.kan.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass KAN(BaseModel):\n    \"\"\" KAN\n\n    Simple Kolmogorov-Arnold Network (KAN).\n    This network uses the Kolmogorov-Arnold approximation theorem, where splines\n    are learned to approximate more complex functions. Unlike the MLP, the\n    non-linear function are learned at the edges, and the nodes simply sum\n    the different learned functions.\n\n    **Parameters:**<br>\n    `h`: int, forecast horizon.<br>\n    `input_size`: int, considered autorregresive inputs (lags), y=[1,2,3,4] input_size=2 -> lags=[1,2].<br>\n    `grid_size`: int, number of intervals used by the splines to approximate the function.<br>\n    `spline_order`: int, order of the B-splines.<br>\n    `scale_noise`: float, regularization coefficient for the splines.<br>\n    `scale_base`: float, scaling coefficient for the base function.<br>\n    `scale_spline`: float, scaling coefficient for the splines.<br>\n    `enable_standalone_scale_spline`: bool, whether each spline is scaled individually.<br>\n    `grid_eps`: float, used for numerical stability.<br>\n    `grid_range`: list, range of the grid used for spline approximation.<br>\n    `n_hidden_layers`: int, number of hidden layers for the KAN.<br>\n    \"\"\"\n```\n\n----------------------------------------\n\nTITLE: SOFTS Forecast Method Implementation\nDESCRIPTION: Implementation of the forecast method for the SOFTS model that handles data normalization, encoding, and prediction generation using the transformer architecture.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.softs.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef forecast(self, x_enc):\n    if self.use_norm:\n        means = x_enc.mean(1, keepdim=True).detach()\n        x_enc = x_enc - means\n        stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)\n        x_enc /= stdev\n\n    _, _, N = x_enc.shape\n    enc_out = self.enc_embedding(x_enc, None)\n    enc_out, attns = self.encoder(enc_out, attn_mask=None)\n    dec_out = self.projection(enc_out).permute(0, 2, 1)[:, :, :N]\n\n    if self.use_norm:\n        dec_out = dec_out * (stdev[:, 0, :].unsqueeze(1).repeat(1, self.h * self.loss.outputsize_multiplier, 1))\n        dec_out = dec_out + (means[:, 0, :].unsqueeze(1).repeat(1, self.h * self.loss.outputsize_multiplier, 1))\n    return dec_out\n```\n\n----------------------------------------\n\nTITLE: Using AutoTimesNet with Custom Configuration and Multiple Backends\nDESCRIPTION: Example demonstrating how to initialize and use the AutoTimesNet class with custom configuration, including fitting, predicting, and using the Optuna backend.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_95\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n# Use your own config or AutoTimesNet.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=32)\nmodel = AutoTimesNet(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n# Optuna\nmodel = AutoTimesNet(h=12, config=None, backend='optuna')\n```\n\n----------------------------------------\n\nTITLE: Making Predictions with Exogenous Variables\nDESCRIPTION: Generates forecasts using the trained model with future exogenous variables.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/03_exogenous_variables.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nY_hat_df = nf.predict(futr_df=futr_df)\nY_hat_df.head()\n```\n\n----------------------------------------\n\nTITLE: Neural Network Decomposition Method with PyTorch Lightning\nDESCRIPTION: Decomposes predictions through network layers for models like ESRNN, NHITS, NBEATS, and NBEATSx. Uses PyTorch Lightning for training infrastructure and supports customizable step sizes and quantile predictions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_37\n\nLANGUAGE: Python\nCODE:\n```\nif random_seed is None:\n    random_seed = self.random_seed\ntorch.manual_seed(random_seed)\nself._set_quantiles(quantiles)\n\nself.predict_step_size = step_size\nself.decompose_forecast = True\ndatamodule = TimeSeriesDataModule(dataset=dataset,\n                                  valid_batch_size=self.valid_batch_size,\n                                  **data_module_kwargs)\ntrainer = pl.Trainer(**self.trainer_kwargs)\nfcsts = trainer.predict(self, datamodule=datamodule)\nself.decompose_forecast = False\nfcsts = torch.vstack(fcsts)\nreturn tensor_to_numpy(fcsts)\n```\n\n----------------------------------------\n\nTITLE: Interpretable Multi-Head Attention for TFT in PyTorch\nDESCRIPTION: Modified version of multi-head attention that improves interpretability by using shared values across heads and additive aggregation. Includes masking for future timesteps and scaling of attention scores.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nclass InterpretableMultiHeadAttention(nn.Module):\n    def __init__(self, n_head, hidden_size, example_length, attn_dropout, dropout):\n        super().__init__()\n        self.n_head = n_head\n        assert hidden_size % n_head == 0\n        self.d_head = hidden_size // n_head\n        self.qkv_linears = nn.Linear(\n            hidden_size, (2 * self.n_head + 1) * self.d_head, bias=False\n        )\n        self.out_proj = nn.Linear(self.d_head, hidden_size, bias=False)\n\n        self.attn_dropout = nn.Dropout(attn_dropout)\n        self.out_dropout = nn.Dropout(dropout)\n        self.scale = self.d_head**-0.5\n        self.register_buffer(\n            \"_mask\",\n            torch.triu(\n                torch.full((example_length, example_length), float(\"-inf\")), 1\n            ).unsqueeze(0),\n        )\n```\n\n----------------------------------------\n\nTITLE: Defining iTransformer Class for Time Series Forecasting in Python\nDESCRIPTION: This code defines the iTransformer class, which is a PyTorch-based implementation of the Inverted Transformer model for time series forecasting. It includes initialization of model parameters, embedding layers, encoder layers, and a projection layer. The class also implements a forecast method for making predictions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.itransformer.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass iTransformer(BaseModel):\n\n    \"\"\" iTransformer\n\n    **Parameters:**<br>\n    `h`: int, Forecast horizon. <br>\n    `input_size`: int, autorregresive inputs size, y=[1,2,3,4] input_size=2 -> y_[t-2:t]=[1,2].<br>\n    `n_series`: int, number of time-series.<br>\n    `futr_exog_list`: str list, future exogenous columns.<br>\n    `hist_exog_list`: str list, historic exogenous columns.<br>\n    `stat_exog_list`: str list, static exogenous columns.<br>\n    `exclude_insample_y`: bool=False, the model skips the autoregressive features y[t-input_size:t] if True.<br>    \n    `hidden_size`: int, dimension of the model.<br>\n    `n_heads`: int, number of heads.<br>\n    `e_layers`: int, number of encoder layers.<br>\n    `d_layers`: int, number of decoder layers.<br>\n    `d_ff`: int, dimension of fully-connected layer.<br>\n    `factor`: int, attention factor.<br>\n    `dropout`: float, dropout rate.<br>\n    `use_norm`: bool, whether to normalize or not.<br>\n    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `max_steps`: int=1000, maximum number of training steps.<br>\n    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n    `batch_size`: int=32, number of different series in each batch.<br>\n    `valid_batch_size`: int=None, number of different series in each validation and test batch, if None uses batch_size.<br>\n    `windows_batch_size`: int=32, number of windows to sample in each training batch, default uses all.<br>\n    `inference_windows_batch_size`: int=32, number of windows to sample in each inference batch, -1 uses all.<br>\n    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n    `step_size`: int=1, step size between each window of temporal data.<br>\n    `scaler_type`: str='identity', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n    `random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n    `alias`: str, optional,  Custom name of the model.<br>\n    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>\n    `dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>\n    \n    **References**<br>\n    - [Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, Mingsheng Long. \"iTransformer: Inverted Transformers Are Effective for Time Series Forecasting\"](https://arxiv.org/abs/2310.06625)\n    \"\"\"\n\n    # Class attributes\n    EXOGENOUS_FUTR = False\n    EXOGENOUS_HIST = False\n    EXOGENOUS_STAT = False\n    MULTIVARIATE = True\n    RECURRENT = False\n\n    def __init__(self,\n                 h,\n                 input_size,\n                 n_series,\n                 futr_exog_list = None,\n                 hist_exog_list = None,\n                 stat_exog_list = None,\n                 exclude_insample_y = False,\n                 hidden_size: int = 512,\n                 n_heads: int = 8,\n                 e_layers: int = 2,\n                 d_layers: int = 1,\n                 d_ff: int = 2048,\n                 factor: int = 1,\n                 dropout: float = 0.1,\n                 use_norm: bool = True,\n                 loss = MAE(),\n                 valid_loss = None,\n                 max_steps: int = 1000,\n                 learning_rate: float = 1e-3,\n                 num_lr_decays: int = -1,\n                 early_stop_patience_steps: int =-1,\n                 val_check_steps: int = 100,\n                 batch_size: int = 32,\n                 valid_batch_size: Optional[int] = None,\n                 windows_batch_size = 32,\n                 inference_windows_batch_size = 32,\n                 start_padding_enabled = False,\n                 step_size: int = 1,\n                 scaler_type: str = 'identity',\n                 random_seed: int = 1,\n                 drop_last_loader: bool = False,\n                 alias: Optional[str] = None,\n                 optimizer = None,\n                 optimizer_kwargs = None,\n                 lr_scheduler = None,\n                 lr_scheduler_kwargs = None,  \n                 dataloader_kwargs = None,          \n                 **trainer_kwargs):\n        \n        super(iTransformer, self).__init__(h=h,\n                                           input_size=input_size,\n                                           n_series=n_series,\n                                           futr_exog_list = futr_exog_list,\n                                           hist_exog_list = hist_exog_list,\n                                           stat_exog_list = stat_exog_list,\n                                           exclude_insample_y = exclude_insample_y,\n                                           loss=loss,\n                                           valid_loss=valid_loss,\n                                           max_steps=max_steps,\n                                           learning_rate=learning_rate,\n                                           num_lr_decays=num_lr_decays,\n                                           early_stop_patience_steps=early_stop_patience_steps,\n                                           val_check_steps=val_check_steps,\n                                           batch_size=batch_size,\n                                           valid_batch_size=valid_batch_size,\n                                           windows_batch_size=windows_batch_size,\n                                           inference_windows_batch_size=inference_windows_batch_size,\n                                           start_padding_enabled=start_padding_enabled,\n                                           step_size=step_size,\n                                           scaler_type=scaler_type,\n                                           random_seed=random_seed,\n                                           drop_last_loader=drop_last_loader,\n                                           alias=alias,\n                                           optimizer=optimizer,\n                                           optimizer_kwargs=optimizer_kwargs,\n                                           lr_scheduler=lr_scheduler,\n                                           lr_scheduler_kwargs=lr_scheduler_kwargs,\n                                           dataloader_kwargs=dataloader_kwargs,\n                                           **trainer_kwargs)\n               \n        self.enc_in = n_series\n        self.dec_in = n_series\n        self.c_out = n_series\n        self.hidden_size = hidden_size\n        self.n_heads = n_heads\n        self.e_layers = e_layers\n        self.d_layers = d_layers\n        self.d_ff = d_ff\n        self.factor = factor\n        self.dropout = dropout\n        self.use_norm = use_norm\n\n        # Architecture\n        self.enc_embedding = DataEmbedding_inverted(input_size, self.hidden_size, self.dropout)\n\n        self.encoder = TransEncoder(\n            [\n                TransEncoderLayer(\n                    AttentionLayer(\n                        FullAttention(False, self.factor, attention_dropout=self.dropout), self.hidden_size, self.n_heads),\n                    self.hidden_size,\n                    self.d_ff,\n                    dropout=self.dropout,\n                    activation=F.gelu\n                ) for l in range(self.e_layers)\n            ],\n            norm_layer=torch.nn.LayerNorm(self.hidden_size)\n        )\n\n        self.projector = nn.Linear(self.hidden_size, h * self.loss.outputsize_multiplier, bias=True)\n\n    def forecast(self, x_enc):\n        if self.use_norm:\n            # Normalization from Non-stationary Transformer\n            means = x_enc.mean(1, keepdim=True).detach()\n            x_enc = x_enc - means\n            stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)\n            x_enc /= stdev\n\n        _, _, N = x_enc.shape # B L N\n        # B: batch_size;       E: hidden_size; \n        # L: input_size;       S: horizon(h);\n        # N: number of variate (tokens), can also includes covariates\n\n        # Embedding\n        # B L N -> B N E                (B L N -> B L E in the vanilla Transformer)\n        enc_out = self.enc_embedding(x_enc, None) # covariates (e.g timestamp) can be also embedded as tokens\n        \n        # B N E -> B N E                (B L E -> B L E in the vanilla Transformer)\n        # the dimensions of embedded time series has been inverted, and then processed by native attn, layernorm and ffn modules\n        enc_out, attns = self.encoder(enc_out, attn_mask=None)\n\n        # B N E -> B N S -> B S N \n```\n\n----------------------------------------\n\nTITLE: Constructing NHITS Block List in PyTorch\nDESCRIPTION: This code creates a list of NHITS blocks for the neural forecasting model. Each block is configured with specific parameters including pooling kernel size, theta parameters, and activation functions, which are then returned as a complete list for model construction.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nhits.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nnbeats_block = NHITSBlock(h=h,\n                                          input_size=input_size,\n                                          futr_input_size=futr_input_size,\n                                          hist_input_size=hist_input_size,\n                                          stat_input_size=stat_input_size,                                          \n                                          n_theta=n_theta,\n                                          mlp_units=mlp_units,\n                                          n_pool_kernel_size=n_pool_kernel_size[i],\n                                          pooling_mode=pooling_mode,\n                                          basis=basis,\n                                          dropout_prob=dropout_prob_theta,\n                                          activation=activation)\n\n                # Select type of evaluation and apply it to all layers of block\n                block_list.append(nbeats_block)\n                \n        return block_list\n```\n\n----------------------------------------\n\nTITLE: Direct Batch Prediction for Neural Forecasting Models\nDESCRIPTION: Implements the direct forecasting strategy, which predicts the entire forecast horizon in a single pass. The method prepares input data, performs model inference, and handles post-processing operations including inverse normalization and sampling from probabilistic distributions if applicable.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_29\n\nLANGUAGE: Python\nCODE:\n```\ndef _predict_step_direct_batch(self, insample_y, insample_mask, hist_exog, futr_exog, stat_exog, y_idx):\n        windows_batch = dict(insample_y=insample_y,                 # [Ws, L, n_series]\n                        insample_mask=insample_mask,                # [Ws, L, n_series]\n                        futr_exog=futr_exog,                        # univariate: [Ws, L, F]; multivariate: [Ws, F, L, n_series]\n                        hist_exog=hist_exog,                        # univariate: [Ws, L, X]; multivariate: [Ws, X, L, n_series]\n                        stat_exog=stat_exog)                        # univariate: [Ws, S]; multivariate: [n_series, S]\n\n        # Model Predictions\n        output_batch = self(windows_batch)\n        output_batch = self.loss.domain_map(output_batch)\n\n        # Inverse normalization and sampling\n        if self.loss.is_distribution_output:\n            y_loc, y_scale = self._get_loc_scale(y_idx)\n            distr_args = self.loss.scale_decouple(output=output_batch, loc=y_loc, scale=y_scale)\n            _, sample_mean, quants = self.loss.sample(distr_args=distr_args)\n            y_hat = torch.concat((sample_mean, quants), axis=-1)\n\n            if self.loss.return_params:\n                distr_args = torch.stack(distr_args, dim=-1)\n                if distr_args.ndim > 4:\n                    distr_args = distr_args.flatten(-2, -1)\n                y_hat = torch.concat((y_hat, distr_args), axis=-1)                \n        else:\n             y_hat = self._inv_normalization(y_hat=output_batch, \n                                            y_idx=y_idx)\n\n        return y_hat\n```\n\n----------------------------------------\n\nTITLE: Using AutoFEDformer with Custom Configuration and Multiple Backends\nDESCRIPTION: This example demonstrates how to use the AutoFEDformer class with a custom configuration and both default (Ray) and Optuna backends. It shows configuration setup, model initialization, fitting, and prediction with minimal configuration for testing purposes.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_82\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n# Use your own config or AutoFEDFormer.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=64)\nmodel = AutoFEDformer(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n# Optuna\nmodel = AutoFEDformer(h=12, config=None, backend='optuna')\n```\n\n----------------------------------------\n\nTITLE: Training NHITS Model with NeuralForecast on Large Dataset in Python\nDESCRIPTION: This snippet demonstrates how to configure and train an NHITS model using NeuralForecast with the large-scale DataLoader. It sets up model parameters and fits the model on the prepared dataset.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/19_large_datasets.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nhorizon = 12\nstacks = 3\nmodels = [NHITS(input_size=5 * horizon,\n                h=horizon,\n                futr_exog_list=['trend', 'y_[lag12]'],\n                stat_exog_list=['airline1', 'airline2'],\n                max_steps=100,\n                stack_types = stacks*['identity'],\n                n_blocks = stacks*[1],\n                mlp_units = [[256,256] for _ in range(stacks)],\n                n_pool_kernel_size = stacks*[1],\n                interpolation_mode=\"nearest\")]\nnf = NeuralForecast(models=models, freq='ME')\nnf.fit(df=files_list, static_df=static, id_col='id_col')\n```\n\n----------------------------------------\n\nTITLE: Implementing Informer Class for Time Series Forecasting in Python\nDESCRIPTION: Defines the Informer class that inherits from BaseModel. The implementation includes initialization of model parameters, embedding layers, and architectural components. The model features ProbSparse self-attention, attention distilling, and MLP decoder for efficient long sequence forecasting.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.informer.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass Informer(BaseModel):\n    \"\"\" Informer\n\n\tThe Informer model tackles the vanilla Transformer computational complexity challenges for long-horizon forecasting. \n\tThe architecture has three distinctive features:\n        1) A ProbSparse self-attention mechanism with an O time and memory complexity Llog(L).\n        2) A self-attention distilling process that prioritizes attention and efficiently handles long input sequences.\n        3) An MLP multi-step decoder that predicts long time-series sequences in a single forward operation rather than step-by-step.\n\n    The Informer model utilizes a three-component approach to define its embedding:\n        1) It employs encoded autoregressive features obtained from a convolution network.\n        2) It uses window-relative positional embeddings derived from harmonic functions.\n        3) Absolute positional embeddings obtained from calendar features are utilized.\n\n    *Parameters:*<br>\n    `h`: int, forecast horizon.<br>\n    `input_size`: int, maximum sequence length for truncated train backpropagation. <br>\n    `futr_exog_list`: str list, future exogenous columns.<br>\n    `hist_exog_list`: str list, historic exogenous columns.<br>\n    `stat_exog_list`: str list, static exogenous columns.<br>\n    `exclude_insample_y`: bool=False, the model skips the autoregressive features y[t-input_size:t] if True.<br>\n\t`decoder_input_size_multiplier`: float = 0.5, .<br>\n    `hidden_size`: int=128, units of embeddings and encoders.<br>\n    `dropout`: float (0, 1), dropout throughout Informer architecture.<br>\n\t`factor`: int=3, Probsparse attention factor.<br>\n    `n_head`: int=4, controls number of multi-head's attention.<br>\n  \t`conv_hidden_size`: int=32, channels of the convolutional encoder.<br>\n\t`activation`: str=`GELU`, activation from ['ReLU', 'Softplus', 'Tanh', 'SELU', 'LeakyReLU', 'PReLU', 'Sigmoid', 'GELU'].<br>\n    `encoder_layers`: int=2, number of layers for the TCN encoder.<br>\n    `decoder_layers`: int=1, number of layers for the MLP decoder.<br>\n    `distil`: bool = True, wether the Informer decoder uses bottlenecks.<br>\n    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>    \n    `max_steps`: int=1000, maximum number of training steps.<br>\n    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n    `batch_size`: int=32, number of different series in each batch.<br>\n    `valid_batch_size`: int=None, number of different series in each validation and test batch, if None uses batch_size.<br>\n    `windows_batch_size`: int=1024, number of windows to sample in each training batch, default uses all.<br>\n    `inference_windows_batch_size`: int=1024, number of windows to sample in each inference batch.<br>\n    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n    `step_size`: int=1, step size between each window of temporal data.<br>\n    `scaler_type`: str='robust', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n    `random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n    `alias`: str, optional,  Custom name of the model.<br>\n    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>\n    `dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>\n\n\t*References*<br>\n\t- [Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, Wancai Zhang. \"Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting\"](https://arxiv.org/abs/2012.07436)<br>\n    \"\"\"\n    # Class attributes\n    EXOGENOUS_FUTR = True\n    EXOGENOUS_HIST = False\n    EXOGENOUS_STAT = False\n    MULTIVARIATE = False\n    RECURRENT = False\n\n    def __init__(self,\n                 h: int, \n                 input_size: int,\n                 futr_exog_list = None,\n                 hist_exog_list = None,\n                 stat_exog_list = None,\n                 exclude_insample_y = False,\n                 decoder_input_size_multiplier: float = 0.5,\n                 hidden_size: int = 128, \n                 dropout: float = 0.05,\n                 factor: int = 3,\n                 n_head: int = 4,\n                 conv_hidden_size: int = 32,\n                 activation: str = 'gelu',\n                 encoder_layers: int = 2, \n                 decoder_layers: int = 1, \n                 distil: bool = True,\n                 loss = MAE(),\n                 valid_loss = None,\n                 max_steps: int = 5000,\n                 learning_rate: float = 1e-4,\n                 num_lr_decays: int = -1,\n                 early_stop_patience_steps: int =-1,\n                 val_check_steps: int = 100,\n                 batch_size: int = 32,\n                 valid_batch_size: Optional[int] = None,\n                 windows_batch_size = 1024,\n                 inference_windows_batch_size = 1024,\n                 start_padding_enabled = False,\n                 step_size: int = 1,\n                 scaler_type: str = 'identity',\n                 random_seed: int = 1,\n                 drop_last_loader: bool = False,\n                 alias: Optional[str] = None,\n                 optimizer = None,\n                 optimizer_kwargs = None,\n                 lr_scheduler = None,\n                 lr_scheduler_kwargs = None,\n                 dataloader_kwargs = None,\n                 **trainer_kwargs):\n        super(Informer, self).__init__(h=h,\n                                       input_size=input_size,\n                                       hist_exog_list=hist_exog_list,\n                                       stat_exog_list=stat_exog_list,\n                                       futr_exog_list = futr_exog_list,\n                                       exclude_insample_y = exclude_insample_y,\n                                       loss=loss,\n                                       valid_loss=valid_loss,\n                                       max_steps=max_steps,\n                                       learning_rate=learning_rate,\n                                       num_lr_decays=num_lr_decays,\n                                       early_stop_patience_steps=early_stop_patience_steps,\n                                       val_check_steps=val_check_steps,\n                                       batch_size=batch_size,\n                                       valid_batch_size=valid_batch_size,\n                                       windows_batch_size=windows_batch_size,\n                                       inference_windows_batch_size = inference_windows_batch_size,\n                                       start_padding_enabled=start_padding_enabled,\n                                       step_size=step_size,\n                                       scaler_type=scaler_type,\n                                       drop_last_loader=drop_last_loader,\n                                       alias=alias,\n                                       random_seed=random_seed,\n                                       optimizer=optimizer,\n                                       optimizer_kwargs=optimizer_kwargs,\n                                       lr_scheduler=lr_scheduler,\n                                       lr_scheduler_kwargs=lr_scheduler_kwargs,\n                                       dataloader_kwargs=dataloader_kwargs,\n                                       **trainer_kwargs)\n\n        # Architecture\n        self.label_len = int(np.ceil(input_size * decoder_input_size_multiplier))\n        if (self.label_len >= input_size) or (self.label_len <= 0):\n            raise Exception(f'Check decoder_input_size_multiplier={decoder_input_size_multiplier}, range (0,1)')\n\n        if activation not in ['relu', 'gelu']:\n            raise Exception(f'Check activation={activation}')\n        \n        self.c_out = self.loss.outputsize_multiplier\n        self.output_attention = False\n        self.enc_in = 1 \n        self.dec_in = 1\n\n        # Embedding\n        self.enc_embedding = DataEmbedding(c_in=self.enc_in,\n                                           exog_input_size=self.futr_exog_size,\n                                           hidden_size=hidden_size, \n                                           pos_embedding=True,\n                                           dropout=dropout)\n        self.dec_embedding = DataEmbedding(self.dec_in,\n                                           exog_input_size=self.futr_exog_size,\n                                           hidden_size=hidden_size, \n\n```\n\n----------------------------------------\n\nTITLE: Training and Forecasting with NeuralForecast using TSMixer Model\nDESCRIPTION: This snippet demonstrates how to train a TSMixer model using NeuralForecast and predict future values. It includes data preparation, model configuration, training, and visualization of the results.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tsmixer.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import TSMixer\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\nfrom neuralforecast.losses.pytorch import MAE, MQLoss\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = TSMixer(h=12,\n                input_size=24,\n                n_series=2, \n                n_block=4,\n                ff_dim=4,\n                dropout=0,\n                revin=True,\n                scaler_type='standard',\n                max_steps=500,\n                early_stop_patience_steps=-1,\n                val_check_steps=5,\n                learning_rate=1e-3,\n                loss=MQLoss(),\n                batch_size=32\n                )\n\nfcst = NeuralForecast(models=[model], freq='ME')\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\n# Plot predictions\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline2'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['TSMixer-median'], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'][-12:], \n                 y1=plot_df['TSMixer-lo-90'][-12:].values,\n                 y2=plot_df['TSMixer-hi-90'][-12:].values,\n                 alpha=0.4, label='level 90')\nax.set_title('AirPassengers Forecast', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Year', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\n```\n\n----------------------------------------\n\nTITLE: Implementing AutoCorrelation Module in PyTorch\nDESCRIPTION: A PyTorch module that implements the AutoCorrelation mechanism with period-based dependencies discovery and time delay aggregation. It includes three different aggregation methods for training, inference, and full computation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.autoformer.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass AutoCorrelation(nn.Module):\n    def __init__(self, mask_flag=True, factor=1, scale=None, attention_dropout=0.1, output_attention=False):\n        super(AutoCorrelation, self).__init__()\n        self.factor = factor\n        self.scale = scale\n        self.mask_flag = mask_flag\n        self.output_attention = output_attention\n        self.dropout = nn.Dropout(attention_dropout)\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-head Attention in PyTorch\nDESCRIPTION: Implementation of multi-head attention mechanism that linearly projects queries, keys, and values and applies scaled dot-product attention. Supports residual attention connections and local self-attention for improved performance.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.patchtst.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nclass _MultiheadAttention(nn.Module):\n    \"\"\"\n    _MultiheadAttention\n    \"\"\"       \n    def __init__(self, hidden_size, n_heads, d_k=None, d_v=None,\n                 res_attention=False, attn_dropout=0., proj_dropout=0., qkv_bias=True, lsa=False):\n        \"\"\"\n        Multi Head Attention Layer\n        Input shape:\n            Q:       [batch_size (bs) x max_q_len x hidden_size]\n            K, V:    [batch_size (bs) x q_len x hidden_size]\n            mask:    [q_len x q_len]\n        \"\"\"\n        super().__init__()\n        d_k = hidden_size // n_heads if d_k is None else d_k\n        d_v = hidden_size // n_heads if d_v is None else d_v\n\n        self.n_heads, self.d_k, self.d_v = n_heads, d_k, d_v\n\n        self.W_Q = nn.Linear(hidden_size, d_k * n_heads, bias=qkv_bias)\n        self.W_K = nn.Linear(hidden_size, d_k * n_heads, bias=qkv_bias)\n        self.W_V = nn.Linear(hidden_size, d_v * n_heads, bias=qkv_bias)\n\n        # Scaled Dot-Product Attention (multiple heads)\n        self.res_attention = res_attention\n        self.sdp_attn = _ScaledDotProductAttention(hidden_size, n_heads, attn_dropout=attn_dropout,\n                                                   res_attention=self.res_attention, lsa=lsa)\n\n        # Poject output\n        self.to_out = nn.Sequential(nn.Linear(n_heads * d_v, hidden_size), nn.Dropout(proj_dropout))\n\n    def forward(self, Q:torch.Tensor, K:Optional[torch.Tensor]=None, V:Optional[torch.Tensor]=None, prev:Optional[torch.Tensor]=None,\n                key_padding_mask:Optional[torch.Tensor]=None, attn_mask:Optional[torch.Tensor]=None):\n\n        bs = Q.size(0)\n        if K is None: K = Q\n        if V is None: V = Q\n\n        # Linear (+ split in multiple heads)\n        q_s = self.W_Q(Q).view(bs, -1, self.n_heads, self.d_k).transpose(1,2)       # q_s    : [bs x n_heads x max_q_len x d_k]\n        k_s = self.W_K(K).view(bs, -1, self.n_heads, self.d_k).permute(0,2,3,1)     # k_s    : [bs x n_heads x d_k x q_len] - transpose(1,2) + transpose(2,3)\n        v_s = self.W_V(V).view(bs, -1, self.n_heads, self.d_v).transpose(1,2)       # v_s    : [bs x n_heads x q_len x d_v]\n\n        # Apply Scaled Dot-Product Attention (multiple heads)\n        if self.res_attention:\n            output, attn_weights, attn_scores = self.sdp_attn(q_s, k_s, v_s,\n                                                    prev=prev, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n        else:\n            output, attn_weights = self.sdp_attn(q_s, k_s, v_s, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n        # output: [bs x n_heads x q_len x d_v], attn: [bs x n_heads x q_len x q_len], scores: [bs x n_heads x max_q_len x q_len]\n\n        # back to the original inputs dimensions\n        output = output.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * self.d_v) # output: [bs x q_len x n_heads * d_v]\n        output = self.to_out(output)\n\n        if self.res_attention: return output, attn_weights, attn_scores\n        else: return output, attn_weights\n```\n\n----------------------------------------\n\nTITLE: Main StemGNN Model Class Implementation\nDESCRIPTION: Primary StemGNN model class that inherits from BaseModel and implements the complete architecture with configuration parameters and model attributes.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.stemgnn.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass StemGNN(BaseModel):\n    \"\"\" StemGNN\n\n    The Spectral Temporal Graph Neural Network (`StemGNN`) is a Graph-based multivariate\n    time-series forecasting model. `StemGNN` jointly learns temporal dependencies and\n    inter-series correlations in the spectral domain, by combining Graph Fourier Transform (GFT)\n    and Discrete Fourier Transform (DFT). \n\n    **Parameters:**<br>\n    `h`: int, Forecast horizon. <br>\n    `input_size`: int, autorregresive inputs size, y=[1,2,3,4] input_size=2 -> y_[t-2:t]=[1,2].<br>\n    `n_series`: int, number of time-series.<br>\n    `futr_exog_list`: str list, future exogenous columns.<br>\n    `hist_exog_list`: str list, historic exogenous columns.<br>\n    `stat_exog_list`: str list, static exogenous columns.<br>\n    \"\"\"\n    # Class attributes\n    EXOGENOUS_FUTR = False\n    EXOGENOUS_HIST = False\n    EXOGENOUS_STAT = False    \n    MULTIVARIATE = True    # If the model produces multivariate forecasts (True) or univariate (False)\n    RECURRENT = False       # If the model produces forecasts recursively (True) or direct (False)\n```\n\n----------------------------------------\n\nTITLE: Implementing FeatureMixing Layer for TSMixerx in Python\nDESCRIPTION: Defines the FeatureMixing class, which performs feature mixing using two linear layers, dropout, and layer normalization. It can also project residuals if input and output feature dimensions differ.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tsmixerx.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass FeatureMixing(nn.Module):\n    \"\"\" \n    FeatureMixing\n    \"\"\"       \n    def __init__(self, in_features, out_features, h, dropout, ff_dim):\n        super().__init__()\n        self.feature_lin_1 = nn.Linear(in_features=in_features, \n                                       out_features=ff_dim)\n        self.feature_lin_2 = nn.Linear(in_features=ff_dim, \n                                       out_features=out_features)\n        self.feature_drop_1 = nn.Dropout(p=dropout)\n        self.feature_drop_2 = nn.Dropout(p=dropout)\n        self.linear_project_residual = False\n        if in_features != out_features:\n            self.project_residual = nn.Linear(in_features = in_features,\n                                        out_features = out_features)\n            self.linear_project_residual = True\n\n        self.feature_norm = nn.LayerNorm(normalized_shape=(h, out_features))\n\n    def forward(self, input):\n        x = F.relu(self.feature_lin_1(input))                           # [B, h, C_in] -> [B, h, ff_dim]\n        x = self.feature_drop_1(x)                                      # [B, h, ff_dim] -> [B, h, ff_dim]\n        x = self.feature_lin_2(x)                                       # [B, h, ff_dim] -> [B, h, C_out]\n        x = self.feature_drop_2(x)                                      # [B, h, C_out] -> [B, h, C_out]\n        if self.linear_project_residual:\n            input = self.project_residual(input)                        # [B, h, C_in] -> [B, h, C_out]\n\n        return self.feature_norm(x + input)\n```\n\n----------------------------------------\n\nTITLE: Forecasting Air Passenger Data with NBEATS and NeuralForecast in Python\nDESCRIPTION: This Python script demonstrates a complete time series forecasting workflow using the NeuralForecast library. It imports necessary libraries (pandas, matplotlib, NeuralForecast components), loads and splits the AirPassengers dataset, configures an NBEATS model with specific parameters like horizon, input size, basis functions, and Poisson distribution loss for probabilistic forecasting. It then initializes the NeuralForecast trainer, fits the model to the training data, predicts future values, and finally plots the historical data, median forecast, and the 90% confidence interval.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeats.ipynb#2025-04-23_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n#| eval: false\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import NBEATS\nfrom neuralforecast.losses.pytorch import DistributionLoss\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = NBEATS(h=12, input_size=24,\n               basis='changepoint',\n               n_basis=2,\n               loss=DistributionLoss(distribution='Poisson', level=[80, 90]),\n               stack_types = ['identity', 'trend', 'seasonality'],\n               max_steps=100,\n               val_check_steps=10,\n               early_stop_patience_steps=2)\n\nfcst = NeuralForecast(\n    models=[model],\n    freq='ME'\n)\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\n# Plot quantile predictions\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['NBEATS-median'], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'][-12:], \n                 y1=plot_df['NBEATS-lo-90'][-12:].values, \n                 y2=plot_df['NBEATS-hi-90'][-12:].values,\n                 alpha=0.4, label='level 90')\nplt.grid()\nplt.legend()\nplt.plot()\n```\n\n----------------------------------------\n\nTITLE: Implementing Huberized Multi-Quantile Loss in PyTorch\nDESCRIPTION: A PyTorch implementation of Huberized Multi-Quantile Loss that extends the Huberized Quantile Loss to handle multiple quantiles simultaneously. This loss function is useful for probabilistic forecasting when prediction intervals are required.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_71\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass HuberMQLoss(BasePointLoss):\n    \"\"\"  Huberized Multi-Quantile loss\n\n    The Huberized Multi-Quantile loss (HuberMQL) is a modified version of the multi-quantile loss function \n    that combines the advantages of the quantile loss and the Huber loss. HuberMQL is commonly used in regression \n    tasks, especially when dealing with data that contains outliers or heavy tails. The loss function pays \n    more attention to under/over-estimation depending on the quantile list $[q_{1},q_{2},\\dots]$ parameter. \n    It controls the trade-off between robustness and prediction accuracy with the parameter $\\\\delta$.\n\n    $$ \\mathrm{HuberMQL}_{\\delta}(\\\\mathbf{y}_{\\\\tau},[\\\\mathbf{\\hat{y}}^{(q_{1})}_{\\\\tau}, ... ,\\hat{y}^{(q_{n})}_{\\\\tau}]) = \n    \\\\frac{1}{n} \\\\sum_{q_{i}} \\mathrm{HuberQL}_{\\\\delta}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{(q_{i})}_{\\\\tau}) $$\n\n    **Parameters:**<br>\n    `level`: int list [0,100]. Probability levels for prediction intervals (Defaults median).\n    `quantiles`: float list [0., 1.]. Alternative to level, quantiles to estimate from y distribution.\n    `delta`: float=1.0, Specifies the threshold at which to change between delta-scaled L1 and L2 loss.<br>   \n    `horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br> \n\n    **References:**<br>\n    [Huber Peter, J (1964). \"Robust Estimation of a Location Parameter\". Annals of Statistics](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-35/issue-1/Robust-Estimation-of-a-Location-Parameter/10.1214/aoms/1177703732.full)<br>\n    [Roger Koenker and Gilbert Bassett, Jr., \"Regression Quantiles\".](https://www.jstor.org/stable/1913643)\n    \"\"\"\n    def __init__(self, level=[80, 90], quantiles=None, delta: float=1.0, horizon_weight=None):\n\n        qs, output_names = level_to_outputs(level)\n        qs = torch.Tensor(qs)\n        # Transform quantiles to homogeneus output names\n        if quantiles is not None:\n            _, output_names = quantiles_to_outputs(quantiles)\n            qs = torch.Tensor(quantiles)\n\n        super(HuberMQLoss, self).__init__(horizon_weight=horizon_weight,\n                                     outputsize_multiplier=len(qs),\n                                     output_names=output_names)\n        \n        self.quantiles = torch.nn.Parameter(qs, requires_grad=False)\n        self.delta = delta\n\n    def domain_map(self, y_hat: torch.Tensor):\n        \"\"\"\n        Input:\n        Univariate: [B, H, 1 * Q]\n        Multivariate: [B, H, N * Q]\n\n        Output: [B, H, N, Q]\n        \"\"\"\n        output = y_hat.reshape(y_hat.shape[0],\n                               y_hat.shape[1],\n                               -1,\n                               self.outputsize_multiplier)\n\n        return output\n    \n    def _compute_weights(self, y, mask):\n        \"\"\"\n        Compute final weights for each datapoint (based on all weights and all masks)\n        Set horizon_weight to a ones[H] tensor if not set.\n        If set, check that it has the same length as the horizon in x.\n        \"\"\"\n\n        if self.horizon_weight is None:\n            weights = torch.ones_like(mask)\n        else:\n            assert mask.shape[1] == len(self.horizon_weight), \\\n                'horizon_weight must have same length as Y'       \n            weights = self.horizon_weight.clone()\n            weights = weights[None, :, None, None].to(mask.device)\n            weights = torch.ones_like(mask, device=mask.device) * weights\n        \n        return weights * mask\n\n    def __call__(self,\n                 y: torch.Tensor,\n                 y_hat: torch.Tensor,\n                 y_insample: torch.Tensor,\n                 mask: Union[torch.Tensor, None] = None,\n                 ) -> torch.Tensor:\n        \"\"\"\n        **Parameters:**<br>\n        `y`: tensor, Actual values.<br>\n        `y_hat`: tensor, Predicted values.<br>\n        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n\n        **Returns:**<br>\n        `hmqloss`: tensor (single value).\n        \"\"\"\n        # [B, h, N] -> [B, h, N, 1]\n        if y_hat.ndim == 3:\n            y_hat = y_hat.unsqueeze(-1)\n\n        y = y.unsqueeze(-1)        \n        if mask is not None:\n            mask = mask.unsqueeze(-1)\n        else:\n            mask = torch.ones_like(y, device=y.device)\n        \n        error  = y_hat - y\n        \n        zero_error = torch.zeros_like(error)        \n        sq     = torch.maximum(-error, torch.zeros_like(error))\n        s1_q   = torch.maximum(error, torch.zeros_like(error))\n        \n        quantiles = self.quantiles[None, None, None, :]\n        losses = F.huber_loss(quantiles * sq, zero_error, \n                                        reduction='none', delta=self.delta) + \\\n                  F.huber_loss((1 - quantiles) * s1_q, zero_error, \n                                reduction='none', delta=self.delta)\n        losses = (1 / len(quantiles)) * losses\n\n        weights = self._compute_weights(y=losses, mask=mask) \n\n        return _weighted_mean(losses=losses, weights=weights)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for GRU Implementation\nDESCRIPTION: Imports necessary Python libraries for implementing the GRU model, including PyTorch for neural networks and typing utilities for type annotations.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.gru.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n#| export\nimport warnings\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\n\nfrom neuralforecast.losses.pytorch import MAE\nfrom neuralforecast.common._base_model import BaseModel\nfrom neuralforecast.common._modules import MLP\n```\n\n----------------------------------------\n\nTITLE: Implementing MASE Loss Function in PyTorch\nDESCRIPTION: Mean Absolute Scaled Error implementation that compares prediction accuracy against a seasonal naive model baseline. Requires seasonality parameter and supports horizon weighting.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass MASE(BasePointLoss):\n    def __init__(self, seasonality: int, horizon_weight=None):\n        super(MASE, self).__init__(horizon_weight=horizon_weight,\n                                   outputsize_multiplier=1,\n                                   output_names=[''])\n        self.seasonality = seasonality\n\n    def __call__(self,\n                 y: torch.Tensor,\n                 y_hat: torch.Tensor,\n                 y_insample: torch.Tensor,\n                 mask: Union[torch.Tensor, None] = None,\n                 ) -> torch.Tensor:\n        delta_y = torch.abs(y - y_hat)\n        scale = torch.mean(torch.abs(y_insample[:, self.seasonality:] - \\\n                                     y_insample[:, :-self.seasonality]), axis=1)\n        losses = _divide_no_nan(delta_y, scale[:, None, None])\n        weights = self._compute_weights(y=y, mask=mask)\n        return _weighted_mean(losses=losses, weights=weights)\n```\n\n----------------------------------------\n\nTITLE: Initializing DistributionLoss Class for Probabilistic Forecasting in PyTorch\nDESCRIPTION: Initializes a DistributionLoss object that creates a probabilistic loss function based on various distribution types. It handles quantile specification, parameter mapping, and distribution setup with support for various distribution types including Normal, Poisson, StudentT, NegativeBinomial, Tweedie, and ISQF distributions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_36\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(self, distribution='Normal', level=None, quantiles=None,\n                 num_samples=1000, return_params=False, horizon_weight = None, **distribution_kwargs):\n       super(DistributionLoss, self).__init__()\n\n       qs, self.output_names = level_to_outputs(level)\n       qs = torch.Tensor(qs)\n\n        # Transform quantiles to homogeneus output names\n       if quantiles is not None:\n              quantiles = sorted(quantiles)\n              _, self.output_names = quantiles_to_outputs(quantiles)\n              qs = torch.Tensor(quantiles)\n       self.quantiles = torch.nn.Parameter(qs, requires_grad=False)\n       num_qk = len(self.quantiles)\n\n       # Generate a horizon weight tensor from the array\n       if horizon_weight is not None:\n           horizon_weight = torch.Tensor(horizon_weight.flatten())\n       self.horizon_weight = horizon_weight\n\n\n       if \"num_pieces\" not in distribution_kwargs:\n            num_pieces = 5\n       else:\n            num_pieces = distribution_kwargs.pop(\"num_pieces\")\n\n       available_distributions = dict(\n                          Bernoulli=Bernoulli,\n                          Normal=Normal,\n                          Poisson=Poisson,\n                          StudentT=StudentT,\n                          NegativeBinomial=NegativeBinomial,\n                          Tweedie=Tweedie,\n                          ISQF=ISQF)\n       scale_decouples = dict(\n                          Bernoulli=bernoulli_scale_decouple,\n                          Normal=normal_scale_decouple,\n                          Poisson=poisson_scale_decouple,\n                          StudentT=student_scale_decouple,\n                          NegativeBinomial=nbinomial_scale_decouple,\n                          Tweedie=tweedie_scale_decouple,\n                          ISQF=isqf_scale_decouple)\n       param_names = dict(Bernoulli=[\"-logits\"],\n                          Normal=[\"-loc\", \"-scale\"],\n                          Poisson=[\"-loc\"],\n                          StudentT=[\"-df\", \"-loc\", \"-scale\"],\n                          NegativeBinomial=[\"-total_count\", \"-logits\"],\n                          Tweedie=[\"-log_mu\"],\n                          ISQF=[f\"-spline_knot_{i + 1}\" for i in range((num_qk - 1) * num_pieces)] + \\\n                               [f\"-spline_height_{i + 1}\" for i in range((num_qk - 1) * num_pieces)] + \\\n                               [\"-beta_l\", \"-beta_r\"] + \\\n                               [f\"-quantile_knot_{i + 1}\" for i in range(num_qk)],\n                          )\n       assert (distribution in available_distributions.keys()), f'{distribution} not available'\n       if distribution == 'ISQF':\n            quantiles = torch.sort(qs).values\n            self.domain_map = partial(isqf_domain_map, \n                                       quantiles=quantiles, \n                                       num_pieces=num_pieces)\n            if return_params:\n               raise Exception(\"ISQF does not support 'return_params=True'\")                 \n       elif distribution == 'Tweedie':\n            rho = distribution_kwargs.pop(\"rho\")\n            self.domain_map = partial(tweedie_domain_map,\n                                      rho=rho)\n            if return_params:\n               raise Exception(\"Tweedie does not support 'return_params=True'\")                 \n       else:\n            self.domain_map = self._domain_map\n\n       self.distribution = distribution\n       self._base_distribution = available_distributions[distribution]\n       self.scale_decouple = scale_decouples[distribution]\n       self.distribution_kwargs = distribution_kwargs\n       self.num_samples = num_samples      \n       self.param_names = param_names[distribution]\n\n       # If True, predict_step will return Distribution's parameters\n       self.return_params = return_params\n       if self.return_params:\n            self.output_names = self.output_names + self.param_names\n\n       # Add first output entry for the sample_mean\n       self.output_names.insert(0, \"\")\n\n       self.outputsize_multiplier = len(self.param_names)\n       self.is_distribution_output = True\n       self.has_predicted = False\n```\n\n----------------------------------------\n\nTITLE: TiDE Forward Pass Implementation in PyTorch\nDESCRIPTION: Forward pass implementation of the TiDE model that processes input windows containing historical values, exogenous variables, and generates forecasts using multiple neural network layers and skip connections.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tide.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, windows_batch):\n    # Parse windows_batch\n    x             = windows_batch['insample_y']                     #   [B, L, 1]\n    hist_exog     = windows_batch['hist_exog']                      #   [B, L, X]\n    futr_exog     = windows_batch['futr_exog']                      #   [B, L + h, F]\n    stat_exog     = windows_batch['stat_exog']                      #   [B, S]\n    batch_size, seq_len = x.shape[:2]                               #   B = batch_size, L = seq_len\n\n    # Flatten insample_y\n    x = x.reshape(batch_size, -1)                                   #   [B, L, 1] -> [B, L]\n\n    # Global skip connection\n    x_skip = self.global_skip(x)                                    #   [B, L] -> [B, h * n_outputs]\n    x_skip = x_skip.reshape(batch_size, self.h, -1)                 #   [B, h * n_outputs] -> [B, h, n_outputs]\n\n    # Process and concatenate various inputs\n    if self.hist_exog_size > 0:\n        x_hist_exog = self.hist_exog_projection(hist_exog)\n        x_hist_exog = x_hist_exog.reshape(batch_size, -1)\n        x = torch.cat((x, x_hist_exog), dim=1)\n\n    if self.futr_exog_size > 0:\n        x_futr_exog = self.futr_exog_projection(futr_exog)\n        x_futr_exog_flat = x_futr_exog.reshape(batch_size, -1)\n        x = torch.cat((x, x_futr_exog_flat), dim=1)\n\n    if self.stat_exog_size > 0:\n        x = torch.cat((x, stat_exog), dim=1)\n\n    # Dense encoder-decoder\n    x = self.dense_encoder(x)\n    x = self.dense_decoder(x)\n    x = x.reshape(batch_size, self.h, -1)\n\n    # Process future exogenous variables\n    if self.futr_exog_size > 0:\n        x_futr_exog_h = x_futr_exog[:, seq_len:]\n        x = torch.cat((x, x_futr_exog_h), dim=2)\n\n    # Temporal decoder and skip connection\n    x = self.temporal_decoder(x)\n    forecast = x + x_skip\n    \n    return forecast\n```\n\n----------------------------------------\n\nTITLE: StockBlockLayer Implementation for StemGNN\nDESCRIPTION: Core processing block of StemGNN that handles spectral-temporal convolutions and transformations using FFT and graph operations.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.stemgnn.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass StockBlockLayer(nn.Module):\n    \"\"\"\n    StockBlockLayer\n    \"\"\"       \n    def __init__(self, time_step, unit, multi_layer, stack_cnt=0):\n        super(StockBlockLayer, self).__init__()\n        self.time_step = time_step\n        self.unit = unit\n        self.stack_cnt = stack_cnt\n        self.multi = multi_layer\n        self.weight = nn.Parameter(\n            torch.Tensor(1, 3 + 1, 1, self.time_step * self.multi,\n                         self.multi * self.time_step))  # [K+1, 1, in_c, out_c]\n        nn.init.xavier_normal_(self.weight)\n        self.forecast = nn.Linear(self.time_step * self.multi, self.time_step * self.multi)\n        self.forecast_result = nn.Linear(self.time_step * self.multi, self.time_step)\n        if self.stack_cnt == 0:\n            self.backcast = nn.Linear(self.time_step * self.multi, self.time_step)\n        self.backcast_short_cut = nn.Linear(self.time_step, self.time_step)\n        self.relu = nn.ReLU()\n        self.GLUs = nn.ModuleList()\n        self.output_channel = 4 * self.multi\n        for i in range(3):\n            if i == 0:\n                self.GLUs.append(GLU(self.time_step * 4, self.time_step * self.output_channel))\n                self.GLUs.append(GLU(self.time_step * 4, self.time_step * self.output_channel))\n            elif i == 1:\n                self.GLUs.append(GLU(self.time_step * self.output_channel, self.time_step * self.output_channel))\n                self.GLUs.append(GLU(self.time_step * self.output_channel, self.time_step * self.output_channel))\n            else:\n                self.GLUs.append(GLU(self.time_step * self.output_channel, self.time_step * self.output_channel))\n                self.GLUs.append(GLU(self.time_step * self.output_channel, self.time_step * self.output_channel))\n```\n\n----------------------------------------\n\nTITLE: Initializing PatchTST Model with Forward Pass\nDESCRIPTION: Core implementation of PatchTST model initialization and forward pass logic. Sets up model parameters and handles the transformation of input windows batch to generate forecasts.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.patchtst.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nmax_seq_len = 1024        # Not used\nkey_padding_mask = 'auto' # Not used\npadding_var = None        # Not used\nattn_mask = None          # Not used\n\nself.model = PatchTST_backbone(c_in=c_in, c_out=c_out, input_size=input_size, h=h, patch_len=patch_len, stride=stride, \n                        max_seq_len=max_seq_len, n_layers=encoder_layers, hidden_size=hidden_size,\n                        n_heads=n_heads, d_k=d_k, d_v=d_v, linear_hidden_size=linear_hidden_size, norm=norm, attn_dropout=attn_dropout,\n                        dropout=dropout, act=activation, key_padding_mask=key_padding_mask, padding_var=padding_var, \n                        attn_mask=attn_mask, res_attention=res_attention, pre_norm=batch_normalization, store_attn=store_attn,\n                        pe=pe, learn_pe=learn_pos_embed, fc_dropout=fc_dropout, head_dropout=head_dropout, padding_patch = padding_patch,\n                        pretrain_head=pretrain_head, head_type=head_type, individual=individual, revin=revin, affine=revin_affine,\n                        subtract_last=revin_subtract_last)\n\n\ndef forward(self, windows_batch):  # x: [batch, input_size]\n    # Parse windows_batch\n    x    = windows_batch['insample_y']\n    x = x.permute(0,2,1)    # x: [Batch, 1, input_size]\n    x = self.model(x)\n    forecast = x.reshape(x.shape[0], self.h, -1) # x: [Batch, h, c_out]\n    \n    return forecast\n```\n\n----------------------------------------\n\nTITLE: Defining and Using NBEATS Stack and Forward Methods in PyTorch - Python\nDESCRIPTION: Implements the NBEATS neural forecasting model stack creation and forward propagation logic. This snippet includes error handling, deprecation warnings, dynamic stack creation for different types (trend, seasonality, identity), and the core forward method for processing input batched time series (windows_batch). Depends on PyTorch and assumes appropriate import and class/context setup. Inputs are windows_batch dicts with insample_y and insample_mask; outputs vary depending on decompose_forecast flag (either block-level or aggregate forecasts). Correct operation requires external basis and block classes (e.g., SeasonalityBasis, TrendBasis, NBEATSBlock).\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeats.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n                                     start_padding_enabled=start_padding_enabled,\n                                     step_size=step_size,\n                                     scaler_type=scaler_type,\n                                     drop_last_loader=drop_last_loader,\n                                     alias=alias,\n                                     random_seed=random_seed,\n                                     optimizer=optimizer,\n                                     optimizer_kwargs=optimizer_kwargs,\n                                     lr_scheduler=lr_scheduler,\n                                     lr_scheduler_kwargs=lr_scheduler_kwargs,\n                                     dataloader_kwargs=dataloader_kwargs,\n                                     **trainer_kwargs)\n\n        # Raise deprecation warning\n        if n_polynomials is not None:\n            warnings.warn(\n                \"The parameter n_polynomials will be deprecated in favor of n_basis and basis and it is currently ignored.\\n\"\n                \"The basis parameter defines the basis function to be used in the trend stack.\\n\"\n                \"The n_basis defines the degree of the basis function used in the trend stack.\",\n                DeprecationWarning\n            )\n        \n        # Architecture\n        blocks = self.create_stack(h=h,\n                                   input_size=input_size,\n                                   stack_types=stack_types, \n                                   n_blocks=n_blocks,\n                                   mlp_units=mlp_units,\n                                   dropout_prob_theta=dropout_prob_theta,\n                                   activation=activation,\n                                   shared_weights=shared_weights,\n                                   n_harmonics=n_harmonics,\n                                   n_basis=n_basis,\n                                   basis_type=basis)\n        self.blocks = torch.nn.ModuleList(blocks)\n\n    def create_stack(self, \n                     stack_types, \n                     n_blocks, \n                     input_size, \n                     h, \n                     mlp_units, \n                     dropout_prob_theta, \n                     activation, \n                     shared_weights,\n                     n_harmonics, \n                     n_basis, \n                     basis_type):                     \n\n        block_list = []\n        for i in range(len(stack_types)):\n            for block_id in range(n_blocks[i]):\n\n                # Shared weights\n                if shared_weights and block_id>0:\n                    nbeats_block = block_list[-1]\n                else:\n                    if stack_types[i] == 'seasonality':\n                        n_theta = 2 * (self.loss.outputsize_multiplier + 1) * \\\n                                  int(np.ceil(n_harmonics / 2 * h) - (n_harmonics - 1))\n                        basis = SeasonalityBasis(harmonics=n_harmonics,\n                                                 backcast_size=input_size,\n                                                 forecast_size=h,\n                                                 out_features=self.loss.outputsize_multiplier)\n\n                    elif stack_types[i] == 'trend':\n                        n_theta = (self.loss.outputsize_multiplier + 1) * (n_basis + 1)\n                        basis = TrendBasis(n_basis=n_basis,\n                                           backcast_size=input_size,\n                                           forecast_size=h,\n                                           out_features=self.loss.outputsize_multiplier,\n                                           basis=basis_type)\n\n                    elif stack_types[i] == 'identity':\n                        n_theta = input_size + self.loss.outputsize_multiplier * h\n                        basis = IdentityBasis(backcast_size=input_size, forecast_size=h,\n                                              out_features=self.loss.outputsize_multiplier)\n                    else:\n                        raise ValueError(f'Block type {stack_types[i]} not found!')\n\n                    nbeats_block = NBEATSBlock(input_size=input_size,\n                                               n_theta=n_theta,\n                                               mlp_units=mlp_units,\n                                               basis=basis,\n                                               dropout_prob=dropout_prob_theta,\n                                               activation=activation)\n\n                # Select type of evaluation and apply it to all layers of block\n                block_list.append(nbeats_block)\n                \n        return block_list\n\n    def forward(self, windows_batch):\n        \n        # Parse windows_batch\n        insample_y    = windows_batch['insample_y'].squeeze(-1)\n        insample_mask = windows_batch['insample_mask'].squeeze(-1)\n\n        # NBEATS' forward\n        residuals = insample_y.flip(dims=(-1,)) # backcast init\n        insample_mask = insample_mask.flip(dims=(-1,))\n        \n        forecast = insample_y[:, -1:, None] # Level with Naive1\n        block_forecasts = [ forecast.repeat(1, self.h, 1) ]\n        for i, block in enumerate(self.blocks):\n            backcast, block_forecast = block(insample_y=residuals)\n            residuals = (residuals - backcast) * insample_mask\n            forecast = forecast + block_forecast\n\n            if self.decompose_forecast:\n                block_forecasts.append(block_forecast)               \n\n        if self.decompose_forecast:\n            # (n_batch, n_blocks, h, out_features)\n            block_forecasts = torch.stack(block_forecasts)\n            block_forecasts = block_forecasts.permute(1,0,2,3)\n            block_forecasts = block_forecasts.squeeze(-1) # univariate output\n            return block_forecasts\n        else:\n            return forecast\n```\n\n----------------------------------------\n\nTITLE: Usage Example of Informer Model with NeuralForecast Framework\nDESCRIPTION: Example demonstrating how to use the Informer model with the NeuralForecast framework. The code shows initialization, training, and prediction steps on the AirPassengers dataset, including visualization of results.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.informer.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n#| eval: false\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import Informer\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic, augment_calendar_df\n\nAirPassengersPanel, calendar_cols = augment_calendar_df(df=AirPassengersPanel, freq='M')\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = Informer(h=12,\n                 input_size=24,\n                 hidden_size = 16,\n                 conv_hidden_size = 32,\n                 n_head = 2,\n                 loss=MAE(),\n                 futr_exog_list=calendar_cols,\n                 scaler_type='robust',\n                 learning_rate=1e-3,\n                 max_steps=200,\n                 val_check_steps=50,\n                 early_stop_patience_steps=2)\n\nnf = NeuralForecast(\n    models=[model],\n    freq='ME'\n)\nnf.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nforecasts = nf.predict(futr_df=Y_test_df)\n\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nif model.loss.is_distribution_output:\n    plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n    plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n    plt.plot(plot_df['ds'], plot_df['Informer-median'], c='blue', label='median')\n    plt.fill_between(x=plot_df['ds'][-12:], \n                    y1=plot_df['Informer-lo-90'][-12:].values, \n                    y2=plot_df['Informer-hi-90'][-12:].values,\n                    alpha=0.4, label='level 90')\n    plt.grid()\n    plt.legend()\n    plt.plot()\nelse:\n    plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n    plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n    plt.plot(plot_df['ds'], plot_df['Informer'], c='blue', label='Forecast')\n    plt.legend()\n    plt.grid()\n```\n\n----------------------------------------\n\nTITLE: Time Series Forecasting with DLinear Model in Python\nDESCRIPTION: Demonstrates the complete workflow of time series forecasting using NeuralForecast library. The code loads the Air Passengers dataset, prepares training and test data, configures and trains a DLinear model, makes predictions, and visualizes the results with matplotlib. Includes handling for both distribution and point forecasts.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.dlinear.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import DLinear\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic, augment_calendar_df\n\nAirPassengersPanel, calendar_cols = augment_calendar_df(df=AirPassengersPanel, freq='M')\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = DLinear(h=12,\n                 input_size=24,\n                 loss=MAE(),\n                 scaler_type='robust',\n                 learning_rate=1e-3,\n                 max_steps=500,\n                 val_check_steps=50,\n                 early_stop_patience_steps=2)\n\nnf = NeuralForecast(\n    models=[model],\n    freq='ME'\n)\nnf.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nforecasts = nf.predict(futr_df=Y_test_df)\n\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nif model.loss.is_distribution_output:\n    plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n    plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n    plt.plot(plot_df['ds'], plot_df['DLinear-median'], c='blue', label='median')\n    plt.fill_between(x=plot_df['ds'][-12:], \n                    y1=plot_df['DLinear-lo-90'][-12:].values, \n                    y2=plot_df['DLinear-hi-90'][-12:].values,\n                    alpha=0.4, label='level 90')\n    plt.grid()\n    plt.legend()\n    plt.plot()\nelse:\n    plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n    plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n    plt.plot(plot_df['ds'], plot_df['DLinear'], c='blue', label='Forecast')\n    plt.legend()\n    plt.grid()\n```\n\n----------------------------------------\n\nTITLE: Implementing Relative Mean Squared Error (relMSE) in PyTorch\nDESCRIPTION: A PyTorch loss function class that implements Relative Mean Squared Error as proposed by Hyndman & Koehler (2006). It compares model predictions against a benchmark by computing the ratio of their respective MSE values. Includes numerical stability handling and optional horizon weighting.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass relMSE(BasePointLoss):\n    def __init__(self, y_train=None, horizon_weight=None):\n        super(relMSE, self).__init__(horizon_weight=horizon_weight,\n                                     outputsize_multiplier=1,\n                                     output_names=[''])\n        if y_train is not None:\n            raise DeprecationWarning(\"y_train will be deprecated in a future release.\")\n        self.mse = MSE(horizon_weight=horizon_weight)\n\n    def __call__(self,\n                 y: torch.Tensor,\n                 y_hat: torch.Tensor,\n                 y_benchmark: torch.Tensor,\n                 mask: Union[torch.Tensor, None] = None\n                 ) -> torch.Tensor:\n        norm = self.mse(y=y, y_hat=y_benchmark, mask=mask) # Already weighted\n        norm = norm + 1e-5 # Numerical stability\n        loss = self.mse(y=y, y_hat=y_hat, mask=mask) # Already weighted\n        loss = _divide_no_nan(loss, norm)\n        return loss\n```\n\n----------------------------------------\n\nTITLE: Creating Autoformer Encoder Components\nDESCRIPTION: Implementation of the Autoformer encoder architecture with progressive decomposition, including attention layers and convolutional processing.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.autoformer.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass EncoderLayer(nn.Module):\n    def __init__(self, attention, hidden_size, conv_hidden_size=None, MovingAvg=25, dropout=0.1, activation=\"relu\"):\n        super(EncoderLayer, self).__init__()\n        conv_hidden_size = conv_hidden_size or 4 * hidden_size\n        self.attention = attention\n        self.conv1 = nn.Conv1d(in_channels=hidden_size, out_channels=conv_hidden_size, kernel_size=1, bias=False)\n        self.conv2 = nn.Conv1d(in_channels=conv_hidden_size, out_channels=hidden_size, kernel_size=1, bias=False)\n        self.decomp1 = SeriesDecomp(MovingAvg)\n        self.decomp2 = SeriesDecomp(MovingAvg)\n        self.dropout = nn.Dropout(dropout)\n        self.activation = F.relu if activation == \"relu\" else F.gelu\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Predictive Maintenance\nDESCRIPTION: Imports necessary Python libraries including neuralforecast models, datasetsforecast for the PHM2008 dataset, and visualization tools for RUL prediction analysis.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/use-cases/predictive_maintenance.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nplt.rcParams['font.family'] = 'serif'\n\nfrom neuralforecast.models import NBEATSx\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.losses.pytorch import HuberLoss\n\nfrom datasetsforecast.phm2008 import PHM2008\n```\n\n----------------------------------------\n\nTITLE: Visualizing Attention-Weighted Past Variable Importance\nDESCRIPTION: Creates a stacked bar chart showing past variable importance weighted by attention at each time step. This combines information about both variable importance and attention weights to provide a more comprehensive view of feature influence.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_32\n\nLANGUAGE: python\nCODE:\n```\n# | eval: false\ndf = feature_importances[\"Past variable importance over time\"]\nmean_attention = (\n    nf.models[0]\n    .attention_weights()[nf.models[0].input_size :, :]\n    .mean(axis=0)[: nf.models[0].input_size]\n)\ndf = df.multiply(mean_attention, axis=0)\n\nfig, ax = plt.subplots(figsize=(20, 10))\nbottom = np.zeros(len(df.index))\n\nfor col in df.columns:\n    p = ax.bar(np.arange(-len(df), 0), df[col].values, 0.6, label=col, bottom=bottom)\n    bottom += df[col]\nax.set_title(\"Past variable importance over time ponderated by attention\")\nax.set_ylabel(\"Importance\")\nax.set_xlabel(\"Time\")\nax.legend()\nax.grid(True)\nplt.plot(\n    np.arange(-len(df), 0),\n    mean_attention,\n    color=\"black\",\n    marker=\"o\",\n    linestyle=\"-\",\n    linewidth=2,\n    label=\"mean_attention\",\n)\nplt.legend()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Single-Step Recurrent Prediction for Neural Forecasting\nDESCRIPTION: Implements a single prediction step in the recurrent forecasting methodology. The method handles input preparation, model forward pass, distribution sampling (if applicable), and inverse normalization of predictions. It can process both distribution-based and point-based predictions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_28\n\nLANGUAGE: Python\nCODE:\n```\ndef _predict_step_recurrent_single(self, insample_y, insample_mask, hist_exog, futr_exog, stat_exog, y_idx):\n        # Input sequence\n        windows_batch = dict(insample_y=insample_y,                 # [Ws, L, n_series]\n                        insample_mask=insample_mask,                # [Ws, L, n_series]\n                        futr_exog=futr_exog,                        # univariate: [Ws, L, F]; multivariate: [Ws, F, L, n_series]\n                        hist_exog=hist_exog,                        # univariate: [Ws, L, X]; multivariate: [Ws, X, L, n_series]\n                        stat_exog=stat_exog)                        # univariate: [Ws, S]; multivariate: [n_series, S]\n\n        # Model Predictions\n        output_batch_unmapped = self(windows_batch)\n        output_batch = self.loss.domain_map(output_batch_unmapped)\n        \n        # Inverse normalization and sampling\n        if self.loss.is_distribution_output:\n            # Sample distribution\n            y_loc, y_scale = self._get_loc_scale(y_idx)\n            distr_args = self.loss.scale_decouple(output=output_batch, loc=y_loc, scale=y_scale)\n            # When predicting, we need to sample to get the quantiles. The mean is an attribute.\n            _, _, quants = self.loss.sample(distr_args=distr_args, num_samples=self.n_samples)\n            mean = self.loss.distr_mean\n\n            # Scale back to feed back as input\n            insample_y = self.scaler.scaler(mean, y_loc, y_scale)\n            \n            # Save predictions\n            y_hat = torch.concat((mean.unsqueeze(-1), quants), axis=-1)\n\n            if self.loss.return_params:\n                distr_args = torch.stack(distr_args, dim=-1)\n                if distr_args.ndim > 4:\n                    distr_args = distr_args.flatten(-2, -1)\n                y_hat = torch.concat((y_hat, distr_args), axis=-1)\n        else:\n            # Todo: for now, we assume that in case of a BasePointLoss with ndim==4, the last dimension\n            # contains a set of predictions for the target (e.g. MQLoss multiple quantiles), for which we use the \n            # mean as feedback signal for the recurrent predictions. A more precise way is to increase the\n            # insample input size of the recurrent network by the number of outputs so that each output\n            # can be fed back to a specific input channel. \n            if output_batch.ndim == 4:\n                output_batch = output_batch.mean(dim=-1)\n\n            insample_y = output_batch\n            y_hat = self._inv_normalization(y_hat=output_batch, y_idx=y_idx)\n            y_hat = y_hat.unsqueeze(-1)\n\n        # Remove horizon dim: [B, 1, N, n_outputs] -> [B, N, n_outputs]\n        y_hat = y_hat.squeeze(1)\n        return y_hat, insample_y\n```\n\n----------------------------------------\n\nTITLE: GLU Layer Implementation for StemGNN\nDESCRIPTION: Gated Linear Unit (GLU) layer implementation that multiplies linear transformations with sigmoid gates for feature selection.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.stemgnn.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass GLU(nn.Module):\n    \"\"\"\n    GLU\n    \"\"\"    \n    def __init__(self, input_channel, output_channel):\n        super(GLU, self).__init__()\n        self.linear_left = nn.Linear(input_channel, output_channel)\n        self.linear_right = nn.Linear(input_channel, output_channel)\n\n    def forward(self, x):\n        return torch.mul(self.linear_left(x), torch.sigmoid(self.linear_right(x)))\n```\n\n----------------------------------------\n\nTITLE: Visualizing Mean Attention Weights Over Time\nDESCRIPTION: Demonstrates how to visualize the mean attention weights of the TFT model over time. This visualization shows which past time steps have the most influence on the model's predictions on average.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n# | eval: false\nplot_attention(nf.models[0], plot=\"time\")\n```\n\n----------------------------------------\n\nTITLE: Implementing PyTorch Reversible Instance Normalization (RevIN)\nDESCRIPTION: Defines the `RevIN` (Reversible Instance Normalization) PyTorch `nn.Module`. This module normalizes input features instance-wise across the time dimension and can reverse the process ('denorm' mode). It supports options for learnable affine parameters (`affine=True`), subtracting the last value instead of the mean (`subtract_last=True`), and skipping normalization (`non_norm=True`). Statistics (mean/stdev or last value) are calculated and stored during the 'norm' forward pass for use in 'denorm'. Depends on `torch` and `torch.nn`.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.modules.ipynb#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n#| export\n\nclass RevIN(nn.Module):\n    \"\"\" RevIN (Reversible-Instance-Normalization)\n    \"\"\"\n    def __init__(self, num_features: int, eps=1e-5, affine=False, subtract_last=False, non_norm=False):\n        \"\"\"\n        :param num_features: the number of features or channels\n        :param eps: a value added for numerical stability\n        :param affine: if True, RevIN has learnable affine parameters\n        :param substract_last: if True, the substraction is based on the last value \n                               instead of the mean in normalization\n        :param non_norm: if True, no normalization performed.\n        \"\"\"\n        super(RevIN, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.affine = affine\n        self.subtract_last = subtract_last\n        self.non_norm = non_norm\n        if self.affine:\n            self._init_params()\n\n    def forward(self, x, mode: str):\n        if mode == 'norm':\n            self._get_statistics(x)\n            x = self._normalize(x)\n        elif mode == 'denorm':\n            x = self._denormalize(x)\n        else:\n            raise NotImplementedError\n        return x\n\n    def _init_params(self):\n        # initialize RevIN params: (C,)\n        self.affine_weight = nn.Parameter(torch.ones(self.num_features))\n        self.affine_bias = nn.Parameter(torch.zeros(self.num_features))\n\n    def _get_statistics(self, x):\n        dim2reduce = tuple(range(1, x.ndim - 1))\n        if self.subtract_last:\n            self.last = x[:, -1, :].unsqueeze(1)\n        else:\n            self.mean = torch.mean(x, dim=dim2reduce, keepdim=True).detach()\n        self.stdev = torch.sqrt(torch.var(x, dim=dim2reduce, keepdim=True, unbiased=False) + self.eps).detach()\n\n    def _normalize(self, x):\n        if self.non_norm:\n            return x\n        if self.subtract_last:\n            x = x - self.last\n        else:\n            x = x - self.mean\n        x = x / self.stdev\n        if self.affine:\n            x = x * self.affine_weight\n            x = x + self.affine_bias\n        return x\n\n    def _denormalize(self, x):\n        if self.non_norm:\n            return x\n        if self.affine:\n            x = x - self.affine_bias\n            x = x / (self.affine_weight + self.eps * self.eps)\n        x = x * self.stdev\n        if self.subtract_last:\n            x = x + self.last\n        else:\n            x = x + self.mean\n        return x\n```\n\n----------------------------------------\n\nTITLE: Using AutoNBEATSx for Time Series Forecasting with Configuration Examples in Python\nDESCRIPTION: Demonstrates how to instantiate and use the AutoNBEATSx class with custom configuration and default parameters. Shows examples of model fitting and prediction with both Ray and Optuna backends. Includes configuration for MLP units specific to NBEATSx architecture.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_39\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n# Use your own config or AutoNBEATSx.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12,\n              mlp_units=3*[[8, 8]])\nmodel = AutoNBEATSx(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n# Optuna\nmodel = AutoNBEATSx(h=12, config=None, backend='optuna')\n```\n\n----------------------------------------\n\nTITLE: Implementing TSMixerx Model Class in Python\nDESCRIPTION: Definition of the TSMixerx class which implements an MLP-based multivariate time-series forecasting model with exogenous input capabilities. The class inherits from BaseModel and supports various customization options including forecast horizon, input size, exogenous variables, and training parameters.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tsmixerx.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass TSMixerx(BaseModel):\n    \"\"\" TSMixerx\n\n    Time-Series Mixer exogenous (`TSMixerx`) is a MLP-based multivariate time-series forecasting model, with capability for additional exogenous inputs. `TSMixerx` jointly learns temporal and cross-sectional representations of the time-series by repeatedly combining time- and feature information using stacked mixing layers. A mixing layer consists of a sequential time- and feature Multi Layer Perceptron (`MLP`).\n\n    **Parameters:**<br>\n    `h`: int, forecast horizon.<br>\n    `input_size`: int, considered autorregresive inputs (lags), y=[1,2,3,4] input_size=2 -> lags=[1,2].<br>\n    `n_series`: int, number of time-series.<br>\n    `futr_exog_list`: str list, future exogenous columns.<br>\n    `hist_exog_list`: str list, historic exogenous columns.<br>\n    `stat_exog_list`: str list, static exogenous columns.<br>\n    `exclude_insample_y`: bool=False, if True excludes insample_y from the model.<br>\n    `n_block`: int=2, number of mixing layers in the model.<br>\n    `ff_dim`: int=64, number of units for the second feed-forward layer in the feature MLP.<br>\n    `dropout`: float=0.0, dropout rate between (0, 1) .<br>\n    `revin`: bool=True, if True uses Reverse Instance Normalization on `insample_y` and applies it to the outputs.<br>    \n    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `max_steps`: int=1000, maximum number of training steps.<br>\n    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n    `batch_size`: int=32, number of different series in each batch.<br>\n    `valid_batch_size`: int=None, number of different series in each validation and test batch, if None uses batch_size.<br>\n    `windows_batch_size`: int=32, number of windows to sample in each training batch. <br>\n    `inference_windows_batch_size`: int=32, number of windows to sample in each inference batch, -1 uses all.<br>\n    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n    `step_size`: int=1, step size between each window of temporal data.<br>\n    `scaler_type`: str='identity', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n    `random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n    `alias`: str, optional,  Custom name of the model.<br>\n    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>    \n    `dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n\n    **References:**<br>\n    - [Chen, Si-An, Chun-Liang Li, Nate Yoder, Sercan O. Arik, and Tomas Pfister (2023). \"TSMixer: An All-MLP Architecture for Time Series Forecasting.\"](http://arxiv.org/abs/2303.06053)\n\n    \"\"\"\n    # Class attributes\n    EXOGENOUS_FUTR = True\n    EXOGENOUS_HIST = True\n    EXOGENOUS_STAT = True\n    MULTIVARIATE = True    # If the model produces multivariate forecasts (True) or univariate (False)\n    RECURRENT = False       # If the model produces forecasts recursively (True) or direct (False)\n\n    def __init__(self,\n                 h,\n                 input_size,\n                 n_series,\n                 futr_exog_list = None,\n                 hist_exog_list = None,\n                 stat_exog_list = None,\n                 exclude_insample_y = False,\n                 n_block = 2,\n                 ff_dim = 64,\n                 dropout = 0.0,\n                 revin = True,\n                 loss = MAE(),\n                 valid_loss = None,\n                 max_steps: int = 1000,\n                 learning_rate: float = 1e-3,\n                 num_lr_decays: int = -1,\n                 early_stop_patience_steps: int =-1,\n                 val_check_steps: int = 100,\n                 batch_size: int = 32,\n                 valid_batch_size: Optional[int] = None,\n                 windows_batch_size = 32,\n                 inference_windows_batch_size = 32,\n                 start_padding_enabled = False,\n                 step_size: int = 1,\n                 scaler_type: str = 'identity',\n                 random_seed: int = 1,\n                 drop_last_loader: bool = False,\n                 alias: Optional[str] = None,\n                 optimizer = None,\n                 optimizer_kwargs = None,\n                 lr_scheduler = None,\n                 lr_scheduler_kwargs = None,\n                 dataloader_kwargs = None,\n                 **trainer_kwargs):\n\n        # Inherit BaseMultvariate class\n        super(TSMixerx, self).__init__(h=h,\n                                    input_size=input_size,\n                                    n_series=n_series,\n                                    futr_exog_list=futr_exog_list,\n                                    hist_exog_list=hist_exog_list,\n                                    stat_exog_list=stat_exog_list,\n                                    exclude_insample_y = exclude_insample_y,\n                                    loss=loss,\n                                    valid_loss=valid_loss,\n                                    max_steps=max_steps,\n                                    learning_rate=learning_rate,\n                                    num_lr_decays=num_lr_decays,\n                                    early_stop_patience_steps=early_stop_patience_steps,\n                                    val_check_steps=val_check_steps,\n                                    batch_size=batch_size,\n                                    valid_batch_size=valid_batch_size,\n                                    windows_batch_size=windows_batch_size,\n                                    inference_windows_batch_size=inference_windows_batch_size,\n                                    start_padding_enabled=start_padding_enabled,\n                                    step_size=step_size,\n                                    scaler_type=scaler_type,\n                                    random_seed=random_seed,\n                                    drop_last_loader=drop_last_loader,\n                                    alias=alias,\n                                    optimizer=optimizer,\n                                    optimizer_kwargs=optimizer_kwargs,\n                                    lr_scheduler=lr_scheduler,\n                                    lr_scheduler_kwargs=lr_scheduler_kwargs,\n                                    dataloader_kwargs=dataloader_kwargs,\n                                    **trainer_kwargs)\n        # Reversible InstanceNormalization layer\n        self.revin = revin\n        if self.revin:\n            self.norm = RevINMultivariate(num_features= n_series, affine=True)\n\n        # Forecast horizon\n        self.h = h\n\n        # Temporal projection and feature mixing of historical variables\n        self.temporal_projection = nn.Linear(in_features=input_size, \n                                            out_features=h)\n\n        self.feature_mixer_hist = FeatureMixing(in_features=n_series * (1 + self.hist_exog_size + self.futr_exog_size),\n                                                out_features=ff_dim,\n                                                h=h, \n                                                dropout=dropout, \n                                                ff_dim=ff_dim)\n        first_mixing_ff_dim_multiplier = 1\n\n        # Feature mixing of future variables\n        if self.futr_exog_size > 0:\n            self.feature_mixer_futr = FeatureMixing(in_features = n_series * self.futr_exog_size,\n                                                    out_features=ff_dim,\n                                                    h=h,\n                                                    dropout=dropout,\n                                                    ff_dim=ff_dim)\n            first_mixing_ff_dim_multiplier += 1\n\n        # Feature mixing of static variables\n        if self.stat_exog_size > 0:\n            self.feature_mixer_stat = FeatureMixing(in_features=self.stat_exog_size * n_series,\n                                                    out_features=ff_dim,\n                                                    h=h,\n                                                    dropout=dropout,\n                                                    ff_dim=ff_dim)            \n            first_mixing_ff_dim_multiplier += 1\n\n        # First mixing layer\n        self.first_mixing = MixingLayer(in_features = first_mixing_ff_dim_multiplier * ff_dim,\n                                        out_features=ff_dim,\n                                        h=h,\n```\n\n----------------------------------------\n\nTITLE: Implementing TimeSeriesDataModule Class in Python\nDESCRIPTION: This class extends LightningDataModule to create data loaders for time series datasets. It includes methods for creating train, validation, and prediction data loaders with customizable batch sizes and other parameters.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/tsdataset.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass TimeSeriesDataModule(pl.LightningDataModule):\n    \n    def __init__(\n            self, \n            dataset: BaseTimeSeriesDataset,\n            batch_size=32, \n            valid_batch_size=1024,\n            drop_last=False,\n            shuffle_train=True,\n            **dataloaders_kwargs\n        ):\n        super().__init__()\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.valid_batch_size = valid_batch_size\n        self.drop_last = drop_last\n        self.shuffle_train = shuffle_train\n        self.dataloaders_kwargs = dataloaders_kwargs\n    \n    def train_dataloader(self):\n        loader = TimeSeriesLoader(\n            self.dataset,\n            batch_size=self.batch_size, \n            shuffle=self.shuffle_train,\n            drop_last=self.drop_last,\n            **self.dataloaders_kwargs\n        )\n        return loader\n    \n    def val_dataloader(self):\n        loader = TimeSeriesLoader(\n            self.dataset, \n            batch_size=self.valid_batch_size, \n            shuffle=False,\n            drop_last=self.drop_last,\n            **self.dataloaders_kwargs\n        )\n        return loader\n    \n    def predict_dataloader(self):\n        loader = TimeSeriesLoader(\n            self.dataset,\n            batch_size=self.valid_batch_size, \n            shuffle=False,\n            **self.dataloaders_kwargs\n        )\n        return loader\n```\n\n----------------------------------------\n\nTITLE: Implementing NBEATS Block for NBEATSx\nDESCRIPTION: Defines the NBEATS block, a core component of the NBEATSx architecture. This block takes a basis function as an argument and processes input data through a multi-layer perceptron (MLP) structure.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeatsx.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nACTIVATIONS = [\"ReLU\", \"Softplus\", \"Tanh\", \"SELU\", \"LeakyReLU\", \"PReLU\", \"Sigmoid\"]\n\n\nclass NBEATSBlock(nn.Module):\n    \"\"\"\n    N-BEATS block which takes a basis function as an argument.\n    \"\"\"\n\n    def __init__(\n        self,\n        input_size: int,\n        h: int,\n        futr_input_size: int,\n        hist_input_size: int,\n        stat_input_size: int,\n        n_theta: int,\n        mlp_units: list,\n        basis: nn.Module,\n        dropout_prob: float,\n        activation: str,\n    ):\n        \"\"\" \"\"\"\n        super().__init__()\n\n        self.dropout_prob = dropout_prob\n        self.futr_input_size = futr_input_size\n        self.hist_input_size = hist_input_size\n        self.stat_input_size = stat_input_size\n\n        assert activation in ACTIVATIONS, f\"{activation} is not in {ACTIVATIONS}\"\n        activ = getattr(nn, activation)()\n\n        # Input vector for the block is\n        # y_lags (input_size) + historical exogenous (hist_input_size*input_size) +\n        # future exogenous (futr_input_size*input_size) + static exogenous (stat_input_size)\n        # [ Y_[t-L:t], X_[t-L:t], F_[t-L:t+H], S ]\n        input_size = (\n            input_size\n            + hist_input_size * input_size\n            + futr_input_size * (input_size + h)\n            + stat_input_size\n        )\n\n        hidden_layers = [\n            nn.Linear(in_features=input_size, out_features=mlp_units[0][0])\n        ]\n        for layer in mlp_units:\n            hidden_layers.append(nn.Linear(in_features=layer[0], out_features=layer[1]))\n            hidden_layers.append(activ)\n\n            if self.dropout_prob > 0:\n                hidden_layers.append(nn.Dropout(p=self.dropout_prob))\n\n        output_layer = [nn.Linear(in_features=mlp_units[-1][1], out_features=n_theta)]\n        layers = hidden_layers + output_layer\n        self.layers = nn.Sequential(*layers)\n        self.basis = basis\n\n    def forward(\n        self,\n        insample_y: torch.Tensor,\n        futr_exog: torch.Tensor,\n        hist_exog: torch.Tensor,\n        stat_exog: torch.Tensor,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        # Flatten MLP inputs [B, L+H, C] -> [B, (L+H)*C]\n        # Contatenate [ Y_t, | X_{t-L},..., X_{t} | F_{t-L},..., F_{t+H} | S ]\n        batch_size = len(insample_y)\n        if self.hist_input_size > 0:\n            insample_y = torch.cat(\n```\n\n----------------------------------------\n\nTITLE: Forward Method for Informer Model in PyTorch\nDESCRIPTION: Implementation of the forward method in the Informer model that processes input data through the encoder-decoder architecture. The method handles in-sample values and future exogenous variables to generate forecasts.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.informer.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, windows_batch):\n    # Parse windows_batch\n    insample_y    = windows_batch['insample_y']\n    futr_exog     = windows_batch['futr_exog']\n\n    if self.futr_exog_size > 0:\n        x_mark_enc = futr_exog[:, :self.input_size, :]\n        x_mark_dec = futr_exog[:, -(self.label_len+self.h):, :]\n    else:\n        x_mark_enc = None\n        x_mark_dec = None\n\n    x_dec = torch.zeros(size=(len(insample_y),self.h,1), device=insample_y.device)\n    x_dec = torch.cat([insample_y[:,-self.label_len:,:], x_dec], dim=1)        \n\n    enc_out = self.enc_embedding(insample_y, x_mark_enc)\n    enc_out, _ = self.encoder(enc_out, attn_mask=None) # attns visualization\n\n    dec_out = self.dec_embedding(x_dec, x_mark_dec)\n    dec_out = self.decoder(dec_out, enc_out, x_mask=None, \n                           cross_mask=None)\n\n    forecast = dec_out[:, -self.h:]\n    return forecast\n```\n\n----------------------------------------\n\nTITLE: Implementing Loss Calculation for NBMM in Python\nDESCRIPTION: This method computes the negative log-likelihood objective function for the NBMM model. It calculates the loss based on the actual values and the distribution parameters, using a weighted average with an optional mask.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_64\n\nLANGUAGE: python\nCODE:\n```\ndef __call__(self,\n             y: torch.Tensor,\n             distr_args: torch.Tensor,\n             mask: Union[torch.Tensor, None] = None):\n    # Instantiate Scaled Decoupled Distribution\n    distr = self.get_distribution(distr_args=distr_args)\n    loss_values = -distr.log_prob(y)\n    loss_weights = mask\n   \n    return weighted_average(loss_values, weights=loss_weights)\n```\n\n----------------------------------------\n\nTITLE: Implementing Transformer Decoder Layer and Decoder in PyTorch\nDESCRIPTION: Defines the transformer decoder layer and full decoder implementation with self-attention and cross-attention mechanisms. Includes layer normalization, dropout, and optional projection layer.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.modules.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass TransDecoderLayer(nn.Module):\n    def __init__(self, self_attention, cross_attention, hidden_size, conv_hidden_size=None,\n                 dropout=0.1, activation=\"relu\"):\n        super(TransDecoderLayer, self).__init__()\n        conv_hidden_size = conv_hidden_size or 4 * hidden_size\n        self.self_attention = self_attention\n        self.cross_attention = cross_attention\n        self.conv1 = nn.Conv1d(in_channels=hidden_size, out_channels=conv_hidden_size, kernel_size=1)\n        self.conv2 = nn.Conv1d(in_channels=conv_hidden_size, out_channels=hidden_size, kernel_size=1)\n        self.norm1 = nn.LayerNorm(hidden_size)\n        self.norm2 = nn.LayerNorm(hidden_size)\n        self.norm3 = nn.LayerNorm(hidden_size)\n        self.dropout = nn.Dropout(dropout)\n        self.activation = F.relu if activation == \"relu\" else F.gelu\n\n    def forward(self, x, cross, x_mask=None, cross_mask=None):\n        x = x + self.dropout(self.self_attention(\n            x, x, x,\n            attn_mask=x_mask\n        )[0])\n        x = self.norm1(x)\n\n        x = x + self.dropout(self.cross_attention(\n            x, cross, cross,\n            attn_mask=cross_mask\n        )[0])\n\n        y = x = self.norm2(x)\n        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n        y = self.dropout(self.conv2(y).transpose(-1, 1))\n\n        return self.norm3(x + y)\n\n\nclass TransDecoder(nn.Module):\n    def __init__(self, layers, norm_layer=None, projection=None):\n        super(TransDecoder, self).__init__()\n        self.layers = nn.ModuleList(layers)\n        self.norm = norm_layer\n        self.projection = projection\n\n    def forward(self, x, cross, x_mask=None, cross_mask=None):\n        for layer in self.layers:\n            x = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask)\n\n        if self.norm is not None:\n            x = self.norm(x)\n\n        if self.projection is not None:\n            x = self.projection(x)\n        return x\n```\n\n----------------------------------------\n\nTITLE: Implementing MixingLayer for TSMixerx in Python\nDESCRIPTION: Defines the MixingLayer class, which combines TemporalMixing and FeatureMixing to create a complete mixing layer for the TSMixerx model.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tsmixerx.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass MixingLayer(nn.Module):\n    \"\"\" \n    MixingLayer\n    \"\"\"      \n    def __init__(self, in_features, out_features, h, dropout, ff_dim):\n        super().__init__()\n        # Mixing layer consists of a temporal and feature mixer\n        self.temporal_mixer = TemporalMixing(num_features=in_features, \n                                             h=h, \n                                             dropout=dropout)\n        self.feature_mixer = FeatureMixing(in_features=in_features, \n                                           out_features=out_features, \n                                           h=h, \n                                           dropout=dropout, \n                                           ff_dim=ff_dim)\n\n    def forward(self, input):\n        x = self.temporal_mixer(input)                                  # [B, h, C_in] -> [B, h, C_in]\n        x = self.feature_mixer(x)                                       # [B, h, C_in] -> [B, h, C_out]\n        return x\n```\n\n----------------------------------------\n\nTITLE: Displaying Documentation for TimeMixer Class in Python\nDESCRIPTION: This snippet uses the `show_doc` function, presumably from a documentation generation library like nbdev, to render the documentation associated with the `TimeMixer` class itself. It requires the `TimeMixer` class to be defined and the `show_doc` function to be available in the execution environment.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timemixer.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(TimeMixer)\n```\n\n----------------------------------------\n\nTITLE: Testing AutoVanillaTransformer Model with Various Configurations in Python\nDESCRIPTION: This snippet contains unit tests for the AutoVanillaTransformer model, verifying its functionality with different configurations and backends (Optuna and Ray). It tests the model's compatibility with custom configurations and ensures all required arguments are present.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_70\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n# Check Optuna\nassert model.config(MockTrial())['h'] == 12\n\n# Unit test to test that Auto* model contains all required arguments from BaseAuto\ntest_args(AutoVanillaTransformer, exclude_args=['cls_model']) \n\n# Unit test for situation: Optuna with updated default config\nmy_config = AutoVanillaTransformer.get_default_config(h=12, backend='optuna')\ndef my_config_new(trial):\n    config = {**my_config(trial)}\n    config.update({'max_steps': 1, 'val_check_steps': 1, 'input_size': 12, 'hidden_size': 8})\n    return config\n\nmodel = AutoVanillaTransformer(h=12, config=my_config_new, backend='optuna', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n\n# Unit test for situation: Ray with updated default config\nmy_config = AutoVanillaTransformer.get_default_config(h=12, backend='ray')\nmy_config['max_steps'] = 1\nmy_config['val_check_steps'] = 1\nmy_config['input_size'] = 12\nmy_config['hidden_size'] = 8\nmodel = AutoVanillaTransformer(h=12, config=my_config, backend='ray', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: Defining NLinear Model Class\nDESCRIPTION: Implements the NLinear class, inheriting from BaseModel. It sets up the model architecture with a linear layer and defines the forward pass for forecasting.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nlinear.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass NLinear(BaseModel):\n    \"\"\" NLinear\n\n    *Parameters:*<br>\n    `h`: int, forecast horizon.<br>\n    `input_size`: int, maximum sequence length for truncated train backpropagation.<br>\n    `stat_exog_list`: str list, static exogenous columns.<br>\n    `hist_exog_list`: str list, historic exogenous columns.<br>\n    `futr_exog_list`: str list, future exogenous columns.<br>\n    `exclude_insample_y`: bool=False, the model skips the autoregressive features y[t-input_size:t] if True.<br>\n    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `max_steps`: int=1000, maximum number of training steps.<br>\n    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n    `batch_size`: int=32, number of different series in each batch.<br>\n    `valid_batch_size`: int=None, number of different series in each validation and test batch, if None uses batch_size.<br>\n    `windows_batch_size`: int=1024, number of windows to sample in each training batch, default uses all.<br>\n    `inference_windows_batch_size`: int=1024, number of windows to sample in each inference batch.<br>\n    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n    `step_size`: int=1, step size between each window of temporal data.<br>    \n    `scaler_type`: str='robust', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n    `random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n    `alias`: str, optional,  Custom name of the model.<br>\n    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>    \n    `dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>\n\n\t*References*<br>\n\t- Zeng, Ailing, et al. \"Are transformers effective for time series forecasting?\" Proceedings of the AAAI conference on artificial intelligence. Vol. 37. No. 9. 2023.\"\n    \"\"\"\n    # Class attributes\n    EXOGENOUS_FUTR = False\n    EXOGENOUS_HIST = False\n    EXOGENOUS_STAT = False\n    MULTIVARIATE = False    # If the model produces multivariate forecasts (True) or univariate (False)\n    RECURRENT = False       # If the model produces forecasts recursively (True) or direct (False)\n\n    def __init__(self,\n                 h: int, \n                 input_size: int,\n                 stat_exog_list = None,\n                 hist_exog_list = None,\n                 futr_exog_list = None,\n                 exclude_insample_y = False,\n                 loss = MAE(),\n                 valid_loss = None,\n                 max_steps: int = 5000,\n                 learning_rate: float = 1e-4,\n                 num_lr_decays: int = -1,\n                 early_stop_patience_steps: int =-1,\n                 val_check_steps: int = 100,\n                 batch_size: int = 32,\n                 valid_batch_size: Optional[int] = None,\n                 windows_batch_size = 1024,\n                 inference_windows_batch_size = 1024,\n                 start_padding_enabled = False,\n                 step_size: int = 1,\n                 scaler_type: str = 'identity',\n                 random_seed: int = 1,\n                 drop_last_loader: bool = False,\n                 alias: Optional[str] = None,\n                 optimizer = None,\n                 optimizer_kwargs = None,\n                 lr_scheduler = None,\n                 lr_scheduler_kwargs = None,\n                 dataloader_kwargs = None,\n                 **trainer_kwargs):\n        super(NLinear, self).__init__(h=h,\n                                       input_size=input_size,\n                                       stat_exog_list=stat_exog_list,\n                                       hist_exog_list=hist_exog_list,\n                                       futr_exog_list = futr_exog_list,\n                                       exclude_insample_y = exclude_insample_y,\n                                       loss=loss,\n                                       valid_loss=valid_loss,\n                                       max_steps=max_steps,\n                                       learning_rate=learning_rate,\n                                       num_lr_decays=num_lr_decays,\n                                       early_stop_patience_steps=early_stop_patience_steps,\n                                       val_check_steps=val_check_steps,\n                                       batch_size=batch_size,\n                                       windows_batch_size=windows_batch_size,\n                                       valid_batch_size=valid_batch_size,\n                                       inference_windows_batch_size=inference_windows_batch_size,\n                                       start_padding_enabled = start_padding_enabled,\n                                       step_size=step_size,\n                                       scaler_type=scaler_type,\n                                       random_seed=random_seed,\n                                       drop_last_loader=drop_last_loader,\n                                       alias=alias,\n                                       optimizer=optimizer,\n                                       optimizer_kwargs=optimizer_kwargs,\n                                       lr_scheduler=lr_scheduler,\n                                       lr_scheduler_kwargs=lr_scheduler_kwargs,\n                                       dataloader_kwargs=dataloader_kwargs,\n                                       **trainer_kwargs)\n\n        # Architecture\n        self.c_out = self.loss.outputsize_multiplier\n        self.output_attention = False\n        self.enc_in = 1 \n        self.dec_in = 1\n\n        self.linear = nn.Linear(self.input_size, self.loss.outputsize_multiplier * h, bias=True)\n\n    def forward(self, windows_batch):\n        # Parse windows_batch\n        insample_y    = windows_batch['insample_y'].squeeze(-1)\n\n        # Parse inputs\n        batch_size = len(insample_y)\n        \n        # Input normalization\n        last_value = insample_y[:, -1:]\n        norm_insample_y = insample_y - last_value\n        \n        # Final\n        forecast = self.linear(norm_insample_y) + last_value\n        forecast = forecast.reshape(batch_size, self.h, self.loss.outputsize_multiplier)\n        return forecast\n```\n\n----------------------------------------\n\nTITLE: Configuring NHITS Hyperparameter Search Space\nDESCRIPTION: Defines the hyperparameter search space for AutoNHITS using Ray Tune, including learning rate, model architecture, and training settings.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/04_longhorizon_nhits.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nhorizon = 96 # 24hrs = 4 * 15 min.\n\n# Use your own config or AutoNHITS.default_config\nnhits_config = {\n       \"learning_rate\": tune.choice([1e-3]),                                     # Initial Learning rate\n       \"max_steps\": tune.choice([1000]),                                         # Number of SGD steps\n       \"input_size\": tune.choice([5 * horizon]),                                 # input_size = multiplier * horizon\n       \"batch_size\": tune.choice([7]),                                           # Number of series in windows\n       \"windows_batch_size\": tune.choice([256]),                                 # Number of windows in batch\n       \"n_pool_kernel_size\": tune.choice([[2, 2, 2], [16, 8, 1]]),               # MaxPool's Kernelsize\n       \"n_freq_downsample\": tune.choice([[168, 24, 1], [24, 12, 1], [1, 1, 1]]), # Interpolation expressivity ratios\n       \"activation\": tune.choice(['ReLU']),                                      # Type of non-linear activation\n       \"n_blocks\":  tune.choice([[1, 1, 1]]),                                    # Blocks per each 3 stacks\n       \"mlp_units\":  tune.choice([[[512, 512], [512, 512], [512, 512]]]),        # 2 512-Layers per block for each stack\n       \"interpolation_mode\": tune.choice(['linear']),                            # Type of multi-step interpolation\n       \"val_check_steps\": tune.choice([100]),                                    # Compute validation every 100 epochs\n       \"random_seed\": tune.randint(1, 10),\n    }\n```\n\n----------------------------------------\n\nTITLE: BiTCN Usage Example with AirPassengers Dataset\nDESCRIPTION: Demonstrates how to use the BiTCN model for time series forecasting using the AirPassengers dataset. Shows model initialization, training, and plotting predictions with confidence intervals.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.bitcn.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.losses.pytorch import GMM\nfrom neuralforecast.models import BiTCN\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nfcst = NeuralForecast(\n    models=[\n            BiTCN(h=12,\n                input_size=24,\n                loss=GMM(n_components=7, level=[80,90]),\n                max_steps=100,\n                scaler_type='standard',\n                futr_exog_list=['y_[lag12]'],\n                hist_exog_list=None,\n                stat_exog_list=['airline1'],\n                windows_batch_size=2048,\n                val_check_steps=10,\n                early_stop_patience_steps=-1,\n                ),     \n    ],\n    freq='ME'\n)\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\n# Plot quantile predictions\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['BiTCN-median'], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'][-12:], \n                 y1=plot_df['BiTCN-lo-90'][-12:].values,\n                 y2=plot_df['BiTCN-hi-90'][-12:].values,\n                 alpha=0.4, label='level 90')\nplt.legend()\nplt.grid()\n```\n\n----------------------------------------\n\nTITLE: Implementing Attention-based LSTM Layer in PyTorch\nDESCRIPTION: Defines an AttentiveLSTMLayer that incorporates attention mechanisms to focus on relevant parts of the input sequence. It uses a standard LSTMCell combined with an attention layer to compute context vectors based on input relevance.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.dilated_rnn.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n#| exporti\nclass AttentiveLSTMLayer(nn.Module):\n    def __init__(self, input_size, hidden_size, dropout=0.0):\n        super(AttentiveLSTMLayer, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        attention_hsize = hidden_size\n        self.attention_hsize = attention_hsize\n\n        self.cell = LSTMCell(input_size, hidden_size)\n        self.attn_layer = nn.Sequential(nn.Linear(2 * hidden_size + input_size, attention_hsize),\n                                        nn.Tanh(),\n                                        nn.Linear(attention_hsize, 1))\n        self.softmax = nn.Softmax(dim=0)\n        self.dropout = dropout\n\n    def forward(self, inputs, hidden):\n        inputs = inputs.unbind(0)\n        outputs = []\n\n        for t in range(len(inputs)):\n            # attention on windows\n            hx, cx = (tensor.squeeze(0) for tensor in hidden)\n            hx_rep = hx.repeat(len(inputs), 1, 1)\n            cx_rep = cx.repeat(len(inputs), 1, 1)\n            x = torch.cat((inputs, hx_rep, cx_rep), dim=-1)\n            l = self.attn_layer(x)\n            beta = self.softmax(l)\n            context = torch.bmm(beta.permute(1, 2, 0),\n                                inputs.permute(1, 0, 2)).squeeze(1)\n            out, hidden = self.cell(context, hidden)\n            outputs += [out]\n        outputs = torch.stack(outputs)\n        return outputs, hidden\n```\n\n----------------------------------------\n\nTITLE: Implementing AutoFEDformer Class for Hyperparameter Tuning\nDESCRIPTION: This code defines the AutoFEDformer class that inherits from BaseAuto for automated hyperparameter tuning of the FEDformer model. It includes default configuration settings, initialization parameters, and methods to adapt configurations for different backends (Ray and Optuna).\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_80\n\nLANGUAGE: python\nCODE:\n```\nclass AutoFEDformer(BaseAuto):\n\n    default_config = {\n        \"input_size_multiplier\": [1, 2, 3, 4, 5],\n        \"h\": None,\n        \"hidden_size\": tune.choice([64, 128, 256]),\n        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n        \"scaler_type\": tune.choice([None, 'robust', 'standard']),\n        \"max_steps\": tune.choice([500, 1000, 2000]),\n        \"batch_size\": tune.choice([32, 64, 128, 256]),\n        \"windows_batch_size\": tune.choice([128, 256, 512, 1024]),\n        \"loss\": None,\n        \"random_seed\": tune.randint(1, 20),\n    }\n\n    def __init__(self,\n                 h,\n                 loss=MAE(),\n                 valid_loss=None,\n                 config=None, \n                 search_alg=BasicVariantGenerator(random_state=1),\n                 num_samples=10,\n                 refit_with_val=False,\n                 cpus=cpu_count(),\n                 gpus=torch.cuda.device_count(),\n                 verbose=False,\n                 alias=None,\n                 backend='ray',\n                 callbacks=None):\n        \n        # Define search space, input/output sizes    \n        if config is None:\n            config = self.get_default_config(h=h, backend=backend)          \n\n        super(AutoFEDformer, self).__init__(\n              cls_model=FEDformer, \n              h=h,\n              loss=loss,\n              valid_loss=valid_loss,\n              config=config,\n              search_alg=search_alg,\n              num_samples=num_samples, \n              refit_with_val=refit_with_val,\n              cpus=cpus,\n              gpus=gpus,\n              verbose=verbose,\n              alias=alias,\n              backend=backend,\n              callbacks=callbacks,            \n        )\n\n    @classmethod\n    def get_default_config(cls, h, backend, n_series=None):\n        config = cls.default_config.copy()\n        config['input_size'] = tune.choice([h*x \\\n                        for x in config['input_size_multiplier']])\n        config['step_size'] = tune.choice([1, h])        \n        del config['input_size_multiplier']\n        if backend == 'optuna':\n            config = cls._ray_config_to_optuna(config)         \n\n        return config   \n```\n\n----------------------------------------\n\nTITLE: Instantiating HINT Model with NHITS in Python\nDESCRIPTION: This code snippet demonstrates how to instantiate the HINT model using NHITS as the base model. It configures various parameters for the NHITS model and the HINT wrapper.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.hint.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Instantiate HINT\n# BaseNetwork + Distribution + Reconciliation\nnhits = NHITS(h=horizon,\n              input_size=24,\n              loss=GMM(n_components=10, level=level),\n              max_steps=2000,\n              early_stop_patience_steps=10,\n              val_check_steps=50,\n              scaler_type='robust',\n              learning_rate=1e-3,\n              valid_loss=sCRPS(level=level))\n\nmodel = HINT(h=horizon, S=S_df.values,\n             model=nhits,  reconciliation='BottomUp')\n\n# Fit and Predict\nnf = NeuralForecast(models=[model], freq='MS')\nY_hat_df = nf.cross_validation(df=Y_df, val_size=12, n_windows=1)\nY_hat_df = Y_hat_df.reset_index()\n```\n\n----------------------------------------\n\nTITLE: Usage Example with Air Passengers Dataset\nDESCRIPTION: Demonstrates how to use the RNN model for time series forecasting using the Air Passengers dataset, including model configuration, training, and visualization.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.rnn.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import RNN\nfrom neuralforecast.losses.pytorch import MQLoss\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nfcst = NeuralForecast(\n    models=[RNN(h=12,\n                input_size=24,\n                inference_input_size=24,\n                loss=MQLoss(level=[80, 90]),\n                valid_loss=MQLoss(level=[80, 90]),\n                scaler_type='standard',\n                encoder_n_layers=2,\n                encoder_hidden_size=128,\n                decoder_hidden_size=128,\n                decoder_layers=2,\n                max_steps=200,\n                futr_exog_list=['y_[lag12]'],\n                stat_exog_list=['airline1'],\n                )\n    ],\n    freq='ME'\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Gaussian Mixture Mesh PyTorch Module\nDESCRIPTION: Main GMM class implementation that inherits from torch.nn.Module. It provides functionality for probabilistic forecasting using Gaussian mixtures, with support for batch and horizon correlations, weighted components, and customizable confidence levels.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_52\n\nLANGUAGE: python\nCODE:\n```\nclass GMM(torch.nn.Module):\n    def __init__(self, n_components=1, level=[80, 90], quantiles=None, \n                 num_samples=1000, return_params=False,\n                 batch_correlation=False, horizon_correlation=False,\n                 weighted=False):\n        super(GMM, self).__init__()\n        # Implementation details...\n\n```\n\n----------------------------------------\n\nTITLE: Distribution Scale Decoupling Functions in PyTorch\nDESCRIPTION: Implementation of scale decoupling functions for various distributions including Bernoulli, Student-t, Normal, Poisson, and Negative Binomial. These functions stabilize model output optimization by learning residual variance and location based on anchoring parameters.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\ndef bernoulli_scale_decouple(output, loc=None, scale=None):\n    \"\"\" Bernoulli Scale Decouple\n\n    Stabilizes model's output optimization, by learning residual\n    variance and residual location based on anchoring `loc`, `scale`.\n    Also adds Bernoulli domain protection to the distribution parameters.\n    \"\"\"\n    probs = output[0]\n    #if (loc is not None) and (scale is not None):\n    #    rate = (rate * scale) + loc\n    probs = F.sigmoid(probs)#.clone()\n    return (probs,)\n\ndef student_scale_decouple(output, loc=None, scale=None, eps: float=0.1):\n    \"\"\" Normal Scale Decouple\n\n    Stabilizes model's output optimization, by learning residual\n    variance and residual location based on anchoring `loc`, `scale`.\n    Also adds StudentT domain protection to the distribution parameters.\n    \"\"\"\n    df, mean, tscale = output\n    tscale = F.softplus(tscale)\n    if (loc is not None) and (scale is not None):\n        mean = (mean * scale) + loc\n        tscale = (tscale + eps) * scale\n    df = 3.0 + F.softplus(df)\n    return (df, mean, tscale)\n\ndef normal_scale_decouple(output, loc=None, scale=None, eps: float=0.2):\n    \"\"\" Normal Scale Decouple\n\n    Stabilizes model's output optimization, by learning residual\n    variance and residual location based on anchoring `loc`, `scale`.\n    Also adds Normal domain protection to the distribution parameters.\n    \"\"\"\n    mean, std = output\n    std = F.softplus(std)\n    if (loc is not None) and (scale is not None):\n        mean = (mean * scale) + loc\n        std = (std + eps) * scale\n    return (mean, std)\n\ndef poisson_scale_decouple(output, loc=None, scale=None):\n    \"\"\" Poisson Scale Decouple\n\n    Stabilizes model's output optimization, by learning residual\n    variance and residual location based on anchoring `loc`, `scale`.\n    Also adds Poisson domain protection to the distribution parameters.\n    \"\"\"\n    eps  = 1e-10\n    rate = output[0]\n    if (loc is not None) and (scale is not None):\n        rate = (rate * scale) + loc\n    rate = F.softplus(rate) + eps\n    return (rate, )\n\ndef nbinomial_scale_decouple(output, loc=None, scale=None):\n    \"\"\" Negative Binomial Scale Decouple\n\n    Stabilizes model's output optimization, by learning total\n    count and logits based on anchoring `loc`, `scale`.\n    Also adds Negative Binomial domain protection to the distribution parameters.\n    \"\"\"\n    mu, alpha = output\n    mu = F.softplus(mu) + 1e-8\n    alpha = F.softplus(alpha) + 1e-8    # alpha = 1/total_counts\n    if (loc is not None) and (scale is not None):\n        mu = mu * scale + loc\n        alpha /= (scale + 1.)\n\n    # mu = total_count * (probs/(1-probs))\n    # => probs = mu / (total_count + mu)\n    # => probs = mu / [total_count * (1 + mu * (1/total_count))]\n    total_count = 1.0 / alpha\n    probs = (mu * alpha / (1.0 + mu * alpha)) + 1e-8\n    return (total_count, probs)\n```\n\n----------------------------------------\n\nTITLE: Implementing BaseISQF Distribution Class in PyTorch\nDESCRIPTION: Implementation of the BaseISQF distribution class that serves as the foundation for the ISQF. It handles the parameterization of spline and quantile knots and provides functionality for quantile function learning without crossing.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\nclass BaseISQF(Distribution):\n    \"\"\"\n    Base distribution class for the Incremental (Spline) Quantile Function.\n    \n    **Parameters:**<br>\n    `spline_knots`: Tensor parametrizing the x-positions of the spline knots. Shape: (*batch_shape, (num_qk-1), num_pieces)\n    `spline_heights`: Tensor parametrizing the y-positions of the spline knots. Shape: (*batch_shape, (num_qk-1), num_pieces)\n    `beta_l`: Tensor containing the non-negative learnable parameter of the left tail. (*batch_shape,)\n    `beta_r`: Tensor containing the non-negative learnable parameter of the right tail. (*batch_shape,)\n    `qk_y`: Tensor containing the increasing y-positions of the quantile knots. Shape: (*batch_shape, num_qk)\n    `qk_x`: Tensor containing the increasing x-positions of the quantile knots. Shape: (*batch_shape, num_qk)\n\n    **References:**<br>\n    [Park, Youngsuk, Danielle Maddix, Franois-Xavier Aubet, Kelvin Kan, Jan Gasthaus, and Yuyang Wang (2022). \"Learning Quantile Functions without Quantile Crossing for Distribution-free Time Series Forecasting\".](https://proceedings.mlr.press/v151/park22a.html)\n        \n    \"\"\"\n\n    def __init__(\n        self,\n        spline_knots: torch.Tensor,\n        spline_heights: torch.Tensor,\n        beta_l: torch.Tensor,\n        beta_r: torch.Tensor,\n        qk_y: torch.Tensor,\n        qk_x: torch.Tensor,\n        tol: float = 1e-4,\n        validate_args: bool = False,\n    ) -> None:\n        self.num_qk, self.num_pieces = qk_y.shape[-1], spline_knots.shape[-1]\n        self.spline_knots, self.spline_heights = spline_knots, spline_heights\n        self.beta_l, self.beta_r = beta_l, beta_r\n        self.qk_y_all = qk_y\n        self.tol = tol\n\n        super().__init__(\n            batch_shape=self.batch_shape, validate_args=validate_args\n        )\n\n        # Get quantile knots (qk) parameters\n        (\n            self.qk_x,\n            self.qk_x_plus,\n            self.qk_x_l,\n            self.qk_x_r,\n        ) = BaseISQF.parameterize_qk(qk_x)\n        (\n            self.qk_y,\n            self.qk_y_plus,\n            self.qk_y_l,\n            self.qk_y_r,\n        ) = BaseISQF.parameterize_qk(qk_y)\n\n        # Get spline knots (sk) parameters\n        self.sk_y, self.delta_sk_y = BaseISQF.parameterize_spline(\n            self.spline_heights,\n            self.qk_y,\n            self.qk_y_plus,\n            self.tol,\n        )\n        self.sk_x, self.delta_sk_x = BaseISQF.parameterize_spline(\n            self.spline_knots,\n            self.qk_x,\n            self.qk_x_plus,\n            self.tol,\n        )\n\n        if self.num_pieces > 1:\n            self.sk_x_plus = torch.cat(\n                [self.sk_x[..., 1:], self.qk_x_plus.unsqueeze(dim=-1)], dim=-1\n            )\n        else:\n            self.sk_x_plus = self.qk_x_plus.unsqueeze(dim=-1)\n\n        # Get tails parameters\n        self.tail_al, self.tail_bl = BaseISQF.parameterize_tail(\n            self.beta_l, self.qk_x_l, self.qk_y_l\n        )\n        self.tail_ar, self.tail_br = BaseISQF.parameterize_tail(\n            -self.beta_r, 1 - self.qk_x_r, self.qk_y_r\n        )\n```\n\n----------------------------------------\n\nTITLE: Implementing TimesNet PyTorch Model Class\nDESCRIPTION: TimesNet class implementation extending BaseModel for univariate time series forecasting. The model handles temporal variations using a combination of embedding layers, TimesBlocks, and linear projections. It supports various parameters for customizing the architecture, training process, and includes comprehensive documentation of all initialization parameters.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timesnet.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass TimesNet(BaseModel):\n    \"\"\" TimesNet\n\n    The TimesNet univariate model tackles the challenge of modeling multiple intraperiod and interperiod temporal variations.\n    \n    **Parameters**<br>\n    `h` : int, Forecast horizon.<br>\n    `input_size` : int, Length of input window (lags).<br>\n    `stat_exog_list` : list of str, optional (default=None), Static exogenous columns.<br>\n    `hist_exog_list` : list of str, optional (default=None), Historic exogenous columns.<br>\n    `futr_exog_list` : list of str, optional (default=None), Future exogenous columns.<br>\n    `exclude_insample_y` : bool (default=False), The model skips the autoregressive features y[t-input_size:t] if True.<br>\n    `hidden_size` : int (default=64), Size of embedding for embedding and encoders.<br>\n    `dropout` : float between [0, 1) (default=0.1), Dropout for embeddings.<br>\n\t`conv_hidden_size`: int (default=64), Channels of the Inception block.<br>\n    `top_k`: int (default=5), Number of periods.<br>\n    `num_kernels`: int (default=6), Number of kernels for the Inception block.<br>\n    `encoder_layers` : int, (default=2), Number of encoder layers.<br>\n    `loss`: PyTorch module (default=MAE()), Instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).\n    `valid_loss`: PyTorch module (default=None, uses loss), Instantiated validation loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `max_steps`: int (default=1000), Maximum number of training steps.<br>\n    `learning_rate` : float (default=1e-4), Learning rate.<br>\n    `num_lr_decays`: int (default=-1), Number of learning rate decays, evenly distributed across max_steps. If -1, no learning rate decay is performed.<br>\n    `early_stop_patience_steps` : int (default=-1), Number of validation iterations before early stopping. If -1, no early stopping is performed.<br>\n    `val_check_steps` : int (default=100), Number of training steps between every validation loss check.<br>\n    `batch_size` : int (default=32), Number of different series in each batch.<br>\n    `valid_batch_size` : int (default=None), Number of different series in each validation and test batch, if None uses batch_size.<br>\n    `windows_batch_size` : int (default=64), Number of windows to sample in each training batch.<br>\n    `inference_windows_batch_size` : int (default=256), Number of windows to sample in each inference batch.<br>\n    `start_padding_enabled` : bool (default=False), If True, the model will pad the time series with zeros at the beginning by input size.<br>\n    `step_size` : int (default=1), Step size between each window of temporal data.<br>\n    `scaler_type` : str (default='standard'), Type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n    `random_seed` : int (default=1), Random_seed for pytorch initializer and numpy generators.<br>\n    `drop_last_loader` : bool (default=False), If True `TimeSeriesDataLoader` drops last non-full batch.<br>\n    `alias` : str, optional (default=None), Custom name of the model.<br>\n    `optimizer`: Subclass of 'torch.optim.Optimizer', optional (default=None), User specified optimizer instead of the default choice (Adam).<br>\n    `optimizer_kwargs`: dict, optional (defualt=None), List of parameters used by the user specified `optimizer`.<br>\n    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>        \n    `dataloader_kwargs`: dict, optional (default=None), List of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n    `**trainer_kwargs`: Keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer)\n\n\tReferences\n\t----------\n    Haixu Wu and Tengge Hu and Yong Liu and Hang Zhou and Jianmin Wang and Mingsheng Long. TimesNet: Temporal 2D-Variation Modeling for General Time Series Analysis. https://openreview.net/pdf?id=ju_Uqw384Oq\n    \"\"\"\n    # Class attributes\n    EXOGENOUS_FUTR = True\n    EXOGENOUS_HIST = False\n    EXOGENOUS_STAT = False    \n    MULTIVARIATE = False    # If the model produces multivariate forecasts (True) or univariate (False)\n    RECURRENT = False       # If the model produces forecasts recursively (True) or direct (False)\n\n    def __init__(self,\n                 h: int, \n                 input_size: int,\n                 stat_exog_list = None,\n                 hist_exog_list = None,\n                 futr_exog_list = None,\n                 exclude_insample_y = False,\n                 hidden_size: int = 64, \n                 dropout: float = 0.1,\n                 conv_hidden_size: int = 64,\n                 top_k: int = 5,\n                 num_kernels: int = 6,\n                 encoder_layers: int = 2,\n                 loss = MAE(),\n                 valid_loss = None,\n                 max_steps: int = 1000,\n                 learning_rate: float = 1e-4,\n                 num_lr_decays: int = -1,\n                 early_stop_patience_steps: int =-1,\n                 val_check_steps: int = 100,\n                 batch_size: int = 32,\n                 valid_batch_size: Optional[int] = None,\n                 windows_batch_size = 64,\n                 inference_windows_batch_size = 256,\n                 start_padding_enabled = False,\n                 step_size: int = 1,\n                 scaler_type: str = 'standard',\n                 random_seed: int = 1,\n                 drop_last_loader: bool = False,\n                 alias: Optional[str] = None,\n                 optimizer = None,\n                 optimizer_kwargs = None,\n                 lr_scheduler = None,\n                 lr_scheduler_kwargs = None,       \n                 dataloader_kwargs = None,          \n                 **trainer_kwargs):\n        super(TimesNet, self).__init__(h=h,\n                                       input_size=input_size,\n                                       hist_exog_list=hist_exog_list,\n                                       stat_exog_list=stat_exog_list,\n                                       futr_exog_list = futr_exog_list,\n                                       exclude_insample_y = exclude_insample_y,\n                                       loss=loss,\n                                       valid_loss=valid_loss,\n                                       max_steps=max_steps,\n                                       learning_rate=learning_rate,\n                                       num_lr_decays=num_lr_decays,\n                                       early_stop_patience_steps=early_stop_patience_steps,\n                                       val_check_steps=val_check_steps,\n                                       batch_size=batch_size,\n                                       windows_batch_size=windows_batch_size,\n                                       valid_batch_size=valid_batch_size,\n                                       inference_windows_batch_size=inference_windows_batch_size,\n                                       start_padding_enabled = start_padding_enabled,\n                                       step_size=step_size,\n                                       scaler_type=scaler_type,\n                                       drop_last_loader=drop_last_loader,\n                                       alias=alias,\n                                       random_seed=random_seed,\n                                       optimizer=optimizer,\n                                       optimizer_kwargs=optimizer_kwargs,\n                                       lr_scheduler=lr_scheduler,\n                                       lr_scheduler_kwargs=lr_scheduler_kwargs,  \n                                       dataloader_kwargs=dataloader_kwargs,                                    \n                                       **trainer_kwargs)\n\n        # Architecture\n        self.c_out = self.loss.outputsize_multiplier\n        self.enc_in = 1 \n        self.dec_in = 1\n\n        self.model = nn.ModuleList([TimesBlock(input_size=input_size,\n                                               h=h,\n                                               k=top_k,\n                                               hidden_size=hidden_size,\n                                               conv_hidden_size=conv_hidden_size,\n                                               num_kernels=num_kernels)\n                                    for _ in range(encoder_layers)])\n\n        self.enc_embedding = DataEmbedding(c_in=self.enc_in,\n                                            exog_input_size=self.futr_exog_size,\n                                            hidden_size=hidden_size, \n                                            pos_embedding=True, # Original implementation uses true\n                                            dropout=dropout)\n        self.encoder_layers = encoder_layers\n        self.layer_norm = nn.LayerNorm(hidden_size)\n        self.predict_linear = nn.Linear(self.input_size, self.h + self.input_size)\n        self.projection = nn.Linear(hidden_size, self.c_out, bias=True)\n\n    def forward(self, windows_batch):\n\n        # Parse windows_batch\n        insample_y    = windows_batch['insample_y']\n        futr_exog     = windows_batch['futr_exog']\n\n        # Parse inputs\n        if self.futr_exog_size > 0:\n            x_mark_enc = futr_exog[:,:self.input_size,:]\n        else:\n            x_mark_enc = None\n\n        # embedding\n        enc_out = self.enc_embedding(insample_y, x_mark_enc)\n        enc_out = self.predict_linear(enc_out.permute(0, 2, 1)).permute(0, 2, 1)  # align temporal dimension\n        # TimesNet\n```\n\n----------------------------------------\n\nTITLE: Using AutoDeepNPTS with Custom Configuration\nDESCRIPTION: Example of instantiating and using the AutoDeepNPTS model with custom configuration and different backends (Ray and Optuna). Shows how to fit the model, predict values, and configure with Optuna.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_59\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n# Use your own config or AutoDeepNPTS.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12)\nmodel = AutoDeepNPTS(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n# Optuna\nmodel = AutoDeepNPTS(h=12, config=None, backend='optuna')\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for PatchTST Model\nDESCRIPTION: Imports necessary Python libraries and modules for implementing the PatchTST model, including PyTorch, numpy, and custom modules from neuralforecast.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.patchtst.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport math\nimport numpy as np\nfrom typing import Optional #, Any, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom neuralforecast.common._base_model import BaseModel\nfrom neuralforecast.common._modules import RevIN\n\nfrom neuralforecast.losses.pytorch import MAE\n```\n\n----------------------------------------\n\nTITLE: Importing NeuralForecast Models\nDESCRIPTION: Imports the NeuralForecast package and specific models (LSTM, NHITS, RNN) for time series forecasting.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/getting-started/02_quickstart.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import LSTM, NHITS, RNN\n```\n\n----------------------------------------\n\nTITLE: Implementing Distributed Training in Python\nDESCRIPTION: This snippet defines methods for distributed training using PyTorch Lightning and PySpark. It handles configuration for multi-node and multi-GPU setups.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ndef _fit_distributed(\n    self,\n    distributed_config,\n    datamodule,\n    val_size,\n    test_size,\n):\n    assert distributed_config is not None\n    from pyspark.ml.torch.distributor import TorchDistributor\n\n    def train_fn(\n        model_cls,\n        model_params,\n        datamodule,\n        trainer_kwargs,\n        num_tasks,\n        num_proc_per_task,\n        val_size,\n        test_size,\n    ):\n        import pytorch_lightning as pl\n\n        model = model_cls(**model_params)\n        model.val_size = val_size\n        model.test_size = test_size\n        for arg in ('devices', 'num_nodes'):\n            trainer_kwargs.pop(arg, None)\n        trainer = pl.Trainer(\n            strategy=\"ddp\",\n            use_distributed_sampler=False,\n            num_nodes=num_tasks,\n            devices=num_proc_per_task,\n            **trainer_kwargs,\n        )\n        trainer.fit(model=model, datamodule=datamodule)\n        model.metrics = trainer.callback_metrics\n        model.__dict__.pop('_trainer', None)\n        return model\n\n    def is_gpu_accelerator(accelerator):\n        from pytorch_lightning.accelerators.cuda import CUDAAccelerator\n\n        return (\n            accelerator == \"gpu\"\n            or isinstance(accelerator, CUDAAccelerator)\n            or (accelerator == \"auto\" and CUDAAccelerator.is_available())\n        )\n\n    local_mode = distributed_config.num_nodes == 1\n    if local_mode:\n        num_tasks = 1\n        num_proc_per_task = distributed_config.devices\n    else:\n        num_tasks = distributed_config.num_nodes * distributed_config.devices\n        num_proc_per_task = 1\n    num_proc = num_tasks * num_proc_per_task\n    use_gpu = is_gpu_accelerator(self.trainer_kwargs[\"accelerator\"])\n    model = TorchDistributor(\n        num_processes=num_proc,\n        local_mode=local_mode,\n        use_gpu=use_gpu,\n    ).run(\n        train_fn,\n        model_cls=type(self),\n        model_params=self.hparams,\n        datamodule=datamodule,\n        trainer_kwargs=self.trainer_kwargs,\n        num_tasks=num_tasks,\n        num_proc_per_task=num_proc_per_task,\n        val_size=val_size,\n        test_size=test_size,\n    )\n    return model\n```\n\n----------------------------------------\n\nTITLE: Suppressing PyTorch Lightning Logs\nDESCRIPTION: Sets the logging level for PyTorch Lightning to ERROR to reduce verbose output during model training.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/getting-started/02_quickstart.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nlogging.getLogger('pytorch_lightning').setLevel(logging.ERROR)\n```\n\n----------------------------------------\n\nTITLE: Implementing Forward Pass Method with Series Decomposition\nDESCRIPTION: Forward pass implementation that processes input through self-attention, cross-attention, and convolution layers with series decomposition. Includes dropout, activation functions, and residual connections. Returns processed sequence and residual trend.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.autoformer.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndef forward(self, x, cross, x_mask=None, cross_mask=None):\n        x = x + self.dropout(self.self_attention(\n            x, x, x,\n            attn_mask=x_mask\n        )[0])\n        x, trend1 = self.decomp1(x)\n        x = x + self.dropout(self.cross_attention(\n            x, cross, cross,\n            attn_mask=cross_mask\n        )[0])\n        x, trend2 = self.decomp2(x)\n        y = x\n        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n        y = self.dropout(self.conv2(y).transpose(-1, 1))\n        x, trend3 = self.decomp3(x + y)\n\n        residual_trend = trend1 + trend2 + trend3\n        residual_trend = self.projection(residual_trend.permute(0, 2, 1)).transpose(1, 2)\n        return x, residual_trend\n```\n\n----------------------------------------\n\nTITLE: Implementing Scaled Dot-Product Attention in PyTorch\nDESCRIPTION: Implementation of scaled dot-product attention with support for residual attention from previous layers and learnable scaling parameter. Handles key padding masks and attention masks for controlling attention flow.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.patchtst.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nclass _ScaledDotProductAttention(nn.Module):\n    \"\"\"\n    Scaled Dot-Product Attention module (Attention is all you need by Vaswani et al., 2017) with optional residual attention from previous layer\n    (Realformer: Transformer likes residual attention by He et al, 2020) and locality self sttention (Vision Transformer for Small-Size Datasets\n    by Lee et al, 2021)\n    \"\"\"\n\n    def __init__(self, hidden_size, n_heads, attn_dropout=0., res_attention=False, lsa=False):\n        super().__init__()\n        self.attn_dropout = nn.Dropout(attn_dropout)\n        self.res_attention = res_attention\n        head_dim = hidden_size // n_heads\n        self.scale = nn.Parameter(torch.tensor(head_dim ** -0.5), requires_grad=lsa)\n        self.lsa = lsa\n\n    def forward(self, q:torch.Tensor, k:torch.Tensor, v:torch.Tensor,\n                prev:Optional[torch.Tensor]=None, key_padding_mask:Optional[torch.Tensor]=None,\n                attn_mask:Optional[torch.Tensor]=None):\n        '''\n        Input shape:\n            q               : [bs x n_heads x max_q_len x d_k]\n            k               : [bs x n_heads x d_k x seq_len]\n            v               : [bs x n_heads x seq_len x d_v]\n            prev            : [bs x n_heads x q_len x seq_len]\n            key_padding_mask: [bs x seq_len]\n            attn_mask       : [1 x seq_len x seq_len]\n        Output shape:\n            output:  [bs x n_heads x q_len x d_v]\n            attn   : [bs x n_heads x q_len x seq_len]\n            scores : [bs x n_heads x q_len x seq_len]\n        '''\n\n        # Scaled MatMul (q, k) - similarity scores for all pairs of positions in an input sequence\n        attn_scores = torch.matmul(q, k) * self.scale      # attn_scores : [bs x n_heads x max_q_len x q_len]\n\n        # Add pre-softmax attention scores from the previous layer (optional)\n        if prev is not None: attn_scores = attn_scores + prev\n\n        # Attention mask (optional)\n        if attn_mask is not None:                                     # attn_mask with shape [q_len x seq_len] - only used when q_len == seq_len\n            if attn_mask.dtype == torch.bool:\n                attn_scores.masked_fill_(attn_mask, -np.inf)\n            else:\n                attn_scores += attn_mask\n\n        # Key padding mask (optional)\n        if key_padding_mask is not None:                              # mask with shape [bs x q_len] (only when max_w_len == q_len)\n            attn_scores.masked_fill_(key_padding_mask.unsqueeze(1).unsqueeze(2), -np.inf)\n\n        # normalize the attention weights\n        attn_weights = F.softmax(attn_scores, dim=-1)                 # attn_weights   : [bs x n_heads x max_q_len x q_len]\n        attn_weights = self.attn_dropout(attn_weights)\n\n        # compute the new values given the attention weights\n        output = torch.matmul(attn_weights, v)                        # output: [bs x n_heads x max_q_len x d_v]\n\n        if self.res_attention: return output, attn_weights, attn_scores\n        else: return output, attn_weights\n```\n\n----------------------------------------\n\nTITLE: Normalizing Time Series Windows in Python using PyTorch\nDESCRIPTION: This method normalizes the temporal data within time series windows to prevent data leakage during training or validation. It extracts temporal data, identifies exogenous columns, applies a scaler transformation (`self.scaler.transform`) using only lag data and an available mask, and updates the original windows dictionary with the normalized data. It requires numpy and PyTorch tensors.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n    def _normalization(self, windows, y_idx):\n        # windows are already filtered by train/validation/test\n        # from the `create_windows_method` nor leakage risk\n        temporal = windows['temporal']                  # [Ws, L + h, C, n_series]\n        temporal_cols = windows['temporal_cols'].copy() # [Ws, L + h, C, n_series]\n\n        # To avoid leakage uses only the lags\n        temporal_data_cols = self._get_temporal_exogenous_cols(temporal_cols=temporal_cols)\n        temporal_idxs = get_indexer_raise_missing(temporal_cols, temporal_data_cols)\n        temporal_idxs = np.append(y_idx, temporal_idxs)\n        temporal_data = temporal[:, :, temporal_idxs] \n        temporal_mask = temporal[:, :, temporal_cols.get_loc('available_mask')].clone()\n        if self.h > 0:\n            temporal_mask[:, -self.h:] = 0.0\n\n        # Normalize. self.scaler stores the shift and scale for inverse transform\n        temporal_mask = temporal_mask.unsqueeze(2) # Add channel dimension for scaler.transform.\n        temporal_data = self.scaler.transform(x=temporal_data, mask=temporal_mask)\n\n        # Replace values in windows dict\n        temporal[:, :, temporal_idxs] = temporal_data\n        windows['temporal'] = temporal\n\n        return windows\n```\n\n----------------------------------------\n\nTITLE: Example Usage of TimeLLM for Air Passengers Forecasting\nDESCRIPTION: Demonstrates how to use the TimeLLM model with the NeuralForecast framework for forecasting air passenger data. Shows data preparation, model initialization with a GPT-2 base model, and execution of the forecasting process.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timellm.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n#| eval: false\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import TimeLLM\nfrom neuralforecast.utils import AirPassengersPanel\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nprompt_prefix = \"The dataset contains data on monthly air passengers. There is a yearly seasonality\"\n\ntimellm = TimeLLM(h=12,\n                 input_size=36,\n                 llm='openai-community/gpt2',\n                 prompt_prefix=prompt_prefix,\n                 batch_size=16,\n                 valid_batch_size=16,\n                 windows_batch_size=16)\n\nnf = NeuralForecast(\n    models=[timellm],\n    freq='ME'\n)\n\nnf.fit(df=Y_train_df, val_size=12)\nforecasts = nf.predict(futr_df=Y_test_df)\n```\n\n----------------------------------------\n\nTITLE: Forward Method Implementation for iTransformer\nDESCRIPTION: Defines the forward pass of the iTransformer model, extracting the insample_y data from the input batch, applying the forecast method, and reshaping the output to the required dimensions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.itransformer.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, windows_batch):\n    insample_y = windows_batch['insample_y']\n\n    y_pred = self.forecast(insample_y)\n    y_pred = y_pred.reshape(insample_y.shape[0],\n                            self.h,\n                            -1)\n\n    return y_pred\n```\n\n----------------------------------------\n\nTITLE: Creating Neural Network Stack Structure in Python\nDESCRIPTION: Method for creating the hierarchical stack structure of the NHITS model. Handles block creation with identity basis functions and configures interpolation parameters for each stack level.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nhits.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndef create_stack(self,\n                     h, \n                     input_size,    \n                     stack_types, \n                     n_blocks,\n                     mlp_units,\n                     n_pool_kernel_size,\n                     n_freq_downsample,\n                     pooling_mode,\n                     interpolation_mode,\n                     dropout_prob_theta, \n                     activation,\n                     futr_input_size, hist_input_size, stat_input_size)\n```\n\n----------------------------------------\n\nTITLE: Implementing MinTraceWLS Reconciliation Matrix in Python\nDESCRIPTION: Defines a function to create a MinTraceWLS reconciliation matrix as proposed by Wickramasuriya et al. It uses a weighted GLS estimator and an estimator of the covariance matrix of coherency errors.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.hint.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef get_mintrace_wls_P(S: np.ndarray):\n    \"\"\"MinTraceOLS Reconciliation Matrix.\n\n    Creates MinTraceOLS reconciliation matrix as proposed by Wickramasuriya et al.\n    Depending on a weighted GLS estimator and an estimator of the covariance matrix of the coherency errors $\\mathbf{W}_{h}$.\n\n    $$ \\mathbf{W}_{h} = \\mathrm{Diag}(\\mathbf{S} \\mathbb{1}_{[b]})$$\n\n    $$\\mathbf{P}_{\\\\text{MinTraceWLS}}=\\\\left(\\mathbf{S}^{\\intercal}\\mathbf{W}_{h}\\mathbf{S}\\\\right)^{-1}\n    \\mathbf{S}^{\\intercal}\\mathbf{W}^{-1}_{h}$$    \n\n    **Parameters:**<br>\n    `S`: Summing matrix of size (`base`, `bottom`).<br>\n      \n    **Returns:**<br>\n    `P`: Reconciliation matrix of size (`bottom`, `base`).<br>\n\n    **References:**<br>\n    - [Wickramasuriya, S.L., Turlach, B.A. & Hyndman, R.J. (2020). \\\"Optimal non-negative\n    forecast reconciliation\". Stat Comput 30, 11671182,\n    https://doi.org/10.1007/s11222-020-09930-0](https://robjhyndman.com/publications/nnmint/).\n    \"\"\"\n    n_hiers, n_bottom = S.shape\n    n_agg = n_hiers - n_bottom\n    \n    W = np.diag(S @ np.ones((n_bottom,)))\n\n    # We compute reconciliation matrix with\n    # Equation 10 from https://robjhyndman.com/papers/MinT.pdf\n    A = S[:n_agg,:]\n    U = np.hstack((np.eye(n_agg), -A)).T\n    J = np.hstack((np.zeros((n_bottom,n_agg)), np.eye(n_bottom)))\n    P = J - (J @ W @ U) @ np.linalg.pinv(U.T @ W @ U) @ U.T\n    return P\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom NHITS Model with ReduceLROnPlateau\nDESCRIPTION: Creates a custom NHITS model class that overrides the configure_optimizers() method to implement ReduceLROnPlateau scheduler and trains both default and custom models.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/21_configure_optimizers.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nhorizon = 12\ninput_size = 24\n\nclass CustomNHITS(NHITS):\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adadelta(params=self.parameters(), rho=0.75)\n        scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(\n            optimizer=optimizer, mode='min',factor=0.5, patience=2,\n        )\n        scheduler_config = {\n            'scheduler': scheduler,\n            'interval': 'step',\n            'frequency': 1,\n            'monitor': 'train_loss',\n            'strict': True,\n            'name': None,\n        }\n        return {'optimizer': optimizer, 'lr_scheduler': scheduler_config}\n\nmodels = [\n    NHITS(h=horizon, input_size=input_size, max_steps=100, alias='NHITS-default-scheduler'),\n    CustomNHITS(h=horizon, input_size=input_size, max_steps=100, alias='NHITS-ReduceLROnPlateau-scheduler'),\n]\nnf = NeuralForecast(models=models, freq='ME')\nnf.fit(AirPassengersPanel_train)\npreds = nf.predict(futr_df=AirPassengersPanel_test)\n```\n\n----------------------------------------\n\nTITLE: Implementing TemporalNorm Class in PyTorch\nDESCRIPTION: Defines a TemporalNorm class that inherits from nn.Module. This class implements temporal normalization for time series data with various scaling options. It includes methods for initialization, transformation, and inverse transformation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.scalers.ipynb#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nclass TemporalNorm(nn.Module):\n    \"\"\" Temporal Normalization\n\n    Standardization of the features is a common requirement for many \n    machine learning estimators, and it is commonly achieved by removing \n    the level and scaling its variance. The `TemporalNorm` module applies \n    temporal normalization over the batch of inputs as defined by the type of scaler.\n\n    $$\\mathbf{z}_{[B,T,C]} = \\textrm{Scaler}(\\mathbf{x}_{[B,T,C]})$$\n\n    If `scaler_type` is `revin` learnable normalization parameters are added on top of\n    the usual normalization technique, the parameters are learned through scale decouple\n    global skip connections. The technique is available for point and probabilistic outputs.\n\n    $$\\mathbf{\\hat{z}}_{[B,T,C]} = \\boldsymbol{\\hat{\\gamma}}_{[1,1,C]} \\mathbf{z}_{[B,T,C]} +\\boldsymbol{\\hat{\\beta}}_{[1,1,C]}$$\n\n    **Parameters:**<br>\n    `scaler_type`: str, defines the type of scaler used by TemporalNorm. Available [`identity`, `standard`, `robust`, `minmax`, `minmax1`, `invariant`, `revin`].<br>\n    `dim` (int, optional): Dimension over to compute scale and shift. Defaults to -1.<br>\n    `eps` (float, optional): Small value to avoid division by zero. Defaults to 1e-6.<br>\n    `num_features`: int=None, for RevIN-like learnable affine parameters initialization.<br>\n\n    **References**<br>\n    - [Kin G. Olivares, David Luo, Cristian Challu, Stefania La Vattiata, Max Mergenthaler, Artur Dubrawski (2023). \"HINT: Hierarchical Mixture Networks For Coherent Probabilistic Forecasting\". Neural Information Processing Systems, submitted. Working Paper version available at arxiv.](https://arxiv.org/abs/2305.07089)<br>\n    \"\"\"\n    def __init__(self, scaler_type='robust', dim=-1, eps=1e-6, num_features=None):\n        super().__init__()\n        compute_statistics = {None: identity_statistics,\n                              'identity': identity_statistics,\n                              'standard': std_statistics,\n                              'revin': std_statistics,\n                              'robust': robust_statistics,\n                              'minmax': minmax_statistics,\n                              'minmax1': minmax1_statistics,\n                              'invariant': invariant_statistics,}\n        scalers = {None: identity_scaler,\n                   'identity': identity_scaler,\n                   'standard': std_scaler,\n                   'revin': std_scaler,\n                   'robust': robust_scaler,\n                   'minmax': minmax_scaler,\n                   'minmax1': minmax1_scaler,\n                   'invariant':invariant_scaler,}\n        inverse_scalers = {None: inv_identity_scaler,\n                    'identity': inv_identity_scaler,\n                    'standard': inv_std_scaler,\n                    'revin': inv_std_scaler,\n                    'robust': inv_robust_scaler,\n                    'minmax': inv_minmax_scaler,\n                    'minmax1': inv_minmax1_scaler,\n                    'invariant': inv_invariant_scaler,}\n        assert (scaler_type in scalers.keys()), f'{scaler_type} not defined'\n        if (scaler_type=='revin') and (num_features is None):\n            raise Exception('You must pass num_features for ReVIN scaler.')\n\n        self.compute_statistics = compute_statistics[scaler_type]\n        self.scaler = scalers[scaler_type]\n        self.inverse_scaler = inverse_scalers[scaler_type]\n        self.scaler_type = scaler_type\n        self.dim = dim\n        self.eps = eps\n\n        if (scaler_type=='revin'):\n            self._init_params(num_features=num_features)\n\n    def _init_params(self, num_features):\n        # Initialize RevIN scaler params to broadcast:\n        if self.dim==1: # [B,T,C]  [1,1,C]\n            self.revin_bias = nn.Parameter(torch.zeros(1, 1, num_features, 1))\n            self.revin_weight = nn.Parameter(torch.ones(1, 1, num_features, 1))\n        elif self.dim==-1: # [B,C,T]  [1,C,1]\n            self.revin_bias = nn.Parameter(torch.zeros(1, num_features, 1, 1))\n            self.revin_weight = nn.Parameter(torch.ones(1, num_features, 1, 1))\n\n    #@torch.no_grad()\n    def transform(self, x, mask):\n        \"\"\" Center and scale the data.\n\n        **Parameters:**<br>\n        `x`: torch.Tensor shape [batch, time, channels].<br>\n        `mask`: torch Tensor bool, shape  [batch, time] where `x` is valid and False\n                where `x` should be masked. Mask should not be all False in any column of\n                dimension dim to avoid NaNs from zero division.<br>\n\n        **Returns:**<br>\n        `z`: torch.Tensor same shape as `x`, except scaled.\n        \"\"\"\n        x_shift, x_scale = self.compute_statistics(x=x, mask=mask, dim=self.dim, eps=self.eps)\n        self.x_shift = x_shift\n        self.x_scale = x_scale\n\n        # Original Revin performs this operation\n        # z = self.revin_weight * z\n        # z = z + self.revin_bias\n        # However this is only valid for point forecast not for\n        # distribution's scale decouple technique.\n        if self.scaler_type=='revin':\n            self.x_shift = self.x_shift + self.revin_bias\n            self.x_scale = self.x_scale * (torch.relu(self.revin_weight) + self.eps)\n\n        z = self.scaler(x, x_shift, x_scale)\n        return z\n\n    #@torch.no_grad()\n    def inverse_transform(self, z, x_shift=None, x_scale=None):\n        \"\"\" Scale back the data to the original representation.\n\n        **Parameters:**<br>\n        `z`: torch.Tensor shape [batch, time, channels], scaled.<br>\n\n        **Returns:**<br>\n        `x`: torch.Tensor original data.\n        \"\"\"\n\n        if x_shift is None:\n            x_shift = self.x_shift\n        if x_scale is None:\n            x_scale = self.x_scale\n\n        # Original Revin performs this operation\n        # z = z - self.revin_bias\n        # z = (z / (self.revin_weight + self.eps))\n        # However this is only valid for point forecast not for\n        # distribution's scale decouple technique.\n\n        x = self.inverse_scaler(z, x_shift, x_scale)\n        return x\n\n    def forward(self, x):\n        # The gradients are optained from BaseWindows/BaseRecurrent forwards.\n        pass\n```\n\n----------------------------------------\n\nTITLE: Tensor Processing and TCN Operations in PyTorch\nDESCRIPTION: Core implementation of BiTCN forward pass that processes historical, static and future exogenous variables through bidirectional TCNs. Handles tensor reshaping, concatenation and applies neural network layers to generate forecasts.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.bitcn.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Concatenate x with historic exogenous\nbatch_size, seq_len = x.shape[:2]                               #   B = batch_size, L = seq_len\nif self.hist_exog_size > 0:\n    x = torch.cat((x, hist_exog), dim=2)                        #   [B, L, 1] + [B, L, X] -> [B, L, 1 + X]\n\n# Concatenate x with static exogenous\nif self.stat_exog_size > 0:\n    stat_exog = stat_exog.unsqueeze(1).repeat(1, seq_len, 1)    #   [B, S] -> [B, L, S]\n    x = torch.cat((x, stat_exog), dim=2)                        #   [B, L, 1 + X] + [B, L, S] -> [B, L, 1 + X + S]\n\n# Concatenate x with future exogenous & apply forward TCN to x_futr\nif self.futr_exog_size > 0:\n    x = torch.cat((x, futr_exog[:, :seq_len]), dim=2)           #   [B, L, 1 + X + S] + [B, L, F] -> [B, L, 1 + X + S + F]\n    x_futr = self.drop_futr(self.lin_futr(futr_exog))           #   [B, L + h, F] -> [B, L + h, hidden_size]\n    x_futr = x_futr.permute(0, 2, 1)                            #   [B, L + h, hidden_size] -> [B, hidden_size, L + h]\n    _, x_futr = self.net_fwd((x_futr, 0))                       #   [B, hidden_size, L + h] -> [B, hidden_size, L + h]\n    x_futr_L = x_futr[:, :, :seq_len]                           #   [B, hidden_size, L + h] -> [B, hidden_size, L]\n    x_futr_h = x_futr[:, :, seq_len:]                           #   [B, hidden_size, L + h] -> [B, hidden_size, h]\n\n# Apply backward TCN to x\nx = self.drop_hist(self.lin_hist(x))                            #   [B, L, 1 + X + S + F] -> [B, L, hidden_size]\nx = x.permute(0, 2, 1)                                          #   [B, L, hidden_size] -> [B, hidden_size, L]\n_, x = self.net_bwd((x, 0))                                     #   [B, hidden_size, L] -> [B, hidden_size, L]\n\n# Concatenate with future exogenous for seq_len\nif self.futr_exog_size > 0:\n    x = torch.cat((x, x_futr_L), dim=1)                         #   [B, hidden_size, L] + [B, hidden_size, L] -> [B, 2 * hidden_size, L]\n\n# Temporal dense layer to go to output horizon\nx = self.drop_temporal(F.gelu(self.temporal_lin1(x)))           #   [B, 2 * hidden_size, L] -> [B, 2 * hidden_size, hidden_size]\nx = self.temporal_lin2(x)                                       #   [B, 2 * hidden_size, hidden_size] -> [B, 2 * hidden_size, h]\n\n# Concatenate with future exogenous for horizon\nif self.futr_exog_size > 0:\n    x = torch.cat((x, x_futr_h), dim=1)                         #   [B, 2 * hidden_size, h] + [B, hidden_size, h] -> [B, 3 * hidden_size, h]\n\n# Output layer to create forecasts\nx = x.permute(0, 2, 1)                                          #   [B, 3 * hidden_size, h] -> [B, h, 3 * hidden_size]\nforecast = self.output_lin(x)                                   #   [B, h, 3 * hidden_size] -> [B, h, n_outputs] \n\nreturn forecast\n```\n\n----------------------------------------\n\nTITLE: Implementing TSMixerx Neural Network Forward Pass in Python\nDESCRIPTION: This code snippet defines the forward pass of the TSMixerx neural network model. It processes input data through various layers including temporal projection, feature mixing, and mixing blocks. The model supports static and future exogenous variables and applies reverse instance normalization if specified.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tsmixerx.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, windows_batch):\n        # Parse batch\n        x             = windows_batch['insample_y']                 #   [batch_size (B), input_size (L), n_series (N)]\n        hist_exog     = windows_batch['hist_exog']                  #   [B, hist_exog_size (X), L, N]\n        futr_exog     = windows_batch['futr_exog']                  #   [B, futr_exog_size (F), L + h, N]\n        stat_exog     = windows_batch['stat_exog']                  #   [N, stat_exog_size (S)]\n        batch_size, input_size = x.shape[:2]\n\n        # Apply revin to x\n        if self.revin:\n            x = self.norm(x, mode=\"norm\")                       #   [B, L, N] -> [B, L, N]\n\n        # Add channel dimension to x\n        x = x.unsqueeze(1)                                      #   [B, L, N] -> [B, 1, L, N]\n\n        # Concatenate x with historical exogenous\n        if self.hist_exog_size > 0:\n            x = torch.cat((x, hist_exog), dim=1)                #   [B, 1, L, N] + [B, X, L, N] -> [B, 1 + X, L, N]\n\n        # Concatenate x with future exogenous of input sequence\n        if self.futr_exog_size > 0:\n            futr_exog_hist = futr_exog[:, :, :input_size]       #   [B, F, L + h, N] -> [B, F, L, N]\n            x = torch.cat((x, futr_exog_hist), dim=1)           #   [B, 1 + X, L, N] + [B, F, L, N] -> [B, 1 + X + F, L, N]\n            \n        # Temporal projection & feature mixing of x\n        x = x.permute(0, 1, 3, 2)                               #   [B, 1 + X + F, L, N] -> [B, 1 + X + F, N, L]\n        x = self.temporal_projection(x)                         #   [B, 1 + X + F, N, L] -> [B, 1 + X + F, N, h]\n        x = x.permute(0, 3, 1, 2)                               #   [B, 1 + X + F, N, h] -> [B, h, 1 + X + F, N]\n        x = x.reshape(batch_size, self.h, -1)                   #   [B, h, 1 + X + F, N] -> [B, h, (1 + X + F) * N]\n        x = self.feature_mixer_hist(x)                          #   [B, h, (1 + X + F) * N] -> [B, h, ff_dim] \n\n        # Concatenate x with future exogenous of output horizon\n        if self.futr_exog_size > 0:\n            x_futr = futr_exog[:, :, input_size:]               #   [B, F, L + h, N] -> [B, F, h, N] \n            x_futr = x_futr.permute(0, 2, 1, 3)                 #   [B, F, h, N] -> [B, h, F, N] \n            x_futr = x_futr.reshape(batch_size, \n                                    self.h, -1)                 #   [B, h, N, F] -> [B, h, N * F]\n            x_futr = self.feature_mixer_futr(x_futr)            #   [B, h, N * F] -> [B, h, ff_dim] \n            x = torch.cat((x, x_futr), dim=2)                   #   [B, h, ff_dim] + [B, h, ff_dim] -> [B, h, 2 * ff_dim]\n\n        # Concatenate x with static exogenous\n        if self.stat_exog_size > 0:\n            stat_exog = stat_exog.reshape(-1)                   #   [N, S] -> [N * S]\n            stat_exog = stat_exog.unsqueeze(0)\\\n                                 .unsqueeze(1)\\\n                                 .repeat(batch_size, \n                                         self.h, \n                                         1)                     #   [N * S] -> [B, h, N * S]\n            x_stat = self.feature_mixer_stat(stat_exog)         #   [B, h, N * S] -> [B, h, ff_dim] \n            x = torch.cat((x, x_stat), dim=2)                   #   [B, h, 2 * ff_dim] + [B, h, ff_dim] -> [B, h, 3 * ff_dim] \n\n        # First mixing layer\n        x = self.first_mixing(x)                                #   [B, h, 3 * ff_dim] -> [B, h, ff_dim] \n\n        # N blocks of mixing layers\n        if self.stat_exog_size > 0:\n            x, _ = self.mixing_block((x, stat_exog))            #   [B, h, ff_dim], [B, h, N * S] -> [B, h, ff_dim]  \n        else:\n            x = self.mixing_block(x)                            #   [B, h, ff_dim] -> [B, h, ff_dim] \n      \n        # Fully connected output layer\n        forecast = self.out(x)                                  #   [B, h, ff_dim] -> [B, h, N * n_outputs]\n        \n        # Reverse Instance Normalization on output\n        if self.revin:\n            forecast = forecast.reshape(batch_size, \n                          self.h * self.loss.outputsize_multiplier,\n                          -1)                                   #   [B, h, N * n_outputs] -> [B, h * n_outputs, N]\n            forecast = self.norm(forecast, \"denorm\")\n            forecast = forecast.reshape(batch_size, self.h, -1) #   [B, h * n_outputs, N] -> [B, h, n_outputs * N]\n\n        return forecast\n```\n\n----------------------------------------\n\nTITLE: LSTM Forward Pass Implementation - PyTorch\nDESCRIPTION: Forward pass implementation that processes input batches, concatenates different types of exogenous variables, and generates predictions using either recurrent or non-recurrent mode.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.lstm.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, windows_batch):\n    # Parse windows_batch\n    encoder_input = windows_batch['insample_y']                         # [B, seq_len, 1]\n    futr_exog     = windows_batch['futr_exog']                          # [B, seq_len, F]\n    hist_exog     = windows_batch['hist_exog']                          # [B, seq_len, X]\n    stat_exog     = windows_batch['stat_exog']                          # [B, S]\n\n    # Concatenate y, historic and static inputs              \n    batch_size, seq_len = encoder_input.shape[:2]\n    if self.hist_exog_size > 0:\n        encoder_input = torch.cat((encoder_input, hist_exog), dim=2)    # [B, seq_len, 1] + [B, seq_len, X] -> [B, seq_len, 1 + X]\n\n    if self.stat_exog_size > 0:\n        stat_exog = stat_exog.unsqueeze(1).repeat(1, seq_len, 1)        # [B, S] -> [B, seq_len, S]\n        encoder_input = torch.cat((encoder_input, stat_exog), dim=2)    # [B, seq_len, 1 + X] + [B, seq_len, S] -> [B, seq_len, 1 + X + S]\n\n    if self.futr_exog_size > 0:\n        encoder_input = torch.cat((encoder_input, \n                                   futr_exog[:, :seq_len]), dim=2)      # [B, seq_len, 1 + X + S] + [B, seq_len, F] -> [B, seq_len, 1 + X + S + F]\n\n    if self.RECURRENT:\n        if self.maintain_state:\n            rnn_state = self.rnn_state\n        else:\n            rnn_state = None\n        \n        output, rnn_state = self.hist_encoder(encoder_input, \n                                                        rnn_state)      # [B, seq_len, n_output]\n        if self.maintain_state:\n            self.rnn_state = rnn_state\n    else:\n        hidden_state, _ = self.hist_encoder(encoder_input, None)       # [B, seq_len, rnn_hidden_state]\n        hidden_state = hidden_state[:, -self.h:]                       # [B, seq_len, rnn_hidden_state] -> [B, h, rnn_hidden_state]\n        \n        if self.futr_exog_size > 0:\n            futr_exog_futr = futr_exog[:, -self.h:]                    # [B, h, F]\n            hidden_state = torch.cat((hidden_state, \n                                      futr_exog_futr), dim=-1)          # [B, h, rnn_hidden_state] + [B, h, F] -> [B, h, rnn_hidden_state + F]\n\n        output = self.mlp_decoder(hidden_state)                        # [B, h, rnn_hidden_state + F] -> [B, seq_len, n_output]\n\n    return output[:, -self.h:]\n```\n\n----------------------------------------\n\nTITLE: Setting Up Trainer Arguments in Python\nDESCRIPTION: This code configures trainer arguments, including max steps, early stopping, GPU acceleration, and checkpointing. It also handles batch sizes and padding for training windows.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ntrainer_kwargs = {**trainer_kwargs, 'max_steps': max_steps}\n\nif 'max_epochs' in trainer_kwargs.keys():\n    raise Exception('max_epochs is deprecated, use max_steps instead.')\n\nif early_stop_patience_steps > 0:\n    if 'callbacks' not in trainer_kwargs:\n        trainer_kwargs['callbacks'] = []\n    trainer_kwargs['callbacks'].append(\n        EarlyStopping(\n            monitor='ptl/val_loss', patience=early_stop_patience_steps\n        )\n    )\n\nif trainer_kwargs.get('accelerator', None) is None:\n    if torch.cuda.is_available():\n        trainer_kwargs['accelerator'] = \"gpu\"\nif trainer_kwargs.get('devices', None) is None:\n    if torch.cuda.is_available():\n        trainer_kwargs['devices'] = -1\n\nif trainer_kwargs.get('enable_checkpointing', None) is None:\n    trainer_kwargs['enable_checkpointing'] = False\n\nself.trainer_kwargs = trainer_kwargs\nself.h = h\nself.input_size = input_size\nself.windows_batch_size = windows_batch_size\nself.start_padding_enabled = start_padding_enabled\n\nif start_padding_enabled:\n    self.padder_train = nn.ConstantPad1d(padding=(self.input_size-1, self.h), value=0.0)\nelse:\n    self.padder_train = nn.ConstantPad1d(padding=(0, self.h), value=0.0)\n```\n\n----------------------------------------\n\nTITLE: Implementing Scaled Continuous Ranked Probability Score (sCRPS) in Python\nDESCRIPTION: Defines the sCRPS class, which calculates a scaled variation of the Continuous Ranked Probability Score to measure the accuracy of predicted quantiles compared to observations. It averages percentual weighted absolute deviations defined by quantile losses.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_76\n\nLANGUAGE: python\nCODE:\n```\nclass sCRPS(BasePointLoss):\n    \"\"\"Scaled Continues Ranked Probability Score\n\n    Calculates a scaled variation of the CRPS, as proposed by Rangapuram (2021),\n    to measure the accuracy of predicted quantiles `y_hat` compared to the observation `y`.\n\n    This metric averages percentual weighted absolute deviations as \n    defined by the quantile losses.\n\n    $$ \\mathrm{sCRPS}(\\mathbf{\\hat{y}}^{(q)}_{\\tau}, \\mathbf{y}_{\\tau}) = \\frac{2}{N} \\sum_{i}\n    \\int^{1}_{0}\n    \\frac{\\mathrm{QL}(\\mathbf{\\hat{y}}^{(q}_{\\tau} y_{i,\\tau})_{q}}{\\sum_{i} | y_{i,\\tau} |} dq $$\n\n    where $\\mathbf{\\hat{y}}^{(q}_{\\tau}$ is the estimated quantile, and $y_{i,\\tau}$\n    are the target variable realizations.\n\n    **Parameters:**<br>\n    `level`: int list [0,100]. Probability levels for prediction intervals (Defaults median).\n    `quantiles`: float list [0., 1.]. Alternative to level, quantiles to estimate from y distribution.\n\n    **References:**<br>\n    - [Gneiting, Tilmann. (2011). \\\"Quantiles as optimal point forecasts\\\". \n    International Journal of Forecasting.](https://www.sciencedirect.com/science/article/pii/S0169207010000063)<br>\n    - [Spyros Makridakis, Evangelos Spiliotis, Vassilios Assimakopoulos, Zhi Chen, Anil Gaba, Ilia Tsetlin, Robert L. Winkler. (2022). \n    \\\"The M5 uncertainty competition: Results, findings and conclusions\\\". \n    International Journal of Forecasting.](https://www.sciencedirect.com/science/article/pii/S0169207021001722)<br>\n    - [Syama Sundar Rangapuram, Lucien D Werner, Konstantinos Benidis, Pedro Mercado, Jan Gasthaus, Tim Januschowski. (2021). \n    \\\"End-to-End Learning of Coherent Probabilistic Forecasts for Hierarchical Time Series\\\". \n    Proceedings of the 38th International Conference on Machine Learning (ICML).](https://proceedings.mlr.press/v139/rangapuram21a.html)\n    \"\"\"\n    def __init__(self, level=[80, 90], quantiles=None):\n        super(sCRPS, self).__init__()\n        self.mql = MQLoss(level=level, quantiles=quantiles)\n        self.is_distribution_output = False\n    \n    def __call__(self,\n                 y: torch.Tensor,\n                 y_hat: torch.Tensor,\n                 y_insample: torch.Tensor,\n                 mask: Union[torch.Tensor, None] = None,\n                 ) -> torch.Tensor:\n        \"\"\"\n        **Parameters:**<br>\n        `y`: tensor, Actual values.<br>\n        `y_hat`: tensor, Predicted values.<br>\n        `mask`: tensor, Specifies date stamps per series to consider in loss.<br>\n\n        **Returns:**<br>\n        `scrps`: tensor (single value).\n        \"\"\"\n        mql = self.mql(y=y, y_hat=y_hat, mask=mask, y_insample=y_insample)\n        norm = torch.sum(torch.abs(y))\n        unmean = torch.sum(mask)\n        scrps = 2 * mql * unmean / (norm + 1e-5)\n        return scrps\n```\n\n----------------------------------------\n\nTITLE: Implementing Static Covariate Encoder in PyTorch for TFT\nDESCRIPTION: The StaticCovariateEncoder processes static covariates to produce context vectors for the model. It uses Variable Selection Networks and Gated Residual Networks to transform static inputs into context vectors that initialize the LSTM states for temporal processing.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass StaticCovariateEncoder(nn.Module):\n    def __init__(\n        self,\n        hidden_size,\n        num_static_vars,\n        dropout,\n        grn_activation,\n        rnn_type=\"lstm\",\n        n_rnn_layers=1,\n        one_rnn_initial_state=False,\n    ):\n        super().__init__()\n        self.vsn = VariableSelectionNetwork(\n            hidden_size=hidden_size,\n            num_inputs=num_static_vars,\n            dropout=dropout,\n            grn_activation=grn_activation,\n        )\n        self.rnn_type = rnn_type.lower()\n\n        self.n_rnn_layers = n_rnn_layers\n\n        self.n_states = 1 if one_rnn_initial_state else n_rnn_layers\n\n        n_contexts = 2 + 2 * self.n_states if rnn_type == \"lstm\" else 2 + self.n_states\n\n        self.context_grns = nn.ModuleList(\n            [\n                GRN(input_size=hidden_size, hidden_size=hidden_size, dropout=dropout)\n                for _ in range(n_contexts)\n            ]\n        )\n\n    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n        variable_ctx, sparse_weights = self.vsn(x)\n\n        # Context vectors:\n        # variable selection context\n        # enrichment context\n        # state_c context\n        # state_h context\n\n        cs, ce = list(m(variable_ctx) for m in self.context_grns[:2])  # type: ignore\n\n        if self.n_states == 1:\n            ch = torch.cat(\n                self.n_rnn_layers\n                * list(\n                    m(variable_ctx).unsqueeze(0)\n                    for m in self.context_grns[2 : self.n_states + 2]\n                )\n            )\n\n            if self.rnn_type == \"lstm\":\n                cc = torch.cat(\n                    self.n_rnn_layers\n                    * list(\n                        m(variable_ctx).unsqueeze(0)\n                        for m in self.context_grns[self.n_states + 2 :]\n                    )\n                )\n\n        else:\n            ch = torch.cat(\n                list(\n                    m(variable_ctx).unsqueeze(0)\n                    for m in self.context_grns[2 : self.n_states + 2]\n                )\n            )\n\n            if self.rnn_type == \"lstm\":\n                cc = torch.cat(\n                    list(\n                        m(variable_ctx).unsqueeze(0)\n                        for m in self.context_grns[self.n_states + 2 :]\n                    )\n                )\n        if self.rnn_type != \"lstm\":\n            cc = ch\n\n        return cs, ce, ch, cc, sparse_weights  # type: ignore\n```\n\n----------------------------------------\n\nTITLE: Initializing Transformer Models with Parameters\nDESCRIPTION: Creates instances of Informer, Autoformer, and PatchTST models with specified forecast horizon, input size, and training parameters for long-horizon forecasting.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/05_longhorizon_transformers.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n%%capture\nhorizon = 96 # 24hrs = 4 * 15 min.\nmodels = [Informer(h=horizon,                 # Forecasting horizon\n                input_size=horizon,           # Input size\n                max_steps=1000,               # Number of training iterations\n                val_check_steps=100,          # Compute validation loss every 100 steps\n                early_stop_patience_steps=3), # Stop training if validation loss does not improve\n          Autoformer(h=horizon,\n                input_size=horizon,\n                max_steps=1000,\n                val_check_steps=100,\n                early_stop_patience_steps=3),\n          PatchTST(h=horizon,\n                input_size=horizon,\n                max_steps=1000,\n                val_check_steps=100,\n                early_stop_patience_steps=3),\n         ]\n```\n\n----------------------------------------\n\nTITLE: Visualizing Past Variable Importance Over Time\nDESCRIPTION: Creates a stacked bar chart showing the importance of each historical variable at different time steps. This visualization helps identify which variables have the most influence at different points in the historical window.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_31\n\nLANGUAGE: python\nCODE:\n```\n# | eval: false\ndf = feature_importances[\"Past variable importance over time\"]\n\nfig, ax = plt.subplots(figsize=(20, 10))\nbottom = np.zeros(len(df.index))\n\nfor col in df.columns:\n    p = ax.bar(np.arange(-len(df), 0), df[col].values, 0.6, label=col, bottom=bottom)\n    bottom += df[col]\nax.set_title(\"Past variable importance over time\")\nax.set_ylabel(\"Importance\")\nax.set_xlabel(\"Time\")\nax.legend()\nax.grid(True)\n\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Defining PyTorch Time Series Embedding Modules\nDESCRIPTION: Defines three PyTorch `nn.Module` classes for embedding time series data. `TimeFeatureEmbedding` (partially shown in constructor context) handles temporal features like hour, weekday, day, and month. `DataEmbedding` combines value embedding (`TokenEmbedding`), optional positional embedding (`PositionalEmbedding`), and optional temporal embedding (`TimeFeatureEmbedding`). `DataEmbedding_inverted` provides an alternative embedding approach using a linear layer and optionally concatenating covariates, permuting dimensions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.modules.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n        self.weekday_embed = Embed(weekday_size, d_model)\n        self.day_embed = Embed(day_size, d_model)\n        self.month_embed = Embed(month_size, d_model)\n\n    def forward(self, x):\n        x = x.long()\n        minute_x = self.minute_embed(x[:, :, 4]) if hasattr(\n            self, 'minute_embed') else 0.\n        hour_x = self.hour_embed(x[:, :, 3])\n        weekday_x = self.weekday_embed(x[:, :, 2])\n        day_x = self.day_embed(x[:, :, 1])\n        month_x = self.month_embed(x[:, :, 0])\n\n        return hour_x + weekday_x + day_x + month_x + minute_x\n\nclass DataEmbedding(nn.Module):\n    def __init__(self, c_in, exog_input_size, hidden_size, pos_embedding=True, dropout=0.1):\n        super(DataEmbedding, self).__init__()\n\n        self.value_embedding = TokenEmbedding(c_in=c_in, hidden_size=hidden_size)\n\n        if pos_embedding:\n            self.position_embedding = PositionalEmbedding(hidden_size=hidden_size)\n        else:\n            self.position_embedding = None\n\n        if exog_input_size > 0:\n            self.temporal_embedding = TimeFeatureEmbedding(input_size=exog_input_size,\n                                                        hidden_size=hidden_size)\n        else:\n            self.temporal_embedding = None\n\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x, x_mark=None):\n\n        # Convolution\n        x = self.value_embedding(x)\n\n        # Add positional (relative withing window) embedding with sines and cosines\n        if self.position_embedding is not None:\n            x = x + self.position_embedding(x)\n\n        # Add temporal (absolute in time series) embedding with linear layer\n        if self.temporal_embedding is not None:\n            x = x + self.temporal_embedding(x_mark)            \n\n        return self.dropout(x)\n\nclass DataEmbedding_inverted(nn.Module):\n    \"\"\"\n    DataEmbedding_inverted\n    \"\"\"       \n    def __init__(self, c_in, hidden_size, dropout=0.1):\n        super(DataEmbedding_inverted, self).__init__()\n        self.value_embedding = nn.Linear(c_in, hidden_size)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x, x_mark):\n        x = x.permute(0, 2, 1)\n        # x: [Batch Variate Time]\n        if x_mark is None:\n            x = self.value_embedding(x)\n        else:\n            # the potential to take covariates (e.g. timestamps) as tokens\n            x = self.value_embedding(torch.cat([x, x_mark.permute(0, 2, 1)], 1)) \n        # x: [Batch Variate hidden_size]\n        return self.dropout(x)\n```\n\n----------------------------------------\n\nTITLE: Cross-Validation for NeuralForecast Model in Python\nDESCRIPTION: This method performs cross-validation on a NeuralForecast model without refitting. It prepares the dataset, generates forecasts for each model, and combines the results into a single dataframe.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\ndef _no_refit_cross_validation(\n    self,\n    df: Optional[DataFrame],\n    static_df: Optional[DataFrame],\n    n_windows: int,\n    step_size: int,\n    val_size: Optional[int], \n    test_size: int,\n    verbose: bool,\n    id_col: str,\n    time_col: str,\n    target_col: str,\n    **data_kwargs\n) -> DataFrame:\n    if (df is None) and not (hasattr(self, 'dataset')):\n        raise Exception('You must pass a DataFrame or have one stored.')\n\n    # Process and save new dataset (in self)\n    if df is not None:\n        validate_freq(df[time_col], self.freq)\n        self.dataset, self.uids, self.last_dates, self.ds = self._prepare_fit(\n            df=df,\n            static_df=static_df,\n            predict_only=False,\n            id_col=id_col,\n            time_col=time_col,\n            target_col=target_col,\n        )\n    else:\n        if verbose: print('Using stored dataset.')\n\n    # ... (rest of the method implementation)\n\n    return ufp.join(\n        fcsts_df,\n        df[[id_col, time_col, target_col]],\n        how='left',\n        on=[id_col, time_col],\n    )\n```\n\n----------------------------------------\n\nTITLE: Testing NBEATSx Model with AirPassengers Dataset\nDESCRIPTION: Code to test the NBEATSx model using the AirPassengers dataset. It creates train and test splits, fits the model with various stack types including exogenous variables, and visualizes the forecast results.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeatsx.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Month\nY_df['month'] = Y_df['ds'].dt.month\nY_df['year'] = Y_df['ds'].dt.year\n\nY_train_df = Y_df[Y_df.ds<Y_df['ds'].values[-12]] # 132 train\nY_test_df = Y_df[Y_df.ds>=Y_df['ds'].values[-12]]   # 12 test\n\ndataset, *_ = TimeSeriesDataset.from_df(df = Y_train_df)\nmodel = NBEATSx(h=12,\n                input_size=24,\n                scaler_type='robust',\n                stack_types = [\"identity\", \"trend\", \"seasonality\", \"exogenous\"],\n                n_blocks = [1,1,1,1],\n                futr_exog_list=['month','year'],\n                windows_batch_size=None,\n                max_steps=1)\nmodel.fit(dataset=dataset)\ndataset2 = dataset.update_dataset(dataset, Y_test_df)\nmodel.set_test_size(12)\ny_hat = model.predict(dataset=dataset2)\nY_test_df['NBEATSx'] = y_hat\n\npd.concat([Y_train_df, Y_test_df]).drop(['unique_id','month'], axis=1).set_index('ds').plot()\n```\n\n----------------------------------------\n\nTITLE: Visualizing Static Variable Importances\nDESCRIPTION: Creates a horizontal bar plot of the importance values for static covariates in the TFT model. This helps identify which static features have the most influence on the model's predictions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_27\n\nLANGUAGE: python\nCODE:\n```\n# | eval: false\nfeature_importances[\"Static covariates\"].sort_values(by=\"importance\").plot(kind=\"barh\")\n```\n\n----------------------------------------\n\nTITLE: Distributed Prediction in NeuralForecast with Spark\nDESCRIPTION: Implements distributed forecasting using Spark and Fugue. Handles data partitioning, static exogenous variables, and future exogenous variables in a distributed setting. Creates a local NeuralForecast instance for each partition.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef _predict_distributed(\n    self,\n    df: Optional[SparkDataFrame],\n    static_df: Optional[SparkDataFrame],\n    futr_df: Optional[SparkDataFrame],\n    engine,\n):\n    import fugue.api as fa\n\n    def _predict(\n        df: pd.DataFrame,\n        static_cols,\n        futr_exog_cols,\n        models,\n        freq,\n        id_col,\n        time_col,\n        target_col,\n    ) -> pd.DataFrame:\n        from neuralforecast import NeuralForecast\n\n        nf = NeuralForecast(models=models, freq=freq)\n        nf.id_col = id_col\n        nf.time_col = time_col\n        nf.target_col = target_col\n        nf.scalers_ = {}\n        nf._fitted = True\n        if futr_exog_cols:\n            # if we have futr_exog we'll have extra rows with the future values\n            futr_rows = df[target_col].isnull()\n            futr_df = df.loc[futr_rows, [self.id_col, self.time_col] + futr_exog_cols].copy()\n            df = df[~futr_rows].copy()\n        else:\n            futr_df = None\n        if static_cols:\n            static_df = df[[self.id_col] + static_cols].groupby(self.id_col, observed=True).head(1)\n            df = df.drop(columns=static_cols)\n        else:\n            static_df = None\n        return nf.predict(df=df, static_df=static_df, futr_df=futr_df)\n\n    # df\n    if isinstance(df, SparkDataFrame):\n        repartition = True\n    else:\n        if engine is None:\n            raise ValueError(\"engine is required for distributed inference\")\n        df = engine.read.parquet(*self.dataset.files)\n        # we save the datataset with partitioning\n        repartition = False\n\n    # static\n    static_cols = set(chain.from_iterable(getattr(m, 'stat_exog_list', []) for m in self.models))\n    if static_df is not None:\n        if not isinstance(static_df, SparkDataFrame):\n            raise ValueError(\n                \"`static_df` must be a spark dataframe when `df` is a spark dataframe \"\n                \"or the models were trained in a distributed setting.\\n\"\n                \"You can also provide local dataframes (pandas or polars) as `df` and `static_df`.\"\n            )\n        missing_static = static_cols - set(static_df.columns)\n        if missing_static:\n            raise ValueError(\n                f\"The following static columns are missing from the static_df: {missing_static}\"\n            )\n        # join is supposed to preserve the partitioning\n        df = df.join(static_df, on=[self.id_col], how=\"left\")\n\n    # exog\n    if futr_df is not None:\n        if not isinstance(futr_df, SparkDataFrame):\n            raise ValueError(\n                \"`futr_df` must be a spark dataframe when `df` is a spark dataframe \"\n                \"or the models were trained in a distributed setting.\\n\"\n                \"You can also provide local dataframes (pandas or polars) as `df` and `futr_df`.\"\n            )\n        if self.target_col in futr_df.columns:\n            raise ValueError(\"`futr_df` must not contain the target column.\")\n        # df has the statics, historic exog and target at this point, futr_df doesnt\n        df = df.unionByName(futr_df, allowMissingColumns=True)\n        # union doesn't guarantee preserving the partitioning\n        repartition = True\n\n    if repartition:\n        df = df.repartitionByRange(df.rdd.getNumPartitions(), self.id_col)    \n\n    # predict\n    base_schema = fa.get_schema(df).extract([self.id_col, self.time_col])\n    models_schema = {model: 'float' for model in self._get_model_names()}\n    return fa.transform(\n        df=df,\n        using=_predict,\n        schema=base_schema.append(models_schema),\n        params=dict(\n            static_cols=list(static_cols),\n            futr_exog_cols=list(self._get_needed_futr_exog()),\n            models=self.models,\n            freq=self.freq,\n            id_col=self.id_col,\n            time_col=self.time_col,\n            target_col=self.target_col,\n        ),\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing MinMax1 Scaler and Inverse Functions in Python\nDESCRIPTION: Provides functions for applying MinMax1 scaling and its inverse transformation using pre-computed statistics.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.scalers.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef minmax1_scaler(x, x_min, x_range):\n    x = (x - x_min) / x_range\n    z = x * (2) - 1\n    return z\n\ndef inv_minmax1_scaler(z, x_min, x_range):\n    z = (z + 1) / 2\n    return z * x_range + x_min\n```\n\n----------------------------------------\n\nTITLE: Splitting Time Series Data for Training and Testing\nDESCRIPTION: Prepares the AirPassengers dataset by splitting it into training (data before 1960) and testing (data from 1960) sets. The training data will be used to fit the model while the test data will be used for evaluating forecasting performance.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/12_using_mlflow.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Split data and declare panel dataset\nY_df = AirPassengersDF\nY_train_df = Y_df[Y_df.ds<='1959-12-31'] # 132 train\nY_test_df = Y_df[Y_df.ds>'1959-12-31'] # 12 test\nY_df.tail()\n```\n\n----------------------------------------\n\nTITLE: Implementing Transformer Encoder Block in PyTorch\nDESCRIPTION: A transformer encoder block implementation with multi-head self-attention, feed-forward network, layer normalization, and residual connections. Supports both pre-norm and post-norm architectures and configurable activation functions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.patchtst.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n    def forward(self, src:torch.Tensor, prev:Optional[torch.Tensor]=None,\n                key_padding_mask:Optional[torch.Tensor]=None,\n                attn_mask:Optional[torch.Tensor]=None): # -> Tuple[torch.Tensor, Any]:\n\n        # Multi-Head attention sublayer\n        if self.pre_norm:\n            src = self.norm_attn(src)\n        ## Multi-Head attention\n        if self.res_attention:\n            src2, attn, scores = self.self_attn(src, src, src, prev,\n                                                key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n        else:\n            src2, attn = self.self_attn(src, src, src, key_padding_mask=key_padding_mask, attn_mask=attn_mask)\n        if self.store_attn:\n            self.attn = attn\n        ## Add & Norm\n        src = src + self.dropout_attn(src2) # Add: residual connection with residual dropout\n        if not self.pre_norm:\n            src = self.norm_attn(src)\n\n        # Feed-forward sublayer\n        if self.pre_norm:\n            src = self.norm_ffn(src)\n        ## Position-wise Feed-Forward\n        src2 = self.ff(src)\n        ## Add & Norm\n        src = src + self.dropout_ffn(src2) # Add: residual connection with residual dropout\n        if not self.pre_norm:\n            src = self.norm_ffn(src)\n\n        if self.res_attention:\n            return src, scores\n        else:\n            return src\n```\n\n----------------------------------------\n\nTITLE: Generating Forecasts from Neural Models\nDESCRIPTION: This code generates predictions from neural forecasting models. It handles trimmed datasets, processes distributional forecasts, combines predictions, and applies inverse scaling if needed. It supports both pandas and polars DataFrame outputs.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nfor i, trimmed_dataset in enumerate(trimmed_datasets):\n    # Set test size to current series length\n    model.set_test_size(test_size=trimmed_dataset.max_size)\n    # Generate predictions\n    model_fcsts = model.predict(trimmed_dataset, step_size=step_size)\n    # Handle distributional forecasts; take only median\n    if len(model_fcsts.shape) > 1 and model_fcsts.shape[1] == 3:\n        model_fcsts = model_fcsts[:, 0]  # Take first column (median)\n    # Ensure consistent 2D shape\n    if len(model_fcsts.shape) == 1:\n        model_fcsts = model_fcsts.reshape(-1, 1)\n    model_series_preds.append(model_fcsts)\nmodel_preds = np.concatenate(model_series_preds, axis=0)\nfcsts_list.append(model_preds)\n# Reset test size to original\nmodel.set_test_size(test_size=test_size)\n\n# Combine all predictions\nfcsts = np.hstack(fcsts_list)\n\n# Add original y values\noriginal_y = {\n    self.id_col: ufp.repeat(self.uids, np.diff(self.dataset.indptr)),\n    self.time_col: self.ds,\n    self.target_col: self.dataset.temporal[:, 0].numpy(),\n}\n\n# Create forecasts DataFrame\ncols = self._get_model_names()\nselected_cols = [col for col in cols if not col.endswith(('-lo', '-hi')) and (not '-' in col or col.endswith('-median'))]\nif isinstance(self.uids, pl_Series):\n    fcsts = pl_DataFrame(dict(zip(selected_cols, fcsts.T)))\n    Y_df = pl_DataFrame(original_y)\nelse:\n    fcsts = pd.DataFrame(fcsts, columns=selected_cols)\n    Y_df = pd.DataFrame(original_y).reset_index(drop=True)\n\n# Combine forecasts with dates\nfcsts_df = ufp.horizontal_concat([fcsts_df, fcsts])\n\n# Add original values\nfcsts_df = ufp.join(fcsts_df, Y_df, how='left', on=[self.id_col, self.time_col])\n\n# Apply scaling if needed\nif self.scalers_:\n    sizes = ufp.counts_by_id(fcsts_df, self.id_col)['counts'].to_numpy()\n    indptr = np.append(0, sizes.cumsum())\n    invert_cols = cols + [self.target_col]\n    fcsts_df[invert_cols] = self._scalers_target_inverse_transform(\n        fcsts_df[invert_cols].to_numpy(),\n        indptr\n    )\nreturn fcsts_df\n```\n\n----------------------------------------\n\nTITLE: Tweedie Distribution Implementation in PyTorch\nDESCRIPTION: Implementation of the Tweedie distribution, a compound probability distribution useful for modeling sparse series. Includes helper functions for parameter estimation and scale decoupling.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\ndef est_lambda(mu, rho):\n    return mu ** (2 - rho) / (2 - rho)\n\ndef est_alpha(rho):\n    return (2 - rho) / (rho - 1)\n\ndef est_beta(mu, rho):\n    return mu ** (1 - rho) / (rho - 1)\n\n\nclass Tweedie(Distribution):\n    \"\"\" Tweedie Distribution\n\n    The Tweedie distribution is a compound probability, special case of exponential\n    dispersion models EDMs defined by its mean-variance relationship.\n    The distribution particularly useful to model sparse series as the probability has\n    possitive mass at zero but otherwise is continuous.\n\n    $Y \\sim \\mathrm{ED}(\\\\mu,\\\\sigma^{2}) \\qquad\n    \\mathbb{P}(y|\\\\mu ,\\\\sigma^{2})=h(\\\\sigma^{2},y) \\\\exp \\\\left({\\\\frac {\\\\theta y-A(\\\\theta )}{\\\\sigma^{2}}}\\\\right)$<br>\n    \n    $\\mu =A'(\\\\theta ) \\qquad \\mathrm{Var}(Y) = \\\\sigma^{2} \\\\mu^{\\\\rho}$\n    \n    Cases of the variance relationship include Normal (`rho` = 0), Poisson (`rho` = 1),\n    Gamma (`rho` = 2), inverse Gaussian (`rho` = 3).\n\n    **Parameters:**<br>\n    `log_mu`: tensor, with log of means.<br>\n    `rho`: float, Tweedie variance power (1,2). Fixed across all observations.<br>\n    `sigma2`: tensor, Tweedie variance. Currently fixed in 1.<br>\n\n    **References:**<br>\n    - [Tweedie, M. C. K. (1984). An index which distinguishes between some important exponential families. Statistics: Applications and New Directions. \n    Proceedings of the Indian Statistical Institute Golden Jubilee International Conference (Eds. J. K. Ghosh and J. Roy), pp. 579-604. Calcutta: Indian Statistical Institute.]()<br>\n    - [Jorgensen, B. (1987). Exponential Dispersion Models. Journal of the Royal Statistical Society. \n       Series B (Methodological), 49(2), 127162. http://www.jstor.org/stable/2345415](http://www.jstor.org/stable/2345415)<br>\n    \"\"\"\n    arg_constraints = {'log_mu': constraints.real}\n    support = constraints.nonnegative\n\n    def __init__(self, log_mu, rho, validate_args=None):\n        # TODO: add sigma2 dispersion\n        # TODO add constraints\n        # support = constraints.real\n        self.log_mu = log_mu\n        self.rho = rho\n        assert rho>1 and rho<2, f'rho={rho} parameter needs to be between (1,2).'\n\n        batch_shape = log_mu.size()\n        super(Tweedie, self).__init__(batch_shape, validate_args=validate_args)\n\n    @property\n    def mean(self):\n        return torch.exp(self.log_mu)\n\n    @property\n    def variance(self):\n        return torch.ones_line(self.log_mu) #TODO need to be assigned\n\n    def sample(self, sample_shape=torch.Size()):\n        shape = self._extended_shape(sample_shape)\n        with torch.no_grad():\n            mu   = self.mean\n            rho  = self.rho * torch.ones_like(mu)\n            sigma2 = 1 #TODO\n\n            rate  = est_lambda(mu, rho) / sigma2  # rate for poisson\n            alpha = est_alpha(rho)                # alpha for Gamma distribution\n            beta  = est_beta(mu, rho) / sigma2    # beta for Gamma distribution\n            \n            # Expand for sample\n            rate = rate.expand(shape)\n            alpha = alpha.expand(shape)\n            beta = beta.expand(shape)\n\n            N = torch.poisson(rate) + 1e-5\n            gamma = Gamma(N*alpha, beta)\n            samples = gamma.sample()\n            samples[N==0] = 0\n\n            return samples\n\n    def log_prob(self, y_true):\n        rho = self.rho\n        y_pred = self.log_mu\n\n        a = y_true * torch.exp((1 - rho) * y_pred) / (1 - rho)\n        b = torch.exp((2 - rho) * y_pred) / (2 - rho)\n\n        return a - b\n\ndef tweedie_domain_map(input: torch.Tensor, rho: float = 1.5):\n    \"\"\"\n    Maps output of neural network to domain of distribution loss\n\n    \"\"\"\n    return (input, rho)\n\ndef tweedie_scale_decouple(output, loc=None, scale=None):\n    \"\"\"Tweedie Scale Decouple\n\n    Stabilizes model's output optimization, by learning total\n    count and logits based on anchoring `loc`, `scale`.\n    Also adds Tweedie domain protection to the distribution parameters.\n    \"\"\"\n    log_mu, rho = output\n    log_mu = F.softplus(log_mu)\n    log_mu = torch.clamp(log_mu, 1e-9, 37)\n    if (loc is not None) and (scale is not None):\n        log_mu += torch.log(loc)\n\n    log_mu = torch.clamp(log_mu, 1e-9, 37)\n    return (log_mu, rho)\n```\n\n----------------------------------------\n\nTITLE: Implementing AutoInformer Class for Automated Time Series Forecasting in Python\nDESCRIPTION: This snippet defines the AutoInformer class that extends BaseAuto for automated hyperparameter tuning of Informer models. It includes default configurations, initialization parameters, and methods to generate search spaces for different backends.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_71\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass AutoInformer(BaseAuto):\n\n    default_config = {\n        \"input_size_multiplier\": [1, 2, 3, 4, 5],\n        \"h\": None,\n        \"hidden_size\": tune.choice([64, 128, 256]),\n        \"n_head\": tune.choice([4, 8]),\n        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n        \"scaler_type\": tune.choice([None, 'robust', 'standard']),\n        \"max_steps\": tune.choice([500, 1000, 2000]),\n        \"batch_size\": tune.choice([32, 64, 128, 256]),\n        \"windows_batch_size\": tune.choice([128, 256, 512, 1024]),\n        \"loss\": None,\n        \"random_seed\": tune.randint(1, 20),\n    }\n\n    def __init__(self,\n                 h,\n                 loss=MAE(),\n                 valid_loss=None,\n                 config=None, \n                 search_alg=BasicVariantGenerator(random_state=1),\n                 num_samples=10,\n                 refit_with_val=False,\n                 cpus=cpu_count(),\n                 gpus=torch.cuda.device_count(),\n                 verbose=False,\n                 alias=None,\n                 backend='ray',\n                 callbacks=None):\n        \n        # Define search space, input/output sizes\n        if config is None:\n            config = self.get_default_config(h=h, backend=backend)          \n\n        super(AutoInformer, self).__init__(\n              cls_model=Informer, \n              h=h,\n              loss=loss,\n              valid_loss=valid_loss,\n              config=config,\n              search_alg=search_alg,\n              num_samples=num_samples, \n              refit_with_val=refit_with_val,\n              cpus=cpus,\n              gpus=gpus,\n              verbose=verbose,\n              alias=alias,\n              backend=backend,\n              callbacks=callbacks,            \n        )\n\n    @classmethod\n    def get_default_config(cls, h, backend, n_series=None):\n        config = cls.default_config.copy()\n        config['input_size'] = tune.choice([h*x \\\n                        for x in config['input_size_multiplier']])\n        config['step_size'] = tune.choice([1, h])        \n        del config['input_size_multiplier']\n        if backend == 'optuna':\n            config = cls._ray_config_to_optuna(config)         \n\n        return config                \n```\n\n----------------------------------------\n\nTITLE: Implementing AutoStemGNN Class for StemGNN Model Tuning in Python\nDESCRIPTION: Defines the AutoStemGNN class for automated hyperparameter tuning of StemGNN models. It includes default configurations, initialization, and methods for getting configurations. The class supports both Ray and Optuna backends for optimization.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_97\n\nLANGUAGE: python\nCODE:\n```\nclass AutoStemGNN(BaseAuto):\n\n    default_config = {\n        \"input_size_multiplier\": [1, 2, 3, 4],\n        \"h\": None,\n        \"n_series\": None,\n        \"n_stacks\": tune.choice([2]),\n        \"multi_layer\": tune.choice([3, 5, 7]),\n        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n        \"scaler_type\": tune.choice([None, 'robust', 'standard']),\n        \"max_steps\": tune.choice([500, 1000, 2000]),\n        \"batch_size\": tune.choice([32, 64, 128, 256]),\n        \"loss\": None,\n        \"random_seed\": tune.randint(1, 20),\n    }\n\n    def __init__(self,\n                 h,\n                 n_series,\n                 loss=MAE(),\n                 valid_loss=None,\n                 config=None, \n                 search_alg=BasicVariantGenerator(random_state=1),\n                 num_samples=10,\n                 refit_with_val=False,\n                 cpus=cpu_count(),\n                 gpus=torch.cuda.device_count(),\n                 verbose=False,\n                 alias=None,\n                 backend='ray',\n                 callbacks=None):\n        \n        # Define search space, input/output sizes\n        if config is None:\n            config = self.get_default_config(h=h, backend=backend, n_series=n_series)                  \n\n        # Always use n_series from parameters, raise exception with Optuna because we can't enforce it\n        if backend == 'ray':\n            config['n_series'] = n_series\n        elif backend == 'optuna':\n            mock_trial = MockTrial()\n            if ('n_series' in config(mock_trial) and config(mock_trial)['n_series'] != n_series) or ('n_series' not in config(mock_trial)):\n                raise Exception(f\"config needs 'n_series': {n_series}\")\n\n        super(AutoStemGNN, self).__init__(\n              cls_model=StemGNN, \n              h=h,\n              loss=loss,\n              valid_loss=valid_loss,\n              config=config,\n              search_alg=search_alg,\n              num_samples=num_samples, \n              refit_with_val=refit_with_val,\n              cpus=cpus,\n              gpus=gpus,\n              verbose=verbose,\n              alias=alias,\n              backend=backend,\n              callbacks=callbacks,            \n        )\n\n    @classmethod\n    def get_default_config(cls, h, backend, n_series):\n        config = cls.default_config.copy()        \n        config['input_size'] = tune.choice([h * x \\\n                        for x in config[\"input_size_multiplier\"]])\n\n        # Rolling windows with step_size=1 or step_size=h\n        # See `BaseWindows` and `BaseRNN`'s create_windows\n        config['step_size'] = tune.choice([1, h])\n        del config[\"input_size_multiplier\"]\n        if backend == 'optuna':\n            # Always use n_series from parameters\n            config['n_series'] = n_series\n            config = cls._ray_config_to_optuna(config)           \n\n        return config        \n```\n\n----------------------------------------\n\nTITLE: Implementing Negative Binomial Mixture Mesh (NBMM) in Python\nDESCRIPTION: This class implements the NBMM model, a statistical model for probabilistic forecasting. It includes methods for initialization, parameter scaling, distribution creation, sampling, and loss calculation. The model assumes independence across groups of data and estimates relationships within the group.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_59\n\nLANGUAGE: python\nCODE:\n```\nclass NBMM(torch.nn.Module):\n    def __init__(self, n_components=1, level=[80, 90], quantiles=None, \n                 num_samples=1000, return_params=False, weighted=False):\n        super(NBMM, self).__init__()\n        # Transform level to MQLoss parameters\n        qs, self.output_names = level_to_outputs(level)\n        qs = torch.Tensor(qs)\n\n        # Transform quantiles to homogeneus output names\n        if quantiles is not None:\n            _, self.output_names = quantiles_to_outputs(quantiles)\n            qs = torch.Tensor(quantiles)\n        self.quantiles = torch.nn.Parameter(qs, requires_grad=False)\n        self.num_samples = num_samples\n        self.weighted = weighted   \n\n        # If True, predict_step will return Distribution's parameters\n        self.return_params = return_params\n\n        total_count_names = [f\"-total_count-{i}\" for i in range(1, n_components + 1)]\n        probs_names = [f\"-probs-{i}\" for i in range(1, n_components + 1)]\n        if weighted:\n            weight_names = [f\"-weight-{i}\" for i in range(1, n_components + 1)]\n            self.param_names = [\n            i for j in zip(total_count_names, probs_names, weight_names) for i in j\n        ]\n        else:\n            self.param_names = [i for j in zip(total_count_names, probs_names) for i in j]\n\n        if self.return_params:\n            self.output_names = self.output_names + self.param_names\n\n        # Add first output entry for the sample_mean\n        self.output_names.insert(0, \"\")            \n\n        self.n_outputs = 2 + weighted\n        self.n_components = n_components\n        self.outputsize_multiplier = self.n_outputs * n_components\n        self.is_distribution_output = True\n        self.has_predicted = False\n```\n\n----------------------------------------\n\nTITLE: NBEATS Block Implementation\nDESCRIPTION: Defines a single NBEATS block module with configurable MLP layers, activation functions, and basis projections. Implements the forward pass for computing local projections.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeats.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n#| exporti\nACTIVATIONS = ['ReLU',\n               'Softplus',\n               'Tanh',\n               'SELU',\n               'LeakyReLU',\n               'PReLU',\n               'Sigmoid']\n\nclass NBEATSBlock(nn.Module):\n    \"\"\"\n    N-BEATS block which takes a basis function as an argument.\n    \"\"\"\n    def __init__(self, \n                 input_size: int,\n                 n_theta: int, \n                 mlp_units: list,\n                 basis: nn.Module, \n                 dropout_prob: float, \n                 activation: str):\n        super().__init__()\n\n        self.dropout_prob = dropout_prob\n        \n        assert activation in ACTIVATIONS, f'{activation} is not in {ACTIVATIONS}'\n        activ = getattr(nn, activation)()\n        \n        hidden_layers = [nn.Linear(in_features=input_size, \n                                   out_features=mlp_units[0][0])]\n        for layer in mlp_units:\n            hidden_layers.append(nn.Linear(in_features=layer[0], \n                                           out_features=layer[1]))\n            hidden_layers.append(activ)\n\n            if self.dropout_prob>0:\n                raise NotImplementedError('dropout')\n\n        output_layer = [nn.Linear(in_features=mlp_units[-1][1], out_features=n_theta)]\n        layers = hidden_layers + output_layer\n        self.layers = nn.Sequential(*layers)\n        self.basis = basis\n\n    def forward(self, insample_y: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        # Compute local projection weights and projection\n        theta = self.layers(insample_y)\n        backcast, forecast = self.basis(theta)\n        return backcast, forecast\n```\n\n----------------------------------------\n\nTITLE: Variable Selection Network for TFT in PyTorch\nDESCRIPTION: Implementation of the Variable Selection Network (VSN) that performs feature selection using Gated Residual Networks (GRN). Processes input variables and outputs transformed embeddings with attention weights.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nclass VariableSelectionNetwork(nn.Module):\n    def __init__(self, hidden_size, num_inputs, dropout, grn_activation):\n        super().__init__()\n        self.joint_grn = GRN(\n            input_size=hidden_size * num_inputs,\n            hidden_size=hidden_size,\n            output_size=num_inputs,\n            context_hidden_size=hidden_size,\n            activation=grn_activation,\n        )\n        self.var_grns = nn.ModuleList(\n            [\n                GRN(\n                    input_size=hidden_size,\n                    hidden_size=hidden_size,\n                    dropout=dropout,\n                    activation=grn_activation,\n                )\n                for _ in range(num_inputs)\n            ]\n        )\n```\n\n----------------------------------------\n\nTITLE: TCN Usage Example with Air Passengers Dataset\nDESCRIPTION: Example demonstrating how to use the TCN model with the Air Passengers dataset, including model configuration, training, forecasting, and visualization with prediction intervals.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tcn.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n#| eval: false\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import TCN\nfrom neuralforecast.losses.pytorch import  DistributionLoss\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nfcst = NeuralForecast(\n    models=[TCN(h=12,\n                input_size=-1,\n                loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n                learning_rate=5e-4,\n                kernel_size=2,\n                dilations=[1,2,4,8,16],\n                encoder_hidden_size=128,\n                context_size=10,\n                decoder_hidden_size=128,\n                decoder_layers=2,\n                max_steps=500,\n                scaler_type='robust',\n                futr_exog_list=['y_[lag12]'],\n                hist_exog_list=None,\n                stat_exog_list=['airline1'],\n                )\n    ],\n    freq='ME'\n)\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\n# Plot quantile predictions\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['TCN-median'], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'][-12:], \n                 y1=plot_df['TCN-lo-90'][-12:].values,\n                 y2=plot_df['TCN-hi-90'][-12:].values,\n                 alpha=0.4, label='level 90')\nplt.legend()\nplt.grid()\nplt.plot()\n```\n\n----------------------------------------\n\nTITLE: Implementing Temporal Convolution Encoder in PyTorch\nDESCRIPTION: Creates a module that applies a stack of dilated causal convolutions for capturing long-term dependencies in time series data, with exponentially increasing dilations to create effective long-term memory.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.modules.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass TemporalConvolutionEncoder(nn.Module):\n    \"\"\" Temporal Convolution Encoder\n\n    Receives `x` input of dim [N,T,C_in], permutes it to  [N,C_in,T]\n    applies a deep stack of exponentially dilated causal convolutions.\n    The exponentially increasing dilations of the convolutions allow for \n    the creation of weighted averages of exponentially large long-term memory.\n\n    **Parameters:**<br>\n    `in_channels`: int, dimension of `x` input's initial channels.<br> \n    `out_channels`: int, dimension of `x` outputs's channels.<br>\n    `kernel_size`: int, size of the convolving kernel.<br>\n    `dilations`: int list, controls the temporal spacing between the kernel points.<br>\n    `activation`: str, identifying activations from PyTorch activations.\n        select from 'ReLU','Softplus','Tanh','SELU', 'LeakyReLU','PReLU','Sigmoid'.<br>\n\n    **Returns:**<br>\n    `x`: tensor, torch tensor of dim [N,T,C_out].<br>\n    \"\"\"\n    # TODO: Add dilations parameter and change layers declaration to for loop\n    def __init__(self, in_channels, out_channels, \n                 kernel_size, dilations,\n                 activation:str='ReLU'):\n        super(TemporalConvolutionEncoder, self).__init__()\n        layers = []\n        for dilation in dilations:\n            layers.append(CausalConv1d(in_channels=in_channels, out_channels=out_channels, \n                                        kernel_size=kernel_size, padding=(kernel_size-1)*dilation, \n                                        activation=activation, dilation=dilation))\n            in_channels = out_channels\n        self.tcn = nn.Sequential(*layers)\n\n    def forward(self, x):\n        # [N,T,C_in] -> [N,C_in,T] -> [N,T,C_out]\n        x = x.permute(0, 2, 1).contiguous()\n        x = self.tcn(x)\n        x = x.permute(0, 2, 1).contiguous()\n        return x\n```\n\n----------------------------------------\n\nTITLE: Loading PyTorch Neural Forecasting Model\nDESCRIPTION: Class method for deserializing a model from disk, handling version-specific loading parameters. Includes compatibility for different PyTorch versions and proper state dictionary loading.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n@classmethod\ndef load(cls, path, **kwargs):\n    if \"weights_only\" in inspect.signature(torch.load).parameters:\n        kwargs[\"weights_only\"] = False\n    with fsspec.open(path, 'rb') as f, warnings.catch_warnings():\n        # ignore possible warnings about weights_only=False\n        warnings.filterwarnings('ignore', category=FutureWarning)\n        content = torch.load(f, **kwargs)\n    with _disable_torch_init():\n        model = cls(**content['hyper_parameters']) \n    if \"assign\" in inspect.signature(model.load_state_dict).parameters:\n        model.load_state_dict(content[\"state_dict\"], strict=True, assign=True)\n    else:  # pytorch<2.1\n        model.load_state_dict(content[\"state_dict\"], strict=True)\n    return model\n```\n\n----------------------------------------\n\nTITLE: Inverse Normalizing Forecast Outputs in Python using PyTorch\nDESCRIPTION: This method performs inverse normalization on the model's predictions (`y_hat`). It retrieves the appropriate location (shift) and scale parameters using `_get_loc_scale` based on the target variable index (`y_idx`) and applies the inverse transformation using `self.scaler.inverse_transform`. It handles broadcasting of scale if needed.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n    def _inv_normalization(self, y_hat, y_idx):\n        # Receives window predictions [Ws, h, output, n_series]\n        # Broadcasts scale if necessary and inverts normalization\n        add_channel_dim = y_hat.ndim > 3\n        y_loc, y_scale = self._get_loc_scale(y_idx, add_channel_dim=add_channel_dim)\n        y_hat = self.scaler.inverse_transform(z=y_hat, x_scale=y_scale, x_shift=y_loc)\n\n        return y_hat\n```\n\n----------------------------------------\n\nTITLE: Creating Time Series DataFrame from Image Data\nDESCRIPTION: Converts the flattened digit images into a DataFrame with the required format for NeuralForecast, including 'unique_id' for series identification, 'ds' for timestamps, 'y' for binary target values, and 'pixels' for original pixel values.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/16_temporal_classification.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# We flat the images and create an input dataframe\n# with 'unique_id' series identifier and 'ds' time stamp identifier.\nY_df = pd.DataFrame.from_dict({\n            'unique_id': np.repeat(np.arange(100), 64),\n            'ds': np.tile(np.arange(64)+1910, 100),\n            'y': ytarget.flatten(), 'pixels': pixels.flatten()})\nY_df\n```\n\n----------------------------------------\n\nTITLE: Model Usage Example with Ray Backend\nDESCRIPTION: Demonstrates configuration and usage of AutoPatchTST model with Ray backend for time series forecasting, including model initialization and fitting.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_86\n\nLANGUAGE: python\nCODE:\n```\nmy_config = AutoFEDformer.get_default_config(h=12, backend='ray')\nmy_config['max_steps'] = 1\nmy_config['val_check_steps'] = 1\nmy_config['input_size'] = 12\nmy_config['hidden_size'] = 64\nmodel = AutoFEDformer(h=12, config=my_config, backend='ray', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: Implementing WaveKAN Layer for RMoK\nDESCRIPTION: Defines the WaveKANLayer class, which implements the Wavelet Kolmogorov-Arnold Network layer for the RMoK model. It supports various wavelet types and includes batch normalization.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.rmok.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass WaveKANLayer(nn.Module):\n    '''This is a sample code for the simulations of the paper:\n    Bozorgasl, Zavareh and Chen, Hao, Wav-KAN: Wavelet Kolmogorov-Arnold Networks (May, 2024)\n\n    https://arxiv.org/abs/2405.12832\n    and also available at:\n    https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4835325\n    We used efficient KAN notation and some part of the code:+\n\n    '''\n\n    def __init__(self, in_features, out_features, wavelet_type='mexican_hat', with_bn=True, device=\"cpu\"):\n        super(WaveKANLayer, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.wavelet_type = wavelet_type\n        self.with_bn = with_bn\n\n        # Parameters for wavelet transformation\n        self.scale = nn.Parameter(torch.ones(out_features, in_features))\n        self.translation = nn.Parameter(torch.zeros(out_features, in_features))\n\n        # self.weight1 is not used; you may use it for weighting base activation and adding it like Spl-KAN paper\n        self.weight1 = nn.Parameter(torch.Tensor(out_features, in_features))\n        self.wavelet_weights = nn.Parameter(torch.Tensor(out_features, in_features))\n\n        nn.init.kaiming_uniform_(self.wavelet_weights, a=math.sqrt(5))\n        nn.init.kaiming_uniform_(self.weight1, a=math.sqrt(5))\n\n        # Base activation function #not used for this experiment\n        self.base_activation = nn.SiLU()\n\n        # Batch normalization\n        if self.with_bn:\n            self.bn = nn.BatchNorm1d(out_features)\n\n    def wavelet_transform(self, x):\n        if x.dim() == 2:\n            x_expanded = x.unsqueeze(1)\n        else:\n            x_expanded = x\n\n        translation_expanded = self.translation.unsqueeze(0).expand(x.size(0), -1, -1)\n        scale_expanded = self.scale.unsqueeze(0).expand(x.size(0), -1, -1)\n        x_scaled = (x_expanded - translation_expanded) / scale_expanded\n\n        # Implementation of different wavelet types\n        if self.wavelet_type == 'mexican_hat':\n            term1 = ((x_scaled ** 2) - 1)\n            term2 = torch.exp(-0.5 * x_scaled ** 2)\n            wavelet = (2 / (math.sqrt(3) * math.pi ** 0.25)) * term1 * term2\n            wavelet_weighted = wavelet * self.wavelet_weights.unsqueeze(0).expand_as(wavelet)\n            wavelet_output = wavelet_weighted.sum(dim=2)\n        elif self.wavelet_type == 'morlet':\n            omega0 = 5.0  # Central frequency\n            real = torch.cos(omega0 * x_scaled)\n            envelope = torch.exp(-0.5 * x_scaled ** 2)\n            wavelet = envelope * real\n            wavelet_weighted = wavelet * self.wavelet_weights.unsqueeze(0).expand_as(wavelet)\n            wavelet_output = wavelet_weighted.sum(dim=2)\n\n        elif self.wavelet_type == 'dog':\n            # Implementing Derivative of Gaussian Wavelet\n            dog = -x_scaled * torch.exp(-0.5 * x_scaled ** 2)\n            wavelet = dog\n            wavelet_weighted = wavelet * self.wavelet_weights.unsqueeze(0).expand_as(wavelet)\n            wavelet_output = wavelet_weighted.sum(dim=2)\n        elif self.wavelet_type == 'meyer':\n            # Implement Meyer Wavelet here\n            # Constants for the Meyer wavelet transition boundaries\n            v = torch.abs(x_scaled)\n            pi = math.pi\n\n            def meyer_aux(v):\n                return torch.where(v <= 1 / 2, torch.ones_like(v),\n                                   torch.where(v >= 1, torch.zeros_like(v), torch.cos(pi / 2 * nu(2 * v - 1))))\n\n            def nu(t):\n                return t ** 4 * (35 - 84 * t + 70 * t ** 2 - 20 * t ** 3)\n\n            # Meyer wavelet calculation using the auxiliary function\n            wavelet = torch.sin(pi * v) * meyer_aux(v)\n            wavelet_weighted = wavelet * self.wavelet_weights.unsqueeze(0).expand_as(wavelet)\n            wavelet_output = wavelet_weighted.sum(dim=2)\n        elif self.wavelet_type == 'shannon':\n            # Windowing the sinc function to limit its support\n            pi = math.pi\n            sinc = torch.sinc(x_scaled / pi)  # sinc(x) = sin(pi*x) / (pi*x)\n\n            # Applying a Hamming window to limit the infinite support of the sinc function\n            window = torch.hamming_window(x_scaled.size(-1), periodic=False, dtype=x_scaled.dtype,\n                                          device=x_scaled.device)\n            # Shannon wavelet is the product of the sinc function and the window\n            wavelet = sinc * window\n            wavelet_weighted = wavelet * self.wavelet_weights.unsqueeze(0).expand_as(wavelet)\n            wavelet_output = wavelet_weighted.sum(dim=2)\n            # You can try many more wavelet types ...\n        else:\n            raise ValueError(\"Unsupported wavelet type\")\n\n        return wavelet_output\n\n    def forward(self, x):\n        wavelet_output = self.wavelet_transform(x)\n        # You may like test the cases like Spl-KAN\n        # wav_output = F.linear(wavelet_output, self.weight)\n        # base_output = F.linear(self.base_activation(x), self.weight1)\n\n        # base_output = F.linear(x, self.weight1)\n        combined_output = wavelet_output  # + base_output\n\n        # Apply batch normalization\n        if self.with_bn:\n            return self.bn(combined_output)\n        else:\n            return combined_output\n```\n\n----------------------------------------\n\nTITLE: MLP Forward Pass Implementation in PyTorch\nDESCRIPTION: Core forward pass logic for the MLP model that handles historical, future, and static exogenous variables before making predictions through the neural network layers.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.mlp.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nif self.hist_exog_size > 0:\n    insample_y = torch.cat(( insample_y, hist_exog.reshape(batch_size,-1) ), dim=1)\n\nif self.futr_exog_size > 0:\n    insample_y = torch.cat(( insample_y, futr_exog.reshape(batch_size,-1) ), dim=1)\n\nif self.stat_exog_size > 0:\n    insample_y = torch.cat(( insample_y, stat_exog.reshape(batch_size,-1) ), dim=1)\n\ny_pred = insample_y.clone()\nfor layer in self.mlp:\n     y_pred = torch.relu(layer(y_pred))\ny_pred = self.out(y_pred)\n\ny_pred = y_pred.reshape(batch_size, self.h, \n                        self.loss.outputsize_multiplier)\nreturn y_pred\n```\n\n----------------------------------------\n\nTITLE: Instantiating AutoNHITS Model with Optuna Backend in Python\nDESCRIPTION: This code instantiates an AutoNHITS model with specified hyperparameters, loss function, search algorithm, and backend. It uses Optuna for hyperparameter optimization.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/04_hyperparameter_tuning.ipynb#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nmodel = AutoNHITS(\n    h=12,\n    loss=MAE(),\n    config=config_nhits,\n    search_alg=optuna.samplers.TPESampler(seed=0),\n    backend='optuna',\n    num_samples=10,\n)\n```\n\n----------------------------------------\n\nTITLE: iTransformer Usage Example with AirPassengers Dataset\nDESCRIPTION: Demonstrates how to use the iTransformer model with the NeuralForecast framework. The example loads the AirPassengers dataset, creates and configures an iTransformer model, trains it, generates predictions, and visualizes the results.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.itransformer.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n#| eval: false\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import iTransformer\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\nfrom neuralforecast.losses.pytorch import MSE\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = iTransformer(h=12,\n                     input_size=24,\n                     n_series=2,\n                     hidden_size=128,\n                     n_heads=2,\n                     e_layers=2,\n                     d_layers=1,\n                     d_ff=4,\n                     factor=1,\n                     dropout=0.1,\n                     use_norm=True,\n                     loss=MSE(),\n                     valid_loss=MAE(),\n                     early_stop_patience_steps=3,\n                     batch_size=32,\n                     max_steps=100)\n\nfcst = NeuralForecast(models=[model], freq='ME')\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\n# Plot predictions\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['iTransformer'], c='blue', label='Forecast')\nax.set_title('AirPassengers Forecast', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Year', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\n```\n\n----------------------------------------\n\nTITLE: RNN Model Initialization and Forward Pass\nDESCRIPTION: Core implementation of RNN model including initialization of encoder, decoder components and forward pass logic. Handles various types of inputs including historical data and exogenous variables.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.rnn.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Context adapter\nif context_size is not None:\n    warnings.warn(\"context_size is deprecated and will be removed in future versions.\")\n\n# Context adapter\nself.context_size = context_size\n\n# MLP decoder\nself.decoder_hidden_size = decoder_hidden_size\nself.decoder_layers = decoder_layers\n\n# RNN input size (1 for target variable y)\ninput_encoder = 1 + self.hist_exog_size + self.stat_exog_size + self.futr_exog_size\n\n# Instantiate model\nself.rnn_state = None\nself.maintain_state = False\nself.hist_encoder = nn.RNN(input_size=input_encoder,\n                            hidden_size=self.encoder_hidden_size,\n                            num_layers=self.encoder_n_layers,\n                            bias=self.encoder_bias,\n                            dropout=self.encoder_dropout,\n                            batch_first=True)\n```\n\n----------------------------------------\n\nTITLE: Testing NBEATSx with Static and Future Exogenous Variables\nDESCRIPTION: Demonstrates NBEATSx model usage with both static and future exogenous variables, using seasonality and exogenous stacks. Tests prediction functionality with dataset updates.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeatsx.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndataset, *_ = TimeSeriesDataset.from_df(df = Y_train_df, static_df=Y_static)\nmodel = NBEATSx(h=12,\n                input_size=24,\n                scaler_type='robust',\n                stack_types = [\"seasonality\", \"exogenous\"],\n                n_blocks = [1,1],\n                futr_exog_list=['month','year'],\n                stat_exog_list=['airline1', 'airline2'],\n                windows_batch_size=None,\n                max_steps=1)\nmodel.fit(dataset=dataset)\ndataset2 = dataset.update_dataset(dataset, Y_test_df)\nmodel.set_test_size(12)\ny_hat = model.predict(dataset=dataset2)\nassert(len(y_hat)==12)\n```\n\n----------------------------------------\n\nTITLE: Implementing AutoTSMixer Class for Time Series Forecasting in Python\nDESCRIPTION: This class implements AutoTSMixer, a subclass of BaseAuto, for automated hyperparameter tuning of TSMixer models. It includes default configurations, initialization method, and utility functions for handling different backend options (Ray and Optuna).\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_104\n\nLANGUAGE: python\nCODE:\n```\nclass AutoTSMixer(BaseAuto):\n\n    default_config = {\n        \"input_size_multiplier\": [1, 2, 3, 4],\n        \"h\": None,\n        \"n_series\": None,\n        \"n_block\": tune.choice([1, 2, 4, 6, 8]),\n        \"learning_rate\": tune.loguniform(1e-4, 1e-2),\n        \"ff_dim\": tune.choice([32, 64, 128]),\n        \"scaler_type\": tune.choice(['identity', 'robust', 'standard']),\n        \"max_steps\": tune.choice([500, 1000, 2000]),\n        \"batch_size\": tune.choice([32, 64, 128, 256]),\n        \"dropout\": tune.uniform(0.0, 0.99),\n        \"loss\": None,\n        \"random_seed\": tune.randint(1, 20),\n    }\n\n    def __init__(self,\n                 h,\n                 n_series,\n                 loss=MAE(),\n                 valid_loss=None,\n                 config=None, \n                 search_alg=BasicVariantGenerator(random_state=1),\n                 num_samples=10,\n                 refit_with_val=False,\n                 cpus=cpu_count(),\n                 gpus=torch.cuda.device_count(),\n                 verbose=False,\n                 alias=None,\n                 backend='ray',\n                 callbacks=None):\n        \n        # Define search space, input/output sizes\n        if config is None:\n            config = self.get_default_config(h=h, backend=backend, n_series=n_series)                 \n\n        # Always use n_series from parameters, raise exception with Optuna because we can't enforce it\n        if backend == 'ray':\n            config['n_series'] = n_series\n        elif backend == 'optuna':\n            mock_trial = MockTrial()\n            if ('n_series' in config(mock_trial) and config(mock_trial)['n_series'] != n_series) or ('n_series' not in config(mock_trial)):\n                raise Exception(f\"config needs 'n_series': {n_series}\")\n\n        super(AutoTSMixer, self).__init__(\n              cls_model=TSMixer, \n              h=h,\n              loss=loss,\n              valid_loss=valid_loss,\n              config=config,\n              search_alg=search_alg,\n              num_samples=num_samples, \n              refit_with_val=refit_with_val,\n              cpus=cpus,\n              gpus=gpus,\n              verbose=verbose,\n              alias=alias,\n              backend=backend,\n              callbacks=callbacks,            \n        )\n\n\n    @classmethod\n    def get_default_config(cls, h, backend, n_series):\n        config = cls.default_config.copy()        \n        config['input_size'] = tune.choice([h * x \\\n                        for x in config[\"input_size_multiplier\"]])\n\n        # Rolling windows with step_size=1 or step_size=h\n        # See `BaseWindows` and `BaseRNN`'s create_windows\n        config['step_size'] = tune.choice([1, h])\n        del config[\"input_size_multiplier\"]\n        if backend == 'optuna':\n            # Always use n_series from parameters\n            config['n_series'] = n_series\n            config = cls._ray_config_to_optuna(config)           \n\n        return config\n```\n\n----------------------------------------\n\nTITLE: DeepAR Model Implementation\nDESCRIPTION: Implements the main DeepAR model class with support for LSTM layers, Monte Carlo sampling, and various configuration options.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.deepar.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass DeepAR(BaseModel):\n    \"\"\" DeepAR\n\n    **Parameters:**<br>\n    `h`: int, Forecast horizon. <br>\n    `input_size`: int, maximum sequence length for truncated train backpropagation. Default -1 uses 3 * horizon <br>\n    `lstm_n_layers`: int=2, number of LSTM layers.<br>\n    `lstm_hidden_size`: int=128, LSTM hidden size.<br>\n    `lstm_dropout`: float=0.1, LSTM dropout.<br>\n    `decoder_hidden_layers`: int=0, number of decoder MLP hidden layers. Default: 0 for linear layer. <br>\n    `decoder_hidden_size`: int=0, decoder MLP hidden size. Default: 0 for linear layer.<br>\n    `trajectory_samples`: int=100, number of Monte Carlo trajectories during inference.<br>\n    `stat_exog_list`: str list, static exogenous columns.<br>\n    `hist_exog_list`: str list, historic exogenous columns.<br>\n    `futr_exog_list`: str list, future exogenous columns.<br>\n    `exclude_insample_y`: bool=False, the model skips the autoregressive features y[t-input_size:t] if True.<br>\n    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `max_steps`: int=1000, maximum number of training steps.<br>\n    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n    `batch_size`: int=32, number of different series in each batch.<br>\n    `valid_batch_size`: int=None, number of different series in each validation and test batch, if None uses batch_size.<br>\n    `windows_batch_size`: int=1024, number of windows to sample in each training batch, default uses all.<br>\n    `inference_windows_batch_size`: int=-1, number of windows to sample in each inference batch, -1 uses all.<br>\n    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n    `step_size`: int=1, step size between each window of temporal data.<br>\n    `scaler_type`: str='identity', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n    `random_seed`: int, random_seed for pytorch initializer and numpy generators.<br>\n    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n    `alias`: str, optional,  Custom name of the model.<br>\n    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>\n    `dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \"\"\"\n    # Class attributes\n    EXOGENOUS_FUTR = True\n    EXOGENOUS_HIST = False\n    EXOGENOUS_STAT = True\n    MULTIVARIATE = False\n    RECURRENT = True\n\n    def __init__(self,\n                 h,\n                 input_size: int = -1,\n                 lstm_n_layers: int = 2,\n                 lstm_hidden_size: int = 128,\n                 lstm_dropout: float = 0.1,\n                 decoder_hidden_layers: int = 0,\n                 decoder_hidden_size: int = 0,\n                 trajectory_samples: int = 100,\n                 stat_exog_list = None,\n                 hist_exog_list = None,\n                 futr_exog_list = None,\n                 exclude_insample_y = False,\n                 loss = DistributionLoss(distribution='StudentT', level=[80, 90], return_params=False),\n                 valid_loss = MAE(),\n                 max_steps: int = 1000,\n                 learning_rate: float = 1e-3,\n                 num_lr_decays: int = 3,\n                 early_stop_patience_steps: int =-1,\n                 val_check_steps: int = 100,\n                 batch_size: int = 32,\n                 valid_batch_size: Optional[int] = None,\n                 windows_batch_size: int = 1024,\n                 inference_windows_batch_size: int = -1,\n                 start_padding_enabled = False,\n                 step_size: int = 1,\n                 scaler_type: str = 'identity',\n                 random_seed: int = 1,\n                 drop_last_loader = False,\n                 alias: Optional[str] = None,\n                 optimizer = None,\n                 optimizer_kwargs = None,\n                 lr_scheduler = None,\n                 lr_scheduler_kwargs = None,\n                 dataloader_kwargs = None,\n                 **trainer_kwargs):\n\n        if exclude_insample_y:\n            raise Exception('DeepAR has no possibility for excluding y.')\n        \n        # Inherit BaseWindows class\n        super(DeepAR, self).__init__(h=h,\n                                    input_size=input_size,\n                                    stat_exog_list=stat_exog_list,\n                                    hist_exog_list=hist_exog_list,\n                                    futr_exog_list=futr_exog_list,\n                                    exclude_insample_y = exclude_insample_y,\n                                    loss=loss,\n                                    valid_loss=valid_loss,\n                                    max_steps=max_steps,\n                                    learning_rate=learning_rate,\n                                    num_lr_decays=num_lr_decays,\n                                    early_stop_patience_steps=early_stop_patience_steps,\n                                    val_check_steps=val_check_steps,\n                                    batch_size=batch_size,\n                                    valid_batch_size=valid_batch_size,\n                                    windows_batch_size=windows_batch_size,\n                                    inference_windows_batch_size=inference_windows_batch_size,\n                                    start_padding_enabled=start_padding_enabled,\n                                    step_size=step_size,\n                                    scaler_type=scaler_type,\n                                    random_seed=random_seed,\n                                    drop_last_loader=drop_last_loader,\n                                    alias=alias,\n```\n\n----------------------------------------\n\nTITLE: Creating Future DataFrame for Forecasting in NeuralForecast\nDESCRIPTION: This method creates a dataframe with all ids and future times in the forecasting horizon. It's used to prepare the data structure for making predictions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\ndef make_future_dataframe(self, df: Optional[DFType] = None) -> DFType:\n    \"\"\"Create a dataframe with all ids and future times in the forecasting horizon.\n\n    Parameters\n    ----------\n    df : pandas or polars DataFrame, optional (default=None)\n        DataFrame with columns [`unique_id`, `ds`, `y`] and exogenous variables.\n        Only required if this is different than the one used in the fit step.\n    \"\"\"\n    if not self._fitted:\n        raise Exception('You must fit the model first.')\n    if df is not None:\n        df = ufp.sort(df, by=[self.id_col, self.time_col])\n        last_times_by_id = ufp.group_by_agg(\n            df,\n            by=self.id_col,\n            aggs={self.time_col: 'max'},\n        )\n```\n\n----------------------------------------\n\nTITLE: Implementing AutoTimeXer Class for Automated Hyperparameter Tuning\nDESCRIPTION: A complete implementation of the AutoTimeXer class, which extends BaseAuto for automatic hyperparameter tuning of the TimeXer model. It defines a default search space, initialization parameters, and configuration methods supporting both Ray and Optuna backends.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_89\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass AutoTimeXer(BaseAuto):\n\n    default_config = {\n        \"input_size_multiplier\": [1, 2, 3, 4, 5],\n        \"h\": None,\n        \"n_series\": None,\n        \"hidden_size\": tune.choice([128, 256, 512]),\n        \"n_heads\": tune.choice([4, 8]),\n        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n        \"scaler_type\": tune.choice([None, 'robust', 'standard']),\n        \"max_steps\": tune.choice([500, 1000, 2000]),\n        \"batch_size\": tune.choice([32, 64, 128, 256]),\n        \"loss\": None,\n        \"random_seed\": tune.randint(1, 20),\n    }\n\n    def __init__(self,\n                 h,\n                 n_series,\n                 loss=MAE(),\n                 valid_loss=None,\n                 config=None, \n                 search_alg=BasicVariantGenerator(random_state=1),\n                 num_samples=10,\n                 refit_with_val=False,\n                 cpus=cpu_count(),\n                 gpus=torch.cuda.device_count(),\n                 verbose=False,\n                 alias=None,\n                 backend='ray',\n                 callbacks=None):\n        \n        # Define search space, input/output sizes\n        if config is None:\n            config = self.get_default_config(h=h, backend=backend, n_series=n_series)                 \n\n        # Always use n_series from parameters, raise exception with Optuna because we can't enforce it\n        if backend == 'ray':\n            config['n_series'] = n_series\n        elif backend == 'optuna':\n            mock_trial = MockTrial()\n            if ('n_series' in config(mock_trial) and config(mock_trial)['n_series'] != n_series) or ('n_series' not in config(mock_trial)):\n                raise Exception(f\"config needs 'n_series': {n_series}\")           \n\n        super(AutoTimeXer, self).__init__(\n              cls_model=TimeXer, \n              h=h,\n              loss=loss,\n              valid_loss=valid_loss,\n              config=config,\n              search_alg=search_alg,\n              num_samples=num_samples, \n              refit_with_val=refit_with_val,\n              cpus=cpus,\n              gpus=gpus,\n              verbose=verbose,\n              alias=alias,\n              backend=backend,\n              callbacks=callbacks,            \n        )\n\n    @classmethod\n    def get_default_config(cls, h, backend, n_series):\n        config = cls.default_config.copy()        \n        config['input_size'] = tune.choice([h * x \\\n                        for x in config[\"input_size_multiplier\"]])\n\n        # Rolling windows with step_size=1 or step_size=h\n        # See `BaseWindows` and `BaseRNN`'s create_windows\n        config['step_size'] = tune.choice([1, h])\n        del config[\"input_size_multiplier\"]\n        if backend == 'optuna':\n            # Always use n_series from parameters\n            config['n_series'] = n_series\n            config = cls._ray_config_to_optuna(config)           \n\n        return config         \n```\n\n----------------------------------------\n\nTITLE: Initializing and Using AutoInformer Model for Time Series Forecasting in Python\nDESCRIPTION: This snippet demonstrates the basic usage of the AutoInformer model for time series forecasting. It shows how to configure, initialize, fit, and predict with the model, including usage with the Optuna backend.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_73\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n# Use your own config or AutoInformer.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=8)\nmodel = AutoInformer(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n# Optuna\nmodel = AutoInformer(h=12, config=None, backend='optuna')\n```\n\n----------------------------------------\n\nTITLE: Displaying Documentation for TimeMixer.predict Method in Python\nDESCRIPTION: This snippet utilizes the `show_doc` function to render the documentation for the `predict` method of the `TimeMixer` class. It depends on the availability of the `TimeMixer` class, its `predict` method, and the `show_doc` function. The `name` argument allows for specifying the title under which the documentation is displayed.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timemixer.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(TimeMixer.predict, name='TimeMixer.predict')\n```\n\n----------------------------------------\n\nTITLE: Neural Forecast Model Mapping Dictionary\nDESCRIPTION: Dictionary mapping model names to their corresponding model classes for various neural forecasting architectures.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nMODEL_FILENAME_DICT = {\n    'autoformer': Autoformer, 'autoautoformer': Autoformer,\n    'deepar': DeepAR, 'autodeepar': DeepAR,\n    'dlinear': DLinear, 'autodlinear': DLinear\n    # ... additional mappings\n```\n\n----------------------------------------\n\nTITLE: Extracting Future Exogenous Variables in NeuralForecast\nDESCRIPTION: Identifies and collects future exogenous variables needed by all models in the ensemble. Handles both regular models and auto models with different configuration structures (ray and optuna).\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef _get_needed_futr_exog(self):\n    futr_exogs = []\n    for m in self.models:\n        if isinstance(m, BaseAuto):\n            if isinstance(m.config, dict):  # ray\n                exogs = m.config.get('futr_exog_list', [])\n                if hasattr(exogs, 'categories'):  # features are being tuned, get possible values\n                    exogs = exogs.categories\n            else:   # optuna\n                exogs = m.config(MockTrial()).get('futr_exog_list', [])\n        else:  # regular model, extract them directly\n            exogs = getattr(m, 'futr_exog_list', [])\n        \n        for exog in exogs:\n            if isinstance(exog, str):\n                futr_exogs.append(exog)\n            else:\n                futr_exogs.extend(exog)\n\n    return set(futr_exogs)\n```\n\n----------------------------------------\n\nTITLE: Implementing Auxiliary Functions for TFT in Python\nDESCRIPTION: Defines helper functions and classes for the TFT model, including activation functions, layer normalization, and Gated Linear Units.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef get_activation_fn(activation_str: str) -> Callable:\n    activation_map = {\n        \"ReLU\": F.relu,\n        \"Softplus\": F.softplus,\n        \"Tanh\": F.tanh,\n        \"SELU\": F.selu,\n        \"LeakyReLU\": F.leaky_relu,\n        \"Sigmoid\": F.sigmoid,\n        \"ELU\": F.elu,\n        \"GLU\": F.glu,\n    }\n    return activation_map.get(activation_str, F.elu)\n\n\nclass MaybeLayerNorm(nn.Module):\n    def __init__(self, output_size, hidden_size, eps):\n        super().__init__()\n        if output_size and output_size == 1:\n            self.ln = nn.Identity()\n        else:\n            self.ln = LayerNorm(output_size if output_size else hidden_size, eps=eps)\n\n    def forward(self, x):\n        return self.ln(x)\n\n\nclass GLU(nn.Module):\n    def __init__(self, hidden_size, output_size):\n        super().__init__()\n        self.lin = nn.Linear(hidden_size, output_size * 2)\n\n    def forward(self, x: Tensor) -> Tensor:\n        x = self.lin(x)\n        x = F.glu(x)\n        return x\n\n\nclass GRN(nn.Module):\n    def __init__(\n        self,\n        input_size,\n        hidden_size,\n        output_size=None,\n        context_hidden_size=None,\n        dropout=0,\n        activation=\"ELU\",\n    ):\n        super().__init__()\n        self.layer_norm = MaybeLayerNorm(output_size, hidden_size, eps=1e-3)\n        self.lin_a = nn.Linear(input_size, hidden_size)\n        if context_hidden_size is not None:\n            self.lin_c = nn.Linear(context_hidden_size, hidden_size, bias=False)\n        self.lin_i = nn.Linear(hidden_size, hidden_size)\n        self.glu = GLU(hidden_size, output_size if output_size else hidden_size)\n        self.dropout = nn.Dropout(dropout)\n        self.out_proj = nn.Linear(input_size, output_size) if output_size else None\n        self.activation_fn = get_activation_fn(activation)\n\n    def forward(self, a: Tensor, c: Optional[Tensor] = None):\n        x = self.lin_a(a)\n        if c is not None:\n            x = x + self.lin_c(c).unsqueeze(1)\n        x = self.activation_fn(x)\n        x = self.lin_i(x)\n        x = self.dropout(x)\n        x = self.glu(x)\n        y = a if not self.out_proj else self.out_proj(a)\n        x = x + y\n        x = self.layer_norm(x)\n        return x\n```\n\n----------------------------------------\n\nTITLE: Implementing Causal Convolution 1D Module in PyTorch\nDESCRIPTION: Defines a causal convolutional layer that ensures the model only uses past information for prediction, with dilation for capturing long-range dependencies efficiently.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.modules.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass CausalConv1d(nn.Module):\n    \"\"\" Causal Convolution 1d\n\n    Receives `x` input of dim [N,C_in,T], and computes a causal convolution\n    in the time dimension. Skipping the H steps of the forecast horizon, through\n    its dilation.\n    Consider a batch of one element, the dilated convolution operation on the\n    $t$ time step is defined:\n\n    $\\mathrm{Conv1D}(\\mathbf{x},\\mathbf{w})(t) = (\\mathbf{x}_{[*d]} \\mathbf{w})(t) = \\sum^{K}_{k=1} w_{k} \\mathbf{x}_{t-dk}$\n\n    where $d$ is the dilation factor, $K$ is the kernel size, $t-dk$ is the index of\n    the considered past observation. The dilation effectively applies a filter with skip\n    connections. If $d=1$ one recovers a normal convolution.\n\n    **Parameters:**<br>\n    `in_channels`: int, dimension of `x` input's initial channels.<br> \n    `out_channels`: int, dimension of `x` outputs's channels.<br> \n    `activation`: str, identifying activations from PyTorch activations.\n        select from 'ReLU','Softplus','Tanh','SELU', 'LeakyReLU','PReLU','Sigmoid'.<br>\n    `padding`: int, number of zero padding used to the left.<br>\n    `kernel_size`: int, convolution's kernel size.<br>\n    `dilation`: int, dilation skip connections.<br>\n    \n    **Returns:**<br>\n    `x`: tensor, torch tensor of dim [N,C_out,T] activation(conv1d(inputs, kernel) + bias). <br>\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size,\n                 padding, dilation, activation, stride:int=1):\n        super(CausalConv1d, self).__init__()\n        assert activation in ACTIVATIONS, f'{activation} is not in {ACTIVATIONS}'\n        \n        self.conv       = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, \n                                    kernel_size=kernel_size, stride=stride, padding=padding,\n                                    dilation=dilation)\n        \n        self.chomp      = Chomp1d(padding)\n        self.activation = getattr(nn, activation)()\n        self.causalconv = nn.Sequential(self.conv, self.chomp, self.activation)\n    \n    def forward(self, x):\n        return self.causalconv(x)\n```\n\n----------------------------------------\n\nTITLE: Prediction Step Implementation for Neural Network Forecasting Model in Python\nDESCRIPTION: This method implements the prediction step for the neural network forecasting model. It handles batching of windows and supports both recurrent and non-recurrent models. The method returns the predicted values for the given input data.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_33\n\nLANGUAGE: Python\nCODE:\n```\ndef predict_step(self, batch, batch_idx):\n        if self.RECURRENT:\n            self.input_size = self.inference_input_size\n\n        # TODO: Hack to compute number of windows\n        windows = self._create_windows(batch, step='predict')\n        n_windows = len(windows['temporal'])\n        y_idx = batch['y_idx']\n\n        # Number of windows in batch\n        windows_batch_size = self.inference_windows_batch_size\n        if windows_batch_size < 0:\n            windows_batch_size = n_windows\n        n_batches = int(np.ceil(n_windows / windows_batch_size))\n        y_hats = []\n        for i in range(n_batches):\n            # Create and normalize windows [Ws, L+H, C]\n            w_idxs = np.arange(i*windows_batch_size, \n                    min((i+1)*windows_batch_size, n_windows))\n            windows = self._create_windows(batch, step='predict', w_idxs=w_idxs)\n            windows = self._normalization(windows=windows, y_idx=y_idx)\n\n            # Parse windows\n            insample_y, insample_mask, _, _, \\\n                hist_exog, futr_exog, stat_exog = self._parse_windows(batch, windows)\n\n            if self.RECURRENT:                \n                y_hat = self._predict_step_recurrent_batch(insample_y=insample_y,\n                                                           insample_mask=insample_mask,\n                                                           futr_exog=futr_exog,\n                                                           hist_exog=hist_exog,\n                                                           stat_exog=stat_exog,\n                                                           y_idx=y_idx)\n            else:\n                y_hat = self._predict_step_direct_batch(insample_y=insample_y,\n                                                           insample_mask=insample_mask,\n                                                           futr_exog=futr_exog,\n                                                           hist_exog=hist_exog,\n                                                           stat_exog=stat_exog,\n                                                           y_idx=y_idx)                \n\n\n            y_hats.append(y_hat)\n        y_hat = torch.cat(y_hats, dim=0)\n        self.input_size = self.input_size_backup\n\n        return y_hat\n```\n\n----------------------------------------\n\nTITLE: Implementing Dilated Recurrent Neural Network (DRNN) in PyTorch\nDESCRIPTION: Core implementation of the Dilated RNN architecture, supporting multiple RNN cell types (GRU, RNN, LSTM, ResLSTM, AttentiveLSTM) with configurable dilations. This module handles the creation of dilated connections between different layers and time steps.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.dilated_rnn.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n#| exporti\nclass DRNN(nn.Module):\n\n    def __init__(self, n_input, n_hidden, n_layers, dilations, dropout=0, cell_type='GRU', batch_first=True):\n        super(DRNN, self).__init__()\n\n        self.dilations = dilations\n        self.cell_type = cell_type\n        self.batch_first = batch_first\n\n        layers = []\n        if self.cell_type == \"GRU\":\n            cell = nn.GRU\n        elif self.cell_type == \"RNN\":\n            cell = nn.RNN\n        elif self.cell_type == \"LSTM\":\n            cell = nn.LSTM\n        elif self.cell_type == \"ResLSTM\":\n            cell = ResLSTMLayer\n        elif self.cell_type == \"AttentiveLSTM\":\n            cell = AttentiveLSTMLayer\n        else:\n            raise NotImplementedError\n\n        for i in range(n_layers):\n            if i == 0:\n                c = cell(n_input, n_hidden, dropout=dropout)\n            else:\n                c = cell(n_hidden, n_hidden, dropout=dropout)\n            layers.append(c)\n        self.cells = nn.Sequential(*layers)\n\n    def forward(self, inputs, hidden=None):\n        if self.batch_first:\n            inputs = inputs.transpose(0, 1)\n        outputs = []\n        for i, (cell, dilation) in enumerate(zip(self.cells, self.dilations)):\n            if hidden is None:\n                inputs, _ = self.drnn_layer(cell, inputs, dilation)\n            else:\n                inputs, hidden[i] = self.drnn_layer(cell, inputs, dilation, hidden[i])\n\n            outputs.append(inputs[-dilation:])\n\n        if self.batch_first:\n            inputs = inputs.transpose(0, 1)\n        return inputs, outputs\n\n    def drnn_layer(self, cell, inputs, rate, hidden=None):\n        n_steps = len(inputs)\n        batch_size = inputs[0].size(0)\n        hidden_size = cell.hidden_size\n\n        inputs, dilated_steps = self._pad_inputs(inputs, n_steps, rate)\n        dilated_inputs = self._prepare_inputs(inputs, rate)\n\n        if hidden is None:\n            dilated_outputs, hidden = self._apply_cell(dilated_inputs, cell, batch_size, rate, hidden_size)\n        else:\n            hidden = self._prepare_inputs(hidden, rate)\n            dilated_outputs, hidden = self._apply_cell(dilated_inputs, cell, batch_size, rate, hidden_size,\n                                                       hidden=hidden)\n\n        splitted_outputs = self._split_outputs(dilated_outputs, rate)\n        outputs = self._unpad_outputs(splitted_outputs, n_steps)\n\n        return outputs, hidden\n\n    def _apply_cell(self, dilated_inputs, cell, batch_size, rate, hidden_size, hidden=None):\n        if hidden is None:\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Hyperparameter Search Space for NHITS in Python\nDESCRIPTION: This function defines a fully customized search space for NHITS model hyperparameters. It includes learning rate, model-specific parameters, and modifies default hyperparameters like max_steps and val_check_steps.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/04_hyperparameter_tuning.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef config_nhits(trial):\n    return {\n        \"max_steps\": 100,                                                                                               # Number of SGD steps\n        \"input_size\": 24,                                                                                               # Size of input window\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-1),                                         # Initial Learning rate\n        \"n_pool_kernel_size\": trial.suggest_categorical(\"n_pool_kernel_size\", [[2, 2, 2], [16, 8, 1]]),                 # MaxPool's Kernelsize\n        \"n_freq_downsample\": trial.suggest_categorical(\"n_freq_downsample\", [[168, 24, 1], [24, 12, 1], [1, 1, 1]]),    # Interpolation expressivity ratios\n        \"val_check_steps\": 50,                                                                                          # Compute validation every 50 steps\n        \"random_seed\": trial.suggest_int(\"random_seed\", 1, 10),                                                         # Random seed\n    }\n```\n\n----------------------------------------\n\nTITLE: Implementing IQLoss Class for Implicit Quantile Loss in PyTorch\nDESCRIPTION: This class extends QuantileLoss to implement the Implicit Quantile Loss. It includes methods for sampling quantiles, initializing the sampling distribution, updating quantiles, and applying the loss function.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclass IQLoss(QuantileLoss):\n    \"\"\"Implicit Quantile Loss\n\n    Computes the quantile loss between `y` and `y_hat`, with the quantile `q` provided as an input to the network. \n    IQL measures the deviation of a quantile forecast.\n    By weighting the absolute deviation in a non symmetric way, the\n    loss pays more attention to under or over estimation.\n\n    $$ \\mathrm{QL}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{(q)}_{\\\\tau}) = \\\\frac{1}{H} \\\\sum^{t+H}_{\\\\tau=t+1} \\Big( (1-q)\\,( \\hat{y}^{(q)}_{\\\\tau} - y_{\\\\tau} )_{+} + q\\,( y_{\\\\tau} - \\hat{y}^{(q)}_{\\\\tau} )_{+} \\Big) $$\n\n    **Parameters:**<br>\n    `quantile_sampling`: str, default='uniform', sampling distribution used to sample the quantiles during training. Choose from ['uniform', 'beta']. <br>\n    `horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n\n    **References:**<br>\n    [Gouttes, Adle, Kashif Rasul, Mateusz Koren, Johannes Stephan, and Tofigh Naghibi, \"Probabilistic Time Series Forecasting with Implicit Quantile Networks\".](http://arxiv.org/abs/2107.03743)\n    \"\"\"\n    def __init__(self, cos_embedding_dim = 64, concentration0 = 1.0, concentration1 = 1.0, horizon_weight=None):\n        self.update_quantile()\n        super(IQLoss, self).__init__(\n            q = self.q,\n            horizon_weight=horizon_weight\n        )\n\n        self.cos_embedding_dim = cos_embedding_dim\n        self.concentration0 = concentration0\n        self.concentration1 = concentration1\n        self.has_sampled = False\n        self.has_predicted = False\n\n        self.quantile_layer = QuantileLayer(\n            num_output=1, cos_embedding_dim=self.cos_embedding_dim\n        )\n        self.output_layer = nn.Sequential(\n            nn.Linear(1, 1), nn.PReLU()\n        )\n        \n    def _sample_quantiles(self, sample_size, device):\n        if not self.has_sampled:\n            self._init_sampling_distribution(device)\n\n        quantiles = self.sampling_distr.sample(sample_size)\n        self.q = quantiles.squeeze(-1)\n        self.has_sampled = True        \n        self.has_predicted = False\n\n        return quantiles\n    \n    def _init_sampling_distribution(self, device):\n        concentration0 = torch.tensor([self.concentration0],\n                                      device=device,\n                                      dtype=torch.float32)\n        concentration1 = torch.tensor([self.concentration1],\n                                      device=device,\n                                      dtype=torch.float32)        \n        self.sampling_distr = Beta(concentration0 = concentration0,\n                                   concentration1 = concentration1)\n\n    def update_quantile(self, q: List[float] = [0.5]):\n        self.q = q[0]\n        self.output_names = [f\"_ql{q[0]}\"]\n        self.has_predicted = True\n\n    def domain_map(self, y_hat):\n        \"\"\"\n        Adds IQN network to output of network\n\n        Input shapes to this function:\n         \n        Univariate: y_hat = [B, h, 1] \n        Multivariate: y_hat = [B, h, N]\n        \"\"\"\n        if self.eval() and self.has_predicted:\n            quantiles = torch.full(size=y_hat.shape, \n                                    fill_value=self.q,\n                                    device=y_hat.device,\n                                    dtype=y_hat.dtype) \n            quantiles = quantiles.unsqueeze(-1)             \n        else:\n            quantiles = self._sample_quantiles(sample_size=y_hat.shape,\n                                        device=y_hat.device)\n\n        # Embed the quantiles and add to y_hat\n        emb_taus = self.quantile_layer(quantiles)\n        emb_inputs = y_hat.unsqueeze(-1) * (1.0 + emb_taus)\n        emb_outputs = self.output_layer(emb_inputs)\n        \n        # Domain map\n        y_hat = emb_outputs.squeeze(-1)\n\n        return y_hat\n```\n\n----------------------------------------\n\nTITLE: Example Usage of AutoStemGNN Class in Python\nDESCRIPTION: Demonstrates how to use the AutoStemGNN class for model fitting and prediction. It includes examples with both custom and default configurations, as well as usage with the Optuna backend.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_98\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n# Use your own config or AutoStemGNN.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12)\nmodel = AutoStemGNN(h=12, n_series=1, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n# Optuna\nmodel = AutoStemGNN(h=12, n_series=1, config=None, backend='optuna')\n```\n\n----------------------------------------\n\nTITLE: Customizing Optuna Config for AutoNBEATSx\nDESCRIPTION: Example of updating the default configuration for AutoNBEATSx model with Optuna backend. The code modifies the default configuration to use fewer training steps, smaller network size, and customized input dimensions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nmy_config = AutoNBEATSx.get_default_config(h=12, backend='optuna')\ndef my_config_new(trial):\n    config = {**my_config(trial)}\n    config.update({'max_steps': 2, 'val_check_steps': 1, 'input_size': 12, 'mlp_units': 3 * [[8, 8]]})\n    return config\n\nmodel = AutoNBEATSx(h=12, config=my_config_new, backend='optuna', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: Implementing Residual LSTM Cell in PyTorch\nDESCRIPTION: Defines a ResLSTMCell with residual connections, improving gradient flow during training. This module handles input and hidden state transformations with additional residual connections, particularly useful for deep networks.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.dilated_rnn.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n#| exporti\nclass ResLSTMCell(nn.Module):\n    def __init__(self, input_size, hidden_size, dropout=0.):\n        super(ResLSTMCell, self).__init__()\n        self.register_buffer('input_size', torch.Tensor([input_size]))\n        self.register_buffer('hidden_size', torch.Tensor([hidden_size]))\n        self.weight_ii = nn.Parameter(torch.randn(3 * hidden_size, input_size))\n        self.weight_ic = nn.Parameter(torch.randn(3 * hidden_size, hidden_size))\n        self.weight_ih = nn.Parameter(torch.randn(3 * hidden_size, hidden_size))\n        self.bias_ii = nn.Parameter(torch.randn(3 * hidden_size))\n        self.bias_ic = nn.Parameter(torch.randn(3 * hidden_size))\n        self.bias_ih = nn.Parameter(torch.randn(3 * hidden_size))\n        self.weight_hh = nn.Parameter(torch.randn(1 * hidden_size, hidden_size))\n        self.bias_hh = nn.Parameter(torch.randn(1 * hidden_size))\n        self.weight_ir = nn.Parameter(torch.randn(hidden_size, input_size))\n        self.dropout = dropout\n\n    def forward(self, inputs, hidden):\n        hx, cx = hidden[0].squeeze(0), hidden[1].squeeze(0)\n\n        ifo_gates = (torch.matmul(inputs, self.weight_ii.t()) + self.bias_ii +\n                                  torch.matmul(hx, self.weight_ih.t()) + self.bias_ih +\n                                  torch.matmul(cx, self.weight_ic.t()) + self.bias_ic)\n        ingate, forgetgate, outgate = ifo_gates.chunk(3, 1)\n\n        cellgate = torch.matmul(hx, self.weight_hh.t()) + self.bias_hh\n\n        ingate = torch.sigmoid(ingate)\n        forgetgate = torch.sigmoid(forgetgate)\n        cellgate = torch.tanh(cellgate)\n        outgate = torch.sigmoid(outgate)\n\n        cy = (forgetgate * cx) + (ingate * cellgate)\n        ry = torch.tanh(cy)\n\n        if self.input_size == self.hidden_size:\n            hy = outgate * (ry + inputs)\n        else:\n            hy = outgate * (ry + torch.matmul(inputs, self.weight_ir.t()))\n        return hy, (hy, cy)\n```\n\n----------------------------------------\n\nTITLE: Creating TimeSeriesDataset from DataFrame in NeuralForecast\nDESCRIPTION: Static method that converts a pandas DataFrame into a TimeSeriesDataset object. It extracts static features, processes the DataFrame to get ids, times, and data, and ensures availability masking is properly handled.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/tsdataset.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@staticmethod\ndef from_df(df, static_df=None, id_col='unique_id', time_col='ds', target_col='y'):\n    # TODO: protect on equality of static_df + df indexes\n    # Define indices if not given and then extract static features\n    static, static_cols = TimeSeriesDataset._extract_static_features(static_df, id_col)\n    \n    ids, times, data, indptr, sort_idxs = ufp.process_df(df, id_col, time_col, target_col)\n    # processor sets y as the first column\n    temporal_cols = pd.Index(\n        [target_col] + [c for c in df.columns if c not in (id_col, time_col, target_col)]\n    )\n    temporal = data.astype(np.float32, copy=False)\n    indices = ids\n    if isinstance(df, pd.DataFrame):\n        dates = pd.Index(times, name=time_col)\n    else:\n        dates = pl_Series(time_col, times)\n\n    # Add Available mask efficiently (without adding column to df)\n    temporal, temporal_cols = TimeSeriesDataset._ensure_available_mask(data, temporal_cols)\n\n    dataset = TimeSeriesDataset(\n        temporal=temporal,\n        temporal_cols=temporal_cols,\n        static=static,\n        static_cols=static_cols,\n        indptr=indptr,\n        y_idx=0,\n    )\n    ds = df[time_col].to_numpy()\n    if sort_idxs is not None:\n        ds = ds[sort_idxs]\n    return dataset, indices, dates, ds\n```\n\n----------------------------------------\n\nTITLE: Creating Neural Network Stack for NBEATSx Model in PyTorch\nDESCRIPTION: Function to create the stack of neural network blocks for the NBEATSx model. It supports various block types including seasonality, trend, identity, and exogenous blocks, with options for shared weights and customizable parameters.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeatsx.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef create_stack(\n    self,\n    h,\n    input_size,\n    stack_types,\n    n_blocks,\n    mlp_units,\n    dropout_prob_theta,\n    activation,\n    shared_weights,\n    n_polynomials,\n    n_harmonics,\n    futr_input_size,\n    hist_input_size,\n    stat_input_size,\n):\n    block_list = []\n    for i in range(len(stack_types)):\n        for block_id in range(n_blocks[i]):\n            # Shared weights\n            if shared_weights and block_id > 0:\n                nbeats_block = block_list[-1]\n            else:\n                if stack_types[i] == \"seasonality\":\n                    n_theta = (\n                        2\n                        * (self.loss.outputsize_multiplier + 1)\n                        * int(np.ceil(n_harmonics / 2 * h) - (n_harmonics - 1))\n                    )\n                    basis = SeasonalityBasis(\n                        harmonics=n_harmonics,\n                        backcast_size=input_size,\n                        forecast_size=h,\n                        out_features=self.loss.outputsize_multiplier,\n                    )\n\n                elif stack_types[i] == \"trend\":\n                    n_theta = (self.loss.outputsize_multiplier + 1) * (\n                        n_polynomials + 1\n                    )\n                    basis = TrendBasis(\n                        degree_of_polynomial=n_polynomials,\n                        backcast_size=input_size,\n                        forecast_size=h,\n                        out_features=self.loss.outputsize_multiplier,\n                    )\n\n                elif stack_types[i] == \"identity\":\n                    n_theta = input_size + self.loss.outputsize_multiplier * h\n                    basis = IdentityBasis(\n                        backcast_size=input_size,\n                        forecast_size=h,\n                        out_features=self.loss.outputsize_multiplier,\n                    )\n\n                elif stack_types[i] == \"exogenous\":\n                    if futr_input_size + stat_input_size > 0:\n                        n_theta = 2*(\n                            futr_input_size + stat_input_size\n                        )\n                        basis = ExogenousBasis(forecast_size=h)\n\n                else:\n                    raise ValueError(f\"Block type {stack_types[i]} not found!\")\n\n                nbeats_block = NBEATSBlock(\n                    input_size=input_size,\n                    h=h,\n                    futr_input_size=futr_input_size,\n                    hist_input_size=hist_input_size,\n                    stat_input_size=stat_input_size,\n                    n_theta=n_theta,\n                    mlp_units=mlp_units,\n                    basis=basis,\n                    dropout_prob=dropout_prob_theta,\n                    activation=activation,\n                )\n\n            # Select type of evaluation and apply it to all layers of block\n            block_list.append(nbeats_block)\n\n    return block_list\n```\n\n----------------------------------------\n\nTITLE: Using AutoTCN for Fitting, Predicting, and Optuna Backend in Python\nDESCRIPTION: This snippet demonstrates how to use the `AutoTCN` class. It first initializes `AutoTCN` with a custom configuration dictionary, limiting `max_steps` and specifying `input_size` and `encoder_hidden_size`. It then fits the model to a dataset and makes predictions. Finally, it shows how to initialize `AutoTCN` to use the Optuna backend for hyperparameter search by setting `backend='optuna'` and `config=None` (to use the default Optuna-compatible search space). The `%%capture` magic command suppresses the output of the cell.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n# Use your own config or AutoTCN.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=-1, encoder_hidden_size=8)\nmodel = AutoTCN(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n# Optuna\nmodel = AutoTCN(h=12, config=None, backend='optuna')\n```\n\n----------------------------------------\n\nTITLE: Defining AutoTimeMixer Class in Python\nDESCRIPTION: This snippet defines the AutoTimeMixer class, which inherits from BaseAuto. It includes the default configuration, initialization method, and a class method for getting the default configuration. The class supports both Ray and Optuna backends for hyperparameter tuning.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_111\n\nLANGUAGE: python\nCODE:\n```\nclass AutoTimeMixer(BaseAuto):\n\n    default_config = {\n        \"input_size_multiplier\": [1, 2, 3, 4, 5],\n        \"h\": None,\n        \"n_series\": None,\n        \"d_model\": tune.choice([16, 32, 64]),\n        \"d_ff\": tune.choice([16, 32, 64]),\n        \"down_sampling_layers\": tune.choice([1, 2]),\n        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n        \"scaler_type\": tune.choice([None, 'robust', 'standard', 'identity']),\n        \"max_steps\": tune.choice([500, 1000, 2000]),\n        \"batch_size\": tune.choice([32, 64, 128, 256]),\n        \"loss\": None,\n        \"random_seed\": tune.randint(1, 20),\n    }\n\n    def __init__(self,\n                 h,\n                 n_series,\n                 loss=MAE(),\n                 valid_loss=None,\n                 config=None, \n                 search_alg=BasicVariantGenerator(random_state=1),\n                 num_samples=10,\n                 refit_with_val=False,\n                 cpus=cpu_count(),\n                 gpus=torch.cuda.device_count(),\n                 verbose=False,\n                 alias=None,\n                 backend='ray',\n                 callbacks=None):\n        \n        # Define search space, input/output sizes\n        if config is None:\n            config = self.get_default_config(h=h, backend=backend, n_series=n_series)                 \n\n        # Always use n_series from parameters, raise exception with Optuna because we can't enforce it\n        if backend == 'ray':\n            config['n_series'] = n_series\n        elif backend == 'optuna':\n            mock_trial = MockTrial()\n            if ('n_series' in config(mock_trial) and config(mock_trial)['n_series'] != n_series) or ('n_series' not in config(mock_trial)):\n                raise Exception(f\"config needs 'n_series': {n_series}\")           \n\n        super(AutoTimeMixer, self).__init__(\n              cls_model=TimeMixer, \n              h=h,\n              loss=loss,\n              valid_loss=valid_loss,\n              config=config,\n              search_alg=search_alg,\n              num_samples=num_samples, \n              refit_with_val=refit_with_val,\n              cpus=cpus,\n              gpus=gpus,\n              verbose=verbose,\n              alias=alias,\n              backend=backend,\n              callbacks=callbacks,            \n        )\n\n    @classmethod\n    def get_default_config(cls, h, backend, n_series):\n        config = cls.default_config.copy()        \n        config['input_size'] = tune.choice([h * x \\\n                        for x in config[\"input_size_multiplier\"]])\n\n        # Rolling windows with step_size=1 or step_size=h\n        # See `BaseWindows` and `BaseRNN`'s create_windows\n        config['step_size'] = tune.choice([1, h])\n        del config[\"input_size_multiplier\"]\n        if backend == 'optuna':\n            # Always use n_series from parameters\n            config['n_series'] = n_series\n            config = cls._ray_config_to_optuna(config)           \n\n        return config         \n```\n\n----------------------------------------\n\nTITLE: Recurrent Multi-Step Forecasting in Neural Time Series Models\nDESCRIPTION: Implements recurrent multi-step forecasting for time series predictions. This method handles the state management of RNNs and iteratively forecasts one step at a time, feeding each prediction back as input for the next step. It also manages exogenous variables across the forecast horizon.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_27\n\nLANGUAGE: Python\nCODE:\n```\ndef _predict_step_recurrent_batch(self, insample_y, insample_mask, futr_exog, hist_exog, stat_exog, y_idx):\n        # Remember state in network and set horizon to 1\n        self.rnn_state = None\n        self.maintain_state = True\n        self.h = 1\n\n        # Initialize results array\n        n_outputs = len(self.loss.output_names)\n        y_hat = torch.zeros((insample_y.shape[0],\n                            self.horizon_backup,\n                            self.n_series,\n                            n_outputs),\n                            device=insample_y.device,\n                            dtype=insample_y.dtype)\n\n        # First step prediction\n        tau = 0\n        \n        # Set exogenous\n        hist_exog_current = None\n        if self.hist_exog_size > 0:\n            hist_exog_current = hist_exog[:, :self.input_size + tau]\n\n        futr_exog_current = None\n        if self.futr_exog_size > 0:\n            futr_exog_current = futr_exog[:, :self.input_size + tau]\n\n        # First forecast step\n        y_hat[:, tau], insample_y = self._predict_step_recurrent_single(\n                                                                insample_y=insample_y[:, :self.input_size + tau],\n                                                                insample_mask=insample_mask[:, :self.input_size + tau],\n                                                                hist_exog=hist_exog_current,\n                                                                futr_exog=futr_exog_current,\n                                                                stat_exog=stat_exog,\n                                                                y_idx=y_idx,\n                                                                )\n\n        # Horizon prediction recursively\n        for tau in range(1, self.horizon_backup):\n            # Set exogenous\n            if self.hist_exog_size > 0:\n                hist_exog_current = hist_exog[:, self.input_size + tau - 1].unsqueeze(1)\n\n            if self.futr_exog_size > 0:\n                futr_exog_current = futr_exog[:, self.input_size + tau - 1].unsqueeze(1)\n            \n            y_hat[:, tau], insample_y = self._predict_step_recurrent_single(\n                                                                insample_y=insample_y,\n                                                                insample_mask=None,\n                                                                hist_exog=hist_exog_current,\n                                                                futr_exog=futr_exog_current,\n                                                                stat_exog=stat_exog,\n                                                                y_idx = y_idx,\n                                                                )\n        \n        # Reset state and horizon\n        self.maintain_state = False\n        self.rnn_state = None\n        self.h = self.horizon_backup\n\n        # Squeeze for univariate case\n        if not self.MULTIVARIATE:\n            y_hat = y_hat.squeeze(2)\n\n        return y_hat\n```\n\n----------------------------------------\n\nTITLE: Initializing and Fitting AutoSOFTS Model in Python\nDESCRIPTION: This snippet demonstrates how to initialize an AutoSOFTS model with custom configuration and fit it to a dataset. It uses the Ray backend for hyperparameter tuning and sets specific parameters such as max_steps, val_check_steps, input_size, and hidden_size.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_110\n\nLANGUAGE: python\nCODE:\n```\nmy_config = AutoSOFTS.get_default_config(h=12, n_series=1, backend='ray')\nmy_config['max_steps'] = 1\nmy_config['val_check_steps'] = 1\nmy_config['input_size'] = 12\nmy_config['hidden_size'] = 16\nmodel = AutoSOFTS(h=12, n_series=1, config=my_config, backend='ray', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: Implementing Residual LSTM Layer in PyTorch\nDESCRIPTION: Creates a ResLSTMLayer that processes sequences using the ResLSTMCell. It applies the cell sequentially to each time step in the input sequence, collecting outputs and maintaining the hidden state throughout the sequence processing.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.dilated_rnn.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n#| exporti\nclass ResLSTMLayer(nn.Module):\n    def __init__(self, input_size, hidden_size, dropout=0.):\n        super(ResLSTMLayer, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.cell = ResLSTMCell(input_size, hidden_size, dropout=0.)\n\n    def forward(self, inputs, hidden):\n        inputs = inputs.unbind(0)\n        outputs = []\n        for i in range(len(inputs)):\n                out, hidden = self.cell(inputs[i], hidden)\n                outputs += [out]\n        outputs = torch.stack(outputs)\n        return outputs, hidden\n```\n\n----------------------------------------\n\nTITLE: Testing Custom Optimizer Behavior\nDESCRIPTION: Validates that custom optimizer configurations produce different results from default settings across different model types.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_60\n\nLANGUAGE: python\nCODE:\n```\nfor nf_model in [NHITS, RNN, StemGNN]:\n    params = {\"h\": 12, \"input_size\": 24, \"max_steps\": 1}\n    if nf_model.__name__ == \"StemGNN\":\n        params.update({\"n_series\": 2})\n    models = [nf_model(**params)]\n    nf = NeuralForecast(models=models, freq='M')\n    nf.fit(AirPassengersPanel_train)\n    default_optimizer_predict = nf.predict()\n    mean = default_optimizer_predict.loc[:, nf_model.__name__].mean()\n\n    params.update({\n        \"optimizer\": torch.optim.Adadelta,\n        \"optimizer_kwargs\": {\"rho\": 0.45}, \n    })\n    models2 = [nf_model(**params)]\n    nf2 = NeuralForecast(models=models2, freq='M')\n    nf2.fit(AirPassengersPanel_train)\n    customized_optimizer_predict = nf2.predict()\n    mean2 = customized_optimizer_predict.loc[:, nf_model.__name__].mean()\n    assert mean2 != mean\n```\n\n----------------------------------------\n\nTITLE: Testing Optimizer Keywords Warning\nDESCRIPTION: Validates that a warning is issued when optimizer_kwargs is provided without optimizer.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_63\n\nLANGUAGE: python\nCODE:\n```\nfor nf_model in [NHITS, RNN, StemGNN]:\n    params = {\n        \"h\": 12, \n        \"input_size\": 24, \n        \"max_steps\": 1,\n        \"optimizer_kwargs\": {\"lr\": 0.8, \"rho\": 0.45}\n    }\n    if nf_model.__name__ == \"StemGNN\":\n        params.update({\"n_series\": 2})\n    models = [nf_model(**params)]\n    nf = NeuralForecast(models=models, freq='M')\n    with warnings.catch_warnings(record=True) as issued_warnings:\n        warnings.simplefilter('always', UserWarning)\n        nf.fit(AirPassengersPanel_train)\n        assert any(\"ignoring optimizer_kwargs as the optimizer is not specified\" in str(w.message) for w in issued_warnings)\n```\n\n----------------------------------------\n\nTITLE: Creating AutoCorrelation Layer in PyTorch\nDESCRIPTION: A layer implementation that wraps the AutoCorrelation mechanism with linear projections for queries, keys, and values. Handles multi-head attention processing.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.autoformer.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass AutoCorrelationLayer(nn.Module):\n    def __init__(self, correlation, hidden_size, n_head, d_keys=None,\n                 d_values=None):\n        super(AutoCorrelationLayer, self).__init__()\n\n        d_keys = d_keys or (hidden_size // n_head)\n        d_values = d_values or (hidden_size // n_head)\n\n        self.inner_correlation = correlation\n        self.query_projection = nn.Linear(hidden_size, d_keys * n_head)\n        self.key_projection = nn.Linear(hidden_size, d_keys * n_head)\n        self.value_projection = nn.Linear(hidden_size, d_values * n_head)\n        self.out_projection = nn.Linear(d_values * n_head, hidden_size)\n        self.n_head = n_head\n```\n\n----------------------------------------\n\nTITLE: Implementing AutoTimesNet Class for CNN-Based Forecasting\nDESCRIPTION: Implementation of the AutoTimesNet class for automated hyperparameter tuning of the TimesNet model. This class extends BaseAuto with configuration specific to CNN-based time series forecasting models, supporting both Ray and Optuna backends.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_93\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass AutoTimesNet(BaseAuto):\n\n    default_config = {\n        \"input_size_multiplier\": [1, 2, 3, 4, 5],\n        \"h\": None,\n        \"hidden_size\": tune.choice([32, 64, 128]),\n        \"conv_hidden_size\": tune.choice([32, 64, 128]),\n        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n        \"scaler_type\": tune.choice(['robust', 'standard']),\n        \"max_steps\": tune.choice([500, 1000, 2000]),\n        \"batch_size\": tune.choice([32, 64, 128]),\n        \"windows_batch_size\": tune.choice([32, 64, 128, 256]),\n        \"loss\": None,\n        \"random_seed\": tune.randint(1, 20),\n    }\n\n    def __init__(self,\n                 h,\n                 loss=MAE(),\n                 valid_loss=None,\n                 config=None, \n                 search_alg=BasicVariantGenerator(random_state=1),\n                 num_samples=10,\n                 refit_with_val=False,\n                 cpus=cpu_count(),\n                 gpus=torch.cuda.device_count(),\n                 verbose=False,\n                 alias=None,\n                 backend='ray',\n                 callbacks=None):\n        \n        # Define search space, input/output sizes\n        if config is None:\n            config = self.get_default_config(h=h, backend=backend)                  \n\n        super(AutoTimesNet, self).__init__(\n              cls_model=TimesNet, \n              h=h,\n              loss=loss,\n              valid_loss=valid_loss,\n              config=config,\n              search_alg=search_alg,\n              num_samples=num_samples, \n              refit_with_val=refit_with_val,\n              cpus=cpus,\n              gpus=gpus,\n              verbose=verbose,\n              alias=alias,\n              backend=backend,\n              callbacks=callbacks,            \n        )\n\n    @classmethod\n    def get_default_config(cls, h, backend, n_series=None):\n        config = cls.default_config.copy()\n        config['input_size'] = tune.choice([h*x \\\n                        for x in config['input_size_multiplier']])\n        config['step_size'] = tune.choice([1, h])        \n        del config['input_size_multiplier']\n        if backend == 'optuna':\n            config = cls._ray_config_to_optuna(config)         \n\n        return config         \n```\n\n----------------------------------------\n\nTITLE: Evaluating Model Performance with Error Metrics\nDESCRIPTION: Imports evaluation utilities and calculates Mean Absolute Error (MAE) and Mean Squared Error (MSE) to quantitatively compare model performance on the test set.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/08_multivariate_tsmixer.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom utilsforecast.evaluation import evaluate\nfrom utilsforecast.losses import mae, mse\n```\n\nLANGUAGE: python\nCODE:\n```\nevaluate(Y_hat_df.drop(columns='cutoff'), metrics=[mae, mse], agg_fn='mean')\n```\n\n----------------------------------------\n\nTITLE: Generating NHITS Decomposition\nDESCRIPTION: Creates a decomposition of the time series using the trained NHITS model to visualize learned components.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/14_interpretable_decompositions.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom neuralforecast.tsdataset import TimeSeriesDataset\n\n# NHITS decomposition plot\nmodel = nf.models[0]\ndataset, *_ = TimeSeriesDataset.from_df(df = Y_train_df)\ny_hat = model.decompose(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: Testing predict_insample Method with Variable-Length Series\nDESCRIPTION: Tests the predict_insample method with time series of different lengths. Ensures that the function correctly handles varying time series lengths while producing the expected number of forecast points.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_35\n\nLANGUAGE: python\nCODE:\n```\n# Test predict_insample (different lengths)\ndiff_len_df = generate_series(n_series=n_series, max_length=100)\n\nnf = NeuralForecast(models=models, freq='D')\ncv = nf.cross_validation(df=diff_len_df, val_size=0, test_size=test_size, n_windows=None)\n\nforecasts = nf.predict_insample(step_size=1)\nexpected_size = get_expected_size(diff_len_df, h, test_size, step_size=1)\nassert len(forecasts) == expected_size, f'Shape mismatch in predict_insample: {len(forecasts)=}, {expected_size=}'\n```\n\n----------------------------------------\n\nTITLE: GRU Forward Pass Implementation\nDESCRIPTION: Implementation of the forward pass for the GRU model. Processes input batches including handling of historical, static and future exogenous variables. Supports both recurrent and non-recurrent modes of operation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.gru.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, windows_batch):\n    # Parse windows_batch\n    encoder_input = windows_batch['insample_y']                         # [B, seq_len, 1]\n    futr_exog     = windows_batch['futr_exog']                          # [B, seq_len, F]\n    hist_exog     = windows_batch['hist_exog']                          # [B, seq_len, X]\n    stat_exog     = windows_batch['stat_exog']                          # [B, S]\n\n    # Concatenate y, historic and static inputs              \n    batch_size, seq_len = encoder_input.shape[:2]\n    if self.hist_exog_size > 0:\n        encoder_input = torch.cat((encoder_input, hist_exog), dim=2)    # [B, seq_len, 1] + [B, seq_len, X] -> [B, seq_len, 1 + X]\n\n    if self.stat_exog_size > 0:\n        stat_exog = stat_exog.unsqueeze(1).repeat(1, seq_len, 1)        # [B, S] -> [B, seq_len, S]\n        encoder_input = torch.cat((encoder_input, stat_exog), dim=2)    # [B, seq_len, 1 + X] + [B, seq_len, S] -> [B, seq_len, 1 + X + S]\n\n    if self.futr_exog_size > 0:\n        encoder_input = torch.cat((encoder_input, \n                                   futr_exog[:, :seq_len]), dim=2)      # [B, seq_len, 1 + X + S] + [B, seq_len, F] -> [B, seq_len, 1 + X + S + F]\n\n    if self.RECURRENT:\n        if self.maintain_state:\n            rnn_state = self.rnn_state\n        else:\n            rnn_state = None\n        \n        output, rnn_state = self.hist_encoder(encoder_input, \n                                                        rnn_state)      # [B, seq_len, rnn_hidden_state]\n        output = self.proj(output)                                      # [B, seq_len, rnn_hidden_state] -> [B, seq_len, n_output]\n        if self.maintain_state:\n            self.rnn_state = rnn_state\n    else:\n        hidden_state, _ = self.hist_encoder(encoder_input, None)       # [B, seq_len, rnn_hidden_state]\n        hidden_state = hidden_state[:, -self.h:]                       # [B, seq_len, rnn_hidden_state] -> [B, h, rnn_hidden_state]\n        \n        if self.futr_exog_size > 0:\n            futr_exog_futr = futr_exog[:, -self.h:]                    # [B, h, F]\n            hidden_state = torch.cat((hidden_state, \n                                      futr_exog_futr), dim=-1)          # [B, h, rnn_hidden_state] + [B, h, F] -> [B, h, rnn_hidden_state + F]\n\n        output = self.mlp_decoder(hidden_state)                        # [B, h, rnn_hidden_state + F] -> [B, seq_len, n_output]\n\n    return output[:, -self.h:]\n```\n\n----------------------------------------\n\nTITLE: Forecasting Air Passengers with VanillaTransformer in Python\nDESCRIPTION: This example demonstrates a typical workflow using the `VanillaTransformer` within the `NeuralForecast` framework. It involves loading and preparing the AirPassengers dataset, configuring the model and trainer (`NeuralForecast`), fitting the model to training data, generating forecasts on test data, and finally plotting the actual values against the predictions (including confidence intervals if applicable).\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.vanillatransformer.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n#| eval: false\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import VanillaTransformer\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = VanillaTransformer(h=12,\n                 input_size=24,\n                 hidden_size=16,\n                 conv_hidden_size=32,\n                 n_head=2,\n                 loss=MAE(),\n                 scaler_type='robust',\n                 learning_rate=1e-3,\n                 max_steps=500,\n                 val_check_steps=50,\n                 early_stop_patience_steps=2)\n\nnf = NeuralForecast(\n    models=[model],\n    freq='ME'\n)\nnf.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nforecasts = nf.predict(futr_df=Y_test_df)\n\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nif model.loss.is_distribution_output:\n    plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n    plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n    plt.plot(plot_df['ds'], plot_df['VanillaTransformer-median'], c='blue', label='median')\n    plt.fill_between(x=plot_df['ds'][-12:], \n                    y1=plot_df['VanillaTransformer-lo-90'][-12:].values, \n                    y2=plot_df['VanillaTransformer-hi-90'][-12:].values,\n                    alpha=0.4, label='level 90')\n    plt.grid()\n    plt.legend()\n    plt.plot()\nelse:\n    plot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\n    plt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\n    plt.plot(plot_df['ds'], plot_df['VanillaTransformer'], c='blue', label='Forecast')\n    plt.legend()\n    plt.grid()\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for TSMixerx in Python\nDESCRIPTION: Imports necessary Python libraries and modules for implementing the TSMixerx model, including PyTorch, typing, and custom NeuralForecast modules.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tsmixerx.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom typing import Optional\nfrom neuralforecast.losses.pytorch import MAE\nfrom neuralforecast.common._base_model import BaseModel\nfrom neuralforecast.common._modules import RevINMultivariate\n```\n\n----------------------------------------\n\nTITLE: Implementing MovingAvg Class for Trend Highlighting\nDESCRIPTION: Defines a MovingAvg class that applies a moving average to highlight the trend in time series data. It uses PyTorch's AvgPool1d for efficient computation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.dlinear.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass MovingAvg(nn.Module):\n    \"\"\"\n    Moving average block to highlight the trend of time series\n    \"\"\"\n    def __init__(self, kernel_size, stride):\n        super(MovingAvg, self).__init__()\n        self.kernel_size = kernel_size\n        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n        \n    def forward(self, x):\n        # padding on the both ends of time series\n        front = x[:, 0:1].repeat(1, (self.kernel_size - 1) // 2)\n        end = x[:, -1:].repeat(1, (self.kernel_size - 1) // 2)\n        x = torch.cat([front, x, end], dim=1)\n        x = self.avg(x)\n        return x\n```\n\n----------------------------------------\n\nTITLE: Testing LSTM Model with Pandas and Polars DataFrame Equivalence\nDESCRIPTION: Verifies that NeuralForecast produces identical results when using either pandas or polars DataFrames as input. Tests fit, predict, predict_insample, and cross_validation methods with both DataFrame types.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_56\n\nLANGUAGE: python\nCODE:\n```\nmodels = [LSTM(h=12, input_size=24, max_steps=5, scaler_type='robust')]\n\n# Pandas\nnf = NeuralForecast(models=models, freq='M')\nnf.fit(AirPassengersPanel_train, static_df=AirPassengersStatic)\ninsample_preds = nf.predict_insample()\npreds = nf.predict()\ncv_res = nf.cross_validation(df=AirPassengersPanel_train, static_df=AirPassengersStatic)\n\n# Polars\nnf = NeuralForecast(models=models, freq='1mo')\nnf.fit(\n    AirPassengers_pl,\n    static_df=AirPassengersStatic_pl,\n    id_col='uid',\n    time_col='time',\n    target_col='target',\n)\ninsample_preds_pl = nf.predict_insample()\npreds_pl = nf.predict()\ncv_res_pl = nf.cross_validation(\n    df=AirPassengers_pl,\n    static_df=AirPassengersStatic_pl,\n    id_col='uid',\n    time_col='time',\n    target_col='target',\n)\n\ndef assert_equal_dfs(pandas_df, polars_df):\n    mapping = {k: v for k, v in inverse_renamer.items() if k in polars_df}\n    pd.testing.assert_frame_equal(\n        pandas_df,\n        polars_df.rename(mapping).to_pandas(),\n    )\n\nassert_equal_dfs(preds, preds_pl)\nassert_equal_dfs(insample_preds, insample_preds_pl)\nassert_equal_dfs(cv_res, cv_res_pl)\n```\n\n----------------------------------------\n\nTITLE: Implementing TST Encoder and Encoder Layer\nDESCRIPTION: Core transformer encoder implementation with support for residual attention and multiple encoder layers.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.patchtst.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass TSTEncoder(nn.Module):\n    def __init__(self, q_len, hidden_size, n_heads, d_k=None, d_v=None, linear_hidden_size=None, \n                        norm='BatchNorm', attn_dropout=0., dropout=0., activation='gelu',\n                        res_attention=False, n_layers=1, pre_norm=False, store_attn=False):\n        super().__init__()\n\n        self.layers = nn.ModuleList([TSTEncoderLayer(q_len, hidden_size, n_heads=n_heads, d_k=d_k, d_v=d_v,\n                                                    linear_hidden_size=linear_hidden_size, norm=norm,\n                                                    attn_dropout=attn_dropout, dropout=dropout,\n                                                    activation=activation, res_attention=res_attention,\n                                                    pre_norm=pre_norm, store_attn=store_attn) for i in range(n_layers)])\n        self.res_attention = res_attention\n```\n\n----------------------------------------\n\nTITLE: Implementing Huberized Implicit Quantile Loss in Python\nDESCRIPTION: Defines the HuberIQLoss class, which computes the huberized quantile loss between actual and predicted values. It supports implicit quantile networks and includes methods for sampling quantiles and domain mapping.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_74\n\nLANGUAGE: python\nCODE:\n```\nclass HuberIQLoss(HuberQLoss):\n    \"\"\"Implicit Huber Quantile Loss\n\n    Computes the huberized quantile loss between `y` and `y_hat`, with the quantile `q` provided as an input to the network. \n    HuberIQLoss measures the deviation of a huberized quantile forecast.\n    By weighting the absolute deviation in a non symmetric way, the\n    loss pays more attention to under or over estimation.\n\n    $$ \\mathrm{HuberQL}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q)}_{\\tau}) = \n    (1-q)\\, L_{\\delta}(y_{\\tau},\\; \\hat{y}^{(q)}_{\\tau}) \\mathbb{1}\\{ \\hat{y}^{(q)}_{\\tau} \\geq y_{\\tau} \\} + \n    q\\, L_{\\delta}(y_{\\tau},\\; \\hat{y}^{(q)}_{\\tau}) \\mathbb{1}\\{ \\hat{y}^{(q)}_{\\tau} < y_{\\tau} \\} $$\n\n    **Parameters:**<br>\n    `quantile_sampling`: str, default='uniform', sampling distribution used to sample the quantiles during training. Choose from ['uniform', 'beta']. <br>\n    `horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n    `delta`: float=1.0, Specifies the threshold at which to change between delta-scaled L1 and L2 loss.<br>\n\n    **References:**<br>\n    [Gouttes, Adle, Kashif Rasul, Mateusz Koren, Johannes Stephan, and Tofigh Naghibi, \"Probabilistic Time Series Forecasting with Implicit Quantile Networks\".](http://arxiv.org/abs/2107.03743)\n    [Huber Peter, J (1964). \"Robust Estimation of a Location Parameter\". Annals of Statistics](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-35/issue-1/Robust-Estimation-of-a-Location-Parameter/10.1214/aoms/1177703732.full)<br>\n    [Roger Koenker and Gilbert Bassett, Jr., \"Regression Quantiles\".](https://www.jstor.org/stable/1913643)\n    \"\"\"\n    def __init__(self, cos_embedding_dim = 64, concentration0 = 1.0, concentration1 = 1.0, delta = 1.0, horizon_weight=None):\n        self.update_quantile()\n        super(HuberIQLoss, self).__init__(\n            q = self.q,\n            delta = delta,\n            horizon_weight=horizon_weight\n        )\n\n        self.cos_embedding_dim = cos_embedding_dim\n        self.concentration0 = concentration0\n        self.concentration1 = concentration1\n        self.has_sampled = False\n        self.has_predicted = False\n\n        self.quantile_layer = QuantileLayer(\n            num_output=1, cos_embedding_dim=self.cos_embedding_dim\n        )\n        self.output_layer = nn.Sequential(\n            nn.Linear(1, 1), nn.PReLU()\n        )\n        \n    def _sample_quantiles(self, sample_size, device):\n        if not self.has_sampled:\n            self._init_sampling_distribution(device)\n\n        quantiles = self.sampling_distr.sample(sample_size)\n        self.q = quantiles.squeeze(-1)\n        self.has_sampled = True        \n        self.has_predicted = False\n\n        return quantiles\n    \n    def _init_sampling_distribution(self, device):\n        concentration0 = torch.tensor([self.concentration0],\n                                      device=device,\n                                      dtype=torch.float32)\n        concentration1 = torch.tensor([self.concentration1],\n                                      device=device,\n                                      dtype=torch.float32)        \n        self.sampling_distr = Beta(concentration0 = concentration0,\n                                   concentration1 = concentration1)\n\n    def update_quantile(self, q: List[float] = [0.5]):\n        self.q = q[0]\n        self.output_names = [f\"_ql{q[0]}\"]\n        self.has_predicted = True\n\n    def domain_map(self, y_hat):\n        \"\"\"\n        Adds IQN network to output of network\n\n        Input shapes to this function:\n         \n        Univariate: y_hat = [B, h, 1] \n        Multivariate: y_hat = [B, h, N]\n        \"\"\"\n        if self.eval() and self.has_predicted:\n            quantiles = torch.full(size=y_hat.shape, \n                                    fill_value=self.q,\n                                    device=y_hat.device,\n                                    dtype=y_hat.dtype) \n            quantiles = quantiles.unsqueeze(-1)             \n        else:\n            quantiles = self._sample_quantiles(sample_size=y_hat.shape,\n                                        device=y_hat.device)\n\n        # Embed the quantiles and add to y_hat\n        emb_taus = self.quantile_layer(quantiles)\n        emb_inputs = y_hat.unsqueeze(-1) * (1.0 + emb_taus)\n        emb_outputs = self.output_layer(emb_inputs)\n        \n        # Domain map\n        y_hat = emb_outputs.squeeze(-1)\n\n        return y_hat\n```\n\n----------------------------------------\n\nTITLE: Computing Validation Loss in Python using PyTorch\nDESCRIPTION: This method calculates the validation loss based on the model's output. It handles both distributional outputs (retrieving distribution arguments, potentially sampling quantiles or means) and point forecasts (applying inverse normalization before calculating loss). It uses the configured validation loss function (`self.valid_loss`) and considers the outsample mask. Dependencies include `self.loss`, `self.valid_loss`, `_get_loc_scale`, `_inv_normalization`, and specific loss classes from the `losses` module.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n    def _compute_valid_loss(self, insample_y, outsample_y, output, outsample_mask, y_idx):\n        if self.loss.is_distribution_output:\n            y_loc, y_scale = self._get_loc_scale(y_idx)\n            distr_args = self.loss.scale_decouple(output=output, loc=y_loc, scale=y_scale)\n            if isinstance(self.valid_loss, (losses.sCRPS, losses.MQLoss, losses.HuberMQLoss)):\n                _, _, quants  = self.loss.sample(distr_args=distr_args)            \n                output = quants\n            elif isinstance(self.valid_loss, losses.BasePointLoss):\n                distr = self.loss.get_distribution(distr_args=distr_args)\n                output = distr.mean\n\n        # Validation Loss evaluation\n        if self.valid_loss.is_distribution_output:\n            valid_loss = self.valid_loss(y=outsample_y, distr_args=distr_args, mask=outsample_mask)\n        else:\n            output = self._inv_normalization(y_hat=output, y_idx=y_idx)\n            valid_loss = self.valid_loss(y=outsample_y, y_hat=output, y_insample=insample_y, mask=outsample_mask)\n        return valid_loss\n```\n\n----------------------------------------\n\nTITLE: Implementing Fourier Transform Components in Python\nDESCRIPTION: Implements Fourier transformation blocks and cross-attention mechanisms for processing frequency domain features. Includes utilities for mode selection and complex number operations.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.fedformer.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef get_frequency_modes(seq_len, modes=64, mode_select_method='random'):\n    \"\"\"\n    Get modes on frequency domain:\n        'random' for sampling randomly\n        'else' for sampling the lowest modes;\n    \"\"\"\n    modes = min(modes, seq_len//2)\n    if mode_select_method == 'random':\n        index = list(range(0, seq_len // 2))\n        np.random.shuffle(index)\n        index = index[:modes]\n    else:\n        index = list(range(0, modes))\n    index.sort()\n    return index\n\n\nclass FourierBlock(nn.Module):\n    \"\"\"\n    Fourier block\n    \"\"\"\n    def __init__(self, in_channels, out_channels, seq_len, modes=0, mode_select_method='random'):\n        super(FourierBlock, self).__init__()\n        # get modes on frequency domain\n        self.index = get_frequency_modes(seq_len, modes=modes, mode_select_method=mode_select_method)\n\n        self.scale = (1 / (in_channels * out_channels))\n        self.weights1 = nn.Parameter(\n            self.scale * torch.rand(8, in_channels // 8, out_channels // 8, len(self.index), dtype=torch.cfloat))\n\n    # Complex multiplication\n    def compl_mul1d(self, input, weights):\n        # (batch, in_channel, x ), (in_channel, out_channel, x) -> (batch, out_channel, x)\n        return torch.einsum(\"bhi,hio->bho\", input, weights)\n\n    def forward(self, q, k, v, mask):\n        # size = [B, L, H, E]\n        B, L, H, E = q.shape\n        \n        x = q.permute(0, 2, 3, 1)\n        # Compute Fourier coefficients\n        x_ft = torch.fft.rfft(x, dim=-1)\n        # Perform Fourier neural operations\n        out_ft = torch.zeros(B, H, E, L // 2 + 1, device=x.device, dtype=torch.cfloat)\n        for wi, i in enumerate(self.index):\n            out_ft[:, :, :, wi] = self.compl_mul1d(x_ft[:, :, :, i], self.weights1[:, :, :, wi])\n        # Return to time domain\n        x = torch.fft.irfft(out_ft, n=x.size(-1))\n        return (x, None)\n\nclass FourierCrossAttention(nn.Module):\n    \"\"\"\n    Fourier Cross Attention layer\n    \"\"\"    \n    def __init__(self, in_channels, out_channels, seq_len_q, seq_len_kv, modes=64, mode_select_method='random',\n                 activation='tanh', policy=0):\n        super(FourierCrossAttention, self).__init__()\n        self.activation = activation\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        # get modes for queries and keys (& values) on frequency domain\n        self.index_q = get_frequency_modes(seq_len_q, modes=modes, mode_select_method=mode_select_method)\n        self.index_kv = get_frequency_modes(seq_len_kv, modes=modes, mode_select_method=mode_select_method)\n\n        self.scale = (1 / (in_channels * out_channels))\n        self.weights1 = nn.Parameter(\n            self.scale * torch.rand(8, in_channels // 8, out_channels // 8, len(self.index_q), dtype=torch.cfloat))\n\n    # Complex multiplication\n    def compl_mul1d(self, input, weights):\n        # (batch, in_channel, x ), (in_channel, out_channel, x) -> (batch, out_channel, x)\n        return torch.einsum(\"bhi,hio->bho\", input, weights)\n\n    def forward(self, q, k, v, mask):\n        # size = [B, L, H, E]\n        B, L, H, E = q.shape\n        xq = q.permute(0, 2, 3, 1)  # size = [B, H, E, L]\n        xk = k.permute(0, 2, 3, 1)\n        #xv = v.permute(0, 2, 3, 1)\n\n        # Compute Fourier coefficients\n        xq_ft_ = torch.zeros(B, H, E, len(self.index_q), device=xq.device, dtype=torch.cfloat)\n        xq_ft = torch.fft.rfft(xq, dim=-1)\n```\n\n----------------------------------------\n\nTITLE: Extracting Attention Weights from TFT Model\nDESCRIPTION: Shows how to extract attention weights from a trained TFT model for interpretability analysis. Attention weights capture the importance assigned to different time steps in making predictions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# | eval: false\nattention = nf.models[0].attention_weights()\n```\n\n----------------------------------------\n\nTITLE: Implementing SeriesDecomp Class for Time Series Decomposition\nDESCRIPTION: Defines a SeriesDecomp class that decomposes a time series into trend and residual components using the MovingAvg class. This is a key component of the DLinear architecture.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.dlinear.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass SeriesDecomp(nn.Module):\n    \"\"\"\n    Series decomposition block\n    \"\"\"\n    def __init__(self, kernel_size):\n        super(SeriesDecomp, self).__init__()\n        self.MovingAvg = MovingAvg(kernel_size, stride=1)\n\n    def forward(self, x):\n        moving_mean = self.MovingAvg(x)\n        res = x - moving_mean\n        return res, moving_mean\n```\n\n----------------------------------------\n\nTITLE: Qualitative Decomposition Evaluation and Visualization - Python\nDESCRIPTION: Calls the decompose method on an NBEATS model to retrieve forecast block contributions, and produces a series of matplotlib subplots for visual inspection: overall forecast, level, identity, trend, and seasonality. This facilitates model interpretability in a real-world scenario. Inputs are predicted decomposition and true/test data. Depends on matplotlib and NBEATS with decomposition support.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeats.ipynb#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n# qualitative decomposition evaluation\ny_hat = model.decompose(dataset=dataset)\n\nfig, ax = plt.subplots(5, 1, figsize=(10, 15))\n\nax[0].plot(Y_test_df['y'].values, label='True', color=\"#9C9DB2\", linewidth=4)\nax[0].plot(y_hat.sum(axis=1).flatten(), label='Forecast', color=\"#7B3841\")\nax[0].grid()\nax[0].legend(prop={'size': 20})\nfor label in (ax[0].get_xticklabels() + ax[0].get_yticklabels()):\n    label.set_fontsize(18)\nax[0].set_ylabel('y', fontsize=20)\n\nax[1].plot(y_hat[0,0], label='level', color=\"#7B3841\")\nax[1].grid()\nax[1].set_ylabel('Level', fontsize=20)\n\nax[2].plot(y_hat[0,1], label='stack1', color=\"#7B3841\")\nax[2].grid()\nax[2].set_ylabel('Identity', fontsize=20)\n\nax[3].plot(y_hat[0,2], label='stack2', color=\"#D9AE9E\")\nax[3].grid()\nax[3].set_ylabel('Trend', fontsize=20)\n\nax[4].plot(y_hat[0,3], label='stack3', color=\"#D9AE9E\")\nax[4].grid()\nax[4].set_ylabel('Seasonality', fontsize=20)\n\nax[4].set_xlabel('Prediction \\u03C4 \\u2208 {t+1,..., t+H}', fontsize=20)\n```\n\n----------------------------------------\n\nTITLE: Testing Model Persistence During Cross-Validation\nDESCRIPTION: Verifies that model state is correctly preserved when using cross-validation with use_init_models=True. The test checks that forecasts before and after cross-validation remain consistent when the initialized models are reused.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n# test fit+cross_validation behaviour\nmodels = [NHITS(h=12, input_size=24, max_steps=10)]\nnf = NeuralForecast(models=models, freq='M')\nnf.fit(AirPassengersPanel_train)\ninit_fcst = nf.predict()\ninit_cv = nf.cross_validation(AirPassengersPanel_train, use_init_models=True)\nafter_cv = nf.cross_validation(AirPassengersPanel_train, use_init_models=True)\nnf.fit(AirPassengersPanel_train, use_init_models=True)\nafter_fcst = nf.predict()\ntest_eq(init_cv, after_cv)\ntest_eq(init_fcst, after_fcst)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Time Series Data with Train/Validation/Test Splits\nDESCRIPTION: Creates a visualization of the temperature data for one transformer with clear markings for validation and test splits to understand the data distribution over time.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/04_longhorizon_nhits.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n\n# We are going to plot the temperature of the transformer \n# and marking the validation and train splits\nu_id = 'HUFL'\nx_plot = pd.to_datetime(Y_df[Y_df.unique_id==u_id].ds)\ny_plot = Y_df[Y_df.unique_id==u_id].y.values\n\nx_val = x_plot[n_time - val_size - test_size]\nx_test = x_plot[n_time - test_size]\n\nfig = plt.figure(figsize=(10, 5))\nfig.tight_layout()\n\nplt.plot(x_plot, y_plot)\nplt.xlabel('Date', fontsize=17)\nplt.ylabel('HUFL [15 min temperature]', fontsize=17)\n\nplt.axvline(x_val, color='black', linestyle='-.')\nplt.axvline(x_test, color='black', linestyle='-.')\nplt.text(x_val, 5, '  Validation', fontsize=12)\nplt.text(x_test, 5, '  Test', fontsize=12)\n\nplt.grid()\n```\n\n----------------------------------------\n\nTITLE: Extracting All Exogenous Variables in NeuralForecast\nDESCRIPTION: Collects both future and historical exogenous variables required by all models. Handles different model types and configuration structures, combining both sets of exogenous variables.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef _get_needed_exog(self):\n    futr_exog = self._get_needed_futr_exog()\n\n    hist_exog = []\n    for m in self.models:\n        if isinstance(m, BaseAuto):\n            if isinstance(m.config, dict):  # ray\n                exogs = m.config.get('hist_exog_list', [])\n                if hasattr(exogs, 'categories'):  # features are being tuned, get possible values\n                    exogs = exogs.categories\n            else:   # optuna\n                exogs = m.config(MockTrial()).get('hist_exog_list', [])\n        else:  # regular model, extract them directly\n            exogs = getattr(m, 'hist_exog_list', [])\n        \n        for exog in exogs:\n            if isinstance(exog, str):\n                hist_exog.append(exog)\n            else:\n                hist_exog.extend(exog)\n\n    return futr_exog | set(hist_exog)\n```\n\n----------------------------------------\n\nTITLE: Loading Static Variables Data\nDESCRIPTION: Loads static variables dataset containing one-hot encoded electricity market information.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/03_exogenous_variables.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nstatic_df = pd.read_csv('https://datasets-nixtla.s3.amazonaws.com/EPF_FR_BE_static.csv')\nstatic_df.head()\n```\n\n----------------------------------------\n\nTITLE: Generating Various Basis Functions for NBEATS\nDESCRIPTION: Implements functions to generate different types of basis functions used in NBEATS, including Legendre, polynomial, changepoint, piecewise linear, linear hat, spline, and Chebyshev bases.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeats.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef generate_legendre_basis(length, n_basis):\n    \"\"\"\n    Generates Legendre polynomial basis functions.\n\n    Parameters:\n    - n_points (int): Number of data points.\n    - n_functions (int): Number of basis functions to generate.\n\n    Returns:\n    - legendre_basis (ndarray): An array of Legendre basis functions.\n    \"\"\"\n    x = np.linspace(-1, 1, length)  # Legendre polynomials are defined on [-1, 1]\n    legendre_basis = np.zeros((length, n_basis))\n    for i in range(n_basis):\n        # Legendre polynomial of degree i\n        P_i = Legendre.basis(i)\n        legendre_basis[:, i] = P_i(x)\n    return legendre_basis\n\ndef generate_polynomial_basis(length, n_basis):\n    \"\"\"\n    Generates standard polynomial basis functions.\n\n    Parameters:\n    - n_points (int): Number of data points.\n    - n_functions (int): Number of polynomial functions to generate.\n\n    Returns:\n    - poly_basis (ndarray): An array of polynomial basis functions.\n    \"\"\"\n    return np.concatenate([np.power(np.arange(length, dtype=float) / length, i)[None, :]\n                                    for i in range(n_basis)]).T\n\n\ndef generate_changepoint_basis(length, n_basis):\n    \"\"\"\n    Generates changepoint basis functions with automatically spaced changepoints.\n\n    Parameters:\n    - n_points (int): Number of data points.\n    - n_functions (int): Number of changepoint functions to generate.\n\n    Returns:\n    - changepoint_basis (ndarray): An array of changepoint basis functions.\n    \"\"\"\n    x = np.linspace(0, 1, length)[:, None]  # Shape: (length, 1)\n    changepoint_locations = np.linspace(0, 1, n_basis + 1)[1:][None, :]  # Shape: (1, n_basis)\n    return np.maximum(0, x - changepoint_locations)\n\ndef generate_piecewise_linear_basis(length, n_basis):\n    \"\"\"\n    Generates piecewise linear basis functions (linear splines).\n\n    Parameters:\n    - n_points (int): Number of data points.\n    - n_functions (int): Number of piecewise linear basis functions to generate.\n\n    Returns:\n    - pw_linear_basis (ndarray): An array of piecewise linear basis functions.\n    \"\"\"\n    x = np.linspace(0, 1, length)\n    knots = np.linspace(0, 1, n_basis+1)\n    pw_linear_basis = np.zeros((length, n_basis))\n    for i in range(1, n_basis):\n        pw_linear_basis[:, i] = np.maximum(0, np.minimum((x - knots[i-1]) / (knots[i] - knots[i-1]), (knots[i+1] - x) / (knots[i+1] - knots[i])))\n    return pw_linear_basis\n\ndef generate_linear_hat_basis(length, n_basis):\n    x = np.linspace(0, 1, length)[:, None]  # Shape: (length, 1)\n    centers = np.linspace(0, 1, n_basis)[None, :]  # Shape: (1, n_basis)\n    width = 1.0 / (n_basis - 1)\n    \n    # Create triangular functions using piecewise linear equations\n    return np.maximum(0, 1 - np.abs(x - centers) / width)\n\ndef generate_spline_basis(length, n_basis):\n    \"\"\"\n    Generates cubic spline basis functions.\n\n    Parameters:\n    - n_points (int): Number of data points.\n    - n_functions (int): Number of basis functions.\n\n    Returns:\n    - spline_basis (ndarray): An array of cubic spline basis functions.\n    \"\"\"\n    if n_basis < 4:\n        raise ValueError(f\"To use the spline basis, n_basis must be set to 4 or more. Current value is {n_basis}\")\n    x = np.linspace(0, 1, length)\n    knots = np.linspace(0, 1, n_basis - 2)\n    t = np.concatenate(([0, 0, 0], knots, [1, 1, 1]))\n    degree = 3\n    # Create basis coefficient matrix once\n    coefficients = np.eye(n_basis)\n    # Create single BSpline object with all coefficients\n    spline = BSpline(t, coefficients.T, degree)\n    return spline(x)\n\ndef generate_chebyshev_basis(length, n_basis):\n    \"\"\"\n    Generates Chebyshev polynomial basis functions.\n\n    Parameters:\n    - n_points (int): Number of data points.\n    - n_functions (int): Number of Chebyshev polynomials to generate.\n\n    Returns:\n    - chebyshev_basis (ndarray): An array of Chebyshev polynomial basis functions.\n    \"\"\"\n    x = np.linspace(-1, 1, length)\n    chebyshev_basis = np.zeros((length, n_basis))\n    for i in range(n_basis):\n        T_i = Chebyshev.basis(i)\n        chebyshev_basis[:, i] = T_i(x)\n    return chebyshev_basis\n\ndef get_basis(length, n_basis, basis):\n    basis_dict = {\n        'legendre': generate_legendre_basis,\n        'polynomial': generate_polynomial_basis,\n        'changepoint': generate_changepoint_basis,\n        'piecewise_linear': generate_piecewise_linear_basis,\n        'linear_hat': generate_linear_hat_basis,\n        'spline': generate_spline_basis,\n        'chebyshev': generate_chebyshev_basis\n    }\n    return basis_dict[basis](length, n_basis+1)\n```\n\n----------------------------------------\n\nTITLE: Implementing AutoTSMixerx Class for Extended Time Series Forecasting in Python\nDESCRIPTION: This class implements AutoTSMixerx, similar to AutoTSMixer but using the TSMixerx model. It includes default configurations, initialization method, and utility functions for handling different backend options (Ray and Optuna).\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_106\n\nLANGUAGE: python\nCODE:\n```\nclass AutoTSMixerx(BaseAuto):\n\n    default_config = {\n        \"input_size_multiplier\": [1, 2, 3, 4],\n        \"h\": None,\n        \"n_series\": None,\n        \"n_block\": tune.choice([1, 2, 4, 6, 8]),\n        \"learning_rate\": tune.loguniform(1e-4, 1e-2),\n        \"ff_dim\": tune.choice([32, 64, 128]),\n        \"scaler_type\": tune.choice(['identity', 'robust', 'standard']),\n        \"max_steps\": tune.choice([500, 1000, 2000]),\n        \"batch_size\": tune.choice([32, 64, 128, 256]),\n        \"dropout\": tune.uniform(0.0, 0.99),\n        \"loss\": None,\n        \"random_seed\": tune.randint(1, 20),\n    }\n\n    def __init__(self,\n                 h,\n                 n_series,\n                 loss=MAE(),\n                 valid_loss=None,\n                 config=None, \n                 search_alg=BasicVariantGenerator(random_state=1),\n                 num_samples=10,\n                 refit_with_val=False,\n                 cpus=cpu_count(),\n                 gpus=torch.cuda.device_count(),\n                 verbose=False,\n                 alias=None,\n                 backend='ray',\n                 callbacks=None):\n        \n        # Define search space, input/output sizes\n        if config is None:\n            config = self.get_default_config(h=h, backend=backend, n_series=n_series)         \n\n        # Always use n_series from parameters, raise exception with Optuna because we can't enforce it\n        if backend == 'ray':\n            config['n_series'] = n_series\n        elif backend == 'optuna':\n            mock_trial = MockTrial()\n            if ('n_series' in config(mock_trial) and config(mock_trial)['n_series'] != n_series) or ('n_series' not in config(mock_trial)):\n                raise Exception(f\"config needs 'n_series': {n_series}\")            \n\n        super(AutoTSMixerx, self).__init__(\n              cls_model=TSMixerx, \n              h=h,\n              loss=loss,\n              valid_loss=valid_loss,\n              config=config,\n              search_alg=search_alg,\n              num_samples=num_samples, \n              refit_with_val=refit_with_val,\n              cpus=cpus,\n              gpus=gpus,\n              verbose=verbose,\n              alias=alias,\n              backend=backend,\n              callbacks=callbacks,            \n        )\n\n    @classmethod\n    def get_default_config(cls, h, backend, n_series):\n        config = cls.default_config.copy()        \n        config['input_size'] = tune.choice([h * x \\\n                        for x in config[\"input_size_multiplier\"]])\n\n        # Rolling windows with step_size=1 or step_size=h\n        # See `BaseWindows` and `BaseRNN`'s create_windows\n        config['step_size'] = tune.choice([1, h])\n        del config[\"input_size_multiplier\"]\n        if backend == 'optuna':\n            # Always use n_series from parameters\n            config['n_series'] = n_series\n            config = cls._ray_config_to_optuna(config)           \n\n        return config\n```\n\n----------------------------------------\n\nTITLE: Testing Seasonality/Trend Basis Protection in NBEATSx\nDESCRIPTION: Test to verify that the NBEATSx model correctly raises an error when the forecast horizon (h) is incompatible with seasonality or trend stack types. The test expects an error message indicating the incompatibility.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeatsx.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# test seasonality/trend basis protection\ntest_fail(NBEATSx.__init__, \n          contains='Horizon `h=1` incompatible with `seasonality` or `trend` in stacks',\n          kwargs=dict(self=BaseModel, h=1, input_size=4))\n```\n\n----------------------------------------\n\nTITLE: Calculating Mean Squared Error (MSE) for Time Series Forecasts\nDESCRIPTION: Implements the Mean Squared Error metric for evaluating time series forecasts. It supports optional weights and axis parameters for flexible calculations across multiple series.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.numpy.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef mse(y: np.ndarray, y_hat: np.ndarray, \n        weights: Optional[np.ndarray] = None,\n        axis: Optional[int] = None) -> Union[float, np.ndarray]:\n    \"\"\"  Mean Squared Error\n\n    Calculates Mean Squared Error between\n    `y` and `y_hat`. MSE measures the relative prediction\n    accuracy of a forecasting method by calculating the \n    squared deviation of the prediction and the true\n    value at a given time, and averages these devations\n    over the length of the series.\n\n    $$ \\mathrm{MSE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} (y_{\\tau} - \\hat{y}_{\\tau})^{2} $$\n\n    **Parameters:**<br>\n    `y`: numpy array, Actual values.<br>\n    `y_hat`: numpy array, Predicted values.<br>\n    `mask`: numpy array, Specifies date stamps per serie to consider in loss.<br>\n\n    **Returns:**<br>\n    `mse`: numpy array, (single value).\n    \"\"\"\n    _metric_protections(y, y_hat, weights)\n\n    delta_y = np.square(y - y_hat)\n    if weights is not None:\n        mse = np.average(delta_y[~np.isnan(delta_y)],\n                         weights=weights[~np.isnan(delta_y)],\n                         axis=axis)\n    else:\n        mse = np.nanmean(delta_y, axis=axis)\n\n    return mse\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom LSTM Cell in PyTorch\nDESCRIPTION: Defines a custom LSTMCell class that implements the LSTM architecture with input, forget, cell, and output gates. It includes parameters for controlling dropout and processes the hidden state and cell state for recurrent connections.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.dilated_rnn.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n#| exporti\nclass LSTMCell(nn.Module):\n    def __init__(self, input_size, hidden_size, dropout=0.):\n        super(LSTMCell, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.weight_ih = nn.Parameter(torch.randn(4 * hidden_size, input_size))\n        self.weight_hh = nn.Parameter(torch.randn(4 * hidden_size, hidden_size))\n        self.bias_ih = nn.Parameter(torch.randn(4 * hidden_size))\n        self.bias_hh = nn.Parameter(torch.randn(4 * hidden_size))\n        self.dropout = dropout\n\n    def forward(self, inputs, hidden):\n        hx, cx = hidden[0].squeeze(0), hidden[1].squeeze(0)\n        gates = (torch.matmul(inputs, self.weight_ih.t()) + self.bias_ih +\n                         torch.matmul(hx, self.weight_hh.t()) + self.bias_hh)\n        ingate, forgetgate, cellgate, outgate = gates.chunk(4, 1)\n\n        ingate = torch.sigmoid(ingate)\n        forgetgate = torch.sigmoid(forgetgate)\n        cellgate = torch.tanh(cellgate)\n        outgate = torch.sigmoid(outgate)\n\n        cy = (forgetgate * cx) + (ingate * cellgate)\n        hy = outgate * torch.tanh(cy)\n\n        return hy, (hy, cy)\n```\n\n----------------------------------------\n\nTITLE: Custom 1D Convolution Implementation\nDESCRIPTION: Implements a custom 1D convolutional layer that supports both forward and backward looking operations with configurable padding, dilation, and groups.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.bitcn.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass CustomConv1d(nn.Module):\n    \"\"\"\n    Forward- and backward looking Conv1D\n    \"\"\"\n    def __init__(self, in_channels, out_channels, kernel_size, padding=0, dilation=1, mode='backward', groups=1):\n        super().__init__()\n        k = np.sqrt(1 / (in_channels * kernel_size))\n        weight_data = -k + 2 * k * torch.rand((out_channels, in_channels // groups, kernel_size))\n        bias_data = -k + 2 * k * torch.rand((out_channels))\n        self.weight = nn.Parameter(weight_data, requires_grad=True)\n        self.bias = nn.Parameter(bias_data, requires_grad=True)  \n        self.dilation = dilation\n        self.groups = groups\n        if mode == 'backward':\n            self.padding_left = padding\n            self.padding_right= 0\n        elif mode == 'forward':\n            self.padding_left = 0\n            self.padding_right= padding            \n\n    def forward(self, x):\n        xp = F.pad(x, (self.padding_left, self.padding_right))\n        return F.conv1d(xp, self.weight, self.bias, dilation=self.dilation, groups=self.groups)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Usage of AutoTSMixer in Python\nDESCRIPTION: This snippet shows how to use the AutoTSMixer class for time series forecasting. It demonstrates initialization with custom and default configurations, fitting the model to a dataset, making predictions, and using different backends (Ray and Optuna).\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_105\n\nLANGUAGE: python\nCODE:\n```\n# Use your own config or AutoTSMixer.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12)\nmodel = AutoTSMixer(h=12, n_series=1, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n# Optuna\nmodel = AutoTSMixer(h=12, n_series=1, config=None, backend='optuna')\n```\n\n----------------------------------------\n\nTITLE: Implementing Attention Weights Visualization Method\nDESCRIPTION: Defines a function to visualize the attention weights of a TFT model in various formats. The function supports different visualization types including time-based plots, horizon-specific plots, and heatmaps.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n# | eval: false\ndef plot_attention(\n    self, plot: str = \"time\", output: str = \"plot\", width: int = 800, height: int = 400\n):\n    \"\"\"\n    Plot the attention weights.\n\n    Args:\n        plot (str, optional): The type of plot to generate. Can be one of the following:\n            - 'time': Display the mean attention weights over time.\n            - 'all': Display the attention weights for each horizon.\n            - 'heatmap': Display the attention weights as a heatmap.\n            - An integer in the range [1, model.h) to display the attention weights for a specific horizon.\n        output (str, optional): The type of output to generate. Can be one of the following:\n            - 'plot': Display the plot directly.\n            - 'figure': Return the plot as a figure object.\n        width (int, optional): Width of the plot in pixels. Default is 800.\n        height (int, optional): Height of the plot in pixels. Default is 400.\n\n    Returns:\n        matplotlib.figure.Figure: If `output` is 'figure', the function returns the plot as a figure object.\n    \"\"\"\n\n    attention = (\n        self.mean_on_batch(self.interpretability_params[\"attn_wts\"])\n        .mean(dim=0)\n        .cpu()\n        .numpy()\n    )\n\n    fig, ax = plt.subplots(figsize=(width / 100, height / 100))\n\n    if plot == \"time\":\n        attention = attention[self.input_size :, :].mean(axis=0)\n        ax.plot(np.arange(-self.input_size, self.h), attention)\n        ax.axvline(\n            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n        )\n        ax.set_title(\"Mean Attention\")\n        ax.set_xlabel(\"time\")\n        ax.set_ylabel(\"Attention\")\n        ax.legend()\n\n    elif plot == \"all\":\n        for i in range(self.input_size, attention.shape[0]):\n            ax.plot(\n                np.arange(-self.input_size, self.h),\n                attention[i, :],\n                label=f\"horizon {i-self.input_size+1}\",\n            )\n        ax.axvline(\n            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n        )\n        ax.set_title(\"Attention per horizon\")\n        ax.set_xlabel(\"time\")\n        ax.set_ylabel(\"Attention\")\n        ax.legend()\n\n    elif plot == \"heatmap\":\n        cax = ax.imshow(\n            attention,\n            aspect=\"auto\",\n            cmap=\"viridis\",\n            extent=[-self.input_size, self.h, -self.input_size, self.h],\n        )\n        fig.colorbar(cax)\n        ax.set_title(\"Attention Heatmap\")\n        ax.set_xlabel(\"Attention (current time step)\")\n        ax.set_ylabel(\"Attention (previous time step)\")\n\n    elif isinstance(plot, int) and (plot in np.arange(1, self.h + 1)):\n        i = self.input_size + plot - 1\n        ax.plot(\n            np.arange(-self.input_size, self.h),\n            attention[i, :],\n            label=f\"horizon {plot}\",\n        )\n        ax.axvline(\n            x=0, color=\"black\", linewidth=3, linestyle=\"--\", label=\"prediction start\"\n        )\n        ax.set_title(f\"Attention weight for horizon {plot}\")\n        ax.set_xlabel(\"time\")\n        ax.set_ylabel(\"Attention\")\n        ax.legend()\n\n    else:\n        raise ValueError(\n            'plot has to be in [\"time\",\"all\",\"heatmap\"] or integer in range(1,model.h)'\n        )\n\n    plt.tight_layout()\n\n    if output == \"plot\":\n        plt.show()\n    elif output == \"figure\":\n        return fig\n    else:\n        raise ValueError(f\"Invalid output: {output}. Expected 'plot' or 'figure'.\")\n```\n\n----------------------------------------\n\nTITLE: Generate Synthetic Panel Time Series Data\nDESCRIPTION: Function to generate synthetic panel data with configurable temporal and static features. Supports multiple series with different lengths and optional temporal/static features.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/utils.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#| export\ndef generate_series(n_series: int,\n                    freq: str = 'D',\n                    min_length: int = 50,\n                    max_length: int = 500,\n                    n_temporal_features: int = 0,\n                    n_static_features: int = 0,\n                    equal_ends: bool = False,\n                    seed: int = 0) -> pd.DataFrame:\n    \"\"\"Generate Synthetic Panel Series.\n\n    Generates `n_series` of frequency `freq` of different lengths in the interval [`min_length`, `max_length`].\n    If `n_temporal_features > 0`, then each serie gets temporal features with random values.\n    If `n_static_features > 0`, then a static dataframe is returned along the temporal dataframe.\n    If `equal_ends == True` then all series end at the same date.\n\n    **Parameters:**<br>\n    `n_series`: int, number of series for synthetic panel.<br>\n    `min_length`: int, minimal length of synthetic panel's series.<br>\n    `max_length`: int, minimal length of synthetic panel's series.<br>\n    `n_temporal_features`: int, default=0, number of temporal exogenous variables for synthetic panel's series.<br>\n    `n_static_features`: int, default=0, number of static exogenous variables for synthetic panel's series.<br>\n    `equal_ends`: bool, if True, series finish in the same date stamp `ds`.<br>\n    `freq`: str, frequency of the data, [panda's available frequencies](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases).<br>\n\n    **Returns:**<br>\n    `freq`: pandas.DataFrame, synthetic panel with columns [`unique_id`, `ds`, `y`] and exogenous.\n    \"\"\"\n    seasonalities = {'D': 7, 'M': 12}\n    season = seasonalities[freq]\n\n    rng = np.random.RandomState(seed)\n    series_lengths = rng.randint(min_length, max_length + 1, n_series)\n    total_length = series_lengths.sum()\n\n    dates = pd.date_range('2000-01-01', periods=max_length, freq=freq).values\n    uids = [\n        np.repeat(i, serie_length) for i, serie_length in enumerate(series_lengths)\n    ]\n    if equal_ends:\n        ds = [dates[-serie_length:] for serie_length in series_lengths]\n    else:\n        ds = [dates[:serie_length] for serie_length in series_lengths]\n\n    y = np.arange(total_length) % season + rng.rand(total_length) * 0.5\n    temporal_df = pd.DataFrame(dict(unique_id=chain.from_iterable(uids),\n                                    ds=chain.from_iterable(ds),\n                                    y=y))\n\n    random.seed(seed)\n    for i in range(n_temporal_features):\n        random.seed(seed)\n        temporal_values = [\n            [random.randint(0, 100)] * serie_length for serie_length in series_lengths\n        ]\n        temporal_df[f'temporal_{i}'] = np.hstack(temporal_values)\n        temporal_df[f'temporal_{i}'] = temporal_df[f'temporal_{i}'].astype('category')\n        if i == 0:\n            temporal_df['y'] = temporal_df['y'] * \\\n                                  (1 + temporal_df[f'temporal_{i}'].cat.codes)\n\n    temporal_df['unique_id'] = temporal_df['unique_id'].astype('category')\n    temporal_df['unique_id'] = temporal_df['unique_id'].cat.as_ordered()\n\n    if n_static_features > 0:\n        static_features = np.random.uniform(low=0.0, high=1.0, \n                        size=(n_series, n_static_features))\n        static_df = pd.DataFrame.from_records(static_features, \n                           columns = [f'static_{i}'for i in  range(n_static_features)])\n        \n        static_df['unique_id'] = np.arange(n_series)\n        static_df['unique_id'] = static_df['unique_id'].astype('category')\n        static_df['unique_id'] = static_df['unique_id'].cat.as_ordered()\n\n        return temporal_df, static_df\n\n    return temporal_df\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Head Attention Layer in PyTorch\nDESCRIPTION: Implements a multi-head attention layer with query, key, and value projections. Handles attention scaling and output projection.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.modules.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nclass AttentionLayer(nn.Module):\n    def __init__(self, attention, hidden_size, n_heads, d_keys=None,\n                 d_values=None):\n        super(AttentionLayer, self).__init__()\n\n        d_keys = d_keys or (hidden_size // n_heads)\n        d_values = d_values or (hidden_size // n_heads)\n\n        self.inner_attention = attention\n        self.query_projection = nn.Linear(hidden_size, d_keys * n_heads)\n        self.key_projection = nn.Linear(hidden_size, d_keys * n_heads)\n        self.value_projection = nn.Linear(hidden_size, d_values * n_heads)\n        self.out_projection = nn.Linear(d_values * n_heads, hidden_size)\n        self.n_heads = n_heads\n\n    def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):\n        B, L, _ = queries.shape\n        _, S, _ = keys.shape\n        H = self.n_heads\n\n        queries = self.query_projection(queries).view(B, L, H, -1)\n        keys = self.key_projection(keys).view(B, S, H, -1)\n        values = self.value_projection(values).view(B, S, H, -1)\n\n        out, attn = self.inner_attention(\n            queries=queries,\n            keys=keys,\n            values=values,\n            attn_mask=attn_mask,\n            tau=tau,\n            delta=delta\n        )\n        out = out.view(B, L, -1)\n\n        return self.out_projection(out), attn\n```\n\n----------------------------------------\n\nTITLE: Initializing Jupyter Notebook Cell Export Configuration\nDESCRIPTION: Configures the Jupyter notebook cell to export content to the models.autoformer module.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.autoformer.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#| default_exp models.autoformer\n```\n\n----------------------------------------\n\nTITLE: Visualizing Harmonic Signal Components\nDESCRIPTION: Creates a plot showing the combined harmonic signal along with its low and high frequency components.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/14_interpretable_decompositions.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfig, ax = plt.subplots(figsize=(6, 2.5))\nplt.plot(y[-80:], label='True')\nplt.plot(y1[-80:], label='Low Frequency', alpha=0.4)\nplt.plot(y2[-80:], label='High Frequency', alpha=0.4)\nplt.ylabel('Harmonic Signal')\nplt.xlabel('Time')\nplt.legend()\nplt.show()\nplt.close()\n```\n\n----------------------------------------\n\nTITLE: Testing Data Scaling Options in NeuralForecast\nDESCRIPTION: Comprehensive test of different scaling methods (standard, robust, robust-iqr, minmax, boxcox) available in NeuralForecast. Verifies that forecasts with scaling remain close to the original scale, and that inverse transformations correctly restore the original scale.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_31\n\nLANGUAGE: python\nCODE:\n```\n# test scaling\nmodels = [NHITS(h=12, input_size=24, max_steps=10)]\nmodels_exog = [NHITS(h=12, input_size=12, max_steps=10, hist_exog_list=['trend'], futr_exog_list=['trend'])]\n\n# fit+predict\nnf = NeuralForecast(models=models, freq='M', local_scaler_type='standard')\nnf.fit(AirPassengersPanel_train)\nscaled_fcst = nf.predict()\n# check that the forecasts are similar to the one without scaling\nnp.testing.assert_allclose(\n    init_fcst['NHITS'].values,\n    scaled_fcst['NHITS'].values,\n    rtol=0.3,\n)\n# with exog\nnf = NeuralForecast(models=models_exog, freq='M', local_scaler_type='standard')\nnf.fit(AirPassengersPanel_train)\nscaled_exog_fcst = nf.predict(futr_df=AirPassengersPanel_test)\n# check that the forecasts are similar to the one without exog\nnp.testing.assert_allclose(\n    scaled_fcst['NHITS'].values,\n    scaled_exog_fcst['NHITS'].values,\n    rtol=0.3,\n)\n\n# CV\nnf = NeuralForecast(models=models, freq='M', local_scaler_type='robust')\ncv_res = nf.cross_validation(AirPassengersPanel)\n# check that the forecasts are similar to the original values (originals are restored directly from the df)\nnp.testing.assert_allclose(\n    cv_res['NHITS'].values,\n    cv_res['y'].values,\n    rtol=0.3,\n)\n# with exog\nnf = NeuralForecast(models=models_exog, freq='M', local_scaler_type='robust-iqr')\ncv_res_exog = nf.cross_validation(AirPassengersPanel)\n# check that the forecasts are similar to the original values (originals are restored directly from the df)\nnp.testing.assert_allclose(\n    cv_res_exog['NHITS'].values,\n    cv_res_exog['y'].values,\n    rtol=0.2,\n)\n\n# fit+predict_insample\nnf = NeuralForecast(models=models, freq='M', local_scaler_type='minmax')\nnf.fit(AirPassengersPanel_train)\ninsample_res = (\n    nf.predict_insample()\n    .groupby('unique_id').tail(-12) # first values aren't reliable\n    .merge(\n        AirPassengersPanel_train[['unique_id', 'ds', 'y']],\n        on=['unique_id', 'ds'],\n        how='left',\n        suffixes=('_actual', '_expected'),\n    )\n)\n# y is inverted correctly\nnp.testing.assert_allclose(\n    insample_res['y_actual'].values,\n    insample_res['y_expected'].values,\n    rtol=1e-5,\n)\n# predictions are in the same scale\nnp.testing.assert_allclose(\n    insample_res['NHITS'].values,\n    insample_res['y_expected'].values,\n    rtol=0.7,\n)\n# with exog\nnf = NeuralForecast(models=models_exog, freq='M', local_scaler_type='minmax')\nnf.fit(AirPassengersPanel_train)\ninsample_res_exog = (\n    nf.predict_insample()\n    .groupby('unique_id').tail(-12) # first values aren't reliable\n    .merge(\n        AirPassengersPanel_train[['unique_id', 'ds', 'y']],\n        on=['unique_id', 'ds'],\n        how='left',\n        suffixes=('_actual', '_expected'),\n    )\n)\n# y is inverted correctly\nnp.testing.assert_allclose(\n    insample_res_exog['y_actual'].values,\n    insample_res_exog['y_expected'].values,\n    rtol=1e-5,\n)\n# predictions are similar than without exog\nnp.testing.assert_allclose(\n    insample_res['NHITS'].values,\n    insample_res_exog['NHITS'].values,\n    rtol=0.2,\n)\n\n# test boxcox\nnf = NeuralForecast(models=models, freq='M', local_scaler_type='boxcox')\nnf.fit(AirPassengersPanel_train)\ninsample_res = (\n    nf.predict_insample()\n    .groupby('unique_id').tail(-12) # first values aren't reliable\n    .merge(\n        AirPassengersPanel_train[['unique_id', 'ds', 'y']],\n        on=['unique_id', 'ds'],\n        how='left',\n        suffixes=('_actual', '_expected'),\n    )\n)\n# y is inverted correctly\nnp.testing.assert_allclose(\n    insample_res['y_actual'].values,\n    insample_res['y_expected'].values,\n    rtol=1e-5,\n)\n# predictions are in the same scale\nnp.testing.assert_allclose(\n    insample_res['NHITS'].values,\n    insample_res['y_expected'].values,\n    rtol=0.7,\n)\n```\n\n----------------------------------------\n\nTITLE: Markdown Tables for Loss Functions Documentation\nDESCRIPTION: Structured tables documenting the various loss functions available in NeuralForecast, organized by categories including point losses (scale-dependent, percentage-errors, scale-independent, robust) and probabilistic losses (parametric and non-parametric).\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/02_objectives.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| Scale-Dependent                                              | Percentage-Errors                                                     | Scale-Independent                                              | Robust                                                 |\n|:-------------------------------------------------------------|:----------------------------------------------------------------------|:---------------------------------------------------------------|:-------------------------------------------------------|\n|[**MAE**](../../losses.pytorch.html#mean-absolute-error-mae)       |[**MAPE**](../../losses.pytorch.html#mean-absolute-percentage-error-mape)   |[**MASE**](../../losses.pytorch.html#mean-absolute-scaled-error-mase)|[**Huber**](../losses.pytorch.html#huber-loss)            |\n|[**MSE**](../../losses.pytorch.html#mean-squared-error-mse)        |[**sMAPE**](../../losses.pytorch.html#symmetric-mape-smape)                 |                                                                |[**Tukey**](../../losses.pytorch.html#tukey-loss)            |\n|[**RMSE**](../../losses.pytorch.html#root-mean-squared-error-rmse) |                                                                       |                                                                |[**HuberMQLoss**](../../losses.pytorch.html#huberized-mqloss)|\n\n|Parametric Probabilities                                      | Non-Parametric Probabilities                                 |\n|:-------------------------------------------------------------|:-------------------------------------------------------------|\n|[**Normal**](../../losses.pytorch.html#distributionloss)           |[**QuantileLoss**](../../losses.pytorch.html#quantile-loss)        |\n|[**StudenT**](../../losses.pytorch.html#distributionloss)          |[**MQLoss**](../../losses.pytorch.html#multi-quantile-loss-mqloss) |\n|[**Poisson**](../../losses.pytorch.html#distributionloss)          |[**HuberQLoss**](../../losses.pytorch.html#huberized-quantile-loss)|\n|[**Negative Binomial**](../../losses.pytorch.html#distributionloss)|[**HuberMQLoss**](../../losses.pytorch.html#huberized-mqloss)      |\n|[**Tweedie**](../../losses.pytorch.html#distributionloss)          |[**IQLoss**](../../losses.pytorch.html#iqloss)  |\n|[**PMM**](../../losses.pytorch.html#poisson-mixture-mesh-pmm) | [**HuberIQLoss**](../../losses.pytorch.html#huberized-iqloss)|\n|[**GMM**](../../losses.pytorch.html#gaussian-mixture-mesh-gmm) | [**ISQF**](../../losses.pytorch.html#isqf)  |\n|[**NBMM**](../../losses.pytorch.html#negative-binomial-mixture-mesh-nbmm) | |\n```\n\n----------------------------------------\n\nTITLE: Initializing and Fitting AutoRMoK Model in Python\nDESCRIPTION: This snippet demonstrates how to initialize an AutoRMoK model with custom configuration and fit it to a dataset. It includes examples for both Ray and Optuna backends, and shows how to make predictions using the fitted model.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_114\n\nLANGUAGE: python\nCODE:\n```\n# Use your own config or AutoRMoK.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12, learning_rate=1e-2)\nmodel = AutoRMoK(h=12, n_series=1, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n# Optuna\nmodel = AutoRMoK(h=12, n_series=1, config=None, backend='optuna')\n```\n\n----------------------------------------\n\nTITLE: Implementing TaylorKAN Layer for RMoK\nDESCRIPTION: Defines the TaylorKANLayer class, which implements the Taylor Kolmogorov-Arnold Network layer for the RMoK model. It uses Taylor series expansion for approximation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.rmok.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass TaylorKANLayer(nn.Module):\n    \"\"\"\n    https://github.com/Muyuzhierchengse/TaylorKAN/\n    \"\"\"\n\n    def __init__(self, input_dim, out_dim, order, addbias=True):\n        super(TaylorKANLayer, self).__init__()\n        self.input_dim = input_dim\n        self.out_dim = out_dim\n        self.order = order\n        self.addbias = addbias\n\n        self.coeffs = nn.Parameter(torch.randn(out_dim, input_dim, order) * 0.01)\n        if self.addbias:\n            self.bias = nn.Parameter(torch.zeros(1, out_dim))\n\n    def forward(self, x):\n        shape = x.shape\n        outshape = shape[0:-1] + (self.out_dim,)\n        x = torch.reshape(x, (-1, self.input_dim))\n        x_expanded = x.unsqueeze(1).expand(-1, self.out_dim, -1)\n\n        y = torch.zeros((x.shape[0], self.out_dim), device=x.device)\n\n        for i in range(self.order):\n            term = (x_expanded ** i) * self.coeffs[:, :, i]\n            y += term.sum(dim=-1)\n\n        if self.addbias:\n            y += self.bias\n\n        y = torch.reshape(y, outshape)\n        return y\n```\n\n----------------------------------------\n\nTITLE: Implementing AutoDeepNPTS Class\nDESCRIPTION: Implementation of the AutoDeepNPTS class that extends BaseAuto for hyperparameter tuning of DeepNPTS models. It defines default configuration with tunable parameters and methods for initialization and configuration generation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_58\n\nLANGUAGE: python\nCODE:\n```\nclass AutoDeepNPTS(BaseAuto):\n\n    default_config = {\n       \"input_size_multiplier\": [1, 2, 3, 4, 5],\n       \"h\": None,\n       \"hidden_size\": tune.choice([16, 32, 64]),\n       \"dropout\": tune.uniform(0.0, 0.99),\n       \"n_layers\": tune.choice([1, 2, 4]),\n       \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n       \"scaler_type\": tune.choice([None, 'robust', 'standard']),\n       \"max_steps\": tune.quniform(lower=500, upper=1500, q=100),\n       \"batch_size\": tune.choice([32, 64, 128, 256]),\n       \"windows_batch_size\": tune.choice([128, 256, 512, 1024]),\n       \"loss\": None,\n       \"random_seed\": tune.randint(lower=1, upper=20),\n    }\n\n    def __init__(self,\n                 h,\n                 loss=MAE(),\n                 valid_loss=None,\n                 config=None, \n                 search_alg=BasicVariantGenerator(random_state=1),\n                 num_samples=10,\n                 refit_with_val=False,\n                 cpus=cpu_count(),\n                 gpus=torch.cuda.device_count(),\n                 verbose=False,\n                 alias=None,\n                 backend='ray',\n                 callbacks=None,\n                ):\n\n        # Define search space, input/output sizes\n        if config is None:\n            config = self.get_default_config(h=h, backend=backend)                 \n\n        super(AutoDeepNPTS, self).__init__(\n              cls_model=DeepNPTS, \n              h=h,\n              loss=loss,\n              valid_loss=valid_loss,\n              config=config,\n              search_alg=search_alg,\n              num_samples=num_samples,\n              refit_with_val=refit_with_val,\n              cpus=cpus,\n              gpus=gpus,\n              verbose=verbose,\n              alias=alias,\n              backend=backend,\n              callbacks=callbacks,\n        )\n\n    @classmethod\n    def get_default_config(cls, h, backend, n_series=None):\n        config = cls.default_config.copy()\n        config['input_size'] = tune.choice([h*x \\\n                        for x in config['input_size_multiplier']])\n        config['step_size'] = tune.choice([1, h])        \n        del config['input_size_multiplier']\n        if backend == 'optuna':\n            config = cls._ray_config_to_optuna(config)         \n\n        return config\n```\n\n----------------------------------------\n\nTITLE: Implementing BiTCN Class for Time Series Forecasting in Python\nDESCRIPTION: This code defines the BiTCN class, a bidirectional temporal convolutional network for time series forecasting. It includes class attributes, initialization parameters, and network architecture setup.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.bitcn.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass BiTCN(BaseModel):\n    \"\"\" BiTCN\n\n    Bidirectional Temporal Convolutional Network (BiTCN) is a forecasting architecture based on two temporal convolutional networks (TCNs). The first network ('forward') encodes future covariates of the time series, whereas the second network ('backward') encodes past observations and covariates. This is a univariate model.\n\n    **Parameters:**<br>\n    `h`: int, forecast horizon.<br>\n    `input_size`: int, considered autorregresive inputs (lags), y=[1,2,3,4] input_size=2 -> lags=[1,2].<br>\n    `hidden_size`: int=16, units for the TCN's hidden state size.<br>\n    `dropout`: float=0.1, dropout rate used for the dropout layers throughout the architecture.<br>\n    `futr_exog_list`: str list, future exogenous columns.<br>\n    `hist_exog_list`: str list, historic exogenous columns.<br>\n    `stat_exog_list`: str list, static exogenous columns.<br>\n    `exclude_insample_y`: bool=False, the model skips the autoregressive features y[t-input_size:t] if True.<br>\n    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `max_steps`: int=1000, maximum number of training steps.<br>\n    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n    `batch_size`: int=32, number of different series in each batch.<br>\n    `valid_batch_size`: int=None, number of different series in each validation and test batch, if None uses batch_size.<br>\n    `windows_batch_size`: int=1024, number of windows to sample in each training batch, default uses all.<br>\n    `inference_windows_batch_size`: int=1024, number of windows to sample in each inference batch, -1 uses all.<br>\n    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n    `step_size`: int=1, step size between each window of temporal data.<br>\n    `scaler_type`: str='identity', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n    `random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n    `alias`: str, optional,  Custom name of the model.<br>\n    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>\n    `dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n\n    **References**<br>  \n    - [Olivier Sprangers, Sebastian Schelter, Maarten de Rijke (2023). Parameter-Efficient Deep Probabilistic Forecasting. International Journal of Forecasting 39, no. 1 (1 January 2023): 33245. URL: https://doi.org/10.1016/j.ijforecast.2021.11.011.](https://doi.org/10.1016/j.ijforecast.2021.11.011)<br>    \n\n    \"\"\"\n    # Class attributes\n    EXOGENOUS_FUTR = True\n    EXOGENOUS_HIST = True\n    EXOGENOUS_STAT = True\n    MULTIVARIATE = False    # If the model produces multivariate forecasts (True) or univariate (False)\n    RECURRENT = False       # If the model produces forecasts recursively (True) or direct (False)\n\n    def __init__(self,\n                 h: int,\n                 input_size: int,\n                 hidden_size: int = 16,\n                 dropout: float = 0.5,\n                 futr_exog_list = None,\n                 hist_exog_list = None,\n                 stat_exog_list = None,\n                 exclude_insample_y = False,\n                 loss = MAE(),\n                 valid_loss = None,\n                 max_steps: int = 1000,\n                 learning_rate: float = 1e-3,\n                 num_lr_decays: int = -1,\n                 early_stop_patience_steps: int =-1,\n                 val_check_steps: int = 100,\n                 batch_size: int = 32,\n                 valid_batch_size: Optional[int] = None,\n                 windows_batch_size = 1024,\n                 inference_windows_batch_size = 1024,\n                 start_padding_enabled = False,\n                 step_size: int = 1,\n                 scaler_type: str = 'identity',\n                 random_seed: int = 1,\n                 drop_last_loader: bool = False,\n                 alias: Optional[str] = None,\n                 optimizer = None,\n                 optimizer_kwargs = None,\n                 lr_scheduler = None,\n                 lr_scheduler_kwargs = None,\n                 dataloader_kwargs=None,\n                 **trainer_kwargs):\n        super(BiTCN, self).__init__(\n            h=h,\n            input_size=input_size,\n            futr_exog_list=futr_exog_list,\n            hist_exog_list=hist_exog_list,\n            stat_exog_list=stat_exog_list,\n            exclude_insample_y = exclude_insample_y,\n            loss=loss,\n            valid_loss=valid_loss,\n            max_steps=max_steps,\n            learning_rate=learning_rate,\n            num_lr_decays=num_lr_decays,\n            early_stop_patience_steps=early_stop_patience_steps,\n            val_check_steps=val_check_steps,\n            batch_size=batch_size,\n            valid_batch_size=valid_batch_size,\n            windows_batch_size=windows_batch_size,\n            inference_windows_batch_size=inference_windows_batch_size,\n            start_padding_enabled=start_padding_enabled,\n            step_size=step_size,\n            scaler_type=scaler_type,\n            random_seed=random_seed,\n            drop_last_loader=drop_last_loader,\n            alias=alias,\n            optimizer=optimizer,\n            optimizer_kwargs=optimizer_kwargs,\n            lr_scheduler=lr_scheduler,\n            lr_scheduler_kwargs=lr_scheduler_kwargs,\n            dataloader_kwargs=dataloader_kwargs,\n            **trainer_kwargs\n        )\n\n        #----------------------------------- Parse dimensions -----------------------------------#\n        # TCN\n        kernel_size = 2  # Not really necessary as parameter, so simplifying the architecture here.\n        self.kernel_size = kernel_size\n        self.hidden_size = hidden_size\n        self.h = h\n        self.input_size = input_size\n        self.dropout = dropout\n        \n        # Calculate required number of TCN layers based on the required receptive field of the TCN\n        self.n_layers_bwd = int(np.ceil(np.log2(((self.input_size - 1) / (self.kernel_size - 1)) + 1)))     \n       \n        #---------------------------------- Instantiate Model -----------------------------------#\n        \n        # Dense layers\n        self.lin_hist = nn.Linear(1 + self.hist_exog_size + self.stat_exog_size + self.futr_exog_size, hidden_size)\n        self.drop_hist = nn.Dropout(dropout)\n        \n        # TCN looking back\n        layers_bwd = [TCNCell(\n                        hidden_size, \n                        hidden_size, \n                        kernel_size, \n                        padding = (kernel_size-1)*2**i, \n                        dilation = 2**i, \n                        mode = 'backward', \n                        groups = 1, \n                        dropout = dropout) for i in range(self.n_layers_bwd)]      \n        self.net_bwd = nn.Sequential(*layers_bwd)\n        \n        # TCN looking forward when future covariates exist\n        output_lin_dim_multiplier = 1\n        if self.futr_exog_size > 0:\n            self.n_layers_fwd = int(np.ceil(np.log2(((self.h + self.input_size - 1) / (self.kernel_size - 1)) + 1)))\n            self.lin_futr = nn.Linear(self.futr_exog_size, hidden_size)\n            self.drop_futr = nn.Dropout(dropout)\n            layers_fwd = [TCNCell(\n                            hidden_size, \n                            hidden_size, \n                            kernel_size, \n                            padding = (kernel_size - 1)*2**i, \n                            dilation = 2**i, \n                            mode = 'forward', \n                            groups = 1, \n                            dropout = dropout) for i in range(self.n_layers_fwd)]             \n            self.net_fwd = nn.Sequential(*layers_fwd)\n            output_lin_dim_multiplier += 2\n\n        # Dense temporal and output layers\n        self.drop_temporal = nn.Dropout(dropout)\n        self.temporal_lin1 = nn.Linear(self.input_size, hidden_size)\n        self.temporal_lin2 = nn.Linear(hidden_size, self.h)\n        self.output_lin = nn.Linear(output_lin_dim_multiplier * hidden_size, self.loss.outputsize_multiplier)\n```\n\n----------------------------------------\n\nTITLE: Calculating Multi-Quantile Loss for Probabilistic Forecasting in Python\nDESCRIPTION: This function computes the Multi-Quantile Loss (MQL) between actual and predicted values for a set of quantiles. It's useful for evaluating probabilistic forecasts and can approximate the Continuous Ranked Probability Score (CRPS) through numerical integration.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.numpy.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef mqloss(y: np.ndarray, y_hat: np.ndarray, \n           quantiles: np.ndarray, \n           weights: Optional[np.ndarray] = None,\n           axis: Optional[int] = None) -> Union[float, np.ndarray]:\n    \"\"\"  Multi-Quantile loss\n\n    Calculates the Multi-Quantile loss (MQL) between `y` and `y_hat`.\n    MQL calculates the average multi-quantile Loss for\n    a given set of quantiles, based on the absolute \n    difference between predicted quantiles and observed values.\n\n    $$ \\mathrm{MQL}(\\mathbf{y}_{\\tau},[\\mathbf{\\hat{y}}^{(q_{1})}_{\\tau}, ... ,\\hat{y}^{(q_{n})}_{\\tau}]) = \\frac{1}{n} \\sum_{q_{i}} \\mathrm{QL}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q_{i})}_{\\tau}) $$\n\n    The limit behavior of MQL allows to measure the accuracy \n    of a full predictive distribution $\\mathbf{\\hat{F}}_{\\tau}$ with \n    the continuous ranked probability score (CRPS). This can be achieved \n    through a numerical integration technique, that discretizes the quantiles \n    and treats the CRPS integral with a left Riemann approximation, averaging over \n    uniformly distanced quantiles.    \n\n    $$ \\mathrm{CRPS}(y_{\\tau}, \\mathbf{\\hat{F}}_{\\tau}) = \\int^{1}_{0} \\mathrm{QL}(y_{\\tau}, \\hat{y}^{(q)}_{\\tau}) dq $$\n\n    **Parameters:**<br>\n    `y`: numpy array, Actual values.<br>\n    `y_hat`: numpy array, Predicted values.<br>\n    `quantiles`: numpy array,(n_quantiles). Quantiles to estimate from the distribution of y.<br>\n    `mask`: numpy array, Specifies date stamps per serie to consider in loss.<br>\n\n    **Returns:**<br>\n    `mqloss`: numpy array, (single value).\n    \n    **References:**<br>\n    [Roger Koenker and Gilbert Bassett, Jr., \"Regression Quantiles\".](https://www.jstor.org/stable/1913643)<br>\n    [James E. Matheson and Robert L. Winkler, \"Scoring Rules for Continuous Probability Distributions\".](https://www.jstor.org/stable/2629907)\n    \"\"\"\n    if weights is None: weights = np.ones(y.shape)\n        \n    _metric_protections(y, y_hat, weights)\n    n_q = len(quantiles)\n    \n    y_rep  = np.expand_dims(y, axis=-1)\n    error  = y_hat - y_rep\n    sq     = np.maximum(-error, np.zeros_like(error))\n    s1_q   = np.maximum(error, np.zeros_like(error))\n    mqloss = (quantiles * sq + (1 - quantiles) * s1_q)\n    \n    # Match y/weights dimensions and compute weighted average\n    weights = np.repeat(np.expand_dims(weights, axis=-1), repeats=n_q, axis=-1)\n    mqloss  = np.average(mqloss, weights=weights, axis=axis)\n\n    return mqloss\n```\n\n----------------------------------------\n\nTITLE: Implementing Quantile Loss for Probabilistic Forecasting in PyTorch\nDESCRIPTION: A PyTorch loss function class that implements Quantile Loss for probabilistic forecasting. It measures the deviation of quantile forecasts by applying asymmetric weighting to over/under predictions. Supports customizable quantile levels and horizon weighting.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass QuantileLoss(BasePointLoss):\n    def __init__(self, q, horizon_weight=None):\n        super(QuantileLoss, self).__init__(horizon_weight=horizon_weight,\n                                           outputsize_multiplier=1,\n                                           output_names=[f'_ql{q}'])\n        self.q = q\n\n    def __call__(self,\n                 y: torch.Tensor,\n                 y_hat: torch.Tensor,\n                 y_insample: torch.Tensor,\n                 mask: Union[torch.Tensor, None] = None,\n                 ) -> torch.Tensor:\n        delta_y = y - y_hat\n        losses = torch.max(torch.mul(self.q, delta_y), torch.mul((self.q - 1), delta_y))\n        weights = self._compute_weights(y=y, mask=mask)\n        return _weighted_mean(losses=losses, weights=weights)\n```\n\n----------------------------------------\n\nTITLE: Testing AutoRNN Model with Manual and Default Configurations\nDESCRIPTION: Demonstrates how to use the AutoRNN class with both custom and default configurations, including examples of training, prediction, and using different hyperparameter tuning backends.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n# Use your own config or AutoRNN.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=-1, encoder_hidden_size=8)\nmodel = AutoRNN(h=12, config=config, num_samples=1, cpus=1)\n\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n# Optuna\nmodel = AutoRNN(h=12, config=None, num_samples=1, cpus=1, backend='optuna')\n```\n\n----------------------------------------\n\nTITLE: Implementing FFT and TimesBlock for TimesNet\nDESCRIPTION: Defines the FFT_for_Period function for finding periods using Fast Fourier Transform, and the TimesBlock class which is the core component of the TimesNet model. It applies 2D convolutions on reshaped time series data.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timesnet.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef FFT_for_Period(x, k=2):\n    # [B, T, C]\n    xf = torch.fft.rfft(x, dim=1)\n    # find period by amplitudes\n    frequency_list = abs(xf).mean(0).mean(-1)\n    frequency_list[0] = 0\n    _, top_list = torch.topk(frequency_list, k)\n    top_list = top_list.detach().cpu().numpy()\n    period = x.shape[1] // top_list\n    return period, abs(xf).mean(-1)[:, top_list]\n\nclass TimesBlock(nn.Module):\n    \"\"\"\n    TimesBlock\n    \"\"\"       \n    def __init__(self, input_size, h, k, hidden_size, conv_hidden_size, num_kernels):\n        super(TimesBlock, self).__init__()\n        self.input_size = input_size\n        self.h = h\n        self.k = k\n        # parameter-efficient design\n        self.conv = nn.Sequential(\n            Inception_Block_V1(hidden_size, conv_hidden_size,\n                               num_kernels=num_kernels),\n            nn.GELU(),\n            Inception_Block_V1(conv_hidden_size, hidden_size,\n                               num_kernels=num_kernels)\n        )\n\n    def forward(self, x):\n        B, T, N = x.size()\n        period_list, period_weight = FFT_for_Period(x, self.k)\n\n        res = []\n        for i in range(self.k):\n            period = period_list[i]\n            # padding\n            if (self.input_size + self.h) % period != 0:\n                length = (\n                                 ((self.input_size + self.h) // period) + 1) * period\n                padding = torch.zeros([x.shape[0], (length - (self.input_size + self.h)), x.shape[2]], device=x.device)\n                out = torch.cat([x, padding], dim=1)\n            else:\n                length = (self.input_size + self.h)\n                out = x\n            # reshape\n            out = out.reshape(B, length // period, period,\n                              N).permute(0, 3, 1, 2).contiguous()\n            # 2D conv: from 1d Variation to 2d Variation\n            out = self.conv(out)\n            # reshape back\n            out = out.permute(0, 2, 3, 1).reshape(B, -1, N)\n            res.append(out[:, :(self.input_size + self.h), :])\n        res = torch.stack(res, dim=-1)\n        # adaptive aggregation\n        period_weight = F.softmax(period_weight, dim=1)\n        period_weight = period_weight.unsqueeze(\n            1).unsqueeze(1).repeat(1, T, N, 1)\n        res = torch.sum(res * period_weight, -1)\n        # residual connection\n        res = res + x\n        return res\n```\n\n----------------------------------------\n\nTITLE: Forward Method for TimeLLM Model Training\nDESCRIPTION: Defines the forward pass of the TimeLLM model during training. Extracts in-sample data from window batches, processes them through the forecast function, and returns predictions for the specified horizon.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timellm.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, windows_batch):\n    x = windows_batch['insample_y']\n\n    y_pred = self.forecast(x)\n    y_pred = y_pred[:, -self.h:, :]\n    \n    return y_pred\n```\n\n----------------------------------------\n\nTITLE: Padding and Preparing Inputs for DilatedRNN\nDESCRIPTION: These methods handle input padding and preparation for the dilated RNN. _pad_inputs adds padding to ensure even division by the dilation rate, while _prepare_inputs rearranges the input for dilated processing.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.dilated_rnn.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\ndef _pad_inputs(self, inputs, n_steps, rate):\n    iseven = (n_steps % rate) == 0\n\n    if not iseven:\n        dilated_steps = n_steps // rate + 1\n\n        zeros_ = torch.zeros(dilated_steps * rate - inputs.size(0),\n                             inputs.size(1),\n                             inputs.size(2), \n                             dtype=inputs.dtype,\n                             device=inputs.device)\n        inputs = torch.cat((inputs, zeros_))\n    else:\n        dilated_steps = n_steps // rate\n\n    return inputs, dilated_steps\n\ndef _prepare_inputs(self, inputs, rate):\n    dilated_inputs = torch.cat([inputs[j::rate, :, :] for j in range(rate)], 1)\n    return dilated_inputs\n```\n\n----------------------------------------\n\nTITLE: Training NHITS Models with Different Loss Functions\nDESCRIPTION: Initializes and trains two NHITS models - one with MAE loss for conformal prediction and another with Normal distribution loss for probabilistic forecasting.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/20_conformal_prediction.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nhorizon = 12\ninput_size = 24\n\nprediction_intervals = PredictionIntervals()\n\nmodels = [NHITS(h=horizon, input_size=input_size, max_steps=100, loss=MAE(), scaler_type=\"robust\"), \n          NHITS(h=horizon, input_size=input_size, max_steps=100, loss=DistributionLoss(\"Normal\", level=[90]), scaler_type=\"robust\")]\nnf = NeuralForecast(models=models, freq='ME')\nnf.fit(AirPassengersPanel_train, prediction_intervals=prediction_intervals)\n```\n\n----------------------------------------\n\nTITLE: Implementing Positional Encodings for PatchTST\nDESCRIPTION: Defines various positional encoding methods including sinusoidal, coordinate-based (1D and 2D), and learnable encodings. These are crucial for providing position information to the Transformer model.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.patchtst.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef PositionalEncoding(q_len, hidden_size, normalize=True):\n    pe = torch.zeros(q_len, hidden_size)\n    position = torch.arange(0, q_len).unsqueeze(1)\n    div_term = torch.exp(torch.arange(0, hidden_size, 2) * -(math.log(10000.0) / hidden_size))\n    pe[:, 0::2] = torch.sin(position * div_term)\n    pe[:, 1::2] = torch.cos(position * div_term)\n    if normalize:\n        pe = pe - pe.mean()\n        pe = pe / (pe.std() * 10)\n    return pe\n\nSinCosPosEncoding = PositionalEncoding\n\ndef Coord2dPosEncoding(q_len, hidden_size, exponential=False, normalize=True, eps=1e-3):\n    x = .5 if exponential else 1\n    i = 0\n    for i in range(100):\n        cpe = 2 * (torch.linspace(0, 1, q_len).reshape(-1, 1) ** x) * (torch.linspace(0, 1, hidden_size).reshape(1, -1) ** x) - 1\n        if abs(cpe.mean()) <= eps: break\n        elif cpe.mean() > eps: x += .001\n        else: x -= .001\n        i += 1\n    if normalize:\n        cpe = cpe - cpe.mean()\n        cpe = cpe / (cpe.std() * 10)\n    return cpe\n\ndef Coord1dPosEncoding(q_len, exponential=False, normalize=True):\n    cpe = (2 * (torch.linspace(0, 1, q_len).reshape(-1, 1)**(.5 if exponential else 1)) - 1)\n    if normalize:\n        cpe = cpe - cpe.mean()\n        cpe = cpe / (cpe.std() * 10)\n    return cpe\n\ndef positional_encoding(pe, learn_pe, q_len, hidden_size):\n    # Positional encoding\n    if pe == None:\n        W_pos = torch.empty((q_len, hidden_size)) # pe = None and learn_pe = False can be used to measure impact of pe\n        nn.init.uniform_(W_pos, -0.02, 0.02)\n        learn_pe = False\n    elif pe == 'zero':\n        W_pos = torch.empty((q_len, 1))\n        nn.init.uniform_(W_pos, -0.02, 0.02)\n    elif pe == 'zeros':\n        W_pos = torch.empty((q_len, hidden_size))\n        nn.init.uniform_(W_pos, -0.02, 0.02)\n    elif pe == 'normal' or pe == 'gauss':\n        W_pos = torch.zeros((q_len, 1))\n        torch.nn.init.normal_(W_pos, mean=0.0, std=0.1)\n    elif pe == 'uniform':\n        W_pos = torch.zeros((q_len, 1))\n        nn.init.uniform_(W_pos, a=0.0, b=0.1)\n    elif pe == 'lin1d': W_pos = Coord1dPosEncoding(q_len, exponential=False, normalize=True)\n    elif pe == 'exp1d': W_pos = Coord1dPosEncoding(q_len, exponential=True, normalize=True)\n    elif pe == 'lin2d': W_pos = Coord2dPosEncoding(q_len, hidden_size, exponential=False, normalize=True)\n    elif pe == 'exp2d': W_pos = Coord2dPosEncoding(q_len, hidden_size, exponential=True, normalize=True)\n    elif pe == 'sincos': W_pos = PositionalEncoding(q_len, hidden_size, normalize=True)\n    else: raise ValueError(f\"{pe} is not a valid pe (positional encoder. Available types: 'gauss'=='normal', \\\n        'zeros', 'zero', uniform', 'lin1d', 'exp1d', 'lin2d', 'exp2d', 'sincos', None.)\")\n    return nn.Parameter(W_pos, requires_grad=learn_pe)\n```\n\n----------------------------------------\n\nTITLE: Implementing Auxiliary Functions for PatchTST\nDESCRIPTION: Defines auxiliary functions including a Transpose module and an activation function getter. These are used as building blocks in the PatchTST model architecture.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.patchtst.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Transpose(nn.Module):\n    \"\"\"\n    Transpose\n    \"\"\"\n    def __init__(self, *dims, contiguous=False): \n        super().__init__()\n        self.dims, self.contiguous = dims, contiguous\n    def forward(self, x):\n        if self.contiguous: return x.transpose(*self.dims).contiguous()\n        else: return x.transpose(*self.dims)\n\ndef get_activation_fn(activation):\n    if callable(activation): return activation()\n    elif activation.lower() == \"relu\": return nn.ReLU()\n    elif activation.lower() == \"gelu\": return nn.GELU()\n    raise ValueError(f'{activation} is not available. You can use \"relu\", \"gelu\", or a callable')\n```\n\n----------------------------------------\n\nTITLE: Tukey Loss Implementation in PyTorch\nDESCRIPTION: Implementation of Tukey's biweight loss function for robust statistics, providing constant loss for large residuals and enhanced outlier resistance compared to Huber loss.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_67\n\nLANGUAGE: python\nCODE:\n```\nclass TukeyLoss(BasePointLoss):\n    def __init__(self, c: float=4.685, normalize: bool=True):\n        super(TukeyLoss, self).__init__()\n        self.outputsize_multiplier = 1\n        self.c = c\n        self.normalize = normalize\n        self.output_names = ['']\n        self.is_distribution_output = False\n\n    def domain_map(self, y_hat: torch.Tensor):\n        return y_hat\n\n    def masked_mean(self, x, mask, dim):\n        x_nan = x.masked_fill(mask < 1, float(\"nan\"))\n        x_mean = x_nan.nanmean(dim=dim, keepdim=True)\n        x_mean = torch.nan_to_num(x_mean, nan=0.0)\n        return x_mean\n\n    def __call__(self,\n                 y: torch.Tensor,\n                 y_hat: torch.Tensor,\n                 y_insample: torch.Tensor,\n                 mask: Union[torch.Tensor, None] = None,\n                 ) -> torch.Tensor:\n        if mask is None:\n            mask = torch.ones_like(y_hat)\n        if self.normalize:\n            y_mean = self.masked_mean(x=y, mask=mask, dim=-1)\n            y_std = torch.sqrt(self.masked_mean(x=(y - y_mean) ** 2, mask=mask, dim=-1)) + 1e-2\n        else:\n            y_std = 1.\n        delta_y = torch.abs(y - y_hat) / y_std\n        tukey_mask = torch.greater_equal(self.c * torch.ones_like(delta_y), delta_y)\n        tukey_loss = tukey_mask * mask * (1-(delta_y/(self.c))**2)**3 + (1-(tukey_mask * 1))\n        tukey_loss = (self.c**2 / 6) * torch.mean(tukey_loss)\n        return tukey_loss\n```\n\n----------------------------------------\n\nTITLE: Preparing Time Series Dataset for Training\nDESCRIPTION: Splits the AirPassengers data into training and testing sets and converts the training data into a TimeSeriesDataset object for model training.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n# Split train/test and declare time series dataset\nY_train_df = Y_df[Y_df.ds<='1959-12-31'] # 132 train\nY_test_df = Y_df[Y_df.ds>'1959-12-31']   # 12 test\ndataset, *_ = TimeSeriesDataset.from_df(Y_train_df)\n```\n\n----------------------------------------\n\nTITLE: Implementing AutoSOFTS Class for Automated SOFTS-based Forecasting in Python\nDESCRIPTION: This class implements an automated machine learning model for time series forecasting using SOFTS as the base model. It supports both Ray and Optuna backends for hyperparameter tuning and includes methods for configuration and initialization.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_109\n\nLANGUAGE: python\nCODE:\n```\nclass AutoSOFTS(BaseAuto):\n\n    default_config = {\n        \"input_size_multiplier\": [1, 2, 3, 4, 5],\n        \"h\": None,\n        \"n_series\": None,\n        \"hidden_size\": tune.choice([64, 128, 256, 512]),\n        \"d_core\": tune.choice([64, 128, 256, 512]),\n        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n        \"scaler_type\": tune.choice([None, 'robust', 'standard', 'identity']),\n        \"max_steps\": tune.choice([500, 1000, 2000]),\n        \"batch_size\": tune.choice([32, 64, 128, 256]),\n        \"loss\": None,\n        \"random_seed\": tune.randint(1, 20),\n    }\n\n    def __init__(self,\n                 h,\n                 n_series,\n                 loss=MAE(),\n                 valid_loss=None,\n                 config=None, \n                 search_alg=BasicVariantGenerator(random_state=1),\n                 num_samples=10,\n                 refit_with_val=False,\n                 cpus=cpu_count(),\n                 gpus=torch.cuda.device_count(),\n                 verbose=False,\n                 alias=None,\n                 backend='ray',\n                 callbacks=None):\n        \n        # Define search space, input/output sizes\n        if config is None:\n            config = self.get_default_config(h=h, backend=backend, n_series=n_series)                 \n\n        # Always use n_series from parameters, raise exception with Optuna because we can't enforce it\n        if backend == 'ray':\n            config['n_series'] = n_series\n        elif backend == 'optuna':\n            mock_trial = MockTrial()\n            if ('n_series' in config(mock_trial) and config(mock_trial)['n_series'] != n_series) or ('n_series' not in config(mock_trial)):\n                raise Exception(f\"config needs 'n_series': {n_series}\")           \n\n        super(AutoSOFTS, self).__init__(\n              cls_model=SOFTS, \n              h=h,\n              loss=loss,\n              valid_loss=valid_loss,\n              config=config,\n              search_alg=search_alg,\n              num_samples=num_samples, \n              refit_with_val=refit_with_val,\n              cpus=cpus,\n              gpus=gpus,\n              verbose=verbose,\n              alias=alias,\n              backend=backend,\n              callbacks=callbacks,            \n        )\n\n    @classmethod\n    def get_default_config(cls, h, backend, n_series):\n        config = cls.default_config.copy()        \n        config['input_size'] = tune.choice([h * x \\\n                        for x in config[\"input_size_multiplier\"]])\n\n        # Rolling windows with step_size=1 or step_size=h\n        # See `BaseWindows` and `BaseRNN`'s create_windows\n        config['step_size'] = tune.choice([1, h])\n        del config[\"input_size_multiplier\"]\n        if backend == 'optuna':\n            # Always use n_series from parameters\n            config['n_series'] = n_series\n            config = cls._ray_config_to_optuna(config)           \n\n        return config         \n```\n\n----------------------------------------\n\nTITLE: Implementing MLPResidual Block for TiDE Model in Python\nDESCRIPTION: Defines the MLPResidual class, which is an MLP block with a residual connection used in the TiDE model. It includes layer normalization, dropout, and skip connections.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tide.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass MLPResidual(nn.Module):\n    \"\"\"\n    MLPResidual\n    \"\"\"   \n    def __init__(self, input_dim, hidden_size, output_dim, dropout, layernorm):\n        super().__init__()\n        self.layernorm = layernorm\n        if layernorm:\n            self.norm = nn.LayerNorm(output_dim)\n\n        self.drop = nn.Dropout(dropout)\n        self.lin1 = nn.Linear(input_dim, hidden_size)\n        self.lin2 = nn.Linear(hidden_size, output_dim)\n        self.skip = nn.Linear(input_dim, output_dim)\n\n    def forward(self, input):\n        # MLP dense\n        x = F.relu(self.lin1(input))                                            \n        x = self.lin2(x)\n        x = self.drop(x)\n\n        # Skip connection\n        x_skip = self.skip(input)\n\n        # Combine\n        x = x + x_skip\n\n        if self.layernorm:\n            return self.norm(x)\n\n        return x\n```\n\n----------------------------------------\n\nTITLE: Analyzing Variable Importance Correlations Over Time\nDESCRIPTION: Demonstrates how to extract and visualize correlations between features based on their importance patterns over time. This helps identify variables that tend to gain or lose importance together across the time series.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_33\n\nLANGUAGE: python\nCODE:\n```\n# | eval: false\nnf.models[0].feature_importance_correlations()\n```\n\n----------------------------------------\n\nTITLE: PMM Loss Calculation Implementation\nDESCRIPTION: Implements the negative log-likelihood objective function calculation for model training. Handles batch and horizon correlations in the loss computation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_47\n\nLANGUAGE: python\nCODE:\n```\ndef __call__(self,\n             y: torch.Tensor,\n             distr_args: torch.Tensor,\n             mask: Union[torch.Tensor, None] = None):\n        # Instantiate Scaled Decoupled Distribution\n        distr = self.get_distribution(distr_args=distr_args)\n        x = distr._pad(y)\n        log_prob_x = distr.component_distribution.log_prob(x)\n        log_mix_prob = torch.log_softmax(distr.mixture_distribution.logits, dim=-1)\n        if self.batch_correlation:\n                log_prob_x = torch.sum(log_prob_x, dim=0, keepdim=True)\n        if self.horizon_correlation:\n                log_prob_x = torch.sum(log_prob_x, dim=1, keepdim=True)\n        \n        loss_values = -torch.logsumexp(log_prob_x + log_mix_prob, dim=-1)  \n       \n        return weighted_average(loss_values, weights=mask)\n```\n\n----------------------------------------\n\nTITLE: Results Processing and Visualization\nDESCRIPTION: Process forecasting results and create final visualization\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/06_longhorizon_probabilistic.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nY_hat_df = Y_hat_df.reset_index(drop=True)\nY_hat_df = Y_hat_df[(Y_hat_df['unique_id']=='OT') & (Y_hat_df['cutoff']=='2018-02-11 12:00:00')]\nY_hat_df = Y_hat_df.drop(columns=['y','cutoff'])\n```\n\nLANGUAGE: python\nCODE:\n```\nplot_df = Y_df.merge(Y_hat_df, on=['unique_id','ds'], how='outer').tail(96*10+50+96*4).head(96*2+96*4)\nplot_series(forecasts_df=plot_df.drop(columns='AutoNHITS').rename(columns={'AutoNHITS-median': 'AutoNHITS'}), level=[90])\n```\n\n----------------------------------------\n\nTITLE: Loading ETTm2 Dataset for Long-Horizon Forecasting\nDESCRIPTION: Loads the ETTm2 benchmark dataset using LongHorizon class and prepares the data for training by creating validation and test splits.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/05_longhorizon_transformers.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\nfrom datasetsforecast.long_horizon import LongHorizon\n```\n\nLANGUAGE: python\nCODE:\n```\n# Change this to your own data to try the model\nY_df, _, _ = LongHorizon.load(directory='./', group='ETTm2')\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\n\nn_time = len(Y_df.ds.unique())\nval_size = int(.2 * n_time)\ntest_size = int(.2 * n_time)\n\nY_df.groupby('unique_id').head(2)\n```\n\n----------------------------------------\n\nTITLE: Testing predict_insample with step_size\nDESCRIPTION: Tests the predict_insample functionality with different step sizes and test sizes. Validates cutoff values and forecast points count per series.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_57\n\nLANGUAGE: python\nCODE:\n```\nh = 12\ntrain_end = AirPassengers_pl['time'].max()\nsizes = AirPassengers_pl['uid'].value_counts().to_numpy()\n\nfor step_size, test_size in [(7, 0), (9, 0), (7, 5), (9, 5)]:\n    models = [NHITS(h=h, input_size=12, max_steps=1)]\n    nf = NeuralForecast(models=models, freq='1mo')\n    nf.fit(\n        AirPassengers_pl,\n        id_col='uid',\n        time_col='time',\n        target_col='target',    \n    )\n    nf.models[0].set_test_size(test_size)    \n    \n    forecasts = nf.predict_insample(step_size=step_size)\n    n_expected_cutoffs = (sizes[0][1] - test_size - nf.h + step_size) // step_size\n\n    last_cutoff = train_end - test_size * pd.offsets.MonthEnd() - h * pd.offsets.MonthEnd()\n    expected_cutoffs = np.flip(np.array([last_cutoff - step_size * i * pd.offsets.MonthEnd() for i in range(n_expected_cutoffs)]))\n    pl_cutoffs = forecasts.filter(polars.col('uid') ==nf.uids[1]).select('cutoff').unique(maintain_order=True)\n    actual_cutoffs = np.sort(np.array([pd.Timestamp(x['cutoff']) for x in pl_cutoffs.rows(named=True)]))\n    np.testing.assert_array_equal(expected_cutoffs, actual_cutoffs, err_msg=f\"{step_size=},{expected_cutoffs=},{actual_cutoffs=}\")\n\n    cutoffs_by_series = forecasts.group_by(['uid', 'cutoff']).count()\n    assert_frame_equal(cutoffs_by_series.filter(polars.col('uid') == \"Airline1\").select(['cutoff', 'count']), cutoffs_by_series.filter(polars.col('uid') == \"Airline2\").select(['cutoff', 'count'] ), check_row_order=False)\n```\n\n----------------------------------------\n\nTITLE: Training Transformer Models with Cross-validation\nDESCRIPTION: Initializes a NeuralForecast object with the models and performs cross-validation on the dataset using specified validation and test sizes.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/05_longhorizon_transformers.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n%%capture\nnf = NeuralForecast(\n    models=models,\n    freq='15min')\n\nY_hat_df = nf.cross_validation(df=Y_df,\n                               val_size=val_size,\n                               test_size=test_size,\n                               n_windows=None)\n```\n\n----------------------------------------\n\nTITLE: Implementing TST Independent Encoder\nDESCRIPTION: Channel-independent transformer encoder that handles positional encoding and input projection for the transformer architecture.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.patchtst.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass TSTiEncoder(nn.Module):\n    def __init__(self, c_in, patch_num, patch_len, max_seq_len=1024,\n                 n_layers=3, hidden_size=128, n_heads=16, d_k=None, d_v=None,\n                 linear_hidden_size=256, norm='BatchNorm', attn_dropout=0., dropout=0., act=\"gelu\", store_attn=False,\n                 key_padding_mask='auto', padding_var=None, attn_mask=None, res_attention=True, pre_norm=False,\n                 pe='zeros', learn_pe=True):\n        \n        super().__init__()\n        self.patch_num = patch_num\n        self.patch_len = patch_len\n        self.W_P = nn.Linear(patch_len, hidden_size)\n        self.W_pos = positional_encoding(pe, learn_pe, q_len, hidden_size)\n        self.dropout = nn.Dropout(dropout)\n        self.encoder = TSTEncoder(q_len, hidden_size, n_heads, d_k=d_k, d_v=d_v, linear_hidden_size=linear_hidden_size,\n                               norm=norm, attn_dropout=attn_dropout, dropout=dropout,\n                               pre_norm=pre_norm, activation=act, res_attention=res_attention, n_layers=n_layers,\n                               store_attn=store_attn)\n```\n\n----------------------------------------\n\nTITLE: Setting Logging Level for PyTorch Lightning in Python\nDESCRIPTION: Configures the logging level for PyTorch Lightning to only show warning messages, reducing the verbosity of the training output.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/07_time_series_scaling.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\n```\n\n----------------------------------------\n\nTITLE: Computing CRPS for Spline Distributions in PyTorch\nDESCRIPTION: This method calculates the Continuous Ranked Probability Score (CRPS) in analytical form for spline distributions. It uses tensor operations to compute various coefficients and returns the aggregate CRPS value.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndef crps_spline(self, z: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    \n    Compute CRPS in analytical form for the spline\n    \n    **Parameters**<br>\n    `z`: Observation to evaluate.\n\n    \"\"\"\n\n    qk_x, qk_x_plus, qk_y = self.qk_x, self.qk_x_plus, self.qk_y\n    sk_x, sk_x_plus = self.sk_x, self.sk_x_plus\n    delta_sk_x, delta_sk_y = self.delta_sk_x, self.delta_sk_y\n\n    z_expand = z.unsqueeze(dim=-1)\n    qk_x_plus_expand = qk_x_plus.unsqueeze(dim=-1)\n\n    alpha_tilde = self.cdf_spline(z)\n    alpha_tilde_expand = alpha_tilde.unsqueeze(dim=-1)\n\n    r = torch.minimum(torch.maximum(alpha_tilde_expand, sk_x), sk_x_plus)\n\n    coeff1 = (\n        -2 / 3 * sk_x_plus**3\n        + sk_x * sk_x_plus**2\n        + sk_x_plus**2\n        - (1 / 3) * sk_x**3\n        - 2 * sk_x * sk_x_plus\n        - r**2\n        + 2 * sk_x * r\n    )\n\n    coeff2 = (\n        -2 * torch.maximum(alpha_tilde_expand, sk_x_plus)\n        + sk_x_plus**2\n        + 2 * qk_x_plus_expand\n        - qk_x_plus_expand**2\n    )\n\n    result = (\n        (qk_x_plus**2 - qk_x**2) * (z_expand - qk_y)\n        + 2 * (qk_x_plus - alpha_tilde) * (qk_y - z_expand)\n        + torch.sum((delta_sk_y / delta_sk_x) * coeff1, dim=-1)\n        + torch.sum(delta_sk_y * coeff2, dim=-1)\n    )\n\n    return torch.sum(result, dim=-1)\n```\n\n----------------------------------------\n\nTITLE: Initializing GMM Parameters and Sampling in PyTorch\nDESCRIPTION: This snippet sets up the parameters for a Gaussian Mixture Model, creates the distribution, and samples from it. It uses PyTorch tensors to handle multi-dimensional data representing different horizons and batches.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_57\n\nLANGUAGE: Python\nCODE:\n```\n# Create single mixture and broadcast to N,H,1,K\nmeans   = torch.Tensor([[5,10,15], [10,20,30]])[None, :, :].unsqueeze(2)\n\n# # Create repetitions for the batch dimension N.\nN=2\nmeans = torch.repeat_interleave(input=means, repeats=N, dim=0)\nweights = torch.ones_like(means)\nstds  = torch.ones_like(means)\n\nprint('weights.shape (N,H,1,K) \\t', weights.shape)\nprint('means.shape (N,H,1,K) \\t', means.shape)\nprint('stds.shape (N,H,1,K) \\t', stds.shape)\n\ndistr = GMM(quantiles=[0.1, 0.40, 0.5, 0.60, 0.9], weighted=True)\ndistr_args = (means, stds, weights)\nsamples, sample_mean, quants = distr.sample(distr_args)\n\nprint('samples.shape (N,H,1,num_samples) ', samples.shape)\nprint('sample_mean.shape (N,H,1,1) ', sample_mean.shape)\nprint('quants.shape  (N,H,1, Q) \\t\\t', quants.shape)\n```\n\n----------------------------------------\n\nTITLE: HINT Model Configuration and Training\nDESCRIPTION: Sets up and trains the Hierarchical Forecast Network (HINT) model with NHITS as the base model, including train-test splits and model parameters configuration.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/09_hierarchical_forecasting.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport numpy as np\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import NHITS, HINT\nfrom neuralforecast.losses.pytorch import GMM, sCRPS\n```\n\nLANGUAGE: python\nCODE:\n```\nhorizon = 12\nY_test_df  = Y_df.groupby('unique_id', observed=True).tail(horizon)\nY_train_df = Y_df.drop(Y_test_df.index)\n```\n\nLANGUAGE: python\nCODE:\n```\nY_df['y'] = Y_df['y'] * (Y_df['y'] > 0)\nnf = NeuralForecast(models=[model], freq='MS')\nnf.fit(df=Y_train_df, val_size=12)\nY_hat_df = nf.predict()\n\nY_hat_df = Y_hat_df.rename(columns=lambda x: x.replace('.0', ''))\n```\n\n----------------------------------------\n\nTITLE: MQLoss Class Implementation for Probabilistic Forecasting in PyTorch\nDESCRIPTION: Implements the Multi-Quantile Loss class that calculates loss across multiple quantiles of a forecast distribution. The class inherits from BasePointLoss and can approximate the Continuous Ranked Probability Score (CRPS) when using many quantiles. It supports custom prediction interval levels and direct quantile specification.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass MQLoss(BasePointLoss):\n    \"\"\"  Multi-Quantile loss\n\n    Calculates the Multi-Quantile loss (MQL) between `y` and `y_hat`.\n    MQL calculates the average multi-quantile Loss for\n    a given set of quantiles, based on the absolute \n    difference between predicted quantiles and observed values.\n    \n    $$ \\mathrm{MQL}(\\\\mathbf{y}_{\\\\tau},[\\\\mathbf{\\hat{y}}^{(q_{1})}_{\\\\tau}, ... ,\\hat{y}^{(q_{n})}_{\\\\tau}]) = \\\\frac{1}{n} \\\\sum_{q_{i}} \\mathrm{QL}(\\\\mathbf{y}_{\\\\tau}, \\\\mathbf{\\hat{y}}^{(q_{i})}_{\\\\tau}) $$\n    \n    The limit behavior of MQL allows to measure the accuracy \n    of a full predictive distribution $\\mathbf{\\hat{F}}_{\\\\tau}$ with \n    the continuous ranked probability score (CRPS). This can be achieved \n    through a numerical integration technique, that discretizes the quantiles \n    and treats the CRPS integral with a left Riemann approximation, averaging over \n    uniformly distanced quantiles.    \n    \n    $$ \\mathrm{CRPS}(y_{\\\\tau}, \\mathbf{\\hat{F}}_{\\\\tau}) = \\int^{1}_{0} \\mathrm{QL}(y_{\\\\tau}, \\hat{y}^{(q)}_{\\\\tau}) dq $$\n\n    **Parameters:**<br>\n    `level`: int list [0,100]. Probability levels for prediction intervals (Defaults median).\n    `quantiles`: float list [0., 1.]. Alternative to level, quantiles to estimate from y distribution.\n    `horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window. <br>\n\n    **References:**<br>\n    [Roger Koenker and Gilbert Bassett, Jr., \"Regression Quantiles\".](https://www.jstor.org/stable/1913643)<br>\n    [James E. Matheson and Robert L. Winkler, \"Scoring Rules for Continuous Probability Distributions\".](https://www.jstor.org/stable/2629907)\n    \"\"\"\n    def __init__(self, level=[80, 90], quantiles=None, horizon_weight=None):\n\n        qs, output_names = level_to_outputs(level)\n        qs = torch.Tensor(qs)\n        # Transform quantiles to homogeneus output names\n        if quantiles is not None:\n            _, output_names = quantiles_to_outputs(quantiles)\n            qs = torch.Tensor(quantiles)\n\n        super(MQLoss, self).__init__(horizon_weight=horizon_weight,\n                                     outputsize_multiplier=len(qs),\n                                     output_names=output_names)\n        \n        self.quantiles = torch.nn.Parameter(qs, requires_grad=False)\n\n    def domain_map(self, y_hat: torch.Tensor):\n        \"\"\"\n        Input:\n        Univariate: [B, H, 1 * Q]\n        Multivariate: [B, H, N * Q]\n\n        Output: [B, H, N, Q]\n        \"\"\"\n        output = y_hat.reshape(y_hat.shape[0],\n                               y_hat.shape[1],\n                               -1,\n                               self.outputsize_multiplier)\n\n        return output\n\n    def _compute_weights(self, y, mask):\n        \"\"\"\n        Compute final weights for each datapoint (based on all weights and all masks)\n        Set horizon_weight to a ones[H] tensor if not set.\n        If set, check that it has the same length as the horizon in x.\n\n        y: [B, h, N, 1]\n        mask: [B, h, N, 1]\n        \"\"\"\n\n        if self.horizon_weight is None:\n            weights = torch.ones_like(mask)\n        else:\n            assert mask.shape[1] == len(self.horizon_weight), \\\n                'horizon_weight must have same length as Y'    \n            weights = self.horizon_weight.clone()\n            weights = weights[None, :, None, None]\n            weights = weights.to(mask.device)\n            weights = torch.ones_like(mask, device=mask.device) * weights\n        \n        return weights * mask\n\n    def __call__(self,\n                 y: torch.Tensor,\n                 y_hat: torch.Tensor,\n                 y_insample: torch.Tensor,\n                 mask: Union[torch.Tensor, None] = None,\n                 ) -> torch.Tensor:\n        \"\"\"\n        **Parameters:**<br>\n        `y`: tensor, Actual values.<br>\n        `y_hat`: tensor, Predicted values.<br>\n        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n\n        **Returns:**<br>\n        `mqloss`: tensor (single value).\n        \"\"\"\n        # [B, h, N] -> [B, h, N, 1]\n        if y_hat.ndim == 3:\n            y_hat = y_hat.unsqueeze(-1)\n\n        y = y.unsqueeze(-1)\n        if mask is not None:\n            mask = mask.unsqueeze(-1)\n        else:\n            mask = torch.ones_like(y, device=y.device)\n\n        error  = y_hat - y\n\n        sq     = torch.maximum(-error, torch.zeros_like(error))\n        s1_q   = torch.maximum(error, torch.zeros_like(error))\n        \n        quantiles = self.quantiles[None, None, None, :]\n        losses = (1 / len(quantiles)) * (quantiles * sq + (1 - quantiles) * s1_q)\n        weights = self._compute_weights(y=losses, mask=mask) # Use losses for extra dim\n\n        return _weighted_mean(losses=losses, weights=weights)\n```\n\n----------------------------------------\n\nTITLE: Training NBEATSx Model with MLflow Tracking\nDESCRIPTION: Trains an NBEATSx time series forecasting model while logging the experiment to MLflow. Records the dataset, model parameters, and training metrics. Also saves the model and its environment requirements for future reproducibility.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/12_using_mlflow.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmlflow.pytorch.autolog(checkpoint=False)\n\nwith mlflow.start_run() as run:\n    # Log the dataset to the MLflow Run. Specify the \"training\" context to indicate that the\n    # dataset is used for model training\n    dataset: PandasDataset = mlflow.data.from_pandas(Y_df, source=\"AirPassengersDF\")\n    mlflow.log_input(dataset, context=\"training\")\n\n    # Define and log parameters\n    horizon = len(Y_test_df)\n    model_params = dict(\n        input_size=1 * horizon,\n        h=horizon,\n        max_steps=300,  \n        loss=MAE(),\n        valid_loss=MAE(),  \n        activation='ReLU',\n        scaler_type='robust',\n        random_seed=42,\n        enable_progress_bar=False,\n    )\n    mlflow.log_params(model_params)\n\n    # Fit NBEATSx model\n    models = [NBEATSx(**model_params)]\n    nf = NeuralForecast(models=models, freq='M')           \n    train = nf.fit(df=Y_train_df, val_size=horizon)\n    \n    # Save conda environment used to run the model\n    mlflow.pytorch.get_default_conda_env()\n    \n    # Save pip requirements\n    mlflow.pytorch.get_default_pip_requirements()\n\nmlflow.pytorch.autolog(disable=True)\n\n# Save the neural forecast model\nnf.save(path='./checkpoints/test_run_1/',\n        model_index=None, \n        overwrite=True,\n        save_dataset=True)\n```\n\n----------------------------------------\n\nTITLE: Training TSMixerx Model and Forecasting Future Values in Python\nDESCRIPTION: This example demonstrates how to train the TSMixerx model using the NeuralForecast framework and make predictions for future values. It includes data preparation, model initialization, fitting, and visualization of the forecasts.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tsmixerx.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import TSMixerx\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\nfrom neuralforecast.losses.pytorch import GMM\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = TSMixerx(h=12,\n                input_size=24,\n                n_series=2,\n                stat_exog_list=['airline1'],\n                futr_exog_list=['trend'],\n                n_block=4,\n                ff_dim=4,\n                revin=True,\n                scaler_type='robust',\n                max_steps=500,\n                early_stop_patience_steps=-1,\n                val_check_steps=5,\n                learning_rate=1e-3,\n                loss = GMM(n_components=10, weighted=True),\n                batch_size=32\n                )\n\nfcst = NeuralForecast(models=[model], freq='ME')\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\n# Plot predictions\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['TSMixerx-median'], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'][-12:], \n                 y1=plot_df['TSMixerx-lo-90'][-12:].values,\n                 y2=plot_df['TSMixerx-hi-90'][-12:].values,\n                 alpha=0.4, label='level 90')\nax.set_title('AirPassengers Forecast', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Year', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\n```\n\n----------------------------------------\n\nTITLE: Implementing Identity Basis Function for NHITS in Python\nDESCRIPTION: This class implements the identity basis function used in the NHITS model. It handles backcast and forecast operations using various interpolation modes.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nhits.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass _IdentityBasis(nn.Module):\n    def __init__(self, backcast_size: int, forecast_size: int, \n                 interpolation_mode: str, out_features: int=1):\n        super().__init__()\n        assert (interpolation_mode in ['linear','nearest']) or ('cubic' in interpolation_mode)\n        self.forecast_size = forecast_size\n        self.backcast_size = backcast_size\n        self.interpolation_mode = interpolation_mode\n        self.out_features = out_features\n \n    def forward(self, theta: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n\n        backcast = theta[:, :self.backcast_size]\n        knots = theta[:, self.backcast_size:]\n\n        # Interpolation is performed on default dim=-1 := H\n        knots = knots.reshape(len(knots), self.out_features, -1)\n        if self.interpolation_mode in ['nearest', 'linear']:\n            #knots = knots[:,None,:]\n            forecast = F.interpolate(knots, size=self.forecast_size, mode=self.interpolation_mode)\n            #forecast = forecast[:,0,:]\n        elif 'cubic' in self.interpolation_mode:\n            if self.out_features>1:\n                raise Exception('Cubic interpolation not available with multiple outputs.')\n            batch_size = len(backcast)\n            knots = knots[:,None,:,:]\n            forecast = torch.zeros((len(knots), self.forecast_size), device=knots.device)\n            n_batches = int(np.ceil(len(knots)/batch_size))\n            for i in range(n_batches):\n                forecast_i = F.interpolate(knots[i*batch_size:(i+1)*batch_size], \n                                           size=self.forecast_size, mode='bicubic')\n                forecast[i*batch_size:(i+1)*batch_size] += forecast_i[:,0,0,:] # [B,None,H,H] -> [B,H]\n            forecast = forecast[:,None,:] # [B,H] -> [B,None,H]\n\n        # [B,Q,H] -> [B,H,Q]\n        forecast = forecast.permute(0, 2, 1)\n        return backcast, forecast\n```\n\n----------------------------------------\n\nTITLE: Calculating Mean Absolute Scaled Error (MASE) in Python\nDESCRIPTION: This function calculates the Mean Absolute Scaled Error between actual and predicted values, comparing the prediction accuracy against a seasonal naive model. It requires actual values, predicted values, in-sample training data, and seasonality as inputs.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.numpy.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef mase(y: np.ndarray, y_hat: np.ndarray, \n         y_train: np.ndarray,\n         seasonality: int,\n         weights: Optional[np.ndarray] = None,\n         axis: Optional[int] = None) -> Union[float, np.ndarray]:\n    \"\"\" Mean Absolute Scaled Error \n    Calculates the Mean Absolute Scaled Error between\n    `y` and `y_hat`. MASE measures the relative prediction\n    accuracy of a forecasting method by comparinng the mean absolute errors\n    of the prediction and the observed value against the mean\n    absolute errors of the seasonal naive model.\n    The MASE partially composed the Overall Weighted Average (OWA), \n    used in the M4 Competition.\n\n    $$ \\mathrm{MASE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}, \\mathbf{\\hat{y}}^{season}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{\\mathrm{MAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{season}_{\\tau})} $$\n\n    **Parameters:**<br>\n    `y`: numpy array, (batch_size, output_size), Actual values.<br>\n    `y_hat`: numpy array, (batch_size, output_size)), Predicted values.<br>\n    `y_insample`: numpy array, (batch_size, input_size), Actual insample Seasonal Naive predictions.<br>\n    `seasonality`: int. Main frequency of the time series; Hourly 24,  Daily 7, Weekly 52, Monthly 12, Quarterly 4, Yearly 1.        \n    `mask`: numpy array, Specifies date stamps per serie to consider in loss.<br>\n\n    **Returns:**<br>\n    `mase`: numpy array, (single value).\n    \n    **References:**<br>\n    [Rob J. Hyndman, & Koehler, A. B. \"Another look at measures of forecast accuracy\".](https://www.sciencedirect.com/science/article/pii/S0169207006000239)<br>\n    [Spyros Makridakis, Evangelos Spiliotis, Vassilios Assimakopoulos, \"The M4 Competition: 100,000 time series and 61 forecasting methods\".](https://www.sciencedirect.com/science/article/pii/S0169207019301128)\n    \"\"\"\n    delta_y = np.abs(y - y_hat)\n    delta_y = np.average(delta_y, weights=weights, axis=axis)\n\n    scale = np.abs(y_train[:-seasonality] - y_train[seasonality:])\n    scale = np.average(scale, axis=axis)\n\n    mase = delta_y / scale\n\n    return mase\n```\n\n----------------------------------------\n\nTITLE: Implementing LocalFilesTimeSeriesDataset for NeuralForecast\nDESCRIPTION: A dataset implementation that handles time series data stored in local parquet files. It provides methods for retrieving items and creating datasets from directories, with support for exogenous variables and static data.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/tsdataset.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass LocalFilesTimeSeriesDataset(BaseTimeSeriesDataset):\n\n    def __init__(self,\n     files_ds: List[str],\n     temporal_cols,\n     id_col: str,\n     time_col: str,\n     target_col: str,\n     last_times,\n     indices,\n     max_size: int, \n     min_size: int, \n     y_idx: int,\n     static=None,\n     static_cols=None,\n    ):\n        super().__init__(\n            temporal_cols=temporal_cols,\n            max_size=max_size,\n            min_size=min_size,\n            y_idx=y_idx,\n            static=static,\n            static_cols=static_cols,\n        )\n        self.files_ds = files_ds\n        self.id_col = id_col\n        self.time_col = time_col\n        self.target_col = target_col\n        #array with the last time for each timeseries\n        self.last_times = last_times\n        self.indices = indices\n        self.n_groups = len(files_ds)\n\n    def __getitem__(self, idx):\n        if not isinstance(idx, int):\n            raise ValueError(f'idx must be int, got {type(idx)}')\n        \n        temporal_cols = self.temporal_cols.copy()\n        data = pd.read_parquet(self.files_ds[idx], columns=temporal_cols.tolist()).to_numpy()\n        data, temporal_cols = TimeSeriesDataset._ensure_available_mask(data, temporal_cols)\n        data = self._as_torch_copy(data)\n\n        # Pad the temporal data to the left\n        temporal = torch.zeros(size=(len(temporal_cols), self.max_size),\n                                dtype=torch.float32)\n        temporal[:len(temporal_cols), -len(data):] = data.permute(1,0)\n\n        # Add static data if available\n        static = None if self.static is None else self.static[idx,:]\n\n        item = dict(temporal=temporal, temporal_cols=temporal_cols,\n                    static=static, static_cols=self.static_cols,\n                    y_idx=self.y_idx)\n\n        return item\n\n    @staticmethod\n    def from_data_directories(directories, static_df=None, exogs=[], id_col='unique_id', time_col='ds', target_col='y'):\n        \"\"\"We expect directories to be a list of directories of the form [unique_id=id_0, unique_id=id_1, ...]. Each directory should contain the timeseries corresponding to that unqiue_id,\n        represented as a pandas or polars DataFrame. The timeseries can be entirely contained in one parquet file or split between multiple, but within each parquet files the timeseries should be sorted by time.\n        Static df should also be a pandas or polars DataFrame\"\"\"\n        import pyarrow as pa\n        \n        # Define indices if not given and then extract static features\n        static, static_cols = TimeSeriesDataset._extract_static_features(static_df, id_col)\n        \n        max_size = 0\n        min_size = float('inf')\n        last_times = []\n        ids = []\n        expected_temporal = {target_col, *exogs}\n        available_mask_seen = True\n\n        for dir in directories:\n            dir_path = Path(dir)\n            if not dir_path.is_dir():\n                raise ValueError(f'paths must be directories, {dir} is not.')\n            uid = dir_path.name.split('=')[-1]\n            total_rows = 0\n            last_time = None\n            for file in dir_path.glob('*.parquet'):\n                meta = pa.parquet.read_metadata(file)\n                rg = meta.row_group(0)\n                col2pos = {rg.column(i).path_in_schema: i for i in range(rg.num_columns)}\n                \n                last_time_file = meta.row_group(meta.num_row_groups -1).column(col2pos[time_col]).statistics.max\n                last_time = max(last_time, last_time_file) if last_time is not None else last_time_file\n                total_rows += sum(meta.row_group(i).num_rows for i in range(meta.num_row_groups))\n\n                # Check all the temporal columns are present\n                missing_cols = expected_temporal - col2pos.keys()\n                if missing_cols:\n                    raise ValueError(f\"Temporal columns: {missing_cols} not found in the file: {file}.\")\n                \n                if 'available_mask' not in col2pos.keys():\n                    available_mask_seen = False\n                elif not available_mask_seen:\n                    # If this is triggered the available_mask column is present in this file but has been missing from previous files.\n                    raise ValueError(\"The available_mask column is present in some files but is missing in others.\")\n                else:\n                    expected_temporal.add(\"available_mask\")\n\n            max_size = max(total_rows, max_size)\n            min_size = min(total_rows, min_size)\n            ids.append(uid)\n            last_times.append(last_time)\n\n        last_times = pd.Index(last_times, name=time_col)\n        ids = pd.Series(ids, name=id_col)\n\n        if \"available_mask\" in expected_temporal:\n            exogs = [\"available_mask\", *exogs]\n        temporal_cols = pd.Index([target_col, *exogs])\n\n        dataset = LocalFilesTimeSeriesDataset(\n            files_ds=directories,\n            temporal_cols=temporal_cols,\n            id_col=id_col,\n            time_col=time_col,\n            target_col=target_col,\n            last_times=last_times,\n            indices=ids,\n            min_size=min_size,\n            max_size=max_size,\n            y_idx=0,\n            static=static,\n            static_cols=static_cols,\n        )\n        return dataset\n```\n\n----------------------------------------\n\nTITLE: Encoder Module Implementation\nDESCRIPTION: Implements the main encoder module that processes input through multiple layers with optional normalization and projection.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timexer.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass Encoder(nn.Module):\n    def __init__(self, layers, norm_layer=None, projection=None):\n        super(Encoder, self).__init__()\n        self.layers = nn.ModuleList(layers)\n        self.norm = norm_layer\n        self.projection = projection\n\n    def forward(self, x, cross, x_mask=None, cross_mask=None, tau=None, delta=None):\n        for layer in self.layers:\n            x = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask, tau=tau, delta=delta)\n\n        if self.norm is not None:\n            x = self.norm(x)\n\n        if self.projection is not None:\n            x = self.projection(x)\n        return x\n```\n\n----------------------------------------\n\nTITLE: Testing iterative vs direct dataset training in NeuralForecast\nDESCRIPTION: Tests whether training with an iterative dataset produces the same results as directly passing the dataset as a pandas DataFrame. Evaluates NHITS, AutoMLP with Optuna, and AutoNBEATSx with Ray Tune models on air passenger data.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_42\n\nLANGUAGE: python\nCODE:\n```\n# test training with an iterative dataset produces the same results as directly passing in the dataset as a pandas dataframe\nAirPassengersPanel_train['id'] = AirPassengersPanel_train['unique_id']\nAirPassengersPanel_test['id'] = AirPassengersPanel_test['unique_id']\n\nmodels = [\n    NHITS(h=12, input_size=12, max_steps=10, futr_exog_list=['trend'], random_seed=1),\n    AutoMLP(h=12, config=config_optuna, num_samples=2, backend='optuna', search_alg=optuna.samplers.TPESampler(seed=0)), # type: ignore\n    AutoNBEATSx(h=12, config=config_ray, cpus=1, num_samples=2)\n]\nnf = NeuralForecast(models=models, freq='M')\n\n# fit+predict with pandas dataframe\nnf.fit(df=AirPassengersPanel_train.drop(columns='unique_id'), use_init_models=True, id_col='id')\npred_dataframe = nf.predict(futr_df=AirPassengersPanel_test.drop(columns='unique_id')).reset_index()\n\n# fit+predict with data directory\nwith tempfile.TemporaryDirectory() as tmpdir:\n    AirPassengersPanel_train.to_parquet(tmpdir, partition_cols=['unique_id'], index=False)\n    data_directory = sorted([str(path) for path in Path(tmpdir).iterdir()])\n    nf.fit(df=data_directory, use_init_models=True, id_col='id')\n\npred_df = AirPassengersPanel_train[AirPassengersPanel_train['unique_id'] == 'Airline2'].drop(columns='unique_id')\nfutr_df = AirPassengersPanel_test[AirPassengersPanel_test['unique_id'] == 'Airline2'].drop(columns='unique_id')\n\npred_iterative = nf.predict(df=pred_df, futr_df=futr_df)\npred_airline2 = pred_dataframe[pred_dataframe['id'] == 'Airline2']\nnp.testing.assert_allclose(pred_iterative['NHITS'], pred_airline2['NHITS'], rtol=0, atol=1)\nnp.testing.assert_allclose(pred_iterative['AutoMLP'], pred_airline2['AutoMLP'], rtol=0, atol=1)\nnp.testing.assert_allclose(pred_iterative['AutoNBEATSx'], pred_airline2['AutoNBEATSx'], rtol=0, atol=1)\n```\n\n----------------------------------------\n\nTITLE: PMM Class Definition and Initialization\nDESCRIPTION: Defines the Poisson Mixture Mesh class that implements a probabilistic forecasting model using Poisson mixtures. Includes initialization of model parameters, confidence levels, and distribution components.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_45\n\nLANGUAGE: python\nCODE:\n```\nclass PMM(torch.nn.Module):\n    def __init__(self, n_components=10, level=[80, 90], quantiles=None,\n                 num_samples=1000, return_params=False,\n                 batch_correlation=False, horizon_correlation=False, \n                 weighted=False):\n        super(PMM, self).__init__()\n        # Transform level to MQLoss parameters\n        qs, self.output_names = level_to_outputs(level)\n        qs = torch.Tensor(qs)\n\n        # Transform quantiles to homogeneus output names\n        if quantiles is not None:\n            _, self.output_names = quantiles_to_outputs(quantiles)\n            qs = torch.Tensor(quantiles)\n        self.quantiles = torch.nn.Parameter(qs, requires_grad=False)\n        self.num_samples = num_samples\n        self.batch_correlation = batch_correlation\n        self.horizon_correlation = horizon_correlation\n        self.weighted = weighted   \n\n        # If True, predict_step will return Distribution's parameters\n        self.return_params = return_params\n```\n\n----------------------------------------\n\nTITLE: Testing Loss Functions with PyTorch and NumPy in NeuralForecast\nDESCRIPTION: A comprehensive test class that validates multiple loss functions, ensuring that the PyTorch implementations match their NumPy counterparts. The class tests MAE, MSE, RMSE, MAPE, SMAPE, and quantile loss using randomly generated data of varying dimensions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.numpy.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n# Test class for pytorch/numpy loss functions\nclass TestLoss(unittest.TestCase):\n    def setUp(self):   \n        self.num_quantiles = np.random.randint(3, 10)\n        self.first_num = np.random.randint(1, 300)\n        self.second_num = np.random.randint(1, 300)\n        \n        self.y = t.rand(self.first_num, self.second_num)\n        self.y_hat = t.rand(self.first_num, self.second_num)\n        self.y_hat2 = t.rand(self.first_num, self.second_num)\n        self.y_hat_quantile = t.rand(self.first_num, self.second_num, self.num_quantiles)\n        \n        self.quantiles = t.rand(self.num_quantiles)\n        self.q_float = np.random.random_sample()\n\n    def test_mae(self):\n        mae_numpy   = mae(self.y, self.y_hat)\n        mae_pytorch = MAE()\n        mae_pytorch = mae_pytorch(self.y, self.y_hat).numpy()\n        self.assertAlmostEqual(mae_numpy, mae_pytorch, places=6)\n\n    def test_mse(self):\n        mse_numpy   = mse(self.y, self.y_hat)\n        mse_pytorch = MSE()\n        mse_pytorch = mse_pytorch(self.y, self.y_hat).numpy()\n        self.assertAlmostEqual(mse_numpy, mse_pytorch, places=6)\n\n    def test_rmse(self):\n        rmse_numpy   = rmse(self.y, self.y_hat)\n        rmse_pytorch = RMSE()\n        rmse_pytorch = rmse_pytorch(self.y, self.y_hat).numpy()\n        self.assertAlmostEqual(rmse_numpy, rmse_pytorch, places=6)\n\n    def test_mape(self):\n        mape_numpy   = mape(y=self.y, y_hat=self.y_hat)\n        mape_pytorch = MAPE()\n        mape_pytorch = mape_pytorch(y=self.y, y_hat=self.y_hat).numpy()\n        self.assertAlmostEqual(mape_numpy, mape_pytorch, places=6)\n\n    def test_smape(self):\n        smape_numpy   = smape(self.y, self.y_hat)\n        smape_pytorch = SMAPE()\n        smape_pytorch = smape_pytorch(self.y, self.y_hat).numpy()\n        self.assertAlmostEqual(smape_numpy, smape_pytorch, places=4)\n    \n    #def test_mase(self):\n    #    y_insample = t.rand(self.first_num, self.second_num)\n    #    seasonality = 24\n    #    # Hourly 24, Daily 7, Weekly 52\n    #    # Monthly 12, Quarterly 4, Yearly 1\n    #    mase_numpy   = mase(y=self.y, y_hat=self.y_hat,\n    #                        y_insample=y_insample, seasonality=seasonality)\n    #    mase_object  = MASE(seasonality=seasonality)\n    #    mase_pytorch = mase_object(y=self.y, y_hat=self.y_hat,\n    #                               y_insample=y_insample).numpy()\n    #    self.assertAlmostEqual(mase_numpy, mase_pytorch, places=2)\n\n    #def test_rmae(self):\n    #    rmae_numpy   = rmae(self.y, self.y_hat, self.y_hat2)\n    #    rmae_object  = RMAE()\n    #    rmae_pytorch = rmae_object(self.y, self.y_hat, self.y_hat2).numpy()\n    #    self.assertAlmostEqual(rmae_numpy, rmae_pytorch, places=4)\n\n    def test_quantile(self):\n        quantile_numpy = quantile_loss(self.y, self.y_hat, q = self.q_float)\n        quantile_pytorch = QuantileLoss(q = self.q_float)\n        quantile_pytorch = quantile_pytorch(self.y, self.y_hat).numpy()\n        self.assertAlmostEqual(quantile_numpy, quantile_pytorch, places=6)\n    \n    # def test_mqloss(self):\n    #     weights = np.ones_like(self.y)\n\n    #     mql_np_w = mqloss(self.y, self.y_hat_quantile, self.quantiles, weights=weights)\n    #     mql_np_default_w = mqloss(self.y, self.y_hat_quantile, self.quantiles)\n\n    #     mql_object = MQLoss(quantiles=self.quantiles)\n    #     mql_py_w = mql_object(y=self.y,\n    #                           y_hat=self.y_hat_quantile,\n    #                           mask=t.Tensor(weights)).numpy()\n        \n    #     print('self.y.shape', self.y.shape)\n    #     print('self.y_hat_quantile.shape', self.y_hat_quantile.shape)\n    #     mql_py_default_w = mql_object(y=self.y,\n    #                                   y_hat=self.y_hat_quantile).numpy()\n\n    #     weights[0,:] = 0\n    #     mql_np_new_w = mqloss(self.y, self.y_hat_quantile, self.quantiles, weights=weights)\n    #     mql_py_new_w = mql_object(y=self.y,\n    #                               y_hat=self.y_hat_quantile,\n    #                               mask=t.Tensor(weights)).numpy()\n\n    #     self.assertAlmostEqual(mql_np_w,  mql_np_default_w)\n    #     self.assertAlmostEqual(mql_py_w,  mql_py_default_w)\n    #     self.assertAlmostEqual(mql_np_new_w,  mql_py_new_w)\n    \n\nunittest.main(argv=[''], verbosity=2, exit=False)\n```\n\n----------------------------------------\n\nTITLE: Implementing FeatureMixing Layer for TSMixer in Python\nDESCRIPTION: Defines the FeatureMixing layer, another crucial component of TSMixer. This layer applies feature normalization, two linear transformations with an intermediate ReLU activation, and dropout, focusing on cross-sectional relationships in the time series.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tsmixer.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass FeatureMixing(nn.Module):\n    \"\"\" \n    FeatureMixing\n    \"\"\"    \n    def __init__(self, n_series, input_size, dropout, ff_dim):\n        super().__init__()\n        self.feature_norm = nn.BatchNorm1d(num_features=n_series * input_size, eps=0.001, momentum=0.01)\n        self.feature_lin_1 = nn.Linear(n_series, ff_dim)\n        self.feature_lin_2 = nn.Linear(ff_dim, n_series)\n        self.feature_drop_1 = nn.Dropout(dropout)\n        self.feature_drop_2 = nn.Dropout(dropout)\n\n    def forward(self, input):\n        # Get shapes\n        batch_size = input.shape[0]\n        input_size = input.shape[1]\n        n_series = input.shape[2]\n\n        # Feature MLP\n        x = input.reshape(batch_size, -1)                               # [B, L, N] -> [B, L * N]\n        x = self.feature_norm(x)                                        # [B, L * N] -> [B, L * N]\n        x = x.reshape(batch_size, input_size, n_series)                 # [B, L * N] -> [B, L, N]\n        x = F.relu(self.feature_lin_1(x))                               # [B, L, N] -> [B, L, ff_dim]\n        x = self.feature_drop_1(x)                                      # [B, L, ff_dim] -> [B, L, ff_dim]\n        x = self.feature_lin_2(x)                                       # [B, L, ff_dim] -> [B, L, N]\n        x = self.feature_drop_2(x)                                      # [B, L, N] -> [B, L, N]\n\n        return x + input \n```\n\n----------------------------------------\n\nTITLE: Calculating Symmetric Mean Absolute Percentage Error (SMAPE) for Time Series Forecasts\nDESCRIPTION: Implements the Symmetric Mean Absolute Percentage Error metric for evaluating time series forecasts. It provides a bounded percentage error between 0% and 200%.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.numpy.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndef smape(y: np.ndarray, y_hat: np.ndarray,\n          weights: Optional[np.ndarray] = None,\n          axis: Optional[int] = None) -> Union[float, np.ndarray]:\n    \"\"\" Symmetric Mean Absolute Percentage Error\n\n    Calculates Symmetric Mean Absolute Percentage Error between\n    `y` and `y_hat`. SMAPE measures the relative prediction\n    accuracy of a forecasting method by calculating the relative deviation\n    of the prediction and the observed value scaled by the sum of the\n    absolute values for the prediction and observed value at a\n    given time, then averages these devations over the length\n    of the series. This allows the SMAPE to have bounds between\n    0% and 200% which is desirable compared to normal MAPE that\n    may be undetermined when the target is zero.\n\n    $$ \\mathrm{sMAPE}_{2}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{|y_{\\tau}|+|\\hat{y}_{\\tau}|} $$\n\n    **Parameters:**<br>\n    `y`: numpy array, Actual values.<br>\n    `y_hat`: numpy array, Predicted values.<br>\n    `mask`: numpy array, Specifies date stamps per serie to consider in loss.<br>\n\n    **Returns:**<br>\n    `smape`: numpy array, (single value).\n    \n    **References:**<br>\n    [Makridakis S., \"Accuracy measures: theoretical and practical concerns\".](https://www.sciencedirect.com/science/article/pii/0169207093900793)\n    \"\"\"\n    _metric_protections(y, y_hat, weights)\n        \n    delta_y = np.abs(y - y_hat)\n    scale = np.abs(y) + np.abs(y_hat)\n    smape = _divide_no_nan(delta_y, scale)\n    smape = 2 * np.average(smape, weights=weights, axis=axis)\n    \n    if isinstance(smape, float):\n        assert smape <= 2, 'SMAPE should be lower than 200'\n    else:\n        assert all(smape <= 2), 'SMAPE should be lower than 200'\n    \n    return smape\n```\n\n----------------------------------------\n\nTITLE: Performing Cross-Validation with NeuralForecast in Python\nDESCRIPTION: This code demonstrates how to use the NeuralForecast class to perform cross-validation on the TSMixer and TSMixerx models, using a sliding window approach for time series data.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/08_multivariate_tsmixer.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n%%capture\nnf = NeuralForecast(models=[model, modelx], freq='15min')\nY_hat_df = nf.cross_validation(df=Y_df, val_size=val_size,\n                               test_size=test_size, n_windows=None)\n```\n\n----------------------------------------\n\nTITLE: Validating NBEATSx Model Steps Configuration\nDESCRIPTION: Sets up NBEATSx model with validation step checks and protection against exceeding max steps. Configures model with identity, trend, seasonality and exogenous stacks while using robust scaling.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeatsx.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndataset, *_ = TimeSeriesDataset.from_df(Y_df)\nmodel = NBEATSx(h=12,\n                input_size=24,\n                scaler_type='robust',\n                stack_types = [\"identity\", \"trend\", \"seasonality\", \"exogenous\"],\n                n_blocks = [1,1,1,1],\n                futr_exog_list=['month','year'],\n                windows_batch_size=None,\n                early_stop_patience_steps=1,\n                max_steps=1,\n                val_check_steps=5\n                )\nmodel.fit(dataset=dataset, test_size=12, val_size=12)\ntest_eq(model.trainer_kwargs['val_check_interval'], 1)\n```\n\n----------------------------------------\n\nTITLE: Example Usage of AutoGRU Model\nDESCRIPTION: Demonstrates initialization and usage of AutoGRU model with custom configuration and different backends. Shows similar workflow to AutoLSTM for consistency.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nconfig = dict(max_steps=1, val_check_steps=1, input_size=-1, encoder_hidden_size=8)\nmodel = AutoGRU(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n# Optuna\nmodel = AutoGRU(h=12, config=None, backend='optuna')\n```\n\n----------------------------------------\n\nTITLE: Implementing Frequency Domain Attention with PyTorch FFT\nDESCRIPTION: Processes input tensors through FFT transformation, applies attention mechanism in frequency domain with configurable activation (tanh/softmax), and returns to time domain via inverse FFT. Handles complex number operations and tensor manipulations using PyTorch operations.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.fedformer.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nfor i, j in enumerate(self.index_q):\n    xq_ft_[:, :, :, i] = xq_ft[:, :, :, j]\nxk_ft_ = torch.zeros(B, H, E, len(self.index_kv), device=xq.device, dtype=torch.cfloat)\nxk_ft = torch.fft.rfft(xk, dim=-1)\nfor i, j in enumerate(self.index_kv):\n    xk_ft_[:, :, :, i] = xk_ft[:, :, :, j]\n\n# Attention mechanism on frequency domain\nxqk_ft = (torch.einsum(\"bhex,bhey->bhxy\", xq_ft_, xk_ft_))\nif self.activation == 'tanh':\n    xqk_ft = xqk_ft.tanh()\nelif self.activation == 'softmax':\n    xqk_ft = torch.softmax(abs(xqk_ft), dim=-1)\n    xqk_ft = torch.complex(xqk_ft, torch.zeros_like(xqk_ft))\nelse:\n    raise Exception('{} actiation function is not implemented'.format(self.activation))\nxqkv_ft = torch.einsum(\"bhxy,bhey->bhex\", xqk_ft, xk_ft_)\nxqkvw = torch.einsum(\"bhex,heox->bhox\", xqkv_ft, self.weights1)\nout_ft = torch.zeros(B, H, E, L // 2 + 1, device=xq.device, dtype=torch.cfloat)\nfor i, j in enumerate(self.index_q):\n    out_ft[:, :, :, j] = xqkvw[:, :, :, i]\n\n# Return to time domain\nout = torch.fft.irfft(out_ft / self.in_channels / self.out_channels, n=xq.size(-1))\nreturn (out, None)\n```\n\n----------------------------------------\n\nTITLE: Implementing ISQF Distribution Class in PyTorch\nDESCRIPTION: Implementation of the ISQF (Incremental Spline Quantile Function) distribution class that extends TransformedDistribution. This class represents the distribution with location and scale transforms applied to the base distribution.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\nclass ISQF(TransformedDistribution):\n    \"\"\"\n    Distribution class for the Incremental (Spline) Quantile Function.\n    \n    **Parameters:**<br>\n    `spline_knots`: Tensor parametrizing the x-positions of the spline knots. Shape: (*batch_shape, (num_qk-1), num_pieces)\n    `spline_heights`: Tensor parametrizing the y-positions of the spline knots. Shape: (*batch_shape, (num_qk-1), num_pieces)\n    `beta_l`: Tensor containing the non-negative learnable parameter of the left tail. Shape: (*batch_shape,)\n    `beta_r`: Tensor containing the non-negative learnable parameter of the right tail. Shape: (*batch_shape,)\n    `qk_y`: Tensor containing the increasing y-positions of the quantile knots. Shape: (*batch_shape, num_qk)\n    `qk_x`: Tensor containing the increasing x-positions of the quantile knots. Shape: (*batch_shape, num_qk)\n    `loc`: Tensor containing the location in case of a transformed random variable. Shape: (*batch_shape,)\n    `scale`: Tensor containing the scale in case of a transformed random variable. Shape: (*batch_shape,)\n\n    **References:**<br>\n    [Park, Youngsuk, Danielle Maddix, Franois-Xavier Aubet, Kelvin Kan, Jan Gasthaus, and Yuyang Wang (2022). \"Learning Quantile Functions without Quantile Crossing for Distribution-free Time Series Forecasting\".](https://proceedings.mlr.press/v151/park22a.html)\n        \n    \"\"\"\n\n    def __init__(\n        self,\n        spline_knots: torch.Tensor,\n        spline_heights: torch.Tensor,\n        beta_l: torch.Tensor,\n        beta_r: torch.Tensor,\n        qk_y: torch.Tensor,\n        qk_x: torch.Tensor,\n        loc: torch.Tensor,\n        scale: torch.Tensor,        \n        validate_args=None,\n    ) -> None:\n        base_distribution = BaseISQF(spline_knots=spline_knots,\n                                 spline_heights=spline_heights,\n                                 beta_l=beta_l,\n                                 beta_r=beta_r,\n                                 qk_y=qk_y,\n                                 qk_x=qk_x,\n                                 validate_args=validate_args)\n        transforms = AffineTransform(loc = loc, scale = scale)\n        super().__init__(\n            base_distribution, transforms, validate_args=validate_args\n        )\n\n    def crps(self, y: torch.Tensor) -> torch.Tensor:\n        z = y\n        scale = 1.0\n        t = self.transforms[0]\n        z = t._inverse(z)\n        scale *= t.scale\n        p = self.base_dist.crps(z)\n        return p * scale\n    \n    @property\n    def mean(self):\n        \"\"\"\n        Function used to compute the empirical mean\n        \"\"\"\n        samples = self.sample([1000])\n        return samples.mean(dim=0)\n```\n\n----------------------------------------\n\nTITLE: Implementing MixingLayer for TSMixer in Python\nDESCRIPTION: Defines the MixingLayer, which combines TemporalMixing and FeatureMixing layers. This layer forms the core of the TSMixer architecture, allowing for joint learning of temporal and cross-sectional representations of the time series data.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tsmixer.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass MixingLayer(nn.Module):\n    \"\"\" \n    MixingLayer\n    \"\"\"  \n    def __init__(self, n_series, input_size, dropout, ff_dim):\n        super().__init__()\n        # Mixing layer consists of a temporal and feature mixer\n        self.temporal_mixer = TemporalMixing(n_series, input_size, dropout)\n        self.feature_mixer = FeatureMixing(n_series, input_size, dropout, ff_dim)\n\n    def forward(self, input):\n        x = self.temporal_mixer(input)\n        x = self.feature_mixer(x)\n        return x\n```\n\n----------------------------------------\n\nTITLE: Multiscale Mixing Components Implementation\nDESCRIPTION: Implements various mixing components including MultiScaleSeasonMixing, MultiScaleTrendMixing, and PastDecomposableMixing for handling different scales of time series patterns.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timemixer.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass MultiScaleSeasonMixing(nn.Module):\n    \"\"\"\n    Bottom-up mixing season pattern\n    \"\"\"\n\n    def __init__(self, seq_len, down_sampling_window, down_sampling_layers):\n        super(MultiScaleSeasonMixing, self).__init__()\n\n        self.down_sampling_layers = torch.nn.ModuleList(\n            [\n                nn.Sequential(\n                    torch.nn.Linear(\n                        math.ceil(seq_len // (down_sampling_window ** i)),\n                        math.ceil(seq_len // (down_sampling_window ** (i + 1))),\n                    ),\n                    nn.GELU(),\n                    torch.nn.Linear(\n                        math.ceil(seq_len // (down_sampling_window ** (i + 1))),\n                        math.ceil(seq_len // (down_sampling_window ** (i + 1))),\n                    ),\n\n                )\n                for i in range(down_sampling_layers)\n            ]\n        )\n\n    def forward(self, season_list):\n\n        # mixing high->low\n        out_high = season_list[0]\n        out_low = season_list[1]\n        out_season_list = [out_high.permute(0, 2, 1)]\n\n        for i in range(len(season_list) - 1):\n            out_low_res = self.down_sampling_layers[i](out_high)\n            out_low = out_low + out_low_res\n            out_high = out_low\n            if i + 2 <= len(season_list) - 1:\n                out_low = season_list[i + 2]\n            out_season_list.append(out_high.permute(0, 2, 1))\n\n        return out_season_list\n\n\nclass MultiScaleTrendMixing(nn.Module):\n    \"\"\"\n    Top-down mixing trend pattern\n    \"\"\"\n\n    def __init__(self, seq_len, down_sampling_window, down_sampling_layers):\n        super(MultiScaleTrendMixing, self).__init__()\n\n        self.up_sampling_layers = torch.nn.ModuleList(\n            [\n                nn.Sequential(\n                    torch.nn.Linear(\n                        math.ceil(seq_len / (down_sampling_window ** (i + 1))),\n                        math.ceil(seq_len / (down_sampling_window ** i)),\n                    ),\n                    nn.GELU(),\n                    torch.nn.Linear(\n                        math.ceil(seq_len / (down_sampling_window ** i)),\n                        math.ceil(seq_len / (down_sampling_window ** i)),\n                    ),\n                )\n                for i in reversed(range(down_sampling_layers))\n            ])\n\n    def forward(self, trend_list):\n\n        # mixing low->high\n        trend_list_reverse = trend_list.copy()\n        trend_list_reverse.reverse()\n        out_low = trend_list_reverse[0]\n        out_high = trend_list_reverse[1]\n        out_trend_list = [out_low.permute(0, 2, 1)]\n\n        for i in range(len(trend_list_reverse) - 1):\n            out_high_res = self.up_sampling_layers[i](out_low)\n            out_high = out_high + out_high_res\n            out_low = out_high\n            if i + 2 <= len(trend_list_reverse) - 1:\n                out_high = trend_list_reverse[i + 2]\n            out_trend_list.append(out_low.permute(0, 2, 1))\n\n        out_trend_list.reverse()\n        return out_trend_list\n\n\nclass PastDecomposableMixing(nn.Module):\n    \"\"\"\n    PastDecomposableMixing\n    \"\"\"\n    def __init__(self, seq_len, pred_len, down_sampling_window, down_sampling_layers, \n                 d_model, dropout, channel_independence, decomp_method, d_ff, moving_avg, top_k):\n        super(PastDecomposableMixing, self).__init__()\n        self.seq_len = seq_len\n        self.pred_len = pred_len\n        self.down_sampling_window = down_sampling_window\n        self.down_sampling_layers = down_sampling_layers\n\n        self.layer_norm = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.channel_independence = channel_independence\n\n        if decomp_method == 'moving_avg':\n            self.decompsition = SeriesDecomp(moving_avg)\n        elif decomp_method == \"dft_decomp\":\n            self.decompsition = DFT_series_decomp(top_k)\n        else:\n            raise ValueError('decompsition is error')\n\n        if self.channel_independence == 0:\n            self.cross_layer = nn.Sequential(\n                nn.Linear(in_features=d_model, out_features=d_ff),\n                nn.GELU(),\n                nn.Linear(in_features=d_ff, out_features=d_model),\n            )\n\n        # Mixing season\n        self.mixing_multi_scale_season = MultiScaleSeasonMixing(self.seq_len, self.down_sampling_window, self.down_sampling_layers)\n\n        # Mxing trend\n        self.mixing_multi_scale_trend = MultiScaleTrendMixing(self.seq_len, self.down_sampling_window, self.down_sampling_layers)\n\n        self.out_cross_layer = nn.Sequential(\n            nn.Linear(in_features=d_model, out_features=d_ff),\n            nn.GELU(),\n            nn.Linear(in_features=d_ff, out_features=d_model),\n        )\n\n    def forward(self, x_list):\n        length_list = []\n        for x in x_list:\n            _, T, _ = x.size()\n            length_list.append(T)\n\n        # Decompose to obtain the season and trend\n        season_list = []\n        trend_list = []\n        for x in x_list:\n            season, trend = self.decompsition(x)\n            if self.channel_independence == 0:\n                season = self.cross_layer(season)\n                trend = self.cross_layer(trend)\n            season_list.append(season.permute(0, 2, 1))\n            trend_list.append(trend.permute(0, 2, 1))\n\n        # bottom-up season mixing\n        out_season_list = self.mixing_multi_scale_season(season_list)\n        # top-down trend mixing\n        out_trend_list = self.mixing_multi_scale_trend(trend_list)\n\n        out_list = []\n        for ori, out_season, out_trend, length in zip(x_list, out_season_list, out_trend_list,\n                                                      length_list):\n            out = out_season + out_trend\n            if self.channel_independence:\n                out = ori + self.out_cross_layer(out)\n            out_list.append(out[:, :length, :])\n        return out_list\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Hyperparameter Search Space\nDESCRIPTION: Creating a fully customized hyperparameter search space for NHITS model with specific parameters including learning rate, kernel size, and downsampling ratios. The configuration also sets training parameters like maximum steps and validation check frequency.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/04_hyperparameter_tuning.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nnhits_config = {\n   \"max_steps\": 100,                                                         # Number of SGD steps\n   \"input_size\": 24,                                                         # Size of input window\n   \"learning_rate\": tune.loguniform(1e-5, 1e-1),                             # Initial Learning rate\n   \"n_pool_kernel_size\": tune.choice([[2, 2, 2], [16, 8, 1]]),               # MaxPool's Kernelsize\n   \"n_freq_downsample\": tune.choice([[168, 24, 1], [24, 12, 1], [1, 1, 1]]), # Interpolation expressivity ratios\n   \"val_check_steps\": 50,                                                    # Compute validation every 50 steps\n   \"random_seed\": tune.randint(1, 10),                                       # Random seed\n}\n```\n\n----------------------------------------\n\nTITLE: Testing Variable Order Insensitivity in NHITS Model\nDESCRIPTION: Verifies that the order of variables in the dataset doesn't affect model validation loss. Tests this with additional dummy variables (zeros, large_number, available_mask) for both scaled and unscaled NHITS models.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_51\n\nLANGUAGE: python\nCODE:\n```\n# Test order of variables does not affect validation loss\n\nAirPassengersPanel_train['zeros'] = 0\nAirPassengersPanel_train['large_number'] = 100000\nAirPassengersPanel_train['available_mask'] = 1\nAirPassengersPanel_train = AirPassengersPanel_train[['unique_id','ds','zeros','y','available_mask','large_number']]\n\nmodels = [NHITS(h=12, input_size=24, max_steps=50, scaler_type='robust')]\nnf = NeuralForecast(models=models, freq='M')\nnf.fit(AirPassengersPanel_train,val_size=12)\nvalid_losses = nf.models[0].valid_trajectories\nassert valid_losses[-1][1] < 40, 'Validation loss is too high'\nassert valid_losses[-1][1] > 10, 'Validation loss is too low'\n\nmodels = [NHITS(h=12, input_size=24, max_steps=50, scaler_type=None)]\nnf = NeuralForecast(models=models, freq='M')\nnf.fit(AirPassengersPanel_train,val_size=12)\nvalid_losses = nf.models[0].valid_trajectories\nassert valid_losses[-1][1] < 40, 'Validation loss is too high'\nassert valid_losses[-1][1] > 10, 'Validation loss is too low'\n```\n\n----------------------------------------\n\nTITLE: Initializing NeuralForecast Models\nDESCRIPTION: Creates NBEATS, NHITS, and AutoMLP models with specified hyperparameters and forecast horizon\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/06_save_load_models.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nhorizon = 12\nmodels = [NBEATS(input_size=2 * horizon, h=horizon, max_steps=50),\n          NHITS(input_size=2 * horizon, h=horizon, max_steps=50),\n          AutoMLP(# Ray tune explore config\n                  config=dict(max_steps=100, # Operates with steps not epochs\n                              input_size=tune.choice([3*horizon]),\n                              learning_rate=tune.choice([1e-3])),\n                  h=horizon,\n                  num_samples=1, cpus=1)]\n```\n\n----------------------------------------\n\nTITLE: Initializing TCN Components in PyTorch\nDESCRIPTION: Code snippet showing the initialization of TCN model components including the temporal convolution encoder, context adapter, and MLP decoder. These components work together to process time series data.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tcn.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Context adapter\nself.context_size = context_size\n\n# MLP decoder\nself.decoder_hidden_size = decoder_hidden_size\nself.decoder_layers = decoder_layers\n\n# TCN input size (1 for target variable y)\ninput_encoder = 1 + self.hist_exog_size + self.stat_exog_size + self.futr_exog_size\n\n\n#---------------------------------- Instantiate Model -----------------------------------#\n# Instantiate historic encoder\nself.hist_encoder = TemporalConvolutionEncoder(\n                           in_channels=input_encoder,\n                           out_channels=self.encoder_hidden_size,\n                           kernel_size=self.kernel_size, # Almost like lags\n                           dilations=self.dilations,\n                           activation=self.encoder_activation)\n\n# Context adapter\nself.context_adapter = nn.Linear(in_features=self.input_size,\n                                 out_features=h)\n\n# Decoder MLP\nself.mlp_decoder = MLP(in_features=self.encoder_hidden_size + self.futr_exog_size,\n                       out_features=self.loss.outputsize_multiplier,\n                       hidden_size=self.decoder_hidden_size,\n                       num_layers=self.decoder_layers,\n                       activation='ReLU',\n                       dropout=0.0)\n```\n\n----------------------------------------\n\nTITLE: Adding Conformal Error Intervals in Python\nDESCRIPTION: Implements the `add_conformal_error_intervals` function to calculate prediction intervals using the conformal error method. It takes model forecasts (`model_fcsts`), conformal scores (`cs_df`), model name, number of windows, series count, horizon, and desired levels or quantiles. It computes quantiles based on absolute errors, constructs the intervals relative to the mean forecast, and returns the forecasts augmented with interval bounds and corresponding column names. Either `level` or `quantiles` must be provided.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/utils.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n#| export\ndef add_conformal_error_intervals(\n    model_fcsts: np.array, \n    cs_df: DFType, \n    model: str,\n    cs_n_windows: int,\n    n_series: int,\n    horizon: int,\n    level: Optional[List[Union[int, float]]] = None,\n    quantiles: Optional[List[float]] = None,\n) -> Tuple[np.array, List[str]]:\n    \"\"\"\n    Adds conformal intervals to a `fcst_df` based on conformal scores `cs_df`.\n    `level` should be already sorted. This startegy creates prediction intervals\n    based on the absolute errors.\n    \"\"\"\n    assert level is not None or quantiles is not None, \"Either level or quantiles must be provided\"\n\n    if quantiles is None and level is not None:\n        cuts = [lv / 100 for lv in level]\n    elif quantiles is not None:\n        cuts = quantiles\n\n    mean = model_fcsts.ravel()\n    scores = cs_df[model].to_numpy().reshape(n_series, cs_n_windows, horizon)\n    scores = scores.transpose(1, 0, 2)\n    # restrict scores to horizon\n    scores = scores[:,:,:horizon]\n    scores_quantiles = np.quantile(\n        scores,\n        cuts,\n        axis=0,\n    )\n    scores_quantiles = scores_quantiles.reshape(len(cuts), -1)\n    if quantiles is None and level is not None:\n        lo_cols = [f\"{model}-lo-{lv}\" for lv in reversed(level)]\n        hi_cols = [f\"{model}-hi-{lv}\" for lv in level]\n        out_cols = lo_cols + hi_cols\n        scores_quantiles = np.vstack([mean - scores_quantiles[::-1], mean + scores_quantiles]).T\n    elif quantiles is not None:\n        out_cols = []\n        scores_quantiles_ls = []\n        for i, q in enumerate(quantiles):\n            out_cols.append(f\"{model}-ql{q}\")\n            if q < 0.5:\n                scores_quantiles_ls.append(mean - scores_quantiles[::-1][i])\n            elif q > 0.5:\n                scores_quantiles_ls.append(mean + scores_quantiles[i])\n            else:\n                scores_quantiles_ls.append(mean)\n        scores_quantiles = np.vstack(scores_quantiles_ls).T                    \n\n    fcsts_with_intervals = np.hstack([model_fcsts, scores_quantiles])\n\n    return fcsts_with_intervals, out_cols\n```\n\n----------------------------------------\n\nTITLE: Defining AutoDilatedRNN Class for Hyperparameter Tuning in Python\nDESCRIPTION: This snippet defines the `AutoDilatedRNN` class, inheriting from `BaseAuto`, for automated hyperparameter optimization of the `DilatedRNN` model. It specifies a default search space (`default_config`) using `ray.tune` for parameters like `cell_type`, `encoder_hidden_size`, `dilations`, etc. The `__init__` method sets up the model using provided or default configuration, loss function, and backend (Ray/Optuna). The `get_default_config` class method retrieves and potentially adapts the configuration based on the forecast horizon `h` and chosen backend.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass AutoDilatedRNN(BaseAuto):\n\n    default_config = {\n        \"input_size_multiplier\": [-1, 4, 16, 64],\n        \"inference_input_size_multiplier\": [-1],\n        \"h\": None,\n        \"cell_type\": tune.choice(['LSTM', 'GRU']),\n        \"encoder_hidden_size\": tune.choice([16, 32, 64, 128]),\n        \"dilations\": tune.choice([ [[1, 2], [4, 8]], [[1, 2, 4, 8]] ]),\n        \"context_size\": tune.choice([5, 10, 50]),\n        \"decoder_hidden_size\": tune.choice([16, 32, 64, 128]),\n        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n        \"max_steps\": tune.choice([500, 1000]),\n        \"batch_size\": tune.choice([16, 32]),\n        \"loss\": None,\n        \"random_seed\": tune.randint(1, 20)\n    }\n\n    def __init__(self,\n                 h,\n                 loss=MAE(),\n                 valid_loss=None,\n                 config=None,\n                 search_alg=BasicVariantGenerator(random_state=1),\n                 num_samples=10,\n                 refit_with_val=False,\n                 cpus=cpu_count(),\n                 gpus=torch.cuda.device_count(),\n                 verbose=False,\n                 alias=None,\n                 backend='ray',\n                 callbacks=None):\n        \n        # Define search space, input/output sizes\n        if config is None:\n            config = self.get_default_config(h=h, backend=backend)         \n\n        super(AutoDilatedRNN, self).__init__(\n              cls_model=DilatedRNN,\n              h=h,\n              loss=loss,\n              valid_loss=valid_loss,\n              config=config,\n              search_alg=search_alg,\n              num_samples=num_samples, \n              refit_with_val=refit_with_val,\n              cpus=cpus,\n              gpus=gpus,\n              verbose=verbose,\n              alias=alias,\n              backend=backend,\n              callbacks=callbacks,\n         )\n        \n    @classmethod\n    def get_default_config(cls, h, backend, n_series=None):\n        config = cls.default_config.copy()\n        config['input_size'] = tune.choice([h*x \\\n                        for x in config['input_size_multiplier']])\n        config['inference_input_size'] = tune.choice([h*x \\\n                        for x in config['inference_input_size_multiplier']])\n        del config['input_size_multiplier'], config['inference_input_size_multiplier']\n        if backend == 'optuna':\n            config = cls._ray_config_to_optuna(config)         \n\n        return config\n```\n\n----------------------------------------\n\nTITLE: Training NHITS Model with NeuralForecast on M4 Data in Python\nDESCRIPTION: Configures and trains an NHITS (Neural Hierarchical Interpolation for Time Series) model using the NeuralForecast framework. Sets parameters including horizon, input size, and neural network architecture.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/17_transfer_learning.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nhorizon = 12\nstacks = 3\nmodels = [NHITS(input_size=5 * horizon,\n                h=horizon,\n                max_steps=100,\n                stack_types = stacks*['identity'],\n                n_blocks = stacks*[1],\n                mlp_units = [[256,256] for _ in range(stacks)],\n                n_pool_kernel_size = stacks*[1],\n                batch_size = 32,\n                scaler_type='standard',\n                n_freq_downsample=[12,4,1],\n                enable_progress_bar=False,\n                interpolation_mode=\"nearest\",\n               )]\nnf = NeuralForecast(models=models, freq='ME')\nnf.fit(df=Y_df)\n```\n\n----------------------------------------\n\nTITLE: Computing Conformity Scores for Prediction Intervals\nDESCRIPTION: This method computes conformity scores for prediction intervals. It performs cross-validation and calculates absolute errors for each model. Requires a minimum number of samples based on horizon and number of windows.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_25\n\nLANGUAGE: Python\nCODE:\n```\ndef _conformity_scores(\n    self,\n    df: DataFrame,\n    id_col: str, \n    time_col: str,\n    target_col: str,\n    static_df: Optional[DataFrame],\n) -> DataFrame:\n    \"\"\"Compute conformity scores.\n    \n    We need at least two cross validation errors to compute\n    quantiles for prediction intervals (`n_windows=2`, specified by self.prediction_intervals).\n    \n    The exception is raised by the PredictionIntervals data class.\n\n    df: DataFrame,\n    id_col: str,\n    time_col: str,\n    target_col: str,\n    static_df: Optional[DataFrame],\n    \"\"\"\n    if self.prediction_intervals is None:\n        raise AttributeError('Please rerun the `fit` method passing a valid prediction_interval setting to compute conformity scores')\n                \n    min_size = ufp.counts_by_id(df, id_col)['counts'].min()\n    min_samples = self.h * self.prediction_intervals.n_windows + 1\n    if min_size < min_samples:\n        raise ValueError(\n            \"Minimum required samples in each serie for the prediction intervals \"\n            f\"settings are: {min_samples}, shortest serie has: {min_size}. \"\n            \"Please reduce the number of windows, horizon or remove those series.\"\n        )\n    \n    self._add_level = True\n    cv_results = self.cross_validation(\n        df=df,\n        static_df=static_df,\n        n_windows=self.prediction_intervals.n_windows,\n        id_col=id_col,\n        time_col=time_col,\n        target_col=target_col,\n    )\n    self._add_level = False\n\n    kept = [time_col, id_col, 'cutoff']\n    # conformity score for each model\n    for model in self._get_model_names(add_level=True):\n        kept.append(model)\n\n        # compute absolute error for each model\n        abs_err = abs(cv_results[model] - cv_results[target_col])\n        cv_results = ufp.assign_columns(cv_results, model, abs_err)\n    dropped = list(set(cv_results.columns) - set(kept))\n    return ufp.drop_columns(cv_results, dropped)\n```\n\n----------------------------------------\n\nTITLE: Using AutoKAN with Different Configurations\nDESCRIPTION: Example of using the AutoKAN model with custom configuration and different backends. Demonstrates model initialization, fitting, prediction, and configuration with Optuna.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_62\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n# Use your own config or AutoKAN.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12)\nmodel = AutoKAN(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n# Optuna\nmodel = AutoKAN(h=12, config=None, backend='optuna')\n```\n\n----------------------------------------\n\nTITLE: Training Models Using Cross-Validation\nDESCRIPTION: Creates a NeuralForecast object with the models and performs cross-validation to train the models and generate forecasts on the test set.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/08_multivariate_tsmixer.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n%%capture\nnf = NeuralForecast(\n    models=models,\n    freq='15min',\n)\n\nY_hat_df = nf.cross_validation(\n    df=Y_df,\n    val_size=val_size,\n    test_size=test_size,\n    n_windows=None,\n)\n```\n\n----------------------------------------\n\nTITLE: TCN Cell Implementation\nDESCRIPTION: Implements a Temporal Convolutional Network cell that uses CustomConv1D modules with GELU activation and dropout for regularization.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.bitcn.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass TCNCell(nn.Module):\n    \"\"\"\n    Temporal Convolutional Network Cell, consisting of CustomConv1D modules.\n    \"\"\"    \n    def __init__(self, in_channels, out_channels, kernel_size, padding, dilation, mode, groups, dropout):\n        super().__init__()\n        self.conv1 = CustomConv1d(in_channels, out_channels, kernel_size, padding, dilation, mode, groups)\n        self.conv2 = CustomConv1d(out_channels, in_channels * 2, 1)\n        self.drop = nn.Dropout(dropout)\n        \n    def forward(self, x):\n        h_prev, out_prev = x\n        h = self.drop(F.gelu(self.conv1(h_prev)))\n        h_next, out_next = self.conv2(h).chunk(2, 1)\n        return (h_prev + h_next, out_prev + out_next)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Hyperparameter Tuning Results\nDESCRIPTION: Retrieving the results of the hyperparameter tuning process as a pandas DataFrame, which includes metrics and hyperparameter values for each configuration tested.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/04_hyperparameter_tuning.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nresults = nf.models[0].results.get_dataframe()\nresults.head()\n```\n\n----------------------------------------\n\nTITLE: Defining MLP Class for Neural Forecasting\nDESCRIPTION: This snippet defines the MLP class, which inherits from BaseModel. It includes the class attributes, initialization method with numerous parameters for customization, and the architecture setup for the multi-layer perceptron.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.mlp.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass MLP(BaseModel):\n    \"\"\" MLP\n\n    Simple Multi Layer Perceptron architecture (MLP). \n    This deep neural network has constant units through its layers, each with\n    ReLU non-linearities, it is trained using ADAM stochastic gradient descent.\n    The network accepts static, historic and future exogenous data, flattens \n    the inputs and learns fully connected relationships against the target variable.\n\n    **Parameters:**<br>\n    `h`: int, forecast horizon.<br>\n    `input_size`: int, considered autorregresive inputs (lags), y=[1,2,3,4] input_size=2 -> lags=[1,2].<br>\n    `stat_exog_list`: str list, static exogenous columns.<br>\n    `hist_exog_list`: str list, historic exogenous columns.<br>\n    `futr_exog_list`: str list, future exogenous columns.<br>\n    `exclude_insample_y`: bool=False, the model skips the autoregressive features y[t-input_size:t] if True.<br>\n    `num_layers`: int, number of layers for the MLP.<br>\n    `hidden_size`: int, number of units for each layer of the MLP.<br>\n    `loss`: PyTorch module, instantiated train loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `valid_loss`: PyTorch module=`loss`, instantiated valid loss class from [losses collection](https://nixtla.github.io/neuralforecast/losses.pytorch.html).<br>\n    `max_steps`: int=1000, maximum number of training steps.<br>\n    `learning_rate`: float=1e-3, Learning rate between (0, 1).<br>\n    `num_lr_decays`: int=-1, Number of learning rate decays, evenly distributed across max_steps.<br>\n    `early_stop_patience_steps`: int=-1, Number of validation iterations before early stopping.<br>\n    `val_check_steps`: int=100, Number of training steps between every validation loss check.<br>\n    `batch_size`: int=32, number of different series in each batch.<br>\n    `valid_batch_size`: int=None, number of different series in each validation and test batch, if None uses batch_size.<br>\n    `windows_batch_size`: int=1024, number of windows to sample in each training batch, default uses all.<br>\n    `inference_windows_batch_size`: int=-1, number of windows to sample in each inference batch, -1 uses all.<br>\n    `start_padding_enabled`: bool=False, if True, the model will pad the time series with zeros at the beginning, by input size.<br>\n    `step_size`: int=1, step size between each window of temporal data.<br>\n    `scaler_type`: str='identity', type of scaler for temporal inputs normalization see [temporal scalers](https://nixtla.github.io/neuralforecast/common.scalers.html).<br>\n    `random_seed`: int=1, random_seed for pytorch initializer and numpy generators.<br>\n    `drop_last_loader`: bool=False, if True `TimeSeriesDataLoader` drops last non-full batch.<br>\n    `alias`: str, optional,  Custom name of the model.<br>\n    `optimizer`: Subclass of 'torch.optim.Optimizer', optional, user specified optimizer instead of the default choice (Adam).<br>\n    `optimizer_kwargs`: dict, optional, list of parameters used by the user specified `optimizer`.<br>\n    `lr_scheduler`: Subclass of 'torch.optim.lr_scheduler.LRScheduler', optional, user specified lr_scheduler instead of the default choice (StepLR).<br>\n    `lr_scheduler_kwargs`: dict, optional, list of parameters used by the user specified `lr_scheduler`.<br>    \n    `dataloader_kwargs`: dict, optional, list of parameters passed into the PyTorch Lightning dataloader by the `TimeSeriesDataLoader`. <br>\n    `**trainer_kwargs`: int,  keyword trainer arguments inherited from [PyTorch Lighning's trainer](https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.trainer.trainer.Trainer.html?highlight=trainer).<br>    \n    \"\"\"\n    # Class attributes\n    EXOGENOUS_FUTR = True\n    EXOGENOUS_HIST = True\n    EXOGENOUS_STAT = True\n    MULTIVARIATE = False    # If the model produces multivariate forecasts (True) or univariate (False)\n    RECURRENT = False       # If the model produces forecasts recursively (True) or direct (False)\n\n    def __init__(self,\n                 h,\n                 input_size,\n                 stat_exog_list = None,\n                 hist_exog_list = None,\n                 futr_exog_list = None,\n                 exclude_insample_y = False,\n                 num_layers = 2,\n                 hidden_size = 1024,\n                 loss = MAE(),\n                 valid_loss = None,\n                 max_steps: int = 1000,\n                 learning_rate: float = 1e-3,\n                 num_lr_decays: int = -1,\n                 early_stop_patience_steps: int =-1,\n                 val_check_steps: int = 100,\n                 batch_size: int = 32,\n                 valid_batch_size: Optional[int] = None,\n                 windows_batch_size = 1024,\n                 inference_windows_batch_size = -1,\n                 start_padding_enabled = False,\n                 step_size: int = 1,\n                 scaler_type: str = 'identity',\n                 random_seed: int = 1,\n                 drop_last_loader: bool = False,\n                 alias: Optional[str] = None,\n                 optimizer = None,\n                 optimizer_kwargs = None,\n                 lr_scheduler = None,\n                 lr_scheduler_kwargs = None,\n                 dataloader_kwargs = None,\n                 **trainer_kwargs):\n\n        # Inherit BaseWindows class\n        super(MLP, self).__init__(h=h,\n                                  input_size=input_size,\n                                  stat_exog_list=stat_exog_list,\n                                  hist_exog_list=hist_exog_list,\n                                  futr_exog_list=futr_exog_list,\n                                  exclude_insample_y = exclude_insample_y,\n                                  loss=loss,\n                                  valid_loss=valid_loss,\n                                  max_steps=max_steps,\n                                  learning_rate=learning_rate,\n                                  num_lr_decays=num_lr_decays,\n                                  early_stop_patience_steps=early_stop_patience_steps,\n                                  val_check_steps=val_check_steps,\n                                  batch_size=batch_size,\n                                  valid_batch_size=valid_batch_size,\n                                  windows_batch_size=windows_batch_size,\n                                  inference_windows_batch_size=inference_windows_batch_size,\n                                  start_padding_enabled=start_padding_enabled,\n                                  step_size=step_size,\n                                  scaler_type=scaler_type,\n                                  random_seed=random_seed,\n                                  drop_last_loader=drop_last_loader,\n                                  alias=alias,\n                                  optimizer=optimizer,\n                                  optimizer_kwargs=optimizer_kwargs,\n                                  lr_scheduler=lr_scheduler,\n                                  lr_scheduler_kwargs=lr_scheduler_kwargs,\n                                  dataloader_kwargs=dataloader_kwargs,\n                                  **trainer_kwargs)\n\n        # Architecture\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n\n        input_size_first_layer = input_size + self.hist_exog_size * input_size + \\\n                                 self.futr_exog_size*(input_size + h) + self.stat_exog_size\n\n        # MultiLayer Perceptron\n        layers = [nn.Linear(in_features=input_size_first_layer, out_features=hidden_size)]\n        for i in range(num_layers - 1):\n            layers += [nn.Linear(in_features=hidden_size, out_features=hidden_size)]\n        self.mlp = nn.ModuleList(layers)\n\n        # Adapter with Loss dependent dimensions\n        self.out = nn.Linear(in_features=hidden_size, \n                             out_features=h * self.loss.outputsize_multiplier)\n```\n\n----------------------------------------\n\nTITLE: Implementing MinMax1 Scaler for Neural Forecasting in Python\nDESCRIPTION: Calculates MinMax1 scaling statistics for temporal features, ensuring the range is between [-1,1]. It handles masked values and includes safeguards against division by zero.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.scalers.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef minmax1_statistics(x, mask, eps=1e-6, dim=-1):\n    mask = mask.clone()\n    mask[mask==0] = torch.inf\n    mask[mask==1] = 0\n    x_max = torch.max(torch.nan_to_num(x-mask,nan=-torch.inf), dim=dim, keepdim=True)[0]\n    x_min = torch.min(torch.nan_to_num(x+mask,nan=torch.inf), dim=dim, keepdim=True)[0]\n    x_max = x_max.type(x.dtype)\n    x_min = x_min.type(x.dtype)\n    \n    x_range = x_max - x_min\n    x_range[x_range==0] = 1.0\n    x_range = x_range + eps\n    return x_min, x_range\n```\n\n----------------------------------------\n\nTITLE: Implementing Inception Block for TimesNet\nDESCRIPTION: Defines the Inception_Block_V1 class, a key component of the TimesNet model. It creates multiple convolutional layers with different kernel sizes to capture various temporal patterns.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timesnet.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Inception_Block_V1(nn.Module):\n    \"\"\"\n    Inception_Block_V1\n    \"\"\"    \n    def __init__(self, in_channels, out_channels, num_kernels=6, init_weight=True):\n        super(Inception_Block_V1, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.num_kernels = num_kernels\n        kernels = []\n        for i in range(self.num_kernels):\n            kernels.append(nn.Conv2d(in_channels, out_channels, kernel_size=2 * i + 1, padding=i))\n        self.kernels = nn.ModuleList(kernels)\n        if init_weight:\n            self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        res_list = []\n        for i in range(self.num_kernels):\n            res_list.append(self.kernels[i](x))\n        res = torch.stack(res_list, dim=-1).mean(-1)\n        return res\n```\n\n----------------------------------------\n\nTITLE: Visualizing GMM Distributions with Matplotlib in Python\nDESCRIPTION: This snippet creates two visualizations: a histogram of the sampled distributions for different forecast horizons, and a plot of the probabilistic predictions over time. It uses Matplotlib to generate these plots.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_58\n\nLANGUAGE: Python\nCODE:\n```\n# Plot synthethic data\nx_plot = range(quants.shape[1]) # H length\ny_plot_hat = quants[0,:,0,:]  # Filter N,G,T -> H,Q\nsamples_hat = samples[0,:,0,:]  # Filter N,G,T -> H,num_samples\n\n# Kernel density plot for single forecast horizon \\tau = t+1\nfig, ax = plt.subplots(figsize=(3.7, 2.9))\n\nax.hist(samples_hat[0,:], alpha=0.5, bins=50,\n        label=r'Horizon $\\tau+1$')\nax.hist(samples_hat[1,:], alpha=0.5, bins=50,\n        label=r'Horizon $\\tau+2$')\nax.set(xlabel='Y values', ylabel='Probability')\nplt.title('Single horizon Distributions')\nplt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\nplt.grid()\nplt.show()\nplt.close()\n\n# Plot simulated trajectory\nfig, ax = plt.subplots(figsize=(3.7, 2.9))\nplt.plot(x_plot, y_plot_hat[:,2], color='black', label='median [q50]')\nplt.fill_between(x_plot,\n                 y1=y_plot_hat[:,1], y2=y_plot_hat[:,3],\n                 facecolor='blue', alpha=0.4, label='[p25-p75]')\nplt.fill_between(x_plot,\n                 y1=y_plot_hat[:,0], y2=y_plot_hat[:,4],\n                 facecolor='blue', alpha=0.2, label='[p1-p99]')\nax.set(xlabel='Horizon', ylabel='Y values')\nplt.title('GMM Probabilistic Predictions')\nplt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\nplt.grid()\nplt.show()\nplt.close()\n```\n\n----------------------------------------\n\nTITLE: Performing Cross-Validation for Electricity Demand Forecasting\nDESCRIPTION: Executes cross-validation on the ERCOT data using the NeuralForecast object, producing daily forecasts for September with a 24-hour step size.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/use-cases/electricity_peak_forecasting.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n%%capture\ncrossvalidation_df = nf.cross_validation(\n    df=Y_df,\n    step_size=24,\n    n_windows=30\n  )\n```\n\n----------------------------------------\n\nTITLE: Training NHITS Model\nDESCRIPTION: Configures and trains the NHITS model with specified hyperparameters including forecast horizon, input size, and network architecture.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/05_predictInsample.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nhorizon = 12\n\n# Try different hyperparmeters to improve accuracy.\nmodels = [NHITS(h=horizon,                      # Forecast horizon\n                input_size=2 * horizon,         # Length of input sequence\n                max_steps=100,                  # Number of steps to train\n                n_freq_downsample=[2, 1, 1],    # Downsampling factors for each stack output\n                mlp_units = 3 * [[1024, 1024]],\n               ) # Number of units in each block.\n          ]\nnf = NeuralForecast(models=models, freq='ME')\nnf.fit(df=Y_df, val_size=horizon)\n```\n\n----------------------------------------\n\nTITLE: Fitting Neural Forecasting Model\nDESCRIPTION: This method fits a neural forecasting model using the provided configuration and dataset. It handles both the initial fitting and refitting with validation data.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_auto.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ndef fit(self, dataset, val_size=0, test_size=0, random_seed=None, distributed_config=None):\n    search_alg = deepcopy(self.search_alg)\n    val_size = val_size if val_size > 0 else self.h\n    if self.backend == 'ray':\n        if distributed_config is not None:\n            raise ValueError('distributed training is not supported for the ray backend.')\n        results = self._tune_model(\n            cls_model=self.cls_model,\n            dataset=dataset,\n            val_size=val_size,\n            test_size=test_size, \n            cpus=self.cpus,\n            gpus=self.gpus,\n            verbose=self.verbose,\n            num_samples=self.num_samples, \n            search_alg=search_alg, \n            config=self.config,\n        )            \n        best_config = results.get_best_result().config            \n    else:\n        results = self._optuna_tune_model(\n            cls_model=self.cls_model,\n            dataset=dataset,\n            val_size=val_size, \n            test_size=test_size, \n            verbose=self.verbose,\n            num_samples=self.num_samples, \n            search_alg=search_alg, \n            config=self.config,\n            distributed_config=distributed_config,\n        )\n        best_config = results.best_trial.user_attrs['ALL_PARAMS']\n    self.model = self._fit_model(\n        cls_model=self.cls_model,\n        config=best_config,\n        dataset=dataset,\n        val_size=val_size * self.refit_with_val,\n        test_size=test_size,\n        distributed_config=distributed_config,\n    )\n    self.results = results\n\n     # Added attributes for compatibility with NeuralForecast core\n    self.futr_exog_list = self.model.futr_exog_list\n    self.hist_exog_list = self.model.hist_exog_list\n    self.stat_exog_list = self.model.stat_exog_list\n    return self\n```\n\n----------------------------------------\n\nTITLE: Implementing Various Embedding Layers in PyTorch\nDESCRIPTION: Implements different types of embeddings including positional, token, time feature, and temporal embeddings for transformer models.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.modules.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclass PositionalEmbedding(nn.Module):\n    def __init__(self, hidden_size, max_len=5000):\n        super(PositionalEmbedding, self).__init__()\n        # Compute the positional encodings once in log space.\n        pe = torch.zeros(max_len, hidden_size).float()\n        pe.require_grad = False\n\n        position = torch.arange(0, max_len).float().unsqueeze(1)\n        div_term = (torch.arange(0, hidden_size, 2).float() * -(math.log(10000.0) / hidden_size)).exp()\n\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        return self.pe[:, :x.size(1)]\n\nclass TokenEmbedding(nn.Module):\n    def __init__(self, c_in, hidden_size):\n        super(TokenEmbedding, self).__init__()\n        padding = 1 if torch.__version__ >= '1.5.0' else 2\n        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=hidden_size,\n                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n        for m in self.modules():\n            if isinstance(m, nn.Conv1d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\n\n    def forward(self, x):\n        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n        return x\n\nclass TimeFeatureEmbedding(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(TimeFeatureEmbedding, self).__init__()\n        self.embed = nn.Linear(input_size, hidden_size, bias=False)\n\n    def forward(self, x):\n        return self.embed(x)\n    \nclass FixedEmbedding(nn.Module):\n    def __init__(self, c_in, d_model):\n        super(FixedEmbedding, self).__init__()\n\n        w = torch.zeros(c_in, d_model, dtype=torch.float32, requires_grad=False)\n        position = torch.arange(0, c_in, dtype=torch.float32).unsqueeze(1)\n        div_term = (torch.arange(0, d_model, 2).float()\n                    * -(math.log(10000.0) / d_model)).exp()\n\n        w[:, 0::2] = torch.sin(position * div_term)\n        w[:, 1::2] = torch.cos(position * div_term)\n\n        self.emb = nn.Embedding(c_in, d_model)\n        self.emb.weight = nn.Parameter(w, requires_grad=False)\n\n    def forward(self, x):\n        return self.emb(x).detach()\n    \nclass TemporalEmbedding(nn.Module):\n    def __init__(self, d_model, embed_type='fixed', freq='h'):\n        super(TemporalEmbedding, self).__init__()\n\n        minute_size = 4\n        hour_size = 24\n        weekday_size = 7\n        day_size = 32\n        month_size = 13\n\n        Embed = FixedEmbedding if embed_type == 'fixed' else nn.Embedding\n        if freq == 't':\n            self.minute_embed = Embed(minute_size, d_model)\n        self.hour_embed = Embed(hour_size, d_model)\n```\n\n----------------------------------------\n\nTITLE: DistributionLoss Class Definition in PyTorch\nDESCRIPTION: This class represents a wrapper for various probability distributions in PyTorch, allowing them to be used as loss functions in NeuralForecast models. It supports multiple distribution types and implements likelihood optimization objectives.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nclass DistributionLoss(torch.nn.Module):\n    \"\"\" DistributionLoss\n\n    This PyTorch module wraps the `torch.distribution` classes allowing it to \n    interact with NeuralForecast models modularly. It shares the negative \n    log-likelihood as the optimization objective and a sample method to \n    generate empirically the quantiles defined by the `level` list.\n\n    Additionally, it implements a distribution transformation that factorizes the\n    scale-dependent likelihood parameters into a base scale and a multiplier \n    efficiently learnable within the network's non-linearities operating ranges.\n\n    Available distributions:<br>\n    - Poisson<br>\n    - Normal<br>\n    - StudentT<br>\n    - NegativeBinomial<br>\n    - Tweedie<br>\n    - Bernoulli (Temporal Classifiers)<br>\n    - ISQF (Incremental Spline Quantile Function) \n\n    **Parameters:**<br>\n    `distribution`: str, identifier of a torch.distributions.Distribution class.<br>\n    `level`: float list [0,100], confidence levels for prediction intervals.<br>\n    `quantiles`: float list [0,1], alternative to level list, target quantiles.<br>\n    `num_samples`: int=500, number of samples for the empirical quantiles.<br>\n    `return_params`: bool=False, wether or not return the Distribution parameters.<br>\n    `horizon_weight`: Tensor of size h, weight for each timestamp of the forecasting window.<br><br>\n\n    **References:**<br>\n    - [PyTorch Probability Distributions Package: StudentT.](https://pytorch.org/docs/stable/distributions.html#studentt)<br>\n    - [David Salinas, Valentin Flunkert, Jan Gasthaus, Tim Januschowski (2020).\n       \"DeepAR: Probabilistic forecasting with autoregressive recurrent networks\". International Journal of Forecasting.](https://www.sciencedirect.com/science/article/pii/S0169207019301888)<br>\n    - [Park, Youngsuk, Danielle Maddix, Franois-Xavier Aubet, Kelvin Kan, Jan Gasthaus, and Yuyang Wang (2022). \"Learning Quantile Functions without Quantile Crossing for Distribution-free Time Series Forecasting\".](https://proceedings.mlr.press/v151/park22a.html)       \n\n    \"\"\"\n    def __init__(self, distribution, level=[80, 90], quantiles=None,\n```\n\n----------------------------------------\n\nTITLE: Implementing JacobiKAN Layer for RMoK\nDESCRIPTION: Defines the JacobiKANLayer class, which implements the Jacobi Kolmogorov-Arnold Network layer for the RMoK model. It uses Jacobi polynomials for approximation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.rmok.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass JacobiKANLayer(nn.Module):\n    \"\"\"\n    https://github.com/SpaceLearner/JacobiKAN/blob/main/JacobiKANLayer.py\n    \"\"\"\n\n    def __init__(self, input_dim, output_dim, degree, a=1.0, b=1.0):\n        super(JacobiKANLayer, self).__init__()\n        self.inputdim = input_dim\n        self.outdim = output_dim\n        self.a = a\n        self.b = b\n        self.degree = degree\n\n        self.jacobi_coeffs = nn.Parameter(torch.empty(input_dim, output_dim, degree + 1))\n\n        nn.init.normal_(self.jacobi_coeffs, mean=0.0, std=1 / (input_dim * (degree + 1)))\n\n    def forward(self, x):\n        x = torch.reshape(x, (-1, self.inputdim))  # shape = (batch_size, inputdim)\n        # Since Jacobian polynomial is defined in [-1, 1]\n        # We need to normalize x to [-1, 1] using tanh\n        x = torch.tanh(x)\n        # Initialize Jacobian polynomial tensors\n        jacobi = torch.ones(x.shape[0], self.inputdim, self.degree + 1, device=x.device)\n        if self.degree > 0:  ## degree = 0: jacobi[:, :, 0] = 1 (already initialized) ; degree = 1: jacobi[:, :, 1] = x ; d\n            jacobi[:, :, 1] = ((self.a - self.b) + (self.a + self.b + 2) * x) / 2\n        for i in range(2, self.degree + 1):\n            theta_k = (2 * i + self.a + self.b) * (2 * i + self.a + self.b - 1) / (2 * i * (i + self.a + self.b))\n            theta_k1 = (2 * i + self.a + self.b - 1) * (self.a * self.a - self.b * self.b) / (\n                    2 * i * (i + self.a + self.b) * (2 * i + self.a + self.b - 2))\n            theta_k2 = (i + self.a - 1) * (i + self.b - 1) * (2 * i + self.a + self.b) / (\n                    i * (i + self.a + self.b) * (2 * i + self.a + self.b - 2))\n            jacobi[:, :, i] = (theta_k * x + theta_k1) * jacobi[:, :, i - 1].clone() - theta_k2 * jacobi[:, :,\n                                                                                                  i - 2].clone()  # 2 * x * jacobi[:, :, i - 1].clone() - jacobi[:, :, i - 2].clone()\n        # Compute the Jacobian interpolation\n        y = torch.einsum('bid,iod->bo', jacobi, self.jacobi_coeffs)  # shape = (batch_size, outdim)\n        y = y.view(-1, self.outdim)\n        return y\n```\n\n----------------------------------------\n\nTITLE: Using AutoBiTCN for Model Training and Prediction in Python\nDESCRIPTION: This snippet demonstrates the usage of the `AutoBiTCN` class. It first defines a minimal custom configuration dictionary. It then instantiates `AutoBiTCN` with this config, specifying the forecast horizon `h`, number of samples for tuning `num_samples`, and `cpus`. The model is fitted to a `dataset` and used to generate predictions. Finally, it shows how to instantiate the model to use the Optuna backend by setting `backend='optuna'` and `config=None` to utilize the default Optuna-compatible search space.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_32\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n# Use your own config or AutoBiTCN.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=8)\nmodel = AutoBiTCN(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n# Optuna\nmodel = AutoBiTCN(h=12, config=None, backend='optuna')\n```\n\n----------------------------------------\n\nTITLE: Training NHITS Model with Stack Specialization\nDESCRIPTION: Configures and trains an NHITS model with specific stack types and hyperparameters to learn the time series components.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/14_interpretable_decompositions.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n%%capture\nmodels = [NHITS(h=horizon,                           # Forecast horizon\n                input_size=2 * horizon,              # Length of input sequence\n                loss=HuberLoss(),                    # Robust Huber Loss\n                max_steps=1000,                      # Number of steps to train\n                dropout_prob_theta=0.5,\n                interpolation_mode='linear',\n                stack_types=['identity']*2,\n                n_blocks=[1, 1],\n                mlp_units=[[64, 64],[64, 64]],\n                n_freq_downsample=[10, 1],           # Inverse expressivity ratios for NHITS' stacks specialization\n                val_check_steps=10,                  # Frequency of validation signal (affects early stopping)\n              )\n          ]\nnf = NeuralForecast(models=models, freq=1)\nnf.fit(df=Y_train_df)\n```\n\n----------------------------------------\n\nTITLE: Initializing AutoGRU Class with Ray Tune Integration\nDESCRIPTION: Implementation of AutoGRU class that extends BaseAuto for automated GRU model configuration and training. Shares similar structure with AutoLSTM including default hyperparameter search space and backend support.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nclass AutoGRU(BaseAuto):\n\n    default_config = {\n        \"input_size_multiplier\": [-1, 4, 16, 64],\n        \"inference_input_size_multiplier\": [-1],\n        \"h\": None,\n        \"encoder_hidden_size\": tune.choice([16, 32, 64, 128]),\n        \"encoder_n_layers\": tune.randint(1, 4),\n        \"context_size\": tune.choice([5, 10, 50]),\n        \"decoder_hidden_size\": tune.choice([16, 32, 64, 128]),\n        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n        \"max_steps\": tune.choice([500, 1000]),\n        \"batch_size\": tune.choice([16, 32]),\n        \"loss\": None,\n        \"random_seed\": tune.randint(1, 20)\n    }\n```\n\n----------------------------------------\n\nTITLE: Implementing AutoDLinear Class for Linear Decomposition Time Series Models\nDESCRIPTION: Complete implementation of the AutoDLinear class which extends BaseAuto to facilitate hyperparameter tuning for DLinear models. It defines default search space for parameters like window size, learning rate, and scaling methods.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_45\n\nLANGUAGE: python\nCODE:\n```\nclass AutoDLinear(BaseAuto):\n\n    default_config = {\n       \"input_size_multiplier\": [1, 2, 3, 4, 5],\n       \"h\": None,\n       \"moving_avg_window\": tune.choice([11, 25, 51]),\n       \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n       \"scaler_type\": tune.choice([None, 'robust', 'standard']),\n       \"max_steps\": tune.quniform(lower=500, upper=1500, q=100),\n       \"batch_size\": tune.choice([32, 64, 128, 256]),\n       \"windows_batch_size\": tune.choice([128, 256, 512, 1024]),\n       \"loss\": None,\n       \"random_seed\": tune.randint(lower=1, upper=20),\n    }\n\n    def __init__(self,\n                 h,\n                 loss=MAE(),\n                 valid_loss=None,\n                 config=None, \n                 search_alg=BasicVariantGenerator(random_state=1),\n                 num_samples=10,\n                 refit_with_val=False,\n                 cpus=cpu_count(),\n                 gpus=torch.cuda.device_count(),\n                 verbose=False,\n                 alias=None,\n                 backend='ray',\n                 callbacks=None,\n                ):\n\n        # Define search space, input/output sizes\n        if config is None:\n            config = self.get_default_config(h=h, backend=backend)                   \n\n        super(AutoDLinear, self).__init__(\n              cls_model=DLinear, \n              h=h,\n              loss=loss,\n              valid_loss=valid_loss,\n              config=config,\n              search_alg=search_alg,\n              num_samples=num_samples,\n              refit_with_val=refit_with_val,\n              cpus=cpus,\n              gpus=gpus,\n              verbose=verbose,\n              alias=alias,\n              backend=backend,\n              callbacks=callbacks,\n        )\n\n    @classmethod\n    def get_default_config(cls, h, backend, n_series=None):\n        config = cls.default_config.copy()\n        config['input_size'] = tune.choice([h*x \\\n                        for x in config['input_size_multiplier']])\n        config['step_size'] = tune.choice([1, h])        \n        del config['input_size_multiplier']\n        if backend == 'optuna':\n            config = cls._ray_config_to_optuna(config)         \n\n        return config           \n```\n\n----------------------------------------\n\nTITLE: Adding Conformal Distribution Intervals in Python\nDESCRIPTION: Implements the `add_conformal_distribution_intervals` function to calculate prediction intervals using the conformal distribution method. It takes model forecasts (`model_fcsts`), conformal scores (`cs_df`), model name, number of windows, series count, horizon, and desired levels or quantiles. It computes quantiles based on forecast errors, constructs the intervals, and returns the forecasts augmented with interval bounds and corresponding column names. Either `level` or `quantiles` must be provided.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/utils.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n#| export\ndef add_conformal_distribution_intervals(\n    model_fcsts: np.array, \n    cs_df: DFType,\n    model: str,\n    cs_n_windows: int,\n    n_series: int,\n    horizon: int,\n    level: Optional[List[Union[int, float]]] = None,\n    quantiles: Optional[List[float]] = None,\n) -> Tuple[np.array, List[str]]:\n    \"\"\"\n    Adds conformal intervals to a `fcst_df` based on conformal scores `cs_df`.\n    `level` should be already sorted. This strategy creates forecasts paths\n    based on errors and calculate quantiles using those paths.\n    \"\"\"\n    assert level is not None or quantiles is not None, \"Either level or quantiles must be provided\"\n    \n    if quantiles is None and level is not None:\n        alphas = [100 - lv for lv in level]\n        cuts = [alpha / 200 for alpha in reversed(alphas)]\n        cuts.extend(1 - alpha / 200 for alpha in alphas)\n    elif quantiles is not None:\n        cuts = quantiles\n    \n    scores = cs_df[model].to_numpy().reshape(n_series, cs_n_windows, horizon)\n    scores = scores.transpose(1, 0, 2)\n    # restrict scores to horizon\n    scores = scores[:,:,:horizon]\n    mean = model_fcsts.reshape(1, n_series, -1)\n    scores = np.vstack([mean - scores, mean + scores])\n    scores_quantiles = np.quantile(\n        scores,\n        cuts,\n        axis=0,\n    )\n    scores_quantiles = scores_quantiles.reshape(len(cuts), -1).T\n    if quantiles is None and level is not None:\n        lo_cols = [f\"{model}-lo-{lv}\" for lv in reversed(level)]\n        hi_cols = [f\"{model}-hi-{lv}\" for lv in level]\n        out_cols = lo_cols + hi_cols\n    elif quantiles is not None:\n        out_cols = [f\"{model}-ql{q}\" for q in quantiles]\n\n    fcsts_with_intervals = np.hstack([model_fcsts, scores_quantiles])\n\n    return fcsts_with_intervals, out_cols\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch Optimizer and Learning Rate Scheduler\nDESCRIPTION: Sets up the model's optimizer and learning rate scheduler with customizable parameters. Handles both user-specified optimizer/scheduler and fallback defaults with appropriate warnings.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef configure_optimizers(self):\n    if self.optimizer:\n        optimizer_signature = inspect.signature(self.optimizer)\n        optimizer_kwargs = deepcopy(self.optimizer_kwargs)\n        if 'lr' in optimizer_signature.parameters:\n            if 'lr' in optimizer_kwargs:\n                warnings.warn(\"ignoring learning rate passed in optimizer_kwargs, using the model's learning rate\")\n            optimizer_kwargs['lr'] = self.learning_rate\n        optimizer = self.optimizer(params=self.parameters(), **optimizer_kwargs)\n    else:\n        if self.optimizer_kwargs:\n            warnings.warn(\n                \"ignoring optimizer_kwargs as the optimizer is not specified\"\n            )            \n        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n    \n    lr_scheduler = {'frequency': 1, 'interval': 'step'}\n    if self.lr_scheduler:\n        lr_scheduler_signature = inspect.signature(self.lr_scheduler)\n        lr_scheduler_kwargs = deepcopy(self.lr_scheduler_kwargs)\n        if 'optimizer' in lr_scheduler_signature.parameters:\n            if 'optimizer' in lr_scheduler_kwargs:\n                warnings.warn(\"ignoring optimizer passed in lr_scheduler_kwargs, using the model's optimizer\")\n                del lr_scheduler_kwargs['optimizer']\n        lr_scheduler['scheduler'] = self.lr_scheduler(optimizer=optimizer, **lr_scheduler_kwargs)\n    else:\n        if self.lr_scheduler_kwargs:\n            warnings.warn(\n                \"ignoring lr_scheduler_kwargs as the lr_scheduler is not specified\"\n            )            \n        lr_scheduler['scheduler'] = torch.optim.lr_scheduler.StepLR(\n            optimizer=optimizer, step_size=self.lr_decay_steps, gamma=0.5\n        )\n    return {'optimizer': optimizer, 'lr_scheduler': lr_scheduler}\n```\n\n----------------------------------------\n\nTITLE: DFT Series Decomposition Implementation\nDESCRIPTION: Implements Discrete Fourier Transform based series decomposition to separate seasonal and trend components from time series data.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timemixer.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass DFT_series_decomp(nn.Module):\n    \"\"\"\n    Series decomposition block\n    \"\"\"\n\n    def __init__(self, top_k):\n        super(DFT_series_decomp, self).__init__()\n        self.top_k = top_k\n\n    def forward(self, x):\n        xf = torch.fft.rfft(x)\n        freq = abs(xf)\n        freq[0] = 0\n        top_k_freq, top_list = torch.topk(freq, self.top_k)\n        xf[freq <= top_k_freq.min()] = 0\n        x_season = torch.fft.irfft(xf)\n        x_trend = x - x_season\n        return x_season, x_trend\n```\n\n----------------------------------------\n\nTITLE: Cross-Validation Forecasting with NeuralForecast\nDESCRIPTION: This snippet shows how to use the cross_validation method of NeuralForecast to forecast multiple historic values. It uses the same TSMixer model and AirPassengers dataset, and visualizes the results for one airline.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tsmixer.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfcst = NeuralForecast(models=[model], freq='M')\nforecasts = fcst.cross_validation(df=AirPassengersPanel, static_df=AirPassengersStatic, n_windows=2, step_size=12)\n\n# Plot predictions\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nY_hat_df = forecasts.loc['Airline1']\nY_df = AirPassengersPanel[AirPassengersPanel['unique_id']=='Airline1']\n\nplt.plot(Y_df['ds'], Y_df['y'], c='black', label='True')\nplt.plot(Y_hat_df['ds'], Y_hat_df['TSMixer-median'], c='blue', label='Forecast')\nax.set_title('AirPassengers Forecast', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Year', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-Layer Perceptron Module in PyTorch\nDESCRIPTION: Defines a customizable MLP class with configurable hidden layers, activation functions, and dropout rates for general-purpose neural network applications.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.modules.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass MLP(nn.Module):\n    \"\"\"Multi-Layer Perceptron Class\n\n    **Parameters:**<br>\n    `in_features`: int, dimension of input.<br>\n    `out_features`: int, dimension of output.<br>\n    `activation`: str, activation function to use.<br>\n    `hidden_size`: int, dimension of hidden layers.<br>\n    `num_layers`: int, number of hidden layers.<br>\n    `dropout`: float, dropout rate.<br>\n    \"\"\"\n    def __init__(self, in_features, out_features, activation, hidden_size, num_layers, dropout):\n        super().__init__()\n        assert activation in ACTIVATIONS, f'{activation} is not in {ACTIVATIONS}'\n        \n        self.activation = getattr(nn, activation)()\n\n        # MultiLayer Perceptron\n        # Input layer\n        layers = [nn.Linear(in_features=in_features, out_features=hidden_size),\n                  self.activation,\n                  nn.Dropout(dropout)]\n        # Hidden layers\n        for i in range(num_layers - 2):\n            layers += [nn.Linear(in_features=hidden_size, out_features=hidden_size),\n                       self.activation,\n                       nn.Dropout(dropout)]\n        # Output layer\n        layers += [nn.Linear(in_features=hidden_size, out_features=out_features)]\n\n        # Store in layers as ModuleList\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.layers(x)\n```\n\n----------------------------------------\n\nTITLE: Implementing Quantile Loss for Probabilistic Forecasting in Python\nDESCRIPTION: This function computes the quantile loss between actual and predicted values, which is useful for probabilistic forecasting. It allows for asymmetric weighting of under- and over-estimations based on the specified quantile level.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.numpy.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef quantile_loss(y: np.ndarray, y_hat: np.ndarray, q: float = 0.5, \n                  weights: Optional[np.ndarray] = None,\n                  axis: Optional[int] = None) -> Union[float, np.ndarray]:\n    \"\"\" Quantile Loss\n\n    Computes the quantile loss between `y` and `y_hat`.\n    QL measures the deviation of a quantile forecast.\n    By weighting the absolute deviation in a non symmetric way, the\n    loss pays more attention to under or over estimation.\n    A common value for q is 0.5 for the deviation from the median (Pinball loss).\n\n    $$ \\mathrm{QL}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{(q)}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\Big( (1-q)\\,( \\hat{y}^{(q)}_{\\tau} - y_{\\tau} )_{+} + q\\,( y_{\\tau} - \\hat{y}^{(q)}_{\\tau} )_{+} \\Big) $$\n\n    **Parameters:**<br>\n    `y`: numpy array, Actual values.<br>\n    `y_hat`: numpy array, Predicted values.<br>\n    `q`: float, between 0 and 1. The slope of the quantile loss, in the context of quantile regression, the q determines the conditional quantile level.<br>\n    `mask`: numpy array, Specifies date stamps per serie to consider in loss.<br>\n\n    **Returns:**<br>\n    `quantile_loss`: numpy array, (single value).\n    \n    **References:**<br>\n    [Roger Koenker and Gilbert Bassett, Jr., \"Regression Quantiles\".](https://www.jstor.org/stable/1913643)\n    \"\"\"\n    _metric_protections(y, y_hat, weights)\n\n    delta_y = y - y_hat\n    loss = np.maximum(q * delta_y, (q - 1) * delta_y)\n\n    if weights is not None:\n        quantile_loss = np.average(loss[~np.isnan(loss)], \n                             weights=weights[~np.isnan(loss)],\n                             axis=axis)\n    else:\n        quantile_loss = np.nanmean(loss, axis=axis)\n        \n    return quantile_loss\n```\n\n----------------------------------------\n\nTITLE: Configuring NHITS with HINT Reconciliation in Python\nDESCRIPTION: This snippet defines a configuration dictionary for NHITS (Neural Hierarchical Interpolation for Time Series Forecasting) with HINT reconciliation. It sets up hyperparameters for the model, including learning rate, input size, batch sizes, and architecture details.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_102\n\nLANGUAGE: python\nCODE:\n```\nnhits_config = {\n       \"learning_rate\": tune.choice([1e-3]),                                     # Initial Learning rate\n       \"max_steps\": tune.choice([1]),                                            # Number of SGD steps\n       \"val_check_steps\": tune.choice([1]),                                      # Number of steps between validation\n       \"input_size\": tune.choice([5 * 12]),                                      # input_size = multiplier * horizon\n       \"batch_size\": tune.choice([7]),                                           # Number of series in windows\n       \"windows_batch_size\": tune.choice([256]),                                 # Number of windows in batch\n       \"n_pool_kernel_size\": tune.choice([[2, 2, 2], [16, 8, 1]]),               # MaxPool's Kernelsize\n       \"n_freq_downsample\": tune.choice([[168, 24, 1], [24, 12, 1], [1, 1, 1]]), # Interpolation expressivity ratios\n       \"activation\": tune.choice(['ReLU']),                                      # Type of non-linear activation\n       \"n_blocks\":  tune.choice([[1, 1, 1]]),                                    # Blocks per each 3 stacks\n       \"mlp_units\":  tune.choice([[[512, 512], [512, 512], [512, 512]]]),        # 2 512-Layers per block for each stack\n       \"interpolation_mode\": tune.choice(['linear']),                            # Type of multi-step interpolation\n       \"random_seed\": tune.randint(1, 10),\n       \"reconciliation\": tune.choice(['BottomUp', 'MinTraceOLS', 'MinTraceWLS'])\n    }\n```\n\n----------------------------------------\n\nTITLE: FlattenHead Layer Implementation\nDESCRIPTION: Implements a flattening head layer that processes multi-variate time series data and applies linear transformation with dropout.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timexer.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass FlattenHead(nn.Module):\n    def __init__(self, n_vars, nf, target_window, head_dropout=0):\n        super().__init__()\n        self.n_vars = n_vars\n        self.flatten = nn.Flatten(start_dim=-2)\n        self.linear = nn.Linear(nf, target_window)\n        self.dropout = nn.Dropout(head_dropout)\n\n    def forward(self, x):  # x: [bs x nvars x d_model x patch_num]\n        x = self.flatten(x)\n        x = self.linear(x)\n        x = self.dropout(x)\n        return x\n```\n\n----------------------------------------\n\nTITLE: Performing Recursive Validation for Recurrent Models in Python using PyTorch\nDESCRIPTION: This method implements the validation step for recurrent models by performing recursive, one-step-ahead predictions over the entire forecast horizon. It initializes the RNN state, sets the prediction horizon (`h`) to 1, iteratively calls `_validate_step_recurrent_single` for each timestep, updates the input sequence (`insample_y`) with the prediction from the previous step, and aggregates the predictions. Finally, it resets the RNN state and prediction horizon. Dependencies include `torch`, model attributes (`rnn_state`, `maintain_state`, `h`, `horizon_backup`, `loss`, `n_series`, exogenous sizes), and `_validate_step_recurrent_single`.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n    def _validate_step_recurrent_batch(self, insample_y, insample_mask, futr_exog, hist_exog, stat_exog, y_idx):\n        # Remember state in network and set horizon to 1\n        self.rnn_state = None\n        self.maintain_state = True\n        self.h = 1\n\n        # Initialize results array\n        n_outputs = self.loss.outputsize_multiplier\n        y_hat = torch.zeros((insample_y.shape[0],\n                            self.horizon_backup,\n                            self.n_series * n_outputs),\n                            device=insample_y.device,\n                            dtype=insample_y.dtype)\n\n        # First step prediction\n        tau = 0\n        \n        # Set exogenous\n        hist_exog_current = None\n        if self.hist_exog_size > 0:\n            hist_exog_current = hist_exog[:, :self.input_size + tau]\n\n        futr_exog_current = None\n        if self.futr_exog_size > 0:\n            futr_exog_current = futr_exog[:, :self.input_size + tau]\n\n        # First forecast step\n        y_hat[:, tau], insample_y = self._validate_step_recurrent_single(\n                                                                insample_y=insample_y[:, :self.input_size + tau],\n                                                                insample_mask=insample_mask[:, :self.input_size + tau],\n                                                                hist_exog=hist_exog_current,\n                                                                futr_exog=futr_exog_current,\n                                                                stat_exog=stat_exog,\n                                                                y_idx=y_idx,\n                                                                )\n\n        # Horizon prediction recursively\n        for tau in range(1, self.horizon_backup):\n            # Set exogenous\n            if self.hist_exog_size > 0:\n                hist_exog_current = hist_exog[:, self.input_size + tau - 1].unsqueeze(1)\n\n            if self.futr_exog_size > 0:\n                futr_exog_current = futr_exog[:, self.input_size + tau - 1].unsqueeze(1)\n            \n            y_hat[:, tau], insample_y = self._validate_step_recurrent_single(\n                                                                insample_y=insample_y,\n                                                                insample_mask=None,\n                                                                hist_exog=hist_exog_current,\n                                                                futr_exog=futr_exog_current,\n                                                                stat_exog=stat_exog,\n                                                                y_idx = y_idx,\n                                                                )\n        \n        # Reset state and horizon\n        self.maintain_state = False\n        self.rnn_state = None\n        self.h = self.horizon_backup\n\n        return y_hat\n```\n\n----------------------------------------\n\nTITLE: Generating and Visualizing Time Series Forecasts\nDESCRIPTION: Makes predictions using the trained NeuralForecast model on the test dataset and visualizes the results. The plot_series function displays the actual historical values alongside the forecasted values.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/12_using_mlflow.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nY_hat_df = nf.predict(futr_df=Y_test_df)\nplot_series(Y_train_df, Y_hat_df, palette='tab20b')\n```\n\n----------------------------------------\n\nTITLE: Implementing Data Embedding for SOFTS Model in Python\nDESCRIPTION: Defines a DataEmbedding_inverted class that handles data embedding for the SOFTS model. It transforms input data and optional covariates into a suitable format for further processing.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.softs.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass DataEmbedding_inverted(nn.Module):\n    \"\"\"\n    Data Embedding\n    \"\"\"    \n    def __init__(self, c_in, d_model, dropout=0.1):\n        super(DataEmbedding_inverted, self).__init__()\n        self.value_embedding = nn.Linear(c_in, d_model)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x, x_mark):\n        x = x.permute(0, 2, 1)\n        # x: [Batch Variate Time]\n        if x_mark is None:\n            x = self.value_embedding(x)\n        else:\n            # the potential to take covariates (e.g. timestamps) as tokens\n            x = self.value_embedding(torch.cat([x, x_mark.permute(0, 2, 1)], 1))\n        # x: [Batch Variate d_model]\n        return self.dropout(x)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Cross-Validation Forecasts\nDESCRIPTION: Creates a plot comparing the actual values with the TFT forecasts from the cross-validation process, highlighting the beginning of September with a vertical line.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/07_forecasting_tft.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nplot_df = Y_df.merge(Y_hat_df, on=['unique_id','ds'], how='outer').tail(test_size+24*7)\n\nplt.figure(figsize=(20,5))\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['AutoTFT'], c='blue', label='Forecast')\nplt.axvline(pd.to_datetime('2022-09-01'), color='red', linestyle='-.')\nplt.legend()\nplt.grid()\nplt.plot()\n```\n\n----------------------------------------\n\nTITLE: Initializing AutoLSTM Class with Ray Tune Integration\nDESCRIPTION: Implementation of AutoLSTM class that extends BaseAuto for automated LSTM model configuration and training. Includes default hyperparameter search space and supports both Ray Tune and Optuna backends for optimization.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nclass AutoLSTM(BaseAuto):\n\n    default_config = {\n        \"input_size_multiplier\": [-1, 4, 16, 64],\n        \"inference_input_size_multiplier\": [-1],\n        \"h\": None,\n        \"encoder_hidden_size\": tune.choice([16, 32, 64, 128]),\n        \"encoder_n_layers\": tune.randint(1, 4),\n        \"context_size\": tune.choice([5, 10, 50]),\n        \"decoder_hidden_size\": tune.choice([16, 32, 64, 128]),\n        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n        \"max_steps\": tune.choice([500, 1000]),\n        \"batch_size\": tune.choice([16, 32]),\n        \"loss\": None,\n        \"random_seed\": tune.randint(1, 20)\n    }\n```\n\n----------------------------------------\n\nTITLE: Implementing AutoNBEATS Class for Time Series Forecasting with Hyperparameter Tuning in Python\nDESCRIPTION: Defines the AutoNBEATS class extending BaseAuto for automatic hyperparameter tuning of NBEATS models for time series forecasting. The class provides default configuration with search spaces for hyperparameters like input size and learning rate, supporting both Ray Tune and Optuna backends.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_36\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass AutoNBEATS(BaseAuto):\n\n    default_config = {\n        \"input_size_multiplier\": [1, 2, 3, 4, 5],\n        \"h\": None,\n        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n        \"scaler_type\": tune.choice([None, 'robust', 'standard']),\n        \"max_steps\": tune.choice([500, 1000]),\n        \"batch_size\": tune.choice([32, 64, 128, 256]),\n        \"windows_batch_size\": tune.choice([128, 256, 512, 1024]),\n        \"loss\": None,\n        \"random_seed\": tune.randint(1, 20),\n    }\n\n    def __init__(self,\n                 h,\n                 loss=MAE(),\n                 valid_loss=None,\n                 config=None, \n                 search_alg=BasicVariantGenerator(random_state=1),\n                 num_samples=10,\n                 refit_with_val=False,\n                 cpus=cpu_count(),\n                 gpus=torch.cuda.device_count(),\n                 verbose=False,\n                 alias=None,\n                 backend='ray',\n                 callbacks=None):\n        \n        # Define search space, input/output sizes \n        if config is None:\n            config = self.get_default_config(h=h, backend=backend)          \n\n        super(AutoNBEATS, self).__init__(\n              cls_model=NBEATS, \n              h=h,\n              loss=loss,\n              valid_loss=valid_loss,\n              config=config,\n              search_alg=search_alg,\n              num_samples=num_samples, \n              refit_with_val=refit_with_val,\n              cpus=cpus,\n              gpus=gpus,\n              verbose=verbose,\n              alias=alias,\n              backend=backend,\n              callbacks=callbacks,            \n        )\n\n    @classmethod\n    def get_default_config(cls, h, backend, n_series=None):\n        config = cls.default_config.copy()\n        config['input_size'] = tune.choice([h*x \\\n                        for x in config['input_size_multiplier']])\n        config['step_size'] = tune.choice([1, h])        \n        del config['input_size_multiplier']\n        if backend == 'optuna':\n            config = cls._ray_config_to_optuna(config)         \n\n        return config                \n```\n\n----------------------------------------\n\nTITLE: Computing Negative Log-Likelihood Loss for Probabilistic Forecasts\nDESCRIPTION: Calculates the negative log-likelihood loss function used for training probabilistic forecasting models. It applies the distribution parameters to create a distribution object, evaluates the log probability of the actual values, and computes a weighted average loss.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_42\n\nLANGUAGE: python\nCODE:\n```\ndef __call__(self,\n                 y: torch.Tensor,\n                 distr_args: torch.Tensor,\n                 mask: Union[torch.Tensor, None] = None):\n         \"\"\"\n         Computes the negative log-likelihood objective function. \n         To estimate the following predictive distribution:\n\n         $$\\mathrm{P}(\\mathbf{y}_{\\\\tau}\\,|\\,\\\\theta) \\\\quad \\mathrm{and} \\\\quad -\\log(\\mathrm{P}(\\mathbf{y}_{\\\\tau}\\,|\\,\\\\theta))$$\n\n         where $\\\\theta$ represents the distributions parameters. It aditionally \n         summarizes the objective signal using a weighted average using the `mask` tensor. \n \n         **Parameters**<br>\n         `y`: tensor, Actual values.<br>\n         `distr_args`: Constructor arguments for the underlying Distribution type.<br>\n         `loc`: Optional tensor, of the same shape as the batch_shape + event_shape\n                of the resulting distribution.<br>\n         `scale`: Optional tensor, of the same shape as the batch_shape+event_shape \n                of the resulting distribution.<br>\n         `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n\n         **Returns**<br>\n         `loss`: scalar, weighted loss function against which backpropagation will be performed.<br>\n         \"\"\"\n         # Instantiate Scaled Decoupled Distribution\n         distr = self.get_distribution(distr_args=distr_args, **self.distribution_kwargs)\n         loss_values = -distr.log_prob(y)\n         loss_weights = self._compute_weights(y=y, mask=mask)\n         return weighted_average(loss_values, weights=loss_weights)\n```\n\n----------------------------------------\n\nTITLE: Implementing Standard Scaler and Inverse Functions in Python\nDESCRIPTION: Provides functions for applying standard scaling and its inverse transformation using pre-computed statistics.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.scalers.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef std_scaler(x, x_means, x_stds):\n    return (x - x_means) / x_stds\n\ndef inv_std_scaler(z, x_mean, x_std):\n    return (z * x_std) + x_mean\n```\n\n----------------------------------------\n\nTITLE: Plotting Time Series Forecast Comparison with Matplotlib in Python\nDESCRIPTION: Creates a visualization comparing actual airline passenger data with StemGNN model predictions. The code extracts specific forecast results for 'Airline1', plots both the actual values and predictions, and formats the graph with appropriate labels and styling.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.stemgnn.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Plot predictions\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nY_hat_df = forecasts.loc['Airline1']\nY_df = AirPassengersPanel[AirPassengersPanel['unique_id']=='Airline1']\n\nplt.plot(Y_df['ds'], Y_df['y'], c='black', label='True')\nplt.plot(Y_hat_df['ds'], Y_hat_df['StemGNN'], c='blue', label='Forecast')\nax.set_title('AirPassengers Forecast', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Year', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\n```\n\n----------------------------------------\n\nTITLE: Using AutoAutoformer with Custom Configuration and Optuna Backend\nDESCRIPTION: This example demonstrates how to use the AutoAutoformer class with a custom configuration and both Ray (default) and Optuna backends. It shows configuration setup, model initialization, fitting, and prediction with minimal configuration for testing purposes.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_78\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n# Use your own config or AutoAutoformer.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=8)\nmodel = AutoAutoformer(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n# Optuna\nmodel = AutoAutoformer(h=12, config=None, backend='optuna')\n```\n\n----------------------------------------\n\nTITLE: Implementing Encoder and Decoder Components in Python\nDESCRIPTION: Defines the Encoder and Decoder architecture components for the FEDformer model, including their respective layers. These components handle the main transformation and processing of input sequences.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.fedformer.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass EncoderLayer(nn.Module):\n    \"\"\"\n    FEDformer encoder layer with the progressive decomposition architecture\n    \"\"\"\n    def __init__(self, attention, hidden_size, conv_hidden_size=None, MovingAvg=25, dropout=0.1, activation=\"relu\"):\n        super(EncoderLayer, self).__init__()\n        conv_hidden_size = conv_hidden_size or 4 * hidden_size\n        self.attention = attention\n        self.conv1 = nn.Conv1d(in_channels=hidden_size, out_channels=conv_hidden_size, kernel_size=1, bias=False)\n        self.conv2 = nn.Conv1d(in_channels=conv_hidden_size, out_channels=hidden_size, kernel_size=1, bias=False)\n        self.decomp1 = SeriesDecomp(MovingAvg)\n        self.decomp2 = SeriesDecomp(MovingAvg)\n        self.dropout = nn.Dropout(dropout)\n        self.activation = F.relu if activation == \"relu\" else F.gelu\n\n    def forward(self, x, attn_mask=None):\n        new_x, attn = self.attention(\n            x, x, x,\n            attn_mask=attn_mask\n        )\n        x = x + self.dropout(new_x)\n        x, _ = self.decomp1(x)\n        y = x\n        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n        y = self.dropout(self.conv2(y).transpose(-1, 1))\n        res, _ = self.decomp2(x + y)\n        return res, attn\n\n\nclass Encoder(nn.Module):\n    \"\"\"\n    FEDformer encoder\n    \"\"\"\n    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n        super(Encoder, self).__init__()\n        self.attn_layers = nn.ModuleList(attn_layers)\n        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n        self.norm = norm_layer\n\n    def forward(self, x, attn_mask=None):\n        attns = []\n        if self.conv_layers is not None:\n            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):\n                x, attn = attn_layer(x, attn_mask=attn_mask)\n                x = conv_layer(x)\n                attns.append(attn)\n            x, attn = self.attn_layers[-1](x)\n            attns.append(attn)\n        else:\n            for attn_layer in self.attn_layers:\n                x, attn = attn_layer(x, attn_mask=attn_mask)\n                attns.append(attn)\n\n        if self.norm is not None:\n            x = self.norm(x)\n\n        return x, attns\n\n\nclass DecoderLayer(nn.Module):\n    \"\"\"\n    FEDformer decoder layer with the progressive decomposition architecture\n    \"\"\"\n    def __init__(self, self_attention, cross_attention, hidden_size, c_out, conv_hidden_size=None,\n                 MovingAvg=25, dropout=0.1, activation=\"relu\"):\n        super(DecoderLayer, self).__init__()\n        conv_hidden_size = conv_hidden_size or 4 * hidden_size\n        self.self_attention = self_attention\n        self.cross_attention = cross_attention\n        self.conv1 = nn.Conv1d(in_channels=hidden_size, out_channels=conv_hidden_size, kernel_size=1, bias=False)\n        self.conv2 = nn.Conv1d(in_channels=conv_hidden_size, out_channels=hidden_size, kernel_size=1, bias=False)\n        self.decomp1 = SeriesDecomp(MovingAvg)\n        self.decomp2 = SeriesDecomp(MovingAvg)\n        self.decomp3 = SeriesDecomp(MovingAvg)\n        self.dropout = nn.Dropout(dropout)\n        self.projection = nn.Conv1d(in_channels=hidden_size, out_channels=c_out, kernel_size=3, stride=1, padding=1,\n                                    padding_mode='circular', bias=False)\n        self.activation = F.relu if activation == \"relu\" else F.gelu\n\n    def forward(self, x, cross, x_mask=None, cross_mask=None):\n        x = x + self.dropout(self.self_attention(\n            x, x, x,\n            attn_mask=x_mask\n        )[0])\n        x, trend1 = self.decomp1(x)\n        x = x + self.dropout(self.cross_attention(\n            x, cross, cross,\n            attn_mask=cross_mask\n        )[0])\n        x, trend2 = self.decomp2(x)\n        y = x\n        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n        y = self.dropout(self.conv2(y).transpose(-1, 1))\n        x, trend3 = self.decomp3(x + y)\n\n        residual_trend = trend1 + trend2 + trend3\n        residual_trend = self.projection(residual_trend.permute(0, 2, 1)).transpose(1, 2)\n        return x, residual_trend\n\n\nclass Decoder(nn.Module):\n    \"\"\"\n    FEDformer decoder\n    \"\"\"\n    def __init__(self, layers, norm_layer=None, projection=None):\n        super(Decoder, self).__init__()\n        self.layers = nn.ModuleList(layers)\n        self.norm = norm_layer\n        self.projection = projection\n\n    def forward(self, x, cross, x_mask=None, cross_mask=None, trend=None):\n        for layer in self.layers:\n            x, residual_trend = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask)\n            trend = trend + residual_trend\n\n        if self.norm is not None:\n            x = self.norm(x)\n\n        if self.projection is not None:\n            x = self.projection(x)\n        return x, trend\n```\n\n----------------------------------------\n\nTITLE: Finalizing Data Window Batch Preparation in Python\nDESCRIPTION: This snippet represents the final steps of a method responsible for creating data window batches. It optionally filters windows based on indices (`w_idxs`), potentially filters static features if applicable, constructs a dictionary `windows_batch` containing temporal and static data along with their column names, and returns this dictionary. If the `step` parameter is unknown, it raises a ValueError.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n            if w_idxs is not None:\n                windows = windows[w_idxs]\n                if static is not None and not self.MULTIVARIATE:\n                    static = static[w_idxs]\n\n            windows_batch = dict(temporal=windows,\n                                 temporal_cols=temporal_cols,\n                                 static=static,\n                                 static_cols=static_cols)\n            return windows_batch\n        else:\n            raise ValueError(f'Unknown step {step}')\n```\n\n----------------------------------------\n\nTITLE: Instantiating AutoTSMixer and AutoTSMixerx Models in Python\nDESCRIPTION: This snippet shows how to create instances of AutoTSMixer and AutoTSMixerx models with specified hyperparameters, loss functions, and search algorithms for optimization.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/08_multivariate_tsmixer.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmodel = AutoTSMixer(h=horizon,\n                    n_series=7,\n                    loss=MAE(),\n                    config=tsmixer_config,\n                    num_samples=10,\n                    search_alg=HyperOptSearch(),\n                    backend='ray',\n                    valid_loss=MAE())\n\nmodelx = AutoTSMixerx(h=horizon,\n                    n_series=7,\n                    loss=MAE(),\n                    config=tsmixerx_config,\n                    num_samples=10,\n                    search_alg=HyperOptSearch(),\n                    backend='ray',\n                    valid_loss=MAE())\n```\n\n----------------------------------------\n\nTITLE: Implementing Robust Median Scaler for Neural Forecasting in Python\nDESCRIPTION: Calculates robust scaling statistics using median and mean absolute deviation. It's useful for noisy data with outliers and includes safeguards against division by zero.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.scalers.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\ndef robust_statistics(x, mask, dim=-1, eps=1e-6):\n    x_median = masked_median(x=x, mask=mask, dim=dim)\n    x_mad = masked_median(x=torch.abs(x-x_median), mask=mask, dim=dim)\n\n    x_means = masked_mean(x=x, mask=mask, dim=dim)\n    x_stds = torch.sqrt(masked_mean(x=(x-x_means)**2, mask=mask, dim=dim))  \n    x_mad_aux = x_stds * 0.6744897501960817\n    x_mad = x_mad * (x_mad>0) + x_mad_aux * (x_mad==0)\n    \n    x_mad[x_mad==0] = 1.0\n    x_mad = x_mad + eps\n    return x_median, x_mad\n```\n\n----------------------------------------\n\nTITLE: Implementing MixingLayerWithStaticExogenous for TSMixerx in Python\nDESCRIPTION: Defines the MixingLayerWithStaticExogenous class, which extends the MixingLayer to incorporate static exogenous variables in the TSMixerx model.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tsmixerx.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass MixingLayerWithStaticExogenous(nn.Module):\n    \"\"\" \n    MixingLayerWithStaticExogenous\n    \"\"\"      \n    def __init__(self, h, dropout, ff_dim, stat_input_size):\n        super().__init__()\n        # Feature mixer for the static exogenous variables\n        self.feature_mixer_stat = FeatureMixing(in_features=stat_input_size, \n                                                out_features=ff_dim, \n                                                h=h, \n                                                dropout=dropout, \n                                                ff_dim=ff_dim)\n        # Mixing layer consists of a temporal and feature mixer\n        self.temporal_mixer = TemporalMixing(num_features=2 * ff_dim, \n                                             h=h, \n                                             dropout=dropout)\n        self.feature_mixer = FeatureMixing(in_features=2 * ff_dim, \n                                           out_features=ff_dim, \n                                           h=h, \n                                           dropout=dropout, \n                                           ff_dim=ff_dim)\n\n    def forward(self, inputs):\n        input, stat_exog = inputs\n        x_stat = self.feature_mixer_stat(stat_exog)                     # [B, h, S] -> [B, h, ff_dim]\n        x = torch.cat((input, x_stat), dim=2)                           # [B, h, ff_dim] + [B, h, ff_dim] -> [B, h, 2 * ff_dim]\n        x = self.temporal_mixer(x)                                      # [B, h, 2 * ff_dim] -> [B, h, 2 * ff_dim]\n        x = self.feature_mixer(x)                                       # [B, h, 2 * ff_dim] -> [B, h, ff_dim]\n        return (x, stat_exog)\n```\n\n----------------------------------------\n\nTITLE: PMM Sampling Method Implementation\nDESCRIPTION: Implements sampling functionality for the PMM model to construct empirical quantiles from the estimated distribution. Returns samples and their statistics.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_46\n\nLANGUAGE: python\nCODE:\n```\ndef sample(self,\n           distr_args: torch.Tensor,\n           num_samples: Optional[int] = None):\n        if num_samples is None:\n            num_samples = self.num_samples\n\n        # Instantiate Scaled Decoupled Distribution\n        distr = self.get_distribution(distr_args=distr_args)\n        samples = distr.sample(sample_shape=(num_samples,))\n        samples = samples.permute(1, 2, 3, 0)                  \n\n        sample_mean = torch.mean(samples, dim=-1, keepdim=True) \n\n        # Compute quantiles\n        quantiles_device = self.quantiles.to(distr_args[0].device)\n        quants = torch.quantile(input=samples, \n                                q=quantiles_device, \n                                dim=-1)\n        quants = quants.permute(1, 2, 3, 0)\n\n        return samples, sample_mean, quants\n```\n\n----------------------------------------\n\nTITLE: StemGNN Usage Example with Air Passengers Dataset\nDESCRIPTION: Demonstrates how to train and use the StemGNN model for time series forecasting using the Air Passengers dataset. Shows model initialization, training, and prediction visualization.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.stemgnn.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import StemGNN\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\nfrom neuralforecast.losses.pytorch import MAE\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]].reset_index(drop=True)\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True)\n\nmodel = StemGNN(h=12,\n                input_size=24,\n                n_series=2,\n                scaler_type='standard',\n                max_steps=500,\n                early_stop_patience_steps=-1,\n                val_check_steps=10,\n                learning_rate=1e-3,\n                loss=MAE(),\n                valid_loss=MAE(),\n                batch_size=32\n                )\n```\n\n----------------------------------------\n\nTITLE: Implementing MAPE Loss Function in PyTorch\nDESCRIPTION: Mean Absolute Percentage Error implementation that calculates the relative prediction accuracy by measuring percentual deviation between predicted and actual values. Includes support for horizon weighting and mask-based filtering.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass MAPE(BasePointLoss):\n    def __init__(self, horizon_weight=None):\n        super(MAPE, self).__init__(horizon_weight=horizon_weight,\n                                  outputsize_multiplier=1,\n                                  output_names=[''])\n\n    def __call__(self,\n                 y: torch.Tensor,\n                 y_hat: torch.Tensor,\n                 y_insample: torch.Tensor,\n                 mask: Union[torch.Tensor, None] = None,\n                 ) -> torch.Tensor:\n        scale = _divide_no_nan(torch.ones_like(y, device=y.device), torch.abs(y))\n        losses = torch.abs(y - y_hat) * scale\n        weights = self._compute_weights(y=y, mask=mask)\n        mape = _weighted_mean(losses=losses, weights=weights)\n        return mape\n```\n\n----------------------------------------\n\nTITLE: Calculating MAE Loss with Incomplete Mask and Incomplete Horizon Weight in PyTorch\nDESCRIPTION: This snippet demonstrates the calculation of MAE loss using both incomplete mask and horizon weight. It sets up a mask and horizon weight that collectively mask two errors and three points, computes the loss, and verifies the result.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_80\n\nLANGUAGE: Python\nCODE:\n```\n# Incomplete mask and incomplete horizon_weight\nmask = torch.Tensor([[0,1,1],[1,1,1]]).unsqueeze(-1)\nhorizon_weight = torch.Tensor([1,1,0]) # 2 errors are masked, and 3 points.\nmae = MAE(horizon_weight=horizon_weight)\nloss = mae(y=y, y_hat=y_hat, mask=mask)\nassert loss==(1/3), 'Should be 1/3'\n```\n\n----------------------------------------\n\nTITLE: Plotting Airline2 forecast results with matplotlib\nDESCRIPTION: Creates a visualization of forecasting results for Airline2 series. The plot combines training data with forecasts from multiple models, showing the historical and predicted passenger numbers over time.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nplot_df = pd.concat([AirPassengersPanel_train, forecasts.reset_index()]).set_index('ds')\n\nplot_df[plot_df['unique_id']=='Airline2'].drop(['unique_id','trend','y_[lag12]'], axis=1).plot(ax=ax, linewidth=2)\n\nax.set_title('AirPassengers Forecast', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\n```\n\n----------------------------------------\n\nTITLE: Implementing Invariant Median Scaler for Neural Forecasting in Python\nDESCRIPTION: Calculates invariant scaling statistics using median, mean absolute deviation, and arcsinh transformation. It's useful for handling extreme values and includes safeguards against division by zero.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.scalers.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndef invariant_statistics(x, mask, dim=-1, eps=1e-6):\n    x_median = masked_median(x=x, mask=mask, dim=dim)\n    x_mad = masked_median(x=torch.abs(x-x_median), mask=mask, dim=dim)\n\n    x_means = masked_mean(x=x, mask=mask, dim=dim)\n    x_stds = torch.sqrt(masked_mean(x=(x-x_means)**2, mask=mask, dim=dim))        \n    x_mad_aux = x_stds * 0.6744897501960817\n    x_mad = x_mad * (x_mad>0) + x_mad_aux * (x_mad==0)\n\n    x_mad[x_mad==0] = 1.0\n    x_mad = x_mad + eps\n    return x_median, x_mad\n```\n\n----------------------------------------\n\nTITLE: Evaluating NeuralForecast Model Predictions in Python\nDESCRIPTION: This snippet shows how to evaluate the model's predictions using multiple metrics (MAE, RMSE, SMAPE) by comparing them with the actual target values.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/19_large_datasets.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntarget = valid_df[60:72]\n```\n\nLANGUAGE: python\nCODE:\n```\nevaluate(\n    predictions.merge(target.drop([\"trend\", \"y_[lag12]\"], axis=1), on=['id_col', 'ds']),\n    metrics=[mae, rmse, smape],\n    id_col='id_col',\n    agg_fn='mean',\n)\n```\n\n----------------------------------------\n\nTITLE: Data Embedding Module Implementation\nDESCRIPTION: Implements a data embedding module without positional encoding. Combines value embedding and temporal embedding for processing time series input data.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timemixer.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass DataEmbedding_wo_pos(nn.Module):\n    \"\"\"\n    DataEmbedding_wo_pos\n    \"\"\"\n    def __init__(self, c_in, d_model, dropout=0.1, embed_type='fixed', freq='h'):\n        super(DataEmbedding_wo_pos, self).__init__()\n\n        self.value_embedding = TokenEmbedding(c_in=c_in, hidden_size=d_model)\n        self.position_embedding = PositionalEmbedding(hidden_size=d_model)\n        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n                                                    freq=freq)\n        self.dropout = nn.Dropout(p=dropout)\n\n    def forward(self, x, x_mark):\n        if x is None and x_mark is not None:\n            return self.temporal_embedding(x_mark)\n        if x_mark is None:\n            x = self.value_embedding(x)\n        else:\n            x = self.value_embedding(x) + self.temporal_embedding(x_mark)\n        return self.dropout(x)\n```\n\n----------------------------------------\n\nTITLE: Loading a Single Time Series from External Source\nDESCRIPTION: Loads the AirPassengers dataset as an example of a single time series. The data is read directly from a GitHub repository into a pandas DataFrame.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/getting-started/05_datarequirements.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nY_df = pd.read_csv('https://raw.githubusercontent.com/Nixtla/transfer-learning-time-series/main/datasets/air_passengers.csv')\nY_df\n```\n\n----------------------------------------\n\nTITLE: Implementing Tail Parameterization Method for ISQF\nDESCRIPTION: Static method that parameterizes the tail parameters for the ISQF distribution. It handles both left and right exponential tails, which are important for extrapolation beyond the range of observed data.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\n    @staticmethod\n    def parameterize_tail(\n        beta: torch.Tensor, qk_x: torch.Tensor, qk_y: torch.Tensor\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        r\"\"\"\n        Function to parameterize the tail parameters\n        Note that the exponential tails are given by\n        q(alpha)\n        = a_l log(alpha) + b_l if left tail\n        = a_r log(1-alpha) + b_r if right tail\n        where\n        a_l=1/beta_l, b_l=-a_l*log(qk_x_l)+q(qk_x_l)\n        a_r=1/beta_r, b_r=a_r*log(1-qk_x_r)+q(qk_x_r)\n        Parameters\n        ----------\n        beta\n            parameterizes the left or right tail, shape: (*batch_shape,)\n        qk_x\n            left- or right-most x-positions of the quantile knots,\n            shape: (*batch_shape,)\n        qk_y\n            left- or right-most y-positions of the quantile knots,\n            shape: (*batch_shape,)\n        Returns\n        -------\n        tail_a\n        \"\"\"\n```\n\n----------------------------------------\n\nTITLE: Implementing AutoiTransformer Class for Time Series Forecasting\nDESCRIPTION: Defines the AutoiTransformer class that extends BaseAuto for automated time series forecasting using iTransformer model. Includes default configurations, initialization logic, and methods for handling different backends (Ray/Optuna).\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_85\n\nLANGUAGE: python\nCODE:\n```\nclass AutoiTransformer(BaseAuto):\n\n    default_config = {\n        \"input_size_multiplier\": [1, 2, 3, 4, 5],\n        \"h\": None,\n        \"n_series\": None,\n        \"hidden_size\": tune.choice([64, 128, 256]),\n        \"n_heads\": tune.choice([4, 8]),\n        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n        \"scaler_type\": tune.choice([None, 'robust', 'standard']),\n        \"max_steps\": tune.choice([500, 1000, 2000]),\n        \"batch_size\": tune.choice([32, 64, 128, 256]),\n        \"loss\": None,\n        \"random_seed\": tune.randint(1, 20),\n    }\n```\n\n----------------------------------------\n\nTITLE: Configuring and Testing AutoTCN Models with Different Loss Functions in Python\nDESCRIPTION: This snippet demonstrates the configuration and testing of AutoTCN models with different combinations of loss functions and validation metrics. It includes examples with point-wise and probabilistic losses.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_117\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n## TODO: Add unit tests for interactions between loss/valid_loss types\n## TODO: Unit tests (2 types of networks x 2 types of loss x 2 types of valid loss)\n## Checking if base recurrent methods run point valid_loss correctly\ntcn_config = {\n       \"learning_rate\": tune.choice([1e-3]),                                     # Initial Learning rate\n       \"max_steps\": tune.choice([1]),                                            # Number of SGD steps\n       \"val_check_steps\": tune.choice([1]),                                      # Number of steps between validation\n       \"input_size\": tune.choice([5 * 12]),                                      # input_size = multiplier * horizon\n       \"batch_size\": tune.choice([7]),                                           # Number of series in windows\n       \"random_seed\": tune.randint(1, 10),\n    }\n\nmodel = AutoTCN(h=12, \n                loss=MAE(), \n                valid_loss=MSE(), \n                config=tcn_config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n## Checking if base recurrent methods run quantile valid_loss correctly\nmodel = AutoTCN(h=12, \n                loss=GMM(n_components=2, level=[80, 90]),\n                valid_loss=sCRPS(level=[80, 90]),\n                config=tcn_config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: Fitting NeuralForecast Model with Temporal Window Normalization in Python\nDESCRIPTION: Creates a NeuralForecast object without time series scaling (local_scaler_type=None by default) since the model itself handles scaling at the window level during training.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/07_time_series_scaling.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nnf = NeuralForecast(models=[model], freq='h')\nnf.fit(df=df)\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for LSTM Model\nDESCRIPTION: Imports necessary modules and classes for the LSTM model implementation, including PyTorch, typing utilities, and custom loss functions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.lstm.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\nimport warnings\n\nfrom neuralforecast.losses.pytorch import MAE\nfrom neuralforecast.common._base_model import BaseModel\nfrom neuralforecast.common._modules import MLP\n```\n\n----------------------------------------\n\nTITLE: Importing NeuralForecast Dependencies\nDESCRIPTION: Python code importing required libraries for using NeuralForecast, including pandas for data manipulation, plotting utilities, and the core NeuralForecast components.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/getting-started/01_introduction.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nimport pandas as pd\nfrom utilsforecast.plotting import plot_series\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import NBEATS, NHITS\nfrom neuralforecast.utils import AirPassengersDF\n```\n\n----------------------------------------\n\nTITLE: Instantiating TimesNet Model with Time Series Scaling in Python\nDESCRIPTION: Creates a TimesNet model for day-ahead electricity price forecasting with specific parameters. The model is configured with a 24-hour forecast horizon, FFT periods, inception modules, and utilizes future exogenous variables but relies on the NeuralForecast core class for scaling.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/07_time_series_scaling.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nhorizon = 24 # day-ahead daily forecast\nmodel = TimesNet(h = horizon,                                   # Horizon\n                 input_size = 5*horizon,                        # Length of input window\n                 max_steps = 100,                               # Training iterations\n                 top_k = 3,                                     # Number of periods (for FFT).\n                 num_kernels = 3,                               # Number of kernels for Inception module\n                 batch_size = 2,                                # Number of time series per batch\n                 windows_batch_size = 32,                       # Number of windows per batch\n                 learning_rate = 0.001,                         # Learning rate\n                 futr_exog_list = ['gen_forecast', 'week_day'], # Future exogenous variables\n                 scaler_type = None)                            # We use the Core scaling method\n```\n\n----------------------------------------\n\nTITLE: Implementing AutoNLinear Class for Neural Network Linear Forecasting\nDESCRIPTION: Definition of the AutoNLinear class which inherits from BaseAuto. It provides automated hyperparameter tuning for the NLinear model, including a default configuration with search spaces for input size, learning rate, batch sizes, and other hyperparameters.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_49\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass AutoNLinear(BaseAuto):\n\n    default_config = {\n       \"input_size_multiplier\": [1, 2, 3, 4, 5],\n       \"h\": None,\n       \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n       \"scaler_type\": tune.choice([None, 'robust', 'standard']),\n       \"max_steps\": tune.quniform(lower=500, upper=1500, q=100),\n       \"batch_size\": tune.choice([32, 64, 128, 256]),\n       \"windows_batch_size\": tune.choice([128, 256, 512, 1024]),\n       \"loss\": None,\n       \"random_seed\": tune.randint(lower=1, upper=20),\n    }\n\n    def __init__(self,\n                 h,\n                 loss=MAE(),\n                 valid_loss=None,\n                 config=None, \n                 search_alg=BasicVariantGenerator(random_state=1),\n                 num_samples=10,\n                 refit_with_val=False,\n                 cpus=cpu_count(),\n                 gpus=torch.cuda.device_count(),\n                 verbose=False,\n                 alias=None,\n                 backend='ray',\n                 callbacks=None,\n                ):\n\n        # Define search space, input/output sizes\n        if config is None:\n            config = self.get_default_config(h=h, backend=backend)                 \n\n        super(AutoNLinear, self).__init__(\n              cls_model=NLinear, \n              h=h,\n              loss=loss,\n              valid_loss=valid_loss,\n              config=config,\n              search_alg=search_alg,\n              num_samples=num_samples,\n              refit_with_val=refit_with_val,\n              cpus=cpus,\n              gpus=gpus,\n              verbose=verbose,\n              alias=alias,\n              backend=backend,\n              callbacks=callbacks,\n        )\n\n    @classmethod\n    def get_default_config(cls, h, backend, n_series=None):\n        config = cls.default_config.copy()\n        config['input_size'] = tune.choice([h*x \\\n                        for x in config['input_size_multiplier']])\n        config['step_size'] = tune.choice([1, h])        \n        del config['input_size_multiplier']\n        if backend == 'optuna':\n            config = cls._ray_config_to_optuna(config)         \n\n        return config           \n```\n\n----------------------------------------\n\nTITLE: Debug Configuration Setup\nDESCRIPTION: Sets up logging and warning configurations for development purposes.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.deepar.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport warnings\n\nfrom fastcore.test import test_eq\nfrom nbdev.showdoc import show_doc\nfrom neuralforecast.common._model_checks import check_model\n```\n\n----------------------------------------\n\nTITLE: Creating PyTorch Distribution Objects for Probabilistic Forecasting\nDESCRIPTION: Method that constructs the PyTorch distribution object based on the provided distribution arguments. It also handles specific support constraints for distributions like Poisson and NegativeBinomial.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_38\n\nLANGUAGE: python\nCODE:\n```\ndef get_distribution(self, distr_args, **distribution_kwargs) -> Distribution:\n        \"\"\"\n        Construct the associated Pytorch Distribution, given the collection of\n        constructor arguments and, optionally, location and scale tensors.\n\n        **Parameters**<br>\n        `distr_args`: Constructor arguments for the underlying Distribution type.<br>\n\n        **Returns**<br>\n        `Distribution`: AffineTransformed distribution.<br>\n        \"\"\"\n        distr = self._base_distribution(*distr_args, **distribution_kwargs)\n        self.distr_mean = distr.mean\n        \n        if self.distribution in ('Poisson', 'NegativeBinomial'):\n              distr.support = constraints.nonnegative\n        return distr\n```\n\n----------------------------------------\n\nTITLE: Setting Default Export Path for Python Module\nDESCRIPTION: Configures the default export path for a Python module named 'models.stemgnn' using Jupyter cell magic.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.stemgnn.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#| default_exp models.stemgnn\n```\n\n----------------------------------------\n\nTITLE: Implementing AutoVanillaTransformer Class for Automated Time Series Forecasting in Python\nDESCRIPTION: This snippet defines the AutoVanillaTransformer class that extends BaseAuto for automated hyperparameter tuning of VanillaTransformer models. It includes default configurations, initialization parameters, and methods to generate search spaces for different backends.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_67\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass AutoVanillaTransformer(BaseAuto):\n\n    default_config = {\n        \"input_size_multiplier\": [1, 2, 3, 4, 5],\n        \"h\": None,\n        \"hidden_size\": tune.choice([64, 128, 256]),\n        \"n_head\": tune.choice([4, 8]),\n        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n        \"scaler_type\": tune.choice([None, 'robust', 'standard']),\n        \"max_steps\": tune.choice([500, 1000, 2000]),\n        \"batch_size\": tune.choice([32, 64, 128, 256]),\n        \"windows_batch_size\": tune.choice([128, 256, 512, 1024]),\n        \"loss\": None,\n        \"random_seed\": tune.randint(1, 20),\n    }\n\n    def __init__(self,\n                 h,\n                 loss=MAE(),\n                 valid_loss=None,\n                 config=None, \n                 search_alg=BasicVariantGenerator(random_state=1),\n                 num_samples=10,\n                 refit_with_val=False,\n                 cpus=cpu_count(),\n                 gpus=torch.cuda.device_count(),\n                 verbose=False,\n                 alias=None,\n                 backend='ray',\n                 callbacks=None):\n        \n        # Define search space, input/output sizes\n        if config is None:\n            config = self.get_default_config(h=h, backend=backend)          \n\n        super(AutoVanillaTransformer, self).__init__(\n              cls_model=VanillaTransformer, \n              h=h,\n              loss=loss,\n              valid_loss=valid_loss,\n              config=config,\n              search_alg=search_alg,\n              num_samples=num_samples, \n              refit_with_val=refit_with_val,\n              cpus=cpus,\n              gpus=gpus,\n              verbose=verbose,\n              alias=alias,\n              backend=backend,\n              callbacks=callbacks,            \n        )\n\n    @classmethod\n    def get_default_config(cls, h, backend, n_series=None):\n        config = cls.default_config.copy()\n        config['input_size'] = tune.choice([h*x \\\n                        for x in config['input_size_multiplier']])\n        config['step_size'] = tune.choice([1, h])        \n        del config['input_size_multiplier']\n        if backend == 'optuna':\n            config = cls._ray_config_to_optuna(config)         \n\n        return config   \n```\n\n----------------------------------------\n\nTITLE: Importing NeuralForecast Models and Components\nDESCRIPTION: Imports the NHITS and NBEATSx models along with loss functions and the NeuralForecast framework for time series forecasting.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/14_interpretable_decompositions.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom neuralforecast.models import NHITS, NBEATSx\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.losses.pytorch import HuberLoss, MQLoss\n```\n\n----------------------------------------\n\nTITLE: MLP Model Usage Example with AirPassengers Dataset\nDESCRIPTION: Complete example showing how to use the MLP model for time series forecasting, including data preparation, model configuration, training, and visualization of predictions with confidence intervals.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.mlp.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import MLP\nfrom neuralforecast.losses.pytorch import DistributionLoss\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nmodel = MLP(h=12, input_size=24,\n            loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n            scaler_type='robust',\n            learning_rate=1e-3,\n            max_steps=200,\n            val_check_steps=10,\n            early_stop_patience_steps=2)\n\nfcst = NeuralForecast(\n    models=[model],\n    freq='ME'\n)\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\n# Plot predictions\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['MLP-median'], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'][-12:], \n                 y1=plot_df['MLP-lo-90'][-12:].values, \n                 y2=plot_df['MLP-hi-90'][-12:].values,\n                 alpha=0.4, label='level 90')\nplt.grid()\nplt.legend()\nplt.plot()\n```\n\n----------------------------------------\n\nTITLE: Implementing AutoTFT Class\nDESCRIPTION: Implementation of the AutoTFT class for Temporal Fusion Transformer forecasting models. Defines transformer-specific hyperparameters, initialization method, and configuration generation with backend support.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_64\n\nLANGUAGE: python\nCODE:\n```\nclass AutoTFT(BaseAuto):\n\n    default_config = {\n        \"input_size_multiplier\": [1, 2, 3, 4, 5],\n        \"h\": None,\n        \"hidden_size\": tune.choice([64, 128, 256]),\n        \"n_head\": tune.choice([4, 8]),\n        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n        \"scaler_type\": tune.choice([None, 'robust', 'standard']),\n        \"max_steps\": tune.choice([500, 1000, 2000]),\n        \"batch_size\": tune.choice([32, 64, 128, 256]),\n        \"windows_batch_size\": tune.choice([128, 256, 512, 1024]),\n        \"loss\": None,\n        \"random_seed\": tune.randint(1, 20),\n    }\n\n    def __init__(self,\n                 h,\n                 loss=MAE(),\n                 valid_loss=None,\n                 config=None, \n                 search_alg=BasicVariantGenerator(random_state=1),\n                 num_samples=10,\n                 refit_with_val=False,\n                 cpus=cpu_count(),\n                 gpus=torch.cuda.device_count(),\n                 verbose=False,\n                 alias=None,\n                 backend='ray',\n                 callbacks=None):\n        \n        # Define search space, input/output sizes\n        if config is None:\n            config = self.get_default_config(h=h, backend=backend)             \n\n        super(AutoTFT, self).__init__(\n              cls_model=TFT, \n              h=h,\n              loss=loss,\n              valid_loss=valid_loss,\n              config=config,\n              search_alg=search_alg,\n              num_samples=num_samples, \n              refit_with_val=refit_with_val,\n              cpus=cpus,\n              gpus=gpus,\n              verbose=verbose,\n              alias=alias,\n              backend=backend,\n              callbacks=callbacks,            \n        )\n\n    @classmethod\n    def get_default_config(cls, h, backend, n_series=None):\n        config = cls.default_config.copy()\n        config['input_size'] = tune.choice([h*x \\\n                        for x in config['input_size_multiplier']])\n        config['step_size'] = tune.choice([1, h])        \n        del config['input_size_multiplier']\n        if backend == 'optuna':\n            config = cls._ray_config_to_optuna(config)         \n\n        return config\n```\n\n----------------------------------------\n\nTITLE: Handling Validation Epoch End in PyTorch Lightning\nDESCRIPTION: Calculates and logs validation metrics at the end of each validation epoch. Manages validation trajectories and memory by clearing outputs after processing.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef on_validation_epoch_end(self):\n    if self.val_size == 0:\n        return\n    losses = torch.stack(self.validation_step_outputs)\n    avg_loss = losses.mean().detach().item()\n    self.log(\n        \"ptl/val_loss\",\n        avg_loss,\n        batch_size=losses.size(0),\n        sync_dist=True,\n    )\n    self.valid_trajectories.append((self.global_step, avg_loss))\n    self.validation_step_outputs.clear() # free memory (compute `avg_loss` per epoch)\n```\n\n----------------------------------------\n\nTITLE: Implementing SMAPE Loss Function in PyTorch\nDESCRIPTION: Symmetric Mean Absolute Percentage Error implementation that provides bounded error values between 0% and 200%. Calculates relative deviation scaled by the sum of absolute values for prediction and observation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass SMAPE(BasePointLoss):\n    def __init__(self, horizon_weight=None):\n        super(SMAPE, self).__init__(horizon_weight=horizon_weight,\n                                  outputsize_multiplier=1,\n                                  output_names=[''])\n\n    def __call__(self,\n                 y: torch.Tensor,\n                 y_hat: torch.Tensor,\n                 mask: Union[torch.Tensor, None] = None,\n                 y_insample: Union[torch.Tensor, None] = None) -> torch.Tensor:\n        delta_y = torch.abs((y - y_hat))\n        scale = torch.abs(y) + torch.abs(y_hat)\n        losses = _divide_no_nan(delta_y, scale)\n        weights = self._compute_weights(y=y, mask=mask)\n        return 2*_weighted_mean(losses=losses, weights=weights)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Location and Scale Parameters in Python using PyTorch\nDESCRIPTION: This helper method retrieves the location (shift) and scale parameters from the instance's scaler (`self.scaler`) corresponding to a specific target variable index (`y_idx`). It optionally adds a channel dimension to the returned tensors based on the `add_channel_dim` flag.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n    def _get_loc_scale(self, y_idx, add_channel_dim=False):\n        # [B, L, C, n_series] -> [B, L, n_series]\n        y_scale = self.scaler.x_scale[:, :, y_idx]\n        y_loc = self.scaler.x_shift[:, :, y_idx]\n        \n        # [B, L, n_series] -> [B, L, n_series, 1]\n        if add_channel_dim:\n            y_scale = y_scale.unsqueeze(-1)\n            y_loc = y_loc.unsqueeze(-1)\n\n        return y_loc, y_scale\n```\n\n----------------------------------------\n\nTITLE: Retrieving and Modifying Default Hyperparameter Configuration\nDESCRIPTION: Getting the default hyperparameter search space for AutoNHITS with a 12-step forecast horizon using Ray backend, and modifying specific parameters such as random seed and kernel size.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/04_hyperparameter_tuning.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nnhits_config = AutoNHITS.get_default_config(h = 12, backend=\"ray\")                      # Extract the default hyperparameter settings\nnhits_config[\"random_seed\"] = tune.randint(1, 10)                                       # Random seed\nnhits_config[\"n_pool_kernel_size\"] = tune.choice([[2, 2, 2], [16, 8, 1]])               # MaxPool's Kernelsize\n```\n\n----------------------------------------\n\nTITLE: Sampling from Probability Distributions for Forecast Quantiles in PyTorch\nDESCRIPTION: Generates samples from a specified probability distribution and computes empirical quantiles. It takes distribution parameters, creates the distribution object, samples from it, and calculates both sample means and specified quantiles.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_39\n\nLANGUAGE: python\nCODE:\n```\ndef sample(self,\n               distr_args: torch.Tensor,\n               num_samples: Optional[int] = None):\n        \"\"\"\n        Construct the empirical quantiles from the estimated Distribution,\n        sampling from it `num_samples` independently.\n\n        **Parameters**<br>\n        `distr_args`: Constructor arguments for the underlying Distribution type.<br>\n        `num_samples`: int, overwrite number of samples for the empirical quantiles.<br>\n\n        **Returns**<br>\n        `samples`: tensor, shape [B,H,`num_samples`].<br>\n        `quantiles`: tensor, empirical quantiles defined by `levels`.<br>\n        \"\"\"\n        if num_samples is None:\n            num_samples = self.num_samples\n\n        # Instantiate Scaled Decoupled Distribution\n        distr = self.get_distribution(distr_args=distr_args, **self.distribution_kwargs)\n        samples = distr.sample(sample_shape=(num_samples,))\n        samples = samples.permute(1, 2, 3, 0)                  # [samples, B, H, N] -> [B, H, N, samples]\n\n        sample_mean = torch.mean(samples, dim=-1, keepdim=True) \n\n        # Compute quantiles\n        quantiles_device = self.quantiles.to(distr_args[0].device)\n        quants = torch.quantile(input=samples, \n                                q=quantiles_device, \n                                dim=-1)\n        quants = quants.permute(1, 2, 3, 0) # [Q, B, H, N] -> [B, H, N, Q]\n\n        return samples, sample_mean, quants\n```\n\n----------------------------------------\n\nTITLE: Testing AutoInformer Model with Optuna Backend in Python\nDESCRIPTION: This snippet contains unit tests for the AutoInformer model, verifying its functionality with the Optuna backend. It checks that the model correctly handles configurations and ensures all required arguments from BaseAuto are present.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_74\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n# Check Optuna\nassert model.config(MockTrial())['h'] == 12\n\n# Unit test to test that Auto* model contains all required arguments from BaseAuto\ntest_args(AutoInformer, exclude_args=['cls_model']) \n\n# Unit test for situation: Optuna with updated default config\nmy_config = AutoInformer.get_default_config(h=12, backend='optuna')\ndef my_config_new(trial):\n    config = {**my_config(trial)}\n    config.update({'max_steps': 1, 'val_check_steps': 1, 'input_size': 12, 'hidden_size': 8})\n    return config\n\nmodel = AutoInformer(h=12, config=my_config_new, backend='optuna', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: Setting Default Export Path for NBEATS Model\nDESCRIPTION: Configures the default export path for the NBEATS model module. This directive specifies where the exported code will be saved in the project structure.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeats.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#| default_exp models.nbeats\n```\n\n----------------------------------------\n\nTITLE: Implementing QuantileLayer in PyTorch for Implicit Quantile Networks\nDESCRIPTION: This class implements the Implicit Quantile Layer from the paper 'IQN for Distributional Reinforcement Learning'. It creates a neural network layer that embeds quantiles using cosine functions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nclass QuantileLayer(nn.Module):\n    r\"\"\"\n    Implicit Quantile Layer from the paper ``IQN for Distributional\n    Reinforcement Learning`` (https://arxiv.org/abs/1806.06923) by\n    Dabney et al. 2018.\n\n    Code from GluonTS: https://github.com/awslabs/gluonts/blob/61133ef6e2d88177b32ace4afc6843ab9a7bc8cd/src/gluonts/torch/distributions/implicit_quantile_network.py\n\n    \"\"\"\n\n    def __init__(self, num_output: int, cos_embedding_dim: int = 128):\n        super().__init__()\n\n        self.output_layer = nn.Sequential(\n            nn.Linear(cos_embedding_dim, cos_embedding_dim),\n            nn.PReLU(),\n            nn.Linear(cos_embedding_dim, num_output),\n        )\n\n        self.register_buffer(\"integers\", torch.arange(0, cos_embedding_dim))\n\n    def forward(self, tau: torch.Tensor) -> torch.Tensor: \n        cos_emb_tau = torch.cos(tau * self.integers * torch.pi)\n        return self.output_layer(cos_emb_tau)\n```\n\n----------------------------------------\n\nTITLE: Instantiating TimesNet Model with Temporal Window Normalization in Python\nDESCRIPTION: Creates a TimesNet model that uses robust scaling applied at the window level during training. This approach normalizes each batch window separately at each training iteration for both target and exogenous variables.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/07_time_series_scaling.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nhorizon = 24 # day-ahead daily forecast\nmodel = TimesNet(h = horizon,                                  # Horizon\n                 input_size = 5*horizon,                       # Length of input window\n                 max_steps = 100,                              # Training iterations\n                 top_k = 3,                                    # Number of periods (for FFT).\n                 num_kernels = 3,                              # Number of kernels for Inception module\n                 batch_size = 2,                               # Number of time series per batch\n                 windows_batch_size = 32,                      # Number of windows per batch\n                 learning_rate = 0.001,                        # Learning rate\n                 futr_exog_list = ['gen_forecast','week_day'], # Future exogenous variables\n                 scaler_type = 'robust')                       # Robust scaling\n```\n\n----------------------------------------\n\nTITLE: Evaluating Quantile Function at Different Alpha Levels in PyTorch\nDESCRIPTION: Method to compute quantiles at specified alpha levels. It determines which region the alpha value falls in (left tail, right tail, or between quantile knots) and calculates the appropriate quantile value using spline interpolation or tail functions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_24\n\nLANGUAGE: Python\nCODE:\n```\ndef quantile(self, alpha: torch.Tensor) -> torch.Tensor:\n    return self.quantile_internal(alpha, dim=0)\n\ndef quantile_internal(\n    self, alpha: torch.Tensor, dim: Optional[int] = None\n) -> torch.Tensor:\n    r\"\"\"\n    Evaluates the quantile function at the quantile levels input_alpha\n    Parameters\n    ----------\n    alpha\n        Tensor of shape = (*batch_shape,) if axis=None, or containing an\n        additional axis on the specified position, otherwise\n    dim\n        Index of the axis containing the different quantile levels which\n        are to be computed.\n        Read the description below for detailed information\n    Returns\n    -------\n    Tensor\n        Quantiles tensor, of the same shape as alpha\n    \"\"\"\n\n    qk_x, qk_x_l, qk_x_plus = self.qk_x, self.qk_x_l, self.qk_x_plus\n\n    # The following describes the parameters reshaping in\n    # quantile_internal, quantile_spline and quantile_tail\n\n    # tail parameters: tail_al, tail_ar, tail_bl, tail_br,\n    # shape = (*batch_shape,)\n    # spline parameters: sk_x, sk_x_plus, sk_y, sk_y_plus,\n    # shape = (*batch_shape, num_qk-1, num_pieces)\n    # quantile knots parameters: qk_x, qk_x_plus, qk_y, qk_y_plus,\n    # shape = (*batch_shape, num_qk-1)\n\n    # dim=None - passed at inference when num_samples is None\n    # shape of input_alpha = (*batch_shape,), will be expanded to\n    # (*batch_shape, 1, 1) to perform operation\n    # The shapes of parameters are as described above,\n    # no reshaping is needed\n\n    # dim=0 - passed at inference when num_samples is not None\n    # shape of input_alpha = (num_samples, *batch_shape)\n    # it will be expanded to\n    # (num_samples, *batch_shape, 1, 1) to perform operation\n    #\n    # The shapes of tail parameters\n    # should be (num_samples, *batch_shape)\n    #\n    # The shapes of spline parameters\n    # should be (num_samples, *batch_shape, num_qk-1, num_pieces)\n    #\n    # The shapes of quantile knots parameters\n    # should be (num_samples, *batch_shape, num_qk-1)\n    #\n    # We expand at dim=0 for all of them\n\n    # dim=-2 - passed at training when we evaluate quantiles at\n    # spline knots in order to compute alpha_tilde\n    #\n    # This is only for the quantile_spline function\n    # shape of input_alpha = (*batch_shape, num_qk-1, num_pieces)\n    # it will be expanded to\n    # (*batch_shape, num_qk-1, num_pieces, 1) to perform operation\n    #\n    # The shapes of spline and quantile knots parameters should be\n    # (*batch_shape, num_qk-1, 1, num_pieces)\n    # and (*batch_shape, num_qk-1, 1), respectively\n    #\n    # We expand at dim=-2 and dim=-1 for\n    # spline and quantile knots parameters, respectively\n\n    if dim is not None:\n        qk_x_l = qk_x_l.unsqueeze(dim=dim)\n        qk_x = qk_x.unsqueeze(dim=dim)\n        qk_x_plus = qk_x_plus.unsqueeze(dim=dim)\n\n    quantile = torch.where(\n        alpha < qk_x_l,\n        self.quantile_tail(alpha, dim=dim, left_tail=True),\n        self.quantile_tail(alpha, dim=dim, left_tail=False),\n    )\n\n    spline_val = self.quantile_spline(alpha, dim=dim)\n\n    for spline_idx in range(self.num_qk - 1):\n        is_in_between = torch.logical_and(\n            qk_x[..., spline_idx] <= alpha,\n            alpha < qk_x_plus[..., spline_idx],\n        )\n\n        quantile = torch.where(\n            is_in_between,\n            spline_val[..., spline_idx],\n            quantile,\n        )\n\n    return quantile\n```\n\n----------------------------------------\n\nTITLE: Visualizing NHITS Decomposition Results\nDESCRIPTION: Plots the NHITS model's decomposition showing the original signal, forecast, and individual stack contributions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/14_interpretable_decompositions.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfig, ax = plt.subplots(3, 1, figsize=(6, 7))\n\nax[0].plot(Y_test_df['y'].values, label='True', linewidth=4)\nax[0].plot(y_hat.sum(axis=1).flatten(), label='Forecast', color=\"#7B3841\")\nax[0].legend()\nax[0].set_ylabel('Harmonic Signal')\n\nax[1].plot(y_hat[0,1]+y_hat[0,0], label='stack1', color=\"green\")\nax[1].set_ylabel('NHITS Stack 1')\n\nax[2].plot(y_hat[0,2], label='stack2', color=\"orange\")\nax[2].set_ylabel('NHITS Stack 2')\nax[2].set_xlabel(r'Prediction $\\tau \\in \\{t+1,..., t+H\\}$')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: SOFTS Forward Pass Implementation\nDESCRIPTION: Implementation of the forward pass method that processes input windows and generates predictions using the forecast method.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.softs.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, windows_batch):\n    insample_y = windows_batch['insample_y']\n\n    y_pred = self.forecast(insample_y)\n    y_pred = y_pred.reshape(insample_y.shape[0],\n                            self.h,\n                            -1)\n\n    return y_pred\n```\n\n----------------------------------------\n\nTITLE: Instantiating NHITS model with GMM loss and hist_exog_list for revin testing\nDESCRIPTION: Creates an NHITS model with GMM loss function using 10 components and tests revin dynamic dimensionality with a lagged exogenous variable. The model is configured for forecasting 12 steps ahead with early stopping and validation checks.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.scalers.ipynb#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# Instantiate BaseWindow model and test revin dynamic dimensionality with hist_exog_list\nmodel = NHITS(h=12,\n              input_size=24,\n              loss=GMM(n_components=10, level=[90]),\n              hist_exog_list=['y_[lag12]'],\n              max_steps=1,\n              early_stop_patience_steps=10,\n              val_check_steps=50,\n              scaler_type='revin',\n              learning_rate=1e-3)\nnf = NeuralForecast(models=[model], freq='MS')\nY_hat_df = nf.cross_validation(df=Y_df, val_size=12, n_windows=1)\n```\n\n----------------------------------------\n\nTITLE: CRPS Calculation for Tail Distributions in PyTorch\nDESCRIPTION: Method to compute the Continuous Ranked Probability Score (CRPS) for the tail regions of a distribution. It provides an analytical form of CRPS calculation for both left and right tails, using the CDF values and tail parameters.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_29\n\nLANGUAGE: Python\nCODE:\n```\ndef crps_tail(\n    self, z: torch.Tensor, left_tail: bool = True\n) -> torch.Tensor:\n    r\"\"\"\n    Compute CRPS in analytical form for left/right tails\n    Parameters\n    ----------\n    z\n        Observation to evaluate. shape = (*batch_shape,)\n    left_tail\n        If True, compute CRPS for the left tail\n        Otherwise, compute CRPS for the right tail\n    Returns\n    -------\n    Tensor\n        Tensor containing the CRPS, of the same shape as z\n    \"\"\"\n\n    alpha_tilde = self.cdf_tail(z, left_tail=left_tail)\n\n    if left_tail:\n        tail_a, tail_b, qk_x, qk_y = (\n            self.tail_al,\n            self.tail_bl,\n            self.qk_x_l,\n            self.qk_y_l,\n        )\n        term1 = (z - tail_b) * (qk_x**2 - 2 * qk_x + 2 * alpha_tilde)\n        term2 = qk_x**2 * tail_a * (-torch.log(qk_x) + 0.5)\n        term2 = term2 + 2 * torch.where(\n            z < qk_y,\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing ETTm2 Dataset\nDESCRIPTION: Loads the ETTm2 benchmark dataset using the LongHorizon class, merges exogenous features, and prepares validation and test splits for time series forecasting.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/08_multivariate_tsmixer.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\nfrom datasetsforecast.long_horizon import LongHorizon\n```\n\nLANGUAGE: python\nCODE:\n```\n# Change this to your own data to try the model\nY_df, X_df, _ = LongHorizon.load(directory='./', group='ETTm2')\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\n\n# X_df contains the exogenous features, which we add to Y_df\nX_df['ds'] = pd.to_datetime(X_df['ds'])\nY_df = Y_df.merge(X_df, on=['unique_id', 'ds'], how='left')\n\n# We make validation and test splits\nn_time = len(Y_df.ds.unique())\nval_size = int(.2 * n_time)\ntest_size = int(.2 * n_time)\n```\n\nLANGUAGE: python\nCODE:\n```\nY_df\n```\n\n----------------------------------------\n\nTITLE: Preparing AirPassengers Dataset for Transfer Learning in Python\nDESCRIPTION: Prepares the AirPassengers dataset by splitting it into training and testing sets, and calculating mean and standard deviation for potential normalization.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/17_transfer_learning.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# We define the train df. \nY_df = AirPassengersDF.copy()\nmean = Y_df[Y_df.ds<='1959-12-31']['y'].mean()\nstd = Y_df[Y_df.ds<='1959-12-31']['y'].std()\n\nY_train_df = Y_df[Y_df.ds<='1959-12-31'] # 132 train\nY_test_df = Y_df[Y_df.ds>'1959-12-31']   # 12 test\n```\n\n----------------------------------------\n\nTITLE: EncoderLayer Implementation\nDESCRIPTION: Implements a single encoder layer combining self-attention and cross-attention mechanisms with feed-forward networks.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timexer.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass EncoderLayer(nn.Module):\n    def __init__(self, self_attention, cross_attention, d_model, d_ff=None,\n                 dropout=0.1, activation=\"relu\"):\n        super(EncoderLayer, self).__init__()\n        d_ff = d_ff or 4 * d_model\n        self.self_attention = self_attention\n        self.cross_attention = cross_attention\n        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.activation = F.relu if activation == \"relu\" else F.gelu\n\n    def forward(self, x, cross, x_mask=None, cross_mask=None, tau=None, delta=None):\n        B, L, D = cross.shape\n        x = x + self.dropout(self.self_attention(\n            x, x, x,\n            attn_mask=x_mask,\n            tau=tau, delta=None\n        )[0])\n        x = self.norm1(x)\n\n        x_glb_ori = x[:, -1, :].unsqueeze(1)\n        x_glb = torch.reshape(x_glb_ori, (B, -1, D))\n        x_glb_attn = self.dropout(self.cross_attention(\n            x_glb, cross, cross,\n            attn_mask=cross_mask,\n            tau=tau, delta=delta\n        )[0])\n        x_glb_attn = torch.reshape(x_glb_attn,\n                                   (x_glb_attn.shape[0] * x_glb_attn.shape[1], x_glb_attn.shape[2])).unsqueeze(1)\n        x_glb = x_glb_ori + x_glb_attn\n        x_glb = self.norm2(x_glb)\n\n        y = x = torch.cat([x[:, :-1, :], x_glb], dim=1)\n\n        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n        y = self.dropout(self.conv2(y).transpose(-1, 1))\n\n        return self.norm3(x + y)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for NeuralForecast with MLflow\nDESCRIPTION: Imports necessary libraries for time series forecasting with NeuralForecast and MLflow tracking. Includes MLflow, pandas, numpy, matplotlib for visualization, and NeuralForecast components for model creation and data handling.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/12_using_mlflow.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport mlflow\nimport mlflow.data\nimport numpy as np\nimport pandas as pd\nfrom mlflow.client import MlflowClient\nfrom mlflow.data.pandas_dataset import PandasDataset\nfrom utilsforecast.plotting import plot_series\n\nfrom neuralforecast.core import NeuralForecast\nfrom neuralforecast.models import NBEATSx\nfrom neuralforecast.utils import AirPassengersDF\nfrom neuralforecast.losses.pytorch import MAE\n```\n\n----------------------------------------\n\nTITLE: Installing NeuralForecast and DatasetsForecast Libraries\nDESCRIPTION: Installs the required Python libraries for the tutorial, including neuralforecast for the forecasting models and datasetsforecast for accessing benchmark datasets.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/08_multivariate_tsmixer.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n!pip install neuralforecast datasetsforecast\n```\n\n----------------------------------------\n\nTITLE: Visualizing Actual vs Predicted Values\nDESCRIPTION: Plots the original time series data against the forecasts generated by the LSTM and NHITS models to visually compare their performance.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/getting-started/02_quickstart.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nplot_series(Y_df, Y_hat_df)\n```\n\n----------------------------------------\n\nTITLE: Using AutoNHITS for Hyperparameter Tuning\nDESCRIPTION: Example of initializing, fitting, and predicting with the AutoNHITS model using both custom configuration and Optuna backend. The code demonstrates simplified setup for quick experimentation with minimal hyperparameter tuning.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_43\n\nLANGUAGE: python\nCODE:\n```\n# Use your own config or AutoNHITS.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12, \n              mlp_units=3 * [[8, 8]])\nmodel = AutoNHITS(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n# Optuna\nmodel = AutoNHITS(h=12, config=None, backend='optuna')\n```\n\n----------------------------------------\n\nTITLE: Preparing Data for Distributed Training in NeuralForecast\nDESCRIPTION: This method prepares the input data for distributed training using Spark DataFrames. It handles data partitioning, file writing, and dataset creation for distributed processing.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndef _prepare_fit_distributed(\n    self,\n    df: SparkDataFrame,\n    static_df: Optional[SparkDataFrame],\n    id_col: str,\n    time_col: str,\n    target_col: str,\n    distributed_config: Optional[DistributedConfig],\n):\n    if distributed_config is None:\n        raise ValueError(\n            \"Must set `distributed_config` when using a spark dataframe\"\n        )\n    if self.local_scaler_type is not None:\n        raise ValueError(\n            \"Historic scaling isn't supported in distributed. \"\n            \"Please open an issue if this would be valuable to you.\"\n        )\n    temporal_cols = [c for c in df.columns if c not in (id_col, time_col)]\n    if static_df is not None:\n        static_cols = [c for c in static_df.columns if c != id_col]\n        df = df.join(static_df, on=[id_col], how=\"left\")\n    else:\n        static_cols = None\n    self.id_col = id_col\n    self.time_col = time_col\n    self.target_col = target_col\n    self.scalers_ = {}\n    num_partitions = distributed_config.num_nodes * distributed_config.devices\n    df = df.repartitionByRange(num_partitions, id_col)\n    df.write.parquet(path=distributed_config.partitions_path, mode=\"overwrite\")\n    fs, _, _ = fsspec.get_fs_token_paths(distributed_config.partitions_path)\n    protocol = fs.protocol \n    if isinstance(protocol, tuple):\n        protocol = protocol[0]\n    files = [\n        f'{protocol}://{file}'\n        for file in fs.ls(distributed_config.partitions_path)\n        if file.endswith(\"parquet\")\n    ]\n    return _FilesDataset(\n        files=files,\n        temporal_cols=temporal_cols,\n        static_cols=static_cols,\n        id_col=id_col,\n        time_col=time_col,\n        target_col=target_col,\n        min_size=df.groupBy(id_col).count().agg({\"count\": \"min\"}).first()[0],\n    )\n```\n\n----------------------------------------\n\nTITLE: Loading NeuralForecast Models\nDESCRIPTION: This static method loads a previously saved NeuralForecast model from disk. It restores model checkpoints, configuration settings, and dataset (if available). It includes backward compatibility handling for different versions of the library.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n@staticmethod\ndef load(path, verbose=False, **kwargs):\n    \"\"\"Load NeuralForecast\n\n    `core.NeuralForecast`'s method to load checkpoint from path.\n\n    Parameters\n    -----------\n    path : str\n        Directory with stored artifacts.\n    kwargs\n        Additional keyword arguments to be passed to the function\n        `load_from_checkpoint`.\n\n    Returns\n    -------\n    result : NeuralForecast\n        Instantiated `NeuralForecast` class.\n    \"\"\"\n    # Standarize path without '/'\n    if path[-1] == '/':\n        path = path[:-1]\n    \n    fs, _, _ = fsspec.get_fs_token_paths(path)\n    files = [f.split('/')[-1] for f in fs.ls(path) if fs.isfile(f)]\n\n    # Load models\n    models_ckpt = [f for f in files if f.endswith('.ckpt')]\n    if len(models_ckpt) == 0:\n        raise Exception('No model found in directory.') \n    \n    if verbose: print(10 * '-' + ' Loading models ' + 10 * '-')\n    models = []\n    try:\n        with fsspec.open(f'{path}/alias_to_model.pkl', 'rb') as f:\n            alias_to_model = pickle.load(f)\n    except FileNotFoundError:\n        alias_to_model = {}\n    for model in models_ckpt:\n        model_name = '_'.join(model.split('_')[:-1])\n        model_class_name = alias_to_model.get(model_name, model_name)\n        loaded_model = MODEL_FILENAME_DICT[model_class_name].load(f'{path}/{model}', **kwargs)\n        loaded_model.alias = model_name\n        models.append(loaded_model)\n        if verbose: print(f\"Model {model_name} loaded.\")\n\n    if verbose: print(10*'-' + ' Loading dataset ' + 10*'-')\n    # Load dataset\n    try:\n        with fsspec.open(f\"{path}/dataset.pkl\", \"rb\") as f:\n            dataset = pickle.load(f)\n        if verbose: print('Dataset loaded.')\n    except FileNotFoundError:\n        dataset = None\n        if verbose: print('No dataset found in directory.')\n\n    if verbose: print(10*'-' + ' Loading configuration ' + 10*'-')\n    # Load configuration\n    try:\n        with fsspec.open(f\"{path}/configuration.pkl\", \"rb\") as f:\n            config_dict = pickle.load(f)\n        if verbose: print('Configuration loaded.')\n    except FileNotFoundError:\n        raise Exception('No configuration found in directory.')\n\n    # in 1.6.4, `local_scaler_type` / `scalers_` lived on the dataset.\n    # in order to preserve backwards-compatibility, we check to see if these are found on the dataset\n    # in case they cannot be found in `config_dict`\n    default_scalar_type = getattr(dataset, \"local_scaler_type\", None)\n    default_scalars_ = getattr(dataset, \"scalers_\", None)\n\n    # Create NeuralForecast object\n    neuralforecast = NeuralForecast(\n        models=models,\n        freq=config_dict['freq'],\n        local_scaler_type=config_dict.get(\"local_scaler_type\", default_scalar_type),\n    )\n\n    attr_to_default = {\n        \"id_col\": \"unique_id\",\n        \"time_col\": \"ds\",\n        \"target_col\": \"y\"\n    }\n    for attr, default in attr_to_default.items():\n        setattr(neuralforecast, attr, config_dict.get(attr, default))\n    # only restore attribute if available\n    for attr in ['prediction_intervals', '_cs_df']:\n        setattr(neuralforecast, attr, config_dict.get(attr, None))\n```\n\n----------------------------------------\n\nTITLE: Implementing MLP Neural Forecasting Model Class\nDESCRIPTION: Defines a Multi-Layer Perceptron model class inheriting from BaseModel with initialization and forward pass methods.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/18_adding_models.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass MLP(BaseModel): # <<---- Inherits from BaseModel\n    # Set class attributes to determine this model's characteristics\n    EXOGENOUS_FUTR = False   # If the model can handle future exogenous variables\n    EXOGENOUS_HIST = False   # If the model can handle historical exogenous variables\n    EXOGENOUS_STAT = False   # If the model can handle static exogenous variables\n    MULTIVARIATE = False    # If the model produces multivariate forecasts (True) or univariate (False)\n    RECURRENT = False       # If the model produces forecasts recursively (True) or direct (False)\n\n    def __init__(self,\n                 # Inhereted hyperparameters with no defaults\n                 h,\n                 input_size,\n                 # Model specific hyperparameters\n                 num_layers = 2,\n                 hidden_size = 1024,\n                 # Inhereted hyperparameters with defaults\n                 futr_exog_list = None,\n                 hist_exog_list = None,\n                 stat_exog_list = None,                 \n                 exclude_insample_y = False,\n                 loss = MAE(),\n                 valid_loss = None,\n                 max_steps: int = 1000,\n                 learning_rate: float = 1e-3,\n                 num_lr_decays: int = -1,\n                 early_stop_patience_steps: int =-1,\n                 val_check_steps: int = 100,\n                 batch_size: int = 32,\n                 valid_batch_size: Optional[int] = None,\n                 windows_batch_size = 1024,\n                 inference_windows_batch_size = -1,\n                 start_padding_enabled = False,\n                 step_size: int = 1,\n                 scaler_type: str = 'identity',\n                 random_seed: int = 1,\n                 drop_last_loader: bool = False,\n                 optimizer = None,\n                 optimizer_kwargs = None,\n                 lr_scheduler = None,\n                 lr_scheduler_kwargs = None,\n                 dataloader_kwargs = None,\n                 **trainer_kwargs):\n    # Inherit BaseWindows class\n    super(MLP, self).__init__(h=h,\n                              input_size=input_size,\n                              ..., # <<--- Add all inhereted hyperparameters\n                              random_seed=random_seed,\n                              **trainer_kwargs)\n\n    # Architecture\n    self.num_layers = num_layers\n    self.hidden_size = hidden_size\n\n    # MultiLayer Perceptron\n    layers = [nn.Linear(in_features=input_size, out_features=hidden_size)]\n    layers += [nn.ReLU()]\n    for i in range(num_layers - 1):\n        layers += [nn.Linear(in_features=hidden_size, out_features=hidden_size)]\n        layers += [nn.ReLU()]\n    self.mlp = nn.ModuleList(layers)\n\n    # Adapter with Loss dependent dimensions\n    self.out = nn.Linear(in_features=hidden_size, \n                         out_features=h * self.loss.outputsize_multiplier) ## <<--- Use outputsize_multiplier to adjust output size\n\n    def forward(self, windows_batch): # <<--- Receives windows_batch dictionary\n        # Parse windows_batch\n        insample_y = windows_batch['insample_y'].squeeze(-1)                            # [batch_size, input_size]\n        # MLP\n        hidden = self.mlp(insample_y)                                                   # [batch_size, hidden_size]\n        y_pred = self.out(hidden)                                                       # [batch_size, h * n_outputs]\n        \n        # Reshape\n        y_pred = y_pred.reshape(batch_size, self.h, self.loss.outputsize_multiplier)    # [batch_size, h, n_outputs]\n\n        return y_pred\n```\n\n----------------------------------------\n\nTITLE: Configuring Loss Functions in Python\nDESCRIPTION: This snippet checks and configures loss functions for training and validation, raising exceptions for incompatible loss types.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nif isinstance(self.loss, (losses.relMSE, losses.Accuracy, losses.sCRPS)):\n    raise Exception(f\"{type(self.loss).__name__} cannot be used for training. Please use another loss function (MAE, MSE, ...)\")\n\nif isinstance(self.valid_loss, (losses.relMSE)):\n    raise Exception(f\"{type(self.valid_loss).__name__} cannot be used for validation. Please use another valid_loss (MAE, MSE, ...)\")\n```\n\n----------------------------------------\n\nTITLE: Visualizing Future Variable Importances\nDESCRIPTION: Creates a horizontal bar plot of the average importance for future variables in the TFT model. This visualization shows which future information has the greatest overall impact on the model's predictions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n# | eval: false\nfeature_importances[\"Future variable importance over time\"].mean().sort_values().plot(\n    kind=\"barh\"\n)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Best Hyperparameter Configurations in Python\nDESCRIPTION: These snippets show how to access the best hyperparameter configurations found during the optimization process for both AutoTSMixer and AutoTSMixerx models.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/08_multivariate_tsmixer.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nnf.models[0].results.get_best_result().config\n```\n\nLANGUAGE: python\nCODE:\n```\nnf.models[1].results.get_best_result().config\n```\n\n----------------------------------------\n\nTITLE: Training NeuralForecast Model with AutoNHITS in Python\nDESCRIPTION: This snippet demonstrates how to train a NeuralForecast model using the AutoNHITS model. It sets the validation set size to twice the forecasting horizon.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/04_hyperparameter_tuning.ipynb#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n%%capture\nnf = NeuralForecast(models=[model], freq='ME')\nnf.fit(df=Y_df, val_size=24)\n```\n\n----------------------------------------\n\nTITLE: Evaluating RUL Predictions with RMSE\nDESCRIPTION: Evaluates the RUL predictions using root mean square error by comparing predicted values with actual RUL values.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/use-cases/predictive_maintenance.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmetrics = evaluate(Y_hat_df.merge(Y_test_df[[\"unique_id\", \"ds\", \"y\"]], on=['unique_id', 'ds']),\n                   metrics=[rmse],\n                   agg_fn='mean')\n\nmetrics\n```\n\n----------------------------------------\n\nTITLE: Using AutoDLinear for Hyperparameter Tuning\nDESCRIPTION: Example of initializing, fitting, and predicting with the AutoDLinear model using both custom configuration and Optuna backend. The code demonstrates quick setup for experimentation with minimal steps.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_46\n\nLANGUAGE: python\nCODE:\n```\n# Use your own config or AutoDLinear.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12)\nmodel = AutoDLinear(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n# Optuna\nmodel = AutoDLinear(h=12, config=None, backend='optuna')\n```\n\n----------------------------------------\n\nTITLE: Forecasting with Trained NeuralForecast Model on Large Dataset in Python\nDESCRIPTION: This code demonstrates how to generate predictions using the trained NeuralForecast model. It prepares the input data for a specific airline and forecasts the next 12 timesteps.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/19_large_datasets.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nvalid_df = valid[valid['id_col'] == 'Airline2']\npred_df = valid_df[:60]\nfutr_df = valid_df[60:72]\nfutr_df = futr_df.drop([\"y\"], axis=1)\n\npredictions = nf.predict(df=pred_df, futr_df=futr_df, static_df=static)\n```\n\nLANGUAGE: python\nCODE:\n```\npredictions\n```\n\n----------------------------------------\n\nTITLE: Spline-Based CDF Calculation in PyTorch\nDESCRIPTION: Method to compute the inverse of the quantile function (effectively the CDF) for given values using spline interpolation. It determines the quantile level alpha_tilde that corresponds to the observation z by evaluating and selecting the appropriate spline piece.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_27\n\nLANGUAGE: Python\nCODE:\n```\ndef cdf_spline(self, z: torch.Tensor) -> torch.Tensor:\n    r\"\"\"\n    For observations z and splines defined in [qk_x[k], qk_x[k+1]]\n    Computes the quantile level alpha_tilde such that\n    alpha_tilde\n    = q^{-1}(z) if z is in-between qk_x[k] and qk_x[k+1]\n    = qk_x[k] if z<qk_x[k]\n    = qk_x[k+1] if z>qk_x[k+1]\n    Parameters\n    ----------\n    z\n        Observation, shape = (*batch_shape,)\n    Returns\n    -------\n    alpha_tilde\n        Corresponding quantile level, shape = (*batch_shape, num_qk-1)\n    \"\"\"\n\n    qk_y, qk_y_plus = self.qk_y, self.qk_y_plus\n    qk_x, qk_x_plus = self.qk_x, self.qk_x_plus\n    sk_x, delta_sk_x, delta_sk_y = (\n        self.sk_x,\n        self.delta_sk_x,\n        self.delta_sk_y,\n    )\n\n    z_expand = z.unsqueeze(dim=-1)\n\n    if self.num_pieces > 1:\n        qk_y_expand = qk_y.unsqueeze(dim=-1)\n        z_expand_twice = z_expand.unsqueeze(dim=-1)\n\n        knots_eval = self.quantile_spline(sk_x, dim=-2)\n\n        # Compute \\sum_{s=0}^{s_0-1} \\Delta sk_y[s],\n        # where \\Delta sk_y[s] = (sk_y[s+1]-sk_y[s])\n        mask_sum_s0 = torch.lt(knots_eval, z_expand_twice)\n        mask_sum_s0_minus = torch.cat(\n            [\n                mask_sum_s0[..., 1:],\n                torch.zeros_like(qk_y_expand, dtype=torch.bool),\n            ],\n            dim=-1,\n        )\n        sum_delta_sk_y = torch.sum(mask_sum_s0_minus * delta_sk_y, dim=-1)\n\n        mask_s0_only = torch.logical_and(\n            mask_sum_s0, torch.logical_not(mask_sum_s0_minus)\n        )\n        # Compute (sk_x[s_0+1]-sk_x[s_0])/(sk_y[s_0+1]-sk_y[s_0])\n        frac_s0 = torch.sum(\n            (mask_s0_only * delta_sk_x) / delta_sk_y, dim=-1\n        )\n\n        # Compute sk_x_{s_0}\n        sk_x_s0 = torch.sum(mask_s0_only * sk_x, dim=-1)\n\n        # Compute alpha_tilde\n        alpha_tilde = (\n            sk_x_s0 + (z_expand - qk_y - sum_delta_sk_y) * frac_s0\n        )\n\n    else:\n        # num_pieces=1, ISQF reduces to IQF\n        alpha_tilde = qk_x + (z_expand - qk_y) / (qk_y_plus - qk_y) * (\n            qk_x_plus - qk_x\n        )\n\n    alpha_tilde = torch.minimum(\n        torch.maximum(alpha_tilde, qk_x), qk_x_plus\n    )\n\n    return alpha_tilde\n```\n\n----------------------------------------\n\nTITLE: Configuring AutoTFT Model with Hyperparameter Tuning\nDESCRIPTION: Defines the forecasting horizon and creates an AutoTFT model instance with Mean Absolute Error loss function and automatic hyperparameter search. Sets the number of configurations to explore to 3.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/07_forecasting_tft.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nhorizon = 24\nmodels = [AutoTFT(h=horizon,\n                  loss=MAE(),\n                  config=None,\n                  num_samples=3)]\n```\n\n----------------------------------------\n\nTITLE: Implementing Forward Method for MLP\nDESCRIPTION: This snippet defines the forward method of the MLP class. It processes the input data, flattens it, and passes it through the multi-layer perceptron to generate forecasts.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.mlp.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, windows_batch):\n\n    # Parse windows_batch\n    insample_y    = windows_batch['insample_y'].squeeze(-1)\n    futr_exog     = windows_batch['futr_exog']\n    hist_exog     = windows_batch['hist_exog']\n    stat_exog     = windows_batch['stat_exog']\n\n    # Flatten MLP inputs [B, L+H, C] -> [B, (L+H)*C]\n    # Contatenate [ Y_t, | X_{t-L},..., X_{t} | F_{t-L},..., F_{t+H} | S ]\n    batch_size = len(insample_y)\n```\n\n----------------------------------------\n\nTITLE: Saving NeuralForecast Models\nDESCRIPTION: This method saves the current state of NeuralForecast models, including model checkpoints, dataset (if specified), and configuration parameters. It supports saving to local or remote storage using fsspec and provides options to control which models to save.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_22\n\nLANGUAGE: python\nCODE:\n```\ndef save(self, path: str, model_index: Optional[List]=None, save_dataset: bool=True, overwrite: bool=False):\n    \"\"\"Save NeuralForecast core class.\n\n    `core.NeuralForecast`'s method to save current status of models, dataset, and configuration.\n    Note that by default the `models` are not saving training checkpoints to save disk memory,\n    to get them change the individual model `**trainer_kwargs` to include `enable_checkpointing=True`.\n\n    Parameters\n    ----------\n    path : str\n        Directory to save current status.\n    model_index : list, optional (default=None)\n        List to specify which models from list of self.models to save.\n    save_dataset : bool (default=True)\n        Whether to save dataset or not.\n    overwrite : bool (default=False)\n        Whether to overwrite files or not.\n    \"\"\"\n    # Standarize path without '/'\n    if path[-1] == '/':\n        path = path[:-1]\n\n    # Model index list\n    if model_index is None:\n        model_index = list(range(len(self.models)))\n\n    fs, _, _ = fsspec.get_fs_token_paths(path)\n    if not fs.exists(path):\n        fs.makedirs(path)\n    else:\n        # Check if directory is empty to protect overwriting files\n        files = fs.ls(path)\n\n        # Checking if the list is empty or not\n        if files:\n            if not overwrite:\n                raise Exception('Directory is not empty. Set `overwrite=True` to overwrite files.')\n            else:\n                fs.rm(path, recursive=True)\n                fs.mkdir(path)\n\n    # Save models\n    count_names = {'model': 0}\n    alias_to_model = {}\n    for i, model in enumerate(self.models):\n        # Skip model if not in list\n        if i not in model_index:\n            continue\n\n        model_name = repr(model)\n        model_class_name = model.__class__.__name__.lower()\n        alias_to_model[model_name] = model_class_name\n        count_names[model_name] = count_names.get(model_name, -1) + 1\n        model.save(f\"{path}/{model_name}_{count_names[model_name]}.ckpt\")\n    with fsspec.open(f\"{path}/alias_to_model.pkl\", \"wb\") as f:\n        pickle.dump(alias_to_model, f)\n\n    # Save dataset\n    if save_dataset and hasattr(self, 'dataset'):\n        if isinstance(self.dataset, _FilesDataset):\n            raise ValueError(\n                \"Cannot save distributed dataset.\\n\"\n                \"You can set `save_dataset=False` and use the `df` argument in the predict method after loading \"\n                \"this model to use it for inference.\"\n            )\n        with fsspec.open(f\"{path}/dataset.pkl\", \"wb\") as f:\n            pickle.dump(self.dataset, f)\n    elif save_dataset:\n        raise Exception('You need to have a stored dataset to save it, \\\n                         set `save_dataset=False` to skip saving dataset.')\n\n    # Save configuration and parameters\n    config_dict = {\n        \"h\": self.h,\n        \"freq\": self.freq,\n        \"_fitted\": self._fitted,\n        \"local_scaler_type\": self.local_scaler_type,\n        \"scalers_\": self.scalers_,\n        \"id_col\": self.id_col,\n        \"time_col\": self.time_col,\n        \"target_col\": self.target_col,\n    }\n    for attr in ['prediction_intervals', '_cs_df']:\n        # conformal prediction related attributes was not available < 1.7.6\n        config_dict[attr] = getattr(self, attr, None)\n        \n\n    if save_dataset:\n        config_dict.update(\n            {\n                \"uids\": self.uids,\n                \"last_dates\": self.last_dates,\n                \"ds\": self.ds,\n            }\n        )\n\n    with fsspec.open(f\"{path}/configuration.pkl\", \"wb\") as f:\n        pickle.dump(config_dict, f)\n```\n\n----------------------------------------\n\nTITLE: Implementing Data Smoothing and Sensor Visualization\nDESCRIPTION: Defines a smoothing function and creates a visualization of original and smoothed sensor readings for a single engine.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/use-cases/predictive_maintenance.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef smooth(s, b = 0.98):\n    v = np.zeros(len(s)+1) #v_0 is already 0.\n    bc = np.zeros(len(s)+1)\n    for i in range(1, len(v)): #v_t = 0.95\n        v[i] = (b * v[i-1] + (1-b) * s[i-1]) \n        bc[i] = 1 - b**i\n    sm = v[1:] / bc[1:]\n    return sm\n\nunique_id = 1\nplot_df = Y_train_df[Y_train_df.unique_id == unique_id].copy()\n\nfig, axes = plt.subplots(2,3, figsize = (8,5))\nfig.tight_layout()\n\nj = -1\n#, 's_11', 's_12', 's_13', 's_14', 's_15', 's_17', 's_20', 's_21'\nfor feature in ['s_2', 's_3', 's_4', 's_7', 's_8', 's_9']:\n    if ('s' in feature) and ('smoothed' not in feature):\n        j += 1\n        axes[j // 3, j % 3].plot(plot_df.ds, plot_df[feature], \n                                 c = '#2D6B8F', label = 'original')\n        axes[j // 3, j % 3].plot(plot_df.ds, smooth(plot_df[feature].values), \n                                 c = '#CA6F6A', label = 'smoothed')\n        #axes[j // 3, j % 3].plot([10,10],[0,1], c = 'black')\n        axes[j // 3, j % 3].set_title(feature)\n        axes[j // 3, j % 3].grid()\n        axes[j // 3, j % 3].legend()\n        \nplt.suptitle(f'Engine {unique_id} sensor records')\nplt.tight_layout()\n```\n\n----------------------------------------\n\nTITLE: Implementing TemporalMixing Layer for TSMixer in Python\nDESCRIPTION: Defines the TemporalMixing layer, a key component of TSMixer. This layer applies temporal normalization, linear transformation, and dropout to the input data, focusing on temporal relationships in the time series.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tsmixer.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass TemporalMixing(nn.Module):\n    \"\"\" \n    TemporalMixing\n    \"\"\"\n    def __init__(self, n_series, input_size, dropout):\n        super().__init__()\n        self.temporal_norm = nn.BatchNorm1d(num_features=n_series * input_size, eps=0.001, momentum=0.01)\n        self.temporal_lin = nn.Linear(input_size, input_size)\n        self.temporal_drop = nn.Dropout(dropout)\n\n    def forward(self, input):\n        # Get shapes\n        batch_size = input.shape[0]\n        input_size = input.shape[1]\n        n_series = input.shape[2]\n\n        # Temporal MLP\n        x = input.permute(0, 2, 1)                                      # [B, L, N] -> [B, N, L]\n        x = x.reshape(batch_size, -1)                                   # [B, N, L] -> [B, N * L]\n        x = self.temporal_norm(x)                                       # [B, N * L] -> [B, N * L]\n        x = x.reshape(batch_size, n_series, input_size)                 # [B, N * L] -> [B, N, L]\n        x = F.relu(self.temporal_lin(x))                                # [B, N, L] -> [B, N, L]\n        x = x.permute(0, 2, 1)                                          # [B, N, L] -> [B, L, N]\n        x = self.temporal_drop(x)                                       # [B, L, N] -> [B, L, N]\n\n        return x + input \n```\n\n----------------------------------------\n\nTITLE: Configuring Batch Sizes and Optimization Parameters in Python\nDESCRIPTION: This snippet sets up batch sizes for training, validation, and inference. It also configures learning rate, decay steps, and other optimization parameters.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nif self.MULTIVARIATE and n_series is not None:\n    self.batch_size = max(batch_size, n_series)\n    if valid_batch_size is not None:\n        valid_batch_size = max(valid_batch_size, n_series)\nelse:\n    self.batch_size = batch_size\n\nif valid_batch_size is None:\n    self.valid_batch_size = self.batch_size\nelse:\n    self.valid_batch_size = valid_batch_size\n\nif inference_windows_batch_size is None:\n    self.inference_windows_batch_size = windows_batch_size\nelse:\n    self.inference_windows_batch_size = inference_windows_batch_size\n\nself.learning_rate = learning_rate\nself.max_steps = max_steps\nself.num_lr_decays = num_lr_decays\nself.lr_decay_steps = (\n    max(max_steps // self.num_lr_decays, 1) if self.num_lr_decays > 0 else 10e7\n)\nself.early_stop_patience_steps = early_stop_patience_steps\nself.val_check_steps = val_check_steps\nself.windows_batch_size = windows_batch_size\nself.step_size = step_size\n```\n\n----------------------------------------\n\nTITLE: Calculating In-Sample Times for Time Series Data\nDESCRIPTION: Function to calculate in-sample timestamps for time series forecasting based on user-provided parameters. Handles multiple time series with varying window sizes and step sizes.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef _insample_times(\n    times: np.ndarray,\n    uids: Series,\n    indptr: np.ndarray,\n    h: int,\n    freq: Union[int, str, pd.offsets.BaseOffset],\n    step_size: int = 1,\n    id_col: str = 'unique_id',\n    time_col: str = 'ds',\n) -> DataFrame:\n```\n\n----------------------------------------\n\nTITLE: Development Dependencies for Testing and Documentation\nDESCRIPTION: Imports additional modules used for development purposes including logging, testing, and documentation utilities.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tcn.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport warnings\nfrom fastcore.test import test_eq\nfrom nbdev.showdoc import show_doc\nfrom neuralforecast.common._model_checks import check_model\n```\n\n----------------------------------------\n\nTITLE: Root Mean Squared Error Implementation in PyTorch\nDESCRIPTION: Implementation of Root Mean Squared Error (RMSE) loss function that calculates the square root of the average squared differences between predicted and actual values.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass RMSE(BasePointLoss):\n    def __init__(self, horizon_weight=None):\n        super(RMSE, self).__init__(horizon_weight=horizon_weight,\n                                  outputsize_multiplier=1,\n                                  output_names=[''])\n\n    def __call__(self,\n                 y: torch.Tensor,\n                 y_hat: torch.Tensor,\n                 mask: Union[torch.Tensor, None] = None,\n                 y_insample: Union[torch.Tensor, None] = None) -> torch.Tensor:\n        losses = (y - y_hat)**2\n        weights = self._compute_weights(y=y, mask=mask)\n        losses = _weighted_mean(losses=losses, weights=weights)\n        return torch.sqrt(losses)\n```\n\n----------------------------------------\n\nTITLE: Using AutoDeepAR for Fitting, Predicting, and Optuna Backend in Python\nDESCRIPTION: This snippet demonstrates the usage of the `AutoDeepAR` class. It initializes the model with a custom configuration, setting `max_steps`, `input_size`, and `lstm_hidden_size`. It then fits the model to a dataset and performs prediction. Subsequently, it shows how to initialize `AutoDeepAR` for use with the Optuna backend by specifying `backend='optuna'` and `config=None`. The `%%capture` magic command suppresses the cell's output.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n# Use your own config or AutoDeepAR.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12, lstm_hidden_size=8)\nmodel = AutoDeepAR(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n# Optuna\nmodel = AutoDeepAR(h=12, config=None, backend='optuna')\n```\n\n----------------------------------------\n\nTITLE: Fitting Neural Forecasting Model with Ray Tune\nDESCRIPTION: This method fits a neural forecasting model using Ray Tune for hyperparameter optimization. It sets up the tuning process, configures resources, and handles platform-specific settings.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_auto.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\ndef _tune_model(self, cls_model, dataset, val_size, test_size,\n                cpus, gpus, verbose, num_samples, search_alg, config):\n    train_fn_with_parameters = tune.with_parameters(\n        self._train_tune,\n        cls_model=cls_model,\n        dataset=dataset,\n        val_size=val_size,\n        test_size=test_size,\n    )\n\n    # Device\n    if gpus > 0:\n        device_dict = {'gpu':gpus}\n    else:\n        device_dict = {'cpu':cpus}\n\n    # on Windows, prevent long trial directory names\n    import platform\n    trial_dirname_creator=(lambda trial: f\"{trial.trainable_name}_{trial.trial_id}\") if platform.system() == 'Windows' else None\n\n    tuner = tune.Tuner(\n        tune.with_resources(train_fn_with_parameters, device_dict),\n        run_config=air.RunConfig(callbacks=self.callbacks, verbose=verbose),\n        tune_config=tune.TuneConfig(\n            metric=\"loss\",\n            mode=\"min\",\n            num_samples=num_samples, \n            search_alg=search_alg,\n            trial_dirname_creator=trial_dirname_creator,\n        ),\n        param_space=config,\n    )\n    results = tuner.fit()\n    return results\n```\n\n----------------------------------------\n\nTITLE: Defining AutoDeepAR Class for Automated Tuning in Python\nDESCRIPTION: This snippet defines the `AutoDeepAR` class for automated hyperparameter tuning of the DeepAR model. Similar to `AutoTCN`, it inherits from `BaseAuto`, defines a `default_config` search space with `ray.tune` parameters specific to DeepAR (like LSTM layers, dropout, scaler type), and includes an `__init__` method. The `get_default_config` class method generates the appropriate configuration, setting `input_size` and `step_size` relative to the horizon `h`, and handles conversion for the Optuna backend.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass AutoDeepAR(BaseAuto):\n\n    default_config = {\n        \"input_size_multiplier\": [1, 2, 3, 4, 5],\n        \"h\": None,\n        \"lstm_hidden_size\": tune.choice([32, 64, 128, 256]),\n        \"lstm_n_layers\": tune.randint(1, 4),\n        \"lstm_dropout\": tune.uniform(0.0, 0.5),\n        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n        \"scaler_type\": tune.choice(['robust', 'minmax1']),\n        \"max_steps\": tune.choice([500, 1000, 2000]),\n        \"batch_size\": tune.choice([32, 64, 128, 256]),\n        \"windows_batch_size\": tune.choice([128, 256, 512, 1024]),\n        \"loss\": None,\n        \"random_seed\": tune.randint(1, 20),\n    }\n\n    def __init__(self,\n                 h,\n                 loss=DistributionLoss(distribution='StudentT', level=[80, 90], return_params=False),\n                 valid_loss=MQLoss(level=[80, 90]),\n                 config=None, \n                 search_alg=BasicVariantGenerator(random_state=1),\n                 num_samples=10,\n                 refit_with_val=False,\n                 cpus=cpu_count(),\n                 gpus=torch.cuda.device_count(),\n                 verbose=False,\n                 alias=None,\n                 backend='ray',\n                 callbacks=None):\n        \n        # Define search space, input/output sizes\n        if config is None:\n            config = self.get_default_config(h=h, backend=backend)          \n\n        super(AutoDeepAR, self).__init__(\n              cls_model=DeepAR, \n              h=h,\n              loss=loss,\n              valid_loss=valid_loss,\n              config=config,\n              search_alg=search_alg,\n              num_samples=num_samples, \n              refit_with_val=refit_with_val,\n              cpus=cpus,\n              gpus=gpus,\n              verbose=verbose,\n              alias=alias,\n              backend=backend,\n              callbacks=callbacks,            \n        )\n\n    @classmethod\n    def get_default_config(cls, h, backend, n_series=None):\n        config = cls.default_config.copy()\n        config['input_size'] = tune.choice([h*x \\\n                        for x in config['input_size_multiplier']])\n        config['step_size'] = tune.choice([1, h])        \n        del config['input_size_multiplier']\n        if backend == 'optuna':\n            config = cls._ray_config_to_optuna(config)         \n\n        return config\n```\n\n----------------------------------------\n\nTITLE: Updating Quantile Levels for Probabilistic Forecasts\nDESCRIPTION: Updates the quantile levels used for forecasting. This method allows for dynamic adjustment of quantile levels, updating both the quantile values stored in the model and the corresponding output names.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_40\n\nLANGUAGE: python\nCODE:\n```\ndef update_quantile(self, q: Optional[List[float]] = None):\n        if q is not None:\n          self.quantiles = nn.Parameter(torch.tensor(q, dtype=torch.float32), requires_grad=False)\n          self.output_names = [\"\"] + [f\"_ql{q_i}\" for q_i in q] + self.return_params * self.param_names\n          self.has_predicted = True\n        elif q is None and self.has_predicted:\n          self.quantiles = nn.Parameter(torch.tensor([0.5], dtype=torch.float32), requires_grad=False)\n          self.output_names = [\"\", \"-median\"] + self.return_params * self.param_names\n```\n\n----------------------------------------\n\nTITLE: Extracting Feature Importance from TFT Model\nDESCRIPTION: Shows how to extract global feature importance metrics from a trained TFT model. This returns a dictionary containing importance measures for static covariates and time-varying variables.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n# | eval: false\n\nfeature_importances = nf.models[0].feature_importances()\nfeature_importances.keys()\n```\n\n----------------------------------------\n\nTITLE: Matrix Operations with PyTorch Einsum\nDESCRIPTION: Performs matrix multiplication using torch.einsum to compute backcast and forecast values from theta parameters and basis functions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeats.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nbackcast = torch.einsum('bp,pt->bt', backcast_theta, self.backcast_basis)\nforecast = torch.einsum('bpq,pt->btq', forecast_theta, self.forecast_basis)\nreturn backcast, forecast\n```\n\n----------------------------------------\n\nTITLE: Getting Prediction Interval Method Function in Python\nDESCRIPTION: Defines the `get_prediction_interval_method` function, which acts as a factory for retrieving the appropriate prediction interval calculation function. It takes a method name string ('conformal_distribution' or 'conformal_error') and returns the corresponding function (`add_conformal_distribution_intervals` or `add_conformal_error_intervals`). Raises a ValueError if an unsupported method name is provided.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/utils.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n#| export\ndef get_prediction_interval_method(method: str):\n    available_methods = {\n        \"conformal_distribution\": add_conformal_distribution_intervals,\n        \"conformal_error\": add_conformal_error_intervals,\n    }\n    if method not in available_methods.keys():\n        raise ValueError(\n            f\"prediction intervals method {method} not supported \"\n            f'please choose one of {\", \".join(available_methods.keys())}'\n        )\n    return available_methods[method]\n```\n\n----------------------------------------\n\nTITLE: Retrieving Best Hyperparameter Configuration\nDESCRIPTION: Retrieves and displays the best hyperparameter configuration found during the automatic tuning process based on validation performance.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/04_longhorizon_nhits.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nnf.models[0].results.get_best_result().config\n```\n\n----------------------------------------\n\nTITLE: Visualizing Forecast Distribution Histograms with Matplotlib\nDESCRIPTION: Creates a histogram plot comparing distributions for two forecast horizons using matplotlib. Visualizes the probability distribution of samples for horizons +1 and +2.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_50\n\nLANGUAGE: python\nCODE:\n```\nx_plot = range(quants.shape[1])\ny_plot_hat = quants[0,:,0,:]\nsamples_hat = samples[0,:,0,:]\n\nfig, ax = plt.subplots(figsize=(3.7, 2.9))\n\nax.hist(samples_hat[0,:], alpha=0.5, label=r'Horizon $\\tau+1$')\nax.hist(samples_hat[1,:], alpha=0.5, label=r'Horizon $\\tau+2$')\nax.set(xlabel='Y values', ylabel='Probability')\nplt.title('Single horizon Distributions')\nplt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\nplt.grid()\nplt.show()\nplt.close()\n```\n\n----------------------------------------\n\nTITLE: Loading ETTm2 Time Series Data\nDESCRIPTION: Loads the ETTm2 dataset using the LongHorizon class and prepares the data for training and evaluation by defining validation and test sizes.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/04_longhorizon_nhits.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom datasetsforecast.long_horizon import LongHorizon\n\n# Change this to your own data to try the model\nY_df, _, _ = LongHorizon.load(directory='./', group='ETTm2')\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\n\n# For this excercise we are going to take 20% of the DataSet\nn_time = len(Y_df.ds.unique())\nval_size = int(.2 * n_time)\ntest_size = int(.2 * n_time)\n\nY_df.groupby('unique_id').head(2)\n```\n\n----------------------------------------\n\nTITLE: Retrieving Hyperparameter Tuning Results in Python\nDESCRIPTION: This code retrieves the results of hyperparameter tuning from the AutoNHITS model and converts them to a pandas DataFrame for analysis.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/04_hyperparameter_tuning.ipynb#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nresults = nf.models[0].results.trials_dataframe()\nresults.drop(columns='user_attrs_ALL_PARAMS')\n```\n\n----------------------------------------\n\nTITLE: Testing AutoTiDE with Various Configurations\nDESCRIPTION: Unit tests for the AutoTiDE class, including testing Optuna integration, argument checking, and different configuration approaches for both Optuna and Ray backends.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_56\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n# Check Optuna\nassert model.config(MockTrial())['h'] == 12\n\n# Unit test to test that Auto* model contains all required arguments from BaseAuto\ntest_args(AutoTiDE, exclude_args=['cls_model']) \n\n# Unit test for situation: Optuna with updated default config\nmy_config = AutoTiDE.get_default_config(h=12, backend='optuna')\ndef my_config_new(trial):\n    config = {**my_config(trial)}\n    config.update({'max_steps': 2, 'val_check_steps': 1, 'input_size': 12})\n    return config\n\nmodel = AutoTiDE(h=12, config=my_config_new, backend='optuna', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: Implementing Quantile Knot Parameterization Method for ISQF\nDESCRIPTION: Static method that parameterizes the x or y positions of quantile knots in the ISQF distribution. This method returns the various positional components needed for spline construction.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\n    @staticmethod\n    def parameterize_qk(\n        quantile_knots: torch.Tensor,\n    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        r\"\"\"\n        Function to parameterize the x or y positions\n        of the num_qk quantile knots\n        Parameters\n        ----------\n        quantile_knots\n            x or y positions of the quantile knots\n            shape: (*batch_shape, num_qk)\n        Returns\n        -------\n        qk\n            x or y positions of the quantile knots (qk),\n            with index=1, ..., num_qk-1,\n            shape: (*batch_shape, num_qk-1)\n        qk_plus\n            x or y positions of the quantile knots (qk),\n            with index=2, ..., num_qk,\n            shape: (*batch_shape, num_qk-1)\n        qk_l\n            x or y positions of the left-most quantile knot (qk),\n            shape: (*batch_shape)\n        qk_r\n            x or y positions of the right-most quantile knot (qk),\n            shape: (*batch_shape)\n        \"\"\"\n\n        qk, qk_plus = quantile_knots[..., :-1], quantile_knots[..., 1:]\n        qk_l, qk_r = quantile_knots[..., 0], quantile_knots[..., -1]\n\n        return qk, qk_plus, qk_l, qk_r\n```\n\n----------------------------------------\n\nTITLE: Running NHITS Experiments\nDESCRIPTION: Executes the NHITS forecasting experiments for a specific dataset and horizon. Example shown is for ETTh1 dataset with 96-step horizon and 20 samples.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/experiments/long_horizon/README.md#2025-04-23_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npython run_nhits.py --dataset 'ETTh1' --horizon 96 --num_samples 20\n```\n\n----------------------------------------\n\nTITLE: Unit Testing HINT Hierarchical Coherence\nDESCRIPTION: Test code to verify hierarchical coherence of the HINT implementation using synthetic data. Tests probabilistic, sample and mean coherence.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.hint.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef sort_df_hier(Y_df, S_df):\n    Y_df.unique_id = Y_df.unique_id.astype('category')\n    Y_df.unique_id = Y_df.unique_id.cat.set_categories(S_df.index)\n    Y_df = Y_df.sort_values(by=['unique_id', 'ds'])\n    return Y_df\n\nnp.random.seed(123)\ntrain_steps = 20\nnum_levels = 7\nlevel = np.arange(0, 100, 0.1)\nqs = [[50-lv/2, 50+lv/2] for lv in level]\nquantiles = np.sort(np.concatenate(qs)/100)\n\nlevels = ['Top', 'Mid1', 'Mid2', 'Bottom1', 'Bottom2', 'Bottom3', 'Bottom4']\nunique_ids = np.repeat(levels, train_steps)\n\nS = np.array([[1., 1., 1., 1.],\n              [1., 1., 0., 0.],\n              [0., 0., 1., 1.],\n              [1., 0., 0., 0.],\n              [0., 1., 0., 0.],\n              [0., 0., 1., 0.],\n              [0., 0., 0., 1.]])\n\nS_dict = {col: S[:, i] for i, col in enumerate(levels[3:])}\nS_df = pd.DataFrame(S_dict, index=levels)\n\nds = pd.date_range(start='2018-03-31', periods=train_steps, freq='Q').tolist() * num_levels\ny_lists = [S @ np.random.uniform(low=100, high=500, size=4) for i in range(train_steps)]\ny = [elem for tup in zip(*y_lists) for elem in tup]\nY_df = pd.DataFrame({'unique_id': unique_ids, 'ds': ds, 'y': y})\nY_df = sort_df_hier(Y_df, S_df)\n\nnhits = NHITS(h=4,\n              input_size=4,\n              loss=GMM(n_components=2, quantiles=quantiles, num_samples=len(quantiles)),\n              max_steps=5,\n              early_stop_patience_steps=2,\n              val_check_steps=1,\n              scaler_type='robust',\n              learning_rate=1e-3)\nmodel = HINT(h=4, model=nhits, S=S, reconciliation='BottomUp')\n```\n\n----------------------------------------\n\nTITLE: Tail Distribution Quantile Function in PyTorch\nDESCRIPTION: Method for evaluating quantiles in the tail regions of the distribution. Uses logarithmic transformations for accurate calculation of extreme quantiles in both left and right tails based on the specified parameters.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_26\n\nLANGUAGE: Python\nCODE:\n```\ndef quantile_tail(\n    self,\n    alpha: torch.Tensor,\n    dim: Optional[int] = None,\n    left_tail: bool = True,\n) -> torch.Tensor:\n    # Refer to the description in quantile_internal\n\n    if left_tail:\n        tail_a, tail_b = self.tail_al, self.tail_bl\n    else:\n        tail_a, tail_b = self.tail_ar, self.tail_br\n        alpha = 1 - alpha\n\n    if dim is not None:\n        tail_a, tail_b = tail_a.unsqueeze(dim=dim), tail_b.unsqueeze(\n            dim=dim\n        )\n\n    return tail_a * torch.log(alpha) + tail_b\n```\n\n----------------------------------------\n\nTITLE: Defining AutoBiTCN Class for Hyperparameter Tuning in Python\nDESCRIPTION: This snippet defines the `AutoBiTCN` class, inheriting from `BaseAuto`, for automated hyperparameter optimization of the `BiTCN` model. It specifies a default search space (`default_config`) using `ray.tune` for parameters like `hidden_size`, `dropout`, `learning_rate`, etc. The `__init__` method configures the model using provided or default settings, loss function, and backend (Ray/Optuna). The `get_default_config` class method retrieves the configuration, adapts it based on the forecast horizon `h` (setting `input_size` and `step_size`), and converts it for Optuna if necessary.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass AutoBiTCN(BaseAuto):\n\n    default_config = {\n        \"input_size_multiplier\": [1, 2, 3, 4, 5],\n        \"h\": None,\n        \"hidden_size\": tune.choice([16, 32]),\n        \"dropout\": tune.uniform(0.0, 0.99),  \n        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n        \"scaler_type\": tune.choice([None, 'robust', 'standard']),\n        \"max_steps\": tune.choice([500, 1000, 2000]),\n        \"batch_size\": tune.choice([32, 64, 128, 256]),\n        \"windows_batch_size\": tune.choice([128, 256, 512, 1024]),\n        \"loss\": None,\n        \"random_seed\": tune.randint(1, 20),\n    }\n\n    def __init__(self,\n                 h,\n                 loss=MAE(),\n                 valid_loss=None,\n                 config=None, \n                 search_alg=BasicVariantGenerator(random_state=1),\n                 num_samples=10,\n                 refit_with_val=False,\n                 cpus=cpu_count(),\n                 gpus=torch.cuda.device_count(),\n                 verbose=False,\n                 alias=None,\n                 backend='ray',\n                 callbacks=None):\n        \n        # Define search space, input/output sizes\n        if config is None:\n            config = self.get_default_config(h=h, backend=backend)          \n\n        super(AutoBiTCN, self).__init__(\n              cls_model=BiTCN, \n              h=h,\n              loss=loss,\n              valid_loss=valid_loss,\n              config=config,\n              search_alg=search_alg,\n              num_samples=num_samples, \n              refit_with_val=refit_with_val,\n              cpus=cpus,\n              gpus=gpus,\n              verbose=verbose,\n              alias=alias,\n              backend=backend,\n              callbacks=callbacks,            \n        )\n\n    @classmethod\n    def get_default_config(cls, h, backend, n_series=None):\n        config = cls.default_config.copy()\n        config['input_size'] = tune.choice([h*x \\\n                        for x in config['input_size_multiplier']])\n        config['step_size'] = tune.choice([1, h])        \n        del config['input_size_multiplier']\n        if backend == 'optuna':\n            config = cls._ray_config_to_optuna(config)         \n\n        return config\n```\n\n----------------------------------------\n\nTITLE: Importing Testing and Visualization Libraries\nDESCRIPTION: Imports additional libraries for testing, visualization, and debugging purposes, including matplotlib, fastcore testing utilities, and nbdev documentation tools.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#| hide\nimport matplotlib.pyplot as plt\n\nfrom fastcore.test import test_eq\nfrom nbdev.showdoc import show_doc\n\nimport logging\nimport warnings\nimport inspect\n\nfrom neuralforecast.losses.pytorch import MSE\n```\n\n----------------------------------------\n\nTITLE: Implementing MinTraceOLS Reconciliation Matrix in Python\nDESCRIPTION: Defines a function to create a MinTraceOLS reconciliation matrix as proposed by Wickramasuriya et al. It takes a summing matrix as input and returns a reconciliation matrix.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.hint.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef get_mintrace_ols_P(S: np.ndarray):\n    \"\"\"MinTraceOLS Reconciliation Matrix.\n\n    Creates MinTraceOLS reconciliation matrix as proposed by Wickramasuriya et al.\n\n    $$\\mathbf{P}_{\\\\text{MinTraceOLS}}=\\\\left(\\mathbf{S}^{\\intercal}\\mathbf{S}\\\\right)^{-1}\\mathbf{S}^{\\intercal}$$\n\n    **Parameters:**<br>\n    `S`: Summing matrix of size (`base`, `bottom`).<br>\n      \n    **Returns:**<br>\n    `P`: Reconciliation matrix of size (`bottom`, `base`).<br>\n\n    **References:**<br>\n    - [Wickramasuriya, S.L., Turlach, B.A. & Hyndman, R.J. (2020). \\\"Optimal non-negative\n    forecast reconciliation\". Stat Comput 30, 11671182,\n    https://doi.org/10.1007/s11222-020-09930-0](https://robjhyndman.com/publications/nnmint/).\n    \"\"\"\n    n_hiers, n_bottom = S.shape\n    n_agg = n_hiers - n_bottom\n\n    W = np.eye(n_hiers)\n\n    # We compute reconciliation matrix with\n    # Equation 10 from https://robjhyndman.com/papers/MinT.pdf\n    A = S[:n_agg,:]\n    U = np.hstack((np.eye(n_agg), -A)).T\n    J = np.hstack((np.zeros((n_bottom,n_agg)), np.eye(n_bottom)))\n    P = J - (J @ W @ U) @ np.linalg.pinv(U.T @ W @ U) @ U.T\n    return P\n```\n\n----------------------------------------\n\nTITLE: Visualizing Past Variable Importances\nDESCRIPTION: Creates a horizontal bar plot of the average importance for past variables in the TFT model. This visualization shows which historical features have the greatest overall impact on predictions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n# | eval: false\nfeature_importances[\"Past variable importance over time\"].mean().sort_values().plot(\n    kind=\"barh\"\n)\n```\n\n----------------------------------------\n\nTITLE: Generating NBEATSx Decomposition\nDESCRIPTION: Creates a decomposition of the time series using the trained NBEATSx model to extract trend and seasonality components.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/14_interpretable_decompositions.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# NBEATSx decomposition plot\nmodel = nf.models[0]\ndataset, *_ = TimeSeriesDataset.from_df(df = Y_train_df)\ny_hat = model.decompose(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: Importing and Configuring Forecasting Models\nDESCRIPTION: Imports necessary libraries and models from neuralforecast, sets up logging and precision configuration for PyTorch.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/08_multivariate_tsmixer.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nimport torch\nfrom neuralforecast.core import NeuralForecast\nfrom neuralforecast.models import TSMixer, TSMixerx, NHITS, MLPMultivariate\nfrom neuralforecast.losses.pytorch import MAE\n```\n\nLANGUAGE: python\nCODE:\n```\nlogging.getLogger('pytorch_lightning').setLevel(logging.ERROR)\ntorch.set_float32_matmul_precision('high')\n```\n\n----------------------------------------\n\nTITLE: Importing Common Dependencies for Neural Network Models\nDESCRIPTION: Imports necessary modules for model demonstration and testing, including logging, warnings, and utility functions for series generation and model checking.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.dilated_rnn.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#| hide\nimport logging\nimport warnings\nfrom nbdev.showdoc import show_doc\nfrom neuralforecast.utils import generate_series\nfrom neuralforecast.common._model_checks import check_model\n```\n\n----------------------------------------\n\nTITLE: Importing Evaluation Metrics\nDESCRIPTION: Imports necessary functions for evaluating the RUL predictions using root mean square error (RMSE).\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/use-cases/predictive_maintenance.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom utilsforecast.evaluation import evaluate\nfrom utilsforecast.losses import rmse\n```\n\n----------------------------------------\n\nTITLE: Generating Forecasts with Inverse Scaling in Python\nDESCRIPTION: Predicts future electricity prices using the trained model. The NeuralForecast class automatically handles the inverse normalization, returning forecasts in the original scale.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/07_time_series_scaling.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nY_hat_df = nf.predict(futr_df=futr_df)\nY_hat_df.head()\n```\n\n----------------------------------------\n\nTITLE: BaseTimeSeriesDataset Implementation\nDESCRIPTION: Base class for time series datasets that handles common functionality for temporal and static features. Provides core methods for data conversion and mask handling.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/tsdataset.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass BaseTimeSeriesDataset(Dataset):\n    def __init__(self, temporal_cols, max_size: int, min_size: int, y_idx: int, static=None, static_cols=None):\n        super().__init__()\n        self.temporal_cols = pd.Index(list(temporal_cols))\n\n        if static is not None:\n            self.static = self._as_torch_copy(static)\n            self.static_cols = static_cols\n        else:\n            self.static = static\n            self.static_cols = static_cols\n\n        self.max_size = max_size\n        self.min_size = min_size\n        self.y_idx = y_idx\n        self.updated = False\n\n    def __len__(self):\n        return self.n_groups\n\n    def _as_torch_copy(self, x: Union[np.ndarray, torch.Tensor], dtype: torch.dtype = torch.float32) -> torch.Tensor:\n        if isinstance(x, np.ndarray):\n            x = torch.from_numpy(x)\n        return x.to(dtype, copy=False).clone()\n    \n    @staticmethod\n    def _ensure_available_mask(data: np.ndarray, temporal_cols):\n        if 'available_mask' not in temporal_cols:\n            available_mask = np.ones((len(data),1), dtype=np.float32)\n            temporal_cols = temporal_cols.append(pd.Index(['available_mask']))\n            data = np.append(data, available_mask, axis=1)\n        return data, temporal_cols\n    \n    @staticmethod\n    def _extract_static_features(static_df, id_col):\n        if static_df is not None:\n            static_df = ufp.sort(static_df, by=id_col)\n            static_cols = [col for col in static_df.columns if col != id_col]\n            static = ufp.to_numpy(static_df[static_cols])\n            static_cols = pd.Index(static_cols)\n        else:\n            static = None\n            static_cols = None\n        return static, static_cols\n```\n\n----------------------------------------\n\nTITLE: Unpadding and Splitting Outputs in DilatedRNN\nDESCRIPTION: These methods handle unpadding of outputs and splitting dilated outputs. The _unpad_outputs method removes padding, while _split_outputs reorganizes the dilated outputs into the original sequence order.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.dilated_rnn.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\ndef _unpad_outputs(self, splitted_outputs, n_steps):\n    return splitted_outputs[:n_steps]\n\ndef _split_outputs(self, dilated_outputs, rate):\n    batchsize = dilated_outputs.size(1) // rate\n\n    blocks = [dilated_outputs[:, i * batchsize: (i + 1) * batchsize, :] for i in range(rate)]\n\n    interleaved = torch.stack((blocks)).transpose(1, 0)\n    interleaved = interleaved.reshape(dilated_outputs.size(0) * rate,\n                                   batchsize,\n                                   dilated_outputs.size(2))\n    return interleaved\n```\n\n----------------------------------------\n\nTITLE: Visualizing Future Variable Importance Over Time\nDESCRIPTION: Creates a stacked bar chart showing how future variable importance changes over the forecasting horizon. This visualization demonstrates how the importance of different features varies across future time steps.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n# | eval: false\ndf = feature_importances[\"Future variable importance over time\"]\n\n\nfig, ax = plt.subplots(figsize=(20, 10))\nbottom = np.zeros(len(df.index))\nfor col in df.columns:\n    p = ax.bar(np.arange(-len(df), 0), df[col].values, 0.6, label=col, bottom=bottom)\n    bottom += df[col]\nax.set_title(\"Future variable importance over time ponderated by attention\")\nax.set_ylabel(\"Importance\")\nax.set_xlabel(\"Time\")\nax.grid(True)\nax.legend()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Initializing Base Point Loss Class in PyTorch\nDESCRIPTION: Base class implementation for point loss functions with support for horizon weights, output size multipliers and output names. Includes methods for domain mapping and weight computation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass BasePointLoss(torch.nn.Module):\n    def __init__(self, horizon_weight=None, outputsize_multiplier=None, output_names=None):\n        super(BasePointLoss, self).__init__()\n        if horizon_weight is not None:\n            horizon_weight = torch.Tensor(horizon_weight.flatten())\n        self.horizon_weight = horizon_weight\n        self.outputsize_multiplier = outputsize_multiplier\n        self.output_names = output_names\n        self.is_distribution_output = False\n\n    def domain_map(self, y_hat: torch.Tensor):\n        return y_hat\n\n    def _compute_weights(self, y, mask):\n        if mask is None:\n            mask = torch.ones_like(y)\n\n        if self.horizon_weight is None:\n            weights = torch.ones_like(mask)\n        else:\n            assert mask.shape[1] == len(self.horizon_weight),\\\n                'horizon_weight must have same length as Y'\n            weights = self.horizon_weight.clone()\n            weights = weights[None, :, None].to(mask.device)\n            weights = torch.ones_like(mask, device=mask.device) * weights\n        \n        return weights * mask\n```\n\n----------------------------------------\n\nTITLE: Mean Squared Error Implementation in PyTorch\nDESCRIPTION: Implementation of Mean Squared Error (MSE) loss function that calculates the average squared difference between predicted and actual values across a time series.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass MSE(BasePointLoss):\n    def __init__(self, horizon_weight=None):\n        super(MSE, self).__init__(horizon_weight=horizon_weight,\n                                  outputsize_multiplier=1,\n                                  output_names=[''])\n\n    def __call__(self,\n                 y: torch.Tensor,\n                 y_hat: torch.Tensor,\n                 y_insample: torch.Tensor,\n                 mask: Union[torch.Tensor, None] = None,\n                 ) -> torch.Tensor:\n        losses = (y - y_hat)**2\n        weights = self._compute_weights(y=y, mask=mask)\n        return _weighted_mean(losses=losses, weights=weights)\n```\n\n----------------------------------------\n\nTITLE: Displaying Autoformer.fit Method Documentation\nDESCRIPTION: This code snippet displays the documentation for the fit method of the Autoformer class using the show_doc function. It provides information on how to train the Autoformer model.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.autoformer.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(Autoformer.fit, name='Autoformer.fit')\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages\nDESCRIPTION: Installation of necessary Python packages for hierarchical forecasting including datasetsforecast, hierarchicalforecast, neuralforecast, and statsforecast.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/09_hierarchical_forecasting.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n!pip install datasetsforecast hierarchicalforecast neuralforecast statsforecast\n```\n\n----------------------------------------\n\nTITLE: Fitting and Predicting with NeuralForecast in Python\nDESCRIPTION: This snippet demonstrates how to fit a NeuralForecast model and perform cross-validation. It also includes a check for hierarchical coherence in the forecasts.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.hint.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Fit and Predict\nnf = NeuralForecast(models=[model], freq='Q')\nforecasts = nf.cross_validation(df=Y_df, val_size=4, n_windows=1)\n\n# ---Check Hierarchical Coherence---\nparent_children_dict = {0: [1, 2], 1: [3, 4], 2: [5, 6]}\n# check coherence for each horizon time step\nfor _, df in forecasts.groupby('ds'):\n    hint_mean = df['HINT'].values\n    for parent_idx, children_list in parent_children_dict.items():\n        parent_value = hint_mean[parent_idx]\n        children_sum = hint_mean[children_list].sum()\n        np.testing.assert_allclose(children_sum, parent_value, rtol=1e-6)\n```\n\n----------------------------------------\n\nTITLE: Implementing MinMax Scaler and Inverse Functions in Python\nDESCRIPTION: Provides functions for applying MinMax scaling and its inverse transformation using pre-computed statistics.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.scalers.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef minmax_scaler(x, x_min, x_range):\n    return (x - x_min) / x_range\n\ndef inv_minmax_scaler(z, x_min, x_range):\n    return z * x_range + x_min\n```\n\n----------------------------------------\n\nTITLE: Loading PHM2008 Aircraft Degradation Dataset\nDESCRIPTION: Loads the Prognosis and Health Management 2008 challenge dataset containing run-to-failure simulations of turbofan engines.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/use-cases/predictive_maintenance.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nY_train_df, Y_test_df = PHM2008.load(directory='./data', group='FD001', clip_rul=False)\nY_train_df\n```\n\n----------------------------------------\n\nTITLE: Customizing Ray Tune Config for AutoNBEATSx\nDESCRIPTION: Example of updating the default configuration for AutoNBEATSx model with Ray Tune backend. The code directly modifies the configuration dictionary to reduce training steps and customize model architecture.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_41\n\nLANGUAGE: python\nCODE:\n```\nmy_config = AutoNBEATSx.get_default_config(h=12, backend='ray')\nmy_config['max_steps'] = 2\nmy_config['val_check_steps'] = 1\nmy_config['input_size'] = 12\nmy_config['mlp_units'] = 3 * [[8, 8]]\nmodel = AutoNBEATSx(h=12, config=my_config, backend='ray', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: Preparing Static Data for NeuralForecast in Python\nDESCRIPTION: This code prepares the static data for the NeuralForecast model, renaming the 'unique_id' column to 'id_col' to match the expected format.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/19_large_datasets.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nstatic = AirPassengersStatic.rename(columns={'unique_id': 'id_col'})\nstatic\n```\n\n----------------------------------------\n\nTITLE: Implementing Model Fitting in Python\nDESCRIPTION: This code defines the main fitting method for the model. It handles data preparation, distributed training configuration, and validation interval setting.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\ndef _fit(\n    self,\n    dataset,\n    batch_size,\n    valid_batch_size=1024,\n    val_size=0,\n    test_size=0,\n    random_seed=None,\n    shuffle_train=True,\n    distributed_config=None,\n):\n    self._check_exog(dataset)\n    self._restart_seed(random_seed)\n\n    self.val_size = val_size\n    self.test_size = test_size\n    is_local = isinstance(dataset, BaseTimeSeriesDataset)\n    if is_local:\n        datamodule_constructor = TimeSeriesDataModule\n    else:\n        datamodule_constructor = _DistributedTimeSeriesDataModule\n    \n    dataloader_kwargs = self.dataloader_kwargs if self.dataloader_kwargs is not None else {}\n    datamodule = datamodule_constructor(\n        dataset=dataset, \n        batch_size=batch_size,\n        valid_batch_size=valid_batch_size,\n        drop_last=self.drop_last_loader,\n        shuffle_train=shuffle_train,\n        **dataloader_kwargs\n    )\n\n    if self.val_check_steps > self.max_steps:\n        warnings.warn(\n            'val_check_steps is greater than max_steps, '\n            'setting val_check_steps to max_steps.'\n        )\n    val_check_interval = min(self.val_check_steps, self.max_steps)\n    self.trainer_kwargs['val_check_interval'] = int(val_check_interval)\n```\n\n----------------------------------------\n\nTITLE: Graph Laplacian Computation Method\nDESCRIPTION: Calculates the graph Laplacian matrix with optional normalization. Takes a graph structure without self-loops and returns the corresponding Laplacian matrix.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.stemgnn.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef get_laplacian(self, graph, normalize):\n    if normalize:\n        D = torch.diag(torch.sum(graph, dim=-1) ** (-1 / 2))\n        L = torch.eye(graph.size(0), device=graph.device, dtype=graph.dtype) - torch.mm(torch.mm(D, graph), D)\n    else:\n        D = torch.diag(torch.sum(graph, dim=-1))\n        L = D - graph\n    return L\n```\n\n----------------------------------------\n\nTITLE: Initializing NHITS Neural Network Model in Python\nDESCRIPTION: Class initialization code for NHITS model with comprehensive parameter configuration including stack types, MLP units, pooling settings, and training parameters. Inherits from BaseWindows class and sets up the neural network architecture.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nhits.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ndef __init__(self, \n                 h,\n                 input_size,\n                 futr_exog_list = None,\n                 hist_exog_list = None,\n                 stat_exog_list = None,\n                 exclude_insample_y = False,\n                 stack_types: list = ['identity', 'identity', 'identity'],\n                 n_blocks: list = [1, 1, 1],\n                 mlp_units: list = 3 * [[512, 512]],\n                 n_pool_kernel_size: list = [2, 2, 1],\n                 n_freq_downsample: list = [4, 2, 1],\n                 pooling_mode: str = 'MaxPool1d',\n                 interpolation_mode: str = 'linear',\n                 dropout_prob_theta = 0.,\n                 activation = 'ReLU',\n                 loss = MAE(),\n                 valid_loss = None,\n                 max_steps: int = 1000,\n                 learning_rate: float = 1e-3,\n                 num_lr_decays: int = 3,\n                 early_stop_patience_steps: int =-1,\n                 val_check_steps: int = 100,\n                 batch_size: int = 32,\n                 valid_batch_size: Optional[int] = None,\n                 windows_batch_size: int = 1024,\n                 inference_windows_batch_size: int = -1,\n                 start_padding_enabled = False,\n                 step_size: int = 1,\n                 scaler_type: str = 'identity',\n                 random_seed: int = 1,\n                 drop_last_loader = False,\n                 alias: Optional[str] = None,\n                 optimizer = None,\n                 optimizer_kwargs = None,\n                 lr_scheduler = None,\n                 lr_scheduler_kwargs = None,\n                 dataloader_kwargs = None,\n                 **trainer_kwargs)\n```\n\n----------------------------------------\n\nTITLE: Calculating Relative Mean Absolute Error (RMAE) in Python\nDESCRIPTION: This function computes the Relative Mean Absolute Error between two sets of forecasts from different forecasting methods. It compares the performance of one model against a baseline model, with values less than one indicating better performance of the first model.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.numpy.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef rmae(y: np.ndarray, \n         y_hat1: np.ndarray, y_hat2: np.ndarray, \n         weights: Optional[np.ndarray] = None,\n         axis: Optional[int] = None) -> Union[float, np.ndarray]:\n    \"\"\" RMAE\n            \n    Calculates Relative Mean Absolute Error (RMAE) between\n    two sets of forecasts (from two different forecasting methods).\n    A number smaller than one implies that the forecast in the \n    numerator is better than the forecast in the denominator.\n    \n    $$ \\mathrm{rMAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}, \\mathbf{\\hat{y}}^{base}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{\\mathrm{MAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}^{base}_{\\tau})} $$\n    \n    **Parameters:**<br>\n    `y`: numpy array, observed values.<br>\n    `y_hat1`: numpy array. Predicted values of first model.<br>\n    `y_hat2`: numpy array. Predicted values of baseline model.<br>\n    `weights`: numpy array, optional. Weights for weighted average.<br>\n    `axis`: None or int, optional.Axis or axes along which to average a.<br> \n        The default, axis=None, will average over all of the elements of\n        the input array.\n    \n    **Returns:**<br>\n    `rmae`: numpy array or double.\n\n    **References:**<br>\n    [Rob J. Hyndman, & Koehler, A. B. \"Another look at measures of forecast accuracy\".](https://www.sciencedirect.com/science/article/pii/S0169207006000239)\n    \"\"\"\n    numerator = mae(y=y, y_hat=y_hat1, weights=weights, axis=axis)\n    denominator = mae(y=y, y_hat=y_hat2, weights=weights, axis=axis)\n    rmae = numerator / denominator\n\n    return rmae\n```\n\n----------------------------------------\n\nTITLE: Time Series Data Scaler Type Mapping\nDESCRIPTION: Dictionary mapping scaler types to their corresponding scaler classes for data normalization.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n_type2scaler = {\n    'standard': LocalStandardScaler,\n    'robust': lambda: LocalRobustScaler(scale='mad'),\n    'robust-iqr': lambda: LocalRobustScaler(scale='iqr'),\n    'minmax': LocalMinMaxScaler,\n    'boxcox': lambda: LocalBoxCoxScaler(method='loglik', lower=0.0)\n}\n```\n\n----------------------------------------\n\nTITLE: Plotting Cross-Validated TimeMixer Forecasts with Matplotlib\nDESCRIPTION: Implements cross-validation for multiple historic forecasts using NeuralForecast and visualizes results. Creates time series plot comparing actual values against cross-validated predictions with 2 windows and 12-month step size.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timemixer.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfcst = NeuralForecast(models=[model], freq='M')\nforecasts = fcst.cross_validation(df=AirPassengersPanel, static_df=AirPassengersStatic, n_windows=2, step_size=12)\n\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nY_hat_df = forecasts.loc['Airline1']\nY_df = AirPassengersPanel[AirPassengersPanel['unique_id']=='Airline1']\n\nplt.plot(Y_df['ds'], Y_df['y'], c='black', label='True')\nplt.plot(Y_hat_df['ds'], Y_hat_df['TimeMixer'], c='blue', label='Forecast')\nax.set_title('AirPassengers Forecast', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Year', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Neural Forecasting\nDESCRIPTION: Imports necessary Python libraries including NeuralForecast components, numpy, pandas, and matplotlib for data manipulation and visualization.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/20_conformal_prediction.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import NHITS\nfrom neuralforecast.utils import AirPassengersPanel\nfrom neuralforecast.utils import PredictionIntervals\nfrom neuralforecast.losses.pytorch import DistributionLoss, MAE\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for NHITS\nDESCRIPTION: Imports the necessary libraries for hyperparameter tuning and neural forecasting using Ray Tune and NeuralForecast.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/04_longhorizon_nhits.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import tune\nfrom neuralforecast.auto import AutoNHITS\nfrom neuralforecast.core import NeuralForecast\n```\n\n----------------------------------------\n\nTITLE: Handling Exogenous Variables and Scaling in Python\nDESCRIPTION: This code checks for exogenous variable support, sets up scaling, and configures data module arguments. It also includes methods for checking exogenous variables and setting quantiles.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nif exclude_insample_y and not (self.EXOGENOUS_FUTR or self.EXOGENOUS_HIST or self.EXOGENOUS_STAT):\n    raise Exception(f'{type(self).__name__} does not support `exclude_insample_y=True`. Please set `exclude_insample_y=False`')\n\nself.exclude_insample_y = exclude_insample_y\n\nself.scaler = TemporalNorm(\n    scaler_type=scaler_type,\n    dim=1,  # Time dimension is 1.\n    num_features= 1 + len(self.hist_exog_list) + len(self.futr_exog_list)\n)\n\nself.val_size = 0\nself.test_size = 0\n\nself.decompose_forecast = False\n\nself.dataloader_kwargs = dataloader_kwargs\nself.drop_last_loader = drop_last_loader\nself.validation_step_outputs: List = []\nself.alias = alias\n\ndef _check_exog(self, dataset):\n    temporal_cols = set(dataset.temporal_cols.tolist())\n    static_cols = set(dataset.static_cols.tolist() if dataset.static_cols is not None else [])\n\n    missing_hist = set(self.hist_exog_list) - temporal_cols\n    missing_futr = set(self.futr_exog_list) - temporal_cols\n    missing_stat = set(self.stat_exog_list) - static_cols\n    if missing_hist:\n        raise Exception(f'{missing_hist} historical exogenous variables not found in input dataset')\n    if missing_futr:\n        raise Exception(f'{missing_futr} future exogenous variables not found in input dataset')\n    if missing_stat:\n        raise Exception(f'{missing_stat} static exogenous variables not found in input dataset')\n\ndef _set_quantiles(self, quantiles=None):\n    if quantiles is None and isinstance(self.loss, (losses.IQLoss, losses.HuberIQLoss)):\n        self.loss.update_quantile(q=[0.5])\n    elif hasattr(self.loss, 'update_quantile') and callable(self.loss.update_quantile):\n        self.loss.update_quantile(q=quantiles)\n```\n\n----------------------------------------\n\nTITLE: Configuring and Training AutoInformer with Ray Backend\nDESCRIPTION: This snippet demonstrates how to configure and fit an AutoInformer model with a Ray backend, using custom configuration parameters. It sets up the model with a specified forecast horizon and minimal training steps for testing purposes.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_75\n\nLANGUAGE: python\nCODE:\n```\nmy_config = AutoInformer.get_default_config(h=12, backend='ray')\nmy_config['max_steps'] = 1\nmy_config['val_check_steps'] = 1\nmy_config['input_size'] = 12\nmy_config['hidden_size'] = 8\nmodel = AutoInformer(h=12, config=my_config, backend='ray', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: DeepNPTS Model Usage Example with AirPassengers Dataset\nDESCRIPTION: Demonstrates how to use the DeepNPTS model with the AirPassengers dataset, including model initialization, training, and prediction visualization using matplotlib.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.deepnpts.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import DeepNPTS\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\n\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nnf = NeuralForecast(\n    models=[DeepNPTS(h=12,\n                   input_size=24,\n                   stat_exog_list=['airline1'],\n                   futr_exog_list=['trend'],\n                   max_steps=1000,\n                   val_check_steps=10,\n                   early_stop_patience_steps=3,\n                   scaler_type='robust',\n                   enable_progress_bar=True),\n    ],\n    freq='ME'\n)\nnf.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nY_hat_df = nf.predict(futr_df=Y_test_df)\n\n# Plot quantile predictions\nY_hat_df = Y_hat_df.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['DeepNPTS'], c='red', label='mean')\nplt.grid()\nplt.plot()\n```\n\n----------------------------------------\n\nTITLE: Cross Validation Model Configuration\nDESCRIPTION: Sets up NeuralForecast configuration with multiple models including DilatedRNN, AutoMLP, and NHITS for cross-validation testing.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_44\n\nLANGUAGE: python\nCODE:\n```\nconfig = {'input_size': tune.choice([12, 24]), \n          'hidden_size': 256,\n          'max_steps': 1,\n          'val_check_steps': 1,\n          'step_size': 12}\n\nconfig_drnn = {'input_size': tune.choice([-1]), \n               'encoder_hidden_size': tune.choice([5, 10]),\n               'max_steps': 1,\n               'val_check_steps': 1,\n               'step_size': 1}\n\nfcst = NeuralForecast(\n    models=[\n        DilatedRNN(h=12, input_size=-1, encoder_hidden_size=10, max_steps=1),\n        AutoMLP(h=12, config=config, cpus=1, num_samples=1),\n        NHITS(h=12, input_size=12, max_steps=1)\n    ],\n    freq='M'\n)\ncv_df = fcst.cross_validation(df=AirPassengersPanel, static_df=AirPassengersStatic, n_windows=3, step_size=1)\n```\n\n----------------------------------------\n\nTITLE: Completing Forward Pass with Denormalization in iTransformer Model\nDESCRIPTION: Completes the forecasting process by applying projection to encoder output and optional denormalization. The method reshapes the output to match the expected prediction format.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.itransformer.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndec_out = self.projector(enc_out).permute(0, 2, 1)[:, :, :N] # filter the covariates\n\nif self.use_norm:\n    # De-Normalization from Non-stationary Transformer\n    dec_out = dec_out * (stdev[:, 0, :].unsqueeze(1).repeat(1, self.h * self.loss.outputsize_multiplier, 1))\n    dec_out = dec_out + (means[:, 0, :].unsqueeze(1).repeat(1, self.h * self.loss.outputsize_multiplier, 1))\n\nreturn dec_out\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model Performance with Custom Metrics in Python\nDESCRIPTION: This code evaluates the performance of the optimized models using custom metrics (MAE and MSE) on the test set, comparing the results with default configurations.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/08_multivariate_tsmixer.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nevaluate(Y_hat_df.drop(columns='cutoff'), metrics=[mae, mse], agg_fn='mean')\n```\n\n----------------------------------------\n\nTITLE: Tuning Model with Optuna\nDESCRIPTION: This method performs hyperparameter tuning using Optuna. It defines an objective function, sets up the study, and optimizes the model based on the provided configuration and dataset.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_auto.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndef _optuna_tune_model(\n    self,\n    cls_model,\n    dataset,\n    val_size,\n    test_size,\n    verbose,\n    num_samples,\n    search_alg,\n    config,\n    distributed_config,\n):\n    import optuna\n\n    def objective(trial):\n        user_cfg = config(trial)\n        cfg = deepcopy(user_cfg)\n        model = self._fit_model(\n            cls_model=cls_model,\n            config=cfg,\n            dataset=dataset,\n            val_size=val_size,\n            test_size=test_size,\n            distributed_config=distributed_config,\n        )\n        trial.set_user_attr('ALL_PARAMS', user_cfg)\n        metrics = model.metrics\n        trial.set_user_attr('METRICS', {\n            \"loss\": metrics[\"ptl/val_loss\"],\n            \"train_loss\": metrics[\"train_loss\"],\n        })\n        return trial.user_attrs['METRICS']['loss']\n\n    if isinstance(search_alg, optuna.samplers.BaseSampler):\n        sampler = search_alg\n    else:\n        sampler = None\n\n    study = optuna.create_study(sampler=sampler, direction='minimize')\n    study.optimize(\n        objective,\n        n_trials=num_samples,\n        show_progress_bar=verbose,\n        callbacks=self.callbacks,\n    )\n    return study\n```\n\n----------------------------------------\n\nTITLE: Implementing Chomp1d for Causal Convolutions in PyTorch\nDESCRIPTION: Creates a module that trims the output of 1D convolutions to ensure causality by removing future timesteps, which is essential for time series forecasting.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.modules.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass Chomp1d(nn.Module):\n    \"\"\" Chomp1d\n\n    Receives `x` input of dim [N,C,T], and trims it so that only\n    'time available' information is used. \n    Used by one dimensional causal convolutions `CausalConv1d`.\n\n    **Parameters:**<br>\n    `horizon`: int, length of outsample values to skip.\n    \"\"\"\n    def __init__(self, horizon):\n        super(Chomp1d, self).__init__()\n        self.horizon = horizon\n\n    def forward(self, x):\n        return x[:, :, :-self.horizon].contiguous()\n```\n\n----------------------------------------\n\nTITLE: Implementing Domain Mapping for NBMM in Python\nDESCRIPTION: This method maps the output tensor to the appropriate domain for the NBMM model. It reshapes the output tensor and splits it into the required number of components.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_60\n\nLANGUAGE: python\nCODE:\n```\ndef domain_map(self, output: torch.Tensor):\n    output = output.reshape(output.shape[0],\n                            output.shape[1],\n                           -1,\n                           self.outputsize_multiplier)\n    \n    return torch.tensor_split(output, self.n_outputs, dim=-1)\n```\n\n----------------------------------------\n\nTITLE: Mapping Neural Network Outputs to Distribution Domains in PyTorch\nDESCRIPTION: Method that maps the raw outputs of a neural network to appropriate domains required by the specified distribution. It splits the input tensor into the required number of parameters for the distribution.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_37\n\nLANGUAGE: python\nCODE:\n```\ndef _domain_map(self, input: torch.Tensor):\n        \"\"\"\n        Maps output of neural network to domain of distribution loss\n\n        \"\"\"\n        output = torch.tensor_split(input, self.outputsize_multiplier, dim=2)\n\n        return output\n```\n\n----------------------------------------\n\nTITLE: Plotting Probabilistic Forecast Trajectories\nDESCRIPTION: Creates a plot showing the median forecast trajectory with confidence intervals. Visualizes the median (q50) prediction along with 25-75 and 1-99 percentile ranges.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_51\n\nLANGUAGE: python\nCODE:\n```\nfig, ax = plt.subplots(figsize=(3.7, 2.9))\nplt.plot(x_plot, y_plot_hat[:,2], color='black', label='median [q50]')\nplt.fill_between(x_plot,\n                 y1=y_plot_hat[:,1], y2=y_plot_hat[:,3],\n                 facecolor='blue', alpha=0.4, label='[p25-p75]')\nplt.fill_between(x_plot,\n                 y1=y_plot_hat[:,0], y2=y_plot_hat[:,4],\n                 facecolor='blue', alpha=0.2, label='[p1-p99]')\nax.set(xlabel='Horizon', ylabel='Y values')\nplt.title('PMM Probabilistic Predictions')\nplt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\nplt.grid()\nplt.show()\nplt.close()\n```\n\n----------------------------------------\n\nTITLE: Loading Pre-trained NeuralForecast Model in Python\nDESCRIPTION: Loads a previously saved NeuralForecast model from disk to use for inference on new time series data.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/17_transfer_learning.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfcst2 = NeuralForecast.load(path='./results/transfer/')\n```\n\n----------------------------------------\n\nTITLE: Testing AutoTiDE Configuration and Usage\nDESCRIPTION: Example demonstrating how to configure, fit, and predict with the AutoTiDE model using both Ray and Optuna backends. The code uses a minimal configuration for quick testing.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_55\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n# Use your own config or AutoTiDE.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12)\nmodel = AutoTiDE(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n# Optuna\nmodel = AutoTiDE(h=12, config=None, backend='optuna')\n```\n\n----------------------------------------\n\nTITLE: Executing Single Validation Step for Recurrent Models in Python using PyTorch\nDESCRIPTION: This method performs a single forward pass for one step of the recurrent validation process. It takes the current input sequence (`insample_y`), associated masks and exogenous variables, packages them into a dictionary `windows_batch`, and feeds this batch to the model (`self(...)`) to get the prediction for the next step. It is called iteratively by `_validate_step_recurrent_batch`.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n    def _validate_step_recurrent_single(self, insample_y, insample_mask, hist_exog, futr_exog, stat_exog, y_idx):\n        # Input sequence\n        windows_batch = dict(insample_y=insample_y,                 # [Ws, L, n_series]\n                        insample_mask=insample_mask,                # [Ws, L, n_series]\n                        futr_exog=futr_exog,                        # univariate: [Ws, L, F]; multivariate: [Ws, F, L, n_series]\n                        hist_exog=hist_exog,                        # univariate: [Ws, L, X]; multivariate: [Ws, X, L, n_series]\n                        stat_exog=stat_exog)                        # univariate: [Ws, S]; multivariate: [n_series, S]\n\n        # Model Predictions\n        output_batch_unmapped = self(windows_batch)\n```\n\n----------------------------------------\n\nTITLE: Displaying Documentation for DLinear.predict Method\nDESCRIPTION: A utility function call to display the documentation specifically for the predict method of the DLinear class, which would show how to generate forecasts with the trained model.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.dlinear.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(DLinear.predict, name='DLinear.predict')\n```\n\n----------------------------------------\n\nTITLE: Chebyshev Polynomial Computation\nDESCRIPTION: Implements Chebyshev polynomial calculation for graph Laplacian matrices. Generates multi-order Laplacian representations used in graph convolution operations.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.stemgnn.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ndef cheb_polynomial(self, laplacian):\n    N = laplacian.size(0)\n    laplacian = laplacian.unsqueeze(0)\n    first_laplacian = torch.zeros([1, N, N], device=laplacian.device, dtype=torch.float)\n    second_laplacian = laplacian\n    third_laplacian = (2 * torch.matmul(laplacian, second_laplacian)) - first_laplacian\n    forth_laplacian = 2 * torch.matmul(laplacian, third_laplacian) - second_laplacian\n    multi_order_laplacian = torch.cat([first_laplacian, second_laplacian, third_laplacian, forth_laplacian], dim=0)\n    return multi_order_laplacian\n```\n\n----------------------------------------\n\nTITLE: Implementing Sampling for NBMM in Python\nDESCRIPTION: This method constructs the empirical quantiles from the estimated Distribution by sampling from it independently. It returns samples, sample mean, and quantiles.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_63\n\nLANGUAGE: python\nCODE:\n```\ndef sample(self,\n           distr_args: torch.Tensor,\n           num_samples: Optional[int] = None):\n    if num_samples is None:\n        num_samples = self.num_samples\n\n    # Instantiate Scaled Decoupled Distribution\n    distr = self.get_distribution(distr_args=distr_args)\n    samples = distr.sample(sample_shape=(num_samples,))\n    samples = samples.permute(1, 2, 3, 0)                  # [samples, B, H, N] -> [B, H, N, samples]\n\n    sample_mean = torch.mean(samples, dim=-1, keepdim=True) \n\n    # Compute quantiles\n    quantiles_device = self.quantiles.to(distr_args[0].device)\n    quants = torch.quantile(input=samples, \n                            q=quantiles_device, \n                            dim=-1)\n    quants = quants.permute(1, 2, 3, 0) # [Q, B, H, N] -> [B, H, N, Q]\n\n    return samples, sample_mean, quants\n```\n\n----------------------------------------\n\nTITLE: Model Setup and Configuration\nDESCRIPTION: Configure NHITS model with hyperparameter search space and setup AutoNHITS for training\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/06_longhorizon_probabilistic.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nimport torch\nfrom neuralforecast.auto import AutoNHITS\nfrom neuralforecast.core import NeuralForecast\nfrom neuralforecast.losses.pytorch import DistributionLoss\nfrom ray import tune\n```\n\nLANGUAGE: python\nCODE:\n```\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\ntorch.set_float32_matmul_precision('high')\n```\n\nLANGUAGE: python\nCODE:\n```\nhorizon = 96\n\nnhits_config = {\n       \"learning_rate\": tune.choice([1e-3]),\n       \"max_steps\": tune.choice([1000]),\n       \"input_size\": tune.choice([5 * horizon]),\n       \"batch_size\": tune.choice([7]),\n       \"windows_batch_size\": tune.choice([256]),\n       \"n_pool_kernel_size\": tune.choice([[2, 2, 2], [16, 8, 1]]),\n       \"n_freq_downsample\": tune.choice([[168, 24, 1], [24, 12, 1], [1, 1, 1]]),\n       \"activation\": tune.choice(['ReLU']),\n       \"n_blocks\":  tune.choice([[1, 1, 1]]),\n       \"mlp_units\":  tune.choice([[[512, 512], [512, 512], [512, 512]]]),\n       \"interpolation_mode\": tune.choice(['linear']),\n       \"random_seed\": tune.randint(1, 10),\n       \"scaler_type\": tune.choice(['robust']),\n       \"val_check_steps\": tune.choice([100])\n    }\n```\n\n----------------------------------------\n\nTITLE: Converting Probabilities to Binary Predictions with Threshold\nDESCRIPTION: Applies a threshold of 0.5 to transform probability predictions from both models into binary (0/1) predictions for final classification results.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/16_temporal_classification.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Define classification threshold for final predictions\n# If (prob > threshold) -> 1\nY_hat_df['NHITS'] = (Y_hat_df['NHITS'] > 0.5) * 1\nY_hat_df['MLP'] = (Y_hat_df['MLP'] > 0.5) * 1\nY_hat_df\n```\n\n----------------------------------------\n\nTITLE: Documenting the feature_importance_correlations Method for TFT Model - Python\nDESCRIPTION: Calls show_doc to expose the feature_importance_correlations method for the TFT model, making the feature-importance/attention correlation information readily accessible in generated or interactive documentation. Method must exist on the TFT class; enhances API user experience.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(TFT.feature_importance_correlations , name='TFT.feature_importance_correlations', title_level=3)\n```\n\n----------------------------------------\n\nTITLE: Initializing AutoNHITS Model for Hyperparameter Tuning\nDESCRIPTION: Creates an AutoNHITS model instance with the specified forecasting horizon and hyperparameter search configuration for automatic tuning.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/04_longhorizon_nhits.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodels = [AutoNHITS(h=horizon,\n                    config=nhits_config, \n                    num_samples=5)]\n```\n\n----------------------------------------\n\nTITLE: ISQF Scale Decoupling in PyTorch\nDESCRIPTION: This function decouples scale parameters from distribution outputs to stabilize optimization. It passes through location and scale parameters to the distribution constructor, allowing for more stable training.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_34\n\nLANGUAGE: python\nCODE:\n```\ndef isqf_scale_decouple(output, loc=None, scale=None):\n    \"\"\" ISQF Scale Decouple\n\n    Stabilizes model's output optimization. We simply pass through\n    the location and the scale to the (transformed) distribution constructor\n    \"\"\"\n    spline_knots, spline_heights, beta_l, beta_r, qk_y, qk_x_repeat = output\n    if loc is None:\n        loc = torch.zeros_like(beta_l)\n    if scale is None:\n        scale = torch.ones_like(beta_l)\n\n    return (spline_knots, spline_heights, beta_l, beta_r, qk_y, qk_x_repeat, loc, scale)\n```\n\n----------------------------------------\n\nTITLE: Implementing Accuracy Metric for Categorical Forecasts in Python\nDESCRIPTION: Defines the Accuracy class, which computes the accuracy between categorical actual and predicted values. This metric is non-differentiable and intended for evaluation purposes only.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_75\n\nLANGUAGE: python\nCODE:\n```\nclass Accuracy(BasePointLoss):\n    \"\"\" Accuracy\n\n    Computes the accuracy between categorical `y` and `y_hat`.\n    This evaluation metric is only meant for evalution, as it\n    is not differentiable.\n\n    $$ \\mathrm{Accuracy}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\mathrm{1}\\{\\mathbf{y}_{\\tau}==\\mathbf{\\hat{y}}_{\\tau}\\} $$\n\n    \"\"\"\n    def __init__(self,):\n        super(Accuracy, self).__init__()\n        self.is_distribution_output = False\n        self.outputsize_multiplier = 1\n\n    def domain_map(self, y_hat: torch.Tensor):\n        \"\"\"\n        Input:\n        Univariate: [B, H, 1]\n        Multivariate: [B, H, N]\n\n        Output: [B, H, N]\n        \"\"\"\n\n        return y_hat\n    \n    def __call__(self,\n                 y: torch.Tensor,\n                 y_hat: torch.Tensor,\n                 y_insample: torch.Tensor,\n                 mask: Union[torch.Tensor, None] = None,\n                 ) -> torch.Tensor:\n        \"\"\"\n        **Parameters:**<br>\n        `y`: tensor, Actual values.<br>\n        `y_hat`: tensor, Predicted values.<br>\n        `mask`: tensor, Specifies date stamps per serie to consider in loss.<br>\n\n        **Returns:**<br>\n        `accuracy`: tensor (single value).\n        \"\"\"\n\n        if mask is None:\n            mask = torch.ones_like(y_hat)\n\n        measure = (y == y_hat) * mask\n        accuracy = torch.mean(measure)\n        return accuracy\n```\n\n----------------------------------------\n\nTITLE: Evaluating Time Series Forecast Accuracy with MAE in Python\nDESCRIPTION: Calculates the Mean Absolute Error (MAE) for the NHITS model's forecasts on the AirPassengers test set and compares it with benchmark models ETS and AutoARIMA.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/17_transfer_learning.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfcst_mae = mae(Y_test_df.merge(Y_hat_df), models=['NHITS'])['NHITS'].item()\nprint(f'NHITS     MAE: {fcst_mae:.3f}')\nprint('ETS       MAE: 16.222')\nprint('AutoARIMA MAE: 18.551')\n```\n\n----------------------------------------\n\nTITLE: Implementing Random Sampling for Distributions in PyTorch\nDESCRIPTION: This method generates random samples from the distribution by drawing uniform random values and applying the quantile function to transform them to the target distribution. It handles different sample shapes appropriately.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_32\n\nLANGUAGE: python\nCODE:\n```\ndef rsample(self, sample_shape: torch.Size = torch.Size()) -> torch.Tensor:\n    \"\"\"\n    Function used to draw random samples\n    \n    **Parameters**\n\n    `num_samples`: number of samples\n\n    \"\"\"\n\n    # if sample_shape=()) then input_alpha should have the same shape\n    # as beta_l, i.e., (*batch_shape,)\n    # else u should be (*sample_shape, *batch_shape)\n    target_shape = (\n        self.beta_l.shape\n        if sample_shape == torch.Size()\n        else torch.Size(sample_shape) + self.beta_l.shape\n    )\n\n    alpha = torch.rand(\n        target_shape,\n        dtype=self.beta_l.dtype,\n        device=self.beta_l.device,\n        layout=self.beta_l.layout,\n    )\n\n    sample = self.quantile(alpha)\n\n    if sample_shape == torch.Size():\n        sample = sample.squeeze(dim=0)\n\n    return sample\n\n@property\ndef batch_shape(self) -> torch.Size:\n    return self.beta_l.shape\n```\n\n----------------------------------------\n\nTITLE: Testing TimeSeriesDataModule with Static Features in Python\nDESCRIPTION: This snippet tests the TimeSeriesDataModule with both temporal and static features, verifying the shape and contents of the batches for both types of features.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/tsdataset.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n\nbatch_size = 128\nn_static_features = 2\nn_temporal_features = 4\ntemporal_df, static_df = generate_series(n_series=1000,\n                                         n_static_features=n_static_features,\n                                         n_temporal_features=n_temporal_features, \n                                         equal_ends=False)\n\ndataset, indices, dates, ds = TimeSeriesDataset.from_df(df=temporal_df, static_df=static_df)\ndata = TimeSeriesDataModule(dataset=dataset,\n                            batch_size=batch_size, drop_last=True)\n\nfor batch in data.train_dataloader():\n    test_eq(batch['temporal'].shape, (batch_size, n_temporal_features + 2, 500))\n    test_eq(batch['temporal_cols'],\n            ['y'] + [f'temporal_{i}' for i in range(n_temporal_features)] + ['available_mask'])\n    \n    test_eq(batch['static'].shape, (batch_size, n_static_features))\n    test_eq(batch['static_cols'], [f'static_{i}' for i in range(n_static_features)])\n```\n\n----------------------------------------\n\nTITLE: Training NBEATSx Model with Interpretable Components\nDESCRIPTION: Configures and trains an NBEATSx model with trend and seasonality stack types to decompose the time series.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/14_interpretable_decompositions.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n%%capture\nmodels = [NBEATSx(h=horizon,                           # Forecast horizon\n                  input_size=2 * horizon,              # Length of input sequence\n                  loss=HuberLoss(),                    # Robust Huber Loss\n                  max_steps=1000,                      # Number of steps to train\n                  dropout_prob_theta=0.5,\n                  stack_types=['trend', 'seasonality'], # Harmonic/Trend projection basis\n                  n_polynomials=0,                      # Lower frequencies can be captured by polynomials\n                  n_blocks=[1, 1],\n                  mlp_units=[[64, 64],[64, 64]],\n                  val_check_steps=10,                  # Frequency of validation signal (affects early stopping)\n              )\n          ]\nnf = NeuralForecast(models=models, freq=1)\nnf.fit(df=Y_train_df)\n```\n\n----------------------------------------\n\nTITLE: Implementing AutoRNN Class for Automatic RNN Model Selection\nDESCRIPTION: Defines the AutoRNN class which extends BaseAuto to provide automatic hyperparameter tuning for RNN-based forecasting models, with configurable search spaces and support for both Ray and Optuna backends.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass AutoRNN(BaseAuto):\n    \n    default_config = {\n        \"input_size_multiplier\": [-1, 4, 16, 64],\n        \"inference_input_size_multiplier\": [-1],\n        \"h\": None,\n        \"encoder_hidden_size\": tune.choice([16, 32, 64, 128]),\n        \"encoder_n_layers\": tune.randint(1, 4),\n        \"context_size\": tune.choice([5, 10, 50]),\n        \"decoder_hidden_size\": tune.choice([16, 32, 64, 128]),\n        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n        \"max_steps\": tune.choice([500, 1000]),\n        \"batch_size\": tune.choice([16, 32]),\n        \"loss\": None,\n        \"random_seed\": tune.randint(1, 20)\n    }\n\n    def __init__(self,\n                 h,\n                 loss=MAE(),\n                 valid_loss=None,\n                 config=None, \n                 search_alg=BasicVariantGenerator(random_state=1),\n                 num_samples=10,\n                 refit_with_val=False,\n                 cpus=cpu_count(),\n                 gpus=torch.cuda.device_count(),\n                 verbose=False,\n                 alias=None,\n                 backend='ray',\n                 callbacks=None,\n                ):\n        \"\"\" Auto RNN\n        \n        **Parameters:**<br>\n        \n        \"\"\"\n        # Define search space, input/output sizes\n        if config is None:\n            config = self.get_default_config(h=h, backend=backend)         \n\n        super(AutoRNN, self).__init__(\n              cls_model=RNN, \n              h=h,\n              loss=loss,\n              valid_loss=valid_loss,\n              config=config, \n              search_alg=search_alg,\n              num_samples=num_samples, \n              refit_with_val=refit_with_val,\n              cpus=cpus,\n              gpus=gpus,\n              verbose=verbose,\n              alias=alias,\n              backend=backend,\n              callbacks=callbacks,\n        )\n\n    @classmethod\n    def get_default_config(cls, h, backend, n_series=None):\n        config = cls.default_config.copy()\n        config['input_size'] = tune.choice([h*x \\\n                        for x in config['input_size_multiplier']])\n        config['inference_input_size'] = tune.choice([h*x \\\n                        for x in config['inference_input_size_multiplier']])\n        del config['input_size_multiplier'], config['inference_input_size_multiplier']\n        if backend == 'optuna':\n            config = cls._ray_config_to_optuna(config)         \n\n        return config\n```\n\n----------------------------------------\n\nTITLE: Unit Testing AutoDilatedRNN with Optuna and Ray Backends in Python\nDESCRIPTION: This snippet contains unit tests for the `AutoDilatedRNN` class. It checks if the Optuna configuration correctly sets the forecast horizon `h`. It verifies that `AutoDilatedRNN` includes all necessary arguments from its `BaseAuto` parent class using `test_args`. It then tests model fitting with modified default configurations for both the Optuna backend (defining a function `my_config_new` to update the config generated per trial) and the Ray backend (directly modifying the configuration dictionary). These tests ensure the class works correctly with custom configurations on both supported backends.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n# Check Optuna\nassert model.config(MockTrial())['h'] == 12\n\n# Unit test to test that Auto* model contains all required arguments from BaseAuto\ntest_args(AutoDilatedRNN, exclude_args=['cls_model']) \n\n# Unit test for situation: Optuna with updated default config\nmy_config = AutoDilatedRNN.get_default_config(h=12, backend='optuna')\ndef my_config_new(trial):\n    config = {**my_config(trial)}\n    config.update({'max_steps': 1, 'val_check_steps': 1, 'input_size': -1, 'encoder_hidden_size': 8})\n    return config\n\nmodel = AutoDilatedRNN(h=12, config=my_config_new, backend='optuna', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n\n# Unit test for situation: Ray with updated default config\nmy_config = AutoDilatedRNN.get_default_config(h=12, backend='ray')\nmy_config['max_steps'] = 1\nmy_config['val_check_steps'] = 1\nmy_config['input_size'] = -1\nmy_config['encoder_hidden_size'] = 8\nmodel = AutoDilatedRNN(h=12, config=my_config, backend='ray', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: Forecast Evaluation Implementation\nDESCRIPTION: Implements evaluation metrics for the hierarchical forecasts using scaled Continuous Ranked Probability Score (sCRPS).\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/09_hierarchical_forecasting.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom utilsforecast.losses import scaled_crps\nfrom hierarchicalforecast.evaluation import evaluate\n\ndf_metrics = Y_hat_df.merge(Y_test_df.drop(columns=\"month\"), on=['unique_id', 'ds'])\ndf_metrics = df_metrics.merge(Y_rec_df, on=['unique_id', 'ds'])\n\nmetrics = evaluate(df = df_metrics,\n                    tags = tags,\n                    metrics = [scaled_crps],\n                    models= [\"NHITS\", \"AutoARIMA\"],\n                    level = np.arange(2, 100, 2),\n                    train_df = Y_train_df.drop(columns=\"month\"),\n                    )\n```\n\n----------------------------------------\n\nTITLE: Installing NeuralForecast with pip\nDESCRIPTION: Installs the NeuralForecast library using pip. The installation is captured to suppress output.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/getting-started/02_quickstart.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n!pip install neuralforecast\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Hyperparameter Optimization in NeuralForecast\nDESCRIPTION: Imports the necessary libraries for hyperparameter optimization including Ray, PyTorch, and PyTorch Lightning. These libraries provide the foundation for the automated optimization framework.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_auto.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#| export\nimport warnings\nfrom copy import deepcopy\nfrom os import cpu_count\n\nimport torch\nimport pytorch_lightning as pl\n\nfrom ray import air, tune\nfrom ray.tune.integration.pytorch_lightning import TuneReportCallback\nfrom ray.tune.search.basic_variant import BasicVariantGenerator\n```\n\n----------------------------------------\n\nTITLE: Testing Future Exogenous Data Validation in NeuralForecast\nDESCRIPTION: Tests error handling for future exogenous data (futr_df) in the predict method. Verifies proper error messages for missing data, insufficient rows, missing features, and null values, while ensuring warnings are issued for extra rows.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_32\n\nLANGUAGE: python\nCODE:\n```\n# test futr_df contents\nmodels = [NHITS(h=6, input_size=24, max_steps=10, hist_exog_list=['trend'], futr_exog_list=['trend'])]\nnf = NeuralForecast(models=models, freq='M')\nnf.fit(AirPassengersPanel_train)\n# not enough rows in futr_df raises an error\ntest_fail(lambda: nf.predict(futr_df=AirPassengersPanel_test.head()), contains='There are missing combinations')\n# extra rows issues a warning\nwith warnings.catch_warnings(record=True) as issued_warnings:\n    warnings.simplefilter('always', UserWarning)\n    nf.predict(futr_df=AirPassengersPanel_test)\nassert any('Dropped 12 unused rows' in str(w.message) for w in issued_warnings)\n# models require futr_df and not provided raises an error\ntest_fail(lambda: nf.predict(), contains=\"Models require the following future exogenous features: {'trend'}\") \n# missing feature in futr_df raises an error\ntest_fail(lambda: nf.predict(futr_df=AirPassengersPanel_test.drop(columns='trend')), contains=\"missing from `futr_df`: {'trend'}\")\n# null values in futr_df raises an error\ntest_fail(lambda: nf.predict(futr_df=AirPassengersPanel_test.assign(trend=np.nan)), contains='Found null values in `futr_df`')\n```\n\n----------------------------------------\n\nTITLE: Testing Validation Scale with NHITS Model and Different Scalers\nDESCRIPTION: Verifies the validation loss behavior for the NHITS model with both robust scaling and no scaling. Ensures that the validation loss falls within expected ranges, indicating proper model training.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_49\n\nLANGUAGE: python\nCODE:\n```\n# test validation scale BaseWindows\n\nmodels = [NHITS(h=12, input_size=24, max_steps=50, scaler_type='robust')]\nnf = NeuralForecast(models=models, freq='M')\nnf.fit(AirPassengersPanel_train,val_size=12)\nvalid_losses = nf.models[0].valid_trajectories\nassert valid_losses[-1][1] < 40, 'Validation loss is too high'\nassert valid_losses[-1][1] > 10, 'Validation loss is too low'\n\nmodels = [NHITS(h=12, input_size=24, max_steps=50, scaler_type=None)]\nnf = NeuralForecast(models=models, freq='M')\nnf.fit(AirPassengersPanel_train,val_size=12)\nvalid_losses = nf.models[0].valid_trajectories\nassert valid_losses[-1][1] < 40, 'Validation loss is too high'\nassert valid_losses[-1][1] > 10, 'Validation loss is too low'\n```\n\n----------------------------------------\n\nTITLE: Generating Predictions with Confidence Intervals\nDESCRIPTION: Generates forecasts with 90% confidence intervals using the trained models.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/20_conformal_prediction.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\npreds = nf.predict(futr_df=AirPassengersPanel_test, level=[90])\n```\n\n----------------------------------------\n\nTITLE: Implementing Weight Initialization Disabling Context Manager in Python\nDESCRIPTION: Defines a context manager to temporarily disable PyTorch's weight initialization methods, useful when loading saved models to avoid unnecessary computations.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@contextmanager\ndef _disable_torch_init():\n    \"\"\"Context manager used to disable pytorch's weight initialization.\n\n    This is especially useful when loading saved models, since when initializing\n    a model the weights are also initialized following some method\n    (e.g. kaiming uniform), and that time is wasted since we'll override them with\n    the saved weights.\"\"\"\n    def noop(*args, **kwargs):\n        return\n        \n    kaiming_uniform = nn.init.kaiming_uniform_\n    kaiming_normal = nn.init.kaiming_normal_\n    xavier_uniform = nn.init.xavier_uniform_\n    xavier_normal = nn.init.xavier_normal_\n    \n    nn.init.kaiming_uniform_ = noop\n    nn.init.kaiming_normal_ = noop\n    nn.init.xavier_uniform_ = noop\n    nn.init.xavier_normal_ = noop\n    try:\n        yield\n    finally:\n        nn.init.kaiming_uniform_ = kaiming_uniform\n        nn.init.kaiming_normal_ = kaiming_normal\n        nn.init.xavier_uniform_ = xavier_uniform\n        nn.init.xavier_normal_ = xavier_normal\n```\n\n----------------------------------------\n\nTITLE: Implementing Forward Method for BiTCN in Python\nDESCRIPTION: This code snippet defines the forward method of the BiTCN class, which processes input data through the network architecture.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.bitcn.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, windows_batch):\n    # Parse windows_batch\n    x             = windows_batch['insample_y'].contiguous()        #   [B, L, 1]\n    hist_exog     = windows_batch['hist_exog']                      #   [B, L, X]\n    futr_exog     = windows_batch['futr_exog']                      #   [B, L + h, F]\n    stat_exog     = windows_batch['stat_exog']                      #   [B, S]\n```\n\n----------------------------------------\n\nTITLE: TimeSeriesDataset Implementation\nDESCRIPTION: Concrete implementation of time series dataset with methods for data access, alignment, and manipulation. Includes functionality for appending future observations and trimming datasets.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/tsdataset.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass TimeSeriesDataset(BaseTimeSeriesDataset):\n    def __init__(self, temporal, temporal_cols, indptr, y_idx: int, static=None, static_cols=None):\n        self.temporal = self._as_torch_copy(temporal)\n        self.indptr = indptr\n        self.n_groups = self.indptr.size - 1\n        sizes = np.diff(indptr)\n        super().__init__(\n            temporal_cols=temporal_cols,\n            max_size=sizes.max().item(),\n            min_size=sizes.min().item(),\n            y_idx=y_idx,\n            static=static,\n            static_cols=static_cols,\n        )\n\n    def __getitem__(self, idx):\n        if isinstance(idx, int):\n            temporal = torch.zeros(size=(len(self.temporal_cols), self.max_size), dtype=torch.float32)\n            ts = self.temporal[self.indptr[idx] : self.indptr[idx + 1], :]\n            temporal[:len(self.temporal_cols), -len(ts):] = ts.permute(1, 0)\n            static = None if self.static is None else self.static[idx,:]\n            item = dict(temporal=temporal, temporal_cols=self.temporal_cols,\n                        static=static, static_cols=self.static_cols,\n                        y_idx=self.y_idx)\n            return item\n        raise ValueError(f'idx must be int, got {type(idx)}')\n```\n\n----------------------------------------\n\nTITLE: Generating Future Time Series Dataframe in NeuralForecast\nDESCRIPTION: Creates a dataframe with future timestamps for forecasting based on the last observed dates for each time series ID. Handles optional input dataframes and maintains the established frequency and forecast horizon.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef get_missing_future(\n    self, futr_df: DFType, df: Optional[DFType] = None\n) -> DFType:\n    \"\"\"Get the missing ids and times combinations in `futr_df`.\n    \n    Parameters\n    ----------\n    futr_df : pandas or polars DataFrame\n        DataFrame with [`unique_id`, `ds`] columns and `df`'s future exogenous.\n    df : pandas or polars DataFrame, optional (default=None)\n        DataFrame with columns [`unique_id`, `ds`, `y`] and exogenous variables.\n        Only required if this is different than the one used in the fit step.\n    \"\"\"\n    expected = self.make_future_dataframe(df)\n    ids = [self.id_col, self.time_col]\n    return ufp.anti_join(expected, futr_df[ids], on=ids)\n```\n\n----------------------------------------\n\nTITLE: Augmenting Air Passenger Data with Calendar Features in Python\nDESCRIPTION: Demonstrates how to apply the augment_calendar_df function to an air passenger dataset using monthly frequency. The result is stored in a new dataframe that contains the original data plus the normalized calendar features.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/utils.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nAirPassengerPanelCalendar, calendar_cols = augment_calendar_df(df=AirPassengersPanel, freq='M')\nAirPassengerPanelCalendar.head()\n```\n\n----------------------------------------\n\nTITLE: Implementing Spline Parameterization Method for ISQF\nDESCRIPTION: Static method that parameterizes the x or y positions of spline knots in the ISQF distribution. It uses a softmax function to ensure the spacing between spline knots is in [0,1] and sums to 1.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\n    @staticmethod\n    def parameterize_spline(\n        spline_knots: torch.Tensor,\n        qk: torch.Tensor,\n        qk_plus: torch.Tensor,\n        tol: float = 1e-4,\n    ) -> Tuple[torch.Tensor, torch.Tensor]:\n        r\"\"\"\n        Function to parameterize the x or y positions of the spline knots\n        Parameters\n        ----------\n        spline_knots\n            variable that parameterizes the spline knot positions\n        qk\n            x or y positions of the quantile knots (qk),\n            with index=1, ..., num_qk-1,\n            shape: (*batch_shape, num_qk-1)\n        qk_plus\n            x or y positions of the quantile knots (qk),\n            with index=2, ..., num_qk,\n            shape: (*batch_shape, num_qk-1)\n        num_pieces\n            number of spline knot pieces\n        tol\n            tolerance hyperparameter for numerical stability\n        Returns\n        -------\n        sk\n            x or y positions of the spline knots (sk),\n            shape: (*batch_shape, num_qk-1, num_pieces)\n        delta_sk\n            difference of x or y positions of the spline knots (sk),\n            shape: (*batch_shape, num_qk-1, num_pieces)\n        \"\"\"\n\n        # The spacing between spline knots is parameterized\n        # by softmax function (in [0,1] and sum to 1)\n        # We add tol to prevent overflow in computing 1/spacing in spline CRPS\n        # After adding tol, it is normalized by\n        # (1 + num_pieces * tol) to keep the sum-to-1 property\n\n        num_pieces = spline_knots.shape[-1]\n\n        delta_x = (F.softmax(spline_knots, dim=-1) + tol) / (\n            1 + num_pieces * tol\n        )\n\n        zero_tensor = torch.zeros_like(\n            delta_x[..., 0:1]\n        )  # 0:1 for keeping dimension\n        x = torch.cat(\n            [zero_tensor, torch.cumsum(delta_x, dim=-1)[..., :-1]], dim=-1\n        )\n\n        qk, qk_plus = qk.unsqueeze(dim=-1), qk_plus.unsqueeze(dim=-1)\n        sk = x * (qk_plus - qk) + qk\n        delta_sk = delta_x * (qk_plus - qk)\n\n        return sk, delta_sk\n```\n\n----------------------------------------\n\nTITLE: Visualizing Attention Weights for All Future Time Steps\nDESCRIPTION: Shows how to visualize attention weights for all future time steps in the TFT model. This helps identify how the model's attention to historical data changes for different prediction horizons.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n# | eval: false\nplot_attention(nf.models[0], plot=\"all\")\n```\n\n----------------------------------------\n\nTITLE: Computing Weighted Masks for Time Series Forecasting\nDESCRIPTION: Calculates weights for each data point in the time series based on masks and horizon weights. This is used to apply differential weighting to different forecast horizons when computing the loss function.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_41\n\nLANGUAGE: python\nCODE:\n```\ndef _compute_weights(self, y, mask):\n        \"\"\"\n        Compute final weights for each datapoint (based on all weights and all masks)\n        Set horizon_weight to a ones[H] tensor if not set.\n        If set, check that it has the same length as the horizon in x.\n        \"\"\"\n        if mask is None:\n            mask = torch.ones_like(y)\n\n        if self.horizon_weight is None:\n            weights = torch.ones_like(mask)\n        else:\n            assert mask.shape[1] == len(self.horizon_weight), \\\n                'horizon_weight must have same length as Y'\n            weights = self.horizon_weight.clone()\n            weights = weights[None, :, None].to(mask.device)\n            weights = torch.ones_like(mask, device=mask.device) * weights\n        \n        return weights * mask\n```\n\n----------------------------------------\n\nTITLE: Processing Input Tensors in NBEATSx Forward Pass\nDESCRIPTION: Handles tensor concatenation for different types of exogenous inputs (historical, future, and static) and computes local projection weights. Returns backcast and forecast values based on the basis type.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeatsx.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nif self.futr_input_size > 0:\n    insample_y = torch.cat(\n        (insample_y, futr_exog.reshape(batch_size, -1)), dim=1\n    )\n\nif self.stat_input_size > 0:\n    insample_y = torch.cat(\n        (insample_y, stat_exog.reshape(batch_size, -1)), dim=1\n    )\n\n# Compute local projection weights and projection\ntheta = self.layers(insample_y)\n\nif isinstance(self.basis, ExogenousBasis):\n    if self.futr_input_size > 0 and self.stat_input_size > 0:                \n        futr_exog = torch.cat(\n            (\n                futr_exog,\n                stat_exog.unsqueeze(1).expand(-1, futr_exog.shape[1], -1)\n            ),\n            dim=2\n        )\n    elif self.futr_input_size >0:\n        futr_exog = futr_exog\n    elif self.stat_input_size >0:\n        futr_exog = stat_exog\n    else:\n        raise(ValueError(\"No stats or future exogenous. ExogenousBlock not supported.\"))    \n    backcast, forecast = self.basis(theta, futr_exog)\n    return backcast, forecast\nelse:\n    backcast, forecast = self.basis(theta)\n    return backcast, forecast\n```\n\n----------------------------------------\n\nTITLE: Calculating Root Mean Squared Error (RMSE) for Time Series Forecasts\nDESCRIPTION: Implements the Root Mean Squared Error metric for evaluating time series forecasts. It uses the MSE function and takes the square root of the result.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.numpy.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef rmse(y: np.ndarray, y_hat: np.ndarray,\n         weights: Optional[np.ndarray] = None,\n         axis: Optional[int] = None) -> Union[float, np.ndarray]:\n    \"\"\" Root Mean Squared Error\n\n    Calculates Root Mean Squared Error between\n    `y` and `y_hat`. RMSE measures the relative prediction\n    accuracy of a forecasting method by calculating the squared deviation\n    of the prediction and the observed value at a given time and\n    averages these devations over the length of the series.\n    Finally the RMSE will be in the same scale\n    as the original time series so its comparison with other\n    series is possible only if they share a common scale.\n    RMSE has a direct connection to the L2 norm.\n\n    $$ \\mathrm{RMSE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\sqrt{\\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} (y_{\\tau} - \\hat{y}_{\\tau})^{2}} $$\n\n    **Parameters:**<br>\n    `y`: numpy array, Actual values.<br>\n    `y_hat`: numpy array, Predicted values.<br>\n    `mask`: numpy array, Specifies date stamps per serie to consider in loss.<br>\n\n    **Returns:**<br>\n    `rmse`: numpy array, (single value).\n    \"\"\"\n    return np.sqrt(mse(y, y_hat, weights, axis))\n```\n\n----------------------------------------\n\nTITLE: Training AutoNHITS Model with Hyperparameter Tuning\nDESCRIPTION: Training the AutoNHITS model using NeuralForecast with automatic hyperparameter tuning. The model is trained on the AirPassengers dataset with a 24-month validation period and monthly frequency.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/04_hyperparameter_tuning.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n%%capture\nnf = NeuralForecast(models=[model], freq='ME')\nnf.fit(df=Y_df, val_size=24)\n```\n\n----------------------------------------\n\nTITLE: Implementing ReversibleInstanceNorm1d for TSMixerx in Python\nDESCRIPTION: Defines the ReversibleInstanceNorm1d class, a reversible instance normalization layer used in the TSMixerx model for normalizing input features.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tsmixerx.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass ReversibleInstanceNorm1d(nn.Module):\n    def __init__(self, n_series, eps=1e-5):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones((1, 1, 1, n_series)))\n        self.bias = nn.Parameter(torch.zeros((1, 1, 1, n_series)))\n        self.eps = eps\n\n    def forward(self, x):\n        # Batch statistics\n        self.batch_mean = torch.mean(x, axis=2, keepdim=True).detach()\n        self.batch_std = torch.sqrt(torch.var(x, axis=2, keepdim=True, unbiased=False) + self.eps).detach()\n        \n        # Instance normalization\n        x = x - self.batch_mean\n        x = x / self.batch_std\n        x = x * self.weight\n        x = x + self.bias\n        \n        return x\n\n    def reverse(self, x):\n        # Reverse the normalization\n        x = x - self.bias\n        x = x / self.weight       \n        x = x * self.batch_std\n        x = x + self.batch_mean       \n\n        return x\n```\n\n----------------------------------------\n\nTITLE: Instantiating RNN model with GMM loss and multiple hist_exog features\nDESCRIPTION: Creates an RNN model with GMM loss function using 10 components and tests revin dynamic dimensionality with trend and lagged exogenous variables. The model is configured for forecasting 12 steps ahead with early stopping and validation checks.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.scalers.ipynb#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# Instantiate BaseRecurrent model and test revin dynamic dimensionality with hist_exog_list\nmodel = RNN(h=12,\n              input_size=24,\n              loss=GMM(n_components=10, level=[90]),\n              hist_exog_list=['trend', 'y_[lag12]'],\n              max_steps=1,\n              early_stop_patience_steps=10,\n              val_check_steps=50,\n              scaler_type='revin',\n              learning_rate=1e-3)\nnf = NeuralForecast(models=[model], freq='MS')\nY_hat_df = nf.cross_validation(df=Y_df, val_size=12, n_windows=1)\n```\n\n----------------------------------------\n\nTITLE: Unit Testing AutoTCN Class Functionality in Python\nDESCRIPTION: This snippet contains several unit tests for the `AutoTCN` class. It checks if the Optuna configuration correctly sets the horizon `h`. It verifies that `AutoTCN` accepts all required arguments from its parent `BaseAuto` class using a helper function `test_args`. It then includes two test scenarios: one for the Optuna backend and one for the Ray backend, both using modified default configurations to ensure the model can be instantiated and fitted correctly under these specific conditions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n# Check Optuna\nassert model.config(MockTrial())['h'] == 12\n\n# Unit test to test that Auto* model contains all required arguments from BaseAuto\ntest_args(AutoTCN, exclude_args=['cls_model']) \n\n# Unit test for situation: Optuna with updated default config\nmy_config = AutoTCN.get_default_config(h=12, backend='optuna')\ndef my_config_new(trial):\n    config = {**my_config(trial)}\n    config.update({'max_steps': 1, 'val_check_steps': 1, 'input_size': -1, 'encoder_hidden_size': 8})\n    return config\n\nmodel = AutoTCN(h=12, config=my_config_new, backend='optuna', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n\n# Unit test for situation: Ray with updated default config\nmy_config = AutoTCN.get_default_config(h=12, backend='ray')\nmy_config['max_steps'] = 1\nmy_config['val_check_steps'] = 1\nmy_config['input_size'] = -1\nmy_config['encoder_hidden_size'] = 8\nmodel = AutoTCN(h=12, config=my_config, backend='ray', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: Displaying Documentation for VanillaTransformer.predict Method in Python\nDESCRIPTION: This snippet utilizes the `show_doc` function to present the documentation for the `predict` method of the `VanillaTransformer` class, setting the displayed name to 'VanillaTransformer.predict'.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.vanillatransformer.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(VanillaTransformer.predict, name='VanillaTransformer.predict')\n```\n\n----------------------------------------\n\nTITLE: Testing predict_insample Method with Fixed-Length Series\nDESCRIPTION: Tests the predict_insample method with regular time series data. Validates that the output size matches expectations based on the input data length, forecast horizon, and step size parameters.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_34\n\nLANGUAGE: python\nCODE:\n```\n# Test predict_insample\ntest_size = 12\nn_series = 2\nh = 12\n\ndef get_expected_size(df, h, test_size, step_size):\n    expected_size = 0\n    uids = df['unique_id'].unique()\n    for uid in uids:\n        input_len = len(df[df['unique_id'] == uid])\n        expected_size += ((input_len - test_size - h) / step_size + 1)*h\n    return expected_size\n        \nmodels = [\n    NHITS(h=h, input_size=24, loss=MQLoss(level=[80]), max_steps=1, alias='NHITS', scaler_type=None),\n    RNN(h=h, input_size=-1, loss=MAE(), max_steps=1, alias='RNN', scaler_type=None),\n    ]\n\nnf = NeuralForecast(models=models, freq='M')\ncv = nf.cross_validation(df=AirPassengersPanel_train, static_df=AirPassengersStatic, val_size=0, test_size=test_size, n_windows=None)\n\nforecasts = nf.predict_insample(step_size=1)\n\nexpected_size = get_expected_size(AirPassengersPanel_train, h, test_size, step_size=1)\nassert len(forecasts) == expected_size, f'Shape mismatch in predict_insample: {len(forecasts)=}, {expected_size=}'\n```\n\n----------------------------------------\n\nTITLE: Converting Pandas to Polars DataFrames for Testing\nDESCRIPTION: Converts pandas DataFrame test data to polars format with appropriate column name mapping for testing NeuralForecast's polars compatibility.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_55\n\nLANGUAGE: python\nCODE:\n```\nrenamer = {'unique_id': 'uid', 'ds': 'time', 'y': 'target'}\ninverse_renamer = {v: k for k, v in renamer.items()}\nAirPassengers_pl = polars.from_pandas(AirPassengersPanel_train)\nAirPassengers_pl = AirPassengers_pl.rename(renamer)\nAirPassengersStatic_pl = polars.from_pandas(AirPassengersStatic)\nAirPassengersStatic_pl = AirPassengersStatic_pl.rename({'unique_id': 'uid'})\n```\n\n----------------------------------------\n\nTITLE: Displaying RMoK.predict Method Documentation\nDESCRIPTION: A utility call to display the documentation for the predict method of the RMoK class, which is likely inherited from the BaseModel parent class.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.rmok.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(RMoK.predict, name='RMoK.predict')\n```\n\n----------------------------------------\n\nTITLE: Implementing Distributed TimeSeriesDataModule in Python\nDESCRIPTION: This class extends TimeSeriesDataModule to handle distributed datasets. It reads data from Parquet files and sets up the dataset for each distributed process.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/tsdataset.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass _DistributedTimeSeriesDataModule(TimeSeriesDataModule):\n    def __init__(\n        self,\n        dataset: _FilesDataset,\n        batch_size=32,\n        valid_batch_size=1024,\n        drop_last=False,\n        shuffle_train=True,\n        **dataloaders_kwargs\n    ):\n        super(TimeSeriesDataModule, self).__init__()\n        self.files_ds = dataset\n        self.batch_size = batch_size\n        self.valid_batch_size = valid_batch_size\n        self.drop_last = drop_last\n        self.shuffle_train = shuffle_train\n        self.dataloaders_kwargs = dataloaders_kwargs\n\n    def setup(self, stage):\n        import torch.distributed as dist\n\n        df = pd.read_parquet(self.files_ds.files[dist.get_rank()])\n        if self.files_ds.static_cols is not None:\n            static_df = (\n                df[[self.files_ds.id_col] + self.files_ds.static_cols.tolist()]\n                .groupby(self.files_ds.id_col, observed=True)\n                .head(1)\n            )\n            df = df.drop(columns=self.files_ds.static_cols)\n        else:\n            static_df = None\n        self.dataset, *_ = TimeSeriesDataset.from_df(\n            df=df,\n            static_df=static_df,\n            id_col=self.files_ds.id_col,\n            time_col=self.files_ds.time_col,\n            target_col=self.files_ds.target_col,\n        )\n```\n\n----------------------------------------\n\nTITLE: Testing NBEATS Validation Step - Python\nDESCRIPTION: Trains NBEATS with a validation set extracted from the training data and predicts on the dataset, adding the prediction result to the test DataFrame. The last lines plot train, test, and forecasted values. This snippet validates model behavior with validation data. Assumes availability of pandas, matplotlib, and NBEATS.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeats.ipynb#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n# test validation step\ndataset, *_ = TimeSeriesDataset.from_df(Y_train_df)\nmodel = NBEATS(input_size=24, h=12, windows_batch_size=None, max_steps=1)\nmodel.fit(dataset=dataset, val_size=12)\ny_hat_w_val = model.predict(dataset=dataset)\nY_test_df['N-BEATS'] = y_hat_w_val\n\npd.concat([Y_train_df, Y_test_df]).drop('unique_id', axis=1).set_index('ds').plot()\n```\n\n----------------------------------------\n\nTITLE: Testing Validation Scale with LSTM Recurrent Model\nDESCRIPTION: Tests the validation scale for the LSTM model, which inherits from BaseRecurrent. Uses a complex LSTM configuration with multiple layers and the MAE loss function, validating that the loss falls within expected bounds.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_50\n\nLANGUAGE: python\nCODE:\n```\n# test validation scale BaseRecurrent\n\nnf = NeuralForecast(\n    models=[LSTM(h=12,\n                 input_size=-1,\n                 loss=MAE(),\n                 scaler_type='robust',\n                 encoder_n_layers=2,\n                 encoder_hidden_size=128,\n                 context_size=10,\n                 decoder_hidden_size=128,\n                 decoder_layers=2,\n                 max_steps=50,\n                 val_check_steps=10,\n                 )\n    ],\n    freq='M'\n)\nnf.fit(AirPassengersPanel_train,val_size=12)\nvalid_losses = nf.models[0].valid_trajectories\nassert valid_losses[-1][1] < 100, 'Validation loss is too high'\nassert valid_losses[-1][1] > 30, 'Validation loss is too low'\n```\n\n----------------------------------------\n\nTITLE: Huber Loss Implementation in PyTorch\nDESCRIPTION: Implementation of Huber Loss class that combines L1 and L2 losses for robust regression, providing reduced sensitivity to outliers compared to squared error loss.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_66\n\nLANGUAGE: python\nCODE:\n```\nclass HuberLoss(BasePointLoss):\n    def __init__(self, delta: float=1., horizon_weight=None):\n        super(HuberLoss, self).__init__(horizon_weight=horizon_weight,\n                                  outputsize_multiplier=1,\n                                  output_names=[''])\n        self.delta = delta\n\n    def __call__(self,\n                 y: torch.Tensor,\n                 y_hat: torch.Tensor,\n                 y_insample: torch.Tensor,\n                 mask: Union[torch.Tensor, None] = None,\n                 ) -> torch.Tensor:\n        losses = F.huber_loss(y, y_hat, reduction='none', delta=self.delta)        \n        weights = self._compute_weights(y=y, mask=mask)\n        return _weighted_mean(losses=losses, weights=weights)\n```\n\n----------------------------------------\n\nTITLE: Processing Forecast Data for Peak Detection\nDESCRIPTION: Filters and processes the cross-validation results to identify the day with maximum load and predict peaks.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/use-cases/electricity_peak_forecasting.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ncrossvalidation_df = crossvalidation_df[['ds','y','AutoNHITS']]\nmax_day = crossvalidation_df.iloc[crossvalidation_df['y'].argmax()].ds.day # Day with maximum load\ncv_df_day = crossvalidation_df.query('ds.dt.day == @max_day')\nmax_hour = cv_df_day['y'].argmax()\npeaks = cv_df_day['AutoNHITS'].argsort().iloc[-npeaks:].values # Predicted peaks\n```\n\n----------------------------------------\n\nTITLE: Importing Transformer Models from NeuralForecast Library\nDESCRIPTION: Imports the NeuralForecast core module and specific Transformer-based forecasting models: Informer, Autoformer, FEDformer, and PatchTST.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/05_longhorizon_transformers.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n%%capture\nfrom neuralforecast.core import NeuralForecast\nfrom neuralforecast.models import Informer, Autoformer, FEDformer, PatchTST\n```\n\n----------------------------------------\n\nTITLE: Implementing Scale Decoupling for NBMM in Python\nDESCRIPTION: This method stabilizes the model's output optimization by learning residual variance and residual location based on anchoring loc and scale. It also adds domain protection to the distribution parameters.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_61\n\nLANGUAGE: python\nCODE:\n```\ndef scale_decouple(self, \n                   output,\n                   loc: Optional[torch.Tensor] = None,\n                   scale: Optional[torch.Tensor] = None,\n                   eps: float=0.2):\n    if self.weighted:\n        mu, alpha, weights = output\n        weights = F.softmax(weights, dim=-1)\n    else:\n        mu, alpha = output\n\n    mu = F.softplus(mu) + 1e-8\n    alpha = F.softplus(alpha) + 1e-8    # alpha = 1/total_counts\n    if (loc is not None) and (scale is not None):\n        if loc.ndim == 3:\n            loc = loc.unsqueeze(-1)\n            scale = scale.unsqueeze(-1)           \n        mu *= loc\n        alpha /= (loc + 1.)\n\n    total_count = 1.0 / alpha\n    probs = (mu * alpha / (1.0 + mu * alpha)) + 1e-8 \n    if self.weighted:\n        return (total_count, probs, weights)\n    else:\n        return (total_count, probs)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Forecasts with Uncertainty Bounds\nDESCRIPTION: Creates a visualization comparing the true values, predictions, and uncertainty bounds for both conformal prediction and distributional approaches.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/20_conformal_prediction.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfig, (ax1, ax2) = plt.subplots(2, 1, figsize = (20, 7))\nplot_df = pd.concat([AirPassengersPanel_train, preds])\n\nplot_df = plot_df[plot_df['unique_id']=='Airline1'].drop(['unique_id','trend','y_[lag12]'], axis=1).iloc[-50:]\n\nax1.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nax1.plot(plot_df['ds'], plot_df['NHITS'], c='blue', label='median')\nax1.fill_between(x=plot_df['ds'][-12:], \n                 y1=plot_df['NHITS-lo-90'][-12:].values,\n                 y2=plot_df['NHITS-hi-90'][-12:].values,\n                 alpha=0.4, label='level 90')\nax1.set_title('AirPassengers Forecast - Uncertainty quantification using Conformal Prediction', fontsize=18)\nax1.set_ylabel('Monthly Passengers', fontsize=15)\nax1.set_xticklabels([])\nax1.legend(prop={'size': 10})\nax1.grid()\n\nax2.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nax2.plot(plot_df['ds'], plot_df['NHITS1'], c='blue', label='median')\nax2.fill_between(x=plot_df['ds'][-12:], \n                 y1=plot_df['NHITS1-lo-90'][-12:].values,\n                 y2=plot_df['NHITS1-hi-90'][-12:].values,\n                 alpha=0.4, label='level 90')\nax2.set_title('AirPassengers Forecast - Uncertainty quantification using Normal distribution', fontsize=18)\nax2.set_ylabel('Monthly Passengers', fontsize=15)\nax2.set_xlabel('Timestamp [t]', fontsize=15)\nax2.legend(prop={'size': 10})\nax2.grid()\n```\n\n----------------------------------------\n\nTITLE: Mean Absolute Error Implementation in PyTorch\nDESCRIPTION: Implementation of Mean Absolute Error (MAE) loss function that calculates the average absolute difference between predicted and actual values across a time series.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass MAE(BasePointLoss):\n    def __init__(self, horizon_weight=None):\n        super(MAE, self).__init__(horizon_weight=horizon_weight,\n                                  outputsize_multiplier=1,\n                                  output_names=[''])\n\n    def __call__(self,\n                 y: torch.Tensor,\n                 y_hat: torch.Tensor,\n                 mask: Union[torch.Tensor, None] = None,\n                 y_insample: Union[torch.Tensor, None] = None) -> torch.Tensor:\n        losses = torch.abs(y - y_hat)\n        weights = self._compute_weights(y=y, mask=mask)\n        return _weighted_mean(losses=losses, weights=weights)\n```\n\n----------------------------------------\n\nTITLE: Testing Cross-Validation with Refit Parameter\nDESCRIPTION: Tests the refit parameter in cross-validation with exogenous variables. Verifies that predictions for windows without refitting are identical to the baseline, while predictions after refitting are different, showing that the model correctly updates its parameters.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n# test cross_validation with refit\nmodels = [\n    NHITS(\n        h=12,\n        input_size=24,\n        max_steps=2,\n        futr_exog_list=['trend'],\n        stat_exog_list=['airline1', 'airline2']\n    )\n]\nnf = NeuralForecast(models=models, freq='M')\ncv_kwargs = dict(\n    df=AirPassengersPanel_train,\n    static_df=AirPassengersStatic,\n    n_windows=4,\n    use_init_models=True,\n)\ncv_res_norefit = nf.cross_validation(refit=False, **cv_kwargs)\ncutoffs = cv_res_norefit['cutoff'].unique()\nfor refit in [True, 2]:\n    cv_res = nf.cross_validation(refit=refit, **cv_kwargs)\n    refit = int(refit)\n    fltr = lambda df: df['cutoff'].isin(cutoffs[:refit])\n    expected = cv_res_norefit[fltr]\n    actual = cv_res[fltr]\n    # predictions for the no-refit windows should be the same\n    pd.testing.assert_frame_equal(\n        actual.reset_index(drop=True),\n        expected.reset_index(drop=True)\n    )\n    # predictions after refit should be different\n    test_fail(\n        lambda: pd.testing.assert_frame_equal(\n            cv_res_norefit.drop(expected.index).reset_index(drop=True),\n            cv_res.drop(actual.index).reset_index(drop=True),\n        ),\n        contains='(column name=\"NHITS\") are different',\n    )\n```\n\n----------------------------------------\n\nTITLE: Implementing BottomUp Reconciliation Matrix in Python\nDESCRIPTION: Defines a function to create a BottomUp hierarchical projection matrix for reconciliation. It takes a summing matrix as input and returns a reconciliation matrix.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.hint.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef get_bottomup_P(S: np.ndarray):\n    \"\"\"BottomUp Reconciliation Matrix.\n\n    Creates BottomUp hierarchical \\\"projection\\\" matrix is defined as:\n    $$\\mathbf{P}_{\\\\text{BU}} = [\\mathbf{0}_{\\mathrm{[b],[a]}}\\;|\\;\\mathbf{I}_{\\mathrm{[b][b]}}]$$    \n\n    **Parameters:**<br>\n    `S`: Summing matrix of size (`base`, `bottom`).<br>\n\n    **Returns:**<br>\n    `P`: Reconciliation matrix of size (`bottom`, `base`).<br>\n\n    **References:**<br>\n    - [Orcutt, G.H., Watts, H.W., & Edwards, J.B.(1968). \\\"Data aggregation and information loss\\\". The American \n    Economic Review, 58 , 773(787)](http://www.jstor.org/stable/1815532).    \n    \"\"\"\n    n_series = len(S)\n    n_agg = n_series-S.shape[1]\n    P = np.zeros_like(S)\n    P[n_agg:,:] = S[n_agg:,:]\n    P = P.T\n    return P\n```\n\n----------------------------------------\n\nTITLE: Model Training and Cross-Validation\nDESCRIPTION: Initialize and train the NHITS model using cross-validation\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/06_longhorizon_probabilistic.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodels = [AutoNHITS(h=horizon,\n                    loss=DistributionLoss(distribution='StudentT', level=[80, 90]), \n                    config=nhits_config,\n                    num_samples=5)]\n```\n\nLANGUAGE: python\nCODE:\n```\nnf = NeuralForecast(models=models, freq='15min')\n```\n\nLANGUAGE: python\nCODE:\n```\n%%capture\nY_hat_df = nf.cross_validation(df=Y_df, val_size=val_size,\n                               test_size=test_size, n_windows=None)\n```\n\n----------------------------------------\n\nTITLE: Processing Neural Forecast Batch Data with PyTorch\nDESCRIPTION: Initializes and processes batch dimensions for weights and lambdas using PyTorch's repeat_interleave function. Creates a PMM distribution object and generates samples and quantiles from the distribution.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_49\n\nLANGUAGE: python\nCODE:\n```\nN=2\nweights = torch.repeat_interleave(input=weights, repeats=N, dim=0)\nlambdas = torch.repeat_interleave(input=lambdas, repeats=N, dim=0)\n\nprint('weights.shape (N,H,1,K) \\t', weights.shape)\nprint('lambdas.shape (N,H,1, K) \\t', lambdas.shape)\n\ndistr = PMM(quantiles=[0.1, 0.40, 0.5, 0.60, 0.9], weighted=True)\nweights = torch.ones_like(lambdas)\ndistr_args = (lambdas, weights)\nsamples, sample_mean, quants = distr.sample(distr_args)\n\nprint('samples.shape (N,H,1,num_samples) ', samples.shape)\nprint('sample_mean.shape (N,H,1,1) ', sample_mean.shape)\nprint('quants.shape  (N,H,1,Q) \\t\\t', quants.shape)\n```\n\n----------------------------------------\n\nTITLE: Benchmark Implementation with AutoARIMA\nDESCRIPTION: Implements benchmark forecasting using AutoARIMA and applies hierarchical reconciliation using BottomUp and MinTrace methods.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/09_hierarchical_forecasting.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom statsforecast import StatsForecast\nfrom statsforecast.models import AutoARIMA\nfrom hierarchicalforecast.methods import BottomUp, MinTrace\nfrom hierarchicalforecast.core import HierarchicalReconciliation\n\nsf = StatsForecast(models=[AutoARIMA()], \n                     freq='MS', n_jobs=-1)\nY_hat_df_arima = sf.forecast(df=Y_train_df, \n                             h=12, \n                             fitted=True, \n                             X_df=Y_test_df.drop(columns=\"y\"), \n                             level = np.arange(2, 100, 2))\nY_fitted_df_arima = sf.forecast_fitted_values()\n```\n\n----------------------------------------\n\nTITLE: Predicting with Fitted Neural Forecasting Model\nDESCRIPTION: This method generates predictions using the best performing model on validation data. It supports specifying step size and additional dataset parameters.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_auto.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ndef predict(self, dataset, step_size=1, **data_kwargs):\n    return self.model.predict(dataset=dataset, \n                              step_size=step_size, **data_kwargs)\n```\n\n----------------------------------------\n\nTITLE: Generating Forecasts with Optimized AutoNHITS Model in Python\nDESCRIPTION: This snippet uses the trained NeuralForecast model with optimized hyperparameters to generate forecasts for the next 12 months.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/04_hyperparameter_tuning.ipynb#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nY_hat_df_optuna = nf.predict()\nY_hat_df_optuna.head()\n```\n\n----------------------------------------\n\nTITLE: Implementing Masked Median Function\nDESCRIPTION: Function to compute median of tensor along specified dimension while handling masked values. Supports custom dimension selection and dimension preservation options.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.scalers.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef masked_median(x, mask, dim=-1, keepdim=True):\n    \"\"\" Masked Median\n\n    Compute the median of tensor `x` along dim, ignoring values where \n    `mask` is False. `x` and `mask` need to be broadcastable.\n\n    **Parameters:**<br>\n    `x`: torch.Tensor to compute median of along `dim` dimension.<br>\n    `mask`: torch Tensor bool with same shape as `x`, where `x` is valid and False\n            where `x` should be masked. Mask should not be all False in any column of\n            dimension dim to avoid NaNs from zero division.<br>\n    `dim` (int, optional): Dimension to take median of. Defaults to -1.<br>\n    `keepdim` (bool, optional): Keep dimension of `x` or not. Defaults to True.<br>\n\n    **Returns:**<br>\n    `x_median`: torch.Tensor with normalized values.\n    \"\"\"\n    x_nan = x.masked_fill(mask<1, float(\"nan\"))\n    x_median, _ = x_nan.nanmedian(dim=dim, keepdim=keepdim)\n    x_median = torch.nan_to_num(x_median, nan=0.0)\n    return x_median\n```\n\n----------------------------------------\n\nTITLE: Testing Ray Configuration for AutoDLinear\nDESCRIPTION: A unit test that demonstrates configuring and fitting an AutoDLinear model with Ray backend using a customized configuration with reduced training steps.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_48\n\nLANGUAGE: python\nCODE:\n```\nmy_config = AutoDLinear.get_default_config(h=12, backend='ray')\nmy_config['max_steps'] = 2\nmy_config['val_check_steps'] = 1\nmy_config['input_size'] = 12\nmodel = AutoDLinear(h=12, config=my_config, backend='ray', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: Plotting Time Series Forecasts in Python\nDESCRIPTION: Visualizes the AirPassengers data and the forecasts generated by the pre-trained NHITS model using the plot_series utility function.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/17_transfer_learning.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nplot_series(Y_train_df, Y_hat_df)\n```\n\n----------------------------------------\n\nTITLE: Spline-Based Quantile Function Calculation in PyTorch\nDESCRIPTION: Method for calculating quantiles using spline interpolation between quantile knots. It handles tensor reshaping based on the dimension parameter and performs linear interpolation within spline pieces.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_25\n\nLANGUAGE: Python\nCODE:\n```\ndef quantile_spline(\n    self,\n    alpha: torch.Tensor,\n    dim: Optional[int] = None,\n) -> torch.Tensor:\n    # Refer to the description in quantile_internal\n\n    qk_y = self.qk_y\n    sk_x, delta_sk_x, delta_sk_y = (\n        self.sk_x,\n        self.delta_sk_x,\n        self.delta_sk_y,\n    )\n\n    if dim is not None:\n        qk_y = qk_y.unsqueeze(dim=0 if dim == 0 else -1)\n        sk_x = sk_x.unsqueeze(dim=dim)\n        delta_sk_x = delta_sk_x.unsqueeze(dim=dim)\n        delta_sk_y = delta_sk_y.unsqueeze(dim=dim)\n\n    if dim is None or dim == 0:\n        alpha = alpha.unsqueeze(dim=-1)\n\n    alpha = alpha.unsqueeze(dim=-1)\n\n    spline_val = (alpha - sk_x) / delta_sk_x\n    spline_val = torch.maximum(\n        torch.minimum(spline_val, torch.ones_like(spline_val)),\n        torch.zeros_like(spline_val),\n    )\n\n    return qk_y + torch.sum(spline_val * delta_sk_y, dim=-1)\n```\n\n----------------------------------------\n\nTITLE: Testing TimeSeriesDataModule Functionality in Python\nDESCRIPTION: This snippet tests the TimeSeriesDataModule by creating an instance and iterating through its train_dataloader to verify the shape and contents of the batches.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/tsdataset.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n\nbatch_size = 128\ndata = TimeSeriesDataModule(dataset=dataset, \n                            batch_size=batch_size, drop_last=True)\nfor batch in data.train_dataloader():\n    test_eq(batch['temporal'].shape, (batch_size, 2, 500))\n    test_eq(batch['temporal_cols'], ['y', 'available_mask'])\n```\n\n----------------------------------------\n\nTITLE: Calculating MAE Loss with Incomplete Mask and Complete Horizon Weight in PyTorch\nDESCRIPTION: This snippet shows the calculation of MAE loss using an incomplete mask and complete horizon weight. It sets up a mask where one error point is masked, initializes the horizon weight, computes the loss, and verifies the result.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_78\n\nLANGUAGE: Python\nCODE:\n```\n# Incomplete mask and complete horizon_weight\nmask = torch.Tensor([[1,1,1],[0,1,1]]).unsqueeze(-1) # Only 1 error and points is masked.\nhorizon_weight = torch.Tensor([1,1,1])\nmae = MAE(horizon_weight=horizon_weight)\nloss = mae(y=y, y_hat=y_hat, mask=mask)\nassert loss==(2/5), 'Should be 2/5'\n```\n\n----------------------------------------\n\nTITLE: Displaying Autoformer.predict Method Documentation\nDESCRIPTION: This code snippet shows the documentation for the predict method of the Autoformer class using the show_doc function. It explains how to use the trained model to make predictions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.autoformer.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(Autoformer.predict, name='Autoformer.predict')\n```\n\n----------------------------------------\n\nTITLE: Visualizing Forecast Results for Specific Time Windows\nDESCRIPTION: Creates visualizations comparing the true values and forecasts for a specific series across multiple time windows to evaluate forecast accuracy visually.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/04_longhorizon_nhits.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfig, axs = plt.subplots(nrows=3, ncols=1, figsize=(10, 11))\nfig.tight_layout()\n\nseries = ['HUFL','HULL','LUFL','LULL','MUFL','MULL','OT']\nseries_idx = 3\n\nfor idx, w_idx in enumerate([200, 300, 400]):\n  axs[idx].plot(y_true[series_idx, w_idx,:],label='True')\n  axs[idx].plot(y_hat[series_idx, w_idx,:],label='Forecast')\n  axs[idx].grid()\n  axs[idx].set_ylabel(series[series_idx]+f' window {w_idx}', \n                      fontsize=17)\n  if idx==2:\n    axs[idx].set_xlabel('Forecast Horizon', fontsize=17)\nplt.legend()\nplt.show()\nplt.close()\n```\n\n----------------------------------------\n\nTITLE: Implementing Standard Scaler for Neural Forecasting in Python\nDESCRIPTION: Calculates standard scaling statistics for features by removing the mean and scaling to unit variance. It handles masked values and includes safeguards against division by zero.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.scalers.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef std_statistics(x, mask, dim=-1, eps=1e-6):\n    x_means = masked_mean(x=x, mask=mask, dim=dim)\n    x_stds = torch.sqrt(masked_mean(x=(x-x_means)**2, mask=mask, dim=dim))\n\n    x_stds[x_stds==0] = 1.0\n    x_stds = x_stds + eps\n    return x_means, x_stds\n```\n\n----------------------------------------\n\nTITLE: Comprehensive testing of core model interactions in NeuralForecast\nDESCRIPTION: Tests a wide variety of time series models available in NeuralForecast, including AutoDilatedRNN, DeepAR, NHITS, MLP, Transformer-based models, and multivariate models. This tests the core model interaction functionality of the framework.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_38\n\nLANGUAGE: python\nCODE:\n```\n# Unit test for core/model interactions\nconfig = {'input_size': tune.choice([12, 24]), \n          'hidden_size': 256,\n          'max_steps': 1,\n          'val_check_steps': 1,\n          'step_size': 12}\n\nconfig_drnn = {'input_size': tune.choice([-1]), \n               'encoder_hidden_size': tune.choice([5, 10]),\n               'max_steps': 1,\n               'val_check_steps': 1,\n               'step_size': 1}\n\nfcst = NeuralForecast(\n    models=[\n        AutoDilatedRNN(h=12, config=config_drnn, cpus=1, num_samples=2),\n        DeepAR(h=12, input_size=24, max_steps=1,\n               stat_exog_list=['airline1'], futr_exog_list=['trend']),\n        DilatedRNN(h=12, input_size=-1, encoder_hidden_size=10, max_steps=1,\n                   stat_exog_list=['airline1'],\n                   futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n        RNN(h=12, input_size=-1, encoder_hidden_size=10, max_steps=1,\n            inference_input_size=24,\n            stat_exog_list=['airline1'],\n            futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n        TCN(h=12, input_size=-1, encoder_hidden_size=10, max_steps=1,\n            stat_exog_list=['airline1'],\n            futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n        AutoMLP(h=12, config=config, cpus=1, num_samples=2),\n        NBEATSx(h=12, input_size=12, max_steps=1,\n                stat_exog_list=['airline1'],\n                futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n        NHITS(h=12, input_size=24, loss=MQLoss(level=[80]), max_steps=1),\n        NHITS(h=12, input_size=12, max_steps=1,\n              stat_exog_list=['airline1'],\n              futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n        DLinear(h=12, input_size=24, max_steps=1),\n        MLP(h=12, input_size=12, max_steps=1,\n            stat_exog_list=['airline1'],\n            futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n        TFT(h=12, input_size=24, max_steps=1),\n        VanillaTransformer(h=12, input_size=24, max_steps=1),\n        Informer(h=12, input_size=24, max_steps=1),\n        Autoformer(h=12, input_size=24, max_steps=1),\n        FEDformer(h=12, input_size=24, max_steps=1),\n        PatchTST(h=12, input_size=24, max_steps=1),\n        TimesNet(h=12, input_size=24, max_steps=1),\n        StemGNN(h=12, input_size=24, n_series=2, max_steps=1, scaler_type='robust'),\n        TSMixer(h=12, input_size=24, n_series=2, max_steps=1, scaler_type='robust'),\n        TSMixerx(h=12, input_size=24, n_series=2, max_steps=1, scaler_type='robust'),\n    ],\n    freq='M'\n)\nfcst.fit(df=AirPassengersPanel_train, static_df=AirPassengersStatic)\nforecasts = fcst.predict(futr_df=AirPassengersPanel_test)\nforecasts\n```\n\n----------------------------------------\n\nTITLE: Importing NeuralForecast Models\nDESCRIPTION: Imports required NeuralForecast models and logging configuration for model training.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/03_exogenous_variables.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nfrom neuralforecast.auto import NHITS, BiTCN\nfrom neuralforecast.core import NeuralForecast\n```\n\n----------------------------------------\n\nTITLE: Implementing a Safer Index Lookup Utility in Python\nDESCRIPTION: Defines a utility function that gets indices for values in a pandas Index object, raising an error if any values are missing. This function helps ensure data integrity when working with time series indices by preventing silent failures.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/utils.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n#| export\ndef get_indexer_raise_missing(idx: pd.Index, vals: List[str]) -> List[int]:\n    idxs = idx.get_indexer(vals)\n    missing = [v for i, v in zip(idxs, vals) if i == -1]\n    if missing:\n        raise ValueError(f'The following values are missing from the index: {missing}')\n    return idxs\n```\n\n----------------------------------------\n\nTITLE: Testing Unused Variables in Prediction with NHITS and LSTM Models\nDESCRIPTION: Verifies that including unused variables in the prediction dataset doesn't affect forecast results. Tests both NHITS and LSTM models with extraneous variables to ensure prediction consistency.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_53\n\nLANGUAGE: python\nCODE:\n```\n# Test passing unused variables in dataframe does not affect forecasts  \n\nmodels = [NHITS(h=12, input_size=24, max_steps=5, hist_exog_list=['zeros'], scaler_type='robust')]\nnf = NeuralForecast(models=models, freq='M')\nnf.fit(AirPassengersPanel_train)\n\nY_hat1 = nf.predict(df=AirPassengersPanel_train[['unique_id','ds','y','zeros','large_number']])\nY_hat2 = nf.predict(df=AirPassengersPanel_train[['unique_id','ds','y','zeros']])\n\npd.testing.assert_frame_equal(\n    Y_hat1,\n    Y_hat2,\n    check_dtype=False,\n)\n\nmodels = [LSTM(h=12, input_size=24, max_steps=5, hist_exog_list=['zeros'], scaler_type='robust')]\nnf = NeuralForecast(models=models, freq='M')\nnf.fit(AirPassengersPanel_train)\n\nY_hat1 = nf.predict(df=AirPassengersPanel_train[['unique_id','ds','y','zeros','large_number']])\nY_hat2 = nf.predict(df=AirPassengersPanel_train[['unique_id','ds','y','zeros']])\n\npd.testing.assert_frame_equal(\n    Y_hat1,\n    Y_hat2,\n    check_dtype=False,\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Matplotlib for Visualization\nDESCRIPTION: Imports the matplotlib.pyplot module for creating visualizations of the forecast and peak detection results.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/use-cases/electricity_peak_forecasting.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n```\n\n----------------------------------------\n\nTITLE: Documenting the attention_weights Method for TFT Model - Python\nDESCRIPTION: Documents the attention_weights method of the TFT class with show_doc, presenting the method in generated documentation with a specified name and title heading. Aids transparency and ease of use for developers wishing to introspect attention behaviors in model outputs.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(TFT.attention_weights, name='TFT.attention_weights', title_level=3)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using AutoVanillaTransformer Model for Time Series Forecasting in Python\nDESCRIPTION: This snippet demonstrates the basic usage of the AutoVanillaTransformer model for time series forecasting. It shows how to configure, initialize, fit, and predict with the model, including usage with the Optuna backend.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_69\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n# Use your own config or AutoVanillaTransformer.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=8)\nmodel = AutoVanillaTransformer(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n# Optuna\nmodel = AutoVanillaTransformer(h=12, config=None, backend='optuna')\n```\n\n----------------------------------------\n\nTITLE: Configuring Ray Backend for AutoiTransformer\nDESCRIPTION: Code snippet showing how to configure and fit an AutoiTransformer model with Ray backend. The configuration includes setting maximum steps, validation check frequency, input and hidden sizes, and running the model with minimal compute resources.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_88\n\nLANGUAGE: python\nCODE:\n```\nmy_config = AutoiTransformer.get_default_config(h=12, n_series=1, backend='ray')\nmy_config['max_steps'] = 1\nmy_config['val_check_steps'] = 1\nmy_config['input_size'] = 12\nmy_config['hidden_size'] = 16\nmodel = AutoiTransformer(h=12, n_series=1, config=my_config, backend='ray', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: Testing LR Scheduler kwargs Warning\nDESCRIPTION: Verifies warning generation when lr_scheduler_kwargs is provided without lr_scheduler parameter. Tests multiple model types.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_67\n\nLANGUAGE: python\nCODE:\n```\nfor nf_model in [NHITS, RNN, StemGNN]:\n    params = {\n        \"h\": 12, \n        \"input_size\": 24, \n        \"max_steps\": 1,\n        \"lr_scheduler_kwargs\": {\"optimizer\": torch.optim.Adadelta, \"factor\": 0.22}\n    }\n    if nf_model.__name__ == \"StemGNN\":\n        params.update({\"n_series\": 2})\n    models = [nf_model(**params)]\n    nf = NeuralForecast(models=models, freq='M')\n    with warnings.catch_warnings(record=True) as issued_warnings:\n        warnings.simplefilter('always', UserWarning)\n        nf.fit(AirPassengersPanel_train)\n        assert any(\"ignoring lr_scheduler_kwargs as the lr_scheduler is not specified\" in str(w.message) for w in issued_warnings)\n```\n\n----------------------------------------\n\nTITLE: Unit Testing AutoDeepAR Class Functionality in Python\nDESCRIPTION: This snippet contains unit tests for the `AutoDeepAR` class. It first asserts that the Optuna configuration correctly incorporates the forecast horizon `h`. It then uses the `test_args` helper function to ensure `AutoDeepAR` accepts all necessary arguments from its `BaseAuto` parent class. Finally, it includes a unit test scenario specifically for the Optuna backend, where it fetches the default Optuna configuration, modifies it with specific values (`max_steps`, `input_size`, etc.), and then instantiates and fits the `AutoDeepAR` model to verify correct operation with the modified configuration.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n# Check Optuna\nassert model.config(MockTrial())['h'] == 12\n\n# Unit test to test that Auto* model contains all required arguments from BaseAuto\ntest_args(AutoDeepAR, exclude_args=['cls_model']) \n\n# Unit test for situation: Optuna with updated default config\nmy_config = AutoDeepAR.get_default_config(h=12, backend='optuna')\ndef my_config_new(trial):\n    config = {**my_config(trial)}\n    config.update({'max_steps': 1, 'val_check_steps': 1, 'input_size': 12, 'lstm_hidden_size': 8})\n    return config\n\nmodel = AutoDeepAR(h=12, config=my_config_new, backend='optuna', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: Implementing AutoHINT Class for HINT Model Tuning in Python\nDESCRIPTION: Defines the AutoHINT class for automated hyperparameter tuning of HINT models. It includes initialization, model fitting, and configuration validation. The class only supports the Ray backend for optimization.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_100\n\nLANGUAGE: python\nCODE:\n```\nclass AutoHINT(BaseAuto):\n\n    def __init__(self,\n                 cls_model,\n                 h,\n                 loss,\n                 valid_loss,\n                 S,\n                 config,\n                 search_alg=BasicVariantGenerator(random_state=1),\n                 num_samples=10,\n                 cpus=cpu_count(),\n                 gpus=torch.cuda.device_count(),\n                 refit_with_val=False,\n                 verbose=False,\n                 alias=None,\n                 backend='ray',\n                 callbacks=None,\n                 ):\n        \n        super(AutoHINT, self).__init__(\n              cls_model=cls_model, \n              h=h,\n              loss=loss,\n              valid_loss=valid_loss,\n              config=config,\n              search_alg=search_alg,\n              num_samples=num_samples, \n              refit_with_val=refit_with_val,\n              cpus=cpus,\n              gpus=gpus,\n              verbose=verbose,\n              alias=alias,\n              backend=backend,\n              callbacks=callbacks,\n        )\n        if backend == 'optuna':\n            raise Exception(\"Optuna is not supported for AutoHINT.\")\n\n        # Validate presence of reconciliation strategy\n        # parameter in configuration space\n        if not ('reconciliation' in config.keys()):\n            raise Exception(\"config needs reconciliation, \\\n                            try tune.choice(['BottomUp', 'MinTraceOLS', 'MinTraceWLS'])\")\n        self.S = S\n\n    def _fit_model(self, cls_model, config,\n                   dataset, val_size, test_size, distributed_config=None):\n        # Overwrite _fit_model for HINT two-stage instantiation\n        reconciliation = config.pop('reconciliation')\n        base_model = cls_model(**config)\n        model = HINT(h=base_model.h, model=base_model, \n                     S=self.S, reconciliation=reconciliation)\n        model.test_size = test_size\n        model = model.fit(\n            dataset,\n            val_size=val_size, \n            test_size=test_size,\n            distributed_config=distributed_config,\n        )\n        return model\n    \n    @classmethod\n    def get_default_config(cls, h, backend, n_series=None):\n        raise Exception(\"AutoHINT has no default configuration.\")\n```\n\n----------------------------------------\n\nTITLE: Generating Forecasts with Pre-trained NHITS Model in Python\nDESCRIPTION: Uses the pre-trained NHITS model to generate forecasts for the AirPassengers dataset without retraining, demonstrating transfer learning capabilities.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/17_transfer_learning.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nY_hat_df = fcst2.predict(df=Y_train_df)\nY_hat_df.head()\n```\n\n----------------------------------------\n\nTITLE: Initializing Forecasting Models with Parameters\nDESCRIPTION: Creates instances of TSMixer, TSMixerx, MLPMultivariate, and NHITS models with specified hyperparameters for multivariate forecasting, including horizon, input size, and training configurations.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/08_multivariate_tsmixer.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nhorizon = 96\ninput_size = 512\nmodels = [\n          TSMixer(h=horizon,\n                input_size=input_size,\n                n_series=7,\n                max_steps=1000,\n                val_check_steps=100,\n                early_stop_patience_steps=5,\n                scaler_type='identity',\n                valid_loss=MAE(),\n                random_seed=12345678,\n                ),  \n          TSMixerx(h=horizon,\n                input_size=input_size,\n                n_series=7,\n                max_steps=1000,\n                val_check_steps=100,\n                early_stop_patience_steps=5,\n                scaler_type='identity',\n                dropout=0.7,\n                valid_loss=MAE(),\n                random_seed=12345678,\n                futr_exog_list=['ex_1', 'ex_2', 'ex_3', 'ex_4'],\n                ),\n          MLPMultivariate(h=horizon,\n                input_size=input_size,\n                n_series=7,\n                max_steps=1000,\n                val_check_steps=100,\n                early_stop_patience_steps=5,\n                scaler_type='standard',\n                hidden_size=256,\n                valid_loss=MAE(),\n                random_seed=12345678,\n                ),                                             \n           NHITS(h=horizon,\n                input_size=horizon,\n                max_steps=1000,\n                val_check_steps=100,\n                early_stop_patience_steps=5,\n                scaler_type='robust',\n                valid_loss=MAE(),\n                random_seed=12345678,\n                ),                                                                       \n         ]\n```\n\n----------------------------------------\n\nTITLE: Converting Levels and Quantiles in Python\nDESCRIPTION: Provides two utility functions: `level_to_quantiles` converts a list of confidence levels (e.g., [80, 95]) into a sorted list of corresponding quantiles (e.g., [0.025, 0.1, 0.9, 0.975]), and `quantiles_to_level` performs the reverse conversion. These are used to translate between user-friendly confidence levels and the quantiles needed for calculation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/utils.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n#| export\ndef level_to_quantiles(level: List[Union[int, float]]) -> List[float]:\n    \"\"\"\n    Converts a list of levels to a list of quantiles.\n    \"\"\"\n    level_set = set(level)\n    return sorted(list(set(sum([[(50 - l / 2) / 100, (50 + l / 2) / 100] for l in level_set], []))))\n\ndef quantiles_to_level(quantiles: List[float]) -> List[Union[int, float]]:\n    \"\"\"\n    Converts a list of quantiles to a list of levels.\n    \"\"\"\n    quantiles_set = set(quantiles)\n    return sorted(set([int(round(100 - 200 * (q * (q < 0.5) + (1 - q) * (q >= 0.5)), 2)) for q in quantiles_set]))\n```\n\n----------------------------------------\n\nTITLE: Generating Insample Predictions\nDESCRIPTION: Uses predict_insample method to generate forecasts for train and validation sets with specified step size.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/05_predictInsample.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nY_hat_insample = nf.predict_insample(step_size=horizon)\n```\n\n----------------------------------------\n\nTITLE: Loading M4 Monthly Time Series Data in Python\nDESCRIPTION: Loads the Monthly subset of the M4 competition dataset using the M4 class from datasetsforecast library. Converts date strings to datetime objects for proper time series handling.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/17_transfer_learning.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nY_df, _, _ = M4.load(directory='./', group='Monthly', cache=True)\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\nY_df\n```\n\n----------------------------------------\n\nTITLE: Testing TSMixer Model with AirPassengers Dataset in Python\nDESCRIPTION: Test code that checks the TSMixer model's functionality by running it on the airpassengers dataset while suppressing logging and warning messages for cleaner output.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tsmixer.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Unit tests for models\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\nlogging.getLogger(\"lightning_fabric\").setLevel(logging.ERROR)\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    check_model(TSMixer, [\"airpassengers\"])\n```\n\n----------------------------------------\n\nTITLE: GMM Unit Tests\nDESCRIPTION: Unit tests to verify the correct initialization of quantiles attribute in the GMM class with different configurations.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_56\n\nLANGUAGE: python\nCODE:\n```\ncheck = GMM(n_components=2, level=[80, 90])\ntest_eq(len(check.quantiles), 5)\n\ncheck = GMM(n_components=2, \n            quantiles=[0.0100, 0.1000, 0.5, 0.9000, 0.9900])\nprint(check.output_names)\nprint(check.quantiles)\ntest_eq(len(check.quantiles), 5)\n\ncheck = GMM(n_components=2,\n            quantiles=[0.0100, 0.1000, 0.9000, 0.9900])\ntest_eq(len(check.quantiles), 4)\n```\n\n----------------------------------------\n\nTITLE: Initializing AutoNBEATS Model\nDESCRIPTION: Sets up the AutoNBEATS model with hyperparameter optimization using HyperOptSearch and specified configuration.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/experiments/nbeats_basis/nbeats_basis_experiment.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.tune.search.hyperopt import HyperOptSearch\nfrom neuralforecast.losses.pytorch import *\n\nmodel = AutoNBEATS(\n    h=96,\n    loss=MAE(),\n    config=nbeats_config,\n    search_alg=HyperOptSearch(),\n    backend='ray',\n    num_samples=20\n)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Forecast and Peak Detection Results\nDESCRIPTION: Creates a plot showing the actual electricity demand, forecasted demand, true peak, and predicted peaks for the day with maximum load in September.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/use-cases/electricity_peak_forecasting.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nplt.figure(figsize=(10, 5))\nplt.axvline(cv_df_day.iloc[max_hour]['ds'], color='black', label='True Peak')\nplt.scatter(cv_df_day.iloc[peaks]['ds'], cv_df_day.iloc[peaks]['AutoNHITS'], color='green', label=f'Predicted Top-{npeaks}')\nplt.plot(cv_df_day['ds'], cv_df_day['y'], label='y', color='blue')\nplt.plot(cv_df_day['ds'], cv_df_day['AutoNHITS'], label='Forecast', color='red')\nplt.xlabel('Time')\nplt.ylabel('Load (MW)')\nplt.grid()\nplt.legend()\n```\n\n----------------------------------------\n\nTITLE: Testing AutoNBEATS with Short Time Series and Hyperparameter Tuning\nDESCRIPTION: Tests the AutoNBEATS model on a shortened version of the AirPassengers dataset. Uses Ray Tune for hyperparameter optimization with a configuration that explores different input sizes.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_48\n\nLANGUAGE: python\nCODE:\n```\n# test short time series\nconfig = {'input_size': tune.choice([12, 24]), \n          'max_steps': 1,\n          'val_check_steps': 1}\n\nfcst = NeuralForecast(\n    models=[\n        AutoNBEATS(h=12, config=config, cpus=1, num_samples=2)],\n    freq='M'\n)\n\nAirPassengersShort = AirPassengersPanel.tail(36+144).reset_index(drop=True)\nforecasts = fcst.cross_validation(AirPassengersShort, val_size=48, n_windows=1)\n```\n\n----------------------------------------\n\nTITLE: Implementing Loss, Log Probability, and CDF Functions in PyTorch\nDESCRIPTION: These methods implement the loss function (CRPS), log probability, and cumulative distribution function (CDF) for a distribution. The CDF computes the quantile level corresponding to a given value.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ndef loss(self, z: torch.Tensor) -> torch.Tensor:\n    return self.crps(z)\n\ndef log_prob(self, z: torch.Tensor) -> torch.Tensor:\n    return -self.crps(z)\n\ndef crps(self, z: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Compute CRPS in analytical form\n    \n    **Parameters**\n\n    `z`: Observation to evaluate.\n\n    \"\"\"\n\n    crps_lt = self.crps_tail(z, left_tail=True)\n    crps_rt = self.crps_tail(z, left_tail=False)\n\n    return crps_lt + crps_rt + self.crps_spline(z)\n\ndef cdf(self, z: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Computes the quantile level alpha_tilde such that\n    q(alpha_tilde) = z\n    \n    **Parameters**\n\n    `z`: Tensor of shape = (*batch_shape,)\n\n    \"\"\"\n\n    qk_y, qk_y_l, qk_y_plus = self.qk_y, self.qk_y_l, self.qk_y_plus\n\n    alpha_tilde = torch.where(\n        z < qk_y_l,\n        self.cdf_tail(z, left_tail=True),\n        self.cdf_tail(z, left_tail=False),\n    )\n\n    spline_alpha_tilde = self.cdf_spline(z)\n\n    for spline_idx in range(self.num_qk - 1):\n        is_in_between = torch.logical_and(\n            qk_y[..., spline_idx] <= z, z < qk_y_plus[..., spline_idx]\n        )\n\n        alpha_tilde = torch.where(\n            is_in_between, spline_alpha_tilde[..., spline_idx], alpha_tilde\n        )\n\n    return alpha_tilde\n```\n\n----------------------------------------\n\nTITLE: Loading Time Series Data\nDESCRIPTION: Loads the main dataset containing electricity price data and exogenous variables from an S3 bucket.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/03_exogenous_variables.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv(\n    'https://datasets-nixtla.s3.amazonaws.com/EPF_FR_BE.csv',\n    parse_dates=['ds'],\n)\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Testing AutoGRU with Modified Ray Config in Python\nDESCRIPTION: This snippet demonstrates a unit test scenario for the `AutoGRU` model using the Ray backend. It retrieves the default Ray configuration, modifies specific hyperparameters like `max_steps`, `val_check_steps`, `input_size`, and `encoder_hidden_size`, and then instantiates and fits the `AutoGRU` model with these customized settings.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# Unit test for situation: Ray with updated default config\nmy_config = AutoGRU.get_default_config(h=12, backend='ray')\nmy_config['max_steps'] = 1\nmy_config['val_check_steps'] = 1\nmy_config['input_size'] = -1\nmy_config['encoder_hidden_size'] = 8\nmodel = AutoGRU(h=12, config=my_config, backend='ray', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: Importing GMM and sCRPS Loss Functions in Python\nDESCRIPTION: This snippet imports the GMM (Gaussian Mixture Model) and sCRPS (Scaled Continuous Ranked Probability Score) loss functions from the neuralforecast.losses.pytorch module.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_116\n\nLANGUAGE: python\nCODE:\n```\n#| hide\nfrom neuralforecast.losses.pytorch import GMM, sCRPS\n```\n\n----------------------------------------\n\nTITLE: Initializing Hidden State for DilatedRNN in PyTorch\nDESCRIPTION: This snippet initializes the hidden state for the dilated RNN. It creates a zero tensor for the hidden state and handles different cell types like LSTM, ResLSTM, and AttentiveLSTM.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.dilated_rnn.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nhidden = torch.zeros(batch_size * rate, hidden_size,\n                         dtype=dilated_inputs.dtype,\n                         device=dilated_inputs.device)\nhidden = hidden.unsqueeze(0)\n\nif self.cell_type in ['LSTM', 'ResLSTM', 'AttentiveLSTM']:\n    hidden = (hidden, hidden)\n    \ndilated_outputs, hidden = cell(dilated_inputs, hidden) # compatibility hack\n\nreturn dilated_outputs, hidden\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom LayerNorm for Seasonal Data\nDESCRIPTION: A specialized implementation of LayerNorm designed specifically for seasonal time series data, including bias removal.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.autoformer.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass LayerNorm(nn.Module):\n    def __init__(self, channels):\n        super(LayerNorm, self).__init__()\n        self.layernorm = nn.LayerNorm(channels)\n\n    def forward(self, x):\n        x_hat = self.layernorm(x)\n        bias = torch.mean(x_hat, dim=1).unsqueeze(1).repeat(1, x.shape[1], 1)\n        return x_hat - bias\n```\n\n----------------------------------------\n\nTITLE: Saving NeuralForecast Models\nDESCRIPTION: Demonstrates saving trained models to disk with configurable options for model selection and dataset preservation\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/06_save_load_models.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nnf.save(path='./checkpoints/test_run/',\n        model_index=None, \n        overwrite=True,\n        save_dataset=True)\n```\n\n----------------------------------------\n\nTITLE: Instantiating AutoNHITS Model with Ray Backend\nDESCRIPTION: Creating an AutoNHITS model with a 12-step forecast horizon, MAE loss function, custom hyperparameter configuration, HyperOptSearch algorithm, Ray backend, and 10 configuration samples for optimization.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/04_hyperparameter_tuning.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmodel = AutoNHITS(\n    h=12,\n    loss=MAE(),\n    config=nhits_config,\n    search_alg=HyperOptSearch(),\n    backend='ray',\n    num_samples=10,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing TimeFeature Classes for Calendar Feature Extraction in Python\nDESCRIPTION: Defines a base TimeFeature class and multiple derived classes that convert datetime indices into normalized values between -0.5 and 0.5. Each subclass handles a specific time unit (second, minute, hour, day, week, month, year) and provides normalization logic to represent cyclical time patterns.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/utils.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass TimeFeature:\n    def __init__(self):\n        pass\n\n    def __call__(self, index: pd.DatetimeIndex):\n        return print('Overwrite with corresponding feature')\n\n    def __repr__(self):\n        return self.__class__.__name__ + \"()\"\n\nclass SecondOfMinute(TimeFeature):\n    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.second / 59.0 - 0.5\n\nclass MinuteOfHour(TimeFeature):\n    \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.minute / 59.0 - 0.5\n\nclass HourOfDay(TimeFeature):\n    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.hour / 23.0 - 0.5\n\nclass DayOfWeek(TimeFeature):\n    \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return index.dayofweek / 6.0 - 0.5\n\nclass DayOfMonth(TimeFeature):\n    \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.day - 1) / 30.0 - 0.5\n\nclass DayOfYear(TimeFeature):\n    \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.dayofyear - 1) / 365.0 - 0.5\n\nclass MonthOfYear(TimeFeature):\n    \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.month - 1) / 11.0 - 0.5\n\nclass WeekOfYear(TimeFeature):\n    \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n    def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n        return (index.week - 1) / 52.0 - 0.5\n\ndef time_features_from_frequency_str(freq_str: str) -> List[TimeFeature]:\n    \"\"\"\n    Returns a list of time features that will be appropriate for the given frequency string.\n    Parameters\n    ----------\n    freq_str\n        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n    \"\"\"\n\n    if freq_str not in ['Q', 'M', 'MS', 'W', 'D', 'B', 'H', 'T', 'S']:\n        raise Exception('Frequency not supported')\n    \n    if freq_str in ['Q','M', 'MS']:\n        return [cls() for cls in [MonthOfYear]]\n    elif freq_str == 'W':\n        return [cls() for cls in [DayOfMonth, WeekOfYear]]\n    elif freq_str in ['D','B']:\n        return [cls() for cls in [DayOfWeek, DayOfMonth, DayOfYear]]\n    elif freq_str == 'H':\n        return [cls() for cls in [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear]]\n    elif freq_str == 'T':\n        return [cls() for cls in [MinuteOfHour, HourOfDay, DayOfWeek, DayOfMonth, DayOfYear]]\n    else:\n        return [cls() for cls in [SecondOfMinute, MinuteOfHour, HourOfDay, DayOfWeek, DayOfMonth, DayOfYear]]\n\ndef augment_calendar_df(df, freq='H'):\n    \"\"\"\n    > * Q - [month]\n    > * M - [month]\n    > * W - [Day of month, week of year]\n    > * D - [Day of week, day of month, day of year]\n    > * B - [Day of week, day of month, day of year]\n    > * H - [Hour of day, day of week, day of month, day of year]\n    > * T - [Minute of hour*, hour of day, day of week, day of month, day of year]\n    > * S - [Second of minute, minute of hour, hour of day, day of week, day of month, day of year]\n    *minute returns a number from 0-3 corresponding to the 15 minute period it falls into.\n    \"\"\"\n    df = df.copy()\n\n    freq_map = {\n        'Q':['month'],\n        'M':['month'],\n        'MS':['month'],\n        'W':['monthday', 'yearweek'],\n        'D':['weekday','monthday','yearday'],\n        'B':['weekday','monthday','yearday'],\n        'H':['dayhour','weekday','monthday','yearday'],\n        'T':['hourminute','dayhour','weekday','monthday','yearday'],\n        'S':['minutesecond','hourminute','dayhour','weekday','monthday','yearday']\n    }\n\n    ds_col = pd.to_datetime(df.ds.values)\n    ds_data = np.vstack([feat(ds_col) for feat in time_features_from_frequency_str(freq)]).transpose(1,0)\n    ds_data = pd.DataFrame(ds_data, columns=freq_map[freq])\n    \n    return pd.concat([df, ds_data], axis=1), freq_map[freq]\n```\n\n----------------------------------------\n\nTITLE: Testing NBEATSx with Different Inference Batch Sizes\nDESCRIPTION: Test to verify that the NBEATSx model produces consistent forecasts when using different inference_windows_batch_size values. The test fits the model with a specific batch size, then changes it and verifies that the predictions remain identical.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeatsx.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# test inference_windows_batch_size\ndataset, *_ = TimeSeriesDataset.from_df(Y_df)\nmodel = NBEATSx(h=12,\n                input_size=24,\n                scaler_type='robust',\n                stack_types = [\"identity\", \"trend\", \"seasonality\", \"exogenous\"],\n                n_blocks = [1,1,1,1],\n                futr_exog_list=['month','year'],\n                windows_batch_size=None,\n                inference_windows_batch_size=1,\n                max_steps=1)\nmodel.fit(dataset=dataset, test_size=12)\ny_hat_test = model.predict(dataset=dataset, step_size=1)\n#test we recover the same forecast with different inference_windows_batch_size\nmodel.inference_windows_batch_size=-1\ny_hat_test2 = model.predict(dataset=dataset, step_size=1)\ntest_eq(y_hat_test, y_hat_test2)\n```\n\n----------------------------------------\n\nTITLE: Converting Ray Config to Optuna Config\nDESCRIPTION: This static method converts a Ray configuration to an Optuna configuration. It handles various sampling methods including Integer, Categorical, Uniform, LogUniform, and Quantized.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_auto.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n@staticmethod\ndef _ray_config_to_optuna(ray_config):\n    def optuna_config(trial):\n        out = {}\n        for k, v in ray_config.items():\n            if hasattr(v, 'sampler'):\n                sampler = v.sampler\n                if isinstance(sampler, tune.search.sample.Integer.default_sampler_cls):\n                    v = trial.suggest_int(k, v.lower, v.upper)\n                elif isinstance(sampler, tune.search.sample.Categorical.default_sampler_cls):\n                    v = trial.suggest_categorical(k, v.categories)                    \n                elif isinstance(sampler, tune.search.sample.Uniform):\n                    v = trial.suggest_uniform(k, v.lower, v.upper)\n                elif isinstance(sampler, tune.search.sample.LogUniform):\n                    v = trial.suggest_loguniform(k, v.lower, v.upper)\n                elif isinstance(sampler, tune.search.sample.Quantized):\n                    if isinstance(sampler.get_sampler(), tune.search.sample.Float._LogUniform):\n                        v = trial.suggest_float(k, v.lower, v.upper, log=True)\n                    elif isinstance(sampler.get_sampler(), tune.search.sample.Float._Uniform):\n                        v = trial.suggest_float(k, v.lower, v.upper, step=sampler.q)\n                else:\n                    raise ValueError(f\"Couldn't translate {type(v)} to optuna.\")\n            out[k] = v\n        return out\n    return optuna_config\n```\n\n----------------------------------------\n\nTITLE: Loading Libraries for NeuralForecast and Data Handling in Python\nDESCRIPTION: This snippet imports necessary libraries for NeuralForecast, data handling with pandas, and evaluation metrics. It also sets the logging level for PyTorch Lightning.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/19_large_datasets.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport os\nimport tempfile\n\nimport pandas as pd\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import NHITS\nfrom utilsforecast.evaluation import evaluate\nfrom utilsforecast.losses import mae, rmse, smape\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\n```\n\nLANGUAGE: python\nCODE:\n```\nlogging.getLogger('pytorch_lightning').setLevel(logging.ERROR)\n```\n\n----------------------------------------\n\nTITLE: Initializing DeepAR Module Configuration\nDESCRIPTION: Module configuration declaration for the DeepAR implementation, setting the default export path for the model.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.deepar.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#| default_exp models.deepar\n```\n\n----------------------------------------\n\nTITLE: Installing NeuralForecast Package\nDESCRIPTION: Installs the neuralforecast package using pip in a Jupyter notebook cell with output capture.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/03_exogenous_variables.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n!pip install neuralforecast\n```\n\n----------------------------------------\n\nTITLE: Plotting Forecasts from Different Backends in Python\nDESCRIPTION: This snippet creates a plot comparing the forecasts produced by the AutoNHITS model using both Ray and Optuna backends.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/04_hyperparameter_tuning.ipynb#2025-04-23_snippet_21\n\nLANGUAGE: python\nCODE:\n```\nplot_series(\n    Y_df,\n    Y_hat_df.merge(\n        Y_hat_df_optuna,\n        on=['unique_id', 'ds'],\n        suffixes=['_ray', '_optuna'],\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: ISQF Domain Mapping in PyTorch\nDESCRIPTION: This function maps neural network outputs to appropriate distribution parameter constraints for the Incremental Spline Quantile Function (ISQF). It reshapes tensors and applies transformations to ensure parameters meet required constraints.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_33\n\nLANGUAGE: python\nCODE:\n```\ndef isqf_domain_map(input: torch.Tensor, tol: float=1e-4, quantiles: torch.Tensor = torch.tensor([0.1, 0.5, 0.9], dtype=torch.float32), num_pieces: int = 5):\n    \"\"\" ISQF Domain Map\n    Maps input into distribution constraints, by construction input's \n    last dimension is of matching `distr_args` length.\n\n    **Parameters:**<br>\n    `input`: tensor, of dimensions [B, H, N * n_outputs].<br>\n    `tol`: float, tolerance.<br>\n    `quantiles`: tensor, quantiles used for ISQF (i.e. x-positions for the knots). <br>\n    `num_pieces`: int, num_pieces used for each quantile spline. <br>\n\n    **Returns:**<br>\n    `(spline_knots, spline_heights, beta_l, beta_r, qk_y, qk_x)`: tuple with tensors of ISQF distribution arguments.<br>\n    \"\"\"\n\n    # Add tol to prevent the y-distance of\n    # two quantile knots from being too small\n    #\n    # Because in this case the spline knots could be squeezed together\n    # and cause overflow in spline CRPS computation\n    num_qk = len(quantiles)\n    n_outputs = 2 * (num_qk - 1) * num_pieces + 2 + num_qk\n    \n    # Reshape: [B, h, N * n_outputs] -> [B, h, N, n_outputs]\n    input = input.reshape(input.shape[0],\n                          input.shape[1],\n                          -1,\n                          n_outputs)\n    start_index = 0\n    spline_knots = input[..., start_index: start_index + (num_qk - 1) * num_pieces]\n    start_index += (num_qk - 1) * num_pieces\n    spline_heights = input[..., start_index: start_index + (num_qk - 1) * num_pieces]\n    start_index += (num_qk - 1) * num_pieces\n    beta_l = input[..., start_index: start_index + 1]\n    start_index += 1\n    beta_r = input[..., start_index: start_index + 1]\n    start_index += 1\n    quantile_knots = F.softplus(input[..., start_index: start_index + num_qk]) + tol\n\n    qk_y = torch.cumsum(quantile_knots, dim=-1)\n\n    # Prevent overflow when we compute 1/beta\n    beta_l = F.softplus(beta_l.squeeze(-1)) + tol\n    beta_r = F.softplus(beta_r.squeeze(-1)) + tol\n\n    # Reshape spline arguments\n    batch_shape = spline_knots.shape[:-1]\n\n    # repeat qk_x from (num_qk,) to (*batch_shape, num_qk)\n    qk_x_repeat = quantiles\\\n                       .repeat(*batch_shape, 1)\\\n                       .to(input.device)\n\n    # knots and heights have shape (*batch_shape, (num_qk-1)*num_pieces)\n    # reshape them to (*batch_shape, (num_qk-1), num_pieces)\n    spline_knots_reshape = spline_knots.reshape(\n        *batch_shape, (num_qk - 1), num_pieces\n    )\n    spline_heights_reshape = spline_heights.reshape(\n        *batch_shape, (num_qk - 1), num_pieces\n    )\n\n    return spline_knots_reshape, spline_heights_reshape, beta_l, beta_r, qk_y, qk_x_repeat\n```\n\n----------------------------------------\n\nTITLE: Generating Synthetic Time Series Data\nDESCRIPTION: Creates a synthetic time series dataset with trend changepoints, seasonality, and random noise components. The function generates hourly data with configurable parameters.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/experiments/nbeats_basis/nbeats_basis_experiment.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nnp.random.seed(42)\n\ndef generate_time_series(n_points=1000, base_value=100):\n    start_date = datetime(2024, 1, 1)\n    timestamps = [start_date + timedelta(hours=x) for x in range(n_points)]\n    \n    x = np.arange(n_points)\n    trend = np.zeros(n_points)\n    \n    changepoints = [200, 400, 700]\n    slopes = [0.05, -0.03, 0.03, 0.01]  # Different slopes for each segment\n    \n    current_pos = 0\n    for i, cp in enumerate(changepoints):\n        trend[current_pos:cp] = x[current_pos:cp] * slopes[i]\n        current_pos = cp\n    trend[current_pos:] = x[current_pos:] * slopes[-1]\n    \n    hours = np.arange(n_points) % 24\n    seasonality = 15 * np.sin(2 * np.pi * hours / 24) + \\\n                 5 * np.cos(4 * np.pi * hours / 24)  \n    \n    noise = np.random.normal(0, 2, n_points)\n    \n    values = base_value + trend + seasonality + noise\n    \n    df = pd.DataFrame({\n        'unique_id': 'series_001',\n        'ds': timestamps,\n        'y': values\n    })\n    \n    return df\n\ndf = generate_time_series()\n```\n\n----------------------------------------\n\nTITLE: Implementing MockTrial Class for Optuna Interface\nDESCRIPTION: Defines a MockTrial class that mimics Optuna's trial interface for hyperparameter suggestion methods. This enables testing and validation of hyperparameter configurations without running actual trials.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_auto.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#| exporti\nclass MockTrial:\n    def suggest_int(*args, **kwargs):\n        return 'int'\n    def suggest_categorical(self, name, choices):\n        return choices\n    def suggest_uniform(*args, **kwargs):\n        return 'uniform'\n    def suggest_loguniform(*args, **kwargs):\n        return 'loguniform'\n    def suggest_float(*args, **kwargs):\n        if 'log' in kwargs:\n            return 'quantized_log'\n        elif 'step' in kwargs:\n            return 'quantized_loguniform'\n        return 'float'\n```\n\n----------------------------------------\n\nTITLE: Instantiating NHITS model with HuberLoss and multiple hist_exog features\nDESCRIPTION: Creates an NHITS model with HuberLoss function and tests revin dynamic dimensionality with trend and lagged exogenous variables. The model is configured for forecasting 12 steps ahead with early stopping and validation checks.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.scalers.ipynb#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# Instantiate BaseWindow model and test revin dynamic dimensionality with hist_exog_list\nmodel = NHITS(h=12,\n              input_size=24,\n              loss=HuberLoss(),\n              hist_exog_list=['trend', 'y_[lag12]'],\n              max_steps=1,\n              early_stop_patience_steps=10,\n              val_check_steps=50,\n              scaler_type='revin',\n              learning_rate=1e-3)\nnf = NeuralForecast(models=[model], freq='MS')\nY_hat_df = nf.cross_validation(df=Y_df, val_size=12, n_windows=1)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Attention Weights for a Specific Future Time Step\nDESCRIPTION: Demonstrates how to plot attention weights for a specific future prediction time step. This example shows the attention pattern for the 8th time step in the forecast horizon.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n# | eval: false\nplot_attention(nf.models[0], plot=8)\n```\n\n----------------------------------------\n\nTITLE: Configuring and Training Temporal Classification Models with NeuralForecast\nDESCRIPTION: Defines and trains MLP and NHITS models for binary sequence prediction with a 12-step horizon. Uses Bernoulli distribution loss for binary classification and Accuracy for validation, along with custom hyperparameters for each model.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/16_temporal_classification.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# %%capture\nhorizon = 12\n\n# Try different hyperparmeters to improve accuracy.\nmodels = [MLP(h=horizon,                           # Forecast horizon\n              input_size=2 * horizon,              # Length of input sequence\n              loss=DistributionLoss('Bernoulli'),  # Binary classification loss\n              valid_loss=Accuracy(),               # Accuracy validation signal\n              max_steps=500,                       # Number of steps to train\n              scaler_type='standard',              # Type of scaler to normalize data\n              hidden_size=64,                      # Defines the size of the hidden state of the LSTM\n              #early_stop_patience_steps=2,         # Early stopping regularization patience\n              val_check_steps=10,                  # Frequency of validation signal (affects early stopping)\n              ),\n          NHITS(h=horizon,                          # Forecast horizon\n                input_size=2 * horizon,             # Length of input sequence\n                loss=DistributionLoss('Bernoulli'), # Binary classification loss\n                valid_loss=Accuracy(),              # Accuracy validation signal                \n                max_steps=500,                      # Number of steps to train\n                n_freq_downsample=[2, 1, 1],        # Downsampling factors for each stack output\n                #early_stop_patience_steps=2,        # Early stopping regularization patience\n                val_check_steps=10,                 # Frequency of validation signal (affects early stopping)\n                interpolation_mode=\"nearest\",\n                )             \n          ]\nnf = NeuralForecast(models=models, freq=1)\nY_hat_df = nf.cross_validation(df=Y_df, n_windows=1)\n```\n\n----------------------------------------\n\nTITLE: Calculating MAE Loss with Complete Mask and Horizon Weight in PyTorch\nDESCRIPTION: This snippet demonstrates the calculation of MAE loss using complete mask and horizon weight. It initializes the mask and horizon weight tensors, creates an MAE object, calculates the loss, and asserts the expected result.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_77\n\nLANGUAGE: Python\nCODE:\n```\n# Complete mask and horizon_weight\nmask = torch.Tensor([[1,1,1],[1,1,1]]).unsqueeze(-1)\nhorizon_weight = torch.Tensor([1,1,1])\n\nmae = MAE(horizon_weight=horizon_weight)\nloss = mae(y=y, y_hat=y_hat, mask=mask)\nassert loss==(3/6), 'Should be 3/6'\n```\n\n----------------------------------------\n\nTITLE: Restoring NeuralForecast Attributes from Config Dictionary\nDESCRIPTION: This snippet restores attributes of a NeuralForecast object from a configuration dictionary. It sets attributes like uids, last_dates, ds, fitted flag, and scalers.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_24\n\nLANGUAGE: Python\nCODE:\n```\nneuralforecast.dataset = dataset\nrestore_attrs = [\n    'uids',\n    'last_dates',\n    'ds',\n]\nfor attr in restore_attrs:\n    setattr(neuralforecast, attr, config_dict[attr])\n\n# Fitted flag\nneuralforecast._fitted = config_dict['_fitted']\n\nneuralforecast.scalers_ = config_dict.get(\"scalers_\", default_scalars_)\n\nreturn neuralforecast\n```\n\n----------------------------------------\n\nTITLE: Testing TimeSeriesDataset Sorting Functionality in Python\nDESCRIPTION: This snippet tests the sort_df=True functionality of TimeSeriesDataset by generating a series, sorting it, and then shuffling it. It then asserts that the dataset created from the unsorted dataframe matches the sorted version.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/tsdataset.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Testing sort_df=True functionality\ntemporal_df = generate_series(n_series=1000, n_temporal_features=0, equal_ends=False)\nsorted_temporal_df = temporal_df.sort_values(['unique_id', 'ds'])\nunsorted_temporal_df = sorted_temporal_df.sample(frac=1.0)\ndataset, indices, dates, ds = TimeSeriesDataset.from_df(df=unsorted_temporal_df)\n\nnp.testing.assert_allclose(dataset.temporal[:,:-1], \n                           sorted_temporal_df.drop(columns=['unique_id', 'ds']).values)\ntest_eq(indices, pd.Series(sorted_temporal_df['unique_id'].unique()))\ntest_eq(dates, temporal_df.groupby('unique_id', observed=True)['ds'].max().values)\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch Model Training Setup with Seed Initialization\nDESCRIPTION: Implements model training flow with support for both local and distributed training. The on_fit_start method ensures reproducibility by setting random seeds.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef on_fit_start(self):\n    torch.manual_seed(self.random_seed)\n    np.random.seed(self.random_seed)\n    random.seed(self.random_seed)\n```\n\n----------------------------------------\n\nTITLE: Plotting Time Series Forecasts with Scaled Data in Python\nDESCRIPTION: Visualizes the original time series data and the forecasts generated with time series scaling. The plot includes a limited window of historical data for clarity.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/07_time_series_scaling.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nplot_series(df, Y_hat_df, max_insample_length=24*5)\n```\n\n----------------------------------------\n\nTITLE: Saving Trained NeuralForecast Model for Transfer Learning in Python\nDESCRIPTION: Saves the trained NeuralForecast model to disk for later use in transfer learning. The save_dataset parameter is set to False to only save the model parameters.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/17_transfer_learning.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nnf.save(path='./results/transfer/', model_index=None, overwrite=True, save_dataset=False)\n```\n\n----------------------------------------\n\nTITLE: Loading Saved NeuralForecast Models\nDESCRIPTION: Shows how to load previously saved models from disk and generate new forecasts\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/06_save_load_models.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nnf2 = NeuralForecast.load(path='./checkpoints/test_run/')\nY_hat_df2 = nf2.predict()\nY_hat_df2.head()\n```\n\n----------------------------------------\n\nTITLE: Configuring AutoNHITS Model for Electricity Demand Forecasting\nDESCRIPTION: Sets up the AutoNHITS model with a 24-hour forecasting horizon and default configuration for hyperparameter search.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/use-cases/electricity_peak_forecasting.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodels = [AutoNHITS(h=24,\n                    config=None, # Uses default config\n                    num_samples=10\n                   )\n         ]\n```\n\n----------------------------------------\n\nTITLE: Training TFT Model on ERCOT Data\nDESCRIPTION: Creates a NeuralForecast object with the AutoTFT model and hourly frequency, then fits it on the ERCOT data. The output is captured to hide training logs.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/07_forecasting_tft.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n%%capture\nnf = NeuralForecast(\n    models=models,\n    freq='h')\n\nnf.fit(df=Y_df)\n```\n\n----------------------------------------\n\nTITLE: Validating No Data Leakage in Forecasting with test_size - Python\nDESCRIPTION: Tests the NBEATS model for absence of test set leakage when test_size is specified, comparing predictions under different evaluation windows. Uses numpy's almost equal assertion and a test_eq for deterministic outputs. Depends on numpy, test_eq, TimeSeriesDataset, and NBEATS instantiated/trained.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeats.ipynb#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n#test no leakage with test_size\ndataset, *_ = TimeSeriesDataset.from_df(Y_df)\nmodel = NBEATS(input_size=24, h=12, \n               windows_batch_size=None, max_steps=1)\nmodel.fit(dataset=dataset, test_size=12)\ny_hat_test = model.predict(dataset=dataset, step_size=1)\nnp.testing.assert_almost_equal(y_hat, y_hat_test, decimal=4)\n#test we recover the same forecast\ny_hat_test2 = model.predict(dataset=dataset, step_size=1)\ntest_eq(y_hat_test, y_hat_test2)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Time Series Forecasting in Python\nDESCRIPTION: Imports necessary Python packages including logging, numpy, pandas, torch, and specific modules from datasetsforecast, neuralforecast, and utilsforecast for time series modeling and evaluation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/17_transfer_learning.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom datasetsforecast.m4 import M4\nfrom neuralforecast.core import NeuralForecast\nfrom neuralforecast.models import NHITS\nfrom neuralforecast.utils import AirPassengersDF\nfrom utilsforecast.losses import mae\nfrom utilsforecast.plotting import plot_series\n```\n\n----------------------------------------\n\nTITLE: Displaying Documentation for Model Class with show_doc - Python\nDESCRIPTION: Utilizes a documentation helper (such as fastcore's show_doc) to render or display the documentation for the NBEATS class. This snippet is for user-facing documentation auto-extraction at runtime. No parameters required except the model class. Assumes show_doc and NBEATS are imported.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeats.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(NBEATS)\n```\n\n----------------------------------------\n\nTITLE: Defining DistributedConfig Dataclass in Python\nDESCRIPTION: Creates a dataclass for distributed configuration settings, including partitions path, number of nodes, and devices.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@dataclass\nclass DistributedConfig:\n    partitions_path: str\n    num_nodes: int\n    devices: int\n```\n\n----------------------------------------\n\nTITLE: Defining AutoRMoK Class in Python\nDESCRIPTION: This snippet defines the AutoRMoK class, which inherits from BaseAuto. It includes the default configuration, initialization method, and a class method for getting the default configuration. The class supports both Ray and Optuna backends for hyperparameter tuning.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_113\n\nLANGUAGE: python\nCODE:\n```\nclass AutoRMoK(BaseAuto):\n\n    default_config = {\n        \"input_size_multiplier\": [1, 2, 3, 4, 5],\n        \"h\": None,\n        \"n_series\": None,\n        \"taylor_order\": tune.choice([3, 4, 5]),\n        \"jacobi_degree\": tune.choice([4, 5, 6]),\n        \"wavelet_function\": tune.choice(['mexican_hat', 'morlet', 'dog', 'meyer', 'shannon']),\n        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n        \"scaler_type\": tune.choice([None, 'robust', 'standard', 'identity']),\n        \"max_steps\": tune.choice([500, 1000, 2000]),\n        \"batch_size\": tune.choice([32, 64, 128, 256]),\n        \"loss\": None,\n        \"random_seed\": tune.randint(1, 20),\n    }\n\n    def __init__(self,\n                 h,\n                 n_series,\n                 loss=MAE(),\n                 valid_loss=None,\n                 config=None, \n                 search_alg=BasicVariantGenerator(random_state=1),\n                 num_samples=10,\n                 refit_with_val=False,\n                 cpus=cpu_count(),\n                 gpus=torch.cuda.device_count(),\n                 verbose=False,\n                 alias=None,\n                 backend='ray',\n                 callbacks=None):\n        \n        # Define search space, input/output sizes\n        if config is None:\n            config = self.get_default_config(h=h, backend=backend, n_series=n_series)                 \n\n        # Always use n_series from parameters, raise exception with Optuna because we can't enforce it\n        if backend == 'ray':\n            config['n_series'] = n_series\n        elif backend == 'optuna':\n            mock_trial = MockTrial()\n            if ('n_series' in config(mock_trial) and config(mock_trial)['n_series'] != n_series) or ('n_series' not in config(mock_trial)):\n                raise Exception(f\"config needs 'n_series': {n_series}\")           \n\n        super(AutoRMoK, self).__init__(\n              cls_model=RMoK, \n              h=h,\n              loss=loss,\n              valid_loss=valid_loss,\n              config=config,\n              search_alg=search_alg,\n              num_samples=num_samples, \n              refit_with_val=refit_with_val,\n              cpus=cpus,\n              gpus=gpus,\n              verbose=verbose,\n              alias=alias,\n              backend=backend,\n              callbacks=callbacks,            \n        )\n\n    @classmethod\n    def get_default_config(cls, h, backend, n_series):\n        config = cls.default_config.copy()        \n        config['input_size'] = tune.choice([h * x \\\n                        for x in config[\"input_size_multiplier\"]])\n\n        # Rolling windows with step_size=1 or step_size=h\n        # See `BaseWindows` and `BaseRNN`'s create_windows\n        config['step_size'] = tune.choice([1, h])\n        del config[\"input_size_multiplier\"]\n        if backend == 'optuna':\n            # Always use n_series from parameters\n            config['n_series'] = n_series\n            config = cls._ray_config_to_optuna(config)           \n\n        return config         \n```\n\n----------------------------------------\n\nTITLE: Testing Optimizer Warning in LR Scheduler\nDESCRIPTION: Validates warning generation when optimizer is passed in lr_scheduler_kwargs. Tests across different model types including NHITS, RNN, and StemGNN.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_66\n\nLANGUAGE: python\nCODE:\n```\nfor nf_model in [NHITS, RNN, StemGNN]:\n    params = {\n        \"h\": 12, \n        \"input_size\": 24, \n        \"max_steps\": 1, \n        \"lr_scheduler\": torch.optim.lr_scheduler.ConstantLR, \n        \"lr_scheduler_kwargs\": {\"optimizer\": torch.optim.Adadelta, \"factor\": 0.22}\n    }\n    if nf_model.__name__ == \"StemGNN\":\n        params.update({\"n_series\": 2})\n    models = [nf_model(**params)]\n    nf = NeuralForecast(models=models, freq='M')\n    with warnings.catch_warnings(record=True) as issued_warnings:\n        warnings.simplefilter('always', UserWarning)\n        nf.fit(AirPassengersPanel_train)\n        assert any(\"ignoring optimizer passed in lr_scheduler_kwargs, using the model's optimizer\" in str(w.message) for w in issued_warnings)\n```\n\n----------------------------------------\n\nTITLE: Implementing Robust Scaler and Inverse Functions in Python\nDESCRIPTION: Provides functions for applying robust scaling and its inverse transformation using pre-computed statistics.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.scalers.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndef robust_scaler(x, x_median, x_mad):\n    return (x - x_median) / x_mad\n\ndef inv_robust_scaler(z, x_median, x_mad):\n    return z * x_mad + x_median\n```\n\n----------------------------------------\n\nTITLE: Testing NaN handling in Polars DataFrame inputs\nDESCRIPTION: Validates error handling for NaN values in Polars DataFrames without explicit available_mask.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_59\n\nLANGUAGE: python\nCODE:\n```\npl_df = polars.DataFrame(\n    {\n        'unique_id': [1]*50,\n        'y': list(range(50)), \n        'temporal_0': list(range(100,150)),\n        'temporal_1': list(range(200,250)),\n        'ds': polars.date_range(start=date(2022, 1, 1), end=date(2022, 2, 19), interval=\"1d\", eager=True), \n    }\n)\n\npl_static_df = polars.DataFrame(\n    {\n        'unique_id': [1],\n        'static_0': [1.2], \n        'static_1': [10.9],\n    }\n)\n\nmodels = [NHITS(h=12, input_size=24, max_steps=20)]\nnf = NeuralForecast(models=models, freq='1d')\n\ntest_pl_df1 = pl_df.clone()\ntest_pl_df1[3, 'y'] = np.nan\ntest_pl_df1[4, 'y'] = None\ntest_fail(lambda: nf.fit(test_pl_df1), contains=\"Found missing values in ['y']\")\n\ntest_pl_df2 = pl_df.clone()\ntest_pl_df2[15, \"temporal_0\"] = np.nan\ntest_pl_df2[5, \"temporal_1\"] = np.nan\ntest_fail(lambda: nf.fit(test_pl_df2), contains=\"Found missing values in ['temporal_0', 'temporal_1']\")\n\ntest_pl_df3 = pl_static_df.clone()\ntest_pl_df3[0, \"static_1\"] = np.nan\ntest_fail(lambda: nf.fit(pl_df, static_df=test_pl_df3), contains=\"Found missing values in ['static_1']\")\n```\n\n----------------------------------------\n\nTITLE: Evaluating Model Performance with MAE\nDESCRIPTION: Calculates and prints the Mean Absolute Error (MAE) for each transformer model to compare their forecasting accuracy.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/05_longhorizon_transformers.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmae_informer = mae(Y_hat_df['y'], Y_hat_df['Informer'])\nmae_autoformer = mae(Y_hat_df['y'], Y_hat_df['Autoformer'])\nmae_patchtst = mae(Y_hat_df['y'], Y_hat_df['PatchTST'])\n\nprint(f'Informer: {mae_informer:.3f}')\nprint(f'Autoformer: {mae_autoformer:.3f}')\nprint(f'PatchTST: {mae_patchtst:.3f}')\n```\n\n----------------------------------------\n\nTITLE: Cross Validation Testing Function\nDESCRIPTION: Comprehensive test function that validates cross-validation functionality with multiple neural network architectures and exogenous variables.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_45\n\nLANGUAGE: python\nCODE:\n```\ndef test_cross_validation(df, static_df, h, test_size):\n    if (test_size - h) % 1:\n        raise Exception(\"`test_size - h` should be module `step_size`\")\n    \n    n_windows = int((test_size - h) / 1) + 1\n    Y_test_df = df.groupby('unique_id').tail(test_size)\n    Y_train_df = df.drop(Y_test_df.index)\n    config = {'input_size': tune.choice([12, 24]),\n              'step_size': 12, 'hidden_size': 256, 'max_steps': 1, 'val_check_steps': 1}\n    config_drnn = {'input_size': tune.choice([-1]), 'encoder_hidden_size': tune.choice([5, 10]),\n                   'max_steps': 1, 'val_check_steps': 1}\n    fcst = NeuralForecast(\n        models=[\n            AutoDilatedRNN(h=12, config=config_drnn, cpus=1, num_samples=1),\n            DilatedRNN(h=12, input_size=-1, encoder_hidden_size=5, max_steps=1),\n            RNN(h=12, input_size=-1, encoder_hidden_size=5, max_steps=1,\n                stat_exog_list=['airline1'], futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n            TCN(h=12, input_size=-1, encoder_hidden_size=5, max_steps=1,\n                stat_exog_list=['airline1'], futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n            AutoMLP(h=12, config=config, cpus=1, num_samples=1),\n            MLP(h=12, input_size=12, max_steps=1, scaler_type='robust'),\n            NBEATSx(h=12, input_size=12, max_steps=1,\n                    stat_exog_list=['airline1'], futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n            NHITS(h=12, input_size=12, max_steps=1, scaler_type='robust'),\n            NHITS(h=12, input_size=12, loss=MQLoss(level=[80]), max_steps=1),\n            TFT(h=12, input_size=24, max_steps=1, scaler_type='robust'),\n            DLinear(h=12, input_size=24, max_steps=1),\n            VanillaTransformer(h=12, input_size=12, max_steps=1, scaler_type=None),\n            Informer(h=12, input_size=12, max_steps=1, scaler_type=None),\n            Autoformer(h=12, input_size=12, max_steps=1, scaler_type=None),\n            FEDformer(h=12, input_size=12, max_steps=1, scaler_type=None),\n            PatchTST(h=12, input_size=24, max_steps=1, scaler_type=None),\n            TimesNet(h=12, input_size=24, max_steps=1, scaler_type='standard'),\n            StemGNN(h=12, input_size=12, n_series=2, max_steps=1, scaler_type='robust'),\n            TSMixer(h=12, input_size=12, n_series=2, max_steps=1, scaler_type='robust'),\n            TSMixerx(h=12, input_size=12, n_series=2, max_steps=1, scaler_type='robust'),\n            DeepAR(h=12, input_size=24, max_steps=1,\n               stat_exog_list=['airline1'], futr_exog_list=['trend']),\n        ],\n        freq='M'\n    )\n    fcst.fit(df=Y_train_df, static_df=static_df)\n    Y_hat_df = fcst.predict(futr_df=Y_test_df)\n    Y_hat_df = Y_hat_df.merge(Y_test_df, how='left', on=['unique_id', 'ds'])\n    last_dates = Y_train_df.groupby('unique_id').tail(1)\n    last_dates = last_dates[['unique_id', 'ds']].rename(columns={'ds': 'cutoff'})\n    Y_hat_df = Y_hat_df.merge(last_dates, how='left', on='unique_id')\n    \n    #cross validation\n    fcst = NeuralForecast(\n        models=[\n            AutoDilatedRNN(h=12, config=config_drnn, cpus=1, num_samples=1),\n            DilatedRNN(h=12, input_size=-1, encoder_hidden_size=5, max_steps=1),\n            RNN(h=12, input_size=-1, encoder_hidden_size=5, max_steps=1,\n                stat_exog_list=['airline1'], futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n            TCN(h=12, input_size=-1, encoder_hidden_size=5, max_steps=1,\n                stat_exog_list=['airline1'], futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n            AutoMLP(h=12, config=config, cpus=1, num_samples=1),\n            MLP(h=12, input_size=12, max_steps=1, scaler_type='robust'),\n            NBEATSx(h=12, input_size=12, max_steps=1,\n                    stat_exog_list=['airline1'], futr_exog_list=['trend'], hist_exog_list=['y_[lag12]']),\n            NHITS(h=12, input_size=12, max_steps=1, scaler_type='robust'),\n            NHITS(h=12, input_size=12, loss=MQLoss(level=[80]), max_steps=1),\n            TFT(h=12, input_size=24, max_steps=1, scaler_type='robust'),\n            DLinear(h=12, input_size=24, max_steps=1),\n            VanillaTransformer(h=12, input_size=12, max_steps=1, scaler_type=None),\n            Informer(h=12, input_size=12, max_steps=1, scaler_type=None),\n            Autoformer(h=12, input_size=12, max_steps=1, scaler_type=None),\n            FEDformer(h=12, input_size=12, max_steps=1, scaler_type=None),\n            PatchTST(h=12, input_size=24, max_steps=1, scaler_type=None),\n            TimesNet(h=12, input_size=24, max_steps=1, scaler_type='standard'),\n            StemGNN(h=12, input_size=12, n_series=2, max_steps=1, scaler_type='robust'),\n            TSMixer(h=12, input_size=12, n_series=2, max_steps=1, scaler_type='robust'),\n            TSMixerx(h=12, input_size=12, n_series=2, max_steps=1, scaler_type='robust'),\n            DeepAR(h=12, input_size=24, max_steps=1,\n               stat_exog_list=['airline1'], futr_exog_list=['trend']),\n        ],\n        freq='M'\n    )\n    Y_hat_df_cv = fcst.cross_validation(df, static_df=static_df, test_size=test_size, \n                                        n_windows=None)\n    for col in ['ds', 'cutoff']:\n        Y_hat_df_cv[col] = pd.to_datetime(Y_hat_df_cv[col].astype(str))\n        Y_hat_df[col] = pd.to_datetime(Y_hat_df[col].astype(str))\n    pd.testing.assert_frame_equal(\n        Y_hat_df[Y_hat_df_cv.columns],\n        Y_hat_df_cv,\n        check_dtype=False,\n        atol=1e-5,\n    )\n```\n\n----------------------------------------\n\nTITLE: Generating Column Names for Forecast Results\nDESCRIPTION: This static method generates column names for forecast results based on the model name, quantile, and whether levels are used. It handles different naming conventions for quantiles and prediction intervals.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_27\n\nLANGUAGE: Python\nCODE:\n```\n@staticmethod\ndef _get_column_name(model_name, quantile, has_level) -> str:\n    if not has_level:\n        col_name = f\"{model_name}_ql{quantile}\" \n    elif quantile < 0.5:\n        level_lo = int(round(100 - 200 * quantile))\n        col_name = f\"{model_name}-lo-{level_lo}\"\n    elif quantile > 0.5:\n        level_hi = int(round(100 - 200 * (1 - quantile)))\n        col_name = f\"{model_name}-hi-{level_hi}\"\n    else:\n        col_name = f\"{model_name}-median\"\n\n    return col_name\n```\n\n----------------------------------------\n\nTITLE: Configuring Unit Tests for DilatedRNN Model (Python)\nDESCRIPTION: Sets up the environment for running unit tests on the DilatedRNN model. It suppresses less critical logging messages from PyTorch Lightning and Lightning Fabric by setting their levels to ERROR. It also uses `warnings.catch_warnings` to ignore warnings during the execution of `check_model`, which likely performs validation checks on the `DilatedRNN` model using the 'airpassengers' dataset. Dependencies include the `logging`, `warnings` modules, and the `check_model` function from neuralforecast or a related testing utility.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.dilated_rnn.ipynb#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n# Unit tests for models\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\nlogging.getLogger(\"lightning_fabric\").setLevel(logging.ERROR)\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    check_model(DilatedRNN, [\"airpassengers\"])\n```\n\n----------------------------------------\n\nTITLE: Processing Cross-Validation Forecast Results\nDESCRIPTION: Prepares the cross-validation forecast results by resetting the index and removing unnecessary columns.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/07_forecasting_tft.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nY_hat_df = fcst_df.reset_index(drop=True)\nY_hat_df = Y_hat_df.drop(columns=['y','cutoff'])\n```\n\n----------------------------------------\n\nTITLE: Displaying Method Documentation for KAN.predict in NeuralForecast Python\nDESCRIPTION: This snippet shows how to programmatically display documentation for the predict method of the KAN class, enabling interactive inspection of prediction logic and arguments. It requires the show_doc utility and the KAN model implementation. It operates on KAN.predict and outputs its documentation summary.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.kan.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(KAN.predict, name='KAN.predict')\n```\n\n----------------------------------------\n\nTITLE: Using AutoDilatedRNN for Model Training and Prediction in Python\nDESCRIPTION: This snippet demonstrates the usage of the `AutoDilatedRNN` class. It first defines a minimal custom configuration dictionary. It then instantiates `AutoDilatedRNN` with this config, specifying the forecast horizon `h`, number of samples for tuning `num_samples`, and `cpus`. The model is fitted to a `dataset` and used to generate predictions. Finally, it shows how to instantiate the model to use the Optuna backend instead of the default Ray backend by setting `backend='optuna'` and `config=None` to use the default Optuna-compatible search space.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n# Use your own config or AutoDilatedRNN.default_config\nconfig = dict(max_steps=1, val_check_steps=1, input_size=-1, encoder_hidden_size=8)\nmodel = AutoDilatedRNN(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n# Optuna\nmodel = AutoDilatedRNN(h=12, config=None, backend='optuna')\n```\n\n----------------------------------------\n\nTITLE: Testing AutoKAN with Different Configurations\nDESCRIPTION: Unit tests for the AutoKAN model with different backend configurations, including assertion tests, parameter checking, and testing with both Optuna and Ray backends using updated configurations.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_63\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n# Check Optuna\nassert model.config(MockTrial())['h'] == 12\n\n# Unit test to test that Auto* model contains all required arguments from BaseAuto\ntest_args(AutoKAN, exclude_args=['cls_model']) \n\n# Unit test for situation: Optuna with updated default config\nmy_config = AutoKAN.get_default_config(h=12, backend='optuna')\ndef my_config_new(trial):\n    config = {**my_config(trial)}\n    config.update({'max_steps': 2, 'val_check_steps': 1, 'input_size': 12})\n    return config\n\nmodel = AutoKAN(h=12, config=my_config_new, backend='optuna', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n\n# Unit test for situation: Ray with updated default config\nmy_config = AutoKAN.get_default_config(h=12, backend='ray')\nmy_config['max_steps'] = 2\nmy_config['val_check_steps'] = 1\nmy_config['input_size'] = 12\nmodel = AutoKAN(h=12, config=my_config, backend='ray', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: Computing Forecast Error Metrics\nDESCRIPTION: Calculates and displays the Mean Absolute Error (MAE) and Mean Squared Error (MSE) for the forecasts to quantitatively evaluate model performance.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/04_longhorizon_nhits.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom neuralforecast.losses.numpy import mae, mse\n\nprint('MAE: ', mae(y_hat, y_true))\nprint('MSE: ', mse(y_hat, y_true))\n```\n\n----------------------------------------\n\nTITLE: Create Panel AirPassengers Dataset\nDESCRIPTION: Extends the classic AirPassengers dataset into a panel format with two airlines and additional features including static and temporal variables.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/utils.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#| export\n\n# Declare Panel Data\nunique_id = np.concatenate([['Airline1']*len(AirPassengers), ['Airline2']*len(AirPassengers)])\nds = np.tile(\n    pd.date_range(\n        start='1949-01-01', periods=len(AirPassengers), freq=pd.offsets.MonthEnd()\n    ).to_numpy(), \n    2,\n)\ny = np.concatenate([AirPassengers, AirPassengers+300])\n\nAirPassengersPanel = pd.DataFrame({'unique_id': unique_id, 'ds': ds, 'y': y})\n\n# For future exogenous variables\n# Declare SeasonalNaive12 and fill first 12 values with y\nsnaive = AirPassengersPanel.groupby('unique_id')['y'].shift(periods=12).reset_index(drop=True)\nAirPassengersPanel['trend'] = range(len(AirPassengersPanel))\nAirPassengersPanel['y_[lag12]'] = snaive.fillna(AirPassengersPanel['y'])\n\n# Declare Static Data\nunique_id = np.array(['Airline1', 'Airline2'])\nairline1_dummy = [0, 1]\nairline2_dummy = [1, 0]\nAirPassengersStatic = pd.DataFrame({'unique_id': unique_id,\n                                    'airline1': airline1_dummy,\n                                    'airline2': airline2_dummy})\n```\n\n----------------------------------------\n\nTITLE: Displaying Autoformer Class Documentation\nDESCRIPTION: This code snippet uses the show_doc function to display the documentation for the Autoformer class. It provides an overview of the class's purpose, parameters, and methods.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.autoformer.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(Autoformer)\n```\n\n----------------------------------------\n\nTITLE: Setting up TFT Model with NeuralForecast\nDESCRIPTION: Initializes a TFT (Temporal Fusion Transformer) model with custom parameters and trains it on the AirPassengers dataset. The model is configured with specific hidden size, RNN type, loss function, and various exogenous variables.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# from neuralforecast.models import TFT\nfrom neuralforecast.losses.pytorch import DistributionLoss\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\n\nAirPassengersPanel[\"month\"] = AirPassengersPanel.ds.dt.month\nY_train_df = AirPassengersPanel[\n    AirPassengersPanel.ds < AirPassengersPanel[\"ds\"].values[-12]\n]  # 132 train\nY_test_df = AirPassengersPanel[\n    AirPassengersPanel.ds >= AirPassengersPanel[\"ds\"].values[-12]\n].reset_index(drop=True)  # 12 test\n\nnf = NeuralForecast(\n    models=[\n        TFT(\n            h=12,\n            input_size=48,\n            hidden_size=20,\n            grn_activation=\"ELU\",\n            rnn_type=\"lstm\",\n            n_rnn_layers=1,\n            one_rnn_initial_state=False,\n            loss=DistributionLoss(distribution=\"StudentT\", level=[80, 90]),\n            learning_rate=0.005,\n            stat_exog_list=[\"airline1\"],\n            futr_exog_list=[\"y_[lag12]\", \"month\"],\n            hist_exog_list=[\"trend\"],\n            max_steps=300,\n            val_check_steps=10,\n            early_stop_patience_steps=10,\n            scaler_type=\"robust\",\n            windows_batch_size=None,\n            enable_progress_bar=True,\n        ),\n    ],\n    freq=\"ME\",\n)\nnf.fit(df=Y_train_df, static_df=AirPassengersStatic, val_size=12)\nY_hat_df = nf.predict(futr_df=Y_test_df)\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for MLPMultivariate Model\nDESCRIPTION: Imports necessary Python modules including PyTorch, typing utilities, and custom loss functions and base models from the neuralforecast package.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.mlpmultivariate.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\n\nfrom typing import Optional\nfrom neuralforecast.losses.pytorch import MAE\nfrom neuralforecast.common._base_model import BaseModel\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for RMoK Model\nDESCRIPTION: Imports necessary Python libraries and modules for implementing the RMoK model.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.rmok.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom neuralforecast.losses.pytorch import MAE\nfrom neuralforecast.common._base_model import BaseModel\nfrom neuralforecast.common._modules import RevINMultivariate\nfrom typing import Optional\n```\n\n----------------------------------------\n\nTITLE: Testing Deterministic Forecasts for Repeated Prediction - Python\nDESCRIPTION: Checks that repeated predictions with the same NBEATS model, dataset, and parameters yield identical outputs, asserting model determinism. Essential for evaluation consistency and reproducibility. Assumes test_eq is a test assertion utility and y_hat is computed prior.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeats.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n#test we recover the same forecast\ny_hat2 = nbeats.predict(dataset=dataset)\ntest_eq(y_hat, y_hat2)\n```\n\n----------------------------------------\n\nTITLE: Testing AutoDLinear Configuration Options\nDESCRIPTION: Unit tests for AutoDLinear configurations with Optuna backend. The tests verify the proper functioning of configuration customization, model initialization, and fitting with a dataset.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_47\n\nLANGUAGE: python\nCODE:\n```\n# Check Optuna\nassert model.config(MockTrial())['h'] == 12\n\n# Unit test to test that Auto* model contains all required arguments from BaseAuto\ntest_args(AutoDLinear, exclude_args=['cls_model']) \n\n# Unit test for situation: Optuna with updated default config\nmy_config = AutoDLinear.get_default_config(h=12, backend='optuna')\ndef my_config_new(trial):\n    config = {**my_config(trial)}\n    config.update({'max_steps': 2, 'val_check_steps': 1, 'input_size': 12})\n    return config\n\nmodel = AutoDLinear(h=12, config=my_config_new, backend='optuna', cpus=1)\nmodel.fit(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: Initializing PredictionIntervals Class in Python\nDESCRIPTION: Defines the `PredictionIntervals` class to store metadata for prediction interval computation. It initializes with the number of windows (`n_windows`) and the chosen method (`method`), validating that `n_windows` is at least 2 and the method is one of the supported conformal methods ('conformal_error', 'conformal_distribution'). Includes a `__repr__` method for object representation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/utils.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n#| export\n\nclass PredictionIntervals:\n    \"\"\"Class for storing prediction intervals metadata information.\"\"\"\n\n    def __init__(\n        self,\n        n_windows: int = 2,\n        method: str = \"conformal_distribution\",\n    ):\n        \"\"\" \n        n_windows : int\n            Number of windows to evaluate.\n        method : str, default is conformal_distribution\n            One of the supported methods for the computation of prediction intervals:\n            conformal_error or conformal_distribution\n        \"\"\"\n        if n_windows < 2:\n            raise ValueError(\n                \"You need at least two windows to compute conformal intervals\"\n            )\n        allowed_methods = [\"conformal_error\", \"conformal_distribution\"]\n        if method not in allowed_methods:\n            raise ValueError(f\"method must be one of {allowed_methods}\")\n        self.n_windows = n_windows\n        self.method = method\n\n    def __repr__(self):\n        return f\"PredictionIntervals(n_windows={self.n_windows}, method='{self.method}')\"\n```\n\n----------------------------------------\n\nTITLE: Unit Testing AutoBiTCN with Optuna and Ray Backends in Python\nDESCRIPTION: This snippet contains unit tests for the `AutoBiTCN` class. It checks if the Optuna configuration correctly sets the forecast horizon `h`. It verifies that `AutoBiTCN` includes all necessary arguments from its `BaseAuto` parent using `test_args`. Subsequently, it tests model fitting using modified default configurations for both the Optuna backend (requiring a function `my_config_new` to update the config per trial) and the Ray backend (directly modifying the configuration dictionary). These tests confirm the class's functionality with custom configurations across both supported backends.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_33\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n# Check Optuna\nassert model.config(MockTrial())['h'] == 12\n\n# Unit test to test that Auto* model contains all required arguments from BaseAuto\ntest_args(AutoBiTCN, exclude_args=['cls_model']) \n\n# Unit test for situation: Optuna with updated default config\nmy_config = AutoBiTCN.get_default_config(h=12, backend='optuna')\ndef my_config_new(trial):\n    config = {**my_config(trial)}\n    config.update({'max_steps': 1, 'val_check_steps': 1, 'input_size': 12, 'hidden_size': 8})\n    return config\n\nmodel = AutoBiTCN(h=12, config=my_config_new, backend='optuna', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n\n# Unit test for situation: Ray with updated default config\nmy_config = AutoBiTCN.get_default_config(h=12, backend='ray')\nmy_config['max_steps'] = 1\nmy_config['val_check_steps'] = 1\nmy_config['input_size'] = 12\nmy_config['hidden_size'] = 8\nmodel = AutoBiTCN(h=12, config=my_config, backend='ray', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: Visualizing TFT Forecasts with Matplotlib\nDESCRIPTION: Creates a plot comparing the actual values from the last 5 days of training data with the TFT forecasts for the next 24 hours.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/07_forecasting_tft.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1, figsize = (10, 3))\nplot_df = pd.concat([Y_df.tail(24*5).reset_index(drop=True), Y_hat_df]).set_index('ds') # Concatenate the train and forecast dataframes\nplot_df[['y', 'AutoTFT']].plot(ax=ax, linewidth=2)\n\nax.set_title('Load [MW]', fontsize=12)\nax.set_ylabel('Monthly Passengers', fontsize=12)\nax.set_xlabel('Date', fontsize=12)\nax.legend(prop={'size': 10})\nax.grid()\n```\n\n----------------------------------------\n\nTITLE: Implementing Auxiliary Functions for Time-LLM in Python\nDESCRIPTION: Defines auxiliary classes and functions for Time-LLM, including ReplicationPad1d, TokenEmbedding, PatchEmbedding, FlattenHead, and ReprogrammingLayer. These components are crucial for the Time-LLM architecture, handling tasks such as padding, embedding, and reprogramming.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timellm.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#| export\n\nclass ReplicationPad1d(nn.Module):\n    \"\"\"\n    ReplicationPad1d\n    \"\"\"       \n    def __init__(self, padding):\n        super(ReplicationPad1d, self).__init__()\n        self.padding = padding\n\n    def forward(self, input):\n        replicate_padding = input[:, :, -1].unsqueeze(-1).repeat(1, 1, self.padding[-1])\n        output = torch.cat([input, replicate_padding], dim=-1)\n        return output\n    \nclass TokenEmbedding(nn.Module):\n    \"\"\"\n    TokenEmbedding\n    \"\"\"       \n    def __init__(self, c_in, d_model):\n        super(TokenEmbedding, self).__init__()\n        padding = 1 if torch.__version__ >= '1.5.0' else 2\n        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n        for m in self.modules():\n            if isinstance(m, nn.Conv1d):\n                nn.init.kaiming_normal_(\n                    m.weight, mode='fan_in', nonlinearity='leaky_relu')\n\n    def forward(self, x):\n        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n        return x\n    \nclass PatchEmbedding(nn.Module):\n    \"\"\"\n    PatchEmbedding\n    \"\"\"      \n    def __init__(self, d_model, patch_len, stride, dropout):\n        super(PatchEmbedding, self).__init__()\n        # Patching\n        self.patch_len = patch_len\n        self.stride = stride\n        self.padding_patch_layer = ReplicationPad1d((0, stride))\n\n        # Backbone, Input encoding: projection of feature vectors onto a d-dim vector space\n        self.value_embedding = TokenEmbedding(patch_len, d_model)\n\n        # Positional embedding\n        # self.position_embedding = PositionalEmbedding(d_model)\n\n        # Residual dropout\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # do patching\n        n_vars = x.shape[1]\n        x = self.padding_patch_layer(x)\n        x = x.unfold(dimension=-1, size=self.patch_len, step=self.stride)\n        x = torch.reshape(x, (x.shape[0] * x.shape[1], x.shape[2], x.shape[3]))\n        # Input encoding\n        x = self.value_embedding(x)\n        return self.dropout(x), n_vars\n    \nclass FlattenHead(nn.Module):\n    \"\"\"\n    FlattenHead\n    \"\"\"       \n    def __init__(self, n_vars, nf, target_window, head_dropout=0):\n        super().__init__()\n        self.n_vars = n_vars\n        self.flatten = nn.Flatten(start_dim=-2)\n        self.linear = nn.Linear(nf, target_window)\n        self.dropout = nn.Dropout(head_dropout)\n\n    def forward(self, x):\n        x = self.flatten(x)\n        x = self.linear(x)\n        x = self.dropout(x)\n        return x\n    \nclass ReprogrammingLayer(nn.Module):\n    \"\"\"\n    ReprogrammingLayer\n    \"\"\"       \n    def __init__(self, d_model, n_heads, d_keys=None, d_llm=None, attention_dropout=0.1):\n        super(ReprogrammingLayer, self).__init__()\n\n        d_keys = d_keys or (d_model // n_heads)\n\n        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n        self.key_projection = nn.Linear(d_llm, d_keys * n_heads)\n        self.value_projection = nn.Linear(d_llm, d_keys * n_heads)\n        self.out_projection = nn.Linear(d_keys * n_heads, d_llm)\n        self.n_heads = n_heads\n        self.dropout = nn.Dropout(attention_dropout)\n\n    def forward(self, target_embedding, source_embedding, value_embedding):\n        B, L, _ = target_embedding.shape\n        S, _ = source_embedding.shape\n        H = self.n_heads\n\n        target_embedding = self.query_projection(target_embedding).view(B, L, H, -1)\n        source_embedding = self.key_projection(source_embedding).view(S, H, -1)\n        value_embedding = self.value_projection(value_embedding).view(S, H, -1)\n\n        out = self.reprogramming(target_embedding, source_embedding, value_embedding)\n\n        out = out.reshape(B, L, -1)\n\n        return self.out_projection(out)\n\n    def reprogramming(self, target_embedding, source_embedding, value_embedding):\n        B, L, H, E = target_embedding.shape\n\n        scale = 1. / math.sqrt(E)\n\n        scores = torch.einsum(\"blhe,she->bhls\", target_embedding, source_embedding)\n\n        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n        reprogramming_embedding = torch.einsum(\"bhls,she->blhe\", A, value_embedding)\n\n        return reprogramming_embedding\n    \n```\n\n----------------------------------------\n\nTITLE: Configuring hyperparameter optimization with Optuna and Ray Tune\nDESCRIPTION: Defines configuration functions for hyperparameter optimization using both Optuna and Ray Tune frameworks. The configurations specify parameters like input_size, exogenous variables lists, and training steps for time series models.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_41\n\nLANGUAGE: python\nCODE:\n```\ndef config_optuna(trial):\n    return {\"input_size\": trial.suggest_categorical('input_size', [12, 24]),\n        \"hist_exog_list\": trial.suggest_categorical('hist_exog_list', [['trend'], ['y_[lag12]'], ['trend', 'y_[lag12]']]),\n        \"futr_exog_list\": ['trend'],\n        \"max_steps\": 10,\n        \"val_check_steps\": 5}\n\nconfig_ray = {'input_size': tune.choice([12, 24]), \n          'hist_exog_list': tune.choice([['trend'], ['y_[lag12]'], ['trend', 'y_[lag12]']]),\n          'futr_exog_list': ['trend'],\n          'max_steps': 10,\n          'val_check_steps': 5}\n```\n\n----------------------------------------\n\nTITLE: Documenting the fit Method for TFT Model - Python\nDESCRIPTION: Uses show_doc utility to document the fit method of the TFT class, supporting automated doc generation and introspection by users or tooling. No dependencies are directly required in the code sample, but the show_doc utility and TFT class must be importable. Input parameters allow specification of the method, the display name, and title level for rendered docs.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(TFT.fit, name=\"TFT.fit\", title_level=3)\n```\n\n----------------------------------------\n\nTITLE: Instantiating RNN model with HuberLoss and trend as hist_exog feature\nDESCRIPTION: Creates an RNN model with HuberLoss function and tests revin dynamic dimensionality with only trend as historical exogenous variable. The model is configured for forecasting 12 steps ahead with early stopping and validation checks.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.scalers.ipynb#2025-04-23_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# Instantiate BaseRecurrent model and test revin dynamic dimensionality with hist_exog_list\nmodel = RNN(h=12,\n              input_size=24,\n              loss=HuberLoss(),\n              hist_exog_list=['trend'],\n              max_steps=1,\n              early_stop_patience_steps=10,\n              val_check_steps=50,\n              scaler_type='revin',\n              learning_rate=1e-3)\nnf = NeuralForecast(models=[model], freq='MS')\nY_hat_df = nf.cross_validation(df=Y_df, val_size=12, n_windows=1)\n```\n\n----------------------------------------\n\nTITLE: Testing DistributionLoss Initialization with Normal Distribution\nDESCRIPTION: Tests the initialization of DistributionLoss with the Normal distribution, verifying that quantiles and output names are correctly set. The code checks different initialization scenarios including using confidence levels and explicitly providing quantiles.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_43\n\nLANGUAGE: python\nCODE:\n```\n# attribute is correctly instantiated\ncheck = DistributionLoss(distribution='Normal', level=[80, 90])\ntest_eq(len(check.quantiles), 5)\n\ncheck = DistributionLoss(distribution='Normal', \n                         quantiles=[0.0100, 0.1000, 0.5, 0.9000, 0.9900])\nprint(check.output_names)\nprint(check.quantiles)\ntest_eq(len(check.quantiles), 5)\n\ncheck = DistributionLoss(distribution='Normal',\n                         quantiles=[0.0100, 0.1000, 0.9000, 0.9900])\ntest_eq(len(check.quantiles), 4)\n```\n\n----------------------------------------\n\nTITLE: Generating Forecasts with Window-Normalized Model in Python\nDESCRIPTION: Predicts future electricity prices using the model with temporal window normalization. The forecasts are automatically returned in the original scale.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/07_time_series_scaling.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nY_hat_df = nf.predict(futr_df=futr_df)\nY_hat_df.head()\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging and Running Unit Tests for FEDformer Model in Python\nDESCRIPTION: This code snippet prepares the environment for unit testing the `FEDformer` model. It suppresses less critical logging messages from `pytorch_lightning` and `lightning_fabric` by setting their levels to `ERROR`. It then uses a context manager to ignore warnings while running `check_model`, a utility function likely designed to perform basic checks or tests on the `FEDformer` model using the 'airpassengers' dataset.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.fedformer.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n# Unit tests for models\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\nlogging.getLogger(\"lightning_fabric\").setLevel(logging.ERROR)\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    check_model(FEDformer, [\"airpassengers\"])\n```\n\n----------------------------------------\n\nTITLE: Analyzing Model Results\nDESCRIPTION: Processes and displays the top 5 best performing model configurations based on loss metrics.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/experiments/nbeats_basis/nbeats_basis_experiment.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nresults = nf.models[0].results.get_dataframe()\n\nconfig_cols = [col for col in results.columns if col.startswith('config')]\ncolumns_to_keep = ['loss', 'train_loss'] + config_cols\nexisting_columns = [col for col in columns_to_keep if col in results.columns]\nfiltered_results = results[existing_columns]\nbest_runs = filtered_results.sort_values('loss', ascending=True).head(5)\nbest_runs\n```\n\n----------------------------------------\n\nTITLE: Testing Error Handling for Missing Variables in Models\nDESCRIPTION: Verifies that appropriate errors are raised when exogenous variables specified in models are not found in the input dataset. Tests both BaseWindows (NHITS) and BaseRecurrent (LSTM) models with missing historical, future, and static exogenous variables.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_52\n\nLANGUAGE: python\nCODE:\n```\n# Test fit fails if variable not in dataframe\n\n# Base Windows\nmodels = [NHITS(h=12, input_size=24, max_steps=1, hist_exog_list=['not_included'], scaler_type='robust')]\nnf = NeuralForecast(models=models, freq='M')\ntest_fail(nf.fit,\n          contains='historical exogenous variables not found in input dataset',\n          args=(AirPassengersPanel_train,))\n\nmodels = [NHITS(h=12, input_size=24, max_steps=1, futr_exog_list=['not_included'], scaler_type='robust')]\nnf = NeuralForecast(models=models, freq='M')\ntest_fail(nf.fit,\n          contains='future exogenous variables not found in input dataset',\n          args=(AirPassengersPanel_train,))\n\nmodels = [NHITS(h=12, input_size=24, max_steps=1, stat_exog_list=['not_included'], scaler_type='robust')]\nnf = NeuralForecast(models=models, freq='M')\ntest_fail(nf.fit,\n          contains='static exogenous variables not found in input dataset',\n          args=(AirPassengersPanel_train,))\n\n# Base Recurrent\nmodels = [LSTM(h=12, input_size=24, max_steps=1, hist_exog_list=['not_included'], scaler_type='robust')]\nnf = NeuralForecast(models=models, freq='M')\ntest_fail(nf.fit,\n          contains='historical exogenous variables not found in input dataset',\n          args=(AirPassengersPanel_train,))\n\nmodels = [LSTM(h=12, input_size=24, max_steps=1, futr_exog_list=['not_included'], scaler_type='robust')]\nnf = NeuralForecast(models=models, freq='M')\ntest_fail(nf.fit,\n          contains='future exogenous variables not found in input dataset',\n          args=(AirPassengersPanel_train,))\n\nmodels = [LSTM(h=12, input_size=24, max_steps=1, stat_exog_list=['not_included'], scaler_type='robust')]\nnf = NeuralForecast(models=models, freq='M')\ntest_fail(nf.fit,\n          contains='static exogenous variables not found in input dataset',\n          args=(AirPassengersPanel_train,))\n```\n\n----------------------------------------\n\nTITLE: Unit Testing NBEATS on Example Dataset - Python\nDESCRIPTION: Runs a unit test for the NBEATS model using the check_model utility and the 'airpassengers' dataset. Temporarily suppresses logging and deprecation warnings during testing. Checks model functionality and fit as part of a test automation pipeline. Relies on external check_model, logging, and warnings modules.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeats.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n# Unit tests for models\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\nlogging.getLogger(\"lightning_fabric\").setLevel(logging.ERROR)\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    check_model(NBEATS, [\"airpassengers\"])\n```\n\n----------------------------------------\n\nTITLE: Testing AutoTimesNet Configuration and Implementations\nDESCRIPTION: Comprehensive unit tests for the AutoTimesNet class, verifying correct behavior with both Optuna and Ray backends, configuration parameter handling, and model fitting functionality.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_96\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n# Check Optuna\nassert model.config(MockTrial())['h'] == 12\n\n# Unit test to test that Auto* model contains all required arguments from BaseAuto\ntest_args(AutoTimesNet, exclude_args=['cls_model']) \n\n# Unit test for situation: Optuna with updated default config\nmy_config = AutoTimesNet.get_default_config(h=12, backend='optuna')\ndef my_config_new(trial):\n    config = {**my_config(trial)}\n    config.update({'max_steps': 2, 'val_check_steps': 1, 'input_size': 12, 'hidden_size': 32})\n    return config\n\nmodel = AutoTimesNet(h=12, config=my_config_new, backend='optuna', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n\n# Unit test for situation: Ray with updated default config\nmy_config = AutoTimesNet.get_default_config(h=12, backend='ray')\nmy_config['max_steps'] = 2\nmy_config['val_check_steps'] = 1\nmy_config['input_size'] = 12\nmy_config['hidden_size'] = 32\nmodel = AutoTimesNet(h=12, config=my_config, backend='ray', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: Displaying TimesNet.fit Method Documentation - Python\nDESCRIPTION: This snippet displays the documentation for the \\\"fit\\\" method of the TimesNet class using the \\\"show_doc\\\" utility and specifies the display name as \\\"TimesNet.fit\\\". It requires the \\\"show_doc\\\" function and the TimesNet class to be defined in the environment. The output provides information about train method parameters and usage.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timesnet.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nshow_doc(TimesNet.fit, name='TimesNet.fit')\n```\n\n----------------------------------------\n\nTITLE: Specifying Default Export Path for NeuralForecast Models\nDESCRIPTION: This code sets the default export path for the NeuralForecast models, specifically pointing to the 'models.kan' module. This helps in organizing and accessing the model files within the project structure.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.kan.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#| default_exp models.kan\n```\n\n----------------------------------------\n\nTITLE: Generating Harmonic Signal with Multiple Frequencies\nDESCRIPTION: Creates a harmonic signal by combining a low-frequency and high-frequency sinusoidal component for demonstration purposes.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/14_interpretable_decompositions.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nN = 10_000\nT = 1.0 / 800.0 # sample spacing\nx = np.linspace(0.0, N*T, N, endpoint=False)\n\ny1 = np.sin(10.0 * 2.0*np.pi*x) \ny2 = 0.5 * np.sin(100 * 2.0*np.pi*x)\ny = y1 + y2\n```\n\n----------------------------------------\n\nTITLE: Displaying AutoInformer Documentation in Python\nDESCRIPTION: This snippet shows how to display the documentation for the AutoInformer class using the show_doc function. It sets the title level to 3 for proper formatting in the documentation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_72\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(AutoInformer, title_level=3)\n```\n\n----------------------------------------\n\nTITLE: Importing Neural Network Model Dependencies\nDESCRIPTION: Imports required PyTorch and custom NeuralForecast modules for implementing a neural forecasting model.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/18_adding_models.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#| export\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\n\nfrom neuralforecast.losses.pytorch import MAE\nfrom neuralforecast.common._base_model import BaseModel\n```\n\n----------------------------------------\n\nTITLE: Testing AutoTiDE with Ray Backend\nDESCRIPTION: A unit test for configuring and fitting an AutoTiDE model with Ray backend and custom configuration parameters.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_57\n\nLANGUAGE: python\nCODE:\n```\nmy_config = AutoTiDE.get_default_config(h=12, backend='ray')\nmy_config['max_steps'] = 2\nmy_config['val_check_steps'] = 1\nmy_config['input_size'] = 12\nmodel = AutoTiDE(h=12, config=my_config, backend='ray', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: Training NeuralForecast Model\nDESCRIPTION: Fits the NeuralForecast model with the configured AutoNBEATS model on the generated time series data.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/experiments/nbeats_basis/nbeats_basis_experiment.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nnf = NeuralForecast(models=[model], freq='M')\nnf.fit(df=df, val_size=192)\n```\n\n----------------------------------------\n\nTITLE: Testing AutoNLinear with Various Configurations\nDESCRIPTION: Unit tests for the AutoNLinear class, including testing Optuna integration, argument checking, and different configuration approaches for both Optuna and Ray backends.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_52\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n# Check Optuna\nassert model.config(MockTrial())['h'] == 12\n\n# Unit test to test that Auto* model contains all required arguments from BaseAuto\ntest_args(AutoNLinear, exclude_args=['cls_model']) \n\n# Unit test for situation: Optuna with updated default config\nmy_config = AutoNLinear.get_default_config(h=12, backend='optuna')\ndef my_config_new(trial):\n    config = {**my_config(trial)}\n    config.update({'max_steps': 2, 'val_check_steps': 1, 'input_size': 12})\n    return config\n\nmodel = AutoNLinear(h=12, config=my_config_new, backend='optuna', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n\n# Unit test for situation: Ray with updated default config\nmy_config = AutoNLinear.get_default_config(h=12, backend='ray')\nmy_config['max_steps'] = 2\nmy_config['val_check_steps'] = 1\nmy_config['input_size'] = 12\nmodel = AutoNLinear(h=12, config=my_config, backend='ray', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: Calculating Mean Absolute Percentage Error (MAPE) for Time Series Forecasts\nDESCRIPTION: Implements the Mean Absolute Percentage Error metric for evaluating time series forecasts. It calculates the percentage deviation between actual and predicted values.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.numpy.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef mape(y: np.ndarray, y_hat: np.ndarray, \n         weights: Optional[np.ndarray] = None,\n         axis: Optional[int] = None) -> Union[float, np.ndarray]:\n    \"\"\" Mean Absolute Percentage Error\n\n    Calculates Mean Absolute Percentage Error  between\n    `y` and `y_hat`. MAPE measures the relative prediction\n    accuracy of a forecasting method by calculating the percentual deviation\n    of the prediction and the observed value at a given time and\n    averages these devations over the length of the series.\n    The closer to zero an observed value is, the higher penalty MAPE loss\n    assigns to the corresponding error.\n\n    $$ \\mathrm{MAPE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} \\frac{|y_{\\tau}-\\hat{y}_{\\tau}|}{|y_{\\tau}|} $$\n\n    **Parameters:**<br>\n    `y`: numpy array, Actual values.<br>\n    `y_hat`: numpy array, Predicted values.<br>\n    `mask`: numpy array, Specifies date stamps per serie to consider in loss.<br>\n\n    **Returns:**<br>\n    `mape`: numpy array, (single value).\n    \"\"\"\n    _metric_protections(y, y_hat, weights)\n        \n    delta_y = np.abs(y - y_hat)\n    scale = np.abs(y)\n    mape = _divide_no_nan(delta_y, scale)\n    mape = np.average(mape, weights=weights, axis=axis)\n    \n    return mape\n```\n\n----------------------------------------\n\nTITLE: Testing AutoNHITS Configuration Options\nDESCRIPTION: Unit tests for AutoNHITS configurations with both Optuna and Ray Tune backends. These tests verify that configurations can be properly customized and that the model can be fitted with both backends.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_44\n\nLANGUAGE: python\nCODE:\n```\n# Check Optuna\nassert model.config(MockTrial())['h'] == 12\n\n# Unit test to test that Auto* model contains all required arguments from BaseAuto\ntest_args(AutoNHITS, exclude_args=['cls_model']) \n\n# Unit test for situation: Optuna with updated default config\nmy_config = AutoNHITS.get_default_config(h=12, backend='optuna')\ndef my_config_new(trial):\n    config = {**my_config(trial)}\n    config.update({'max_steps': 2, 'val_check_steps': 1, 'input_size': 12, 'mlp_units': 3 * [[8, 8]]})\n    return config\n\nmodel = AutoNHITS(h=12, config=my_config_new, backend='optuna', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n\n# Unit test for situation: Ray with updated default config\nmy_config = AutoNHITS.get_default_config(h=12, backend='ray')\nmy_config['max_steps'] = 2\nmy_config['val_check_steps'] = 1\nmy_config['input_size'] = 12\nmy_config['mlp_units'] = 3 * [[8, 8]]\nmodel = AutoNHITS(h=12, config=my_config, backend='ray', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: Implementing Weighted Average in PyTorch\nDESCRIPTION: A function that computes weighted averages across tensor dimensions while handling zero weights appropriately. Supports optional weight tensors and dimension specifications.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\ndef weighted_average(x: torch.Tensor, \n                     weights: Optional[torch.Tensor]=None, dim=None) -> torch.Tensor:\n    \"\"\"\n    Computes the weighted average of a given tensor across a given dim, masking\n    values associated with weight zero,\n    meaning instead of `nan * 0 = nan` you will get `0 * 0 = 0`.\n\n    **Parameters:**<br>\n    `x`: Input tensor, of which the average must be computed.<br>\n    `weights`: Weights tensor, of the same shape as `x`.<br>\n    `dim`: The dim along which to average `x`.<br>\n\n    **Returns:**<br>\n    `Tensor`: The tensor with values averaged along the specified `dim`.<br>\n    \"\"\"\n    if weights is not None:\n        weighted_tensor = torch.where(\n            weights != 0, x * weights, torch.zeros_like(x)\n        )\n        sum_weights = torch.clamp(\n            weights.sum(dim=dim) if dim else weights.sum(), min=1.0\n        )\n        return (\n            weighted_tensor.sum(dim=dim) if dim else weighted_tensor.sum()\n        ) / sum_weights\n    else:\n        return x.mean(dim=dim)\n```\n\n----------------------------------------\n\nTITLE: Displaying TimeXer Model Documentation in Python\nDESCRIPTION: This code snippet uses the show_doc function to display documentation for the TimeXer model and its fit and predict methods.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timexer.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(TimeXer)\n\nshow_doc(TimeXer.fit, name='TimeXer.fit')\n\nshow_doc(TimeXer.predict, name='TimeXer.predict')\n```\n\n----------------------------------------\n\nTITLE: Generating Documentation for AutoTCN Class in Python\nDESCRIPTION: This snippet uses the `show_doc` function, likely from a documentation generation framework like nbdev, to automatically generate documentation for the `AutoTCN` class defined previously. The `title_level=3` argument specifies the heading level for the generated documentation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(AutoTCN, title_level=3)\n```\n\n----------------------------------------\n\nTITLE: PMM Unit Tests\nDESCRIPTION: Unit tests to verify the correct initialization and functionality of the PMM model's quantiles attribute with different configurations.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_48\n\nLANGUAGE: python\nCODE:\n```\ncheck = PMM(n_components=2, level=[80, 90])\ntest_eq(len(check.quantiles), 5)\n\ncheck = PMM(n_components=2, \n            quantiles=[0.0100, 0.1000, 0.5, 0.9000, 0.9900])\nprint(check.output_names)\nprint(check.quantiles)\ntest_eq(len(check.quantiles), 5)\n\ncheck = PMM(n_components=2,\n            quantiles=[0.0100, 0.1000, 0.9000, 0.9900])\ntest_eq(len(check.quantiles), 4)\n```\n\n----------------------------------------\n\nTITLE: Testing DistributionLoss Horizon Weight Functionality\nDESCRIPTION: Verifies that the horizon weight functionality of DistributionLoss works correctly. It creates test tensors for forecasts, actual values, and location/scale parameters, then compares loss calculations with and without horizon weights to ensure they match when weights are uniform.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_44\n\nLANGUAGE: python\nCODE:\n```\n# | hide\n# Unit tests to check DistributionLoss' horizon weight\nbatch_size, horizon, n_outputs = 10, 3, 2\ny_hat = torch.rand(batch_size, horizon, n_outputs).chunk(2, dim=-1)\ny = torch.rand(batch_size, horizon, 1)\ny_loc = torch.rand(batch_size, 1, 1)\ny_scale = torch.rand(batch_size, 1, 1)\n\nloss = DistributionLoss(distribution='Normal', level=[80, 90])\nloss_with_hweights = DistributionLoss(distribution='Normal', level=[80, 90], horizon_weight = torch.ones(horizon))\n\ndistr_args = loss.scale_decouple(y_hat, y_loc, y_scale)\ndistr_args_weighted = loss_with_hweights.scale_decouple(y_hat, y_loc, y_scale)\n\ntest_eq(loss(y, distr_args), loss_with_hweights(y, distr_args_weighted))\n```\n\n----------------------------------------\n\nTITLE: Visualizing Predicted vs Actual Binary Sequences\nDESCRIPTION: Creates a plot comparing the actual binary target values with predictions from both MLP and NHITS models for a single sequence (unique_id=10), displaying how well each model captures the temporal binary patterns.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/16_temporal_classification.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nplot_df = Y_hat_df[Y_hat_df.unique_id==10]\n\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nplt.plot(plot_df.ds, plot_df.y, label='target signal')\nplt.plot(plot_df.ds, plot_df['MLP'] * 1.1, label='MLP prediction')\nplt.plot(plot_df.ds, plot_df['NHITS'] * .9, label='NHITS prediction')\nax.set_title('Binary Sequence Forecast', fontsize=22)\nax.set_ylabel('Pixel Threshold and Prediction', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\n```\n\n----------------------------------------\n\nTITLE: Converting Single Time Series to NeuralForecast Format\nDESCRIPTION: Transforms a single time series DataFrame to the format required by NeuralForecast by adding a unique_id column and renaming the timestamp and value columns to ds and y respectively.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/getting-started/05_datarequirements.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nY_df['unique_id'] = 1. # We can add an integer as identifier\nY_df = Y_df.rename(columns={'timestamp': 'ds', 'value': 'y'})\nY_df = Y_df[['unique_id', 'ds', 'y']]\nY_df\n```\n\n----------------------------------------\n\nTITLE: Training NeuralForecast Model\nDESCRIPTION: Initializes and fits the NeuralForecast model with both temporal and static data.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/03_exogenous_variables.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nnf = NeuralForecast(models=models, freq='h')\nnf.fit(df=df, static_df=static_df)\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for KAN Implementation in Python\nDESCRIPTION: This snippet imports necessary modules and classes for implementing Kolmogorov-Arnold Networks, including PyTorch and custom loss functions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.kan.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional, Union\n\nimport math\n\nimport torch\nimport torch.nn.functional as F\n\nfrom neuralforecast.losses.pytorch import MAE\nfrom neuralforecast.common._base_model import BaseModel\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for SOFTS Model in Python\nDESCRIPTION: Imports necessary Python libraries and modules for implementing the SOFTS model, including PyTorch, typing, and custom neuralforecast modules.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.softs.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom typing import Optional\nfrom neuralforecast.losses.pytorch import MAE\nfrom neuralforecast.common._base_model import BaseModel\nfrom neuralforecast.common._modules import TransEncoder, TransEncoderLayer\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging and Running Unit Tests for VanillaTransformer in Python\nDESCRIPTION: This snippet configures logging levels to reduce output noise from PyTorch Lightning and Fabric during testing. It then uses `warnings.catch_warnings` to ignore potential warnings and runs the `check_model` utility function to perform unit tests on the `VanillaTransformer` model using the \"airpassengers\" dataset.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.vanillatransformer.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n# Unit tests for models\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\nlogging.getLogger(\"lightning_fabric\").setLevel(logging.ERROR)\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    check_model(VanillaTransformer, [\"airpassengers\"])\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Imports necessary modules for model training including logging, ray.tune, and neuralforecast components\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/06_save_load_models.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nfrom ray import tune\n\nfrom neuralforecast.core import NeuralForecast\nfrom neuralforecast.auto import AutoMLP\nfrom neuralforecast.models import NBEATS, NHITS\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Data Manipulation\nDESCRIPTION: Imports numpy and pandas libraries for data handling and analysis in Python.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/use-cases/electricity_peak_forecasting.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\n```\n\n----------------------------------------\n\nTITLE: Importing NeuralForecast for Model Training\nDESCRIPTION: Importing the NeuralForecast class, which is used to train and manage forecasting models.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/04_hyperparameter_tuning.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom neuralforecast import NeuralForecast\n```\n\n----------------------------------------\n\nTITLE: Initializing _FilesDataset Class for NeuralForecast\nDESCRIPTION: Initialization method for a helper class that handles datasets distributed across files. It stores configuration for columns, identifiers, and minimum size requirements for working with time series data.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/tsdataset.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass _FilesDataset:\n    def __init__(\n        self,\n        files: Sequence[str],\n        temporal_cols,\n        id_col: str,\n        time_col: str,\n        target_col: str,\n        min_size: int,\n        static_cols: Optional[List[str]] = None,\n    ):\n        self.files = files\n        self.temporal_cols = pd.Index(temporal_cols)\n        self.static_cols = pd.Index(static_cols) if static_cols is not None else None\n        self.id_col = id_col\n        self.time_col = time_col\n        self.target_col = target_col\n        self.min_size = min_size\n```\n\n----------------------------------------\n\nTITLE: Plotting Time Series Data\nDESCRIPTION: Visualizes the generated time series data using matplotlib with properly formatted date axis.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/experiments/nbeats_basis/nbeats_basis_experiment.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfig, ax = plt.subplots()\n\nax.plot(df['ds'], df['y'])\nax.set_xlabel('Time')\nax.set_ylabel('Value')\n\nfig.autofmt_xdate()\nplt.tight_layout()\n```\n\n----------------------------------------\n\nTITLE: Testing AutoAutoformer with Different Backends and Configurations\nDESCRIPTION: This hidden test code verifies that the AutoAutoformer class works correctly with different configurations and backends. It tests Optuna's configuration handling, validates required arguments from BaseAuto, and checks both Optuna and Ray backends with custom configurations.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_79\n\nLANGUAGE: python\nCODE:\n```\n# Check Optuna\nassert model.config(MockTrial())['h'] == 12\n\n# Unit test to test that Auto* model contains all required arguments from BaseAuto\ntest_args(AutoAutoformer, exclude_args=['cls_model']) \n\n# Unit test for situation: Optuna with updated default config\nmy_config = AutoAutoformer.get_default_config(h=12, backend='optuna')\ndef my_config_new(trial):\n    config = {**my_config(trial)}\n    config.update({'max_steps': 1, 'val_check_steps': 1, 'input_size': 12, 'hidden_size': 8})\n    return config\n\nmodel = AutoAutoformer(h=12, config=my_config_new, backend='optuna', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n\n# Unit test for situation: Ray with updated default config\nmy_config = AutoAutoformer.get_default_config(h=12, backend='ray')\nmy_config['max_steps'] = 1\nmy_config['val_check_steps'] = 1\nmy_config['input_size'] = 12\nmy_config['hidden_size'] = 8\nmodel = AutoAutoformer(h=12, config=my_config, backend='ray', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Forecasting Results\nDESCRIPTION: Imports plotting utilities and creates visualizations of the forecasts for the 'OT' variable across all models in the test set.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/08_multivariate_tsmixer.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom utilsforecast.plotting import plot_series\n```\n\nLANGUAGE: python\nCODE:\n```\ncutoffs = Y_hat_df['cutoff'].unique()[::horizon]\nY_plot = Y_hat_df[Y_hat_df['cutoff'].isin(cutoffs)].drop(columns='cutoff')\nplot_series(forecasts_df=Y_plot, ids=['OT'])\n```\n\n----------------------------------------\n\nTITLE: Trimming Time Series Data in NeuralForecast\nDESCRIPTION: A function that trims the beginning and end of time series datasets by removing specified numbers of observations from left and right sides. It creates a new temporal tensor and updates indptr accordingly to maintain proper series boundaries.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/tsdataset.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nlen_temporal, col_temporal = dataset.temporal.shape\ntotal_trim = (left_trim + right_trim) * dataset.n_groups\nnew_temporal = torch.zeros(size=(len_temporal-total_trim, col_temporal))\nnew_indptr = [0]\n\nacum = 0\nfor i in range(dataset.n_groups):\n    series_length = dataset.indptr[i + 1] - dataset.indptr[i]\n    new_length = series_length - left_trim - right_trim\n    new_temporal[acum:(acum+new_length), :] = dataset.temporal[dataset.indptr[i]+left_trim : \\\n                                                               dataset.indptr[i + 1]-right_trim, :]\n    acum += new_length\n    new_indptr.append(acum)\n\n# Define new dataset\nreturn TimeSeriesDataset(\n    temporal=new_temporal,\n    temporal_cols=dataset.temporal_cols.copy(),\n    indptr=np.array(new_indptr, dtype=np.int32),\n    y_idx=dataset.y_idx,\n    static=dataset.static,\n    static_cols=dataset.static_cols,\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing HINT Class for Hierarchical Forecasting in Python\nDESCRIPTION: Core implementation of the HINT class that combines neural forecasting with hierarchical reconciliation. Includes initialization with model configuration, reconciliation method selection, and parameter validation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.hint.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass HINT:\n    def __init__(self,\n                 h: int,\n                 S: np.ndarray,\n                 model,\n                 reconciliation: str,\n                 alias: Optional[str] = None):\n        if model.h != h:\n            raise Exception(f\"Model h {model.h} does not match HINT h {h}\")\n        \n        if not model.loss.is_distribution_output:\n            raise Exception(f\"The NeuralForecast model's loss {model.loss} is not a probabilistic objective\")\n        \n        self.h = h\n        self.model = model\n        self.early_stop_patience_steps = model.early_stop_patience_steps\n        self.S = S\n        self.reconciliation = reconciliation\n        self.loss = model.loss\n\n        available_reconciliations = dict(\n                                BottomUp=get_bottomup_P,\n                                MinTraceOLS=get_mintrace_ols_P,\n                                MinTraceWLS=get_mintrace_wls_P,\n                                Identity=get_identity_P,\n                                )\n\n        if reconciliation not in available_reconciliations:\n            raise Exception(f\"Reconciliation {reconciliation} not available\")\n\n        self.reconciliation = reconciliation\n        if reconciliation== 'Identity':\n            self.SP = None\n        else:\n            P = available_reconciliations[reconciliation](S=S)\n            self.SP = S @ P\n\n        qs = torch.Tensor((np.arange(self.loss.num_samples)/self.loss.num_samples))\n        self.sample_quantiles = torch.nn.Parameter(qs, requires_grad=False)\n        self.alias = alias\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for BaseModel in Python\nDESCRIPTION: Imports necessary libraries and modules for the BaseModel implementation, including PyTorch, PyTorch Lightning, and custom NeuralForecast components.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport inspect\nimport random\nimport warnings\nfrom contextlib import contextmanager\nfrom copy import deepcopy\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Union\n\nimport fsspec\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nimport neuralforecast.losses.pytorch as losses\n\nfrom neuralforecast.losses.pytorch import BasePointLoss, DistributionLoss\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom neuralforecast.tsdataset import (\n    TimeSeriesDataModule,\n    BaseTimeSeriesDataset,\n    _DistributedTimeSeriesDataModule,\n)\nfrom neuralforecast.common._scalers import TemporalNorm\nfrom neuralforecast.utils import get_indexer_raise_missing\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for TFT Model\nDESCRIPTION: Imports necessary libraries and modules for the Temporal Fusion Transformer model, including Ray's Tune for hyperparameter optimization, NeuralForecast components, and configures logging levels.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/07_forecasting_tft.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import tune\n\nfrom neuralforecast.auto import AutoTFT\nfrom neuralforecast.core import NeuralForecast\nfrom neuralforecast.losses.pytorch import MAE\n\nimport logging\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\n```\n\n----------------------------------------\n\nTITLE: Displaying Documentation for DLinear Model\nDESCRIPTION: A utility function call to display the full documentation for the DLinear class, showing all parameters, methods, and usage information for time series forecasting.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.dlinear.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(DLinear)\n```\n\n----------------------------------------\n\nTITLE: Importing Plotting Utilities\nDESCRIPTION: Imports the plot_series function from utilsforecast.plotting to visualize the actual and predicted time series data.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/getting-started/02_quickstart.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom utilsforecast.plotting import plot_series\n```\n\n----------------------------------------\n\nTITLE: Generating Forecasts with Optimized Model\nDESCRIPTION: Using the trained model with optimal hyperparameters to generate forecasts for the next 12 months and displaying the first few forecast results.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/04_hyperparameter_tuning.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nY_hat_df = nf.predict()\nY_hat_df.head()\n```\n\n----------------------------------------\n\nTITLE: Calculating and Comparing Model Accuracy for Binary Classification\nDESCRIPTION: Defines an accuracy function and uses it to evaluate the classification performance of both MLP and NHITS models, displaying the percentage of correctly classified binary values for each model.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/16_temporal_classification.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef accuracy(y, y_hat):\n    return np.mean(y==y_hat)\n\nmlp_acc = accuracy(y=Y_hat_df['y'], y_hat=Y_hat_df['MLP'])\nnhits_acc = accuracy(y=Y_hat_df['y'], y_hat=Y_hat_df['NHITS'])\n\nprint(f'MLP Accuracy: {mlp_acc:.1%}')\nprint(f'NHITS Accuracy: {nhits_acc:.1%}')\n```\n\n----------------------------------------\n\nTITLE: Visualizing Predicted vs True RUL Values\nDESCRIPTION: Creates a plot comparing the predicted and true RUL values for three different engines to visualize model performance.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/use-cases/predictive_maintenance.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nplot_df1 = Y_hat_df2[Y_hat_df2['unique_id']==1]\nplot_df2 = Y_hat_df2[Y_hat_df2['unique_id']==2]\nplot_df3 = Y_hat_df2[Y_hat_df2['unique_id']==3]\n\nplt.plot(plot_df1.ds, plot_df1['y'], c='#2D6B8F', label='E1 true RUL')\nplt.plot(plot_df1.ds, plot_df1[model_name]+1, c='#2D6B8F', linestyle='--', label='E1 predicted RUL')\n\nplt.plot(plot_df1.ds, plot_df2['y'], c='#CA6F6A', label='E2 true RUL')\nplt.plot(plot_df1.ds, plot_df2[model_name]+1, c='#CA6F6A', linestyle='--', label='E2 predicted RUL')\n\nplt.plot(plot_df1.ds, plot_df3['y'], c='#D5BC67', label='E3 true RUL')\nplt.plot(plot_df1.ds, plot_df3[model_name]+1, c='#D5BC67', linestyle='--', label='E3 predicted RUL')\n\nplt.legend()\nplt.grid()\n```\n\n----------------------------------------\n\nTITLE: Defining AutoTCN Class for Automated Tuning in Python\nDESCRIPTION: This snippet defines the `AutoTCN` class, which enables automated hyperparameter tuning for the TCN model. It inherits from `BaseAuto`, specifies a default search space (`default_config`) using `ray.tune` primitives, and provides an `__init__` method for configuration and instantiation. The `get_default_config` class method dynamically creates the configuration based on the forecast horizon (`h`) and chosen backend ('ray' or 'optuna'), converting Ray Tune configurations to Optuna format if necessary.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n#| export\nclass AutoTCN(BaseAuto):\n\n    default_config = {\n        \"input_size_multiplier\": [-1, 4, 16, 64],\n        \"inference_input_size_multiplier\": [-1],\n        \"h\": None,\n        \"encoder_hidden_size\": tune.choice([16, 32, 64, 128]),\n        \"context_size\": tune.choice([5, 10, 50]),\n        \"decoder_hidden_size\": tune.choice([32, 64]),\n        \"learning_rate\": tune.loguniform(1e-4, 1e-1),\n        \"max_steps\": tune.choice([500, 1000]),\n        \"batch_size\": tune.choice([16, 32]),\n        \"loss\": None,\n        \"random_seed\": tune.randint(1, 20)\n    }\n\n    def __init__(self,\n                 h,\n                 loss=MAE(),\n                 valid_loss=None,\n                 config=None,\n                 search_alg=BasicVariantGenerator(random_state=1),\n                 num_samples=10,\n                 refit_with_val=False,\n                 cpus=cpu_count(),\n                 gpus=torch.cuda.device_count(),\n                 verbose=False,\n                 alias=None,\n                 backend='ray',\n                 callbacks=None):\n        \n        # Define search space, input/output sizes\n        if config is None:\n            config = self.get_default_config(h=h, backend=backend)         \n\n        super(AutoTCN, self).__init__(\n              cls_model=TCN,\n              h=h,\n              loss=loss,\n              valid_loss=valid_loss,\n              config=config,\n              search_alg=search_alg,\n              num_samples=num_samples,\n              refit_with_val=refit_with_val,\n              cpus=cpus,\n              gpus=gpus,\n              verbose=verbose,\n              alias=alias,\n              backend=backend,\n              callbacks=callbacks,            \n        )\n\n    @classmethod\n    def get_default_config(cls, h, backend, n_series=None):\n        config = cls.default_config.copy()\n        config['input_size'] = tune.choice([h*x \\\n                        for x in config['input_size_multiplier']])\n        config['inference_input_size'] = tune.choice([h*x \\\n                        for x in config['inference_input_size_multiplier']])\n        del config['input_size_multiplier'], config['inference_input_size_multiplier']\n        if backend == 'optuna':\n            config = cls._ray_config_to_optuna(config)         \n\n        return config\n```\n\n----------------------------------------\n\nTITLE: Displaying TimesNet.predict Method Documentation - Python\nDESCRIPTION: This snippet invokes \\\"show_doc\\\" on the TimesNet class's \\\"predict\\\" method and labels the output as \\\"TimesNet.predict\\\". It is used to help users understand the expected arguments and behavior of the prediction API. Dependencies include the existence of the TimesNet class and the \\\"show_doc\\\" utility in scope.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timesnet.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nshow_doc(TimesNet.predict, name='TimesNet.predict')\n```\n\n----------------------------------------\n\nTITLE: Running Model Tests (Disabled Evaluation Code)\nDESCRIPTION: Hidden code block for executing tests against various NeuralForecast models. The block is marked with eval: false to prevent automatic execution as it's a slow test suite.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.model_checks.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#| eval: false\n#| hide\n# Run tests in this file. This is a slow test\nimport warnings\nimport logging\nfrom neuralforecast.models import RNN, GRU, TCN, LSTM, DeepAR, DilatedRNN, BiTCN, MLP, NBEATS, NBEATSx, NHITS, DLinear, NLinear, TiDE, DeepNPTS, TFT, VanillaTransformer, Informer, Autoformer, FEDformer, TimesNet, iTransformer, KAN, RMoK, StemGNN, TSMixer, TSMixerx, MLPMultivariate, SOFTS, TimeMixer\n\nmodels = [RNN, GRU, TCN, LSTM, DeepAR, DilatedRNN, BiTCN, MLP, NBEATS, NBEATSx, NHITS, DLinear, NLinear, TiDE, DeepNPTS, TFT, VanillaTransformer, Informer, Autoformer, FEDformer, TimesNet, iTransformer, KAN, RMoK, StemGNN, TSMixer, TSMixerx, MLPMultivariate, SOFTS, TimeMixer]\n\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\nlogging.getLogger(\"lightning_fabric\").setLevel(logging.ERROR)\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    for model in models:\n        check_model(model, checks=[\"losses\"])\n```\n\n----------------------------------------\n\nTITLE: Displaying TimeSeriesDataset Documentation\nDESCRIPTION: A utility call to show documentation for the TimeSeriesDataset class, likely using a custom documentation display function within the NeuralForecast library.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/tsdataset.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(TimeSeriesDataset)\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for DLinear Model\nDESCRIPTION: Hidden code block that runs unit tests for the DLinear model using the airpassengers dataset. It suppresses logging and warning messages to keep output clean during testing.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.dlinear.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n# Unit tests for models\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\nlogging.getLogger(\"lightning_fabric\").setLevel(logging.ERROR)\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    check_model(DLinear, [\"airpassengers\"])\n```\n\n----------------------------------------\n\nTITLE: Installing Datasetsforecast Library for Time Series Examples\nDESCRIPTION: This code installs the datasetsforecast library which provides access to standard time series datasets for testing and examples. The capture magic command suppresses the installation output.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/getting-started/05_datarequirements.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n!pip install datasetsforecast\n```\n\n----------------------------------------\n\nTITLE: Displaying FEDformer Class Documentation in Python\nDESCRIPTION: This snippet uses the `show_doc` function, likely from a library like `nbdev`, to render and display the documentation associated with the `FEDformer` class. It's typically used within a Jupyter Notebook or similar environment for interactive documentation viewing.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.fedformer.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(FEDformer)\n```\n\n----------------------------------------\n\nTITLE: Viewing Forecast Results\nDESCRIPTION: Displays the first few rows of the forecast DataFrame, which contains predictions for each unique_id, date, and model.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/getting-started/02_quickstart.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nY_hat_df = Y_hat_df\nY_hat_df.head()\n```\n\n----------------------------------------\n\nTITLE: Testing Checkpoint Generation in NeuralForecast Models\nDESCRIPTION: Tests that when enable_checkpointing=True is set, the models properly generate checkpoint files. This validates the model's ability to save state during training.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_46\n\nLANGUAGE: python\nCODE:\n```\n# test `enable_checkpointing=True` should generate chkpt\nshutil.rmtree('lightning_logs')\nfcst = NeuralForecast(\n    models=[\n        MLP(h=12, input_size=12, max_steps=10, val_check_steps=5, enable_checkpointing=True),\n        RNN(h=12, input_size=-1, max_steps=10, val_check_steps=5, enable_checkpointing=True)\n    ],\n    freq='M'\n)\nfcst.fit(AirPassengersPanel_train)\nlast_log = f\"lightning_logs/{os.listdir('lightning_logs')[-1]}\"\nno_chkpt_found = ~np.any([file.endswith('checkpoints') for file in os.listdir(last_log)])\ntest_eq(no_chkpt_found, False)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries and Models for NeuralForecast\nDESCRIPTION: Imports necessary dependencies including PyTorch, Ray for hyperparameter tuning, and various neural network forecasting models from the neuralforecast library.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#| export\nfrom os import cpu_count\nimport torch\n\nfrom ray import tune\nfrom ray.tune.search.basic_variant import BasicVariantGenerator\n\nfrom neuralforecast.common._base_auto import BaseAuto\nfrom neuralforecast.common._base_auto import MockTrial\n\nfrom neuralforecast.models.rnn import RNN\nfrom neuralforecast.models.gru import GRU\nfrom neuralforecast.models.tcn import TCN\nfrom neuralforecast.models.lstm import LSTM\nfrom neuralforecast.models.deepar import DeepAR\nfrom neuralforecast.models.dilated_rnn import DilatedRNN\nfrom neuralforecast.models.bitcn import BiTCN\n\nfrom neuralforecast.models.mlp import MLP\nfrom neuralforecast.models.nbeats import NBEATS\nfrom neuralforecast.models.nbeatsx import NBEATSx\nfrom neuralforecast.models.nhits import NHITS\nfrom neuralforecast.models.dlinear import DLinear\nfrom neuralforecast.models.nlinear import NLinear\nfrom neuralforecast.models.tide import TiDE\nfrom neuralforecast.models.deepnpts import DeepNPTS\n\nfrom neuralforecast.models.tft import TFT\nfrom neuralforecast.models.vanillatransformer import VanillaTransformer\nfrom neuralforecast.models.informer import Informer\nfrom neuralforecast.models.autoformer import Autoformer\nfrom neuralforecast.models.fedformer import FEDformer\nfrom neuralforecast.models.patchtst import PatchTST\nfrom neuralforecast.models.timesnet import TimesNet\nfrom neuralforecast.models.itransformer import iTransformer\nfrom neuralforecast.models.timexer import TimeXer\n\nfrom neuralforecast.models.kan import KAN\nfrom neuralforecast.models.rmok import RMoK\n\nfrom neuralforecast.models.stemgnn import StemGNN\nfrom neuralforecast.models.hint import HINT\nfrom neuralforecast.models.tsmixer import TSMixer\nfrom neuralforecast.models.tsmixerx import TSMixerx\nfrom neuralforecast.models.mlpmultivariate import MLPMultivariate\nfrom neuralforecast.models.softs import SOFTS\nfrom neuralforecast.models.timemixer import TimeMixer\n\nfrom neuralforecast.losses.pytorch import MAE, MQLoss, DistributionLoss\n```\n\n----------------------------------------\n\nTITLE: MLP Decoder Implementation\nDESCRIPTION: Implements a Multi-Layer Perceptron Decoder class with configurable layers and dimensions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.deepar.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass Decoder(nn.Module):\n    \"\"\"Multi-Layer Perceptron Decoder\n\n    **Parameters:**<br>\n    `in_features`: int, dimension of input.<br>\n    `out_features`: int, dimension of output.<br>\n    `hidden_size`: int, dimension of hidden layers.<br>\n    `num_layers`: int, number of hidden layers.<br>\n    \"\"\"\n\n    def __init__(self, in_features, out_features, hidden_size, hidden_layers):\n        super().__init__()\n\n        if hidden_layers == 0:\n            # Input layer\n            layers = [nn.Linear(in_features=in_features, out_features=out_features)]\n        else:\n            # Input layer\n            layers = [nn.Linear(in_features=in_features, out_features=hidden_size), nn.ReLU()]\n            # Hidden layers\n            for i in range(hidden_layers - 2):\n                layers += [nn.Linear(in_features=hidden_size, out_features=hidden_size), nn.ReLU()]\n            # Output layer\n            layers += [nn.Linear(in_features=hidden_size, out_features=out_features)]\n\n        # Store in layers as ModuleList\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.layers(x)\n```\n\n----------------------------------------\n\nTITLE: Defining Hyperparameter Search Spaces for TSMixer and TSMixerx in Python\nDESCRIPTION: This code defines the hyperparameter search spaces for TSMixer and TSMixerx models, including parameters like input size, max steps, learning rate, and dropout.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/08_multivariate_tsmixer.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nhorizon = 96 # 24hrs = 4 * 15 min.\n\ntsmixer_config = {\n       \"input_size\": input_size,                                                 # Size of input window\n       \"max_steps\": tune.choice([500, 1000, 2000]),                              # Number of training iterations\n       \"val_check_steps\": 100,                                                   # Compute validation every x steps\n       \"early_stop_patience_steps\": 5,                                           # Early stopping steps\n       \"learning_rate\": tune.loguniform(1e-4, 1e-2),                             # Initial Learning rate\n       \"n_block\": tune.choice([1, 2, 4, 6, 8]),                                  # Number of mixing layers\n       \"dropout\": tune.uniform(0.0, 0.99),                                       # Dropout\n       \"ff_dim\": tune.choice([32, 64, 128]),                                     # Dimension of the feature linear layer\n       \"scaler_type\": 'identity',       \n    }\n\ntsmixerx_config = tsmixer_config.copy()\ntsmixerx_config['futr_exog_list'] = ['ex_1', 'ex_2', 'ex_3', 'ex_4']\n```\n\n----------------------------------------\n\nTITLE: Initializing NeuralForecast with AutoNHITS Model\nDESCRIPTION: Creates a NeuralForecast object with the configured AutoNHITS model and hourly frequency for electricity demand forecasting.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/use-cases/electricity_peak_forecasting.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Instantiate StatsForecast class as sf\nnf = NeuralForecast(\n    models=models,\n    freq='h', \n)\n```\n\n----------------------------------------\n\nTITLE: Testing AutoDeepNPTS with Different Configurations\nDESCRIPTION: Unit tests for AutoDeepNPTS model with different backend configurations, including Optuna with updated default config and Ray with updated default config. Also includes assertion tests for model configuration.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_60\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n# Check Optuna\nassert model.config(MockTrial())['h'] == 12\n\n# Unit test to test that Auto* model contains all required arguments from BaseAuto\ntest_args(AutoDeepNPTS, exclude_args=['cls_model']) \n\n# Unit test for situation: Optuna with updated default config\nmy_config = AutoDeepNPTS.get_default_config(h=12, backend='optuna')\ndef my_config_new(trial):\n    config = {**my_config(trial)}\n    config.update({'max_steps': 2, 'val_check_steps': 1, 'input_size': 12})\n    return config\n\nmodel = AutoDeepNPTS(h=12, config=my_config_new, backend='optuna', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n\n# Unit test for situation: Ray with updated default config\nmy_config = AutoDeepNPTS.get_default_config(h=12, backend='ray')\nmy_config['max_steps'] = 2\nmy_config['val_check_steps'] = 1\nmy_config['input_size'] = 12\nmodel = AutoDeepNPTS(h=12, config=my_config, backend='ray', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies\nDESCRIPTION: Imports required Python libraries and custom modules for TimeXer implementation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timexer.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom neuralforecast.losses.pytorch import MAE\nfrom neuralforecast.common._base_model import BaseModel\nfrom neuralforecast.common._modules import (\n    DataEmbedding_inverted, \n    PositionalEmbedding,\n    FullAttention,\n    AttentionLayer\n)\nfrom typing import Optional\n```\n\n----------------------------------------\n\nTITLE: Importing Required Dependencies for AutoNHITS with HyperOpt\nDESCRIPTION: Importing HyperOptSearch from Ray Tune, Mean Absolute Error (MAE) loss function from NeuralForecast, and AutoNHITS for hyperparameter optimization.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/04_hyperparameter_tuning.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom ray.tune.search.hyperopt import HyperOptSearch\nfrom neuralforecast.losses.pytorch import MAE\nfrom neuralforecast.auto import AutoNHITS\n```\n\n----------------------------------------\n\nTITLE: Visualizing Normalized Month Features in Time Series Data with Python\nDESCRIPTION: Creates a plot showing the normalized month feature for a specific airline over time. The plot demonstrates how the month feature is normalized to values between -0.5 and 0.5, creating a cyclical pattern that can be used by time series models.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/utils.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nplot_df = AirPassengerPanelCalendar[AirPassengerPanelCalendar.unique_id=='Airline1'].set_index('ds')\nplt.plot(plot_df['month'])\nplt.grid()\nplt.xlabel('Datestamp')\nplt.ylabel('Normalized Month')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Generating Documentation for DilatedRNN.fit Method (Python)\nDESCRIPTION: Calls the `show_doc` function to specifically generate and display documentation for the `fit` method of the `DilatedRNN` class. The `name` parameter ensures the documentation is correctly attributed to `DilatedRNN.fit`. This requires the `DilatedRNN` class and its `fit` method to be defined, and the `show_doc` function to be available.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.dilated_rnn.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(DilatedRNN.fit, name='DilatedRNN.fit')\n```\n\n----------------------------------------\n\nTITLE: Plotting Forecast Results\nDESCRIPTION: Visualizes the forecasting results using plot_series function to compare predictions from both models.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/21_configure_optimizers.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nplot_series(AirPassengersPanel_train, preds)\n```\n\n----------------------------------------\n\nTITLE: Loading Electricity Price Forecasting Data in Python\nDESCRIPTION: Loads the electricity price forecasting datasets from S3, including historical data for training and future exogenous variables for forecasting. The data contains market identifiers, timestamps, electricity prices, and forecasted generation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/07_time_series_scaling.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndf = pd.read_csv(\n    'https://datasets-nixtla.s3.amazonaws.com/EPF_FR_BE.csv',\n    parse_dates=['ds'],\n)\nfutr_df = pd.read_csv(\n    'https://datasets-nixtla.s3.amazonaws.com/EPF_FR_BE_futr.csv',\n    parse_dates=['ds'],\n)\ndf.head()\n```\n\n----------------------------------------\n\nTITLE: Generating Model Names with Output Variables in NeuralForecast\nDESCRIPTION: Creates unique names for each model in the ensemble, handling duplicate models by adding identifiers. Manages special cases for models with multiple outputs and specific loss functions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef _get_model_names(self, add_level=False) -> List[str]:\n    names: List[str] = []\n    count_names = {'model': 0}\n    for model in self.models:\n        model_name = repr(model)\n        count_names[model_name] = count_names.get(model_name, -1) + 1\n        if count_names[model_name] > 0:\n            model_name += str(count_names[model_name])\n\n        if add_level and (model.loss.outputsize_multiplier > 1 or isinstance(model.loss, (IQLoss, HuberIQLoss))):\n            continue\n\n        names.extend(model_name + n for n in model.loss.output_names)\n    return names\n```\n\n----------------------------------------\n\nTITLE: Viewing the AirPassengers DataFrame\nDESCRIPTION: Displays the first few rows of the AirPassengers DataFrame to show its structure with the required columns: unique_id, ds, and y.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/getting-started/02_quickstart.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nY_df = AirPassengersDF\nY_df.head()\n```\n\n----------------------------------------\n\nTITLE: Importing Plotting Utilities for Time Series in Python\nDESCRIPTION: Imports the plot_series function from utilsforecast.plotting for visualizing the time series data and forecast results.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/07_time_series_scaling.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom utilsforecast.plotting import plot_series\n```\n\n----------------------------------------\n\nTITLE: Placeholder for Identity Reconciliation Matrix in Python\nDESCRIPTION: Defines an empty function as a placeholder for implementing an identity reconciliation matrix (no reconciliation).\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.hint.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef get_identity_P(S: np.ndarray):\n    # Placeholder function for identity P (no reconciliation).\n    pass\n```\n\n----------------------------------------\n\nTITLE: Setting Number of Peaks for Detection\nDESCRIPTION: Defines the number of peaks to detect in the electricity demand forecast.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/use-cases/electricity_peak_forecasting.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nnpeaks = 1 # Number of peaks\n```\n\n----------------------------------------\n\nTITLE: Converting PyTorch Tensor to NumPy Array in Python\nDESCRIPTION: Defines a utility function to convert a PyTorch tensor to a NumPy array, handling bfloat16 dtype conversion.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef tensor_to_numpy(tensor: torch.Tensor) -> np.ndarray:\n    \"\"\"Convert a tensor to numpy\"\"\"\n    if tensor.dtype == torch.bfloat16:\n        return tensor.float().numpy()\n    \n    return tensor.numpy()\n```\n\n----------------------------------------\n\nTITLE: Displaying Method Documentation for KAN.fit in NeuralForecast Python\nDESCRIPTION: Displays inline documentation for the KAN.fit method using the show_doc utility in Python. This is useful for interactively exploring or generating documentation for model training routines. The only dependency is access to show_doc and the KAN class implementation. It takes the KAN.fit class method and shows its docstring.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.kan.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(KAN.fit, name='KAN.fit')\n```\n\n----------------------------------------\n\nTITLE: Testing Learning Rate Scheduler Validation\nDESCRIPTION: Tests validation of learning rate scheduler parameter across different model types. Verifies that passing invalid lr_scheduler raises appropriate error message.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_65\n\nLANGUAGE: python\nCODE:\n```\ntest_fail(lambda: NHITS(h=12, input_size=24, max_steps=10, lr_scheduler=torch.nn.Module), contains=\"lr_scheduler is not a valid subclass of torch.optim.lr_scheduler.LRScheduler\")\ntest_fail(lambda: RNN(h=12, input_size=24, max_steps=10, lr_scheduler=torch.nn.Module), contains=\"lr_scheduler is not a valid subclass of torch.optim.lr_scheduler.LRScheduler\")\ntest_fail(lambda: StemGNN(h=12, input_size=24, max_steps=10, n_series=2, lr_scheduler=torch.nn.Module), contains=\"lr_scheduler is not a valid subclass of torch.optim.lr_scheduler.LRScheduler\")\n```\n\n----------------------------------------\n\nTITLE: Preparing Dataset with Clipped RUL Values\nDESCRIPTION: Loads the PHM2008 dataset with clipped RUL values and prepares the test set by aligning timestamps with the training set.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/use-cases/predictive_maintenance.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nY_train_df, Y_test_df = PHM2008.load(directory='./data', group='FD001', clip_rul=True)\nmax_ds = Y_train_df.groupby('unique_id')[\"ds\"].max()\nY_test_df = Y_test_df.merge(max_ds, on='unique_id', how='left', suffixes=('', '_train_max_date'))\nY_test_df[\"ds\"] = Y_test_df[\"ds\"] + Y_test_df[\"ds_train_max_date\"]\nY_test_df = Y_test_df.drop(columns=[\"ds_train_max_date\"])\n```\n\n----------------------------------------\n\nTITLE: Configuring AutoNBEATS Hyperparameters\nDESCRIPTION: Defines the hyperparameter search space for AutoNBEATS model using Ray Tune's configuration options.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/experiments/nbeats_basis/nbeats_basis_experiment.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nnbeats_config = {\n   \"max_steps\": 100,\n   \"input_size\": tune.choice([192, 384]),\n   \"basis\": tune.choice(['legendre', 'polynomial', 'changepoint', 'piecewise_linear', 'linear_hat', 'spline', 'chebyshev']),\n   \"n_basis\": 5,\n   \"random_seed\": tune.randint(1, 10),\n}\n```\n\n----------------------------------------\n\nTITLE: Forward Propagation Method for NBEATSx Neural Network in PyTorch\nDESCRIPTION: Implementation of the forward pass for the NBEATSx model. It processes input windows batch, handles exogenous variables, and produces forecasts with optional decomposition of the forecast into individual block contributions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeatsx.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, windows_batch):\n    # Parse windows_batch\n    insample_y = windows_batch[\"insample_y\"].squeeze(-1)\n    insample_mask = windows_batch[\"insample_mask\"].squeeze(-1)\n    futr_exog = windows_batch[\"futr_exog\"]\n    hist_exog = windows_batch[\"hist_exog\"]\n    stat_exog = windows_batch[\"stat_exog\"]\n\n    # NBEATSx' forward\n    residuals = insample_y.flip(dims=(-1,))  # backcast init\n    insample_mask = insample_mask.flip(dims=(-1,))\n\n    forecast = insample_y[:, -1:, None]  # Level with Naive1\n    block_forecasts = [forecast.repeat(1, self.h, 1)]\n    for i, block in enumerate(self.blocks):\n        backcast, block_forecast = block(\n            insample_y=residuals,\n            futr_exog=futr_exog,\n            hist_exog=hist_exog,\n            stat_exog=stat_exog,\n        )\n        residuals = (residuals - backcast) * insample_mask\n        forecast = forecast + block_forecast\n\n        if self.decompose_forecast:\n            block_forecasts.append(block_forecast)\n\n    if self.decompose_forecast:\n        # (n_batch, n_blocks, h)\n        block_forecasts = torch.stack(block_forecasts)\n        block_forecasts = block_forecasts.permute(1, 0, 2, 3)\n        block_forecasts = block_forecasts.squeeze(-1)  # univariate output\n        return block_forecasts\n    else:\n        return forecast\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Vanilla Transformer in Python\nDESCRIPTION: This snippet imports necessary libraries and modules for implementing the Vanilla Transformer model. It includes NumPy for numerical operations, PyTorch for deep learning, and custom modules from neuralforecast for specific transformer components.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.vanillatransformer.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\n\nfrom neuralforecast.common._modules import (\n    TransEncoderLayer, TransEncoder,\n    TransDecoderLayer, TransDecoder,\n    DataEmbedding, AttentionLayer, FullAttention\n)\nfrom neuralforecast.common._base_model import BaseModel\n\nfrom neuralforecast.losses.pytorch import MAE\n```\n\n----------------------------------------\n\nTITLE: Importing NeuralForecast and AutoNHITS Model\nDESCRIPTION: Imports the NeuralForecast core class and the AutoNHITS model for time series forecasting.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/use-cases/electricity_peak_forecasting.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom neuralforecast.core import NeuralForecast\nfrom neuralforecast.auto import AutoNHITS\n```\n\n----------------------------------------\n\nTITLE: Unit Testing and Error Suppression for TFT Model - PyTorch Lightning - Python\nDESCRIPTION: Runs unit tests for the TFT model, suppressing warnings and setting log levels for torch Lightning and Lightning Fabric. Uses a test harness to validate the TFT model on example data, ensuring correctness and integration compliance. Dependencies include pytest-like check_model function, warnings, and logging modules; affects logging and does not produce outputs unless tests fail.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n#| hide\\n# Unit tests for models\\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\\nlogging.getLogger(\"lightning_fabric\").setLevel(logging.ERROR)\\nwith warnings.catch_warnings():\\n    warnings.simplefilter(\"ignore\")\\n    check_model(TFT, [\"airpassengers\"])\n```\n\n----------------------------------------\n\nTITLE: Viewing Sample Rows from Multiple Time Series\nDESCRIPTION: Shows the first two rows of each unique time series in the dataset using pandas groupby functionality, demonstrating the structure of multiple time series in long format.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/getting-started/05_datarequirements.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nY_df.groupby('unique_id').head(2)\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for HINT Implementation in Python\nDESCRIPTION: Imports required libraries and modules for implementing HINT, including typing, numpy, and torch.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.hint.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\n\nimport numpy as np\nimport torch\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging Level\nDESCRIPTION: Sets the PyTorch Lightning logging level to ERROR to reduce verbose output during model training.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/use-cases/predictive_maintenance.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n```\n\n----------------------------------------\n\nTITLE: Documenting the feature_importances Method for TFT Model - Python\nDESCRIPTION: Invokes show_doc to render documentation for the feature_importances method of TFT, aiding users to understand its API and parameterization. Only metadata changes are enacted; method must exist and be importable. Supports doc generation frameworks and interactive browsing.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(TFT.feature_importances, name='TFT.feature_importances,', title_level=3)\n```\n\n----------------------------------------\n\nTITLE: Testing In-Sample Times Generation\nDESCRIPTION: Test code for verifying the correct generation of in-sample times across different frequencies and step sizes using pandas date ranges.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nuids = pd.Series(['id_0', 'id_1'])\nindptr = np.array([0, 4, 10], dtype=np.int32)\nh = 2\nfor step_size, freq, days in zip([1, 2], ['D', 'W-THU'], [1, 14]):\n```\n\n----------------------------------------\n\nTITLE: Plotting Time Series Forecasts with Window-Normalized Data in Python\nDESCRIPTION: Visualizes the original time series data and the forecasts generated with temporal window normalization, showing a limited window of historical data for clarity.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/07_time_series_scaling.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nplot_series(df, Y_hat_df, max_insample_length=24*5)\n```\n\n----------------------------------------\n\nTITLE: Unit Tests for DeepAR Model\nDESCRIPTION: This code snippet contains unit tests for the DeepAR model. It sets up logging configuration and runs tests on the model using the AirPassengers dataset while suppressing warnings.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.deepar.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n# Unit tests for models\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\nlogging.getLogger(\"lightning_fabric\").setLevel(logging.ERROR)\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    check_model(DeepAR, [\"airpassengers\"])\n```\n\n----------------------------------------\n\nTITLE: Initializing Jupyter Notebook Environment for Model Testing\nDESCRIPTION: Sets up the notebook environment with cell magic and autoreload extension for development purposes.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.model_checks.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#| default_exp common._model_checks\n```\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Importing PyTorch and Type Dependencies\nDESCRIPTION: Imports PyTorch modules and typing utilities needed for the RNN model implementation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.rnn.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#| export\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\nimport warnings\n\nfrom neuralforecast.losses.pytorch import MAE\nfrom neuralforecast.common._base_model import BaseModel\nfrom neuralforecast.common._modules import MLP\n```\n\n----------------------------------------\n\nTITLE: Displaying HuberMQLoss Initialization Documentation\nDESCRIPTION: Code to display the documentation for the HuberMQLoss class initialization method using a documentation display function.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_72\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(HuberMQLoss, name='HuberMQLoss.__init__', title_level=3)\n```\n\n----------------------------------------\n\nTITLE: Displaying Documentation for AutoAutoformer Class\nDESCRIPTION: This snippet generates and displays documentation for the AutoAutoformer class. It uses the show_doc function to create a documentation view with a level 3 header.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_77\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(AutoAutoformer, title_level=3)\n```\n\n----------------------------------------\n\nTITLE: Fitting NeuralForecast Model with Time Series Scaling in Python\nDESCRIPTION: Creates a NeuralForecast object with standard scaling applied at the time series level. The scaling is performed on each time series individually before training and the scaling statistics are stored for later inverse transformation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/07_time_series_scaling.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nnf = NeuralForecast(models=[model], freq='h', local_scaler_type='standard')\nnf.fit(df=df)\n```\n\n----------------------------------------\n\nTITLE: Configuring Jupyter Notebook for iTransformer Development\nDESCRIPTION: These cells set up the Jupyter notebook environment for developing the iTransformer model. They load the autoreload extension and import necessary modules for testing and documentation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.itransformer.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#| default_exp models.itransformer\n```\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n%load_ext autoreload\n%autoreload 2\n```\n\nLANGUAGE: python\nCODE:\n```\n#| hide\nimport logging\nimport warnings\nfrom fastcore.test import test_eq\nfrom nbdev.showdoc import show_doc\nfrom neuralforecast.common._model_checks import check_model\n```\n\n----------------------------------------\n\nTITLE: Testing Disabled Checkpoint Generation in NeuralForecast Models\nDESCRIPTION: Verifies that when enable_checkpointing is not explicitly set (defaults to False), no checkpoint files are generated during training, which is the expected default behavior.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_47\n\nLANGUAGE: python\nCODE:\n```\n# test `enable_checkpointing=False` should not generate chkpt\nshutil.rmtree('lightning_logs')\nfcst = NeuralForecast(\n    models=[\n        MLP(h=12, input_size=12, max_steps=10, val_check_steps=5),\n        RNN(h=12, input_size=-1, max_steps=10, val_check_steps=5)\n    ],\n    freq='M'\n)\nfcst.fit(AirPassengersPanel_train)\nlast_log = f\"lightning_logs/{os.listdir('lightning_logs')[-1]}\"\nno_chkpt_found = ~np.any([file.endswith('checkpoints') for file in os.listdir(last_log)])\ntest_eq(no_chkpt_found, True)\n```\n\n----------------------------------------\n\nTITLE: Plotting Forecast Results for the OT Variable\nDESCRIPTION: Creates a plot comparing the actual values of the OT variable with the forecasts from Informer, Autoformer, and PatchTST models.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/05_longhorizon_transformers.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nY_plot = Y_hat_df[Y_hat_df['unique_id']=='OT'] # OT dataset\ncutoffs = Y_hat_df['cutoff'].unique()[::horizon]\nY_plot = Y_plot[Y_hat_df['cutoff'].isin(cutoffs)]\n\nplt.figure(figsize=(20,5))\nplt.plot(Y_plot['ds'], Y_plot['y'], label='True')\nplt.plot(Y_plot['ds'], Y_plot['Informer'], label='Informer')\nplt.plot(Y_plot['ds'], Y_plot['Autoformer'], label='Autoformer')\nplt.plot(Y_plot['ds'], Y_plot['PatchTST'], label='PatchTST')\nplt.xlabel('Datestamp')\nplt.ylabel('OT')\nplt.grid()\nplt.legend()\n```\n\n----------------------------------------\n\nTITLE: Saving PyTorch Neural Forecasting Model\nDESCRIPTION: Serializes the model's hyperparameters and state dictionary to disk using fsspec for filesystem abstraction, allowing for cloud storage compatibility.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef save(self, path):\n    with fsspec.open(path, 'wb') as f:\n        torch.save(\n            {'hyper_parameters': self.hparams, 'state_dict': self.state_dict()},\n            f,\n        )\n```\n\n----------------------------------------\n\nTITLE: Importing Packages and Loading Hierarchical Dataset in Python\nDESCRIPTION: This snippet shows the import statements for required packages and the loading of the TourismLarge dataset. It also includes a helper function for sorting the dataframe hierarchically.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.hint.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast.losses.pytorch import GMM, sCRPS\nfrom datasetsforecast.hierarchical import HierarchicalData\n\n# Auxiliary sorting\ndef sort_df_hier(Y_df, S_df):\n    # NeuralForecast core, sorts unique_id lexicographically\n    # by default, this class matches S_df and Y_hat_df order.    \n    Y_df.unique_id = Y_df.unique_id.astype('category')\n    Y_df.unique_id = Y_df.unique_id.cat.set_categories(S_df.index)\n    Y_df = Y_df.sort_values(by=['unique_id', 'ds'])\n    return Y_df\n\n# Load TourismSmall dataset\nhorizon = 12\nY_df, S_df, tags = HierarchicalData.load('./data', 'TourismLarge')\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\nY_df = sort_df_hier(Y_df, S_df)\nlevel = [80,90]\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Time-LLM in Python\nDESCRIPTION: Imports necessary libraries and modules for the Time-LLM implementation, including PyTorch, neuralforecast, and transformers. It also sets up autoreload for IPython environment.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timellm.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#| default_exp models.timellm\n```\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n\n%load_ext autoreload\n%autoreload 2\n```\n\nLANGUAGE: python\nCODE:\n```\n#| export\nimport math\nfrom typing import Optional\n\nimport neuralforecast.losses.pytorch as losses\nimport torch\nimport torch.nn as nn\n\nfrom neuralforecast.common._base_model import BaseModel\nfrom neuralforecast.common._modules import RevIN\nfrom neuralforecast.losses.pytorch import MAE\n\ntry:\n    from transformers import AutoModel, AutoTokenizer, AutoConfig\n    IS_TRANSFORMERS_INSTALLED = True\nexcept ImportError:\n    IS_TRANSFORMERS_INSTALLED = False\n\nimport warnings\n```\n\nLANGUAGE: python\nCODE:\n```\n#| hide\nimport logging\nimport warnings\nfrom fastcore.test import test_eq\nfrom nbdev.showdoc import show_doc\nfrom neuralforecast.common._model_checks import check_model\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for DeepAR Model\nDESCRIPTION: Imports required PyTorch modules and custom neuralforecast components for model implementation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.deepar.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\n\nfrom typing import Optional\n\nfrom neuralforecast.common._base_model import BaseModel\nfrom neuralforecast.losses.pytorch import DistributionLoss, MAE\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for TimeMixer Model in Python\nDESCRIPTION: This code snippet configures logging levels for PyTorch Lightning and Lightning Fabric to ERROR, effectively reducing console output during testing. It then suppresses warnings using `warnings.catch_warnings` and executes a model check (likely a form of unit or integration test) using the `check_model` function on the `TimeMixer` class with the 'airpassengers' dataset configuration. Requires `logging`, `warnings`, `check_model` function, and the `TimeMixer` class.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timemixer.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n# Unit tests for models\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\nlogging.getLogger(\"lightning_fabric\").setLevel(logging.ERROR)\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    check_model(TimeMixer, [\"airpassengers\"])\n```\n\n----------------------------------------\n\nTITLE: Displaying TSMixer.predict Method Documentation in Python\nDESCRIPTION: Command to display documentation for the predict method of the TSMixer class, which would include information on generating forecasts with a trained model.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tsmixer.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(TSMixer.predict, name='TSMixer.predict')\n```\n\n----------------------------------------\n\nTITLE: Displaying HuberMQLoss Call Method Documentation\nDESCRIPTION: Code to display the documentation for the HuberMQLoss.__call__ method using a documentation display function.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_73\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(HuberMQLoss.__call__, name='HuberMQLoss.__call__', title_level=3)\n```\n\n----------------------------------------\n\nTITLE: Creating Parquet File Structure with Spark for NeuralForecast\nDESCRIPTION: This snippet demonstrates how to create the required directory structure for large datasets using a Spark DataFrame, partitioning by ID and sorting within partitions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/19_large_datasets.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nspark.conf.set(\"spark.sql.parquet.outputTimestampType\", \"TIMESTAMP_MICROS\")\n(\n  spark_df\n  .repartition(id_col)\n  .sortWithinPartitions(id_col, time_col)\n  .write\n  .partitionBy(id_col)\n  .parquet(out_dir)\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring Default Hyperparameters for AutoNHITS in Python\nDESCRIPTION: This code extracts the default hyperparameter configuration for AutoNHITS and modifies specific parameters. It demonstrates how to customize the search space while keeping other default values.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/04_hyperparameter_tuning.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nnhits_default_config = AutoNHITS.get_default_config(h = 12, backend=\"optuna\")                   # Extract the default hyperparameter settings\n\ndef config_nhits(trial):\n    config = {**nhits_default_config(trial)}\n    config.update({\n        \"random_seed\": trial.suggest_int(\"random_seed\", 1, 10), \n        \"n_pool_kernel_size\": trial.suggest_categorical(\"n_pool_kernel_size\", [[2, 2, 2], [16, 8, 1]])\n    })\n    return config\n```\n\n----------------------------------------\n\nTITLE: Setting PyTorch Environment Variable in Python\nDESCRIPTION: Sets an environment variable to enable MPS fallback for PyTorch.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%set_env PYTORCH_ENABLE_MPS_FALLBACK=1\n```\n\n----------------------------------------\n\nTITLE: Creating Conda Environment\nDESCRIPTION: Command to create a new conda environment using the provided environment.yml configuration file.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/getting-started/04_installation.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nconda env create -f environment.yml\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging and Warnings\nDESCRIPTION: Sets up logging configuration and warning filters for PyTorch Lightning and testing utilities.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.deepnpts.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport warnings\n\nfrom fastcore.test import test_eq\nfrom nbdev.showdoc import show_doc\nfrom neuralforecast.common._model_checks import check_model\n```\n\n----------------------------------------\n\nTITLE: Loading AirPassengers Dataset\nDESCRIPTION: Imports the pre-processed AirPassengers dataset from neuralforecast utils\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/06_save_load_models.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom neuralforecast.utils import AirPassengersDF\n```\n\n----------------------------------------\n\nTITLE: Displaying Documentation for CausalConv1d Class\nDESCRIPTION: Calls the nbdev documentation display function to show documentation for the CausalConv1d class in the notebook.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.modules.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(CausalConv1d, title_level=3)\n```\n\n----------------------------------------\n\nTITLE: Preparing AirPassengers Dataset\nDESCRIPTION: Splits the AirPassengers dataset into training and test sets, with data preparation for time series forecasting.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/21_configure_optimizers.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nAirPassengersPanel_train = AirPassengersPanel[AirPassengersPanel['ds'] < AirPassengersPanel['ds'].values[-12]].reset_index(drop=True)\nAirPassengersPanel_test = AirPassengersPanel[AirPassengersPanel['ds'] >= AirPassengersPanel['ds'].values[-12]].reset_index(drop=True)\nAirPassengersPanel_test['y'] = np.nan\nAirPassengersPanel_test['y_[lag12]'] = np.nan\n```\n\n----------------------------------------\n\nTITLE: Importing Core PyTorch and NeuralForecast Dependencies\nDESCRIPTION: Imports essential libraries for implementing neural network models, including typing utilities, PyTorch's nn module, and custom components from the NeuralForecast library like loss functions and base model classes.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.dilated_rnn.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#| export\nfrom typing import List, Optional\n\nimport torch\nimport torch.nn as nn\n\nfrom neuralforecast.losses.pytorch import MAE\nfrom neuralforecast.common._base_model import BaseModel\nfrom neuralforecast.common._modules import MLP\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for RMoK Model\nDESCRIPTION: Hidden code block that runs unit tests for the RMoK model using the 'airpassengers' dataset, with logging and warnings suppressed to reduce output noise.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.rmok.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n# Unit tests for models\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\nlogging.getLogger(\"lightning_fabric\").setLevel(logging.ERROR)\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    check_model(RMoK, [\"airpassengers\"])\n```\n\n----------------------------------------\n\nTITLE: Implementing Masked Mean Function\nDESCRIPTION: Function to compute mean of tensor along specified dimension while handling masked values. Supports custom dimension selection and dimension preservation options.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.scalers.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef masked_mean(x, mask, dim=-1, keepdim=True):\n    \"\"\" Masked  Mean\n\n    Compute the mean of tensor `x` along dimension, ignoring values where \n    `mask` is False. `x` and `mask` need to be broadcastable.\n\n    **Parameters:**<br>\n    `x`: torch.Tensor to compute mean of along `dim` dimension.<br>\n    `mask`: torch Tensor bool with same shape as `x`, where `x` is valid and False\n            where `x` should be masked. Mask should not be all False in any column of\n            dimension dim to avoid NaNs from zero division.<br>\n    `dim` (int, optional): Dimension to take mean of. Defaults to -1.<br>\n    `keepdim` (bool, optional): Keep dimension of `x` or not. Defaults to True.<br>\n\n    **Returns:**<br>\n    `x_mean`: torch.Tensor with normalized values.\n    \"\"\"\n    x_nan = x.masked_fill(mask<1, float(\"nan\"))\n    x_mean = x_nan.nanmean(dim=dim, keepdim=keepdim)\n    x_mean = torch.nan_to_num(x_mean, nan=0.0)\n    return x_mean\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Hyperparameter Tuning in Python\nDESCRIPTION: This snippet imports the necessary libraries for hyperparameter tuning, including Ray Tune and the AutoTSMixer classes from neuralforecast.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/08_multivariate_tsmixer.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import tune\nfrom ray.tune.search.hyperopt import HyperOptSearch\nfrom neuralforecast.auto import AutoTSMixer, AutoTSMixerx\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for TFT Implementation in Python\nDESCRIPTION: Imports necessary libraries and modules for implementing the Temporal Fusion Transformer model.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Callable, Optional, Tuple\n\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch import Tensor\nfrom torch.nn import LayerNorm\nfrom neuralforecast.losses.pytorch import MAE\nfrom neuralforecast.common._base_model import BaseModel\n```\n\n----------------------------------------\n\nTITLE: GRU Model Usage Example with AirPassengers Dataset\nDESCRIPTION: Demonstration of how to use the GRU model for time series forecasting using the AirPassengers dataset. Shows model initialization, training, and visualization of predictions with confidence intervals.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.gru.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.losses.pytorch import DistributionLoss\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic\nY_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\nY_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\nfcst = NeuralForecast(\n    models=[GRU(h=12, input_size=24,\n                loss=DistributionLoss(distribution='Normal', level=[80, 90]),\n                scaler_type='robust',\n                encoder_n_layers=2,\n                encoder_hidden_size=128,\n                decoder_hidden_size=128,\n                decoder_layers=2,\n                max_steps=200,\n                futr_exog_list=None,\n                hist_exog_list=['y_[lag12]'],\n                stat_exog_list=['airline1'],\n                )\n    ],\n    freq='ME'\n)\nfcst.fit(df=Y_train_df, static_df=AirPassengersStatic)\nforecasts = fcst.predict(futr_df=Y_test_df)\n\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['GRU-median'], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'][-12:], \n                 y1=plot_df['GRU-lo-90'][-12:].values, \n                 y2=plot_df['GRU-hi-90'][-12:].values,\n                 alpha=0.4, label='level 90')\nplt.legend()\nplt.grid()\nplt.plot()\n```\n\n----------------------------------------\n\nTITLE: Installing NeuralForecast using pip\nDESCRIPTION: Command to install the NeuralForecast library using pip package manager. This is the recommended installation method for most users.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/README.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\npip install neuralforecast\n```\n\n----------------------------------------\n\nTITLE: Tail Distribution CDF Calculation in PyTorch\nDESCRIPTION: Method to compute the inverse of the quantile function in the tail regions of the distribution. It calculates the quantile level alpha_tilde corresponding to an observation z in either the left or right tail region using the tail parameters.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_28\n\nLANGUAGE: Python\nCODE:\n```\ndef cdf_tail(\n    self, z: torch.Tensor, left_tail: bool = True\n) -> torch.Tensor:\n    r\"\"\"\n    Computes the quantile level alpha_tilde such that\n    alpha_tilde\n    = q^{-1}(z) if z is in the tail region\n    = qk_x_l or qk_x_r if z is in the non-tail region\n    Parameters\n    ----------\n    z\n        Observation, shape = (*batch_shape,)\n    left_tail\n        If True, compute alpha_tilde for the left tail\n        Otherwise, compute alpha_tilde for the right tail\n    Returns\n    -------\n    alpha_tilde\n        Corresponding quantile level, shape = (*batch_shape,)\n    \"\"\"\n\n    if left_tail:\n        tail_a, tail_b, qk_x = self.tail_al, self.tail_bl, self.qk_x_l\n    else:\n        tail_a, tail_b, qk_x = self.tail_ar, self.tail_br, 1 - self.qk_x_r\n\n    log_alpha_tilde = torch.minimum((z - tail_b) / tail_a, torch.log(qk_x))\n    alpha_tilde = torch.exp(log_alpha_tilde)\n    return alpha_tilde if left_tail else 1 - alpha_tilde\n```\n\n----------------------------------------\n\nTITLE: Importing Core Dependencies for Neural Network Modules\nDESCRIPTION: Imports essential libraries including PyTorch for neural network construction, numpy for numerical operations, and math for mathematical functions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.modules.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#| export\nimport math\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n```\n\n----------------------------------------\n\nTITLE: Loading AirPassengers Dataset\nDESCRIPTION: Imports and loads the AirPassengers dataset from NeuralForecast utils, which is formatted with the required columns: unique_id, ds, and y.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/getting-started/02_quickstart.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom neuralforecast.utils import AirPassengersDF\n```\n\n----------------------------------------\n\nTITLE: Displaying HuberQLoss Initialization Documentation\nDESCRIPTION: Code to display the documentation for the HuberQLoss class initialization method using a documentation display function.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_69\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(HuberQLoss, name='HuberQLoss.__init__', title_level=3)\n```\n\n----------------------------------------\n\nTITLE: Generating Documentation for DilatedRNN.predict Method (Python)\nDESCRIPTION: Calls the `show_doc` function to specifically generate and display documentation for the `predict` method of the `DilatedRNN` class. The `name` parameter ensures the documentation is correctly attributed to `DilatedRNN.predict`. This requires the `DilatedRNN` class and its `predict` method to be defined, and the `show_doc` function to be available.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.dilated_rnn.ipynb#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(DilatedRNN.predict, name='DilatedRNN.predict')\n```\n\n----------------------------------------\n\nTITLE: Displaying Documentation for TemporalConvolutionEncoder Class\nDESCRIPTION: Calls the nbdev documentation display function to show documentation for the TemporalConvolutionEncoder class in the notebook.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.modules.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(TemporalConvolutionEncoder, title_level=3)\n```\n\n----------------------------------------\n\nTITLE: GMM Call Method Documentation\nDESCRIPTION: Code to display documentation for the GMM __call__ method using a show_doc utility function.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_55\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(GMM.__call__, name='GMM.__call__', title_level=3)\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging Settings for MLflow and PyTorch\nDESCRIPTION: Sets up logging configuration by suppressing verbose output from MLflow and PyTorch Lightning. Also filters out warnings to maintain a clean output during model training and evaluation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/12_using_mlflow.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nlogging.getLogger(\"mlflow\").setLevel(logging.ERROR)\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\nwarnings.filterwarnings(\"ignore\")\n```\n\n----------------------------------------\n\nTITLE: Unit Tests for MQLoss Class Quantile Initialization in Python\nDESCRIPTION: Tests that verify the MQLoss class correctly initializes its quantiles attribute based on different inputs. The tests check both level-based initialization and direct quantile specification, confirming the expected number of quantiles and output names.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# | hide\n# Unit tests to check MQLoss' stored quantiles\n# attribute is correctly instantiated\ncheck = MQLoss(level=[80, 90])\ntest_eq(len(check.quantiles), 5)\n\ncheck = MQLoss(quantiles=[0.0100, 0.1000, 0.5, 0.9000, 0.9900])\nprint(check.output_names)\nprint(check.quantiles)\ntest_eq(len(check.quantiles), 5)\n\ncheck = MQLoss(quantiles=[0.0100, 0.1000, 0.9000, 0.9900])\ntest_eq(len(check.quantiles), 4)\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for AutoRNN Implementation\nDESCRIPTION: Executes unit tests to verify the AutoRNN class implementation, including checking that configuration parameters are properly handled and that the class inherits all necessary arguments from BaseAuto.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n# Check Optuna\nassert model.config(MockTrial())['h'] == 12\n\n# Unit test to test that Auto* model contains all required arguments from BaseAuto\ntest_args(AutoRNN, exclude_args=['cls_model']) \n\n# Unit test for situation: Optuna with updated default config\nmy_config = AutoRNN.get_default_config(h=12, backend='optuna')\ndef my_config_new(trial):\n    config = {**my_config(trial)}\n    config.update({'max_steps': 1, 'val_check_steps': 1, 'input_size': -1, 'encoder_hidden_size': 8})\n    return config\n\nmodel = AutoRNN(h=12, config=my_config_new, backend='optuna', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: Displaying RMoK.fit Method Documentation\nDESCRIPTION: A utility call to display the documentation for the fit method of the RMoK class, which is likely inherited from the BaseModel parent class.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.rmok.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(RMoK.fit, name='RMoK.fit')\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for Autoformer Model\nDESCRIPTION: This code snippet runs unit tests for the Autoformer model using the check_model function. It suppresses warnings and sets logging levels to focus on essential output.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.autoformer.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n# Unit tests for models\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\nlogging.getLogger(\"lightning_fabric\").setLevel(logging.ERROR)\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    check_model(Autoformer, [\"airpassengers\"])\n```\n\n----------------------------------------\n\nTITLE: Visualizing RUL for Multiple Engines\nDESCRIPTION: Creates a plot showing the Remaining Useful Life (RUL) trajectories for three different engines from the training dataset.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/use-cases/predictive_maintenance.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nplot_df1 = Y_train_df[Y_train_df['unique_id']==1]\nplot_df2 = Y_train_df[Y_train_df['unique_id']==2]\nplot_df3 = Y_train_df[Y_train_df['unique_id']==3]\n\nplt.plot(plot_df1.ds, np.minimum(plot_df1.y, 125), color='#2D6B8F', linestyle='--')\nplt.plot(plot_df1.ds, plot_df1.y, color='#2D6B8F', label='Engine 1')\n\nplt.plot(plot_df2.ds, np.minimum(plot_df2.y, 125)+1.5, color='#CA6F6A', linestyle='--')\nplt.plot(plot_df2.ds, plot_df2.y+1.5, color='#CA6F6A', label='Engine 2')\n\nplt.plot(plot_df3.ds, np.minimum(plot_df3.y, 125)-1.5, color='#D5BC67', linestyle='--')\nplt.plot(plot_df3.ds, plot_df3.y-1.5, color='#D5BC67', label='Engine 3')\n\nplt.ylabel('Remaining Useful Life (RUL)', fontsize=15)\nplt.xlabel('Time Cycle', fontsize=15)\nplt.legend()\nplt.grid()\n```\n\n----------------------------------------\n\nTITLE: Processing Exogenous Variables in MLPMultivariate Model\nDESCRIPTION: Processes historical, future, and static exogenous variables for the MLPMultivariate model. Reshapes and concatenates input tensors for MLP processing, applying ReLU activation between layers.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.mlpmultivariate.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nhist_exog     = windows_batch['hist_exog']              #   [B, hist_exog_size (X), L, N]\nfutr_exog     = windows_batch['futr_exog']              #   [B, futr_exog_size (F), L + h, N]\nstat_exog     = windows_batch['stat_exog']              #   [N, stat_exog_size (S)]\n\n# Flatten MLP inputs [B, C, L+H, N] -> [B, C * (L+H) * N]\n# Contatenate [ Y^1_t, ..., Y^N_t | X^1_{t-L},..., X^1_{t}, ..., X^N_{t} | F^1_{t-L},..., F^1_{t+H}, ...., F^N_{t+H} | S^1, ..., S^N ]\nbatch_size = x.shape[0]\nx = x.reshape(batch_size, -1)\nif self.hist_exog_size > 0:\n    x = torch.cat(( x, hist_exog.reshape(batch_size, -1) ), dim=1)\n\nif self.futr_exog_size > 0:\n    x = torch.cat(( x, futr_exog.reshape(batch_size, -1) ), dim=1)\n\nif self.stat_exog_size > 0:\n    stat_exog = stat_exog.reshape(-1)                   #   [N, S] -> [N * S]\n    stat_exog = stat_exog.unsqueeze(0)\\\n                         .repeat(batch_size, \n                                 1)                     #   [N * S] -> [B, N * S]            \n    x = torch.cat((x, stat_exog), dim=1)\n\nfor layer in self.mlp:\n     x = torch.relu(layer(x))\nx = self.out(x)\n\nforecast = x.reshape(batch_size, self.h, -1)\n\nreturn forecast\n```\n\n----------------------------------------\n\nTITLE: Loading and Displaying AirPassengers Dataset\nDESCRIPTION: Loading the AirPassengers dataset, which contains monthly airline passenger counts from 1949 to 1960, and displaying the first few rows.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/04_hyperparameter_tuning.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nY_df = AirPassengersDF\nY_df.head()\n```\n\n----------------------------------------\n\nTITLE: Generating Documentation for AutoNLinear\nDESCRIPTION: Command to generate documentation for the AutoNLinear class with a specified title level of 3.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_50\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(AutoNLinear, title_level=3)\n```\n\n----------------------------------------\n\nTITLE: Suppressing Logging and Running Unit Tests for KAN Model in NeuralForecast Python\nDESCRIPTION: This unit test block suppresses excessive logging from pytorch_lightning and lightning_fabric, ignoring related warnings for a clean test run. It then calls the check_model routine to test the KAN implementation on the airpassengers dataset. Dependencies include logging, warnings, and the check_model utility, and is suitable for test frameworks or notebooks needing minimal output.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.kan.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n# Unit tests for models\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\nlogging.getLogger(\"lightning_fabric\").setLevel(logging.ERROR)\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    check_model(KAN, checks=[\"airpassengers\"])\n```\n\n----------------------------------------\n\nTITLE: Data Preprocessing - Removing ID Columns\nDESCRIPTION: Removes 'id' columns from training and test datasets to prevent impact on future tests.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_43\n\nLANGUAGE: python\nCODE:\n```\nAirPassengersPanel_train = AirPassengersPanel_train.drop(columns='id')\nAirPassengersPanel_test = AirPassengersPanel_test.drop(columns='id')\n```\n\n----------------------------------------\n\nTITLE: Documenting the predict Method for TFT Model - Python\nDESCRIPTION: Documents the predict method of the TFT class by calling show_doc with configuration for name and title level. This enhances in-notebook or static documentation by surfacing method details to end-users or developers. Requires show_doc and the TFT class in scope; provides configuration but not functional model output.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(TFT.predict, name=\"TFT.predict\", title_level=3)\n```\n\n----------------------------------------\n\nTITLE: Preparing Train-Test Data Split\nDESCRIPTION: Splits the dataset into training and testing sets, with the last 96 observations reserved for testing model performance.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/14_interpretable_decompositions.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Split dataset into train/test\n# Last horizon observations for test\nhorizon = 96\nY_df = pd.DataFrame(dict(unique_id=1, ds=np.arange(len(x)), y=y))\nY_train_df = Y_df.groupby('unique_id').head(len(Y_df)-horizon)\nY_test_df = Y_df.groupby('unique_id').tail(horizon)\nY_test_df\n```\n\n----------------------------------------\n\nTITLE: Loading Documentation Tools\nDESCRIPTION: Imports the documentation display utility from nbdev for showing documentation in the notebook.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.modules.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#| hide\nfrom nbdev.showdoc import show_doc\n```\n\n----------------------------------------\n\nTITLE: Setting PyTorch Environment Variable for MPS Fallback\nDESCRIPTION: This snippet sets an environment variable to enable MPS (Metal Performance Shaders) fallback for PyTorch. This is likely used to ensure compatibility or performance on certain hardware configurations.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.kan.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%set_env PYTORCH_ENABLE_MPS_FALLBACK=1\n```\n\n----------------------------------------\n\nTITLE: Displaying HuberQLoss Call Method Documentation\nDESCRIPTION: Code to display the documentation for the HuberQLoss.__call__ method using a documentation display function.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_70\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(HuberQLoss.__call__, name='HuberQLoss.__call__', title_level=3)\n```\n\n----------------------------------------\n\nTITLE: Creating Distribution for NBMM in Python\nDESCRIPTION: This method constructs the associated PyTorch Distribution given the collection of constructor arguments. It creates a mixture of Negative Binomial distributions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_62\n\nLANGUAGE: python\nCODE:\n```\ndef get_distribution(self, distr_args) -> Distribution:\n    if self.weighted:\n        total_count, probs, weights = distr_args\n    else:\n        total_count, probs = distr_args\n        weights = torch.full_like(total_count, fill_value=1 / self.n_components)\n\n    mix = Categorical(weights)\n    components = NegativeBinomial(total_count, probs)\n    components.support = constraints.nonnegative\n    distr = MixtureSameFamily(mixture_distribution=mix,\n                                  component_distribution=components)    \n\n    self.distr_mean = distr.mean\n    \n    return distr\n```\n\n----------------------------------------\n\nTITLE: Configuring Tokenizer Padding in TimeLLM\nDESCRIPTION: Sets up tokenizer padding by either using the existing EOS token or adding a special [PAD] token. This ensures proper handling of sequences during processing.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timellm.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nif self.llm_tokenizer.eos_token:\n    self.llm_tokenizer.pad_token = self.llm_tokenizer.eos_token\nelse:\n    pad_token = '[PAD]'\n    self.llm_tokenizer.add_special_tokens({'pad_token': pad_token})\n    self.llm_tokenizer.pad_token = pad_token\n```\n\n----------------------------------------\n\nTITLE: Importing Polars for DataFrame Testing\nDESCRIPTION: Sets up the polars library and testing utilities for comparing pandas and polars DataFrames in subsequent tests.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_54\n\nLANGUAGE: python\nCODE:\n```\nimport polars\nfrom polars.testing import assert_frame_equal\n```\n\n----------------------------------------\n\nTITLE: Displaying FEDformer fit Method Documentation in Python\nDESCRIPTION: This snippet utilizes the `show_doc` function to specifically display the documentation for the `fit` method of the `FEDformer` class. The `name` argument ensures the displayed title reflects the method being documented. This helps in understanding how to train the FEDformer model.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.fedformer.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(FEDformer.fit, name='FEDformer.fit')\n```\n\n----------------------------------------\n\nTITLE: Displaying FEDformer predict Method Documentation in Python\nDESCRIPTION: This snippet employs the `show_doc` function to render the documentation for the `predict` method of the `FEDformer` class. Similar to the previous `show_doc` calls, it uses the `name` argument for clarity and is intended for use in environments like Jupyter Notebooks to explain the prediction process.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.fedformer.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(FEDformer.predict, name='FEDformer.predict')\n```\n\n----------------------------------------\n\nTITLE: Implementing Basis Functions for NBEATSx\nDESCRIPTION: Defines various basis functions used in the NBEATSx model, including Identity, Trend, Exogenous, and Seasonality bases. These functions are essential for capturing different components of time series data.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeatsx.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass IdentityBasis(nn.Module):\n    def __init__(self, backcast_size: int, forecast_size: int, out_features: int = 1):\n        super().__init__()\n        self.out_features = out_features\n        self.forecast_size = forecast_size\n        self.backcast_size = backcast_size\n\n    def forward(self, theta: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        backcast = theta[:, : self.backcast_size]\n        forecast = theta[:, self.backcast_size :]\n        forecast = forecast.reshape(len(forecast), -1, self.out_features)\n        return backcast, forecast\n\n\nclass TrendBasis(nn.Module):\n    def __init__(\n        self,\n        degree_of_polynomial: int,\n        backcast_size: int,\n        forecast_size: int,\n        out_features: int = 1,\n    ):\n        super().__init__()\n        self.out_features = out_features\n        polynomial_size = degree_of_polynomial + 1\n        self.backcast_basis = nn.Parameter(\n            torch.tensor(\n                np.concatenate(\n                    [\n                        np.power(\n                            np.arange(backcast_size, dtype=float) / backcast_size, i\n                        )[None, :]\n                        for i in range(polynomial_size)\n                    ]\n                ),\n                dtype=torch.float32,\n            ),\n            requires_grad=False,\n        )\n        self.forecast_basis = nn.Parameter(\n            torch.tensor(\n                np.concatenate(\n                    [\n                        np.power(\n                            np.arange(forecast_size, dtype=float) / forecast_size, i\n                        )[None, :]\n                        for i in range(polynomial_size)\n                    ]\n                ),\n                dtype=torch.float32,\n            ),\n            requires_grad=False,\n        )\n\n    def forward(self, theta: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        polynomial_size = self.forecast_basis.shape[0]  # [polynomial_size, L+H]\n        backcast_theta = theta[:, :polynomial_size]\n        forecast_theta = theta[:, polynomial_size:]\n        forecast_theta = forecast_theta.reshape(\n            len(forecast_theta), polynomial_size, -1\n        )\n        backcast = torch.einsum(\"bp,pt->bt\", backcast_theta, self.backcast_basis)\n        forecast = torch.einsum(\"bpq,pt->btq\", forecast_theta, self.forecast_basis)\n        return backcast, forecast\n\nclass ExogenousBasis(nn.Module):\n    # Reference: https://github.com/cchallu/nbeatsx\n    def __init__(self, forecast_size: int):\n        super().__init__()\n        self.forecast_size = forecast_size\n\n    def forward(self, theta: torch.Tensor, futr_exog: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        backcast_basis = futr_exog[:, :-self.forecast_size, :].permute(0, 2, 1)\n        forecast_basis = futr_exog[:, -self.forecast_size:, :].permute(0, 2, 1)\n        cut_point = forecast_basis.shape[1]\n        backcast_theta=theta[:, cut_point:]\n        forecast_theta=theta[:, :cut_point].reshape(\n            len(theta), cut_point, -1\n        )\n     \n        backcast = torch.einsum('bp,bpt->bt', backcast_theta, backcast_basis)\n        forecast = torch.einsum('bpq,bpt->btq', forecast_theta, forecast_basis)\n        \n        return backcast, forecast\n\nclass SeasonalityBasis(nn.Module):\n    def __init__(\n        self,\n        harmonics: int,\n        backcast_size: int,\n        forecast_size: int,\n        out_features: int = 1,\n    ):\n        super().__init__()\n        self.out_features = out_features\n        frequency = np.append(\n            np.zeros(1, dtype=float),\n            np.arange(harmonics, harmonics / 2 * forecast_size, dtype=float)\n            / harmonics,\n        )[None, :]\n        backcast_grid = (\n            -2\n            * np.pi\n            * (np.arange(backcast_size, dtype=float)[:, None] / forecast_size)\n            * frequency\n        )\n        forecast_grid = (\n            2\n            * np.pi\n            * (np.arange(forecast_size, dtype=float)[:, None] / forecast_size)\n            * frequency\n        )\n\n        backcast_cos_template = torch.tensor(\n            np.transpose(np.cos(backcast_grid)), dtype=torch.float32\n        )\n        backcast_sin_template = torch.tensor(\n            np.transpose(np.sin(backcast_grid)), dtype=torch.float32\n        )\n        backcast_template = torch.cat(\n            [backcast_cos_template, backcast_sin_template], dim=0\n        )\n\n        forecast_cos_template = torch.tensor(\n            np.transpose(np.cos(forecast_grid)), dtype=torch.float32\n        )\n        forecast_sin_template = torch.tensor(\n            np.transpose(np.sin(forecast_grid)), dtype=torch.float32\n        )\n        forecast_template = torch.cat(\n            [forecast_cos_template, forecast_sin_template], dim=0\n        )\n\n        self.backcast_basis = nn.Parameter(backcast_template, requires_grad=False)\n        self.forecast_basis = nn.Parameter(forecast_template, requires_grad=False)\n\n    def forward(self, theta: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        harmonic_size = self.forecast_basis.shape[0]  # [harmonic_size, L+H]\n        backcast_theta = theta[:, :harmonic_size]\n        forecast_theta = theta[:, harmonic_size:]\n        forecast_theta = forecast_theta.reshape(len(forecast_theta), harmonic_size, -1)\n        backcast = torch.einsum(\"bp,pt->bt\", backcast_theta, self.backcast_basis)\n        forecast = torch.einsum(\"bpq,pt->btq\", forecast_theta, self.forecast_basis)\n        return backcast, forecast\n```\n\n----------------------------------------\n\nTITLE: Importing Required Dependencies for Autoformer Model\nDESCRIPTION: Imports necessary Python libraries including PyTorch, numpy and custom modules for implementing the Autoformer architecture. The imports include core functionality for neural networks, mathematical operations, and custom loss functions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.autoformer.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#| export\nimport math\nimport numpy as np\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom neuralforecast.common._modules import DataEmbedding, SeriesDecomp\nfrom neuralforecast.common._base_model import BaseModel\n\nfrom neuralforecast.losses.pytorch import MAE\n```\n\n----------------------------------------\n\nTITLE: Testing Level and Quantile Conversion Functions in Python\nDESCRIPTION: Contains test code (marked with `#| hide`) to verify the correctness of the `level_to_quantiles` and `quantiles_to_level` functions. It defines base level and quantile lists and asserts that converting back and forth yields the original lists.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/utils.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n# Test level_to_quantiles\nlevel_base = [80, 90]\nquantiles_base = [0.05, 0.1, 0.9, 0.95]\nquantiles = level_to_quantiles(level_base)\nlevel = quantiles_to_level(quantiles_base)\n\nassert quantiles == quantiles_base\nassert level == level_base\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for TCN Implementation in Python\nDESCRIPTION: Imports required PyTorch and custom modules for implementing the TCN model. Includes typing utilities, core PyTorch components, and custom loss functions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tcn.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List, Optional\n\nimport torch\nimport torch.nn as nn\n\nfrom neuralforecast.losses.pytorch import MAE\nfrom neuralforecast.common._base_model import BaseModel\nfrom neuralforecast.common._modules import MLP, TemporalConvolutionEncoder\n```\n\n----------------------------------------\n\nTITLE: Generating Forecasts with Trained Models\nDESCRIPTION: Generates forecasts using the trained models for the specified horizon beyond the training data.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/getting-started/02_quickstart.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nY_hat_df = nf.predict()\n```\n\n----------------------------------------\n\nTITLE: Displaying RMoK Class Documentation\nDESCRIPTION: A utility call to display the RMoK class documentation, likely using a helper function that formats and shows class documentation in a readable format.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.rmok.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(RMoK)\n```\n\n----------------------------------------\n\nTITLE: Testing Learning Rate Parameter Warning\nDESCRIPTION: Validates that a warning is issued when learning rate is passed in optimizer_kwargs.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_62\n\nLANGUAGE: python\nCODE:\n```\nfor nf_model in [NHITS, RNN, StemGNN]:\n    params = {\n        \"h\": 12, \n        \"input_size\": 24, \n        \"max_steps\": 1, \n        \"optimizer\": torch.optim.Adadelta, \n        \"optimizer_kwargs\": {\"lr\": 0.8, \"rho\": 0.45}\n    }\n    if nf_model.__name__ == \"StemGNN\":\n        params.update({\"n_series\": 2})\n    models = [nf_model(**params)]\n    nf = NeuralForecast(models=models, freq='M')\n    with warnings.catch_warnings(record=True) as issued_warnings:\n        warnings.simplefilter('always', UserWarning)\n        nf.fit(AirPassengersPanel_train)\n        assert any(\"ignoring learning rate passed in optimizer_kwargs, using the model's learning rate\" in str(w.message) for w in issued_warnings)\n```\n\n----------------------------------------\n\nTITLE: Plotting Time Series Predictions\nDESCRIPTION: Imports plotting utility and visualizes the forecasts, excluding the cutoff column from the results.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/05_predictInsample.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom utilsforecast.plotting import plot_series\n\nplot_series(forecasts_df=Y_hat_insample.drop(columns='cutoff'))\n```\n\n----------------------------------------\n\nTITLE: Displaying KAN Documentation in NeuralForecast Python\nDESCRIPTION: This snippet uses the show_doc utility to programmatically display documentation for the KAN class in NeuralForecast. No explicit dependencies are except that show_doc should be available (likely from fastai or a similar documentation utility). It enables quick access to docstrings and method summaries in interactive notebooks or documentation pipelines. Input is the KAN class; output is its documentation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.kan.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(KAN)\n```\n\n----------------------------------------\n\nTITLE: Generating Documentation for AutoTiDE\nDESCRIPTION: Command to generate documentation for the AutoTiDE class with a specified title level of 3.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_54\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(AutoTiDE, title_level=3)\n```\n\n----------------------------------------\n\nTITLE: Loading and Processing ETTm2 Dataset\nDESCRIPTION: Load the ETTm2 dataset using LongHorizon class and prepare validation/test splits\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/06_longhorizon_probabilistic.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom datasetsforecast.long_horizon import LongHorizon\n\nY_df, _, _ = LongHorizon.load(directory='./', group='ETTm2')\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\n\nn_time = len(Y_df.ds.unique())\nval_size = 96*10\ntest_size = 96*10\n\nY_df.groupby('unique_id').head(2)\n```\n\n----------------------------------------\n\nTITLE: Unit Testing IQLoss Initialization and Quantile Update in PyTorch\nDESCRIPTION: This code snippet contains unit tests for the IQLoss class. It checks if the default quantile is set to 0.5 at initialization and if the quantiles are correctly updated after calling the update_quantile method.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# | hide\n# Unit tests\n# Check that default quantile is set to 0.5 at initialization\ncheck = IQLoss()\ntest_eq(check.q, 0.5)\n\n# Check that quantiles are correctly updated - prediction\ncheck = IQLoss()\ncheck.update_quantile([0.7])\ntest_eq(check.q, 0.7)\n```\n\n----------------------------------------\n\nTITLE: Testing Early Stopping Parameter Validation in NeuralForecast\nDESCRIPTION: Tests that NeuralForecast raises an error when early stopping is enabled without a validation set. The test verifies that the library correctly prompts users to set val_size>0 when using early stopping.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n# Unitest for early stopping without val_size protection\nmodels = [\n    NHITS(h=12, input_size=12, max_steps=1, early_stop_patience_steps=5)\n]\nnf = NeuralForecast(models=models, freq='M')\ntest_fail(nf.fit,\n          contains='Set val_size>0 if early stopping is enabled.',\n          args=(AirPassengersPanel_train,))\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for NBEATS Implementation\nDESCRIPTION: Imports necessary libraries and modules for implementing the NBEATS model, including NumPy, PyTorch, and custom loss functions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeats.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\nfrom typing import Tuple, Optional\n\nimport numpy as np\nfrom numpy.polynomial.legendre import Legendre\nfrom numpy.polynomial.chebyshev import Chebyshev\nimport torch\nimport torch.nn as nn\nfrom scipy.interpolate import BSpline\n\nfrom neuralforecast.losses.pytorch import MAE\nfrom neuralforecast.common._base_model import BaseModel\n```\n\n----------------------------------------\n\nTITLE: Preparing Training and Test Data\nDESCRIPTION: Splits the AirPassengers dataset into training and test sets, with test set containing the last 12 months of data.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/20_conformal_prediction.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nAirPassengersPanel_train = AirPassengersPanel[AirPassengersPanel['ds'] < AirPassengersPanel['ds'].values[-12]].reset_index(drop=True)\nAirPassengersPanel_test = AirPassengersPanel[AirPassengersPanel['ds'] >= AirPassengersPanel['ds'].values[-12]].reset_index(drop=True)\nAirPassengersPanel_test['y'] = np.nan\nAirPassengersPanel_test['y_[lag12]'] = np.nan\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for NeuralForecast Core in Python\nDESCRIPTION: Imports various Python libraries and modules required for the NeuralForecast core functionality, including data processing, model implementation, and utility functions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pickle\nimport warnings\nfrom copy import deepcopy\nfrom itertools import chain\nfrom typing import Any, Dict, List, Optional, Sequence, Union\n\nimport fsspec\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport torch\nimport utilsforecast.processing as ufp\nfrom coreforecast.grouped_array import GroupedArray\nfrom coreforecast.scalers import (\n    LocalBoxCoxScaler,\n    LocalMinMaxScaler,\n    LocalRobustScaler,\n    LocalStandardScaler,\n)\nfrom utilsforecast.compat import DataFrame, DFType, Series, pl_DataFrame, pl_Series\nfrom utilsforecast.validation import validate_freq\n\nfrom neuralforecast.common._base_model import DistributedConfig\nfrom neuralforecast.compat import SparkDataFrame\nfrom neuralforecast.losses.pytorch import IQLoss, HuberIQLoss\nfrom neuralforecast.tsdataset import _FilesDataset, TimeSeriesDataset, LocalFilesTimeSeriesDataset\nfrom neuralforecast.models import (\n    GRU, LSTM, RNN, TCN, DeepAR, DilatedRNN,\n    MLP, NHITS, NBEATS, NBEATSx, DLinear, NLinear,\n    TFT, VanillaTransformer,\n    Informer, Autoformer, FEDformer,\n    StemGNN, PatchTST, TimesNet, TimeLLM, TSMixer, TSMixerx,\n    MLPMultivariate, iTransformer,\n    BiTCN, TiDE, DeepNPTS, SOFTS,\n    TimeMixer, KAN, RMoK, TimeXer\n)\nfrom neuralforecast.common._base_auto import BaseAuto, MockTrial\nfrom neuralforecast.utils import PredictionIntervals, get_prediction_interval_method, level_to_quantiles, quantiles_to_level\n```\n\n----------------------------------------\n\nTITLE: Testing predict_insample step_size with NHITS model\nDESCRIPTION: Tests the predict_insample function with various step_size and test_size parameters. The test verifies that the cutoff dates and forecast-points count per series match expected values.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nh = 12\ntrain_end = AirPassengersPanel_train['ds'].max()\nsizes = AirPassengersPanel_train['unique_id'].value_counts().to_numpy()\nfor step_size, test_size in [(7, 0), (9, 0), (7, 5), (9, 5)]:\n    models = [NHITS(h=h, input_size=12, max_steps=1)]\n    nf = NeuralForecast(models=models, freq='M')\n    nf.fit(AirPassengersPanel_train)\n    # Note: only apply set_test_size() upon nf.fit(), otherwise it would have set the test_size = 0\n    nf.models[0].set_test_size(test_size)\n    \n    forecasts = nf.predict_insample(step_size=step_size)\n    last_cutoff = train_end - test_size * pd.offsets.MonthEnd() - h * pd.offsets.MonthEnd()\n    n_expected_cutoffs = (sizes[0] - test_size - nf.h + step_size) // step_size\n\n    # compare cutoff values\n    expected_cutoffs = np.flip(np.array([last_cutoff - step_size * i * pd.offsets.MonthEnd() for i in range(n_expected_cutoffs)]))\n    actual_cutoffs = np.array([pd.Timestamp(x) for x in forecasts[forecasts['unique_id']==nf.uids[1]]['cutoff'].unique()])\n    np.testing.assert_array_equal(expected_cutoffs, actual_cutoffs, err_msg=f\"{step_size=},{expected_cutoffs=},{actual_cutoffs=}\")\n    \n    # check forecast-points count per series\n    cutoffs_by_series = forecasts.groupby(['unique_id', 'cutoff']).size().unstack('unique_id')\n    pd.testing.assert_series_equal(cutoffs_by_series['Airline1'], cutoffs_by_series['Airline2'], check_names=False)\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies and Loading Data\nDESCRIPTION: Imports required libraries and loads the hierarchical tourism dataset with necessary data preprocessing steps.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/09_hierarchical_forecasting.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n\nfrom datasetsforecast.hierarchical import HierarchicalData\nfrom hierarchicalforecast.utils import aggregate, HierarchicalPlot\nfrom neuralforecast.utils import augment_calendar_df\nfrom utilsforecast.plotting import plot_series\n```\n\nLANGUAGE: python\nCODE:\n```\n# Load hierarchical dataset\nY_df, S_df, tags = HierarchicalData.load('./data', 'TourismLarge')\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\nY_df, _ = augment_calendar_df(df=Y_df, freq='M')\nS_df = S_df.reset_index(names=\"unique_id\")\n```\n\n----------------------------------------\n\nTITLE: Enabling Autoreload Extension in Python\nDESCRIPTION: This code snippet loads the autoreload extension and sets it to automatically reload modules. It uses Jupyter notebook magic commands to configure the autoreload behavior.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.dilated_rnn.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Installing NeuralForecast and Dependencies\nDESCRIPTION: Installation of the NeuralForecast library and hyperopt package using pip. This is captured to suppress output in a Jupyter notebook environment.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/04_hyperparameter_tuning.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n!pip install neuralforecast hyperopt\n```\n\n----------------------------------------\n\nTITLE: Displaying Forecast Results\nDESCRIPTION: Displays the first rows of the forecast results dataframe, showing predictions from different models alongside the actual values.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/05_longhorizon_transformers.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nY_hat_df.head()\n```\n\n----------------------------------------\n\nTITLE: Displaying Documentation for Method NBEATS.predict - Python\nDESCRIPTION: Displays generated or docstring-based documentation for the NBEATS.predict method using show_doc. The name argument provides a readable label. This assumes show_doc and the class are in scope.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeats.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(NBEATS.predict, name='NBEATS.predict')\n```\n\n----------------------------------------\n\nTITLE: Displaying AutoBiTCN Documentation in Python\nDESCRIPTION: This snippet uses the `show_doc` function, likely from a documentation generation library like nbdev or fastdoc, to display the documentation for the `AutoBiTCN` class. It specifies the title level for the generated documentation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(AutoBiTCN, title_level=3)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Matplotlib for Visualization\nDESCRIPTION: Configures Matplotlib for better visualization by enabling grid lines on all plots.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/14_interpretable_decompositions.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nplt.rcParams[\"axes.grid\"]=True\n```\n\n----------------------------------------\n\nTITLE: Generating Documentation for AutoDeepAR Class in Python\nDESCRIPTION: This snippet uses the `show_doc` function to automatically generate documentation for the `AutoDeepAR` class. Similar to the `AutoTCN` example, `title_level=3` sets the heading level for the documentation output.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_22\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(AutoDeepAR, title_level=3)\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for TimeXer Model in Python\nDESCRIPTION: This snippet sets up and runs unit tests for the TimeXer model using the check_model function. It suppresses warnings and adjusts logging levels for cleaner output.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timexer.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\nlogging.getLogger(\"lightning_fabric\").setLevel(logging.ERROR)\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    check_model(TimeXer, [\"airpassengers\"])\n```\n\n----------------------------------------\n\nTITLE: Implementing NBEATS Basis Classes\nDESCRIPTION: Defines PyTorch modules for different basis types used in NBEATS, including IdentityBasis, TrendBasis, and SeasonalityBasis. These classes are essential components of the NBEATS architecture.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeats.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass IdentityBasis(nn.Module):\n    def __init__(self, backcast_size: int, forecast_size: int,\n                 out_features: int=1):\n        super().__init__()\n        self.out_features = out_features\n        self.forecast_size = forecast_size\n        self.backcast_size = backcast_size\n \n    def forward(self, theta: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        backcast = theta[:, :self.backcast_size]\n        forecast = theta[:, self.backcast_size:]\n        forecast = forecast.reshape(len(forecast), -1, self.out_features)\n        return backcast, forecast\n\nclass TrendBasis(nn.Module):\n    def __init__(self, \n                 n_basis: int,\n                 backcast_size: int,\n                 forecast_size: int,\n                 out_features: int=1,\n                 basis='polynomial'):\n        super().__init__()\n        self.out_features = out_features\n        self.backcast_basis = nn.Parameter(\n            torch.tensor(get_basis(backcast_size, n_basis, basis).T, dtype=torch.float32), requires_grad=False)\n        self.forecast_basis = nn.Parameter(\n            torch.tensor(get_basis(forecast_size, n_basis, basis).T, dtype=torch.float32), requires_grad=False)\n\n    def forward(self, theta: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        polynomial_size = self.forecast_basis.shape[0] # [polynomial_size, L+H]\n        backcast_theta = theta[:, :polynomial_size]\n        forecast_theta = theta[:, polynomial_size:]\n        forecast_theta = forecast_theta.reshape(len(forecast_theta),polynomial_size,-1)\n        backcast = torch.einsum('bp,pt->bt', backcast_theta, self.backcast_basis)\n        forecast = torch.einsum('bpq,pt->btq', forecast_theta, self.forecast_basis)\n        return backcast, forecast\n\nclass SeasonalityBasis(nn.Module):\n    def __init__(self, \n                 harmonics: int, \n                 backcast_size: int, \n                 forecast_size: int,\n                 out_features: int=1):\n        super().__init__()\n        self.out_features = out_features\n        frequency = np.append(np.zeros(1, dtype=float),\n                                        np.arange(harmonics, harmonics / 2 * forecast_size,\n                                                    dtype=float) / harmonics)[None, :]\n        backcast_grid = -2 * np.pi * (\n                np.arange(backcast_size, dtype=float)[:, None] / forecast_size) * frequency\n        forecast_grid = 2 * np.pi * (\n                np.arange(forecast_size, dtype=float)[:, None] / forecast_size) * frequency\n\n        backcast_cos_template = torch.tensor(np.transpose(np.cos(backcast_grid)), dtype=torch.float32)\n        backcast_sin_template = torch.tensor(np.transpose(np.sin(backcast_grid)), dtype=torch.float32)\n        backcast_template = torch.cat([backcast_cos_template, backcast_sin_template], dim=0)\n\n        forecast_cos_template = torch.tensor(np.transpose(np.cos(forecast_grid)), dtype=torch.float32)\n        forecast_sin_template = torch.tensor(np.transpose(np.sin(forecast_grid)), dtype=torch.float32)\n        forecast_template = torch.cat([forecast_cos_template, forecast_sin_template], dim=0)\n\n        self.backcast_basis = nn.Parameter(backcast_template, requires_grad=False)\n        self.forecast_basis = nn.Parameter(forecast_template, requires_grad=False)\n\n    def forward(self, theta: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        harmonic_size = self.forecast_basis.shape[0] # [harmonic_size, L+H]\n        backcast_theta = theta[:, :harmonic_size]\n        forecast_theta = theta[:, harmonic_size:]\n        forecast_theta = forecast_theta.reshape(len(forecast_theta),harmonic_size,-1)\n\n\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Informer Model in Python\nDESCRIPTION: Imports necessary Python libraries and modules for implementing the Informer model, including PyTorch, numpy, and custom modules from neuralforecast.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.informer.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport math\nimport numpy as np\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\n\nfrom neuralforecast.common._modules import (\n    TransEncoderLayer, TransEncoder,\n    TransDecoderLayer, TransDecoder,\n    DataEmbedding, AttentionLayer,\n)\nfrom neuralforecast.common._base_model import BaseModel\n\nfrom neuralforecast.losses.pytorch import MAE\n```\n\n----------------------------------------\n\nTITLE: Loading and Preparing AirPassengers Dataset\nDESCRIPTION: Imports necessary libraries and loads the AirPassengers dataset, then splits it into training and testing sets for forecasting examples.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n%%capture\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast.tsdataset import TimeSeriesDataset\nfrom neuralforecast.utils import AirPassengersDF as Y_df\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for Time Series Dataset\nDESCRIPTION: Imports required Python packages and modules for time series data processing and PyTorch integration.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/tsdataset.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom collections.abc import Mapping\nfrom pathlib import Path\nfrom typing import List, Optional, Sequence, Union\n\nimport numpy as np\nimport pandas as pd\nimport pytorch_lightning as pl\nimport torch\nimport utilsforecast.processing as ufp\nfrom torch.utils.data import Dataset, DataLoader\nfrom utilsforecast.compat import DataFrame, pl_Series\n```\n\n----------------------------------------\n\nTITLE: Loading and Visualizing Digit Image Data for Binary Sequence Creation\nDESCRIPTION: Loads digit images from sklearn datasets, reshapes them into sequences, and creates binary targets based on pixel thresholds. Includes visualization of both the original pixel values and the thresholded binary values.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/16_temporal_classification.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndigits = datasets.load_digits()\nimages = digits.images[:100]\n\nplt.imshow(images[0,:,:], cmap=plt.cm.gray, \n           vmax=16, interpolation=\"nearest\")\n\npixels = np.reshape(images, (len(images), 64))\nytarget = (pixels > 10) * 1\n\nfig, ax1 = plt.subplots()\nax2 = ax1.twinx()\nax1.plot(pixels[10])\nax2.plot(ytarget[10], color='purple')\nax1.set_xlabel('Pixel index')\nax1.set_ylabel('Pixel value')\nax2.set_ylabel('Pixel threshold', color='purple')\nplt.grid()\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Running TimesNet Unit Tests with Logging Control - Python\nDESCRIPTION: This code configures logging to reduce output noise from PyTorch Lightning components, suppresses warning messages, and runs basic model integrity checks on TimesNet using the \\\"check_model\\\" function with the AirPassengers dataset. It is intended to ensure that the TimesNet class works as expected with the provided data. Dependencies include the logging, warnings modules, TimesNet, and the check_model test utility.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timesnet.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n#| hide\n# Unit tests for models\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\nlogging.getLogger(\"lightning_fabric\").setLevel(logging.ERROR)\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    check_model(TimesNet, [\"airpassengers\"])\n```\n\n----------------------------------------\n\nTITLE: Displaying Documentation for Method NBEATS.fit - Python\nDESCRIPTION: Displays generated or docstring-based documentation for the NBEATS.fit method via the show_doc utility. The 'name' argument customizes display output. Depends on show_doc being available and NBEATS class being imported.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeats.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(NBEATS.fit, name='NBEATS.fit')\n```\n\n----------------------------------------\n\nTITLE: Installing NeuralForecast via pip\nDESCRIPTION: Command to install the NeuralForecast package from the Python Package Index (PyPI) using pip.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/getting-started/01_introduction.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\npip install neuralforecast\n```\n\n----------------------------------------\n\nTITLE: Loading M3 Competition Yearly Data\nDESCRIPTION: Loads the yearly data from the M3 forecasting competition dataset, which contains multiple time series. The data is stored in the specified directory.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/getting-started/05_datarequirements.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nY_df, *_ = M3.load('./data', group='Yearly')\n```\n\n----------------------------------------\n\nTITLE: Displaying AutoTimesNet Documentation\nDESCRIPTION: Code to display the documentation for the AutoTimesNet class, using a custom documentation display function.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_94\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(AutoTimesNet, title_level=3)\n```\n\n----------------------------------------\n\nTITLE: Importing MAE Loss Function\nDESCRIPTION: Imports the Mean Absolute Error (MAE) loss function from the neuralforecast.losses.numpy module for model evaluation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/05_longhorizon_transformers.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom neuralforecast.losses.numpy import mae\n```\n\n----------------------------------------\n\nTITLE: Testing model aliases with various NeuralForecast models\nDESCRIPTION: Tests the use of model aliases in NeuralForecast with multiple models including AutoDilatedRNN, NHITS with MQLoss, RNN with exogenous variables, and StemGNN for multivariate forecasting. Verifies the correct column names in the forecasts output.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_37\n\nLANGUAGE: python\nCODE:\n```\n# tests aliases\nconfig_drnn = {'input_size': tune.choice([-1]), \n               'encoder_hidden_size': tune.choice([5, 10]),\n               'max_steps': 1,\n               'val_check_steps': 1,\n               'step_size': 1}\nmodels = [\n    # test Auto\n    AutoDilatedRNN(h=12, config=config_drnn, cpus=1, num_samples=2, alias='AutoDIL'),\n    # test BaseWindows\n    NHITS(h=12, input_size=24, loss=MQLoss(level=[80]), max_steps=1, alias='NHITSMQ'),\n    # test BaseRecurrent\n    RNN(h=12, input_size=-1, encoder_hidden_size=10, max_steps=1,\n            stat_exog_list=['airline1'],\n            futr_exog_list=['trend'], hist_exog_list=['y_[lag12]'], alias='MyRNN'),\n    # test BaseMultivariate\n    StemGNN(h=12, input_size=24, n_series=2, max_steps=1, scaler_type='robust', alias='StemMulti'),\n    # test model without alias\n    NHITS(h=12, input_size=24, max_steps=1),\n]\nnf = NeuralForecast(models=models, freq='M')\nnf.fit(df=AirPassengersPanel_train, static_df=AirPassengersStatic)\nforecasts = nf.predict(futr_df=AirPassengersPanel_test)\ntest_eq(\n    forecasts.columns.to_list(),\n    ['unique_id', 'ds', 'AutoDIL', 'NHITSMQ-median', 'NHITSMQ-lo-80', 'NHITSMQ-hi-80', 'MyRNN', 'StemMulti', 'NHITS']\n)\n```\n\n----------------------------------------\n\nTITLE: Displaying AutoDilatedRNN Documentation in Python\nDESCRIPTION: This snippet uses the `show_doc` function, likely from a documentation generation library like nbdev or fastdoc, to display the documentation for the `AutoDilatedRNN` class. It specifies the title level for the generated documentation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(AutoDilatedRNN, title_level=3)\n```\n\n----------------------------------------\n\nTITLE: Importing DeepNPTS Dependencies\nDESCRIPTION: Imports required PyTorch modules, custom losses, and typing utilities for the DeepNPTS implementation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.deepnpts.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport neuralforecast.losses.pytorch as losses\nfrom typing import Optional\n\nfrom neuralforecast.common._base_model import BaseModel\nfrom neuralforecast.losses.pytorch import MAE\n```\n\n----------------------------------------\n\nTITLE: Initialize AirPassengers Dataset\nDESCRIPTION: Creates the classic Box & Jenkins airline data as a NumPy array and converts it to a pandas DataFrame with datetime index.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/utils.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#| export\nAirPassengers = np.array([112., 118., 132., 129., 121., 135., 148., 148., 136., 119., 104.,\n                          118., 115., 126., 141., 135., 125., 149., 170., 170., 158., 133.,\n                          114., 140., 145., 150., 178., 163., 172., 178., 199., 199., 184.,\n                          162., 146., 166., 171., 180., 193., 181., 183., 218., 230., 242.,\n                          209., 191., 172., 194., 196., 196., 236., 235., 229., 243., 264.,\n                          272., 237., 211., 180., 201., 204., 188., 235., 227., 234., 264.,\n                          302., 293., 259., 229., 203., 229., 242., 233., 267., 269., 270.,\n                          315., 364., 347., 312., 274., 237., 278., 284., 277., 317., 313.,\n                          318., 374., 413., 405., 355., 306., 271., 306., 315., 301., 356.,\n                          348., 355., 422., 465., 467., 404., 347., 305., 336., 340., 318.,\n                          362., 348., 363., 435., 491., 505., 404., 359., 310., 337., 360.,\n                          342., 406., 396., 420., 472., 548., 559., 463., 407., 362., 405.,\n                          417., 391., 419., 461., 472., 535., 622., 606., 508., 461., 390.,\n                          432.], dtype=np.float32)\n\n#| export\nAirPassengersDF = pd.DataFrame({'unique_id': np.ones(len(AirPassengers)),\n                                'ds': pd.date_range(start='1949-01-01',\n                                                    periods=len(AirPassengers), freq=pd.offsets.MonthEnd()),\n                                'y': AirPassengers})\n```\n\n----------------------------------------\n\nTITLE: Importing Testing and Documentation Libraries\nDESCRIPTION: Imports libraries for testing, documentation, and logging, which are hidden in the final output.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.fedformer.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#| hide\nimport logging\nimport warnings\nfrom fastcore.test import test_eq\nfrom nbdev.showdoc import show_doc\nfrom neuralforecast.common._model_checks import check_model\n```\n\n----------------------------------------\n\nTITLE: Displaying TimesNet Class Documentation - Python\nDESCRIPTION: This snippet uses the \\\"show_doc\\\" utility to display the documentation for the TimesNet model class. \\\"show_doc\\\" is typically used in interactive environments such as Jupyter notebooks to render class or function docstrings in a human-readable format. It assumes that the TimesNet class has been previously defined and that \\\"show_doc\\\" is available in the scope.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timesnet.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nshow_doc(TimesNet)\n```\n\n----------------------------------------\n\nTITLE: Loading Future Exogenous Data\nDESCRIPTION: Loads future exogenous variables required for making predictions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/03_exogenous_variables.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfutr_df = pd.read_csv(\n    'https://datasets-nixtla.s3.amazonaws.com/EPF_FR_BE_futr.csv',\n    parse_dates=['ds'],\n)\nfutr_df.head()\n```\n\n----------------------------------------\n\nTITLE: Configuring Python Autoreload Extension\nDESCRIPTION: Loads and enables the autoreload extension in Jupyter which automatically reloads Python modules before code execution.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.stemgnn.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Configuring Jupyter Autoreload Extension\nDESCRIPTION: Loads and enables the autoreload extension in Jupyter notebook to automatically reload external Python modules before executing code.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.rnn.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Installing Required Packages via pip\nDESCRIPTION: Alternative installation method using pip to install neuralforecast and datasetsforecast directly from GitHub repositories.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/experiments/long_horizon/README.md#2025-04-23_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npip install git+https://github.com/Nixtla/datasetsforecast.git\npip install git+https://github.com/Nixtla/neuralforecast.git\n```\n\n----------------------------------------\n\nTITLE: Duplicated Documentation for attention_weights Method - Python\nDESCRIPTION: A duplicate call to show_doc for TFT.attention_weights, which may be redundant but reflects repeated invocation for documentation, possibly for different documentation outputs. No further functional change; same import/dependency requirements as other show_doc usages.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(TFT.attention_weights , name='TFT.attention_weights', title_level=3)\n```\n\n----------------------------------------\n\nTITLE: Enabling Autoreload Extensions in Jupyter Notebook\nDESCRIPTION: This code loads the autoreload extension and configures it to automatically reload modules before executing user code. This allows for seamless development where code changes are immediately reflected without manual reloading.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.mlp.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Displaying AutoVanillaTransformer Documentation in Python\nDESCRIPTION: This snippet shows how to display the documentation for the AutoVanillaTransformer class using the show_doc function. It sets the title level to 3 for proper formatting in the documentation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_68\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(AutoVanillaTransformer, title_level=3)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Time Series Forecasts with Matplotlib\nDESCRIPTION: Imports matplotlib library for creating visualizations of the forecasts.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/05_longhorizon_transformers.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\n```\n\n----------------------------------------\n\nTITLE: Displaying Documentation for AutoFEDformer Class\nDESCRIPTION: This snippet generates and displays documentation for the AutoFEDformer class. It uses the show_doc function to create a documentation view with a level 3 header.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_81\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(AutoFEDformer, title_level=3)\n```\n\n----------------------------------------\n\nTITLE: Implementing STAD Module for SOFTS Model in Python\nDESCRIPTION: Defines the STAD (STar Aggregate Dispatch) class, which is a key component of the SOFTS model. It performs series aggregation and dispatching, with different behavior during training and inference.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.softs.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass STAD(nn.Module):\n    \"\"\"\n    STar Aggregate Dispatch Module\n    \"\"\"\n    def __init__(self, d_series, d_core):\n        super(STAD, self).__init__()\n\n\n        self.gen1 = nn.Linear(d_series, d_series)\n        self.gen2 = nn.Linear(d_series, d_core)\n        self.gen3 = nn.Linear(d_series + d_core, d_series)\n        self.gen4 = nn.Linear(d_series, d_series)\n\n    def forward(self, input, *args, **kwargs):\n        batch_size, channels, d_series = input.shape\n\n        # set FFN\n        combined_mean = F.gelu(self.gen1(input))\n        combined_mean = self.gen2(combined_mean)\n\n        # stochastic pooling\n        if self.training:\n            ratio = F.softmax(torch.nan_to_num(combined_mean), dim=1)\n            ratio = ratio.permute(0, 2, 1)\n            ratio = ratio.reshape(-1, channels)\n            indices = torch.multinomial(ratio, 1)\n            indices = indices.view(batch_size, -1, 1).permute(0, 2, 1)\n            combined_mean = torch.gather(combined_mean, 1, indices)\n            combined_mean = combined_mean.repeat(1, channels, 1)\n        else:\n            weight = F.softmax(combined_mean, dim=1)\n            combined_mean = torch.sum(combined_mean * weight, dim=1, keepdim=True).repeat(1, channels, 1)\n\n        # mlp fusion\n        combined_mean_cat = torch.cat([input, combined_mean], -1)\n        combined_mean_cat = F.gelu(self.gen3(combined_mean_cat))\n        combined_mean_cat = self.gen4(combined_mean_cat)\n        output = combined_mean_cat\n\n        return output, None\n```\n\n----------------------------------------\n\nTITLE: Displaying Documentation for VanillaTransformer Class in Python\nDESCRIPTION: This snippet calls the `show_doc` function to automatically generate and display the documentation for the previously defined `VanillaTransformer` class. This is common in environments like Jupyter notebooks using documentation tools.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.vanillatransformer.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(VanillaTransformer)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Temporal Classification\nDESCRIPTION: Imports necessary libraries including numpy, pandas, sklearn for datasets, matplotlib for visualization, and NeuralForecast components for time series modeling with distribution losses and accuracy metrics.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/16_temporal_classification.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\nfrom sklearn import datasets\n\nimport matplotlib.pyplot as plt\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import MLP, NHITS, LSTM\nfrom neuralforecast.losses.pytorch import DistributionLoss, Accuracy\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging and Visualization Settings\nDESCRIPTION: Sets up the logging level, disables warnings, and configures matplotlib visualization parameters for consistent plotting in the notebook.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n#| hide\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\nwarnings.filterwarnings(\"ignore\")\n\nplt.rcParams[\"axes.grid\"]=True\nplt.rcParams['font.family'] = 'serif'\nplt.rcParams[\"figure.figsize\"] = (6,4)\n```\n\n----------------------------------------\n\nTITLE: Importing NeuralForecast Models and Core Components in Python\nDESCRIPTION: Imports the TimesNet model and NeuralForecast core class from the neuralforecast library, along with logging configuration to suppress verbose pytorch lightning messages.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/07_time_series_scaling.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nfrom neuralforecast.models import TimesNet\nfrom neuralforecast.core import NeuralForecast\n```\n\n----------------------------------------\n\nTITLE: Example: Importing NeuralForecast and Visualization Packages - Python\nDESCRIPTION: Provides user guidance for importing necessary libraries such as matplotlib, numpy, pandas, and NeuralForecast for downstream usage or visualization of the TFT model results. Serves as an entry to practical usage and ensures that plotting and analytical dependencies are in place. Code should be run after installing corresponding packages; no model function is executed, only imports.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# | eval: false\\nimport matplotlib.pyplot as plt\\nimport numpy as np\\nimport pandas as pd\\n\\nfrom neuralforecast import NeuralForecast\\n\n```\n\n----------------------------------------\n\nTITLE: Setting Default Export for FEDformer Model\nDESCRIPTION: Sets the default export for the FEDformer model implementation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.fedformer.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#| default_exp models.fedformer\n```\n\n----------------------------------------\n\nTITLE: Importing Pandas Library\nDESCRIPTION: Imports the pandas library for data manipulation and analysis.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/07_forecasting_tft.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\n```\n\n----------------------------------------\n\nTITLE: Implementing Identity Scaling Functions in Python\nDESCRIPTION: Defines two functions for identity scaling: one for forward scaling and one for inverse scaling. These functions return the input unchanged, acting as a pass-through scaler.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.scalers.ipynb#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\ndef identity_scaler(x, x_shift, x_scale):\n    return x\n\ndef inv_identity_scaler(z, x_shift, x_scale):\n    return z\n```\n\n----------------------------------------\n\nTITLE: Testing AutoFEDformer with Different Backends and Configurations\nDESCRIPTION: This hidden test code verifies that the AutoFEDformer class works correctly with different configurations and backends. It tests Optuna's configuration handling, validates required arguments from BaseAuto, and checks custom configuration with the Optuna backend.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_83\n\nLANGUAGE: python\nCODE:\n```\n# Check Optuna\nassert model.config(MockTrial())['h'] == 12\n\n# Unit test to test that Auto* model contains all required arguments from BaseAuto\ntest_args(AutoFEDformer, exclude_args=['cls_model']) \n\n# Unit test for situation: Optuna with updated default config\nmy_config = AutoFEDformer.get_default_config(h=12, backend='optuna')\ndef my_config_new(trial):\n    config = {**my_config(trial)}\n    config.update({'max_steps': 1, 'val_check_steps': 1, 'input_size': 12, 'hidden_size': 64})\n    return config\n\nmodel = AutoFEDformer(h=12, config=my_config_new, backend='optuna', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: Testing Model Integrity with use_init_models Parameter\nDESCRIPTION: Tests that the use_init_models parameter preserves the original model weights during training. Verifies that initial models remain unchanged while the trained models contain training trajectory information.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_33\n\nLANGUAGE: python\nCODE:\n```\n# Test inplace model fitting\nmodels = [MLP(h=12, input_size=12, max_steps=1, scaler_type='robust')]\ninitial_weights = models[0].mlp[0].weight.detach().clone()\nfcst = NeuralForecast(models=models, freq='M')\nfcst.fit(df=AirPassengersPanel_train, static_df=AirPassengersStatic, use_init_models=True)\nafter_weights = fcst.models_init[0].mlp[0].weight.detach().clone()\nassert np.allclose(initial_weights, after_weights), 'init models should not be modified'\nassert len(fcst.models[0].train_trajectories)>0, 'models stored trajectories should not be empty'\n```\n\n----------------------------------------\n\nTITLE: Importing Testing and Documentation Modules in Python\nDESCRIPTION: This snippet imports additional modules for testing, logging, and documentation purposes. It includes fastcore for testing, nbdev for documentation, and custom model checking functions from neuralforecast.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.vanillatransformer.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport warnings\nfrom fastcore.test import test_eq\nfrom nbdev.showdoc import show_doc\nfrom neuralforecast.common._model_checks import check_model\n```\n\n----------------------------------------\n\nTITLE: Installing NeuralForecast and DatasetsForecast Libraries in Python\nDESCRIPTION: Installs the required libraries for time series forecasting using pip. The %%capture magic command suppresses the installation output.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/17_transfer_learning.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n!pip install datasetsforecast neuralforecast\n```\n\n----------------------------------------\n\nTITLE: Loading Jupyter Extensions for Autoreload\nDESCRIPTION: Configures Jupyter notebook to automatically reload modules before executing code, which is useful during development.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for DLinear Model\nDESCRIPTION: Imports necessary modules and classes for implementing the DLinear model, including PyTorch components and custom loss functions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.dlinear.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\n\nfrom neuralforecast.common._base_model import BaseModel\n\nfrom neuralforecast.losses.pytorch import MAE\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for NBEATSx\nDESCRIPTION: Imports necessary Python libraries and modules for implementing the NBEATSx model, including PyTorch for neural network operations.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeatsx.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Tuple, Optional\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom neuralforecast.losses.pytorch import MAE\nfrom neuralforecast.common._base_model import BaseModel\n```\n\n----------------------------------------\n\nTITLE: Importing Data Processing Libraries in Python\nDESCRIPTION: Imports pandas for data manipulation and matplotlib for visualization which are essential for handling time series data and visualizing forecasting results.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/07_time_series_scaling.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nimport matplotlib.pyplot as plt\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging Level\nDESCRIPTION: Sets PyTorch Lightning logging level to ERROR to reduce output verbosity.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/05_predictInsample.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlogging.getLogger('pytorch_lightning').setLevel(logging.ERROR)\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for MLP Implementation\nDESCRIPTION: This snippet imports the necessary modules and classes for implementing the MLP model. It includes imports from PyTorch, NeuralForecast, and typing utilities.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.mlp.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\n\nfrom neuralforecast.losses.pytorch import MAE\nfrom neuralforecast.common._base_model import BaseModel\n```\n\n----------------------------------------\n\nTITLE: Importing NeuralForecast Dependencies\nDESCRIPTION: Imports required libraries including logging, pandas, and NeuralForecast components for model training.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/05_predictInsample.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport pandas as pd\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import NHITS\n```\n\n----------------------------------------\n\nTITLE: Logging Configuration\nDESCRIPTION: Configures PyTorch Lightning logging level and suppresses warnings.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.deepar.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\nwarnings.filterwarnings(\"ignore\")\n```\n\n----------------------------------------\n\nTITLE: Importing Required Dependencies\nDESCRIPTION: Imports necessary Python packages and modules including PyTorch, numpy, and custom loss functions/base models.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.bitcn.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\nfrom neuralforecast.losses.pytorch import MAE\nfrom neuralforecast.common._base_model import BaseModel\n```\n\n----------------------------------------\n\nTITLE: Documentation Display Commands for MQLoss Class in Python\nDESCRIPTION: Commands to display documentation for the MQLoss class and its __call__ method using a custom show_doc function. These commands are meant to generate documentation when this code is run in a notebook environment.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(MQLoss, name='MQLoss.__init__', title_level=3)\n```\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(MQLoss.__call__, name='MQLoss.__call__', title_level=3)\n```\n\n----------------------------------------\n\nTITLE: Core PyTorch Imports for StemGNN Neural Network\nDESCRIPTION: Essential PyTorch imports including nn module and functional utilities needed for neural network implementation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.stemgnn.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom typing import Optional\nfrom neuralforecast.losses.pytorch import MAE\nfrom neuralforecast.common._base_model import BaseModel\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch Lightning Logging Level in Python\nDESCRIPTION: Sets the logging level for PyTorch Lightning to WARNING to reduce verbose output during model training and inference.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/17_transfer_learning.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\n```\n\n----------------------------------------\n\nTITLE: Configuring Autoreload Extension\nDESCRIPTION: Loads and enables the autoreload extension which automatically reloads Python modules before executing code cells.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/tsdataset.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Installing NeuralForecast and Dependencies in Python\nDESCRIPTION: Sets up the required packages for neural forecasting experiments by installing the NeuralForecast library and hyperopt for hyperparameter optimization.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/07_time_series_scaling.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n!pip install neuralforecast\n!pip install hyperopt\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for TSMixer Implementation in Python\nDESCRIPTION: Imports necessary PyTorch modules and custom components for implementing the TSMixer model. This includes neural network layers, functional operations, and custom loss functions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tsmixer.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom typing import Optional\nfrom neuralforecast.losses.pytorch import MAE\nfrom neuralforecast.common._base_model import BaseModel\nfrom neuralforecast.common._modules import RevINMultivariate\n```\n\n----------------------------------------\n\nTITLE: Loading Jupyter Notebook Extensions\nDESCRIPTION: Code to load and configure Jupyter notebook extensions, specifically enabling the autoreload functionality which automatically reloads modules before executing user code.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/getting-started/01_introduction.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Setting Default Export for TCN Model in Python\nDESCRIPTION: This code snippet sets the default export for the TCN model in the NeuralForecast project. It uses a Jupyter notebook cell magic command to specify the module path.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tcn.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#| default_exp models.tcn\n```\n\n----------------------------------------\n\nTITLE: GMM Documentation Display\nDESCRIPTION: Code to display documentation for the GMM class initialization method using a show_doc utility function.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_53\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(GMM, name='GMM.__init__', title_level=3)\n```\n\n----------------------------------------\n\nTITLE: Performing Cross-Validation for Multiple Historic Forecasts\nDESCRIPTION: Conducts time-series cross-validation by producing forecasts for multiple historical windows, with validation and test sizes of 90 days and 30 days respectively. The step size is set to 24 hours to produce daily forecasts.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/07_forecasting_tft.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n%%capture\nval_size  = 90*24 # 90 days x 24 hours\ntest_size = 30*24 # 30 days x 24 hours\nfcst_df = nf.cross_validation(df=Y_df, val_size=val_size, test_size=test_size,\n                                n_windows=None, step_size=horizon)\n```\n\n----------------------------------------\n\nTITLE: GMM Sample Method Documentation\nDESCRIPTION: Code to display documentation for the GMM sample method using a show_doc utility function.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_54\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(GMM.sample, name='GMM.sample', title_level=3)\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for StemGNN Implementation\nDESCRIPTION: Required imports for the StemGNN implementation including PyTorch, logging and testing utilities.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.stemgnn.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport logging\nimport warnings\nfrom fastcore.test import test_eq\nfrom nbdev.showdoc import show_doc\nfrom neuralforecast.common._model_checks import check_model\n```\n\n----------------------------------------\n\nTITLE: Creating Conda Environment for Long Horizon Experiments\nDESCRIPTION: Creates a new conda environment named 'long_horizon' using the environment.yml configuration file.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/experiments/long_horizon/README.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nconda env create -f environment.yml\n```\n\n----------------------------------------\n\nTITLE: Installing NeuralForecast Library\nDESCRIPTION: Installs the NeuralForecast library using pip in a Jupyter notebook cell with output capture to avoid verbose installation logs.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/14_interpretable_decompositions.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n!pip install neuralforecast\n```\n\n----------------------------------------\n\nTITLE: Updating NeuralForecast Environment with CPU or CUDA Support\nDESCRIPTION: Commands for updating the conda environment with either CPU-only or CUDA-enabled dependencies using the appropriate environment files.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/CONTRIBUTING.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda env update -f environment-cpu.yml  # choose this if you want to install the CPU-only version of neuralforecast\nconda env update -f environment-cuda.yml # choose this if you want to install the CUDA-enabled version of neuralforecast\n```\n\n----------------------------------------\n\nTITLE: Activating Conda Environment\nDESCRIPTION: Command to activate the newly created neuralforecast conda environment.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/getting-started/04_installation.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nconda activate neuralforecast\n```\n\n----------------------------------------\n\nTITLE: Setting Logging Levels\nDESCRIPTION: Configures logging levels for PyTorch Lightning components and suppresses warnings.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.deepnpts.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\nlogging.getLogger(\"lightning_fabric\").setLevel(logging.ERROR)\nwarnings.filterwarnings(\"ignore\")\n```\n\n----------------------------------------\n\nTITLE: Setting Default Export for LSTM Model in Python\nDESCRIPTION: This code snippet sets the default export for the LSTM model file. It specifies that the 'models.lstm' module should be the default export when this file is imported.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.lstm.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#| default_exp models.lstm\n```\n\n----------------------------------------\n\nTITLE: Installing NeuralForecast using conda\nDESCRIPTION: Command to install the NeuralForecast library using conda package manager from the conda-forge channel. This is an alternative installation method for users who prefer conda environments.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/README.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nconda install -c conda-forge neuralforecast\n```\n\n----------------------------------------\n\nTITLE: Configuring Jupyter Development Environment for NBEATSx Model in Python\nDESCRIPTION: Sets up Jupyter notebook development environment by loading the autoreload extension and enabling automatic reloading of imported modules, which helps with development and debugging of the NBEATSx model implementation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeatsx.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for iTransformer Model in Python\nDESCRIPTION: This snippet imports necessary Python libraries and modules for implementing the iTransformer model. It includes PyTorch for neural network operations and custom modules from neuralforecast for specific components.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.itransformer.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom typing import Optional\nfrom neuralforecast.losses.pytorch import MAE\nfrom neuralforecast.common._base_model import BaseModel\n\nfrom neuralforecast.common._modules import (\n    TransEncoder, \n    TransEncoderLayer, \n    AttentionLayer, \n    FullAttention, \n    DataEmbedding_inverted\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing MinMax Scaler for Neural Forecasting in Python\nDESCRIPTION: Calculates MinMax scaling statistics for temporal features, ensuring the range is between [0,1]. It handles masked values and includes safeguards against division by zero.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.scalers.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef minmax_statistics(x, mask, eps=1e-6, dim=-1):\n    mask = mask.clone()\n    mask[mask==0] = torch.inf\n    mask[mask==1] = 0\n    x_max = torch.max(torch.nan_to_num(x-mask,nan=-torch.inf), dim=dim, keepdim=True)[0]\n    x_min = torch.min(torch.nan_to_num(x+mask,nan=torch.inf), dim=dim, keepdim=True)[0]\n    x_max = x_max.type(x.dtype)\n    x_min = x_min.type(x.dtype)\n\n    x_range = x_max - x_min\n    x_range[x_range==0] = 1.0\n    x_range = x_range + eps\n    return x_min, x_range\n```\n\n----------------------------------------\n\nTITLE: Setting Default Export for Dilated RNN Module in Python\nDESCRIPTION: This code snippet sets the default export for the dilated_rnn module within the models package. It uses a Jupyter notebook cell magic command to configure the export.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.dilated_rnn.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#| default_exp models.dilated_rnn\n```\n\n----------------------------------------\n\nTITLE: Setting Default Export Variable in NeuralForecast\nDESCRIPTION: Sets the default export variable name to 'compat' using Jupyter notebook cell magic.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/compat.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#| default_exp compat\n```\n\n----------------------------------------\n\nTITLE: Enabling Autoreload Extension\nDESCRIPTION: Loads and configures the autoreload extension for Jupyter notebooks.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.rmok.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Import Required Dependencies\nDESCRIPTION: Imports necessary Python libraries for time series data manipulation and visualization.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/utils.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#| export\nimport random\nfrom itertools import chain\nfrom typing import List, Union, Optional, Tuple\nfrom utilsforecast.compat import DFType\n\nimport numpy as np\nimport pandas as pd\n```\n\n----------------------------------------\n\nTITLE: Initializing Default Export Settings in Python\nDESCRIPTION: Sets up the default export settings for the nbdev file to specify the module path for the new model.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/18_adding_models.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#| default_exp models.mlp\n```\n\n----------------------------------------\n\nTITLE: Initializing Jupyter Notebook Default Export Configuration\nDESCRIPTION: Sets the default export cell for the Jupyter notebook using nbdev directives.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#| default_exp auto\n```\n\n----------------------------------------\n\nTITLE: Setting Default Export Path for RNN Models\nDESCRIPTION: Configures the default export path for RNN model modules using a Jupyter notebook cell magic command.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.rnn.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#| default_exp models.rnn\n```\n\n----------------------------------------\n\nTITLE: Generating Test Data for Model Validation\nDESCRIPTION: Creates multiple test datasets with different configurations including single/multiple series and with/without exogenous variables to thoroughly test models under various conditions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.model_checks.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#| export\nseed = 0\ntest_size = 14\nFREQ = \"D\"\n\n# 1 series, no exogenous\nN_SERIES_1 = 1\ndf = generate_series(n_series=N_SERIES_1, seed=seed, freq=FREQ, equal_ends=True)\nmax_ds = df.ds.max() - pd.Timedelta(test_size, FREQ)\nY_TRAIN_DF_1 = df[df.ds < max_ds]\nY_TEST_DF_1 = df[df.ds >= max_ds]\n\n# 5 series, no exogenous\nN_SERIES_2 = 5\ndf = generate_series(n_series=N_SERIES_2, seed=seed, freq=FREQ, equal_ends=True)\nmax_ds = df.ds.max() - pd.Timedelta(test_size, FREQ)\nY_TRAIN_DF_2 = df[df.ds < max_ds]\nY_TEST_DF_2 = df[df.ds >= max_ds]\n\n# 1 series, with static and temporal exogenous\nN_SERIES_3 = 1\ndf, STATIC_3 = generate_series(n_series=N_SERIES_3, n_static_features=2, \n                     n_temporal_features=2, seed=seed, freq=FREQ, equal_ends=True)\nmax_ds = df.ds.max() - pd.Timedelta(test_size, FREQ)\nY_TRAIN_DF_3 = df[df.ds < max_ds]\nY_TEST_DF_3 = df[df.ds >= max_ds]\n\n# 5 series, with static and temporal exogenous\nN_SERIES_4 = 5\ndf, STATIC_4 = generate_series(n_series=N_SERIES_4, n_static_features=2, \n                     n_temporal_features=2, seed=seed, freq=FREQ, equal_ends=True)\nmax_ds = df.ds.max() - pd.Timedelta(test_size, FREQ)\nY_TRAIN_DF_4 = df[df.ds < max_ds]\nY_TEST_DF_4 = df[df.ds >= max_ds]\n\n# Generic test for a given config for a model\ndef _run_model_tests(model_class, config):\n    if model_class.RECURRENT:\n        config[\"inference_input_size\"] = config[\"input_size\"]\n\n    # DF_1\n    if model_class.MULTIVARIATE:\n        config[\"n_series\"] = N_SERIES_1\n    if isinstance(config[\"loss\"], losses.relMSE):\n        config[\"loss\"].y_train = Y_TRAIN_DF_1[\"y\"].values   \n    if isinstance(config[\"valid_loss\"], losses.relMSE):\n        config[\"valid_loss\"].y_train = Y_TRAIN_DF_1[\"y\"].values   \n\n    model = model_class(**config)\n    fcst = NeuralForecast(models=[model], freq=FREQ)\n    fcst.fit(df=Y_TRAIN_DF_1, val_size=24)\n    _ = fcst.predict(futr_df=Y_TEST_DF_1)\n    # DF_2\n    if model_class.MULTIVARIATE:\n        config[\"n_series\"] = N_SERIES_2\n    if isinstance(config[\"loss\"], losses.relMSE):\n        config[\"loss\"].y_train = Y_TRAIN_DF_2[\"y\"].values   \n    if isinstance(config[\"valid_loss\"], losses.relMSE):\n        config[\"valid_loss\"].y_train = Y_TRAIN_DF_2[\"y\"].values\n    model = model_class(**config)\n    fcst = NeuralForecast(models=[model], freq=FREQ)\n    fcst.fit(df=Y_TRAIN_DF_2, val_size=24)\n    _ = fcst.predict(futr_df=Y_TEST_DF_2)\n\n    if model.EXOGENOUS_STAT and model.EXOGENOUS_FUTR:\n        # DF_3\n        if model_class.MULTIVARIATE:\n            config[\"n_series\"] = N_SERIES_3\n        if isinstance(config[\"loss\"], losses.relMSE):\n            config[\"loss\"].y_train = Y_TRAIN_DF_3[\"y\"].values   \n        if isinstance(config[\"valid_loss\"], losses.relMSE):\n            config[\"valid_loss\"].y_train = Y_TRAIN_DF_3[\"y\"].values\n        model = model_class(**config)\n        fcst = NeuralForecast(models=[model], freq=FREQ)\n        fcst.fit(df=Y_TRAIN_DF_3, static_df=STATIC_3, val_size=24)\n        _ = fcst.predict(futr_df=Y_TEST_DF_3)\n\n        # DF_4\n        if model_class.MULTIVARIATE:\n            config[\"n_series\"] = N_SERIES_4\n        if isinstance(config[\"loss\"], losses.relMSE):\n            config[\"loss\"].y_train = Y_TRAIN_DF_4[\"y\"].values   \n        if isinstance(config[\"valid_loss\"], losses.relMSE):\n            config[\"valid_loss\"].y_train = Y_TRAIN_DF_4[\"y\"].values \n        model = model_class(**config)\n        fcst = NeuralForecast(models=[model], freq=FREQ)\n        fcst.fit(df=Y_TRAIN_DF_4, static_df=STATIC_4, val_size=24)\n        _ = fcst.predict(futr_df=Y_TEST_DF_4) \n\n# Tests a model against every loss function\ndef check_loss_functions(model_class):\n    loss_list = [losses.MAE(), losses.MSE(), losses.RMSE(), losses.MAPE(), losses.SMAPE(), losses.MASE(seasonality=7), \n              losses.QuantileLoss(q=0.5), losses.MQLoss(), losses.IQLoss(), losses.HuberIQLoss(), losses.DistributionLoss(\"Normal\"), \n              losses.DistributionLoss(\"StudentT\"), losses.DistributionLoss(\"Poisson\"), losses.DistributionLoss(\"NegativeBinomial\"), \n              losses.DistributionLoss(\"Tweedie\", rho=1.5), losses.DistributionLoss(\"ISQF\"), losses.PMM(), losses.PMM(weighted=True), \n              losses.GMM(), losses.GMM(weighted=True), losses.NBMM(), losses.NBMM(weighted=True), losses.HuberLoss(), \n            losses.TukeyLoss(), losses.HuberQLoss(q=0.5), losses.HuberMQLoss()]\n    for loss in loss_list:\n        test_name = f\"{model_class.__name__}: checking {loss._get_name()}\"\n        print(f\"{test_name}\")\n        config = {'max_steps': 2,\n            'h': 7,\n            'input_size': 28,\n            'loss': loss,\n            'valid_loss': None,\n            'enable_progress_bar': False,\n            'enable_model_summary': False,\n            'val_check_steps': 2}        \n        try:\n            _run_model_tests(model_class, config) \n        except RuntimeError:\n            raise Exception(f\"{test_name} failed.\")\n        except Exception:\n            print(f\"{test_name} skipped on raised Exception.\")\n            pass\n\n# Tests a model against the AirPassengers dataset\ndef check_airpassengers(model_class):\n    print(f\"{model_class.__name__}: checking forecast AirPassengers dataset\")\n    Y_train_df = AirPassengersPanel[AirPassengersPanel.ds<AirPassengersPanel['ds'].values[-12]] # 132 train\n    Y_test_df = AirPassengersPanel[AirPassengersPanel.ds>=AirPassengersPanel['ds'].values[-12]].reset_index(drop=True) # 12 test\n\n    config = {'max_steps': 2,\n        'h': 12,\n        'input_size': 24,\n        'enable_progress_bar': False,\n        'enable_model_summary': False,\n        'val_check_steps': 2,\n        }\n\n    if model_class.MULTIVARIATE:\n        config[\"n_series\"] = Y_train_df[\"unique_id\"].nunique()\n    # Normal forecast\n    fcst = NeuralForecast(models=[model_class(**config)], freq='M')\n    fcst.fit(df=Y_train_df, static_df=AirPassengersStatic)\n    _ = fcst.predict(futr_df=Y_test_df)   \n\n    # Cross-validation\n    fcst = NeuralForecast(models=[model_class(**config)], freq='M')\n    _ = fcst.cross_validation(df=AirPassengersPanel, static_df=AirPassengersStatic, n_windows=2, step_size=12)\n\n# Add unit test functions to this function\ndef check_model(model_class, checks=[\"losses\", \"airpassengers\"]):\n    \"\"\"\n    Check model with various tests. Options for checks are:<br>\n    \"losses\": test the model against all loss functions<br>\n    \"airpassengers\": test the model against the airpassengers dataset for forecasting and cross-validation<br>\n    \n    \"\"\"\n    if \"losses\" in checks:\n        check_loss_functions(model_class)   \n    if \"airpassengers\" in checks:\n        try:\n            check_airpassengers(model_class)   \n        except RuntimeError:\n            raise Exception(f\"{model_class.__name__}: AirPassengers forecast test failed.\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Division with NaN Handling in NumPy\nDESCRIPTION: Defines an auxiliary function to handle division by zero and infinity cases in NumPy arrays, replacing them with zero values.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.numpy.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef _divide_no_nan(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n    \"\"\"\n    Auxiliary funtion to handle divide by 0\n    \"\"\"\n    div = a / b\n    div[div != div] = 0.0\n    div[div == float('inf')] = 0.0\n    return div\n```\n\n----------------------------------------\n\nTITLE: Installing NeuralForecast in Development Mode\nDESCRIPTION: Bash commands to clone the NeuralForecast repository from GitHub and install it in development mode, allowing code modifications to take effect without reinstallation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/getting-started/01_introduction.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/Nixtla/neuralforecast.git\ncd neuralforecast\npip install -e .\n```\n\n----------------------------------------\n\nTITLE: Handling PySpark DataFrame Import with Fallback Implementation\nDESCRIPTION: Attempts to import the SparkDataFrame class from PySpark. If PySpark is not installed (ImportError), it creates an empty class definition as a placeholder to avoid errors.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/compat.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#| export\ntry:\n    from pyspark.sql import DataFrame as SparkDataFrame\nexcept ImportError:\n    class SparkDataFrame: ...\n```\n\n----------------------------------------\n\nTITLE: Setting Default Export for RMoK Model\nDESCRIPTION: Sets the default export for the RMoK model implementation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.rmok.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#| default_exp models.rmok\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging Level for PyTorch Lightning\nDESCRIPTION: Setting the logging level for PyTorch Lightning to ERROR to reduce verbose output during model training.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/getting-started/01_introduction.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nlogging.getLogger('pytorch_lightning').setLevel(logging.ERROR)\n```\n\n----------------------------------------\n\nTITLE: Enabling Autoreload for Development\nDESCRIPTION: Sets up the autoreload extension to automatically reload Python modules before executing code. This is useful during development to ensure the latest version of the code is always used.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeats.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Importing Data Visualization Libraries\nDESCRIPTION: Imports pandas and plotting utilities required for data manipulation and visualization.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/03_exogenous_variables.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom utilsforecast.plotting import plot_series\n```\n\n----------------------------------------\n\nTITLE: Running KAN Benchmark Experiments\nDESCRIPTION: Executes the benchmark experiments using the run_experiment.py script. The --dataset parameter specifies which dataset to use, with options including various M3 and M4 dataset frequencies.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/experiments/kan_benchmark/README.md#2025-04-23_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npython run_experiment.py --dataset\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for FEDformer Model\nDESCRIPTION: Imports necessary libraries and modules for implementing the FEDformer model, including PyTorch, numpy, and custom modules from neuralforecast.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.fedformer.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom neuralforecast.common._modules import DataEmbedding\nfrom neuralforecast.common._modules import SeriesDecomp\nfrom neuralforecast.common._base_model import BaseModel\n\nfrom neuralforecast.losses.pytorch import MAE\n```\n\n----------------------------------------\n\nTITLE: Defining Module Name for NBEATSx Model in Python\nDESCRIPTION: Sets the default export name for the Jupyter notebook as 'models.nbeatsx', indicating this file defines the NBEATSx model implementation within the models package.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeatsx.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#| default_exp models.nbeatsx\n```\n\n----------------------------------------\n\nTITLE: Loading AirPassengers Dataset\nDESCRIPTION: Imports and loads the AirPassengers dataset from NeuralForecast utilities.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/05_predictInsample.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom neuralforecast.utils import AirPassengersDF\n```\n\n----------------------------------------\n\nTITLE: Plotting Coherent Probabilistic Forecast in Python\nDESCRIPTION: This snippet shows how to plot the coherent probabilistic forecast for a specific unique_id in the dataset. It includes the true values, mean and median predictions, and confidence intervals.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.hint.ipynb#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Plot coherent probabilistic forecast\nunique_id = 'TotalAll'\nY_plot_df = Y_df[Y_df.unique_id==unique_id]\nplot_df = Y_hat_df[Y_hat_df.unique_id==unique_id]\nplot_df = Y_plot_df.merge(plot_df, on=['ds', 'unique_id'], how='left')\nn_years = 5\n\nplt.plot(plot_df['ds'][-12*n_years:], plot_df['y_x'][-12*n_years:], c='black', label='True')\nplt.plot(plot_df['ds'][-12*n_years:], plot_df['HINT'][-12*n_years:], c='purple', label='mean')\nplt.plot(plot_df['ds'][-12*n_years:], plot_df['HINT-median'][-12*n_years:], c='blue', label='median')\nplt.fill_between(x=plot_df['ds'][-12*n_years:],\n                 y1=plot_df['HINT-lo-90'][-12*n_years:].values,\n                 y2=plot_df['HINT-hi-90'][-12*n_years:].values,\n                 alpha=0.4, label='level 90')\nplt.legend()\nplt.grid()\nplt.plot()\n```\n\n----------------------------------------\n\nTITLE: Installing NeuralForecast via conda\nDESCRIPTION: Command to install the NeuralForecast package from the conda-forge channel using conda.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/getting-started/01_introduction.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nconda install -c conda-forge neuralforecast\n```\n\n----------------------------------------\n\nTITLE: Importing PyTorch Dependencies\nDESCRIPTION: Basic PyTorch imports required for implementing neural network components and tensor operations.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.scalers.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\n```\n\n----------------------------------------\n\nTITLE: Setting Default Export Module in Jupyter\nDESCRIPTION: Configures the default export module name for the notebook as 'models.mlpmultivariate'.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.mlpmultivariate.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#| default_exp models.mlpmultivariate\n```\n\n----------------------------------------\n\nTITLE: Installing NeuralForecast Packages\nDESCRIPTION: Installs the required Python packages neuralforecast and datasetsforecast for implementing predictive maintenance models.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/use-cases/predictive_maintenance.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n!pip install neuralforecast datasetsforecast\n```\n\n----------------------------------------\n\nTITLE: Enabling Autoreload Extension in Jupyter Notebook\nDESCRIPTION: This code snippet loads the autoreload extension and sets it to automatically reload modules before executing user code. This is useful for development to ensure the latest version of imported modules is always used.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tcn.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Importing Libraries for Data Manipulation\nDESCRIPTION: Imports NumPy and pandas libraries for numerical operations and data manipulation in the time series analysis process.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/14_interpretable_decompositions.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\n```\n\n----------------------------------------\n\nTITLE: Activating Conda Environment for KAN Benchmark\nDESCRIPTION: Activates the 'kan_benchmark' conda environment, which contains the required dependencies for running the benchmark experiments.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/experiments/kan_benchmark/README.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nconda activate kan_benchmark\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries\nDESCRIPTION: Install neuralforecast and datasetsforecast packages using pip\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/06_longhorizon_probabilistic.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n!pip install neuralforecast datasetsforecast\n```\n\n----------------------------------------\n\nTITLE: Plotting TFT Model Forecasts with Quantiles\nDESCRIPTION: Creates a visualization of the TFT model's forecasts alongside the actual values, including median prediction and uncertainty intervals. The plot shows both training and test data with 90% confidence intervals for the forecast period.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tft.ipynb#2025-04-23_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# Plot quantile predictions\nY_hat_df = Y_hat_df.reset_index(drop=False).drop(columns=[\"unique_id\", \"ds\"])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id == \"Airline1\"].drop(\"unique_id\", axis=1)\nplt.plot(plot_df[\"ds\"], plot_df[\"y\"], c=\"black\", label=\"True\")\nplt.plot(plot_df[\"ds\"], plot_df[\"TFT\"], c=\"purple\", label=\"mean\")\nplt.plot(plot_df[\"ds\"], plot_df[\"TFT-median\"], c=\"blue\", label=\"median\")\nplt.fill_between(\n    x=plot_df[\"ds\"][-12:],\n    y1=plot_df[\"TFT-lo-90\"][-12:].values,\n    y2=plot_df[\"TFT-hi-90\"][-12:].values,\n    alpha=0.4,\n    label=\"level 90\",\n)\nplt.legend()\nplt.grid()\nplt.plot()\n```\n\n----------------------------------------\n\nTITLE: Configuring Autoreload for Development in Python\nDESCRIPTION: This code snippet sets up the development environment by enabling autoreload. It loads the autoreload extension and sets it to automatically reload modules before executing user code.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.lstm.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Calculating Mean Absolute Error (MAE) for Time Series Forecasts\nDESCRIPTION: Implements the Mean Absolute Error metric for evaluating time series forecasts. It supports optional weights and axis parameters for flexible calculations across multiple series.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.numpy.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef mae(y: np.ndarray, y_hat: np.ndarray,\n        weights: Optional[np.ndarray] = None,\n        axis: Optional[int] = None) -> Union[float, np.ndarray]:\n    \"\"\"Mean Absolute Error\n\n    Calculates Mean Absolute Error between\n    `y` and `y_hat`. MAE measures the relative prediction\n    accuracy of a forecasting method by calculating the\n    deviation of the prediction and the true\n    value at a given time and averages these devations\n    over the length of the series.\n\n    $$ \\mathrm{MAE}(\\mathbf{y}_{\\tau}, \\mathbf{\\hat{y}}_{\\tau}) = \\frac{1}{H} \\sum^{t+H}_{\\tau=t+1} |y_{\\tau} - \\hat{y}_{\\tau}| $$\n\n    **Parameters:**<br>\n    `y`: numpy array, Actual values.<br>\n    `y_hat`: numpy array, Predicted values.<br>\n    `mask`: numpy array, Specifies date stamps per serie to consider in loss.<br>\n\n    **Returns:**<br>\n    `mae`: numpy array, (single value).    \n    \"\"\"\n    _metric_protections(y, y_hat, weights)\n    \n    delta_y = np.abs(y - y_hat)\n    if weights is not None:\n        mae = np.average(delta_y[~np.isnan(delta_y)], \n                         weights=weights[~np.isnan(delta_y)],\n                         axis=axis)\n    else:\n        mae = np.nanmean(delta_y, axis=axis)\n        \n    return mae\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging Level\nDESCRIPTION: Sets the logging level for PyTorch Lightning to warnings only.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/03_exogenous_variables.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nlogging.getLogger(\"pytorch_lightning\").setLevel(logging.WARNING)\n```\n\n----------------------------------------\n\nTITLE: Calculating Time Series Lags with FFT-based Correlation\nDESCRIPTION: Calculates the lag correlations of the time series using Fast Fourier Transform (FFT). This method identifies the top k lag values which are used to inform the forecasting process.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timellm.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef calcute_lags(self, x_enc):\n    q_fft = torch.fft.rfft(x_enc.permute(0, 2, 1).contiguous(), dim=-1)\n    k_fft = torch.fft.rfft(x_enc.permute(0, 2, 1).contiguous(), dim=-1)\n    res = q_fft * torch.conj(k_fft)\n    corr = torch.fft.irfft(res, dim=-1)\n    mean_value = torch.mean(corr, dim=1)\n    _, lags = torch.topk(mean_value, self.top_k, dim=-1)\n    return lags\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Time Series Analysis\nDESCRIPTION: Imports the pandas library for data manipulation and the M3 class from datasetsforecast.m3 module which provides access to the M3 competition datasets.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/getting-started/05_datarequirements.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport pandas as pd\nfrom datasetsforecast.m3 import M3\n```\n\n----------------------------------------\n\nTITLE: Defining Supported Activation Functions\nDESCRIPTION: Defines a list of supported activation functions that can be used in the neural network modules.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.modules.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#| export\nACTIVATIONS = ['ReLU','Softplus','Tanh','SELU','LeakyReLU','PReLU','Sigmoid']\n```\n\n----------------------------------------\n\nTITLE: Enabling Autoreload for Development in Jupyter Notebook\nDESCRIPTION: Loads the autoreload extension and configures it to automatically reload modules before executing user code. This is useful during development to ensure changes in imported modules are reflected without restarting the kernel.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_auto.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Implementing Identity Scaler for Neural Forecasting in Python\nDESCRIPTION: Provides a placeholder identity scaler that returns the original input without any transformation. It's argument-insensitive and maintains the input's shape.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.scalers.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndef identity_statistics(x, mask, dim=-1, eps=1e-6):\n    shape = list(x.shape)\n    return None, None\n```\n\n----------------------------------------\n\nTITLE: Installing Required Libraries for Transformer-based Forecasting\nDESCRIPTION: Installs the necessary packages neuralforecast and datasetsforecast for implementing transformer-based forecasting models.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/05_longhorizon_transformers.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n!pip install neuralforecast datasetsforecast\n```\n\n----------------------------------------\n\nTITLE: Configuring Jupyter Notebook Cell for DeepNPTS Model\nDESCRIPTION: This code snippet sets up a Jupyter Notebook cell for the DeepNPTS model implementation. It specifies the default export location for the model code.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.deepnpts.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#| default_exp models.deepnpts\n```\n\n----------------------------------------\n\nTITLE: Installing NeuralForecast Package\nDESCRIPTION: Installation of the neuralforecast package using pip\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/06_save_load_models.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n!pip install neuralforecast\n```\n\n----------------------------------------\n\nTITLE: Plotting Airline1 forecast results with matplotlib\nDESCRIPTION: Creates a visualization of forecasting results for Airline1 series. The plot combines training data with forecasts from multiple models, showing the historical and predicted passenger numbers over time.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nplot_df = pd.concat([AirPassengersPanel_train, forecasts.reset_index()]).set_index('ds')\n\nplot_df[plot_df['unique_id']=='Airline1'].drop(['unique_id','trend','y_[lag12]'], axis=1).plot(ax=ax, linewidth=2)\n\nax.set_title('AirPassengers Forecast', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Timestamp [t]', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\n```\n\n----------------------------------------\n\nTITLE: Normalizing and Projecting Encoder Outputs in TimesNet - Python\nDESCRIPTION: This snippet shows a core section of the TimesNet neural network's forward method, where outputs from a sequence of encoder layers are normalized, projected into the forecast dimension, and then sliced to yield the final forecast. It expects layer normalization and projection modules to be initialized, and the variable \\\"self.h\\\" determines the forecast horizon. Inputs and outputs are PyTorch tensors. This code is intended to be part of a larger class implementing TimesNet for time series forecasting.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timesnet.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfor i in range(self.encoder_layers):\n    enc_out = self.layer_norm(self.model[i](enc_out))\n# porject back\ndec_out = self.projection(enc_out)\n\nforecast = dec_out[:, -self.h:]\nreturn forecast\n```\n\n----------------------------------------\n\nTITLE: Matplotlib Configuration Setup\nDESCRIPTION: Configuration settings for matplotlib visualization, setting grid, font family and figure size parameters.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.scalers.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nplt.rcParams[\"axes.grid\"]=True\nplt.rcParams['font.family'] = 'serif'\nplt.rcParams[\"figure.figsize\"] = (4,2)\n```\n\n----------------------------------------\n\nTITLE: Importing Time Series Test Utilities and Data - Python\nDESCRIPTION: Imports pandas and matplotlib for data manipulation and plotting, and loads AirPassengers example data and the TimeSeriesDataset utility from neuralforecast. Used to prepare, train, and visualize time series forecasts. Prerequisite for running model training and evaluation examples.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeats.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n#| hide\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom neuralforecast.tsdataset import TimeSeriesDataset\nfrom neuralforecast.utils import AirPassengersDF as Y_df\n```\n\n----------------------------------------\n\nTITLE: Installing NeuralForecast Library\nDESCRIPTION: Installs the NeuralForecast library using pip in a Jupyter notebook cell with output capture.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/05_predictInsample.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n!pip install neuralforecast\n```\n\n----------------------------------------\n\nTITLE: Testing Invalid Optimizer Class\nDESCRIPTION: Validates that an error is raised when invalid optimizer class is provided.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_61\n\nLANGUAGE: python\nCODE:\n```\ntest_fail(lambda: NHITS(h=12, input_size=24, max_steps=10, optimizer=torch.nn.Module), contains=\"optimizer is not a valid subclass of torch.optim.Optimizer\")\ntest_fail(lambda: RNN(h=12, input_size=24, max_steps=10, optimizer=torch.nn.Module), contains=\"optimizer is not a valid subclass of torch.optim.Optimizer\")\ntest_fail(lambda: StemGNN(h=12, input_size=24, max_steps=10, n_series=2, optimizer=torch.nn.Module), contains=\"optimizer is not a valid subclass of torch.optim.Optimizer\")\n```\n\n----------------------------------------\n\nTITLE: Installing NeuralForecast Package\nDESCRIPTION: Installs the NeuralForecast package using pip with output capture to hide installation logs.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/16_temporal_classification.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n!pip install neuralforecast\n```\n\n----------------------------------------\n\nTITLE: Installing NeuralForecast via conda\nDESCRIPTION: Alternative installation method using conda package manager from conda-forge channel.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/getting-started/04_installation.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nconda install -c conda-forge neuralforecast\n```\n\n----------------------------------------\n\nTITLE: Installing NeuralForecast via pip\nDESCRIPTION: Basic installation command using pip package manager to install NeuralForecast from PyPI.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/getting-started/04_installation.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install neuralforecast\n```\n\n----------------------------------------\n\nTITLE: Visualizing NBEATSx Decomposition Results\nDESCRIPTION: Plots the NBEATSx model's decomposition showing the original signal, forecast, and separate trend and seasonality components.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/14_interpretable_decompositions.ipynb#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfig, ax = plt.subplots(3, 1, figsize=(6, 7))\n\nax[0].plot(Y_test_df['y'].values, label='True', linewidth=4)\nax[0].plot(y_hat.sum(axis=1).flatten(), label='Forecast', color=\"#7B3841\")\nax[0].legend()\nax[0].set_ylabel('Harmonic Signal')\n\nax[1].plot(y_hat[0,1]+y_hat[0,0], label='stack1', color=\"green\")\nax[1].set_ylabel('NBEATSx Trend Stack')\n\nax[2].plot(y_hat[0,2], label='stack2', color=\"orange\")\nax[2].set_ylabel('NBEATSx Seasonality Stack')\nax[2].set_xlabel(r'Prediction $\\tau \\in \\{t+1,..., t+H\\}$')\nplt.show()\n```\n\n----------------------------------------\n\nTITLE: Initializing Jupyter Notebook Extensions in Python\nDESCRIPTION: Loads the autoreload extension and sets it to automatically reload modules before executing user code.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Development Testing Imports\nDESCRIPTION: Imports modules used for development and testing purposes, including logging, warnings, and testing utilities. These imports are hidden in the final output.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.autoformer.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#| hide\nimport logging\nimport warnings\n\nfrom fastcore.test import test_eq\nfrom nbdev.showdoc import show_doc\nfrom neuralforecast.common._model_checks import check_model\n```\n\n----------------------------------------\n\nTITLE: Loading Autoreload Extension for Jupyter Notebook\nDESCRIPTION: This snippet loads the autoreload extension for Jupyter notebooks and sets it to mode 2. This allows for automatic reloading of external Python modules, which is useful during development for seeing changes without manual reloads.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.kan.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Installing NeuralForecast Package\nDESCRIPTION: Installs the NeuralForecast package using pip, with the output capture flag to hide installation output.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/07_forecasting_tft.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n!pip install neuralforecast\n```\n\n----------------------------------------\n\nTITLE: Cloning NeuralForecast Repository\nDESCRIPTION: Command to clone the NeuralForecast GitHub repository and navigate to its directory for development setup.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/getting-started/04_installation.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/Nixtla/neuralforecast.git && cd neuralforecast\n```\n\n----------------------------------------\n\nTITLE: Visualizing Probabilistic Predictions with PyTorch and Matplotlib\nDESCRIPTION: Creates and visualizes probabilistic predictions using NBMM model, including kernel density plots and trajectory visualization with confidence intervals.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_65\n\nLANGUAGE: python\nCODE:\n```\nN=2\ncounts = torch.repeat_interleave(input=counts, repeats=N, dim=0)\nweights = torch.ones_like(counts)\nprobs  = torch.ones_like(counts) * 0.5\n\nprint('weights.shape (N,H,1,K) \\t', weights.shape)\nprint('counts.shape (N,H,1,K) \\t', counts.shape)\nprint('probs.shape (N,H,1,K) \\t', probs.shape)\n\nmodel = NBMM(quantiles=[0.1, 0.40, 0.5, 0.60, 0.9], weighted=True)\ndistr_args = (counts, probs, weights)\nsamples, sample_mean, quants = model.sample(distr_args, num_samples=2000)\n\nprint('samples.shape (N,H,1,num_samples) ', samples.shape)\nprint('sample_mean.shape (N,H,1,1) ', sample_mean.shape)\nprint('quants.shape  (N,H,1,Q) \\t\\t', quants.shape)\n\n# Plot synthethic data\nx_plot = range(quants.shape[1])\ny_plot_hat = quants[0,:,0,:]\nsamples_hat = samples[0,:,0,:]\n\nfig, ax = plt.subplots(figsize=(3.7, 2.9))\nax.hist(samples_hat[0,:], alpha=0.5, bins=30, label=r'Horizon $\\tau+1$')\nax.hist(samples_hat[1,:], alpha=0.5, bins=30, label=r'Horizon $\\tau+2$')\nax.set(xlabel='Y values', ylabel='Probability')\nplt.title('Single horizon Distributions')\nplt.legend(bbox_to_anchor=(1, 1), loc='upper left', ncol=1)\nplt.grid()\nplt.show()\nplt.close()\n```\n\n----------------------------------------\n\nTITLE: Displaying Cross-Validation Results\nDESCRIPTION: Shows the first few rows of the cross-validation results dataframe for inspection.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/use-cases/electricity_peak_forecasting.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ncrossvalidation_df.head()\n```\n\n----------------------------------------\n\nTITLE: Creating Conda Environment for KAN Benchmark\nDESCRIPTION: Creates a conda environment named 'kan_benchmark' using the provided environment.yml file. This sets up the necessary dependencies for running the benchmark experiments.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/experiments/kan_benchmark/README.md#2025-04-23_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nconda env create -f environment.yml\n```\n\n----------------------------------------\n\nTITLE: Generating Forecasts with Trained TFT Model\nDESCRIPTION: Uses the trained NeuralForecast model to predict the next 24 hours after the training data and displays the first few rows of the forecast dataframe.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/07_forecasting_tft.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nY_hat_df = nf.predict()\nY_hat_df.head()\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging Settings\nDESCRIPTION: Sets the logging level for PyTorch Lightning to ERROR to reduce verbose output.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/20_conformal_prediction.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nlogging.getLogger('pytorch_lightning').setLevel(logging.ERROR)\n```\n\n----------------------------------------\n\nTITLE: Configuring Jupyter Autoreload Extension\nDESCRIPTION: Loads and configures the autoreload extension to automatically reload modules before executing code, which is useful during development to ensure the latest version of imported modules is used.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.gru.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Configuring Auto-reload for Python Modules\nDESCRIPTION: This code snippet sets up auto-reloading of Python modules in a Jupyter notebook environment. It loads the autoreload extension and sets it to automatically reload all modules before executing user code.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nhits.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Plotting Direct TimeMixer Forecast Predictions with Matplotlib\nDESCRIPTION: Creates a matplotlib visualization comparing actual vs predicted values for airline passenger data. Combines training and test data into a single plot with true values in black and predictions in blue.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timemixer.ipynb#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfig, ax = plt.subplots(1, 1, figsize = (20, 7))\nY_hat_df = forecasts.reset_index(drop=False).drop(columns=['unique_id','ds'])\nplot_df = pd.concat([Y_test_df, Y_hat_df], axis=1)\nplot_df = pd.concat([Y_train_df, plot_df])\n\nplot_df = plot_df[plot_df.unique_id=='Airline1'].drop('unique_id', axis=1)\nplt.plot(plot_df['ds'], plot_df['y'], c='black', label='True')\nplt.plot(plot_df['ds'], plot_df['TimeMixer'], c='blue', label='median')\nax.set_title('AirPassengers Forecast', fontsize=22)\nax.set_ylabel('Monthly Passengers', fontsize=20)\nax.set_xlabel('Year', fontsize=20)\nax.legend(prop={'size': 15})\nax.grid()\n```\n\n----------------------------------------\n\nTITLE: Setting Default Export Cell\nDESCRIPTION: Configures the default export cell name for the Jupyter notebook using a magic command.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/tsdataset.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#| default_exp tsdataset\n```\n\n----------------------------------------\n\nTITLE: Model Usage Example with Custom Configuration\nDESCRIPTION: Shows how to initialize and use AutoPatchTST and AutoiTransformer models with custom configurations and different backends.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_87\n\nLANGUAGE: python\nCODE:\n```\nconfig = dict(max_steps=1, val_check_steps=1, input_size=12, hidden_size=16)\nmodel = AutoPatchTST(h=12, config=config, num_samples=1, cpus=1)\n\n# Fit and predict\nmodel.fit(dataset=dataset)\ny_hat = model.predict(dataset=dataset)\n\n# Optuna\nmodel = AutoPatchTST(h=12, config=None, backend='optuna')\n```\n\n----------------------------------------\n\nTITLE: Configuring Jupyter Autoreload Extension\nDESCRIPTION: Loads and enables the autoreload extension in Jupyter to automatically reload external Python modules before executing code.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.mlpmultivariate.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Setting Default Export for NHiTS Model in Python\nDESCRIPTION: This code snippet sets the default export for the NHiTS model. It specifies the module path for the model, which is likely used in a Jupyter notebook environment.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nhits.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#| default_exp models.nhits\n```\n\n----------------------------------------\n\nTITLE: Unit Testing AutoDeepAR with Custom Ray Configuration in Python\nDESCRIPTION: This snippet demonstrates setting up a unit test for the `AutoDeepAR` model using the Ray backend. It retrieves the default Ray configuration, modifies specific hyperparameters like `max_steps`, `input_size`, and `lstm_hidden_size`, instantiates the `AutoDeepAR` model with these custom settings, and then fits the model to a dataset.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_25\n\nLANGUAGE: python\nCODE:\n```\n# Unit test for situation: Ray with updated default config\nmy_config = AutoDeepAR.get_default_config(h=12, backend='ray')\nmy_config['max_steps'] = 1\nmy_config['val_check_steps'] = 1\nmy_config['input_size'] = 12\nmy_config['lstm_hidden_size'] = 8\nmodel = AutoDeepAR(h=12, config=my_config, backend='ray', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: Setting PyTorch MPS Environment\nDESCRIPTION: Configures PyTorch Metal Performance Shaders (MPS) fallback setting for Apple Silicon compatibility.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/experiments/nbeats_basis/nbeats_basis_experiment.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%set_env PYTORCH_ENABLE_MPS_FALLBACK=1\n```\n\n----------------------------------------\n\nTITLE: Loading and Initializing Language Models in TimeLLM\nDESCRIPTION: Initializes the language model components for TimeLLM by loading pre-trained models, tokenizers, and configurations. Handles failures by falling back to a default model and configures token settings.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timellm.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntry:\n    self.llm_config = AutoConfig.from_pretrained(model_name)\n    self.llm = AutoModel.from_pretrained(model_name, config=self.llm_config)\n    self.llm_tokenizer = AutoTokenizer.from_pretrained(model_name)\n    print(f\"Successfully loaded model: {model_name}\")\nexcept EnvironmentError:\n    print(f\"Failed to load {model_name}. Loading the default model ({DEFAULT_MODEL})...\")\n    self.llm_config = AutoConfig.from_pretrained(DEFAULT_MODEL)\n    self.llm = AutoModel.from_pretrained(DEFAULT_MODEL, config=self.llm_config)\n    self.llm_tokenizer = AutoTokenizer.from_pretrained(DEFAULT_MODEL)\n```\n\n----------------------------------------\n\nTITLE: Preparing Large Dataset for NeuralForecast in Python\nDESCRIPTION: This code prepares the AirPassengers dataset by splitting it into training and validation sets, and storing it in a temporary directory with the required file structure for the large-scale DataLoader.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/19_large_datasets.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nY_df = AirPassengersPanel.copy()\nY_df\n```\n\nLANGUAGE: python\nCODE:\n```\nvalid = Y_df.groupby('unique_id').tail(72)\nvalid = valid.rename(columns={'unique_id': 'id_col'})\n\ntrain = Y_df.drop(valid.index)\ntrain['id_col'] = train['unique_id'].copy()\n\ntmpdir = tempfile.TemporaryDirectory()\ntrain.to_parquet(tmpdir.name, partition_cols=['unique_id'], index=False)\nfiles_list = [f\"{tmpdir.name}/{dir}\" for dir in os.listdir(tmpdir.name)]\nfiles_list\n```\n\n----------------------------------------\n\nTITLE: Utility Functions for Converting Between Probability Levels and Quantiles in PyTorch\nDESCRIPTION: Contains two functions: level_to_outputs converts prediction interval levels to quantiles and output names, while quantiles_to_outputs generates appropriate output names for given quantiles. These functions support the MQLoss class by preparing the required quantile values.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n#| exporti\ndef level_to_outputs(level):\n    qs = sum([[50-l/2, 50+l/2] for l in level], [])\n    output_names = sum([[f'-lo-{l}', f'-hi-{l}'] for l in level], [])\n\n    sort_idx = np.argsort(qs)\n    quantiles = np.array(qs)[sort_idx]\n\n    # Add default median\n    quantiles = np.concatenate([np.array([50]), quantiles])\n    quantiles = torch.Tensor(quantiles) / 100\n    output_names = list(np.array(output_names)[sort_idx])\n    output_names.insert(0, '-median')\n    \n    return quantiles, output_names\n\ndef quantiles_to_outputs(quantiles):\n    output_names = []\n    for q in quantiles:\n        if q<.50:\n            output_names.append(f'-lo-{np.round(100-200*q,2)}')\n        elif q>.50:\n            output_names.append(f'-hi-{np.round(100-200*(1-q),2)}')\n        else:\n            output_names.append('-median')\n    return quantiles, output_names\n```\n\n----------------------------------------\n\nTITLE: Implementing Unit Test for Auto Models Argument Validation\nDESCRIPTION: Defines a test function to verify that Auto model classes properly inherit all required arguments from the BaseAuto class, with compatibility for different Python versions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n# Unit test to test that Auto* model contains all required arguments from BaseAuto class.\n\n# Patch for Python 3.11 on get arg spec\nif not hasattr(inspect, 'getargspec'):\n    getargspec = inspect.getfullargspec\nelse:\n    getargspec = inspect.getargspec\n\ndef test_args(auto_model, exclude_args=None):\n    base_auto_args = getargspec(BaseAuto)[0]\n    auto_model_args = getargspec(auto_model)[0]\n    if exclude_args is not None:\n        base_auto_args = [arg for arg in base_auto_args if arg not in exclude_args]\n    args_diff = set(base_auto_args) - set(auto_model_args)\n    assert not args_diff, f\"__init__ of {auto_model.__name__} does not contain the following required variables from BaseAuto class:\\n\\t\\t{args_diff}\"\n```\n\n----------------------------------------\n\nTITLE: Calculating MAE Loss with Complete Mask and Incomplete Horizon Weight in PyTorch\nDESCRIPTION: This snippet illustrates the calculation of MAE loss using a complete mask and incomplete horizon weight. It creates a mask covering all points, sets up a horizon weight that masks two errors, calculates the loss, and checks the expected outcome.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.pytorch.ipynb#2025-04-23_snippet_79\n\nLANGUAGE: Python\nCODE:\n```\n# Complete mask and incomplete horizon_weight\nmask = torch.Tensor([[1,1,1],[1,1,1]]).unsqueeze(-1)\nhorizon_weight = torch.Tensor([1,1,0]) # 2 errors and points are masked.\nmae = MAE(horizon_weight=horizon_weight)\nloss = mae(y=y, y_hat=y_hat, mask=mask)\nassert loss==(1/4), 'Should be 1/4'\n```\n\n----------------------------------------\n\nTITLE: Validating Input Dimensions for Metric Calculations\nDESCRIPTION: Defines a function to check if the input arrays and weights have valid dimensions and non-zero sum for weights, raising assertions if conditions are not met.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.numpy.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndef _metric_protections(y: np.ndarray, y_hat: np.ndarray, \n                        weights: Optional[np.ndarray]) -> None:\n    assert (weights is None) or (np.sum(weights) > 0), 'Sum of weights cannot be 0'\n    assert (weights is None) or (weights.shape == y.shape),\\\n        f'Wrong weight dimension weights.shape {weights.shape}, y.shape {y.shape}'\n```\n\n----------------------------------------\n\nTITLE: Installing NeuralForecast Development Environment with Conda\nDESCRIPTION: Commands for setting up a conda environment for NeuralForecast development. Creates a Python 3.10 environment and activates it.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/CONTRIBUTING.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nconda create -n neuralforecast python=3.10\nconda activate neuralforecast\n```\n\n----------------------------------------\n\nTITLE: Ensuring No Data Leakage with val_size and test_size Together - Python\nDESCRIPTION: Tests the NBEATS model for proper handling and no leakage when using both validation and test set splits. Trains two models: one with only validation, another with val and test sizes. Asserts almost-equal predictions across settings for functional equivalence. Uses numpy and TimeSeriesDataset.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nbeats.ipynb#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n# test no leakage with test_size and val_size\ndataset, *_ = TimeSeriesDataset.from_df(Y_train_df)\nmodel = NBEATS(input_size=24, h=12, windows_batch_size=None, max_steps=1)\nmodel.fit(dataset=dataset, val_size=12)\ny_hat_w_val = model.predict(dataset=dataset)\n\ndataset, *_ = TimeSeriesDataset.from_df(Y_df)\nmodel = NBEATS(input_size=24, h=12, windows_batch_size=None, max_steps=1)\nmodel.fit(dataset=dataset, val_size=12, test_size=12)\n\ny_hat_test_w_val = model.predict(dataset=dataset, step_size=1)\n\nnp.testing.assert_almost_equal(y_hat_test_w_val, y_hat_w_val, decimal=4)\n```\n\n----------------------------------------\n\nTITLE: Reshaping Forecast Results for Evaluation\nDESCRIPTION: Reshapes the true values and forecasts for proper evaluation, organizing them by series, windows, and forecast horizon.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/04_longhorizon_nhits.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ny_true = Y_hat_df.y.values\ny_hat = Y_hat_df['AutoNHITS'].values\n\nn_series = len(Y_df.unique_id.unique())\n\ny_true = y_true.reshape(n_series, -1, horizon)\ny_hat = y_hat.reshape(n_series, -1, horizon)\n\nprint('Parsed results')\nprint('2. y_true.shape (n_series, n_windows, n_time_out):\\t', y_true.shape)\nprint('2. y_hat.shape  (n_series, n_windows, n_time_out):\\t', y_hat.shape)\n```\n\n----------------------------------------\n\nTITLE: Displaying Documentation for TimeMixer.fit Method in Python\nDESCRIPTION: This snippet uses the `show_doc` function to display the documentation specifically for the `fit` method of the `TimeMixer` class. It requires the `TimeMixer` class and its `fit` method to be defined, along with the `show_doc` function. The `name` argument specifies the desired display name for the documentation section.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timemixer.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(TimeMixer.fit, name='TimeMixer.fit')\n```\n\n----------------------------------------\n\nTITLE: Displaying AutoTimeXer Documentation\nDESCRIPTION: Code to display the documentation for the AutoTimeXer class, likely using a custom documentation display function.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_90\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(AutoTimeXer, title_level=3)\n```\n\n----------------------------------------\n\nTITLE: Loading and Filtering ERCOT Electricity Demand Data\nDESCRIPTION: Loads the ERCOT electricity demand data from a CSV file, parses dates, and filters the data for the year 2022 up to October 1st.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/use-cases/electricity_peak_forecasting.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Load data\nY_df = pd.read_csv('https://datasets-nixtla.s3.amazonaws.com/ERCOT-clean.csv', parse_dates=['ds'])\nY_df = Y_df.query(\"ds >= '2022-01-01' & ds <= '2022-10-01'\")\n```\n\n----------------------------------------\n\nTITLE: Displaying AutoRNN Documentation\nDESCRIPTION: Uses nbdev's show_doc function to display the documentation for the AutoRNN class in the notebook.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(AutoRNN, title_level=3)\n```\n\n----------------------------------------\n\nTITLE: Loading and Preprocessing ERCOT Electricity Demand Data\nDESCRIPTION: Loads historical electricity demand data from the ERCOT market, converts the date string to datetime format, and displays the first few rows of the dataframe.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/07_forecasting_tft.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nY_df = pd.read_csv('https://datasets-nixtla.s3.amazonaws.com/ERCOT-clean.csv')\nY_df['ds'] = pd.to_datetime(Y_df['ds'])\nY_df.head()\n```\n\n----------------------------------------\n\nTITLE: Displaying Model Prediction Results with Probability Intervals\nDESCRIPTION: Shows the forecast results DataFrame which includes probability-based predictions from both models, with lo-x and high-x levels representing probability bounds for the binary predictions.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/16_temporal_classification.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# By default NeuralForecast produces forecast intervals\n# In this case the lo-x and high-x levels represent the \n# low and high bounds of the prediction accumulating x% probability\nY_hat_df\n```\n\n----------------------------------------\n\nTITLE: Managing Test Size for Neural Forecasting Model\nDESCRIPTION: Simple getter and setter methods for the model's test size parameter, allowing external control of the test set proportion.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_model.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndef get_test_size(self):\n    return self.test_size\n\ndef set_test_size(self, test_size):\n    self.test_size = test_size\n```\n\n----------------------------------------\n\nTITLE: Testing Conformal Distribution Prediction\nDESCRIPTION: Tests conformal prediction functionality using conformal_distribution method with multiple model types.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_68\n\nLANGUAGE: python\nCODE:\n```\nprediction_intervals = PredictionIntervals()\n\nmodels = []\nfor nf_model in [NHITS, RNN, TSMixer]:\n    params = {\"h\": 12, \"input_size\": 24, \"max_steps\": 1}\n    if nf_model.__name__ == \"TSMixer\":\n        params.update({\"n_series\": 2})\n    models.append(nf_model(**params))\n\nnf = NeuralForecast(models=models, freq='M')\nnf.fit(AirPassengersPanel_train, prediction_intervals=prediction_intervals)\npreds = nf.predict(futr_df=AirPassengersPanel_test, level=[90])\n```\n\n----------------------------------------\n\nTITLE: Importing Ray Tune and AutoNHITS\nDESCRIPTION: Importing the Ray Tune library for hyperparameter tuning and AutoNHITS model from NeuralForecast for automatic hyperparameter optimization.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/04_hyperparameter_tuning.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom ray import tune\nfrom neuralforecast.auto import AutoNHITS\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for TimeMixer\nDESCRIPTION: Imports required Python libraries and modules for implementing the TimeMixer neural network architecture including PyTorch, numpy and custom neuralforecast components.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timemixer.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport math\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\n\nfrom neuralforecast.common._base_model import BaseModel\nfrom neuralforecast.common._modules import PositionalEmbedding, TokenEmbedding, TemporalEmbedding, SeriesDecomp, RevIN\nfrom neuralforecast.losses.pytorch import MAE\nfrom typing import Optional\n```\n\n----------------------------------------\n\nTITLE: Displaying Documentation for VanillaTransformer.fit Method in Python\nDESCRIPTION: This snippet uses the `show_doc` function to display the documentation specifically for the `fit` method of the `VanillaTransformer` class, renaming the section header to 'VanillaTransformer.fit'.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.vanillatransformer.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(VanillaTransformer.fit, name='VanillaTransformer.fit')\n```\n\n----------------------------------------\n\nTITLE: Generating Documentation for DilatedRNN Class (Python)\nDESCRIPTION: Calls the `show_doc` function to automatically generate and display documentation for the `DilatedRNN` class. This is likely part of a documentation generation process, possibly using tools like nbdev, within a Jupyter Notebook or similar environment. It requires the `DilatedRNN` class to be defined and the `show_doc` function to be available.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.dilated_rnn.ipynb#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(DilatedRNN)\n```\n\n----------------------------------------\n\nTITLE: Installing Git Hooks for NeuralForecast Development\nDESCRIPTION: Commands to install git hooks that handle notebook metadata stripping and pre-commit checks.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/CONTRIBUTING.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nnbdev_install_hooks\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: Testing NaN handling in Pandas DataFrame inputs\nDESCRIPTION: Validates error handling when NaN values are present in target variables, exogenous variables, and static features with available_mask=1.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_58\n\nLANGUAGE: python\nCODE:\n```\nn_static_features = 2\nn_temporal_features = 4\ntemporal_df, static_df = generate_series(n_series=4,\n                                         min_length=50,\n                                         max_length=50,\n                                         n_static_features=n_static_features,\n                                         n_temporal_features=n_temporal_features, \n                                         equal_ends=False) \ntemporal_df[\"available_mask\"] = 1\ntemporal_df.loc[10:20, \"available_mask\"] = 0\nmodels = [NHITS(h=12, input_size=24, max_steps=20)]\nnf = NeuralForecast(models=models, freq='D')\n\ntest_df1 = temporal_df.copy()\ntest_df1.loc[5:7, \"y\"] = np.nan\ntest_fail(lambda: nf.fit(test_df1), contains=\"Found missing values in ['y']\")\n\ntest_df2 = temporal_df.copy()\ntest_df2.loc[15:18, \"temporal_0\"] = np.nan\ntest_df2.loc[5, \"temporal_1\"] = np.nan\ntest_df2.loc[25, \"temporal_2\"] = np.nan\ntest_fail(lambda: nf.fit(test_df2), contains=\"Found missing values in ['temporal_1', 'temporal_2']\")\n\ntest_df3 = static_df.copy()\ntest_df3.loc[3, \"static_1\"] = np.nan\ntest_fail(lambda: nf.fit(temporal_df, static_df=test_df3), contains=\"Found missing values in ['static_1']\")\n```\n\n----------------------------------------\n\nTITLE: Setting Default Export Path for NeuralForecast Module\nDESCRIPTION: Defines the default export path for the module as 'common._base_auto'. This is a Jupyter notebook directive that specifies where the code in this notebook should be exported.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.base_auto.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#| default_exp common._base_auto\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for NeuralForecast\nDESCRIPTION: Imports necessary Python libraries for time series forecasting, including numpy, pandas, torch, and neuralforecast components.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/21_configure_optimizers.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\nimport torch\nimport matplotlib.pyplot as plt\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import NHITS\nfrom neuralforecast.utils import AirPassengersPanel\n\nfrom utilsforecast.plotting import plot_series\n```\n\n----------------------------------------\n\nTITLE: Importing Optuna for Hyperparameter Tuning in Python\nDESCRIPTION: This snippet imports the Optuna library for hyperparameter optimization and sets the logging verbosity to suppress training prints.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/04_hyperparameter_tuning.ipynb#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport optuna\noptuna.logging.set_verbosity(optuna.logging.WARNING) # Use this to disable training prints from optuna\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries\nDESCRIPTION: Imports essential Python libraries for data manipulation, visualization, and datetime operations.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/experiments/nbeats_basis/nbeats_basis_experiment.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime, timedelta\n```\n\n----------------------------------------\n\nTITLE: Plotting ERCOT Electricity Demand Data\nDESCRIPTION: Creates a line plot of the ERCOT electricity demand data using pandas plotting functionality.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/use-cases/electricity_peak_forecasting.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nY_df.plot(x='ds', y='y', figsize=(20, 7))\n```\n\n----------------------------------------\n\nTITLE: Visualization Setup and Data Plotting\nDESCRIPTION: Import visualization libraries and create time series plot with validation/test splits\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/06_longhorizon_probabilistic.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport matplotlib.pyplot as plt\nfrom utilsforecast.plotting import plot_series\n```\n\nLANGUAGE: python\nCODE:\n```\nu_id = 'HUFL'\nfig = plot_series(Y_df, ids=[u_id])\nax = fig.axes[0]\n\nx_plot = pd.to_datetime(Y_df[Y_df.unique_id==u_id].ds)\ny_plot = Y_df[Y_df.unique_id==u_id].y.values\nx_val = x_plot[n_time - val_size - test_size]\nx_test = x_plot[n_time - test_size]\n\nax.axvline(x_val, color='black', linestyle='-.')\nax.axvline(x_test, color='black', linestyle='-.')\nax.text(x_val, 5, '  Validation', fontsize=12)\nax.text(x_test, 3, '  Test', fontsize=12)\nfig\n```\n\n----------------------------------------\n\nTITLE: Importing Data Utility Functions\nDESCRIPTION: Importing required modules including logging and AirPassengersDF utility from NeuralForecast to load the dataset for time series forecasting.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/04_hyperparameter_tuning.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\nfrom neuralforecast.utils import AirPassengersDF\n```\n\n----------------------------------------\n\nTITLE: Installing NeuralForecast Dependencies\nDESCRIPTION: Installs the necessary Python packages for time series forecasting with NeuralForecast and datasetsforecast.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/04_longhorizon_nhits.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n!pip install neuralforecast datasetsforecast\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variable for PyTorch MPS Fallback\nDESCRIPTION: Sets an environment variable to enable PyTorch MPS fallback.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.rmok.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%set_env PYTORCH_ENABLE_MPS_FALLBACK=1\n```\n\n----------------------------------------\n\nTITLE: Building and Testing NeuralForecast Library\nDESCRIPTION: Commands for exporting notebooks to library code and running tests with nbdev.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/CONTRIBUTING.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnbdev_export\n```\n\n----------------------------------------\n\nTITLE: Installing Development Version\nDESCRIPTION: Command to install NeuralForecast in development mode with additional development dependencies.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/getting-started/04_installation.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install -e \".[dev]\"\n```\n\n----------------------------------------\n\nTITLE: Activating Long Horizon Conda Environment\nDESCRIPTION: Activates the previously created conda environment for running the experiments.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/experiments/long_horizon/README.md#2025-04-23_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nconda activate long_horizon\n```\n\n----------------------------------------\n\nTITLE: Updating Core Model Imports\nDESCRIPTION: Shows how to import and register the new model in the core NeuralForecast module.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/18_adding_models.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom neuralforecast.models import (\nGRU, LSTM, RNN, TCN, DilatedRNN,\nMLP, NHITS, NBEATS, NBEATSx,\nTFT, VanillaTransformer,\nInformer, Autoformer, FEDformer,\nStemGNN, PatchTST\n)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Dependencies for GRU Model Testing\nDESCRIPTION: Hidden imports for testing the GRU model, including logging, testing utilities, and documentation generation tools from nbdev.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.gru.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#| hide\nimport logging\nfrom fastcore.test import test_eq\nfrom nbdev.showdoc import show_doc\nfrom neuralforecast.common._model_checks import check_model\n```\n\n----------------------------------------\n\nTITLE: Setting Default Export Path for MLP Module in Python\nDESCRIPTION: This code sets the default export path for the MLP (Multi-Layer Perceptron) module in the NeuralForecast library using Jupyter notebook cell magic.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.mlp.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#| default_exp models.mlp\n```\n\n----------------------------------------\n\nTITLE: Importing Plotting Utility for Time Series Visualization in Python\nDESCRIPTION: This code imports the plot_series function from utilsforecast.plotting for visualizing time series data.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/04_hyperparameter_tuning.ipynb#2025-04-23_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nfrom utilsforecast.plotting import plot_series\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging Level\nDESCRIPTION: Sets the logging level for pytorch_lightning to ERROR to reduce output verbosity\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/06_save_load_models.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nlogging.getLogger('pytorch_lightning').setLevel(logging.ERROR)\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging Level\nDESCRIPTION: Setting the logging level for PyTorch Lightning to ERROR to reduce verbosity during model training and evaluation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/capabilities/04_hyperparameter_tuning.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nlogging.getLogger('pytorch_lightning').setLevel(logging.ERROR)\n```\n\n----------------------------------------\n\nTITLE: Testing Custom Learning Rate Scheduler\nDESCRIPTION: Validates that custom learning rate scheduler configurations produce different results from default settings.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/core.ipynb#2025-04-23_snippet_64\n\nLANGUAGE: python\nCODE:\n```\nfor nf_model in [NHITS, RNN, StemGNN]:\n    params = {\"h\": 12, \"input_size\": 24, \"max_steps\": 1}\n    if nf_model.__name__ == \"StemGNN\":\n        params.update({\"n_series\": 2})\n    models = [nf_model(**params)]\n    nf = NeuralForecast(models=models, freq='M')\n    nf.fit(AirPassengersPanel_train)\n    default_optimizer_predict = nf.predict()\n    mean = default_optimizer_predict.loc[:, nf_model.__name__].mean()\n\n    params.update({\n        \"lr_scheduler\": torch.optim.lr_scheduler.ConstantLR,\n        \"lr_scheduler_kwargs\": {\"factor\": 0.78}, \n    })\n    models2 = [nf_model(**params)]\n    nf2 = NeuralForecast(models=models2, freq='M')\n    nf2.fit(AirPassengersPanel_train)\n    customized_optimizer_predict = nf2.predict()\n    mean2 = customized_optimizer_predict.loc[:, nf_model.__name__].mean()\n    assert mean2 != mean\n```\n\n----------------------------------------\n\nTITLE: Configuring Jupyter Notebook Autoreload Extension\nDESCRIPTION: Enables automatic reloading of Python modules before executing code in Jupyter notebook.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/18_adding_models.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Unit Tests for AutoStemGNN Class in Python\nDESCRIPTION: Contains unit tests for the AutoStemGNN class, including checks for Optuna configuration, required arguments, and different configuration scenarios with both Optuna and Ray backends.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.ipynb#2025-04-23_snippet_99\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n# Check Optuna\nassert model.config(MockTrial())['h'] == 12\nassert model.config(MockTrial())['n_series'] == 1\n\n# Unit test to test that Auto* model contains all required arguments from BaseAuto\ntest_args(AutoStemGNN, exclude_args=['cls_model']) \n\n# Unit test for situation: Optuna with updated default config\nmy_config = AutoStemGNN.get_default_config(h=12, backend='optuna', n_series=1)\ndef my_config_new(trial):\n    config = {**my_config(trial)}\n    config.update({'max_steps': 1, 'val_check_steps': 1, 'input_size': 12})\n    return config\n\nmodel = AutoStemGNN(h=12, n_series=1, config=my_config_new, backend='optuna')\nmodel.fit(dataset=dataset)\n\n# Unit test for situation: Ray with updated default config\nmy_config = AutoStemGNN.get_default_config(h=12, backend='ray', n_series=1)\nmy_config['max_steps'] = 1\nmy_config['val_check_steps'] = 1\nmy_config['input_size'] = 12\nmodel = AutoStemGNN(h=12, n_series=1, config=my_config, backend='ray', num_samples=1, cpus=1)\nmodel.fit(dataset=dataset)\n```\n\n----------------------------------------\n\nTITLE: Checking CUDA Availability for GPU Acceleration in PyTorch\nDESCRIPTION: Verifies whether CUDA is available for GPU acceleration, which can significantly improve the training speed of neural network models.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/docs/tutorials/17_transfer_learning.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntorch.cuda.is_available()\n```\n\n----------------------------------------\n\nTITLE: Enabling PyTorch MPS Fallback for Apple Silicon\nDESCRIPTION: Sets an environment variable to enable MPS (Metal Performance Shaders) fallback for PyTorch on Apple Silicon devices, which helps resolve compatibility issues when hardware acceleration isn't fully supported.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.gru.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%set_env PYTORCH_ENABLE_MPS_FALLBACK=1\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for TiDE Model in Python\nDESCRIPTION: Imports necessary libraries and modules for implementing the TiDE model, including PyTorch and custom NeuralForecast components.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.tide.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom neuralforecast.losses.pytorch import MAE\nfrom neuralforecast.common._base_model import BaseModel\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for TimesNet Model\nDESCRIPTION: Imports necessary libraries and modules for implementing the TimesNet model, including PyTorch, typing, and custom modules from neuralforecast.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.timesnet.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.fft\n\nfrom neuralforecast.common._modules import DataEmbedding\nfrom neuralforecast.common._base_model import BaseModel\n\nfrom neuralforecast.losses.pytorch import MAE\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for NLinear Model\nDESCRIPTION: Imports necessary modules and classes for implementing the NLinear model, including PyTorch neural network components and custom base model class.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.nlinear.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\n\nimport torch.nn as nn\n\nfrom neuralforecast.common._base_model import BaseModel\n\nfrom neuralforecast.losses.pytorch import MAE\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for RNN Implementation\nDESCRIPTION: Imports required testing and logging utilities for the RNN implementation.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.rnn.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n#| hide\nimport logging\nfrom fastcore.test import test_eq\nfrom nbdev.showdoc import show_doc\nfrom neuralforecast.common._model_checks import check_model\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for NumPy Loss Functions\nDESCRIPTION: Imports the necessary libraries and modules for implementing loss functions, including typing for type hints and NumPy for numerical operations.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/losses.numpy.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional, Union\n\nimport numpy as np\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for Model Testing\nDESCRIPTION: Imports necessary packages including pandas and neuralforecast modules for testing forecasting models.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.model_checks.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#| export\nimport pandas as pd\nimport neuralforecast.losses.pytorch as losses\n\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.utils import AirPassengersPanel, AirPassengersStatic, generate_series\n```\n\n----------------------------------------\n\nTITLE: Setting Default Export Module Path for NeuralForecast\nDESCRIPTION: Configures the default export path for the notebook to the 'models.gru' module, allowing code in this notebook to be easily exported to the specified Python module.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.gru.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#| default_exp models.gru\n```\n\n----------------------------------------\n\nTITLE: Setting up Jupyter Notebook Environment with Cell Magic\nDESCRIPTION: Configuration of Jupyter notebook cell magic for module export and autoreload functionality.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/common.modules.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n#| default_exp common._modules\n```\n\nLANGUAGE: python\nCODE:\n```\n#| hide\n%load_ext autoreload\n%autoreload 2\n```\n\n----------------------------------------\n\nTITLE: Displaying Documentation for DLinear.fit Method\nDESCRIPTION: A utility function call to display the documentation specifically for the fit method of the DLinear class, which would show how to train the model on time series data.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/nbs/models.dlinear.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nshow_doc(DLinear.fit, name='DLinear.fit')\n```\n\n----------------------------------------\n\nTITLE: Rendering Contributors Table in HTML/Markdown\nDESCRIPTION: An HTML table structure embedded in markdown that displays contributor information including avatars, GitHub profiles, and contribution types using the all-contributors emoji key system.\nSOURCE: https://github.com/nixtla/neuralforecast/blob/main/README.md#2025-04-23_snippet_3\n\nLANGUAGE: html\nCODE:\n```\n<table>\n  <tbody>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/AzulGarza\"><img src=\"https://avatars.githubusercontent.com/u/10517170?v=4?s=100\" width=\"100px;\" alt=\"azul\"/><br /><sub><b>azul</b></sub></a><br /><a href=\"https://github.com/Nixtla/neuralforecast/commits?author=AzulGarza\" title=\"Code\"></a> <a href=\"#maintenance-AzulGarza\" title=\"Maintenance\"></a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/cchallu\"><img src=\"https://avatars.githubusercontent.com/u/31133398?v=4?s=100\" width=\"100px;\" alt=\"Cristian Challu\"/><br /><sub><b>Cristian Challu</b></sub></a><br /><a href=\"https://github.com/Nixtla/neuralforecast/commits?author=cchallu\" title=\"Code\"></a> <a href=\"#maintenance-cchallu\" title=\"Maintenance\"></a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/jmoralez\"><img src=\"https://avatars.githubusercontent.com/u/8473587?v=4?s=100\" width=\"100px;\" alt=\"Jos Morales\"/><br /><sub><b>Jos Morales</b></sub></a><br /><a href=\"https://github.com/Nixtla/neuralforecast/commits?author=jmoralez\" title=\"Code\"></a> <a href=\"#maintenance-jmoralez\" title=\"Maintenance\"></a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/mergenthaler\"><img src=\"https://avatars.githubusercontent.com/u/4086186?v=4?s=100\" width=\"100px;\" alt=\"mergenthaler\"/><br /><sub><b>mergenthaler</b></sub></a><br /><a href=\"https://github.com/Nixtla/neuralforecast/commits?author=mergenthaler\" title=\"Documentation\"></a> <a href=\"https://github.com/Nixtla/neuralforecast/commits?author=mergenthaler\" title=\"Code\"></a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/kdgutier\"><img src=\"https://avatars.githubusercontent.com/u/19935241?v=4?s=100\" width=\"100px;\" alt=\"Kin\"/><br /><sub><b>Kin</b></sub></a><br /><a href=\"https://github.com/Nixtla/neuralforecast/commits?author=kdgutier\" title=\"Code\"></a> <a href=\"https://github.com/Nixtla/neuralforecast/issues?q=author%3Akdgutier\" title=\"Bug reports\"></a> <a href=\"#data-kdgutier\" title=\"Data\"></a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/gdevos010\"><img src=\"https://avatars.githubusercontent.com/u/15316026?v=4?s=100\" width=\"100px;\" alt=\"Greg DeVos\"/><br /><sub><b>Greg DeVos</b></sub></a><br /><a href=\"#ideas-gdevos010\" title=\"Ideas, Planning, & Feedback\"></a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/alejandroxag\"><img src=\"https://avatars.githubusercontent.com/u/64334543?v=4?s=100\" width=\"100px;\" alt=\"Alejandro\"/><br /><sub><b>Alejandro</b></sub></a><br /><a href=\"https://github.com/Nixtla/neuralforecast/commits?author=alejandroxag\" title=\"Code\"></a></td>\n    </tr>\n    <tr>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"http://lavattiata.com\"><img src=\"https://avatars.githubusercontent.com/u/48966177?v=4?s=100\" width=\"100px;\" alt=\"stefanialvs\"/><br /><sub><b>stefanialvs</b></sub></a><br /><a href=\"#design-stefanialvs\" title=\"Design\"></a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://bandism.net/\"><img src=\"https://avatars.githubusercontent.com/u/22633385?v=4?s=100\" width=\"100px;\" alt=\"Ikko Ashimine\"/><br /><sub><b>Ikko Ashimine</b></sub></a><br /><a href=\"https://github.com/Nixtla/neuralforecast/issues?q=author%3Aeltociear\" title=\"Bug reports\"></a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/vglaucus\"><img src=\"https://avatars.githubusercontent.com/u/75549033?v=4?s=100\" width=\"100px;\" alt=\"vglaucus\"/><br /><sub><b>vglaucus</b></sub></a><br /><a href=\"https://github.com/Nixtla/neuralforecast/issues?q=author%3Avglaucus\" title=\"Bug reports\"></a></td>\n      <td align=\"center\" valign=\"top\" width=\"14.28%\"><a href=\"https://github.com/pitmonticone\"><img src=\"https://avatars.githubusercontent.com/u/38562595?v=4?s=100\" width=\"100px;\" alt=\"Pietro Monticone\"/><br /><sub><b>Pietro Monticone</b></sub></a><br /><a href=\"https://github.com/Nixtla/neuralforecast/issues?q=author%3Apitmonticone\" title=\"Bug reports\"></a></td>\n    </tr>\n  </tbody>\n</table>\n```"
  }
]
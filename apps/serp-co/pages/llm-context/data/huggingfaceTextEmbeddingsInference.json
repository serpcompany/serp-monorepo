[
  {
    "owner": "huggingface",
    "repo": "text-embeddings-inference",
    "content": "TITLE: Running Text Embeddings Inference Docker Container\nDESCRIPTION: This snippet demonstrates how to run a Text Embeddings Inference Docker container with GPU support, port mapping, and volume mounting. It uses the BAAI/bge-large-en-v1.5 model.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/README.md#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nmodel=BAAI/bge-large-en-v1.5\nvolume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run\n\ndocker run --gpus all -p 8080:80 -v $volume:/data --pull always ghcr.io/huggingface/text-embeddings-inference:1.7 --model-id $model\n```\n\n----------------------------------------\n\nTITLE: Deploying TEI Model with Docker\nDESCRIPTION: Docker command to deploy a text embedding model using GPU support and volume mounting for weight persistence.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/quick_tour.md#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nmodel=BAAI/bge-large-en-v1.5\nvolume=$PWD/data\n\ndocker run --gpus all -p 8080:80 -v $volume:/data --pull always ghcr.io/huggingface/text-embeddings-inference:1.7 --model-id $model\n```\n\n----------------------------------------\n\nTITLE: Making API Requests to Text Embeddings Inference\nDESCRIPTION: This curl command shows how to make a POST request to the Text Embeddings Inference API to embed the text 'What is Deep Learning?'.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/README.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl 127.0.0.1:8080/embed \\\n    -X POST \\\n    -d '{\"inputs\":\"What is Deep Learning?\"}' \\\n    -H 'Content-Type: application/json'\n```\n\n----------------------------------------\n\nTITLE: Embedding Generation using cURL\nDESCRIPTION: HTTP POST request to generate embeddings for text input using cURL.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/quick_tour.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncurl 127.0.0.1:8080/embed \\\n    -X POST \\\n    -d '{\"inputs\":\"What is Deep Learning?\"}' \\\n    -H 'Content-Type: application/json'\n```\n\n----------------------------------------\n\nTITLE: Embedding Generation using HuggingFace Hub Client\nDESCRIPTION: Python code using HuggingFace's InferenceClient to generate embeddings.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/quick_tour.md#2025-04-11_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import InferenceClient\n\nclient = InferenceClient()\n\nembedding = client.feature_extraction(\"What is deep learning?\",\n                                      model=\"http://localhost:8080/embed\")\nprint(len(embedding[0]))\n```\n\n----------------------------------------\n\nTITLE: Embedding Generation using OpenAI Client\nDESCRIPTION: Python code using OpenAI client to generate embeddings with custom endpoint.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/quick_tour.md#2025-04-11_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(base_url=\"http://localhost:8080/v1/embeddings\")\n\nresponse = client.embeddings.create(\n  model=\"tei\",\n  input=\"What is deep learning?\"\n)\n\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Making POST requests to TEI embedding endpoint with cURL\nDESCRIPTION: Example of sending a text embedding request to the TEI service using cURL. The request targets the /embed endpoint with a sample text to generate embeddings.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/tei_cloud_run.md#2025-04-11_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/embed \\\n    -X POST \\\n    -H 'Content-Type: application/json' \\\n    -d '{\n        \"model\": \"tei\",\n        \"text\": \"What is deep learning?\"\n    }'\n```\n\n----------------------------------------\n\nTITLE: Making POST requests to TEI OpenAI-compatible endpoint with cURL\nDESCRIPTION: Example of sending a text embedding request to the TEI service using the OpenAI-compatible endpoint. This demonstrates TEI's compatibility with OpenAI's embedding API format.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/tei_cloud_run.md#2025-04-11_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncurl http://localhost:8080/v1/embeddings \\\n    -X POST \\\n    -H 'Content-Type: application/json' \\\n    -d '{\n        \"model\": \"tei\",\n        \"text\": \"What is deep learning?\"\n    }'\n```\n\n----------------------------------------\n\nTITLE: Using huggingface_hub Python SDK for TEI embeddings\nDESCRIPTION: Example of using the huggingface_hub Python SDK to generate embeddings from the TEI service. The code configures the InferenceClient to point to the local proxy endpoint.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/tei_cloud_run.md#2025-04-11_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom huggingface_hub import InferenceClient\n\nclient = InferenceClient()\nembedding = client.feature_extraction(\"What is deep learning?\",\n                                  model=\"http://localhost:8080/embed\")\nprint(len(embedding[0]))\n```\n\n----------------------------------------\n\nTITLE: Using OpenAI Python SDK for TEI embeddings\nDESCRIPTION: Example of using the OpenAI Python SDK to generate embeddings from the TEI service. The code configures the OpenAI client to point to the local proxy endpoint that implements OpenAI-compatible API.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/tei_cloud_run.md#2025-04-11_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom openai import OpenAI\n\nclient = OpenAI(base_url=\"http://localhost:8080/v1/embeddings\", api_key=\"\")\n\nresponse = client.embeddings.create(\n  model=\"tei\",\n  input=\"What is deep learning?\"\n)\n\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Using gRPC API Endpoint for Embeddings\nDESCRIPTION: Example of using the gRPC API to generate embeddings with TEI. This command uses grpcurl to send a request to the gRPC endpoint for generating text embeddings.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/README.md#2025-04-11_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\ngrpcurl -d '{\"inputs\": \"What is Deep Learning\"}' -plaintext 0.0.0.0:8080 tei.v1.Embed/Embed\n```\n\n----------------------------------------\n\nTITLE: Using Re-ranker API Endpoint for Text Similarity Scoring\nDESCRIPTION: Example of using the rerank endpoint to score similarity between a query and multiple candidate texts. This curl command sends a POST request with a query and candidate texts to get ranked results.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/README.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl 127.0.0.1:8080/rerank \\\n    -X POST \\\n    -d '{\"query\": \"What is Deep Learning?\", \"texts\": [\"Deep Learning is not...\", \"Deep learning is...\"]}' \\\n    -H 'Content-Type: application/json'\n```\n\n----------------------------------------\n\nTITLE: Text Re-ranking using cURL\nDESCRIPTION: HTTP POST request to re-rank text similarity using cURL.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/quick_tour.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncurl 127.0.0.1:8080/rerank \\\n    -X POST \\\n    -d '{\"query\":\"What is Deep Learning?\", \"texts\": [\"Deep Learning is not...\", \"Deep learning is...\"], \"raw_scores\": false}' \\\n    -H 'Content-Type: application/json'\n```\n\n----------------------------------------\n\nTITLE: Batch Embedding Generation\nDESCRIPTION: HTTP POST request to generate embeddings for multiple text inputs in batch.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/quick_tour.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl 127.0.0.1:8080/embed \\\n    -X POST \\\n    -d '{\"inputs\":[\"Today is a nice day\", \"I like you\"]}' \\\n    -H 'Content-Type: application/json'\n```\n\n----------------------------------------\n\nTITLE: Using Sequence Classification API Endpoint for Prediction\nDESCRIPTION: Example of using the predict endpoint to get emotions associated with an input text. This endpoint works with sequence classification models to classify text into predefined categories.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/README.md#2025-04-11_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\ncurl 127.0.0.1:8080/predict \\\n    -X POST \\\n    -d '{\"inputs\":\"I like you.\"}' \\\n    -H 'Content-Type: application/json'\n```\n\n----------------------------------------\n\nTITLE: Using Sparse Embedding API Endpoint\nDESCRIPTION: Example of using the embed_sparse endpoint to retrieve sparse embeddings from a SPLADE model. Sparse embeddings can be more efficient for certain retrieval tasks compared to dense embeddings.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/README.md#2025-04-11_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\ncurl 127.0.0.1:8080/embed_sparse \\\n    -X POST \\\n    -d '{\"inputs\":\"I like you.\"}' \\\n    -H 'Content-Type: application/json'\n```\n\n----------------------------------------\n\nTITLE: Deploying Re-ranker Models with TEI\nDESCRIPTION: Instructions for deploying re-ranker models using Text Embeddings Inference. Re-rankers are sequence classification cross-encoders that score similarity between queries and texts, useful for improving RAG pipelines.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/README.md#2025-04-11_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nmodel=BAAI/bge-reranker-large\nvolume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run\n\ndocker run --gpus all -p 8080:80 -v $volume:/data --pull always ghcr.io/huggingface/text-embeddings-inference:1.7 --model-id $model\n```\n\n----------------------------------------\n\nTITLE: Deploying Re-ranker Model\nDESCRIPTION: Docker command to deploy a re-ranker model for text similarity scoring.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/quick_tour.md#2025-04-11_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nmodel=BAAI/bge-reranker-large\nvolume=$PWD/data\n\ndocker run --gpus all -p 8080:80 -v $volume:/data --pull always ghcr.io/huggingface/text-embeddings-inference:1.7 --model-id $model\n```\n\n----------------------------------------\n\nTITLE: Deploying Sequence Classification Models with TEI\nDESCRIPTION: Instructions for deploying traditional sequence classification models like emotion classifiers. This example shows how to run a model that can identify emotions in text.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/README.md#2025-04-11_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nmodel=SamLowe/roberta-base-go_emotions\nvolume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run\n\ndocker run --gpus all -p 8080:80 -v $volume:/data --pull always ghcr.io/huggingface/text-embeddings-inference:1.7 --model-id $model\n```\n\n----------------------------------------\n\nTITLE: Deploying SPLADE Models with TEI for Sparse Embeddings\nDESCRIPTION: Instructions for deploying models with SPLADE pooling for Bert and Distilbert MaskedLM architectures. SPLADE (SParse Lexical AnD Expansion) models produce sparse embeddings useful for efficient information retrieval.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/README.md#2025-04-11_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nmodel=naver/efficient-splade-VI-BT-large-query\nvolume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run\n\ndocker run --gpus all -p 8080:80 -v $volume:/data --pull always ghcr.io/huggingface/text-embeddings-inference:1.7 --model-id $model --pooling splade\n```\n\n----------------------------------------\n\nTITLE: Deploying TEI with gRPC API Support\nDESCRIPTION: Instructions for running Text Embeddings Inference with gRPC API support instead of HTTP for high-performance deployments. The gRPC implementation follows the protobuf definition in the TEI repository.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/README.md#2025-04-11_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\nmodel=BAAI/bge-large-en-v1.5\nvolume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run\n\ndocker run --gpus all -p 8080:80 -v $volume:/data --pull always ghcr.io/huggingface/text-embeddings-inference:1.7-grpc --model-id $model\n```\n\n----------------------------------------\n\nTITLE: Deploying TEI in Air-Gapped Environments with Docker\nDESCRIPTION: Instructions for downloading model weights and deploying Text Embeddings Inference in an air-gapped environment using Docker volumes. This approach ensures the model is available without requiring internet access during deployment.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/README.md#2025-04-11_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\n# (Optional) create a `models` directory\nmkdir models\ncd models\n\n# Make sure you have git-lfs installed (https://git-lfs.com)\ngit lfs install\ngit clone https://huggingface.co/Alibaba-NLP/gte-base-en-v1.5\n\n# Set the models directory as the volume path\nvolume=$PWD\n\n# Mount the models directory inside the container with a volume and set the model ID\ndocker run --gpus all -p 8080:80 -v $volume:/data --pull always ghcr.io/huggingface/text-embeddings-inference:1.7 --model-id /data/gte-base-en-v1.5\n```\n\n----------------------------------------\n\nTITLE: Air-gapped Deployment Setup\nDESCRIPTION: Shell commands for downloading and deploying TEI in air-gapped environment using local model files.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/quick_tour.md#2025-04-11_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\n# (Optional) create a `models` directory\nmkdir models\ncd models\n\n# Make sure you have git-lfs installed (https://git-lfs.com)\ngit lfs install\ngit clone https://huggingface.co/Alibaba-NLP/gte-base-en-v1.5\n\n# Set the models directory as the volume path\nvolume=$PWD\n\n# Mount the models directory inside the container with a volume and set the model ID\ndocker run --gpus all -p 8080:80 -v $volume:/data --pull always ghcr.io/huggingface/text-embeddings-inference:1.7 --model-id /data/gte-base-en-v1.5\n```\n\n----------------------------------------\n\nTITLE: Running Text Embeddings Inference with Private Model Access\nDESCRIPTION: This Docker command demonstrates how to run Text Embeddings Inference with access to a private model using an authentication token.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/README.md#2025-04-11_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nmodel=<your private model>\nvolume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run\ntoken=<your cli READ token>\n\ndocker run --gpus all -e HF_TOKEN=$token -p 8080:80 -v $volume:/data --pull always ghcr.io/huggingface/text-embeddings-inference:1.7 --model-id $model\n```\n\n----------------------------------------\n\nTITLE: Deploying Private Model with Docker\nDESCRIPTION: Docker command to deploy a private model using Text Embeddings Inference. Configures GPU access, port mapping, data volume mounting, and authentication token.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/private_models.md#2025-04-11_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nmodel=<your private model>\nvolume=$PWD/data\ntoken=<your cli Hugging Face Hub token>\n\ndocker run --gpus all -e HF_TOKEN=$token -p 8080:80 -v $volume:/data --pull always ghcr.io/huggingface/text-embeddings-inference:1.7 --model-id $model\n```\n\n----------------------------------------\n\nTITLE: Setting HF Token Environment Variable\nDESCRIPTION: Sets up authentication by configuring the HF_TOKEN environment variable with a read token from Hugging Face Hub.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/private_models.md#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nexport HF_TOKEN=<YOUR READ TOKEN>\n```\n\n----------------------------------------\n\nTITLE: Building TEI Docker Container for CPU\nDESCRIPTION: Command for building a CPU-based Docker container for Text Embeddings Inference. This creates a portable deployment package that can run on any system with Docker.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/README.md#2025-04-11_snippet_20\n\nLANGUAGE: shell\nCODE:\n```\ndocker build .\n```\n\n----------------------------------------\n\nTITLE: Building TEI Docker Container for Apple M1/M2 ARM64\nDESCRIPTION: Command for building a Docker container for Apple M1/M2 ARM64 architectures. Note that Metal/MPS is not supported via Docker, so inference will be CPU-bound and likely slower.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/README.md#2025-04-11_snippet_22\n\nLANGUAGE: shell\nCODE:\n```\ndocker build . -f Dockerfile --platform=linux/arm64\n```\n\n----------------------------------------\n\nTITLE: Building TEI Docker Container for CUDA GPUs\nDESCRIPTION: Instructions for building a CUDA-enabled Docker container for TEI with various compute capability options for different NVIDIA GPU architectures. The build process requires specifying the correct compute capability for the target GPU.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/README.md#2025-04-11_snippet_21\n\nLANGUAGE: shell\nCODE:\n```\n# Get submodule dependencies\ngit submodule update --init\n\n# Example for Turing (T4, RTX 2000 series, ...)\nruntime_compute_cap=75\n\n# Example for A100\nruntime_compute_cap=80\n\n# Example for A10\nruntime_compute_cap=86\n\n# Example for Ada Lovelace (RTX 4000 series, ...)\nruntime_compute_cap=89\n\n# Example for H100\nruntime_compute_cap=90\n\ndocker build . -f Dockerfile-cuda --build-arg CUDA_COMPUTE_CAP=$runtime_compute_cap\n```\n\n----------------------------------------\n\nTITLE: Building CPU Container for TEI\nDESCRIPTION: Command to build a basic CPU-based Docker container for Text Embeddings Inference.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/custom_container.md#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ndocker build .\n```\n\n----------------------------------------\n\nTITLE: Building CUDA Container for TEI\nDESCRIPTION: Commands to build a CUDA-enabled Docker container for Text Embeddings Inference. Includes initialization of submodules and setting the GPU compute capability for specific hardware compatibility.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/custom_container.md#2025-04-11_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# Get submodule dependencies\ngit submodule update --init\n\nruntime_compute_cap=80\n\ndocker build . -f Dockerfile-cuda --build-arg CUDA_COMPUTE_CAP=$runtime_compute_cap\n```\n\n----------------------------------------\n\nTITLE: Installing TEI for GPU Acceleration\nDESCRIPTION: Instructions for installing Text Embeddings Inference with GPU acceleration for different NVIDIA GPU architectures. Different compilation flags are used for Turing GPUs versus Ampere and Hopper GPUs.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/README.md#2025-04-11_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\n# This can take a while as we need to compile a lot of cuda kernels\n\n# On Turing GPUs (T4, RTX 2000 series ... )\ncargo install --path router -F candle-cuda-turing -F http --no-default-features\n\n# On Ampere and Hopper\ncargo install --path router -F candle-cuda -F http --no-default-features\n```\n\n----------------------------------------\n\nTITLE: Running TEI Locally on GPU\nDESCRIPTION: Command for running Text Embeddings Inference locally on GPU after installation. This enables hardware acceleration for faster inference using NVIDIA GPUs.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/README.md#2025-04-11_snippet_19\n\nLANGUAGE: shell\nCODE:\n```\nmodel=BAAI/bge-large-en-v1.5\n\ntext-embeddings-router --model-id $model --port 8080\n```\n\n----------------------------------------\n\nTITLE: Installing TEI Locally with Different Backend Options\nDESCRIPTION: Instructions for installing Text Embeddings Inference locally with different backend options including ONNX, Intel MKL, and Apple Metal for M1/M2 processors. Each option targets specific hardware configurations.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/README.md#2025-04-11_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\n# On x86 with ONNX backend (recommended)\ncargo install --path router -F ort\n# On x86 with Intel backend\ncargo install --path router -F mkl\n# On M1 or M2\ncargo install --path router -F metal\n```\n\n----------------------------------------\n\nTITLE: Running TEI Locally on CPU\nDESCRIPTION: Command for running Text Embeddings Inference locally on CPU after installation. This allows using TEI without Docker, directly on the host machine.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/README.md#2025-04-11_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\nmodel=BAAI/bge-large-en-v1.5\n\ntext-embeddings-router --model-id $model --port 8080\n```\n\n----------------------------------------\n\nTITLE: Installing Rust for Local TEI Deployment\nDESCRIPTION: Instructions for installing Rust, which is a prerequisite for local installation of Text Embeddings Inference. Rust is used to compile the TEI codebase for local execution.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/README.md#2025-04-11_snippet_13\n\nLANGUAGE: shell\nCODE:\n```\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for TEI on Linux\nDESCRIPTION: Instructions for installing OpenSSL libraries and gcc on Linux, which may be required for local TEI installation. These are common dependencies for Rust applications that use cryptography features.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/README.md#2025-04-11_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\nsudo apt-get install libssl-dev gcc -y\n```\n\n----------------------------------------\n\nTITLE: Setting NVIDIA CUDA Path for TEI GPU Installation\nDESCRIPTION: Command for adding NVIDIA CUDA binaries to the system path, which is necessary for GPU-accelerated TEI deployment. This ensures the CUDA tools are accessible during compilation and runtime.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/README.md#2025-04-11_snippet_17\n\nLANGUAGE: shell\nCODE:\n```\nexport PATH=$PATH:/usr/local/cuda/bin\n```\n\n----------------------------------------\n\nTITLE: Installing Rust using Shell Command\nDESCRIPTION: This command downloads and runs the Rust installation script, prompting the user to follow on-screen instructions.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/local_gpu.md#2025-04-11_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n```\n\n----------------------------------------\n\nTITLE: Installing TEI for Turing GPUs using Cargo\nDESCRIPTION: This command installs Text Embeddings Inference with support for Turing GPUs (T4, RTX 2000 series) using Cargo, Rust's package manager.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/local_gpu.md#2025-04-11_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncargo install --path router -F candle-cuda-turing -F http --no-default-features\n```\n\n----------------------------------------\n\nTITLE: Installing TEI for Ampere and Hopper GPUs using Cargo\nDESCRIPTION: This command installs Text Embeddings Inference with support for Ampere and Hopper GPUs using Cargo, Rust's package manager.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/local_gpu.md#2025-04-11_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\ncargo install --path router -F candle-cuda -F http --no-default-features\n```\n\n----------------------------------------\n\nTITLE: Launching Text Embeddings Inference on GPU\nDESCRIPTION: This shell script sets up environment variables and launches Text Embeddings Inference on GPU, specifying the model, revision, and port.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/local_gpu.md#2025-04-11_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nmodel=BAAI/bge-large-en-v1.5\nrevision=refs/pr/5\n\ntext-embeddings-router --model-id $model --revision $revision --port 8080\n```\n\n----------------------------------------\n\nTITLE: Installing Rust for Text Embeddings Inference\nDESCRIPTION: Command to install Rust on your machine, which is a prerequisite for running Text Embeddings Inference locally.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/local_cpu.md#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n```\n\n----------------------------------------\n\nTITLE: Installing TEI on x86 Machines with MKL Support\nDESCRIPTION: Command to install Text Embeddings Inference on x86 architecture machines with Math Kernel Library (MKL) support.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/local_cpu.md#2025-04-11_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncargo install --path router -F mkl\n```\n\n----------------------------------------\n\nTITLE: Installing TEI on M1/M2 Machines with Metal Support\nDESCRIPTION: Command to install Text Embeddings Inference on Apple M1 or M2 architecture machines with Metal support for optimized performance.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/local_cpu.md#2025-04-11_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ncargo install --path router -F metal\n```\n\n----------------------------------------\n\nTITLE: Launching Text Embeddings Inference on CPU\nDESCRIPTION: Command to start the Text Embeddings Inference service on CPU using a specific model. This example uses the BAAI/bge-large-en-v1.5 model with a specific revision.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/local_cpu.md#2025-04-11_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nmodel=BAAI/bge-large-en-v1.5\nrevision=refs/pr/5\n\ntext-embeddings-router --model-id $model --revision $revision --port 8080\n```\n\n----------------------------------------\n\nTITLE: Installing Required Dependencies on Linux\nDESCRIPTION: Command to install OpenSSL libraries and gcc on Linux machines, which might be required for Text Embeddings Inference to function properly.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/local_cpu.md#2025-04-11_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nsudo apt-get install libssl-dev gcc -y\n```\n\n----------------------------------------\n\nTITLE: Installing Rust Environment\nDESCRIPTION: Command to download and install Rust programming language using rustup installer script\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/local_metal.md#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n```\n\n----------------------------------------\n\nTITLE: Installing TEI with Metal Support\nDESCRIPTION: Cargo command to install text-embeddings-inference with Metal feature flag enabled\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/local_metal.md#2025-04-11_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncargo install --path router -F metal\n```\n\n----------------------------------------\n\nTITLE: Launching TEI Server\nDESCRIPTION: Commands to set model parameters and launch the text-embeddings-inference server on port 8080\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/local_metal.md#2025-04-11_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nmodel=BAAI/bge-large-en-v1.5\nrevision=refs/pr/5\n\ntext-embeddings-router --model-id $model --revision $revision --port 8080\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Intel CPU\nDESCRIPTION: Command to build a Docker image optimized for Intel CPUs. Sets the platform to 'cpu' and creates an image tagged as 'tei_cpu_ipex'.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/intel_container.md#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nplatform=\"cpu\"\n\ndocker build . -f Dockerfile-intel --build-arg PLATFORM=$platform -t tei_cpu_ipex\n```\n\n----------------------------------------\n\nTITLE: Deploying Docker Container on Intel CPU\nDESCRIPTION: Command to deploy a model on an Intel CPU using the built Docker image. Maps port 8080 to the container's port 80 and mounts a local volume for data storage.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/intel_container.md#2025-04-11_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nmodel='BAAI/bge-large-en-v1.5'\nvolume=$PWD/data\n\ndocker run -p 8080:80 -v $volume:/data tei_cpu_ipex --model-id $model\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Intel XPU\nDESCRIPTION: Command to build a Docker image optimized for Intel XPUs. Sets the platform to 'xpu' and creates an image tagged as 'tei_xpu_ipex'.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/intel_container.md#2025-04-11_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nplatform=\"xpu\"\n\ndocker build . -f Dockerfile-intel --build-arg PLATFORM=$platform -t tei_xpu_ipex\n```\n\n----------------------------------------\n\nTITLE: Deploying Docker Container on Intel XPU\nDESCRIPTION: Command to deploy a model on an Intel XPU using the built Docker image. Includes device mapping for XPU access and sets the data type to float16 for optimal performance.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/intel_container.md#2025-04-11_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nmodel='BAAI/bge-large-en-v1.5'\nvolume=$PWD/data\n\ndocker run -p 8080:80 -v $volume:/data --device=/dev/dri -v /dev/dri/by-path:/dev/dri/by-path tei_xpu_ipex --model-id $model --dtype float16\n```\n\n----------------------------------------\n\nTITLE: Building Docker Image for Intel HPU (Gaudi)\nDESCRIPTION: Command to build a Docker image optimized for Intel HPUs (Gaudi accelerators). Sets the platform to 'hpu' and creates an image tagged as 'tei_hpu'.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/intel_container.md#2025-04-11_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nplatform=\"hpu\"\n\ndocker build . -f Dockerfile-intel --build-arg PLATFORM=$platform -t tei_hpu\n```\n\n----------------------------------------\n\nTITLE: Deploying Docker Container on Intel HPU (Gaudi)\nDESCRIPTION: Command to deploy a model on an Intel HPU (Gaudi) using the built Docker image. Uses Habana runtime, configures visible devices, and sets the data type to bfloat16 for optimal performance.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/intel_container.md#2025-04-11_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nmodel='BAAI/bge-large-en-v1.5'\nvolume=$PWD/data\n\ndocker run -p 8080:80 -v $volume:/data --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e MAX_WARMUP_SEQUENCE_LENGTH=512 tei_hpu --model-id $model --dtype bfloat16\n```\n\n----------------------------------------\n\nTITLE: Pulling Prebuilt Docker Image for Intel CPU\nDESCRIPTION: Command to pull a prebuilt Docker image optimized for Intel CPUs from GitHub Container Registry, removing the need to build the image manually.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/intel_container.md#2025-04-11_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\ndocker pull ghcr.io/huggingface/text-embeddings-inference:cpu-ipex-latest\n```\n\n----------------------------------------\n\nTITLE: Pulling Prebuilt Docker Image for Intel XPU\nDESCRIPTION: Command to pull a prebuilt Docker image optimized for Intel XPUs from GitHub Container Registry, removing the need to build the image manually.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/intel_container.md#2025-04-11_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\ndocker pull ghcr.io/huggingface/text-embeddings-inference:xpu-ipex-latest\n```\n\n----------------------------------------\n\nTITLE: Pulling Prebuilt Docker Image for Intel HPU (Gaudi)\nDESCRIPTION: Command to pull a prebuilt Docker image optimized for Intel HPUs (Gaudi) from GitHub Container Registry, removing the need to build the image manually.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/intel_container.md#2025-04-11_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\ndocker pull ghcr.io/huggingface/text-embeddings-inference:hpu-latest\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Google Cloud Platform Deployment\nDESCRIPTION: Sets up environment variables for the Google Cloud Platform project, including project ID, location, container URI, service name, and model ID.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/tei_cloud_run.md#2025-04-11_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport PROJECT_ID=your-project-id\nexport LOCATION=europe-west1  # or any location you prefer: https://cloud.google.com/run/docs/locations\nexport CONTAINER_URI=\"gcr.io/deeplearning-platform-release/huggingface-text-embeddings-inference-cpu.1-6\"\nexport SERVICE_NAME=\"text-embedding-server\" # choose a name for your service\nexport MODEL_ID=\"ibm-granite/granite-embedding-278m-multilingual\" # choose any embedding model\n```\n\n----------------------------------------\n\nTITLE: Authenticating and Configuring Google Cloud CLI\nDESCRIPTION: Logs into Google Cloud account, sets up application default credentials, and configures the project ID for deployment.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/tei_cloud_run.md#2025-04-11_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngcloud auth login\ngcloud auth application-default login  # For local development\ngcloud config set project $PROJECT_ID\n```\n\n----------------------------------------\n\nTITLE: Enabling Cloud Run API\nDESCRIPTION: Enables the Cloud Run API, which is required for deploying Text Embeddings Inference on Google Cloud Run.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/tei_cloud_run.md#2025-04-11_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngcloud services enable run.googleapis.com\n```\n\n----------------------------------------\n\nTITLE: Deploying Text Embeddings Inference on Cloud Run (CPU)\nDESCRIPTION: Deploys Text Embeddings Inference on Google Cloud Run using CPU resources. Specifies model, resource allocation, and security settings.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/tei_cloud_run.md#2025-04-11_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngcloud run deploy $SERVICE_NAME \\\n    --image=$CONTAINER_URI \\\n    --args=\"--model-id=$MODEL_ID,--max-concurrent-requests=64\" \\\n    --set-env-vars=HF_HUB_ENABLE_HF_TRANSFER=1 \\\n    --port=8080 \\\n    --cpu=8 \\\n    --memory=32Gi \\\n    --region=$LOCATION \\\n    --no-allow-unauthenticated\n```\n\n----------------------------------------\n\nTITLE: Deploying Text Embeddings Inference on Cloud Run (GPU)\nDESCRIPTION: Deploys Text Embeddings Inference on Google Cloud Run using GPU resources. Includes GPU-specific settings and resource allocation.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/tei_cloud_run.md#2025-04-11_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngcloud run deploy $SERVICE_NAME \\\n    --image=$CONTAINER_URI \\\n    --args=\"--model-id=$MODEL_ID,--max-concurrent-requests=64\" \\\n    --set-env-vars=HF_HUB_ENABLE_HF_TRANSFER=1 \\\n    --port=8080 \\\n    --cpu=8 \\\n    --memory=32Gi \\\n    --no-cpu-throttling \\\n    --gpu=1 \\\n    --gpu-type=nvidia-l4 \\\n    --max-instances=3 \\\n    --concurrency=64 \\\n    --region=$LOCATION \\\n    --no-allow-unauthenticated\n```\n\n----------------------------------------\n\nTITLE: Deploying Text Embeddings Inference with Cloud NAT\nDESCRIPTION: Deploys Text Embeddings Inference on Google Cloud Run with GPU and Cloud NAT configuration for improved network performance.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/tei_cloud_run.md#2025-04-11_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngcloud beta run deploy $SERVICE_NAME \\\n    --image=$CONTAINER_URI \\\n    --args=\"--model-id=$MODEL_ID,--max-concurrent-requests=64\" \\\n    --set-env-vars=HF_HUB_ENABLE_HF_TRANSFER=1 \\\n    --port=8080 \\\n    --cpu=8 \\\n    --memory=32Gi \\\n    --no-cpu-throttling \\\n    --gpu=1 \\\n    --gpu-type=nvidia-l4 \\\n    --max-instances=3 \\\n    --concurrency=64 \\\n    --region=$LOCATION \\\n    --no-allow-unauthenticated \\\n    --vpc-egress=all-traffic \\\n    --subnet=default\n```\n\n----------------------------------------\n\nTITLE: Using Cloud Run Proxy to connect to TEI service\nDESCRIPTION: Creates a local proxy server that forwards requests to the deployed Cloud Run service with credentials attached. This allows for easy testing without handling authentication explicitly.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/tei_cloud_run.md#2025-04-11_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ngcloud run services proxy $SERVICE_NAME --region $LOCATION\n```\n\n----------------------------------------\n\nTITLE: Retrieving Cloud Run service URL\nDESCRIPTION: Command to extract the URL of the deployed Cloud Run service. This URL is required when directly calling the service with authentication instead of using the proxy.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/tei_cloud_run.md#2025-04-11_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nSERVICE_URL=$(gcloud run services describe $SERVICE_NAME --region $LOCATION --format 'value(status.url)')\n```\n\n----------------------------------------\n\nTITLE: Generating identity token for authentication\nDESCRIPTION: Command to generate an identity token from Google Cloud SDK, which can be used to authenticate requests to the Cloud Run service.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/tei_cloud_run.md#2025-04-11_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ngcloud auth print-identity-token\n```\n\n----------------------------------------\n\nTITLE: Generating identity token with Python\nDESCRIPTION: Python code to generate a Google Cloud identity token that can be used for authenticating requests to the Cloud Run service.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/tei_cloud_run.md#2025-04-11_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nimport google.auth\nfrom google.auth.transport.requests import Request as GoogleAuthRequest\n\nauth_req = GoogleAuthRequest()\ncreds, _ = google.auth.default()\ncreds.refresh(auth_req)\n\nid_token = creds.id_token\n```\n\n----------------------------------------\n\nTITLE: Setting service account name for TEI invocation\nDESCRIPTION: Sets an environment variable for the service account name that will be used to access the TEI service.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/tei_cloud_run.md#2025-04-11_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nexport SERVICE_ACCOUNT_NAME=tei-invoker\n```\n\n----------------------------------------\n\nTITLE: Creating service account for TEI invocation\nDESCRIPTION: Creates a Google Cloud service account that will be used to invoke the TEI service.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/tei_cloud_run.md#2025-04-11_snippet_15\n\nLANGUAGE: bash\nCODE:\n```\ngcloud iam service-accounts create $SERVICE_ACCOUNT_NAME\n```\n\n----------------------------------------\n\nTITLE: Granting Cloud Run Invoker role to service account\nDESCRIPTION: Grants the necessary permissions to the service account to invoke the TEI Cloud Run service.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/tei_cloud_run.md#2025-04-11_snippet_16\n\nLANGUAGE: bash\nCODE:\n```\ngcloud run services add-iam-policy-binding $SERVICE_NAME \\\n    --member=\"serviceAccount:$SERVICE_ACCOUNT_NAME@$PROJECT_ID.iam.gserviceaccount.com\" \\\n    --role=\"roles/run.invoker\" \\\n    --region=$LOCATION\n```\n\n----------------------------------------\n\nTITLE: Generating access token for service account\nDESCRIPTION: Creates an access token for the service account that can be used to authenticate requests to the TEI service.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/tei_cloud_run.md#2025-04-11_snippet_17\n\nLANGUAGE: bash\nCODE:\n```\nexport ACCESS_TOKEN=$(gcloud auth print-access-token --impersonate-service-account=$SERVICE_ACCOUNT_NAME@$PROJECT_ID.iam.gserviceaccount.com)\n```\n\n----------------------------------------\n\nTITLE: Making authenticated cURL requests to TEI service\nDESCRIPTION: Example of sending an authenticated request to the TEI service using cURL with the access token.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/tei_cloud_run.md#2025-04-11_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\ncurl $SERVICE_URL/v1/embeddigs \\\n    -X POST \\\n    -H \"Authorization: Bearer $ACCESS_TOKEN\" \\\n    -H 'Content-Type: application/json' \\\n    -d '{\n        \"model\": \"tei\",\n        \"text\": \"What is deep learning?\"\n    }'\n```\n\n----------------------------------------\n\nTITLE: Using huggingface_hub Python SDK with authentication\nDESCRIPTION: Example of using the huggingface_hub Python SDK with proper authentication to generate embeddings from the deployed TEI service.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/tei_cloud_run.md#2025-04-11_snippet_19\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom huggingface_hub import InferenceClient\n\nclient = InferenceClient(\n    base_url=os.getenv(\"SERVICE_URL\"),\n    api_key=os.getenv(\"ACCESS_TOKEN\"),\n)\n\nembedding = client.feature_extraction(\"What is deep learning?\",\n                                  model=\"http://localhost:8080/embed\")\nprint(len(embedding[0]))\n```\n\n----------------------------------------\n\nTITLE: Using OpenAI Python SDK with authentication\nDESCRIPTION: Example of using the OpenAI Python SDK with proper authentication to generate embeddings from the deployed TEI service.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/tei_cloud_run.md#2025-04-11_snippet_20\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=os.getenv(\"SERVICE_URL\"),\n    api_key=os.getenv(\"ACCESS_TOKEN\"),\n)\n\nresponse = client.embeddings.create(\n  model=\"tei\",\n  input=\"What is deep learning?\"\n)\n\nprint(response)\n```\n\n----------------------------------------\n\nTITLE: Deleting Cloud Run service\nDESCRIPTION: Command to remove the TEI Cloud Run service to avoid incurring unnecessary costs after use.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/tei_cloud_run.md#2025-04-11_snippet_21\n\nLANGUAGE: bash\nCODE:\n```\ngcloud run services delete $SERVICE_NAME --region $LOCATION\n```\n\n----------------------------------------\n\nTITLE: Revoking service account access token\nDESCRIPTION: Command to revoke the access token generated for the service account once it's no longer needed.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/tei_cloud_run.md#2025-04-11_snippet_22\n\nLANGUAGE: bash\nCODE:\n```\ngcloud auth revoke --impersonate-service-account=$SERVICE_ACCOUNT_NAME@$PROJECT_ID.iam.gserviceaccount.com\n```\n\n----------------------------------------\n\nTITLE: Deleting service account\nDESCRIPTION: Command to remove the service account created for TEI service invocation.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/tei_cloud_run.md#2025-04-11_snippet_23\n\nLANGUAGE: bash\nCODE:\n```\ngcloud iam service-accounts delete $SERVICE_ACCOUNT_NAME@$PROJECT_ID.iam.gserviceaccount.com\n```\n\n----------------------------------------\n\nTITLE: Removing Cloud NAT resources\nDESCRIPTION: Commands to clean up Cloud NAT resources if they were enabled for VPC networking with the TEI service.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/tei_cloud_run.md#2025-04-11_snippet_24\n\nLANGUAGE: bash\nCODE:\n```\ngcloud compute routers nats delete vm-nat --router=nat-router --region=$LOCATION\ngcloud compute routers delete nat-router --region=$LOCATION\n```\n\n----------------------------------------\n\nTITLE: Model Support Table - Embeddings Models\nDESCRIPTION: Markdown table listing supported embedding models with their MTEB rank, size, type, and model ID references.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/supported_models.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n| MTEB Rank | Model Size          | Model Type  | Model ID                                                                                         |\n|-----------|---------------------|-------------|--------------------------------------------------------------------------------------------------|\n| 3         | 7B (Very Expensive) | Qwen2       | [Alibaba-NLP/gte-Qwen2-7B-instruct](https://hf.co/Alibaba-NLP/gte-Qwen2-7B-instruct)             |\n| 11        | 1.5B (Expensive)    | Qwen2       | [Alibaba-NLP/gte-Qwen2-1.5B-instruct](https://hf.co/Alibaba-NLP/gte-Qwen2-1.5B-instruct)         |\n```\n\n----------------------------------------\n\nTITLE: Model Support Table - Re-rankers and Sequence Classification\nDESCRIPTION: Markdown table showing supported re-ranker and sequence classification models with their respective tasks, types, and model IDs.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/supported_models.md#2025-04-11_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n| Task               | Model Type  | Model ID                                                                                                        |\n|--------------------|-------------|-----------------------------------------------------------------------------------------------------------------|\n| Re-Ranking         | XLM-RoBERTa | [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large)                                       |\n```\n\n----------------------------------------\n\nTITLE: Hardware Support Table - Docker Images\nDESCRIPTION: Markdown table listing supported hardware architectures and their corresponding Docker image specifications for deployment.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/supported_models.md#2025-04-11_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n| Architecture                        | Image                                                                    |\n|-------------------------------------|--------------------------------------------------------------------------|\n| CPU                                 | ghcr.io/huggingface/text-embeddings-inference:cpu-1.7                    |\n| Volta                               | NOT SUPPORTED                                                            |\n```\n\n----------------------------------------\n\nTITLE: Displaying Text Embeddings Router Help Command\nDESCRIPTION: Command to display all available options and arguments for the text-embeddings-router CLI tool.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/cli_arguments.md#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ntext-embeddings-router --help\n```\n\n----------------------------------------\n\nTITLE: Text Embeddings Router CLI Options Reference\nDESCRIPTION: Complete reference of all available CLI options including model configuration, server settings, batch processing, and monitoring parameters.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/cli_arguments.md#2025-04-11_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nUsage: text-embeddings-router [OPTIONS]\n\nOptions:\n      --model-id <MODEL_ID>\n          The name of the model to load. Can be a MODEL_ID as listed on <https://hf.co/models> like `thenlper/gte-base`.\n          Or it can be a local directory containing the necessary files as saved by `save_pretrained(...)` methods of\n          transformers\n\n          [env: MODEL_ID=]\n          [default: thenlper/gte-base]\n\n      --revision <REVISION>\n          The actual revision of the model if you're referring to a model on the hub. You can use a specific commit id\n          or a branch like `refs/pr/2`\n\n          [env: REVISION=]\n\n      --tokenization-workers <TOKENIZATION_WORKERS>\n          Optionally control the number of tokenizer workers used for payload tokenization, validation and truncation.\n          Default to the number of CPU cores on the machine\n\n          [env: TOKENIZATION_WORKERS=]\n\n      --dtype <DTYPE>\n          The dtype to be forced upon the model\n\n          [env: DTYPE=]\n          [possible values: float16, float32]\n\n      --pooling <POOLING>\n          Optionally control the pooling method for embedding models.\n\n          If `pooling` is not set, the pooling configuration will be parsed from the model `1_Pooling/config.json` configuration.\n\n          If `pooling` is set, it will override the model pooling configuration\n\n          [env: POOLING=]\n\n          Possible values:\n          - cls:        Select the CLS token as embedding\n          - mean:       Apply Mean pooling to the model embeddings\n          - splade:     Apply SPLADE (Sparse Lexical and Expansion) to the model embeddings. This option is only\n          available if the loaded model is a `ForMaskedLM` Transformer model\n          - last-token: Select the last token as embedding\n\n      --max-concurrent-requests <MAX_CONCURRENT_REQUESTS>\n          The maximum amount of concurrent requests for this particular deployment.\n          Having a low limit will refuse clients requests instead of having them wait for too long and is usually good\n          to handle backpressure correctly\n\n          [env: MAX_CONCURRENT_REQUESTS=]\n          [default: 512]\n\n      --max-batch-tokens <MAX_BATCH_TOKENS>\n          **IMPORTANT** This is one critical control to allow maximum usage of the available hardware.\n\n          This represents the total amount of potential tokens within a batch.\n\n          For `max_batch_tokens=1000`, you could fit `10` queries of `total_tokens=100` or a single query of `1000` tokens.\n\n          Overall this number should be the largest possible until the model is compute bound. Since the actual memory\n          overhead depends on the model implementation, text-embeddings-inference cannot infer this number automatically.\n\n          [env: MAX_BATCH_TOKENS=]\n          [default: 16384]\n\n      --max-batch-requests <MAX_BATCH_REQUESTS>\n          Optionally control the maximum number of individual requests in a batch\n\n          [env: MAX_BATCH_REQUESTS=]\n\n      --max-client-batch-size <MAX_CLIENT_BATCH_SIZE>\n          Control the maximum number of inputs that a client can send in a single request\n\n          [env: MAX_CLIENT_BATCH_SIZE=]\n          [default: 32]\n\n      --auto-truncate\n          Automatically truncate inputs that are longer than the maximum supported size\n\n          Unused for gRPC servers\n\n          [env: AUTO_TRUNCATE=]\n\n      --default-prompt-name <DEFAULT_PROMPT_NAME>\n          The name of the prompt that should be used by default for encoding. If not set, no prompt will be applied.\n\n          Must be a key in the `sentence-transformers` configuration `prompts` dictionary.\n\n          For example if ``default_prompt_name`` is \"query\" and the ``prompts`` is {\"query\": \"query: \", ...}, then the\n          sentence \"What is the capital of France?\" will be encoded as \"query: What is the capital of France?\" because\n          the prompt text will be prepended before any text to encode.\n\n          The argument '--default-prompt-name <DEFAULT_PROMPT_NAME>' cannot be used with '--default-prompt <DEFAULT_PROMPT>`\n\n          [env: DEFAULT_PROMPT_NAME=]\n\n      --default-prompt <DEFAULT_PROMPT>\n          The prompt that should be used by default for encoding. If not set, no prompt will be applied.\n\n          For example if ``default_prompt`` is \"query: \" then the sentence \"What is the capital of France?\" will be\n          encoded as \"query: What is the capital of France?\" because the prompt text will be prepended before any text\n          to encode.\n\n          The argument '--default-prompt <DEFAULT_PROMPT>' cannot be used with '--default-prompt-name <DEFAULT_PROMPT_NAME>`\n\n          [env: DEFAULT_PROMPT=]\n\n      --hf-token <HF_TOKEN>\n          Your Hugging Face Hub token\n\n          [env: HF_TOKEN=]\n\n      --hostname <HOSTNAME>\n          The IP address to listen on\n\n          [env: HOSTNAME=]\n          [default: 0.0.0.0]\n\n      -p, --port <PORT>\n          The port to listen on\n\n          [env: PORT=]\n          [default: 3000]\n\n      --uds-path <UDS_PATH>\n          The name of the unix socket some text-embeddings-inference backends will use as they communicate internally\n          with gRPC\n\n          [env: UDS_PATH=]\n          [default: /tmp/text-embeddings-inference-server]\n\n      --huggingface-hub-cache <HUGGINGFACE_HUB_CACHE>\n          The location of the huggingface hub cache. Used to override the location if you want to provide a mounted disk\n          for instance\n\n          [env: HUGGINGFACE_HUB_CACHE=]\n\n      --payload-limit <PAYLOAD_LIMIT>\n          Payload size limit in bytes\n\n          Default is 2MB\n\n          [env: PAYLOAD_LIMIT=]\n          [default: 2000000]\n\n      --api-key <API_KEY>\n          Set an api key for request authorization.\n\n          By default the server responds to every request. With an api key set, the requests must have the Authorization\n          header set with the api key as Bearer token.\n\n          [env: API_KEY=]\n\n      --json-output\n          Outputs the logs in JSON format (useful for telemetry)\n\n          [env: JSON_OUTPUT=]\n\n      --disable-spans\n          Disables the span logging trace\n\n          [env: DISABLE_SPANS=]\n\n      --otlp-endpoint <OTLP_ENDPOINT>\n          The grpc endpoint for opentelemetry. Telemetry is sent to this endpoint as OTLP over gRPC. e.g. `http://localhost:4317`\n\n          [env: OTLP_ENDPOINT=]\n\n      --otlp-service-name <OTLP_SERVICE_NAME>\n          The service name for opentelemetry. e.g. `text-embeddings-inference.server`\n\n          [env: OTLP_SERVICE_NAME=]\n          [default: text-embeddings-inference.server]\n\n      --cors-allow-origin <CORS_ALLOW_ORIGIN>\n          Unused for gRPC servers\n\n          [env: CORS_ALLOW_ORIGIN=]\n```\n\n----------------------------------------\n\nTITLE: Displaying Text Embeddings Router Help Information\nDESCRIPTION: This command shows how to display the help information for the text-embeddings-router, listing all available options and their descriptions.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/README.md#2025-04-11_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ntext-embeddings-router --help\n```\n\n----------------------------------------\n\nTITLE: Installing Text Embeddings Inference Python gRPC Server\nDESCRIPTION: This command installs the necessary dependencies for the Text Embeddings Inference Python gRPC server. It uses a Makefile to simplify the installation process.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/backends/python/server/README.md#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nmake install\n```\n\n----------------------------------------\n\nTITLE: Running Text Embeddings Inference Python gRPC Server in Development Mode\nDESCRIPTION: This command starts the Text Embeddings Inference Python gRPC server in development mode. It uses a Makefile to execute the server with the appropriate configuration for development purposes.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/backends/python/server/README.md#2025-04-11_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nmake run-dev\n```\n\n----------------------------------------\n\nTITLE: Flash Attention v1 Layer Documentation\nDESCRIPTION: Documentation describing the purpose and compatibility of Flash Attention v1 layer as an alternative to the official Candle flash attention layer for Turing GPUs.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/candle-extensions/candle-flash-attn-v1/README.md#2025-04-11_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n# Candle Flash Attention v1 Layer\n\nFlash Attention v2 does not support Turing GPUs (T4, RTX 2080). This layer can be used in replacement of the official\nflash attention Candle layer in the meantime.\n```\n\n----------------------------------------\n\nTITLE: Adding NVIDIA Binaries to PATH in Shell\nDESCRIPTION: This command adds the NVIDIA binaries to the system PATH, allowing access to CUDA tools.\nSOURCE: https://github.com/huggingface/text-embeddings-inference/blob/main/docs/source/en/local_gpu.md#2025-04-11_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nexport PATH=$PATH:/usr/local/cuda/bin\n```"
  }
]
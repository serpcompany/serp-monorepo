[
  {
    "owner": "kubeflow",
    "repo": "website",
    "content": "TITLE: Invalid Component with External Constant\nDESCRIPTION: Example demonstrating incorrect component implementation that violates hermeticity by referencing an external constant.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/lightweight-python-components.md#2025-04-10_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# non-example!\nINVALID_CONSTANT = 2\n\n@dsl.component\ndef errored_double(a: int) -> int:\n    \"\"\"Fails at runtime.\"\"\"\n    return INVALID_CONSTANT * a\n```\n\n----------------------------------------\n\nTITLE: Basic Kubeflow Pipeline Component Definition\nDESCRIPTION: Example YAML specification for an XGBoost classifier training component showing the basic structure including metadata, inputs, outputs and container implementation.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/reference/component-spec.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nname: xgboost4j - Train classifier\ndescription: Trains a boosted tree ensemble classifier using xgboost4j\n\ninputs:\n- {name: Training data}\n- {name: Rounds, type: Integer, default: '30', description: 'Number of training rounds'}\n\noutputs:\n- {name: Trained model, type: XGBoost model, description: 'Trained XGBoost model'}\n\nimplementation:\n  container:\n    image: gcr.io/ml-pipeline/xgboost-classifier-train@sha256:b3a64d57\n    command: [\n      /ml/train.py,\n      --train-set, {inputPath: Training data},\n      --rounds,    {inputValue: Rounds},\n      --out-model, {outputPath: Trained model},\n    ]\n```\n\n----------------------------------------\n\nTITLE: Deploying Kubeflow Pipelines Using Kustomize\nDESCRIPTION: Commands to deploy Kubeflow Pipelines components using kustomize manifests. Sets pipeline version, applies cluster-scoped resources, waits for CRD establishment, and deploys development environment configuration.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/operator-guides/installation/_index.md#2025-04-10_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport PIPELINE_VERSION={{% pipelines/latest-version %}}\nkubectl apply -k \"github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref=$PIPELINE_VERSION\"\nkubectl wait --for condition=established --timeout=60s crd/applications.app.k8s.io\nkubectl apply -k \"github.com/kubeflow/pipelines/manifests/kustomize/env/dev?ref=$PIPELINE_VERSION\"\n```\n\n----------------------------------------\n\nTITLE: Creating and Running an Advanced ML Pipeline with Kubeflow Pipelines in Python\nDESCRIPTION: This code snippet defines and executes an advanced ML pipeline using Kubeflow Pipelines. It includes components for dataset creation, normalization, and model training. The pipeline demonstrates the use of input/output artifacts, package installation at runtime, and parallel execution for hyperparameter tuning.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/build-advanced-pipeline.md#2025-04-10_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\n\nfrom kfp import client\nfrom kfp import dsl\nfrom kfp.dsl import Dataset\nfrom kfp.dsl import Input\nfrom kfp.dsl import Model\nfrom kfp.dsl import Output\n\n\n@dsl.component(packages_to_install=['pandas==1.3.5'])\ndef create_dataset(iris_dataset: Output[Dataset]):\n    import pandas as pd\n\n    csv_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n    col_names = [\n        'Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width', 'Labels'\n    ]\n    df = pd.read_csv(csv_url, names=col_names)\n\n    with open(iris_dataset.path, 'w') as f:\n        df.to_csv(f)\n\n\n@dsl.component(packages_to_install=['pandas==1.3.5', 'scikit-learn==1.0.2'])\ndef normalize_dataset(\n    input_iris_dataset: Input[Dataset],\n    normalized_iris_dataset: Output[Dataset],\n    standard_scaler: bool,\n    min_max_scaler: bool,\n):\n    if standard_scaler is min_max_scaler:\n        raise ValueError(\n            'Exactly one of standard_scaler or min_max_scaler must be True.')\n\n    import pandas as pd\n    from sklearn.preprocessing import MinMaxScaler\n    from sklearn.preprocessing import StandardScaler\n\n    with open(input_iris_dataset.path) as f:\n        df = pd.read_csv(f)\n    labels = df.pop('Labels')\n\n    if standard_scaler:\n        scaler = StandardScaler()\n    if min_max_scaler:\n        scaler = MinMaxScaler()\n\n    df = pd.DataFrame(scaler.fit_transform(df))\n    df['Labels'] = labels\n    with open(normalized_iris_dataset.path, 'w') as f:\n        df.to_csv(f)\n\n\n@dsl.component(packages_to_install=['pandas==1.3.5', 'scikit-learn==1.0.2'])\ndef train_model(\n    normalized_iris_dataset: Input[Dataset],\n    model: Output[Model],\n    n_neighbors: int,\n):\n    import pickle\n\n    import pandas as pd\n    from sklearn.model_selection import train_test_split\n    from sklearn.neighbors import KNeighborsClassifier\n\n    with open(normalized_iris_dataset.path) as f:\n        df = pd.read_csv(f)\n\n    y = df.pop('Labels')\n    X = df\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n    clf.fit(X_train, y_train)\n    with open(model.path, 'wb') as f:\n        pickle.dump(clf, f)\n\n\n@dsl.pipeline(name='iris-training-pipeline')\ndef my_pipeline(\n    standard_scaler: bool,\n    min_max_scaler: bool,\n    neighbors: List[int],\n):\n    create_dataset_task = create_dataset()\n\n    normalize_dataset_task = normalize_dataset(\n        input_iris_dataset=create_dataset_task.outputs['iris_dataset'],\n        standard_scaler=True,\n        min_max_scaler=False)\n\n    with dsl.ParallelFor(neighbors) as n_neighbors:\n        train_model(\n            normalized_iris_dataset=normalize_dataset_task\n            .outputs['normalized_iris_dataset'],\n            n_neighbors=n_neighbors)\n\n\nendpoint = '<KFP_UI_URL>'\nkfp_client = client.Client(host=endpoint)\nrun = kfp_client.create_run_from_pipeline_func(\n    my_pipeline,\n    arguments={\n        'min_max_scaler': True,\n        'standard_scaler': False,\n        'neighbors': [3, 6, 9]\n    },\n)\nurl = f'{endpoint}/#/runs/details/{run.run_id}'\nprint(url)\n```\n\n----------------------------------------\n\nTITLE: Implementing Nested Pipelines in Kubeflow using Python\nDESCRIPTION: This code snippet demonstrates how to create nested pipelines in Kubeflow. It defines several component functions (square, add, square_root) and two pipeline functions (square_and_sum, pythagorean). The square_and_sum pipeline is used as a component within the pythagorean pipeline, showcasing the ability to compose complex workflows.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/compose-components-into-pipelines.md#2025-04-10_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\n\n@dsl.component\ndef square(x: float) -> float:\n    return x ** 2\n\n@dsl.component\ndef add(x: float, y: float) -> float:\n    return x + y\n\n@dsl.component\ndef square_root(x: float) -> float:\n    return x ** .5\n\n@dsl.pipeline\ndef square_and_sum(a: float, b: float) -> float:\n    a_sq_task = square(x=a)\n    b_sq_task = square(x=b)\n    return add(x=a_sq_task.output, y=b_sq_task.output).output\n\n@dsl.pipeline\ndef pythagorean(a: float = 1.2, b: float = 1.2) -> float:\n    sq_and_sum_task = square_and_sum(a=a, b=b)\n    return square_root(x=sq_and_sum_task.output).output\n```\n\n----------------------------------------\n\nTITLE: Implementing XGBoost Training Pipeline in Python using Kubeflow\nDESCRIPTION: A complete pipeline implementation for distributed XGBoost model training using Kubeflow Pipelines. The pipeline includes steps for cluster creation, data analysis, transformation, model training, prediction, and evaluation metrics generation. It uses Dataproc for processing and includes confusion matrix and ROC curve generation.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/introduction.md#2025-04-10_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@dsl.pipeline(\n    name='XGBoost Trainer',\n    description='A trainer that does end-to-end distributed training for XGBoost models.'\n)\ndef xgb_train_pipeline(\n    output='gs://your-gcs-bucket',\n    project='your-gcp-project',\n    cluster_name='xgb-%s' % dsl.RUN_ID_PLACEHOLDER,\n    region='us-central1',\n    train_data='gs://ml-pipeline-playground/sfpd/train.csv',\n    eval_data='gs://ml-pipeline-playground/sfpd/eval.csv',\n    schema='gs://ml-pipeline-playground/sfpd/schema.json',\n    target='resolution',\n    rounds=200,\n    workers=2,\n    true_label='ACTION',\n):\n    output_template = str(output) + '/' + dsl.RUN_ID_PLACEHOLDER + '/data'\n\n    # Current GCP pyspark/spark op do not provide outputs as return values, instead,\n    # we need to use strings to pass the uri around.\n    analyze_output = output_template\n    transform_output_train = os.path.join(output_template, 'train', 'part-*')\n    transform_output_eval = os.path.join(output_template, 'eval', 'part-*')\n    train_output = os.path.join(output_template, 'train_output')\n    predict_output = os.path.join(output_template, 'predict_output')\n\n    with dsl.ExitHandler(exit_op=dataproc_delete_cluster_op(\n        project_id=project,\n        region=region,\n        name=cluster_name\n    )):\n        _create_cluster_op = dataproc_create_cluster_op(\n            project_id=project,\n            region=region,\n            name=cluster_name,\n            initialization_actions=[\n              os.path.join(_PYSRC_PREFIX,\n                           'initialization_actions.sh'),\n            ],\n            image_version='1.2'\n        )\n\n        _analyze_op = dataproc_analyze_op(\n            project=project,\n            region=region,\n            cluster_name=cluster_name,\n            schema=schema,\n            train_data=train_data,\n            output=output_template\n        ).after(_create_cluster_op).set_display_name('Analyzer')\n\n        _transform_op = dataproc_transform_op(\n            project=project,\n            region=region,\n            cluster_name=cluster_name,\n            train_data=train_data,\n            eval_data=eval_data,\n            target=target,\n            analysis=analyze_output,\n            output=output_template\n        ).after(_analyze_op).set_display_name('Transformer')\n\n        _train_op = dataproc_train_op(\n            project=project,\n            region=region,\n            cluster_name=cluster_name,\n            train_data=transform_output_train,\n            eval_data=transform_output_eval,\n            target=target,\n            analysis=analyze_output,\n            workers=workers,\n            rounds=rounds,\n            output=train_output\n        ).after(_transform_op).set_display_name('Trainer')\n\n        _predict_op = dataproc_predict_op(\n            project=project,\n            region=region,\n            cluster_name=cluster_name,\n            data=transform_output_eval,\n            model=train_output,\n            target=target,\n            analysis=analyze_output,\n            output=predict_output\n        ).after(_train_op).set_display_name('Predictor')\n\n        _cm_op = confusion_matrix_op(\n            predictions=os.path.join(predict_output, 'part-*.csv'),\n            output_dir=output_template\n        ).after(_predict_op)\n\n        _roc_op = roc_op(\n            predictions_dir=os.path.join(predict_output, 'part-*.csv'),\n            true_class=true_label,\n            true_score_column=true_label,\n            output_dir=output_template\n        ).after(_predict_op)\n\n    dsl.get_pipeline_conf().add_op_transformer(\n        gcp.use_gcp_secret('user-gcp-sa'))\n```\n\n----------------------------------------\n\nTITLE: Implementing PyTorch Distributed Training with Kubeflow Training Operator\nDESCRIPTION: This code demonstrates how to create a distributed PyTorch training job using Kubeflow Training Operator. It includes a complete training function implementing a CNN model for FashionMNIST with PyTorch Distributed Data Parallel and code to initiate a multi-node PyTorchJob with 3 workers and 1 GPU per worker.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/getting-started.md#2025-04-10_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef train_func():\n    import torch\n    import torch.nn.functional as F\n    from torch.utils.data import DistributedSampler\n    from torchvision import datasets, transforms\n    import torch.distributed as dist\n\n    # [1] Setup PyTorch DDP. Distributed environment will be set automatically by Training Operator.\n    dist.init_process_group(backend=\"nccl\")\n    Distributor = torch.nn.parallel.DistributedDataParallel\n    local_rank = int(os.getenv(\"LOCAL_RANK\", 0))\n    print(\n        \"Distributed Training for WORLD_SIZE: {}, RANK: {}, LOCAL_RANK: {}\".format(\n            dist.get_world_size(),\n            dist.get_rank(),\n            local_rank,\n        )\n    )\n\n    # [2] Create PyTorch CNN Model.\n    class Net(torch.nn.Module):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.conv1 = torch.nn.Conv2d(1, 20, 5, 1)\n            self.conv2 = torch.nn.Conv2d(20, 50, 5, 1)\n            self.fc1 = torch.nn.Linear(4 * 4 * 50, 500)\n            self.fc2 = torch.nn.Linear(500, 10)\n\n        def forward(self, x):\n            x = F.relu(self.conv1(x))\n            x = F.max_pool2d(x, 2, 2)\n            x = F.relu(self.conv2(x))\n            x = F.max_pool2d(x, 2, 2)\n            x = x.view(-1, 4 * 4 * 50)\n            x = F.relu(self.fc1(x))\n            x = self.fc2(x)\n            return F.log_softmax(x, dim=1)\n\n    # [3] Attach model to the correct GPU device and distributor.\n    device = torch.device(f\"cuda:{local_rank}\")\n    model = Net().to(device)\n    model = Distributor(model)\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n\n    # [4] Setup FashionMNIST dataloader and distribute data across PyTorchJob workers.\n    dataset = datasets.FashionMNIST(\n        \"./data\",\n        download=True,\n        train=True,\n        transform=transforms.Compose([transforms.ToTensor()]),\n    )\n    train_loader = torch.utils.data.DataLoader(\n        dataset=dataset,\n        batch_size=128,\n        sampler=DistributedSampler(dataset),\n    )\n\n    # [5] Start model Training.\n    for epoch in range(3):\n        for batch_idx, (data, target) in enumerate(train_loader):\n            # Attach Tensors to the device.\n            data = data.to(device)\n            target = target.to(device)\n\n            optimizer.zero_grad()\n            output = model(data)\n            loss = F.nll_loss(output, target)\n            loss.backward()\n            optimizer.step()\n            if batch_idx % 10 == 0 and dist.get_rank() == 0:\n                print(\n                    \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tloss={:.4f}\".format(\n                        epoch,\n                        batch_idx * len(data),\n                        len(train_loader.dataset),\n                        100.0 * batch_idx / len(train_loader),\n                        loss.item(),\n                    )\n                )\n\n\nfrom kubeflow.training import TrainingClient\n\n# Start PyTorchJob with 3 Workers and 1 GPU per Worker (e.g. multi-node, multi-worker job).\nTrainingClient().create_job(\n    name=\"pytorch-ddp\",\n    train_func=train_func,\n    num_procs_per_worker=\"auto\",\n    num_workers=3,\n    resources_per_worker={\"gpu\": \"1\"},\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Basic Kubeflow Pipeline\nDESCRIPTION: Creates a simple pipeline with a single component that prints and returns a greeting message. Uses type annotations and KFP decorators to define the component and pipeline structure.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/getting-started.md#2025-04-10_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\n\n@dsl.component\ndef say_hello(name: str) -> str:\n    hello_text = f'Hello, {name}!'\n    print(hello_text)\n    return hello_text\n\n@dsl.pipeline\ndef hello_pipeline(recipient: str) -> str:\n    hello_task = say_hello(name=recipient)\n    return hello_task.output\n```\n\n----------------------------------------\n\nTITLE: Defining a Basic Lightweight Python Component in KFP\nDESCRIPTION: A simple Python component that adds two integers and returns the result, using the KFP @dsl.component decorator for a lightweight component.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/containerized-python-components.md#2025-04-10_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\n\n@dsl.component\ndef add(a: int, b: int) -> int:\n    return a + b\n```\n\n----------------------------------------\n\nTITLE: Deploying Kubeflow Pipelines on Kubernetes\nDESCRIPTION: This snippet demonstrates how to deploy Kubeflow Pipelines on a Kubernetes cluster using kustomize. It applies the necessary cluster-scoped resources and environment-specific configurations, and waits for the CRD establishment.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/standalone-deployment.md#2025-04-10_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport PIPELINE_VERSION={{% pipelines/latest-version %}}\nkubectl apply -k \"github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref=$PIPELINE_VERSION\"\nkubectl wait --for condition=established --timeout=60s crd/applications.app.k8s.io\nkubectl apply -k \"github.com/kubeflow/pipelines/manifests/kustomize/env/dev?ref=$PIPELINE_VERSION\"\n```\n\n----------------------------------------\n\nTITLE: Basic Hyperparameter Tuning with Katib Python SDK\nDESCRIPTION: A complete example demonstrating how to use Katib Python SDK for hyperparameter tuning. It defines an objective function, creates a search space, runs an experiment with 12 trials, waits for completion, and retrieves the optimal parameters.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/getting-started.md#2025-04-10_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# [1] Create an objective function.\ndef objective(parameters):\n    # Import required packages.\n    import time\n    time.sleep(5)\n    # Calculate objective function.\n    result = 4 * int(parameters[\"a\"]) - float(parameters[\"b\"]) ** 2\n    # Katib parses metrics in this format: <metric-name>=<metric-value>.\n    print(f\"result={result}\")\n\nimport kubeflow.katib as katib\n\n# [2] Create hyperparameter search space.\nparameters = {\n    \"a\": katib.search.int(min=10, max=20),\n    \"b\": katib.search.double(min=0.1, max=0.2)\n}\n\n# [3] Create Katib Experiment with 12 Trials and 2 CPUs per Trial.\nkatib_client = katib.KatibClient(namespace=\"kubeflow\")\n\nname = \"tune-experiment\"\nkatib_client.tune(\n    name=name,\n    objective=objective,\n    parameters=parameters,\n    objective_metric_name=\"result\",\n    max_trial_count=12,\n    resources_per_trial={\"cpu\": \"2\"},\n)\n\n# [4] Wait until Katib Experiment is complete\nkatib_client.wait_for_experiment_condition(name=name)\n\n# [5] Get the best hyperparameters.\nprint(katib_client.get_optimal_hyperparameters(name))\n```\n\n----------------------------------------\n\nTITLE: Fine-tuning BERT Model with Kubeflow Training API\nDESCRIPTION: This snippet demonstrates how to use the Kubeflow Training Client API to fine-tune a BERT model with the Yelp Review dataset from HuggingFace Hub. It configures model parameters, dataset selection, training arguments including LoRA configuration for parameter-efficient fine-tuning, and distributed worker setup.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/fine-tuning.md#2025-04-10_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport transformers\nfrom peft import LoraConfig\n\nfrom kubeflow.training import TrainingClient\nfrom kubeflow.storage_initializer.hugging_face import (\n    HuggingFaceModelParams,\n    HuggingFaceTrainerParams,\n    HuggingFaceDatasetParams,\n)\n\nTrainingClient().train(\n    name=\"fine-tune-bert\",\n    # BERT model URI and type of Transformer to train it.\n    model_provider_parameters=HuggingFaceModelParams(\n        model_uri=\"hf://google-bert/bert-base-cased\",\n        transformer_type=transformers.AutoModelForSequenceClassification,\n    ),\n    # Use 3000 samples from Yelp dataset.\n    dataset_provider_parameters=HuggingFaceDatasetParams(\n        repo_id=\"yelp_review_full\",\n        split=\"train[:3000]\",\n    ),\n    # Specify HuggingFace Trainer parameters. In this example, we will skip evaluation and model checkpoints.\n    trainer_parameters=HuggingFaceTrainerParams(\n        training_parameters=transformers.TrainingArguments(\n            output_dir=\"test_trainer\",\n            save_strategy=\"no\",\n            evaluation_strategy=\"no\",\n            do_eval=False,\n            disable_tqdm=True,\n            log_level=\"info\",\n        ),\n        # Set LoRA config to reduce number of trainable model parameters.\n        lora_config=LoraConfig(\n            r=8,\n            lora_alpha=8,\n            lora_dropout=0.1,\n            bias=\"none\",\n        ),\n    ),\n    num_workers=4, # nnodes parameter for torchrun command.\n    num_procs_per_worker=2, # nproc-per-node parameter for torchrun command.\n    resources_per_worker={\n        \"gpu\": 2,\n        \"cpu\": 5,\n        \"memory\": \"10G\",\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Compiling Kubeflow Pipeline to YAML\nDESCRIPTION: Demonstrates how to compile the defined pipeline into a YAML file using the KFP SDK Compiler. The resulting YAML contains the complete pipeline specification.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/getting-started.md#2025-04-10_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import compiler\n\ncompiler.Compiler().compile(hello_pipeline, 'pipeline.yaml')\n```\n\n----------------------------------------\n\nTITLE: Compiling Component to YAML with KFP SDK\nDESCRIPTION: Demonstrates how to compile an individual component to IR YAML format, since components can be treated as pipelines in Kubeflow.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/compile-a-pipeline.md#2025-04-10_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@dsl.component\ndef comp(message: str) -> str:\n    print(message)\n    return message\n\ncompiler.Compiler().compile(comp, package_path='component.yaml')\n```\n\n----------------------------------------\n\nTITLE: Installing KFP SDK\nDESCRIPTION: Command to install the Kubeflow Pipelines SDK version 2 using pip package manager.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/getting-started.md#2025-04-10_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install kfp\n```\n\n----------------------------------------\n\nTITLE: Implementing PyTorch Distributed Training Function\nDESCRIPTION: Complete implementation of a distributed training function using PyTorch DDP. Includes model definition, data loading, and training loop configuration for distributed environment.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/getting-started.md#2025-04-10_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef train_pytorch():\n    import os\n\n    import torch\n    from torch import nn\n    import torch.nn.functional as F\n\n    from torchvision import datasets, transforms\n    import torch.distributed as dist\n    from torch.utils.data import DataLoader, DistributedSampler\n\n    device, backend = (\"cuda\", \"nccl\") if torch.cuda.is_available() else (\"cpu\", \"gloo\")\n    dist.init_process_group(backend=backend)\n\n    local_rank = int(os.getenv(\"LOCAL_RANK\", 0))\n    print(\n        \"Distributed Training with WORLD_SIZE: {}, RANK: {}, LOCAL_RANK: {}.\".format(\n            dist.get_world_size(),\n            dist.get_rank(),\n            local_rank,\n        )\n    )\n\n    class Net(nn.Module):\n        def __init__(self):\n            super(Net, self).__init__()\n            self.conv1 = nn.Conv2d(1, 20, 5, 1)\n            self.conv2 = nn.Conv2d(20, 50, 5, 1)\n            self.fc1 = nn.Linear(4 * 4 * 50, 500)\n            self.fc2 = nn.Linear(500, 10)\n\n        def forward(self, x):\n            x = F.relu(self.conv1(x))\n            x = F.max_pool2d(x, 2, 2)\n            x = F.relu(self.conv2(x))\n            x = F.max_pool2d(x, 2, 2)\n            x = x.view(-1, 4 * 4 * 50)\n            x = F.relu(self.fc1(x))\n            x = self.fc2(x)\n            return F.log_softmax(x, dim=1)\n\n    device = torch.device(f\"{device}:{local_rank}\")\n    model = nn.parallel.DistributedDataParallel(Net().to(device))\n    model.train()\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n\n    dataset = datasets.FashionMNIST(\n        \"./data\",\n        train=True,\n        download=True,\n        transform=transforms.Compose([transforms.ToTensor()]),\n    )\n    train_loader = DataLoader(\n        dataset,\n        batch_size=100,\n        sampler=DistributedSampler(dataset),\n    )\n\n    for epoch in range(3):\n        for batch_idx, (inputs, labels) in enumerate(train_loader):\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = F.nll_loss(outputs, labels)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            if batch_idx % 10 == 0 and dist.get_rank() == 0:\n                print(\n                    \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n                        epoch,\n                        batch_idx * len(inputs),\n                        len(train_loader.dataset),\n                        100.0 * batch_idx / len(train_loader),\n                        loss.item(),\n                    )\n                )\n\n    dist.barrier()\n    if dist.get_rank() == 0:\n        print(\"Training is finished\")\n    dist.destroy_process_group()\n```\n\n----------------------------------------\n\nTITLE: Using a Container Component in a Pipeline\nDESCRIPTION: Demonstrates how to use a container component within a Kubeflow pipeline and compile it to YAML format using the KFP compiler.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/container-components.md#2025-04-10_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\nfrom kfp import compiler\n\n@dsl.pipeline\ndef hello_pipeline():\n    say_hello()\n\ncompiler.Compiler().compile(hello_pipeline, 'pipeline.yaml')\n```\n\n----------------------------------------\n\nTITLE: Optimizing Llama-3.2 Hyperparameters for Binary Classification using Katib\nDESCRIPTION: This comprehensive example demonstrates how to set up and execute a hyperparameter optimization experiment for the Llama-3.2 model on the IMDB dataset using Kubeflow's Katib. It includes model, dataset, and trainer configurations, as well as Katib experiment settings.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/llm-hp-optimization.md#2025-04-10_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport kubeflow.katib as katib\nfrom kubeflow.katib import KatibClient\n\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments\nfrom peft import LoraConfig\n\nfrom kubeflow.storage_initializer.hugging_face import (\n\tHuggingFaceModelParams,\n\tHuggingFaceDatasetParams,\n\tHuggingFaceTrainerParams,\n)\n\nhf_model = HuggingFaceModelParams(\n    model_uri = \"hf://meta-llama/Llama-3.2-1B\",\n    transformer_type = AutoModelForSequenceClassification,\n)\n\n# Train the model on 1000 movie reviews from imdb\n# https://huggingface.co/datasets/stanfordnlp/imdb\nhf_dataset = HuggingFaceDatasetParams(\n    repo_id = \"imdb\",\n    split = \"train[:1000]\",\n)\n\nhf_tuning_parameters = HuggingFaceTrainerParams(\n    training_parameters = TrainingArguments(\n        output_dir = \"results\",\n        save_strategy = \"no\",\n        learning_rate = katib.search.double(min=1e-05, max=5e-05),\n        num_train_epochs=3,\n    ),\n    # Set LoRA config to reduce number of trainable model parameters.\n    lora_config = LoraConfig(\n        r = katib.search.int(min=8, max=32),\n        lora_alpha = 8,\n        lora_dropout = 0.1,\n        bias = \"none\",\n    ),\n)\n\ncl = KatibClient(namespace=\"kubeflow\")\n\n# Optimizing Hyperparameters for Binary Classification\nexp_name = \"llama\"\ncl.tune(\n\tname = exp_name,\n\tmodel_provider_parameters = hf_model,\n\tdataset_provider_parameters = hf_dataset,\n\ttrainer_parameters = hf_tuning_parameters,\n\tobjective_metric_name = \"train_loss\",\n\tobjective_type = \"minimize\",\n\talgorithm_name = \"random\",\n\tmax_trial_count = 10,\n\tparallel_trial_count = 2,\n\tresources_per_trial={\n\t\t\"gpu\": \"2\",\n\t\t\"cpu\": \"4\",\n\t\t\"memory\": \"10G\",\n\t},\n)\n\ncl.wait_for_experiment_condition(name=exp_name)\n\n# Get the best hyperparameters.\nprint(cl.get_optimal_hyperparameters(exp_name))\n```\n\n----------------------------------------\n\nTITLE: Using Pipeline Decorator Arguments in KFP v2\nDESCRIPTION: This example shows how to use optional arguments in the @dsl.pipeline decorator to provide metadata for a pipeline, including name, description, storage location, and display name.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/compose-components-into-pipelines.md#2025-04-10_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@dsl.pipeline(name='pythagorean-theorem-pipeline',\n              description='Solve for the length of a hypotenuse of a triangle with sides length `a` and `b`.',\n              pipeline_root='gs://my-pipelines-bucket',\n              display_name='Pythagorean pipeline.')\ndef pythagorean(a: float, b: float) -> float:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Running Pipeline from YAML Package using KFP Client\nDESCRIPTION: Demonstrates how to submit a pipeline run from a compiled IR YAML file using the KFP client's create_run_from_pipeline_package method. Includes example of passing runtime arguments.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/run-a-pipeline.md#2025-04-10_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nkfp_client.create_run_from_pipeline_package(\n    \"./add-pipeline.yaml\", \n    arguments={\n        \"a\": 1,\n        \"b\": 2,\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Using dsl.If, dsl.Elif, and dsl.Else for Multiple Conditions in Kubeflow Pipelines\nDESCRIPTION: This example shows how to use dsl.If, dsl.Elif, and dsl.Else together to handle multiple conditional branches based on the output of a three-sided coin flip.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/control-flow.md#2025-04-10_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\n\n@dsl.component\ndef flip_three_sided_coin() -> str:\n    import random\n    return random.choice(['heads', 'tails', 'draw'])\n\n@dsl.component\ndef print_comp(text: str):\n    print(text)\n\n@dsl.pipeline\ndef my_pipeline():\n    coin_flip_task = flip_three_sided_coin()\n    with dsl.If(coin_flip_task.output == 'heads'):\n        print_comp(text='Got heads!')\n    with dsl.Elif(coin_flip_task.output == 'tails'):\n        print_comp(text='Got tails!')\n    with dsl.Else():\n        print_comp(text='Draw!')\n```\n\n----------------------------------------\n\nTITLE: Handling File I/O in Python with Kubeflow Pipelines SDK\nDESCRIPTION: This function demonstrates how to accept a file as input and return two files as outputs using the Kubeflow Pipelines SDK. It uses InputPath and OutputPath annotations to specify file parameters.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/python-function-components.ipynb#2025-04-10_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef split_text_lines(\n    source_path: comp.InputPath(str),\n    odd_lines_path: comp.OutputPath(str),\n    even_lines_path: comp.OutputPath(str)):\n    \"\"\"Splits a text file into two files, with even lines going to one file\n    and odd lines to the other.\"\"\"\n\n    with open(source_path, 'r') as reader:\n        with open(odd_lines_path, 'w') as odd_writer:\n            with open(even_lines_path, 'w') as even_writer:\n                while True:\n                    line = reader.readline()\n                    if line == \"\":\n                        break\n                    odd_writer.write(line)\n                    line = reader.readline()\n                    if line == \"\":\n                        break\n                    even_writer.write(line)\n```\n\n----------------------------------------\n\nTITLE: Implementing Conditional Execution with dsl.If in Kubeflow Pipelines\nDESCRIPTION: This snippet demonstrates how to use dsl.If to conditionally execute tasks based on the output of an upstream task. The conditional_task only executes if the coin_flip_task returns 'heads'.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/control-flow.md#2025-04-10_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\n\n@dsl.component\ndef flip_coin() -> str:\n    import random\n    return random.choice(['heads', 'tails'])\n\n#@dsl.component\n#def my_comp():\n#    print('Conditional task executed!')\n\n@dsl.pipeline\ndef my_pipeline():\n    coin_flip_task = flip_coin()\n    with dsl.If(coin_flip_task.output == 'heads'):\n        conditional_task = my_comp()\n```\n\n----------------------------------------\n\nTITLE: Creating a Pipeline with Components in Python using KFP v2\nDESCRIPTION: This code demonstrates how to create a complete pipeline implementing the Pythagorean theorem by composing three component functions (square, add, and square_root). Components are defined with @dsl.component decorators and composed into a pipeline with the @dsl.pipeline decorator.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/compose-components-into-pipelines.md#2025-04-10_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\n\n@dsl.component\ndef square(x: float) -> float:\n    return x ** 2\n\n@dsl.component\ndef add(x: float, y: float) -> float:\n    return x + y\n\n@dsl.component\ndef square_root(x: float) -> float:\n    return x ** .5\n\n@dsl.pipeline\ndef pythagorean(a: float, b: float) -> float:\n    a_sq_task = square(x=a)\n    b_sq_task = square(x=b)\n    sum_task = add(x=a_sq_task.output, y=b_sq_task.output)\n    return square_root(x=sum_task.output).output\n```\n\n----------------------------------------\n\nTITLE: Creating Kubeflow Pipeline Component Factory\nDESCRIPTION: Creates a reusable component from the merge_csv function with specified base image and package dependencies.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/build-pipeline.ipynb#2025-04-10_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncreate_step_merge_csv = kfp.components.create_component_from_func(\n    func=merge_csv,\n    output_component_file='component.yaml', # This is optional. It saves the component spec for future use.\n    base_image='python:3.7',\n    packages_to_install=['pandas==1.1.4'])\n```\n\n----------------------------------------\n\nTITLE: Executing KFP Components and Pipelines Locally with DockerRunner\nDESCRIPTION: This snippet demonstrates how to initialize a local session using DockerRunner and execute both individual components and pipelines. It shows how to access task outputs and assert results.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/execute-kfp-pipelines-locally.md#2025-04-10_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import local\nfrom kfp import dsl\n\nlocal.init(runner=local.DockerRunner())\n\n@dsl.component\ndef add(a: int, b: int) -> int:\n    return a + b\n\n# run a single component\ntask = add(a=1, b=2)\nassert task.output == 3\n\n# or run it in a pipeline\n@dsl.pipeline\ndef math_pipeline(x: int, y: int, z: int) -> int:\n    t1 = add(a=x, b=y)\n    t2 = add(a=t1.output, b=z)\n    return t2.output\n\npipeline_task = math_pipeline(x=1, y=2, z=3)\nassert pipeline_task.output == 6\n```\n\n----------------------------------------\n\nTITLE: Loading Component from File in Kubeflow Pipeline\nDESCRIPTION: Demonstrates how to load a component from a YAML file and use it in a pipeline definition using the KFP components module.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/load-and-share-components.md#2025-04-10_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import components\n\nloaded_comp = components.load_component_from_file('component.yaml')\n\n@dsl.pipeline\ndef my_pipeline():\n    loaded_comp()\n```\n\n----------------------------------------\n\nTITLE: Accessing Pipeline Status Metadata with PipelineTaskFinalStatus in Kubeflow Pipelines\nDESCRIPTION: This example demonstrates how to use dsl.PipelineTaskFinalStatus to obtain information about pipeline and task execution status, even after component failure. The exit_op component receives pipeline status metadata automatically from the backend at runtime, allowing access to state, resource name, task name, error codes and messages.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/control-flow.md#2025-04-10_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\nfrom kfp.dsl import PipelineTaskFinalStatus\n\n@dsl.component\ndef print_op(message: str):\n    print(message)\n\n@dsl.component\ndef exit_op(user_input: str, status: PipelineTaskFinalStatus):\n    \"\"\"Prints pipeline run status.\"\"\"\n    print(user_input)\n    print('Pipeline status: ', status.state)\n    print('Job resource name: ', status.pipeline_job_resource_name)\n    print('Pipeline task name: ', status.pipeline_task_name)\n    print('Error code: ', status.error_code)\n    print('Error message: ', status.error_message)\n\n@dsl.component\ndef fail_op():\n    import sys\n    sys.exit(1)\n\n@dsl.pipeline\ndef my_pipeline():\n    print_op(message='Starting pipeline...')\n    print_status_task = exit_op(user_input='Task execution status:')\n    with dsl.ExitHandler(exit_task=print_status_task):\n        fail_op()\n```\n\n----------------------------------------\n\nTITLE: Creating Pipeline Run using KFP CLI\nDESCRIPTION: Demonstrates how to submit a pipeline run using the KFP CLI, specifying the experiment name and pipeline package file.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/run-a-pipeline.md#2025-04-10_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nkfp run create \\\n  --experiment-name \"my-experiment\" \\\n  --package-file \"./path/to/pipeline.yaml\"\n```\n\n----------------------------------------\n\nTITLE: Creating a Pipeline with Artifact Inputs and Outputs in Python\nDESCRIPTION: Shows how to create a pipeline that consumes and produces artifacts. This pipeline takes a Dataset as input, passes it through an augmentation component, then to a training component, and finally returns the Model output.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/data-handling/artifacts.md#2025-04-10_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\nfrom kfp.dsl import Dataset, Model\n\n@dsl.pipeline\ndef augment_and_train(dataset: Dataset) -> Model:\n    augment_task = augment_dataset(dataset=dataset)\n    return train_model(dataset=augment_task.output).output\n```\n\n----------------------------------------\n\nTITLE: Running Pipeline from Python Function using KFP Client\nDESCRIPTION: Shows how to directly submit a pipeline function for execution using the create_run_from_pipeline_func method, which handles both compilation and submission.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/run-a-pipeline.md#2025-04-10_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nkfp_client.create_run_from_pipeline_func(\n    add_pipeline,\n    arguments={\n        \"a\": 1,\n        \"b\": 2,\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Implementing Kubeflow Pipeline Component for CSV Merging\nDESCRIPTION: Modified version of the merge function to work as a Kubeflow Pipeline component using InputPath and OutputPath decorators.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/build-pipeline.ipynb#2025-04-10_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef merge_csv(file_path: comp.InputPath('Tarball'),\n              output_csv: comp.OutputPath('CSV')):\n  import glob\n  import pandas as pd\n  import tarfile\n\n  tarfile.open(name=file_path, mode=\"r|gz\").extractall('data')\n  df = pd.concat(\n      [pd.read_csv(csv_file, header=None) \n       for csv_file in glob.glob('data/*.csv')])\n  df.to_csv(output_csv, index=False, header=False)\n```\n\n----------------------------------------\n\nTITLE: Creating KFP Components with Various Parameter Types\nDESCRIPTION: Illustrates how to define components and a pipeline that accept various parameter types including strings, integers, floats, booleans, dictionaries, and arrays. The example includes both a Python Component and a Container Component.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/data-handling/parameters.md#2025-04-10_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Dict, List\nfrom kfp import dsl\n\n@dsl.component\ndef python_comp(\n    string: str = 'hello',\n    integer: int = 1,\n    floating_pt: float = 0.1,\n    boolean: bool = True,\n    dictionary: Dict = {'key': 'value'},\n    array: List = [1, 2, 3],\n):\n    print(string)\n    print(integer)\n    print(floating_pt)\n    print(boolean)\n    print(dictionary)\n    print(array)\n\n\n@dsl.container_component\ndef container_comp(\n    string: str = 'hello',\n    integer: int = 1,\n    floating_pt: float = 0.1,\n    boolean: bool = True,\n    dictionary: Dict = {'key': 'value'},\n    array: List = [1, 2, 3],\n):\n    return dsl.ContainerSpec(\n        image='alpine',\n        command=['sh', '-c', \"\"\"echo $0 $1 $2 $3 $4 $5 $6\"\"\"],\n        args=[\n            string,\n            integer,\n            floating_pt,\n            boolean,\n            dictionary,\n            array,\n        ])\n\n@dsl.pipeline\ndef my_pipeline(\n    string: str = 'Hey!',\n    integer: int = 100,\n    floating_pt: float = 0.1,\n    boolean: bool = False,\n    dictionary: Dict = {'key': 'value'},\n    array: List = [1, 2, 3],\n):\n    python_comp(\n        string='howdy',\n        integer=integer,\n        array=[4, 5, 6],\n    )\n    container_comp(\n        string=string,\n        integer=20,\n        dictionary={'other key': 'other val'},\n        boolean=boolean,\n    )\n```\n\n----------------------------------------\n\nTITLE: Compiling Kubeflow Pipeline\nDESCRIPTION: Compiles the pipeline function into a YAML file that can be uploaded to Kubeflow Pipelines.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/build-pipeline.ipynb#2025-04-10_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nkfp.compiler.Compiler().compile(\n    pipeline_func=my_pipeline,\n    package_path='pipeline.yaml')\n```\n\n----------------------------------------\n\nTITLE: Disabling Caching for a Pipeline Component in Python\nDESCRIPTION: This snippet demonstrates how to disable caching for a specific component in a Kubeflow Pipeline by using the set_caching_options() method with False parameter on a task object.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/caching.md#2025-04-10_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\n\n@dsl.component\ndef say_hello(name: str) -> str:\n    hello_text = f'Hello, {name}!'\n    print(hello_text)\n    return hello_text\n\n@dsl.pipeline\ndef hello_pipeline(recipient: str = 'World!') -> str:\n    hello_task = say_hello(name=recipient)\n    hello_task.set_caching_options(False)\n    return hello_task.output\n```\n\n----------------------------------------\n\nTITLE: Defining Output Parameters in KFP Container Components\nDESCRIPTION: Demonstrates how to define output parameters in Container Components using dsl.OutputPath. This component writes an integer value to a file path that KFP provides, which is then accessed in the pipeline.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/data-handling/parameters.md#2025-04-10_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\n\n@dsl.container_component\ndef my_comp(int_path: dsl.OutputPath(int)):\n    return dsl.ContainerSpec(\n        image='alpine',\n        command=[\n            'sh', '-c', f\"\"\"mkdir -p $(dirname {int_path})\\\n                            && echo 1 > {int_path}\"\"\"\n        ])\n\n@dsl.pipeline\ndef my_pipeline() -> int:\n    task = my_comp()\n    return task.outputs['int_path']\n```\n\n----------------------------------------\n\nTITLE: Training Model with Traditional Artifact Syntax in Python\nDESCRIPTION: Illustrates a component that trains a model using an input dataset and outputs a model artifact with metadata. The component demonstrates reading from an input artifact and writing to an output artifact.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/data-handling/artifacts.md#2025-04-10_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\nfrom kfp.dsl import Dataset, Input, Model, Output\n\n@dsl.component\ndef train_model(dataset: Input[Dataset], model: Output[Model]):\n    with open(dataset.path) as f:\n        dataset_lines = f.readlines()\n\n    # train a model\n    trained_model = ...\n    \n    trained_model.save(model.path)\n    model.metadata['samples'] = len(dataset_lines)\n```\n\n----------------------------------------\n\nTITLE: Defining Kubeflow Pipeline Function\nDESCRIPTION: Defines a pipeline that combines web downloading and CSV merging components into a complete workflow.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/build-pipeline.ipynb#2025-04-10_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ndef my_pipeline(url):\n  web_downloader_task = web_downloader_op(url=url)\n  merge_csv_task = create_step_merge_csv(file=web_downloader_task.outputs['data'])\n```\n\n----------------------------------------\n\nTITLE: Defining Calculation Pipeline with Multiple Operations\nDESCRIPTION: Creates a Kubeflow pipeline that chains multiple operations including addition and division, demonstrating how to pass parameters and handle operation outputs.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/python-function-components.ipynb#2025-04-10_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport kfp.dsl as dsl\n@dsl.pipeline(\n   name='Calculation pipeline',\n   description='An example pipeline that performs arithmetic calculations.'\n)\ndef calc_pipeline(\n   a='1',\n   b='7',\n   c='17',\n):\n    # Passes a pipeline parameter and a constant value as operation arguments.\n    add_task = add_op(a, 4) # The add_op factory function returns\n                            # a dsl.ContainerOp class instance. \n\n    # Passes the output of the add_task and a pipeline parameter as operation\n    # arguments. For an operation with a single return value, the output\n    # reference is accessed using `task.output` or\n    # `task.outputs['output_name']`.\n    divmod_task = divmod_op(add_task.output, b)\n\n    # For an operation with multiple return values, output references are\n    # accessed as `task.outputs['output_name']`.\n    result_task = add_op(divmod_task.outputs['quotient'], c)\n```\n\n----------------------------------------\n\nTITLE: Defining Kubeflow Pipeline with Parameters in Python\nDESCRIPTION: Demonstrates how to define a Kubeflow pipeline function with typed parameters using the @kfp.dsl.pipeline decorator. Shows parameter definition with default values for integer, string, and URL inputs that can be configured through the Kubeflow Pipelines UI.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/parameters.md#2025-04-10_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n@kfp.dsl.pipeline(\n  name='My pipeline',\n  description='My machine learning pipeline'\n)\ndef my_pipeline(\n    my_num: int = 1000, \n    my_name: str = 'some text', \n    my_url: str = 'http://example.com'\n):\n  ...\n  # In the pipeline function body you can use the `my_num`, `my_name`, \n  # `my_url` arguments as PipelineParam objects.\n```\n\n----------------------------------------\n\nTITLE: Defining Output Parameters in KFP Python Components\nDESCRIPTION: Shows how to define output parameters in Python Components and pipelines using return type annotations. The component returns an integer value which is then accessed in the pipeline.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/data-handling/parameters.md#2025-04-10_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\n\n@dsl.component\ndef my_comp() -> int:\n    return 1\n\n@dsl.pipeline\ndef my_pipeline() -> int:\n    task = my_comp()\n    return task.output\n```\n\n----------------------------------------\n\nTITLE: Using Components with Outputs in a Pipeline\nDESCRIPTION: Shows how to use a component with output parameters in a pipeline, including accessing the component's outputs and returning them from the pipeline.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/container-components.md#2025-04-10_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\nfrom kfp import compiler\n\n@dsl.pipeline\ndef hello_pipeline(person_to_greet: str) -> str:\n    # greeting argument is provided automatically at runtime!\n    hello_task = say_hello(name=person_to_greet)\n    return hello_task.outputs['greeting']\n\ncompiler.Compiler().compile(hello_pipeline, 'pipeline.yaml')\n```\n\n----------------------------------------\n\nTITLE: Ignoring Upstream Task Failures in Kubeflow Pipelines\nDESCRIPTION: This example shows how to use the .ignore_upstream_failure() method to ensure a task executes regardless of upstream task failures. The cleanup_op component requires default values for all inputs to ensure it can run even when upstream tasks fail to produce outputs, making it useful for implementing cleanup operations.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/control-flow.md#2025-04-10_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\n\n@dsl.component\ndef cleanup_op(message: str = 'Cleaning up...'):\n    print(message)\n\n@dsl.component\ndef fail_op(message: str):\n    print(message)\n    raise ValueError('Task failed!')\n\n@dsl.pipeline()\ndef my_pipeline(text: str = 'message'):\n    fail_task = fail_op(message=text)\n    clean_up_task = cleanup_op(\n        message=fail_task.output\n    ).ignore_upstream_failure()\n```\n\n----------------------------------------\n\nTITLE: Implementing Exit Handling with dsl.ExitHandler in Kubeflow Pipelines\nDESCRIPTION: This snippet demonstrates how to use dsl.ExitHandler to specify a cleanup task that runs after the main pipeline tasks finish execution, regardless of their success or failure. The cleanup task is executed after both create_dataset and train_and_save_models finish.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/control-flow.md#2025-04-10_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\nfrom kfp.dsl import Dataset\n\n#@dsl.component\n#def clean_up_resources():\n#    ...\n\n#@dsl.component\n#def create_datasets():\n#    ...\n\n#@dsl.component\n#def train_and_save_models(dataset: Dataset):\n#    ...\n\n@dsl.pipeline\ndef my_pipeline():\n    clean_up_task = clean_up_resources()\n    with dsl.ExitHandler(exit_task=clean_up_task):\n        dataset_task = create_datasets()\n        train_task = train_and_save_models(dataset=dataset_task.output)\n```\n\n----------------------------------------\n\nTITLE: Adding Input Parameters to a Container Component\nDESCRIPTION: Enhances the container component to accept a string input parameter and use it in the echo command through string formatting.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/container-components.md#2025-04-10_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\n\n@dsl.container_component\ndef say_hello(name: str):\n    return dsl.ContainerSpec(image='alpine', command=['echo'], args=[f'Hello, {name}!'])\n```\n\n----------------------------------------\n\nTITLE: Creating Basic Integer Addition Component\nDESCRIPTION: Simple example of a Lightweight Python Component that adds two integers using the @dsl.component decorator. Shows basic type annotation usage.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/lightweight-python-components.md#2025-04-10_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\n\n@dsl.component\ndef add(a: int, b: int) -> int:\n    return a + b\n```\n\n----------------------------------------\n\nTITLE: Installing Kubeflow Pipelines SDK\nDESCRIPTION: Command to install the Kubeflow Pipelines SDK version 1.8 using pip. This installs the necessary tools for building and managing machine learning pipelines.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/install-sdk.md#2025-04-10_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install kfp==1.8\n```\n\n----------------------------------------\n\nTITLE: Requesting GPU Resources for Spark Driver and Executor\nDESCRIPTION: This YAML snippet demonstrates how to request GPU resources for Spark driver and executor pods, specifying the GPU resource name and quantity. Requires the mutating admission webhook to be enabled.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/writing-sparkapplication.md#2025-04-10_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  driver:\n    cores: 0.1\n    coreLimit: \"200m\"\n    memory: \"512m\"\n    gpu:\n      name: \"amd.com/gpu\"   # GPU resource name\n      quantity: 1           # number of GPUs to request\n    labels:\n      version: 3.1.1\n    serviceAccount: spark\n  executor:\n    cores: 1\n    instances: 1\n    memory: \"512m\"\n    serviceAccount: spark\n    gpu:\n      name: \"nvidia.com/gpu\"\n      quantity: 1\n```\n\n----------------------------------------\n\nTITLE: Using dsl.Collected with dsl.ParallelFor in Kubeflow Pipelines\nDESCRIPTION: This snippet demonstrates how to use dsl.Collected to gather outputs from a parallel loop of tasks. The example trains models with different epochs and then finds the model with the highest accuracy.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/control-flow.md#2025-04-10_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\nfrom kfp.dsl import Model, Input\n\n#def score_model(model: Model) -> float:\n#    return ...\n\n#@dsl.component\n#def train_model(epochs: int) -> Model:\n#    ...\n\n@dsl.component\ndef max_accuracy(models: Input[List[Model]]) -> float:\n    return max(score_model(model) for model in models)\n\n@dsl.pipeline\ndef my_pipeline():\n    \n    # Train a model for 1, 5, 10, and 25 epochs\n    with dsl.ParallelFor(\n        items=[1, 5, 10, 25],\n    ) as epochs:\n        train_model_task = train_model(epochs=epochs)\n        \n    # Find the model with the highest accuracy\n    max_accuracy(\n        models=dsl.Collected(train_model_task.outputs['model'])\n    )\n```\n\n----------------------------------------\n\nTITLE: Training Model with Pythonic Artifact Syntax in Python\nDESCRIPTION: Shows a component using the Pythonic artifact syntax to create and return a model artifact. This component reads a dataset, trains a model, and returns a new Model artifact instance with metadata.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/data-handling/artifacts.md#2025-04-10_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\nfrom kfp.dsl import Dataset, Model\n\n@dsl.component\ndef train_model(dataset: Dataset) -> Model:\n    with open(dataset.path) as f:\n        dataset_lines = f.readlines()\n\n    # train a model\n    trained_model = ...\n\n    model_artifact = Model(uri=dsl.get_uri(), metadata={'samples': len(dataset_lines)})\n    trained_model.save(model_artifact.path)\n    \n    return model_artifact\n```\n\n----------------------------------------\n\nTITLE: Loading and Using Components in Python Pipeline\nDESCRIPTION: Python code demonstrating how to load component specifications from text, create pipeline steps, and define a pipeline using the components.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/component-development.md#2025-04-10_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport kfp\nimport kfp.components as comp\n\ncreate_step_get_lines = comp.load_component_from_text(\"\"\"\nname: Get Lines\ndescription: Gets the specified number of lines from the input file.\n\ninputs:\n- {name: Input 1, type: Data, description: 'Data for input 1'}\n- {name: Parameter 1, type: Integer, default: '100', description: 'Number of lines to copy'}\n\noutputs:\n- {name: Output 1, type: Data, description: 'Output 1 data.'}\n\nimplementation:\n  container:\n    image: gcr.io/my-org/my-image@sha256:a172..752f\n    command: [\n      python3, \n      /pipelines/component/src/program.py,\n      --input1-path,\n      {inputPath: Input 1},\n      --param1, \n      {inputValue: Parameter 1},\n      --output1-path, \n      {outputPath: Output 1},\n    ]\"\"\")\n\ncreate_step_write_lines = comp.load_component_from_text(\"\"\"\nname: Write Lines\ndescription: Writes text to a file.\n\ninputs:\n- {name: text, type: String}\n\noutputs:\n- {name: data, type: Data}\n\nimplementation:\n  container:\n    image: busybox\n    command:\n    - sh\n    - -c\n    - |\n      mkdir -p \"$(dirname \"$1\")\"\n      echo \"$0\" > \"$1\"\n    args:\n    - {inputValue: text}\n    - {outputPath: data}\n\"\"\")\n\ndef my_pipeline():\n    write_lines_step = create_step_write_lines(\n        text='one\\ntwo\\nthree\\nfour\\nfive\\nsix\\nseven\\neight\\nnine\\nten')\n\n    get_lines_step = create_step_get_lines(\n        input_1=write_lines_step.outputs['data'],\n        parameter_1='5',\n    )\n\nclient = kfp.Client(host='<your-kubeflow-pipelines-host-name>')\n```\n\n----------------------------------------\n\nTITLE: Returning dsl.Collected from a Kubeflow Pipeline\nDESCRIPTION: This example shows how to return a dsl.Collected object from a pipeline, using a List of artifacts in the return annotation. The pipeline trains models with different epochs and returns a collection of the resulting models.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/control-flow.md#2025-04-10_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import List\n\nfrom kfp import dsl\nfrom kfp.dsl import Model\n\n#@dsl.component\n#def train_model(epochs: int) -> Model:\n#    ...\n\n@dsl.pipeline\ndef my_pipeline() -> List[Model]:\n    with dsl.ParallelFor(\n        items=[1, 5, 10, 25],\n    ) as epochs:\n        train_model_task = train_model(epochs=epochs)\n    return dsl.Collected(train_model_task.outputs['model'])\n```\n\n----------------------------------------\n\nTITLE: Using DSL Importer Component in Kubeflow Pipeline\nDESCRIPTION: Demonstrates how to create a pipeline that imports an external artifact using dsl.importer. The example shows importing a dataset from Google Cloud Storage, setting metadata with dynamic values from upstream tasks, and passing the imported artifact to another component.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/importer-component.md#2025-04-10_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\n\n@dsl.pipeline\ndef my_pipeline():\n    task = get_date_string()\n    importer_task = dsl.importer(\n        artifact_uri='gs://ml-pipeline-playground/shakespeare1.txt',\n        artifact_class=dsl.Dataset,\n        reimport=True,\n        metadata={'date': task.output})\n    other_component(dataset=importer_task.output)\n```\n\n----------------------------------------\n\nTITLE: Running a Pipeline with Python Function Components in Kubeflow Pipelines\nDESCRIPTION: Executes the defined pipeline by creating a run using the Kubeflow Pipelines client. Specifies argument values for the pipeline parameters.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/python-function-components.ipynb#2025-04-10_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Specify argument values for your pipeline run.\narguments = {'a': '7', 'b': '8'}\n\n# Create a pipeline run, using the client you initialized in a prior step.\nclient.create_run_from_pipeline_func(add_pipeline, arguments=arguments)\n```\n\n----------------------------------------\n\nTITLE: Creating Kubeflow Pipeline Component from Function\nDESCRIPTION: Converts the Python function into a Kubeflow Pipelines component using the component factory, specifying TensorFlow as the base image.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/python-function-components.ipynb#2025-04-10_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ndivmod_op = comp.create_component_from_func(\n    my_divmod, base_image='tensorflow/tensorflow:1.11.0-py3')\n```\n\n----------------------------------------\n\nTITLE: Installing Latest Katib SDK from Source\nDESCRIPTION: Command to install the latest development version of Katib Python SDK from the master branch.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/installation.md#2025-04-10_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npip install git+https://github.com/kubeflow/katib.git@master#subdirectory=sdk/python/v1beta1\n```\n\n----------------------------------------\n\nTITLE: Implementing Parallel Loops with dsl.ParallelFor in Kubeflow Pipelines\nDESCRIPTION: This example shows how to use dsl.ParallelFor to execute tasks in parallel over a static set of items. The pipeline trains models with different numbers of epochs, with a maximum of two training tasks running concurrently.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/control-flow.md#2025-04-10_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\n\n#@dsl.component\n#def train_model(epochs: int) -> Model:\n#    ...\n\n@dsl.pipeline\ndef my_pipeline():\n    with dsl.ParallelFor(\n        items=[1, 5, 10, 25],\n        parallelism=2\n    ) as epochs:\n        train_model(epochs=epochs)\n```\n\n----------------------------------------\n\nTITLE: Defining a Pipeline Using Python Function Components in Kubeflow Pipelines\nDESCRIPTION: Creates a pipeline that uses the previously defined add_op component. The pipeline takes two parameters and performs two addition operations.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/python-function-components.ipynb#2025-04-10_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport kfp.dsl as dsl\n@dsl.pipeline(\n  name='Addition pipeline',\n  description='An example pipeline that performs addition calculations.'\n)\ndef add_pipeline(\n  a='1',\n  b='7',\n):\n  # Passes a pipeline parameter and a constant value to the `add_op` factory\n  # function.\n  first_add_task = add_op(a, 4)\n  # Passes an output reference from `first_add_task` and a pipeline parameter\n  # to the `add_op` factory function. For operations with a single return\n  # value, the output reference can be accessed as `task.output` or\n  # `task.outputs['output_name']`.\n  second_add_task = add_op(first_add_task.output, b)\n```\n\n----------------------------------------\n\nTITLE: Returning Multiple Outputs in Python with Kubeflow Pipelines SDK\nDESCRIPTION: This function demonstrates how to return multiple values, including component metadata and metrics, using the Kubeflow Pipelines SDK. It uses NamedTuple for type hinting and namedtuple for returning multiple outputs.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/python-function-components.ipynb#2025-04-10_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import NamedTuple\ndef multiple_return_values_example(a: float, b: float) -> NamedTuple(\n  'ExampleOutputs',\n  [\n    ('sum', float),\n    ('product', float),\n    ('mlpipeline_ui_metadata', 'UI_metadata'),\n    ('mlpipeline_metrics', 'Metrics')\n  ]):\n  \"\"\"Example function that demonstrates how to return multiple values.\"\"\"  \n  sum_value = a + b\n  product_value = a * b\n\n  # Export a sample tensorboard\n  metadata = {\n    'outputs' : [{\n      'type': 'tensorboard',\n      'source': 'gs://ml-pipeline-dataset/tensorboard-train',\n    }]\n  }\n\n  # Export two metrics\n  metrics = {\n    'metrics': [\n      {\n        'name': 'sum',\n        'numberValue':  float(sum_value),\n      },{\n        'name': 'product',\n        'numberValue':  float(product_value),\n      }\n    ]  \n  }\n\n  from collections import namedtuple\n  example_output = namedtuple(\n      'ExampleOutputs',\n      ['sum', 'product', 'mlpipeline_ui_metadata', 'mlpipeline_metrics'])\n  return example_output(sum_value, product_value, metadata, metrics)\n```\n\n----------------------------------------\n\nTITLE: Creating a Component from Python Function in Kubeflow Pipelines\nDESCRIPTION: Uses the create_component_from_func function to generate a component specification YAML and return a factory function for creating ContainerOp instances.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/python-function-components.ipynb#2025-04-10_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nadd_op = create_component_from_func(\n    add, output_component_file='add_component.yaml')\n```\n\n----------------------------------------\n\nTITLE: Gathering Outputs from Mutually Exclusive Branches with dsl.OneOf in Kubeflow Pipelines\nDESCRIPTION: This snippet demonstrates how to use dsl.OneOf to gather outputs from mutually exclusive conditional branches into a single task output, which can then be consumed by downstream tasks or returned from the pipeline.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/control-flow.md#2025-04-10_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\n\n@dsl.component\ndef flip_three_sided_coin() -> str:\n    import random\n    return random.choice(['heads', 'tails', 'draw'])\n\n@dsl.component\ndef print_and_return(text: str) -> str:\n    print(text)\n    return text\n\n@dsl.component\ndef announce_result(result: str):\n    print(f'The result is: {result}')\n\n@dsl.pipeline\ndef my_pipeline() -> str:\n    coin_flip_task = flip_three_sided_coin()\n    with dsl.If(coin_flip_task.output == 'heads'):\n        t1 = print_and_return(text='Got heads!')\n    with dsl.Elif(coin_flip_task.output == 'tails'):\n        t2 = print_and_return(text='Got tails!')\n    with dsl.Else():\n        t3 = print_and_return(text='Draw!')\n    \n    oneof = dsl.OneOf(t1.output, t2.output, t3.output)\n    announce_result(oneof)\n    return oneof\n```\n\n----------------------------------------\n\nTITLE: Accessing Artifact Properties in Python Component\nDESCRIPTION: Shows how to access the properties of an input artifact including name, URI, path, and metadata. This component reads a dataset file and prints information about the artifact.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/data-handling/artifacts.md#2025-04-10_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\nfrom kfp.dsl import Dataset\nfrom kfp.dsl import Input\n\n@dsl.component\ndef print_artifact_properties(dataset: Input[Dataset]):\n    with open(dataset.path) as f:\n        lines = f.readlines()\n    \n    print('Information about the artifact')\n    print('Name:', dataset.name)\n    print('URI:', dataset.uri)\n    print('Path:', dataset.path)\n    print('Metadata:', dataset.metadata)\n    \n    return len(lines)\n```\n\n----------------------------------------\n\nTITLE: Implementing MNIST Classification using TensorFlow in Kubeflow Notebooks\nDESCRIPTION: This code implements a simple neural network to classify MNIST handwritten digits using TensorFlow. It loads the MNIST dataset, defines a softmax regression model, trains the model using gradient descent, and evaluates the model accuracy. The example is adapted from the official TensorFlow tutorials.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/notebooks/jupyter-tensorflow-examples.md#2025-04-10_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n\nimport tensorflow as tf\n\nx = tf.placeholder(tf.float32, [None, 784])\n\nW = tf.Variable(tf.zeros([784, 10]))\nb = tf.Variable(tf.zeros([10]))\n\ny = tf.nn.softmax(tf.matmul(x, W) + b)\n\ny_ = tf.placeholder(tf.float32, [None, 10])\ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n\ntrain_step = tf.train.GradientDescentOptimizer(0.05).minimize(cross_entropy)\n\nsess = tf.InteractiveSession()\ntf.global_variables_initializer().run()\n\nfor _ in range(1000):\n  batch_xs, batch_ys = mnist.train.next_batch(100)\n  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n\ncorrect_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nprint(\"Accuracy: \", sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n```\n\n----------------------------------------\n\nTITLE: Implementing Recursive Function with Condition in Kubeflow Pipelines DSL (Python)\nDESCRIPTION: Demonstrates how to create a recursive function using the @dsl.graph_component decorator. The function includes a conditional statement and calls itself recursively. It also shows how to use this function within a pipeline definition.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/dsl-recursion.md#2025-04-10_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport kfp.dsl as dsl\n@dsl.graph_component\ndef graph_component_a(input_x):\n  with dsl.Condition(input_x == 'value_x'):\n    op_a = task_factory_a(input_x)\n    op_b = task_factory_b().after(op_a)\n    graph_component_a(op_b.output)\n    \n@dsl.pipeline(\n  name='pipeline',\n  description='shows how to use the recursion.'\n)\ndef pipeline():\n  op_a = task_factory_a()\n  op_b = task_factory_b()\n  graph_op_a = graph_component_a(op_a.output)\n  graph_op_a.after(op_b)\n  task_factory_c(op_a.output).after(graph_op_a)\n```\n\n----------------------------------------\n\nTITLE: Loading Component from URL in Kubeflow Pipeline\nDESCRIPTION: Shows how to load a component directly from a GitHub URL or any other accessible URL source.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/load-and-share-components.md#2025-04-10_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nloaded_comp = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/2.0.0/sdk/python/test_data/components/add_numbers.yaml')\n```\n\n----------------------------------------\n\nTITLE: Configuring Metrics Strategies in a Katib Experiment\nDESCRIPTION: This YAML snippet demonstrates how to define metric strategies in a Katib experiment. It shows setting an objective metric to 'accuracy' with a maximize type, and configuring the metric strategy to use the latest reported value.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/hp-tuning/configure-experiment.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n  . . .\n  objectiveMetricName: accuracy\n  type: maximize\n  metricStrategies:\n    - name: accuracy\n      value: latest\n  . . .\n```\n\n----------------------------------------\n\nTITLE: Training Component with Traditional Artifact Syntax in Python\nDESCRIPTION: Demonstrates a component that accepts an input Dataset artifact and outputs a Model artifact. The component reads from the dataset path, trains a model, and saves it with metadata using the traditional artifact syntax.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/data-handling/artifacts.md#2025-04-10_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp.dsl import Input, Output, Dataset, Model\n\n@dsl.component\ndef training_component(dataset: Input[Dataset], model: Output[Model]):\n    \"\"\"Trains an output Model on an input Dataset.\"\"\"\n    with open(dataset.path) as f:\n        contents = f.read()\n\n    # ... train tf_model model on contents of dataset ...\n\n    tf_model.save(model.path)\n    model.metadata['framework'] = 'tensorflow'\n```\n\n----------------------------------------\n\nTITLE: Implementing Division Component with Multiple Outputs in Python\nDESCRIPTION: Defines a Python function that performs division operations using numpy, returns quotient and remainder along with visualization metadata and metrics. The function demonstrates handling multiple outputs using NamedTuple and includes TensorBoard integration.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/python-function-components.ipynb#2025-04-10_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import NamedTuple\n\ndef my_divmod(\n  dividend: float,\n  divisor: float) -> NamedTuple(\n    'MyDivmodOutput',\n    [\n      ('quotient', float),\n      ('remainder', float),\n      ('mlpipeline_ui_metadata', 'UI_metadata'),\n      ('mlpipeline_metrics', 'Metrics')\n    ]):\n    '''Divides two numbers and calculate  the quotient and remainder'''\n\n    # Import the numpy package inside the component function\n    import numpy as np\n\n    # Define a helper function\n    def divmod_helper(dividend, divisor):\n        return np.divmod(dividend, divisor)\n\n    (quotient, remainder) = divmod_helper(dividend, divisor)\n\n    from tensorflow.python.lib.io import file_io\n    import json\n\n    # Export a sample tensorboard\n    metadata = {\n      'outputs' : [{\n        'type': 'tensorboard',\n        'source': 'gs://ml-pipeline-dataset/tensorboard-train',\n      }]\n    }\n\n    # Export two metrics\n    metrics = {\n      'metrics': [{\n          'name': 'quotient',\n          'numberValue':  float(quotient),\n        },{\n          'name': 'remainder',\n          'numberValue':  float(remainder),\n        }]}\n\n    from collections import namedtuple\n    divmod_output = namedtuple('MyDivmodOutput',\n        ['quotient', 'remainder', 'mlpipeline_ui_metadata',\n         'mlpipeline_metrics'])\n    return divmod_output(quotient, remainder, json.dumps(metadata),\n                         json.dumps(metrics))\n```\n\n----------------------------------------\n\nTITLE: Defining a SparkApplication in YAML for Kubernetes\nDESCRIPTION: This YAML snippet demonstrates the basic structure of a SparkApplication custom resource. It includes essential fields such as apiVersion, kind, metadata, and spec. The spec section defines the application type, deployment mode, container image, main class, and the location of the application JAR file.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/using-sparkapplication.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: sparkoperator.k8s.io/v1beta2\nkind: SparkApplication\nmetadata:\n  name: spark-pi\n  namespace: default\nspec:\n  type: Scala\n  mode: cluster\n  image: spark:3.5.1\n  mainClass: org.apache.spark.examples.SparkPi\n  mainApplicationFile: local:///opt/spark/examples/jars/spark-examples_2.12-3.5.1.jar\n```\n\n----------------------------------------\n\nTITLE: Registering Model Metadata\nDESCRIPTION: Example of registering a scikit-learn model with metadata including accuracy and license information\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/model-registry/getting-started.md#2025-04-10_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nrm = registry.register_model(\n    \"iris\",\n    \"gs://kfserving-examples/models/sklearn/1.0/model\",\n    model_format_name=\"sklearn\",\n    model_format_version=\"1\",\n    version=\"v1\",\n    description=\"Iris scikit-learn model\",\n    metadata={\n        \"accuracy\": 3.14,\n        \"license\": \"BSD 3-Clause License\",\n    }\n)\n```\n\n----------------------------------------\n\nTITLE: Executing Pipeline with Arguments\nDESCRIPTION: Demonstrates how to submit a pipeline run with specific argument values using the Kubeflow Pipelines client.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/python-function-components.ipynb#2025-04-10_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Specify pipeline argument values\narguments = {'a': '7', 'b': '8'}\n\n# Submit a pipeline run\nclient.create_run_from_pipeline_func(calc_pipeline, arguments=arguments)\n```\n\n----------------------------------------\n\nTITLE: Training Multiple Models with Pythonic Artifact Syntax in Python\nDESCRIPTION: Demonstrates a component that outputs multiple model artifacts using Pythonic syntax. The component creates and returns two model artifacts with different URIs and paths using a NamedTuple for multiple outputs.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/data-handling/artifacts.md#2025-04-10_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\nfrom kfp.dsl import Dataset, Model\nfrom typing import NamedTuple\n\n@dsl.component\ndef train_multiple_models(\n    dataset: Dataset,\n) -> NamedTuple('outputs', model1=Model, model2=Model):\n    with open(dataset.path) as f:\n        dataset_lines = f.readlines()\n\n    # train a model\n    trained_model1 = ...\n    trained_model2 = ...\n    \n    model_artifact1 = Model(uri=dsl.get_uri(suffix='model1'), metadata={'samples': len(dataset_lines)})\n    trained_model1.save(model_artifact1.path)\n    \n    model_artifact2 = Model(uri=dsl.get_uri(suffix='model2'), metadata={'samples': len(dataset_lines)})\n    trained_model2.save(model_artifact2.path)\n    \n    outputs = NamedTuple('outputs', model1=Model, model2=Model)\n    return outputs(model1=model_artifact1, model2=model_artifact2)\n```\n\n----------------------------------------\n\nTITLE: Defining HuggingFace Trainer Parameters with Katib Search Space\nDESCRIPTION: Configures training and LoRA parameters with hyperparameter search spaces using Katib's search API. This example defines search ranges for learning rate and LoRA rank parameter, enabling hyperparameter optimization during fine-tuning.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/llm-hp-optimization.md#2025-04-10_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport kubeflow.katib as katib\nfrom kubeflow.storage_initializer.hugging_face import HuggingFaceTrainerParams\n\nfrom transformers import TrainingArguments\nfrom peft import LoraConfig\n\n# Set up training and LoRA configuration\ntrainer_params = HuggingFaceTrainerParams(\n    training_parameters=TrainingArguments(\n        output_dir=\"results\",\n        # Using katib search api to define a search space for the parameter\n        learning_rate=katib.search.double(min=1e-05, max=5e-05),\n        num_train_epochs=3,\n        per_device_train_batch_size=8,\n    ),\n    lora_config=LoraConfig(\n        r=katib.search.int(min=8, max=32),\n        lora_alpha=16,\n        lora_dropout=0.1,\n        bias=\"none\",\n    ),\n)\n```\n\n----------------------------------------\n\nTITLE: Using IfPresentPlaceholder for Conditional Arguments\nDESCRIPTION: Shows how to use dsl.IfPresentPlaceholder to conditionally include command-line arguments based on whether an input parameter is provided.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/container-components.md#2025-04-10_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@dsl.container_component\ndef hello_someone(optional_name: str = None):\n    return dsl.ContainerSpec(\n        image='python:3.7',\n        command=[\n            'say_hello',\n            dsl.IfPresentPlaceholder(\n                input_name='optional_name', then=['--name', optional_name])\n        ])\n```\n\n----------------------------------------\n\nTITLE: Defining NAS Configuration in Katib Experiment YAML\nDESCRIPTION: Example structure of nasConfig in a Katib Experiment YAML file for neural architecture search. It includes graphConfig for defining the neural network structure and operations for specifying the range of operations to tune.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/nas/configure-experiment.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nnasConfig:\n  graphConfig:\n    numLayers:\n    inputSizes:\n    outputSizes:\n  operations:\n    - operationType:\n      parameters:\n        - name:\n          parameterType:\n          feasibleSpace:\n```\n\n----------------------------------------\n\nTITLE: Initializing Kubeflow Pipelines Client in Python\nDESCRIPTION: Creates an instance of the kfp.Client class to interact with the Kubeflow Pipelines API. Requires proper configuration for connecting to the Kubeflow Pipelines instance.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/python-function-components.ipynb#2025-04-10_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclient = kfp.Client() # change arguments accordingly\n```\n\n----------------------------------------\n\nTITLE: Importing Key Kubeflow Pipelines SDK Packages\nDESCRIPTION: Core SDK packages for working with Kubeflow Pipelines, including compiler, components, DSL, and client functionality\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/sdk-overview.md#2025-04-10_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp.compiler import Compiler\nfrom kfp.components import func_to_container_op, load_component_from_file, load_component_from_url\nfrom kfp.dsl import ContainerOp, PipelineParam, pipeline, python_component\nfrom kfp import Client\n```\n\n----------------------------------------\n\nTITLE: Using IfPresentPlaceholder with Default Values\nDESCRIPTION: Demonstrates using dsl.IfPresentPlaceholder with the else_ parameter to provide default values when an input parameter is not provided.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/container-components.md#2025-04-10_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n@dsl.container_component\ndef hello_someone(optional_name: str = None):\n    return dsl.ContainerSpec(\n        image='python:3.7',\n        command=[\n            'say_hello',\n            dsl.IfPresentPlaceholder(\n                input_name='optional_name',\n                then=['--name', optional_name],\n                else_=['--name', 'friend'])\n        ])\n```\n\n----------------------------------------\n\nTITLE: Configuring HuggingFace Dataset Parameters\nDESCRIPTION: Sets up parameters for loading a dataset from HuggingFace, specifying the repository ID, dataset split, and optional access token. This example loads the IMDB dataset's training split.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/llm-hp-optimization.md#2025-04-10_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom kubeflow.storage_initializer.hugging_face import HuggingFaceDatasetParams\n\n\ndataset_params = HuggingFaceDatasetParams(\n    repo_id=\"imdb\",            # Public dataset repository ID on Hugging Face\n    split=\"train\",             # Dataset split to load\n    access_token=None          # Not needed for public datasets\n)\n```\n\n----------------------------------------\n\nTITLE: Creating a Simple Container Component\nDESCRIPTION: Defines a basic container component using the dsl.container_component decorator that returns a ContainerSpec object. This component runs an echo command in an Alpine container.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/container-components.md#2025-04-10_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\n\n@dsl.container_component\ndef say_hello():\n    return dsl.ContainerSpec(image='alpine', command=['echo'], args=['Hello'])\n```\n\n----------------------------------------\n\nTITLE: Importing HuggingFace Components in Kubeflow\nDESCRIPTION: Imports necessary classes for HuggingFace integration with Kubeflow, including model, dataset, and trainer parameter classes. These components enable seamless integration of HuggingFace models and datasets for training and evaluation.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/llm-hp-optimization.md#2025-04-10_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kubeflow.storage_initializer.hugging_face import (\n    HuggingFaceModelParams,\n    HuggingFaceDatasetParams,\n    HuggingFaceTrainerParams,\n)\n```\n\n----------------------------------------\n\nTITLE: Specifying Container Images for the Component\nDESCRIPTION: Modified component decorator that specifies the base image and target image for the containerized component, enabling KFP to build and use a custom container for the component.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/containerized-python-components.md#2025-04-10_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@dsl.component(base_image='python:3.11',\n               target_image='gcr.io/my-project/my-component:v1')\ndef add(a: int, b: int) -> int:\n    from math_utils import add_numbers\n    return add_numbers(a, b)\n```\n\n----------------------------------------\n\nTITLE: Using ConcatPlaceholder for String Concatenation\nDESCRIPTION: Demonstrates how to use dsl.ConcatPlaceholder to concatenate strings without space separators in container commands or arguments.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/container-components.md#2025-04-10_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\n\n@dsl.container_component\ndef concatenator(prefix: str, suffix: str):\n    return dsl.ContainerSpec(\n        image='alpine',\n        command=[\n            'my_program.sh'\n        ],\n        args=['--input', dsl.ConcatPlaceholder([prefix, suffix, '.txt'])]\n    )\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding Katib UI Service in Kubernetes\nDESCRIPTION: Command to port-forward the Katib UI service to access it locally on port 8080. This enables standalone access to Katib UI without requiring the Kubeflow Central Dashboard.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/katib-ui.md#2025-04-10_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nkubectl port-forward svc/katib-ui -n kubeflow 8080:80\n```\n\n----------------------------------------\n\nTITLE: Defining a Component with Parameter Types in KFP\nDESCRIPTION: Demonstrates how to create a KFP component with input parameters and return type annotation. This component takes a word string and count integer (with default value 10) and returns a string with the word repeated count times.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/data-handling/parameters.md#2025-04-10_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\n\n@dsl.component\ndef join_words(word: str, count: int = 10) -> str:\n    return ' '.join(word for _ in range(count))\n```\n\n----------------------------------------\n\nTITLE: Compiling Pipeline with Parameters\nDESCRIPTION: Example showing how to compile a pipeline with JSON parameters\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/cli.md#2025-04-10_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nkfp dsl compile --py path/to/pipeline.py --output path/to/output.yaml --pipeline-parameters '{\"param1\": 2.0, \"param2\": \"my_val\"}'\n```\n\n----------------------------------------\n\nTITLE: Creating a Helper Module for Math Utilities\nDESCRIPTION: A Python module containing a utility function for adding numbers, which will be imported by the main component. This demonstrates the ability to use external modules in Containerized Components.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/containerized-python-components.md#2025-04-10_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# src/math_utils.py\ndef add_numbers(num1, num2):\n    return num1 + num2\n```\n\n----------------------------------------\n\nTITLE: Integrating TensorBoard Visualization in Kubeflow Pipelines\nDESCRIPTION: This code adds a TensorBoard visualization to the pipeline output. It creates a Start TensorBoard button that allows users to launch and interact with a TensorBoard instance pointing to the specified log directory.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/output-viewer.md#2025-04-10_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\ndef tensorboard_vis(mlpipeline_ui_metadata_path: kfp.components.OutputPath()):\n  import json\n\n  metadata = {\n    'outputs' : [{\n      'type': 'tensorboard',\n      'source': args.job_dir,\n    }]\n  }\n\n  with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:\n    json.dump(metadata, metadata_file)\n```\n\n----------------------------------------\n\nTITLE: Complete Component Specification in YAML\nDESCRIPTION: Full YAML configuration for a component, including inputs, outputs, and implementation details such as container image and command.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/component-development.md#2025-04-10_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\ninputs:\n- {name: Input 1, type: String, description: 'Data for input 1'}\n- {name: Parameter 1, type: Integer, default: '100', description: 'Number of lines to copy'}\n\noutputs:\n- {name: Output 1, type: String, description: 'Output 1 data.'}\n\nimplementation:\n  container:\n    image: gcr.io/my-org/my-image@sha256:a172..752f\n    command: [\n      python3, \n      /pipelines/component/src/program.py,\n      --input1-path,\n      {inputPath: Input 1},\n      --param1, \n      {inputValue: Parameter 1},\n      --output1-path, \n      {outputPath: Output 1},\n    ]\n```\n\n----------------------------------------\n\nTITLE: Using VolumeOp and PipelineVolume in Kubeflow Pipeline Steps\nDESCRIPTION: This example shows how to create a VolumeOp for PVC creation and use PipelineVolumes to mount volumes in ContainerOp steps. It demonstrates volume referencing and dependency expression through volume mounting.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/manipulate-resources.md#2025-04-10_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nvop = dsl.VolumeOp(\n    name=\"volume_creation\",\n    resource_name=\"mypvc\",\n    size=\"1Gi\"\n)\nstep1 = dsl.ContainerOp(\n    name=\"step1\",\n    ...\n    pvolumes={\"/mnt\": vop.volume}  # Implies execution after vop\n)\nstep2 = dsl.ContainerOp(\n    name=\"step2\",\n    ...\n    pvolumes={\"/data\": step1.pvolume,  # Implies execution after step1\n              \"/mnt\": dsl.PipelineVolume(pvc=\"existing-pvc\")}\n)\nstep3 = dsl.ContainerOp(\n    name=\"step3\",\n    ...\n    pvolumes={\"/common\": step2.pvolumes[\"/mnt\"]}  # Implies execution after step2\n)\n```\n\n----------------------------------------\n\nTITLE: Initializing KFP Client in Python\nDESCRIPTION: Creates a KFP client instance for interacting with the Kubeflow Pipelines API. Authentication may be required depending on the KFP instance configuration.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/run-a-pipeline.md#2025-04-10_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport kfp\n\n# TIP: you may need to authenticate with the KFP instance\nkfp_client = kfp.Client()\n```\n\n----------------------------------------\n\nTITLE: Verifying Katib Installation\nDESCRIPTION: Command to check the status of Katib control plane pods in the kubeflow namespace.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/installation.md#2025-04-10_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get pods -n kubeflow\n```\n\n----------------------------------------\n\nTITLE: Working with Multiple Named Output Parameters in KFP\nDESCRIPTION: Shows how to specify and access multiple named output parameters using typing.NamedTuple. The component returns two values (an integer and a string) which are then accessed in the pipeline and returned as pipeline outputs.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/data-handling/parameters.md#2025-04-10_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\nfrom typing import NamedTuple\n\n@dsl.component\ndef my_comp() -> NamedTuple('outputs', a=int, b=str):\n    outputs = NamedTuple('outputs', a=int, b=str)\n    return outputs(1, 'hello')\n\n@dsl.pipeline\ndef my_pipeline() -> NamedTuple('pipeline_outputs', c=int, d=str):\n    task = my_comp()\n    pipeline_outputs = NamedTuple('pipeline_outputs', c=int, d=str)\n    return pipeline_outputs(task.outputs['a'], task.outputs['b'])\n```\n\n----------------------------------------\n\nTITLE: Checking KFP CLI Version\nDESCRIPTION: Command to verify the installation and check the version of KFP CLI\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/cli.md#2025-04-10_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkfp --version\n```\n\n----------------------------------------\n\nTITLE: Connecting to Standalone KFP Inside the Cluster (Different Namespace)\nDESCRIPTION: Python code for connecting to Kubeflow Pipelines when running in a different Kubernetes namespace, using the fully qualified internal service DNS.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/connect-api.md#2025-04-10_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport kfp\n\n# the namespace in which you deployed Kubeflow Pipelines\nnamespace = \"kubeflow\" \n\nclient = kfp.Client(host=f\"http://ml-pipeline-ui.{namespace}\")\n\nprint(client.list_experiments())\n```\n\n----------------------------------------\n\nTITLE: Python Implementation for Text Line Extractor Component\nDESCRIPTION: A sample Python program that reads a specified number of lines from an input file and writes them to an output file. The program demonstrates how to handle file paths as inputs and outputs in a pipeline component.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/component-development.md#2025-04-10_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n#!/usr/bin/env python3\nimport argparse\nfrom pathlib import Path\n\n# Function doing the actual work (Outputs first N lines from a text file)\ndef do_work(input1_file, output1_file, param1):\n  for x, line in enumerate(input1_file):\n    if x >= param1:\n      break\n    _ = output1_file.write(line)\n  \n# Defining and parsing the command-line arguments\nparser = argparse.ArgumentParser(description='My program description')\n# Paths must be passed in, not hardcoded\nparser.add_argument('--input1-path', type=str,\n  help='Path of the local file containing the Input 1 data.')\nparser.add_argument('--output1-path', type=str,\n  help='Path of the local file where the Output 1 data should be written.')\nparser.add_argument('--param1', type=int, default=100,\n  help='The number of lines to read from the input and write to the output.')\nargs = parser.parse_args()\n\n# Creating the directory where the output file is created (the directory\n# may or may not exist).\nPath(args.output1_path).parent.mkdir(parents=True, exist_ok=True)\n\nwith open(args.input1_path, 'r') as input1_file:\n    with open(args.output1_path, 'w') as output1_file:\n        do_work(input1_file, output1_file, args.param1)\n```\n\n----------------------------------------\n\nTITLE: Initializing Model Registry Client\nDESCRIPTION: Creates a Model Registry client instance with server configuration and authentication details\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/model-registry/getting-started.md#2025-04-10_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom model_registry import ModelRegistry\n\nregistry = ModelRegistry(\n    server_address=\"http://model-registry-service.kubeflow.svc.cluster.local\",\n    port=8080,\n    author=\"your name\",\n    is_secure=False\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring ScheduledSparkApplication in YAML for Kubernetes\nDESCRIPTION: This YAML snippet defines a ScheduledSparkApplication resource for running a Spark Pi example job every 5 minutes. It specifies the schedule, concurrency policy, history limits, and the SparkApplication template including driver and executor configurations.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/running-sparkapplication-on-schedule.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"sparkoperator.k8s.io/v1beta2\"\nkind: ScheduledSparkApplication\nmetadata:\n  name: spark-pi-scheduled\n  namespace: default\nspec:\n  schedule: \"@every 5m\"\n  concurrencyPolicy: Allow\n  successfulRunHistoryLimit: 1\n  failedRunHistoryLimit: 3\n  template:\n    type: Scala\n    mode: cluster\n    image: gcr.io/spark/spark:v3.1.1\n    mainClass: org.apache.spark.examples.SparkPi\n    mainApplicationFile: local:///opt/spark/examples/jars/spark-examples_2.12-3.1.1.jar\n    driver:\n      cores: 1\n      memory: 512m\n    executor:\n      cores: 1\n      instances: 1\n      memory: 512m\n    restartPolicy:\n      type: Never\n```\n\n----------------------------------------\n\nTITLE: Initializing Kubeflow Pipeline Client\nDESCRIPTION: Creates a Kubeflow Pipelines API client instance for programmatic pipeline execution.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/build-pipeline.ipynb#2025-04-10_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclient = kfp.Client() # change arguments accordingly\n```\n\n----------------------------------------\n\nTITLE: Configuring Prometheus Monitoring for Spark Applications in YAML\nDESCRIPTION: Shows how to configure Prometheus monitoring for a Spark application, including specifying the JMX exporter JAR and exposing driver metrics.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/writing-sparkapplication.md#2025-04-10_snippet_27\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  deps:\n    jars:\n    - http://central.maven.org/maven2/io/prometheus/jmx/jmx_prometheus_javaagent/0.11.0/jmx_prometheus_javaagent-0.11.0.jar\n  monitoring:\n    exposeDriverMetrics: true\n    prometheus:\n      jmxExporterJar: \"/var/spark-data/spark-jars/jmx_prometheus_javaagent-0.11.0.jar\"\n```\n\n----------------------------------------\n\nTITLE: Submitting a Kubeflow Pipeline for Execution using Python Client\nDESCRIPTION: Uses the Kubeflow Pipelines client to create and execute a pipeline run from a pipeline function. The code submits the pipeline defined in 'my_pipeline' with an empty arguments dictionary.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/component-development.md#2025-04-10_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Compile, upload, and submit this pipeline for execution.\nclient.create_run_from_pipeline_func(my_pipeline, arguments={})\n```\n\n----------------------------------------\n\nTITLE: Installing KFP CLI via pip\nDESCRIPTION: Command to install the KFP CLI through pip package manager\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/cli.md#2025-04-10_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npip install kfp\n```\n\n----------------------------------------\n\nTITLE: Compiling Basic Pipeline to YAML with KFP SDK\nDESCRIPTION: Example showing how to define and compile a basic pipeline with a single component using the KFP SDK compiler. The pipeline takes a string message parameter and returns it after printing.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/compile-a-pipeline.md#2025-04-10_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import compiler, dsl\n\n@dsl.component\ndef comp(message: str) -> str:\n    print(message)\n    return message\n\n@dsl.pipeline\ndef my_pipeline(message: str) -> str:\n    \"\"\"My ML pipeline.\"\"\"\n    return comp(message=message).output\n\ncompiler.Compiler().compile(my_pipeline, package_path='pipeline.yaml')\n```\n\n----------------------------------------\n\nTITLE: AWS Environment Credentials Configuration\nDESCRIPTION: Python code snippet showing how to configure AWS credentials as environment variables in a Kubeflow pipeline task.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/operator-guides/configure-object-store.md#2025-04-10_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nkubernetes.use_secret_as_env(\n    your_task,\n    secret_name='aws-s3-creds',\n    secret_key_to_env={'AWS_SECRET_ACCESS_KEY': 'AWS_SECRET_ACCESS_KEY'})\nkubernetes.use_secret_as_env(\n    your_task,\n    secret_name='aws-s3-creds',\n    secret_key_to_env={'AWS_ACCESS_KEY_ID': 'AWS_ACCESS_KEY_ID'})\nkubernetes.use_secret_as_env(\n    your_task,\n    secret_name='aws-s3-creds',\n    secret_key_to_env={'AWS_REGION': 'AWS_REGION'})\n...\n```\n\n----------------------------------------\n\nTITLE: Creating a Kubeflow Profile with YAML Definition\nDESCRIPTION: A YAML specification for creating a Kubeflow Profile, which includes the profile name, owner details, and optional plugins and resource quotas. This represents the core structure needed for profile creation.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/central-dash/profiles.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: kubeflow.org/v1\nkind: Profile\nmetadata:\n  ## the profile name will be the namespace name\n  ## WARNING: unexpected behavior may occur if the namespace already exists\n  name: my-profile\nspec:\n  ## the owner of the profile\n  ## NOTE: you may wish to make a global super-admin the owner of all profiles\n  ##       and only give end-users view or modify access to profiles to prevent\n  ##       them from adding/removing contributors\n  owner:\n    kind: User\n    name: admin@example.com\n\n  ## plugins extend the functionality of the profile\n  ## https://github.com/kubeflow/kubeflow/tree/master/components/profile-controller#plugins\n  plugins: []\n  \n  ## optionally create a ResourceQuota for the profile\n  ## https://github.com/kubeflow/kubeflow/tree/master/components/profile-controller#resourcequotaspec\n  ## https://kubernetes.io/docs/reference/kubernetes-api/policy-resources/resource-quota-v1/#ResourceQuotaSpec\n  resourceQuotaSpec: {}\n```\n\n----------------------------------------\n\nTITLE: Migrating Container Components\nDESCRIPTION: Shows how to migrate from v1's ContainerOp to v2's @dsl.container_component decorator for defining container-based components.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/migration.md#2025-04-10_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\n\n@dsl.container_component\ndef flip_coin(rand: int, result: dsl.OutputPath(str)):\n  return ContainerSpec(\n    image='gcr.io/flip-image'\n    command=['flip'],\n    arguments=['--seed', rand, '--result-file', result])\n```\n\n----------------------------------------\n\nTITLE: Using Loaded Pipeline as Component in Kubeflow\nDESCRIPTION: Demonstrates loading a pipeline from a URL and using it as a component within another pipeline, passing parameters to the loaded pipeline.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/load-and-share-components.md#2025-04-10_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import components\n\nloaded_pipeline = components.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/2.0.0/sdk/python/test_data/pipelines/pipeline_in_pipeline_complex.yaml')\n\n@dsl.pipeline\ndef my_pipeline():\n    loaded_pipeline(msg='Hello KFP v2!')\n```\n\n----------------------------------------\n\nTITLE: Configuring Maven Dependencies in SparkApplication\nDESCRIPTION: Example demonstrating how to specify Maven package dependencies, exclusions, and custom repositories in a SparkApplication manifest.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/writing-sparkapplication.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  deps:\n    repositories:\n      - https://repository.example.com/prod\n    packages:\n      - com.example:some-package:1.0.0\n    excludePackages:\n      - com.example:other-package\n```\n\n----------------------------------------\n\nTITLE: Generating Scalar Metrics Visualization in Python\nDESCRIPTION: This snippet illustrates how to create a component that generates scalar metric visualizations using the Kubeflow Pipelines v2 SDK. It uses scikit-learn to train a logistic regression model on the iris dataset and log the accuracy metric.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/output-viewer.md#2025-04-10_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\n@component(\n    packages_to_install=['sklearn'],\n    base_image='python:3.9',\n)\ndef digit_classification(metrics: Output[Metrics]):\n    from sklearn import model_selection\n    from sklearn.linear_model import LogisticRegression\n    from sklearn import datasets\n    from sklearn.metrics import accuracy_score\n\n    # Load digits dataset\n    iris = datasets.load_iris()\n\n    # # Create feature matrix\n    X = iris.data\n\n    # Create target vector\n    y = iris.target\n\n    #test size\n    test_size = 0.33\n\n    seed = 7\n    #cross-validation settings\n    kfold = model_selection.KFold(n_splits=10, random_state=seed, shuffle=True)\n\n    #Model instance\n    model = LogisticRegression()\n    scoring = 'accuracy'\n    results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n\n    #split data\n    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=test_size, random_state=seed)\n    #fit model\n    model.fit(X_train, y_train)\n\n    #accuracy on test set\n    result = model.score(X_test, y_test)\n    metrics.log_metric('accuracy', (result*100.0))\n\n@dsl.pipeline(\n    name='metrics-visualization-pipeline')\ndef metrics_visualization_pipeline():\n    digit_classification_op = digit_classification()\n```\n\n----------------------------------------\n\nTITLE: Example IR YAML Output Structure\nDESCRIPTION: Shows the header section of a compiled pipeline YAML file containing pipeline metadata including name, description, inputs and outputs.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/compile-a-pipeline.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n# PIPELINE DEFINITION\n# Name: my-pipeline\n# Description: My ML pipeline.\n# Inputs:\n#    message: str\n# Outputs:\n#    Output: str\n...\n```\n\n----------------------------------------\n\nTITLE: Example Pipeline with Mixed Caching Settings\nDESCRIPTION: This code shows a pipeline definition with multiple tasks where caching is selectively disabled for one task. Used to demonstrate environment variable caching control.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/caching.md#2025-04-10_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@dsl.pipeline(name='my-pipeline')\ndef my_pipeline():\n    task_1 = create_dataset()\n    task_2 = create_dataset()\n    task_1.set_caching_options(False)\n\nCompiler().compile(\n    pipeline_func=my_pipeline,\n    package_path='my_pipeline.yaml',\n\n)\n```\n\n----------------------------------------\n\nTITLE: Examining Katib Experiment Status with kubectl\nDESCRIPTION: A shell command and its YAML output showing how to retrieve the status of a Katib experiment. The status includes information about the current optimal trial, running trials, succeeded trials, and overall experiment progress.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/hp-tuning/configure-experiment.md#2025-04-10_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nkubectl -n kubeflow get experiment random -o yaml\n```\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: kubeflow.org/v1beta1\nkind: Experiment\nmetadata:\n  ...\n  name: random\n  namespace: kubeflow\n  ...\nspec:\n  ...\nstatus:\n  currentOptimalTrial:\n    bestTrialName: random-hpsrsdqp\n    observation:\n      metrics:\n        - latest: \"0.11513\"\n          max: \"0.53415\"\n          min: \"0.01235\"\n          name: loss\n    parameterAssignments:\n      - name: lr\n        value: \"0.024736875661534784\"\n      - name: momentum\n        value: \"0.6612351235123\"\n  runningTrialList:\n    - random-2dwxbwcg\n    - random-6jd8hmnd\n    - random-7gks8bmf\n  startTime: \"2021-10-07T21:12:06Z\"\n  succeededTrialList:\n    - random-xhpcrt2p\n    - random-hpsrsdqp\n    - random-kddxqqg9\n    - random-4lkr5cjp\n  trials: 7\n  trialsRunning: 3\n  trialsSucceeded: 4\n```\n\n----------------------------------------\n\nTITLE: Testing Division Component Function\nDESCRIPTION: Simple test execution of the my_divmod function with sample inputs to verify functionality.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/python-function-components.ipynb#2025-04-10_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nmy_divmod(100, 7)\n```\n\n----------------------------------------\n\nTITLE: Initializing Kubeflow Pipelines Client Inside Cluster\nDESCRIPTION: Creates a KFP client using a ServiceAccount token for authentication when running inside a Kubernetes cluster with Kubeflow.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/connect-api.md#2025-04-10_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport kfp\n\n# by default, when run from inside a Kubernetes cluster:\n#  - the token is read from the `KF_PIPELINES_SA_TOKEN_PATH` path\n#  - the host is set to `http://ml-pipeline-ui.kubeflow.svc.cluster.local`\nkfp_client = kfp.Client()\n\n# test the client by listing experiments\nexperiments = kfp_client.list_experiments(namespace=\"my-profile\")\nprint(experiments)\n```\n\n----------------------------------------\n\nTITLE: TFJob Example with Gang Scheduling\nDESCRIPTION: A complete TFJob specification example that can be used with gang scheduling. It defines a parameter server and worker setup for TensorFlow benchmarking.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/job-scheduling.md#2025-04-10_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"kubeflow.org/v1beta1\"\nkind: \"TFJob\"\nmetadata:\n  name: \"tfjob-gang-scheduling\"\nspec:\n  tfReplicaSpecs:\n    Worker:\n      replicas: 1\n      template:\n        spec:\n          containers:\n            - args:\n                - python\n                - tf_cnn_benchmarks.py\n                - --batch_size=32\n                - --model=resnet50\n                - --variable_update=parameter_server\n                - --flush_stdout=true\n                - --num_gpus=1\n                - --local_parameter_device=cpu\n                - --device=gpu\n                - --data_format=NHWC\n              image: gcr.io/kubeflow/tf-benchmarks-gpu:v20171202-bdab599-dirty-284af3\n              name: tensorflow\n              resources:\n                limits:\n                  nvidia.com/gpu: 1\n              workingDir: /opt/tf-benchmarks/scripts/tf_cnn_benchmarks\n          restartPolicy: OnFailure\n    PS:\n      replicas: 1\n      template:\n        spec:\n          containers:\n            - args:\n                - python\n                - tf_cnn_benchmarks.py\n                - --batch_size=32\n                - --model=resnet50\n                - --variable_update=parameter_server\n                - --flush_stdout=true\n                - --num_gpus=1\n                - --local_parameter_device=cpu\n                - --device=cpu\n                - --data_format=NHWC\n              image: gcr.io/kubeflow/tf-benchmarks-cpu:v20171202-bdab599-dirty-284af3\n              name: tensorflow\n              resources:\n                limits:\n                  cpu: \"1\"\n              workingDir: /opt/tf-benchmarks/scripts/tf_cnn_benchmarks\n          restartPolicy: OnFailure\n```\n\n----------------------------------------\n\nTITLE: Implementing HTML Output in Kubeflow Pipeline Component\nDESCRIPTION: Demonstrates creating a component that generates HTML visualization output. Uses Output[HTML] type to create a self-contained HTML file.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/output-viewer.md#2025-04-10_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\n@component\ndef html_visualization(html_artifact: Output[HTML]):\n    html_content = '<!DOCTYPE html><html><body><h1>Hello world</h1></body></html>'\n    with open(html_artifact.path, 'w') as f:\n        f.write(html_content)\n```\n\n----------------------------------------\n\nTITLE: Configuring KFP Local Execution Settings\nDESCRIPTION: This snippet shows how to configure local execution settings such as error handling and pipeline root directory. It demonstrates the use of raise_on_error and pipeline_root parameters.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/execute-kfp-pipelines-locally.md#2025-04-10_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nlocal.init(runner=...,\n           raise_on_error=False,\n           pipeline_root='~/my/component/outputs')\n```\n\n----------------------------------------\n\nTITLE: Initializing KFP Local Execution with SubprocessRunner\nDESCRIPTION: This example demonstrates how to initialize local execution using the SubprocessRunner, which is recommended when Docker cannot be installed. It also shows how to disable the default virtual environment usage.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/execute-kfp-pipelines-locally.md#2025-04-10_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import local\n\nlocal.init(runner=local.SubprocessRunner())\n\n# Disabling virtual environment usage\nlocal.init(runner=local.SubprocessRunner(use_venv=False))\n```\n\n----------------------------------------\n\nTITLE: Installing Katib with External Database\nDESCRIPTION: This command deploys Katib with a custom DB backend. It allows the use of a custom instance of MySQL DB instead of katib-mysql. Users need to modify the appropriate environment variables for katib-db-manager in the secrets.env file with their MySQL DB values.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/installation-options.md#2025-04-10_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -k \"github.com/kubeflow/katib.git/manifests/v1beta1/installs/katib-external-db?ref=master\"\n```\n\n----------------------------------------\n\nTITLE: Installing Kubeflow Pipelines with K3ai (GPU support)\nDESCRIPTION: This command installs Kubeflow Pipelines using K3ai with GPU support. It uses a curl command to download and execute the K3ai installation script with GPU-specific parameters.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/localcluster-deployment.md#2025-04-10_snippet_12\n\nLANGUAGE: SHELL\nCODE:\n```\ncurl -sfL https://get.k3ai.in | bash -s -- --gpu --plugin_kfpipelines\n```\n\n----------------------------------------\n\nTITLE: Running Pipeline with Environment-Based Cache Disabling\nDESCRIPTION: Example command showing how to execute a pipeline script with caching disabled via environment variable, which affects all tasks that don't explicitly override caching settings.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/caching.md#2025-04-10_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nKFP_DISABLE_EXECUTION_CACHING_BY_DEFAULT=true \\\npython my_pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Configuring Resources for Katib Trials in Python\nDESCRIPTION: This code snippet shows how to configure resources for each trial in a Katib experiment using the TrainerResources object, including settings for distributed training.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/llm-hp-optimization.md#2025-04-10_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport kubeflow.katib as katib\n\nresources_per_trial=katib.TrainerResources(\n   num_workers=1,\n   num_procs_per_worker=1,\n   resources_per_worker={\"gpu\": 0, \"cpu\": 1, \"memory\": \"10G\",},\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Component Types with Python Decorator\nDESCRIPTION: Example of defining a component with type annotations using the Python decorator approach. The component accepts an Integer input and returns outputs with GCSPath, customized_type, and GCRPath types.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/static-type-checking.md#2025-04-10_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp.dsl import component\nfrom kfp.dsl.types import Integer, GCRPath\n\n\n@component\ndef task_factory_a(field_l: Integer()) -> {\n    'field_m': {\n        'GCSPath': {\n            'openapi_schema_validator':\n                '{\"type\": \"string\", \"pattern\": \"^gs://.*$\"}'\n        }\n    },\n    'field_n': 'customized_type',\n    'field_o': GCRPath()\n}:\n  return ContainerOp(\n      name='operator a',\n      image='gcr.io/ml-pipeline/component-a',\n      command=['python3', '/pipelines/component/src/train.py'],\n      arguments=[\n          '--field-l',\n          field_l,\n      ],\n      file_outputs={\n          'field_m': '/schema.txt',\n          'field_n': '/feature.txt',\n          'field_o': '/output.txt'\n      })\n```\n\n----------------------------------------\n\nTITLE: Documenting a Pipeline with KFP Docstring Style in Python\nDESCRIPTION: Example of a pipeline function with properly formatted docstring that follows the KFP docstring style. The docstring includes descriptions for pipeline inputs (string, in_dataset) and outputs that will be automatically extracted by the KFP SDK when compiling the pipeline.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/additional-functionality.md#2025-04-10_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@dsl.pipeline(display_name='Concatenation pipeline')\ndef dataset_concatenator(\n    string: str,\n    in_dataset: Input[Dataset],\n) -> Dataset:\n    \"\"\"Pipeline to convert string to a Dataset, then concatenate with\n    in_dataset.\n\n    Args:\n        string: String to concatenate to in_artifact.\n        in_dataset: Dataset to which to concatenate string.\n\n    Returns:\n        Output: The final concatenated dataset.\n    \"\"\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Implementing Dex Authentication for Kubeflow Pipelines in Python\nDESCRIPTION: A Python class that handles Dex authentication to create authenticated KFP client instances. It handles various authentication scenarios including OAuth2 proxy, different auth types, and session management.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/connect-api.md#2025-04-10_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport re\nfrom urllib.parse import urlsplit, urlencode\n\nimport kfp\nimport requests\nimport urllib3\n\n\nclass KFPClientManager:\n    \"\"\"\n    A class that creates `kfp.Client` instances with Dex authentication.\n    \"\"\"\n\n    def __init__(\n        self,\n        api_url: str,\n        dex_username: str,\n        dex_password: str,\n        dex_auth_type: str = \"local\",\n        skip_tls_verify: bool = False,\n    ):\n        \"\"\"\n        Initialize the KfpClient\n\n        :param api_url: the Kubeflow Pipelines API URL\n        :param skip_tls_verify: if True, skip TLS verification\n        :param dex_username: the Dex username\n        :param dex_password: the Dex password\n        :param dex_auth_type: the auth type to use if Dex has multiple enabled, one of: ['ldap', 'local']\n        \"\"\"\n        self._api_url = api_url\n        self._skip_tls_verify = skip_tls_verify\n        self._dex_username = dex_username\n        self._dex_password = dex_password\n        self._dex_auth_type = dex_auth_type\n        self._client = None\n\n        # disable SSL verification, if requested\n        if self._skip_tls_verify:\n            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n\n        # ensure `dex_default_auth_type` is valid\n        if self._dex_auth_type not in [\"ldap\", \"local\"]:\n            raise ValueError(\n                f\"Invalid `dex_auth_type` '{self._dex_auth_type}', must be one of: ['ldap', 'local']\"\n            )\n\n    def _get_session_cookies(self) -> str:\n        \"\"\"\n        Get the session cookies by authenticating against Dex\n        :return: a string of session cookies in the form \"key1=value1; key2=value2\"\n        \"\"\"\n\n        # use a persistent session (for cookies)\n        s = requests.Session()\n\n        # GET the api_url, which should redirect to Dex\n        resp = s.get(\n            self._api_url, allow_redirects=True, verify=not self._skip_tls_verify\n        )\n        if resp.status_code == 200:\n            pass\n        elif resp.status_code == 403:\n            # if we get 403, we might be at the oauth2-proxy sign-in page\n            # the default path to start the sign-in flow is `/oauth2/start?rd=<url>`\n            url_obj = urlsplit(resp.url)\n            url_obj = url_obj._replace(\n                path=\"/oauth2/start\", query=urlencode({\"rd\": url_obj.path})\n            )\n            resp = s.get(\n                url_obj.geturl(), allow_redirects=True, verify=not self._skip_tls_verify\n            )\n        else:\n            raise RuntimeError(\n                f\"HTTP status code '{resp.status_code}' for GET against: {self._api_url}\"\n            )\n\n        # if we were NOT redirected, then the endpoint is unsecured\n        if len(resp.history) == 0:\n            # no cookies are needed\n            return \"\"\n\n        # if we are at `../auth` path, we need to select an auth type\n        url_obj = urlsplit(resp.url)\n        if re.search(r\"/auth$\", url_obj.path):\n            url_obj = url_obj._replace(\n                path=re.sub(r\"/auth$\", f\"/auth/{self._dex_auth_type}\", url_obj.path)\n            )\n\n        # if we are at `../auth/xxxx/login` path, then we are at the login page\n        if re.search(r\"/auth/.*/login$\", url_obj.path):\n            dex_login_url = url_obj.geturl()\n        else:\n            # otherwise, we need to follow a redirect to the login page\n            resp = s.get(\n                url_obj.geturl(), allow_redirects=True, verify=not self._skip_tls_verify\n            )\n            if resp.status_code != 200:\n                raise RuntimeError(\n                    f\"HTTP status code '{resp.status_code}' for GET against: {url_obj.geturl()}\"\n                )\n            dex_login_url = resp.url\n\n        # attempt Dex login\n        resp = s.post(\n            dex_login_url,\n            data={\"login\": self._dex_username, \"password\": self._dex_password},\n            allow_redirects=True,\n            verify=not self._skip_tls_verify,\n        )\n        if resp.status_code != 200:\n            raise RuntimeError(\n                f\"HTTP status code '{resp.status_code}' for POST against: {dex_login_url}\"\n            )\n\n        # if we were NOT redirected, then the login credentials were probably invalid\n        if len(resp.history) == 0:\n            raise RuntimeError(\n                f\"Login credentials are probably invalid - \"\n                f\"No redirect after POST to: {dex_login_url}\"\n            )\n\n        # if we are at `../approval` path, we need to approve the login\n        url_obj = urlsplit(resp.url)\n        if re.search(r\"/approval$\", url_obj.path):\n            dex_approval_url = url_obj.geturl()\n\n            # approve the login\n            resp = s.post(\n                dex_approval_url,\n                data={\"approval\": \"approve\"},\n                allow_redirects=True,\n                verify=not self._skip_tls_verify,\n            )\n            if resp.status_code != 200:\n                raise RuntimeError(\n                    f\"HTTP status code '{resp.status_code}' for POST against: {url_obj.geturl()}\"\n                )\n\n        return \"; \".join([f\"{c.name}={c.value}\" for c in s.cookies])\n\n    def _create_kfp_client(self) -> kfp.Client:\n        try:\n            session_cookies = self._get_session_cookies()\n        except Exception as ex:\n            raise RuntimeError(f\"Failed to get Dex session cookies\") from ex\n\n        # monkey patch the kfp.Client to support disabling SSL verification\n        # kfp only added support in v2: https://github.com/kubeflow/pipelines/pull/7174\n        original_load_config = kfp.Client._load_config\n\n        def patched_load_config(client_self, *args, **kwargs):\n            config = original_load_config(client_self, *args, **kwargs)\n            config.verify_ssl = not self._skip_tls_verify\n            return config\n\n        patched_kfp_client = kfp.Client\n        patched_kfp_client._load_config = patched_load_config\n\n        return patched_kfp_client(\n            host=self._api_url,\n            cookies=session_cookies,\n        )\n\n    def create_kfp_client(self) -> kfp.Client:\n        \"\"\"Get a newly authenticated Kubeflow Pipelines client.\"\"\"\n        return self._create_kfp_client()\n```\n\n----------------------------------------\n\nTITLE: Configuring RBAC RoleBinding for Kubeflow Pipelines Access\nDESCRIPTION: YAML configuration for a RoleBinding to allow a ServiceAccount to manage Kubeflow Pipelines across namespaces.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/connect-api.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: allow-namespace-2-kubeflow-edit\n  ## this RoleBinding is in `namespace-1`, because it grants access to `namespace-1`\n  namespace: namespace-1\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: kubeflow-edit\nsubjects:\n  - kind: ServiceAccount\n    name: default-editor\n    ## the ServiceAccount lives in `namespace-2`\n    namespace: namespace-2\n```\n\n----------------------------------------\n\nTITLE: Optimal Hyperparameters JSON Output from Katib\nDESCRIPTION: This JSON represents the output from a Katib experiment, showing the best trial name, parameter assignments, and observation metrics. It displays the optimal values found during hyperparameter tuning.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/getting-started.md#2025-04-10_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"best_trial_name\": \"tune-experiment-nmggpxx2\",\n  \"parameter_assignments\": [\n    {\n      \"name\": \"a\",\n      \"value\": \"19\"\n    },\n    {\n      \"name\": \"b\",\n      \"value\": \"0.13546396192975868\"\n    }\n  ],\n  \"observation\": {\n    \"metrics\": [\n      {\n        \"latest\": \"75.98164951501829\",\n        \"max\": \"75.98164951501829\",\n        \"min\": \"75.98164951501829\",\n        \"name\": \"result\"\n      }\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Creating Experiment and Run using Kubeflow Pipelines SDK in Python\nDESCRIPTION: This snippet demonstrates how to create an experiment and associated run from a Pod inside a full Kubeflow cluster using the Kubeflow Pipelines SDK. It includes setting up credentials, creating a client, and interacting with experiments and runs in a specific namespace.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/operator-guides/multi-user.md#2025-04-10_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport kfp\n\n# the namespace in which you deployed Kubeflow Pipelines\nkubeflow_namespace = \"kubeflow\"\n\n# the namespace of your pipelines user (where the pipeline will be executed)\nuser_namespace = \"jane-doe\"\n\n# the KF_PIPELINES_SA_TOKEN_PATH environment variable is used when no `path` is set\n# the default KF_PIPELINES_SA_TOKEN_PATH is /var/run/secrets/kubeflow/pipelines/token\ncredentials = kfp.auth.ServiceAccountTokenVolumeCredentials(path=None)\n\n# create a client\nclient = kfp.Client(host=f\"http://ml-pipeline-ui.{kubeflow_namespace}\", credentials=credentials)\n\n# create an experiment\nclient.create_experiment(name=\"<YOUR_EXPERIMENT_ID>\", namespace=user_namespace)\nprint(client.list_experiments(namespace=user_namespace))\n\n# create a pipeline run\nclient.run_pipeline(\n    experiment_id=\"<YOUR_EXPERIMENT_ID>\",  # the experiment determines the namespace\n    job_name=\"<YOUR_RUN_NAME>\",\n    pipeline_id=\"<YOUR_PIPELINE_ID>\"  # the pipeline definition to run\n)\nprint(client.list_runs(experiment_id=\"<YOUR_EXPERIMENT_ID>\"))\nprint(client.list_runs(namespace=user_namespace))\n```\n\n----------------------------------------\n\nTITLE: Configuring PodDefault for Kubeflow Pipelines Access\nDESCRIPTION: YAML configuration for a PodDefault resource to inject ServiceAccount token volume for Kubeflow Pipelines access.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/connect-api.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: kubeflow.org/v1alpha1\nkind: PodDefault\nmetadata:\n  name: access-ml-pipeline\n  namespace: \"<YOUR_USER_PROFILE_NAMESPACE>\"\nspec:\n  desc: Allow access to Kubeflow Pipelines\n  selector:\n    matchLabels:\n      access-ml-pipeline: \"true\"\n  env:\n    - ## this environment variable is automatically read by `kfp.Client()`\n      ## this is the default value, but we show it here for clarity\n      name: KF_PIPELINES_SA_TOKEN_PATH\n      value: /var/run/secrets/kubeflow/pipelines/token\n  volumes:\n    - name: volume-kf-pipeline-token\n      projected:\n        sources:\n          - serviceAccountToken:\n              path: token\n              expirationSeconds: 7200\n              ## defined by the `TOKEN_REVIEW_AUDIENCE` environment variable on the `ml-pipeline` deployment\n              audience: pipelines.kubeflow.org      \n  volumeMounts:\n    - mountPath: /var/run/secrets/kubeflow/pipelines\n      name: volume-kf-pipeline-token\n      readOnly: true\n```\n\n----------------------------------------\n\nTITLE: Building Table Visualization in Kubeflow Pipelines\nDESCRIPTION: This code creates a table visualization from CSV data. The table viewer builds an HTML table from the specified source data, using the header field to define column names. The resulting table supports pagination.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/output-viewer.md#2025-04-10_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\ndef table_vis(mlpipeline_ui_metadata_path: kfp.components.OutputPath()):\n  import json\n\n  metadata = {\n    'outputs' : [{\n      'type': 'table',\n      'storage': 'gcs',\n      'format': 'csv',\n      'header': [x['name'] for x in schema],\n      'source': prediction_results\n    }]\n  }\n\n  with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:\n    json.dump(metadata, metadata_file)\n```\n\n----------------------------------------\n\nTITLE: Creating ROC Curve Visualization in Kubeflow Pipelines\nDESCRIPTION: This code generates a receiver operating characteristic (ROC) curve visualization by saving fpr, tpr, and thresholds data to a CSV file. The ROC viewer plots this data and allows users to hover over the curve to see threshold values.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/output-viewer.md#2025-04-10_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\ndef roc_vis(roc_csv_file_path: str, mlpipeline_ui_metadata_path: kfp.components.OutputPath()):\n  import json\n\n  df_roc = pd.DataFrame({'fpr': fpr, 'tpr': tpr, 'thresholds': thresholds})\n  roc_file = os.path.join(roc_csv_file_path, 'roc.csv')\n  with file_io.FileIO(roc_file, 'w') as f:\n    df_roc.to_csv(f, columns=['fpr', 'tpr', 'thresholds'], header=False, index=False)\n\n  metadata = {\n    'outputs': [{\n      'type': 'roc',\n      'format': 'csv',\n      'schema': [\n        {'name': 'fpr', 'type': 'NUMBER'},\n        {'name': 'tpr', 'type': 'NUMBER'},\n        {'name': 'thresholds', 'type': 'NUMBER'},\n      ],\n      'source': roc_file\n    }]\n  }\n\n  with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:\n    json.dump(metadata, metadata_file)\n```\n\n----------------------------------------\n\nTITLE: Configuring AWS Authentication for S3 Pipeline Root in Kubeflow Pipelines\nDESCRIPTION: This code snippet demonstrates how to add AWS authentication for S3 access when using a pipeline root in Kubeflow Pipelines. It uses the add_op_transformer method with AWS secret credentials.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/concepts/pipeline-root.md#2025-04-10_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndsl.get_pipeline_conf().add_op_transformer(aws.use_aws_secret('xxx', 'xxx', 'xxx'))\n```\n\n----------------------------------------\n\nTITLE: Mounting and Using PVC in Pipeline Tasks\nDESCRIPTION: Shows how to mount the PVC to pipeline tasks and coordinate their execution order to prevent concurrent access.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/platform-specific-features.md#2025-04-10_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n    # write to the PVC\n    task1 = producer()\n    kubernetes.mount_pvc(\n        task1,\n        pvc_name=pvc1.outputs['name'],\n        mount_path='/data',\n    )\n\n    # read to the PVC\n    task2 = consumer()\n    kubernetes.mount_pvc(\n        task2,\n        pvc_name=pvc1.outputs['name'],\n        mount_path='/reused_data',\n    )\n    task2.after(task1)\n```\n\n----------------------------------------\n\nTITLE: Pip Install Command Example\nDESCRIPTION: Shell command showing equivalent pip install command for component with custom package indices.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/lightweight-python-components.md#2025-04-10_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\npip install custom-ml-package==0.0.1 numpy==1.21.6 kfp==2 --index-url http://myprivaterepo.com/simple --trusted-host http://myprivaterepo.com/simple --extra-index-url http://pypi.org/simple --trusted-host http://pypi.org/simple\n```\n\n----------------------------------------\n\nTITLE: Configuring PodDefault for GCP Secret in Kubeflow\nDESCRIPTION: YAML configuration for setting up a PodDefault resource that adds GCP credentials to notebook pods. This configuration mounts a secret volume containing GCP credentials at /secret/gcp path.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/notebooks/quickstart-guide.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: kubeflow.org/v1alpha1\nkind: PodDefault\nmetadata:\n  name: add-gcp-secret\n  namespace: MY_PROFILE_NAMESPACE\nspec:\n selector:\n  matchLabels:\n    add-gcp-secret: \"true\"\n desc: \"add gcp credential\"\n volumeMounts:\n - name: secret-volume\n   mountPath: /secret/gcp\n volumes:\n - name: secret-volume\n   secret:\n    secretName: gcp-secret\n```\n\n----------------------------------------\n\nTITLE: Configuring Dynamic Allocation for Spark Applications in YAML\nDESCRIPTION: Demonstrates how to enable and configure dynamic allocation for a Spark application, including setting initial, minimum, and maximum executor counts.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/writing-sparkapplication.md#2025-04-10_snippet_28\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  dynamicAllocation:\n    enabled: true\n    initialExecutors: 2\n    minExecutors: 2\n    maxExecutors: 10\n```\n\n----------------------------------------\n\nTITLE: Deploying Custom Spark Operator with Helm\nDESCRIPTION: These commands deploy a customized Spark Operator using Helm. The first command adds the Spark Operator Helm repository, and the second command installs the operator with custom image registry, repository, and tag settings.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/customizing-spark-operator.md#2025-04-10_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nhelm repo add --force-update spark-operator https://kubeflow.github.io/spark-operator\n\nhelm install spark-operator spark-operator/spark-operator \\\n    --namespace spark-operator \\\n    --create-namespace \\\n    --set image.registry=docker.io \\\n    --set image.repository=kubeflow/spark-operator \\\n    --set image.tag=latest\n```\n\n----------------------------------------\n\nTITLE: Creating Web App Visualization in Kubeflow Pipelines\nDESCRIPTION: This code demonstrates how to create custom HTML visualizations in Kubeflow Pipelines. It supports both file-based HTML content and inline HTML strings, providing flexibility for rendering custom output in a sandboxed iframe.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/output-viewer.md#2025-04-10_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\ndef tensorboard_vis(mlpipeline_ui_metadata_path: kfp.components.OutputPath()):\n  import json\n\n  static_html_path = os.path.join(output_dir, _OUTPUT_HTML_FILE)\n  file_io.write_string_to_file(static_html_path, rendered_template)\n\n  metadata = {\n    'outputs' : [{\n      'type': 'web-app',\n      'storage': 'gcs',\n      'source': static_html_path,\n    }, {\n      'type': 'web-app',\n      'storage': 'inline',\n      'source': '<h1>Hello, World!</h1>',\n    }]\n  }\n\n  with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:\n    json.dump(metadata, metadata_file)\n```\n\n----------------------------------------\n\nTITLE: Generating Markdown Visualization in Kubeflow Pipelines\nDESCRIPTION: This code creates a metadata file that defines two markdown visualizations: one with inline content and another that references an external file. The markdown viewer renders these strings in the Kubeflow Pipelines UI.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/output-viewer.md#2025-04-10_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\ndef markdown_vis(mlpipeline_ui_metadata_path: kfp.components.OutputPath()):\n  import json\n    \n  metadata = {\n    'outputs' : [\n    # Markdown that is hardcoded inline\n    {\n      'storage': 'inline',\n      'source': '# Inline Markdown\\n[A link](https://www.kubeflow.org/)',\n      'type': 'markdown',\n    },\n    # Markdown that is read from a file\n    {\n      'source': 'gs://your_project/your_bucket/your_markdown_file',\n      'type': 'markdown',\n    }]\n  }\n\n  with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:\n    json.dump(metadata, metadata_file)\n```\n\n----------------------------------------\n\nTITLE: Running the Pipeline with Kubeflow Pipelines SDK\nDESCRIPTION: This code demonstrates how to run the defined pipeline using the Kubeflow Pipelines SDK Client. It creates a pipeline run with empty arguments, executing the environment_pipeline function.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/enviroment_variables.md#2025-04-10_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n#Specify pipeline argument values\narguments = {}\n\n#Submit a pipeline run\nkfp.Client().create_run_from_pipeline_func(environment_pipeline,\n                                           arguments=arguments)\n```\n\n----------------------------------------\n\nTITLE: Listing Pipelines with Filter in KFP SDK\nDESCRIPTION: Example showing how to list pipelines with a specific name filter using the Kubeflow Pipelines SDK. Demonstrates the use of JSON filters with predicates.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/tutorials/sdk-examples.md#2025-04-10_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport kfp\nimport json\n\n# 'host' is your Kubeflow Pipelines API server's host address.\nhost = <host>\n# 'pipeline_name' is the name of the pipeline you want to list.\npipeline_name = <pipeline name>\n\nclient = kfp.Client(host)\n# To filter on pipeline name, you can use a predicate indicating that the pipeline\n# name is equal to the given name.\n# A predicate includes 'key', 'op' and 'string_value' fields.\n# The 'key' specifies the property you want to apply the filter to. For example,\n# if you want to filter on the pipeline name, then 'key' is set to 'name' as\n# shown below.\n# The 'op' specifies the operator used in a predicate. The operator can be\n# EQUALS, NOT_EQUALS, GREATER_THAN, etc. The complete list is at [filter.proto](https://github.com/kubeflow/pipelines/blob/sdk/release-1.8/backend/api/filter.proto#L32)\n# When using the operator in a string-typed predicate, you need to use the\n# corresponding integer value of the enum. For Example, you can use the integer\n# value 1 to indicate EQUALS as shown below.\n# The 'string_value' specifies the value you want to filter with.\nfilter = json.dumps({'predicates': [{'key': 'name', 'op': 1, 'string_value': '{}'.format(pipeline_name)}]})\npipelines = client.pipelines.list_pipelines(filter=filter)\n# The pipeline with the given pipeline_name, if exists, is in pipelines.pipelines[0].\n```\n\n----------------------------------------\n\nTITLE: Input Value Placeholder Usage in Kubeflow Pipeline\nDESCRIPTION: Demonstrates how to use input value placeholders in component command line arguments and how they are resolved during pipeline execution.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/reference/component-spec.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ncommand: [program.py, --rounds, {inputValue: Rounds}]\n```\n\nLANGUAGE: python\nCODE:\n```\ntask1 = component1(rounds=150)\n```\n\nLANGUAGE: shell\nCODE:\n```\nprogram.py --rounds 150\n```\n\n----------------------------------------\n\nTITLE: Checking Notebook Resource Events in Kubernetes\nDESCRIPTION: Command to describe a Notebook resource and check its events section for potential errors. This helps diagnose why a notebook might not be starting properly.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/notebooks/troubleshooting.md#2025-04-10_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nkubectl describe notebooks \"${MY_NOTEBOOK_NAME}\" --namespace \"${MY_PROFILE_NAMESPACE}\"\n```\n\n----------------------------------------\n\nTITLE: Creating Pipeline and Pipeline Version with KFP SDK\nDESCRIPTION: Example demonstrating how to create a pipeline and add a version using the Kubeflow Pipelines SDK. Uses kfp.Client to upload pipeline files and create versions.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/tutorials/sdk-examples.md#2025-04-10_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport kfp\nimport os\n\nhost = <host>\npipeline_file_path = <path to pipeline file>\npipeline_name = <pipeline name>\npipeline_version_file_path = <path to pipeline version file>\npipeline_version_name = <pipeline version name>\n\nclient = kfp.Client(host)\npipeline_file = os.path.join(pipeline_file_path)\npipeline = client.pipeline_uploads.upload_pipeline(pipeline_file, name=pipeline_name)\npipeline_version_file = os.path.join(pipeline_version_file_path)\npipeline_version = client.pipeline_uploads.upload_pipeline_version(pipeline_version_file,\n                                                                   name=pipeline_version_name,\n                                                                   pipelineid=pipeline.id)\n```\n\n----------------------------------------\n\nTITLE: Input Path Placeholder Usage in Kubeflow Pipeline\nDESCRIPTION: Shows how to use input path placeholders for file-based inputs in component specifications and their resolution at runtime.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/reference/component-spec.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\ncommand: [program.py, --train-set, {inputPath: training_data}]\n```\n\nLANGUAGE: python\nCODE:\n```\ntask2 = component1(training_data=some_task1.outputs['some_data'])\n```\n\nLANGUAGE: shell\nCODE:\n```\nprogram.py --train-set /inputs/train_data/data\n```\n\n----------------------------------------\n\nTITLE: Implementing Confusion Matrix Visualization in KFP v1\nDESCRIPTION: Shows how to generate a confusion matrix visualization by writing metadata to the required mlpipeline-ui-metadata path.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/output-viewer.md#2025-04-10_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ndef confusion_matrix_viz(mlpipeline_ui_metadata_path: kfp.components.OutputPath()):\n  import json\n    \n  metadata = {\n    'outputs' : [{\n      'type': 'confusion_matrix',\n      'format': 'csv',\n      'schema': [\n        {'name': 'target', 'type': 'CATEGORY'},\n        {'name': 'predicted', 'type': 'CATEGORY'},\n        {'name': 'count', 'type': 'NUMBER'},\n      ],\n      'source': <CONFUSION_MATRIX_CSV_FILE>,\n      'labels': list(map(str, vocab)),\n    }]\n  }\n\n  with open(mlpipeline_ui_metadata_path, 'w') as metadata_file:\n    json.dump(metadata, metadata_file)\n```\n\n----------------------------------------\n\nTITLE: Specifying Endpoint for Kubeflow Pipelines CLI\nDESCRIPTION: The kfp --endpoint command allows specifying the endpoint that the Kubeflow Pipelines CLI should connect to when performing operations.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/sdk-overview.md#2025-04-10_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkfp --endpoint <ENDPOINT>\n```\n\n----------------------------------------\n\nTITLE: Overriding Component Caching Settings at Pipeline Level\nDESCRIPTION: This example shows how to enable or disable caching for all components in a pipeline when creating a run, which overrides individual component caching settings.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/caching.md#2025-04-10_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp.client import Client\n\nclient = Client()\nclient.create_run_from_pipeline_func(\n    hello_pipeline,\n    enable_caching=True,  # overrides the above disabling of caching\n)\n```\n\n----------------------------------------\n\nTITLE: Type Casting Example in KFP SDK v2\nDESCRIPTION: Illustrates the stricter type checking in KFP SDK v2 compared to v1, showing how string values are no longer automatically converted to float types.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/migration.md#2025-04-10_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp.v2 import compiler\nfrom kfp.v2 import dsl\nfrom kfp import components\n\n\n@dsl.component\ndef train(\n    number_of_epochs: int,\n    learning_rate: float,\n):\n    print(f\"number_of_epochs={number_of_epochs}\")\n    print(f\"learning_rate={learning_rate}\")\n\n\ndef training_pipeline(number_of_epochs: int = 1):\n    train(\n        number_of_epochs=number_of_epochs,\n        learning_rate=\"0.1\",  # string cannot be passed to float parameter using KFP SDK v2\n    )\n```\n\n----------------------------------------\n\nTITLE: Defining Component Implementation in YAML for Kubeflow Pipelines\nDESCRIPTION: This YAML snippet defines the implementation section of a Kubeflow Pipelines component specification. It specifies the container image and command to run, including placeholders for input and output paths.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/component-development.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nimplementation:\n  container:\n    image: gcr.io/my-org/my-image@sha256:a172..752f\n    # command is a list of strings (command-line arguments). \n    # The YAML language has two syntaxes for lists and you can use either of them. \n    # Here we use the \"flow syntax\" - comma-separated strings inside square brackets.\n    command: [\n      python3, \n      # Path of the program inside the container\n      /pipelines/component/src/program.py,\n      --input1-path,\n      {inputPath: Input 1},\n      --param1, \n      {inputValue: Parameter 1},\n      --output1-path, \n      {outputPath: Output 1},\n    ]\n```\n\n----------------------------------------\n\nTITLE: Database Connection String Format for PostgreSQL\nDESCRIPTION: Format string used by Katib DB Manager to create PostgreSQL database connections using specified environment variables for host, port, credentials and database name.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/env-variables.md#2025-04-10_snippet_1\n\nLANGUAGE: text\nCODE:\n```\npostgresql://[DB_USER[:DB_PASSWORD]@][KATIB_POSTGRESQL_DB_HOST][:KATIB_POSTGRESQL_DB_PORT][/KATIB_POSTGRESQL_DB_DATABASE]\n```\n\n----------------------------------------\n\nTITLE: Creating Component with Output Parameters\nDESCRIPTION: Implements a container component that both logs a greeting and writes it to an output file path that gets captured as a component output.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/container-components.md#2025-04-10_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@dsl.container_component\ndef say_hello(name: str, greeting: dsl.OutputPath(str)):\n    \"\"\"Log a greeting and return it as an output.\"\"\"\n\n    return dsl.ContainerSpec(\n        image='alpine',\n        command=[\n            'sh', '-c', '''RESPONSE=\"Hello, $0!\"\\\n                            && echo $RESPONSE\\\n                            && mkdir -p $(dirname $1)\\\n                            && echo $RESPONSE > $1\n                            '''\n        ],\n        args=[name, greeting])\n```\n\n----------------------------------------\n\nTITLE: Checking Training Job Status\nDESCRIPTION: Code to monitor the status of training job steps and device allocation.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/getting-started.md#2025-04-10_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfor s in TrainerClient().get_job(name=job_id).steps:\n    print(f\"Step: {s.name}, Status: {s.status}, Devices: {s.device} x {s.device_count}\")\n```\n\n----------------------------------------\n\nTITLE: Custom TFDV Statistics Visualization\nDESCRIPTION: Python code snippet that uses TensorFlow Data Validation to load and visualize statistics from a pipeline output file.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/python-based-visualizations.md#2025-04-10_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport tensorflow_data_validation as tfdv\nstats = tfdv.load_statistics('[output file path]/stats_tfrecord')\ntfdv.visualize_statistics(stats)\n```\n\n----------------------------------------\n\nTITLE: Adding In-Cluster Application Links to Kubeflow Dashboard\nDESCRIPTION: ConfigMap configuration for adding in-cluster application links to the Kubeflow dashboard sidebar. Shows how to add a custom application with icon and display text.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/central-dash/customize.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: centraldashboard-config\n  namespace: kubeflow\ndata:\n  settings: |-\n    ...\n  links: |-\n    {\n      \"menuLinks\": [\n        ...\n        {\n          \"type\": \"item\",\n          \"link\": \"/my-app/\",\n          \"text\": \"My App\",\n          \"icon\": \"social:mood\"\n        },\n        ...\n      ],\n      \"externalLinks\": [\n        ...\n      ],\n      \"quickLinks\": [\n        ...\n      ],\n      \"documentationItems\": [\n        ...\n      ]\n    }\n```\n\n----------------------------------------\n\nTITLE: Connecting to Standalone KFP from Outside the Cluster\nDESCRIPTION: Python code for connecting to Kubeflow Pipelines from outside the cluster using port forwarding. This connects to the local port that was forwarded to the ml-pipeline-ui service.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/connect-api.md#2025-04-10_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport kfp\n\nclient = kfp.Client(host=\"http://localhost:3000\")\n\nprint(client.list_experiments())\n```\n\n----------------------------------------\n\nTITLE: Sample Run Status Response\nDESCRIPTION: JSON response example showing what to expect when retrieving run details from the API, including status, creation time, and pipeline specification information.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/tutorials/api-pipelines.md#2025-04-10_snippet_7\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"run\": {\n    \"id\": \"4ff0debd-d6d7-4681-8593-21ec002e6e0c\",\n    \"name\": \"sequential_run\",\n    \"pipeline_spec\": {\n      \"pipeline_id\": \"d30d28d7-0bfc-4f0c-8a57-6844a8ec9742\",\n      \"pipeline_name\": \"sequential.tar.gz\",\n      \"workflow_manifest\": \"{...}\"\n    },\n    \"resource_references\": [\n      {\n        \"key\": {\n          \"type\": \"EXPERIMENT\",\n          \"id\": \"27af7eee-ce0a-44ba-a44d-07142abfc83c\"\n        },\n        \"name\": \"Default\",\n        \"relationship\": \"OWNER\"\n      }\n    ],\n    \"created_at\": \"2020-02-20T16:18:58Z\",\n    \"scheduled_at\": \"1970-01-01T00:00:00Z\",\n    \"finished_at\": \"1970-01-01T00:00:00Z\",\n    \"status\": \"Succeeded\"\n  },\n  \"pipeline_runtime\": {\n    \"workflow_manifest\": \"{...}\"\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Configuring Pod with ServiceAccount Token Volume\nDESCRIPTION: YAML configuration for a Pod with a mounted ServiceAccount token volume to access Kubeflow Pipelines API.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/connect-api.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: access-kfp-example\nspec:\n  containers:\n  - image: hello-world:latest\n    name: hello-world\n    env:\n      - ## this environment variable is automatically read by `kfp.Client()`\n        ## this is the default value, but we show it here for clarity\n        name: KF_PIPELINES_SA_TOKEN_PATH\n        value: /var/run/secrets/kubeflow/pipelines/token\n    volumeMounts:\n      - mountPath: /var/run/secrets/kubeflow/pipelines\n        name: volume-kf-pipeline-token\n        readOnly: true\n  volumes:\n    - name: volume-kf-pipeline-token\n      projected:\n        sources:\n          - serviceAccountToken:\n              path: token\n              expirationSeconds: 7200\n              ## defined by the `TOKEN_REVIEW_AUDIENCE` environment variable on the `ml-pipeline` deployment\n              audience: pipelines.kubeflow.org\n```\n\n----------------------------------------\n\nTITLE: Configuring MPI Operator for Volcano Scheduler\nDESCRIPTION: Configuration diff showing how to modify the MPI Operator deployment to use Volcano for gang-scheduling.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/job-scheduling.md#2025-04-10_snippet_5\n\nLANGUAGE: diff\nCODE:\n```\n...\n    spec:\n      containers:\n      - args:\n+       - --gang-scheduling=volcano\n        - -alsologtostderr\n        - --lock-namespace=mpi-operator\n        image: mpioperator/mpi-operator:0.4.0\n        name: mpi-operator\n...\n```\n\n----------------------------------------\n\nTITLE: Applying GCP Environment Configuration for Kubeflow Pipelines\nDESCRIPTION: Commands to apply Kubeflow Pipelines deployment with GCP Cloud SQL and Google Cloud Storage. These kubectl commands apply the customized kustomize configurations with GCP-specific settings.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/standalone-deployment.md#2025-04-10_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -k manifests/kustomize/cluster-scoped-resources\nkubectl apply -k manifests/kustomize/env/gcp\n```\n\n----------------------------------------\n\nTITLE: Retrieving Current Optimal Trial from Katib Experiment\nDESCRIPTION: A kubectl command using jsonpath to extract information about the current optimal trial from a Katib experiment. The output shows the best trial name, observation metrics, and parameter assignments for the most optimal hyperparameters found.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/hp-tuning/configure-experiment.md#2025-04-10_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get experiment random -n kubeflow -o=jsonpath='{.status.currentOptimalTrial}'\n```\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"bestTrialName\": \"random-hpsrsdqp\",\n  \"observation\": {\n    \"metrics\": [\n      {\n        \"latest\": \"0.11513\",\n        \"max\": \"0.53415\",\n        \"min\": \"0.01235\",\n        \"name\": \"loss\"\n      }\n    ]\n  },\n  \"parameterAssignments\": [\n    {\n      \"name\": \"lr\",\n      \"value\": \"0.024736875661534784\",\n    },\n    {\n      \"name\": \"momentum\",\n      \"value\": \"0.6612351235123\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Deploying KServe Inference Service with Model Registry Metadata\nDESCRIPTION: Creates a KServe inference endpoint using model metadata retrieved from Model Registry\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/model-registry/getting-started.md#2025-04-10_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom kubernetes import client\nimport kserve\n\nisvc = kserve.V1beta1InferenceService(\n    api_version=kserve.constants.KSERVE_GROUP + \"/v1beta1\",\n    kind=kserve.constants.KSERVE_KIND,\n    metadata=client.V1ObjectMeta(\n        name=\"iris-model\",\n        namespace=kserve.utils.get_default_target_namespace(),\n        labels={\n            \"modelregistry/registered-model-id\": model.id,\n            \"modelregistry/model-version-id\": version.id,\n        },\n    ),\n    spec=kserve.V1beta1InferenceServiceSpec(\n        predictor=kserve.V1beta1PredictorSpec(\n            model=kserve.V1beta1ModelSpec(\n                storage_uri=art.uri,\n                model_format=kserve.V1beta1ModelFormat(\n                    name=art.model_format_name, version=art.model_format_version\n                ),\n            )\n        )\n    ),\n)\nks_client = kserve.KServeClient()\nks_client.create(isvc)\n```\n\n----------------------------------------\n\nTITLE: Installing Standalone Model Registry\nDESCRIPTION: Deploy Model Registry as a standalone component with specific version.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/model-registry/installation.md#2025-04-10_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nMODEL_REGISTRY_VERSION={{% model-registry/latest-version %}}\nkubectl apply -k \"https://github.com/kubeflow/model-registry/manifests/kustomize/overlays/db?ref=v${MODEL_REGISTRY_VERSION}\"\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding Istio Ingressgateway for External Access\nDESCRIPTION: Command to set up port forwarding to the Istio ingressgateway service, allowing external access to Kubeflow services on port 8080.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/connect-api.md#2025-04-10_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl port-forward --namespace istio-system svc/istio-ingressgateway 8080:80\n```\n\n----------------------------------------\n\nTITLE: Port-forwarding the Kubeflow Pipelines Service\nDESCRIPTION: Commands to expose the ml-pipeline service using kubectl port-forwarding, making the API accessible on localhost for further operations.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/tutorials/api-pipelines.md#2025-04-10_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nSVC_PORT=$(kubectl -n kubeflow get svc/ml-pipeline -o json | jq \".spec.ports[0].port\")\nkubectl port-forward -n kubeflow svc/ml-pipeline ${SVC_PORT}:8888\n```\n\n----------------------------------------\n\nTITLE: Increasing gRPC Message Size Limit for Visualizations in Golang\nDESCRIPTION: Golang code snippet showing how to modify the gRPC server options to increase the maximum call receive message size from the default 4MB to 50MB specifically for the Visualization service. This allows for larger visualization content to be generated and transmitted.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/python-based-visualizations.md#2025-04-10_snippet_3\n\nLANGUAGE: golang\nCODE:\n```\nvar maxCallRecvMsgSize = 4 * 1024 * 1024\nif serviceName == \"Visualization\" {\n      // Only change the maxCallRecvMesSize if it is for visualizations\n      maxCallRecvMsgSize = 50 * 1024 * 1024\n}\nopts := []grpc.DialOption{\n      grpc.WithDefaultCallOptions(grpc.MaxCallRecvMsgSize(maxCallRecvMsgSize)),\n      grpc.WithInsecure(),\n}\n```\n\n----------------------------------------\n\nTITLE: Defining Component Types in YAML\nDESCRIPTION: Example of a component YAML definition that specifies input and output types. The component expects an Integer input and produces outputs of types GCSPath, customized_type, and GCRPath.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/static-type-checking.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nname: component a\ndescription: component desc\ninputs:\n  - {name: field_l, type: Integer}\noutputs:\n  - {name: field_m, type: {GCSPath: {openapi_schema_validator: {type: string, pattern: \"^gs://.*$\" } }}}\n  - {name: field_n, type: customized_type}\n  - {name: field_o, type: GCRPath} \nimplementation:\n  container:\n    image: gcr.io/ml-pipeline/component-a\n    command: [python3, /pipelines/component/src/train.py]\n    args: [\n      --field-l, {inputValue: field_l},\n    ]\n    fileOutputs: \n      field_m: /schema.txt\n      field_n: /feature.txt\n      field_o: /output.txt\n```\n\n----------------------------------------\n\nTITLE: Component with Custom Base Image\nDESCRIPTION: Example showing how to specify a custom base image for the component container.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/lightweight-python-components.md#2025-04-10_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n@dsl.component(base_image='python:3.8')\ndef print_py_version():\n    import sys\n    print(sys.version)\n```\n\n----------------------------------------\n\nTITLE: Compiling Pipeline Definition\nDESCRIPTION: Commands for compiling pipeline Python files to IR YAML, including examples with and without specific function names\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/cli.md#2025-04-10_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nkfp dsl compile --py path/to/pipeline.py --output path/to/output.yaml\n```\n\nLANGUAGE: shell\nCODE:\n```\nkfp dsl compile --py path/to/pipeline.py --output path/to/output.yaml --function my_pipeline\n```\n\nLANGUAGE: shell\nCODE:\n```\nkfp dsl compile --py path/to/pipeline.py --output path/to/output.yaml --function my_component\n```\n\n----------------------------------------\n\nTITLE: Configuring Application Dependencies in SparkApplication\nDESCRIPTION: Example showing how to specify both local and remote dependencies including jars and files in a SparkApplication manifest.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/writing-sparkapplication.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  deps:\n    jars:\n      - local:///opt/spark-jars/gcs-connector.jar\n    files:\n      - gs://spark-data/data-file-1.txt\n      - gs://spark-data/data-file-2.txt\n```\n\n----------------------------------------\n\nTITLE: Cloning Model Registry Repository\nDESCRIPTION: Clone the model-registry repository with a specific version tag for installation.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/model-registry/installation.md#2025-04-10_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone --depth 1 -b v{{% model-registry/latest-version %}} https://github.com/kubeflow/model-registry.git\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding for External Access to Standalone KFP\nDESCRIPTION: Command to set up port forwarding from localhost to the ml-pipeline-ui service in the Kubernetes cluster.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/connect-api.md#2025-04-10_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\n# change `--namespace` if you deployed Kubeflow Pipelines into a different namespace\nkubectl port-forward --namespace kubeflow svc/ml-pipeline-ui 3000:80\n```\n\n----------------------------------------\n\nTITLE: Querying Pipeline Runs using HTTP GET in Kubeflow\nDESCRIPTION: This HTTP GET request lists pipeline runs in the 'team-1' namespace using the Kubeflow Pipelines API. It demonstrates how to construct a query with resource reference keys for namespace filtering.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/reference/api/kubeflow-pipeline-api-spec.md#2025-04-10_snippet_0\n\nLANGUAGE: HTTP\nCODE:\n```\nGET https://kubeflow.example.com/pipeline/apis/v1beta1/runs?resource_reference_key.type=NAMESPACE&resource_reference_key.id=team-1\n```\n\n----------------------------------------\n\nTITLE: Using the Kubeflow Pipelines CLI pipeline Commands\nDESCRIPTION: The kfp pipeline commands help manage pipelines, including getting detailed information about pipelines, listing uploaded pipelines, and uploading new pipelines to a Kubeflow Pipelines cluster.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/sdk-overview.md#2025-04-10_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkfp pipeline <COMMAND>\n```\n\n----------------------------------------\n\nTITLE: Installing Spark Operator with Helm\nDESCRIPTION: Basic command to install the Spark Operator using Helm. This creates a new release with the specified name using the spark-operator chart.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/getting-started.md#2025-04-10_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nhelm install [RELEASE_NAME] spark-operator/spark-operator\n```\n\n----------------------------------------\n\nTITLE: Building Containerized Component\nDESCRIPTION: Example command for building a containerized Python component with source code from a directory\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/cli.md#2025-04-10_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nkfp component build src/ --component-filepattern my_component --push-image\n```\n\n----------------------------------------\n\nTITLE: Setting ALLOWED_ARTIFACT_DOMAIN_REGEX in Python for Multi-User Deployment\nDESCRIPTION: This Python code snippet from sync.py demonstrates how to configure the ALLOWED_ARTIFACT_DOMAIN_REGEX environment variable for user namespaces in a full-fledged Kubeflow deployment. It shows how to add an entry for the ml-pipeline-ui-artifact.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/operator-guides/server-config.md#2025-04-10_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nsync.py#L304-L310\n```\n\n----------------------------------------\n\nTITLE: Spark Application Configuration with Volcano Scheduler\nDESCRIPTION: YAML configuration for a Spark application that uses Volcano as the batch scheduler. This example defines a SparkPi application with specific resource requirements and volume mounts.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/volcano-integration.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: sparkoperator.k8s.io/v1beta2\nkind: SparkApplication\nmetadata:\n  name: spark-pi\n  namespace: default\nspec:\n  type: Scala\n  mode: cluster\n  image: spark:3.5.1\n  imagePullPolicy: Always\n  mainClass: org.apache.spark.examples.SparkPi\n  mainApplicationFile: local:///opt/spark/examples/jars/spark-examples_2.12-v3.5.1.jar\n  sparkVersion: 3.5.1\n  batchScheduler: volcano # Note: the batch scheduler name must be specified with `volcano`\n  restartPolicy:\n    type: Never\n  volumes:\n    - name: test-volume\n      hostPath:\n        path: /tmp\n        type: Directory\n  driver:\n    cores: 1\n    coreLimit: 1200m\n    memory: 512m\n    labels:\n      version: 3.5.1\n    serviceAccount: spark\n    volumeMounts:\n      - name: test-volume\n        mountPath: /tmp\n  executor:\n    cores: 1\n    instances: 1\n    memory: 512m\n    labels:\n      version: 3.5.1\n    volumeMounts:\n      - name: test-volume\n        mountPath: \"/tmp\"\n```\n\n----------------------------------------\n\nTITLE: Removing Resource Requirements from Metrics Collector\nDESCRIPTION: YAML configuration showing how to set negative values for resource requests and limits to prevent the metrics collector from requesting specific resources from the Kubernetes cluster.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/katib-config.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nresources:\n  requests:\n    cpu: -1\n    memory: -1\n    ephemeral-storage: -1\n  limits:\n    cpu: -1\n    memory: -1\n    ephemeral-storage: -1\n```\n\n----------------------------------------\n\nTITLE: Using the Kubeflow Pipelines CLI diagnose_me Command\nDESCRIPTION: The kfp diagnose_me command runs environment diagnostics with specified parameters, allowing for JSON output, namespace specification, and GCP project identification.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/sdk-overview.md#2025-04-10_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkfp diagnose_me\n```\n\n----------------------------------------\n\nTITLE: Adding Local Bin to PATH\nDESCRIPTION: Command to add the local bin directory to PATH, necessary when the SDK is installed with the --user option. This ensures that the installed binaries are accessible.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/install-sdk.md#2025-04-10_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nexport PATH=$PATH:~/.local/bin\n```\n\n----------------------------------------\n\nTITLE: Configuring Basic Executor Specification in SparkApplication\nDESCRIPTION: This YAML snippet shows how to configure basic executor properties in a SparkApplication resource, including cores, instances, memory, labels, and service account.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/writing-sparkapplication.md#2025-04-10_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  executor:\n    cores: 1\n    instances: 1\n    memory: 512m\n    labels:\n      version: 3.1.1\n    serviceAccount: spark\n```\n\n----------------------------------------\n\nTITLE: Modifying Katib Experiment with kubectl\nDESCRIPTION: Command to edit a running Katib experiment using kubectl to modify parameters like parallelTrialCount, maxTrialCount, and maxFailedTrialCount.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/resume-experiment.md#2025-04-10_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nkubectl edit experiment <experiment-name> -n <experiment-namespace>\n```\n\n----------------------------------------\n\nTITLE: Configuring External Links in Kubeflow Dashboard\nDESCRIPTION: Example of configuring external links in the Kubeflow dashboard using a ConfigMap. Shows how to add a link to the Kubeflow website with custom icon and display text.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/central-dash/customize.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: centraldashboard-config\n  namespace: kubeflow\ndata:\n  settings: |-\n    ...\n  links: |-\n    {\n      \"menuLinks\": [\n        ...\n      ],\n      \"externalLinks\": [\n        {\n          \"type\": \"item\",\n          \"iframe\": false,\n          \"text\": \"Kubeflow Website\",\n          \"link\": \"https://www.kubeflow.org/\",\n          \"icon\": \"launch\"\n        }\n      ],\n      \"quickLinks\": [\n        ...\n      ],\n      \"documentationItems\": [\n        ...\n      ]\n    }\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Suggestion Service in Python for Katib\nDESCRIPTION: This code snippet demonstrates how to create a custom suggestion service for Katib by implementing the SuggestionServicer interface. It includes methods for validating algorithm settings and generating suggestions based on the experiment's search space and previous trials.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/hp-tuning/configure-algorithm.md#2025-04-10_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom pkg.apis.manager.v1beta1.python import api_pb2\nfrom pkg.apis.manager.v1beta1.python import api_pb2_grpc\nfrom pkg.suggestion.v1beta1.internal.search_space import HyperParameter, HyperParameterSearchSpace\nfrom pkg.suggestion.v1beta1.internal.trial import Trial, Assignment\nfrom pkg.suggestion.v1beta1.hyperopt.base_service import BaseHyperoptService\nfrom pkg.suggestion.v1beta1.internal.base_health_service import HealthServicer\n\n\n# Inherit SuggestionServicer and implement GetSuggestions.\nclass HyperoptService(\n        api_pb2_grpc.SuggestionServicer, HealthServicer):\n    def ValidateAlgorithmSettings(self, request, context):\n        # Optional, it is used to validate algorithm settings defined by users.\n        pass\n    def GetSuggestions(self, request, context):\n        # Convert the Experiment in GRPC request to the search space.\n        # search_space example:\n        #   HyperParameterSearchSpace(\n        #       goal: MAXIMIZE,\n        #       params: [HyperParameter(name: param-1, type: INTEGER, min: 1, max: 5, step: 0),\n        #                HyperParameter(name: param-2, type: CATEGORICAL, list: cat1, cat2, cat3),\n        #                HyperParameter(name: param-3, type: DISCRETE, list: 3, 2, 6),\n        #                HyperParameter(name: param-4, type: DOUBLE, min: 1, max: 5, step: )]\n        #   )\n        search_space = HyperParameterSearchSpace.convert(request.experiment)\n        # Convert the Trials in GRPC request to the Trials in algorithm side.\n        # Trials example:\n        #   [Trial(\n        #       assignment: [Assignment(name=param-1, value=2),\n        #                    Assignment(name=param-2, value=cat1),\n        #                    Assignment(name=param-3, value=2),\n        #                    Assignment(name=param-4, value=3.44)],\n        #       target_metric: Metric(name=\"metric-2\" value=\"5643\"),\n        #       additional_metrics: [Metric(name=metric-1, value=435),\n        #                            Metric(name=metric-3, value=5643)],\n        #   Trial(\n        #       assignment: [Assignment(name=param-1, value=3),\n        #                    Assignment(name=param-2, value=cat2),\n        #                    Assignment(name=param-3, value=6),\n        #                    Assignment(name=param-4, value=4.44)],\n        #       target_metric: Metric(name=\"metric-2\" value=\"3242\"),\n        #       additional_metrics: [Metric(name=metric=1, value=123),\n        #                            Metric(name=metric-3, value=543)],\n        trials = Trial.convert(request.trials)\n        #--------------------------------------------------------------\n        # Your code here\n        # Implement the logic to generate new assignments for the given current request number.\n        # For example, if request.current_request_number is 2, you should return:\n        # [\n        #   [Assignment(name=param-1, value=3),\n        #    Assignment(name=param-2, value=cat2),\n        #    Assignment(name=param-3, value=3),\n        #    Assignment(name=param-4, value=3.22)\n        #   ],\n        #   [Assignment(name=param-1, value=4),\n        #    Assignment(name=param-2, value=cat4),\n        #    Assignment(name=param-3, value=2),\n        #    Assignment(name=param-4, value=4.32)\n        #   ],\n        # ]\n        list_of_assignments = your_logic(search_space, trials, request.current_request_number)\n        #--------------------------------------------------------------\n        # Convert list_of_assignments to\n        return api_pb2.GetSuggestionsReply(\n            trials=Assignment.generate(list_of_assignments)\n        )\n```\n\n----------------------------------------\n\nTITLE: Configuring ClusterRole Rules for Tekton Pipelines in YAML\nDESCRIPTION: YAML configuration for adding Tekton pipeline permissions to Katib controller ClusterRole. This allows Katib to access PipelineRuns and TaskRuns resources.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/trial-template.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\n- apiGroups:\n    - tekton.dev\n  resources:\n    - pipelineruns\n    - taskruns\n  verbs:\n    - \"get\"\n    - \"list\"\n    - \"watch\"\n    - \"create\"\n    - \"delete\"\n```\n\n----------------------------------------\n\nTITLE: Default Resource Configuration for Metrics Collector\nDESCRIPTION: YAML configuration showing the default resource requests and limits for the StdOut metrics collector container in Katib.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/katib-config.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nmetricsCollectors:\n  - kind: StdOut\n    image: docker.io/kubeflowkatib/file-metrics-collector:latest\n    resources:\n      requests:\n        cpu: 50m\n        memory: 10Mi\n        ephemeral-storage: 500Mi\n      limits:\n        cpu: 500m\n        memory: 100Mi\n        ephemeral-storage: 5Gi\n```\n\n----------------------------------------\n\nTITLE: Using the Kubeflow Pipelines CLI run Commands\nDESCRIPTION: The kfp run commands help manage pipeline runs, including displaying run details, listing recent runs, and submitting new pipeline runs.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/sdk-overview.md#2025-04-10_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkfp run <COMMAND>\n```\n\n----------------------------------------\n\nTITLE: Command-line Invocation for Text Line Extractor\nDESCRIPTION: The bash command demonstrating how to execute the sample Python program with appropriate command-line arguments for input path, parameter value, and output path.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/component-development.md#2025-04-10_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython3 program.py --input1-path <path-to-the-input-file> \\\n  --param1 <number-of-lines-to-read> \\\n  --output1-path <path-to-write-the-output-to> \n```\n\n----------------------------------------\n\nTITLE: Comprehensive KFP Storage Provider Overrides Configuration\nDESCRIPTION: Complete YAML configuration demonstrating provider overrides for both GCS and S3, including multiple path-specific configurations with different credentials and endpoints.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/operator-guides/configure-object-store.md#2025-04-10_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\ngs:\n  default:\n    credentials:\n      fromEnv: false\n      secretRef:\n        secretName: gs-secret-1\n        tokenKey: gs-tokenKey\n  overrides:\n    # Matches pipeline root: gs://your-bucket/some/subfolder\n    - bucketName: your-bucket\n      keyPrefix: some/subfolder\n      credentials:\n        fromEnv: false\n        secretRef:\n          secretName: gcs-secret-2\n          tokenKey: gs-tokenKey-2\n    # Matches pipeline root: gs://your-bucket/some/othersubfolder\n    - bucketName: your-bucket\n      keyPrefix: some/othersubfolder\n      credentials:\n        fromEnv: true\ns3:\n  default:\n    endpoint: http://some-s3-compliant-store-endpoint.com\n    disableSSL: true\n    region: minio\n    credentials:\n      fromEnv: false\n      secretRef:\n        secretName: your-secret\n        accessKeyKey: accesskey\n        secretKeyKey: secretkey\n  overrides:\n    # Matches pipeline root: s3://your-bucket/subfolder\n    # aws-s3-creds secret is used for static credentials\n    - bucketName: your-bucket\n      keyPrefix: subfolder\n      endpoint: s3.amazonaws.com\n      region: us-east-2\n      disableSSL: false\n      credentials:\n        fromEnv: false\n        secretRef:\n          secretName: aws-s3-creds\n          accessKeyKey: AWS_ACCESS_KEY_ID\n          secretKeyKey: AWS_SECRET_ACCESS_KEY\n    # Matches pipeline root: s3://your-bucket/some/s3/path/a/b\n    - bucketName: your-bucket\n      keyPrefix: some/s3/path/a/b\n      endpoint: s3.amazonaws.com\n      region: us-east-2\n      credentials:\n        fromEnv: true\n    # Matches pipeline root: s3://your-bucket/some/s3/path/a/c\n    - bucketName: your-bucket\n      keyPrefix: some/s3/path/a/c\n      endpoint: s3.amazonaws.com\n      region: us-east-2\n      credentials:\n        fromEnv: false\n        secretRef:\n          secretName: aws-s3-creds\n          accessKeyKey: AWS_ACCESS_KEY_ID\n          secretKeyKey: AWS_SECRET_ACCESS_KEY\n    # Matches pipeline root: s3://your-bucket/some/s3/path/b/a\n    - bucketName: your-bucket\n      keyPrefix: some/s3/path/b/a\n      endpoint: https://s3.amazonaws.com\n      region: us-east-2\n      credentials:\n        fromEnv: false\n        secretRef:\n          secretName: aws-s3-creds\n          accessKeyKey: AWS_ACCESS_KEY_ID\n          secretKeyKey: AWS_SECRET_ACCESS_KEY\n```\n\n----------------------------------------\n\nTITLE: Specifying NotebookSpec in YAML\nDESCRIPTION: YAML structure for the NotebookSpec, which defines the desired state of a Notebook. It includes a template field of type NotebookTemplateSpec.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/notebooks/api-reference/notebook-v1.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  template:\n    spec:\n      # Kubernetes core/v1.PodSpec fields\n```\n\n----------------------------------------\n\nTITLE: Inserting SVG Image in Markdown\nDESCRIPTION: This snippet demonstrates how to insert an SVG image in Markdown with specific styling and alt text for the distributed PyTorchJob diagram.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/reference/distributed-training.md#2025-04-10_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n<img src=\"/docs/components/trainer/legacy-v1/images/distributed-pytorchjob.drawio.svg\"\n  alt=\"Distributed PyTorchJob\"\n  class=\"mt-3 mb-3 border rounded p-3 bg-white\">\n```\n\n----------------------------------------\n\nTITLE: Checking Available Training Runtimes\nDESCRIPTION: Code to list available Kubeflow Training Runtimes using the TrainerClient.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/getting-started.md#2025-04-10_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom kubeflow.trainer import TrainerClient, CustomTrainer\n\nfor r in TrainerClient().list_runtimes():\n    print(f\"Runtime: {r.name}\")\n```\n\n----------------------------------------\n\nTITLE: Building and Pushing Container Image for Kubeflow Pipeline Component\nDESCRIPTION: This bash script builds a Docker container image, pushes it to a container registry, and outputs the strict image name. It's used to prepare a containerized component for use in Kubeflow Pipelines.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/component-development.md#2025-04-10_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash -e\nimage_name=gcr.io/my-org/my-image\nimage_tag=latest\nfull_image_name=${image_name}:${image_tag}\n\ncd \"$(dirname \"$0\")\" \ndocker build -t \"${full_image_name}\" .\ndocker push \"$full_image_name\"\n\n# Output the strict image name, which contains the sha256 image digest\ndocker inspect --format=\"{{index .RepoDigests 0}}\" \"${full_image_name}\"\n```\n\n----------------------------------------\n\nTITLE: Runtime Parameters Structure in Katib Config\nDESCRIPTION: YAML structure showing how runtime parameters are organized in the Katib Config. It includes configurations for metrics collectors, suggestions, and early stopping algorithms.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/katib-config.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: config.kubeflow.org/v1beta1\nkind: KatibConfig\nruntime:\n  metricsCollectors:\n    - kind: StdOut\n      image: docker.io/kubeflowkatib/file-metrics-collector:latest\n    ...\n  suggestions:\n    - algorithmName: random\n      image: docker.io/kubeflowkatib/suggestion-hyperopt:latest\n    ...\n  earlyStoppings:\n    - algorithmName: medianstop\n      image: docker.io/kubeflowkatib/earlystopping-medianstop:latest\n    ...\n```\n\n----------------------------------------\n\nTITLE: Installing KFP SDK with Docker Support\nDESCRIPTION: Command to install KFP SDK with additional Docker dependencies for containerized components\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/cli.md#2025-04-10_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\npip install kfp[all]\n```\n\n----------------------------------------\n\nTITLE: Example S3 Provider Override Configuration\nDESCRIPTION: Sample YAML configuration showing a specific S3 provider override for a particular bucket path.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/operator-guides/configure-object-store.md#2025-04-10_snippet_9\n\nLANGUAGE: yaml\nCODE:\n```\n- bucketName: your-bucket\n  keyPrefix: some/s3/path/b/a\n  endpoint: https://s3.amazonaws.com\n  region: us-east-2\n  credentials:\n    fromEnv: false\n    secretRef:\n      secretName: aws-s3-creds\n      accessKeyKey: AWS_ACCESS_KEY_ID\n      secretKeyKey: AWS_SECRET_ACCESS_KEY\n```\n\n----------------------------------------\n\nTITLE: Installing Training SDK from Specific GitHub Commit\nDESCRIPTION: This pip command demonstrates how to install the Kubeflow Training SDK from a specific GitHub commit.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/installation.md#2025-04-10_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\npip install git+https://github.com/kubeflow/training-operator.git@7345e33b333ba5084127efe027774dd7bed8f6e6#subdirectory=sdk/python\n```\n\n----------------------------------------\n\nTITLE: Monitoring Pod Status\nDESCRIPTION: Check the status of Model Registry pods in the specified namespace.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/model-registry/installation.md#2025-04-10_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get pods -n $PROFILE_NAME -w\n```\n\n----------------------------------------\n\nTITLE: S3 Provider Configuration for KFP Launcher\nDESCRIPTION: ConfigMap configuration for KFP Launcher using AWS S3 with static credentials, including endpoint and region settings.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/operator-guides/configure-object-store.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\ndata:\n  defaultPipelineRoot: s3://mlpipeline\n  providers: |-\n    s3:\n      default:\n        endpoint: s3.amazonaws.com\n        disableSSL: false\n        region: us-east-2\n        credentials:\n          fromEnv: false\n          secretRef:\n            secretName: your-k8s-secret\n            accessKeyKey: some-key-1\n            secretKeyKey: some-key-2\nkind: ConfigMap\nmetadata:\n  name: kfp-launcher\n  namespace: user-namespace\n```\n\n----------------------------------------\n\nTITLE: Configuring Scratch Space Volume in Spark Application YAML\nDESCRIPTION: This YAML snippet shows how to configure a volume for scratch space in a Spark application, using a hostPath volume as an example.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/writing-sparkapplication.md#2025-04-10_snippet_22\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  volumes:\n    - name: \"spark-local-dir-1\"\n      hostPath:\n        path: \"/tmp/spark-local-dir\"\n  executor:\n    volumeMounts:\n      - name: \"spark-local-dir-1\"\n        mountPath: \"/tmp/spark-local-dir\"\n    ...\n```\n\n----------------------------------------\n\nTITLE: Creating a Dockerfile for Kubeflow Pipeline Component\nDESCRIPTION: This Dockerfile sets up a Python 3.7 environment, installs the Keras package, and copies source files into the container. It serves as a base for creating a containerized component for Kubeflow Pipelines.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/component-development.md#2025-04-10_snippet_3\n\nLANGUAGE: dockerfile\nCODE:\n```\nFROM python:3.7\nRUN python3 -m pip install keras\nCOPY ./src /pipelines/component/src\n```\n\n----------------------------------------\n\nTITLE: Accessing MPI Job Logs from Launcher Pod\nDESCRIPTION: Commands to retrieve the pod name of the launcher and access its logs. This allows you to monitor the training progress in real-time.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/mpi.md#2025-04-10_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nPODNAME=$(kubectl get pods -l mpi_job_name=tensorflow-benchmarks,mpi_role_type=launcher -o name)\nkubectl logs -f ${PODNAME}\n```\n\n----------------------------------------\n\nTITLE: Creating Python Virtual Environment for KFP-Tekton\nDESCRIPTION: Commands to create a Python virtual environment for the KFP-Tekton SDK using the Python 3 venv module. This sets up an isolated environment for working with the Kubeflow Pipelines SDK for Tekton.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/pipelines-with-tekton.md#2025-04-10_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\npython3 -m venv .venv-kfp-tekton\nsource .venv-kfp-tekton/bin/activate\n```\n\n----------------------------------------\n\nTITLE: Setting GCS Credentials in Python Pipeline Task\nDESCRIPTION: Python code example showing how to set GCS credentials directly in a pipeline task using environment variables and secret volume mounting.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/operator-guides/configure-object-store.md#2025-04-10_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Specify the default APP Credential path\nyour_task.set_env_variable(name='GOOGLE_APPLICATION_CREDENTIALS', value='/gcloud/credentials.json')\n# Mount the GCS Credentials JSON\nkubernetes.use_secret_as_volume(your_task, secret_name='gcs-secret', mount_path='/gcloud')\n```\n\n----------------------------------------\n\nTITLE: Creating Contributor AuthorizationPolicy for Kubeflow Profiles in YAML\nDESCRIPTION: YAML template for creating an AuthorizationPolicy that grants a user access to a Kubeflow profile. This configuration defines access rules using Istio's security framework, allowing specified users to interact with the profile through both Kubeflow notebooks and pipelines. The policy uses principal definitions and header-based authentication to control access.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/central-dash/profiles.md#2025-04-10_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: user-<SAFE_USER_EMAIL>-clusterrole-<USER_ROLE>\n  namespace: <PROFILE_NAME>\n  annotations:\n    role: <USER_ROLE>\n    user: <RAW_USER_EMAIL>\nspec:\n  rules:\n    - from:\n        - source:\n            ## for more information see the KFAM code:\n            ## https://github.com/kubeflow/kubeflow/blob/v1.8.0/components/access-management/kfam/bindings.go#L79-L110\n            principals:\n              ## required for kubeflow notebooks\n              ## TEMPLATE: \"cluster.local/ns/<ISTIO_GATEWAY_NAMESPACE>/sa/<ISTIO_GATEWAY_SERVICE_ACCOUNT>\"\n              - \"cluster.local/ns/istio-system/sa/istio-ingressgateway-service-account\"\n\n              ## required for kubeflow pipelines\n              ## TEMPLATE: \"cluster.local/ns/<KUBEFLOW_NAMESPACE>/sa/<KFP_UI_SERVICE_ACCOUNT>\"\n              - \"cluster.local/ns/kubeflow/sa/ml-pipeline-ui\"\n      when:\n        - key: request.headers[kubeflow-userid]\n          values:\n            - <RAW_USER_EMAIL>\n```\n\n----------------------------------------\n\nTITLE: Creating a Pipeline Run\nDESCRIPTION: Command to trigger a pipeline run using the pipeline ID via a POST request to the runs API endpoint.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/tutorials/api-pipelines.md#2025-04-10_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nRUN_ID=$((curl -H \"Content-Type: application/json\" -X POST ${SVC}/apis/v1beta1/runs \\\n-d @- << EOF\n{\n   \"name\":\"${PIPELINE_NAME}_run\",\n   \"pipeline_spec\":{\n      \"pipeline_id\":\"${PIPELINE_ID}\"\n   }\n}\nEOF\n) | jq -r .run.id)\n```\n\n----------------------------------------\n\nTITLE: Viewing Notebook Pod Logs\nDESCRIPTION: Command to retrieve the logs from a Notebook's Pod for troubleshooting. Logs often contain critical information about container startup failures or runtime errors.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/notebooks/troubleshooting.md#2025-04-10_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nkubectl logs \"${MY_NOTEBOOK_NAME}-0\" --namespace \"${MY_PROFILE_NAMESPACE}\"\n```\n\n----------------------------------------\n\nTITLE: Setting Termination Grace Period in Spark Application YAML\nDESCRIPTION: This YAML snippet demonstrates how to set a termination grace period for the driver pod in a Spark application.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/writing-sparkapplication.md#2025-04-10_snippet_23\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  driver:\n    terminationGracePeriodSeconds: 60\n```\n\n----------------------------------------\n\nTITLE: Configuring TFJob with Google Cloud Credentials in Kubernetes\nDESCRIPTION: This YAML snippet demonstrates how to configure a TFJob to use Google Cloud credentials. It shows how to mount a secret containing the credentials and set the necessary environment variables for authentication.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/tensorflow.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: kubeflow.org/v1\nkind: TFJob\nmetadata:\n  generateName: tfjob\n  namespace: your-user-namespace\nspec:\n  tfReplicaSpecs:\n    PS:\n      replicas: 1\n      restartPolicy: OnFailure\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n        spec:\n          containers:\n            - name: tensorflow\n              image: gcr.io/your-project/your-image\n              command:\n                - python\n                - -m\n                - trainer.task\n                - --batch_size=32\n                - --training_steps=1000\n              env:\n                - name: GOOGLE_APPLICATION_CREDENTIALS\n                  value: \"/etc/secrets/user-gcp-sa.json\"\n              volumeMounts:\n                - name: sa\n                  mountPath: \"/etc/secrets\"\n                  readOnly: true\n          volumes:\n            - name: sa\n              secret:\n                secretName: user-gcp-sa\n    Worker:\n      replicas: 1\n      restartPolicy: OnFailure\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n        spec:\n          containers:\n            - name: tensorflow\n              image: gcr.io/your-project/your-image\n              command:\n                - python\n                - -m\n                - trainer.task\n                - --batch_size=32\n                - --training_steps=1000\n              env:\n                - name: GOOGLE_APPLICATION_CREDENTIALS\n                  value: \"/etc/secrets/user-gcp-sa.json\"\n              volumeMounts:\n                - name: sa\n                  mountPath: \"/etc/secrets\"\n                  readOnly: true\n          volumes:\n            - name: sa\n              secret:\n                secretName: user-gcp-sa\n```\n\n----------------------------------------\n\nTITLE: MPI Job Configuration and Status YAML Output\nDESCRIPTION: Example YAML output showing a complete MPI job configuration with launcher and worker specifications, along with status conditions indicating job completion.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/mpi.md#2025-04-10_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: kubeflow.org/v2beta1\nkind: MPIJob\nmetadata:\n  creationTimestamp: \"2019-07-09T22:15:51Z\"\n  generation: 1\n  name: tensorflow-benchmarks\n  namespace: default\n  resourceVersion: \"5645868\"\n  selfLink: /apis/kubeflow.org/v1alpha2/namespaces/default/mpijobs/tensorflow-benchmarks\n  uid: 1c5b470f-a297-11e9-964d-88d7f67c6e6d\nspec:\n  runPolicy:\n    cleanPodPolicy: Running\n  mpiReplicaSpecs:\n    Launcher:\n      replicas: 1\n      template:\n        spec:\n          containers:\n          - command:\n            - mpirun\n            - --allow-run-as-root\n            - -np\n            - \"2\"\n            - -bind-to\n            - none\n            - -map-by\n            - slot\n            - -x\n            - NCCL_DEBUG=INFO\n            - -x\n            - LD_LIBRARY_PATH\n            - -x\n            - PATH\n            - -mca\n            - pml\n            - ob1\n            - -mca\n            - btl\n            - ^openib\n            - python\n            - scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py\n            - --model=resnet101\n            - --batch_size=64\n            - --variable_update=horovod\n            image: mpioperator/tensorflow-benchmarks:latest\n            name: tensorflow-benchmarks\n    Worker:\n      replicas: 1\n      template:\n        spec:\n          containers:\n          - image: mpioperator/tensorflow-benchmarks:latest\n            name: tensorflow-benchmarks\n            resources:\n              limits:\n                nvidia.com/gpu: 2\n  slotsPerWorker: 2\nstatus:\n  completionTime: \"2019-07-09T22:17:06Z\"\n  conditions:\n  - lastTransitionTime: \"2019-07-09T22:15:51Z\"\n    lastUpdateTime: \"2019-07-09T22:15:51Z\"\n    message: MPIJob default/tensorflow-benchmarks is created.\n    reason: MPIJobCreated\n    status: \"True\"\n    type: Created\n  - lastTransitionTime: \"2019-07-09T22:15:54Z\"\n    lastUpdateTime: \"2019-07-09T22:15:54Z\"\n    message: MPIJob default/tensorflow-benchmarks is running.\n    reason: MPIJobRunning\n    status: \"False\"\n    type: Running\n  - lastTransitionTime: \"2019-07-09T22:17:06Z\"\n    lastUpdateTime: \"2019-07-09T22:17:06Z\"\n    message: MPIJob default/tensorflow-benchmarks successfully completed.\n    reason: MPIJobSucceeded\n    status: \"True\"\n    type: Succeeded\n  replicaStatuses:\n    Launcher:\n      succeeded: 1\n    Worker: {}\n  startTime: \"2019-07-09T22:15:51Z\"\n```\n\n----------------------------------------\n\nTITLE: Installing KFP-Tekton SDK from PyPi\nDESCRIPTION: Command to install the latest release of the KFP-Tekton compiler from PyPi using pip. This installs the necessary Python packages to compile and run Kubeflow Pipelines on a Tekton backend.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/pipelines-with-tekton.md#2025-04-10_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\npip3 install kfp-tekton --upgrade\n```\n\n----------------------------------------\n\nTITLE: KFP Launcher Custom Path Configuration\nDESCRIPTION: ConfigMap configuration for KFP Launcher with a custom artifact storage path in the Minio bucket.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/operator-guides/configure-object-store.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kfp-launcher\n  namespace: user-namespace\ndata:\n  defaultPipelineRoot: \"minio://mlpipeline/some/other/path\"\n```\n\n----------------------------------------\n\nTITLE: Disabling Type Checking Per-Argument\nDESCRIPTION: Example of selectively disabling type checking for specific arguments in a pipeline. This is useful when components have type differences that should be allowed, like when a downstream component can accept multiple input types.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/static-type-checking.md#2025-04-10_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n@dsl.pipeline(name='type_check_a', description='')\ndef pipeline():\n  a = task_factory_a(field_l=12)\n  # For each of the arguments, you can also ignore the types by calling\n  # ignore_type function.\n  b = task_factory_b(\n      field_x=a.outputs['field_n'],\n      field_y=a.outputs['field_o'],\n      field_z=a.outputs['field_m'].ignore_type())\n\ncompiler.Compiler().compile(pipeline, 'pipeline.tar.gz', type_check=True)\n```\n\n----------------------------------------\n\nTITLE: Building Multi-architecture Spark Operator Docker Image\nDESCRIPTION: This command builds and pushes a multi-architecture Spark Operator Docker image to a specified registry. It requires docker buildx and supports both linux/amd64 and linux/arm64 platforms.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/customizing-spark-operator.md#2025-04-10_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmake docker-build IMAGE_REGISTRY=docker.io IMAGE_REPOSITORY=kubeflow/spark-operator IMAGE_TAG=latest PLATFORMS=linux/amd64,linux/arm64\n```\n\n----------------------------------------\n\nTITLE: Configuring Container Lifecycle Hooks for Spark Driver in YAML\nDESCRIPTION: Demonstrates how to specify a preStop lifecycle hook for a Spark driver container using YAML. This example shows a hook that touches a file and sleeps before termination.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/writing-sparkapplication.md#2025-04-10_snippet_24\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  driver:\n    lifecycle:\n      preStop:\n        exec:\n          command:\n          - /bin/bash\n          - -c\n          - touch /var/run/killspark && sleep 65\n```\n\n----------------------------------------\n\nTITLE: Configuring Security Context in Spark Application YAML\nDESCRIPTION: This YAML snippet shows how to configure security contexts for both the container and pod levels in a Spark application for driver and executor pods.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/writing-sparkapplication.md#2025-04-10_snippet_18\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  driver:\n    podSecurityContext:\n      runAsUser: 1000\n    securityContext:\n      allowPrivilegeEscalation: false\n      runAsUser: 2000\n  executor:\n    podSecurityContext:\n      runAsUser: 1000\n    securityContext:\n      allowPrivilegeEscalation: false\n      runAsUser: 2000\n```\n\n----------------------------------------\n\nTITLE: Retrieving MPI Job Status with kubectl\nDESCRIPTION: Command to get the YAML output of an MPI job status using kubectl. This allows you to monitor the job's configuration and current state.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/mpi.md#2025-04-10_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get -o yaml mpijobs tensorflow-benchmarks\n```\n\n----------------------------------------\n\nTITLE: Accessing Kubeflow Pipelines UI URL\nDESCRIPTION: This command retrieves the public URL for the Kubeflow Pipelines UI by examining the inverse-proxy-config ConfigMap in the kubeflow namespace.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/standalone-deployment.md#2025-04-10_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl describe configmap inverse-proxy-config -n kubeflow | grep googleusercontent.com\n```\n\n----------------------------------------\n\nTITLE: Compiling Pipeline Directly with Python\nDESCRIPTION: Command to compile a Kubeflow Pipeline by directly executing a Python script that contains the compilation code in a __main__ method. This produces a Tekton YAML file in the same directory.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/pipelines-with-tekton.md#2025-04-10_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npython3 pipeline.py\n```\n\n----------------------------------------\n\nTITLE: Disabling Type Checking in Pipeline Compilation\nDESCRIPTION: Examples of how to disable type checking when compiling a pipeline, either programmatically or using the command-line dsl-compiler tool.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/static-type-checking.md#2025-04-10_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncompiler.Compiler().compile(pipeline_a, 'pipeline_a.tar.gz', type_check=False)\n```\n\n----------------------------------------\n\nTITLE: Installing Training SDK with HuggingFace Support\nDESCRIPTION: This pip command installs the Kubeflow Training SDK with additional packages from HuggingFace for LLM fine-tuning capabilities.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/installation.md#2025-04-10_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\npip install -U \"kubeflow-training[huggingface]\"\n```\n\n----------------------------------------\n\nTITLE: Applying Istio Compatibility\nDESCRIPTION: Configure Istio compatibility for Model Registry deployment.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/model-registry/installation.md#2025-04-10_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nMODEL_REGISTRY_VERSION={{% model-registry/latest-version %}}\nkubectl apply -k \"https://github.com/kubeflow/model-registry/manifests/kustomize/options/istio?ref=v${MODEL_REGISTRY_VERSION}\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Python Support for Spark Applications in YAML\nDESCRIPTION: Shows how to enable Python support in a Spark application by specifying the main application file and Python version. It also demonstrates how to include additional Python dependencies.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/writing-sparkapplication.md#2025-04-10_snippet_25\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  type: Python\n  pythonVersion: 2\n  mainApplicationFile: local:///opt/spark/examples/src/main/python/pyfiles.py\n```\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  deps:\n    pyFiles:\n       - local:///opt/spark/examples/src/main/python/py_container_checks.py\n       - gs://spark-data/python-dep.zip\n```\n\n----------------------------------------\n\nTITLE: Deploying an MPI Job\nDESCRIPTION: Command to deploy an MPIJob resource to start training\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/mpi.md#2025-04-10_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -f examples/v2beta1/tensorflow-benchmarks/tensorflow-benchmarks.yaml\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding Kubeflow Pipelines UI\nDESCRIPTION: Command to set up port forwarding for accessing the Kubeflow Pipelines UI locally through port 8080.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/operator-guides/installation/_index.md#2025-04-10_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl port-forward -n kubeflow svc/ml-pipeline-ui 8080:80\n```\n\n----------------------------------------\n\nTITLE: Compiling KFP DSL Scripts Using Command Line\nDESCRIPTION: Example of using the dsl-compile-tekton command line tool to compile a Kubeflow Pipeline DSL Python script into a Tekton PipelineRun YAML specification. This converts pipeline code into a format that can be executed by the Tekton backend.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/pipelines-with-tekton.md#2025-04-10_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\ndsl-compile-tekton \\\n    --py sdk/python/tests/compiler/testdata/parallel_join.py \\\n    --output pipeline.yaml\n```\n\n----------------------------------------\n\nTITLE: Compiling Kubeflow Pipeline with DSL Compiler\nDESCRIPTION: Commands for compiling a Python pipeline definition into a deployable .tar.gz format using the dsl-compile tool. Includes both generic and specific examples using the sequential.py sample.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/tutorials/build-pipeline.md#2025-04-10_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndsl-compile --py [path/to/python/file] --output [path/to/output/tar.gz]\n```\n\nLANGUAGE: bash\nCODE:\n```\nexport DIR=[YOUR PIPELINES REPO DIRECTORY]/samples/core/sequential\ndsl-compile --py ${DIR}/sequential.py --output ${DIR}/sequential.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Listing Kubeflow Profiles with kubectl\nDESCRIPTION: Command to list all existing profiles in a Kubeflow cluster using kubectl. This is useful for administrators to view all available profiles.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/central-dash/profiles.md#2025-04-10_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get profiles\n```\n\n----------------------------------------\n\nTITLE: Sample Kustomization YAML for Kubeflow Pipelines Deployment\nDESCRIPTION: Example kustomization.yaml file for deploying Kubeflow Pipelines. This configuration sets the namespace to 'kubeflow' and uses the development environment from the Kubeflow Pipelines GitHub repository.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/standalone-deployment.md#2025-04-10_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\n# kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n# Edit the following to change the deployment to your custom namespace.\nnamespace: kubeflow\n# You can add other customizations here using kustomize.\n# Edit ref in the following link to deploy a different version of Kubeflow Pipelines.\nbases:\n- github.com/kubeflow/pipelines/manifests/kustomize/env/dev?ref={{% pipelines/latest-version %}}\n```\n\n----------------------------------------\n\nTITLE: Recommended Directory Structure for Kubeflow Pipeline Components\nDESCRIPTION: Shows the recommended file and directory organization for Kubeflow Pipeline components. This structure includes source code, tests, documentation, Dockerfile, build scripts, and component definition files.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/component-development.md#2025-04-10_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ncomponents/<component group>/<component name>/\n\n    src/*            # Component source code files\n    tests/*          # Unit tests\n    run_tests.sh     # Small script that runs the tests\n    README.md        # Documentation. If multiple files are needed, move to docs/.\n\n    Dockerfile       # Dockerfile to build the component container image\n    build_image.sh   # Small script that runs docker build and docker push\n\n    component.yaml   # Component definition in YAML format\n```\n\n----------------------------------------\n\nTITLE: Installing Spark Operator with Mutating Admission Webhook\nDESCRIPTION: Command to install the Spark Operator with the mutating admission webhook enabled. The webhook allows for Spark pod customization at creation time.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/getting-started.md#2025-04-10_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nhelm install my-release spark-operator/spark-operator \\\n    --namespace spark-operator \\\n    --create-namespace \\\n    --set webhook.enable=true\n```\n\n----------------------------------------\n\nTITLE: Basic KFP Launcher ConfigMap Configuration\nDESCRIPTION: Simple ConfigMap configuration for KFP Launcher showing the default pipeline root setting in the user namespace.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/operator-guides/configure-object-store.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kfp-launcher\n  namespace: user-namespace\ndata:\n  defaultPipelineRoot: \"\"\n```\n\n----------------------------------------\n\nTITLE: Checking PyTorchJob Pods Status\nDESCRIPTION: Command to list all pods associated with a specific PyTorchJob training instance.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/pytorch.md#2025-04-10_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pods -l training.kubeflow.org/job-name=pytorch-simple -n kubeflow\n```\n\n----------------------------------------\n\nTITLE: Installing Kubeflow Pipelines with Emissary Executor\nDESCRIPTION: Command to deploy Kubeflow Pipelines standalone with Emissary executor using kubectl.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/choose-executor.md#2025-04-10_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -k \"github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic-emissary?ref=$PIPELINE_VERSION\"\n```\n\n----------------------------------------\n\nTITLE: Sample Kustomization YAML for Kubeflow Pipelines Deployment\nDESCRIPTION: Example kustomization.yaml file for deploying Kubeflow Pipelines. This configuration sets the namespace to 'kubeflow' and uses the development environment from the Kubeflow Pipelines GitHub repository.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/standalone-deployment.md#2025-04-10_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\n# kustomization.yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n# Edit the following to change the deployment to your custom namespace.\nnamespace: kubeflow\n# You can add other customizations here using kustomize.\n# Edit ref in the following link to deploy a different version of Kubeflow Pipelines.\nbases:\n- github.com/kubeflow/pipelines/manifests/kustomize/env/dev?ref={{% pipelines/latest-version %}}\n```\n\n----------------------------------------\n\nTITLE: Visualizing Kubeflow Notebook Image Hierarchy with Mermaid\nDESCRIPTION: A mermaid diagram showing the relationships between different Kubeflow notebook container images, from base images to specialized variants including PyTorch, TensorFlow, and other implementations. Each node represents a container image with clickable links to their respective Dockerfiles.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/notebooks/container-images.md#2025-04-10_snippet_0\n\nLANGUAGE: mermaid\nCODE:\n```\n%%{init: {'theme':'forest'}}%%\ngraph TD\n  Base[Base] --> Jupyter[Jupyter]\n  Base --> Code-Server[code-server]\n  Base --> RStudio[RStudio]\n  \n  Jupyter --> PyTorch[PyTorch]\n  Jupyter --> SciPy[SciPy]\n  Jupyter --> TensorFlow[TensorFlow]\n  \n  Code-Server --> Code-Server-Conda-Python[Conda Python]\n  RStudio --> Tidyverse[Tidyverse]\n\n  PyTorch --> PyTorchFull[PyTorch Full]\n  TensorFlow --> TensorFlowFull[TensorFlow Full]\n\n  Jupyter --> PyTorchCuda[PyTorch CUDA]\n  Jupyter --> TensorFlowCuda[TensorFlow CUDA]\n  Jupyter --> PyTorchGaudi[PyTorch Gaudi]\n\n  PyTorchCuda --> PyTorchCudaFull[PyTorch CUDA Full]\n  TensorFlowCuda --> TensorFlowCudaFull[TensorFlow CUDA Full]\n  PyTorchGaudi --> PyTorchGaudiFull[PyTorch Gaudi Full]\n\n  click Base \"https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/base\"\n  click Jupyter \"https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter\"\n  click Code-Server \"https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/codeserver\"\n  click RStudio \"https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/rstudio\"\n  click PyTorch \"https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-pytorch\"\n  click SciPy \"https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-scipy\"\n  click TensorFlow \"https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-tensorflow\"\n  click Code-Server-Conda-Python \"https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/codeserver-python\"\n  click Tidyverse \"https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/rstudio-tidyverse\"\n  click PyTorchFull \"https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-pytorch-full\"\n  click TensorFlowFull \"https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-tensorflow-full\"\n  click PyTorchCuda \"https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-pytorch-cuda\"\n  click TensorFlowCuda \"https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-tensorflow-cuda\"\n  click PyTorchCudaFull \"https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-pytorch-cuda-full\"\n  click TensorFlowCudaFull \"https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-tensorflow-cuda-full\"\n  click PyTorchGaudi \"https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-pytorch-gaudi\"\n  click PyTorchGaudiFull \"https://github.com/kubeflow/kubeflow/tree/master/components/example-notebook-servers/jupyter-pytorch-gaudi-full\"\n```\n\n----------------------------------------\n\nTITLE: Enabling Cache Webhook\nDESCRIPTION: kubectl command to modify webhook rules for enabling caching\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/overview/caching.md#2025-04-10_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl patch mutatingwebhookconfiguration cache-webhook-${NAMESPACE} --type='json' -p='[{\"op\":\"replace\", \"path\": \"/webhooks/0/rules/0/operations/0\", \"value\": \"CREATE\"}]'\n```\n\n----------------------------------------\n\nTITLE: Configuring Namespace Labels for Katib Metrics Collection\nDESCRIPTION: YAML configuration to create a new namespace with the required Katib metrics collector injection label enabled.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/metrics-collector.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: <your-namespace>\n  labels:\n    katib.kubeflow.org/metrics-collector-injection: enabled\n```\n\n----------------------------------------\n\nTITLE: MPIJob Scheduling Policy Configuration\nDESCRIPTION: YAML configuration showing how to modify PodGroup parameters for gang-scheduling in an MPIJob\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/mpi.md#2025-04-10_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: kubeflow.org/v2beta1\nkind: MPIJob\nmetadata:\n  name: tensorflow-benchmarks\nspec:\n  slotsPerWorker: 1\n  runPolicy:\n    cleanPodPolicy: Running\n    schedulingPolicy:\n      minAvailable: 10\n      queue: test-queue\n      minResources:\n        cpu: 3000m\n      priorityClass: high\n      scheduleTimeoutSeconds: 180\n  mpiReplicaSpecs:\n...\n```\n\n----------------------------------------\n\nTITLE: XGBoostJob YAML Configuration Example\nDESCRIPTION: Complete YAML configuration example of a successfully completed XGBoostJob, showing the job specification and status with both Master and Worker replicas.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/xgboost.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: kubeflow.org/v1\nkind: XGBoostJob\nmetadata:\n  creationTimestamp: \"2021-09-06T18:34:06Z\"\n  generation: 1\n  name: xgboost-dist-iris-test-train\n  namespace: default\n  resourceVersion: \"5844304\"\n  selfLink: /apis/kubeflow.org/v1/namespaces/default/xgboostjobs/xgboost-dist-iris-test-train\n  uid: a1ea6675-3cb5-482b-95dd-68b2c99b8adc\nspec:\n  runPolicy:\n    cleanPodPolicy: None\n  xgbReplicaSpecs:\n    Master:\n      replicas: 1\n      restartPolicy: Never\n      template:\n        spec:\n          containers:\n            - args:\n                - --job_type=Train\n                - --xgboost_parameter=objective:multi:softprob,num_class:3\n                - --n_estimators=10\n                - --learning_rate=0.1\n                - --model_path=/tmp/xgboost-model\n                - --model_storage_type=local\n              image: docker.io/merlintang/xgboost-dist-iris:1.1\n              imagePullPolicy: Always\n              name: xgboost\n              ports:\n                - containerPort: 9991\n                  name: xgboostjob-port\n                  protocol: TCP\n    Worker:\n      replicas: 2\n      restartPolicy: ExitCode\n      template:\n        spec:\n          containers:\n            - args:\n                - --job_type=Train\n                - --xgboost_parameter=\"objective:multi:softprob,num_class:3\"\n                - --n_estimators=10\n                - --learning_rate=0.1\n              image: docker.io/merlintang/xgboost-dist-iris:1.1\n              imagePullPolicy: Always\n              name: xgboost\n              ports:\n                - containerPort: 9991\n                  name: xgboostjob-port\n                  protocol: TCP\nstatus:\n  completionTime: \"2021-09-06T18:34:23Z\"\n  conditions:\n    - lastTransitionTime: \"2021-09-06T18:34:06Z\"\n      lastUpdateTime: \"2021-09-06T18:34:06Z\"\n      message: xgboostJob xgboost-dist-iris-test-train is created.\n      reason: XGBoostJobCreated\n      status: \"True\"\n      type: Created\n    - lastTransitionTime: \"2021-09-06T18:34:06Z\"\n      lastUpdateTime: \"2021-09-06T18:34:06Z\"\n      message: XGBoostJob xgboost-dist-iris-test-train is running.\n      reason: XGBoostJobRunning\n      status: \"False\"\n      type: Running\n    - lastTransitionTime: \"2021-09-06T18:34:23Z\"\n      lastUpdateTime: \"2021-09-06T18:34:23Z\"\n      message: XGBoostJob xgboost-dist-iris-test-train is successfully completed.\n      reason: XGBoostJobSucceeded\n      status: \"True\"\n      type: Succeeded\n  replicaStatuses:\n    Master:\n      succeeded: 1\n    Worker:\n      succeeded: 2\n```\n\n----------------------------------------\n\nTITLE: Hello World Component Definition\nDESCRIPTION: Example YAML component specification for a hello-world container in Kubeflow Pipelines.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/choose-executor.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nname: hello-world\nimplementation:\n  container:\n    image: hello-world\n```\n\n----------------------------------------\n\nTITLE: Deploying Kubeflow Pipelines v0.1.x\nDESCRIPTION: This command is used to deploy older versions of Kubeflow Pipelines (v0.1.x) which had a simpler deployment structure without separate CRD manifests.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/standalone-deployment.md#2025-04-10_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport PIPELINE_VERSION=<kfp-version-0.1.x>\nkubectl apply -k \"github.com/kubeflow/pipelines/manifests/kustomize/env/dev?ref=$PIPELINE_VERSION\"\n```\n\n----------------------------------------\n\nTITLE: Example SparkApplication Resource YAML\nDESCRIPTION: Example YAML representation of a SparkApplication resource. This shows the configuration and status of a Spark Pi application after it has been deployed and executed.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/getting-started.md#2025-04-10_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: sparkoperator.k8s.io/v1beta2\nkind: SparkApplication\nmetadata:\n  ...\nspec:\n  deps: {}\n  driver:\n    coreLimit: 1200m\n    cores: 1\n    labels:\n      version: 2.3.0\n    memory: 512m\n    serviceAccount: spark\n  executor:\n    cores: 1\n    instances: 1\n    labels:\n      version: 2.3.0\n    memory: 512m\n  image: gcr.io/ynli-k8s/spark:v3.1.1\n  mainApplicationFile: local:///opt/spark/examples/jars/spark-examples_2.12-3.1.1.jar\n  mainClass: org.apache.spark.examples.SparkPi\n  mode: cluster\n  restartPolicy:\n      type: OnFailure\n      onFailureRetries: 3\n      onFailureRetryInterval: 10\n      onSubmissionFailureRetries: 5\n      onSubmissionFailureRetryInterval: 20\n  type: Scala\nstatus:\n  sparkApplicationId: spark-5f4ba921c85ff3f1cb04bef324f9154c9\n  applicationState:\n    state: COMPLETED\n  completionTime: 2018-02-20T23:33:55Z\n  driverInfo:\n    podName: spark-pi-83ba921c85ff3f1cb04bef324f9154c9-driver\n    webUIAddress: 35.192.234.248:31064\n    webUIPort: 31064\n    webUIServiceName: spark-pi-2402118027-ui-svc\n    webUIIngressName: spark-pi-ui-ingress\n    webUIIngressAddress: spark-pi.ingress.cluster.com\n  executorState:\n    spark-pi-83ba921c85ff3f1cb04bef324f9154c9-exec-1: COMPLETED\n  LastSubmissionAttemptTime: 2018-02-20T23:32:27Z\n```\n\n----------------------------------------\n\nTITLE: Manually Deleting a Notebook Resource\nDESCRIPTION: Command to manually delete a Notebook resource that may be stuck or problematic. This is useful when the normal deletion process via the UI doesn't work.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/notebooks/troubleshooting.md#2025-04-10_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nkubectl delete notebook \"${MY_NOTEBOOK_NAME}\" --namespace \"${MY_PROFILE_NAMESPACE}\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Suggestion Volume Parameters in Katib\nDESCRIPTION: Example configuration for Suggestion volume settings using the random algorithm. Demonstrates how to set up PersistentVolume and PersistentVolumeClaim specifications for Katib Suggestions.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/katib-config.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nsuggestions:\n  - algorithmName: random\n    image: docker.io/kubeflowkatib/suggestion-hyperopt:latest\n    volumeMountPath: /opt/suggestion/data\n    persistentVolumeClaimSpec:\n      accessModes:\n        - ReadWriteMany\n      resources:\n        requests:\n          storage: 3Gi\n      storageClassName: katib-suggestion\n    persistentVolumeSpec:\n      accessModes:\n        - ReadWriteMany\n      capacity:\n        storage: 3Gi\n      hostPath:\n        path: /tmp/suggestion/unique/path\n      storageClassName: katib-suggestion\n    persistentVolumeLabels:\n      type: local\n```\n\n----------------------------------------\n\nTITLE: Verifying Kubernetes Cluster on Docker-Desktop\nDESCRIPTION: This command checks if the Kubernetes cluster is up and running on Docker-Desktop. It uses kubectl to retrieve cluster information and should display the Kubernetes control plane and CoreDNS status.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/localcluster-deployment.md#2025-04-10_snippet_13\n\nLANGUAGE: SHELL\nCODE:\n```\nkubectl cluster-info\n```\n\n----------------------------------------\n\nTITLE: Defining Python Main Method for Pipeline Compilation\nDESCRIPTION: Python code snippet showing how to define a __main__ method that calls the TektonCompiler to compile a pipeline function. This allows direct compilation by running the Python script without using the command line tool.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/pipelines-with-tekton.md#2025-04-10_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nif __name__ == \"__main__\":\n    from kfp_tekton.compiler import TektonCompiler\n    TektonCompiler().compile(pipeline_func, \"pipeline.yaml\")\n```\n\n----------------------------------------\n\nTITLE: Monitoring XGBoostJob Status\nDESCRIPTION: Command to retrieve detailed information about the XGBoostJob resource in YAML format.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/xgboost.md#2025-04-10_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get -o yaml xgboostjobs xgboost-dist-iris-test-train\n```\n\n----------------------------------------\n\nTITLE: Updated Hello World Component with Command\nDESCRIPTION: Modified YAML component specification that includes the required command field for Emissary executor compatibility.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/choose-executor.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nname: hello-world\nimplementation:\n  container:\n    image: hello-world\n    command: [\"/hello\"]\n```\n\n----------------------------------------\n\nTITLE: Creating GKE Cluster for Kubeflow Pipelines Standalone\nDESCRIPTION: This bash snippet configures and creates a Google Kubernetes Engine (GKE) cluster suitable for running Kubeflow Pipelines with sufficient resources. It sets cluster parameters including name, zone, machine type, and scopes.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/standalone-deployment.md#2025-04-10_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# The following parameters can be customized based on your needs.\n\nCLUSTER_NAME=\"kubeflow-pipelines-standalone\"\nZONE=\"us-central1-a\"\nMACHINE_TYPE=\"e2-standard-2\" # A machine with 2 CPUs and 8GB memory.\nSCOPES=\"cloud-platform\" # This scope is needed for running some pipeline samples. Read the warning below for its security implication\n\ngcloud container clusters create $CLUSTER_NAME \\\n     --zone $ZONE \\\n     --machine-type $MACHINE_TYPE \\\n     --scopes $SCOPES\n```\n\n----------------------------------------\n\nTITLE: Installing Spark Operator with Custom Job Namespace\nDESCRIPTION: Command to install the Spark Operator with a custom job namespace. This configures the operator to support running Spark jobs in the specified namespace.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/getting-started.md#2025-04-10_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nhelm install my-release spark-operator/spark-operator --namespace spark-operator --set \"spark.jobNamespaces={test-ns}\"\n```\n\n----------------------------------------\n\nTITLE: Verifying Controller Manager Pods\nDESCRIPTION: Command to verify that the JobSet and Trainer controller manager pods are running in the kubeflow-system namespace.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/operator-guides/installation.md#2025-04-10_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pods -n kubeflow-system\n```\n\n----------------------------------------\n\nTITLE: Checking kubectl Installation\nDESCRIPTION: Command to verify if kubectl is installed on the system and view its location\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/overview/caching.md#2025-04-10_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nwhich kubectl\n```\n\n----------------------------------------\n\nTITLE: Adding a node to K3s cluster\nDESCRIPTION: Adds an agent node to an existing K3s cluster using the node token from the server.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/localcluster-deployment.md#2025-04-10_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nsudo k3s agent --server https://myserver:6443 --token {YOUR_NODE_TOKEN}\n```\n\n----------------------------------------\n\nTITLE: Configuring Random Search Algorithm in Katib\nDESCRIPTION: This snippet outlines the configuration options for the Random Search algorithm in Katib. It uses Hyperopt, Goptuna, or Optuna optimization frameworks and supports setting a random state for reproducible results.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/hp-tuning/configure-algorithm.md#2025-04-10_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n<div class=\"table-responsive\">\n  <table class=\"table table-bordered\">\n    <thead class=\"thead-light\">\n      <tr>\n        <th>Setting name</th>\n        <th>Description</th>\n        <th>Example</th>\n      </tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td>random_state</td>\n        <td>[int]: Set <code>random_state</code> to something other than None\n          for reproducible results.</td>\n        <td>10</td>\n      </tr>\n    </tbody>\n  </table>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Viewing XGBoostJob Configuration in Bash\nDESCRIPTION: Command to display the contents of the XGBoostJob YAML configuration file.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/xgboost.md#2025-04-10_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ncat xgboostjob.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing kind on Linux\nDESCRIPTION: Downloads and installs the kind executable on a Linux system, placing it in the user's PATH. Requires curl and chmod commands.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/localcluster-deployment.md#2025-04-10_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ncurl -Lo ./kind https://kind.sigs.k8s.io/dl/{KIND_VERSION}/kind-linux-amd64 && \\\nchmod +x ./kind && \\\nmv ./kind /{YOUR_KIND_DIRECTORY}/kind\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Kubeflow Pipelines (Custom Manifest)\nDESCRIPTION: This command uninstalls Kubeflow Pipelines using a custom manifest file. Replace {YOUR_MANIFEST_FILE} with the actual manifest file name.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/localcluster-deployment.md#2025-04-10_snippet_16\n\nLANGUAGE: SHELL\nCODE:\n```\nkubectl delete -k {YOUR_MANIFEST_FILE}\n```\n\n----------------------------------------\n\nTITLE: Sample TFJob Status Output in Kubeflow\nDESCRIPTION: This YAML output shows a complete TFJob status example including conditions, replica statuses, and timestamps. It demonstrates a successfully completed job with two Worker replicas and the progression through different condition states.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/tensorflow.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: kubeflow.org/v1\nkind: TFJob\nmetadata:\n  creationTimestamp: \"2021-09-06T11:48:09Z\"\n  generation: 1\n  name: tfjob-simple\n  namespace: kubeflow\n  resourceVersion: \"5764004\"\n  selfLink: /apis/kubeflow.org/v1/namespaces/kubeflow/tfjobs/tfjob-simple\n  uid: 3a67a9a9-cb89-4c1f-a189-f49f0b581e29\nspec:\n  tfReplicaSpecs:\n    Worker:\n      replicas: 2\n      restartPolicy: OnFailure\n      template:\n        spec:\n          containers:\n            - command:\n                - python\n                - /var/tf_mnist/mnist_with_summaries.py\n              image: gcr.io/kubeflow-ci/tf-mnist-with-summaries:1.0\n              name: tensorflow\nstatus:\n  completionTime: \"2021-09-06T11:49:30Z\"\n  conditions:\n    - lastTransitionTime: \"2021-09-06T11:48:09Z\"\n      lastUpdateTime: \"2021-09-06T11:48:09Z\"\n      message: TFJob tfjob-simple is created.\n      reason: TFJobCreated\n      status: \"True\"\n      type: Created\n    - lastTransitionTime: \"2021-09-06T11:48:12Z\"\n      lastUpdateTime: \"2021-09-06T11:48:12Z\"\n      message: TFJob kubeflow/tfjob-simple is running.\n      reason: TFJobRunning\n      status: \"False\"\n      type: Running\n    - lastTransitionTime: \"2021-09-06T11:49:30Z\"\n      lastUpdateTime: \"2021-09-06T11:49:30Z\"\n      message: TFJob kubeflow/tfjob-simple successfully completed.\n      reason: TFJobSucceeded\n      status: \"True\"\n      type: Succeeded\n  replicaStatuses:\n    Worker:\n      succeeded: 2\n  startTime: \"2021-09-06T11:48:10Z\"\n```\n\n----------------------------------------\n\nTITLE: Sample JAXJob Status Output\nDESCRIPTION: Example YAML output showing a successfully completed JAXJob with its configuration and status details. It shows the job specification with worker replicas and the status conditions tracking the job lifecycle.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/jax.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: kubeflow.org/v1\nkind: JAXJob\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"kubeflow.org/v1\",\"kind\":\"JAXJob\",\"metadata\":{\"annotations\":{},\"name\":\"jaxjob-simple\",\"namespace\":\"kubeflow\"},\"spec\":{\"jaxReplicaSpecs\":{\"Worker\":{\"replicas\":2,\"restartPolicy\":\"OnFailure\",\"template\":{\"spec\":{\"containers\":[{\"command\":[\"python3\",\"train.py\"],\"image\":\"docker.io/kubeflow/jaxjob-simple:latest\",\"imagePullPolicy\":\"Always\",\"name\":\"jax\"}]}}}}}}\n  creationTimestamp: \"2024-09-22T20:07:59Z\"\n  generation: 1\n  name: jaxjob-simple\n  namespace: kubeflow\n  resourceVersion: \"1972\"\n  uid: eb20c874-44fc-459b-b9a8-09f5c3ff46d3\nspec:\n  jaxReplicaSpecs:\n    Worker:\n      replicas: 2\n      restartPolicy: OnFailure\n      template:\n        spec:\n          containers:\n            - command:\n                - python3\n                - train.py\n              image: docker.io/kubeflow/jaxjob-simple:latest\n              imagePullPolicy: Always\n              name: jax\nstatus:\n  completionTime: \"2024-09-22T20:11:34Z\"\n  conditions:\n    - lastTransitionTime: \"2024-09-22T20:07:59Z\"\n      lastUpdateTime: \"2024-09-22T20:07:59Z\"\n      message: JAXJob jaxjob-simple is created.\n      reason: JAXJobCreated\n      status: \"True\"\n      type: Created\n    - lastTransitionTime: \"2024-09-22T20:11:28Z\"\n      lastUpdateTime: \"2024-09-22T20:11:28Z\"\n      message: JAXJob kubeflow/jaxjob-simple is running.\n      reason: JAXJobRunning\n      status: \"False\"\n      type: Running\n    - lastTransitionTime: \"2024-09-22T20:11:34Z\"\n      lastUpdateTime: \"2024-09-22T20:11:34Z\"\n      message: JAXJob kubeflow/jaxjob-simple successfully completed.\n      reason: JAXJobSucceeded\n      status: \"True\"\n      type: Succeeded\n  replicaStatuses:\n    Worker:\n      selector: training.kubeflow.org/job-name=jaxjob-simple,training.kubeflow.org/operator-name=jaxjob-controller,training.kubeflow.org/replica-type=worker\n      succeeded: 2\n  startTime: \"2024-09-22T20:07:59Z\"\n```\n\n----------------------------------------\n\nTITLE: Verifying Webhook Configuration\nDESCRIPTION: Commands to check the existence of mutatingwebhookconfiguration in the cluster\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/overview/caching.md#2025-04-10_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport NAMESPACE=<Namespace where KFP is installed>\nkubectl get mutatingwebhookconfiguration cache-webhook-${NAMESPACE}\n```\n\n----------------------------------------\n\nTITLE: Using Keyword Arguments in Pipeline Tasks\nDESCRIPTION: Demonstrates the required use of keyword arguments when instantiating components as tasks within a pipeline definition in v2.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/migration.md#2025-04-10_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef my_pipeline():\n    trainer_component(epochs=100, learning_rate=0.1)\n```\n\n----------------------------------------\n\nTITLE: Installing Kubeflow Pipelines with K3ai (CPU-only)\nDESCRIPTION: This command installs Kubeflow Pipelines using K3ai with CPU-only support. It uses a curl command to download and execute the K3ai installation script with specific parameters.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/localcluster-deployment.md#2025-04-10_snippet_11\n\nLANGUAGE: SHELL\nCODE:\n```\ncurl -sfL https://get.k3ai.in | bash -s -- --cpu --plugin_kfpipelines\n```\n\n----------------------------------------\n\nTITLE: Configuring Multivariate TPE Algorithm in Katib\nDESCRIPTION: This snippet outlines the configuration options for the Multivariate TPE algorithm in Katib. It uses the Optuna optimization framework and supports settings for the number of EI candidates, random state, and number of startup trials.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/hp-tuning/configure-algorithm.md#2025-04-10_snippet_4\n\nLANGUAGE: markdown\nCODE:\n```\n<div class=\"table-responsive\">\n  <table class=\"table table-bordered\">\n    <thead class=\"thead-light\">\n      <tr>\n        <th>Setting name</th>\n        <th>Description</th>\n        <th>Example</th>\n      </tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td>n_ei_candidates</td>\n        <td>[int]: Number of Trials used to calculate the expected improvement.</td>\n        <td>25</td>\n      </tr>\n      <tr>\n        <td>random_state</td>\n        <td>[int]: Set <code>random_state</code> to something other than None\n          for reproducible results.</td>\n        <td>10</td>\n      </tr>\n      <tr>\n        <td>n_startup_trials</td>\n        <td>[int]: Number of initial Trials for which the random search algorithm generates\n        hyperparameters.</td>\n        <td>5</td>\n      </tr>\n    </tbody>\n  </table>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Kubectl Port Forward Command for Metrics Access\nDESCRIPTION: Command to access metrics locally by forwarding the Training Operator's metrics port to localhost.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/prometheus.md#2025-04-10_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl port-forward -n kubeflow deployment/training-operator 8080:8080\n```\n\n----------------------------------------\n\nTITLE: Creating a Kubernetes cluster with K3s\nDESCRIPTION: Bootstraps a Kubernetes cluster using K3s and writes the kubeconfig to /etc/rancher/k3s/k3s.yaml.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/localcluster-deployment.md#2025-04-10_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nsudo k3s server &\n```\n\n----------------------------------------\n\nTITLE: Installing K3s using the official installation script\nDESCRIPTION: Installs K3s as a service on systemd- or openrc-based systems using the official K3s installation script.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/localcluster-deployment.md#2025-04-10_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\ncurl -sfL https://get.k3s.io | sh -\n```\n\n----------------------------------------\n\nTITLE: Configuring MPI Operator for Default Scheduler Plugins\nDESCRIPTION: Configuration diff showing how to modify the MPI Operator deployment when Scheduler Plugins is installed as the default Kubernetes scheduler.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/job-scheduling.md#2025-04-10_snippet_1\n\nLANGUAGE: diff\nCODE:\n```\n...\n    spec:\n      containers:\n      - args:\n+       - --gang-scheduling=default-scheduler\n        - -alsologtostderr\n        - --lock-namespace=mpi-operator\n        image: mpioperator/mpi-operator:0.4.0\n        name: mpi-operator\n...\n```\n\n----------------------------------------\n\nTITLE: Setting Cache Staleness Period\nDESCRIPTION: Python code showing how to set maximum cache staleness period for a pipeline task\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/overview/caching.md#2025-04-10_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef some_pipeline():\n      # task is a target step in a pipeline\n      task = some_op()\n      task.execution_options.caching_strategy.max_cache_staleness = \"P30D\"\n```\n\n----------------------------------------\n\nTITLE: Running Services as Root with s6-overlay\nDESCRIPTION: Technique for running services as root in Kubeflow Notebook containers. Uses s6-setuidgid to switch to the notebook user for user-facing services while maintaining root privileges for system operations.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/notebooks/container-images.md#2025-04-10_snippet_3\n\nLANGUAGE: dockerfile\nCODE:\n```\nUSER root\n```\n\nLANGUAGE: bash\nCODE:\n```\ns6-setuidgid $NB_USER\n```\n\n----------------------------------------\n\nTITLE: Configuring MPI Operator for Secondary Scheduler Plugins\nDESCRIPTION: Configuration diff showing how to modify the MPI Operator deployment when Scheduler Plugins is installed as a secondary Kubernetes scheduler.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/job-scheduling.md#2025-04-10_snippet_2\n\nLANGUAGE: diff\nCODE:\n```\n...\n    spec:\n      containers:\n      - args:\n+       - --gang-scheduling=scheduler-plugins-scheduler\n        - -alsologtostderr\n        - --lock-namespace=mpi-operator\n        image: mpioperator/mpi-operator:0.4.0\n        name: mpi-operator\n...\n```\n\n----------------------------------------\n\nTITLE: Creating a RoleBinding for Profile Contributors\nDESCRIPTION: YAML definition for creating a RoleBinding that grants a user access to a Kubeflow profile. This binding connects a user to either the kubeflow-edit or kubeflow-view ClusterRole.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/central-dash/profiles.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: user-<SAFE_USER_EMAIL>-clusterrole-<USER_ROLE>\n  namespace: <PROFILE_NAME>\n  annotations:\n    role: <USER_ROLE>\n    user: <RAW_USER_EMAIL>\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: kubeflow-<USER_ROLE>\nsubjects:\n  - apiGroup: rbac.authorization.k8s.io\n    kind: User\n    name: <RAW_USER_EMAIL>\n```\n\n----------------------------------------\n\nTITLE: Checking PaddleJob Pods Status\nDESCRIPTION: Command to list all pods associated with the PaddleJob training job using label selectors.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/paddle.md#2025-04-10_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pods -l job-name=paddle-simple-cpu -n kubeflow\n```\n\n----------------------------------------\n\nTITLE: Adding execute permission to K3s binary on WSL\nDESCRIPTION: Adds execute permission to the downloaded K3s binary on Windows Subsystem for Linux.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/localcluster-deployment.md#2025-04-10_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nchmod +x k3s\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Visualizations in Kubeflow Frontend Deployment\nDESCRIPTION: YAML configuration to enable custom visualizations in Kubeflow Pipelines by setting environment variables in the frontend deployment.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/python-based-visualizations.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n- env:\n  - name: ALLOW_CUSTOM_VISUALIZATIONS\n    value: true\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Spark Operator Helm Release\nDESCRIPTION: Command to uninstall a Spark Operator Helm release. This removes all Kubernetes resources associated with the chart, except for CRDs which must be removed manually.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/getting-started.md#2025-04-10_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nhelm uninstall [RELEASE_NAME]\n```\n\n----------------------------------------\n\nTITLE: Defining Hugo Front Matter for Kubeflow Notebooks Documentation\nDESCRIPTION: This YAML-formatted Hugo front matter defines the metadata for the Kubeflow Notebooks documentation page. It specifies the title as 'Kubeflow Notebooks', provides a description, and sets the weight to 10 for determining the order of pages in navigation.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/notebooks/_index.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n+++\ntitle = \"Kubeflow Notebooks\"\ndescription = \"Documentation for Kubeflow Notebooks\"\nweight = 10\n+++\n```\n\n----------------------------------------\n\nTITLE: Checking Spark Operator Helm Release Status\nDESCRIPTION: Command to check the status of the Spark Operator Helm release after installation. This provides information about the deployment status.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/getting-started.md#2025-04-10_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nhelm status --namespace spark-operator my-release\n```\n\n----------------------------------------\n\nTITLE: Retrieving Training Logs\nDESCRIPTION: Code to fetch and display training logs from the master node.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/getting-started.md#2025-04-10_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nlogs = TrainerClient().get_job_logs(name=job_id)\n\nprint(logs[\"node-0\"])\n```\n\n----------------------------------------\n\nTITLE: Example JSON Structure for KFP v1 Output Metadata\nDESCRIPTION: Shows the JSON structure required for defining output viewer metadata in KFP v1, including schema and format specifications.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/output-viewer.md#2025-04-10_snippet_5\n\nLANGUAGE: JSON\nCODE:\n```\n{\n  \"version\": 1,\n  \"outputs\": [\n    {\n      \"type\": \"confusion_matrix\",\n      \"format\": \"csv\",\n      \"source\": \"my-dir/my-matrix.csv\",\n      \"schema\": [\n        {\"name\": \"target\", \"type\": \"CATEGORY\"},\n        {\"name\": \"predicted\", \"type\": \"CATEGORY\"},\n        {\"name\": \"count\", \"type\": \"NUMBER\"}\n      ],\n      \"labels\": \"vocab\"\n    }\n  ]\n}\n```\n\n----------------------------------------\n\nTITLE: Downloading and Compiling a Sample Pipeline\nDESCRIPTION: Commands to download the sequential.py sample pipeline from GitHub and compile it to a .tar.gz file using the Kubeflow Pipelines DSL compiler.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/tutorials/api-pipelines.md#2025-04-10_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nPIPELINE_URL=https://raw.githubusercontent.com/kubeflow/pipelines/master/samples/core/sequential/sequential.py\nPIPELINE_FILE=${PIPELINE_URL##*/}\nPIPELINE_NAME=${PIPELINE_FILE%.*}\n\nwget -O ${PIPELINE_FILE} ${PIPELINE_URL}\ndsl-compile --py ${PIPELINE_FILE} --output ${PIPELINE_NAME}.tar.gz\n```\n\n----------------------------------------\n\nTITLE: Output Path Placeholder Usage in Kubeflow Pipeline\nDESCRIPTION: Demonstrates how to use output path placeholders to specify where component output files should be written.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/reference/component-spec.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\ncommand: [program.py, --out-model, {outputPath: trained_model}]\n```\n\nLANGUAGE: python\nCODE:\n```\ntask1 = component1()\n# You can now pass `task1.outputs['trained_model']` to other components as argument.\n```\n\nLANGUAGE: shell\nCODE:\n```\nprogram.py --out-model /outputs/trained_model/data\n```\n\n----------------------------------------\n\nTITLE: Retrieving TFJob Status in Kubernetes\nDESCRIPTION: This command retrieves the YAML representation of a TFJob in a specific namespace. It's used to check if a status is present for the job, which can indicate whether the job spec is valid.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/tensorflow.md#2025-04-10_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nkubectl -n ${USER_NAMESPACE} get tfjobs -o yaml ${JOB_NAME}\n```\n\n----------------------------------------\n\nTITLE: Creating a Kubernetes cluster with kind\nDESCRIPTION: Creates a Kubernetes cluster using kind with default settings.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/localcluster-deployment.md#2025-04-10_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nkind create cluster\n```\n\n----------------------------------------\n\nTITLE: Using KFPClientManager for Authenticated Access to Kubeflow Pipelines\nDESCRIPTION: Example code demonstrating how to instantiate the KFPClientManager class and use it to create an authenticated client for interacting with Kubeflow Pipelines API.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/connect-api.md#2025-04-10_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# initialize a KFPClientManager\nkfp_client_manager = KFPClientManager(\n    api_url=\"http://localhost:8080/pipeline\",\n    skip_tls_verify=True,\n\n    dex_username=\"user@example.com\",\n    dex_password=\"12341234\",\n\n    # can be 'ldap' or 'local' depending on your Dex configuration\n    dex_auth_type=\"local\",\n)\n\n# get a newly authenticated KFP client\n# TIP: long-lived sessions might need to get a new client when their session expires\nkfp_client = kfp_client_manager.create_kfp_client()\n\n# test the client by listing experiments\nexperiments = kfp_client.list_experiments(namespace=\"my-profile\")\nprint(experiments)\n```\n\n----------------------------------------\n\nTITLE: Checking TF Operator Logs in Kubernetes\nDESCRIPTION: This command retrieves the logs of the TF job operator pod. It's useful for finding error messages related to invalid TFJob specs.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/tensorflow.md#2025-04-10_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nkubectl -n ${KUBEFLOW_NAMESPACE} logs `kubectl get pods --selector=name=tf-job-operator -o jsonpath='{.items[0].metadata.name}'`\n```\n\n----------------------------------------\n\nTITLE: Deploying PyTorchJob on Kubernetes\nDESCRIPTION: Command to create a PyTorchJob resource using a predefined configuration file from the training-operator repository.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/pytorch.md#2025-04-10_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -f https://raw.githubusercontent.com/kubeflow/training-operator/refs/heads/release-1.9/examples/pytorch/simple.yaml\n```\n\n----------------------------------------\n\nTITLE: Applying Namespace Changes for Kubeflow Pipelines in Kubernetes\nDESCRIPTION: Commands to apply namespace changes to Kubeflow Pipelines deployment. These kubectl commands apply the customized kustomize configurations after changing the namespace in the respective YAML files.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/standalone-deployment.md#2025-04-10_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -k manifests/kustomize/cluster-scoped-resources\nkubectl apply -k manifests/kustomize/env/dev\n```\n\n----------------------------------------\n\nTITLE: Checking Python Version for Kubeflow Pipelines SDK\nDESCRIPTION: This command checks the installed Python version to ensure compatibility with the Kubeflow Pipelines SDK. Python 3.5 or higher is required.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/troubleshooting.md#2025-04-10_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3 -V\n```\n\n----------------------------------------\n\nTITLE: KFP CLI Help Command\nDESCRIPTION: Shows the basic syntax for the KFP CLI run create command help option.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/run-a-pipeline.md#2025-04-10_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nkfp run create [OPTIONS] [ARGS]...\n```\n\n----------------------------------------\n\nTITLE: Running Helm Chart Lint Tests\nDESCRIPTION: Command and sample output for running Helm chart lint tests for the Spark operator project.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/developer-guide.md#2025-04-10_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\n$ make helm-lint\nLinting charts...\n\n------------------------------------------------------------------------------------------------------------------------\n Charts to be processed:\n------------------------------------------------------------------------------------------------------------------------\n spark-operator => (version: \"1.2.4\", path: \"charts/spark-operator-chart\")\n------------------------------------------------------------------------------------------------------------------------\n\nLinting chart \"spark-operator => (version: \\\"1.2.4\\\", path: \\\"charts/spark-operator-chart\\\")\"\nChecking chart \"spark-operator => (version: \\\"1.2.4\\\", path: \\\"charts/spark-operator-chart\\\")\" for a version bump...\nOld chart version: 1.2.1\nNew chart version: 1.2.4\nChart version ok.\nValidating /Users/user/go/src/github.com/kubeflow/spark-operator/charts/spark-operator-chart/Chart.yaml...\nValidation success! 👍\nValidating maintainers...\n\nLinting chart with values file \"charts/spark-operator-chart/ci/ci-values.yaml\"...\n\n==> Linting charts/spark-operator-chart\n[INFO] Chart.yaml: icon is recommended\n\n1 chart(s) linted, 0 chart(s) failed\n\n------------------------------------------------------------------------------------------------------------------------\n ✔︎ spark-operator => (version: \"1.2.4\", path: \"charts/spark-operator-chart\")\n------------------------------------------------------------------------------------------------------------------------\nAll charts linted successfully\n```\n\n----------------------------------------\n\nTITLE: Checking K3s cluster status\nDESCRIPTION: Verifies the status of the K3s cluster by listing the nodes using the embedded kubectl command.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/localcluster-deployment.md#2025-04-10_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nsudo k3s kubectl get node\n```\n\n----------------------------------------\n\nTITLE: Specifying Extra Java Options for Spark Executors\nDESCRIPTION: This YAML snippet demonstrates how to specify additional Java options for Spark executors using the javaOptions field, which gets converted to the spark.executor.extraJavaOptions configuration property.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/writing-sparkapplication.md#2025-04-10_snippet_7\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  executor:\n    javaOptions: \"-XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap\"\n```\n\n----------------------------------------\n\nTITLE: Monitoring JAXJob Status\nDESCRIPTION: Command to retrieve the detailed YAML output for a JAXJob, which includes the status section for monitoring job progress and completion.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/jax.md#2025-04-10_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get -o yaml jaxjobs jaxjob-simple -n kubeflow\n```\n\n----------------------------------------\n\nTITLE: Viewing PyTorchJob Training Logs\nDESCRIPTION: Commands to fetch and follow logs from the master pod of a PyTorchJob training instance.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/pytorch.md#2025-04-10_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nPODNAME=$(kubectl get pods -l training.kubeflow.org/job-name=pytorch-simple,training.kubeflow.org/replica-type=master,training.kubeflow.org/replica-index=0 -o name -n kubeflow)\nkubectl logs -f ${PODNAME} -n kubeflow\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Kubeflow Pipelines (GitHub Manifests)\nDESCRIPTION: These commands uninstall Kubeflow Pipelines using manifests from the Kubeflow Pipelines GitHub repository. They remove platform-agnostic resources and cluster-scoped resources.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/localcluster-deployment.md#2025-04-10_snippet_17\n\nLANGUAGE: SHELL\nCODE:\n```\nexport PIPELINE_VERSION={{% pipelines/latest-version %}}\nkubectl delete -k \"github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic?ref=$PIPELINE_VERSION\"\nkubectl delete -k \"github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref=$PIPELINE_VERSION\"\n```\n\n----------------------------------------\n\nTITLE: Adding Local Bin to PATH for Kubeflow Pipelines SDK\nDESCRIPTION: This command adds the ~/.local/bin directory to the PATH environment variable, allowing the use of 'kfp' and 'dsl-compile' commands when the SDK is installed with the --user flag.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/troubleshooting.md#2025-04-10_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nexport $PATH=$PATH:~/.local/bin\n```\n\n----------------------------------------\n\nTITLE: Accessing Command Help\nDESCRIPTION: Command to get help documentation for the run command\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/cli.md#2025-04-10_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nkfp run --help\n```\n\n----------------------------------------\n\nTITLE: Running helm-docs Command for Spark Operator Chart\nDESCRIPTION: Command to generate README.md documentation for Helm charts using the helm-docs tool. The output shows successful documentation generation for the spark-operator-chart.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/developer-guide.md#2025-04-10_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\n$ make helm-docs\nINFO[2024-04-14T07:29:26Z] Found Chart directories [charts/spark-operator-chart] \nINFO[2024-04-14T07:29:26Z] Generating README Documentation for chart charts/spark-operator-chart \n```\n\n----------------------------------------\n\nTITLE: Defining Metadata for Kubeflow Pipelines V2 Reference Page\nDESCRIPTION: This snippet defines the metadata for a web page containing reference documentation for Kubeflow Pipelines Version 2. It specifies the title, description, and weight (likely for ordering) of the page.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/reference/_index.md#2025-04-10_snippet_0\n\nLANGUAGE: Markdown\nCODE:\n```\n+++\ntitle = \"Reference\"\ndescription = \"Reference docs for Kubeflow Pipelines Version 2\"\nweight = 8\n+++\n```\n\n----------------------------------------\n\nTITLE: Specifying Scheduler Name in TFJob with Secondary Scheduler\nDESCRIPTION: Diff showing how to specify the scheduler name in a TFJob resource when using Scheduler Plugins as a secondary scheduler.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/job-scheduling.md#2025-04-10_snippet_3\n\nLANGUAGE: diff\nCODE:\n```\napiVersion: \"kubeflow.org/v1\"\nkind: TFJob\nmetadata:\n  name: tfjob-simple\n  namespace: kubeflow\nspec:\n  tfReplicaSpecs:\n    Worker:\n      replicas: 2\n      restartPolicy: OnFailure\n      template:\n        spec:\n+         schedulerName: scheduler-plugins-scheduler\n          containers:\n            - name: tensorflow\n              image: kubeflow/tf-mnist-with-summaries:latest\n              command:\n                - \"python\"\n                - \"/var/tf_mnist/mnist_with_summaries.py\"\n```\n\n----------------------------------------\n\nTITLE: Building MPI Operator Docker Image\nDESCRIPTION: Command to build a custom MPI operator Docker image using make. This allows you to specify a custom release version and image repository.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/mpi.md#2025-04-10_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nmake RELEASE_VERSION=dev IMAGE_NAME=registry.example.com/mpi-operator images\n```\n\n----------------------------------------\n\nTITLE: Describing SparkApplication Resource Events\nDESCRIPTION: Command to describe a SparkApplication resource and view its events. This is useful for monitoring the lifecycle and troubleshooting issues with Spark applications.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/getting-started.md#2025-04-10_snippet_12\n\nLANGUAGE: shell\nCODE:\n```\nkubectl describe sparkapplication spark-pi\n```\n\n----------------------------------------\n\nTITLE: Testing from Notebook Environment\nDESCRIPTION: Test Model Registry connectivity from within a Notebook environment using curl or wget.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/model-registry/installation.md#2025-04-10_snippet_11\n\nLANGUAGE: shell\nCODE:\n```\ncurl model-registry-service.kubeflow.svc.cluster.local:8080/api/model_registry/v1alpha3/registered_models\n```\n\nLANGUAGE: shell\nCODE:\n```\nwget -nv -O- model-registry-service.kubeflow.svc.cluster.local:8080/api/model_registry/v1alpha3/registered_models\n```\n\n----------------------------------------\n\nTITLE: Disabling Cache Webhook\nDESCRIPTION: kubectl command to modify webhook rules for disabling caching\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/overview/caching.md#2025-04-10_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl patch mutatingwebhookconfiguration cache-webhook-${NAMESPACE} --type='json' -p='[{\"op\":\"replace\", \"path\": \"/webhooks/0/rules/0/operations/0\", \"value\": \"DELETE\"}]'\n```\n\n----------------------------------------\n\nTITLE: Disabling Caching via CLI Compilation Flag\nDESCRIPTION: Example of using the --disable-execution-caching-by-default command-line flag to disable caching for all pipeline tasks when compiling a pipeline.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/caching.md#2025-04-10_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkfp dsl compile --py my_pipeline.py --output my_pipeline.yaml --disable-execution-caching-by-default\n```\n\n----------------------------------------\n\nTITLE: Installing kind on Windows using Chocolatey\nDESCRIPTION: Installs kind on Windows using the Chocolatey package manager.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/localcluster-deployment.md#2025-04-10_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nchoco install kind\n```\n\n----------------------------------------\n\nTITLE: Configuring Hugo Front Matter for Kubeflow Pipelines Documentation\nDESCRIPTION: TOML configuration block that defines metadata for a Hugo documentation page about Kubeflow Pipelines. Sets the title, description and weight for page ordering.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/_index.md#2025-04-10_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n+++\ntitle = \"Kubeflow Pipelines\"\ndescription = \"Documentation for Kubeflow Pipelines.\"\nweight = 15\n+++\n```\n\n----------------------------------------\n\nTITLE: Prometheus Metrics Output Format Example\nDESCRIPTION: Example of the metrics output format showing job creation counter with framework and namespace labels.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/prometheus.md#2025-04-10_snippet_2\n\nLANGUAGE: plaintext\nCODE:\n```\n# HELP training_operator_jobs_created_total Counts number of jobs created\n# TYPE training_operator_jobs_created_total counter\ntraining_operator_jobs_created_total{framework=\"tensorflow\",job_namespace=\"kubeflow\"} 7\n```\n\n----------------------------------------\n\nTITLE: Committing Changes to Local Git Repository\nDESCRIPTION: Examples of how to commit your documentation changes to your local Git repository. The first command commits all modified tracked files, while the second shows how to add and commit a new file.\nSOURCE: https://github.com/kubeflow/website/blob/master/quick-github-guide.md#2025-04-10_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit commit -a -m \"Fixed some doc errors.\"\n```\n\nLANGUAGE: bash\nCODE:\n```\ngit add add-this-doc.md\ngit commit -a -m \"Added a shiny new doc.\"\n```\n\n----------------------------------------\n\nTITLE: Sample SparkApplication Events Output\nDESCRIPTION: Example output showing events for a SparkApplication resource. This displays the events generated by the Spark Operator during the lifecycle of a Spark application.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/getting-started.md#2025-04-10_snippet_13\n\nLANGUAGE: text\nCODE:\n```\nEvents:\n  Type    Reason                      Age   From            Message\n  ----    ------                      ----  ----            -------\n  Normal  SparkApplicationAdded       5m    spark-operator  SparkApplication spark-pi was added, enqueued it for submission\n  Normal  SparkApplicationTerminated  4m    spark-operator  SparkApplication spark-pi terminated with state: COMPLETED\n```\n\n----------------------------------------\n\nTITLE: Testing REST API Access\nDESCRIPTION: Port forward and test the Model Registry REST API endpoint.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/model-registry/installation.md#2025-04-10_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nkubectl port-forward svc/model-registry-service -n kubeflow 8081:8080\n# in another terminal:\ncurl -X 'GET' \\\n  'http://localhost:8081/api/model_registry/v1alpha3/registered_models?pageSize=100&orderBy=ID&sortOrder=DESC' \\\n  -H 'accept: application/json' | jq\n```\n\n----------------------------------------\n\nTITLE: Setting Up Source Directory Structure\nDESCRIPTION: The initial directory structure for organizing component source code, showing an empty src/ directory that will contain the component modules.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/containerized-python-components.md#2025-04-10_snippet_1\n\nLANGUAGE: txt\nCODE:\n```\nsrc/\n```\n\n----------------------------------------\n\nTITLE: Component with Custom Package Index\nDESCRIPTION: Example demonstrating use of custom pip index URLs for package installation.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/lightweight-python-components.md#2025-04-10_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n@dsl.component(packages_to_install=['custom-ml-package==0.0.1', 'numpy==1.21.6'],\n               pip_index_urls=['http://myprivaterepo.com/simple', 'http://pypi.org/simple'],\n)\ndef comp():\n    from custom_ml_package import model_trainer\n    import numpy as np\n    ...\n```\n\n----------------------------------------\n\nTITLE: Signing Off Git Commits with DCO\nDESCRIPTION: Command showing how to sign off git commits using the -s flag to pass DCO (Developer Certificate of Origin) verification checks in CI pipelines.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/developer-guide.md#2025-04-10_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\ngit commit -s -m \"Your commit message\"\n```\n\n----------------------------------------\n\nTITLE: Defining Metadata for Hyperparameter Tuning Page in TOML\nDESCRIPTION: This TOML snippet defines metadata for a web page about Hyperparameter Tuning in Kubeflow. It specifies the title, description, and weight for organizing the page within the website structure.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/hp-tuning/_index.md#2025-04-10_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n+++\ntitle = \"Hyperparameter Tuning\"\ndescription = \"User guides to run Hyperparameter Tuning Experiments\"\nweight = 10\n+++\n```\n\n----------------------------------------\n\nTITLE: Creating a Branch in Git\nDESCRIPTION: Command to create and switch to a new branch for your documentation updates. Using descriptive branch names helps track the purpose of your changes.\nSOURCE: https://github.com/kubeflow/website/blob/master/quick-github-guide.md#2025-04-10_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout -b doc-updates\n```\n\n----------------------------------------\n\nTITLE: Configuring GPU Resources in Kubeflow TFJob YAML\nDESCRIPTION: This YAML configuration demonstrates how to set up a TFJob with GPU resources. It includes parameter server and worker replicas with the worker configured to use 1 NVIDIA GPU through the resources.limits specification.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/tensorflow.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"kubeflow.org/v1\"\nkind: \"TFJob\"\nmetadata:\n  name: \"tf-smoke-gpu\"\nspec:\n  tfReplicaSpecs:\n    PS:\n      replicas: 1\n      template:\n        metadata:\n          creationTimestamp: null\n        spec:\n          containers:\n            - args:\n                - python\n                - tf_cnn_benchmarks.py\n                - --batch_size=32\n                - --model=resnet50\n                - --variable_update=parameter_server\n                - --flush_stdout=true\n                - --num_gpus=1\n                - --local_parameter_device=cpu\n                - --device=cpu\n                - --data_format=NHWC\n              image: gcr.io/kubeflow/tf-benchmarks-cpu:v20171202-bdab599-dirty-284af3\n              name: tensorflow\n              ports:\n                - containerPort: 2222\n                  name: tfjob-port\n              resources:\n                limits:\n                  cpu: \"1\"\n              workingDir: /opt/tf-benchmarks/scripts/tf_cnn_benchmarks\n          restartPolicy: OnFailure\n    Worker:\n      replicas: 1\n      template:\n        metadata:\n          creationTimestamp: null\n        spec:\n          containers:\n            - args:\n                - python\n                - tf_cnn_benchmarks.py\n                - --batch_size=32\n                - --model=resnet50\n                - --variable_update=parameter_server\n                - --flush_stdout=true\n                - --num_gpus=1\n                - --local_parameter_device=cpu\n                - --device=gpu\n                - --data_format=NHWC\n              image: gcr.io/kubeflow/tf-benchmarks-gpu:v20171202-bdab599-dirty-284af3\n              name: tensorflow\n              ports:\n                - containerPort: 2222\n                  name: tfjob-port\n              resources:\n                limits:\n                  nvidia.com/gpu: 1\n              workingDir: /opt/tf-benchmarks/scripts/tf_cnn_benchmarks\n          restartPolicy: OnFailure\n```\n\n----------------------------------------\n\nTITLE: Installing Spark Operator without Metrics using Helm\nDESCRIPTION: Command to install the Spark Operator using Helm without enabling metrics collection. This creates the operator in a dedicated namespace with metrics explicitly disabled.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/getting-started.md#2025-04-10_snippet_15\n\nLANGUAGE: shell\nCODE:\n```\nhelm install my-release spark-operator/spark-operator \\\n    --namespace spark-operator \\\n    --create-namespace \\\n    --set metrics.enable=false\n```\n\n----------------------------------------\n\nTITLE: Changing Directory to Manifests\nDESCRIPTION: Navigate to the kustomize manifests directory for deployment configuration.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/model-registry/installation.md#2025-04-10_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\ncd model-registry/manifests/kustomize\n```\n\n----------------------------------------\n\nTITLE: Final Project Directory Structure\nDESCRIPTION: The complete directory structure after creating the pipeline and component files, showing the organization of both pipeline definition and component source code.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/containerized-python-components.md#2025-04-10_snippet_8\n\nLANGUAGE: txt\nCODE:\n```\npipeline.py\nsrc/\n├── my_component.py\n└── math_utils.py\n```\n\n----------------------------------------\n\nTITLE: Converting to a Containerized Python Component\nDESCRIPTION: A modified version of the add component that imports and uses the helper function from the math_utils module, demonstrating non-hermetic dependencies in Containerized Components.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/containerized-python-components.md#2025-04-10_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# src/my_component.py\nfrom kfp import dsl\n\n@dsl.component\ndef add(a: int, b: int) -> int:\n    from math_utils import add_numbers\n    return add_numbers(a, b)\n```\n\n----------------------------------------\n\nTITLE: Checking MPI Job CRD Installation\nDESCRIPTION: Command to verify if MPIJob custom resource definition is installed in the cluster\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/mpi.md#2025-04-10_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get crd\n```\n\n----------------------------------------\n\nTITLE: Installing kind on Windows using PowerShell\nDESCRIPTION: Downloads and installs the kind executable on Windows using PowerShell commands. Requires administrative privileges.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/localcluster-deployment.md#2025-04-10_snippet_2\n\nLANGUAGE: powershell\nCODE:\n```\ncurl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/{KIND_VERSION}/kind-windows-amd64\nMove-Item .\\kind-windows-amd64.exe c:\\{YOUR_KIND_DIRECTORY}\\kind.exe\n```\n\n----------------------------------------\n\nTITLE: ENAS GetSuggestion() Output - Neural Network Configuration (JSON)\nDESCRIPTION: Example of the nn_config JSON string returned by GetSuggestion(), detailing the neural network configuration including layer count, input/output sizes, and operation definitions.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/reference/nas-algorithms.md#2025-04-10_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"num_layers\": 8,\n    \"input_sizes\": [32, 32, 3],\n    \"output_sizes\": [10],\n    \"embedding\": {\n        \"27\": {\n            \"opt_id\": 27,\n            \"opt_type\": \"convolution\",\n            \"opt_params\": {\n                \"filter_size\": \"7\",\n                \"num_filter\": \"96\",\n                \"stride\": \"2\"\n            }\n        },\n        \"29\": {\n            \"opt_id\": 29,\n            \"opt_type\": \"convolution\",\n            \"opt_params\": {\n                \"filter_size\": \"7\",\n                \"num_filter\": \"128\",\n                \"stride\": \"2\"\n            }\n        },\n        \"22\": {\n            \"opt_id\": 22,\n            \"opt_type\": \"convolution\",\n            \"opt_params\": {\n                \"filter_size\": \"7\",\n                \"num_filter\": \"48\",\n                \"stride\": \"1\"\n            }\n        },\n        \"13\": {\n            \"opt_id\": 13,\n            \"opt_type\": \"convolution\",\n            \"opt_params\": {\n                \"filter_size\": \"5\",\n                \"num_filter\": \"48\",\n                \"stride\": \"2\"\n            }\n        },\n        \"26\": {\n            \"opt_id\": 26,\n            \"opt_type\": \"convolution\",\n            \"opt_params\": {\n                \"filter_size\": \"7\",\n                \"num_filter\": \"96\",\n                \"stride\": \"1\"\n            }\n        },\n        \"30\": {\n            \"opt_id\": 30,\n            \"opt_type\": \"reduction\",\n            \"opt_params\": {\n                \"reduction_type\": \"max_pooling\",\n                \"pool_size\": 2\n            }\n        },\n        \"11\": {\n            \"opt_id\": 11,\n            \"opt_type\": \"convolution\",\n            \"opt_params\": {\n                \"filter_size\": \"5\",\n                \"num_filter\": \"32\",\n                \"stride\": \"2\"\n            }\n        },\n        \"9\": {\n            \"opt_id\": 9,\n            \"opt_type\": \"convolution\",\n            \"opt_params\": {\n                \"filter_size\": \"3\",\n                \"num_filter\": \"128\",\n                \"stride\": \"2\"\n            }\n        }\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Installing Katib with Controller Leader Election\nDESCRIPTION: This command deploys Katib with Controller Leader Election support, making the katib-controller Highly Available (HA). It's recommended for environments requiring high Service Level Agreements (SLAs) and Service Level Objectives (SLOs), such as production environments.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/installation-options.md#2025-04-10_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -k \"github.com/kubeflow/katib.git/manifests/v1beta1/installs/katib-leader-election?ref=master\"\n```\n\n----------------------------------------\n\nTITLE: Upgrading Spark Operator Helm Release\nDESCRIPTION: Command to upgrade an existing Spark Operator Helm release. This allows for updating configurations or upgrading to newer versions.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/getting-started.md#2025-04-10_snippet_6\n\nLANGUAGE: shell\nCODE:\n```\nhelm upgrade [RELEASE_NAME] spark-operator/spark-operator [flags]\n```\n\n----------------------------------------\n\nTITLE: Editing Katib Configuration Using kubectl\nDESCRIPTION: Command to edit the katib-config ConfigMap in the kubeflow namespace using kubectl.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/katib-config.md#2025-04-10_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nkubectl edit configMap katib-config -n kubeflow\n```\n\n----------------------------------------\n\nTITLE: Valid Component with Internal Constant\nDESCRIPTION: Example showing proper hermetic function definition with constants defined inside the component function body.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/lightweight-python-components.md#2025-04-10_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n@dsl.component\ndef double(a: int) -> int:\n    \"\"\"Succeeds at runtime.\"\"\"\n    VALID_CONSTANT = 2\n    return VALID_CONSTANT * a\n```\n\n----------------------------------------\n\nTITLE: Retrieving Model Metadata\nDESCRIPTION: Demonstrates how to retrieve registered model, version, and artifact metadata using the Model Registry client\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/model-registry/getting-started.md#2025-04-10_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel = registry.get_registered_model(\"iris\")\nprint(\"Registered Model:\", model, \"with ID\", model.id)\n\nversion = registry.get_model_version(\"iris\", \"v1\")\nprint(\"Model Version:\", version, \"with ID\", version.id)\n\nart = registry.get_model_artifact(\"iris\", \"v1\")\nprint(\"Model Artifact:\", art, \"with ID\", art.id)\n```\n\n----------------------------------------\n\nTITLE: Hugo Alert Shortcode for Old Version Warning\nDESCRIPTION: Hugo shortcode that creates a warning alert notifying users they are viewing documentation for an old version (v1) of the Kubeflow Training Operator and providing links to the latest documentation and migration guide.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/_index.md#2025-04-10_snippet_1\n\nLANGUAGE: hugo\nCODE:\n```\n{{% alert title=\"Old Version\" color=\"warning\" %}}\nThis page is about **Kubeflow Training Operator V1**, for the latest information check\n[the Kubeflow Trainer V2 documentation](/docs/components/trainer).\n\nFollow [this guide for migrating to Kubeflow Trainer V2](/docs/components/trainer/operator-guides/migration).\n{{% /alert %}}\n```\n\n----------------------------------------\n\nTITLE: Installing kind on macOS using Homebrew\nDESCRIPTION: Installs kind on macOS using the Homebrew package manager.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/localcluster-deployment.md#2025-04-10_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nbrew install kind\n```\n\n----------------------------------------\n\nTITLE: Sample TFJob Events Output in YAML\nDESCRIPTION: This YAML snippet shows an example of the events section from a TFJob description. It demonstrates successful creation of pods and services for a TensorFlow job.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/tensorflow.md#2025-04-10_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\nEvents:\n Type    Reason                   Age   From              Message\n ----    ------                   ----  ----              -------\n Normal  SuccessfulCreatePod      90s   tfjob-controller  Created pod: tfjob2-worker-0\n Normal  SuccessfulCreatePod      90s   tfjob-controller  Created pod: tfjob2-ps-0\n Normal  SuccessfulCreateService  90s   tfjob-controller  Created service: tfjob2-worker-0\n Normal  SuccessfulCreateService  90s   tfjob-controller  Created service: tfjob2-ps-0\n```\n\n----------------------------------------\n\nTITLE: Success Condition for Kubernetes Job\nDESCRIPTION: Default GJSON format condition to determine if a Kubernetes Job Trial has succeeded.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/trial-template.md#2025-04-10_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nstatus.conditions.#(type==\"Complete\")#|#(status==\"True\")#\n```\n\n----------------------------------------\n\nTITLE: Describing TFJob Events in Kubernetes\nDESCRIPTION: This command describes a TFJob, including its events. It's useful for checking if pods were created successfully and is most effective for jobs less than 1 hour old due to Kubernetes event retention policies.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/tensorflow.md#2025-04-10_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nkubectl -n ${USER_NAMESPACE} describe tfjobs ${JOB_NAME}\n```\n\n----------------------------------------\n\nTITLE: Installing Latest Stable Katib SDK\nDESCRIPTION: Command to install the latest stable release of Katib Python SDK using pip.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/installation.md#2025-04-10_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\npip install -U kubeflow-katib\n```\n\n----------------------------------------\n\nTITLE: Loading Component from String in Kubeflow Pipeline\nDESCRIPTION: Illustrates loading a component from a YAML string by first reading from a file and using load_component_from_text.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/load-and-share-components.md#2025-04-10_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nwith open('component.yaml') as f:\n    component_str = f.read()\n\nloaded_comp = components.load_component_from_text(component_str)\n```\n\n----------------------------------------\n\nTITLE: Deploying KServe Inference Service with Custom Storage Initializer\nDESCRIPTION: Creates a KServe inference endpoint using Model Registry's Custom Storage Initializer for model deployment\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/model-registry/getting-started.md#2025-04-10_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom kubernetes import client\nimport kserve\n\nisvc = kserve.V1beta1InferenceService(\n    api_version=kserve.constants.KSERVE_GROUP + \"/v1beta1\",\n    kind=kserve.constants.KSERVE_KIND,\n    metadata=client.V1ObjectMeta(\n        name=\"iris-model\",\n        namespace=kserve.utils.get_default_target_namespace(),\n        labels={\n            \"modelregistry/registered-model-id\": model.id,\n            \"modelregistry/model-version-id\": version.id,\n        },\n    ),\n    spec=kserve.V1beta1InferenceServiceSpec(\n        predictor=kserve.V1beta1PredictorSpec(\n            model=kserve.V1beta1ModelSpec(\n                storage_uri=\"model-registry://iris/v1\",\n                model_format=kserve.V1beta1ModelFormat(\n                    name=art.model_format_name, version=art.model_format_version\n                ),\n            )\n        )\n    ),\n)\nks_client = kserve.KServeClient()\nks_client.create(isvc)\n```\n\n----------------------------------------\n\nTITLE: Creating Alert Shortcode for Old Version Warning in Hugo Markdown\nDESCRIPTION: This snippet uses a Hugo shortcode to create an alert box warning users about the old version of the documentation and providing links to the latest information and migration guide.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/_index.md#2025-04-10_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n{{% alert title=\"Old Version\" color=\"warning\" %}}\nThis page is about **Kubeflow Training Operator V1**, for the latest information check\n[the Kubeflow Trainer V2 documentation](/docs/components/trainer).\n\nFollow [this guide for migrating to Kubeflow Trainer V2](/docs/components/trainer/operator-guides/migration).\n{{% /alert %}}\n```\n\n----------------------------------------\n\nTITLE: Loading External Kubeflow Pipeline Component\nDESCRIPTION: Loads a pre-built web downloader component from a GitHub repository URL.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/build-pipeline.ipynb#2025-04-10_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nweb_downloader_op = kfp.components.load_component_from_url(\n    'https://raw.githubusercontent.com/kubeflow/pipelines/3fa2ac5f4e04111bf5758fd5c01a2f0d7ac4b866/components/contrib/web/Download/component.yaml')\n```\n\n----------------------------------------\n\nTITLE: Using Kubernetes Version Shortcode in Markdown\nDESCRIPTION: Demonstrates how to use the Kubernetes version shortcode within a Markdown document to specify the minimum required version.\nSOURCE: https://github.com/kubeflow/website/blob/master/README.md#2025-04-10_snippet_8\n\nLANGUAGE: markdown\nCODE:\n```\nYou need Kubernetes version {{% kubernetes-min-version %}} or later.\n```\n\n----------------------------------------\n\nTITLE: Cloning a Forked Repository in Git\nDESCRIPTION: Commands to create a local directory, clone your forked repository of the Kubeflow website, and set up the working directory. This establishes your local development environment.\nSOURCE: https://github.com/kubeflow/website/blob/master/quick-github-guide.md#2025-04-10_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nmkdir kubeflow\ncd kubeflow/\ngit clone git@github.com:<your-github-username>/website.git\ncd website/\n```\n\n----------------------------------------\n\nTITLE: Sample Pod Events Output in YAML\nDESCRIPTION: This YAML snippet shows an example of the events section from a pod description. It demonstrates successful scheduling, volume mounting, image pulling, and container startup.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/tensorflow.md#2025-04-10_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\nEvents:\nType    Reason                 Age   From                                                  Message\n----    ------                 ----  ----                                                  -------\nNormal  Scheduled              18s   default-scheduler                                     Successfully assigned tfjob2-ps-0 to gke-jl-kf-v0-2-2-default-pool-347936c1-1qkt\nNormal  SuccessfulMountVolume  17s   kubelet, gke-jl-kf-v0-2-2-default-pool-347936c1-1qkt  MountVolume.SetUp succeeded for volume \"default-token-h8rnv\"\nNormal  Pulled                 17s   kubelet, gke-jl-kf-v0-2-2-default-pool-347936c1-1qkt  Container image \"gcr.io/kubeflow/tf-benchmarks-cpu:v20171202-bdab599-dirty-284af3\" already present on machine\nNormal  Created                17s   kubelet, gke-jl-kf-v0-2-2-default-pool-347936c1-1qkt  Created container\nNormal  Started                16s   kubelet, gke-jl-kf-v0-2-2-default-pool-347936c1-1qkt  Started container\n```\n\n----------------------------------------\n\nTITLE: Installing Katib on OpenShift\nDESCRIPTION: This command deploys Katib on OpenShift v4.4+. It uses OpenShift service controller instead of Katib certificate generator to provision Katib webhooks certificates.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/installation-options.md#2025-04-10_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -k \"github.com/kubeflow/katib.git/manifests/v1beta1/installs/katib-openshift?ref=master\"\n```\n\n----------------------------------------\n\nTITLE: Executing Python Program with Input and Output Paths\nDESCRIPTION: Demonstrates the recommended way to pass input and output file paths to a Python program in a Kubeflow Pipeline component. This approach allows for flexibility and avoids hardcoding paths.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/best-practices.md#2025-04-10_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nprogram.py --input-data <input path> --output-data <output path> --param 42\n```\n\n----------------------------------------\n\nTITLE: Example GET Request for Listing Artifacts in Kubeflow Model Registry API\nDESCRIPTION: Example HTTP GET request to list all Artifact entities in the Kubeflow Model Registry API. This request includes pagination parameters and sorting by ID.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/model-registry/reference/rest-api.md#2025-04-10_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nhttps://kubeflow.example.com/api/model_registry/v1alpha3/artifacts?pageSize=100&orderBy=ID\n```\n\n----------------------------------------\n\nTITLE: Configuring GCS and BigQuery Authentication Properties\nDESCRIPTION: Hadoop configuration properties required for authenticating with Google Cloud services using a service account JSON key file.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/gcp.md#2025-04-10_snippet_0\n\nLANGUAGE: properties\nCODE:\n```\ngoogle.cloud.auth.service.account.enable=true\ngoogle.cloud.auth.service.account.json.keyfile=<path to the service account JSON key file in the container>\n```\n\n----------------------------------------\n\nTITLE: Describing Kubeflow Edit Permissions in Kubernetes\nDESCRIPTION: Command to view the full list of RBAC permissions assigned to the kubeflow-edit ClusterRole that is bound to Notebook pods.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/notebooks/submit-kubernetes.md#2025-04-10_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nkubectl describe clusterrole kubeflow-edit\n```\n\n----------------------------------------\n\nTITLE: Accessing TFJob Pod Logs in Kubeflow\nDESCRIPTION: Command to retrieve logs from a specific TensorFlow job pod in Kubeflow. This helps debug application issues by examining standard output from running containers.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/tensorflow.md#2025-04-10_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nkubectl logs ${PODNAME}\n```\n\n----------------------------------------\n\nTITLE: Installing Kubeflow Python SDK\nDESCRIPTION: Command to install the latest Kubeflow Python SDK directly from the source repository.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/getting-started.md#2025-04-10_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install git+https://github.com/kubeflow/trainer.git@master#subdirectory=sdk\n```\n\n----------------------------------------\n\nTITLE: Creating PVC in Pipeline Definition\nDESCRIPTION: Pipeline definition showing how to create a PersistentVolumeClaim using the kubernetes.CreatePVC component with specified storage parameters.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/platform-specific-features.md#2025-04-10_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import kubernetes\n\n@dsl.pipeline\ndef my_pipeline():\n    pvc1 = kubernetes.CreatePVC(\n        # can also use pvc_name instead of pvc_name_suffix to use a pre-existing PVC\n        pvc_name_suffix='-my-pvc',\n        access_modes=['ReadWriteMany'],\n        size='5Gi',\n        storage_class_name='standard',\n    )\n```\n\n----------------------------------------\n\nTITLE: Setting up gRPC Testing for Hyperopt Service in Katib\nDESCRIPTION: This code creates a unit test for the Hyperopt suggestion service in Katib using Python's unittest framework and gRPC testing utilities. It demonstrates how to set up a test gRPC server with the HyperoptService servicer for testing Katib's hyperparameter optimization algorithms.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/hp-tuning/configure-algorithm.md#2025-04-10_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport grpc\nimport grpc_testing\nimport unittest\n\nfrom pkg.apis.manager.v1beta1.python import api_pb2_grpc\nfrom pkg.apis.manager.v1beta1.python import api_pb2\n\nfrom pkg.suggestion.v1beta1.hyperopt.service import HyperoptService\n\nclass TestHyperopt(unittest.TestCase):\n    def setUp(self):\n        servicers = {\n            api_pb2.DESCRIPTOR.services_by_name['Suggestion']: HyperoptService()\n        }\n\n        self.test_server = grpc_testing.server_from_dictionary(\n            servicers, grpc_testing.strict_real_time())\n\n\nif __name__ == '__main__':\n    unittest.main()\n```\n\n----------------------------------------\n\nTITLE: Kubeflow 1.6.0 Release Information Table Structure\nDESCRIPTION: HTML table structure defining the release information including release date, media links, manifests, and release team details.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/releases/kubeflow-1.6.md#2025-04-10_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"table-responsive\">\n<table class=\"table table-bordered\">\n  <tbody>\n    <tr>\n      <th class=\"table-light\">Release Date</th>\n      <td>\n        2022-09-07\n      </td>\n    </tr>\n    <!-- Additional table content... -->\n  </tbody>\n</table>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Chaining Task Configuration Methods in KFP v2\nDESCRIPTION: This example shows how to chain multiple task configuration methods to apply multiple settings to a single pipeline task in a concise manner.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/compose-components-into-pipelines.md#2025-04-10_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nprint_env_var().set_env_variable('MY_ENV_VAR', 'hello').set_env_variable('OTHER_VAR', 'world')\n```\n\n----------------------------------------\n\nTITLE: YuniKorn Pod Events for Gang Scheduled Spark Driver\nDESCRIPTION: Output showing the Kubernetes events for a Spark driver pod when scheduled with YuniKorn. Shows the gang scheduling process including queuing, allocation, and binding to a node.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/yunikorn-integration.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nType    Reason             Age   From      Message\n----    ------             ----  ----      -------\nNormal  Scheduling         20s   yunikorn  default/spark-pi-yunikorn-driver is queued and waiting for allocation\nNormal  GangScheduling     20s   yunikorn  Pod belongs to the taskGroup spark-driver, it will be scheduled as a gang member\nNormal  Scheduled          19s   yunikorn  Successfully assigned default/spark-pi-yunikorn-driver to node spark-operator-worker\nNormal  PodBindSuccessful  19s   yunikorn  Pod default/spark-pi-yunikorn-driver is successfully bound to node spark-operator-worker\nNormal  TaskCompleted      4s    yunikorn  Task default/spark-pi-yunikorn-driver is completed\nNormal  Pulling            20s   kubelet   Pulling image \"spark:3.5.2\"\nNormal  Pulled             13s   kubelet   Successfully pulled image \"spark:3.5.2\" in 6.162s (6.162s including waiting)\nNormal  Created            13s   kubelet   Created container spark-kubernetes-driver\nNormal  Started            13s   kubelet   Started container spark-kubernetes-driver\n```\n\n----------------------------------------\n\nTITLE: Initialization Parameters Structure in Katib Config\nDESCRIPTION: YAML structure showing how initialization parameters are organized in the Katib Config. It includes settings for the certificate generator and controller components.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/katib-config.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: config.kubeflow.org/v1beta1\nkind: KatibConfig\ninit:\n  certGenerator:\n    enable: true\n    ...\n  controller:\n    trialResources:\n      - Job.v1.batch\n      - TFJob.v1.kubeflow.org\n    ...\n```\n\n----------------------------------------\n\nTITLE: Retrieving TFJob Logs with gcloud in GKE\nDESCRIPTION: Shell commands for retrieving TensorFlow job logs using gcloud CLI in Google Kubernetes Engine. This provides an alternative to the Stackdriver UI for filtering and viewing logs.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/tensorflow.md#2025-04-10_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\nQUERY=\"resource.type=\\\"k8s_container\\\" \"\nQUERY=\"${QUERY} resource.labels.cluster_name=\\\"${CLUSTER}\\\" \"\nQUERY=\"${QUERY} metadata.userLabels.job-name=\\\"${JOB_NAME}\\\" \"\nQUERY=\"${QUERY} metadata.userLabels.replica-type=\\\"${TYPE}\\\" \"\nQUERY=\"${QUERY} metadata.userLabels.replica-index=\\\"${INDEX}\\\" \"\ngcloud --project=${PROJECT} logging read  \\\n     --freshness=24h \\\n     --order asc  ${QUERY}\n```\n\n----------------------------------------\n\nTITLE: Building the Spark Operator Binary\nDESCRIPTION: Command to build the Spark operator binary using the Makefile, which will be placed in the 'bin' directory.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/developer-guide.md#2025-04-10_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nmake build-operator\n```\n\n----------------------------------------\n\nTITLE: Installing Katib Control Plane Stable Release\nDESCRIPTION: Command to install the stable v0.17.0 release of Katib control plane using kubectl and kustomize.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/installation.md#2025-04-10_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -k \"github.com/kubeflow/katib.git/manifests/v1beta1/installs/katib-standalone?ref=v0.17.0\"\n```\n\n----------------------------------------\n\nTITLE: Modifying Katib Controller Parameters for Tekton Integration\nDESCRIPTION: YAML configuration showing how to add Tekton PipelineRun as a trial resource in Katib config.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/trial-template.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\ntrialResources:\n  - PipelineRun.v1beta1.tekton.dev\n```\n\n----------------------------------------\n\nTITLE: HTML Dependency Version Table\nDESCRIPTION: HTML table structure displaying dependency versions for various components like Kubernetes, Istio, cert-manager, and others with validation notes.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/releases/kubeflow-1.8.md#2025-04-10_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"table-responsive\">\n<table class=\"table table-bordered\">\n    <thead class=\"thead-light\">\n      <tr>\n        <th>Dependency</th>\n        <th>Validated or Included Version(s)</th>\n        <th>Notes</th>\n      </tr>\n    </thead>\n  <tbody>\n      <!-- Dependency rows -->\n  </tbody>\n</table>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Hadoop Configuration in SparkApplication\nDESCRIPTION: Example demonstrating how to specify Hadoop-specific configuration properties in a SparkApplication manifest.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/writing-sparkapplication.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  hadoopConf:\n    \"fs.gs.project.id\": spark\n    \"fs.gs.system.bucket\": spark\n    \"google.cloud.auth.service.account.enable\": true\n    \"google.cloud.auth.service.account.json.keyfile\": /mnt/secrets/key.json\n```\n\n----------------------------------------\n\nTITLE: Table Structure - Component Versions\nDESCRIPTION: HTML table structure displaying component versions organized by working groups including AutoML, Notebooks, Pipelines, Serving, Training, and Data groups.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/releases/kubeflow-1.9.md#2025-04-10_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"table-responsive\">\n<table class=\"table table-bordered\">\n    <thead class=\"thead-light\">\n      <tr>\n        <th>Maintainers</th>\n        <th>Component Name</th>\n        <th>Version</th>\n      </tr>\n    </thead>\n  <tbody>\n      <!-- Component rows -->\n  </tbody>\n</table>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Listing XGBoostJob Pods\nDESCRIPTION: Command to list all pods created by the XGBoostJob with a specific job name label.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/xgboost.md#2025-04-10_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pods -l job-name=xgboost-dist-iris-test-train\n```\n\n----------------------------------------\n\nTITLE: Migrating Custom Job Component Creation in Python\nDESCRIPTION: Demonstrates the change from KFP v1's experimental run_as_aiplatform_custom_job to using Google Cloud Pipeline Component's create_custom_training_job_from_component function.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/migration.md#2025-04-10_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import components\nfrom kfp.v2 import dsl\nfrom kfp.v2.google.experimental import run_as_aiplatform_custom_job\n\ntraining_op = components.load_component_from_url(...)\n\n@dsl.pipeline(name='my-pipeline')\ndef pipeline():\n  training_task = training_op(...)\n  run_as_aiplatform_custom_job(\n      training_task, ...)\n```\n\nLANGUAGE: python\nCODE:\n```\n# pip install google-cloud-pipeline-components\nfrom kfp import components\nfrom kfp import dsl\nfrom google_cloud_pipeline_components.v1.custom_job import utils\n\ntraining_op = components.load_component_from_url(...)\n\n@dsl.pipeline(name='my-pipeline')\ndef pipeline():\n    utils.create_custom_training_job_from_component(training_op, ...)\n```\n\n----------------------------------------\n\nTITLE: Configuring Bayesian Optimization Algorithm in Katib\nDESCRIPTION: This snippet details the configuration options for the Bayesian Optimization algorithm in Katib. It uses the Scikit-Optimize (skopt) framework and supports various settings including base estimator, number of initial points, acquisition function, and optimizer.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/hp-tuning/configure-algorithm.md#2025-04-10_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n<div class=\"table-responsive\">\n  <table class=\"table table-bordered\">\n    <thead class=\"thead-light\">\n      <tr>\n        <th>Setting Name</th>\n        <th>Description</th>\n        <th>Example</th>\n      </tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td>base_estimator</td>\n        <td>[\"GP\", \"RF\", \"ET\", \"GBRT\" or sklearn regressor, default=\"GP\"]:\n          Should inherit from <code>sklearn.base.RegressorMixin</code>.\n          The <code>predict</code> method should have an optional\n          <code>return_std</code> argument, which returns\n          <code>std(Y | x)</code> along with <code>E[Y | x]</code>. If\n          <code>base_estimator</code> is one of\n          [\"GP\", \"RF\", \"ET\", \"GBRT\"], the system uses a default surrogate model\n          of the corresponding type. Learn more information in the\n          <a href=\"https://scikit-optimize.github.io/stable/modules/generated/skopt.Optimizer.html#skopt.Optimizer\">skopt\n          documentation</a>.</td>\n        <td>GP</td>\n      </tr>\n      <tr>\n        <td>n_initial_points</td>\n        <td>[int, default=10]: Number of evaluations of <code>func</code> with\n          initialization points before approximating it with\n          <code>base_estimator</code>. Points provided as <code>x0</code> count\n          as initialization points.\n          If <code>len(x0) &lt; n_initial_points</code>, the\n          system samples additional points at random. Learn more information in the\n          <a href=\"https://scikit-optimize.github.io/stable/modules/generated/skopt.Optimizer.html#skopt.Optimizer\">skopt\n          documentation</a>.</td>\n        <td>10</td>\n      </tr>\n      <tr>\n        <td>acq_func</td>\n        <td>[string, default=<code>&quot;gp_hedge&quot;</code>]: The function to\n          minimize over the posterior distribution. Learn more information in the\n          <a href=\"https://scikit-optimize.github.io/stable/modules/generated/skopt.Optimizer.html#skopt.Optimizer\">skopt\n          documentation</a>.</td>\n        <td>gp_hedge</td>\n      </tr>\n      <tr>\n        <td>acq_optimizer</td>\n        <td>[string, \"sampling\" or \"lbfgs\", default=\"auto\"]: The method to\n          minimize the acquisition function. The system updates the fit model\n          with the optimal value obtained by optimizing <code>acq_func</code>\n          with <code>acq_optimizer</code>. Learn more information in the\n          <a href=\"https://scikit-optimize.github.io/stable/modules/generated/skopt.Optimizer.html#skopt.Optimizer\">skopt\n          documentation</a>.</td>\n        <td>auto</td>\n      </tr>\n      <tr>\n        <td>random_state</td>\n        <td>[int]: Set <code>random_state</code> to something other than None\n          for reproducible results.</td>\n        <td>10</td>\n      </tr>\n    </tbody>\n  </table>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Adding Katib Metrics Label to Existing Namespace\nDESCRIPTION: Kubectl command to add the metrics collector injection label to an existing namespace.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/metrics-collector.md#2025-04-10_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl label namespace <your-namespace> katib.kubeflow.org/metrics-collector-injection=enabled\n```\n\n----------------------------------------\n\nTITLE: HTML Dependency Version Table Structure\nDESCRIPTION: HTML markup showing dependency versions table for Kubeflow manifests including Kubernetes, Istio, cert-manager and other infrastructure components\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/releases/kubeflow-1.8.md#2025-04-10_snippet_5\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"table-responsive\">\n<table class=\"table table-bordered\">\n    <thead class=\"thead-light\">\n      <tr>\n        <th>Dependency</th>\n        <th>Validated or Included Version(s)</th>\n        <th>Notes</th>\n      </tr>\n    </thead>\n    <!-- Dependency version content -->\n</table>\n</div>\n```\n\n----------------------------------------\n\nTITLE: HTML Dependency Version Table\nDESCRIPTION: HTML table structure showing Kubeflow dependency versions including Kubernetes, Istio, cert-manager, and other infrastructure components with their validated versions.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/releases/kubeflow-1.9.md#2025-04-10_snippet_4\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"table-responsive\">\n<table class=\"table table-bordered\">\n    <thead class=\"thead-light\">\n      <tr>\n        <th>Dependency</th>\n        <th>Validated or Included Version(s)</th>\n        <th>Notes</th>\n      </tr>\n    </thead>\n  <tbody>\n      <tr>\n        <td>\n          <a href=\"https://kubernetes.io/\">Kubernetes</a>\n        </td>\n        <td>1.29</td>\n        <td rowspan=\"4\" class=\"align-middle\">\n          <i>Other versions may work, but have not been validated</i>\n        </td>\n      </tr>\n      <!-- Additional rows omitted for brevity -->\n  </tbody>\n</table>\n</div>\n```\n\n----------------------------------------\n\nTITLE: YAML Front Matter Configuration for SparkApplication Documentation\nDESCRIPTION: YAML front matter block defining metadata for a documentation page about the SparkApplication v1beta2 API. Includes title, description, manual link reference, icon specification and weight for page ordering.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/reference/api-docs.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: SparkApplication (v1beta2)\ndescription: Spark Operator `v1beta2` API documentation\nmanualLink: https://github.com/kubeflow/spark-operator/blob/master/docs/api-docs.md#sparkapplication\nicon: \"fa-solid fa-arrow-up-right-from-square\"\nweight: 50\n---\n```\n\n----------------------------------------\n\nTITLE: Deleting PVC After Pipeline Completion\nDESCRIPTION: Demonstrates how to clean up the PVC resource after the pipeline tasks are completed using kubernetes.DeletePVC.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/platform-specific-features.md#2025-04-10_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n    delete_pvc1 = kubernetes.DeletePVC(\n        pvc_name=pvc1.outputs['name']\n    ).after(task2)\n```\n\n----------------------------------------\n\nTITLE: Deploying a JAXJob Resource for Training\nDESCRIPTION: Command to deploy a JAXJob resource using kubectl to start the training process. It uses a predefined example from the Kubeflow training-operator repository.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/jax.md#2025-04-10_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -f https://raw.githubusercontent.com/kubeflow/training-operator/refs/heads/release-1.9/examples/jax/cpu-demo/demo.yaml\n```\n\n----------------------------------------\n\nTITLE: Migrating AIPlatformClient Integration in Python\nDESCRIPTION: Shows the transition from KFP SDK v1's AIPlatformClient to using the official Python Vertex SDK's PipelineJob class for submitting pipelines to Vertex AI Pipelines.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/migration.md#2025-04-10_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp.v2.google.client import AIPlatformClient\n\napi_client = AIPlatformClient(\n    project_id=PROJECT_ID,\n    region=REGION,\n)\n\nresponse = api_client.create_run_from_job_spec(\n    job_spec_path=PACKAGE_PATH, pipeline_root=PIPELINE_ROOT,\n)\n```\n\nLANGUAGE: python\nCODE:\n```\n# pip install google-cloud-aiplatform\nfrom google.cloud import aiplatform\n\naiplatform.init(\n    project=PROJECT_ID,\n    location=REGION,\n)\n\njob = aiplatform.PipelineJob(\n    display_name=DISPLAY_NAME,\n    template_path=PACKAGE_PATH,\n    pipeline_root=PIPELINE_ROOT,\n)\n\njob.submit()\n```\n\n----------------------------------------\n\nTITLE: Configuring Tree of Parzen Estimators (TPE) Algorithm in Katib\nDESCRIPTION: This snippet shows the configuration options for the Tree of Parzen Estimators (TPE) algorithm in Katib. It uses Hyperopt, Goptuna, or Optuna optimization frameworks and supports settings for the number of EI candidates, random state, gamma, and prior weight.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/hp-tuning/configure-algorithm.md#2025-04-10_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n<div class=\"table-responsive\">\n  <table class=\"table table-bordered\">\n    <thead class=\"thead-light\">\n      <tr>\n        <th>Setting name</th>\n        <th>Description</th>\n        <th>Example</th>\n      </tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td>n_EI_candidates</td>\n        <td>[int]: Number of candidate samples used to calculate the expected improvement.</td>\n        <td>25</td>\n      </tr>\n      <tr>\n        <td>random_state</td>\n        <td>[int]: Set <code>random_state</code> to something other than None\n          for reproducible results.</td>\n        <td>10</td>\n      </tr>\n      <tr>\n        <td>gamma</td>\n        <td>[float]: The threshold to split between l(x) and g(x), check equation 2 in\n        <a href=\"https://papers.nips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf\">\n        this Paper</a>. Value must be in (0, 1) range.</td>\n        <td>0.25</td>\n      </tr>\n      <tr>\n        <td>prior_weight</td>\n        <td>[float]: Smoothing factor for counts, to avoid having 0 probability.\n        Value must be > 0.</td>\n        <td>1.1</td>\n      </tr>\n    </tbody>\n  </table>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Installing Katib with Cert Manager\nDESCRIPTION: This command deploys Katib with Cert Manager requirement. It uses Cert Manager instead of Katib certificate generator to provision Katib webhooks certificates. Cert Manager must be deployed on the Kubernetes cluster before deploying Katib using this installation.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/installation-options.md#2025-04-10_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -k \"github.com/kubeflow/katib.git/manifests/v1beta1/installs/katib-cert-manager?ref=master\"\n```\n\n----------------------------------------\n\nTITLE: Configuring Emeritus Approvers in Kubeflow OWNERS File\nDESCRIPTION: Example showing how to specify emeritus approvers in an OWNERS file. Emeritus approvers are previous contributors who are no longer actively approving code but remain domain experts and can provide valuable insight.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/about/contributing.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nemeritus_approvers:\n  - david\n  - emily\n```\n\n----------------------------------------\n\nTITLE: Configuring Pipeline Benchmark Parameters\nDESCRIPTION: Essential configuration parameters needed to run the Kubeflow Pipelines benchmark script. This includes the API server host URL, pipeline manifest location, number of runs to execute, and polling interval for run status checks.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/tutorials/benchmark-examples.md#2025-04-10_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nhost = \"<your-kfp-api-server-url>\" \npipeline_file_url = \"https://storage.googleapis.com/ml-pipeline/sample-benchmark/taxi_updated_pool.yaml\"\nnum_runs = 50 \nrun_status_polling_interval_sec = 60\n```\n\n----------------------------------------\n\nTITLE: Configuring Frontend Server Environment Variables in YAML\nDESCRIPTION: This YAML snippet shows how to set environment variables for the ml-pipeline-ui deployment in Kubeflow Pipelines. It demonstrates the configuration of ALLOWED_ARTIFACT_DOMAIN_REGEX for controlling artifact storage endpoint access.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/operator-guides/server-config.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nml-pipeline-ui-deployment.yaml#L32-L50\n```\n\n----------------------------------------\n\nTITLE: Viewing JAXJob Training Pods\nDESCRIPTION: Command to list pods created by the JAXJob with specific labels to track the training job's resources.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/jax.md#2025-04-10_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get pods -n kubeflow -l training.kubeflow.org/job-name=jaxjob-simple\n```\n\n----------------------------------------\n\nTITLE: Migrating Function-Based Components in Python\nDESCRIPTION: Shows how to migrate from v1's create_component_from_func and func_to_container_op to v2's @dsl.component decorator for defining pipeline components.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/migration.md#2025-04-10_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\n\n@dsl.component\ndef component1(...):\n    ...\n\n@dsl.component\ndef component2(...):\n    ...\n\n@dsl.component\ndef component3(...):\n    ...\n\n@dsl.pipeline(name='my-pipeline')\ndef pipeline():\n    component1(...)\n    component2(...)\n    component3(...)\n```\n\n----------------------------------------\n\nTITLE: Disabling Istio Sidecar Injection in Katib Trial Template\nDESCRIPTION: This YAML snippet shows how to disable Istio sidecar injection for a Katib Trial by adding the appropriate annotation to the trial specification. This is necessary when trials require internet access to download datasets.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/hp-tuning/configure-experiment.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ntrialSpec:\n  apiVersion: batch/v1\n  kind: Job\n  spec:\n    template:\n      metadata:\n        annotations:\n          \"sidecar.istio.io/inject\": \"false\"\n```\n\n----------------------------------------\n\nTITLE: Installing Katib with PostgreSQL Database\nDESCRIPTION: This command deploys Katib with PostgreSQL database instead of the default MySQL. It provides an alternative database option for Katib installation.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/installation-options.md#2025-04-10_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -k \"github.com/kubeflow/katib.git/manifests/v1beta1/installs/katib-standalone-postgres?ref=master\"\n```\n\n----------------------------------------\n\nTITLE: Using Hugo Shortcode in Markdown\nDESCRIPTION: Demonstrates how to invoke a Hugo shortcode within a Markdown document. The shortcode name is wrapped in braces and percent signs.\nSOURCE: https://github.com/kubeflow/website/blob/master/README.md#2025-04-10_snippet_5\n\nLANGUAGE: markdown\nCODE:\n```\n{{% shortcode-name %}}\n```\n\n----------------------------------------\n\nTITLE: Hugo Shortcode Alert Notice\nDESCRIPTION: Alert shortcode providing a notice about external add-on ownership using Hugo templating syntax\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/external-add-ons/_index.md#2025-04-10_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n{{% alert title=\"Ownership of External Add-Ons\" color=\"info\" %}}\nThese add-ons are <strong>not owned or maintained</strong> by the Kubeflow project, they are developed and supported by their respective maintainers.\n{{% /alert %}}\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding to Access Kubeflow Pipelines UI\nDESCRIPTION: Command to set up port forwarding to access the Kubeflow Pipelines UI after disabling the public endpoint. This forwards local port 8080 to the ml-pipeline-ui service in the kubeflow namespace.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/standalone-deployment.md#2025-04-10_snippet_10\n\nLANGUAGE: bash\nCODE:\n```\nkubectl port-forward -n kubeflow svc/ml-pipeline-ui 8080:80\n```\n\n----------------------------------------\n\nTITLE: HTML Table Structure for Kubeflow 1.2 Release Details\nDESCRIPTION: HTML table structure displaying key information about Kubeflow 1.2 release including release date, media links, manifests, and release team details. Uses responsive table classes and organized layout.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/releases/kubeflow-1.2.md#2025-04-10_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"table-responsive\">\n<table class=\"table table-bordered\">\n  <tbody>\n    <tr>\n      <th class=\"table-light\">Release Date</th>\n      <td>\n        2020-11-18\n      </td>\n    </tr>\n    <tr>\n      <th class=\"table-light\">Media</th>\n      <td>\n        <b>Blog:</b> \n          <a href=\"https://blog.kubeflow.org/release/official/2020/11/18/kubeflow-1.2-blog-post.html\">Kubeflow 1.2 Release Announcement</a>\n        <br>\n        <b>Roadmap:</b>\n          <a href=\"https://github.com/kubeflow/kubeflow/blob/master/ROADMAP.md#kubeflow-12-features-release-date-november-2020\">Kubeflow 1.2 Features</a>\n      </td>\n    </tr>\n    <tr>\n      <th class=\"table-light\">Manifests</th>\n      <td>\n        <b>Release:</b> \n          <a href=\"https://github.com/kubeflow/manifests/releases/tag/v1.2.0\">v1.2.0</a>\n        <br>\n        <b>Branch:</b>\n          <a href=\"https://github.com/kubeflow/manifests/tree/v1.2-branch\">v1.2-branch</a>\n      </td>\n    </tr>\n    <tr>\n      <th class=\"table-light\">Release Team</th>\n      <td>\n        <b>Lead:</b> Jiaxin Shan (<a href=\"https://github.com/Jeffwan\">@Jeffwan</a>)\n      </td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Monitoring PaddleJob Training Logs\nDESCRIPTION: Commands to fetch and stream logs from the worker pod to monitor training progress.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/paddle.md#2025-04-10_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nPODNAME=$(kubectl get pods -l job-name=paddle-simple-cpu,replica-type=worker,replica-index=0 -o name -n kubeflow)\nkubectl logs -f ${PODNAME} -n kubeflow\n```\n\n----------------------------------------\n\nTITLE: Accessing Katib UI Local URL\nDESCRIPTION: URL to access the Katib UI interface after port-forwarding the service. This endpoint provides access to the standalone Katib interface.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/katib-ui.md#2025-04-10_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nhttp://localhost:8080/katib/\n```\n\n----------------------------------------\n\nTITLE: Configuring Early Stopping Algorithm Settings in YAML\nDESCRIPTION: YAML configuration structure for enabling early stopping in Katib Experiments. The configuration includes the algorithm name and settings under the .spec.earlyStopping parameter, similar to .spec.algorithm configuration.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/early-stopping.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n.spec.earlyStopping:\n  algorithmName: medianstop\n  algorithmSettings:\n    - name: min_trials_required\n      value: \"3\"\n    - name: start_step\n      value: \"4\"\n```\n\n----------------------------------------\n\nTITLE: Installing Specific Version of Katib SDK\nDESCRIPTION: Command to install Katib Python SDK from a specific GitHub commit.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/installation.md#2025-04-10_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npip install git+https://github.com/kubeflow/katib.git@ea46a7f2b73b2d316b6b7619f99eb440ede1909b#subdirectory=sdk/python/v1beta1\n```\n\n----------------------------------------\n\nTITLE: Example Markdown Front Matter\nDESCRIPTION: Sample front matter configuration for a Markdown page showing title, description, and weight properties for page ordering.\nSOURCE: https://github.com/kubeflow/website/blob/master/README.md#2025-04-10_snippet_3\n\nLANGUAGE: markdown\nCODE:\n```\n+++\ntitle = \"Getting Started with Kubeflow\"\ndescription = \"Overview\"\nweight = 1\n+++\n```\n\n----------------------------------------\n\nTITLE: Creating Responsive Table for GSoC 2025 Key Dates in HTML\nDESCRIPTION: This HTML snippet creates a responsive table displaying the key dates for Google Summer of Code 2025. It uses Bootstrap classes for styling and responsiveness.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/events/upcoming-events/gsoc-2025.md#2025-04-10_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"table-responsive\">\n<div class=\"table table-bordered\">\n\n| Event                            | Date                 |\n| -------------------------------- | -------------------- |\n| **Applications Open**            | March 24 @ 18:00 UTC |\n| **Applications Deadline**        | April 8 @ 18:00 UTC  |\n| **Accepted Proposals Announced** | May 8                |\n| **Community Bonding**            | May 8 - June 1       |\n| **Coding Begins**                | June 2               |\n| **Midterm Evaluations**          | July 14 - 18         |\n| **Coding Ends**                  | September 1          |\n| **Final Evaluations**            | September 1 - 8      |\n\n</div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Kubeflow Pipelines Using Local Manifests\nDESCRIPTION: Commands to uninstall Kubeflow Pipelines using manifests from a local repository or file system. These kubectl delete commands remove both environment-specific and cluster-scoped resources.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/standalone-deployment.md#2025-04-10_snippet_12\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete -k manifests/kustomize/env/dev\nkubectl delete -k manifests/kustomize/cluster-scoped-resources\n```\n\n----------------------------------------\n\nTITLE: Defining Markdown Frontmatter for Spark Operator Guide\nDESCRIPTION: YAML frontmatter block that defines metadata for a documentation page about Spark Operator user guides, including title, description and navigation weight.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/_index.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n---\ntitle: User Guide\ndescription: User guides for Spark Operator\nweight: 40\n---\n```\n\n----------------------------------------\n\nTITLE: Embedding Google Calendar in HTML with JavaScript\nDESCRIPTION: This code snippet embeds a Google Calendar in an HTML page with JavaScript that detects the user's timezone. The calendar displays Kubeflow and KServe community meetings in an AGENDA view, allowing participants to see upcoming community events.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/events/past-events/2024/gsoc-2024.md#2025-04-10_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<style>\n#calendar-container {\n   overflow: auto;\n}\n</style>\n<div id=\"calendar-container\"></div>\n```\n\nLANGUAGE: javascript\nCODE:\n```\nconst timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;\nconst calender_src_list = [\n  // Kubeflow Community\n  \"kubeflow.org_7l5vnbn8suj2se10sen81d9428%40group.calendar.google.com\",\n  // KServe Community\n  \"4fqdmu5fp4l0bgdlf4lm1atnsl2j4612%40import.calendar.google.com\",\n];\nlet calender_src = calender_src_list.map(src => `&src=${src}&color=%23A79B8E`).join('');\nconst html = `<iframe src=\"https://calendar.google.com/calendar/embed?ctz=${timezone}&height=600&wkst=1&bgcolor=%23ffffff&showPrint=0&showDate=1&mode=AGENDA&showTitle=0${calender_src}\" style=\"border:solid 1px #777\" width=\"800\" height=\"600\" frameborder=\"0\" scrolling=\"no\"></iframe>`;\ndocument.getElementById('calendar-container').innerHTML = html;\n```\n\n----------------------------------------\n\nTITLE: PaddleJob Status YAML Example\nDESCRIPTION: Example YAML output showing the complete status of a successfully completed PaddleJob, including job specifications and status conditions.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/paddle.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: kubeflow.org/v1\nkind: PaddleJob\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"kubeflow.org/v1\",\"kind\":\"PaddleJob\",\"metadata\":{\"annotations\":{},\"name\":\"paddle-simple-cpu\",\"namespace\":\"kubeflow\"},\"spec\":{\"paddleReplicaSpecs\":{\"Worker\":{\"replicas\":2,\"restartPolicy\":\"OnFailure\",\"template\":{\"spec\":{\"containers\":[{\"args\":[\"-m\",\"paddle.distributed.launch\",\"run_check\"],\"command\":[\"python\"],\"image\":\"registry.baidubce.com/paddlepaddle/paddle:2.4.0rc0-cpu\",\"imagePullPolicy\":\"Always\",\"name\":\"paddle\",\"ports\":[{\"containerPort\":37777,\"name\":\"master\"}]}]}}}}}}\n  creationTimestamp: \"2022-10-24T03:47:45Z\"\n  generation: 3\n  name: paddle-simple-cpu\n  namespace: kubeflow\n  resourceVersion: \"266235056\"\n  selfLink: /apis/kubeflow.org/v1/namespaces/kubeflow/paddlejobs/paddle-simple-cpu\n  uid: 7ef4f92f-0ed4-4a35-b10a-562b79538cc6\nspec:\n  paddleReplicaSpecs:\n    Worker:\n      replicas: 2\n      restartPolicy: OnFailure\n      template:\n        spec:\n          containers:\n          - args:\n            - -m\n            - paddle.distributed.launch\n            - run_check\n            command:\n            - python\n            image: registry.baidubce.com/paddlepaddle/paddle:2.4.0rc0-cpu\n            imagePullPolicy: Always\n            name: paddle\n            ports:\n            - containerPort: 37777\n              name: master\n              protocol: TCP\nstatus:\n  completionTime: \"2022-10-24T04:04:43Z\"\n  conditions:\n  - lastTransitionTime: \"2022-10-24T03:47:45Z\"\n    lastUpdateTime: \"2022-10-24T03:47:45Z\"\n    message: PaddleJob paddle-simple-cpu is created.\n    reason: PaddleJobCreated\n    status: \"True\"\n    type: Created\n  - lastTransitionTime: \"2022-10-24T04:04:28Z\"\n    lastUpdateTime: \"2022-10-24T04:04:28Z\"\n    message: PaddleJob kubeflow/paddle-simple-cpu is running.\n    reason: JobRunning\n    status: \"False\"\n    type: Running\n  - lastTransitionTime: \"2022-10-24T04:04:43Z\"\n    lastUpdateTime: \"2022-10-24T04:04:43Z\"\n    message: PaddleJob kubeflow/paddle-simple-cpu successfully completed.\n    reason: JobSucceeded\n    status: \"True\"\n    type: Succeeded\n  replicaStatuses:\n    Worker:\n      labelSelector:\n        matchLabels:\n          group-name: kubeflow.org\n          job-name: paddle-simple-cpu\n          training.kubeflow.org/job-name: paddle-simple-cpu\n          training.kubeflow.org/operator-name: paddlejob-controller\n          training.kubeflow.org/replica-type: Worker\n      succeeded: 2\n  startTime: \"2022-10-24T03:47:45Z\"\n```\n\n----------------------------------------\n\nTITLE: Katib DB Manager Service Address Format\nDESCRIPTION: Format string used by Katib Controller to connect to DB Manager service using environment variables for namespace, service name and port.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/env-variables.md#2025-04-10_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nKATIB_DB_MANAGER_SERVICE_IP.KATIB_DB_MANAGER_SERVICE_NAMESPACE:KATIB_DB_MANAGER_SERVICE_PORT\n```\n\n----------------------------------------\n\nTITLE: JSON Format for File-based Metrics Collection\nDESCRIPTION: Example of JSON format for metrics logging when using file-based collection. Each metric must be line-separated by epoch or step and include a timestamp.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/metrics-collector.md#2025-04-10_snippet_2\n\nLANGUAGE: json\nCODE:\n```\n{\"epoch\": 0, \"foo\": \"bar\", \"fizz\": \"buzz\", \"timestamp\": \"2021-12-02T14:27:51\"}\n{\"epoch\": 1, \"foo\": \"bar\", \"fizz\": \"buzz\", \"timestamp\": \"2021-12-02T14:27:52\"}\n{\"epoch\": 2, \"foo\": \"bar\", \"fizz\": \"buzz\", \"timestamp\": \"2021-12-02T14:27:53\"}\n{\"epoch\": 3, \"foo\": \"bar\", \"fizz\": \"buzz\", \"timestamp\": \"2021-12-02T14:27:54\"}\n```\n\n----------------------------------------\n\nTITLE: Setting Kernel Timeout in Visualization Deployment YAML\nDESCRIPTION: YAML configuration example showing how to set the KERNEL_TIMEOUT environment variable in the visualization service deployment to increase the default timeout length from 30 seconds to 100 seconds. This allows visualizations to run for longer periods before timing out.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/python-based-visualizations.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\n- env:\n  - name: KERNEL_TIMEOUT\n    value: 100\n```\n\n----------------------------------------\n\nTITLE: Running Helm Chart Unit Tests\nDESCRIPTION: Command to run Helm chart unit tests for the Spark operator project.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/developer-guide.md#2025-04-10_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\n$ make helm-unittest \n```\n\n----------------------------------------\n\nTITLE: Getting TFJob Logs with Kubeflow Training Operator\nDESCRIPTION: This code demonstrates how to fetch logs from a running TFJob created with the Kubeflow Training Operator. The follow parameter is set to true to stream logs continuously.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/getting-started.md#2025-04-10_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nTrainingClient().get_job_logs(\n    name=\"tensorflow-dist\",\n    job_kind=\"TFJob\",\n    follow=True,\n)\n```\n\n----------------------------------------\n\nTITLE: Database Connection String Format for MySQL\nDESCRIPTION: Format string used by Katib DB Manager to create MySQL database connections using specified environment variables for host, port, credentials and database name.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/env-variables.md#2025-04-10_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nDB_USER:DB_PASSWORD@tcp(KATIB_MYSQL_DB_HOST:KATIB_MYSQL_DB_PORT)/KATIB_MYSQL_DB_DATABASE?timeout=5s\n```\n\n----------------------------------------\n\nTITLE: Configuring ENAS Algorithm Settings in YAML\nDESCRIPTION: YAML configuration for the Efficient Neural Architecture Search (ENAS) algorithm in Katib. It includes settings for controller parameters, learning rates, and training steps.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/nas/configure-algorithm.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nalgorithmName: enas\nalgorithmSettings:\n  - name: controller_hidden_size\n    value: \"64\"\n  - name: controller_temperature\n    value: \"5.0\"\n  - name: controller_tanh_const\n    value: \"2.25\"\n  - name: controller_entropy_weight\n    value: \"1e-5\"\n  - name: controller_baseline_decay\n    value: \"0.999\"\n  - name: controller_learning_rate\n    value: \"5e-5\"\n  - name: controller_skip_target\n    value: \"0.4\"\n  - name: controller_skip_weight\n    value: \"0.8\"\n  - name: controller_train_steps\n    value: \"50\"\n  - name: controller_log_every_steps\n    value: \"10\"\n```\n\n----------------------------------------\n\nTITLE: Creating Kubernetes Resources from Notebook Pod\nDESCRIPTION: Command to create Kubernetes resources defined in a YAML file from within a Notebook pod using the default-editor ServiceAccount permissions.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/notebooks/submit-kubernetes.md#2025-04-10_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkubectl create -f \"test.yaml\" --namespace \"MY_PROFILE_NAMESPACE\"\n```\n\n----------------------------------------\n\nTITLE: Implementing Markdown Output in Kubeflow Pipeline Component\nDESCRIPTION: Shows how to create a component that generates Markdown output visualization. Uses the Output[Markdown] type and writes content to the artifact path.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/output-viewer.md#2025-04-10_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\n@component\ndef markdown_visualization(markdown_artifact: Output[Markdown]):\n    markdown_content = '## Hello world \\n\\n Markdown content'\n    with open(markdown_artifact.path, 'w') as f:\n        f.write(markdown_content)\n```\n\n----------------------------------------\n\nTITLE: Uploading a Pipeline via the API\nDESCRIPTION: Command to upload the compiled pipeline package to Kubeflow Pipelines using the REST API and capture the returned pipeline ID.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/tutorials/api-pipelines.md#2025-04-10_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nSVC=localhost:8888\nPIPELINE_ID=$(curl -F \"uploadfile=@${PIPELINE_NAME}.tar.gz\" ${SVC}/apis/v1beta1/pipelines/upload | jq -r .id)\n```\n\n----------------------------------------\n\nTITLE: Configuring Metrics Port in Training Operator Deployment\nDESCRIPTION: YAML configuration showing how to customize the metrics port and bind address in the Training Operator deployment. Demonstrates setting the metrics-bind-address argument to specify custom port and IP restrictions.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/prometheus.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# deployment.yaml for the Training Operator\nspec:\n    containers:\n    - command:\n        - /manager\n        image: kubeflow/training-operator\n        name: training-operator\n        ports:\n        - containerPort: 8080\n        - containerPort: 9443\n            name: webhook-server\n            protocol: TCP\n        args:\n        - \"--metrics-bind-address=192.168.1.100:8082\"\n```\n\n----------------------------------------\n\nTITLE: HTML Component Version Table Structure\nDESCRIPTION: HTML markup for displaying component versions organized by working groups including AutoML, Notebooks, Pipelines, Serving, and Training components\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/releases/kubeflow-1.8.md#2025-04-10_snippet_4\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"table-responsive\">\n<table class=\"table table-bordered\">\n    <thead class=\"thead-light\">\n      <tr>\n        <th>Maintainers</th>\n        <th>Component Name</th>\n        <th>Version</th>\n      </tr>\n    </thead>\n    <!-- Table content for components -->\n</table>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Configuring DARTS Algorithm Settings in YAML\nDESCRIPTION: YAML configuration for the Differentiable Architecture Search (DARTS) algorithm in Katib. It includes settings for learning rates, weight decay, batch size, and model architecture parameters.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/nas/configure-algorithm.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nalgorithmName: darts\nalgorithmSettings:\n  - name: num_epochs\n    value: \"50\"\n  - name: w_lr\n    value: \"0.025\"\n  - name: w_lr_min\n    value: \"0.001\"\n  - name: w_momentum\n    value: \"0.9\"\n  - name: w_weight_decay\n    value: \"3e-4\"\n  - name: w_grad_clip\n    value: \"5.0\"\n  - name: alpha_lr\n    value: \"3e-4\"\n  - name: alpha_weight_decay\n    value: \"1e-3\"\n  - name: batch_size\n    value: \"128\"\n  - name: num_workers\n    value: \"4\"\n  - name: init_channels\n    value: \"16\"\n  - name: print_step\n    value: \"50\"\n  - name: num_nodes\n    value: \"4\"\n  - name: stem_multiplier\n    value: \"3\"\n```\n\n----------------------------------------\n\nTITLE: Examining Notebook Pod YAML Configuration\nDESCRIPTION: Command to retrieve the full YAML configuration of a Notebook's Pod for inspection. This helps verify that the Pod was created with the expected configuration.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/notebooks/troubleshooting.md#2025-04-10_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get pod \"${MY_NOTEBOOK_NAME}-0\" --namespace \"${MY_PROFILE_NAMESPACE}\" -o yaml\n```\n\n----------------------------------------\n\nTITLE: Generating ROC Curve Visualization in Python\nDESCRIPTION: This snippet shows how to create a component that generates an ROC curve visualization using the Kubeflow Pipelines v2 SDK. It uses scikit-learn to train a random forest classifier on the wine dataset and log the ROC curve data.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/output-viewer.md#2025-04-10_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n@component(\n    packages_to_install=['sklearn'],\n    base_image='python:3.9',\n)\ndef wine_classification(metrics: Output[ClassificationMetrics]):\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.metrics import roc_curve\n    from sklearn.datasets import load_wine\n    from sklearn.model_selection import train_test_split, cross_val_predict\n\n    X, y = load_wine(return_X_y=True)\n    # Binary classification problem for label 1.\n    y = y == 1\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n    rfc = RandomForestClassifier(n_estimators=10, random_state=42)\n    rfc.fit(X_train, y_train)\n    y_scores = cross_val_predict(rfc, X_train, y_train, cv=3, method='predict_proba')\n    y_predict = cross_val_predict(rfc, X_train, y_train, cv=3, method='predict')\n    fpr, tpr, thresholds = roc_curve(y_true=y_train, y_score=y_scores[:,1], pos_label=True)\n    metrics.log_roc_curve(fpr, tpr, thresholds)\n\n@dsl.pipeline(\n    name='metrics-visualization-pipeline')\ndef metrics_visualization_pipeline():\n    wine_classification_op = wine_classification()\n```\n\n----------------------------------------\n\nTITLE: Upgrading Kubeflow Pipelines to v0.4.0+\nDESCRIPTION: This snippet shows the commands to upgrade an existing Kubeflow Pipelines installation to version 0.4.0 or higher, applying cluster-scoped resources and environment configurations sequentially.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/standalone-deployment.md#2025-04-10_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nexport PIPELINE_VERSION=<version-you-want-to-upgrade-to>\nkubectl apply -k \"github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref=$PIPELINE_VERSION\"\nkubectl wait --for condition=established --timeout=60s crd/applications.app.k8s.io\nkubectl apply -k \"github.com/kubeflow/pipelines/manifests/kustomize/env/dev?ref=$PIPELINE_VERSION\"\n```\n\n----------------------------------------\n\nTITLE: Inserting SVG Image for TensorFlow Diagram\nDESCRIPTION: This snippet shows how to insert an SVG image in Markdown with specific styling and alt text for the distributed TFJob diagram.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/reference/distributed-training.md#2025-04-10_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n<img src=\"/docs/components/trainer/legacy-v1/images/distributed-tfjob.drawio.svg\"\n  alt=\"Distributed TFJob\"\n  class=\"mt-3 mb-3 border rounded p-3 bg-white\">\n```\n\n----------------------------------------\n\nTITLE: HTML Table Structure for Release Information\nDESCRIPTION: HTML markup defining tables that display release information, team members, and version details for Kubeflow 1.8.0\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/releases/kubeflow-1.8.md#2025-04-10_snippet_3\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"table-responsive\">\n<table class=\"table table-bordered\">\n  <tbody>\n    <tr>\n      <th class=\"table-light\">Release Date</th>\n      <td>\n        2023-11-01\n      </td>\n    </tr>\n    <!-- Additional table content -->\n  </tbody>\n</table>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Katib Experiment YAML Front Matter\nDESCRIPTION: YAML front matter for the Katib Experiment documentation page, defining the title, description, and weight of the content in the documentation hierarchy.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/reference/experiment-cr.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n+++\ntitle = \"Katib Experiment Lifecycle\"\ndescription = \"What happens after an Experiment is created\"\nweight = 10\n+++\n```\n\n----------------------------------------\n\nTITLE: Adding Documentation Links to Kubeflow Dashboard\nDESCRIPTION: Example of adding documentation links to the Kubeflow dashboard's home page using ConfigMap configuration. Demonstrates how to add a link with description to the Kubeflow documentation.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/central-dash/customize.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: centraldashboard-config\n  namespace: kubeflow\ndata:\n  settings: |-\n    ...\n  links: |-\n    {\n      \"menuLinks\": [\n        ...\n      ],\n      \"externalLinks\": [\n        ...\n      ],\n      \"quickLinks\": [\n        ...\n      ],\n      \"documentationItems\": [\n        {\n          \"text\": \"Kubeflow Website\",\n          \"desc\": \"Kubeflow website documentation\",\n          \"link\": \"https://www.kubeflow.org/docs/\"\n        }\n      ]\n    }\n```\n\n----------------------------------------\n\nTITLE: Generating Confusion Matrix Visualization in Python\nDESCRIPTION: This snippet demonstrates how to create a component that generates a confusion matrix visualization using the Kubeflow Pipelines v2 SDK. It uses scikit-learn to train a classifier on the iris dataset and log the confusion matrix data.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/output-viewer.md#2025-04-10_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n@component(\n    packages_to_install=['sklearn'],\n    base_image='python:3.9'\n)\ndef iris_sgdclassifier(test_samples_fraction: float, metrics: Output[ClassificationMetrics]):\n    from sklearn import datasets, model_selection\n    from sklearn.linear_model import SGDClassifier\n    from sklearn.metrics import confusion_matrix\n\n    iris_dataset = datasets.load_iris()\n    train_x, test_x, train_y, test_y = model_selection.train_test_split(\n        iris_dataset['data'], iris_dataset['target'], test_size=test_samples_fraction)\n\n\n    classifier = SGDClassifier()\n    classifier.fit(train_x, train_y)\n    predictions = model_selection.cross_val_predict(classifier, train_x, train_y, cv=3)\n    metrics.log_confusion_matrix(\n        ['Setosa', 'Versicolour', 'Virginica'],\n        confusion_matrix(train_y, predictions).tolist() # .tolist() to convert np array to list.\n    )\n\n@dsl.pipeline(\n    name='metrics-visualization-pipeline')\ndef metrics_visualization_pipeline():\n    iris_sgdclassifier_op = iris_sgdclassifier(test_samples_fraction=0.3)\n```\n\n----------------------------------------\n\nTITLE: Checking Run Status\nDESCRIPTION: Command to retrieve the current status of a pipeline run using its run ID, which can be used to monitor progress until completion.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/tutorials/api-pipelines.md#2025-04-10_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\ncurl ${SVC}/apis/v1beta1/runs/${RUN_ID} | jq\n```\n\n----------------------------------------\n\nTITLE: Setting Profile Namespace\nDESCRIPTION: Configure the manifests to deploy Model Registry in a specific profile namespace using kustomize.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/model-registry/installation.md#2025-04-10_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nPROFILE_NAME=<your-profile>\nfor DIR in options/istio overlays/db ; do (cd $DIR; kustomize edit set namespace $PROFILE_NAME); done\n```\n\n----------------------------------------\n\nTITLE: Rendering Kubeflow Components Version Table in HTML\nDESCRIPTION: HTML table structure that displays Kubeflow component versions organized by working groups. The table includes maintainer groups, component names, and version numbers with links to respective GitHub repositories. Uses Bootstrap classes for responsive design and styling.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/releases/kubeflow-1.4.md#2025-04-10_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"table-responsive\">\n<table class=\"table table-bordered\">\n    <thead class=\"thead-light\">\n      <tr>\n        <th>Maintainers</th>\n        <th>Component Name</th>\n        <th>Version</th>\n      </tr>\n    </thead>\n  <tbody>\n      <!-- ======================= -->\n      <!-- AutoML Working Group -->\n      <!-- ======================= -->\n      <tr>\n        <td rowspan=\"1\" class=\"align-middle\">AutoML Working Group</td>\n        <td>Katib</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/katib/releases/tag/v0.12.0\">v0.12.0</a>\n        </td>\n      </tr>\n      <!-- ======================= -->\n      <!-- Notebooks Working Group -->\n      <!-- ======================= -->\n      <tr>\n        <td rowspan=\"9\" class=\"align-middle\">Notebooks Working Group</td>\n        <td>Admission Webhook (PodDefaults)</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/kubeflow/tree/v1.4.0/components/admission-webhook\">v1.4.0</a>\n        </td>\n      </tr>\n      <tr>\n        <td>Central Dashboard</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/kubeflow/tree/v1.4.0/components/centraldashboard\">v1.4.0</a>\n        </td>\n      </tr>\n      <tr>\n        <td>Jupyter Web App</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/kubeflow/tree/v1.4.0/components/crud-web-apps/jupyter\">v1.4.0</a>\n        </td>\n      </tr>\n      <tr>\n        <td>Kubeflow Access Management API</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/kubeflow/tree/v1.4.0/components/access-management\">v1.4.0</a>\n        </td>\n      </tr>\n      <tr>\n        <td>Notebook Controller</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/kubeflow/tree/v1.4.0/components/notebook-controller\">v1.4.0</a>\n        </td>\n      </tr>\n      <tr>\n        <td>Profile Controller</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/kubeflow/tree/v1.4.0/components/profile-controller\">v1.4.0</a>\n        </td>\n      </tr>\n      <tr>\n        <td>Tensorboard Controller</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/kubeflow/tree/v1.4.0/components/notebook-controller\">v1.4.0</a>\n        </td>\n      </tr>\n      <tr>\n        <td>Tensorboard Web App</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/kubeflow/tree/v1.4.0/components/crud-web-apps/volumes\">v1.4.0</a>\n        </td>\n      </tr>\n      <tr>\n        <td>Volumes Web App</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/kubeflow/tree/v1.4.0/components/crud-web-apps/tensorboards\">v1.4.0</a>\n        </td>\n      </tr>\n      <!-- ======================= -->\n      <!-- Pipelines Working Group -->\n      <!-- ======================= -->\n      <tr>\n        <td rowspan=\"2\" class=\"align-middle\">Pipelines Working Group</td>\n        <td>Kubeflow Pipelines</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/pipelines/releases/tag/1.7.0\">v1.7.0</a>\n        </td>\n      </tr>\n      <tr>\n        <td>Kubeflow Pipelines Tekton</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/kfp-tekton/releases/tag/v1.0.0\">v1.0.0</a>\n        </td>\n      </tr>\n      <!-- ======================= -->\n      <!-- Serving Working Group -->\n      <!-- ======================= -->\n      <tr>\n        <td rowspan=\"1\" class=\"align-middle\">Serving Working Group</td>\n        <td>KFServing (KServe)</td>\n        <td>\n          <a href=\"https://github.com/kserve/kserve/releases/tag/v0.6.1\">v0.6.1</a>\n        </td>\n      </tr>\n      <!-- ======================= -->\n      <!-- Training Working Group -->\n      <!-- ======================= -->\n      <tr>\n        <td rowspan=\"2\" class=\"align-middle\">Training Working Group</td>\n        <td>MPI Operator</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/mpi-operator/releases/tag/v0.3.0\">v0.3.0</a>\n        </td>\n      </tr>\n      <tr>\n        <td>Training Operator</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/training-operator/releases/tag/v1.3.0\">v1.3.0</a>\n        </td>\n      </tr>\n  </tbody>\n</table>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Representing Neural Network Architecture in ENAS (JSON)\nDESCRIPTION: Example of how Katib represents a neural network architecture in ENAS, showing the structure for a 12-layer network with 6 possible operations.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/reference/nas-algorithms.md#2025-04-10_snippet_0\n\nLANGUAGE: json\nCODE:\n```\n[\n[2]\n[0 0]\n[1 1 0]\n[5 1 0 1]\n[1 1 1 0 1]\n[5 0 0 1 0 1]\n[1 1 1 0 0 1 0]\n[2 0 0 0 1 1 0 1]\n[0 0 0 1 1 1 1 1 0]\n[2 0 1 0 1 1 1 0 0 0]\n[3 1 1 1 1 1 1 0 0 1 1]\n[0 1 1 1 1 0 0 1 1 1 1 0]\n]\n```\n\n----------------------------------------\n\nTITLE: Deleting a Kubeflow Profile with kubectl\nDESCRIPTION: Command to delete an existing Kubeflow profile using kubectl. This removes the profile and its associated namespace, along with all resources in that namespace.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/central-dash/profiles.md#2025-04-10_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete profile MY_PROFILE_NAME\n```\n\n----------------------------------------\n\nTITLE: Installing Latest Katib Control Plane\nDESCRIPTION: Command to install the latest development version of Katib control plane from the master branch.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/installation.md#2025-04-10_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -k \"github.com/kubeflow/katib.git/manifests/v1beta1/installs/katib-standalone?ref=master\"\n```\n\n----------------------------------------\n\nTITLE: Creating Distributed Training Job\nDESCRIPTION: Configuration and creation of a distributed training job using torch-distributed runtime with 4 nodes and 1 GPU per node.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/getting-started.md#2025-04-10_snippet_3\n\nLANGUAGE: python\nCODE:\n```\njob_id = TrainerClient().train(\n    trainer=CustomTrainer(\n        func=train_pytorch,\n        num_nodes=4,\n        resources_per_node={\n            \"cpu\": 5,\n            \"memory\": \"16Gi\",\n            \"gpu\": 1, # Comment this line if you don't have GPUs.\n        },\n    ),\n    runtime=TrainerClient().get_runtime(\"torch-distributed\"),\n)\n```\n\n----------------------------------------\n\nTITLE: Applying Kubeflow Installation Manifests\nDESCRIPTION: Deploy Model Registry components using kubectl apply commands for database, Istio, and UI overlays.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/model-registry/installation.md#2025-04-10_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply -k overlays/db\nkubectl apply -k options/istio\nkubectl apply -k options/ui/overlays/istio\n```\n\n----------------------------------------\n\nTITLE: Styling Images with Bootstrap in HTML\nDESCRIPTION: HTML code examples showing how to style images in Kubeflow documentation using Bootstrap classes. Demonstrates proper formatting for both wide and tall images with borders and spacing.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/about/style-guide.md#2025-04-10_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<!-- for wide images -->\n<img src=\"/docs/images/my-image.png\"\n     alt=\"My image\"\n     class=\"mt-3 mb-3 border rounded\">\n\n<!-- for tall images -->\n<img src=\"/docs/images/my-image.png\"\n     alt=\"My image\"\n     class=\"mt-3 mb-3 border rounded\"\n     style=\"width: 100%; max-width: 30em\">\n```\n\n----------------------------------------\n\nTITLE: Defining Markdown Frontmatter for Katib Reference Documentation\nDESCRIPTION: This code snippet defines the frontmatter for a Markdown document, specifying the title, description, and weight of the page. It is used to structure the reference documentation for Katib within the Kubeflow project.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/reference/_index.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n+++\ntitle = \"Reference\"\ndescription = \"Reference docs for Katib\"\nweight = 50\n+++\n```\n\n----------------------------------------\n\nTITLE: Setting DNS Configuration in Spark Application YAML\nDESCRIPTION: This YAML snippet demonstrates how to set custom DNS configuration for driver and executor pods in a Spark application.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/writing-sparkapplication.md#2025-04-10_snippet_21\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  driver:\n    dnsConfig:\n      nameservers:\n        - 1.2.3.4\n      searches:\n        - ns1.svc.cluster.local\n        - my.dns.search.suffix\n      options:\n        - name: ndots\n          value: \"2\"\n        - name: edns0\n```\n\n----------------------------------------\n\nTITLE: Deploying a Katib Experiment with Random Search\nDESCRIPTION: Shell command to create a Katib experiment using a predefined YAML file that implements random search for hyperparameter tuning. The example uses a PyTorch model for image classification on the FashionMNIST dataset.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/hp-tuning/configure-experiment.md#2025-04-10_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nkubectl create -f https://raw.githubusercontent.com/kubeflow/katib/master/examples/v1beta1/hp-tuning/random.yaml\n```\n\n----------------------------------------\n\nTITLE: Creating Data Producer and Consumer Components\nDESCRIPTION: Defines two KFP components that read and write data to a mounted volume at /data directory. The producer writes 'Hello world' to a file and the consumer reads from it.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/platform-specific-features.md#2025-04-10_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\n\n@dsl.component\ndef producer() -> str:\n    with open('/data/file.txt', 'w') as file:\n        file.write('Hello world')\n    with open('/data/file.txt', 'r') as file:\n        content = file.read()\n    print(content)\n    return content\n\n@dsl.component\ndef consumer() -> str:\n    with open('/data/file.txt', 'r') as file:\n        content = file.read()\n    print(content)\n    return content\n```\n\n----------------------------------------\n\nTITLE: Configuring Dashboard Link\nDESCRIPTION: Add Model Registry link to the Kubeflow Dashboard configuration.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/model-registry/installation.md#2025-04-10_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get configmap centraldashboard-config -n kubeflow -o json | jq '.data.links |= (fromjson | .menuLinks += [{\"icon\": \"assignment\", \"link\": \"/model-registry/\", \"text\": \"Model Registry\", \"type\": \"item\"}] | tojson)' | kubectl apply -f - -n kubeflow\n```\n\n----------------------------------------\n\nTITLE: Community Roles Table Structure in HTML\nDESCRIPTION: HTML table structure defining the responsibilities and requirements for different Kubeflow community roles including Members, Reviewers, Approvers, WG Leads, and Steering Committee members.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/about/membership.md#2025-04-10_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"table-responsive\">\n<div class=\"table table-bordered\">\n\n| Role | Responsibilities | Requirements | Defined by |\n| -----| ---------------- | ------------ | -------|\n| Member | Active contributor in the community | Sponsored by 2 Kubeflow members and multiple contributions to the project | Kubeflow GitHub org member|\n| Reviewer | Review contributions from other members | History of review and authorship in a repository | [OWNERS](/docs/about/contributing/#owners) file reviewer entry |\n| Approver | Contributions acceptance approval| Highly experienced active reviewer and contributor to a repository | [OWNERS](/docs/about/contributing/#owners) file approver entry|\n| WG Lead  | Provides technical leadership for a Working Group | Have sufficient domain knowledge to provide effective technical leadership | [wgs.yaml] entry |\n| WG Chair | Provides overall leadership for a Working Group | Have sufficient domain knowledge to provide effective leadership | [wgs.yaml] entry |\n| Kubeflow Steering Commitee Member | The KSC provides leadership for the overall Kubeflow project | [Details](https://github.com/kubeflow/community/blob/master/KUBEFLOW-STEERING-COMMITTEE.md#charter) | [Members](https://github.com/kubeflow/community/blob/master/KUBEFLOW-STEERING-COMMITTEE.md#charter) |\n\n</div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Defining Notebook Resource Structure in YAML\nDESCRIPTION: YAML representation of the Notebook resource structure, including spec and status fields. The spec defines the desired state while status reflects the current state.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/notebooks/api-reference/notebook-v1.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: kubeflow.org/v1\nkind: Notebook\nmetadata:\n  # Standard Kubernetes metadata\nspec:\n  template:\n    # NotebookTemplateSpec fields\nstatus:\n  # NotebookStatus fields\n```\n\n----------------------------------------\n\nTITLE: Setting Tolerations in Spark Application YAML\nDESCRIPTION: This YAML snippet demonstrates how to set tolerations for driver and executor pods in a Spark application to control pod scheduling.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/writing-sparkapplication.md#2025-04-10_snippet_17\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  driver:\n    tolerations:\n    - key: Key\n      operator: Exists\n      effect: NoSchedule\n\n  executor:\n    tolerations:\n    - key: Key\n      operator: Equal\n      value: Value\n      effect: NoSchedule\n```\n\n----------------------------------------\n\nTITLE: Verifying Katib Controller Resource Integration\nDESCRIPTION: Shell command to check Katib controller logs for successful PipelineRun integration.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/trial-template.md#2025-04-10_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\n$ kubectl logs $(kubectl get pods -n kubeflow -o name | grep katib-controller) -n kubeflow | grep '\"CRD Kind\":\"PipelineRun\"'\n\n{\"level\":\"info\",\"ts\":1628032648.6285546,\"logger\":\"trial-controller\",\"msg\":\"Job watch added successfully\",\"CRD Group\":\"tekton.dev\",\"CRD Version\":\"v1beta1\",\"CRD Kind\":\"PipelineRun\"}\n```\n\n----------------------------------------\n\nTITLE: Installing Miniconda on MacOS\nDESCRIPTION: Command to install Miniconda on MacOS systems. This sets up the base environment for Python and package management on Mac.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/install-sdk.md#2025-04-10_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbash Miniconda3-latest-MacOSX-x86_64.sh\n```\n\n----------------------------------------\n\nTITLE: Checking Deployment Status\nDESCRIPTION: Verify Model Registry deployment status and check logs.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/model-registry/installation.md#2025-04-10_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nkubectl wait --for=condition=available -n kubeflow deployment/model-registry-deployment --timeout=1m\nkubectl logs -n kubeflow deployment/model-registry-deployment\n```\n\n----------------------------------------\n\nTITLE: Configuring Hugo Frontmatter for Kubeflow Events Page\nDESCRIPTION: TOML frontmatter configuration that defines metadata properties for the Kubeflow events webpage including title, description, weight, manual link and icon.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/about/events.md#2025-04-10_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n+++\ntitle =  \"Events\"\ndescription = \"Kubeflow Community Events and Meetups\"\nweight = 15\nmanualLink = \"/events/\"\nicon = \"fa-solid fa-arrow-up-right-from-square\"\n+++\n```\n\n----------------------------------------\n\nTITLE: Installing Linux Packages in Kubeflow Notebook Images\nDESCRIPTION: Guidance for installing system packages in Kubeflow Notebook images using apt-get. Emphasizes the need to switch to root user before package installation and back to the notebook user afterward.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/notebooks/container-images.md#2025-04-10_snippet_2\n\nLANGUAGE: dockerfile\nCODE:\n```\nUSER root\napt-get ...\nUSER $NB_USER\n```\n\n----------------------------------------\n\nTITLE: Mounting Volumes in Spark Application YAML\nDESCRIPTION: This YAML snippet demonstrates how to mount volumes in a Spark application, specifying volume definitions and mounts for both driver and executor pods.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/writing-sparkapplication.md#2025-04-10_snippet_13\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  volumes:\n    - name: spark-data\n      persistentVolumeClaim:\n        claimName: my-pvc\n    - name: spark-work\n      emptyDir:\n        sizeLimit: 5Gi\n  driver:\n    volumeMounts:\n      - name: spark-work\n        mountPath: /mnt/spark/work\n  executor:\n    volumeMounts:\n      - name: spark-data\n        mountPath: /mnt/spark/data\n      - name: spark-work\n        mountPath: /mnt/spark/work\n```\n\n----------------------------------------\n\nTITLE: Success Condition for Kubeflow Jobs\nDESCRIPTION: Default GJSON format condition to determine if a Kubeflow job (TFJob, PyTorchJob, MXJob, XGBoostJob) Trial has succeeded.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/trial-template.md#2025-04-10_snippet_1\n\nLANGUAGE: text\nCODE:\n```\nstatus.conditions.#(type==\"Succeeded\")#|#(status==\"True\")#\n```\n\n----------------------------------------\n\nTITLE: Creating Experiment and Run using Kubeflow Pipelines REST API in Python\nDESCRIPTION: This snippet shows how to create an experiment and pipeline run using the Kubeflow Pipelines REST API with the generated Python API client. It demonstrates setting up credentials, creating a client, and making API calls to create and retrieve experiments and runs in a specific namespace.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/operator-guides/multi-user.md#2025-04-10_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport kfp\nfrom kfp_server_api import *\n\n# the namespace in which you deployed Kubeflow Pipelines\nkubeflow_namespace = \"kubeflow\"\n\n# the namespace of your pipelines user (where the pipeline will be executed)\nuser_namespace = \"jane-doe\"\n\n# the KF_PIPELINES_SA_TOKEN_PATH environment variable is used when no `path` is set\n# the default KF_PIPELINES_SA_TOKEN_PATH is /var/run/secrets/kubeflow/pipelines/token\ncredentials = kfp.auth.ServiceAccountTokenVolumeCredentials(path=None)\n\n# create a client\nclient = kfp.Client(host=f\"http://ml-pipeline-ui.{kubeflow_namespace}\", credentials=credentials)\n\n# create an experiment\nexperiment: ApiExperiment = client._experiment_api.create_experiment(\n    body=ApiExperiment(\n        name=\"<YOUR_EXPERIMENT_ID>\",\n        resource_references=[\n            ApiResourceReference(\n                key=ApiResourceKey(\n                    id=user_namespace,\n                    type=ApiResourceType.NAMESPACE,\n                ),\n                relationship=ApiRelationship.OWNER,\n            )\n        ],\n    )\n)\nprint(\"-------- BEGIN: EXPERIMENT --------\")\nprint(experiment)\nprint(\"-------- END: EXPERIMENT ----------\")\n\n# get the experiment by name (only necessary if you comment out the `create_experiment()` call)\n# experiment: ApiExperiment = client.get_experiment(\n#     experiment_name=\"<YOUR_EXPERIMENT_ID>\",\n#     namespace=user_namespace\n# )\n\n# create a pipeline run\nrun: ApiRunDetail = client._run_api.create_run(\n    body=ApiRun(\n        name=\"<YOUR_RUN_NAME>\",\n        pipeline_spec=ApiPipelineSpec(\n            # replace <YOUR_PIPELINE_ID> with the UID of a pipeline definition you have previously uploaded\n            pipeline_id=\"<YOUR_PIPELINE_ID>\",\n        ),\n        resource_references=[ApiResourceReference(\n            key=ApiResourceKey(\n                id=experiment.id,\n                type=ApiResourceType.EXPERIMENT,\n            ),\n            relationship=ApiRelationship.OWNER,\n        )\n        ],\n    )\n)\nprint(\"-------- BEGIN: RUN --------\")\nprint(run)\nprint(\"-------- END: RUN ----------\")\n\n# view the pipeline run\nruns: ApiListRunsResponse = client._run_api.list_runs(\n    resource_reference_key_type=ApiResourceType.EXPERIMENT,\n    resource_reference_key_id=experiment.id,\n)\nprint(\"-------- BEGIN: RUNS --------\")\nprint(runs)\nprint(\"-------- END: RUNS ----------\")\n```\n\n----------------------------------------\n\nTITLE: Port Forwarding Istio Gateway for Kubeflow Dashboard Access\nDESCRIPTION: Commands to set up kubectl port forwarding to access the Kubeflow dashboard locally. This forwards the Istio ingress gateway service from port 80 to local port 8080, allowing dashboard access through localhost.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/central-dash/access.md#2025-04-10_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nexport ISTIO_NAMESPACE=istio-system\nkubectl port-forward svc/istio-ingressgateway -n ${ISTIO_NAMESPACE} 8080:80\n```\n\n----------------------------------------\n\nTITLE: Defining Hugo Front Matter for Kubeflow About Page\nDESCRIPTION: TOML configuration block that defines the metadata for the About page including title, description and page weight in the navigation hierarchy.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/about/_index.md#2025-04-10_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n+++\ntitle = \"About\"\ndescription = \"About Kubeflow and its community\"\nweight = 10\n+++\n```\n\n----------------------------------------\n\nTITLE: Creating VirtualService for Kubeflow Dashboard Integration\nDESCRIPTION: Configuration for exposing a non-Kubeflow application through the Kubeflow dashboard using Istio VirtualService. Shows routing and header configuration for integration.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/central-dash/customize.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: my-custom-app\n  namespace: <MY_APP_NAMESPACE>\nspec:\n  gateways:\n    - kubeflow/kubeflow-gateway\n  hosts:\n    - '*'\n  http:\n    - headers:\n        request:\n          add:\n            x-forwarded-prefix: /my-app\n      match:\n        - uri:\n            prefix: /my-app/\n      rewrite:\n        uri: /\n      route:\n        - destination:\n            host: <MY_APP_SERVICE_NAME>.<MY_APP_NAMESPACE>.svc.cluster.local\n            port:\n              number: 80\n```\n\n----------------------------------------\n\nTITLE: Configuring Environment Variables for Spark Driver and Executor\nDESCRIPTION: This YAML snippet shows how to configure environment variables for both driver and executor containers using env and envFrom fields, including support for ConfigMaps and Secrets as environment variable sources.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/writing-sparkapplication.md#2025-04-10_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  driver:\n    env:\n      - name: ENV1\n        value: VAL1\n      - name: ENV2\n        value: VAL2\n      - name: ENV3\n        valueFrom:\n          configMapKeyRef:\n            name: some-config-map\n            key: env3-key\n      - name: AUTH_KEY\n        valueFrom:\n          secretKeyRef:\n            name: some-secret\n            key: auth-key\n    envFrom:\n      - configMapRef:\n          name: env-config-map\n      - secretRef:\n          name: env-secret\n  executor:\n    env:\n      - name: ENV1\n        value: VAL1\n      - name: ENV2\n        value: VAL2\n      - name: ENV3\n        valueFrom:\n          configMapKeyRef:\n            name: some-config-map\n            key: env3-key\n      - name: AUTH_KEY\n        valueFrom:\n          secretKeyRef:\n            name: some-secret\n            key: auth-key\n    envFrom:\n      - configMapRef:\n          name: my-env-config-map\n      - secretRef:\n          name: my-env-secret\n```\n\n----------------------------------------\n\nTITLE: Checking Conda Installation\nDESCRIPTION: Command to verify if conda is installed and available in the system path. This ensures that conda can be used for managing Python environments.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/install-sdk.md#2025-04-10_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nwhich conda\n```\n\n----------------------------------------\n\nTITLE: Checking Pod Events for Notebook Instance\nDESCRIPTION: Command to describe the Pod associated with a Notebook and check its events section for errors. This provides more detailed information about runtime issues.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/notebooks/troubleshooting.md#2025-04-10_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkubectl describe pod \"${MY_NOTEBOOK_NAME}-0\" --namespace \"${MY_PROFILE_NAMESPACE}\"\n```\n\n----------------------------------------\n\nTITLE: Creating Local Kubernetes Cluster with Kind\nDESCRIPTION: Command to create a local Kubernetes cluster using Kind for development and testing purposes.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/operator-guides/installation.md#2025-04-10_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkind create cluster # or minikube start\n```\n\n----------------------------------------\n\nTITLE: Configuring Hugo Page Metadata in TOML\nDESCRIPTION: Defines the title and description metadata for a Hugo documentation page using TOML front matter syntax. This configuration will be used by the Hugo static site generator to generate the documentation landing page.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/_index.md#2025-04-10_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n+++\ntitle = \"Documentation\"\ndescription = \"All of Kubeflow documentation\"\n+++\n```\n\n----------------------------------------\n\nTITLE: Configuring KServe Support\nDESCRIPTION: Apply CustomStorageContainer CR for KServe URI format support.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/model-registry/installation.md#2025-04-10_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\nMODEL_REGISTRY_VERSION={{% model-registry/latest-version %}}\nkubectl apply -k \"https://github.com/kubeflow/model-registry/manifests/kustomize/options/csi?ref=v${MODEL_REGISTRY_VERSION}\"\n```\n\n----------------------------------------\n\nTITLE: Driver Specification in SparkApplication\nDESCRIPTION: Example showing how to configure driver-specific settings including resources, labels, and service account in a SparkApplication manifest.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/writing-sparkapplication.md#2025-04-10_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  driver:\n    cores: 1\n    coreLimit: 200m\n    memory: 512m\n    labels:\n      version: 3.1.1\n    serviceAccount: spark\n```\n\n----------------------------------------\n\nTITLE: Adding Miniconda to PATH\nDESCRIPTION: Command to add Miniconda to the system PATH if it's not automatically added during installation. This allows conda commands to be recognized by the shell.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/install-sdk.md#2025-04-10_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport PATH=<YOUR_MINICONDA_PATH>/bin:$PATH\n```\n\n----------------------------------------\n\nTITLE: Embedding Google Calendar for Kubeflow Community Events\nDESCRIPTION: JavaScript code to dynamically embed a Google Calendar iframe showing Kubeflow community events. It detects the user's timezone and displays the calendar in agenda mode.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/about/community.md#2025-04-10_snippet_1\n\nLANGUAGE: javascript\nCODE:\n```\nconst timezone = Intl.DateTimeFormat().resolvedOptions().timeZone;\nconst calender_src_list = [\n  // Kubeflow Community\n  \"kubeflow.org_7l5vnbn8suj2se10sen81d9428%40group.calendar.google.com\",\n];\nlet calender_src = calender_src_list.map(src => `&src=${src}&color=%23A79B8E`).join('');\nconst html = `<iframe src=\"https://calendar.google.com/calendar/embed?ctz=${timezone}&height=600&wkst=1&bgcolor=%23ffffff&showPrint=0&showDate=1&mode=AGENDA&showTitle=0${calender_src}\" width=\"800\" height=\"600\" frameborder=\"0\" scrolling=\"no\"></iframe>`;\ndocument.getElementById('calendar-container').innerHTML = html;\n```\n\n----------------------------------------\n\nTITLE: Installing Kubeflow Trainer Controller Manager\nDESCRIPTION: Command to deploy the Kubeflow Trainer controller manager using kubectl apply with server-side application.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/operator-guides/installation.md#2025-04-10_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply --server-side -k \"https://github.com/kubeflow/trainer.git/manifests/overlays/manager?ref=master\"\n```\n\n----------------------------------------\n\nTITLE: Defining Frontmatter for 2024 Events Page in Hugo\nDESCRIPTION: This code snippet defines the frontmatter for a Hugo content file. It specifies the title and description for a page listing Kubeflow events in 2024.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/events/past-events/2024/_index.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n+++\ntitle = \"2024\"\ndescription = \"Events from 2024\"\n+++\n```\n\n----------------------------------------\n\nTITLE: Installing Model Registry Python Dependencies\nDESCRIPTION: Commands to install the required Python packages for Model Registry and KServe\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/model-registry/getting-started.md#2025-04-10_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n!pip install model-registry==\"{{% model-registry/latest-version %}}\"\n!pip install kserve==\"0.13\"\n```\n\n----------------------------------------\n\nTITLE: Basic SparkApplication Configuration in YAML\nDESCRIPTION: Basic example of a SparkApplication manifest showing core configuration including API version, application type, deployment mode, container image, and main class specification.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/writing-sparkapplication.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: sparkoperator.k8s.io/v1beta2\nkind: SparkApplication\nmetadata:\n  name: spark-pi\n  namespace: default\nspec:\n  type: Scala\n  mode: cluster\n  image: spark:3.5.1\n  mainClass: org.apache.spark.examples.SparkPi\n  mainApplicationFile: local:///opt/spark/examples/jars/spark-examples_2.12-3.5.1.jar\n```\n\n----------------------------------------\n\nTITLE: Installing Kubeflow Pipelines SDK with User Option\nDESCRIPTION: Alternative command to install the Kubeflow Pipelines SDK with the --user option, useful when facing permission issues in non-virtual environments.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/install-sdk.md#2025-04-10_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install kfp==1.8 --user\n```\n\n----------------------------------------\n\nTITLE: Deploying Kubeflow Pipelines v0.2.0-0.3.0\nDESCRIPTION: This command sequence deploys Kubeflow Pipelines versions 0.2.0 through 0.3.0, which require a different deployment approach. It applies CRDs first and then deploys the environment-specific configurations.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/standalone-deployment.md#2025-04-10_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport PIPELINE_VERSION=<kfp-version-between-0.2.0-and-0.3.0>\nkubectl apply -k \"github.com/kubeflow/pipelines/manifests/kustomize/base/crds?ref=$PIPELINE_VERSION\"\nkubectl wait --for condition=established --timeout=60s crd/applications.app.k8s.io\nkubectl apply -k \"github.com/kubeflow/pipelines/manifests/kustomize/env/dev?ref=$PIPELINE_VERSION\"\n```\n\n----------------------------------------\n\nTITLE: Failure Condition for All Jobs\nDESCRIPTION: Default GJSON format condition to determine if a Trial (both Kubernetes and Kubeflow jobs) has failed.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/trial-template.md#2025-04-10_snippet_2\n\nLANGUAGE: text\nCODE:\n```\nstatus.conditions.#(type==\"Failed\")#|#(status==\"True\")#\n```\n\n----------------------------------------\n\nTITLE: Hugo Frontmatter Configuration for Past Events Page\nDESCRIPTION: TOML frontmatter block defining the title, description and weight for a Hugo page about past Kubeflow events. The weight parameter determines the page's position in navigation menus.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/events/past-events/_index.md#2025-04-10_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n+++\ntitle = \"Past Events\"\ndescription = \"Past Kubeflow events\"\nweight = 200\n+++\n```\n\n----------------------------------------\n\nTITLE: Adding Spark Operator Helm Repository\nDESCRIPTION: Commands to add the Spark Operator Helm repository and update local repositories. This is the first step in installing the Spark Operator using Helm.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/getting-started.md#2025-04-10_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nhelm repo add spark-operator https://kubeflow.github.io/spark-operator\n\nhelm repo update\n```\n\n----------------------------------------\n\nTITLE: Using Secrets as Environment Variables in Spark Application YAML\nDESCRIPTION: This YAML snippet shows how to use Kubernetes secrets as environment variables in a Spark application, specifically for the driver pod.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/writing-sparkapplication.md#2025-04-10_snippet_14\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  driver:\n    envSecretKeyRefs:\n      SECRET_USERNAME:\n        name: mysecret\n        key: username\n      SECRET_PASSWORD:\n        name: mysecret\n        key: password\n```\n\n----------------------------------------\n\nTITLE: Defining Hugo Shortcode File Name\nDESCRIPTION: Shows the file naming convention for a Hugo shortcode that defines the minimum required version of Kubernetes.\nSOURCE: https://github.com/kubeflow/website/blob/master/README.md#2025-04-10_snippet_6\n\nLANGUAGE: plaintext\nCODE:\n```\nkubernetes-min-version.html\n```\n\n----------------------------------------\n\nTITLE: DARTS Best Genotype Representation (Python)\nDESCRIPTION: Example of the Best Genotype representation in DARTS, showing the structure of normal and reduction cells with their operations and connections.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/reference/nas-algorithms.md#2025-04-10_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nGenotype(\n  normal=[\n      [('max_pooling_3x3',0),('max_pooling_3x3',1)],\n      [('max_pooling_3x3',0),('max_pooling_3x3',1)],\n      [('max_pooling_3x3',0),('dilated_convolution_3x3',3)],\n      [('max_pooling_3x3',0),('max_pooling_3x3',1)]\n    ],\n    normal_concat=range(2,6),\n  reduce=[\n      [('dilated_convolution_5x5',1),('separable_convolution_3x3',0)],\n      [('max_pooling_3x3',2),('dilated_convolution_5x5',1)],\n      [('dilated_convolution_5x5',3),('dilated_convolution_5x5',2)],\n      [('dilated_convolution_5x5',3),('dilated_convolution_5x5',4)]\n    ],\n    reduce_concat=range(2,6)\n)\n```\n\n----------------------------------------\n\nTITLE: Creating and Activating Conda Environment\nDESCRIPTION: Commands to create a new conda environment named 'mlpipeline' with Python 3.7 and activate it. This sets up an isolated environment for Kubeflow Pipelines SDK installation.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/install-sdk.md#2025-04-10_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nconda create --name mlpipeline python=3.7\nconda activate mlpipeline\n```\n\n----------------------------------------\n\nTITLE: Defining Hugo Page Metadata in TOML\nDESCRIPTION: TOML frontmatter block that configures the metadata for a Hugo documentation page about Kubeflow events. Sets the page title, description and cascading document type property.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/events/_index.md#2025-04-10_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n+++\ntitle = \"Kubeflow Events\"\ndescription = \"Kubeflow Community Events\"\n\n[[cascade]]\ntype = \"docs\"\n+++\n```\n\n----------------------------------------\n\nTITLE: Metrics Configuration Options for Spark Operator\nDESCRIPTION: List of all configuration options the Spark Operator supports for metrics collection. Includes options for enabling metrics, specifying ports, endpoints, prefix, and labels.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/getting-started.md#2025-04-10_snippet_16\n\nLANGUAGE: shell\nCODE:\n```\n-enable-metrics=true\n-metrics-port=10254\n-metrics-endpoint=/metrics\n-metrics-prefix=myServiceName\n-metrics-label=label1Key\n-metrics-label=label2Key\n```\n\n----------------------------------------\n\nTITLE: Mounting ConfigMaps in Spark Driver\nDESCRIPTION: This YAML snippet shows how to mount Kubernetes ConfigMaps into the Spark driver pod, specifying the ConfigMap name and mount path. Requires the mutating admission webhook to be enabled.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/writing-sparkapplication.md#2025-04-10_snippet_12\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  driver:\n    configMaps:\n      - name: configmap1\n        path: /mnt/config-maps\n```\n\n----------------------------------------\n\nTITLE: Implementing Do-While Loop with Recursion in Kubeflow Pipelines DSL (Python)\nDESCRIPTION: Shows how to create a recursive function that behaves like a do-while loop. The recursive call is placed at the end of the function body, inside a conditional statement. It also demonstrates how to use this function within a pipeline definition.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/dsl-recursion.md#2025-04-10_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport kfp.dsl as dsl\n@dsl.graph_component\ndef graph_component_a(input_x):\n  op_a = task_factory_a(input_x)\n  op_b = task_factory_b().after(op_a)\n  with dsl.Condition(op_b.output == 'value_x'):\n    graph_component_a(op_b.output)\n \n@dsl.pipeline(\n  name='pipeline',\n  description='shows how to use the recursion.'\n)\ndef pipeline():\n  op_a = task_factory_a()\n  op_b = task_factory_b()\n  graph_op_a = graph_component_a(op_a.output)\n  graph_op_a.after(op_b)\n  task_factory_c(op_a.output).after(graph_op_a)\n```\n\n----------------------------------------\n\nTITLE: Verifying DSL-Compile Installation\nDESCRIPTION: Command to check if the dsl-compile binary is available after installing the Kubeflow Pipelines SDK. This verifies that the SDK installation was successful.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/install-sdk.md#2025-04-10_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nwhich dsl-compile\n```\n\n----------------------------------------\n\nTITLE: Using Version Shortcode in GitHub Links\nDESCRIPTION: Shows how to use the version shortcode in links to GitHub source code, ensuring that links in versioned webpages point to the correct branch.\nSOURCE: https://github.com/kubeflow/website/blob/master/README.md#2025-04-10_snippet_9\n\nLANGUAGE: markdown\nCODE:\n```\nhttps://github.com/kubeflow/kubeflow/blob/{{< params \"githubbranch\" >}}/scripts/gke/deploy.sh\n```\n\n----------------------------------------\n\nTITLE: Spark Configuration Settings in SparkApplication\nDESCRIPTION: Example showing how to specify custom Spark configuration properties in a SparkApplication manifest.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/writing-sparkapplication.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  sparkConf:\n    spark.ui.port: \"4045\"\n    spark.eventLog.enabled: \"true\"\n    spark.eventLog.dir: \"hdfs://hdfs-namenode-1:8020/spark/spark-events\"\n```\n\n----------------------------------------\n\nTITLE: Deploying Spark Operator Instance 2 with Helm in Bash\nDESCRIPTION: This snippet demonstrates the installation of a second Spark operator instance using Helm. It creates a 'spark-2' namespace and configures the operator to watch this namespace for Spark jobs.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/running-multiple-instances-of-the-operator.md#2025-04-10_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# Create the spark-2 namespace if it does not exist\nkubectl create ns spark-2\n\n# Install the Spark operator with release name spark-operator-2\nhelm install spark-operator-2 spark-operator/spark-operator \\\n    --namespace spark-operator \\\n    --create-namespace \\\n    --set 'spark.jobNamespaces={spark-2}'\n```\n\n----------------------------------------\n\nTITLE: Configuring Emissary Executor in Kubernetes\nDESCRIPTION: Command to update the workflow controller configuration to use the Emissary executor.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/choose-executor.md#2025-04-10_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl patch configmap workflow-controller-configmap --patch '{\"data\":{\"containerRuntimeExecutor\":\"emissary\"}}'\n```\n\n----------------------------------------\n\nTITLE: Incorrect Hardcoding of Output Path in Python\nDESCRIPTION: Shows an example of how not to handle file paths in a Kubeflow Pipeline component. Hardcoding paths makes the component less flexible and harder to test.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/best-practices.md#2025-04-10_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nopen(\"/output.txt\", \"w\")\n```\n\n----------------------------------------\n\nTITLE: Hugo Shortcode for Financial Time Series Example\nDESCRIPTION: Hugo shortcode block defining the financial time series analysis example with metadata including Kubeflow version, repository URL and API endpoint.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/started/kubeflow-examples.md#2025-04-10_snippet_2\n\nLANGUAGE: markdown\nCODE:\n```\n{{% blocks/sample-section title=\"Financial time series\"\n  kfctl=\"v0.7\"\n  url=\"https://github.com/kubeflow/examples/tree/master/financial_time_series\"\n  api=\"https://api.github.com/repos/kubeflow/examples/commits?path=financial_time_series&sha=master\" %}}\n```\n\n----------------------------------------\n\nTITLE: Configuring Host Network for Spark Driver and Executor\nDESCRIPTION: This YAML snippet shows how to enable host networking for Spark driver and executor pods using the hostNetwork field. When enabled, pods will use the host's network namespace. Requires the mutating admission webhook.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/writing-sparkapplication.md#2025-04-10_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  driver:\n    cores: 0.1\n    coreLimit: \"200m\"\n    memory: \"512m\"\n    hostNetwork: true\n    labels:\n      version: 3.1.1\n    serviceAccount: spark\n  executor:\n    cores: 1\n    instances: 1\n    memory: \"512m\"\n```\n\n----------------------------------------\n\nTITLE: Granting Cluster Admin Privileges for GKE Installation\nDESCRIPTION: Command to grant cluster-admin privileges for a user on GKE clusters. This is required before installing the Spark Operator on GKE clusters version 1.6 and up.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/getting-started.md#2025-04-10_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\nkubectl create clusterrolebinding <user>-cluster-admin-binding --clusterrole=cluster-admin --user=<user>@<domain>\n```\n\n----------------------------------------\n\nTITLE: Dependency Versions Table Structure\nDESCRIPTION: HTML table structure documenting the validated dependency versions for Kubeflow manifests, including Kubernetes, Istio, cert-manager, and other core dependencies.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/releases/kubeflow-1.6.md#2025-04-10_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"table-responsive\">\n<table class=\"table table-bordered\">\n    <thead class=\"thead-light\">\n      <tr>\n        <th>Dependency</th>\n        <th>Validated or Included Version(s)</th>\n        <th>Notes</th>\n      </tr>\n    </thead>\n    <!-- Table content... -->\n</table>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Retrieving Pipeline Details\nDESCRIPTION: Command to fetch detailed information about the uploaded pipeline using its ID with the REST API.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/tutorials/api-pipelines.md#2025-04-10_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ncurl ${SVC}/apis/v1beta1/pipelines/${PIPELINE_ID} | jq\n```\n\n----------------------------------------\n\nTITLE: Configuring Training Operator for Scheduler Plugins with Coscheduling\nDESCRIPTION: Configuration diff showing how to modify the Training Operator deployment to use scheduler-plugins for gang-scheduling by adding the appropriate command-line flag.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/job-scheduling.md#2025-04-10_snippet_0\n\nLANGUAGE: diff\nCODE:\n```\n...\n    spec:\n      containers:\n        - command:\n            - /manager\n+           - --gang-scheduler-name=scheduler-plugins\n          image: kubeflow/training-operator\n          name: training-operator\n...\n```\n\n----------------------------------------\n\nTITLE: Mounting Secrets in Spark Driver\nDESCRIPTION: This YAML snippet demonstrates how to mount Kubernetes Secrets into the Spark driver pod, specifying the Secret name, mount path, and Secret type. Special handling is provided for GCPServiceAccount and HadoopDelegationToken types.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/writing-sparkapplication.md#2025-04-10_snippet_11\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  driver:\n    secrets:\n      - name: gcp-svc-account\n        path: /mnt/secrets\n        secretType: GCPServiceAccount\n```\n\n----------------------------------------\n\nTITLE: Spark UI Ingress Configuration with NGINX\nDESCRIPTION: YAML configuration for setting up ingress annotations to expose Spark UI through the NGINX Ingress Controller. This configuration is added to the SparkApplication spec.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/getting-started.md#2025-04-10_snippet_17\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  sparkUIOptions:\n    ingressAnnotations:\n        kubernetes.io/ingress.class: nginx\n```\n\n----------------------------------------\n\nTITLE: Creating a Kubernetes Job ResourceOp in Python\nDESCRIPTION: This snippet demonstrates how to create a ResourceOp to manipulate a Kubernetes Job resource. It shows how to set up attribute outputs to capture the job's metadata name.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/manipulate-resources.md#2025-04-10_snippet_0\n\nLANGUAGE: python\nCODE:\n```\njob = kubernetes_client.V1Job(...)\n\nrop = kfp.dsl.ResourceOp(\n    name=\"create-job\",\n    k8s_resource=job,\n    action=\"create\",\n    attribute_outputs={\"name\": \"{.metadata.name}\"}\n)\n```\n\n----------------------------------------\n\nTITLE: Installing kfp-kubernetes Library\nDESCRIPTION: Command to install the kfp-kubernetes library using pip package manager.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/platform-specific-features.md#2025-04-10_snippet_0\n\nLANGUAGE: sh\nCODE:\n```\npip install kfp[kubernetes]\n```\n\n----------------------------------------\n\nTITLE: Markdown Front Matter Configuration\nDESCRIPTION: Hugo front matter configuration block defining the page title, description and weight for the Training Operator reference documentation.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/reference/_index.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n+++\ntitle = \"Reference\"\ndescription = \"Reference docs for the Training Operator\"\nweight = 50\n+++\n```\n\n----------------------------------------\n\nTITLE: Adding Sidecar Containers in Spark Application YAML\nDESCRIPTION: This YAML snippet demonstrates how to add sidecar containers to driver and executor pods in a Spark application.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/writing-sparkapplication.md#2025-04-10_snippet_19\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  driver:\n    sidecars:\n    - name: \"sidecar1\"\n      image: \"sidecar1:latest\"\n      ...\n  executor:\n    sidecars:\n    - name: \"sidecar1\"\n      image: \"sidecar1:latest\"\n      ...\n```\n\n----------------------------------------\n\nTITLE: Retrieving SparkApplication Resource Details\nDESCRIPTION: Command to get detailed information about a SparkApplication resource. This retrieves the YAML representation of the resource showing its configuration and status.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/getting-started.md#2025-04-10_snippet_10\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get sparkapplication spark-pi -o=yaml\n```\n\n----------------------------------------\n\nTITLE: Creating a Kubeflow Profile with kubectl\nDESCRIPTION: Command to apply a profile YAML definition to create a new Kubeflow profile using kubectl. This command is executed by a cluster administrator.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/central-dash/profiles.md#2025-04-10_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -f my-profile.yaml\n```\n\n----------------------------------------\n\nTITLE: Docker Image Inspection Command\nDESCRIPTION: Command to inspect Docker image configuration for ENTRYPOINT and CMD values.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/choose-executor.md#2025-04-10_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker image inspect -f '{{.Config.Entrypoint}} {{.Config.Cmd}}' hello-world\n```\n\n----------------------------------------\n\nTITLE: Configuring Restart Policy in SparkApplicationSpec YAML\nDESCRIPTION: The RestartPolicy field in SparkApplicationSpec allows configuring the application restart policy. This determines how the operator handles application restarts based on termination state and policy type.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/overview/_index.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"sparkoperator.k8s.io/v1beta2\"\nkind: SparkApplication\nmetadata:\n  name: spark-app\nspec:\n  type: Scala\n  mode: cluster\n  image: \"spark:v3.1.1\"\n  restartPolicy:\n    type: OnFailure\n    onFailureRetries: 3\n    onFailureRetryInterval: 10\n    onSubmissionFailureRetries: 5\n    onSubmissionFailureRetryInterval: 20\n```\n\n----------------------------------------\n\nTITLE: SparkApplication YAML Configuration for GCS/BigQuery Integration\nDESCRIPTION: Kubernetes SparkApplication configuration that sets up Spark with GCS and BigQuery connectors, including service account authentication, resource allocation, and environment configuration.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/gcp.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: \"sparkoperator.k8s.io/v1beta2\"\nkind: SparkApplication\nmetadata:\n  name: foo-gcs-bg\nspec:\n  type: Java\n  mode: cluster\n  image: gcr.io/ynli-k8s/spark:v2.3.0-gcs\n  imagePullPolicy: Always\n  hadoopConf:\n    \"fs.gs.project.id\": \"foo\"\n    \"fs.gs.system.bucket\": \"foo-bucket\"\n    \"google.cloud.auth.service.account.enable\": \"true\"\n    \"google.cloud.auth.service.account.json.keyfile\": \"/mnt/secrets/key.json\"\n  driver:\n    cores: 1\n    secrets:\n    - name: \"gcs-bq\"\n      path: \"/mnt/secrets\"\n      secretType: GCPServiceAccount\n    envVars:\n      GCS_PROJECT_ID: foo\n    serviceAccount: spark\n  executor:\n    instances: 2\n    cores: 1\n    memory: \"512m\"\n    secrets:\n    - name: \"gcs-bq\"\n      path: \"/mnt/secrets\"\n      secretType: GCPServiceAccount\n    envVars:\n      GCS_PROJECT_ID: foo\n```\n\n----------------------------------------\n\nTITLE: Displaying Available Makefile Targets\nDESCRIPTION: Command to display the full list of available Makefile targets for the Spark operator project.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/developer-guide.md#2025-04-10_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n$ make help         \n\nUsage:\n  make <target>\n\nGeneral\n  help                            Display this help.\n  version                         Print version information.\n\nDevelopment\n  manifests                       Generate CustomResourceDefinition, RBAC and WebhookConfiguration manifests.\n  generate                        Generate code containing DeepCopy, DeepCopyInto, and DeepCopyObject method implementations.\n  update-crd                      Update CRD files in the Helm chart.\n  go-clean                        Clean up caches and output.\n  go-fmt                          Run go fmt against code.\n  go-vet                          Run go vet against code.\n  lint                            Run golangci-lint linter.\n  lint-fix                        Run golangci-lint linter and perform fixes.\n  unit-test                       Run unit tests.\n  e2e-test                        Run the e2e tests against a Kind k8s instance that is spun up.\n\nBuild\n  build-operator                  Build Spark operator.\n  build-sparkctl                  Build sparkctl binary.\n  install-sparkctl                Install sparkctl binary.\n  clean                           Clean spark-operator and sparkctl binaries.\n  build-api-docs                  Build api documentation.\n  docker-build                    Build docker image with the operator.\n  docker-push                     Push docker image with the operator.\n  docker-buildx                   Build and push docker image for the operator for cross-platform support\n\nHelm\n  detect-crds-drift               Detect CRD drift.\n  helm-unittest                   Run Helm chart unittests.\n  helm-lint                       Run Helm chart lint test.\n  helm-docs                       Generates markdown documentation for helm charts from requirements and values files.\n\nDeployment\n  kind-create-cluster             Create a kind cluster for integration tests.\n  kind-load-image                 Load the image into the kind cluster.\n  kind-delete-custer              Delete the created kind cluster.\n  install-crd                     Install CRDs into the K8s cluster specified in ~/.kube/config.\n  uninstall-crd                   Uninstall CRDs from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found=true to ignore resource not found errors during deletion.\n  deploy                          Deploy controller to the K8s cluster specified in ~/.kube/config.\n  undeploy                        Undeploy controller from the K8s cluster specified in ~/.kube/config. Call with ignore-not-found=true to ignore resource not found errors during deletion.\n\nDependencies\n  kustomize                       Download kustomize locally if necessary.\n  controller-gen                  Download controller-gen locally if necessary.\n  kind                            Download kind locally if necessary.\n  envtest                         Download setup-envtest locally if necessary.\n  golangci-lint                   Download golangci-lint locally if necessary.\n  gen-crd-api-reference-docs      Download gen-crd-api-reference-docs locally if necessary.\n  helm                            Download helm locally if necessary.\n  helm-unittest-plugin            Download helm unittest plugin locally if necessary.\n  helm-docs-plugin                Download helm-docs plugin locally if necessary.\n```\n\n----------------------------------------\n\nTITLE: Hugo Shortcode Content for Kubernetes Version\nDESCRIPTION: Displays the content of a Hugo shortcode file that defines the minimum required version of Kubernetes.\nSOURCE: https://github.com/kubeflow/website/blob/master/README.md#2025-04-10_snippet_7\n\nLANGUAGE: html\nCODE:\n```\n1.8\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Kubeflow Pipelines (Local Manifests)\nDESCRIPTION: These commands uninstall Kubeflow Pipelines using manifests from the local file system. They remove platform-agnostic resources and cluster-scoped resources.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/localcluster-deployment.md#2025-04-10_snippet_18\n\nLANGUAGE: SHELL\nCODE:\n```\nkubectl delete -k manifests/kustomize/env/platform-agnostic\nkubectl delete -k manifests/kustomize/cluster-scoped-resources\n```\n\n----------------------------------------\n\nTITLE: Configuring SparkApplication for YuniKorn Batch Scheduling\nDESCRIPTION: YAML configuration snippet showing how to specify YuniKorn as the batch scheduler in a SparkApplication manifest, including queue configuration. This tells the Spark operator to use YuniKorn for scheduling.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/yunikorn-integration.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  ...\n  batchScheduler: yunikorn\n  batchSchedulerOptions:\n    queue: root.default\n```\n\n----------------------------------------\n\nTITLE: Installing Spark Operator with Volcano Integration using Helm\nDESCRIPTION: Helm commands to install Kubernetes Operator for Apache Spark with Volcano batch scheduling enabled. This installs the operator in the spark-operator namespace with webhook and batch scheduler features enabled.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/volcano-integration.md#2025-04-10_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\nhelm repo add spark-operator https://kubeflow.github.io/spark-operator\n\nhelm install my-release spark-operator/spark-operator \\\n    --namespace spark-operator \\\n    --set webhook.enable=true \\\n    --set batchScheduler.enable=true\n```\n\n----------------------------------------\n\nTITLE: Running E2E Tests for Spark Operator\nDESCRIPTION: Series of commands to set up a kind cluster, build and load the Docker image, run e2e tests, and clean up for the Spark operator project.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/developer-guide.md#2025-04-10_snippet_8\n\nLANGUAGE: shell\nCODE:\n```\n# Create a kind cluster\nmake kind-create-cluster\n\n# Build docker image\nmake docker-build IMAGE_TAG=local\n\n# Load docker image to kind cluster\nmake kind-load-image\n\n# Run e2e tests\nmake e2e-test\n\n# Delete the kind cluster\nmake kind-delete-cluster\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variables for Tasks in KFP v2\nDESCRIPTION: This example demonstrates how to set environment variables for a pipeline task using the set_env_variable() method, which allows components to access these variables at runtime through the os.environ dictionary.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/compose-components-into-pipelines.md#2025-04-10_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import dsl\n\n@dsl.component\ndef print_env_var():\n    import os\n    print(os.environ.get('MY_ENV_VAR'))\n\n@dsl.pipeline()\ndef my_pipeline():\n    task = print_env_var()\n    task.set_env_variable('MY_ENV_VAR', 'hello')\n```\n\n----------------------------------------\n\nTITLE: Creating a K3s cluster on Windows Subsystem for Linux\nDESCRIPTION: Bootstraps a Kubernetes cluster using K3s on Windows Subsystem for Linux (WSL). Requires manual download of K3s binary.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/localcluster-deployment.md#2025-04-10_snippet_9\n\nLANGUAGE: shell\nCODE:\n```\nsudo ./k3s server\n```\n\n----------------------------------------\n\nTITLE: Configuring Batch Scheduling in Spark Operator Controller\nDESCRIPTION: YAML configuration for enabling batch scheduling in the Spark operator controller, with optional default batch scheduler setting. This enables integration with YuniKorn for all SparkApplications.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/yunikorn-integration.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\ncontroller:\n  batchScheduler:\n    enable: true\n    # Setting the default batch scheduler is optional. The default only\n    # applies if the batchScheduler field on the SparkApplication spec is not set\n    default: yunikorn\n```\n\n----------------------------------------\n\nTITLE: Spark Driver Pod with YuniKorn Annotations and Labels\nDESCRIPTION: YAML representation of a Spark driver pod with YuniKorn-specific annotations and labels. Shows task group definitions, queue assignment, and other metadata added by the Spark operator for YuniKorn integration.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/yunikorn-integration.md#2025-04-10_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    yunikorn.apache.org/allow-preemption: \"true\"\n    yunikorn.apache.org/task-group-name: spark-driver\n    yunikorn.apache.org/task-groups: '[{\"name\":\"spark-driver\",\"minMember\":1,\"minResource\":{\"cpu\":\"1\",\"memory\":\"896Mi\"},\"labels\":{\"queue\":\"root.default\",\"version\":\"3.5.2\"}},{\"name\":\"spark-executor\",\"minMember\":2,\"minResource\":{\"cpu\":\"1\",\"memory\":\"896Mi\"},\"labels\":{\"queue\":\"root.default\",\"version\":\"3.5.2\"}}]'\n    yunikorn.apache.org/user.info: '{\"user\":\"system:serviceaccount:spark-operator:spark-operator-controller\",\"groups\":[\"system:serviceaccounts\",\"system:serviceaccounts:spark-operator\",\"system:authenticated\"]}'\n  creationTimestamp: \"2024-09-10T04:40:37Z\"\n  labels:\n    queue: root.default\n    spark-app-name: spark-pi-yunikorn\n    spark-app-selector: spark-1bfe85bb77df4d5594337249b38c9648\n    spark-role: driver\n    spark-version: 3.5.2\n    sparkoperator.k8s.io/app-name: spark-pi-yunikorn\n    sparkoperator.k8s.io/launched-by-spark-operator: \"true\"\n    sparkoperator.k8s.io/submission-id: 1a71de55-cdc7-4e62-b997-197883dc4cbe\n    version: 3.5.2\n  name: spark-pi-yunikorn-driver\n  ...\n```\n\n----------------------------------------\n\nTITLE: Running Unit Tests for Spark Operator\nDESCRIPTION: Command to run unit tests for the Spark operator project.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/developer-guide.md#2025-04-10_snippet_7\n\nLANGUAGE: shell\nCODE:\n```\nmake unit-test\n```\n\n----------------------------------------\n\nTITLE: Specifying Component Metadata in YAML\nDESCRIPTION: YAML configuration for defining component metadata, including name and description, along with the full component specification.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/component-development.md#2025-04-10_snippet_8\n\nLANGUAGE: yaml\nCODE:\n```\nname: Get Lines\ndescription: Gets the specified number of lines from the input file.\n\ninputs:\n- {name: Input 1, type: String, description: 'Data for input 1'}\n- {name: Parameter 1, type: Integer, default: '100', description: 'Number of lines to copy'}\n\noutputs:\n- {name: Output 1, type: String, description: 'Output 1 data.'}\n\nimplementation:\n  container:\n    image: gcr.io/my-org/my-image@sha256:a172..752f\n    command: [\n      python3, \n      /pipelines/component/src/program.py,\n      --input1-path,\n      {inputPath: Input 1},\n      --param1, \n      {inputValue: Parameter 1},\n      --output1-path, \n      {outputPath: Output 1},\n    ]\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Kubeflow Pipelines Using GitHub Manifests\nDESCRIPTION: Commands to uninstall Kubeflow Pipelines using manifests from a GitHub repository. This sets the pipeline version environment variable and uses kubectl delete with kustomize to remove both environment-specific and cluster-scoped resources.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/standalone-deployment.md#2025-04-10_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\nexport PIPELINE_VERSION={{% pipelines/latest-version %}}\nkubectl delete -k \"github.com/kubeflow/pipelines/manifests/kustomize/env/dev?ref=$PIPELINE_VERSION\"\nkubectl delete -k \"github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref=$PIPELINE_VERSION\"\n```\n\n----------------------------------------\n\nTITLE: Adding Remote Python Dependencies in PySpark\nDESCRIPTION: Demonstrates how to add remote Python dependencies to a PySpark application using SparkFiles in Spark 2.4.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/writing-sparkapplication.md#2025-04-10_snippet_26\n\nLANGUAGE: python\nCODE:\n```\npython_dep_file_path = SparkFiles.get(\"python-dep.zip\")\nspark.sparkContext.addPyFile(dep_file_path)\n```\n\n----------------------------------------\n\nTITLE: Cloning the Spark Operator Repository\nDESCRIPTION: Commands to clone the Spark operator repository from GitHub and change to the project directory.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/developer-guide.md#2025-04-10_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone git@github.com:kubeflow/spark-operator.git\n\ncd spark-operator\n```\n\n----------------------------------------\n\nTITLE: Deploying XGBoostJob with kubectl\nDESCRIPTION: Command to create the XGBoostJob resource in Kubernetes to start the training job.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/xgboost.md#2025-04-10_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -f xgboostjob.yaml\n```\n\n----------------------------------------\n\nTITLE: Pushing Branch to GitHub Fork\nDESCRIPTION: Commands to push your local branch with changes to your fork on GitHub. This ensures your changes are uploaded to your GitHub repository before creating a pull request.\nSOURCE: https://github.com/kubeflow/website/blob/master/quick-github-guide.md#2025-04-10_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ngit checkout doc-updates\ngit push origin doc-updates\n```\n\n----------------------------------------\n\nTITLE: Uninstalling Kubeflow Pipelines with GCP Configuration\nDESCRIPTION: Commands to uninstall Kubeflow Pipelines when using GCP Cloud SQL and Google Cloud Storage. These kubectl delete commands remove both GCP-specific environment and cluster-scoped resources.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/standalone-deployment.md#2025-04-10_snippet_13\n\nLANGUAGE: bash\nCODE:\n```\nkubectl delete -k manifests/kustomize/env/gcp\nkubectl delete -k manifests/kustomize/cluster-scoped-resources\n```\n\n----------------------------------------\n\nTITLE: Configuring Init Containers in Spark Application YAML\nDESCRIPTION: This YAML snippet shows how to configure init containers for driver and executor pods in a Spark application.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/writing-sparkapplication.md#2025-04-10_snippet_20\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  driver:\n    initContainers:\n    - name: \"init-container1\"\n      image: \"init-container1:latest\"\n      ...\n  executor:\n    initContainers:\n    - name: \"init-container1\"\n      image: \"init-container1:latest\"\n      ...\n```\n\n----------------------------------------\n\nTITLE: Building the Spark Operator Docker Image\nDESCRIPTION: Commands to build the Spark operator Docker image, including an example of using a custom Spark base image.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/developer-guide.md#2025-04-10_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\nmake docker-build IMAGE_TAG=<image-tag>\n```\n\nLANGUAGE: shell\nCODE:\n```\ndocker build --build-arg SPARK_IMAGE=<your Spark image> -t <image-tag> .\n```\n\n----------------------------------------\n\nTITLE: Accessing XGBoostJob Master Logs\nDESCRIPTION: Commands to get the master pod name and stream its logs to monitor the training progress.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/xgboost.md#2025-04-10_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nPODNAME=$(kubectl get pods -l job-name=xgboost-dist-iris-test-train,replica-type=master,replica-index=0 -o name)\nkubectl logs -f ${PODNAME}\n```\n\n----------------------------------------\n\nTITLE: Hugo Frontmatter Configuration for Kubeflow Distributions Page\nDESCRIPTION: This code snippet defines the Hugo frontmatter configuration for a page about Kubeflow distributions. It specifies the page title, description, and weight (which determines the page order in navigation).\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/distributions/_index.md#2025-04-10_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n+++\ntitle = \"Distributions\"\ndescription = \"Distributions of Kubeflow\"\nweight = 40\n+++\n```\n\n----------------------------------------\n\nTITLE: Checking for Obsolete Kubeflow Pipelines Resources\nDESCRIPTION: These commands check for the existence of obsolete resources like metadata-deployment and metadata-service when upgrading from Kubeflow Pipelines earlier than v0.4.0.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/standalone-deployment.md#2025-04-10_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nkubectl -n <KFP_NAMESPACE> get deployments | grep metadata-deployment\nkubectl -n <KFP_NAMESPACE> get service | grep metadata-service\n```\n\n----------------------------------------\n\nTITLE: Configuring Pod Affinity in Spark Application YAML\nDESCRIPTION: This YAML snippet shows how to configure pod affinity and anti-affinity for driver and executor pods in a Spark application.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/writing-sparkapplication.md#2025-04-10_snippet_16\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  driver:\n    affinity:\n      podAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          ...\n  executor:\n    affinity:\n      podAntiAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n          ...\n```\n\n----------------------------------------\n\nTITLE: Installing Locust Dependencies in Virtual Environment\nDESCRIPTION: Commands to set up a Python virtual environment and install required dependencies for running Locust benchmarking tests\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/performance/benchmarking.md#2025-04-10_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython3.12 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Viewing MPI Job Configuration Example\nDESCRIPTION: Command to display the example MPIJob configuration file\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/mpi.md#2025-04-10_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\ncat examples/v2beta1/tensorflow-benchmarks/tensorflow-benchmarks.yaml\n```\n\n----------------------------------------\n\nTITLE: Checking Pipeline Input Types Against Component Types\nDESCRIPTION: Example of a pipeline that defines input types which are checked against component I/O types. The pipeline uses type annotations for its inputs and passes them to a component with matching type requirements.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/static-type-checking.md#2025-04-10_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@component\ndef task_factory_a(\n    field_m: {\n        'GCSPath': {\n            'openapi_schema_validator':\n                '{\"type\": \"string\", \"pattern\": \"^gs://.*$\"}'\n        }\n    }, field_o: 'Integer'):\n  return ContainerOp(\n      name='operator a',\n      image='gcr.io/ml-pipeline/component-a',\n      arguments=[\n          '--field-l',\n          field_m,\n          '--field-o',\n          field_o,\n      ],\n  )\n\n\n# Pipeline input types are also checked against the component I/O types.\n@dsl.pipeline(name='type_check', description='')\ndef pipeline(\n    a: {\n        'GCSPath': {\n            'openapi_schema_validator':\n                '{\"type\": \"string\", \"pattern\": \"^gs://.*$\"}'\n        }\n    } = 'good',\n    b: Integer() = 12):\n  task_factory_a(field_m=a, field_o=b)\n\n\ntry:\n  compiler.Compiler().compile(pipeline, 'pipeline.tar.gz', type_check=True)\nexcept InconsistentTypeException as e:\n  print(e)\n```\n\n----------------------------------------\n\nTITLE: Deleting Obsolete Kubeflow Pipelines Resources\nDESCRIPTION: These commands delete obsolete resources (metadata-deployment and metadata-service) that may exist after upgrading from Kubeflow Pipelines earlier than v0.4.0 to v0.4.0 or higher.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/standalone-deployment.md#2025-04-10_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nkubectl -n <KFP_NAMESPACE> delete deployment metadata-deployment\nkubectl -n <KFP_NAMESPACE> delete service metadata-service\n```\n\n----------------------------------------\n\nTITLE: Specifying Image Pull Secrets in Spark Application YAML\nDESCRIPTION: This YAML snippet demonstrates how to specify image pull secrets for a Spark application to use when pulling container images.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/writing-sparkapplication.md#2025-04-10_snippet_15\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  imagePullSecrets:\n    - secret1\n    - secret2\n```\n\n----------------------------------------\n\nTITLE: Installing Latest Changes of Training Operator Control Plane\nDESCRIPTION: This command installs the latest changes of the Training Operator control plane from the master branch using kubectl.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/installation.md#2025-04-10_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\nkubectl apply --server-side -k \"github.com/kubeflow/training-operator.git/manifests/overlays/standalone?ref=master\"\n```\n\n----------------------------------------\n\nTITLE: Retrieving and Monitoring JAXJob Logs\nDESCRIPTION: Commands to get the worker pod name and follow its logs during the distributed JAX computation. This is useful for monitoring the training progress and debugging issues.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/jax.md#2025-04-10_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nPODNAME=$(kubectl get pods -l training.kubeflow.org/job-name=jaxjob-simple,training.kubeflow.org/replica-type=worker,training.kubeflow.org/replica-index=0 -o name -n kubeflow)\nkubectl logs -f ${PODNAME} -n kubeflow\n```\n\n----------------------------------------\n\nTITLE: Accessing Kubeflow Pipelines UI\nDESCRIPTION: This command sets up port-forwarding to access the Kubeflow Pipelines UI. It forwards local port 8080 to the ml-pipeline-ui service in the kubeflow namespace.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/localcluster-deployment.md#2025-04-10_snippet_15\n\nLANGUAGE: SHELL\nCODE:\n```\nkubectl port-forward -n kubeflow svc/ml-pipeline-ui 8080:80\n```\n\n----------------------------------------\n\nTITLE: Accessing Kubeflow Pipelines UI\nDESCRIPTION: This command sets up port-forwarding to access the Kubeflow Pipelines UI. It forwards local port 8080 to the ml-pipeline-ui service in the kubeflow namespace.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/localcluster-deployment.md#2025-04-10_snippet_15\n\nLANGUAGE: SHELL\nCODE:\n```\nkubectl port-forward -n kubeflow svc/ml-pipeline-ui 8080:80\n```\n\n----------------------------------------\n\nTITLE: Deploying Spark Operator Instance 1 with Helm in Bash\nDESCRIPTION: This snippet shows how to create a namespace 'spark-1' and install the first Spark operator instance using Helm. The operator is configured to watch the 'spark-1' namespace for Spark jobs.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/running-multiple-instances-of-the-operator.md#2025-04-10_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Create the spark-1 namespace if it does not exist\nkubectl create ns spark-1\n\n# Install the Spark operator with release name spark-operator-1\nhelm install spark-operator-1 spark-operator/spark-operator \\\n    --namespace spark-operator \\\n    --create-namespace \\\n    --set 'spark.jobNamespaces={spark-1}'\n```\n\n----------------------------------------\n\nTITLE: Enabling Resource Quota Enforcement via Command Line Arguments\nDESCRIPTION: This command line argument enables resource quota enforcement in the Spark Operator. It's recommended to use this in conjunction with setting the webhook to fail on error for better resource management in namespaces with quota constraints.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/resource-quota-enforcement.md#2025-04-10_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\n-enable-resource-quota-enforcement=true\n```\n\nLANGUAGE: shell\nCODE:\n```\n-webhook-fail-on-error=true\n```\n\n----------------------------------------\n\nTITLE: Installing Latest Stable Release of Training SDK\nDESCRIPTION: This pip command installs the latest stable release of the Kubeflow Training SDK from PyPI.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/installation.md#2025-04-10_snippet_4\n\nLANGUAGE: shell\nCODE:\n```\npip install -U kubeflow-training\n```\n\n----------------------------------------\n\nTITLE: Configuring Training Operator for Volcano Scheduler\nDESCRIPTION: Configuration diff showing how to modify the Training Operator deployment to use Volcano for gang-scheduling by adding the appropriate command-line flag.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/job-scheduling.md#2025-04-10_snippet_4\n\nLANGUAGE: diff\nCODE:\n```\n...\n    spec:\n      containers:\n        - command:\n            - /manager\n+           - --gang-scheduler-name=volcano\n          image: kubeflow/training-operator\n          name: training-operator\n...\n```\n\n----------------------------------------\n\nTITLE: Configuring Hugo Front Matter for YouTube Link\nDESCRIPTION: Hugo front matter configuration that defines a manual link to a YouTube playlist and sets the icon to YouTube's brand icon. The configuration is used to create a link component for the Kubeflow Summit 2023 video content.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/events/past-events/2023/watch--kubeflow-summit-2023.md#2025-04-10_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n+++\ntitle = \"Watch: Kubeflow Summit 2023\"\n\nmanualLink = \"https://www.youtube.com/playlist?list=PL2gwy7BdKoGdrkYIWGeAdKi9ntfxq8FYt\"\nicon = \"fa-brands fa-youtube\"\n+++\n```\n\n----------------------------------------\n\nTITLE: Setting Up Git Pre-Commit Hooks\nDESCRIPTION: Commands to install and set up Git pre-commit hooks for the Spark operator project.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/developer-guide.md#2025-04-10_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\npre-commit install\n\npre-commit install-hooks\n```\n\n----------------------------------------\n\nTITLE: Installing Spark Operator with Custom Webhook Port using Helm\nDESCRIPTION: Command to install the Spark Operator with mutating admission webhook enabled on a custom port (443) for private GKE or EKS clusters. The configuration specifies the webhook port and target job namespaces.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/getting-started.md#2025-04-10_snippet_18\n\nLANGUAGE: shell\nCODE:\n```\nhelm install my-release spark-operator/spark-operator \\\n   --namespace spark-operator  \\\n   --create-namespace \\\n   --set \"spark.jobNamespaces={spark}\" \\\n   --set webhook.enable=true \\\n   --set webhook.port=443\n```\n\n----------------------------------------\n\nTITLE: Installing Kubeflow Training Runtimes\nDESCRIPTION: Command to deploy the Kubeflow Training Runtimes using kubectl apply with server-side application.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/operator-guides/installation.md#2025-04-10_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply --server-side -k \"https://github.com/kubeflow/trainer.git/manifests/overlays/runtimes?ref=master\"\n```\n\n----------------------------------------\n\nTITLE: Installing Latest Changes of Training SDK\nDESCRIPTION: This pip command installs the latest changes of the Kubeflow Training SDK directly from the GitHub repository's master branch.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/installation.md#2025-04-10_snippet_5\n\nLANGUAGE: shell\nCODE:\n```\npip install git+https://github.com/kubeflow/training-operator.git@master#subdirectory=sdk/python\n```\n\n----------------------------------------\n\nTITLE: Connecting to Standalone KFP Inside the Cluster (Same Namespace)\nDESCRIPTION: Python code for connecting to Kubeflow Pipelines when running inside the same Kubernetes namespace, using the internal service DNS.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/connect-api.md#2025-04-10_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport kfp\n\nclient = kfp.Client(host=\"http://ml-pipeline-ui:80\")\n\nprint(client.list_experiments())\n```\n\n----------------------------------------\n\nTITLE: Configuring Spark Application Resource Version Polling\nDESCRIPTION: Configuration setting to enable resource version polling for Spark executors. This setting helps reduce API server load but may cause inconsistency in HA setups.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/performance/benchmarking.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nspark.kubernetes.executor.enablePollingWithResourceVersion: \"true\"\n```\n\n----------------------------------------\n\nTITLE: Upgrading Spark Operator with New Image\nDESCRIPTION: Command to upgrade the Spark Operator Helm release with a new container image. This allows for updating to newer versions of the operator.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/getting-started.md#2025-04-10_snippet_14\n\nLANGUAGE: shell\nCODE:\n```\nhelm upgrade <YOUR-HELM-RELEASE-NAME> --set image.repository=org/image --set image.tag=newTag\n```\n\n----------------------------------------\n\nTITLE: Installing MPI Operator via Git\nDESCRIPTION: Commands to clone and deploy the MPI operator with default settings using kubectl\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/mpi.md#2025-04-10_snippet_0\n\nLANGUAGE: shell\nCODE:\n```\ngit clone https://github.com/kubeflow/mpi-operator\ncd mpi-operator\nkubectl apply -f deploy/v2beta1/mpi-operator.yaml\n```\n\n----------------------------------------\n\nTITLE: Verifying Training Operator Controller Installation\nDESCRIPTION: This command checks if the Training Operator controller pod is running in the kubeflow namespace.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/installation.md#2025-04-10_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get pods -n kubeflow\n```\n\n----------------------------------------\n\nTITLE: Importing S3 Dataset Parameters for Object Storage Integration\nDESCRIPTION: Imports the S3DatasetParams class from Kubeflow's storage initializer module, which allows loading datasets from S3-compatible object storage platforms like Amazon S3.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/llm-hp-optimization.md#2025-04-10_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom kubeflow.storage_initializer.s3 import S3DatasetParams\n```\n\n----------------------------------------\n\nTITLE: Viewing Merged CSV Data\nDESCRIPTION: Shell command to display the first few rows of the merged CSV file.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/build-pipeline.ipynb#2025-04-10_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n!head merged_data.csv\n```\n\n----------------------------------------\n\nTITLE: Installing Spark Operator to a Specific Namespace\nDESCRIPTION: Command to install the Spark Operator to a specific namespace. This example creates a release named 'spark-operator' in the 'spark-operator' namespace and creates the namespace if it doesn't exist.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/getting-started.md#2025-04-10_snippet_2\n\nLANGUAGE: shell\nCODE:\n```\nhelm install spark-operator spark-operator/spark-operator \\\n    --namespace spark-operator \\\n    --create-namespace\n```\n\n----------------------------------------\n\nTITLE: Installing MPI Operator with kubectl kustomize (K8s v1.21+)\nDESCRIPTION: Command to install MPI operator using kubectl's built-in kustomize functionality for Kubernetes v1.21+\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/mpi.md#2025-04-10_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl apply -k manifests/overlays/kubeflow\n```\n\n----------------------------------------\n\nTITLE: Checking Installed Kubernetes CRDs for ML Frameworks\nDESCRIPTION: This command lists the Custom Resource Definitions (CRDs) installed for various ML frameworks supported by the Training Operator.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/installation.md#2025-04-10_snippet_3\n\nLANGUAGE: shell\nCODE:\n```\nkubectl get crd\n```\n\n----------------------------------------\n\nTITLE: Setting TTL for SparkApplication in YAML\nDESCRIPTION: This YAML snippet shows how to set the Time-To-Live (TTL) duration for a SparkApplication using the timeToLiveSeconds field. The SparkApplication object will be garbage collected after the specified duration following its termination.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/user-guide/working-with-sparkapplication.md#2025-04-10_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\nspec:\n  timeToLiveSeconds: 3600\n```\n\n----------------------------------------\n\nTITLE: Installing Python Packages in Kubeflow Notebook Images\nDESCRIPTION: Instructions for extending Kubeflow Notebook images with Python packages using pip or conda. References example Dockerfiles for both pip and conda installation approaches.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/notebooks/container-images.md#2025-04-10_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install ...\n```\n\nLANGUAGE: bash\nCODE:\n```\nconda install ...\n```\n\n----------------------------------------\n\nTITLE: Installing Pre-Commit Package Manager\nDESCRIPTION: Commands to install the pre-commit package manager using pip, conda, or Homebrew, which is required for running Git hooks.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/spark-operator/developer-guide.md#2025-04-10_snippet_1\n\nLANGUAGE: shell\nCODE:\n```\n# Using pip\npip install pre-commit\n\n# Using conda\nconda install -c conda-forge pre-commit\n\n# Using Homebrew\nbrew install pre-commit\n```\n\n----------------------------------------\n\nTITLE: TensorFlow Benchmark Training Logs Output\nDESCRIPTION: Example log output from a TensorFlow benchmark training job showing the model configuration, training progress, and images processed per second metrics.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/mpi.md#2025-04-10_snippet_10\n\nLANGUAGE: yaml\nCODE:\n```\nTensorFlow:  1.14\nModel:       resnet101\nDataset:     imagenet (synthetic)\nMode:        training\nSingleSess:  False\nBatch size:  128 global\n             64 per device\nNum batches: 100\nNum epochs:  0.01\nDevices:     ['horovod/gpu:0', 'horovod/gpu:1']\nNUMA bind:   False\nData format: NCHW\nOptimizer:   sgd\nVariables:   horovod\n\n...\n\n40\timages/sec: 154.4 +/- 0.7 (jitter = 4.0)\t8.280\n40\timages/sec: 154.4 +/- 0.7 (jitter = 4.1)\t8.482\n50\timages/sec: 154.8 +/- 0.6 (jitter = 4.0)\t8.397\n50\timages/sec: 154.8 +/- 0.6 (jitter = 4.2)\t8.450\n60\timages/sec: 154.5 +/- 0.5 (jitter = 4.1)\t8.321\n60\timages/sec: 154.5 +/- 0.5 (jitter = 4.4)\t8.349\n70\timages/sec: 154.5 +/- 0.5 (jitter = 4.0)\t8.433\n70\timages/sec: 154.5 +/- 0.5 (jitter = 4.4)\t8.430\n80\timages/sec: 154.8 +/- 0.4 (jitter = 3.6)\t8.199\n80\timages/sec: 154.8 +/- 0.4 (jitter = 3.8)\t8.404\n90\timages/sec: 154.6 +/- 0.4 (jitter = 3.7)\t8.418\n90\timages/sec: 154.6 +/- 0.4 (jitter = 3.6)\t8.459\n100\timages/sec: 154.2 +/- 0.4 (jitter = 4.0)\t8.372\n100\timages/sec: 154.2 +/- 0.4 (jitter = 4.0)\t8.542\n----------------------------------------------------------------\ntotal images/sec: 308.27\n```\n\n----------------------------------------\n\nTITLE: Checking Remote Repositories in Git\nDESCRIPTION: Command to list all remote repositories linked to your local Git repository with their URLs. This helps verify that both your fork (origin) and the upstream repository are correctly configured.\nSOURCE: https://github.com/kubeflow/website/blob/master/quick-github-guide.md#2025-04-10_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit remote -vv\n```\n\n----------------------------------------\n\nTITLE: Installing Kubeflow Pipelines SDK in Python\nDESCRIPTION: Installs the Kubeflow Pipelines SDK using pip. This step is required before building pipelines with Kubeflow.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/build-pipeline.ipynb#2025-04-10_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install kfp --upgrade\n```\n\n----------------------------------------\n\nTITLE: Installing Kubeflow Pipelines SDK in Python\nDESCRIPTION: Installs the Kubeflow Pipelines SDK using pip. This step is required before building pipelines with Kubeflow.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/build-pipeline.ipynb#2025-04-10_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n!pip install kfp --upgrade\n```\n\n----------------------------------------\n\nTITLE: Installing Hugo via Homebrew Script\nDESCRIPTION: Commands to install a specific version (0.124.1) of Hugo using Homebrew on macOS or Linux, including pinning the version to prevent unwanted updates.\nSOURCE: https://github.com/kubeflow/website/blob/master/README.md#2025-04-10_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nHOMEBREW_COMMIT=\"9d025105a8be086b2eeb3b1b2697974f848dbaac\" # 0.124.1\ncurl -fL -o \"hugo.rb\" \"https://raw.githubusercontent.com/Homebrew/homebrew-core/${HOMEBREW_COMMIT}/Formula/h/hugo.rb\"\nbrew install ./hugo.rb\nbrew pin hugo\n```\n\n----------------------------------------\n\nTITLE: Initializing KFP Local Execution with DockerRunner\nDESCRIPTION: This code snippet shows how to initialize local execution using the DockerRunner, which requires Docker to be installed and provides the strongest form of local runtime environment isolation.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/execute-kfp-pipelines-locally.md#2025-04-10_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom kfp import local\n\nlocal.init(runner=local.DockerRunner())\n```\n\n----------------------------------------\n\nTITLE: Viewing Intel MPI Example Configuration\nDESCRIPTION: Command to display an example YAML configuration for an Intel MPI job. This provides a template for creating Intel MPI jobs.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/mpi.md#2025-04-10_snippet_11\n\nLANGUAGE: bash\nCODE:\n```\ncat examples/pi/pi-intel.yaml\n```\n\n----------------------------------------\n\nTITLE: Installing Node Dependencies\nDESCRIPTION: Command to install required Node.js dependencies for site styling as specified in package.json.\nSOURCE: https://github.com/kubeflow/website/blob/master/README.md#2025-04-10_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nnpm install -D\n```\n\n----------------------------------------\n\nTITLE: Hugo Front Matter Configuration in TOML\nDESCRIPTION: TOML-formatted front matter block defining metadata for a Hugo documentation page about Kubeflow Pipelines concepts. Specifies the page title, description and navigation weight.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/concepts/_index.md#2025-04-10_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n+++\ntitle = \"Concepts\"\ndescription = \"Concepts used in Kubeflow Pipelines\"\nweight = 4\n+++\n```\n\n----------------------------------------\n\nTITLE: Rendering Alert Box in Markdown/HTML\nDESCRIPTION: This snippet defines an alert box using a combination of Markdown and HTML syntax, likely for use in a static site generator. It provides a warning that packaged distributions are not endorsed by the Kubeflow community.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/started/installing-kubeflow/index.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{{% alert title=\"\" color=\"info\" %}}\nPackaged distributions are developed and supported by their respective maintainers.\nThe Kubeflow community <strong>does not endorse or certify</strong> any specific distribution.\n{{% /alert %}}\n```\n\n----------------------------------------\n\nTITLE: Disabling Caching via Environment Variable\nDESCRIPTION: Demonstrates how to disable caching by default for all pipelines using the KFP_DISABLE_EXECUTION_CACHING_BY_DEFAULT environment variable during pipeline compilation.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/core-functions/caching.md#2025-04-10_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nKFP_DISABLE_EXECUTION_CACHING_BY_DEFAULT=true \\\nkfp dsl compile --py my_pipeline.py --output my_pipeline.yaml\n```\n\n----------------------------------------\n\nTITLE: Deploying PaddleJob Training Resource\nDESCRIPTION: Command to create a PaddleJob by applying a predefined YAML configuration file that sets up a distributed PaddlePaddle training environment.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/paddle.md#2025-04-10_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl create -f https://raw.githubusercontent.com/kubeflow/training-operator/refs/heads/release-1.9/examples/paddlepaddle/simple-cpu.yaml\n```\n\n----------------------------------------\n\nTITLE: Adding Kubeflow Calendar to Google Calendar\nDESCRIPTION: Instructions for manually adding the Kubeflow community calendar to a user's Google Calendar. This allows users to stay updated on Kubeflow community meetings and events.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/about/community.md#2025-04-10_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nkubeflow.org_7l5vnbn8suj2se10sen81d9428@group.calendar.google.com\n```\n\n----------------------------------------\n\nTITLE: Speaker Cards Grid Structure in HTML\nDESCRIPTION: HTML container and grid structure for displaying speaker cards with names and affiliations\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/events/past-events/2023/kubeflow-summit-2023.md#2025-04-10_snippet_1\n\nLANGUAGE: HTML\nCODE:\n```\n<div class=\"container\">\n  <div class=\"row\">\n    <div class=\"col-auto mb-3\">\n      {{< card title=\"Josh Bottom\" \n               subtitle=\"Kubeflow Steering Committee\">}}\n      {{< /card >}}\n      <!-- Additional speaker cards -->\n    </div>\n  </div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Creating HTML Table for Kubeflow Distributions\nDESCRIPTION: This HTML snippet creates a responsive table that lists various Kubeflow distributions, their maintainers, supported Kubeflow versions, target platforms, and links to their websites.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/started/installing-kubeflow/index.md#2025-04-10_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"table-responsive distributions-table\">\n  <table class=\"table table-bordered\">\n    <thead>\n      <tr>\n        <th>Maintainer\n          <br><small>Distribution Name</small>\n        </th>\n        <th>Kubeflow Version</th>\n        <th>Target Platform</th>\n        <th>Link</th>\n      </tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td>\n          Amazon Web Services\n        </td>\n        <td>\n          {{< kf-version-notice >}}{{% aws/latest-version %}}{{< /kf-version-notice >}}\n          <sup><a href=\"https://github.com/awslabs/kubeflow-manifests/releases\">[release notes]</a></sup>\n        </td>\n        <td>\n          Amazon Elastic Kubernetes Service (EKS)\n        </td>\n        <td>\n          <a href=\"https://awslabs.github.io/kubeflow-manifests\">Website</a>\n        </td>\n      </tr>\n      <!-- ... more table rows ... -->\n    </tbody>\n  </table>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Sample Pipeline Details Response\nDESCRIPTION: JSON response example showing what to expect when retrieving pipeline details from the API, including ID, creation time, parameters, and default version information.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/tutorials/api-pipelines.md#2025-04-10_snippet_4\n\nLANGUAGE: json\nCODE:\n```\n{\n  \"id\": \"d30d28d7-0bfc-4f0c-8a57-6844a8ec9742\",\n  \"created_at\": \"2020-02-20T16:15:02Z\",\n  \"name\": \"sequential.tar.gz\",\n  \"parameters\": [\n    {\n      \"name\": \"url\",\n      \"value\": \"gs://ml-pipeline-playground/shakespeare1.txt\"\n    }\n  ],\n  \"default_version\": {\n    \"id\": \"d30d28d7-0bfc-4f0c-8a57-6844a8ec9742\",\n    \"name\": \"sequential.tar.gz\",\n    \"created_at\": \"2020-02-20T16:15:02Z\",\n    \"parameters\": [\n      {\n        \"name\": \"url\",\n        \"value\": \"gs://ml-pipeline-playground/shakespeare1.txt\"\n      }\n    ],\n    \"resource_references\": [\n      {\n        \"key\": {\n          \"type\": \"PIPELINE\",\n          \"id\": \"d30d28d7-0bfc-4f0c-8a57-6844a8ec9742\"\n        },\n        \"relationship\": \"OWNER\"\n      }\n    ]\n  }\n}\n```\n\n----------------------------------------\n\nTITLE: Getting PaddleJob Status\nDESCRIPTION: Command to retrieve detailed status information about the PaddleJob in YAML format.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/paddle.md#2025-04-10_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get -o yaml paddlejobs paddle-simple-cpu -n kubeflow\n```\n\n----------------------------------------\n\nTITLE: Hugo Frontmatter Configuration for Legacy Kubeflow Training Operator Documentation\nDESCRIPTION: Hugo frontmatter configuration that sets up the page title, description, and weight for the legacy version (v1) of the Kubeflow Training Operator documentation.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/_index.md#2025-04-10_snippet_0\n\nLANGUAGE: hugo\nCODE:\n```\n+++\ntitle = \"Legacy Kubeflow Training Operator (v1)\"\ndescription = \"Kubeflow Training Operator V1 Documentation\"\nweight = 999\n+++\n```\n\n----------------------------------------\n\nTITLE: API Request Example for Kubeflow Pipelines\nDESCRIPTION: Example HTTP GET request to list pipeline runs in a specific namespace using the v2beta1 API endpoint.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/reference/api/kubeflow-pipeline-api-spec.md#2025-04-10_snippet_0\n\nLANGUAGE: http\nCODE:\n```\nhttps://kubeflow.example.com/pipeline/apis/v2beta1/runs?namespace=team-1\n```\n\n----------------------------------------\n\nTITLE: Markdown Frontmatter Configuration\nDESCRIPTION: Hugo page configuration frontmatter defining the title, description and weight of the examples documentation page.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/started/kubeflow-examples.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n+++\ntitle = \"Examples\"\ndescription = \"Examples that demonstrate machine learning with Kubeflow\"\nweight = 99\n+++\n```\n\n----------------------------------------\n\nTITLE: Node Pool Configuration Example\nDESCRIPTION: YAML configuration showing node pool selection for pipeline execution, demonstrating how to explicitly schedule pipeline runs on specific node pools using nodeSelector.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/tutorials/benchmark-examples.md#2025-04-10_snippet_1\n\nLANGUAGE: YAML\nCODE:\n```\nnodeSelector:\n  cloud.google.com/gke-nodepool: pool-1\n```\n\n----------------------------------------\n\nTITLE: Monitoring PyTorchJob Status\nDESCRIPTION: Command to get detailed YAML output of a PyTorchJob's status and configuration.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/pytorch.md#2025-04-10_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl get -o yaml pytorchjobs pytorch-simple -n kubeflow\n```\n\n----------------------------------------\n\nTITLE: HTML Table Structure for Release Information\nDESCRIPTION: HTML table structure displaying release date, media links, manifests information and release team details.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/releases/kubeflow-1.8.md#2025-04-10_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"table-responsive\">\n<table class=\"table table-bordered\">\n  <tbody>\n    <tr>\n      <th class=\"table-light\">Release Date</th>\n      <td>\n        2023-11-01\n      </td>\n    </tr>\n    <!-- Additional rows -->\n  </tbody>\n</table>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Building the Component with KFP CLI\nDESCRIPTION: Command-line instruction to build the containerized component using the KFP CLI, which packages the source code into a Docker image without pushing it to a registry.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/containerized-python-components.md#2025-04-10_snippet_6\n\nLANGUAGE: sh\nCODE:\n```\nkfp component build src/ --component-filepattern my_component.py --no-push-image\n```\n\n----------------------------------------\n\nTITLE: Hugo Shortcode for MNIST Example\nDESCRIPTION: Hugo shortcode block defining the MNIST image classification example with metadata including Kubeflow version, repository URL and API endpoint.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/started/kubeflow-examples.md#2025-04-10_snippet_1\n\nLANGUAGE: markdown\nCODE:\n```\n{{% blocks/sample-section title=\"MNIST image classification\"\n  kfctl=\"v1.0.0\"\n  url=\"https://github.com/kubeflow/examples/tree/master/mnist\"\n  api=\"https://api.github.com/repos/kubeflow/examples/commits?path=mnist&sha=master\" %}}\n```\n\n----------------------------------------\n\nTITLE: Disabling Type Checking with dsl-compiler CLI\nDESCRIPTION: Using the dsl-compiler command-line tool to compile a pipeline with type checking disabled.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/static-type-checking.md#2025-04-10_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ndsl-compiler --py pipeline.py --output pipeline.zip --disable-type-check\n```\n\n----------------------------------------\n\nTITLE: PyTorchJob Configuration Example\nDESCRIPTION: Complete YAML configuration example showing a distributed PyTorch training job with master and worker nodes, including status information.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/pytorch.md#2025-04-10_snippet_4\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: kubeflow.org/v1\nkind: PyTorchJob\nmetadata:\n  clusterName: \"\"\n  creationTimestamp: 2018-12-16T21:39:09Z\n  generation: 1\n  name: pytorch-tcp-dist-mnist\n  namespace: default\n  resourceVersion: \"15532\"\n  selfLink: /apis/kubeflow.org/v1/namespaces/default/pytorchjobs/pytorch-tcp-dist-mnist\n  uid: 059391e8-017b-11e9-bf13-06afd8f55a5c\nspec:\n  cleanPodPolicy: None\n  pytorchReplicaSpecs:\n    Master:\n      replicas: 1\n      restartPolicy: OnFailure\n      template:\n        metadata:\n          creationTimestamp: null\n        spec:\n          containers:\n            - image: gcr.io/kubeflow-ci/pytorch-dist-mnist_test:1.0\n              name: pytorch\n              ports:\n                - containerPort: 23456\n                  name: pytorchjob-port\n              resources: {}\n    Worker:\n      replicas: 3\n      restartPolicy: OnFailure\n      template:\n        metadata:\n          creationTimestamp: null\n        spec:\n          containers:\n            - image: gcr.io/kubeflow-ci/pytorch-dist-mnist_test:1.0\n              name: pytorch\n              ports:\n                - containerPort: 23456\n                  name: pytorchjob-port\n              resources: {}\nstatus:\n  completionTime: 2018-12-16T21:43:27Z\n  conditions:\n    - lastTransitionTime: 2018-12-16T21:39:09Z\n      lastUpdateTime: 2018-12-16T21:39:09Z\n      message: PyTorchJob pytorch-tcp-dist-mnist is created.\n      reason: PyTorchJobCreated\n      status: \"True\"\n      type: Created\n    - lastTransitionTime: 2018-12-16T21:39:09Z\n      lastUpdateTime: 2018-12-16T21:40:45Z\n      message: PyTorchJob pytorch-tcp-dist-mnist is running.\n      reason: PyTorchJobRunning\n      status: \"False\"\n      type: Running\n    - lastTransitionTime: 2018-12-16T21:39:09Z\n      lastUpdateTime: 2018-12-16T21:43:27Z\n      message: PyTorchJob pytorch-tcp-dist-mnist is successfully completed.\n      reason: PyTorchJobSucceeded\n      status: \"True\"\n      type: Succeeded\n  replicaStatuses:\n    Master: {}\n    Worker: {}\n  startTime: 2018-12-16T21:40:45Z\n```\n\n----------------------------------------\n\nTITLE: HTML Component Version Table\nDESCRIPTION: HTML table structure displaying Kubeflow component versions organized by working groups including AutoML, Notebooks, Pipelines, Serving, Training and Data components.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/releases/kubeflow-1.9.md#2025-04-10_snippet_3\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"table-responsive\">\n<table class=\"table table-bordered\">\n    <thead class=\"thead-light\">\n      <tr>\n        <th>Maintainers</th>\n        <th>Component Name</th>\n        <th>Version</th>\n      </tr>\n    </thead>\n  <tbody>\n      <tr>\n        <td rowspan=\"1\" class=\"align-middle\">AutoML Working Group</td>\n        <td>Katib</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/katib/releases/tag/v0.17.0\">v0.17.0</a>\n        </td>\n      </tr>\n      <!-- Additional rows omitted for brevity -->\n  </tbody>\n</table>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Importing Kubeflow Pipelines Modules in Python\nDESCRIPTION: Imports the necessary Kubeflow Pipelines modules (kfp and kfp.components) to build pipelines and components.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/build-pipeline.ipynb#2025-04-10_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport kfp\nimport kfp.components as comp\n```\n\n----------------------------------------\n\nTITLE: Markdown Content Structure for Kubeflow Documentation\nDESCRIPTION: The root markdown file that structures the Kubeflow architecture documentation, including front matter and section organization.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/started/architecture.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n+++\ntitle = \"Architecture\"\ndescription = \"An overview of Kubeflow's architecture\"\nweight = 10\n+++\n```\n\n----------------------------------------\n\nTITLE: Importing Kubeflow Pipelines SDK in Python\nDESCRIPTION: Imports the necessary modules from the Kubeflow Pipelines SDK to work with Python function-based components.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/python-function-components.ipynb#2025-04-10_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport kfp\nfrom kfp.components import create_component_from_func\n```\n\n----------------------------------------\n\nTITLE: Defining a Basic TFJob for TensorFlow Training in Kubernetes\nDESCRIPTION: This YAML snippet defines a basic TFJob custom resource for running a distributed TensorFlow training job. It specifies parameter server and worker replicas, along with their configurations including container image, command, and restart policy.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/tensorflow.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napiVersion: kubeflow.org/v1\nkind: TFJob\nmetadata:\n  generateName: tfjob\n  namespace: your-user-namespace\nspec:\n  tfReplicaSpecs:\n    PS:\n      replicas: 1\n      restartPolicy: OnFailure\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n        spec:\n          containers:\n            - name: tensorflow\n              image: gcr.io/your-project/your-image\n              command:\n                - python\n                - -m\n                - trainer.task\n                - --batch_size=32\n                - --training_steps=1000\n    Worker:\n      replicas: 3\n      restartPolicy: OnFailure\n      template:\n        metadata:\n          annotations:\n            sidecar.istio.io/inject: \"false\"\n        spec:\n          containers:\n            - name: tensorflow\n              image: gcr.io/your-project/your-image\n              command:\n                - python\n                - -m\n                - trainer.task\n                - --batch_size=32\n                - --training_steps=1000\n```\n\n----------------------------------------\n\nTITLE: Displaying Kubeflow 0.7 Release Information Table in HTML\nDESCRIPTION: HTML table structure displaying key information about Kubeflow 0.7.0 release including release date, media links, manifest locations, and release team details. Uses responsive table design with Bootstrap classes.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/releases/kubeflow-0.7.md#2025-04-10_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"table-responsive\">\n<table class=\"table table-bordered\">\n  <tbody>\n    <tr>\n      <th class=\"table-light\">Release Date</th>\n      <td>\n        2019-10-17\n      </td>\n    </tr>\n    <tr>\n      <th class=\"table-light\">Media</th>\n      <td>\n        <b>Blog:</b> \n          <a href=\"https://medium.com/kubeflow/kubeflow-v0-7-delivers-beta-functionality-in-the-leadup-to-v1-0-1e63036c07b8\">Kubeflow 0.7 Release Announcement</a>\n        <br>\n        <b>Roadmap:</b>\n          <a href=\"https://github.com/kubeflow/kubeflow/blob/master/ROADMAP.md#kubeflow-07\">Kubeflow 0.7 Features</a>\n      </td>\n    </tr>\n    <tr>\n      <th class=\"table-light\">Manifests</th>\n      <td>\n        <b>Release:</b> \n          <a href=\"https://github.com/kubeflow/manifests/releases/tag/v0.7.0-rc.2\">v0.7.0-rc.2</a>\n        <br>\n        <b>Branch:</b>\n          <a href=\"https://github.com/kubeflow/manifests/tree/v0.7-branch\">v0.7-branch</a>\n      </td>\n    </tr>\n    <tr>\n      <th class=\"table-light\">Release Team</th>\n      <td>\n        <b>Lead:</b> Richard Liu (<a href=\"https://github.com/richardsliu\">@richardsliu</a>)\n      </td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Implementing CSV File Download and Merge Function in Python\nDESCRIPTION: A Python function that downloads a tar.gz file containing CSV files from a URL, extracts them, and merges them into a single CSV file. Uses urllib for downloading, tarfile for extraction, and pandas for CSV operations.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/build-pipeline.ipynb#2025-04-10_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport glob\nimport pandas as pd\nimport tarfile\nimport urllib.request\n    \ndef download_and_merge_csv(url: str, output_csv: str):\n  with urllib.request.urlopen(url) as res:\n    tarfile.open(fileobj=res, mode=\"r|gz\").extractall('data')\n  df = pd.concat(\n      [pd.read_csv(csv_file, header=None) \n       for csv_file in glob.glob('data/*.csv')])\n  df.to_csv(output_csv, index=False, header=False)\n```\n\n----------------------------------------\n\nTITLE: Defining NotebookStatus in YAML\nDESCRIPTION: YAML structure for NotebookStatus, representing the observed state of a Notebook. It includes conditions, readyReplicas, and containerState fields.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/notebooks/api-reference/notebook-v1.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nstatus:\n  conditions:\n    # Array of NotebookCondition\n  readyReplicas: 0\n  containerState:\n    # Kubernetes core/v1.ContainerState\n```\n\n----------------------------------------\n\nTITLE: Fetching TFJob Status with kubectl\nDESCRIPTION: This bash command shows how to retrieve the detailed status of a TFJob named 'tfjob-simple' in the 'kubeflow' namespace using kubectl. The output is formatted as YAML.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/tensorflow.md#2025-04-10_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nkubectl -n kubeflow get -o yaml tfjobs tfjob-simple\n```\n\n----------------------------------------\n\nTITLE: HTML Table Structure for Kubeflow 0.6.1 Release Information\nDESCRIPTION: HTML table containing release information for Kubeflow 0.6.1, including release date, media links, manifests, and release team details.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/releases/kubeflow-0.6.md#2025-04-10_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"table-responsive\">\n<table class=\"table table-bordered\">\n  <tbody>\n    <tr>\n      <th class=\"table-light\">Release Date</th>\n      <td>\n        2019-07-31\n      </td>\n    </tr>\n    <tr>\n      <th class=\"table-light\">Media</th>\n      <td>\n        <b>Blog:</b> \n          <a href=\"https://medium.com/kubeflow/kubeflow-v0-6-a-robust-foundation-for-artifact-tracking-data-versioning-multi-user-support-9896d329412c\">Kubeflow 0.6 Release Announcement</a>\n        <br>\n        <b>Video:</b> \n          <a href=\"https://www.youtube.com/watch?v=fiFk5FB7il8\">Kubeflow 0.6 Release Feature Review</a>\n        <br>\n        <b>Roadmap:</b>\n          <a href=\"https://github.com/kubeflow/kubeflow/blob/master/ROADMAP.md#kubeflow-06\">Kubeflow 0.6 Features</a>\n      </td>\n    </tr>\n    <tr>\n      <th class=\"table-light\">Manifests</th>\n      <td>\n        <b>Release:</b> \n          <a href=\"https://github.com/kubeflow/manifests/releases/tag/v0.6.1\">v0.6.1</a>\n        <br>\n        <b>Branch:</b>\n          <a href=\"https://github.com/kubeflow/manifests/tree/v0.6-branch\">v0.6-branch</a>\n      </td>\n    </tr>\n    <tr>\n      <th class=\"table-light\">Release Team</th>\n      <td>\n        <b>Lead:</b> Zhenghui Wang (<a href=\"https://github.com/zhenghuiwang\">@zhenghuiwang</a>)\n      </td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Testing CSV Download Function with Sample Data\nDESCRIPTION: Example usage of the download_and_merge_csv function with a specific Google Cloud Storage URL containing iris dataset files.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/build-pipeline.ipynb#2025-04-10_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndownload_and_merge_csv(\n    url='https://storage.googleapis.com/ml-pipeline-playground/iris-csv-files.tar.gz', \n    output_csv='merged_data.csv')\n```\n\n----------------------------------------\n\nTITLE: Hugo Front Matter Configuration in Markdown\nDESCRIPTION: Front matter configuration for a Hugo documentation page, specifying the title, description and weight for page ordering.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/explanation/_index.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n+++\ntitle = \"Explanation\"\ndescription = \"Explanation for Training Operator Features\"\nweight = 60\n+++\n```\n\n----------------------------------------\n\nTITLE: Creating a Pipeline with Environment Variables\nDESCRIPTION: This code defines a Kubeflow pipeline that uses the component created earlier. It creates a Kubernetes V1EnvVar object to define the environment variable and adds it to the container operation using the add_env_variable() method.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/enviroment_variables.md#2025-04-10_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport kfp.dsl as dsl\nfrom kubernetes.client.models import V1EnvVar\n\n@dsl.pipeline(\n  name='Env example',\n  description='A pipeline showing how to use environment variables'\n)\ndef environment_pipeline():\n  env_var = V1EnvVar(name='example_env', value='env_variable')\n  #Returns a dsl.ContainerOp class instance. \n  container_op = logg_env_function_op().add_env_variable(env_var) \n```\n\n----------------------------------------\n\nTITLE: Viewing TFJob Events with kubectl in Kubeflow\nDESCRIPTION: Command to view recent events for a TensorFlow job in Kubeflow. This helps monitor job execution status, pod creation/deletion, and other important events.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/tensorflow.md#2025-04-10_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nkubectl -n kubeflow describe tfjobs tfjob-simple\n```\n\n----------------------------------------\n\nTITLE: Event Details Table Structure in HTML\nDESCRIPTION: HTML table structure displaying key event information including date, time, location, cost and registration buttons\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/events/past-events/2023/kubeflow-summit-2023.md#2025-04-10_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<div class=\"table-responsive\">\n  <table class=\"table table-bordered\">\n    <tr class=\"thead-light\">\n      <th>\n        Date\n      </th>\n      <td>\n        October 6th, 2023\n      </td>\n    </tr>\n    <tr class=\"thead-light\">\n      <th>\n        Time\n      </th>\n      <td>\n        7:30 AM - 5:30 PM CDT\n      </td>\n    </tr>\n    <tr class=\"thead-light\">\n      <th>\n        Location\n      </th>\n      <td>\n        <a href=\"https://maps.app.goo.gl/Xnf4Y1ffVLRiPNGR9\">Irving Convention Center at Las Colinas, Irving, TX, USA</a>\n      </td>\n    </tr>\n    <tr class=\"thead-light\">\n      <th>\n        Cost\n      </th>\n      <td>\n        <strong>FREE</strong>, but registration is required.\n      </td>\n    </tr>\n    <tr class=\"thead-light\">\n      <th>\n        Registration\n      </th>\n      <td>\n        <a href=\"https://www.eventbrite.com/e/kubeflow-summit-2023-virtual-registration-tickets-726298186427\">\n          <button class=\"btn btn-warning py-2 px-3 mx-3 my-3\">Register to Attend<br>(VIRTUAL)</button>\n        </a>\n        <a href=\"https://www.eventbrite.com/e/kubeflow-summit-2023-in-person-registration-tickets-726236511957\">\n          <button class=\"btn btn-warning py-2 px-3 mx-3 my-3\">Register to Attend<br>(IN-PERSON)</button>\n        </a>\n      </td>\n    </tr>\n  </table>\n</div>\n```\n\n----------------------------------------\n\nTITLE: TOML Frontmatter Configuration for Reference Documentation\nDESCRIPTION: TOML-formatted frontmatter that configures the metadata for a reference documentation page. Sets the page title, description and navigation weight for the Model Registry documentation.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/model-registry/reference/_index.md#2025-04-10_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n+++\ntitle = \"Reference\"\ndescription = \"Reference docs for Kubeflow Model Registry\"\nweight = 100\n+++\n```\n\n----------------------------------------\n\nTITLE: Defining a Python Function that Accesses Environment Variables\nDESCRIPTION: This code defines a lightweight Python function that retrieves an environment variable named 'example_env' and logs its value. The function uses the os.getenv() method to access the environment variable.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/enviroment_variables.md#2025-04-10_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef logg_env_function():\n  import os\n  import logging\n  logging.basicConfig(level=logging.INFO)\n  env_variable = os.getenv('example_env')\n  logging.info('The environment variable is: {}'.format(env_variable))\n```\n\n----------------------------------------\n\nTITLE: TFJob Pod Naming Format in Kubeflow\nDESCRIPTION: Pattern for TensorFlow job pod names in Kubeflow. This naming format is used when identifying specific pods for log retrieval or troubleshooting.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/tensorflow.md#2025-04-10_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n${JOBNAME}-${REPLICA-TYPE}-${INDEX}\n```\n\n----------------------------------------\n\nTITLE: Defining Metadata for Kubeflow 2023 Events Page in Markdown\nDESCRIPTION: This code snippet defines the title and description for a webpage or documentation section about Kubeflow events in 2023. It uses the TOML format within Markdown frontmatter delimiters.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/events/past-events/2023/_index.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n+++\ntitle = \"2023\"\ndescription = \"Events from 2023\"\n+++\n```\n\n----------------------------------------\n\nTITLE: Defining Hugo Front Matter for Katib Documentation\nDESCRIPTION: TOML-formatted front matter block that defines metadata for a Hugo documentation page about Kubeflow Katib. Specifies the title, description and navigation weight of the page.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/_index.md#2025-04-10_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n+++\ntitle = \"Katib\"\ndescription = \"Documentation for Kubeflow Katib\"\nweight = 70\n+++\n```\n\n----------------------------------------\n\nTITLE: Defining Component Inputs and Outputs in YAML\nDESCRIPTION: YAML configuration for specifying component inputs and outputs, including name, type, description, and default values.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/sdk/component-development.md#2025-04-10_snippet_6\n\nLANGUAGE: yaml\nCODE:\n```\ninputs:\n- {name: Input 1, type: String, description: 'Data for input 1'}\n- {name: Parameter 1, type: Integer, default: '100', description: 'Number of lines to copy'}\n\noutputs:\n- {name: Output 1, type: String, description: 'Output 1 data.'}\n```\n\n----------------------------------------\n\nTITLE: Stackdriver Query for TFJob Logs on GKE\nDESCRIPTION: Query string for filtering TensorFlow job logs in Google Kubernetes Engine (GKE) using Stackdriver. This helps aggregate and analyze logs across distributed training jobs.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/tensorflow.md#2025-04-10_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\nresource.type=\"k8s_container\"\nresource.labels.cluster_name=\"${CLUSTER}\"\nmetadata.userLabels.job-name=\"${JOB_NAME}\"\nmetadata.userLabels.replica-type=\"${TYPE}\"\nmetadata.userLabels.replica-index=\"${INDEX}\"\n```\n\n----------------------------------------\n\nTITLE: Defining Hugo Front Matter for External YouTube Content Link\nDESCRIPTION: A TOML-formatted Hugo front matter block that configures a link to the Kubeflow Summit 2024 YouTube playlist. It specifies a title for the link, the target URL, and a Font Awesome icon class to visually represent YouTube.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/events/past-events/2024/watch--kubeflow-summit-2024.md#2025-04-10_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n+++\ntitle = \"Watch: Kubeflow Summit 2024\"\n\nmanualLink = \"https://www.youtube.com/playlist?list=PLj6h78yzYM2Nk-8Zyjaefz9yFJ-NxC-qn\"\nicon = \"fa-brands fa-youtube\"\n+++\n```\n\n----------------------------------------\n\nTITLE: Defining Frontmatter in Hugo Markdown\nDESCRIPTION: Hugo frontmatter configuration defining the page title, description and weight for external add-ons documentation\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/external-add-ons/_index.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n+++\ntitle = \"External Add-Ons\"\ndescription = \"Externally developed projects that integrate with Kubeflow\"\nweight = 30\n+++\n```\n\n----------------------------------------\n\nTITLE: Checking Current Workflow Executor Configuration in Kubectl\nDESCRIPTION: Commands to verify the current workflow executor configuration in a Kubernetes cluster using kubectl.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/legacy-v1/installation/choose-executor.md#2025-04-10_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nkubectl config set-context --current --namespace <your-kfp-namespace>\n```\n\nLANGUAGE: bash\nCODE:\n```\nkubectl describe configmap workflow-controller-configmap | grep -A 2 containerRuntimeExecutor\n```\n\n----------------------------------------\n\nTITLE: Describing Pod Events in Kubernetes\nDESCRIPTION: This command describes a specific pod, including its events. It's useful for checking if the pod was successfully scheduled and started, and is most effective for pods less than 1 hour old.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/tensorflow.md#2025-04-10_snippet_14\n\nLANGUAGE: bash\nCODE:\n```\nkubectl -n ${USER_NAMESPACE} describe pods ${POD_NAME}\n```\n\n----------------------------------------\n\nTITLE: Displaying Alert Box in Markdown\nDESCRIPTION: This snippet shows how to create an alert box in Markdown to warn users about an old version of the documentation and provide migration instructions.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/reference/distributed-training.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n{{% alert title=\"Old Version\" color=\"warning\" %}}\nThis page is about **Kubeflow Training Operator V1**, for the latest information check\n[the Kubeflow Trainer V2 documentation](/docs/components/trainer).\n\nFollow [this guide for migrating to Kubeflow Trainer V2](/docs/components/trainer/operator-guides/migration).\n{{% /alert %}}\n```\n\n----------------------------------------\n\nTITLE: Defining Hugo Frontmatter for Data Handling Documentation\nDESCRIPTION: YAML frontmatter configuration for a Hugo documentation page about data handling in Kubeflow Pipelines. Sets the title, description, and weight for page ordering.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/data-handling/_index.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n+++\ntitle = \"Data Handling\"\ndescription = \"Learn how to handle data in Kubeflow Pipelines.\"\nweight = 4\n+++\n```\n\n----------------------------------------\n\nTITLE: Initializing S3 Dataset Parameters in Python\nDESCRIPTION: This snippet demonstrates how to create an S3DatasetParams object with necessary parameters for accessing an S3 bucket containing dataset files.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/llm-hp-optimization.md#2025-04-10_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom kubeflow.storage_initializer.s3 import S3DatasetParams\n\ns3_params = S3DatasetParams(\n    endpoint_url=\"https://s3.amazonaws.com\",\n    bucket_name=\"my-dataset-bucket\",\n    file_key=\"datasets/train.csv\",\n    region_name=\"us-west-2\",\n    access_key=\"YOUR_ACCESS_KEY\",\n    secret_key=\"YOUR_SECRET_KEY\"\n)\n```\n\n----------------------------------------\n\nTITLE: Defining RBAC ServiceAccount Reference for KServe Models Web App\nDESCRIPTION: Reference to the YAML configuration file that defines the ServiceAccount permissions for listing namespaces in standalone mode. This configuration is essential for the web app Pod to access namespace information via the Kubernetes API Server.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/external-add-ons/kserve/webapp.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nconfig/base/rbac.yaml\n```\n\n----------------------------------------\n\nTITLE: Defining Metadata for Kubeflow Training Operator User Guides Page in Hugo\nDESCRIPTION: This snippet defines the front matter for a Hugo webpage, specifying the title, description, and weight of the page in the site structure.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/_index.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n+++\ntitle = \"User Guides\"\ndescription = \"User guides for Training Operator\"\nweight = 40\n+++\n```\n\n----------------------------------------\n\nTITLE: Defining YAML Front Matter for Kubeflow Search Results Page\nDESCRIPTION: This YAML snippet defines the front matter for a search results page. It sets the page title to 'Search results' and specifies the layout to be used as 'search'.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/search.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n---\ntitle: Search results\nlayout: search\n---\n```\n\n----------------------------------------\n\nTITLE: Configuring HuggingFace Model Parameters\nDESCRIPTION: Creates a configuration for a HuggingFace model with a specific model URI, transformer type, access token, and number of labels. This example configures BERT for a binary classification task.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/llm-hp-optimization.md#2025-04-10_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom transformers import AutoModelForSequenceClassification\n\nfrom kubeflow.storage_initializer.hugging_face import HuggingFaceModelParams\n\n\nparams = HuggingFaceModelParams(\n    model_uri=\"bert-base-uncased\",\n    transformer_type=AutoModelForSequenceClassification,\n    access_token=\"huggingface_access_token\",\n    num_labels=2  # For binary classification\n)\n```\n\n----------------------------------------\n\nTITLE: Defining Frontmatter Metadata in TOML\nDESCRIPTION: TOML frontmatter configuration block defining metadata for a Feast documentation page. Specifies the title, description and weight for page ordering.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/external-add-ons/feast/_index.md#2025-04-10_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n+++\ntitle = \"Feast\"\ndescription = \"Feast | Feature Store\"\nweight = 20\n+++\n```\n\n----------------------------------------\n\nTITLE: Defining Frontmatter Metadata for Kubeflow Trainer Operator Documentation\nDESCRIPTION: This code snippet contains Hugo frontmatter metadata that defines the title, description, and weight of the documentation page for Kubeflow Trainer operators. The weight parameter (50) determines the page's position in the navigation hierarchy.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/operator-guides/_index.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n+++\ntitle = \"Operator Guides\"\ndescription = \"Documentation for cluster operators of Kubeflow Trainer\"\nweight = 50\n+++\n```\n\n----------------------------------------\n\nTITLE: HTML Table Structure for Kubeflow Working Groups\nDESCRIPTION: HTML table structure defining the mapping between Kubeflow Working Groups and their maintained components. The table includes responsive design classes and formatting for better readability.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/about/community.md#2025-04-10_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"table-responsive\">\n<table class=\"table table-bordered\">\n    <thead class=\"thead-light\">\n      <tr>\n        <th>Working Group</th>\n        <th>Maintained Components</th>\n      </tr>\n    </thead>\n  <tbody>\n      <!-- ======================= -->\n      <!-- AutoML Working Group -->\n      <!-- ======================= -->\n      <tr>\n        <td rowspan=\"1\" class=\"align-middle\">\n          <a href=\"https://github.com/kubeflow/community/tree/master/wg-automl\">AutoML</a>\n        </td>\n        <td>\n          <a href=\"https://github.com/kubeflow/katib\">Katib</a>\n        </td>\n      </tr>\n      <!-- ======================= -->\n      <!-- Manifests Working Group -->\n      <!-- ======================= -->\n      <tr>\n        <td rowspan=\"1\" class=\"align-middle\">\n          <a href=\"https://github.com/kubeflow/community/tree/master/wg-manifests\">Manifests</a>\n        </td>\n        <td>\n          <a href=\"https://github.com/kubeflow/manifests\">Manifests Repository</a>\n        </td>\n      </tr>\n      <!-- ======================= -->\n      <!-- Notebooks Working Group -->\n      <!-- ======================= -->\n      <tr>\n        <td rowspan=\"9\" class=\"align-middle\">\n          <a href=\"https://github.com/kubeflow/community/tree/master/wg-notebooks\">Notebooks</a>\n        </td>\n        <td>\n          <a href=\"https://github.com/kubeflow/kubeflow/tree/master/components/admission-webhook\">Admission Webhook (PodDefaults)</a>\n        </td>\n      </tr>\n      <tr>\n        <td>\n          <a href=\"https://github.com/kubeflow/kubeflow/tree/master/components/centraldashboard\">Central Dashboard</a>\n        </td>\n      </tr>\n      <tr>\n        <td>\n          <a href=\"https://github.com/kubeflow/kubeflow/tree/master/components/crud-web-apps/jupyter\">Jupyter Web App</a>\n        </td>\n      </tr>\n      <tr>\n        <td>\n          <a href=\"https://github.com/kubeflow/kubeflow/tree/master/components/access-management\">Kubeflow Access Management API (KFAM)</a>\n        </td>\n      </tr>\n      <tr>\n        <td>\n          <a href=\"https://github.com/kubeflow/kubeflow/tree/master/components/notebook-controller\">Notebook Controller</a>\n        </td>\n      </tr>\n      <tr>\n        <td>\n          <a href=\"https://github.com/kubeflow/kubeflow/tree/master/components/profile-controller\">Profile Controller</a>\n        </td>\n      </tr>\n      <tr>\n        <td>\n          <a href=\"https://github.com/kubeflow/kubeflow/tree/master/components/tensorboard-controller\">Tensorboard Controller</a>\n        </td>\n      </tr>\n      <tr>\n        <td>\n          <a href=\"https://github.com/kubeflow/kubeflow/tree/master/components/crud-web-apps/tensorboards\">Tensorboard Web App</a>\n        </td>\n      </tr>\n      <tr>\n        <td>\n          <a href=\"https://github.com/kubeflow/kubeflow/tree/master/components/crud-web-apps/volumes\">Volumes Web App</a>\n        </td>\n      </tr>\n      <!-- ======================= -->\n      <!-- Pipelines Working Group -->\n      <!-- ======================= -->\n      <tr>\n        <td rowspan=\"2\" class=\"align-middle\">\n          <a href=\"https://github.com/kubeflow/community/tree/master/wg-pipelines\">Pipelines</a>\n        </td>\n        <td>\n          <a href=\"https://github.com/kubeflow/pipelines\">Kubeflow Pipelines</a>\n        </td>\n      </tr>\n      <tr>\n        <td>\n          <a href=\"https://github.com/kubeflow/kfp-tekton\">Kubeflow Pipelines on Tekton</a>\n        </td>\n      </tr>\n      <!-- ======================= -->\n      <!-- Serving Working Group -->\n      <!-- ======================= -->\n      <tr>\n        <td rowspan=\"1\" class=\"align-middle\">\n          <a href=\"https://github.com/kubeflow/community/tree/master/wg-serving\">Serving</a>\n        </td>\n        <td>\n          <a href=\"https://github.com/kserve/kserve\">KServe (formerly KFServing)</a>\n        </td>\n      </tr>\n      <!-- ======================= -->\n      <!-- Training Working Group -->\n      <!-- ======================= -->\n      <tr>\n        <td rowspan=\"1\" class=\"align-middle\">\n          <a href=\"https://github.com/kubeflow/community/tree/master/wg-training\">Training</a>\n        </td>\n        <td>\n          <a href=\"https://github.com/kubeflow/trainer\">Training Operator</a>\n        </td>\n      </tr>\n  </tbody>\n</table>\n</div>\n```\n\n----------------------------------------\n\nTITLE: ENAS GetSuggestion() Output - Architecture (JSON)\nDESCRIPTION: Example output of the GetSuggestion() method from the ENAS algorithm service, showing the architecture definition as a JSON string.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/reference/nas-algorithms.md#2025-04-10_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n[[27], [29, 0], [22, 1, 0], [13, 0, 0, 0], [26, 1, 1, 0, 0], [30, 1, 0, 1, 0, 0], [11, 0, 1, 1, 0, 1, 1], [9, 1, 0, 0, 1, 0, 0, 0]]\n```\n\n----------------------------------------\n\nTITLE: Creating OWNERS File for Code Review in Kubeflow\nDESCRIPTION: Example of a typical OWNERS file in YAML format used in Kubeflow repositories. This file designates approvers and reviewers responsible for code review in a specific directory. Comments can be added after usernames for additional context.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/about/contributing.md#2025-04-10_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\napprovers:\n  - alice\n  - bob # this is a comment\nreviewers:\n  - alice\n  - carol # this is another comment\n  - sig-foo # this is an alias\n```\n\n----------------------------------------\n\nTITLE: Defining Front Matter in Hugo Markdown for Kubeflow Components Page\nDESCRIPTION: This snippet contains Hugo front matter (enclosed with +++ delimiters) that defines metadata for a webpage about Kubeflow components. It sets the title, description, and weight value which determines the page's position in navigation.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/_index.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n+++\ntitle = \"Components\"\ndescription = \"Logical components that make up Kubeflow\"\nweight = 30\n+++\n```\n\n----------------------------------------\n\nTITLE: HTML Table Structure for Kubeflow 0.6.0 Release Information\nDESCRIPTION: HTML table containing release information for Kubeflow 0.6.0, including release date, media links, manifests, and release team details.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/releases/kubeflow-0.6.md#2025-04-10_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"table-responsive\">\n<table class=\"table table-bordered\">\n  <tbody>\n    <tr>\n      <th class=\"table-light\">Release Date</th>\n      <td>\n        2019-07-19\n      </td>\n    </tr>\n    <tr>\n      <th class=\"table-light\">Media</th>\n      <td>\n        <b>Blog:</b> \n          <a href=\"https://medium.com/kubeflow/kubeflow-v0-6-a-robust-foundation-for-artifact-tracking-data-versioning-multi-user-support-9896d329412c\">Kubeflow 0.6 Release Announcement</a>\n        <br>\n        <b>Video:</b> \n          <a href=\"https://www.youtube.com/watch?v=fiFk5FB7il8\">Kubeflow 0.6 Release Feature Review</a>\n        <br>\n        <b>Roadmap:</b>\n          <a href=\"https://github.com/kubeflow/kubeflow/blob/master/ROADMAP.md#kubeflow-06\">Kubeflow 0.6 Features</a>\n      </td>\n    </tr>\n    <tr>\n      <th class=\"table-light\">Manifests</th>\n      <td>\n        <b>Release:</b> \n          <a href=\"https://github.com/kubeflow/manifests/releases/tag/v0.6.0\">v0.6.0</a>\n        <br>\n        <b>Branch:</b>\n          <a href=\"https://github.com/kubeflow/manifests/tree/v0.6-branch\">v0.6-branch</a>\n      </td>\n    </tr>\n    <tr>\n      <th class=\"table-light\">Release Team</th>\n      <td>\n        <b>Lead:</b> Kunming Qu (<a href=\"https://github.com/kunmingg\">@kunmingg</a>)\n      </td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Configuring Grid Search Algorithm in Katib\nDESCRIPTION: This snippet shows the configuration options for the Grid Search algorithm in Katib. It uses the Optuna optimization framework and supports setting a random state for reproducible results.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/katib/user-guides/hp-tuning/configure-algorithm.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n<div class=\"table-responsive\">\n  <table class=\"table table-bordered\">\n    <thead class=\"thead-light\">\n      <tr>\n        <th>Setting name</th>\n        <th>Description</th>\n        <th>Example</th>\n      </tr>\n    </thead>\n    <tbody>\n      <tr>\n        <td>random_state</td>\n        <td>[int]: Set <code>random_state</code> to something other than None\n          for reproducible results.</td>\n        <td>10</td>\n      </tr>\n    </tbody>\n  </table>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Creating OWNERS_ALIASES File for Kubeflow Repositories\nDESCRIPTION: Example of an OWNERS_ALIASES file used to define groups of users that can be referenced in OWNERS files. This allows for easier management of reviewer and approver groups across the codebase.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/about/contributing.md#2025-04-10_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\naliases:\n  sig-foo:\n    - david\n    - erin\n  sig-bar:\n    - bob\n    - frank\n```\n\n----------------------------------------\n\nTITLE: Defining Hugo Front Matter for Kubeflow Releases Page\nDESCRIPTION: This code snippet defines the front matter for a Hugo-generated webpage about Kubeflow releases. It specifies the page title, description, and weight for ordering in the site structure.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/releases/_index.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n+++\ntitle = \"Releases\"\ndescription = \"Information about past and future Kubeflow releases\"\nweight = 100\n+++\n```\n\n----------------------------------------\n\nTITLE: HTML Table Structure for Kubeflow 1.1 Release Information\nDESCRIPTION: HTML markup defining a responsive table that displays key information about the Kubeflow 1.1 release, including release date, media resources, manifest links, and release team details.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/releases/kubeflow-1.1.md#2025-04-10_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"table-responsive\">\n<table class=\"table table-bordered\">\n  <tbody>\n    <tr>\n      <th class=\"table-light\">Release Date</th>\n      <td>\n        2020-07-31\n      </td>\n    </tr>\n    <tr>\n      <th class=\"table-light\">Media</th>\n      <td>\n        <b>Blog:</b> \n          <a href=\"https://blog.kubeflow.org/release/official/2020/07/31/kubeflow-1.1-blog-post.html\">Kubeflow 1.1 Release Announcement</a>\n        <br>\n        <b>Video:</b> \n          <a href=\"https://www.youtube.com/watch?v=kd-mWl1cq48\">Kubeflow 1.1 Community Release Update</a>\n        <br>\n        <b>Roadmap:</b>\n          <a href=\"https://github.com/kubeflow/kubeflow/blob/master/ROADMAP.md#kubeflow-11-features-release-date-late-june-2020\">Kubeflow 1.1 Features</a>\n      </td>\n    </tr>\n    <tr>\n      <th class=\"table-light\">Manifests</th>\n      <td>\n        <b>Release:</b> \n          <a href=\"https://github.com/kubeflow/manifests/releases/tag/v1.1.0\">v1.1.0</a>\n        <br>\n        <b>Branch:</b>\n          <a href=\"https://github.com/kubeflow/manifests/tree/v1.1-branch\">v1.1-branch</a>\n      </td>\n    </tr>\n    <tr>\n      <th class=\"table-light\">Release Team</th>\n      <td>\n        <b>Lead:</b> Jeremy Lewi (<a href=\"https://github.com/jlewi\">@jlewi</a>)\n      </td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Adding Upstream Remote Repository in Git\nDESCRIPTION: Command to add the original Kubeflow website repository as an upstream remote to your local Git repository. This allows you to fetch updates from the main project.\nSOURCE: https://github.com/kubeflow/website/blob/master/quick-github-guide.md#2025-04-10_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit remote add upstream https://github.com/kubeflow/website.git\n```\n\n----------------------------------------\n\nTITLE: Defining Metadata for Kubeflow Getting Started Page\nDESCRIPTION: This code snippet defines the metadata for a web page about getting started with Kubeflow. It specifies the title, description, and weight of the page using the TOML format commonly used in static site generators.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/started/_index.md#2025-04-10_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\n+++\ntitle = \"Getting Started\"\ndescription = \"How to get started with Kubeflow\"\nweight = 20\n+++\n```\n\n----------------------------------------\n\nTITLE: Rendering HTML Table of Kubeflow Component Versions\nDESCRIPTION: HTML table structure for displaying Kubeflow component versions. The table includes three columns: Maintainers, Component Name, and Version. It is organized by working groups with proper styling and GitHub links to component repositories.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/releases/kubeflow-1.3.md#2025-04-10_snippet_0\n\nLANGUAGE: HTML\nCODE:\n```\n<div class=\"table-responsive\">\n<table class=\"table table-bordered\">\n    <thead class=\"thead-light\">\n      <tr>\n        <th>Maintainers</th>\n        <th>Component Name</th>\n        <th>Version</th>\n      </tr>\n    </thead>\n  <tbody>\n      <!-- ======================= -->\n      <!-- AutoML Working Group -->\n      <!-- ======================= -->\n      <tr>\n        <td rowspan=\"1\">AutoML Working Group</td>\n        <td>Katib</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/katib/releases/tag/v0.11.0\">v0.11.0</a>\n        </td>\n      </tr>\n      <!-- ======================= -->\n      <!-- Notebooks Working Group -->\n      <!-- ======================= -->\n      <tr>\n        <td rowspan=\"9\" class=\"align-middle\">Notebooks Working Group</td>\n        <td>Admission Webhook (PodDefaults)</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/kubeflow/tree/v1.3.0-rc.1/components/admission-webhook\">v1.3.0-rc.1</a>\n        </td>\n      </tr>\n      <tr>\n        <td>Central Dashboard</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/kubeflow/tree/v1.3.0-rc.1/components/centraldashboard\">v1.3.0-rc.1</a>\n        </td>\n      </tr>\n      <tr>\n        <td>Jupyter Web App</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/kubeflow/tree/v1.3.0-rc.1/components/crud-web-apps/jupyter\">v1.3.0-rc.1</a>\n        </td>\n      </tr>\n      <tr>\n        <td>Kubeflow Access Management API</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/kubeflow/tree/v1.3.0-rc.1/components/access-management\">v1.3.0-rc.1</a>\n        </td>\n      </tr>\n      <tr>\n        <td>Notebook Controller</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/kubeflow/tree/v1.3.0-rc.1/components/notebook-controller\">v1.3.0-rc.1</a>\n        </td>\n      </tr>\n      <tr>\n        <td>Profile Controller</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/kubeflow/tree/v1.3.0-rc.1/components/profile-controller\">v1.3.0-rc.1</a>\n        </td>\n      </tr>\n      <tr>\n        <td>Tensorboard Controller</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/kubeflow/tree/v1.3.0-rc.1/components/notebook-controller\">v1.3.0-rc.1</a>\n        </td>\n      </tr>\n      <tr>\n        <td>Tensorboard Web App</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/kubeflow/tree/v1.3.0-rc.1/components/crud-web-apps/volumes\">v1.3.0-rc.1</a>\n        </td>\n      </tr>\n      <tr>\n        <td>Volumes Web App</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/kubeflow/tree/v1.3.0-rc.1/components/crud-web-apps/tensorboards\">v1.3.0-rc.1</a>\n        </td>\n      </tr>\n      <!-- ======================= -->\n      <!-- Pipelines Working Group -->\n      <!-- ======================= -->\n      <tr>\n        <td rowspan=\"2\" class=\"align-middle\">Pipelines Working Group</td>\n        <td>Kubeflow Pipelines</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/pipelines/releases/tag/1.5.0\">v1.5.0</a>\n        </td>\n      </tr>\n      <tr>\n        <td>Kubeflow Pipelines Tekton</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/kfp-tekton/releases/tag/v0.8.0-rc0\">v0.8.0-rc0</a>\n        </td>\n      </tr>\n      <!-- ======================= -->\n      <!-- Serving Working Group -->\n      <!-- ======================= -->\n      <tr>\n        <td rowspan=\"1\" class=\"align-middle\">Serving Working Group</td>\n        <td>KFServing</td>\n        <td>\n          <a href=\"https://github.com/kserve/kserve/releases/tag/v0.5.0\">v0.5.0</a>\n        </td>\n      </tr>\n      <!-- ======================= -->\n      <!-- Training Working Group -->\n      <!-- ======================= -->\n      <tr>\n        <td rowspan=\"5\" class=\"align-middle\">Training Working Group</td>\n        <td>MPI Operator</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/mpi-operator/releases/tag/v0.2.3\">v0.2.3</a>\n        </td>\n      </tr>\n      <tr>\n        <td>MXNet Operator</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/mxnet-operator/releases/tag/v1.1.0\">v1.1.0</a>\n        </td>\n      </tr>\n      <tr>\n        <td>PyTorch Operator</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/pytorch-operator/releases/tag/v0.7.0\">v0.7.0</a>\n        </td>\n      </tr>\n      <tr>\n        <td>Training Operator</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/training-operator/releases/tag/v1.1.0\">v1.1.0</a>\n        </td>\n      </tr>\n      <tr>\n        <td>XGBoost Operator</td>\n        <td>\n          <a href=\"https://github.com/kubeflow/xgboost-operator/releases/tag/v0.2.0\">v0.2.0</a>\n        </td>\n      </tr>\n  </tbody>\n</table>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Installing MPI Operator with Kustomize\nDESCRIPTION: Commands to install MPI operator using kustomize build tool\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/mpi.md#2025-04-10_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/kubeflow/mpi-operator\ncd mpi-operator\nkustomize build manifests/overlays/kubeflow | kubectl apply -f -\n```\n\n----------------------------------------\n\nTITLE: Updating Docsy Theme Version\nDESCRIPTION: Commands to update the Docsy theme submodule to a specific version tag.\nSOURCE: https://github.com/kubeflow/website/blob/master/README.md#2025-04-10_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ngit -C themes/docsy fetch --tags\ngit -C themes/docsy checkout tags/v0.6.0\n```\n\n----------------------------------------\n\nTITLE: Defining Hugo Frontmatter for KServe Documentation Page\nDESCRIPTION: This TOML-formatted Hugo frontmatter defines metadata for a web page about KServe. It specifies the page title, description highlighting KServe's serverless inferencing capabilities on Kubernetes, and a weight value of 10 for ordering in the navigation.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/external-add-ons/kserve/_index.md#2025-04-10_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n+++\ntitle = \"KServe\"\ndescription = \"KServe | Serverless Inferencing on Kubernetes\"\nweight = 10\n+++\n```\n\n----------------------------------------\n\nTITLE: Component Versions Table Structure\nDESCRIPTION: HTML table structure listing all Kubeflow components and their versions, organized by working groups including AutoML, Notebooks, Pipelines, Serving, and Training.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/releases/kubeflow-1.6.md#2025-04-10_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"table-responsive\">\n<table class=\"table table-bordered\">\n    <thead class=\"thead-light\">\n      <tr>\n        <th>Maintainers</th>\n        <th>Component Name</th>\n        <th>Version</th>\n      </tr>\n    </thead>\n    <!-- Table content... -->\n</table>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Customizing Storage Initializer and Trainer Images for Fine-Tuning\nDESCRIPTION: This snippet shows how to customize the container images used in the Kubeflow fine-tuning process. It demonstrates setting environment variables for the storage initializer and trainer transformer images before executing the train command.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/trainer/legacy-v1/user-guides/fine-tuning.md#2025-04-10_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n...\nos.environ['STORAGE_INITIALIZER_IMAGE'] = 'docker.io/<username>/<custom-storage-initiailizer_image>'\nos.environ['TRAINER_TRANSFORMER_IMAGE'] = 'docker.io/<username>/<custom-trainer_transformer_image>'\n\nTrainingClient().train(...)\n```\n\n----------------------------------------\n\nTITLE: Configuring Hugo Front Matter for Future Kubeflow Events Page\nDESCRIPTION: This Hugo front matter snippet defines the metadata for a web page about future Kubeflow events. It sets the page title, description, and weight for sorting in the site structure.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/events/upcoming-events/_index.md#2025-04-10_snippet_0\n\nLANGUAGE: hugo\nCODE:\n```\n+++\ntitle = \"Future Events\"\ndescription = \"Future Kubeflow events\"\nweight = 100\n+++\n```\n\n----------------------------------------\n\nTITLE: Defining Elyra Website Frontmatter in TOML\nDESCRIPTION: TOML frontmatter configuration for a website link page that directs users to the Elyra documentation. Includes title, description, weight for ordering, manual link URL, and icon specification.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/external-add-ons/elyra/website.md#2025-04-10_snippet_0\n\nLANGUAGE: toml\nCODE:\n```\n+++\ntitle = \"Elyra Website\"\ndescription = \"LINK | Elyra Documentation Website\"\nweight = 998\nmanualLink = \"https://elyra.readthedocs.io/\"\nicon = \"fa-solid fa-arrow-up-right-from-square\"\n+++\n```\n\n----------------------------------------\n\nTITLE: Table Structure - Release Information\nDESCRIPTION: HTML table structure showing key release information including release date, media links, manifests, and release team details.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/releases/kubeflow-1.9.md#2025-04-10_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"table-responsive\">\n<table class=\"table table-bordered\">\n  <tbody>\n    <tr>\n      <th class=\"table-light\">Release Date</th>\n      <td>\n        2024-07-22\n      </td>\n    </tr>\n    <!-- Additional table rows -->\n  </tbody>\n</table>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Component with Package Installation\nDESCRIPTION: Example showing how to specify required packages using packages_to_install parameter in the decorator.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/lightweight-python-components.md#2025-04-10_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n@dsl.component(packages_to_install=['numpy==1.21.6'])\ndef sin(val: float = 3.14) -> float:\n    return np.sin(val).item()\n```\n\n----------------------------------------\n\nTITLE: Configuring Hugo Front Matter for Kubeflow Summit 2025 External Link\nDESCRIPTION: This Hugo front matter configuration defines metadata for the Kubeflow Summit 2025 page. It sets the page title, defines a manual external link to the Linux Foundation event page, and specifies a Font Awesome icon to indicate an external link.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/events/upcoming-events/kubeflow-summit-2025.md#2025-04-10_snippet_0\n\nLANGUAGE: hugo\nCODE:\n```\n+++\ntitle = \"Kubeflow Summit 2025\"\n\nmanualLink = \"https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/co-located-events/kubeflow-summit/\"\nicon = \"fa-solid fa-arrow-up-right-from-square\"\n+++\n```\n\n----------------------------------------\n\nTITLE: Table Structure - Dependency Versions\nDESCRIPTION: HTML table structure listing validated dependency versions for the Kubeflow manifests repository, including Kubernetes, Istio, cert-manager, and other core dependencies.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/releases/kubeflow-1.9.md#2025-04-10_snippet_2\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"table-responsive\">\n<table class=\"table table-bordered\">\n    <thead class=\"thead-light\">\n      <tr>\n        <th>Dependency</th>\n        <th>Validated or Included Version(s)</th>\n        <th>Notes</th>\n      </tr>\n    </thead>\n  <tbody>\n      <!-- Dependency rows -->\n  </tbody>\n</table>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Specifying Task Dependencies with .after() Method in KFP v2\nDESCRIPTION: This example demonstrates how to explicitly set execution ordering between pipeline tasks even when they don't exchange data, using the .after() method to specify that b_sq_task should run after a_sq_task.\nSOURCE: https://github.com/kubeflow/website/blob/master/content/en/docs/components/pipelines/user-guides/components/compose-components-into-pipelines.md#2025-04-10_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n@dsl.pipeline\ndef pythagorean(a: float, b: float) -> float:\n    a_sq_task = square(x=a)\n    b_sq_task = square(x=b)\n    b_sq_task.after(a_sq_task)\n    ...\n```"
  }
]
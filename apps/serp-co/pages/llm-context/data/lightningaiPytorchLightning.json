[
  {
    "owner": "lightning-ai",
    "repo": "pytorch-lightning",
    "content": "TITLE: Production-Ready Classification Task Implementation\nDESCRIPTION: Shows a complete implementation of a classification task suitable for production use, with training, validation, testing, and prediction steps.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/lightning_module.rst#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom torchmetrics.functional import accuracy\n\n\nclass ClassificationTask(L.LightningModule):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        loss, acc = self._shared_eval_step(batch, batch_idx)\n        metrics = {\"val_acc\": acc, \"val_loss\": loss}\n        self.log_dict(metrics)\n        return metrics\n\n    def test_step(self, batch, batch_idx):\n        loss, acc = self._shared_eval_step(batch, batch_idx)\n        metrics = {\"test_acc\": acc, \"test_loss\": loss}\n        self.log_dict(metrics)\n        return metrics\n\n    def _shared_eval_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.model(x)\n        loss = F.cross_entropy(y_hat, y)\n        acc = accuracy(y_hat, y)\n        return loss, acc\n\n    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n        x, y = batch\n        y_hat = self.model(x)\n        return y_hat\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.model.parameters(), lr=0.02)\n```\n\n----------------------------------------\n\nTITLE: BERT Fine-tuning Implementation for MNLI Task\nDESCRIPTION: Demonstrates how to implement BERT fine-tuning for the MNLI (Multi-Genre Natural Language Inference) task using HuggingFace transformers and PyTorch Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/transfer_learning.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass BertMNLIFinetuner(LightningModule):\n    def __init__(self):\n        super().__init__()\n\n        self.bert = BertModel.from_pretrained(\"bert-base-cased\", output_attentions=True)\n        self.bert.train()\n        self.W = nn.Linear(bert.config.hidden_size, 3)\n        self.num_classes = 3\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        h, _, attn = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n\n        h_cls = h[:, 0]\n        logits = self.W(h_cls)\n        return logits, attn\n```\n\n----------------------------------------\n\nTITLE: PyTorch Lightning Training Loop Structure\nDESCRIPTION: Detailed pseudocode showing the complete training loop structure including fit, training, and validation loops with all hook points.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/lightning_module.rst#2025-04-23_snippet_21\n\nLANGUAGE: python\nCODE:\n```\ndef fit(self):\n    configure_callbacks()\n\n    if local_rank == 0:\n        prepare_data()\n\n    setup(\"fit\")\n    configure_model()\n    configure_optimizers()\n\n    on_fit_start()\n\n    # the sanity check runs here\n\n    on_train_start()\n    for epoch in epochs:\n        fit_loop()\n    on_train_end()\n\n    on_fit_end()\n    teardown(\"fit\")\n\n\ndef fit_loop():\n    torch.set_grad_enabled(True)\n\n    on_train_epoch_start()\n\n    for batch_idx, batch in enumerate(train_dataloader()):\n        on_train_batch_start()\n\n        on_before_batch_transfer()\n        transfer_batch_to_device()\n        on_after_batch_transfer()\n\n        out = training_step()\n\n        on_before_zero_grad()\n        optimizer_zero_grad()\n\n        on_before_backward()\n        backward()\n        on_after_backward()\n\n        on_before_optimizer_step()\n        configure_gradient_clipping()\n        optimizer_step()\n\n        on_train_batch_end(out, batch, batch_idx)\n\n        if should_check_val:\n            val_loop()\n\n    on_train_epoch_end()\n\n\ndef val_loop():\n    on_validation_model_eval()  # calls `model.eval()`\n    torch.set_grad_enabled(False)\n\n    on_validation_start()\n    on_validation_epoch_start()\n\n    for batch_idx, batch in enumerate(val_dataloader()):\n        on_validation_batch_start(batch, batch_idx)\n\n        batch = on_before_batch_transfer(batch)\n        batch = transfer_batch_to_device(batch)\n        batch = on_after_batch_transfer(batch)\n\n        out = validation_step(batch, batch_idx)\n\n        on_validation_batch_end(out, batch, batch_idx)\n\n    on_validation_epoch_end()\n    on_validation_end()\n\n    # set up for train\n    on_validation_model_train()  # calls `model.train()`\n    torch.set_grad_enabled(True)\n```\n\n----------------------------------------\n\nTITLE: Complete Training Example with Tensor Parallelism in Lightning Fabric\nDESCRIPTION: A full training example demonstrating tensor parallelism implementation with Lightning Fabric. It includes model definition, parallelization function, strategy setup, model initialization, optimizer configuration, data loading, and a simplified training loop with memory usage reporting.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/model_parallel/tp.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.distributed.tensor.parallel import ColwiseParallel, RowwiseParallel\nfrom torch.distributed.tensor.parallel import parallelize_module\n\nimport lightning as L\nfrom lightning.pytorch.demos.boring_classes import RandomDataset\nfrom lightning.fabric.strategies import ModelParallelStrategy\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim):\n        super().__init__()\n        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n\n    def forward(self, x):\n        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n\n\ndef parallelize_feedforward(model, device_mesh):\n    # Lightning will set up a device mesh for you\n    tp_mesh = device_mesh[\"tensor_parallel\"]\n    # Use PyTorch's distributed tensor APIs to parallelize the model\n    plan = {\n        \"w1\": ColwiseParallel(),\n        \"w2\": RowwiseParallel(),\n        \"w3\": ColwiseParallel(),\n    }\n    parallelize_module(model, tp_mesh, plan)\n    return model\n\n\nstrategy = ModelParallelStrategy(parallelize_fn=parallelize_feedforward)\nfabric = L.Fabric(accelerator=\"cuda\", devices=2, strategy=strategy)\nfabric.launch()\n\n# Initialize the model\nmodel = FeedForward(8192, 8192)\nmodel = fabric.setup(model)\n\n# Define the optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=3e-3)\noptimizer = fabric.setup_optimizers(optimizer)\n\n# Define dataset/dataloader\ndataset = RandomDataset(8192, 64)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=8)\ndataloader = fabric.setup_dataloaders(dataloader)\n\n# Simplified training loop\nfor i, batch in enumerate(dataloader):\n    output = model(batch)\n    loss = output.sum()\n    fabric.backward(loss)\n    optimizer.step()\n    optimizer.zero_grad()\n    fabric.print(f\"Iteration {i} complete\")\n\nfabric.print(f\"Peak memory usage: {torch.cuda.max_memory_allocated() / 1e9:.02f} GB\")\n```\n\n----------------------------------------\n\nTITLE: Loading PyTorch Lightning Checkpoint with Pure PyTorch\nDESCRIPTION: This snippet demonstrates how to load a PyTorch Lightning checkpoint and use it with a standard PyTorch model. It involves creating a PyTorch model, loading the checkpoint, and applying the state dict.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/deploy/production_intermediate.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\n\n\nclass MyModel(nn.Module):\n    ...\n\n\nmodel = MyModel()\ncheckpoint = torch.load(\"path/to/lightning/checkpoint.ckpt\")\nmodel.load_state_dict(checkpoint[\"state_dict\"])\nmodel.eval()\n```\n\n----------------------------------------\n\nTITLE: Complete PyTorch to Fabric Conversion Example\nDESCRIPTION: A comprehensive diff showing all changes needed to convert a PyTorch training script to use Fabric, including initialization, device setup, model/optimizer preparation, and backward pass.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/convert.rst#2025-04-23_snippet_6\n\nLANGUAGE: diff\nCODE:\n```\n  import torch\n  from lightning.pytorch.demos import WikiText2, Transformer\n+ import lightning as L\n\n- device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n+ fabric = L.Fabric(accelerator=\"cuda\", devices=8, strategy=\"ddp\")\n+ fabric.launch()\n\n  dataset = WikiText2()\n  dataloader = torch.utils.data.DataLoader(dataset)\n  model = Transformer(vocab_size=dataset.vocab_size)\n  optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n\n- model = model.to(device)\n+ model, optimizer = fabric.setup(model, optimizer)\n+ dataloader = fabric.setup_dataloaders(dataloader)\n\n  model.train()\n  for epoch in range(20):\n      for batch in dataloader:\n          input, target = batch\n-         input, target = input.to(device), target.to(device)\n          optimizer.zero_grad()\n          output = model(input, target)\n          loss = torch.nn.functional.nll_loss(output, target.view(-1))\n-         loss.backward()\n+         fabric.backward(loss)\n          optimizer.step()\n```\n\n----------------------------------------\n\nTITLE: Implementing a Basic LightningModule for Transformer Model\nDESCRIPTION: This code defines a LightningModule for a Transformer model, including initialization, forward pass, training step, and optimizer configuration.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/lightning_module.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\nimport torch\n\nfrom lightning.pytorch.demos import Transformer\n\n\nclass LightningTransformer(L.LightningModule):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.model = Transformer(vocab_size=vocab_size)\n\n    def forward(self, inputs, target):\n        return self.model(inputs, target)\n\n    def training_step(self, batch, batch_idx):\n        inputs, target = batch\n        output = self(inputs, target)\n        loss = torch.nn.functional.nll_loss(output, target.view(-1))\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.SGD(self.model.parameters(), lr=0.1)\n```\n\n----------------------------------------\n\nTITLE: Configuring Lightning Fabric for Different Hardware and Strategies in Python\nDESCRIPTION: This snippet shows various configurations for Lightning Fabric, including setup for different hardware (CPU, GPU, TPU), multi-GPU and multi-node training, and different distributed training strategies and precision options.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning_fabric/README.md#2025-04-23_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# Use your available hardware\n# no code changes needed\nfabric = Fabric()\n\n# Run on GPUs (CUDA or MPS)\nfabric = Fabric(accelerator=\"gpu\")\n\n# 8 GPUs\nfabric = Fabric(accelerator=\"gpu\", devices=8)\n\n# 256 GPUs, multi-node\nfabric = Fabric(accelerator=\"gpu\", devices=8, num_nodes=32)\n\n# Run on TPUs\nfabric = Fabric(accelerator=\"tpu\")\n\n# Use state-of-the-art distributed training techniques\nfabric = Fabric(strategy=\"ddp\")\nfabric = Fabric(strategy=\"deepspeed\")\nfabric = Fabric(strategy=\"fsdp\")\n\n# Switch the precision\nfabric = Fabric(precision=\"16-mixed\")\nfabric = Fabric(precision=\"64\")\n```\n\n----------------------------------------\n\nTITLE: Implementing TBPTT with PyTorch Lightning\nDESCRIPTION: Complete implementation of TBPTT using PyTorch Lightning framework. The code includes a custom dataset (AverageDataset) and a Lightning module (LitModel) with manual optimization. The implementation splits sequences along time dimension and performs backpropagation every k steps while maintaining hidden states between splits.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/tbptt.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nimport lightning as L\n\n\nclass AverageDataset(Dataset):\n    def __init__(self, dataset_len=300, sequence_len=100):\n        self.dataset_len = dataset_len\n        self.sequence_len = sequence_len\n        self.input_seq = torch.randn(dataset_len, sequence_len, 10)\n        top, bottom = self.input_seq.chunk(2, -1)\n        self.output_seq = top + bottom.roll(shifts=1, dims=-1)\n\n    def __len__(self):\n        return self.dataset_len\n\n    def __getitem__(self, item):\n        return self.input_seq[item], self.output_seq[item]\n\n\nclass LitModel(L.LightningModule):\n\n    def __init__(self):\n        super().__init__()\n\n        self.batch_size = 10\n        self.in_features = 10\n        self.out_features = 5\n        self.hidden_dim = 20\n\n        # 1. Switch to manual optimization\n        self.automatic_optimization = False\n        self.truncated_bptt_steps = 10\n\n        self.rnn = nn.LSTM(self.in_features, self.hidden_dim, batch_first=True)\n        self.linear_out = nn.Linear(in_features=self.hidden_dim, out_features=self.out_features)\n\n    def forward(self, x, hs):\n        seq, hs = self.rnn(x, hs)\n        return self.linear_out(seq), hs\n\n    # 2. Remove the `hiddens` argument\n    def training_step(self, batch, batch_idx):\n        # 3. Split the batch in chunks along the time dimension\n        x, y = batch\n        split_x, split_y = [\n            x.tensor_split(self.truncated_bptt_steps, dim=1),\n            y.tensor_split(self.truncated_bptt_steps, dim=1)\n        ]\n\n        hiddens = None\n        optimizer = self.optimizers()\n        losses = []\n\n        # 4. Perform the optimization in a loop\n        for x, y in zip(split_x, split_y):\n            y_pred, hiddens = self(x, hiddens)\n            loss = F.mse_loss(y_pred, y)\n\n            optimizer.zero_grad()\n            self.manual_backward(loss)\n            optimizer.step()\n\n            # 5. \"Truncate\"\n            hiddens = [h.detach() for h in hiddens]\n            losses.append(loss.detach())\n\n        avg_loss = sum(losses) / len(losses)\n        self.log(\"train_loss\", avg_loss, prog_bar=True)\n\n        # 6. Remove the return of `hiddens`\n        # Returning loss in manual optimization is not needed\n        return None\n\n    def configure_optimizers(self):\n        return optim.Adam(self.parameters(), lr=0.001)\n\n    def train_dataloader(self):\n        return DataLoader(AverageDataset(), batch_size=self.batch_size)\n\n\nif __name__ == \"__main__\":\n    model = LitModel()\n    trainer = L.Trainer(max_epochs=5)\n    trainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Creating Lightning Module for Autoencoder\nDESCRIPTION: Definition of LightningModule that combines Encoder and Decoder, implements the training step, and configures the optimizer with Adam and learning rate of 1e-3.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/model/train_model_basic.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass LitAutoEncoder(L.LightningModule):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def training_step(self, batch, batch_idx):\n        # training_step defines the train loop.\n        x, _ = batch\n        x = x.view(x.size(0), -1)\n        z = self.encoder(x)\n        x_hat = self.decoder(z)\n        loss = F.mse_loss(x_hat, x)\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n        return optimizer\n```\n\n----------------------------------------\n\nTITLE: Implementing CIFAR10 Training with Lightning Fabric in Python\nDESCRIPTION: This snippet demonstrates how to modify a standard PyTorch training loop for CIFAR10 to use Lightning Fabric. It shows the setup of the Fabric object, model, optimizer, and dataloader, as well as the training loop modifications.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning_fabric/README.md#2025-04-23_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport lightning as L\nimport torch; import torchvision as tv\n\ndataset = tv.datasets.CIFAR10(\"data\", download=True,\n                              train=True,\n                              transform=tv.transforms.ToTensor())\n\nfabric = L.Fabric()\nfabric.launch()\n\nmodel = tv.models.resnet18()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001)\nmodel, optimizer = fabric.setup(model, optimizer)\n\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=8)\ndataloader = fabric.setup_dataloaders(dataloader)\n\nmodel.train()\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        inputs, labels = batch\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = torch.nn.functional.cross_entropy(outputs, labels)\n        fabric.backward(loss)\n        optimizer.step()\n```\n\n----------------------------------------\n\nTITLE: Configuring Advanced Training Features with PyTorch Lightning Trainer\nDESCRIPTION: Demonstrates how to initialize a PyTorch Lightning Trainer with advanced features like DeepSpeed/FSDP for large model training, mixed precision, and multi-GPU support. Shows configuration for training parameters and integration of state-of-the-art techniques.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/model/build_model_intermediate.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# train 1T+ parameter models with DeepSpeed/FSDP\ntrainer = Trainer(\n    devices=4,\n    accelerator=\"gpu\",\n    strategy=\"deepspeed_stage_2\",\n    precision=\"16-mixed\",\n)\n\n# 20+ helpful arguments for rapid idea iteration\ntrainer = Trainer(\n    max_epochs=10,\n    min_epochs=5,\n    overfit_batches=1\n)\n\n# access the latest state of the art techniques\ntrainer = Trainer(callbacks=[StochasticWeightAveraging(...)])\n```\n\n----------------------------------------\n\nTITLE: Custom Training Loop Example\nDESCRIPTION: Example of customizing the training loop using LightningModule hooks.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/starter/introduction.rst#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass LitAutoEncoder(L.LightningModule):\n    def backward(self, loss):\n        loss.backward()\n```\n\n----------------------------------------\n\nTITLE: Implementing LightningModule with Essential Hooks for Organizing PyTorch Code\nDESCRIPTION: This snippet shows how to create a basic LightningModule that encapsulates model definition, training step with loss calculation, optimizer configuration, and dataloader setup. It demonstrates the fundamental hooks structure that helps organize deep learning code in a standardized format.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/lightning_module.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\n\n\nclass LitModel(L.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = ...\n\n    def training_step(self, batch, batch_idx):\n        # Main forward, loss computation, and metrics goes here\n        x, y = batch\n        y_hat = self.model(x)\n        loss = self.loss_fn(y, y_hat)\n        acc = self.accuracy(y, y_hat)\n        ...\n        return loss\n\n    def configure_optimizers(self):\n        # Return one or several optimizers\n        return torch.optim.Adam(self.parameters(), ...)\n\n    def train_dataloader(self):\n        # Return your dataloader for training\n        return DataLoader(...)\n\n    def on_train_start(self):\n        # Do something at the beginning of training\n        ...\n\n    def any_hook_you_like(self, *args, **kwargs):\n        ...\n```\n\n----------------------------------------\n\nTITLE: Defining a PyTorch Lightning Autoencoder Model\nDESCRIPTION: Example of defining a LightningModule for an autoencoder model, including the model architecture, training step, and optimizer configuration.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/README.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# main.py\n# ! pip install torchvision\nimport torch, torch.nn as nn, torch.utils.data as data, torchvision as tv, torch.nn.functional as F\nimport lightning as L\n\n# --------------------------------\n# Step 1: Define a LightningModule\n# --------------------------------\n# A LightningModule (nn.Module subclass) defines a full *system*\n# (ie: an LLM, diffusion model, autoencoder, or simple image classifier).\n\n\nclass LitAutoEncoder(L.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.Sequential(nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 3))\n        self.decoder = nn.Sequential(nn.Linear(3, 128), nn.ReLU(), nn.Linear(128, 28 * 28))\n\n    def forward(self, x):\n        # in lightning, forward defines the prediction/inference actions\n        embedding = self.encoder(x)\n        return embedding\n\n    def training_step(self, batch, batch_idx):\n        # training_step defines the train loop. It is independent of forward\n        x, _ = batch\n        x = x.view(x.size(0), -1)\n        z = self.encoder(x)\n        x_hat = self.decoder(z)\n        loss = F.mse_loss(x_hat, x)\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n        return optimizer\n\n\n# -------------------\n# Step 2: Define data\n# -------------------\ndataset = tv.datasets.MNIST(\".\", download=True, transform=tv.transforms.ToTensor())\ntrain, val = data.random_split(dataset, [55000, 5000])\n\n# -------------------\n# Step 3: Train\n# -------------------\nautoencoder = LitAutoEncoder()\ntrainer = L.Trainer()\ntrainer.fit(autoencoder, data.DataLoader(train), data.DataLoader(val))\n```\n\n----------------------------------------\n\nTITLE: Defining PyTorch Neural Network Modules\nDESCRIPTION: Implementation of Encoder and Decoder neural network modules using PyTorch's nn.Module. The Encoder compresses 28x28 images to 3 dimensions, while the Decoder reconstructs the original dimensions.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/model/train_model_basic.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Encoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\n\n    def forward(self, x):\n        return self.l1(x)\n\n\nclass Decoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.l1 = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n\n    def forward(self, x):\n        return self.l1(x)\n```\n\n----------------------------------------\n\nTITLE: Implementing Full Training, Validation, and Testing Pipeline with Fabric\nDESCRIPTION: Complete implementation structure for training, validation, and testing loops using Fabric, including periodic validation during training and final testing on unseen data.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/code_structure.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\n\n\ndef train(fabric, model, optimizer, train_dataloader, val_dataloader):\n    # Training loop with validation every few epochs\n    model.train()\n    for epoch in range(num_epochs):\n        for i, batch in enumerate(train_dataloader):\n            ...\n\n        if epoch % validate_every_n_epoch == 0:\n            validate(fabric, model, val_dataloader)\n\n\ndef validate(fabric, model, dataloader):\n    # Validation loop\n    model.eval()\n    for i, batch in enumerate(dataloader):\n        ...\n\n\ndef test(fabric, model, dataloader):\n    # Test/Prediction loop\n    model.eval()\n    for i, batch in enumerate(dataloader):\n        ...\n\n\ndef main():\n    ...\n\n    # Run training loop with validation\n    train(fabric, model, optimizer, train_dataloader, val_dataloader)\n\n    # Test on unseen data\n    test(fabric, model, test_dataloader)\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Loading LightningModule from Checkpoint in PyTorch Lightning\nDESCRIPTION: Shows how to load a LightningModule from a checkpoint file, including its weights and hyperparameters. It also demonstrates how to use the loaded model for predictions.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/checkpointing_basic.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel = MyLightningModule.load_from_checkpoint(\"/path/to/checkpoint.ckpt\")\n\n# disable randomness, dropout, etc...\nmodel.eval()\n\n# predict with the model\ny_hat = model(x)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Prediction Step in PyTorch Lightning\nDESCRIPTION: Shows how to implement a custom prediction step in a Lightning Module and different ways to use the predict functionality.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/lightning_module.rst#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass LightningTransformer(L.LightningModule):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.model = Transformer(vocab_size=vocab_size)\n\n    def predict_step(self, batch):\n        inputs, target = batch\n        return self.model(inputs, target)\n```\n\n----------------------------------------\n\nTITLE: Configuring GPU Training with PyTorch Lightning\nDESCRIPTION: Examples of setting up GPU training with different strategies including single GPU, DDP (Distributed Data Parallel), and multi-node training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/speed.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# run on 1 gpu\ntrainer = Trainer(accelerator=\"gpu\", devices=1)\n\n# train on 8 GPUs, using the DDP strategy\ntrainer = Trainer(accelerator=\"gpu\", devices=8, strategy=\"ddp\")\n\n# train on multiple GPUs across nodes (uses 8 GPUs in total)\ntrainer = Trainer(accelerator=\"gpu\", devices=2, num_nodes=4)\n```\n\n----------------------------------------\n\nTITLE: Complex LightningDataModule Implementation for MNIST\nDESCRIPTION: Presents a more comprehensive implementation of LightningDataModule for MNIST. This example includes data preparation, setup for different stages, and data transformation, showcasing the full potential of DataModules.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/data/datamodule.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\nfrom torch.utils.data import random_split, DataLoader\n\n# Note - you must have torchvision installed for this example\nfrom torchvision.datasets import MNIST\nfrom torchvision import transforms\n\n\nclass MNISTDataModule(L.LightningDataModule):\n    def __init__(self, data_dir: str = \"./\"):\n        super().__init__()\n        self.data_dir = data_dir\n        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n\n    def prepare_data(self):\n        # download\n        MNIST(self.data_dir, train=True, download=True)\n        MNIST(self.data_dir, train=False, download=True)\n\n    def setup(self, stage: str):\n        # Assign train/val datasets for use in dataloaders\n        if stage == \"fit\":\n            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n            self.mnist_train, self.mnist_val = random_split(\n                mnist_full, [55000, 5000], generator=torch.Generator().manual_seed(42)\n            )\n\n        # Assign test dataset for use in dataloader(s)\n        if stage == \"test\":\n            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n\n        if stage == \"predict\":\n            self.mnist_predict = MNIST(self.data_dir, train=False, transform=self.transform)\n\n    def train_dataloader(self):\n        return DataLoader(self.mnist_train, batch_size=32)\n\n    def val_dataloader(self):\n        return DataLoader(self.mnist_val, batch_size=32)\n\n    def test_dataloader(self):\n        return DataLoader(self.mnist_test, batch_size=32)\n\n    def predict_dataloader(self):\n        return DataLoader(self.mnist_predict, batch_size=32)\n```\n\n----------------------------------------\n\nTITLE: Manual Optimization Example in PyTorch Lightning\nDESCRIPTION: Demonstrates basic manual optimization with a single optimizer, showing how to perform gradient zeroing, backward pass and optimization steps manually.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/lightning_module.rst#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\nopt = self.optimizers(use_pl_optimizer=True)\n\nloss = ...\nopt.zero_grad()\nself.manual_backward(loss)\nopt.step()\n```\n\n----------------------------------------\n\nTITLE: Implementing Early Stopping in PyTorch Lightning\nDESCRIPTION: Example of adding early stopping functionality to PyTorch Lightning training. This monitors validation loss to stop training when performance plateaus.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/pytorch_lightning/README.md#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nes = EarlyStopping(monitor=\"val_loss\")\ntrainer = Trainer(callbacks=[es])\n```\n\n----------------------------------------\n\nTITLE: Implementing Epoch-End Operations in LightningModule\nDESCRIPTION: This snippet shows how to implement epoch-end operations in a LightningModule, collecting outputs from all training steps and processing them at the end of each epoch.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/lightning_module.rst#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass LightningTransformer(L.LightningModule):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.model = Transformer(vocab_size=vocab_size)\n        self.training_step_outputs = []\n\n    def training_step(self, batch, batch_idx):\n        inputs, target = batch\n        output = self.model(inputs, target)\n        loss = torch.nn.functional.nll_loss(output, target.view(-1))\n        preds = ...\n        self.training_step_outputs.append(preds)\n        return loss\n\n    def on_train_epoch_end(self):\n        all_preds = torch.stack(self.training_step_outputs)\n        # do something with all preds\n        ...\n        self.training_step_outputs.clear()  # free memory\n```\n\n----------------------------------------\n\nTITLE: Implementing Lightning Fabric for PyTorch Training\nDESCRIPTION: This snippet demonstrates how to integrate Lightning Fabric into a PyTorch training loop for the CIFAR10 dataset. It shows the necessary modifications to enable device-agnostic training and simplified distributed execution.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/README.md#2025-04-23_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\nimport lightning as L\nimport torch; import torchvision as tv\n\ndataset = tv.datasets.CIFAR10(\"data\", download=True,\n                              train=True,\n                              transform=tv.transforms.ToTensor())\n\nfabric = L.Fabric()\nfabric.launch()\n\nmodel = tv.models.resnet18()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001)\nmodel, optimizer = fabric.setup(model, optimizer)\n\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=8)\ndataloader = fabric.setup_dataloaders(dataloader)\n\nmodel.train()\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        inputs, labels = batch\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = torch.nn.functional.cross_entropy(outputs, labels)\n        fabric.backward(loss)\n        optimizer.step()\n        print(loss.data)\n```\n\n----------------------------------------\n\nTITLE: Configuring Training Strategies in PyTorch Lightning\nDESCRIPTION: Demonstrates how to set up different distributed training strategies using the strategy parameter in the Trainer, including data-parallel and model-parallel approaches.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_30\n\nLANGUAGE: python\nCODE:\n```\n# Data-parallel training with the DDP strategy on 4 GPUs\ntrainer = Trainer(strategy=\"ddp\", accelerator=\"gpu\", devices=4)\n\n# Model-parallel training with the FSDP strategy on 4 GPUs\ntrainer = Trainer(strategy=\"fsdp\", accelerator=\"gpu\", devices=4)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.strategies import DDPStrategy\n\ntrainer = Trainer(strategy=DDPStrategy(static_graph=True), accelerator=\"gpu\", devices=2)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Training a LightningModule in Python\nDESCRIPTION: This snippet demonstrates how to create a LightningModule instance and train it using a Trainer. It shows the basic structure without need for manual device management.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/lightning_module.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nnet = MyLightningModuleNet()\ntrainer = Trainer()\ntrainer.fit(net)\n```\n\n----------------------------------------\n\nTITLE: Implementing AutoEncoder with Lightning Module\nDESCRIPTION: Example implementation of an autoencoder using Lightning's LightningModule class, including encoder/decoder architecture, training step, and optimizer configuration.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/starter/introduction.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom torch import optim, nn, utils, Tensor\nfrom torchvision.datasets import MNIST\nfrom torchvision.transforms import ToTensor\nimport lightning as L\n\n# define any number of nn.Modules (or use your current ones)\nencoder = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\ndecoder = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n\n\n# define the LightningModule\nclass LitAutoEncoder(L.LightningModule):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def training_step(self, batch, batch_idx):\n        # training_step defines the train loop.\n        # it is independent of forward\n        x, _ = batch\n        x = x.view(x.size(0), -1)\n        z = self.encoder(x)\n        x_hat = self.decoder(z)\n        loss = nn.functional.mse_loss(x_hat, x)\n        # Logging to TensorBoard (if installed) by default\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n        return optimizer\n\n\n# init the autoencoder\nautoencoder = LitAutoEncoder(encoder, decoder)\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for PyTorch Lightning\nDESCRIPTION: Essential imports for working with PyTorch Lightning, including torch, torchvision, and lightning frameworks.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/model/train_model_basic.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\nfrom torch.utils.data import DataLoader\nimport lightning as L\n```\n\n----------------------------------------\n\nTITLE: Loading Checkpoints with Fabric\nDESCRIPTION: Load checkpoint contents and restore the state of objects in-place. This method handles distributed scenarios correctly and can be used to restore model and optimizer states during training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_methods.rst#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Define the state of your program/loop\nstate = {\n    \"model1\": model1,\n    \"model2\": model2,\n    \"optimizer\": optimizer,\n    \"iteration\": iteration,\n}\n\n# Restore the state of objects (in-place)\nfabric.load(\"path/to/checkpoint.ckpt\", state)\n\n# Or load everything and restore your objects manually\ncheckpoint = fabric.load(\"./checkpoints/version_2/checkpoint.ckpt\")\nmodel.load_state_dict(checkpoint[\"model\"])\n...\n```\n\n----------------------------------------\n\nTITLE: Saving Checkpoints with Fabric\nDESCRIPTION: Demonstrates how to save program state to a checkpoint file using Fabric's save method.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/checkpoint/checkpoint.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfabric.save(\"path/to/checkpoint.ckpt\", state)\n```\n\n----------------------------------------\n\nTITLE: Configuring ArgumentParser for PyTorch Lightning Parameters\nDESCRIPTION: Demonstrates how to set up command-line argument parsing using Python's ArgumentParser to configure both trainer settings and model hyperparameters. Shows initialization of parser, adding arguments, and using parsed values in trainer and model setup.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/hyperparameters.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom argparse import ArgumentParser\n\nparser = ArgumentParser()\n\n# Trainer arguments\nparser.add_argument(\"--devices\", type=int, default=2)\n\n# Hyperparameters for the model\nparser.add_argument(\"--layer_1_dim\", type=int, default=128)\n\n# Parse the user inputs and defaults (returns a argparse.Namespace)\nargs = parser.parse_args()\n\n# Use the parsed arguments in your program\ntrainer = Trainer(devices=args.devices)\nmodel = MyModel(layer_1_dim=args.layer_1_dim)\n```\n\n----------------------------------------\n\nTITLE: Initializing and Using Lightning Tuner for Learning Rate Finding in Python\nDESCRIPTION: This snippet demonstrates how to create a LightningModule, initialize a Trainer and Tuner, and use the Tuner to automatically find an optimal learning rate. The found learning rate is automatically set to the model's learning_rate or lr attribute.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/training_tricks.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.tuner import Tuner\n\n\nclass LitModel(LightningModule):\n    def __init__(self, learning_rate):\n        super().__init__()\n        self.learning_rate = learning_rate\n        self.model = Model(...)\n\n    def configure_optimizers(self):\n        return Adam(self.parameters(), lr=(self.lr or self.learning_rate))\n\n\nmodel = LitModel()\ntrainer = Trainer(...)\n\n# Create a Tuner\ntuner = Tuner(trainer)\n\n# finds learning rate automatically\n# sets hparams.lr or hparams.learning_rate to that learning rate\ntuner.lr_find(model)\n```\n\n----------------------------------------\n\nTITLE: Setting up MNIST Dataset and DataLoader\nDESCRIPTION: Configuration of MNIST dataset with PyTorch DataLoader for training data preparation.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/model/train_model_basic.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())\ntrain_loader = DataLoader(dataset)\n```\n\n----------------------------------------\n\nTITLE: Implementing Training Step with Metric Logging in LightningModule\nDESCRIPTION: This code demonstrates how to implement a training step in a LightningModule, including loss calculation and metric logging using the log method.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/lightning_module.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef training_step(self, batch, batch_idx):\n    inputs, target = batch\n    output = self.model(inputs, target)\n    loss = torch.nn.functional.nll_loss(output, target.view(-1))\n\n    # logs metrics for each training_step,\n    # and the average across the epoch, to the progress bar and logger\n    self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n    return loss\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Trainer with Lightning Fabric\nDESCRIPTION: This snippet demonstrates how to create a custom trainer class using Lightning Fabric primitives. It includes setup for the model, optimizer, and dataloader, as well as the training loop with automatic handling of distributed training and device management.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/README.md#2025-04-23_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\nimport lightning as L\n\n\nclass MyCustomTrainer:\n    def __init__(self, accelerator=\"auto\", strategy=\"auto\", devices=\"auto\", precision=\"32-true\"):\n        self.fabric = L.Fabric(accelerator=accelerator, strategy=strategy, devices=devices, precision=precision)\n\n    def fit(self, model, optimizer, dataloader, max_epochs):\n        self.fabric.launch()\n\n        model, optimizer = self.fabric.setup(model, optimizer)\n        dataloader = self.fabric.setup_dataloaders(dataloader)\n        model.train()\n\n        for epoch in range(max_epochs):\n            for batch in dataloader:\n                input, target = batch\n                optimizer.zero_grad()\n                output = model(input)\n                loss = loss_fn(output, target)\n                self.fabric.backward(loss)\n                optimizer.step()\n```\n\n----------------------------------------\n\nTITLE: Initializing LightningModule with Custom Parameters from Checkpoint\nDESCRIPTION: Shows how to load a LightningModule from a checkpoint while overriding some of its original hyperparameters with new values.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/checkpointing_basic.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# if you train and save the model like this it will use these values when loading\n# the weights. But you can overwrite this\nLitModel(in_dim=32, out_dim=10)\n\n# uses in_dim=32, out_dim=10\nmodel = LitModel.load_from_checkpoint(PATH)\n\n# uses in_dim=128, out_dim=10\nmodel = LitModel.load_from_checkpoint(PATH, in_dim=128, out_dim=10)\n```\n\n----------------------------------------\n\nTITLE: Using NVIDIA DALI with PyTorch Lightning\nDESCRIPTION: Demonstrates how to integrate NVIDIA DALI with PyTorch Lightning for GPU-accelerated data loading and preprocessing. This example sets up a DALI pipeline for image decoding and augmentation.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/data/alternatives.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\nfrom nvidia.dali.pipeline import pipeline_def\nimport nvidia.dali.types as types\nimport nvidia.dali.fn as fn\nfrom nvidia.dali.plugin.pytorch import DALIGenericIterator\nimport os\n\n# To run with different data, see documentation of nvidia.dali.fn.readers.file\n# points to https://github.com/NVIDIA/DALI_extra\ndata_root_dir = os.environ[\"DALI_EXTRA_PATH\"]\nimages_dir = os.path.join(data_root_dir, \"db\", \"single\", \"jpeg\")\n\n\n@pipeline_def(num_threads=4, device_id=self.trainer.local_rank)\ndef get_dali_pipeline():\n    images, labels = fn.readers.file(file_root=images_dir, random_shuffle=True, name=\"Reader\")\n    # decode data on the GPU\n    images = fn.decoders.image_random_crop(images, device=\"mixed\", output_type=types.RGB)\n    # the rest of processing happens on the GPU as well\n    images = fn.resize(images, resize_x=256, resize_y=256)\n    images = fn.crop_mirror_normalize(\n        images,\n        crop_h=224,\n        crop_w=224,\n        mean=[0.485 * 255, 0.456 * 255, 0.406 * 255],\n        std=[0.229 * 255, 0.224 * 255, 0.225 * 255],\n        mirror=fn.random.coin_flip(),\n    )\n    return images, labels\n\n\ntrain_dataloader = DALIGenericIterator(\n    [get_dali_pipeline(batch_size=16)],\n    [\"data\", \"label\"],\n    reader_name=\"Reader\",\n)\n\nmodel = ...\ntrainer = L.Trainer()\ntrainer.fit(model, train_dataloader)\n```\n\n----------------------------------------\n\nTITLE: Custom Learning Rate Scheduler Implementation in PyTorch Lightning\nDESCRIPTION: Demonstrates how to implement custom learning rate schedulers using Timm's TanhLRScheduler as an example. Shows configuration of optimizer and scheduler with custom step logic.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/optimization.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom timm.scheduler import TanhLRScheduler\n\n\ndef configure_optimizers(self):\n    optimizer = ...\n    scheduler = TanhLRScheduler(optimizer, ...)\n    return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"epoch\"}]\n\n\ndef lr_scheduler_step(self, scheduler, metric):\n    scheduler.step(epoch=self.current_epoch)  # timm's scheduler need the epoch value\n```\n\n----------------------------------------\n\nTITLE: Replacing Backward Pass with Fabric\nDESCRIPTION: Use Fabric's backward method instead of directly calling backward on the loss tensor to ensure proper gradient handling in distributed settings.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning_fabric/README.md#2025-04-23_snippet_6\n\nLANGUAGE: diff\nCODE:\n```\n- loss.backward()\n+ fabric.backward(loss)\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSpeed ZeRO Stage 3 Offload in PyTorch Lightning\nDESCRIPTION: Shows how to enable CPU offloading with DeepSpeed ZeRO Stage 3 in PyTorch Lightning. It includes examples for basic CPU offloading and advanced configuration with parameter offloading.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/deepspeed.rst#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch import Trainer\nfrom lightning.pytorch.strategies import DeepSpeedStrategy\n\n# Enable CPU Offloading\nmodel = MyModel()\ntrainer = Trainer(accelerator=\"gpu\", devices=4, strategy=\"deepspeed_stage_3_offload\", precision=16)\ntrainer.fit(model)\n\n# Enable CPU Offloading, and offload parameters to CPU\nmodel = MyModel()\ntrainer = Trainer(\n    accelerator=\"gpu\",\n    devices=4,\n    strategy=DeepSpeedStrategy(\n        stage=3,\n        offload_optimizer=True,\n        offload_parameters=True,\n    ),\n    precision=16,\n)\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Gradient Accumulation with no_backward_sync in PyTorch Lightning\nDESCRIPTION: Shows how to use the no_backward_sync context manager for efficient gradient accumulation in distributed training. Demonstrates proper usage with model forward pass and backward propagation.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_methods.rst#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# Accumulate gradient 8 batches at a time\nis_accumulating = batch_idx % 8 != 0\n\nwith fabric.no_backward_sync(model, enabled=is_accumulating):\n    output = model(input)\n    loss = ...\n    fabric.backward(loss)\n    ...\n\n# Step the optimizer every 8 batches\nif not is_accumulating:\n    optimizer.step()\n    optimizer.zero_grad()\n```\n\n----------------------------------------\n\nTITLE: Configuring LBFGS Optimizer with Closure in PyTorch Lightning\nDESCRIPTION: This snippet demonstrates how to use a closure function with an LBFGS optimizer in PyTorch Lightning. It includes the __init__ method to disable automatic optimization, the configure_optimizers method to set up the LBFGS optimizer, and the training_step method with a closure function.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/model/manual_optimization.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(self):\n    super().__init__()\n    self.automatic_optimization = False\n\n\ndef configure_optimizers(self):\n    return torch.optim.LBFGS(...)\n\n\ndef training_step(self, batch, batch_idx):\n    opt = self.optimizers()\n\n    def closure():\n        loss = self.compute_loss(batch)\n        opt.zero_grad()\n        self.manual_backward(loss)\n        return loss\n\n    opt.step(closure=closure)\n```\n\n----------------------------------------\n\nTITLE: Configuring Lightning Fabric for Different Hardware and Distributed Strategies\nDESCRIPTION: This snippet shows how to configure Lightning Fabric for various hardware setups including CPU, GPU, multi-GPU, and multi-node training. It also demonstrates setting up different distributed training strategies and precision options.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/README.md#2025-04-23_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n# Use your available hardware\n# no code changes needed\nfabric = Fabric()\n\n# Run on GPUs (CUDA or MPS)\nfabric = Fabric(accelerator=\"gpu\")\n\n# 8 GPUs\nfabric = Fabric(accelerator=\"gpu\", devices=8)\n\n# 256 GPUs, multi-node\nfabric = Fabric(accelerator=\"gpu\", devices=8, num_nodes=32)\n\n# Run on TPUs\nfabric = Fabric(accelerator=\"tpu\")\n\n# Use state-of-the-art distributed training techniques\nfabric = Fabric(strategy=\"ddp\")\nfabric = Fabric(strategy=\"deepspeed\")\nfabric = Fabric(strategy=\"fsdp\")\n\n# Switch the precision\nfabric = Fabric(precision=\"16-mixed\")\nfabric = Fabric(precision=\"64\")\n```\n\n----------------------------------------\n\nTITLE: Loading and Using Trained Model\nDESCRIPTION: Example of loading a trained model from checkpoint and using it for predictions.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/starter/introduction.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncheckpoint = \"./lightning_logs/version_0/checkpoints/epoch=0-step=100.ckpt\"\nautoencoder = LitAutoEncoder.load_from_checkpoint(checkpoint, encoder=encoder, decoder=decoder)\n\nencoder = autoencoder.encoder\nencoder.eval()\n\nfake_image_batch = torch.rand(4, 28 * 28, device=autoencoder.device)\nembeddings = encoder(fake_image_batch)\nprint(\"⚡\" * 20, \"\\nPredictions (4 image embeddings):\\n\", embeddings, \"\\n\", \"⚡\" * 20)\n```\n\n----------------------------------------\n\nTITLE: torch.compile with ModelParallelStrategy\nDESCRIPTION: Example showing how to use torch.compile with ModelParallelStrategy and tensor parallelism for distributed training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/compile.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom lightning.pytorch.demos import Transformer\nfrom lightning.fabric.strategies.model_parallel import ModelParallelStrategy\nfrom torch.distributed._composable.fsdp.fully_shard import fully_shard\nfrom torch.distributed.device_mesh import DeviceMesh\n\ndef parallelize(model: nn.Module, device_mesh: DeviceMesh) -> nn.Module:\n    for module in model.modules():\n        if isinstance(module, (torch.nn.TransformerEncoderLayer, torch.nn.TransformerDecoderLayer)):\n            fully_shard(module, mesh=device_mesh)\n\n    fully_shard(model, mesh=device_mesh)\n\n    return torch.compile(model)\n\ndef train():\n    L.seed_everything(42)\n\n    with torch.device(\"meta\"):\n        model = Transformer(\n            vocab_size=50257,\n            nlayers=16,\n            nhid=4096,\n            ninp=1024,\n            nhead=32,\n        )\n\n    strategy = ModelParallelStrategy(data_parallel_size=4, tensor_parallel_size=1, parallelize_fn=parallelize)\n\n    fabric = L.Fabric(precision=\"bf16-true\", strategy=strategy)\n    fabric.launch()\n\n    model = fabric.setup(model)\n```\n\n----------------------------------------\n\nTITLE: ImageNet Transfer Learning Implementation\nDESCRIPTION: Shows how to use a pretrained ResNet50 model for transfer learning on CIFAR-10 dataset. The model uses the pretrained backbone as a feature extractor and adds a custom classifier layer.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/transfer_learning.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass ImagenetTransferLearning(LightningModule):\n    def __init__(self):\n        super().__init__()\n\n        # init a pretrained resnet\n        backbone = models.resnet50(weights=\"DEFAULT\")\n        num_filters = backbone.fc.in_features\n        layers = list(backbone.children())[:-1]\n        self.feature_extractor = nn.Sequential(*layers)\n        self.feature_extractor.eval()\n\n        # use the pretrained model to classify cifar-10 (10 image classes)\n        num_target_classes = 10\n        self.classifier = nn.Linear(num_filters, num_target_classes)\n\n    def forward(self, x):\n        with torch.no_grad():\n            representations = self.feature_extractor(x).flatten(1)\n        x = self.classifier(representations)\n        ...\n```\n\n----------------------------------------\n\nTITLE: Loading PyTorch Lightning Model Checkpoint for Prediction\nDESCRIPTION: Demonstrates how to load a saved model checkpoint and perform basic prediction using PyTorch Lightning. The model is set to evaluation mode and predictions are made with gradient computation disabled.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/deploy/production_basic.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmodel = LitModel.load_from_checkpoint(\"best_model.ckpt\")\nmodel.eval()\nx = torch.randn(1, 64)\n\nwith torch.no_grad():\n    y_hat = model(x)\n```\n\n----------------------------------------\n\nTITLE: Extracting nn.Module from Lightning Checkpoints\nDESCRIPTION: This comprehensive example shows how to define a PyTorch Lightning model, train it, and then extract the underlying nn.Module for use in production. It includes defining encoder and decoder modules, creating a production-ready AutoEncoder, and a Lightning training system.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/deploy/production_intermediate.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass Encoder(nn.Module):\n    ...\n\n\nclass Decoder(nn.Module):\n    ...\n\n\nclass AutoEncoderProd(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = Encoder()\n        self.decoder = Decoder()\n\n    def forward(self, x):\n        return self.encoder(x)\n\n\nclass AutoEncoderSystem(LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.auto_encoder = AutoEncoderProd()\n\n    def forward(self, x):\n        return self.auto_encoder.encoder(x)\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.auto_encoder.encoder(x)\n        y_hat = self.auto_encoder.decoder(y_hat)\n        loss = ...\n        return loss\n\n\n# train it\ntrainer = Trainer(devices=2, accelerator=\"gpu\", strategy=\"ddp\")\nmodel = AutoEncoderSystem()\ntrainer.fit(model, train_dataloader, val_dataloader)\ntrainer.save_checkpoint(\"best_model.ckpt\")\n\n\n# create the PyTorch model and load the checkpoint weights\nmodel = AutoEncoderProd()\ncheckpoint = torch.load(\"best_model.ckpt\")\nhyper_parameters = checkpoint[\"hyper_parameters\"]\n\n# if you want to restore any hyperparameters, you can pass them too\nmodel = AutoEncoderProd(**hyper_parameters)\n\nmodel_weights = checkpoint[\"state_dict\"]\n\n# update keys by dropping `auto_encoder.`\nfor key in list(model_weights):\n    model_weights[key.replace(\"auto_encoder.\", \"\")] = model_weights.pop(key)\n\nmodel.load_state_dict(model_weights)\nmodel.eval()\nx = torch.randn(1, 64)\n\nwith torch.no_grad():\n    y_hat = model(x)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Model and Optimizer with Fabric\nDESCRIPTION: Prepare a model and optimizer for accelerated training. This method moves the model to the correct device and prepares it for the selected precision type, ensuring operations during forward pass are properly handled.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_methods.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel = nn.Linear(32, 64)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n\n# Set up model and optimizer for accelerated training\nmodel, optimizer = fabric.setup(model, optimizer)\n\n# If you don't want Fabric to set the device\nmodel, optimizer = fabric.setup(model, optimizer, move_to_device=False)\n```\n\n----------------------------------------\n\nTITLE: Defining BoringModel Class in PyTorch Lightning\nDESCRIPTION: This snippet implements a basic PyTorch Lightning model called BoringModel. It includes a linear layer and defines methods for forward pass, training step, validation step, test step, and optimizer configuration.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/pytorch/bug_report/bug_report_model.ipynb#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass BoringModel(LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.layer = torch.nn.Linear(32, 2)\n\n    def forward(self, x):\n        return self.layer(x)\n\n    def training_step(self, batch, batch_idx):\n        loss = self(batch).sum()\n        self.log(\"train_loss\", loss)\n        return {\"loss\": loss}\n\n    def validation_step(self, batch, batch_idx):\n        loss = self(batch).sum()\n        self.log(\"valid_loss\", loss)\n\n    def test_step(self, batch, batch_idx):\n        loss = self(batch).sum()\n        self.log(\"test_loss\", loss)\n\n    def configure_optimizers(self):\n        return torch.optim.SGD(self.layer.parameters(), lr=0.1)\n```\n\n----------------------------------------\n\nTITLE: Advanced Training Configuration\nDESCRIPTION: Examples of configuring the Trainer for advanced training scenarios including multi-GPU and distributed training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/starter/introduction.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# train on 4 GPUs\ntrainer = L.Trainer(\n    devices=4,\n    accelerator=\"gpu\",\n)\n\n# train 1TB+ parameter models with Deepspeed/fsdp\ntrainer = L.Trainer(\n    devices=4,\n    accelerator=\"gpu\",\n    strategy=\"deepspeed_stage_2\",\n    precision=16\n)\n\n# 20+ helpful flags for rapid idea iteration\ntrainer = L.Trainer(\n    max_epochs=10,\n    min_epochs=5,\n    overfit_batches=1\n)\n\n# access the latest state of the art techniques\ntrainer = L.Trainer(callbacks=[StochasticWeightAveraging(...)])\n```\n\n----------------------------------------\n\nTITLE: Explicitly Setting MPI Environment in Lightning Fabric\nDESCRIPTION: Example showing how to bypass automatic MPI detection by explicitly setting the MPI environment plugin when initializing Fabric.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/multi_node/other.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.fabric.plugins.environments import MPIEnvironment\n\nfabric = Fabric(..., plugins=[MPIEnvironment()])\n```\n\n----------------------------------------\n\nTITLE: Initializing Modules with Fabric\nDESCRIPTION: Optimize model initialization by creating parameters directly on the target device with the desired precision. This speeds up initialization and avoids the need to transfer parameters from CPU to the target device.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_methods.rst#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfabric = Fabric(accelerator=\"cuda\", precision=\"16-true\")\n\nwith fabric.init_module():\n    # models created here will be on GPU and in float16\n    model = MyModel()\n```\n\n----------------------------------------\n\nTITLE: Implementing DataModule State Management in PyTorch Lightning\nDESCRIPTION: Shows how to implement state_dict and load_state_dict methods in a LightningDataModule to track and restore custom state variables during checkpointing. The example tracks a current_train_batch_index variable, demonstrating the pattern for state persistence.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/extensions/datamodules_state.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\n\n\nclass LitDataModule(L.LightningDataModule):\n    def state_dict(self):\n        # track whatever you want here\n        state = {\"current_train_batch_index\": self.current_train_batch_index}\n        return state\n\n    def load_state_dict(self, state_dict):\n        # restore the state based on what you tracked in (def state_dict)\n        self.current_train_batch_index = state_dict[\"current_train_batch_index\"]\n```\n\n----------------------------------------\n\nTITLE: Enabling 16-bit Precision in PyTorch Lightning\nDESCRIPTION: Example of enabling 16-bit precision training in PyTorch Lightning. This reduces memory usage and speeds up training on compatible hardware.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/pytorch_lightning/README.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# no code changes needed\ntrainer = Trainer(precision=16)\n```\n\n----------------------------------------\n\nTITLE: Configuring GPU Training with PyTorch Lightning\nDESCRIPTION: Examples of setting up PyTorch Lightning Trainer for single and multi-node GPU training. Shows how to configure 8 GPUs on a single node and scale to 256 GPUs across 32 nodes without code changes to the model.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/pytorch_lightning/README.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# no code changes needed\ntrainer = Trainer(max_epochs=1, accelerator=\"gpu\", devices=8)\n\n# 256 GPUs\ntrainer = Trainer(max_epochs=1, accelerator=\"gpu\", devices=8, num_nodes=32)\n```\n\n----------------------------------------\n\nTITLE: Implementing DeepSpeed Activation Checkpointing in PyTorch Lightning\nDESCRIPTION: Shows how to use DeepSpeed activation checkpointing in a PyTorch Lightning model to save memory during training of large models. It includes examples of checkpointing specific model blocks.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/deepspeed.rst#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch import Trainer\nimport deepspeed\n\n\nclass MyModel(LightningModule):\n    ...\n\n    def __init__(self):\n        super().__init__()\n        self.block_1 = nn.Sequential(nn.Linear(32, 32), nn.ReLU())\n        self.block_2 = torch.nn.Linear(32, 2)\n\n    def forward(self, x):\n        # Use the DeepSpeed checkpointing function instead of calling the module directly\n        # checkpointing self.block_1 means the activations are deleted after use,\n        # and re-calculated during the backward passes\n        x = deepspeed.checkpointing.checkpoint(self.block_1, x)\n        return self.block_2(x)\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch import Trainer\nfrom lightning.pytorch.strategies import DeepSpeedStrategy\nimport deepspeed\n\n\nclass MyModel(LightningModule):\n    ...\n\n    def configure_model(self):\n        self.block_1 = nn.Sequential(nn.Linear(32, 32), nn.ReLU())\n        self.block_2 = torch.nn.Linear(32, 2)\n\n    def forward(self, x):\n        # Use the DeepSpeed checkpointing function instead of calling the module directly\n        x = deepspeed.checkpointing.checkpoint(self.block_1, x)\n        return self.block_2(x)\n\n\nmodel = MyModel()\n\ntrainer = Trainer(accelerator=\"gpu\", devices=4, strategy=\"deepspeed_stage_3_offload\", precision=16)\n\n# Enable CPU Activation Checkpointing\ntrainer = Trainer(\n    accelerator=\"gpu\",\n    devices=4,\n    strategy=DeepSpeedStrategy(\n        stage=3,\n        offload_optimizer=True,  # Enable CPU Offloading\n        cpu_checkpointing=True,  # (Optional) offload activations to CPU\n    ),\n    precision=16,\n)\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Implementing Test Logic in Lightning\nDESCRIPTION: Demonstrates how to implement the test_step method in a LightningModule to define the testing logic for a batch of data.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/starter/converting.rst#2025-04-23_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nclass LitModel(L.LightningModule):\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.encoder(x)\n        test_loss = F.cross_entropy(y_hat, y)\n        self.log(\"test_loss\", test_loss)\n```\n\n----------------------------------------\n\nTITLE: Compiling PyTorch Model with Dynamic Shapes\nDESCRIPTION: This snippet demonstrates how to compile a PyTorch model with dynamic input shapes using torch.compile. It's applicable for PyTorch versions prior to 2.2, where dynamism is not automatically detected.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/compile.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# On PyTorch < 2.2\nmodel = torch.compile(model, dynamic=True)\n```\n\n----------------------------------------\n\nTITLE: Implementing Validation Logic in Lightning\nDESCRIPTION: Shows how to implement the validation_step method in a LightningModule to define the validation logic for a batch of data.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/starter/converting.rst#2025-04-23_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nclass LitModel(L.LightningModule):\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.encoder(x)\n        val_loss = F.cross_entropy(y_hat, y)\n        self.log(\"val_loss\", val_loss)\n```\n\n----------------------------------------\n\nTITLE: Configuring Model-Parallel Training in PyTorch Lightning\nDESCRIPTION: Demonstrates the proper way to initialize models for FSDP or DeepSpeed training by overriding the configure_model hook instead of using init_module(). This prevents memory issues with large models.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_init.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass MyModel(LightningModule):\n    def __init__(self):\n        super().__init__()\n        # don't instantiate layers here\n        # move the creation of layers to `configure_model`\n\n    def configure_model(self):\n        # create all your layers here\n        self.layers = nn.Sequential(...)\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSpeed Infinity (NVMe Offloading) in PyTorch Lightning\nDESCRIPTION: Demonstrates how to set up NVMe offloading with DeepSpeed in PyTorch Lightning for training extremely large models. It includes configuration for offloading to NVMe drives.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/deepspeed.rst#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch import Trainer\nfrom lightning.pytorch.strategies import DeepSpeedStrategy\n\n# Enable CPU Offloading\nmodel = MyModel()\ntrainer = Trainer(accelerator=\"gpu\", devices=4, strategy=\"deepspeed_stage_3_offload\", precision=16)\ntrainer.fit(model)\n\n# Enable CPU Offloading, and offload parameters to CPU\nmodel = MyModel()\ntrainer = Trainer(\n    accelerator=\"gpu\",\n    devices=4,\n    strategy=DeepSpeedStrategy(\n        stage=3,\n        offload_optimizer=True,\n        offload_parameters=True,\n        remote_device=\"nvme\",\n        offload_params_device=\"nvme\",\n        offload_optimizer_device=\"nvme\",\n        nvme_path=\"/local_nvme\",\n    ),\n    precision=16,\n)\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Configuring DataLoaders with Fabric\nDESCRIPTION: Configure DataLoaders for distributed training. This method automatically handles sampler replacement for distributed strategies and configures data loaders to move tensors to the correct device.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_methods.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrain_data = torch.utils.DataLoader(train_dataset, ...)\ntest_data = torch.utils.DataLoader(test_dataset, ...)\n\ntrain_data, test_data = fabric.setup_dataloaders(train_data, test_data)\n\n# If you don't want Fabric to move the data to the device\ntrain_data, test_data = fabric.setup_dataloaders(train_data, test_data, move_to_device=False)\n\n# If you don't want Fabric to replace the sampler in the context of distributed training\ntrain_data, test_data = fabric.setup_dataloaders(train_data, test_data, use_distributed_sampler=False)\n```\n\n----------------------------------------\n\nTITLE: Implementing DeepSpeed ZeRO Stage 3 with FusedAdam\nDESCRIPTION: Code example for training with DeepSpeed ZeRO Stage 3, which shards optimizer states, gradients, model parameters, and optionally activations across GPUs for maximum memory efficiency.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/deepspeed.rst#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch import Trainer\nfrom deepspeed.ops.adam import FusedAdam\n\n\nclass MyModel(LightningModule):\n    ...\n\n    def configure_optimizers(self):\n        return FusedAdam(self.parameters())\n\n\nmodel = MyModel()\ntrainer = Trainer(accelerator=\"gpu\", devices=4, strategy=\"deepspeed_stage_3\", precision=16)\ntrainer.fit(model)\n\ntrainer.test()\ntrainer.predict()\n```\n\n----------------------------------------\n\nTITLE: Running MNIST Classifier with Lightning Fabric\nDESCRIPTION: Commands to train a CNN over MNIST using Lightning Fabric, which enables scaling to GPU and multi-GPU environments. Multiple options are shown for different hardware configurations.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/fabric/image_classifier/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# CPU\nfabric run train_fabric.py\n\n# GPU (CUDA or M1 Mac)\nfabric run train_fabric.py --accelerator=gpu\n\n# Multiple GPUs\nfabric run train_fabric.py --accelerator=gpu --devices=4\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Gradient Accumulation with PyTorch Lightning Fabric\nDESCRIPTION: This code snippet shows how to implement basic gradient accumulation using PyTorch Lightning Fabric. It accumulates gradients over 8 batches before updating the model parameters.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/gradient_accumulation.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfor iteration, batch in enumerate(dataloader):\n    # Accumulate gradient 8 batches at a time\n    is_accumulating = iteration % 8 != 0\n\n    output = model(input)\n    loss = ...\n\n    # .backward() accumulates when .zero_grad() wasn't called\n    fabric.backward(loss)\n    ...\n\n    if not is_accumulating:\n        # Step the optimizer after the accumulation phase is over\n        optimizer.step()\n        optimizer.zero_grad()\n```\n\n----------------------------------------\n\nTITLE: Implementing DeepSpeed ZeRO Stage 2 with CPU Offload\nDESCRIPTION: Code example for using ZeRO-Offload in PyTorch Lightning, which leverages CPU to offload optimizer memory and computation, reducing GPU memory consumption.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/deepspeed.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch import Trainer\n\nmodel = MyModel()\ntrainer = Trainer(accelerator=\"gpu\", devices=4, strategy=\"deepspeed_stage_2_offload\", precision=16)\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Converting PyTorch Lightning Model to TorchScript\nDESCRIPTION: Demonstrates creating a simple PyTorch Lightning model and converting it to TorchScript format for production deployment. The example shows how to create a basic linear model, convert it to TorchScript, and save it to disk.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/deploy/production_advanced_2.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass SimpleModel(LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(in_features=64, out_features=4)\n\n    def forward(self, x):\n        return torch.relu(self.l1(x.view(x.size(0), -1)))\n\n\n# create the model\nmodel = SimpleModel()\nscript = model.to_torchscript()\n\n# save for use in production environment\ntorch.jit.save(script, \"model.pt\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Optimizer and Scheduler with Dependency Injection\nDESCRIPTION: Example showing how to use dependency injection for both an optimizer and learning rate scheduler in a PyTorch Lightning model, with default implementations.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_advanced_3.rst#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.cli import OptimizerCallable, LRSchedulerCallable, LightningCLI\n\n\nclass MyModel(LightningModule):\n    def __init__(\n        self,\n        optimizer: OptimizerCallable = torch.optim.Adam,\n        scheduler: LRSchedulerCallable = torch.optim.lr_scheduler.ConstantLR,\n    ):\n        super().__init__()\n        self.save_hyperparameters()\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n\n    def configure_optimizers(self):\n        optimizer = self.optimizer(self.parameters())\n        scheduler = self.scheduler(optimizer)\n        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}\n\n\ncli = MyLightningCLI(MyModel, auto_configure_optimizers=False)\n```\n\n----------------------------------------\n\nTITLE: Research-focused Model Implementation with Forward Method\nDESCRIPTION: Demonstrates implementing a forward method for research purposes, including proper evaluation mode and gradient handling.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/lightning_module.rst#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nclass LightningTransformer(L.LightningModule):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.model = Transformer(vocab_size=vocab_size)\n\n    def forward(self, batch):\n        inputs, target = batch\n        return self.model(inputs, target)\n\n    def training_step(self, batch, batch_idx):\n        inputs, target = batch\n        output = self.model(inputs, target)\n        loss = torch.nn.functional.nll_loss(output, target.view(-1))\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.SGD(self.model.parameters(), lr=0.1)\n```\n\n----------------------------------------\n\nTITLE: Manual Optimization Control in PyTorch Lightning\nDESCRIPTION: Example of advanced manual control over optimizers in PyTorch Lightning. This demonstrates how to disable automatic optimization and handle multiple optimizers manually for complex training scenarios.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/pytorch_lightning/README.md#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nclass LitAutoEncoder(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.automatic_optimization = False\n\n    def training_step(self, batch, batch_idx):\n        # access your optimizers with use_pl_optimizer=False. Default is True\n        opt_a, opt_b = self.optimizers(use_pl_optimizer=True)\n\n        loss_a = ...\n        self.manual_backward(loss_a, opt_a)\n        opt_a.step()\n        opt_a.zero_grad()\n\n        loss_b = ...\n        self.manual_backward(loss_b, opt_b, retain_graph=True)\n        self.manual_backward(loss_b, opt_b)\n        opt_b.step()\n        opt_b.zero_grad()\n```\n\n----------------------------------------\n\nTITLE: Complete Tensor Parallelism Implementation Example\nDESCRIPTION: A comprehensive example demonstrating tensor parallelism with PyTorch Lightning, including model definition, parallelization strategy, data loading, and training configuration. This example shows the complete workflow for training a model with tensor parallelism.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/tp.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom torch.distributed.tensor.parallel import ColwiseParallel, RowwiseParallel\nfrom torch.distributed.tensor.parallel import parallelize_module\n\nimport lightning as L\nfrom lightning.pytorch.demos.boring_classes import RandomDataset\nfrom lightning.pytorch.strategies import ModelParallelStrategy\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim):\n        super().__init__()\n        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n\n    def forward(self, x):\n        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n\n\nclass LitModel(L.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = FeedForward(8192, 8192)\n\n    def configure_model(self):\n        if self.device_mesh is None:\n            return\n\n        # Lightning will set up a `self.device_mesh` for you\n        tp_mesh = self.device_mesh[\"tensor_parallel\"]\n        # Use PyTorch's distributed tensor APIs to parallelize the model\n        plan = {\n            \"w1\": ColwiseParallel(),\n            \"w2\": RowwiseParallel(),\n            \"w3\": ColwiseParallel(),\n        }\n        parallelize_module(self.model, tp_mesh, plan)\n\n    def training_step(self, batch):\n        output = self.model(batch)\n        loss = output.sum()\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.AdamW(self.model.parameters(), lr=3e-3)\n\n    def train_dataloader(self):\n        # Trainer configures the sampler automatically for you such that\n        # all batches in a tensor-parallel group are identical\n        dataset = RandomDataset(8192, 64)\n        return torch.utils.data.DataLoader(dataset, batch_size=8, num_workers=2)\n\n\nstrategy = ModelParallelStrategy()\ntrainer = L.Trainer(\n    accelerator=\"cuda\",\n    devices=2,\n    strategy=strategy,\n    max_epochs=1,\n)\n\nmodel = LitModel()\ntrainer.fit(model)\n\ntrainer.print(f\"Peak memory usage: {torch.cuda.max_memory_allocated() / 1e9:.02f} GB\")\n```\n\n----------------------------------------\n\nTITLE: Running Test Loop with PyTorch Lightning Trainer\nDESCRIPTION: Demonstrates how to use the Trainer class to test a model using a test dataset\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/evaluation_basic.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.utils.data import DataLoader\n\n# initialize the Trainer\ntrainer = Trainer()\n\n# test the model\ntrainer.test(model, dataloaders=DataLoader(test_set))\n```\n\n----------------------------------------\n\nTITLE: Training a PyTorch Lightning Model\nDESCRIPTION: Code for loading the MNIST dataset, initializing the LightningModule autoencoder, and training it using the Lightning Trainer.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/pytorch_lightning/README.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\ndataset = MNIST(os.getcwd(), download=True, transform=transforms.ToTensor())\ntrain, val = random_split(dataset, [55000, 5000])\n\nautoencoder = LitAutoEncoder()\ntrainer = pl.Trainer()\ntrainer.fit(autoencoder, DataLoader(train), DataLoader(val))\n```\n\n----------------------------------------\n\nTITLE: Implementing Multi-GPU Training with Fabric in Notebooks\nDESCRIPTION: Shows how to structure code for multi-GPU training in notebooks by defining a training function and launching it across multiple GPUs. The function receives a Fabric object that can be used to set up models and optimizers for distributed training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/notebooks.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Notebook Cell\ndef train(fabric):\n    model = ...\n    optimizer = ...\n    model, optimizer = fabric.setup(model, optimizer)\n    ...\n\n\n# Notebook Cell\nfabric = Fabric(accelerator=\"cuda\", devices=2)\nfabric.launch(train)  # Launches the `train` function on two GPUs\n```\n\n----------------------------------------\n\nTITLE: Exporting PyTorch Lightning Model to ONNX\nDESCRIPTION: Demonstrates how to export a PyTorch Lightning model to ONNX format using the to_onnx method. It includes examples with and without explicitly providing an input sample.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/deploy/production_advanced.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass SimpleModel(LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(in_features=64, out_features=4)\n\n    def forward(self, x):\n        return torch.relu(self.l1(x.view(x.size(0), -1)))\n\n\n# create the model\nmodel = SimpleModel()\nfilepath = \"model.onnx\"\ninput_sample = torch.randn((1, 64))\nmodel.to_onnx(filepath, input_sample, export_params=True)\n```\n\nLANGUAGE: python\nCODE:\n```\nclass SimpleModel(LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.l1 = torch.nn.Linear(in_features=64, out_features=4)\n        self.example_input_array = torch.randn(7, 64)\n\n    def forward(self, x):\n        return torch.relu(self.l1(x.view(x.size(0), -1)))\n\n\n# create the model\nmodel = SimpleModel()\nfilepath = \"model.onnx\"\nmodel.to_onnx(filepath, export_params=True)\n```\n\n----------------------------------------\n\nTITLE: Configuring torch.compile with FSDP and DDP in Lightning\nDESCRIPTION: This snippet shows how to use torch.compile with Fully Sharded Data Parallel (FSDP) and Distributed Data Parallel (DDP) strategies in Lightning. It demonstrates the option to reapply compilation after DDP/FSDP wrapping.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/compile.rst#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Choose a distributed strategy like DDP or FSDP\nfabric = L.Fabric(devices=2, strategy=\"ddp\")\n\n# Compile the model\nmodel = torch.compile(model)\n\n# Default: `fabric.setup()` will configure compilation over DDP/FSDP for you\nmodel = fabric.setup(model, _reapply_compile=True)\n\n# Turn it off if you see issues with DDP/FSDP\nmodel = fabric.setup(model, _reapply_compile=False)\n```\n\n----------------------------------------\n\nTITLE: Initializing a LightningModule with configure_model Hook\nDESCRIPTION: The configure_model hook allows efficient instantiation of large models directly on the target device with proper dtype and sharding support. This is used for FSDP models before wrapping and Zero stage 3 initialization for DeepSpeed before sharding.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Example of using the configure_model hook\ndef configure_model(self):\n    # Initialize large models efficiently on desired device and dtype\n    # This handles FSDP and DeepSpeed initialization automatically\n    # Model parameters will be created with proper settings\n```\n\n----------------------------------------\n\nTITLE: Logging a Single Metric with Fabric\nDESCRIPTION: Example of logging a single metric value to Weights and Biases using the Fabric logging method. This can be used with either Python scalar values or tensor scalar values.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/loggers/wandb.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nvalue = ...  # Python scalar or tensor scalar\nfabric.log(\"some_value\", value)\n```\n\n----------------------------------------\n\nTITLE: Defining a LightningModule Autoencoder\nDESCRIPTION: Implementation of a simple autoencoder as a LightningModule class, which includes the model architecture, training step logic, and optimizer configuration.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/pytorch_lightning/README.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass LitAutoEncoder(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.Sequential(nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 3))\n        self.decoder = nn.Sequential(nn.Linear(3, 128), nn.ReLU(), nn.Linear(128, 28 * 28))\n\n    def forward(self, x):\n        # in lightning, forward defines the prediction/inference actions\n        embedding = self.encoder(x)\n        return embedding\n\n    def training_step(self, batch, batch_idx):\n        # training_step defines the train loop. It is independent of forward\n        x, _ = batch\n        x = x.view(x.size(0), -1)\n        z = self.encoder(x)\n        x_hat = self.decoder(z)\n        loss = F.mse_loss(x_hat, x)\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n        return optimizer\n```\n\n----------------------------------------\n\nTITLE: Implementing Basic Model Training with Fabric\nDESCRIPTION: Template for implementing basic model training using Fabric, including setup of model, optimizer, and data loader with proper initialization and training loop structure.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/code_structure.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\n\n\ndef train(fabric, model, optimizer, dataloader):\n    # Training loop\n    model.train()\n    for epoch in range(num_epochs):\n        for i, batch in enumerate(dataloader):\n            ...\n\n\ndef main():\n    # (Optional) Parse command line options\n    args = parse_args()\n\n    # Configure Fabric\n    fabric = L.Fabric(...)\n\n    # Instantiate objects\n    model = ...\n    optimizer = ...\n    train_dataloader = ...\n\n    # Set up objects\n    model, optimizer = fabric.setup(model, optimizer)\n    train_dataloader = fabric.setup_dataloaders(train_dataloader)\n\n    # Run training loop\n    train(fabric, model, optimizer, train_dataloader)\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Disabling Checkpointing in PyTorch Lightning Trainer\nDESCRIPTION: Demonstrates how to disable automatic checkpointing in PyTorch Lightning by setting the enable_checkpointing parameter to False when initializing the Trainer.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/checkpointing_basic.rst#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntrainer = Trainer(enable_checkpointing=False)\n```\n\n----------------------------------------\n\nTITLE: Extracting nn.Module Weights from PyTorch Lightning Checkpoint\nDESCRIPTION: Shows how to load a PyTorch Lightning checkpoint and extract weights for specific nn.Modules, allowing integration with plain PyTorch models.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/checkpointing_basic.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ncheckpoint = torch.load(CKPT_PATH)\nprint(checkpoint.keys())\n\nclass Encoder(nn.Module):\n    ...\n\n\nclass Decoder(nn.Module):\n    ...\n\n\nclass Autoencoder(L.LightningModule):\n    def __init__(self, encoder, decoder, *args, **kwargs):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n\nautoencoder = Autoencoder(Encoder(), Decoder())\n\ncheckpoint = torch.load(CKPT_PATH)\nencoder_weights = {k: v for k, v in checkpoint[\"state_dict\"].items() if k.startswith(\"encoder.\")}\ndecoder_weights = {k: v for k, v in checkpoint[\"state_dict\"].items() if k.startswith(\"decoder.\")}\n```\n\n----------------------------------------\n\nTITLE: Saving Checkpoints in Distributed Training with PyTorch Lightning\nDESCRIPTION: This snippet shows how to save checkpoints in a distributed training setup using PyTorch Lightning, which handles strategy-specific saving logic automatically.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/checkpointing_intermediate.rst#2025-04-23_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\ntrainer = Trainer(strategy=\"ddp\")\nmodel = MyLightningModule(hparams)\ntrainer.fit(model)\n\n# Saves only on the main process\n# Handles strategy-specific saving logic like XLA, FSDP, DeepSpeed etc.\ntrainer.save_checkpoint(\"example.ckpt\")\n```\n\n----------------------------------------\n\nTITLE: Basic PyTorch Data Loading Example\nDESCRIPTION: Shows a typical PyTorch data loading process without using LightningDataModule. This example loads MNIST data and creates DataLoaders for train, validation, test, and predict sets.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/data/datamodule.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# regular PyTorch\ntest_data = MNIST(my_path, train=False, download=True)\npredict_data = MNIST(my_path, train=False, download=True)\ntrain_data = MNIST(my_path, train=True, download=True)\ntrain_data, val_data = random_split(train_data, [55000, 5000])\n\ntrain_loader = DataLoader(train_data, batch_size=32)\nval_loader = DataLoader(val_data, batch_size=32)\ntest_loader = DataLoader(test_data, batch_size=32)\npredict_loader = DataLoader(predict_data, batch_size=32)\n```\n\n----------------------------------------\n\nTITLE: Defining PyTorch Neural Network Model\nDESCRIPTION: Demonstrates how to define a basic neural network model using PyTorch's nn.Module. This model architecture remains unchanged when converting to Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/starter/converting.rst#2025-04-23_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport lightning as L\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer_1 = nn.Linear(28 * 28, 128)\n        self.layer_2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = x.view(x.size(0), -1)\n        x = self.layer_1(x)\n        x = F.relu(x)\n        x = self.layer_2(x)\n        return x\n```\n\n----------------------------------------\n\nTITLE: Saving Checkpoints with Fabric in FSDP\nDESCRIPTION: This code shows how to save checkpoints efficiently using Fabric's method. It demonstrates the correct way to define the state and save it, avoiding inefficient approaches.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/model_parallel/fsdp.rst#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# 1. Define model, optimizer, and other training loop state\nstate = {\"model\": model, \"optimizer\": optimizer, \"iter\": iteration}\n\n# DON'T do this (inefficient):\n# state = {\"model\": model.state_dict(), \"optimizer\": optimizer.state_dict(), ...}\n\n# 2. Save using Fabric's method\nfabric.save(\"path/to/checkpoint/file\", state)\n\n# DON'T do this (inefficient):\n# torch.save(\"path/to/checkpoint/file\", state)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Multiple Models with Multiple Optimizers in Fabric\nDESCRIPTION: Demonstrates how to set up multiple independent models, each with its own optimizer in Lightning Fabric. This pattern is commonly used in adversarial networks like GANs where models need separate optimization.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/multiple_setup.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Two models\ngenerator = Generator()\ndiscriminator = Discriminator()\n\n# Two optimizers\noptimizer_gen = torch.optim.SGD(generator.parameters(), lr=0.01)\noptimizer_dis = torch.optim.SGD(discriminator.parameters(), lr=0.001)\n\n# Set up generator\ngenerator, optimizer_gen = fabric.setup(generator, optimizer_gen)\n# Set up discriminator\ndiscriminator, optimizer_dis = fabric.setup(discriminator, optimizer_dis)\n```\n\n----------------------------------------\n\nTITLE: Setting Up One Model with One Optimizer in Fabric\nDESCRIPTION: Demonstrates the basic pattern for setting up a single model with a single optimizer in Lightning Fabric. The setup method prepares both the model and optimizer to work with the selected strategy.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/multiple_setup.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom lightning.fabric import Fabric\n\nfabric = Fabric()\n\n# Instantiate model and optimizer\nmodel = LitModel()\noptimizer = torch.optim.Adam(model.parameters())\n\n# Set up the model and optimizer together\nmodel, optimizer = fabric.setup(model, optimizer)\n```\n\n----------------------------------------\n\nTITLE: Implementing Static Pruning with ModelPruning Callback in PyTorch Lightning\nDESCRIPTION: This snippet demonstrates how to use the ModelPruning callback in PyTorch Lightning to apply static pruning. It prunes 50% of the model parameters using the l1_unstructured method.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/pruning_quantization.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.callbacks import ModelPruning\n\n# set the amount to be the fraction of parameters to prune\ntrainer = Trainer(callbacks=[ModelPruning(\"l1_unstructured\", amount=0.5)])\n```\n\n----------------------------------------\n\nTITLE: GAN Implementation with Multiple Optimizers in PyTorch Lightning\nDESCRIPTION: Shows how to implement a GAN using manual optimization with separate optimizers for generator and discriminator networks.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/model/manual_optimization.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom torch import Tensor\nfrom lightning.pytorch import LightningModule\n\nclass SimpleGAN(LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.G = Generator()\n        self.D = Discriminator()\n        self.automatic_optimization = False\n\n    def sample_z(self, n) -> Tensor:\n        sample = self._Z.sample((n,))\n        return sample\n\n    def sample_G(self, n) -> Tensor:\n        z = self.sample_z(n)\n        return self.G(z)\n\n    def training_step(self, batch, batch_idx):\n        g_opt, d_opt = self.optimizers()\n        X, _ = batch\n        batch_size = X.shape[0]\n        real_label = torch.ones((batch_size, 1), device=self.device)\n        fake_label = torch.zeros((batch_size, 1), device=self.device)\n        g_X = self.sample_G(batch_size)\n        d_x = self.D(X)\n        errD_real = self.criterion(d_x, real_label)\n        d_z = self.D(g_X.detach())\n        errD_fake = self.criterion(d_z, fake_label)\n        errD = errD_real + errD_fake\n        d_opt.zero_grad()\n        self.manual_backward(errD)\n        d_opt.step()\n        d_z = self.D(g_X)\n        errG = self.criterion(d_z, real_label)\n        g_opt.zero_grad()\n        self.manual_backward(errG)\n        g_opt.step()\n        self.log_dict({\"g_loss\": errG, \"d_loss\": errD}, prog_bar=True)\n\n    def configure_optimizers(self):\n        g_opt = torch.optim.Adam(self.G.parameters(), lr=1e-5)\n        d_opt = torch.optim.Adam(self.D.parameters(), lr=1e-5)\n        return g_opt, d_opt\n```\n\n----------------------------------------\n\nTITLE: Enabling True Half Precision in PyTorch Lightning\nDESCRIPTION: These code snippets demonstrate how to use true half precision (FP16 or BF16) for model training in PyTorch Lightning. It also shows how to initialize model parameters directly in the desired precision.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/precision_intermediate.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Select FP16 precision\ntrainer = Trainer(precision=\"16-true\")\ntrainer.fit(model)  # model gets cast to torch.float16\n\n# Select BF16 precision\ntrainer = Trainer(precision=\"bf16-true\")\ntrainer.fit(model)  # model gets cast to torch.bfloat16\n```\n\nLANGUAGE: python\nCODE:\n```\ntrainer = Trainer(precision=\"bf16-true\")\n\n# init the model directly on the device and with parameters in half-precision\nwith trainer.init_module():\n    model = MyModel()\n\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Dual Optimizer Manual Training in PyTorch Lightning\nDESCRIPTION: Example showing manual optimization with two optimizers, commonly used in GANs or other multi-optimizer scenarios. Includes initialization and training step implementation.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/lightning_module.rst#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(self):\n    self.automatic_optimization = False\n\n\ndef training_step(self, batch, batch_idx):\n    # access your optimizers with use_pl_optimizer=False. Default is True\n    opt_a, opt_b = self.optimizers(use_pl_optimizer=True)\n\n    gen_loss = ...\n    opt_a.zero_grad()\n    self.manual_backward(gen_loss)\n    opt_a.step()\n\n    disc_loss = ...\n    opt_b.zero_grad()\n    self.manual_backward(disc_loss)\n    opt_b.step()\n```\n\n----------------------------------------\n\nTITLE: Using Stateful StreamingDataLoader for Training Resumption\nDESCRIPTION: Shows how to implement a stateful StreamingDataLoader that can save and restore its state, making it easy to resume training over large datasets. The example saves the dataloader state every 1000 batches and loads it if available.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/data/README.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport torch\nfrom lightning.data import StreamingDataset, StreamingDataLoader\n\ndataset = StreamingDataset(\"s3://my-bucket/my-data\", shuffle=True)\ndataloader = StreamingDataLoader(dataset, num_workers=os.cpu_count(), batch_size=64)\n\n# Restore the dataLoader state if it exists\nif os.path.isfile(\"dataloader_state.pt\"):\n    state_dict = torch.load(\"dataloader_state.pt\")\n    dataloader.load_state_dict(state_dict)\n\n# Iterate over the data\nfor batch_idx, batch in enumerate(dataloader):\n  \n    # Store the state every 1000 batches\n    if batch_idx % 1000 == 0:\n        torch.save(dataloader.state_dict(), \"dataloader_state.pt\")\n```\n\n----------------------------------------\n\nTITLE: Initializing Trainer for Multi-GPU Training with DDP in PyTorch Lightning\nDESCRIPTION: This snippet demonstrates how to initialize a PyTorch Lightning Trainer for distributed training on multiple GPUs using the DDP strategy. It shows examples for training on a single machine with 8 GPUs and across 4 nodes with 32 GPUs total.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/accelerators/gpu_intermediate.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# train on 8 GPUs (same machine (ie: node))\ntrainer = Trainer(accelerator=\"gpu\", devices=8, strategy=\"ddp\")\n\n# train on 32 GPUs (4 nodes)\ntrainer = Trainer(accelerator=\"gpu\", devices=8, strategy=\"ddp\", num_nodes=4)\n```\n\n----------------------------------------\n\nTITLE: Synchronizing Validation and Test Logging in Distributed PyTorch Lightning\nDESCRIPTION: Demonstrates how to properly log metrics during validation and testing when running in distributed mode by adding sync_dist=True parameter to logging calls.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/accelerators/accelerator_prepare.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef validation_step(self, batch, batch_idx):\n    x, y = batch\n    logits = self(x)\n    loss = self.loss(logits, y)\n    # Add sync_dist=True to sync logging across all GPU workers (may have performance impact)\n    self.log(\"validation_loss\", loss, on_step=True, on_epoch=True, sync_dist=True)\n\n\ndef test_step(self, batch, batch_idx):\n    x, y = batch\n    logits = self(x)\n    loss = self.loss(logits, y)\n    # Add sync_dist=True to sync logging across all GPU workers (may have performance impact)\n    self.log(\"test_loss\", loss, on_step=True, on_epoch=True, sync_dist=True)\n```\n\n----------------------------------------\n\nTITLE: Implementing Validation Step in LightningModule\nDESCRIPTION: This code shows how to implement a validation step in a LightningModule, including loss calculation and logging.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/lightning_module.rst#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass LightningTransformer(L.LightningModule):\n    def validation_step(self, batch, batch_idx):\n        inputs, target = batch\n        output = self.model(inputs, target)\n        loss = F.cross_entropy(y_hat, y)\n        self.log(\"val_loss\", loss)\n```\n\n----------------------------------------\n\nTITLE: Saving Hyperparameters in LightningModule\nDESCRIPTION: Demonstrates how to use save_hyperparameters() in the LightningModule initialization to automatically store arguments for later reference and model checkpointing.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/lightning_module.rst#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nclass LitMNIST(L.LightningModule):\n    def __init__(self, layer_1_dim=128, learning_rate=1e-2):\n        super().__init__()\n        # call this to save (layer_1_dim=128, learning_rate=1e-4) to the checkpoint\n        self.save_hyperparameters()\n\n        # equivalent\n        self.save_hyperparameters(\"layer_1_dim\", \"learning_rate\")\n\n        # Now possible to access layer_1_dim from hparams\n        self.hparams.layer_1_dim\n```\n\n----------------------------------------\n\nTITLE: Configuring Optimizers and LR Schedulers in Lightning\nDESCRIPTION: Demonstrates how to move optimizer and learning rate scheduler configurations to the configure_optimizers method in a LightningModule.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/starter/converting.rst#2025-04-23_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nclass LitModel(L.LightningModule):\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.encoder.parameters(), lr=1e-3)\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)\n        return [optimizer], [lr_scheduler]\n```\n\n----------------------------------------\n\nTITLE: Enabling BFloat16 Mixed Precision in Fabric (Python)\nDESCRIPTION: Shows how to enable BFloat16 mixed precision when initializing a Fabric object.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/precision.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Select BF16 precision\nfabric = Fabric(precision=\"bf16-mixed\")\n```\n\n----------------------------------------\n\nTITLE: Inspecting and Plotting Lightning Tuner Learning Rate Finder Results in Python\nDESCRIPTION: This snippet demonstrates how to run the learning rate finder, inspect its results, plot the findings, and update the model's learning rate based on the suggestion. It also shows how to fit the model with the new learning rate.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/training_tricks.rst#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nmodel = MyModelClass(hparams)\ntrainer = Trainer()\ntuner = Tuner(trainer)\n\n# Run learning rate finder\nlr_finder = tuner.lr_find(model)\n\n# Results can be found in\nprint(lr_finder.results)\n\n# Plot with\nfig = lr_finder.plot(suggest=True)\nfig.show()\n\n# Pick point based on plot, or get suggestion\nnew_lr = lr_finder.suggestion()\n\n# update hparams of the model\nmodel.hparams.lr = new_lr\n\n# Fit model\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Precision Plugin in PyTorch Lightning\nDESCRIPTION: This code snippet demonstrates how to create a custom precision plugin by subclassing the Precision class from PyTorch Lightning. It shows the basic structure of a custom precision class and how to use it with a Trainer object.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/precision_expert.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass CustomPrecision(Precision):\n    precision = \"16-mixed\"\n\n    ...\n\n\ntrainer = Trainer(plugins=[CustomPrecision()])\n```\n\n----------------------------------------\n\nTITLE: Loading Checkpoints Efficiently in PyTorch Lightning\nDESCRIPTION: Shows how to load model checkpoints efficiently using empty_init=True parameter, which avoids expensive memory initialization when loading models for inference or fine-tuning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_init.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nwith trainer.init_module(empty_init=True):\n    # creation of the model is fast\n    # and depending on the strategy allocates no memory, or uninitialized memory\n    model = MyLightningModule.load_from_checkpoint(\"my/checkpoint/path.ckpt\")\n\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Initializing Half-Precision Model with PyTorch Lightning\nDESCRIPTION: Demonstrates how to initialize a model directly on GPU with float16 precision using trainer.init_module(). This approach improves speed by avoiding redundant CPU to GPU transfers and reduces memory usage by preventing float32 allocations.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_init.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntrainer = Trainer(accelerator=\"cuda\", precision=\"16-true\")\n\nwith trainer.init_module():\n    # models created here will be on GPU and in float16\n    model = MyLightningModule()\n```\n\n----------------------------------------\n\nTITLE: Mixed Precision Training Configuration\nDESCRIPTION: Example of setting up 16-bit mixed precision training for improved performance on compatible GPUs.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/speed.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# 16-bit precision\ntrainer = Trainer(precision=16, accelerator=\"gpu\", devices=4)\n```\n\n----------------------------------------\n\nTITLE: Basic torch.compile Implementation with Fabric\nDESCRIPTION: Shows how to compile a PyTorch model using Fabric with proper setup order. The compilation occurs before fabric.setup() for optimal integration.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/compile.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport lightning as L\n\n# Set up Fabric\nfabric = L.Fabric(devices=1)\n\n# Define the model\nmodel = ...\n\n# Compile the model\nmodel = torch.compile(model)\n\n# `fabric.setup()` should come after `torch.compile()`\nmodel = fabric.setup(model)\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Training Time in PyTorch Lightning\nDESCRIPTION: Examples showing how to limit the total training time in the Trainer using different formats. This allows stopping training after a specific duration regardless of epochs or steps.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_21\n\nLANGUAGE: python\nCODE:\n```\n# Default (disabled)\ntrainer = Trainer(max_time=None)\n\n# Stop after 12 hours of training or when reaching 10 epochs (string)\ntrainer = Trainer(max_time=\"00:12:00:00\", max_epochs=10)\n\n# Stop after 1 day and 5 hours (dict)\ntrainer = Trainer(max_time={\"days\": 1, \"hours\": 5})\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Strategy with DeepSpeed in Fabric\nDESCRIPTION: Example of passing a custom strategy by configuring DeepSpeed with additional parameters, allowing for fine-grained control over the training strategy implementation.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_args.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.fabric.strategies import DeepSpeedStrategy\n\nfabric = Fabric(strategy=DeepSpeedStrategy(stage=2), accelerator=\"gpu\", devices=2)\n```\n\n----------------------------------------\n\nTITLE: Configuring Top-K Checkpoints in PyTorch Lightning\nDESCRIPTION: This snippet demonstrates how to configure the ModelCheckpoint callback to save the top-K checkpoints based on validation loss.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/checkpointing_intermediate.rst#2025-04-23_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom lightning.pytorch.callbacks import ModelCheckpoint\n\n# saves top-K checkpoints based on \"val_loss\" metric\ncheckpoint_callback = ModelCheckpoint(\n    save_top_k=10,\n    monitor=\"val_loss\",\n    mode=\"min\",\n    dirpath=\"my/path/\",\n    filename=\"sample-mnist-{epoch:02d}-{val_loss:.2f}\",\n)\n\n# saves last-K checkpoints based on \"global_step\" metric\n# make sure you log it inside your LightningModule\ncheckpoint_callback = ModelCheckpoint(\n    save_top_k=10,\n    monitor=\"global_step\",\n    mode=\"max\",\n    dirpath=\"my/path/\",\n    filename=\"sample-mnist-{epoch:02d}-{global_step}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Running PPO Example with Lightning Fabric\nDESCRIPTION: This command runs the PPO implementation using Lightning Fabric with distributed data parallel (DDP) strategy across 2 CPU devices.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/fabric/reinforcement_learning/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nfabric run --accelerator=cpu --strategy=ddp --devices=2 train_fabric.py\n```\n\n----------------------------------------\n\nTITLE: Creating a Fabric Object in PyTorch\nDESCRIPTION: Initialize a Lightning Fabric object at the beginning of your training code to enable distributed training capabilities.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning_fabric/README.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport Lightning as L\n\nfabric = L.Fabric()\n```\n\n----------------------------------------\n\nTITLE: Loading Model Checkpoints Efficiently for Inference or Finetuning\nDESCRIPTION: Uses empty_init=True to avoid expensive and redundant memory initialization when loading models from checkpoints, particularly useful for finetuning scenarios. This significantly reduces initialization time and memory usage.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/model_init.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nwith fabric.init_module(empty_init=True):\n    # creation of the model is fast\n    # and depending on the strategy allocates no memory, or uninitialized memory\n    model = MyModel()\n\n# weights get loaded into the model\nmodel.load_state_dict(checkpoint[\"state_dict\"])\n```\n\n----------------------------------------\n\nTITLE: Using torch.compile in configure_model Hook with Model Parallel Strategy\nDESCRIPTION: Shows how to integrate torch.compile within the configure_model hook, which is especially useful when combining with distributed strategies like ModelParallelStrategy.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/compile.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom lightning.pytorch.demos import Transformer\nfrom lightning.pytorch.strategies.model_parallel import ModelParallelStrategy\nfrom torch.distributed.device_mesh import DeviceMesh\nfrom torch.distributed._composable.fsdp.fully_shard import fully_shard\n\nclass LanguageModel(L.LightningModule):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.model = None\n\n    def configure_model(self):\n        if self.model is not None:\n            return\n\n        with torch.device(\"meta\"):\n            model = Transformer(\n                vocab_size=self.vocab_size,\n                nlayers=16,\n                nhid=4096,\n                ninp=1024,\n                nhead=32,\n            )\n\n        for module in model.modules():\n            if isinstance(module, (nn.TransformerEncoderLayer, nn.TransformerDecoderLayer)):\n                fully_shard(module, mesh=self.device_mesh)\n\n        fully_shard(model, mesh=self.device_mesh)\n\n        self.model = torch.compile(model)\n\n    def training_step(self, batch):\n        input, target = batch\n        output = self.model(input, target)\n        loss = F.nll_loss(output, target.view(-1))\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=1e-4)\n```\n\n----------------------------------------\n\nTITLE: Testing a Model After Training in PyTorch Lightning\nDESCRIPTION: Demonstrates how to test a model after completing the training process using different checkpoint options.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/evaluation_intermediate.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# run full training\ntrainer.fit(model)\n\n# (1) load the best checkpoint automatically (lightning tracks this for you during .fit())\ntrainer.test(ckpt_path=\"best\")\n\n# (2) load the last available checkpoint (only works if `ModelCheckpoint(save_last=True)`)\ntrainer.test(ckpt_path=\"last\")\n\n# (3) test using a specific checkpoint\ntrainer.test(ckpt_path=\"/path/to/my_checkpoint.ckpt\")\n\n# (4) test with an explicit model (will use this model and not load a checkpoint)\ntrainer.test(model)\n```\n\n----------------------------------------\n\nTITLE: Enabling Activation Checkpointing in FSDP Strategy\nDESCRIPTION: Demonstrates how to enable activation checkpointing for specific layers in the FSDP strategy to reduce memory consumption during training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/fsdp.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nstrategy = FSDPStrategy(\n    # Enable activation checkpointing on these layers\n    activation_checkpointing_policy={\n        nn.TransformerEncoderLayer,\n        nn.TransformerDecoderLayer,\n    },\n)\ntrainer = L.Trainer(..., strategy=strategy)\n```\n\n----------------------------------------\n\nTITLE: Training Epoch Control Configuration\nDESCRIPTION: Examples of controlling training duration through epoch and step limits.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/speed.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# DEFAULT\ntrainer = Trainer(min_epochs=1, max_epochs=1000)\n\ntrainer = Trainer(max_steps=1000)\n\ntrainer = Trainer(min_steps=100)\n```\n\n----------------------------------------\n\nTITLE: FSDP Activation Checkpointing Configuration\nDESCRIPTION: Setup for activation checkpointing in FSDP to reduce memory usage by trading off computation speed.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/model_parallel/fsdp.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nstrategy = FSDPStrategy(\n    # Enable activation checkpointing on these layers\n    activation_checkpointing_policy={\n        nn.TransformerEncoderLayer,\n        nn.TransformerDecoderLayer,\n    },\n)\nfabric = L.Fabric(..., strategy=strategy)\n```\n\n----------------------------------------\n\nTITLE: Implementing Hyperparameters in DataModule\nDESCRIPTION: Shows how to implement hyperparameter support in DataModule using save_hyperparameters.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/data/datamodule.rst#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\n\nclass CustomDataModule(L.LightningDataModule):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        self.save_hyperparameters()\n\n    def configure_optimizers(self):\n        opt = optim.Adam(self.parameters(), lr=self.hparams.lr)\n```\n\n----------------------------------------\n\nTITLE: Loading LightningModule with Custom Modules from Checkpoint\nDESCRIPTION: Demonstrates how to load a LightningModule from a checkpoint when the __init__ method requires passing entire PyTorch modules as arguments.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/checkpointing_basic.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass LitAutoencoder(L.LightningModule):\n    def __init__(self, encoder, decoder):\n        ...\n\n    ...\n\n\nmodel = LitAutoEncoder.load_from_checkpoint(PATH, encoder=encoder, decoder=decoder)\n```\n\n----------------------------------------\n\nTITLE: Enabling Gradient as Bucket View in DDP Strategy\nDESCRIPTION: Configures DDP to use gradients as bucket views to reduce peak memory usage by making gradients point to different offsets of the allreduce communication buckets, eliminating the need to copy gradients.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/ddp_optimizations.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\nfrom lightning.pytorch.strategies import DDPStrategy\n\nmodel = MyModel()\ntrainer = L.Trainer(devices=4, strategy=DDPStrategy(gradient_as_bucket_view=True))\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Implementing setup in LightningDataModule\nDESCRIPTION: Demonstrates the implementation of the setup method in a LightningDataModule. This method is used for operations that should be done on every GPU, such as splitting data and creating datasets.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/data/datamodule.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\n\n\nclass MNISTDataModule(L.LightningDataModule):\n    def setup(self, stage: str):\n        # Assign Train/val split(s) for use in Dataloaders\n        if stage == \"fit\":\n            mnist_full = MNIST(self.data_dir, train=True, download=True, transform=self.transform)\n            self.mnist_train, self.mnist_val = random_split(\n                mnist_full, [55000, 5000], generator=torch.Generator().manual_seed(42)\n            )\n\n        # Assign Test split(s) for use in Dataloaders\n        if stage == \"test\":\n            self.mnist_test = MNIST(self.data_dir, train=False, download=True, transform=self.transform)\n```\n\n----------------------------------------\n\nTITLE: Saving Hyperparameters in PyTorch Lightning LightningModule\nDESCRIPTION: Demonstrates how to automatically save hyperparameters passed to the __init__ method of a LightningModule using self.save_hyperparameters(). It also shows how to access these hyperparameters later.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/checkpointing_basic.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass MyLightningModule(LightningModule):\n    def __init__(self, learning_rate, another_parameter, *args, **kwargs):\n        super().__init__()\n        self.save_hyperparameters()\n\ncheckpoint = torch.load(checkpoint, map_location=lambda storage, loc: storage)\nprint(checkpoint[\"hyper_parameters\"])\n# {\"learning_rate\": the_value, \"another_parameter\": the_other_value}\n\nmodel = MyLightningModule.load_from_checkpoint(\"/path/to/checkpoint.ckpt\")\nprint(model.learning_rate)\n```\n\n----------------------------------------\n\nTITLE: Using FFCV with PyTorch Lightning\nDESCRIPTION: Demonstrates how to integrate FFCV (Fast Forward Computer Vision) with PyTorch Lightning. This example sets up data decoding, augmentation pipelines, and uses the FFCV Loader instead of PyTorch's DataLoader.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/data/alternatives.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\nfrom ffcv.loader import Loader, OrderOption\nfrom ffcv.transforms import ToTensor, ToDevice, ToTorchImage, Cutout\nfrom ffcv.fields.decoders import IntDecoder, RandomResizedCropRGBImageDecoder\n\n# Random resized crop\ndecoder = RandomResizedCropRGBImageDecoder((224, 224))\n# Data decoding and augmentation\nimage_pipeline = [decoder, Cutout(), ToTensor(), ToTorchImage()]\nlabel_pipeline = [IntDecoder(), ToTensor()]\n# Pipeline for each data field\npipelines = {\"image\": image_pipeline, \"label\": label_pipeline}\n# Replaces PyTorch data loader (`torch.utils.data.Dataloader`)\ntrain_dataloader = Loader(\n    write_path, batch_size=bs, num_workers=num_workers, order=OrderOption.RANDOM, pipelines=pipelines\n)\n\nmodel = ...\ntrainer = L.Trainer()\ntrainer.fit(model, train_dataloader)\n```\n\n----------------------------------------\n\nTITLE: Basic LightningDataModule Implementation for MNIST\nDESCRIPTION: Demonstrates a simple implementation of LightningDataModule for the MNIST dataset. This class encapsulates the data loading and preprocessing steps, making the code more organized and reusable.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/data/datamodule.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass MNISTDataModule(L.LightningDataModule):\n    def __init__(self, data_dir: str = \"path/to/dir\", batch_size: int = 32):\n        super().__init__()\n        self.data_dir = data_dir\n        self.batch_size = batch_size\n\n    def setup(self, stage: str):\n        self.mnist_test = MNIST(self.data_dir, train=False)\n        self.mnist_predict = MNIST(self.data_dir, train=False)\n        mnist_full = MNIST(self.data_dir, train=True)\n        self.mnist_train, self.mnist_val = random_split(\n            mnist_full, [55000, 5000], generator=torch.Generator().manual_seed(42)\n        )\n\n    def train_dataloader(self):\n        return DataLoader(self.mnist_train, batch_size=self.batch_size)\n\n    def val_dataloader(self):\n        return DataLoader(self.mnist_val, batch_size=self.batch_size)\n\n    def test_dataloader(self):\n        return DataLoader(self.mnist_test, batch_size=self.batch_size)\n\n    def predict_dataloader(self):\n        return DataLoader(self.mnist_predict, batch_size=self.batch_size)\n\n    def teardown(self, stage: str):\n        # Used to clean-up when the run is finished\n        ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Fabric for Different Accelerators in Python\nDESCRIPTION: This snippet demonstrates how to explicitly set up Fabric for various accelerators including CPU, GPU (single and multiple), Apple Silicon, NVIDIA CUDA, and TPU.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/accelerators.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# CPU (slow)\nfabric = Fabric(accelerator=\"cpu\")\n\n# GPU\nfabric = Fabric(accelerator=\"gpu\", devices=1)\n\n# GPU (multiple)\nfabric = Fabric(accelerator=\"gpu\", devices=8)\n\n# GPU: Apple M1/M2 only\nfabric = Fabric(accelerator=\"mps\")\n\n# GPU: NVIDIA CUDA only\nfabric = Fabric(accelerator=\"cuda\", devices=8)\n\n# TPU\nfabric = Fabric(accelerator=\"tpu\", devices=8)\n```\n\n----------------------------------------\n\nTITLE: Basic Manual Optimization Implementation in PyTorch Lightning\nDESCRIPTION: Demonstrates the minimal implementation of manual optimization by disabling automatic optimization and manually handling gradient operations.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/model/manual_optimization.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch import LightningModule\n\nclass MyModel(LightningModule):\n    def __init__(self):\n        super().__init__()\n        # Important: This property activates manual optimization.\n        self.automatic_optimization = False\n\n    def training_step(self, batch, batch_idx):\n        opt = self.optimizers()\n        opt.zero_grad()\n        loss = self.compute_loss(batch)\n        self.manual_backward(loss)\n        opt.step()\n```\n\n----------------------------------------\n\nTITLE: Using Custom Strategy in PyTorch Lightning Trainer\nDESCRIPTION: Examples of how to use a custom strategy with the PyTorch Lightning Trainer. This includes passing the custom strategy directly and creating a strategy with custom accelerator and plugins.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/extensions/strategy.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# custom strategy\ntrainer = Trainer(strategy=CustomDDPStrategy())\n\n# custom strategy, with new accelerator and plugins\naccelerator = MyAccelerator()\nprecision_plugin = MyPrecisionPlugin()\nstrategy = CustomDDPStrategy(accelerator=accelerator, precision_plugin=precision_plugin)\ntrainer = Trainer(strategy=strategy)\n```\n\n----------------------------------------\n\nTITLE: Using Trainer.init_module Context Manager\nDESCRIPTION: Context manager for instantiating large models efficiently on specified device and dtype. It creates model parameters in the desired precision based on the Trainer's precision setting.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Using the Trainer.init_module() context manager\nwith trainer.init_module():\n    model = LargeModel()\n    \n# With meta-device initialization for FSDP\nwith trainer.init_module(empty_init=True):\n    model = LargeModel()\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Compiled vs Uncompiled Models in PyTorch Lightning\nDESCRIPTION: A complete example that measures and compares the execution speed of a compiled InceptionV3 model versus its uncompiled version using a custom callback for timing.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/compile.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport statistics\nimport torch\nimport torchvision.models as models\nimport lightning as L\nfrom torch.utils.data import DataLoader\n\n\nclass MyLightningModule(L.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = models.inception_v3()\n\n    def training_step(self, batch):\n        return self.model(batch).logits.sum()\n\n    def train_dataloader(self):\n        return DataLoader([torch.randn(3, 512, 512) for _ in range(256)], batch_size=16)\n\n    def configure_optimizers(self):\n        return torch.optim.SGD(self.parameters(), lr=0.01)\n\n\nclass Benchmark(L.Callback):\n    \"\"\"A callback that measures the median execution time between the start and end of a batch.\"\"\"\n    def __init__(self):\n        self.start = torch.cuda.Event(enable_timing=True)\n        self.end = torch.cuda.Event(enable_timing=True)\n        self.times = []\n\n    def median_time(self):\n        return statistics.median(self.times)\n\n    def on_train_batch_start(self, trainer, *args, **kwargs):\n        self.start.record()\n\n    def on_train_batch_end(self, trainer, *args, **kwargs):\n        # Exclude the first iteration to let the model warm up\n        if trainer.global_step > 1:\n            self.end.record()\n            torch.cuda.synchronize()\n            self.times.append(self.start.elapsed_time(self.end) / 1000)\n\n\nmodel = MyLightningModule()\n\n# Compile!\ncompiled_model = torch.compile(model)\n\n# Measure the median iteration time with uncompiled model\nbenchmark = Benchmark()\ntrainer = L.Trainer(accelerator=\"cuda\", devices=1, max_steps=10, callbacks=[benchmark])\ntrainer.fit(model)\neager_time = benchmark.median_time()\n\n# Measure the median iteration time with compiled model\nbenchmark = Benchmark()\ntrainer = L.Trainer(accelerator=\"cuda\", devices=1, max_steps=10, callbacks=[benchmark])\ntrainer.fit(compiled_model)\ncompile_time = benchmark.median_time()\n\n# Compare the speedup for the compiled execution\nspeedup = eager_time / compile_time\nprint(f\"Eager median time: {eager_time:.4f} seconds\")\nprint(f\"Compile median time: {compile_time:.4f} seconds\")\nprint(f\"Speedup: {speedup:.1f}x\")\n```\n\n----------------------------------------\n\nTITLE: Configuring GPU Usage in PyTorch Lightning Trainer\nDESCRIPTION: Demonstrates various ways to configure GPU usage in the PyTorch Lightning Trainer. This includes running on all available GPUs, a single GPU, multiple GPUs, or automatically choosing the number of devices.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/accelerators/gpu_basic.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# run on as many GPUs as available by default\ntrainer = Trainer(accelerator=\"auto\", devices=\"auto\", strategy=\"auto\")\n# equivalent to\ntrainer = Trainer()\n\n# run on one GPU\ntrainer = Trainer(accelerator=\"gpu\", devices=1)\n# run on multiple GPUs\ntrainer = Trainer(accelerator=\"gpu\", devices=8)\n# choose the number of devices automatically\ntrainer = Trainer(accelerator=\"gpu\", devices=\"auto\")\n```\n\n----------------------------------------\n\nTITLE: Proper Tensor Device Management in PyTorch Lightning\nDESCRIPTION: Demonstrates best practices for creating and managing tensors on correct devices to avoid unnecessary CPU to GPU transfers.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/speed.rst#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# bad\nt = torch.rand(2, 2).cuda()\n\n# good (self is LightningModule)\nt = torch.rand(2, 2, device=self.device)\n\n# bad\nself.t = torch.rand(2, 2, device=self.device)\n\n# good\nself.register_buffer(\"t\", torch.rand(2, 2))\n```\n\n----------------------------------------\n\nTITLE: Configuring Lightning Fabric with Auto-Detection\nDESCRIPTION: Example of properly configuring Lightning Fabric to automatically detect the available accelerator and devices. This allows the same code to run seamlessly in different environments, from local to multi-node clusters.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/multi_node/cloud.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# These are the defaults\nfabric = L.Fabric(accelerator=\"auto\", devices=\"auto\")\n\n# DON'T hardcode these, leave them default/auto\n# fabric = L.Fabric(accelerator=\"cpu\", devices=3)\n```\n\n----------------------------------------\n\nTITLE: Implementing Test Loop in PyTorch Lightning\nDESCRIPTION: Shows implementation of test_step method in a Lightning Module for model evaluation\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/evaluation_basic.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass LitAutoEncoder(L.LightningModule):\n    def training_step(self, batch, batch_idx):\n        ...\n\n    def test_step(self, batch, batch_idx):\n        # this is the test loop\n        x, _ = batch\n        x = x.view(x.size(0), -1)\n        z = self.encoder(x)\n        x_hat = self.decoder(z)\n        test_loss = F.mse_loss(x_hat, x)\n        self.log(\"test_loss\", test_loss)\n```\n\n----------------------------------------\n\nTITLE: Running Transformer Model Training with Lightning Fabric\nDESCRIPTION: A complete example showing how to train a Transformer model on the WikiText2 dataset using Lightning Fabric. The script demonstrates initialization, data loading, model setup, optimization, and training with distributed capabilities built in.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/multi_node/cloud.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\nimport torch\nimport torch.nn.functional as F\nfrom lightning.pytorch.demos import Transformer, WikiText2\nfrom torch.utils.data import DataLoader\n\n\ndef main():\n    L.seed_everything(42)\n\n    fabric = L.Fabric()\n    fabric.launch()\n\n    # Data\n    with fabric.rank_zero_first():\n        dataset = WikiText2()\n\n    train_dataloader = DataLoader(dataset, batch_size=20, shuffle=True)\n\n    # Model\n    model = Transformer(vocab_size=dataset.vocab_size)\n\n    # Optimizer\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n\n    model, optimizer = fabric.setup(model, optimizer)\n    train_dataloader = fabric.setup_dataloaders(train_dataloader)\n\n    for batch_idx, batch in enumerate(train_dataloader):\n        input, target = batch\n        output = model(input, target)\n        loss = F.nll_loss(output, target.view(-1))\n        fabric.backward(loss)\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if batch_idx % 10 == 0:\n            fabric.print(f\"iteration: {batch_idx} - loss {loss.item():.4f}\")\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Exporting PyTorch Lightning Models to ONNX Format\nDESCRIPTION: Example of exporting a trained PyTorch Lightning model to ONNX format for production deployment. ONNX enables interoperability with various ML frameworks and hardware accelerators.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/pytorch_lightning/README.md#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nautoencoder = LitAutoEncoder()\ninput_sample = torch.randn((1, 64))\nwith tempfile.NamedTemporaryFile(suffix=\".onnx\", delete=False) as tmpfile:\n    autoencoder.to_onnx(tmpfile.name, input_sample, export_params=True)\n```\n\n----------------------------------------\n\nTITLE: Implementing Predict DataLoader in Lightning DataModule\nDESCRIPTION: Shows how to implement predict_dataloader method for creating prediction data loaders with batch size 64.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/data/datamodule.rst#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\n\nclass MNISTDataModule(L.LightningDataModule):\n    def predict_dataloader(self):\n        return DataLoader(self.mnist_predict, batch_size=64)\n```\n\n----------------------------------------\n\nTITLE: Loading Raw PyTorch Checkpoints with Fabric\nDESCRIPTION: Load model or optimizer state from a raw PyTorch checkpoint not saved by Fabric. This is useful when working with pre-trained models or checkpoints from other frameworks.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_methods.rst#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nmodel = MyModel()\n\n# A model weights file saved by your friend who doesn't use Fabric\nfabric.load_raw(\"path/to/model.pt\", model)\n\n# Equivalent to this:\n# model.load_state_dict(torch.load(\"path/to/model.pt\"))\n```\n\n----------------------------------------\n\nTITLE: Implementing DeepSpeed ZeRO Stage 2 in PyTorch Lightning\nDESCRIPTION: Code example for setting up a Trainer with DeepSpeed ZeRO Stage 2, which partitions both optimizer states and gradients across GPUs for improved memory efficiency.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/deepspeed.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch import Trainer\n\nmodel = MyModel()\ntrainer = Trainer(accelerator=\"gpu\", devices=4, strategy=\"deepspeed_stage_2\", precision=16)\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Sharing Datasets Across Process Boundaries in PyTorch Lightning\nDESCRIPTION: This snippet demonstrates how to share datasets across process boundaries when using distributed training strategies like 'ddp_spawn'. It shows how to pre-load data in the DataModule's __init__ method to leverage automatic tensor sharing via shared memory.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/training_tricks.rst#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nclass MNISTDataModule(L.LightningDataModule):\n    def __init__(self, data_dir: str):\n        self.mnist = MNIST(data_dir, download=True, transform=T.ToTensor())\n\n    def train_loader(self):\n        return DataLoader(self.mnist, batch_size=128)\n\n\nmodel = Model(...)\ndatamodule = MNISTDataModule(\"data/MNIST\")\n\ntrainer = Trainer(accelerator=\"gpu\", devices=2, strategy=\"ddp_spawn\")\ntrainer.fit(model, datamodule)\n```\n\n----------------------------------------\n\nTITLE: Implementing Weight Sharing in PyTorch Lightning for TPU Training\nDESCRIPTION: Demonstrates how to implement weight sharing between neural network layers in a PyTorch Lightning module for TPU training. The example shows automatic weight tying after moving to XLA device, with shared weights between layer_1 and layer_3.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/accelerators/tpu_advanced.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.core.module import LightningModule\nfrom torch import nn\nfrom lightning.pytorch.trainer.trainer import Trainer\n\n\nclass WeightSharingModule(LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.layer_1 = nn.Linear(32, 10, bias=False)\n        self.layer_2 = nn.Linear(10, 32, bias=False)\n        self.layer_3 = nn.Linear(32, 10, bias=False)\n        # Lightning automatically ties these weights after moving to the XLA device,\n        # so all you need is to write the following just like on other accelerators.\n        self.layer_3.weight = self.layer_1.weight\n\n    def forward(self, x):\n        x = self.layer_1(x)\n        x = self.layer_2(x)\n        x = self.layer_3(x)\n        return x\n\n\nmodel = WeightSharingModule()\ntrainer = Trainer(max_epochs=1, accelerator=\"tpu\")\n```\n\n----------------------------------------\n\nTITLE: Adding Metrics to Progress Bar in PyTorch Lightning\nDESCRIPTION: Demonstrates how to add custom metrics to the training progress bar by using the log method with prog_bar=True parameter. This allows for real-time monitoring of important metrics during training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/extensions/logging.rst#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef training_step(self, batch, batch_idx):\n    self.log(\"my_loss\", loss, prog_bar=True)\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch Lightning Trainer for Multi-Node Training\nDESCRIPTION: This snippet demonstrates how to set up the PyTorch Lightning Trainer for distributed training across multiple nodes and GPUs. It uses the 'ddp' strategy for distributed data parallel training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/clouds/cluster_intermediate_1.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# train on 32 GPUs across 4 nodes\ntrainer = Trainer(accelerator=\"gpu\", devices=8, num_nodes=4, strategy=\"ddp\")\n```\n\n----------------------------------------\n\nTITLE: Testing a Pre-Trained Model in PyTorch Lightning\nDESCRIPTION: Illustrates how to load a pre-trained model from a checkpoint and test it using PyTorch Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/evaluation_intermediate.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel = MyLightningModule.load_from_checkpoint(\n    checkpoint_path=\"/path/to/pytorch_checkpoint.ckpt\",\n    hparams_file=\"/path/to/experiment/version/hparams.yaml\",\n    map_location=None,\n)\n\n# init trainer with whatever options\ntrainer = Trainer(...)\n\n# test (pass in the model)\ntrainer.test(model)\n```\n\n----------------------------------------\n\nTITLE: Using Batch Size Finder in PyTorch Lightning\nDESCRIPTION: Demonstrates how to use the Batch Size Finder feature in PyTorch Lightning to automatically scale the batch size. It shows examples of using both 'power' and 'binsearch' modes for scaling.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/training_tricks.rst#2025-04-23_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nfrom lightning.pytorch.tuner import Tuner\n\n# Create a tuner for the trainer\ntrainer = Trainer(...)\ntuner = Tuner(trainer)\n\n# Auto-scale batch size by growing it exponentially (default)\ntuner.scale_batch_size(model, mode=\"power\")\n\n# Auto-scale batch size with binary search\ntuner.scale_batch_size(model, mode=\"binsearch\")\n\n# Fit as normal with new batch size\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Compiling a LightningModule with torch.compile\nDESCRIPTION: Basic example showing how to apply torch.compile to a LightningModule before passing it to the Trainer. This demonstrates the recommended way to compile models in Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/compile.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport lightning as L\n\n# Define the model\nmodel = MyLightningModule()\n\n# Compile the model\nmodel = torch.compile(model)\n\n# Run with the Trainer\ntrainer = L.Trainer()\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Initializing Large Distributed Models with Model Parallelism\nDESCRIPTION: Demonstrates efficient initialization of large models using FSDP, Tensor Parallelism, or DeepSpeed strategies. This approach prevents CPU memory exhaustion and significantly speeds up initialization for large models by using meta-device placement.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/model_init.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Recommended for FSDP, TP and DeepSpeed\nwith fabric.init_module(empty_init=True):\n    model = GPT3()  # parameters are placed on the meta-device\n\nmodel = fabric.setup(model)  # parameters get sharded and initialized at once\n\n# Make sure to create the optimizer only after the model has been set up\noptimizer = torch.optim.Adam(model.parameters())\noptimizer = fabric.setup_optimizers(optimizer)\n```\n\n----------------------------------------\n\nTITLE: Executing Distributed Transformer Training Script\nDESCRIPTION: Command to run the distributed training implementation for the Transformer model. The script handles all the setup for distributed training, low-precision operations, and model parallelization.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/pytorch/fp8_distributed_transformer/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython train.py\n```\n\n----------------------------------------\n\nTITLE: Configuring TPU Training Options in PyTorch Lightning\nDESCRIPTION: Examples of different TPU training configurations using PyTorch Lightning's Trainer class, including options for single core, multiple cores, and automatic device selection.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/accelerators/tpu_basic.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# run on as many TPUs as available by default\ntrainer = Trainer(accelerator=\"auto\", devices=\"auto\", strategy=\"auto\")\n# equivalent to\ntrainer = Trainer()\n\n# run on one TPU core\ntrainer = Trainer(accelerator=\"tpu\", devices=1)\n# run on multiple TPU cores\ntrainer = Trainer(accelerator=\"tpu\", devices=8)\n# run on one specific TPU core: the 2nd core (index 1)\ntrainer = Trainer(accelerator=\"tpu\", devices=[1])\n# choose the number of cores automatically\ntrainer = Trainer(accelerator=\"tpu\", devices=\"auto\")\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Callback in PyTorch Lightning\nDESCRIPTION: This code snippet demonstrates how to create a custom callback class that prints messages at the start and end of training. It inherits from the Callback base class and overrides the on_train_start and on_train_end methods.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/extensions/callbacks.rst#2025-04-23_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom lightning.pytorch.callbacks import Callback\n\n\nclass MyPrintingCallback(Callback):\n    def on_train_start(self, trainer, pl_module):\n        print(\"Training is starting\")\n\n    def on_train_end(self, trainer, pl_module):\n        print(\"Training is ending\")\n\n\ntrainer = Trainer(callbacks=[MyPrintingCallback()])\n```\n\n----------------------------------------\n\nTITLE: Manually Saving and Loading Checkpoints in PyTorch Lightning\nDESCRIPTION: This code demonstrates how to manually save a checkpoint using the Trainer and load a model from a checkpoint using the LightningModule.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/checkpointing_intermediate.rst#2025-04-23_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nmodel = MyLightningModule(hparams)\ntrainer.fit(model)\ntrainer.save_checkpoint(\"example.ckpt\")\n\n# load the checkpoint later as normal\nnew_model = MyLightningModule.load_from_checkpoint(checkpoint_path=\"example.ckpt\")\n```\n\n----------------------------------------\n\nTITLE: Complete FSDP Training Example\nDESCRIPTION: Full implementation example showing FSDP training setup with a Transformer model including model initialization, optimization, and training loop.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/model_parallel/fsdp.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport lightning as L\nfrom lightning.fabric.strategies import FSDPStrategy\nfrom lightning.pytorch.demos import Transformer, WikiText2\n\nfabric = L.Fabric(accelerator=\"cuda\", devices=2, strategy=FSDPStrategy())\nfabric.launch()\n\nfabric.seed_everything(42)\n\nwith fabric.rank_zero_first():\n    dataset = WikiText2()\n\n# 1B parameters\nmodel = Transformer(vocab_size=dataset.vocab_size, nlayers=32, nhid=4096, ninp=1024, nhead=64)\n\nmodel = fabric.setup(model)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\noptimizer = fabric.setup_optimizers(optimizer)\n\n\nfor i in range(10):\n    input, target = fabric.to_device(dataset[i])\n    output = model(input.unsqueeze(0), target.unsqueeze(0))\n    loss = F.nll_loss(output, target.view(-1))\n    fabric.backward(loss)\n    optimizer.step()\n    optimizer.zero_grad()\n    fabric.print(loss.item())\n\nfabric.print(torch.cuda.memory_summary())\n```\n\n----------------------------------------\n\nTITLE: Complete FSDP Language Model Implementation\nDESCRIPTION: Full example showing a Language Model implementation with FSDP strategy, including model definition, data loading, and training setup.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/fsdp.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nimport lightning as L\nfrom lightning.pytorch.strategies import FSDPStrategy\nfrom lightning.pytorch.demos import Transformer, WikiText2\n\n\nclass LanguageModel(L.LightningModule):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.model = Transformer(  # 1B parameters\n            vocab_size=vocab_size,\n            nlayers=32,\n            nhid=4096,\n            ninp=1024,\n            nhead=64,\n        )\n\n    def training_step(self, batch):\n        input, target = batch\n        output = self.model(input, target)\n        loss = F.nll_loss(output, target.view(-1))\n        self.log(\"train_loss\", loss, prog_bar=True)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=0.1)\n\n\nL.seed_everything(42)\n\n# Data\ndataset = WikiText2()\ntrain_dataloader = DataLoader(dataset)\n\n# Model\nmodel = LanguageModel(vocab_size=dataset.vocab_size)\n\n# Trainer\ntrainer = L.Trainer(accelerator=\"cuda\", devices=2, strategy=FSDPStrategy())\ntrainer.fit(model, train_dataloader)\ntrainer.print(torch.cuda.memory_summary())\n```\n\n----------------------------------------\n\nTITLE: Combining torch.compile with FP8 and Model Parallelism\nDESCRIPTION: Advanced example demonstrating how to apply multiple model transformations in the correct sequence: FP8 quantization, distributed sharding, and finally compilation.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/compile.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom lightning.pytorch.demos import Transformer\nfrom lightning.pytorch.strategies.model_parallel import ModelParallelStrategy\nfrom torch.distributed._composable.fsdp.fully_shard import fully_shard\nfrom torch.distributed.device_mesh import DeviceMesh\nfrom torchao.float8 import Float8LinearConfig, convert_to_float8_training\n\nclass LanguageModel(L.LightningModule):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.model = None\n\n    def configure_model(self):\n        if self.model is not None:\n            return\n\n        with torch.device(\"meta\"):\n            model = Transformer(\n                vocab_size=self.vocab_size,\n                nlayers=16,\n                nhid=4096,\n                ninp=1024,\n                nhead=32,\n            )\n\n        float8_config = Float8LinearConfig(\n            pad_inner_dim=True,\n        )\n\n        def module_filter_fn(mod: torch.nn.Module, fqn: str):\n            return fqn != \"decoder\"\n\n        convert_to_float8_training(model, config=float8_config, module_filter_fn=module_filter_fn)\n\n        for module in model.modules():\n            if isinstance(module, (nn.TransformerEncoderLayer, nn.TransformerDecoderLayer)):\n                fully_shard(module, mesh=self.device_mesh)\n\n        fully_shard(model, mesh=self.device_mesh)\n\n        self.model = torch.compile(model)\n```\n\n----------------------------------------\n\nTITLE: NLP Task Example in LightningDataModule\nDESCRIPTION: Illustrates how to handle an NLP task within a LightningDataModule, including tokenization in prepare_data and loading the processed data in setup.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/data/datamodule.rst#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nclass LitDataModule(L.LightningDataModule):\n    def prepare_data(self):\n        dataset = load_Dataset(...)\n        train_dataset = ...\n        val_dataset = ...\n        # tokenize\n        # save it to disk\n\n    def setup(self, stage):\n        # load it back here\n        dataset = load_dataset_from_disk(...)\n```\n\n----------------------------------------\n\nTITLE: Loading Remote Checkpoints for Training\nDESCRIPTION: Example of resuming model training using a checkpoint stored in a remote filesystem (S3).\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/remote_fs.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntrainer = Trainer(default_root_dir=tmpdir, max_steps=3)\ntrainer.fit(model, ckpt_path=\"s3://my_bucket/ckpts/classifier.ckpt\")\n```\n\n----------------------------------------\n\nTITLE: Controlling Logging Frequency\nDESCRIPTION: Example of reducing logging overhead by controlling the frequency of logging operations.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/logging.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfor iteration in range(num_iterations):\n    if iteration % log_every_n_steps == 0:\n        value = ...\n        fabric.log(\"some_value\", value)\n```\n\n----------------------------------------\n\nTITLE: Initializing Fabric for Simple Multi-Process Launch - Python\nDESCRIPTION: Basic setup for launching Fabric across multiple processes on a single machine using the launch() method. Configures the number of devices and other parameters.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/launch.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# train.py\n...\n\n# Configure accelerator, devices, num_nodes, etc.\nfabric = Fabric(devices=4, ...)\n\n# This launches itself into multiple processes\nfabric.launch()\n```\n\n----------------------------------------\n\nTITLE: Creating and Registering Callbacks in Fabric\nDESCRIPTION: Example of creating a custom callback class and registering it with Fabric to inject custom logic at specific points in the training loop, such as at the end of an epoch.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_args.rst#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass MyCallback:\n    def on_train_epoch_end(self, results):\n        ...\n```\n\n----------------------------------------\n\nTITLE: Configuring DDP Training with PyTorch Lightning\nDESCRIPTION: Sets up distributed data parallel training across multiple GPUs and nodes using PyTorch Lightning's Trainer class.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/clouds/cluster_advanced.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# train on 32 GPUs across 4 nodes\ntrainer = Trainer(accelerator=\"gpu\", devices=8, num_nodes=4, strategy=\"ddp\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-GPU Training with PyTorch Lightning\nDESCRIPTION: Demonstrates how to set up PyTorch Lightning for training on multiple GPUs or nodes without changing the model code. It shows configurations for 8 GPUs and 256 GPUs across multiple nodes.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/README.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# 8 GPUs\n# no code changes needed\ntrainer = Trainer(accelerator=\"gpu\", devices=8)\n\n# 256 GPUs\ntrainer = Trainer(accelerator=\"gpu\", devices=8, num_nodes=32)\n```\n\n----------------------------------------\n\nTITLE: Implementing Alternating Optimizer Steps for GANs in PyTorch Lightning\nDESCRIPTION: This code snippet shows how to implement an alternating schedule for optimizer steps, typically used in GANs. It updates the discriminator every other step and the generator every step.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/model/manual_optimization.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef training_step(self, batch, batch_idx):\n    g_opt, d_opt = self.optimizers()\n    ...\n\n    # update discriminator every other step\n    d_opt.zero_grad()\n    self.manual_backward(errD)\n    if (batch_idx + 1) % 2 == 0:\n        d_opt.step()\n\n    ...\n\n    # update generator every step\n    g_opt.zero_grad()\n    self.manual_backward(errG)\n    g_opt.step()\n```\n\n----------------------------------------\n\nTITLE: Implementing EarlyStopping Callback in PyTorch Lightning\nDESCRIPTION: This snippet demonstrates how to implement the EarlyStopping callback in a PyTorch Lightning model. It shows how to log a metric in the validation step and set up the EarlyStopping callback to monitor that metric.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/early_stopping.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.callbacks.early_stopping import EarlyStopping\n\n\nclass LitModel(LightningModule):\n    def validation_step(self, batch, batch_idx):\n        loss = ...\n        self.log(\"val_loss\", loss)\n\n\nmodel = LitModel()\ntrainer = Trainer(callbacks=[EarlyStopping(monitor=\"val_loss\", mode=\"min\")])\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Accessing Trainer State in PyTorch Lightning\nDESCRIPTION: Shows how to access the current state of the Trainer, including the function being executed, the stage of execution, and the status.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_40\n\nLANGUAGE: python\nCODE:\n```\n# fn in (\"fit\", \"validate\", \"test\", \"predict\")\ntrainer.state.fn\n# status in (\"initializing\", \"running\", \"finished\", \"interrupted\")\ntrainer.state.status\n# stage in (\"train\", \"sanity_check\", \"validate\", \"test\", \"predict\")\ntrainer.state.stage\n```\n\n----------------------------------------\n\nTITLE: Implementing Training and Testing Function in PyTorch Lightning\nDESCRIPTION: This function sets up data loaders, initializes the BoringModel, configures a PyTorch Lightning Trainer, and runs the fit and test processes. It uses the previously defined RandomDataset class.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/pytorch/bug_report/bug_report_model.ipynb#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\ndef run():\n    train_data = DataLoader(RandomDataset(32, 64), batch_size=2)\n    val_data = DataLoader(RandomDataset(32, 64), batch_size=2)\n    test_data = DataLoader(RandomDataset(32, 64), batch_size=2)\n\n    model = BoringModel()\n    trainer = Trainer(\n        default_root_dir=os.getcwd(),\n        limit_train_batches=1,\n        limit_val_batches=1,\n        limit_test_batches=1,\n        num_sanity_val_steps=0,\n        max_epochs=1,\n        enable_model_summary=False,\n    )\n    trainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\n    trainer.test(model, dataloaders=test_data)\n```\n\n----------------------------------------\n\nTITLE: FSDP Layer Wrapping Policy Configuration\nDESCRIPTION: Configuration of FSDP wrapping policy to specify which model layers should be managed by FSDP for efficient memory sharding.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/model_parallel/fsdp.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npolicy = {nn.TransformerEncoderLayer, nn.TransformerDecoderLayer}\n\nstrategy = FSDPStrategy(auto_wrap_policy=policy)\n\nfabric = L.Fabric(..., strategy=strategy)\n```\n\n----------------------------------------\n\nTITLE: Configuring Training Logic in Lightning\nDESCRIPTION: Shows how to implement the training_step method in a LightningModule to define the training logic for a batch of data.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/starter/converting.rst#2025-04-23_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nclass LitModel(L.LightningModule):\n    def __init__(self, encoder):\n        super().__init__()\n        self.encoder = encoder\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.encoder(x)\n        loss = F.cross_entropy(y_hat, y)\n        return loss\n```\n\n----------------------------------------\n\nTITLE: Basic FSDP Trainer Configuration in PyTorch Lightning\nDESCRIPTION: Shows how to enable FSDP strategy in the Lightning Trainer with minimal configuration.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/fsdp.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntrainer = L.Trainer(accelerator=\"cuda\", devices=2, strategy=\"fsdp\")\n```\n\n----------------------------------------\n\nTITLE: Converting Distributed Checkpoints to Single-File Format\nDESCRIPTION: Demonstrates how to convert a distributed checkpoint to a regular single-file checkpoint using Fabric's consolidate utility. Useful for deployment or evaluation without FSDP.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/checkpoint/distributed_checkpoint.rst#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nfabric consolidate path/to/my/checkpoint\n```\n\n----------------------------------------\n\nTITLE: Enabling Stochastic Weight Averaging in PyTorch Lightning\nDESCRIPTION: Shows how to enable Stochastic Weight Averaging (SWA) in PyTorch Lightning using a callback. SWA can improve model generalization at virtually no additional cost.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/training_tricks.rst#2025-04-23_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# Enable Stochastic Weight Averaging using the callback\ntrainer = Trainer(callbacks=[StochasticWeightAveraging(swa_lrs=1e-2)])\n```\n\n----------------------------------------\n\nTITLE: Initializing Trainer for Multi-GPU Training with DDP Spawn in PyTorch Lightning\nDESCRIPTION: This snippet shows how to initialize a PyTorch Lightning Trainer for distributed training on multiple GPUs using the DDP Spawn strategy. It's recommended for debugging purposes or when converting existing code that relies on spawn.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/accelerators/gpu_intermediate.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# train on 8 GPUs (same machine (ie: node))\ntrainer = Trainer(accelerator=\"gpu\", devices=8, strategy=\"ddp_spawn\")\n```\n\n----------------------------------------\n\nTITLE: Monitoring Gradient Norms in PyTorch Lightning\nDESCRIPTION: Shows how to implement gradient norm monitoring in a LightningModule to detect potential gradient explosion issues. Uses the grad_norm utility to compute and log 2-norm for each layer.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/debug/debugging_intermediate.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.utilities import grad_norm\n\n\ndef on_before_optimizer_step(self, optimizer):\n    # Compute the 2-norm for each layer\n    # If using mixed precision, the gradients are already unscaled here\n    norms = grad_norm(self.layer, norm_type=2)\n    self.log_dict(norms)\n```\n\n----------------------------------------\n\nTITLE: Controlling Precision Application in PyTorch Lightning\nDESCRIPTION: Illustrates how to control where precision gets applied in the model, both within the forward pass and in external operations using the autocast context manager.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/precision.rst#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfabric = Fabric(precision=\"bf16-mixed\")\n\nmodel = ...\noptimizer = ...\n\n# Here, Fabric sets up the `model.forward` for precision auto-casting\nmodel, optimizer = fabric.setup(model, optimizer)\n\n# Precision casting gets handled in your forward, no code changes required\noutput = model.forward(input)\n\n# Precision does NOT get applied here (only in forward)\nloss = loss_function(output, target)\n\n# Precision now gets also handled in this part of the code:\nwith fabric.autocast():\n    loss = loss_function(output, target)\n```\n\n----------------------------------------\n\nTITLE: Basic Trainer Usage in Python\nDESCRIPTION: Shows the basic initialization and usage of PyTorch Lightning Trainer with a model and dataloaders.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmodel = MyLightningModule()\n\ntrainer = Trainer()\ntrainer.fit(model, train_dataloader, val_dataloader)\n```\n\n----------------------------------------\n\nTITLE: Configuring Devices for Training in Fabric\nDESCRIPTION: Examples of different ways to specify devices for training including using a specific number of devices, selecting specific device indices, or using all available devices.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_args.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# default used by Fabric, i.e., use the CPU\nfabric = Fabric(devices=None)\n\n# equivalent\nfabric = Fabric(devices=0)\n\n# int: run on two GPUs\nfabric = Fabric(devices=2, accelerator=\"gpu\")\n\n# list: run on the 2nd (idx 1) and 5th (idx 4) GPUs (by bus ordering)\nfabric = Fabric(devices=[1, 4], accelerator=\"gpu\")\nfabric = Fabric(devices=\"1, 4\", accelerator=\"gpu\")  # equivalent\n\n# -1: run on all GPUs\nfabric = Fabric(devices=-1, accelerator=\"gpu\")\nfabric = Fabric(devices=\"-1\", accelerator=\"gpu\")  # equivalent\n```\n\n----------------------------------------\n\nTITLE: Creating a Fabric Object\nDESCRIPTION: Initialize a Fabric object at the beginning of your training code. This is the first step in converting PyTorch code to Fabric.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/convert.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.fabric import Fabric\n\nfabric = Fabric()\n```\n\n----------------------------------------\n\nTITLE: Configuring 16-bit Mixed Precision in PyTorch Lightning\nDESCRIPTION: Sets up the Trainer to use 16-bit mixed precision, which can speed up training and inference, especially on Tensor Core GPUs.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/precision_basic.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nTrainer(precision=\"16-mixed\")\n```\n\n----------------------------------------\n\nTITLE: Clipping Gradients with Fabric\nDESCRIPTION: Clip gradients to prevent exploding gradients during training. This method supports clipping by value or by norm and works correctly with any precision or strategy being used.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_methods.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Clip gradients to a max value of +/- 0.5\nfabric.clip_gradients(model, optimizer, clip_val=0.5)\n\n# Clip gradients such that their total norm is no bigger than 2.0\nfabric.clip_gradients(model, optimizer, max_norm=2.0)\n\n# By default, clipping by norm uses the 2-norm\nfabric.clip_gradients(model, optimizer, max_norm=2.0, norm_type=2)\n\n# You can also choose the infinity-norm, which clips the largest\n# element among all\nfabric.clip_gradients(model, optimizer, max_norm=2.0, norm_type=\"inf\")\n```\n\n----------------------------------------\n\nTITLE: Training Termination with Minimum Epochs\nDESCRIPTION: Shows how to implement conditional training termination that respects a minimum number of epochs. Even though should_stop is set at epoch 2, training continues until 5 epochs are completed due to min_epochs setting.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_42\n\nLANGUAGE: python\nCODE:\n```\nclass LitModel(LightningModule):\n    def training_step(self, *args, **kwargs):\n        if self.current_epoch == 2:\n            self.trainer.should_stop = True\n\n\ntrainer = Trainer(min_epochs=5, max_epochs=100)\nmodel = LitModel()\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Using StreamingDataset with PyTorch DataLoader\nDESCRIPTION: Implementation example showing how to use StreamingDataset to load data from cloud storage and create a PyTorch DataLoader.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/data/README.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.data import StreamingDataset\nfrom torch.utils.data import DataLoader\n\n# Remote path where full dataset is persistently stored\ninput_dir = 's3://pl-flash-data/my_dataset'\n\n# Create streaming dataset\ndataset = StreamingDataset(input_dir, shuffle=True)\n\n# Check any elements\nsample = dataset[50]\nimg = sample['image']\ncls = sample['class']\n\n# Create PyTorch DataLoader\ndataloader = DataLoader(dataset)\n```\n\n----------------------------------------\n\nTITLE: Initializing Fabric for Distributed Processing\nDESCRIPTION: Demonstrates how to launch multiple distributed processes using Fabric by specifying the number of devices and nodes.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/distributed_communication.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.fabric import Fabric\n\n# Devices and num_nodes determine how many processes there are\nfabric = Fabric(devices=2, num_nodes=3)\nfabric.launch()\n```\n\n----------------------------------------\n\nTITLE: Running Transformer Training with PyTorch Lightning Fabric on Different Hardware\nDESCRIPTION: Commands to run the Transformer model training script using PyTorch Lightning's Fabric on CPU, GPU (CUDA or M1 Mac), and multiple GPUs. The script is named 'train.py' and uses the Fabric CLI for execution.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/fabric/language_model/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# CPU\nfabric run --accelerator=cpu train.py\n\n# GPU (CUDA or M1 Mac)\nfabric run --accelerator=gpu train.py\n\n# Multiple GPUs\nfabric run --accelerator=gpu --devices=4 train.py\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSpeed via Environment Variables in PyTorch Lightning\nDESCRIPTION: Shows how to specify DeepSpeed configuration through environment variables when launching a PyTorch Lightning training script. This method is useful for changing configurations without modifying code.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/deepspeed.rst#2025-04-23_snippet_18\n\nLANGUAGE: bash\nCODE:\n```\nPL_DEEPSPEED_CONFIG_PATH=/path/to/deepspeed_config.json python train.py --strategy deepspeed\n```\n\n----------------------------------------\n\nTITLE: Initializing Advanced Profiler in PyTorch Lightning\nDESCRIPTION: Implements detailed function-level profiling using Python's cProfiler to analyze time spent within individual functions.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/tuning/profiler_basic.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntrainer = Trainer(profiler=\"advanced\")\n```\n\n----------------------------------------\n\nTITLE: Modifying Logging Frequency in PyTorch Lightning Trainer\nDESCRIPTION: This code snippet shows how to change the logging frequency in PyTorch Lightning by setting the log_every_n_steps parameter in the Trainer constructor.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/logging_advanced.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nk = 10\ntrainer = Trainer(log_every_n_steps=k)\n```\n\n----------------------------------------\n\nTITLE: Initializing Fabric with Default Settings in Python\nDESCRIPTION: This snippet shows how to initialize Fabric with default settings, which automatically selects the best available accelerator and device configuration.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/accelerators.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Default settings\nfabric = Fabric(accelerator=\"auto\", devices=\"auto\", strategy=\"auto\")\n\n# Same as\nfabric = Fabric()\n```\n\n----------------------------------------\n\nTITLE: Implementing Predict Step in LightningModule\nDESCRIPTION: Shows how to implement a basic predict_step method in a LightningModule class for streamlined prediction pipeline.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/deploy/production_basic.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass MyModel(LightningModule):\n    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n        return self(batch)\n```\n\n----------------------------------------\n\nTITLE: Initializing Sharded Model with DeepSpeed in PyTorch Lightning\nDESCRIPTION: Demonstrates how to use DeepSpeed Stage 3 with PyTorch Lightning to instantly shard large models across multiple GPUs. The example shows model definition, optimizer configuration, and trainer setup.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/deepspeed.rst#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nimport torch.nn as nn\nfrom lightning.pytorch import Trainer\nfrom deepspeed.ops.adam import FusedAdam\n\n\nclass MyModel(LightningModule):\n    ...\n\n    def configure_model(self):\n        # Created within sharded model context, modules are instantly sharded across processes\n        # as soon as they are made.\n        self.block = nn.Sequential(nn.Linear(32, 32), nn.ReLU())\n\n    def configure_optimizers(self):\n        return FusedAdam(self.parameters())\n\n\nmodel = MyModel()\ntrainer = Trainer(accelerator=\"gpu\", devices=4, strategy=\"deepspeed_stage_3\", precision=16)\ntrainer.fit(model)\n\ntrainer.test()\ntrainer.predict()\n```\n\n----------------------------------------\n\nTITLE: Retrieving Hyperparameters from Lightning Checkpoints\nDESCRIPTION: Shows how to load a Lightning checkpoint and extract the hyperparameters that were used during training. Lightning automatically stores hyperparameters in checkpoints under the 'hyper_parameters' key.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/extensions/logging.rst#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nlightning_checkpoint = torch.load(filepath, map_location=lambda storage, loc: storage)\nhyperparams = lightning_checkpoint[\"hyper_parameters\"]\n```\n\n----------------------------------------\n\nTITLE: Configuring TPU Cores in PyTorch Lightning Trainer\nDESCRIPTION: Shows how to configure a PyTorch Lightning Trainer to use TPUs. This snippet demonstrates setting up 8 TPU cores for training a model.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/accelerators/tpu_intermediate.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\n\nmy_model = MyLightningModule()\ntrainer = L.Trainer(accelerator=\"tpu\", devices=8)\ntrainer.fit(my_model)\n```\n\n----------------------------------------\n\nTITLE: Setting up a model with Fabric wrapper\nDESCRIPTION: Demonstrates how Fabric automatically wraps models in a FabricModule when using setup(), and shows how to inspect the wrapped model type.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/wrappers.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport lightning as L\n\nfabric = L.Fabric()\nmodel = torch.nn.Linear(10, 2)\nmodel = fabric.setup(model)\n\nprint(type(model))  # <class 'lightning.fabric.wrappers._FabricModule'>\n```\n\n----------------------------------------\n\nTITLE: Implementing 2D Parallelization Function in PyTorch\nDESCRIPTION: Function to apply both tensor parallelism and FSDP to a model using PyTorch's distributed tensor APIs and FSDP2 APIs.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/model_parallel/tp_fsdp.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.distributed.tensor.parallel import ColwiseParallel, RowwiseParallel\nfrom torch.distributed.tensor.parallel import parallelize_module\nfrom torch.distributed._composable.fsdp.fully_shard import fully_shard\n\ndef parallelize_feedforward(model, device_mesh):\n    tp_mesh = device_mesh[\"tensor_parallel\"]\n    dp_mesh = device_mesh[\"data_parallel\"]\n\n    if tp_mesh.size() > 1:\n        plan = {\n            \"w1\": ColwiseParallel(),\n            \"w2\": RowwiseParallel(),\n            \"w3\": ColwiseParallel(),\n        }\n        parallelize_module(model, tp_mesh, plan)\n\n    if dp_mesh.size() > 1:\n        fully_shard(model.w1, mesh=dp_mesh)\n        fully_shard(model.w2, mesh=dp_mesh)\n        fully_shard(model.w3, mesh=dp_mesh)\n        fully_shard(model, mesh=dp_mesh)\n\n    return model\n```\n\n----------------------------------------\n\nTITLE: Configuring Weights and Biases Logger in PyTorch Lightning\nDESCRIPTION: Demonstrates how to configure the Weights and Biases logger and pass it to the PyTorch Lightning Trainer. Also shows how to access the logger within a LightningModule and log images.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/supported_exp_managers.rst#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.loggers import WandbLogger\n\nwandb_logger = WandbLogger(project=\"MNIST\", log_model=\"all\")\ntrainer = Trainer(logger=wandb_logger)\n\n# log gradients and model topology\nwandb_logger.watch(model)\n```\n\nLANGUAGE: python\nCODE:\n```\nclass MyModule(LightningModule):\n    def any_lightning_module_function_or_hook(self):\n        wandb_logger = self.logger.experiment\n        fake_images = torch.Tensor(32, 3, 28, 28)\n\n        # Option 1\n        wandb_logger.log({\"generated_images\": [wandb.Image(fake_images, caption=\"...\")]})\n\n        # Option 2 for specifically logging images\n        wandb_logger.log_image(key=\"generated_images\", images=[fake_images])\n```\n\n----------------------------------------\n\nTITLE: Customizing EarlyStopping Callback Parameters in PyTorch Lightning\nDESCRIPTION: This code snippet shows how to customize the EarlyStopping callback by adjusting its parameters. It sets the monitored metric, minimum delta, patience, verbosity, and mode.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/early_stopping.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nearly_stop_callback = EarlyStopping(monitor=\"val_accuracy\", min_delta=0.00, patience=3, verbose=False, mode=\"max\")\ntrainer = Trainer(callbacks=[early_stop_callback])\n```\n\n----------------------------------------\n\nTITLE: Enabling CUDA Graphs Optimization\nDESCRIPTION: Demonstrates how to enable CUDA Graphs optimization for static models, which can significantly improve performance by recording and replaying computations. Requires consistent input shapes and operations.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/compile.rst#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Enable CUDA Graphs\ncompiled_model = torch.compile(model, mode=\"reduce-overhead\")\n\n# This does the same\ncompiled_model = torch.compile(model, options={\"triton.cudagraphs\": True})\n```\n\n----------------------------------------\n\nTITLE: Setting Up One Model with Multiple Optimizers in Fabric\nDESCRIPTION: Shows how to configure a single model with multiple optimizers in Lightning Fabric. This approach is useful when different parts of the model require different optimization strategies or learning rates.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/multiple_setup.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Instantiate model and optimizers\nmodel = LitModel()\noptimizer1 = torch.optim.SGD(model.layer1.parameters(), lr=0.003)\noptimizer2 = torch.optim.SGD(model.layer2.parameters(), lr=0.01)\n\n# Set up the model and optimizers together\nmodel, optimizer1, optimizer2 = fabric.setup(model, optimizer1, optimizer2)\n```\n\n----------------------------------------\n\nTITLE: Customizing Learning Rate Finder Callback in PyTorch Lightning\nDESCRIPTION: This snippet shows how to create a custom LearningRateFinder callback that runs at specific epochs during training. This is useful for fine-tuning models where you want to find optimal learning rates at different stages of training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/training_tricks.rst#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.callbacks import LearningRateFinder\n\n\nclass FineTuneLearningRateFinder(LearningRateFinder):\n    def __init__(self, milestones, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.milestones = milestones\n\n    def on_fit_start(self, *args, **kwargs):\n        return\n\n    def on_train_epoch_start(self, trainer, pl_module):\n        if trainer.current_epoch in self.milestones or trainer.current_epoch == 0:\n            self.lr_find(trainer, pl_module)\n\n\ntrainer = Trainer(callbacks=[FineTuneLearningRateFinder(milestones=(5, 10))])\ntrainer.fit(...)\n```\n\n----------------------------------------\n\nTITLE: Testing Model with PyTorch Lightning Trainer\nDESCRIPTION: Demonstrates different ways to perform model testing using PyTorch Lightning's Trainer, including testing after training and testing with pretrained models.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/lightning_module.rst#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nmodel = LightningTransformer(vocab_size=dataset.vocab_size)\ndataloader = DataLoader(dataset)\ntrainer = L.Trainer()\ntrainer.fit(model=model, train_dataloaders=dataloader)\n\n# automatically loads the best weights for you\ntrainer.test(model)\n```\n\nLANGUAGE: python\nCODE:\n```\n# call after training\ntrainer = L.Trainer()\ntrainer.fit(model=model, train_dataloaders=dataloader)\n\n# automatically auto-loads the best weights from the previous run\ntrainer.test(dataloaders=test_dataloaders)\n\n# or call with pretrained model\nmodel = LightningTransformer.load_from_checkpoint(PATH)\ndataset = WikiText2()\ntest_dataloader = DataLoader(dataset)\ntrainer = L.Trainer()\ntrainer.test(model, dataloaders=test_dataloader)\n```\n\n----------------------------------------\n\nTITLE: Logging to Custom Cloud Filesystem in PyTorch Lightning\nDESCRIPTION: This example demonstrates how to configure PyTorch Lightning to log data to a custom cloud filesystem, such as Amazon S3, using the TensorBoardLogger.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/logging_advanced.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.loggers import TensorBoardLogger\n\nlogger = TensorBoardLogger(save_dir=\"s3://my_bucket/logs/\")\n\ntrainer = Trainer(logger=logger)\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Using Float8 Mixed Precision with Nvidia's TransformerEngine in PyTorch Lightning\nDESCRIPTION: These snippets show how to enable Float8 mixed precision using Nvidia's TransformerEngine in PyTorch Lightning. It includes options for different weight precisions and customizing the FP8 recipe.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/precision_intermediate.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Select 8bit mixed precision via TransformerEngine, with model weights in bfloat16\ntrainer = Trainer(precision=\"transformer-engine\")\n\n# Select 8bit mixed precision via TransformerEngine, with model weights in float16\ntrainer = Trainer(precision=\"transformer-engine-float16\")\n\n# Customize the fp8 recipe or set a different base precision:\nfrom lightning.trainer.plugins import TransformerEnginePrecision\n\nrecipe = {\"fp8_format\": \"HYBRID\", \"amax_history_len\": 16, \"amax_compute_algo\": \"max\"}\nprecision = TransformerEnginePrecision(weights_dtype=torch.bfloat16, recipe=recipe)\ntrainer = Trainer(plugins=precision)\n```\n\n----------------------------------------\n\nTITLE: Logging Various Artifacts in PyTorch Lightning Training Step\nDESCRIPTION: This code demonstrates how to access the logger's experiment object within the training_step method to log different types of artifacts such as images, histograms, and figures using TensorBoard.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/logging_intermediate.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef training_step(self):\n    tensorboard = self.logger.experiment\n    tensorboard.add_image()\n    tensorboard.add_histogram(...)\n    tensorboard.add_figure(...)\n```\n\n----------------------------------------\n\nTITLE: Handling Tensor Device Placement in LightningModule\nDESCRIPTION: This code shows the recommended way to handle tensor device placement in Lightning, avoiding manual .cuda() or .to(device) calls and letting Lightning handle it automatically.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/lightning_module.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# don't do in Lightning\nx = torch.Tensor(2, 3)\nx = x.cuda()\nx = x.to(device)\n\n# do this instead\nx = x  # leave it alone!\n\n# or to init a new tensor\nnew_x = torch.Tensor(2, 3)\nnew_x = new_x.to(x)\n```\n\n----------------------------------------\n\nTITLE: FSDP Sharding Strategy Configuration\nDESCRIPTION: Shows various sharding strategy options for FSDP to balance memory usage and training speed.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/fsdp.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nstrategy = FSDPStrategy(\n    # Default: Shard weights, gradients, optimizer state (1 + 2 + 3)\n    sharding_strategy=\"FULL_SHARD\",\n    # Shard gradients, optimizer state (2 + 3)\n    sharding_strategy=\"SHARD_GRAD_OP\",\n    # Full-shard within a machine, replicate across machines\n    sharding_strategy=\"HYBRID_SHARD\",\n    # Don't shard anything (similar to DDP)\n    sharding_strategy=\"NO_SHARD\",\n)\ntrainer = L.Trainer(..., strategy=strategy)\n```\n\n----------------------------------------\n\nTITLE: Configuring DDP Strategy with Custom Process Group Backend in PyTorch Lightning\nDESCRIPTION: This snippet shows how to explicitly specify the process group backend when initializing the DDP strategy in PyTorch Lightning. It demonstrates setting the backend to 'nccl' and configuring the Trainer with the custom strategy.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/accelerators/gpu_intermediate.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.strategies import DDPStrategy\n\n# Explicitly specify the process group backend if you choose to\nddp = DDPStrategy(process_group_backend=\"nccl\")\n\n# Configure the strategy on the Trainer\ntrainer = Trainer(strategy=ddp, accelerator=\"gpu\", devices=8)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Models, Optimizers, and Dataloaders with Fabric\nDESCRIPTION: Prepare models, optimizers, and dataloaders for distributed training by wrapping them with Fabric's setup methods.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/convert.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel, optimizer = fabric.setup(model, optimizer)\ndataloader = fabric.setup_dataloaders(dataloader)\n```\n\n----------------------------------------\n\nTITLE: Saving PyTorch Lightning Checkpoints to Cloud Storage\nDESCRIPTION: Demonstrates how to configure a PyTorch Lightning Trainer to save checkpoints to cloud storage by specifying a protocol prefix (e.g. 's3://') in the default_root_dir parameter.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/checkpointing_advanced.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# `default_root_dir` is the default path used for logs and checkpoints\ntrainer = Trainer(default_root_dir=\"s3://my_bucket/data/\")\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Working with CombinedLoader in PyTorch Lightning\nDESCRIPTION: This snippet demonstrates how to use CombinedLoader in PyTorch Lightning, including accessing the original iterables, flattening the loaders, and replacing them with updated versions.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/data/access.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.utilities import CombinedLoader\n\niterables = {\"dl1\": dl1, \"dl2\": dl2}\ncombined_loader = CombinedLoader(iterables)\n# access the original iterables\nassert combined_loader.iterables is iterables\n# the `.flattened` property can be convenient\nassert combined_loader.flattened == [dl1, dl2]\n# for example, to do a simple loop\nupdated = []\nfor dl in combined_loader.flattened:\n    new_dl = apply_some_transformation_to(dl)\n    updated.append(new_dl)\n# it also allows you to easily replace the dataloaders\ncombined_loader.flattened = updated\n```\n\n----------------------------------------\n\nTITLE: Quantizing PyTorch Lightning Model with Intel Neural Compressor\nDESCRIPTION: Python code to quantize a PyTorch Lightning model using Intel Neural Compressor's fit function and save the quantized model.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/post_training_quantization.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom neural_compressor.quantization import fit\n\nq_model = fit(model=model.model, conf=conf, calib_dataloader=val_dataloader(), eval_func=eval_func)\n\nq_model.save(\"./saved_model/\")\n```\n\n----------------------------------------\n\nTITLE: Enabling Dynamic Compilation in PyTorch\nDESCRIPTION: Shows how to enable dynamic compilation for models with varying input shapes to avoid frequent recompilation overhead. This is specifically for PyTorch versions before 2.2.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/compile.rst#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# On PyTorch < 2.2\nmodel = torch.compile(model, dynamic=True)\n```\n\n----------------------------------------\n\nTITLE: Using Built-in Training Strategies in PyTorch Lightning\nDESCRIPTION: Examples of initializing PyTorch Lightning Trainer with different built-in training strategies including DDP, DeepSpeed, and TPU. Shows how to configure strategies with different accelerators and device configurations.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/strategy_registry.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Training with the DDP Strategy\ntrainer = Trainer(strategy=\"ddp\", accelerator=\"gpu\", devices=4)\n\n# Training with DeepSpeed ZeRO Stage 3 and CPU Offload\ntrainer = Trainer(strategy=\"deepspeed_stage_3_offload\", accelerator=\"gpu\", devices=3)\n\n# Training with the TPU Spawn Strategy with `debug` as True\ntrainer = Trainer(strategy=\"xla_debug\", accelerator=\"tpu\", devices=8)\n```\n\n----------------------------------------\n\nTITLE: Basic FSDP Setup with Lightning Fabric\nDESCRIPTION: Basic configuration to enable FSDP training using Lightning Fabric with CUDA acceleration across multiple devices.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/model_parallel/fsdp.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfabric = L.Fabric(accelerator=\"cuda\", devices=2, strategy=\"fsdp\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Validation DataLoader in Lightning DataModule\nDESCRIPTION: Shows implementation of val_dataloader method to create validation data loaders with batch size 64.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/data/datamodule.rst#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\n\nclass MNISTDataModule(L.LightningDataModule):\n    def val_dataloader(self):\n        return DataLoader(self.mnist_val, batch_size=64)\n```\n\n----------------------------------------\n\nTITLE: Metric Logging in PyTorch Lightning Fabric\nDESCRIPTION: Shows how to log scalar metrics using Fabric's logging methods. Includes examples of both single metric logging and batch metric logging with dictionaries.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_methods.rst#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# Set the logger in Fabric\nfabric = Fabric(loggers=TensorBoardLogger(...))\n\n# Anywhere in your training loop or model:\nfabric.log(\"loss\", loss)\n\n# Or send multiple metrics at once:\nfabric.log_dict({\"loss\": loss, \"accuracy\": acc})\n```\n\n----------------------------------------\n\nTITLE: Logging Multiple Metrics with Fabric\nDESCRIPTION: Example of logging multiple metrics at once to Weights and Biases using the log_dict method in Fabric. This allows tracking multiple values like loss, accuracy, and other metrics in a single call.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/loggers/wandb.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nvalues = {\"loss\": loss, \"acc\": acc, \"other\": other}\nfabric.log_dict(values)\n```\n\n----------------------------------------\n\nTITLE: Forcing Full Graph Compilation to Identify Graph Breaks\nDESCRIPTION: Example showing how to use the fullgraph parameter with torch.compile to detect graph breaks in a model, which forces an error if parts of the model cannot be compiled.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/compile.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Force an error if there is a graph break in the model\nmodel = torch.compile(model, fullgraph=True)\n```\n\n----------------------------------------\n\nTITLE: Running Decoupled PPO Example with Lightning Fabric\nDESCRIPTION: This command runs the decoupled PPO implementation using Lightning Fabric with 3 devices and 4 environments.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/fabric/reinforcement_learning/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nfabric run --devices=3 train_fabric_decoupled.py --num-envs 4\n```\n\n----------------------------------------\n\nTITLE: Setting Up a Composite Model with One Optimizer in Fabric\nDESCRIPTION: Shows how to set up a composite model that contains multiple sub-models with a single optimizer in Lightning Fabric. This approach treats the composite model as a single entity.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/multiple_setup.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Instantiate the big model\nautoencoder = AutoEncoder()\noptimizer = ...\n\n# Set up the model(s) and optimizer together\nautoencoder, optimizer = fabric.setup(autoencoder, optimizer)\n```\n\n----------------------------------------\n\nTITLE: Customizing Batch Size Finder in PyTorch Lightning\nDESCRIPTION: Shows how to customize the BatchSizeFinder callback to run at different epochs, which is useful for fine-tuning models. It also demonstrates how to run batch size finder for validate/test/predict operations.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/training_tricks.rst#2025-04-23_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom lightning.pytorch.callbacks import BatchSizeFinder\n\n\nclass FineTuneBatchSizeFinder(BatchSizeFinder):\n    def __init__(self, milestones, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.milestones = milestones\n\n    def on_fit_start(self, *args, **kwargs):\n        return\n\n    def on_train_epoch_start(self, trainer, pl_module):\n        if trainer.current_epoch in self.milestones or trainer.current_epoch == 0:\n            self.scale_batch_size(trainer, pl_module)\n\n\ntrainer = Trainer(callbacks=[FineTuneBatchSizeFinder(milestones=(5, 10))])\ntrainer.fit(...)\n```\n\nLANGUAGE: Python\nCODE:\n```\nfrom lightning.pytorch.callbacks import BatchSizeFinder\n\n\nclass EvalBatchSizeFinder(BatchSizeFinder):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    def on_fit_start(self, *args, **kwargs):\n        return\n\n    def on_test_start(self, trainer, pl_module):\n        self.scale_batch_size(trainer, pl_module)\n\n\ntrainer = Trainer(callbacks=[EvalBatchSizeFinder()])\ntrainer.test(...)\n```\n\n----------------------------------------\n\nTITLE: Configuring Comet.ml Logger in PyTorch Lightning\nDESCRIPTION: Demonstrates how to configure the Comet.ml logger and pass it to the PyTorch Lightning Trainer. Also shows how to access the logger within a LightningModule.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/supported_exp_managers.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.loggers import CometLogger\n\ncomet_logger = CometLogger(api_key=\"YOUR_COMET_API_KEY\")\ntrainer = Trainer(logger=comet_logger)\n```\n\nLANGUAGE: python\nCODE:\n```\nclass LitModel(LightningModule):\n    def any_lightning_module_function_or_hook(self):\n        comet = self.logger.experiment\n        fake_images = torch.Tensor(32, 3, 28, 28)\n        comet.add_image(\"generated_images\", fake_images, 0)\n```\n\n----------------------------------------\n\nTITLE: Adding Model Summary Callback in PyTorch Lightning\nDESCRIPTION: Demonstrates how to add a ModelSummary callback to print a detailed summary of the model architecture in PyTorch Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/debug/debugging_basic.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.callbacks import ModelSummary\n\ntrainer = Trainer(callbacks=[ModelSummary(max_depth=-1)])\n```\n\n----------------------------------------\n\nTITLE: Gradient Clipping with Manual Optimization in PyTorch Lightning\nDESCRIPTION: Demonstrates how to implement gradient clipping when using manual optimization.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/model/manual_optimization.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch import LightningModule\n\nclass SimpleModel(LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.automatic_optimization = False\n\n    def training_step(self, batch, batch_idx):\n        opt = self.optimizers()\n        loss = self.compute_loss(batch)\n        opt.zero_grad()\n        self.manual_backward(loss)\n        self.clip_gradients(opt, gradient_clip_val=0.5, gradient_clip_algorithm=\"norm\")\n        opt.step()\n```\n\n----------------------------------------\n\nTITLE: Monitoring Validation Loss for Checkpointing in PyTorch Lightning\nDESCRIPTION: This code example shows how to set up a LightningModule to log validation loss and configure a ModelCheckpoint to monitor it for saving checkpoints.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/checkpointing_intermediate.rst#2025-04-23_snippet_3\n\nLANGUAGE: Python\nCODE:\n```\nfrom lightning.pytorch.callbacks import ModelCheckpoint\n\nclass LitAutoEncoder(LightningModule):\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self.backbone(x)\n\n        # 1. calculate loss\n        loss = F.cross_entropy(y_hat, y)\n\n        # 2. log val_loss\n        self.log(\"val_loss\", loss)\n\n# 3. Init ModelCheckpoint callback, monitoring \"val_loss\"\ncheckpoint_callback = ModelCheckpoint(monitor=\"val_loss\")\n\n# 4. Add your callback to the callbacks list\ntrainer = Trainer(callbacks=[checkpoint_callback])\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Training Strategy in PyTorch Lightning\nDESCRIPTION: Demonstrates how to create and register a custom training strategy with a custom checkpoint I/O implementation. Shows the implementation of a CustomCheckpointIO class and its registration with the DDP Strategy.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/strategy_registry.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.strategies import DDPStrategy, StrategyRegistry, CheckpointIO\n\n\nclass CustomCheckpointIO(CheckpointIO):\n    def save_checkpoint(self, checkpoint: Dict[str, Any], path: Union[str, Path]) -> None:\n        ...\n\n    def load_checkpoint(self, path: Union[str, Path]) -> Dict[str, Any]:\n        ...\n\n\ncustom_checkpoint_io = CustomCheckpointIO()\n\n# Register the DDP Strategy with your custom CheckpointIO plugin\nStrategyRegistry.register(\n    \"ddp_custom_checkpoint_io\",\n    DDPStrategy,\n    description=\"DDP Strategy with custom checkpoint io plugin\",\n    checkpoint_io=custom_checkpoint_io,\n)\n\ntrainer = Trainer(strategy=\"ddp_custom_checkpoint_io\", accelerator=\"gpu\", devices=2)\n```\n\n----------------------------------------\n\nTITLE: Initializing Modules with Fabric in Python\nDESCRIPTION: Uses Fabric's context manager to efficiently instantiate large models directly on device, with desired dtype and sharding support. Handles initialization for FSDP and DeepSpeed.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/fabric/CHANGELOG.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nFabric.init_module()\n```\n\n----------------------------------------\n\nTITLE: Implementing Rich Progress Bar in PyTorch Lightning\nDESCRIPTION: Demonstrates how to use and customize the RichProgressBar with custom themes for enhanced visual formatting.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/logging_expert.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.callbacks import RichProgressBar\n\ntrainer = Trainer(callbacks=[RichProgressBar()])\n```\n\n----------------------------------------\n\nTITLE: Using DeepSpeed with JSON Configuration File in PyTorch Lightning\nDESCRIPTION: Demonstrates how to initialize a PyTorch Lightning Trainer with DeepSpeed using an external JSON configuration file. This approach allows for better separation of model code and training configuration.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/deepspeed.rst#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch import Trainer\nfrom lightning.pytorch.strategies import DeepSpeedStrategy\n\nmodel = MyModel()\ntrainer = Trainer(\n    accelerator=\"gpu\", devices=4, strategy=DeepSpeedStrategy(config=\"/path/to/deepspeed_config.json\"), precision=16\n)\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Implementing Validation Epoch Metrics in PyTorch Lightning\nDESCRIPTION: Shows how to implement validation step and epoch-end methods in a Lightning Module to process validation outputs. Demonstrates collecting predictions during validation steps and processing them at epoch end.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/lightning_module.rst#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nclass LightningTransformer(L.LightningModule):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.model = Transformer(vocab_size=vocab_size)\n        self.validation_step_outputs = []\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        inputs, target = batch\n        output = self.model(inputs, target)\n        loss = torch.nn.functional.nll_loss(output, target.view(-1))\n        pred = ...\n        self.validation_step_outputs.append(pred)\n        return pred\n\n    def on_validation_epoch_end(self):\n        all_preds = torch.stack(self.validation_step_outputs)\n        # do something with all preds\n        ...\n        self.validation_step_outputs.clear()  # free memory\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Metric for Distributed Training\nDESCRIPTION: This code demonstrates how to create a custom accuracy metric using torchmetrics for use in distributed training environments with PyTorch Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/logging_advanced.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torchmetrics\n\nclass MyAccuracy(Metric):\n    def __init__(self, dist_sync_on_step=False):\n        # call `self.add_state`for every internal state that is needed for the metrics computations\n        # dist_reduce_fx indicates the function that should be used to reduce\n        # state from multiple processes\n        super().__init__(dist_sync_on_step=dist_sync_on_step)\n\n        self.add_state(\"correct\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n        self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n\n    def update(self, preds: torch.Tensor, target: torch.Tensor):\n        # update metric states\n        preds, target = self._input_format(preds, target)\n        assert preds.shape == target.shape\n\n        self.correct += torch.sum(preds == target)\n        self.total += target.numel()\n\n    def compute(self):\n        # compute final result\n        return self.correct.float() / self.total\n```\n\n----------------------------------------\n\nTITLE: Logging Multiple Metrics in PyTorch Lightning\nDESCRIPTION: Shows how to log multiple metrics at once using self.log_dict within a LightningModule.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/logging_basic.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nvalues = {\"loss\": loss, \"acc\": acc, \"metric_n\": metric_n}  # add more items if needed\nself.log_dict(values)\n```\n\n----------------------------------------\n\nTITLE: Integrating WebDataset with PyTorch Lightning\nDESCRIPTION: Shows how to use WebDataset with PyTorch Lightning. This example creates a WebDataset instance and uses it with the WebLoader, which is a wrapper around the standard DataLoader.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/data/alternatives.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\nimport webdataset as wds\n\ndataset = wds.WebDataset(urls)\ntrain_dataloader = wds.WebLoader(dataset)\n\nmodel = ...\ntrainer = L.Trainer()\ntrainer.fit(model, train_dataloader)\n```\n\n----------------------------------------\n\nTITLE: Replacing backward() with Fabric's backward()\nDESCRIPTION: Replace the standard PyTorch loss.backward() with fabric.backward(loss) to enable distributed backward passes.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/convert.rst#2025-04-23_snippet_4\n\nLANGUAGE: diff\nCODE:\n```\n- loss.backward()\n+ fabric.backward(loss)\n```\n\n----------------------------------------\n\nTITLE: Implementing Multiple Optimizers with Dependency Injection in PyTorch Lightning\nDESCRIPTION: Example of a PyTorch Lightning model that uses dependency injection to configure two different optimizers. Uses type hints to define optimizer callables that can be configured via CLI.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_advanced_3.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Iterable\nfrom torch.optim import Optimizer\n\n\nOptimizerCallable = Callable[[Iterable], Optimizer]\n\n\nclass MyModel(LightningModule):\n    def __init__(self, optimizer1: OptimizerCallable, optimizer2: OptimizerCallable):\n        super().__init__()\n        self.save_hyperparameters()\n        self.optimizer1 = optimizer1\n        self.optimizer2 = optimizer2\n\n    def configure_optimizers(self):\n        optimizer1 = self.optimizer1(self.parameters())\n        optimizer2 = self.optimizer2(self.parameters())\n        return [optimizer1, optimizer2]\n\n\ncli = MyLightningCLI(MyModel, auto_configure_optimizers=False)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Reproducibility in Trainer\nDESCRIPTION: Example of configuring reproducibility settings for deterministic training results.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch import Trainer, seed_everything\n\nseed_everything(42, workers=True)\n# sets seeds for numpy, torch and python.random.\nmodel = Model()\ntrainer = Trainer(deterministic=True)\n```\n\n----------------------------------------\n\nTITLE: Configuring ModelParallelStrategy for Multi-Node Training in Python\nDESCRIPTION: This code snippet shows how to configure ModelParallelStrategy for multi-node training, applying Tensor Parallelism intra-node and Data Parallelism inter-node.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/tp_fsdp.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.strategies import ModelParallelStrategy\n\nstrategy = ModelParallelStrategy(\n    # Default is \"auto\"\n    # Applies TP intra-node and DP inter-node\n    data_parallel_size=\"auto\",\n    tensor_parallel_size=\"auto\",\n)\n```\n\n----------------------------------------\n\nTITLE: Loading MNIST Dataset Splits in PyTorch\nDESCRIPTION: Demonstrates how to load the MNIST dataset and create train/test splits using torchvision\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/evaluation_basic.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch.utils.data as data\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\n\n# Load data sets\ntransform = transforms.ToTensor()\ntrain_set = datasets.MNIST(root=\"MNIST\", download=True, train=True, transform=transform)\ntest_set = datasets.MNIST(root=\"MNIST\", download=True, train=False, transform=transform)\n```\n\n----------------------------------------\n\nTITLE: Implementing BitsandBytes Precision in PyTorch Lightning\nDESCRIPTION: Demonstrates how to use BitsandbytesPrecision plugin for model quantization with different modes and customization options. Supports automatic replacement of linear layers with BNB alternatives.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/precision.rst#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.fabric.plugins import BitsandbytesPrecision\n\n# this will pick out the compute dtype automatically, by default `bfloat16`\nprecision = BitsandbytesPrecision(mode=\"nf4-dq\")\nfabric = Fabric(plugins=precision)\n\n# Customize the dtype, or ignore some modules\nprecision = BitsandbytesPrecision(mode=\"int8-training\", dtype=torch.float16, ignore_modules={\"lm_head\"})\nfabric = Fabric(plugins=precision)\n\nmodel = MyModel()\nmodel = fabric.setup(model)\n```\n\n----------------------------------------\n\nTITLE: Implementing Train DataLoader in Lightning DataModule\nDESCRIPTION: Demonstrates how to implement the train_dataloader method in a Lightning DataModule to generate training data loaders with batch size 64.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/data/datamodule.rst#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\n\nclass MNISTDataModule(L.LightningDataModule):\n    def train_dataloader(self):\n        return DataLoader(self.mnist_train, batch_size=64)\n```\n\n----------------------------------------\n\nTITLE: Implementing Language Model Training with PyTorch Lightning\nDESCRIPTION: This code snippet demonstrates a complete PyTorch Lightning implementation for training a language model. It includes a custom DataModule for the WikiText2 dataset, a LightningModule for the language model, and a main function to set up and run the training process.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/clouds/lightning_ai.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\nimport torch\nimport torch.nn.functional as F\nfrom lightning.pytorch.demos import Transformer, WikiText2\nfrom torch.utils.data import DataLoader, random_split\n\n\nclass LanguageDataModule(L.LightningDataModule):\n    def __init__(self, batch_size):\n        super().__init__()\n        self.batch_size = batch_size\n        self.vocab_size = 33278\n\n    def prepare_data(self):\n        WikiText2(download=True)\n\n    def setup(self, stage):\n        dataset = WikiText2()\n\n        # Split data in to train, val, test\n        n = len(dataset)\n        self.train_dataset, self.val_dataset, self.test_dataset = random_split(dataset, [n - 4000, 2000, 2000])\n\n    def train_dataloader(self):\n        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n\n    def val_dataloader(self):\n        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False)\n\n    def test_dataloader(self):\n        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False)\n\n\nclass LanguageModel(L.LightningModule):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.model = None\n\n    def configure_model(self):\n        if self.model is None:\n            self.model = Transformer(vocab_size=self.vocab_size)\n\n    def training_step(self, batch, batch_idx):\n        input, target = batch\n        output = self.model(input, target)\n        loss = F.nll_loss(output, target.view(-1))\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        input, target = batch\n        output = self.model(input, target)\n        loss = F.nll_loss(output, target.view(-1))\n        self.log(\"val_loss\", loss)\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        input, target = batch\n        output = self.model(input, target)\n        loss = F.nll_loss(output, target.view(-1))\n        self.log(\"test_loss\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.SGD(self.parameters(), lr=0.1)\n\n\ndef main():\n    L.seed_everything(42)\n\n    datamodule = LanguageDataModule(batch_size=20)\n    model = LanguageModel(datamodule.vocab_size)\n\n    # Trainer\n    trainer = L.Trainer(gradient_clip_val=0.25, max_epochs=2, strategy=\"ddp\")\n    trainer.fit(model, datamodule=datamodule)\n    trainer.test(model, datamodule=datamodule)\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\n----------------------------------------\n\nTITLE: Configuring Distributed Sampler in PyTorch Lightning\nDESCRIPTION: Shows how to configure distributed sampling in PyTorch Lightning, either by using the default Trainer parameter or by implementing a custom distributed sampler in a data loader.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_29\n\nLANGUAGE: python\nCODE:\n```\n# default used by the Trainer\ntrainer = Trainer(use_distributed_sampler=True)\n```\n\nLANGUAGE: python\nCODE:\n```\n# in your LightningModule or LightningDataModule\ndef train_dataloader(self):\n    dataset = ...\n    # default used by the Trainer\n    sampler = torch.utils.data.DistributedSampler(dataset, shuffle=True)\n    dataloader = DataLoader(dataset, batch_size=32, sampler=sampler)\n    return dataloader\n```\n\n----------------------------------------\n\nTITLE: Using Batch Size and Rank Zero Only Parameters with log_dict in PyTorch Lightning\nDESCRIPTION: Example of using the newly added batch_size and rank_zero_only arguments with the log_dict method, matching the functionality of the log method.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_28\n\nLANGUAGE: python\nCODE:\n```\ndef training_step(self, batch, batch_idx):\n    # Calculate metrics\n    loss = self.compute_loss(batch)\n    accuracy = self.compute_accuracy(batch)\n    f1 = self.compute_f1(batch)\n    \n    # Log multiple metrics at once with new parameters\n    metrics = {\n        \"train_loss\": loss,\n        \"train_accuracy\": accuracy,\n        \"train_f1\": f1\n    }\n    \n    # Use the new parameters: batch_size and rank_zero_only\n    self.log_dict(metrics, batch_size=len(batch), rank_zero_only=True)\n    \n    return loss\n```\n\n----------------------------------------\n\nTITLE: Configuring Devices for Training in PyTorch Lightning\nDESCRIPTION: Specifies the number and type of devices to use for training. It supports CPU, GPU, and TPU configurations, with options for automatic device detection.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\n# Training with CPU Accelerator using 2 processes\ntrainer = Trainer(devices=2, accelerator=\"cpu\")\n\n# Training with GPU Accelerator using GPUs 1 and 3\ntrainer = Trainer(devices=[1, 3], accelerator=\"gpu\")\n\n# Training with TPU Accelerator using 8 tpu cores\ntrainer = Trainer(devices=8, accelerator=\"tpu\")\n\n# Use whatever hardware your machine has available\ntrainer = Trainer(devices=\"auto\", accelerator=\"auto\")\n```\n\n----------------------------------------\n\nTITLE: Customizing DeepSpeed Strategy Parameters in PyTorch Lightning\nDESCRIPTION: Example showing how to customize DeepSpeed parameters such as bucket sizes when initializing the strategy, which can be tuned for optimal performance based on model size.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/deepspeed.rst#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch import Trainer\nfrom lightning.pytorch.strategies import DeepSpeedStrategy\n\nmodel = MyModel()\ntrainer = Trainer(\n    accelerator=\"gpu\",\n    devices=4,\n    strategy=DeepSpeedStrategy(offload_optimizer=True, allgather_bucket_size=5e8, reduce_bucket_size=5e8),\n    precision=16,\n)\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Custom TQDM Progress Bar Implementation in PyTorch Lightning\nDESCRIPTION: Shows how to create a custom progress bar by extending the TQDMProgressBar class and overriding specific methods.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/progress_bar.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass LitProgressBar(TQDMProgressBar):\n    def init_validation_tqdm(self):\n        bar = super().init_validation_tqdm()\n        bar.set_description(\"running validation...\")\n        return bar\n\n\ntrainer = Trainer(callbacks=[LitProgressBar()])\n```\n\n----------------------------------------\n\nTITLE: Accessing DataLoaders from Trainer in PyTorch Lightning\nDESCRIPTION: This snippet shows how to access DataLoaders for different steps (train, validation, test, predict) using Trainer properties in PyTorch Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/data/access.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndataloaders = trainer.train_dataloader\ndataloaders = trainer.val_dataloaders\ndataloaders = trainer.test_dataloaders\ndataloaders = trainer.predict_dataloaders\n```\n\n----------------------------------------\n\nTITLE: Training a LightningTransformer Model\nDESCRIPTION: This snippet shows how to create a dataset, dataloader, and LightningTransformer model, then train it using a Lightning Trainer.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/lightning_module.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.demos import WikiText2\nfrom torch.utils.data import DataLoader\n\ndataset = WikiText2()\ndataloader = DataLoader(dataset)\nmodel = LightningTransformer(vocab_size=dataset.vocab_size)\n\ntrainer = L.Trainer(fast_dev_run=100)\ntrainer.fit(model=model, train_dataloaders=dataloader)\n```\n\n----------------------------------------\n\nTITLE: Running the Tensor Parallel Training Script\nDESCRIPTION: Commands to navigate to the example folder and execute the training script that demonstrates tensor parallelism with the Llama 3 7B model.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/fabric/tensor_parallel/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/fabric/tensor_parallel\npython train.py\n```\n\n----------------------------------------\n\nTITLE: Configuring MLflow Logger in PyTorch Lightning\nDESCRIPTION: Demonstrates how to configure the MLflow logger and pass it to the PyTorch Lightning Trainer. Also shows how to access the logger within a LightningModule.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/supported_exp_managers.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.loggers import MLFlowLogger\n\nmlf_logger = MLFlowLogger(experiment_name=\"lightning_logs\", tracking_uri=\"file:./ml-runs\")\ntrainer = Trainer(logger=mlf_logger)\n```\n\nLANGUAGE: python\nCODE:\n```\nclass LitModel(LightningModule):\n    def any_lightning_module_function_or_hook(self):\n        mlf_logger = self.logger.experiment\n        fake_images = torch.Tensor(32, 3, 28, 28)\n        mlf_logger.add_image(\"generated_images\", fake_images, 0)\n```\n\n----------------------------------------\n\nTITLE: Implementing Validation Loop in PyTorch Lightning\nDESCRIPTION: Shows implementation of validation_step method in a Lightning Module for model validation\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/evaluation_basic.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass LitAutoEncoder(L.LightningModule):\n    def training_step(self, batch, batch_idx):\n        ...\n\n    def validation_step(self, batch, batch_idx):\n        # this is the validation loop\n        x, _ = batch\n        x = x.view(x.size(0), -1)\n        z = self.encoder(x)\n        x_hat = self.decoder(z)\n        val_loss = F.mse_loss(x_hat, x)\n        self.log(\"val_loss\", val_loss)\n```\n\n----------------------------------------\n\nTITLE: Implementing Distributed Inference with Custom Writer\nDESCRIPTION: Demonstrates how to implement distributed inference using a custom prediction writer callback. Supports both batch-level and epoch-level writing of predictions across multiple GPU processes.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/deploy/production_basic.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom lightning.pytorch.callbacks import BasePredictionWriter\n\n\nclass CustomWriter(BasePredictionWriter):\n    def __init__(self, output_dir, write_interval):\n        super().__init__(write_interval)\n        self.output_dir = output_dir\n\n    def write_on_epoch_end(self, trainer, pl_module, predictions, batch_indices):\n        # this will create N (num processes) files in `output_dir` each containing\n        # the predictions of it's respective rank\n        torch.save(predictions, os.path.join(self.output_dir, f\"predictions_{trainer.global_rank}.pt\"))\n\n        # optionally, you can also save `batch_indices` to get the information about the data index\n        # from your prediction data\n        torch.save(batch_indices, os.path.join(self.output_dir, f\"batch_indices_{trainer.global_rank}.pt\"))\n\n\n# or you can set `write_interval=\"batch\"` and override `write_on_batch_end` to save\n# predictions at batch level\npred_writer = CustomWriter(output_dir=\"pred_path\", write_interval=\"epoch\")\ntrainer = Trainer(accelerator=\"gpu\", strategy=\"ddp\", devices=8, callbacks=[pred_writer])\nmodel = BoringModel()\ntrainer.predict(model, return_predictions=False)\n```\n\n----------------------------------------\n\nTITLE: Selecting Specific GPU Devices in PyTorch Lightning\nDESCRIPTION: Shows how to select specific GPU devices using ranges, a list of indices, or a string containing a comma-separated list of GPU ids. It also demonstrates how to use all available GPUs.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/accelerators/gpu_basic.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# DEFAULT (int) specifies how many GPUs to use per node\nTrainer(accelerator=\"gpu\", devices=k)\n\n# Above is equivalent to\nTrainer(accelerator=\"gpu\", devices=list(range(k)))\n\n# Specify which GPUs to use (don't use when running on cluster)\nTrainer(accelerator=\"gpu\", devices=[0, 1])\n\n# Equivalent using a string\nTrainer(accelerator=\"gpu\", devices=\"0, 1\")\n\n# To use all available GPUs put -1 or '-1'\n# equivalent to `list(range(torch.cuda.device_count())) and `\"auto\"`\nTrainer(accelerator=\"gpu\", devices=-1)\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Progress Bar\nDESCRIPTION: Demonstrates how to build a completely custom progress bar by subclassing the base ProgressBar class.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/logging_expert.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.callbacks import ProgressBar\n\n\nclass LitProgressBar(ProgressBar):\n    def __init__(self):\n        super().__init__()  # don't forget this :)\n        self.enable = True\n\n    def disable(self):\n        self.enable = False\n\n    def on_train_batch_end(self, trainer, pl_module, outputs, batch_idx):\n        super().on_train_batch_end(trainer, pl_module, outputs, batch_idx)  # don't forget this :)\n        percent = (self.train_batch_idx / self.total_train_batches) * 100\n        sys.stdout.flush()\n        sys.stdout.write(f\"{percent:.01f} percent complete \\r\")\n\n\nbar = LitProgressBar()\ntrainer = Trainer(callbacks=[bar])\n```\n\n----------------------------------------\n\nTITLE: Reducing Data Across Processes\nDESCRIPTION: Shows how to perform reduction operations (sum, mean) on tensor data across multiple processes.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/distributed_communication.rst#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfabric = Fabric(devices=4, accelerator=\"cpu\")\nfabric.launch()\n\n# Data is different in each process\ndata = torch.tensor(10 * fabric.global_rank)\n\n# Sum the tensors from every process\nresult = fabric.all_reduce(data, reduce_op=\"sum\")\n\n# sum(0 + 10 + 20 + 30) = tensor(60)\nprint(\"Result of all-reduce:\", result)\n```\n\n----------------------------------------\n\nTITLE: Configuring Precision Settings in Fabric\nDESCRIPTION: Examples of setting different precision modes for training including 32-bit, 16-bit mixed precision, bfloat16, and other advanced options that affect performance and memory usage.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_args.rst#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Default used by the Fabric\nfabric = Fabric(precision=\"32-true\", devices=1)\n\n# the same as:\nfabric = Fabric(precision=\"32\", devices=1)\n\n# 16-bit mixed precision (model weights remain in torch.float32)\nfabric = Fabric(precision=\"16-mixed\", devices=1)\n\n# 16-bit bfloat mixed precision (model weights remain in torch.float32)\nfabric = Fabric(precision=\"bf16-mixed\", devices=1)\n\n# 8-bit mixed precision via TransformerEngine (model weights get cast to torch.bfloat16)\nfabric = Fabric(precision=\"transformer-engine\", devices=1)\n\n# 16-bit precision (model weights get cast to torch.float16)\nfabric = Fabric(precision=\"16-true\", devices=1)\n\n# 16-bit bfloat precision (model weights get cast to torch.bfloat16)\nfabric = Fabric(precision=\"bf16-true\", devices=1)\n\n# 64-bit (double) precision (model weights get cast to torch.float64)\nfabric = Fabric(precision=\"64-true\", devices=1)\n```\n\n----------------------------------------\n\nTITLE: Training and Using ImageNet Transfer Learning Model\nDESCRIPTION: Example of how to train and use the transfer learning model with PyTorch Lightning. Shows model training and inference process.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/transfer_learning.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nmodel = ImagenetTransferLearning()\ntrainer = Trainer()\ntrainer.fit(model)\n\nmodel = ImagenetTransferLearning.load_from_checkpoint(PATH)\nmodel.freeze()\n\nx = some_images_from_cifar10()\npredictions = model(x)\n```\n\n----------------------------------------\n\nTITLE: Implementing Monte Carlo Dropout for Predictions\nDESCRIPTION: Shows implementation of Monte Carlo Dropout in a LightningModule for uncertainty estimation in predictions. Takes multiple forward passes with dropout enabled during inference.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/deploy/production_basic.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass LitMCdropoutModel(L.LightningModule):\n    def __init__(self, model, mc_iteration):\n        super().__init__()\n        self.model = model\n        self.dropout = nn.Dropout()\n        self.mc_iteration = mc_iteration\n\n    def predict_step(self, batch, batch_idx):\n        # enable Monte Carlo Dropout\n        self.dropout.train()\n\n        # take average of `self.mc_iteration` iterations\n        pred = [self.dropout(self.model(x)).unsqueeze(0) for _ in range(self.mc_iteration)]\n        pred = torch.vstack(pred).mean(dim=0)\n        return pred\n```\n\n----------------------------------------\n\nTITLE: Combining Communication Hooks in DDP Strategy\nDESCRIPTION: Sets up a DDP strategy that combines multiple communication hooks (PowerSGD with FP16 compression) to gain accumulated benefits from both optimization techniques for distributed training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/ddp_optimizations.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\nfrom lightning.pytorch.strategies import DDPStrategy\nfrom torch.distributed.algorithms.ddp_comm_hooks import (\n    default_hooks as default,\n    powerSGD_hook as powerSGD,\n)\n\nmodel = MyModel()\ntrainer = L.Trainer(\n    accelerator=\"gpu\",\n    devices=4,\n    strategy=DDPStrategy(\n        ddp_comm_state=powerSGD.PowerSGDState(\n            process_group=None,\n            matrix_approximation_rank=1,\n            start_powerSGD_iter=5000,\n        ),\n        ddp_comm_hook=powerSGD.powerSGD_hook,\n        ddp_comm_wrapper=default.fp16_compress_wrapper,\n    ),\n)\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Initializing Trainer for Multi-GPU Training in Notebooks with PyTorch Lightning\nDESCRIPTION: This snippet demonstrates how to initialize a PyTorch Lightning Trainer for distributed training on multiple GPUs in interactive environments like Jupyter notebooks. It shows both automatic detection and explicit strategy setting.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/accelerators/gpu_intermediate.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# train on 8 GPUs in a Jupyter notebook\ntrainer = Trainer(accelerator=\"gpu\", devices=8)\n\n# can be set explicitly\ntrainer = Trainer(accelerator=\"gpu\", devices=8, strategy=\"ddp_notebook\")\n\n# can also be used in non-interactive environments\ntrainer = Trainer(accelerator=\"gpu\", devices=8, strategy=\"ddp_fork\")\n```\n\n----------------------------------------\n\nTITLE: Using Tensor.to for Device-Agnostic Tensor Creation in PyTorch Lightning\nDESCRIPTION: Demonstrates the proper way to create new tensors that can scale to any device using Tensor.to(), referencing the device from an existing tensor.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/accelerators/accelerator_prepare.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# before lightning\ndef forward(self, x):\n    z = torch.Tensor(2, 3)\n    z = z.cuda(0)\n\n\n# with lightning\ndef forward(self, x):\n    z = torch.Tensor(2, 3)\n    z = z.to(x)\n```\n\n----------------------------------------\n\nTITLE: Customizing LightningCLI with Email Notifications in Python\nDESCRIPTION: Extends the LightningCLI class to add a custom argument for email notifications and implements methods to send emails before and after the fit command execution.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_expert.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass MyLightningCLI(LightningCLI):\n    def add_arguments_to_parser(self, parser):\n        parser.add_argument(\"--notification_email\", default=\"will@email.com\")\n\n    def before_fit(self):\n        send_email(address=self.config[\"notification_email\"], message=\"trainer.fit starting\")\n\n    def after_fit(self):\n        send_email(address=self.config[\"notification_email\"], message=\"trainer.fit finished\")\n\n\ncli = MyLightningCLI(MyModel)\n```\n\n----------------------------------------\n\nTITLE: Custom Logger Implementation\nDESCRIPTION: Example of implementing a custom logger by inheriting from Lightning's Logger class with required methods.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/extensions/logging.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.loggers.logger import Logger, rank_zero_experiment\nfrom lightning.pytorch.utilities import rank_zero_only\n\n\nclass MyLogger(Logger):\n    @property\n    def name(self):\n        return \"MyLogger\"\n\n    @property\n    def version(self):\n        # Return the experiment version, int or str.\n        return \"0.1\"\n\n    @rank_zero_only\n    def log_hyperparams(self, params):\n        # params is an argparse.Namespace\n        # your code to record hyperparameters goes here\n        pass\n\n    @rank_zero_only\n    def log_metrics(self, metrics, step):\n        # metrics is a dictionary of metric names and values\n        # your code to record metrics goes here\n        pass\n\n    @rank_zero_only\n    def save(self):\n        # Optional. Any code necessary to save logger data goes here\n        pass\n\n    @rank_zero_only\n    def finalize(self, status):\n        # Optional. Any code that needs to be run after training\n        # finishes goes here\n        pass\n```\n\n----------------------------------------\n\nTITLE: Initializing TQDM Progress Bar in PyTorch Lightning\nDESCRIPTION: Shows how to use the built-in TQDMProgressBar with the Lightning Trainer.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/logging_expert.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.callbacks import TQDMProgressBar\n\ntrainer = Trainer(callbacks=[TQDMProgressBar()])\n```\n\n----------------------------------------\n\nTITLE: Configuring Quantization Precision in Fabric\nDESCRIPTION: Example of using quantization plugins like BitsandbytesPrecision to enable 4-bit and 8-bit precision for model training, which can significantly reduce memory requirements.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_args.rst#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.fabric.plugins import BitsandbytesPrecision\n\nprecision = BitsandbytesPrecision(mode=\"nf4-dq\", dtype=torch.bfloat16)\nfabric = Fabric(plugins=precision)\n```\n\n----------------------------------------\n\nTITLE: Configuring Precision Settings in PyTorch Lightning\nDESCRIPTION: Examples showing various precision options for training, including 16-bit, bfloat16, mixed precision, and 64-bit precision. These settings affect both performance and memory usage.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n# Default used by the Trainer\nfabric = Fabric(precision=\"32-true\", devices=1)\n\n# the same as:\ntrainer = Trainer(precision=\"32\", devices=1)\n\n# 16-bit mixed precision (model weights remain in torch.float32)\ntrainer = Trainer(precision=\"16-mixed\", devices=1)\n\n# 16-bit bfloat mixed precision (model weights remain in torch.float32)\ntrainer = Trainer(precision=\"bf16-mixed\", devices=1)\n\n# 8-bit mixed precision via TransformerEngine (model weights get cast to torch.bfloat16)\ntrainer = Trainer(precision=\"transformer-engine\", devices=1)\n\n# 16-bit precision (model weights get cast to torch.float16)\ntrainer = Trainer(precision=\"16-true\", devices=1)\n\n# 16-bit bfloat precision (model weights get cast to torch.bfloat16)\ntrainer = Trainer(precision=\"bf16-true\", devices=1)\n\n# 64-bit (double) precision (model weights get cast to torch.float64)\ntrainer = Trainer(precision=\"64-true\", devices=1)\n```\n\n----------------------------------------\n\nTITLE: Specifying Reduction Function for Logged Metrics in PyTorch Lightning\nDESCRIPTION: Shows how to specify a custom reduction function (mean, min, max, sum) for logged metrics using the reduce_fx argument.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/logging_basic.rst#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# default function\nself.log(..., reduce_fx=\"mean\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Forced Callbacks in LightningCLI with Python\nDESCRIPTION: Demonstrates how to add a forced callback (EarlyStopping) to the LightningCLI, ensuring it's always present and configurable for the model.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_expert.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.callbacks import EarlyStopping\n\n\nclass MyLightningCLI(LightningCLI):\n    def add_arguments_to_parser(self, parser):\n        parser.add_lightning_class_args(EarlyStopping, \"my_early_stopping\")\n        parser.set_defaults({\"my_early_stopping.monitor\": \"val_loss\", \"my_early_stopping.patience\": 5})\n\n\ncli = MyLightningCLI(MyModel)\n```\n\n----------------------------------------\n\nTITLE: Training with Validation in PyTorch Lightning\nDESCRIPTION: Demonstrates how to train a model with both training and validation data using the Trainer\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/evaluation_basic.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(train_set)\nvalid_loader = DataLoader(valid_set)\nmodel = LitAutoEncoder(...)\n\n# train with both splits\ntrainer = L.Trainer()\ntrainer.fit(model, train_loader, valid_loader)\n```\n\n----------------------------------------\n\nTITLE: Custom Learning Rate Scheduler Implementation\nDESCRIPTION: Implementation showing how to create and use custom learning rate scheduler with LightningCLI\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_intermediate_2.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# main.py\nimport torch\nfrom lightning.pytorch.cli import LightningCLI\nfrom lightning.pytorch.demos.boring_classes import DemoModel, BoringDataModule\n\n\nclass LitLRScheduler(torch.optim.lr_scheduler.CosineAnnealingLR):\n    def step(self):\n        print(\"⚡\", \"using LitLRScheduler\", \"⚡\")\n        super().step()\n\n\ncli = LightningCLI(DemoModel, BoringDataModule)\n```\n\n----------------------------------------\n\nTITLE: Manual Metric Reduction in Distributed PyTorch Lightning\nDESCRIPTION: Shows how to manually collect outputs across all processes, compute a reduction, and log only on rank 0 to avoid synchronization issues in distributed training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/accelerators/accelerator_prepare.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(self):\n    super().__init__()\n    self.outputs = []\n\n\ndef test_step(self, batch, batch_idx):\n    x, y = batch\n    tensors = self(x)\n    self.outputs.append(tensors)\n    return tensors\n\n\ndef on_test_epoch_end(self):\n    mean = torch.mean(self.all_gather(self.outputs))\n    self.outputs.clear()  # free memory\n\n    # When you call `self.log` only on rank 0, don't forget to add\n    # `rank_zero_only=True` to avoid deadlocks on synchronization.\n    # Caveat: monitoring this is unimplemented, see https://github.com/Lightning-AI/pytorch-lightning/issues/15852\n    if self.trainer.is_global_zero:\n        self.log(\"my_reduced_metric\", mean, rank_zero_only=True)\n```\n\n----------------------------------------\n\nTITLE: Implementing Multiple Iterable Types in PyTorch Lightning\nDESCRIPTION: Examples of different ways to return iterables including single DataLoader, list range, dictionary of DataLoaders, list of DataLoaders, and nested collections. Shows various data structure patterns supported by the Trainer.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/data/iterables.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nreturn DataLoader(...)\nreturn list(range(1000))\n\n# pass loaders as a dict. This will create batches like this:\n# {'a': batch_from_loader_a, 'b': batch_from_loader_b}\nreturn {\"a\": DataLoader(...), \"b\": DataLoader(...)}\n\n# pass loaders as list. This will create batches like this:\n# [batch_from_dl_1, batch_from_dl_2]\nreturn [DataLoader(...), DataLoader(...)]\n\n# {'a': [batch_from_dl_1, batch_from_dl_2], 'b': [batch_from_dl_3, batch_from_dl_4]}\nreturn {\"a\": [dl1, dl2], \"b\": [dl3, dl4]}\n```\n\n----------------------------------------\n\nTITLE: Running Decoupled PPO Example with GPU Trainers\nDESCRIPTION: This command executes the decoupled PPO implementation with trainers running on GPU and 4 environments.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/fabric/reinforcement_learning/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\nfabric run --devices=3 train_fabric_decoupled.py --num-envs 4 --cuda\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Action Count Profiler in PyTorch Lightning\nDESCRIPTION: Creates a custom profiler that tracks the first occurrence and total count of each action. Extends the base Profiler class with methods to start, stop, summarize, and teardown profiling sessions.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/tuning/profiler_expert.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.profilers import Profiler\nfrom collections import defaultdict\nimport time\n\n\nclass ActionCountProfiler(Profiler):\n    def __init__(self, dirpath=None, filename=None):\n        super().__init__(dirpath=dirpath, filename=filename)\n        self._action_count = defaultdict(int)\n        self._action_first_occurrence = {}\n\n    def start(self, action_name):\n        if action_name not in self._action_first_occurrence:\n            self._action_first_occurrence[action_name] = time.strftime(\"%m/%d/%Y, %H:%M:%S\")\n\n    def stop(self, action_name):\n        self._action_count[action_name] += 1\n\n    def summary(self):\n        res = f\"\\nProfile Summary: \\n\"\n        max_len = max(len(x) for x in self._action_count)\n\n        for action_name in self._action_count:\n            # generate summary for actions called more than once\n            if self._action_count[action_name] > 1:\n                res += (\n                    f\"{action_name:<{max_len}s} \\t \"\n                    + \"self._action_first_occurrence[action_name]} \\t \"\n                    + \"{self._action_count[action_name]} \\n\"\n                )\n\n        return res\n\n    def teardown(self, stage):\n        self._action_count = {}\n        self._action_first_occurrence = {}\n        super().teardown(stage=stage)\n```\n\n----------------------------------------\n\nTITLE: Configuring Checkpoint Saving in PyTorch Lightning\nDESCRIPTION: Illustrates different ways to save checkpoints in PyTorch Lightning, including automatic saving and manual triggering.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/fsdp.rst#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Default: Saves a checkpoint every epoch\ntrainer = L.Trainer()\ntrainer.fit(model)\n\n# You can also manually trigger a checkpoint at any time\ntrainer.save_checkpoint(\"path/to/checkpoint/file\")\n\n# DON'T do this (inefficient):\n# torch.save(\"path/to/checkpoint/file\", model.state_dict())\n```\n\n----------------------------------------\n\nTITLE: Overriding Backward Method in PyTorch Lightning\nDESCRIPTION: This snippet demonstrates how to customize the backward pass in a PyTorch Lightning model by overriding the backward method of the LightningModule. It shows a simple implementation where the loss is directly backpropagated.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/model/build_model_advanced.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\n\n\nclass LitModel(L.LightningModule):\n    def backward(self, loss):\n        loss.backward()\n```\n\n----------------------------------------\n\nTITLE: Logging Model Topology with TensorBoard in PyTorch Lightning\nDESCRIPTION: This code shows how to log the model topology using TensorBoard in any Lightning Module function or hook. It creates a prototype input tensor and uses the log_graph method of the TensorBoard logger to visualize the model structure.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/logging_intermediate.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef any_lightning_module_function_or_hook(self):\n    tensorboard_logger = self.logger\n\n    prototype_array = torch.Tensor(32, 1, 28, 27)\n    tensorboard_logger.log_graph(model=self, input_array=prototype_array)\n```\n\n----------------------------------------\n\nTITLE: Proper Data Loading for Multi-GPU Training in Notebooks\nDESCRIPTION: Illustrates the correct pattern for data loading in multi-GPU notebook environments. Moving dataset creation and DataLoader initialization inside the training function helps prevent slowdowns and crashes like segmentation faults.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/notebooks.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# BAD: Don't load data in the main process\n# dataset = MyDataset(\"data/\")\n# dataloader = torch.utils.data.DataLoader(dataset)\n\n\ndef train(fabric):\n    # GOOD: Move data loading code into the training function\n    dataset = MyDataset(\"data/\")\n    dataloader = torch.utils.data.DataLoader(dataset)\n    ...\n\n\nfabric = Fabric(accelerator=\"cuda\", devices=2)\nfabric.launch(train)\n```\n\n----------------------------------------\n\nTITLE: Automatic Optimization Loop Implementation in PyTorch Lightning\nDESCRIPTION: Demonstrates how Lightning automatically handles the optimization process by implementing training loop with zero_grad(), backward() and step() operations. Shows the underlying implementation of the automatic optimization process.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/optimization.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfor epoch in epochs:\n    for batch in data:\n\n        def closure():\n            loss = model.training_step(batch, batch_idx)\n            optimizer.zero_grad()\n            loss.backward()\n            return loss\n\n        optimizer.step(closure)\n\n    lr_scheduler.step()\n```\n\n----------------------------------------\n\nTITLE: Custom Gradient Clipping Configuration in PyTorch Lightning\nDESCRIPTION: Shows how to implement custom gradient clipping by overriding the configure_gradient_clipping method. This example demonstrates applying stronger gradient clipping after a certain number of epochs.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/optimization.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef configure_gradient_clipping(self, optimizer, gradient_clip_val, gradient_clip_algorithm):\n    if self.current_epoch > 5:\n        gradient_clip_val = gradient_clip_val * 2\n\n    # Lightning will handle the gradient clipping\n    self.clip_gradients(optimizer, gradient_clip_val=gradient_clip_val, gradient_clip_algorithm=gradient_clip_algorithm)\n```\n\n----------------------------------------\n\nTITLE: Callback Configuration - YAML\nDESCRIPTION: Example YAML configuration for defining trainer callbacks with ModelCheckpoint and LearningRateMonitor.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_advanced_3.rst#2025-04-23_snippet_1\n\nLANGUAGE: yaml\nCODE:\n```\ntrainer:\n  callbacks:\n    - class_path: lightning.pytorch.callbacks.ModelCheckpoint\n      init_args:\n        save_weights_only: true\n    - class_path: lightning.pytorch.callbacks.LearningRateMonitor\n      init_args:\n        logging_interval: 'epoch'\n```\n\n----------------------------------------\n\nTITLE: Implementing Test DataLoader in Lightning DataModule\nDESCRIPTION: Demonstrates implementation of test_dataloader method for generating test data loaders with batch size 64.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/data/datamodule.rst#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\n\nclass MNISTDataModule(L.LightningDataModule):\n    def test_dataloader(self):\n        return DataLoader(self.mnist_test, batch_size=64)\n```\n\n----------------------------------------\n\nTITLE: Testing Multiple Models in PyTorch Lightning\nDESCRIPTION: Shows how to test multiple models using the same Trainer instance in PyTorch Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/evaluation_intermediate.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel1 = LitModel()\nmodel2 = GANModel()\n\ntrainer = Trainer()\ntrainer.test(model1)\ntrainer.test(model2)\n```\n\n----------------------------------------\n\nTITLE: Registering Callbacks with Fabric\nDESCRIPTION: Code example showing how to register one or multiple callbacks with the Fabric instance, allowing for custom hooks to be executed during training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_args.rst#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfabric = Fabric(callbacks=[MyCallback()])\n```\n\n----------------------------------------\n\nTITLE: Customizing Checkpoint Filename in PyTorch Lightning\nDESCRIPTION: This snippet shows how to customize the filename of saved checkpoints using the ModelCheckpoint callback, including dynamic metric values in the filename.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/checkpointing_intermediate.rst#2025-04-23_snippet_4\n\nLANGUAGE: Python\nCODE:\n```\nfrom lightning.pytorch.callbacks import ModelCheckpoint\n\n# saves a file like: my/path/sample-mnist-epoch=02-val_loss=0.32.ckpt\ncheckpoint_callback = ModelCheckpoint(\n    dirpath=\"my/path/\",\n    filename=\"sample-mnist-{epoch:02d}-{val_loss:.2f}\",\n)\n```\n\n----------------------------------------\n\nTITLE: Training AutoEncoder with PyTorch Lightning Trainer\nDESCRIPTION: Shows how to initialize and train the AutoEncoder model using PyTorch Lightning's Trainer class with training and validation dataloaders.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/child_modules.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nauto_encoder = AutoEncoder()\nlightning_module = LitAutoEncoder(auto_encoder)\ntrainer = Trainer()\ntrainer.fit(lightning_module, train_dataloader, val_dataloader)\n```\n\n----------------------------------------\n\nTITLE: Setting up MNIST Dataset and DataLoader\nDESCRIPTION: Code to initialize MNIST dataset and create a DataLoader for training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/starter/introduction.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndataset = MNIST(os.getcwd(), download=True, transform=ToTensor())\ntrain_loader = utils.data.DataLoader(dataset)\n```\n\n----------------------------------------\n\nTITLE: Configuring Hyperparameter Logging in PyTorch Lightning\nDESCRIPTION: Shows how to enable or disable automatic hyperparameter logging and how to implement custom hyperparameter logging for different logger types.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_35\n\nLANGUAGE: python\nCODE:\n```\n# default used by the Trainer\ntrainer = Trainer(enable_autolog_hparams=True)\n\n# disable logging hyperparams\ntrainer = Trainer(enable_autolog_hparams=False)\n```\n\nLANGUAGE: python\nCODE:\n```\nmodel = LitModel()\ntrainer = Trainer(enable_autolog_hparams=False)\nfor logger in trainer.loggers:\n    if isinstance(logger, lightning.pytorch.loggers.CSVLogger):\n        logger.log_hyperparams(hparams_dict_1)\n    else:\n        logger.log_hyperparams(hparams_dict_2)\n```\n\n----------------------------------------\n\nTITLE: Integrating Callback with Fabric Training Loop\nDESCRIPTION: Shows how to integrate a custom callback into a Fabric training loop, including initialization and calling the callback at the appropriate point in training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/callbacks.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.fabric import Fabric\n\n# The code of a callback can live anywhere, away from the training loop\nfrom my_callbacks import MyCallback\n\n# Add one or several callbacks:\nfabric = Fabric(callbacks=[MyCallback()])\n\n...\n\nfor iteration, batch in enumerate(train_dataloader):\n    ...\n    fabric.backward(loss)\n    optimizer.step()\n\n    # Let a callback add some arbitrary processing at the appropriate place\n    # Give the callback access to some variables\n    fabric.call(\"on_train_batch_end\", loss=loss, output=...)\n```\n\n----------------------------------------\n\nTITLE: Full Example of Distributed Checkpoint Saving\nDESCRIPTION: Complete implementation showing how to save distributed checkpoints with a transformer model across 4 GPUs using FSDP strategy.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/checkpointing_expert.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\nfrom lightning.pytorch.strategies import FSDPStrategy\nfrom lightning.pytorch.demos import LightningTransformer\n\nmodel = LightningTransformer()\n\nstrategy = FSDPStrategy(state_dict_type=\"sharded\")\ntrainer = L.Trainer(\n    accelerator=\"cuda\",\n    devices=4,\n    strategy=strategy,\n    max_steps=3,\n)\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Model Checkpointing in PyTorch Lightning\nDESCRIPTION: Shows how to configure model checkpointing in PyTorch Lightning using the ModelCheckpoint callback.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/README.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ncheckpointing = ModelCheckpoint(monitor=\"val_loss\")\ntrainer = Trainer(callbacks=[checkpointing])\n```\n\n----------------------------------------\n\nTITLE: Selecting Precision Modes in Fabric (Python)\nDESCRIPTION: Demonstrates how to select different precision modes when initializing a Fabric object. Includes options for FP32, FP16, BFloat16, Float8, and double precision.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/precision.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.fabric import Fabric\n\n# This is the default\nfabric = Fabric(precision=\"32-true\")\n\n# Also FP32 (legacy)\nfabric = Fabric(precision=32)\n\n# FP32 as well (legacy)\nfabric = Fabric(precision=\"32\")\n\n# Float16 mixed precision\nfabric = Fabric(precision=\"16-mixed\")\n\n# Float16 true half precision\nfabric = Fabric(precision=\"16-true\")\n\n# BFloat16 mixed precision (Volta GPUs and later)\nfabric = Fabric(precision=\"bf16-mixed\")\n\n# BFloat16 true half precision (Volta GPUs and later)\nfabric = Fabric(precision=\"bf16-true\")\n\n# 8-bit mixed precision via TransformerEngine (Hopper GPUs and later)\nfabric = Fabric(precision=\"transformer-engine\")\n\n# Double precision\nfabric = Fabric(precision=\"64-true\")\n\n# Or (legacy)\nfabric = Fabric(precision=\"64\")\n\n# Or (legacy)\nfabric = Fabric(precision=64)\n```\n\n----------------------------------------\n\nTITLE: Integrating StreamingDataset with PyTorch Lightning\nDESCRIPTION: Shows how to use the StreamingDataset library with PyTorch Lightning. This example includes creating a custom dataset class, writing data, and using it with a DataLoader in the training process.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/data/alternatives.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\nfrom streaming import MDSWriter, StreamingDataset\n\n\nclass YourDataset(StreamingDataset):\n    ...\n\n\n# you could do this in the `prepare_data` hook too\nwith MDSWriter(out=\"...\", columns=...) as out:\n    out.write(...)\n\ntrain_dataset = YourDataset()\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size)\nmodel = ...\ntrainer = L.Trainer()\ntrainer.fit(model, train_dataloader)\n```\n\n----------------------------------------\n\nTITLE: Using Transformer Engine for 8-bit Precision\nDESCRIPTION: Integration with NVIDIA's Transformer Engine to enable mixed 8-bit precision for more efficient training of large models, especially useful for transformer architectures.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Using NVIDIA's Transformer Engine for 8-bit precision\ntrainer = Trainer(precision=\"transformer-engine\")\n```\n\n----------------------------------------\n\nTITLE: Multiple Models Configuration - YAML\nDESCRIPTION: Demonstrates how to configure multiple models and datasets with their respective parameters in YAML format.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_advanced_3.rst#2025-04-23_snippet_2\n\nLANGUAGE: yaml\nCODE:\n```\nmodel:\n  class_path: mycode.mymodels.MyModel\n  init_args:\n    decoder_layers:\n    - 2\n    - 4\n    encoder_layers: 12\ndata:\n  class_path: mycode.mydatamodules.MyDataModule\n  init_args:\n    ...\ntrainer:\n  callbacks:\n    - class_path: lightning.pytorch.callbacks.EarlyStopping\n      init_args:\n        patience: 5\n    ...\n```\n\n----------------------------------------\n\nTITLE: Initializing CPU-based Distributed Training\nDESCRIPTION: Sets up a PyTorch Lightning Trainer for distributed training on CPU using DDP strategy across 2 devices for debugging purposes.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/debug/debugging_advanced.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntrainer = Trainer(accelerator=\"cpu\", strategy=\"ddp\", devices=2)\n```\n\n----------------------------------------\n\nTITLE: Optimized Model Initialization with configure_model\nDESCRIPTION: Shows how to optimize model initialization for large models using the configure_model hook.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/fsdp.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass LanguageModel(L.LightningModule):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.model = None\n\n    def configure_model(self):\n        if self.model is not None:\n            return\n        self.model = Transformer(  # 1B parameters\n            vocab_size=self.vocab_size,\n            nlayers=32,\n            nhid=4096,\n            ninp=1024,\n            nhead=64,\n        )\n```\n\n----------------------------------------\n\nTITLE: Running PPO on LunarLander-v2 with Lightning Fabric\nDESCRIPTION: This command demonstrates running the PPO algorithm on the LunarLander-v2 environment using Lightning Fabric with specific hyperparameters and settings.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/fabric/reinforcement_learning/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nfabric run \\\n  --accelerator=cpu \\\n  --strategy=ddp \\\n  --devices=2 \\\n  train_fabric.py \\\n  --capture-video \\\n  --env-id LunarLander-v2 \\\n  --total-timesteps 500000 \\\n  --ortho-init \\\n  --num-envs 2 \\\n  --num-steps 2048 \\\n  --seed 1\n```\n\n----------------------------------------\n\nTITLE: Custom Action Profiling in LightningModule\nDESCRIPTION: Demonstrates how to profile custom actions within a LightningModule using the profiler.profile() context manager.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/tuning/profiler_expert.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass MyModel(LightningModule):\n    def custom_processing_step(self, data):\n        with self.profiler.profile(\"my_custom_action\"):\n            ...\n        return data\n```\n\n----------------------------------------\n\nTITLE: Custom Method Export with TorchScript\nDESCRIPTION: Demonstrates how to export specific methods using TorchScript decorators. The example shows a Monte Carlo Dropout model implementation with a custom predict_step method marked for export.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/deploy/production_advanced_2.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass LitMCdropoutModel(L.LightningModule):\n    def __init__(self, model, mc_iteration):\n        super().__init__()\n        self.model = model\n        self.dropout = nn.Dropout()\n        self.mc_iteration = mc_iteration\n\n    @torch.jit.export\n    def predict_step(self, batch, batch_idx):\n        # enable Monte Carlo Dropout\n        self.dropout.train()\n\n        # take average of `self.mc_iteration` iterations\n        pred = [self.dropout(self.model(x)).unsqueeze(0) for _ in range(self.mc_iteration)]\n        pred = torch.vstack(pred).mean(dim=0)\n        return pred\n\n\nmodel = LitMCdropoutModel(...)\nscript = model.to_torchscript(file_path=\"model.pt\", method=\"script\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Dynamic Pruning with ModelPruning Callback in PyTorch Lightning\nDESCRIPTION: This snippet shows how to use the ModelPruning callback for dynamic pruning. It defines a custom function to compute the pruning amount based on the current epoch, allowing for iterative pruning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/pruning_quantization.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef compute_amount(epoch):\n    # the sum of all returned values need to be smaller than 1\n    if epoch == 10:\n        return 0.5\n\n    elif epoch == 50:\n        return 0.25\n\n    elif 75 < epoch < 99:\n        return 0.01\n\n\n# the amount can be also be a callable\ntrainer = Trainer(callbacks=[ModelPruning(\"l1_unstructured\", amount=compute_amount)])\n```\n\n----------------------------------------\n\nTITLE: Optimizing Gradient Accumulation for Distributed Training with PyTorch Lightning Fabric\nDESCRIPTION: This code snippet demonstrates how to optimize gradient accumulation for distributed training using Fabric's no_backward_sync context manager. It skips gradient synchronization during the accumulation phase to improve performance.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/gradient_accumulation.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfor iteration, batch in enumerate(dataloader):\n\n    # Accumulate gradient 8 batches at a time\n    is_accumulating = iteration % 8 != 0\n\n    with fabric.no_backward_sync(model, enabled=is_accumulating):\n        output = model(input)\n        loss = ...\n\n        # .backward() accumulates when .zero_grad() wasn't called\n        fabric.backward(loss)\n\n    ...\n\n    if not is_accumulating:\n        # Step the optimizer after accumulation phase is over\n        optimizer.step()\n        optimizer.zero_grad()\n```\n\n----------------------------------------\n\nTITLE: Using BFloat16 Mixed Precision in PyTorch Lightning\nDESCRIPTION: This code snippet shows how to enable BFloat16 mixed precision training in PyTorch Lightning using the Trainer class. It works for both GPU and CPU training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/precision_intermediate.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nTrainer(accelerator=\"gpu\", devices=1, precision=\"bf16-mixed\")\n```\n\nLANGUAGE: python\nCODE:\n```\nTrainer(precision=\"bf16-mixed\")\n```\n\n----------------------------------------\n\nTITLE: Launching TensorBoard from Command Line for PyTorch Lightning Logs\nDESCRIPTION: Command to launch TensorBoard dashboard for visualizing PyTorch Lightning logs.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/logging_basic.rst#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ntensorboard --logdir=lightning_logs/\n```\n\n----------------------------------------\n\nTITLE: Configuring 8-bit Adam Optimizer with BitsandBytes\nDESCRIPTION: Shows how to implement 8-bit Adam optimizer for additional memory savings and configure embedding layers to use 32-bit precision for stability.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/precision.rst#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nimport bitsandbytes as bnb\n\noptimizer = bnb.optim.Adam8bit(model.parameters(), lr=0.001, betas=(0.9, 0.995))\n\n# (optional) force embedding layers to use 32 bit for numerical stability\n# https://github.com/huggingface/transformers/issues/14819#issuecomment-1003445038\nfor module in model.modules():\n    if isinstance(module, torch.nn.Embedding):\n        bnb.optim.GlobalOptimManager.get_instance().register_module_override(module, \"weight\", {\"optim_bits\": 32})\n```\n\n----------------------------------------\n\nTITLE: Saving Checkpoints with Fabric\nDESCRIPTION: Save model and optimizer state to a checkpoint file, handling distributed training scenarios correctly. This method replaces torch.save() and automatically extracts state dictionaries from model and optimizer objects.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_methods.rst#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Define the state of your program/loop\nstate = {\n    \"model1\": model1,\n    \"model2\": model2,\n    \"optimizer\": optimizer,\n    \"iteration\": iteration,\n}\n\n# Instead of `torch.save(...)`\nfabric.save(\"path/to/checkpoint.ckpt\", state)\n```\n\n----------------------------------------\n\nTITLE: Trainer Usage in Python Scripts\nDESCRIPTION: Example showing how to properly structure a Python script using the Trainer with command line arguments.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom argparse import ArgumentParser\n\n\ndef main(hparams):\n    model = LightningModule()\n    trainer = Trainer(accelerator=hparams.accelerator, devices=hparams.devices)\n    trainer.fit(model)\n\n\nif __name__ == \"__main__\":\n    parser = ArgumentParser()\n    parser.add_argument(\"--accelerator\", default=None)\n    parser.add_argument(\"--devices\", default=None)\n    args = parser.parse_args()\n\n    main(args)\n```\n\nLANGUAGE: bash\nCODE:\n```\npython main.py --accelerator 'gpu' --devices 2\n```\n\n----------------------------------------\n\nTITLE: Implementing a CLI with PyTorch Lightning\nDESCRIPTION: Python code demonstrating how to implement a CLI using the LightningCLI class with a LightningModule and LightningDataModule.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_intermediate.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# main.py\nfrom lightning.pytorch.cli import LightningCLI\n\n# simple demo classes for your convenience\nfrom lightning.pytorch.demos.boring_classes import DemoModel, BoringDataModule\n\n\ndef cli_main():\n    cli = LightningCLI(DemoModel, BoringDataModule)\n    # note: don't call fit!!\n\n\nif __name__ == \"__main__\":\n    cli_main()\n    # note: it is good practice to implement the CLI in a function and call it in the main if block\n```\n\n----------------------------------------\n\nTITLE: Defining Custom Callback Class\nDESCRIPTION: Demonstrates how to create a custom callback class with a training batch end hook. The callback can execute arbitrary code at the end of each training step.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/callbacks.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass MyCallback:\n    def on_train_batch_end(self, loss, output):\n        # Here, put any code you want to run at the end of a training step\n        ...\n```\n\n----------------------------------------\n\nTITLE: Full Example of Quantizing PyTorch Lightning Model\nDESCRIPTION: Complete Python code example showing how to quantize a PyTorch Lightning model using Intel Neural Compressor, including configuration and evaluation function setup.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/post_training_quantization.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom neural_compressor.quantization import fit as fit\nfrom neural_compressor.config import PostTrainingQuantConfig\n\n\ndef eval_func_for_nc(model_n, trainer_n):\n    setattr(model, \"model\", model_n)\n    result = trainer_n.validate(model=model, dataloaders=dm.val_dataloader())\n    return result[0][\"accuracy\"]\n\n\ndef eval_func(model):\n    return eval_func_for_nc(model, trainer)\n\n\nconf = PostTrainingQuantConfig()\nq_model = fit(model=model.model, conf=conf, calib_dataloader=dm.val_dataloader(), eval_func=eval_func)\n\nq_model.save(\"./saved_model/\")\n```\n\n----------------------------------------\n\nTITLE: Callback Hook Execution in PyTorch Lightning Fabric\nDESCRIPTION: Demonstrates how to define and execute callback hooks in Fabric. Shows different ways to call hooks with and without additional arguments.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_methods.rst#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\nclass MyCallback:\n    def on_train_start(self):\n        ...\n\n    def on_train_epoch_end(self, model, results):\n        ...\n\n\nfabric = Fabric(callbacks=[MyCallback()])\n\n# Call any hook by name\nfabric.call(\"on_train_start\")\n\n# Pass in additional arguments that the hook requires\nfabric.call(\"on_train_epoch_end\", model=..., results={...})\n\n# Only the callbacks that have this method defined will be executed\nfabric.call(\"undefined\")\n```\n\n----------------------------------------\n\nTITLE: Initializing PyTorch Profiler for Distributed Training\nDESCRIPTION: Configures PyTorch profiler for distributed model training by specifying a filename parameter. This setup generates separate performance reports for each rank in distributed training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/tuning/profiler_intermediate.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.profilers import PyTorchProfiler\n\nprofiler = PyTorchProfiler(filename=\"perf-logs\")\ntrainer = Trainer(profiler=profiler)\n```\n\n----------------------------------------\n\nTITLE: Calling Callback Hooks in Fabric\nDESCRIPTION: Example of calling a callback hook by name during the training loop, which will execute this hook in all registered callbacks that implement it.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_args.rst#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Call any hook by name\nfabric.call(\"on_train_epoch_end\", results={...})\n```\n\n----------------------------------------\n\nTITLE: Creating a Custom Strategy in PyTorch Lightning\nDESCRIPTION: Example of creating a custom strategy by subclassing an existing strategy (DDPStrategy). This custom strategy overrides the configure_ddp and setup methods to provide custom behavior.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/extensions/strategy.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.strategies import DDPStrategy\n\n\nclass CustomDDPStrategy(DDPStrategy):\n    def configure_ddp(self):\n        self.model = MyCustomDistributedDataParallel(\n            self.model,\n            device_ids=...,\n        )\n\n    def setup(self, trainer):\n        # you can access the accelerator and plugins directly\n        self.accelerator.setup()\n        self.precision_plugin.connect(...)\n```\n\n----------------------------------------\n\nTITLE: Gathering Data Across Processes\nDESCRIPTION: Demonstrates how to gather and stack tensor data from all processes in a distributed setup.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/distributed_communication.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfabric = Fabric(devices=4, accelerator=\"cpu\")\nfabric.launch()\n\n# Data is different in each process\ndata = torch.tensor(10 * fabric.global_rank)\n\n# Every process gathers the tensors from all other processes\n# and stacks the result:\nresult = fabric.all_gather(data)\nprint(\"Result of all-gather:\", result)  # tensor([ 0, 10, 20, 30])\n```\n\n----------------------------------------\n\nTITLE: Configuring Sharded Checkpoint Saving in FSDP Strategy\nDESCRIPTION: Demonstrates how to configure the FSDP strategy to save sharded checkpoints for improved efficiency with large models.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/fsdp.rst#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Default: Save a single, consolidated checkpoint file\nstrategy = FSDPStrategy(state_dict_type=\"full\")\n\n# Save individual files with state from each process\nstrategy = FSDPStrategy(state_dict_type=\"sharded\")\n```\n\n----------------------------------------\n\nTITLE: Implementing Early Stopping in PyTorch Lightning\nDESCRIPTION: Demonstrates how to add early stopping functionality to a PyTorch Lightning trainer using the EarlyStopping callback.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/README.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nes = EarlyStopping(monitor=\"val_loss\")\ntrainer = Trainer(callbacks=[es])\n```\n\n----------------------------------------\n\nTITLE: Logging Metrics to Command Line Progress Bar in PyTorch Lightning\nDESCRIPTION: Demonstrates how to log metrics to the command line progress bar by setting the prog_bar argument to True.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/logging_basic.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nself.log(..., prog_bar=True)\n```\n\n----------------------------------------\n\nTITLE: Using MetricCollection with native logging in PyTorch Lightning\nDESCRIPTION: Added support for native logging of MetricCollection with enabled compute groups.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_16\n\nLANGUAGE: Python\nCODE:\n```\n# Example usage:\nfrom torchmetrics import MetricCollection\nfrom pytorch_lightning import LightningModule\n\nclass MyModule(LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.metrics = MetricCollection(...)\n    \n    def training_step(self, batch, batch_idx):\n        # metrics are logged automatically\n        self.metrics(predictions, targets)\n\n```\n\n----------------------------------------\n\nTITLE: Configuring Data Loading with Tensor Parallelism in PyTorch Lightning\nDESCRIPTION: Example demonstrating how to set up a data loader with PyTorch Lightning when using Tensor Parallelism. Shows proper configuration of dataset with fixed seed for randomization and automatic sampler configuration by the Lightning Trainer.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/tp_fsdp.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\n\ntrainer = L.Trainer(...)\n\n# Define dataset/dataloader\n# If there is randomness/augmentation in the dataset, fix the seed\ndataset = MyDataset(seed=42)\ndataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n\n# PyTorch Lightning configures the sampler automatically for you such that\n# all batches in a tensor-parallel group are identical,\n# while still sharding the dataset across the data-parallel group\ntrainer.fit(model, dataloader)\n\nfor i, batch in enumerate(dataloader):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Configuring Advanced Profiler with Custom Output\nDESCRIPTION: Sets up advanced profiling with custom directory path and filename for storing performance logs.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/tuning/profiler_basic.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.profilers import AdvancedProfiler\n\nprofiler = AdvancedProfiler(dirpath=\".\", filename=\"perf_logs\")\ntrainer = Trainer(profiler=profiler)\n```\n\n----------------------------------------\n\nTITLE: Initializing Profiler in LightningModule\nDESCRIPTION: Shows how to initialize a profiler within a LightningModule class, with a fallback to PassThroughProfiler if none is provided.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/tuning/profiler_expert.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.profilers import SimpleProfiler, PassThroughProfiler\n\n\nclass MyModel(LightningModule):\n    def __init__(self, profiler=None):\n        self.profiler = profiler or PassThroughProfiler()\n```\n\n----------------------------------------\n\nTITLE: Initializing ModelCheckpoint Callback in PyTorch Lightning\nDESCRIPTION: This snippet demonstrates how to initialize a ModelCheckpoint callback with custom options for saving checkpoints based on validation loss.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/checkpointing_intermediate.rst#2025-04-23_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nfrom lightning.pytorch.callbacks import ModelCheckpoint\n\ncheckpoint_callback = ModelCheckpoint(dirpath=\"my/path/\", save_top_k=2, monitor=\"val_loss\")\ntrainer = Trainer(callbacks=[checkpoint_callback])\ntrainer.fit(model)\ncheckpoint_callback.best_model_path\n```\n\n----------------------------------------\n\nTITLE: Implementing Feature Extraction with AutoEncoder in PyTorch Lightning\nDESCRIPTION: Demonstrates how to use a pretrained AutoEncoder as a feature extractor for a CIFAR10 classifier. The AutoEncoder is loaded from a checkpoint and frozen to prevent further training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/transfer_learning.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass Encoder(torch.nn.Module):\n    ...\n\n\nclass AutoEncoder(LightningModule):\n    def __init__(self):\n        self.encoder = Encoder()\n        self.decoder = Decoder()\n\n\nclass CIFAR10Classifier(LightningModule):\n    def __init__(self):\n        # init the pretrained LightningModule\n        self.feature_extractor = AutoEncoder.load_from_checkpoint(PATH)\n        self.feature_extractor.freeze()\n\n        # the autoencoder outputs a 100-dim representation and CIFAR-10 has 10 classes\n        self.classifier = nn.Linear(100, 10)\n\n    def forward(self, x):\n        representations = self.feature_extractor(x)\n        x = self.classifier(representations)\n        ...\n```\n\n----------------------------------------\n\nTITLE: Creating Validation Split from Training Data\nDESCRIPTION: Shows how to split training data into training and validation sets using PyTorch\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/evaluation_basic.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# use 20% of training data for validation\ntrain_set_size = int(len(train_set) * 0.8)\nvalid_set_size = len(train_set) - train_set_size\n\n# split the train set into two\nseed = torch.Generator().manual_seed(42)\ntrain_set, valid_set = data.random_split(train_set, [train_set_size, valid_set_size], generator=seed)\n```\n\n----------------------------------------\n\nTITLE: Initializing Fabric for Multi-Node Training in Python\nDESCRIPTION: Configure Fabric for distributed training across multiple nodes and GPUs. Supports different distributed strategies like classic DDP, DeepSpeed, and FSDP.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/multi_node/slurm.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.fabric import Fabric\n\n# Train on 32 GPUs across 4 nodes\nfabric = Fabric(accelerator=\"gpu\", devices=8, num_nodes=4)\n```\n\nLANGUAGE: python\nCODE:\n```\n# DeepSpeed\nfabric = Fabric(accelerator=\"gpu\", devices=8, num_nodes=4, strategy=\"deepspeed\")\n\n# Fully Sharded Data Parallel (FSDP)\nfabric = Fabric(accelerator=\"gpu\", devices=8, num_nodes=4, strategy=\"fsdp\")\n```\n\nLANGUAGE: python\nCODE:\n```\nfabric = Fabric(...)\nfabric.launch()\n```\n\n----------------------------------------\n\nTITLE: Basic TorchRun Command Structure for Distributed Training\nDESCRIPTION: Generic command structure for launching distributed training with TorchRun, showing all required parameters for multi-node setup including process count, node count, rank, master address and port.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/clouds/cluster_intermediate_2.rst#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ntorchrun \\\n    --nproc_per_node=<GPUS_PER_NODE> \\\n    --nnodes=<NUM_NODES> \\\n    --node_rank <NODE_RANK> \\\n    --master_addr <MASTER_ADDR> \\\n    --master_port <MASTER_PORT> \\\n    train.py --arg1 --arg2\n```\n\n----------------------------------------\n\nTITLE: Enabling 16-bit Precision for TPU Training in PyTorch Lightning\nDESCRIPTION: Demonstrates how to enable 16-bit precision when training on TPUs. This uses BFloat16 format under the hood via the XLA library.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/accelerators/tpu_intermediate.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\n\nmy_model = MyLightningModule()\ntrainer = L.Trainer(accelerator=\"tpu\", precision=\"16-true\")\ntrainer.fit(my_model)\n```\n\n----------------------------------------\n\nTITLE: Training a PyTorch Lightning Model via CLI\nDESCRIPTION: Examples of using the CLI to train a model with different hyperparameters, including changing learning rate, output dimensions, and trainer options.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_intermediate.rst#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# change the learning_rate\npython main.py fit --model.learning_rate 0.1\n\n# change the output dimensions also\npython main.py fit --model.out_dim 10 --model.learning_rate 0.1\n\n# change trainer and data arguments too\npython main.py fit --model.out_dim 2 --model.learning_rate 0.1 --data.data_dir '~/' --trainer.logger False\n```\n\n----------------------------------------\n\nTITLE: Complete Implementation of Custom Profiling\nDESCRIPTION: Full example showing how to implement and use custom profiling in a PyTorch Lightning model, including initialization and trainer setup.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/tuning/profiler_expert.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.profilers import SimpleProfiler, PassThroughProfiler\n\n\nclass MyModel(LightningModule):\n    def __init__(self, profiler=None):\n        self.profiler = profiler or PassThroughProfiler()\n\n    def custom_processing_step(self, data):\n        with self.profiler.profile(\"my_custom_action\"):\n            ...\n        return data\n\n\nprofiler = SimpleProfiler()\nmodel = MyModel(profiler)\ntrainer = Trainer(profiler=profiler, max_epochs=1)\n```\n\n----------------------------------------\n\nTITLE: Accessing TensorBoard Logger API in Training Step\nDESCRIPTION: Demonstrates how to access the TensorBoard logger's experiment API within the training_step method to log various artifacts like images, histograms, and figures.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/experiment_managers.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef training_step(self):\n    tensorboard = self.logger.experiment\n    tensorboard.add_image()\n    tensorboard.add_histogram(...)\n    tensorboard.add_figure(...)\n```\n\n----------------------------------------\n\nTITLE: Configuring Gradient Clipping in PyTorch Lightning Trainer\nDESCRIPTION: Demonstrates how to enable gradient clipping in the PyTorch Lightning Trainer. It shows examples of not clipping gradients, clipping the global norm, and clipping the maximum magnitude of gradients.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/training_tricks.rst#2025-04-23_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# DEFAULT (ie: don't clip)\ntrainer = Trainer(gradient_clip_val=0)\n\n# clip gradients' global norm to <=0.5 using gradient_clip_algorithm='norm' by default\ntrainer = Trainer(gradient_clip_val=0.5)\n\n# clip gradients' maximum magnitude to <=0.5\ntrainer = Trainer(gradient_clip_val=0.5, gradient_clip_algorithm=\"value\")\n```\n\n----------------------------------------\n\nTITLE: Using Trainer.print for Rank-Zero-Only Output\nDESCRIPTION: Using the Trainer.print method to print messages only from the local rank zero process, which prevents duplicate output in distributed training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Print only from local rank zero\ntrainer.print(\"This message appears only once per node\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Maximum Steps in PyTorch Lightning\nDESCRIPTION: Examples demonstrating how to set the maximum number of global steps for training. This provides an alternative way to limit training duration based on iterations rather than epochs.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# Default (disabled)\ntrainer = Trainer(max_steps=-1)\n\n# Stop after 100 steps\ntrainer = Trainer(max_steps=100)\n```\n\n----------------------------------------\n\nTITLE: Training with 16-bit Precision in PyTorch Lightning\nDESCRIPTION: Shows how to configure the PyTorch Lightning Trainer to use 16-bit precision training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/extensions/plugins.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Training with 16-bit precision\ntrainer = Trainer(precision=16)\n```\n\n----------------------------------------\n\nTITLE: Configuring Console Logging in PyTorch Lightning\nDESCRIPTION: Demonstrates how to configure the Python logging system for PyTorch Lightning, including setting the log level and redirecting specific module logs to files. This provides control over the verbosity and destination of Lightning's console output.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/extensions/logging.rst#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\n# configure logging at the root level of Lightning\nlogging.getLogger(\"lightning.pytorch\").setLevel(logging.ERROR)\n\n# configure logging on module level, redirect to file\nlogger = logging.getLogger(\"lightning.pytorch.core\")\nlogger.addHandler(logging.FileHandler(\"core.log\"))\n```\n\n----------------------------------------\n\nTITLE: Configuring DeepSpeed with Inline Dictionary in PyTorch Lightning\nDESCRIPTION: Sets up a PyTorch Lightning Trainer with an inline DeepSpeed configuration dictionary. The configuration includes optimizer settings, learning rate scheduler with warmup, and ZeRO stage 2 optimization with CPU offloading and communication optimizations.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/deepspeed.rst#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nmodel = MyModel()\ntrainer = Trainer(accelerator=\"gpu\", devices=4, strategy=DeepSpeedStrategy(config=deepspeed_config), precision=16)\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom DeepSpeed Settings in PyTorch Lightning\nDESCRIPTION: Shows how to use a custom DeepSpeed configuration in PyTorch Lightning to access advanced parameters and use custom optimizers. This allows for fine-grained control over the DeepSpeed integration.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/deepspeed.rst#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch import Trainer\nfrom lightning.pytorch.strategies import DeepSpeedStrategy\n\ndeepspeed_config = {\n    \"zero_allow_untested_optimizer\": True,\n    \"optimizer\": {\n        \"type\": \"OneBitAdam\",\n        \"params\": {\n            \"lr\": 3e-5,\n            \"betas\": [0.998, 0.999],\n            \"eps\": 1e-5,\n```\n\n----------------------------------------\n\nTITLE: Modifying Checkpoint Content Using Callbacks in PyTorch Lightning\nDESCRIPTION: This snippet demonstrates how to modify checkpoint content using callback methods instead of LightningModule hooks.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/checkpointing_intermediate.rst#2025-04-23_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nimport lightning as L\n\nclass LitCallback(L.Callback):\n    def on_save_checkpoint(self, checkpoint):\n        checkpoint[\"something_cool_i_want_to_save\"] = my_cool_pickable_object\n\n    def on_load_checkpoint(self, checkpoint):\n        my_cool_pickable_object = checkpoint[\"something_cool_i_want_to_save\"]\n```\n\n----------------------------------------\n\nTITLE: Implementing a Custom Trainer with Lightning Fabric in Python\nDESCRIPTION: This snippet demonstrates how to create a custom trainer class using Lightning Fabric. It includes initialization with configurable parameters and a fit method that sets up the model, optimizer, and dataloader using Fabric, then runs the training loop.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning_fabric/README.md#2025-04-23_snippet_2\n\nLANGUAGE: Python\nCODE:\n```\nimport lightning as L\n\n\nclass MyCustomTrainer:\n    def __init__(self, accelerator=\"auto\", strategy=\"auto\", devices=\"auto\", precision=\"32-true\"):\n        self.fabric = L.Fabric(accelerator=accelerator, strategy=strategy, devices=devices, precision=precision)\n\n    def fit(self, model, optimizer, dataloader, max_epochs):\n        self.fabric.launch()\n\n        model, optimizer = self.fabric.setup(model, optimizer)\n        dataloader = self.fabric.setup_dataloaders(dataloader)\n        model.train()\n\n        for epoch in range(max_epochs):\n            for batch in dataloader:\n                input, target = batch\n                optimizer.zero_grad()\n                output = model(input)\n                loss = loss_fn(output, target)\n                self.fabric.backward(loss)\n                optimizer.step()\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom ClusterEnvironment in PyTorch Lightning\nDESCRIPTION: This code snippet demonstrates how to create a custom ClusterEnvironment class in PyTorch Lightning. It overrides methods to retrieve cluster information from environment variables, such as world size, global rank, local rank, node rank, main address, and main port.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/clouds/cluster_expert.rst#2025-04-23_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\nimport os\nfrom lightning.pytorch.plugins.environments import ClusterEnvironment\n\n\nclass MyClusterEnvironment(ClusterEnvironment):\n    @property\n    def creates_processes_externally(self) -> bool:\n        \"\"\"Return True if the cluster is managed (you don't launch processes yourself)\"\"\"\n        return True\n\n    def world_size(self) -> int:\n        return int(os.environ[\"WORLD_SIZE\"])\n\n    def global_rank(self) -> int:\n        return int(os.environ[\"RANK\"])\n\n    def local_rank(self) -> int:\n        return int(os.environ[\"LOCAL_RANK\"])\n\n    def node_rank(self) -> int:\n        return int(os.environ[\"NODE_RANK\"])\n\n    def main_address(self) -> str:\n        return os.environ[\"MASTER_ADDRESS\"]\n\n    def main_port(self) -> int:\n        return int(os.environ[\"MASTER_PORT\"])\n\n\ntrainer = Trainer(plugins=[MyClusterEnvironment()])\n```\n\n----------------------------------------\n\nTITLE: Configuring Auto-parallelism Strategy\nDESCRIPTION: Example of configuring automatic parallelism strategy for multi-node training scenarios.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/model_parallel/tp_fsdp.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.fabric.strategies import ModelParallelStrategy\n\nstrategy = ModelParallelStrategy(\n    data_parallel_size=\"auto\",\n    tensor_parallel_size=\"auto\",\n)\n```\n\n----------------------------------------\n\nTITLE: Switching to GPU-based Distributed Training\nDESCRIPTION: Converts the debugging setup to production by changing the accelerator to GPU while maintaining the same distributed strategy.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/debug/debugging_advanced.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntrainer = Trainer(accelerator=\"gpu\", strategy=\"ddp\", devices=2)\n```\n\n----------------------------------------\n\nTITLE: Configuring Activation Checkpointing Policy in FSDP\nDESCRIPTION: Setting a custom layer policy for automatic activation checkpointing in FSDP strategy, which helps manage memory usage for large models by trading computation for memory.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# Custom activation checkpointing policy for FSDP (requires torch>=2.1)\nstrategy = FSDPStrategy(activation_checkpointing_policy=CustomLayerPolicy)\n```\n\n----------------------------------------\n\nTITLE: Using Estimated Stepping Batches for Learning Rate Scheduling\nDESCRIPTION: Shows how to access the estimated number of optimizer steps during training, which is useful for configuring learning rate schedulers that depend on the total number of steps.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_39\n\nLANGUAGE: python\nCODE:\n```\ndef configure_optimizers(self):\n    optimizer = ...\n    stepping_batches = self.trainer.estimated_stepping_batches\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=1e-3, total_steps=stepping_batches)\n    return {\n        \"optimizer\": optimizer,\n        \"lr_scheduler\": {\"scheduler\": scheduler, \"interval\": \"step\"},\n    }\n```\n\n----------------------------------------\n\nTITLE: Using LightningModule Hooks with Fabric for Training Loop\nDESCRIPTION: This snippet demonstrates how to integrate a LightningModule with Fabric in a training loop. It shows how to obtain optimizers and dataloaders from the module, set them up with Fabric, and call the appropriate hooks at the right time during training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/lightning_module.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\n\nfabric = L.Fabric(...)\n\n# Instantiate the LightningModule\nmodel = LitModel()\n\n# Get the optimizer(s) from the LightningModule\noptimizer = model.configure_optimizers()\n\n# Get the training data loader from the LightningModule\ntrain_dataloader = model.train_dataloader()\n\n# Set up objects\nmodel, optimizer = fabric.setup(model, optimizer)\ntrain_dataloader = fabric.setup_dataloaders(train_dataloader)\n\n# Call the hooks at the right time\nmodel.on_train_start()\n\nmodel.train()\nfor epoch in range(num_epochs):\n    for i, batch in enumerate(dataloader):\n        optimizer.zero_grad()\n        loss = model.training_step(batch, i)\n        fabric.backward(loss)\n        optimizer.step()\n\n        # Control when hooks are called\n        if condition:\n            model.any_hook_you_like()\n```\n\n----------------------------------------\n\nTITLE: Configuring Accuracy-driven Quantization with Intel Neural Compressor\nDESCRIPTION: Python code to set up accuracy-driven automatic tuning for INT8 model quantization using Intel Neural Compressor.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/post_training_quantization.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom neural_compressor.config import PostTrainingQuantConfig, TuningCriterion, AccuracyCriterion\n\naccuracy_criterion = AccuracyCriterion(tolerable_loss=0.01)\ntuning_criterion = TuningCriterion(max_trials=600)\nconf = PostTrainingQuantConfig(\n    approach=\"static\", backend=\"default\", tuning_criterion=tuning_criterion, accuracy_criterion=accuracy_criterion\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Profilers in PyTorch Lightning\nDESCRIPTION: Examples demonstrating how to configure performance profilers to identify bottlenecks during training. Different profiler options provide varying levels of detail.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_27\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.profilers import SimpleProfiler, AdvancedProfiler\n\n# default used by the Trainer\ntrainer = Trainer(profiler=None)\n\n# to profile standard training events, equivalent to `profiler=SimpleProfiler()`\ntrainer = Trainer(profiler=\"simple\")\n\n# advanced profiler for function-level stats, equivalent to `profiler=AdvancedProfiler()`\ntrainer = Trainer(profiler=\"advanced\")\n```\n\n----------------------------------------\n\nTITLE: Training Termination with Both Minimum Steps and Epochs\nDESCRIPTION: Shows training termination that respects both minimum steps and epochs. Training continues until both conditions (5 steps and 5 epochs) are satisfied, even when should_stop is set at step 7.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_44\n\nLANGUAGE: python\nCODE:\n```\nclass LitModel(LightningModule):\n    def training_step(self, *args, **kwargs):\n        if self.global_step == 7:\n            self.trainer.should_stop = True\n\n\ntrainer = Trainer(min_steps=5, min_epochs=5, max_epochs=100)\nmodel = LitModel()\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Dynamic Gradient Accumulation Scheduling in PyTorch Lightning\nDESCRIPTION: Shows how to implement dynamic gradient accumulation using GradientAccumulationScheduler callback. The scheduler changes accumulation batches based on epoch number, transitioning from 8 to 4 to 1 batch accumulation.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/gradient_accumulation.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.callbacks import GradientAccumulationScheduler\n\n# till 5th epoch, it will accumulate every 8 batches. From 5th epoch\n# till 9th epoch it will accumulate every 4 batches and after that no accumulation\n# will happen. Note that you need to use zero-indexed epoch keys here\naccumulator = GradientAccumulationScheduler(scheduling={0: 8, 4: 4, 8: 1})\ntrainer = Trainer(callbacks=accumulator)\n```\n\n----------------------------------------\n\nTITLE: Setting Disk Cache Limits for StreamingDataset\nDESCRIPTION: Shows how to limit the disk cache size used by StreamingDataset. This is useful when working with large datasets to control memory usage.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/data/README.md#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.data import StreamingDataset\n\ndataset = StreamingDataset(..., max_cache_size=\"10GB\")\n```\n\n----------------------------------------\n\nTITLE: Moving Data to Device with Fabric\nDESCRIPTION: Move tensors or collections of tensors to the current device. This is typically used when manual device management is needed beyond what setup() and setup_dataloaders() provide.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_methods.rst#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndata = torch.load(\"dataset.pt\")\ndata = fabric.to_device(data)\n```\n\n----------------------------------------\n\nTITLE: Training Model with Lightning Trainer\nDESCRIPTION: Instantiation of the autoencoder model and training using Lightning Trainer.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/model/train_model_basic.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# model\nautoencoder = LitAutoEncoder(Encoder(), Decoder())\n\n# train model\ntrainer = L.Trainer()\ntrainer.fit(model=autoencoder, train_dataloaders=train_loader)\n```\n\n----------------------------------------\n\nTITLE: Using LightningDataModule in PyTorch Lightning\nDESCRIPTION: Demonstrates how to use a LightningDataModule with a model and trainer in PyTorch Lightning. This example shows the simplicity of swapping datasets using DataModules.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/data/datamodule.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nmodel = LitClassifier()\ntrainer = Trainer()\n\nimagenet = ImagenetDataModule()\ntrainer.fit(model, datamodule=imagenet)\n\ncifar10 = CIFAR10DataModule()\ntrainer.fit(model, datamodule=cifar10)\n```\n\n----------------------------------------\n\nTITLE: Updating LightningModule Step Methods in PyTorch Lightning\nDESCRIPTION: Changes to the signature of training_step, validation_step, test_step, and predict_step methods in LightningModule. The batch_idx parameter is removed, and dataloader_iter now returns a triplet (batch, batch_idx, dataloader_idx).\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/2_0_advanced.rst#2025-04-23_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\ndef training_step(self, dataloader_iter):\n    batch, batch_idx, dataloader_idx = next(dataloader_iter)\n    # Rest of the method implementation\n```\n\nLANGUAGE: Python\nCODE:\n```\ndef validation_step(self, dataloader_iter):\n    batch, batch_idx, dataloader_idx = next(dataloader_iter)\n    # Rest of the method implementation\n```\n\nLANGUAGE: Python\nCODE:\n```\ndef test_step(self, dataloader_iter):\n    batch, batch_idx, dataloader_idx = next(dataloader_iter)\n    # Rest of the method implementation\n```\n\nLANGUAGE: Python\nCODE:\n```\ndef predict_step(self, dataloader_iter):\n    batch, batch_idx, dataloader_idx = next(dataloader_iter)\n    # Rest of the method implementation\n```\n\n----------------------------------------\n\nTITLE: Resizing Images with Lightning's Map Function\nDESCRIPTION: Demonstrates how to use Lightning's map function to resize multiple images in parallel. The example loads images from an S3 bucket, resizes them to 224x224 pixels, and saves them to a new location with multiple workers.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/data/README.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom lightning.data import map\nfrom PIL import Image\n\ninput_dir = \"s3://my-bucket/my_images\"\ninputs = [os.path.join(input_dir, f) for f in os.listdir(input_dir)]\n\ndef resize_image(image_path, output_dir):\n  output_image_path = os.path.join(output_dir, os.path.basename(image_path))\n  Image.open(image_path).resize((224, 224)).save(output_image_path)\n  \nif __name__ == \"__main__\":\n    map(\n        fn=resize_image,\n        inputs=inputs, \n        output_dir=\"s3://my-bucket/my_resized_images\",\n        num_workers=4,\n    )\n```\n\n----------------------------------------\n\nTITLE: Running CLI Programmatically in Python\nDESCRIPTION: Examples of how to run the Lightning CLI programmatically using either command line style arguments or a dictionary configuration.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_advanced_3.rst#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom my_module.my_cli import cli_main\n\ncli_main([\"--trainer.max_epochs=100\", \"--model.encoder_layers=24\"])\n```\n\nLANGUAGE: python\nCODE:\n```\nargs = {\n    \"trainer\": {\n        \"max_epochs\": 100,\n    },\n    \"model\": {},\n}\n\nargs[\"model\"][\"encoder_layers\"] = 8\ncli_main(args)\nargs[\"model\"][\"encoder_layers\"] = 12\ncli_main(args)\nargs[\"trainer\"][\"max_epochs\"] = 200\ncli_main(args)\n```\n\n----------------------------------------\n\nTITLE: Loading and Running TorchScript Model\nDESCRIPTION: Shows how to load a saved TorchScript model and run inference with it using PyTorch. The example demonstrates creating input data and getting output from the scripted module.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/deploy/production_advanced_2.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ninp = torch.rand(1, 64)\nscripted_module = torch.jit.load(\"model.pt\")\noutput = scripted_module(inp)\n```\n\n----------------------------------------\n\nTITLE: Configuring Sanity Check Steps in PyTorch Lightning\nDESCRIPTION: Shows how to set the number of validation steps to run as a sanity check before training in PyTorch Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/debug/debugging_basic.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrainer = Trainer(num_sanity_val_steps=2)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Remote TensorBoard Logging\nDESCRIPTION: Demonstrates how to configure TensorBoard logger to use remote storage for logging data using S3 bucket path.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/remote_fs.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.loggers import TensorBoardLogger\n\nlogger = TensorBoardLogger(save_dir=\"s3://my_bucket/logs/\")\n\ntrainer = Trainer(logger=logger)\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Linking Arguments with Delayed Instantiation in Python\nDESCRIPTION: Demonstrates how to link arguments that are only available after class instantiation, such as linking the number of classes from a data module to a model.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_expert.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nclass MyLightningCLI(LightningCLI):\n    def add_arguments_to_parser(self, parser):\n        parser.link_arguments(\"data.num_classes\", \"model.num_classes\", apply_on=\"instantiate\")\n\n\ncli = MyLightningCLI(MyClassModel, MyDataModule)\n```\n\n----------------------------------------\n\nTITLE: Exporting PyTorch Lightning Model to ONNX\nDESCRIPTION: Shows how to export a PyTorch Lightning model to ONNX format for production use, including input sample generation and file handling.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/README.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# onnx\nwith tempfile.NamedTemporaryFile(suffix=\".onnx\", delete=False) as tmpfile:\n    autoencoder = LitAutoEncoder()\n    input_sample = torch.randn((1, 64))\n    autoencoder.to_onnx(tmpfile.name, input_sample, export_params=True)\n    os.path.isfile(tmpfile.name)\n```\n\n----------------------------------------\n\nTITLE: Basic Gradient Accumulation Configuration in PyTorch Lightning\nDESCRIPTION: Demonstrates how to configure basic gradient accumulation in PyTorch Lightning trainer. Shows default behavior with no accumulation and accumulation over 7 batches.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/gradient_accumulation.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# DEFAULT (ie: no accumulated grads)\ntrainer = Trainer(accumulate_grad_batches=1)\n\n# Accumulate gradients for 7 batches\ntrainer = Trainer(accumulate_grad_batches=7)\n```\n\n----------------------------------------\n\nTITLE: Advanced Model Compilation with Float8 and Distribution\nDESCRIPTION: Demonstrates combining torch.compile with Float8 quantization and model distribution strategies.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/compile.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom lightning.pytorch.demos import Transformer\nfrom torch.distributed._composable.fsdp.fully_shard import fully_shard\nfrom torch.distributed.device_mesh import DeviceMesh\nfrom torchao.float8 import Float8LinearConfig, convert_to_float8_training\n\ndef parallelize(model: nn.Module, device_mesh: DeviceMesh) -> nn.Module:\n    float8_config = Float8LinearConfig(\n        pad_inner_dim=True,\n    )\n\n    def module_filter_fn(mod: torch.nn.Module, fqn: str):\n        return fqn != \"decoder\"\n\n    convert_to_float8_training(model, config=float8_config, module_filter_fn=module_filter_fn)\n\n    for module in model.modules():\n        if isinstance(module, (torch.nn.TransformerEncoderLayer, torch.nn.TransformerDecoderLayer)):\n            fully_shard(module, mesh=device_mesh)\n\n    fully_shard(model, mesh=device_mesh)\n\n    return torch.compile(model)\n\ndef train():\n    L.seed_everything(42)\n\n    with torch.device(\"meta\"):\n        model = Transformer(\n            vocab_size=50257,\n            nlayers=16,\n            nhid=4096,\n            ninp=1024,\n            nhead=32,\n        )\n\n    strategy = ModelParallelStrategy(data_parallel_size=4, tensor_parallel_size=1, parallelize_fn=parallelize)\n\n    fabric = L.Fabric(precision=\"bf16-true\", strategy=strategy)\n    fabric.launch()\n\n    model = fabric.setup(model)\n```\n\n----------------------------------------\n\nTITLE: Configuring Model Summary in PyTorch Lightning\nDESCRIPTION: Shows how to enable or disable model summarization in PyTorch Lightning, including using custom model summary callbacks.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_33\n\nLANGUAGE: python\nCODE:\n```\n# default used by the Trainer\ntrainer = Trainer(enable_model_summary=True)\n\n# disable summarization\ntrainer = Trainer(enable_model_summary=False)\n\n# enable custom summarization\nfrom lightning.pytorch.callbacks import ModelSummary\n\ntrainer = Trainer(enable_model_summary=True, callbacks=[ModelSummary(max_depth=-1)])\n```\n\n----------------------------------------\n\nTITLE: Configuring Trainer for Overfitting Tests in PyTorch Lightning\nDESCRIPTION: Demonstrates how to configure the PyTorch Lightning Trainer to overfit on a small subset of training data for debugging purposes. Shows different ways to limit the number of batches used for training and validation.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/debug/debugging_intermediate.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# use only 1% of training data\ntrainer = Trainer(overfit_batches=0.01)\n\n# similar, but with a fixed 10 batches\ntrainer = Trainer(overfit_batches=10)\n\n# equivalent to\ntrainer = Trainer(limit_train_batches=10, limit_val_batches=10)\n```\n\n----------------------------------------\n\nTITLE: Enabling Autograd Anomaly Detection in PyTorch Lightning\nDESCRIPTION: Shows how to enable PyTorch's built-in autograd anomaly detection in the Lightning Trainer to help identify issues in the autograd engine.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/debug/debugging_intermediate.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrainer = Trainer(detect_anomaly=True)\n```\n\n----------------------------------------\n\nTITLE: Configuring Trainer with Remote Storage Path\nDESCRIPTION: Shows how to initialize a PyTorch Lightning Trainer with a remote storage path for logs and checkpoints using S3 as an example.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/remote_fs.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# `default_root_dir` is the default path used for logs and checkpoints\ntrainer = Trainer(default_root_dir=\"s3://my_bucket/data/\")\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Setting Minimum Steps in PyTorch Lightning\nDESCRIPTION: Examples showing how to set the minimum number of global steps for training. This ensures the model trains for at least the specified number of iterations.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_20\n\nLANGUAGE: python\nCODE:\n```\n# Default (disabled)\ntrainer = Trainer(min_steps=None)\n\n# Run at least for 100 steps (disable min_epochs)\ntrainer = Trainer(min_steps=100, min_epochs=0)\n```\n\n----------------------------------------\n\nTITLE: Extending TQDM Progress Bar\nDESCRIPTION: Example of customizing the TQDMProgressBar by subclassing and overriding specific methods.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/logging_expert.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.callbacks import TQDMProgressBar\n\n\nclass LitProgressBar(TQDMProgressBar):\n    def init_validation_tqdm(self):\n        bar = super().init_validation_tqdm()\n        bar.set_description(\"running validation...\")\n        return bar\n```\n\n----------------------------------------\n\nTITLE: Model Compilation with PyTorch\nDESCRIPTION: Shows how to compile a LightningModule for improved performance on modern GPUs.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/speed.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel = MyLightningModule()\n\n# Compile the model\nmodel = torch.compile(model)\n\ntrainer = Trainer()\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Launching a Function with Fabric\nDESCRIPTION: Launch a function for distributed training across multiple devices. This approach is particularly useful when working in Jupyter notebooks or when you need to encapsulate training logic in a function.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_methods.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ndef run(fabric):\n    # Your distributed code here\n    ...\n\n\n# Launch a function on 2 devices and init distributed backend\nfabric = Fabric(devices=2)\nfabric.launch(run)\n```\n\n----------------------------------------\n\nTITLE: Accessing Callback Metrics in PyTorch Lightning\nDESCRIPTION: Shows how to access metrics that were logged and are available to callbacks during training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_36\n\nLANGUAGE: python\nCODE:\n```\ndef training_step(self, batch, batch_idx):\n    self.log(\"a_val\", 2.0)\n\n\ncallback_metrics = trainer.callback_metrics\nassert callback_metrics[\"a_val\"] == 2.0\n```\n\n----------------------------------------\n\nTITLE: Resuming Training from Checkpoint in PyTorch Lightning\nDESCRIPTION: This code shows how to resume training from a checkpoint using PyTorch Lightning's Trainer. The strict_loading parameter is set to False, allowing for flexibility when loading weights that might not perfectly match the current model structure.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/checkpointing_intermediate.rst#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\ntrainer = Trainer()\nmodel = LitModel()\n\n# Will load weights with `.load_state_dict(strict=model.strict_loading)`\ntrainer.fit(model, ckpt_path=\"path/to/checkpoint\")\n```\n\n----------------------------------------\n\nTITLE: Implementing AutoEncoder and LightningModule Classes\nDESCRIPTION: Defines the core AutoEncoder architecture and its Lightning wrapper. Includes encoder-decoder implementation, forward pass, and training/validation/test steps with MSE loss metric.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/child_modules.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass Encoder(torch.nn.Module):\n    ...\n\n\nclass Decoder(torch.nn.Module):\n    ...\n\n\nclass AutoEncoder(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = Encoder()\n        self.decoder = Decoder()\n\n    def forward(self, x):\n        return self.decoder(self.encoder(x))\n\n\nclass LitAutoEncoder(LightningModule):\n    def __init__(self, auto_encoder):\n        super().__init__()\n        self.auto_encoder = auto_encoder\n        self.metric = torch.nn.MSELoss()\n\n    def forward(self, x):\n        return self.auto_encoder.encoder(x)\n\n    def training_step(self, batch, batch_idx):\n        x, _ = batch\n        x_hat = self.auto_encoder(x)\n        loss = self.metric(x, x_hat)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        self._shared_eval(batch, batch_idx, \"val\")\n\n    def test_step(self, batch, batch_idx):\n        self._shared_eval(batch, batch_idx, \"test\")\n\n    def _shared_eval(self, batch, batch_idx, prefix):\n        x, _ = batch\n        x_hat = self.auto_encoder(x)\n        loss = self.metric(x, x_hat)\n        self.log(f\"{prefix}_loss\", loss)\n```\n\n----------------------------------------\n\nTITLE: Configuring DataLoader for GPU Training\nDESCRIPTION: Shows how to optimize DataLoader configuration for GPU training with worker and memory settings.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/speed.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nDataloader(dataset, num_workers=8, pin_memory=True)\n```\n\n----------------------------------------\n\nTITLE: Saving Hyperparameters in PyTorch Lightning Module\nDESCRIPTION: This snippet shows how to automatically save and track hyperparameters in a PyTorch Lightning Module using the save_hyperparameters() method in the module's __init__ function.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/logging_intermediate.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass MyLightningModule(LightningModule):\n    def __init__(self, learning_rate, another_parameter, *args, **kwargs):\n        super().__init__()\n        self.save_hyperparameters()\n```\n\n----------------------------------------\n\nTITLE: Implementing Autoencoder with PyTorch Lightning in Notebooks\nDESCRIPTION: Full example of implementing an autoencoder using PyTorch Lightning in a notebook environment. Includes model definition, data loading, and training setup.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/notebooks.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\nfrom torch import nn, optim, utils\nimport torchvision\n\nencoder = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\ndecoder = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n\n\nclass LitAutoEncoder(L.LightningModule):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def training_step(self, batch, batch_idx):\n        x, _ = batch\n        x = x.view(x.size(0), -1)\n        z = self.encoder(x)\n        x_hat = self.decoder(z)\n        loss = nn.functional.mse_loss(x_hat, x)\n        self.log(\"train_loss\", loss)\n        return loss\n\n    def configure_optimizers(self):\n        return optim.Adam(self.parameters(), lr=1e-3)\n\n    def prepare_data(self):\n        torchvision.datasets.MNIST(\".\", download=True)\n\n    def train_dataloader(self):\n        dataset = torchvision.datasets.MNIST(\".\", transform=torchvision.transforms.ToTensor())\n        return utils.data.DataLoader(dataset, batch_size=64)\n\n\nautoencoder = LitAutoEncoder(encoder, decoder)\ntrainer = L.Trainer(max_epochs=2, devices=\"auto\")\ntrainer.fit(model=autoencoder)\n```\n\n----------------------------------------\n\nTITLE: Implementing Multiple Callbacks in Fabric\nDESCRIPTION: Demonstrates how to use multiple callbacks simultaneously in Fabric, showing both initialization and calling of multiple callback implementations.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/callbacks.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Add multiple callback implementations in a list\ncallback1 = LearningRateMonitor()\ncallback2 = Profiler()\nfabric = Fabric(callbacks=[callback1, callback2])\n\n# Let Fabric call the implementations (if they exist)\nfabric.call(\"any_callback_method\", arg1=..., arg2=...)\n\n# fabric.call is the same as doing this\ncallback1.any_callback_method(arg1=..., arg2=...)\ncallback2.any_callback_method(arg1=..., arg2=...)\n```\n\n----------------------------------------\n\nTITLE: Saving Distributed Checkpoints with FSDP Strategy\nDESCRIPTION: Demonstrates how to save a distributed checkpoint using Fabric's FSDP strategy. The checkpoint is sharded across multiple GPUs to improve efficiency and avoid memory issues.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/checkpoint/distributed_checkpoint.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\nfrom lightning.fabric.strategies import FSDPStrategy\n\n# 1. Select the FSDP strategy\nstrategy = FSDPStrategy(\n    # Default: sharded/distributed checkpoint\n    state_dict_type=\"sharded\",\n    # Full checkpoint (not distributed)\n    # state_dict_type=\"full\",\n)\n\nfabric = L.Fabric(devices=2, strategy=strategy, ...)\nfabric.launch()\n...\nmodel, optimizer = fabric.setup(model, optimizer)\n\n# 2. Define model, optimizer, and other training loop state\nstate = {\"model\": model, \"optimizer\": optimizer, \"iter\": iteration}\n\n# DON'T do this (inefficient):\n# state = {\"model\": model.state_dict(), \"optimizer\": optimizer.state_dict(), ...}\n\n# 3. Save using Fabric's method\nfabric.save(\"path/to/checkpoint/file\", state)\n\n# DON'T do this (inefficient):\n# torch.save(\"path/to/checkpoint/file\", state)\n```\n\n----------------------------------------\n\nTITLE: Using Auto-Detection for Accelerators in Fabric\nDESCRIPTION: Example of using the 'auto' option for accelerator configuration which automatically selects the available hardware accelerator based on the machine's capabilities.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_args.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# If your machine has GPUs, it will use the GPU Accelerator\nfabric = Fabric(devices=2, accelerator=\"auto\")\n```\n\n----------------------------------------\n\nTITLE: Launching Fabric for Multi-Device Training\nDESCRIPTION: Call the launch method when using multiple devices (e.g., multi-GPU) to initiate the distributed environment.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/convert.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfabric.launch()\n```\n\n----------------------------------------\n\nTITLE: Enabling Variable Interpolation in Lightning CLI YAML Config\nDESCRIPTION: This YAML snippet shows an example of variable interpolation in a Lightning CLI configuration file. It demonstrates how to reference one value in another within the same configuration.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_advanced_2.rst#2025-04-23_snippet_5\n\nLANGUAGE: yaml\nCODE:\n```\nmodel:\n  encoder_layers: 12\n  decoder_layers:\n  - ${model.encoder_layers}\n  - 4\n```\n\n----------------------------------------\n\nTITLE: Creating a Composite Model for Multiple Models with One Optimizer\nDESCRIPTION: Demonstrates how to group multiple models under a single parent module to use with one optimizer. This pattern creates a composite model that can be treated as a single entity.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/multiple_setup.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass AutoEncoder(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        # Group all models under a common nn.Module\n        self.encoder = Encoder()\n        self.decoder = Decoder()\n```\n\n----------------------------------------\n\nTITLE: Configuring Lightning Fabric with Model Parallel Strategy\nDESCRIPTION: Code to configure the ModelParallelStrategy in Lightning Fabric. The strategy takes the custom parallelization function as input, and Fabric is set up to use multiple CUDA devices for tensor parallelism.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/model_parallel/tp.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\nfrom lightning.fabric.strategies import ModelParallelStrategy\n\n# 1. Pass the parallelization function to the strategy\nstrategy = ModelParallelStrategy(parallelize_fn=parallelize_feedforward)\n\n# 2. Configure devices and set the strategy in Fabric\nfabric = L.Fabric(accelerator=\"cuda\", devices=2, strategy=strategy)\nfabric.launch()\n```\n\n----------------------------------------\n\nTITLE: Complete Example of Distributed Checkpoint Saving\nDESCRIPTION: Full implementation example showing how to save a distributed checkpoint for a large transformer model with 1B parameters using FSDP strategy across multiple GPUs.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/checkpoint/distributed_checkpoint.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport torch\nimport torch.nn.functional as F\n\nimport lightning as L\nfrom lightning.fabric.strategies import FSDPStrategy\nfrom lightning.pytorch.demos import Transformer, WikiText2\n\nstrategy = FSDPStrategy(state_dict_type=\"sharded\")\nfabric = L.Fabric(accelerator=\"cuda\", devices=4, strategy=strategy)\nfabric.launch()\n\nwith fabric.rank_zero_first():\n    dataset = WikiText2()\n\n# 1B parameters\nmodel = Transformer(vocab_size=dataset.vocab_size, nlayers=32, nhid=4096, ninp=1024, nhead=64)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n\nmodel, optimizer = fabric.setup(model, optimizer)\n\nstate = {\"model\": model, \"optimizer\": optimizer, \"iteration\": 0}\n\nfor i in range(10):\n    input, target = fabric.to_device(dataset[i])\n    output = model(input.unsqueeze(0), target.unsqueeze(0))\n    loss = F.nll_loss(output, target.view(-1))\n    fabric.backward(loss)\n    optimizer.step()\n    optimizer.zero_grad()\n    fabric.print(loss.item())\n\nfabric.print(\"Saving checkpoint ...\")\nt0 = time.time()\nfabric.save(\"my-checkpoint.ckpt\", state)\nfabric.print(f\"Took {time.time() - t0:.2f} seconds.\")\n```\n\n----------------------------------------\n\nTITLE: Setting Class Type Defaults in LightningCLI with Python\nDESCRIPTION: Shows how to properly set defaults for class type arguments in LightningCLI to ensure reproducibility and avoid issues with mutable defaults.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_expert.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndefault_backbone = {\n    \"class_path\": \"import.path.of.MyModel\",\n    \"init_args\": {\n        \"encoder_layers\": 24,\n    },\n}\n\n\nclass MyLightningCLI(LightningCLI):\n    def add_arguments_to_parser(self, parser):\n        parser.set_defaults({\"model.backbone\": default_backbone})\n```\n\n----------------------------------------\n\nTITLE: Implementing FeedForward Neural Network Model in Python\nDESCRIPTION: Basic implementation of a feed-forward neural network with three linear layers for tensor parallelism demonstration.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/model_parallel/tp_fsdp.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim):\n        super().__init__()\n        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n\n    def forward(self, x):\n        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n```\n\n----------------------------------------\n\nTITLE: Implementing PowerSGD Communication Hook in DDP Strategy\nDESCRIPTION: Configures DDP to use PowerSGD communication hook for improved multi-node throughput. PowerSGD provides a compression mechanism for gradient communication that can significantly reduce communication overhead.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/ddp_optimizations.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\nfrom lightning.pytorch.strategies import DDPStrategy\nfrom torch.distributed.algorithms.ddp_comm_hooks import powerSGD_hook as powerSGD\n\nmodel = MyModel()\ntrainer = L.Trainer(\n    accelerator=\"gpu\",\n    devices=4,\n    strategy=DDPStrategy(\n        ddp_comm_state=powerSGD.PowerSGDState(\n            process_group=None,\n            matrix_approximation_rank=1,\n            start_powerSGD_iter=5000,\n        ),\n        ddp_comm_hook=powerSGD.powerSGD_hook,\n    ),\n)\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Setting FSDP Sharding Strategy via String\nDESCRIPTION: Configuration for setting the torch.distributed.fsdp.ShardingStrategy in FSDPStrategy using a simple string identifier instead of importing and using the enum values directly.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Setting FSDP sharding strategy with string\nstrategy = FSDPStrategy(sharding_strategy=\"FULL_SHARD\")\n```\n\n----------------------------------------\n\nTITLE: Configuring LearningRateMonitor with Weight Decay Logging\nDESCRIPTION: Setting up the LearningRateMonitor callback to log not only learning rates but also weight decay values, providing more complete optimization parameter tracking.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Monitor both learning rate and weight decay\ncallback = LearningRateMonitor(log_weight_decay=True)\n```\n\n----------------------------------------\n\nTITLE: Benchmarking Compiled vs Uncompiled Models\nDESCRIPTION: Complete example demonstrating how to benchmark a compiled InceptionV3 model against its uncompiled version, including proper warmup and timing mechanisms.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/compile.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport statistics\nimport torch\nimport torchvision.models as models\nimport lightning as L\n\n\n@torch.no_grad()\ndef benchmark(model, input, num_iters=10):\n    \"\"\"Runs the model on the input several times and returns the median execution time.\"\"\"\n    start = torch.cuda.Event(enable_timing=True)\n    end = torch.cuda.Event(enable_timing=True)\n    times = []\n    for _ in range(num_iters):\n        start.record()\n        model(input)\n        end.record()\n        torch.cuda.synchronize()\n        times.append(start.elapsed_time(end) / 1000)\n    return statistics.median(times)\n\n\nfabric = L.Fabric(accelerator=\"cuda\", devices=1)\n\nmodel = models.inception_v3()\ninput = torch.randn(16, 3, 510, 512, device=fabric.device)\n\n# Compile!\ncompiled_model = torch.compile(model)\n\n# Set up the model with Fabric\nmodel = fabric.setup(model)\ncompiled_model = fabric.setup(compiled_model)\n\n# warm up the compiled model before we benchmark\ncompiled_model(input)\n\n# Run multiple forward passes and time them\neager_time = benchmark(model, input)\ncompile_time = benchmark(compiled_model, input)\n\n# Compare the speedup for the compiled execution\nspeedup = eager_time / compile_time\nprint(f\"Eager median time: {eager_time:.4f} seconds\")\nprint(f\"Compile median time: {compile_time:.4f} seconds\")\nprint(f\"Speedup: {speedup:.1f}x\")\n```\n\n----------------------------------------\n\nTITLE: Initializing XLAProfiler with PyTorch Lightning\nDESCRIPTION: Sets up the XLAProfiler with a specified port and configures it with a PyTorch Lightning Trainer. The profiler is used to analyze TPU model performance.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/tuning/profiler_advanced.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.profilers import XLAProfiler\n\nprofiler = XLAProfiler(port=9001)\ntrainer = Trainer(profiler=profiler)\n```\n\n----------------------------------------\n\nTITLE: Using Custom Metric in PyTorch Lightning Module\nDESCRIPTION: This snippet shows how to integrate a custom metric into a PyTorch Lightning module, including initialization, computation, and logging.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/logging_advanced.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass LitModel(LightningModule):\n    def __init__(self):\n        # 1. initialize the metric\n        self.accuracy = MyAccuracy()\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        preds = self(x)\n\n        # 2. compute the metric\n        self.accuracy(preds, y)\n\n        # 3. log it\n        self.log(\"train_acc_step\", self.accuracy)\n```\n\n----------------------------------------\n\nTITLE: Registering Buffers in PyTorch Lightning Models\nDESCRIPTION: Shows how to register tensors as buffers in a LightningModule to ensure they move to the correct device along with the module.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/accelerators/accelerator_prepare.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass LitModel(LightningModule):\n    def __init__(self):\n        ...\n        self.register_buffer(\"sigma\", torch.eye(3))\n        # you can now access self.sigma anywhere in your module\n```\n\n----------------------------------------\n\nTITLE: Running ONNX Model with ONNX Runtime\nDESCRIPTION: Shows how to use the ONNX runtime to run an exported ONNX model. It demonstrates setting up an inference session and performing inference with random input data.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/deploy/production_advanced.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport onnxruntime\n\nort_session = onnxruntime.InferenceSession(filepath)\ninput_name = ort_session.get_inputs()[0].name\nort_inputs = {input_name: np.random.randn(1, 64)}\nort_outs = ort_session.run(None, ort_inputs)\n```\n\n----------------------------------------\n\nTITLE: Partial Checkpoint Loading Example in PyTorch\nDESCRIPTION: Demonstrates how to load checkpoints with non-strict matching for transfer learning scenarios.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/checkpoint/checkpoint.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport lightning as L\n\nfabric = L.Fabric()\n\n# Save a checkpoint of a trained model\nmodel1 = torch.nn.Linear(2, 2, bias=True)\nstate = {\"model\": model1}\nfabric.save(\"state.ckpt\", state)\n\n# Later on, make a new model that misses a parameter\nmodel2 = torch.nn.Linear(2, 2, bias=False)\nstate = {\"model\": model2}\n\n# `strict=True` would lead to an error, because the bias\n# parameter is missing, but we can load the rest of the\n# parameters successfully\nfabric.load(\"state.ckpt\", state, strict=False)\n```\n\n----------------------------------------\n\nTITLE: Configuring Gradient Clipping in PyTorch Lightning\nDESCRIPTION: Demonstrates different methods of gradient clipping configuration in the PyTorch Lightning Trainer to prevent gradient explosion. Shows both norm-based and value-based clipping approaches.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/debug/debugging_intermediate.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# DEFAULT (ie: don't clip)\ntrainer = Trainer(gradient_clip_val=0)\n\n# clip gradients' global norm to <=0.5 using gradient_clip_algorithm='norm' by default\ntrainer = Trainer(gradient_clip_val=0.5)\n\n# clip gradients' maximum magnitude to <=0.5\ntrainer = Trainer(gradient_clip_val=0.5, gradient_clip_algorithm=\"value\")\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorBoard Logger in PyTorch Lightning\nDESCRIPTION: Demonstrates how to configure the TensorBoard logger and pass it to the PyTorch Lightning Trainer. Also shows how to access the logger within a LightningModule.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/supported_exp_managers.rst#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.loggers import TensorBoardLogger\n\nlogger = TensorBoardLogger()\ntrainer = Trainer(logger=logger)\n```\n\nLANGUAGE: python\nCODE:\n```\nclass LitModel(LightningModule):\n    def any_lightning_module_function_or_hook(self):\n        tensorboard_logger = self.logger.experiment\n        fake_images = torch.Tensor(32, 3, 28, 28)\n        tensorboard_logger.add_image(\"generated_images\", fake_images, 0)\n```\n\n----------------------------------------\n\nTITLE: Testing with Additional DataLoaders in PyTorch Lightning\nDESCRIPTION: Demonstrates how to test a model using additional data loaders that were not defined in the LightningModule.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/evaluation_intermediate.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# setup your data loader\ntest_dataloader = DataLoader(...)\n\n# test (pass in the loader)\ntrainer.test(dataloaders=test_dataloader)\n```\n\n----------------------------------------\n\nTITLE: Implementing DistributedSampler for TPU in PyTorch Lightning\nDESCRIPTION: Demonstrates how to manually create a DistributedSampler for TPU support in the train_dataloader method, though this is usually handled automatically by Lightning. It shows how to determine the number of replicas and rank using XLA's utilities.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/accelerators/tpu_intermediate.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch_xla.core.xla_model as xm\n\n\ndef train_dataloader(self):\n    dataset = MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor())\n\n    # required for TPU support\n    sampler = None\n    if use_tpu:\n        sampler = torch.utils.data.distributed.DistributedSampler(\n            dataset, num_replicas=xm.xrt_world_size(), rank=xm.get_ordinal(), shuffle=True\n        )\n\n    loader = DataLoader(dataset, sampler=sampler, batch_size=32)\n\n    return loader\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-GPU Training with PyTorch Lightning\nDESCRIPTION: Examples of Trainer configuration for multi-GPU and multi-node training, showing how effective batch size scales with devices and nodes.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/accelerators/gpu_faq.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# effective batch size = 7 * 8\nTrainer(accelerator=\"gpu\", devices=8, strategy=...)\n\n# effective batch size = 7 * 8 * 10\nTrainer(accelerator=\"gpu\", devices=8, num_nodes=10, strategy=...)\n```\n\n----------------------------------------\n\nTITLE: Using Random List of Batches in PyTorch Lightning\nDESCRIPTION: Demonstrates how to use a random list of batches as a data iterable in PyTorch Lightning. This example creates a list of tuples containing random tensors for input data and labels.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/data/alternatives.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# random list of batches\ndata = [(torch.rand(32, 3, 32, 32), torch.randint(0, 10, (32,))) for _ in range(100)]\nmodel = LitClassifier()\ntrainer = Trainer()\ntrainer.fit(model, data)\n```\n\n----------------------------------------\n\nTITLE: Running DeepSpeed ZeRO Stage 2 Offload via Command Line\nDESCRIPTION: Command line instruction to run PyTorch Lightning with DeepSpeed ZeRO Stage 2 Offload strategy for memory optimization.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/deepspeed.rst#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython train.py --strategy deepspeed_stage_2_offload --precision 16 --accelerator 'gpu' --devices 4\n```\n\n----------------------------------------\n\nTITLE: Validation and Testing with Trainer\nDESCRIPTION: Examples of using the Trainer for validation and testing phases.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrainer.validate(model=model, dataloaders=val_dataloaders)\n\ntrainer.test(dataloaders=test_dataloaders)\n```\n\n----------------------------------------\n\nTITLE: Using New Training Step Variant with Dataloader Iterator in PyTorch Lightning\nDESCRIPTION: A new flavor of the training_step method that accepts dataloader_iter as an argument, allowing more control over data iteration.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_27\n\nLANGUAGE: python\nCODE:\n```\ndef training_step(self, dataloader_iter):\n    # Get batch from the iterator directly\n    batch = next(dataloader_iter)\n    # Process the batch\n    x, y = batch\n    y_hat = self(x)\n    loss = self.loss_fn(y_hat, y)\n    return loss\n```\n\n----------------------------------------\n\nTITLE: Adding Plugins to PyTorch Lightning Trainer\nDESCRIPTION: Demonstrates how to add one or multiple plugins to the PyTorch Lightning Trainer using the 'plugins' argument.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/extensions/plugins.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntrainer = Trainer(plugins=[plugin1, plugin2, ...])\n```\n\n----------------------------------------\n\nTITLE: Configuring ModelParallelStrategy for 2D Parallelism in Python\nDESCRIPTION: This code snippet demonstrates how to configure the ModelParallelStrategy with specific data parallel and tensor parallel sizes, and set it up in the Lightning Trainer.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/tp_fsdp.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\nfrom lightning.pytorch.strategies import ModelParallelStrategy\n\n# 1. Create the strategy\nstrategy = ModelParallelStrategy(\n    # Define the size of the 2D parallelism\n    # Set these to \"auto\" (default) to apply TP intra-node and FSDP inter-node\n    data_parallel_size=2,\n    tensor_parallel_size=2,\n)\n\n# 2. Configure devices and set the strategy in Trainer\ntrainer = L.Trainer(accelerator=\"cuda\", devices=4, strategy=strategy)\ntrainer.fit(...)\n```\n\n----------------------------------------\n\nTITLE: Implementing prepare_data in LightningDataModule\nDESCRIPTION: Shows how to implement the prepare_data method in a LightningDataModule. This method is used for one-time operations like downloading data, and is called only once on a single GPU.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/data/datamodule.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass MNISTDataModule(L.LightningDataModule):\n    def prepare_data(self):\n        # download\n        MNIST(os.getcwd(), train=True, download=True, transform=transforms.ToTensor())\n        MNIST(os.getcwd(), train=False, download=True, transform=transforms.ToTensor())\n```\n\n----------------------------------------\n\nTITLE: Enabling DDP Static Graph Optimization\nDESCRIPTION: Configures DDP to use static graph optimization, which assumes the model employs the same set of parameters in every iteration, allowing PyTorch to apply special optimizations during runtime.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/ddp_optimizations.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\nfrom lightning.pytorch.strategies import DDPStrategy\n\ntrainer = L.Trainer(devices=4, strategy=DDPStrategy(static_graph=True))\n```\n\n----------------------------------------\n\nTITLE: Launching TensorBoard in Notebook\nDESCRIPTION: Commands to launch TensorBoard in notebook environments like Jupyter, Colab, or Kaggle.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/logging.rst#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\n%reload_ext tensorboard\n%tensorboard --logdir=./logs\n```\n\n----------------------------------------\n\nTITLE: Running MAML with Lightning Fabric\nDESCRIPTION: Command to run the Lightning Fabric implementation of MAML using the fabric run command with 2 CPU devices and DDP strategy.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/fabric/meta_learning/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nfabric run train_fabric.py --devices 2 --strategy ddp --accelerator cpu\n```\n\n----------------------------------------\n\nTITLE: Trainer Internal Training Loop Implementation\nDESCRIPTION: Pseudocode showing the internal implementation of the training loop within the Trainer class.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# enable grads\ntorch.set_grad_enabled(True)\n\nlosses = []\nfor batch in train_dataloader:\n    # calls hooks like this one\n    on_train_batch_start()\n\n    # train step\n    loss = training_step(batch)\n\n    # clear gradients\n    optimizer.zero_grad()\n\n    # backward\n    loss.backward()\n\n    # update parameters\n    optimizer.step()\n\n    losses.append(loss)\n```\n\n----------------------------------------\n\nTITLE: Removing .cuda() and .to() Calls in PyTorch Lightning\nDESCRIPTION: Demonstrates how to remove device-specific .cuda() and .to() calls from forward methods when switching to PyTorch Lightning. Device management is handled automatically by Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/accelerators/accelerator_prepare.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# before lightning\ndef forward(self, x):\n    x = x.cuda(0)\n    layer_1.cuda(0)\n    x_hat = layer_1(x)\n\n\n# after lightning\ndef forward(self, x):\n    x_hat = layer_1(x)\n```\n\n----------------------------------------\n\nTITLE: Implementing FP16 Compress Hook in DDP Strategy\nDESCRIPTION: Enables the FP16 compression hook for DDP communication to improve throughput in multi-node setups by compressing gradients to FP16 before communication.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/ddp_optimizations.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\nfrom lightning.pytorch.strategies import DDPStrategy\nfrom torch.distributed.algorithms.ddp_comm_hooks import default_hooks as default\n\nmodel = MyModel()\ntrainer = L.Trainer(accelerator=\"gpu\", devices=4, strategy=DDPStrategy(ddp_comm_hook=default.fp16_compress_hook))\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Loading Distributed Checkpoints\nDESCRIPTION: Shows how to load distributed checkpoints in PyTorch Lightning using FSDP strategy, supporting different world sizes from the original saved checkpoint.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/checkpointing_expert.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\nfrom lightning.pytorch.strategies import FSDPStrategy\n\n# 1. Select the FSDP strategy and set the sharded/distributed checkpoint format\nstrategy = FSDPStrategy(state_dict_type=\"sharded\")\n\n# 2. Pass the strategy to the Trainer\ntrainer = L.Trainer(devices=2, strategy=strategy, ...)\n\n# 3. Set the checkpoint path to load\ntrainer.fit(model, ckpt_path=\"path/to/checkpoint\")\n```\n\n----------------------------------------\n\nTITLE: Computing Backward Pass with Fabric\nDESCRIPTION: Perform a backward pass on the loss in a way that's compatible with the selected accelerator and precision settings. This replaces the standard loss.backward() call making code accelerator and precision agnostic.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_methods.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\noutput = model(input)\nloss = loss_fn(output, target)\n\n# loss.backward()\nfabric.backward(loss)\n```\n\n----------------------------------------\n\nTITLE: Exporting PyTorch Lightning Models to TorchScript\nDESCRIPTION: Example of exporting a trained PyTorch Lightning model to TorchScript format for production deployment. This enables faster inference and deployment on various platforms.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/pytorch_lightning/README.md#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\n# torchscript\nautoencoder = LitAutoEncoder()\ntorch.jit.save(autoencoder.to_torchscript(), \"model.pt\")\n```\n\n----------------------------------------\n\nTITLE: Executing DDP Training Script with Environment Variables in PyTorch Lightning\nDESCRIPTION: This bash snippet shows how PyTorch Lightning executes the DDP training script multiple times with the correct environment variables set for each GPU process. It demonstrates the command for a 3-GPU DDP setup.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/accelerators/gpu_intermediate.rst#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# example for 3 GPUs DDP\nMASTER_ADDR=localhost MASTER_PORT=random() WORLD_SIZE=3 NODE_RANK=0 LOCAL_RANK=0 python my_file.py --accelerator 'gpu' --devices 3 --etc\nMASTER_ADDR=localhost MASTER_PORT=random() WORLD_SIZE=3 NODE_RANK=0 LOCAL_RANK=1 python my_file.py --accelerator 'gpu' --devices 3 --etc\nMASTER_ADDR=localhost MASTER_PORT=random() WORLD_SIZE=3 NODE_RANK=0 LOCAL_RANK=2 python my_file.py --accelerator 'gpu' --devices 3 --etc\n```\n\n----------------------------------------\n\nTITLE: Logging Hyperparameter Metrics with TensorBoard in Lightning\nDESCRIPTION: Demonstrates two approaches for tracking metrics in TensorBoard's hyperparameter tab: using the default hp_metric key and using custom metrics with default_hp_metric=False. Shows how to log both single and multiple metrics for hyperparameter tracking.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/extensions/logging.rst#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Using default_hp_metric\ndef validation_step(self, batch, batch_idx):\n    self.log(\"hp_metric\", some_scalar)\n\n\n# Using custom or multiple metrics (default_hp_metric=False)\ndef on_train_start(self):\n    self.logger.log_hyperparams(self.hparams, {\"hp/metric_1\": 0, \"hp/metric_2\": 0})\n\n\ndef validation_step(self, batch, batch_idx):\n    self.log(\"hp/metric_1\", some_scalar_1)\n    self.log(\"hp/metric_2\", some_scalar_2)\n```\n\n----------------------------------------\n\nTITLE: Configuring Experiment Loggers in PyTorch Lightning\nDESCRIPTION: Shows how to set up various experiment logging integrations in PyTorch Lightning, including TensorBoard, Weights & Biases, Comet, MLflow, and Neptune.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/README.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning import loggers\n\n# tensorboard\ntrainer = Trainer(logger=TensorBoardLogger(\"logs/\"))\n\n# weights and biases\ntrainer = Trainer(logger=loggers.WandbLogger())\n\n# comet\ntrainer = Trainer(logger=loggers.CometLogger())\n\n# mlflow\ntrainer = Trainer(logger=loggers.MLFlowLogger())\n\n# neptune\ntrainer = Trainer(logger=loggers.NeptuneLogger())\n\n# ... and dozens more\n```\n\n----------------------------------------\n\nTITLE: Preloading Data to RAM Using tmpfs\nDESCRIPTION: Shows how to preload data into RAM using tmpfs for faster data access during training, particularly useful for datasets requiring frequent operations.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/speed.rst#2025-04-23_snippet_9\n\nLANGUAGE: bash\nCODE:\n```\ncp -r /path/to/data/on/disk /dev/shm/\n```\n\nLANGUAGE: python\nCODE:\n```\ndatamodule = MyDataModule(data_root=\"/dev/shm/my_data\")\n```\n\n----------------------------------------\n\nTITLE: Configuring MLflow Logger in PyTorch Lightning\nDESCRIPTION: Demonstrates how to set up and use the MLflow logger in PyTorch Lightning with checkpoint path prefixing. Shows integration with both Trainer and LightningModule, including custom experiment logging capabilities.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/loggers.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\nfrom lightning.pytorch.loggers import MLFlowLogger\n\nmlf_logger = MLFlowLogger(\n    experiment_name=\"lightning_logs\",\n    tracking_uri=\"file:./ml-runs\",\n    checkpoint_path_prefix=\"my_prefix\"\n)\ntrainer = L.Trainer(logger=mlf_logger)\n\n# Your LightningModule definition\nclass LitModel(L.LightningModule):\n    def training_step(self, batch, batch_idx):\n        # example\n        self.logger.experiment.whatever_ml_flow_supports(...)\n\n    def any_lightning_module_function_or_hook(self):\n        self.logger.experiment.whatever_ml_flow_supports(...)\n\n# Train your model\nmodel = LitModel()\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Exporting PyTorch Lightning Model to TorchScript\nDESCRIPTION: Demonstrates how to export a PyTorch Lightning model to TorchScript format for production use.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/README.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# torchscript\nautoencoder = LitAutoEncoder()\ntorch.jit.save(autoencoder.to_torchscript(), \"model.pt\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Shape Padding in torch.compile\nDESCRIPTION: Shows how to enable shape padding optimization which can improve memory alignment at the cost of increased memory usage.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/compile.rst#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Default is False\ncompiled_model = torch.compile(model, options={\"shape_padding\": True})\n```\n\n----------------------------------------\n\nTITLE: Manual Training Loop Example\nDESCRIPTION: Demonstration of the equivalent manual training loop that Lightning handles automatically under the hood.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/model/train_model_basic.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nautoencoder = LitAutoEncoder(Encoder(), Decoder())\noptimizer = autoencoder.configure_optimizers()\n\nfor batch_idx, batch in enumerate(train_loader):\n    loss = autoencoder.training_step(batch, batch_idx)\n\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorBoard Logger with Custom Directory\nDESCRIPTION: Demonstrates how to initialize a TensorBoard logger with a custom save directory and pass it to the Trainer.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/extensions/logging.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch import loggers as pl_loggers\n\ntb_logger = pl_loggers.TensorBoardLogger(save_dir=\"logs/\")\ntrainer = Trainer(logger=tb_logger)\n```\n\n----------------------------------------\n\nTITLE: Using Registered Accelerator via Python and CLI\nDESCRIPTION: Examples of using the registered accelerator through Python code and command-line interface.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/extensions/accelerator.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrainer = Trainer(accelerator=\"xpu\")\n```\n\nLANGUAGE: bash\nCODE:\n```\npython train.py fit --trainer.accelerator=xpu --trainer.devices=2\n```\n\n----------------------------------------\n\nTITLE: Configuring DDP Training Strategies in PyTorch Lightning\nDESCRIPTION: Examples showing different DDP strategy configurations, including spawn and regular DDP modes.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/accelerators/gpu_faq.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nTrainer(accelerator=\"gpu\", devices=4, strategy=\"ddp_spawn\")\n\nTrainer(accelerator=\"gpu\", devices=4, strategy=\"ddp\")\n```\n\n----------------------------------------\n\nTITLE: Running DeepSpeed ZeRO Stage 2 via Command Line\nDESCRIPTION: Command line instruction to run PyTorch Lightning training with DeepSpeed ZeRO Stage 2 strategy, specifying precision and GPU configuration.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/deepspeed.rst#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython train.py --strategy deepspeed_stage_2 --precision 16 --accelerator 'gpu' --devices 4\n```\n\n----------------------------------------\n\nTITLE: Implementing DataLoaders in LightningDataModule\nDESCRIPTION: Example implementation of a LightningDataModule class showing how to define multiple DataLoaders for different phases (train, validation, test, predict).\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/data/iterables.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass DataModule(LightningDataModule):\n    def train_dataloader(self):\n        # any iterable or collection of iterables\n        return DataLoader(self.train_dataset)\n\n    def val_dataloader(self):\n        # any iterable or collection of iterables\n        return [DataLoader(self.val_dataset_1), DataLoader(self.val_dataset_2)]\n\n    def test_dataloader(self):\n        # any iterable or collection of iterables\n        return DataLoader(self.test_dataset)\n\n    def predict_dataloader(self):\n        # any iterable or collection of iterables\n        return DataLoader(self.predict_dataset)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Fabric error with custom forward methods\nDESCRIPTION: Shows how using custom methods that call forward indirectly can cause errors in distributed settings with Fabric, while direct forward calls work correctly.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/wrappers.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfabric = L.Fabric(accelerator=\"cpu\", devices=2)\nfabric.launch()\nmodel = MyModel()\nmodel = fabric.setup(model)\n\n# OK: Calling the model directly\noutput = model(torch.randn(10))\n\n# OK: Calling the model's forward (equivalent to the abvoe)\noutput = model.forward(torch.randn(10))\n\n# ERROR: Calling another method that calls forward indirectly\noutput = model.generate()\n```\n\n----------------------------------------\n\nTITLE: Using log_graph Parameter with WandbLogger in PyTorch Lightning\nDESCRIPTION: Example of using the newly added log_graph parameter with the watch method of WandbLogger to control whether to log the model graph to Weights & Biases.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_29\n\nLANGUAGE: python\nCODE:\n```\nfrom pytorch_lightning.loggers import WandbLogger\n\n# Initialize the WandbLogger\nwandb_logger = WandbLogger(project=\"my_project\")\n\ndef setup(self, trainer, stage):\n    # Use the new log_graph parameter\n    wandb_logger.watch(self.model, log=\"all\", log_graph=True)\n```\n\n----------------------------------------\n\nTITLE: Enabling or Disabling Progress Bar in PyTorch Lightning\nDESCRIPTION: Examples showing how to enable or disable the training progress bar. This can be useful when running on environments where progress bars are not needed or supported.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_28\n\nLANGUAGE: python\nCODE:\n```\n# default used by the Trainer\ntrainer = Trainer(enable_progress_bar=True)\n\n# disable progress bar\ntrainer = Trainer(enable_progress_bar=False)\n```\n\n----------------------------------------\n\nTITLE: Implementing Evaluation Function for Intel Neural Compressor\nDESCRIPTION: Python code defining custom evaluation functions for use with Intel Neural Compressor during the quantization process.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/post_training_quantization.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef eval_func_for_nc(model_n, trainer_n):\n    setattr(model, \"model\", model_n)\n    result = trainer_n.validate(model=model, dataloaders=dm.val_dataloader())\n    return result[0][\"accuracy\"]\n\n\ndef eval_func(model):\n    return eval_func_for_nc(model, trainer)\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch Lightning Trainer for Automatic Accelerator Selection\nDESCRIPTION: This code snippet shows how to configure the PyTorch Lightning Trainer to automatically select the appropriate accelerator and devices. It's recommended to use these default settings for multi-node training on Lightning Studios.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/clouds/lightning_ai.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# These are the defaults\ntrainer = L.Trainer(accelerator=\"auto\", devices=\"auto\")\n\n# DON'T hardcode these, leave them default/auto\n# trainer = L.Trainer(accelerator=\"cpu\", devices=3)\n```\n\n----------------------------------------\n\nTITLE: Defining DataLoader with Batch Size in PyTorch Lightning\nDESCRIPTION: Example showing how to define a training DataLoader with a specific batch size in a Lightning Module class.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/accelerators/gpu_faq.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass LitModel(LightningModule):\n    def train_dataloader(self):\n        return Dataset(..., batch_size=7)\n```\n\n----------------------------------------\n\nTITLE: Optimizing Gradient Reset in PyTorch Lightning\nDESCRIPTION: Shows how to override optimizer_zero_grad method to improve performance by setting gradients to None instead of zero.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/speed.rst#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass Model(LightningModule):\n    def optimizer_zero_grad(self, epoch, batch_idx, optimizer):\n        optimizer.zero_grad(set_to_none=True)\n```\n\n----------------------------------------\n\nTITLE: Resuming Training from Checkpoint in PyTorch Lightning\nDESCRIPTION: Shows the correct way to resume training from a checkpoint in PyTorch Lightning using the ckpt_path argument in the fit() method. Also includes a warning about the deprecated resume_from_checkpoint argument.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/checkpointing_basic.rst#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nmodel = LitModel()\ntrainer = Trainer()\n\n# automatically restores model, epoch, step, LR schedulers, etc...\ntrainer.fit(model, ckpt_path=\"path/to/your/checkpoint.ckpt\")\n\n# Incorrect (deprecated) usage:\ntrainer = Trainer(resume_from_checkpoint=\"path/to/your/checkpoint.ckpt\")\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Installing Lightning with pip\nDESCRIPTION: This command installs the Lightning package using pip. This installation will also include the latest stable PyTorch version if not already installed.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/installation.rst#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install lightning\n```\n\n----------------------------------------\n\nTITLE: Implementing GAN Training with Model Toggling\nDESCRIPTION: Advanced implementation of GAN training using model toggling for gradient accumulation with multiple optimizers in a distributed setting.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/speed.rst#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nclass SimpleGAN(LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.automatic_optimization = False\n\n    def training_step(self, batch, batch_idx):\n        g_opt, d_opt = self.optimizers()\n\n        X, _ = batch\n        X.requires_grad = True\n        batch_size = X.shape[0]\n\n        real_label = torch.ones((batch_size, 1), device=self.device)\n        fake_label = torch.zeros((batch_size, 1), device=self.device)\n\n        is_last_batch_to_accumulate = (batch_idx + 1) % 2 == 0 or self.trainer.is_last_batch\n\n        g_X = self.sample_G(batch_size)\n\n        with d_opt.toggle_model(sync_grad=is_last_batch_to_accumulate):\n            d_x = self.D(X)\n            errD_real = self.criterion(d_x, real_label)\n\n            d_z = self.D(g_X.detach())\n            errD_fake = self.criterion(d_z, fake_label)\n\n            errD = errD_real + errD_fake\n\n            self.manual_backward(errD)\n            if is_last_batch_to_accumulate:\n                d_opt.step()\n                d_opt.zero_grad()\n\n        with g_opt.toggle_model(sync_grad=is_last_batch_to_accumulate):\n            d_z = self.D(g_X)\n            errG = self.criterion(d_z, real_label)\n\n            self.manual_backward(errG)\n            if is_last_batch_to_accumulate:\n                g_opt.step()\n                g_opt.zero_grad()\n\n        self.log_dict({\"g_loss\": errG, \"d_loss\": errD}, prog_bar=True)\n```\n\n----------------------------------------\n\nTITLE: Saving Distributed Checkpoints with FSDP Strategy\nDESCRIPTION: Demonstrates how to configure and save distributed checkpoints using FSDP strategy in PyTorch Lightning. This reduces memory peaks by saving model state across multiple GPUs.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/checkpointing_expert.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\nfrom lightning.pytorch.strategies import FSDPStrategy\n\n# 1. Select the FSDP strategy and set the sharded/distributed checkpoint format\nstrategy = FSDPStrategy(state_dict_type=\"sharded\")\n\n# 2. Pass the strategy to the Trainer\ntrainer = L.Trainer(devices=2, strategy=strategy, ...)\n\n# 3. Run the trainer\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Distributed Tensor Operations with PyTorch Lightning Fabric\nDESCRIPTION: Demonstrates various distributed tensor operations including broadcast, all_gather, and all_reduce. Shows how to work with both single tensors and collections of tensors across processes.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_methods.rst#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Send the value of a tensor from rank 0 to all others\nresult = fabric.broadcast(tensor, src=0)\n\n# Every process gets the stack of tensors from everybody else\nall_tensors = fabric.all_gather(tensor)\n\n# Sum a tensor across processes (everyone gets the result)\nreduced_tensor = fabric.all_reduce(tensor, reduce_op=\"sum\")\n\n# Also works with a collection of tensors (dict, list, tuple):\ncollection = {\"loss\": torch.tensor(...), \"data\": ...}\ngathered_collection = fabric.all_gather(collection, ...)\nreduced_collection = fabric.all_reduce(collection, ...)\n```\n\n----------------------------------------\n\nTITLE: Automatic Metric Logging in LightningModule\nDESCRIPTION: Examples of using log() and log_dict() methods to log metrics in a training step.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/extensions/logging.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef training_step(self, batch, batch_idx):\n    self.log(\"my_metric\", x)\n\n\n# or a dict to log all metrics at once with individual plots\ndef training_step(self, batch, batch_idx):\n    self.log_dict({\"acc\": acc, \"recall\": recall})\n```\n\n----------------------------------------\n\nTITLE: Accessing Fabric Instance within LightningModule\nDESCRIPTION: This example demonstrates how to access the Fabric instance from within LightningModule hooks via the self.fabric attribute. It requires that the module has been set up with fabric.setup() beforehand.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/lightning_module.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\n\n\nclass LitModel(L.LightningModule):\n    def on_train_start(self):\n        # Access Fabric and its attributes\n        print(self.fabric.world_size)\n\n\nfabric = L.Fabric()\nmodel = fabric.setup(LitModel())\nmodel.on_train_start()\n```\n\n----------------------------------------\n\nTITLE: Defining Callback Factory Function in Python\nDESCRIPTION: Creates a factory function that returns multiple custom callbacks for PyTorch Lightning. This function is designed to be registered as an entry point that Lightning will automatically discover.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/extensions/entry_points.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef my_custom_callbacks_factory():\n    return [MyCallback1(), MyCallback2()]\n```\n\n----------------------------------------\n\nTITLE: Disabling All PossibleUserWarnings Programmatically\nDESCRIPTION: Demonstrates how to disable all PossibleUserWarning messages using the PyTorch Lightning utility function.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/warnings.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.utilities import disable_possible_user_warnings\n\n# ignore all warnings that could be false positives\ndisable_possible_user_warnings()\n```\n\n----------------------------------------\n\nTITLE: Creating a Yielding Training Step in PyTorch Lightning\nDESCRIPTION: Example of implementing a yielding_training_step that can generate multiple outputs, allowing for more complex training workflows.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_30\n\nLANGUAGE: python\nCODE:\n```\ndef training_step(self, batch, batch_idx):\n    x, y = batch\n    \n    # First forward pass\n    y_hat = self(x)\n    loss_1 = self.loss_fn(y_hat, y)\n    yield loss_1\n    \n    # Second forward pass with some modification\n    y_hat_2 = self(x + torch.randn_like(x) * 0.1)  # add some noise\n    loss_2 = self.loss_fn(y_hat_2, y)\n    yield loss_2\n    \n    # Third forward pass\n    y_hat_3 = self(torch.flip(x, dims=[0]))  # flip the batch\n    loss_3 = self.loss_fn(y_hat_3, y)\n    yield loss_3\n```\n\n----------------------------------------\n\nTITLE: Launching TensorBoard from Command Line\nDESCRIPTION: Command to launch TensorBoard dashboard to view logged metrics.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/logging.rst#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ntensorboard --logdir=./logs\n```\n\n----------------------------------------\n\nTITLE: Limiting Training Batches in PyTorch Lightning Trainer\nDESCRIPTION: Controls how much of the training dataset to use in each epoch. Useful for debugging or testing end-of-epoch behavior.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_11\n\nLANGUAGE: Python\nCODE:\n```\n# default used by the Trainer\ntrainer = Trainer(limit_train_batches=1.0)\n\n# run through only 25% of the training set each epoch\ntrainer = Trainer(limit_train_batches=0.25)\n\n# run through only 10 batches of the training set each epoch\ntrainer = Trainer(limit_train_batches=10)\n```\n\n----------------------------------------\n\nTITLE: Limiting All-Gathers in FSDP Strategy for Memory Optimization\nDESCRIPTION: This code demonstrates how to enable the limit_all_gathers option in the FSDP strategy. This can help manage GPU memory usage when training close to the maximum limit.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/model_parallel/fsdp.rst#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nstrategy = FSDPStrategy(\n    # Default: The CPU will schedule the transfer of weights between GPUs\n    # at will, sometimes too aggressively\n    limit_all_gathers=False,\n    # Enable this if you are close to the max. GPU memory usage\n    limit_all_gathers=True,\n)\nfabric = L.Fabric(..., strategy=strategy)\n```\n\n----------------------------------------\n\nTITLE: Defining PyTorch Lightning BERT Model for Sequence Classification\nDESCRIPTION: Python code to define a PyTorch Lightning module for a BERT model used in sequence classification tasks.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/post_training_quantization.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nfrom lightning.pytorch import LightningModule\nfrom transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n\n\n# BERT Model definition\nclass GLUETransformer(LightningModule):\n    def __init__(self):\n        self.config = AutoConfig.from_pretrained(model_name_or_path, num_labels=num_labels)\n        self.model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, config=self.config)\n\n    def forward(self, **inputs):\n        return self.model(**inputs)\n\n\nmodel = GLUETransformer(model_name_or_path=\"Intel/bert-base-uncased-mrpc\")\n```\n\n----------------------------------------\n\nTITLE: Loading Model from Checkpoint\nDESCRIPTION: Demonstrates how to load a LightningModule from a checkpoint using load_from_checkpoint method, reusing the saved hyperparameters.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/lightning_module.rst#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\n# to load specify the other args\nmodel = LitMNIST.load_from_checkpoint(PATH, loss_fx=torch.nn.SomeOtherLoss, generator_network=MyGenerator())\n```\n\n----------------------------------------\n\nTITLE: Auto-Detection Launch Configuration - Bash\nDESCRIPTION: Command for launching training with automatic device and accelerator detection using the Fabric CLI.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/launch.rst#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nfabric run ./path/to/train.py \\\n    --devices=auto \\\n    --accelerator=auto \\\n    --precision=16\n```\n\n----------------------------------------\n\nTITLE: Initializing Fabric in Python for PyTorch Lightning\nDESCRIPTION: This snippet shows how to initialize Fabric in a Python script for PyTorch Lightning. It demonstrates the basic setup without specifying strategy, devices, or num_nodes, as these will be provided via CLI later.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/multi_node/barebones.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.fabric import Fabric\n\nfabric = Fabric()\n\n# The rest of the training script\n...\n```\n\n----------------------------------------\n\nTITLE: Accessing model attributes through Fabric wrapper\nDESCRIPTION: Shows how the FabricModule transparently redirects attribute and method access to the original model, allowing normal interaction with the wrapped model.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/wrappers.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport lightning as L\n\nfabric = L.Fabric()\nmodel = torch.nn.Linear(10, 2)\nfabric_model = fabric.setup(model)\n\n# You can access attributes and methods normally\nprint(fabric_model.weight is model.weight)  # True\n```\n\n----------------------------------------\n\nTITLE: Setting Rich Progress Bar Leave Option in PyTorch Lightning\nDESCRIPTION: Shows how to configure the RichProgressBar to display a new progress bar at the end of every epoch using the leave parameter.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/progress_bar.rst#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.callbacks import RichProgressBar\n\ntrainer = Trainer(callbacks=[RichProgressBar(leave=True)])\n```\n\n----------------------------------------\n\nTITLE: Validating Servable PyTorch Lightning Model\nDESCRIPTION: Illustrates how to validate that a PyTorch Lightning model can be served before training. It involves subclassing ServableModule and using a ServableModuleValidator callback.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/deploy/production_advanced.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# This code snippet is referenced but not provided in the text.\n# It would typically include a LightningModule subclassing ServableModule\n# and implementing its hooks, along with a Trainer using ServableModuleValidator.\n```\n\n----------------------------------------\n\nTITLE: Using True Half Precision in Fabric (Python)\nDESCRIPTION: Shows how to use true half precision (FP16 or BF16) in Fabric, including model setup and initialization.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/precision.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Select FP16 precision\nfabric = Fabric(precision=\"16-true\")\nmodel = MyModel()\nmodel = fabric.setup(model)  # model gets cast to torch.float16\n\n# Select BF16 precision\nfabric = Fabric(precision=\"bf16-true\")\nmodel = MyModel()\nmodel = fabric.setup(model)  # model gets cast to torch.bfloat16\n\n# Tip: For faster initialization, you can create model parameters with the desired dtype directly on the device:\nfabric = Fabric(precision=\"bf16-true\")\n\n# init the model directly on the device and with parameters in half-precision\nwith fabric.init_module():\n    model = MyModel()\n\nmodel = fabric.setup(model)\n```\n\n----------------------------------------\n\nTITLE: Initializing PyTorchProfiler with NVTX\nDESCRIPTION: Sets up the PyTorch Lightning profiler with NVTX emission enabled for performance profiling. The profiler is integrated with the Lightning Trainer for monitoring training operations.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/tuning/profiler_intermediate.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.profilers import PyTorchProfiler\n\nprofiler = PyTorchProfiler(emit_nvtx=True)\ntrainer = Trainer(profiler=profiler)\n```\n\n----------------------------------------\n\nTITLE: Monitoring Learning Rate Without Schedulers in PyTorch Lightning\nDESCRIPTION: Example of using the LearningRateMonitor callback to track learning rates even without schedulers.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_33\n\nLANGUAGE: python\nCODE:\n```\nfrom pytorch_lightning import Trainer\nfrom pytorch_lightning.callbacks import LearningRateMonitor\n\n# Initialize the LearningRateMonitor callback\nlr_monitor = LearningRateMonitor(logging_interval='step')\n\n# Add it to the trainer\ntrainer = Trainer(\n    callbacks=[lr_monitor],\n    max_epochs=10\n)\n\n# This will now work even if you're only using optimizers without schedulers\ndef configure_optimizers(self):\n    optimizer = torch.optim.Adam(self.parameters(), lr=0.01)\n    return optimizer\n```\n\n----------------------------------------\n\nTITLE: Handling Distributed Sampling in LightningModule\nDESCRIPTION: This snippet illustrates how Lightning handles distributed sampling automatically, eliminating the need for manual DistributedSampler setup.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/lightning_module.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Don't do in Lightning...\ndata = MNIST(...)\nsampler = DistributedSampler(data)\nDataLoader(data, sampler=sampler)\n\n# do this instead\ndata = MNIST(...)\nDataLoader(data)\n```\n\n----------------------------------------\n\nTITLE: Training Time Limit Configuration\nDESCRIPTION: Examples of setting time-based limits for training duration.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/speed.rst#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Stop after 12 hours of training or when reaching 10 epochs (string)\ntrainer = Trainer(max_time=\"00:12:00:00\", max_epochs=10)\n\n# Stop after 1 day and 5 hours (dict)\ntrainer = Trainer(max_time={\"days\": 1, \"hours\": 5})\n```\n\n----------------------------------------\n\nTITLE: Custom Rich Progress Bar Components in PyTorch Lightning\nDESCRIPTION: Shows how to customize the components of RichProgressBar by creating a custom implementation with custom columns.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/progress_bar.rst#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom rich.progress import TextColumn\n\ncustom_column = TextColumn(\"[progress.description]Custom Rich Progress Bar!\")\n\n\nclass CustomRichProgressBar(RichProgressBar):\n    def configure_columns(self, trainer):\n        return [custom_column]\n\n\nprogress_bar = CustomRichProgressBar()\n```\n\n----------------------------------------\n\nTITLE: Enabling CUDA Graphs in PyTorch Compilation\nDESCRIPTION: This snippet shows how to enable CUDA Graphs when compiling a PyTorch model. CUDA Graphs can significantly speed up static models but may increase memory usage.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/compile.rst#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Enable CUDA Graphs\ncompiled_model = torch.compile(model, mode=\"reduce-overhead\")\n\n# This does the same\ncompiled_model = torch.compile(model, options={\"triton.cudagraphs\": True})\n```\n\n----------------------------------------\n\nTITLE: Setting Maximum Epochs in PyTorch Lightning\nDESCRIPTION: Example demonstrating how to set the maximum number of training epochs in the Trainer. This parameter controls when training will stop.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# default used by the Trainer\ntrainer = Trainer(max_epochs=1000)\n```\n\n----------------------------------------\n\nTITLE: Running Validation Loop in LightningModule\nDESCRIPTION: This snippet demonstrates how to run only the validation loop on validation dataloaders using a LightningModule and Trainer.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/lightning_module.rst#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nmodel = LightningTransformer(vocab_size=dataset.vocab_size)\ntrainer = L.Trainer()\ntrainer.validate(model)\n```\n\n----------------------------------------\n\nTITLE: Enabling Synchronized Batch Normalization in PyTorch Lightning\nDESCRIPTION: Shows how to enable synchronization between batch normalization layers across all GPUs for distributed training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_31\n\nLANGUAGE: python\nCODE:\n```\ntrainer = Trainer(sync_batchnorm=True)\n```\n\n----------------------------------------\n\nTITLE: Enabling or Disabling Checkpointing in PyTorch Lightning Trainer\nDESCRIPTION: Controls automatic checkpointing behavior. By default, Lightning saves a checkpoint after each epoch. This can be disabled or customized using ModelCheckpoint callbacks.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\n# default used by Trainer, saves the most recent model to a single checkpoint after each epoch\ntrainer = Trainer(enable_checkpointing=True)\n\n# turn off automatic checkpointing\ntrainer = Trainer(enable_checkpointing=False)\n\nfrom lightning.pytorch.callbacks import ModelCheckpoint\n\n# Init ModelCheckpoint callback, monitoring 'val_loss'\ncheckpoint_callback = ModelCheckpoint(monitor=\"val_loss\")\n\n# Add your callback to the callbacks list\ntrainer = Trainer(callbacks=[checkpoint_callback])\n```\n\n----------------------------------------\n\nTITLE: Using DataModule with Lightning Trainer\nDESCRIPTION: Demonstrates basic usage pattern of DataModule with Lightning Trainer for training, testing, validation and prediction.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/data/datamodule.rst#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\ndm = MNISTDataModule()\nmodel = Model()\ntrainer.fit(model, datamodule=dm)\ntrainer.test(datamodule=dm)\ntrainer.validate(datamodule=dm)\ntrainer.predict(datamodule=dm)\n```\n\n----------------------------------------\n\nTITLE: Clearing Background TPU Programs in Bash\nDESCRIPTION: This command kills all running Python processes, which can help clear up programs using TPUs in the background. It's useful when TPUs are unavailable due to old running processes.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/accelerators/tpu_faq.rst#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npgrep python |  awk '{print $2}' | xargs -r kill -9\n```\n\n----------------------------------------\n\nTITLE: Configuring Neptune.ai Logger in PyTorch Lightning\nDESCRIPTION: Demonstrates how to configure the Neptune.ai logger and pass it to the PyTorch Lightning Trainer. Also shows how to access the logger within a LightningModule.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/supported_exp_managers.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport neptune\nfrom lightning.pytorch.loggers import NeptuneLogger\n\nneptune_logger = NeptuneLogger(\n    api_key=neptune.ANONYMOUS_API_TOKEN,  # replace with your own\n    project=\"common/pytorch-lightning-integration\",  # format \"<WORKSPACE/PROJECT>\"\n)\ntrainer = Trainer(logger=neptune_logger)\n```\n\nLANGUAGE: python\nCODE:\n```\nclass LitModel(LightningModule):\n    def any_lightning_module_function_or_hook(self):\n        neptune_logger = self.logger.experiment[\"your/metadata/structure\"]\n        neptune_logger.append(metadata)\n```\n\n----------------------------------------\n\nTITLE: Using Multiple Loggers\nDESCRIPTION: Demonstrates how to configure and use multiple loggers simultaneously with Fabric.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/logging.rst#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.fabric import Fabric\nfrom lightning.fabric.loggers import CSVLogger, TensorBoardLogger\n\ntb_logger = TensorBoardLogger(root_dir=\"logs/tensorboard\")\ncsv_logger = CSVLogger(root_dir=\"logs/csv\")\n\n# Add multiple loggers in a list\nfabric = Fabric(loggers=[tb_logger, csv_logger])\n\n# Calling .log() or .log_dict() always logs to all loggers simultaneously\nfabric.log(\"some_value\", value)\n```\n\n----------------------------------------\n\nTITLE: Using Barriers for Process Synchronization\nDESCRIPTION: Demonstrates how to use barriers to synchronize processes in distributed computing scenarios.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/distributed_communication.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfabric = Fabric(accelerator=\"cpu\", devices=4)\nfabric.launch()\n\n# Simulate each process taking a different amount of time\nsleep(2 * fabric.global_rank)\nprint(f\"Process {fabric.global_rank} is done.\")\n\n# Wait for all processes to reach the barrier\nfabric.barrier()\nprint(\"All processes reached the barrier!\")\n```\n\n----------------------------------------\n\nTITLE: FSDP Layer Wrapping Policy Configuration\nDESCRIPTION: Demonstrates how to configure FSDP wrapping policy to specify which layers should be managed by FSDP.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/fsdp.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\npolicy = {nn.TransformerEncoderLayer, nn.TransformerDecoderLayer}\n\nstrategy = FSDPStrategy(auto_wrap_policy=policy)\n\ntrainer = L.Trainer(..., strategy=strategy)\n```\n\n----------------------------------------\n\nTITLE: Handling Race Conditions in Distributed Processing\nDESCRIPTION: Examples of preventing race conditions when writing files or downloading data in a distributed environment.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/distributed_communication.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Only write files from one process (rank 0) ...\nif fabric.global_rank == 0:\n    with open(\"output.txt\", \"w\") as file:\n        file.write(...)\n\n# ... or save from all processes but don't write to the same file\nwith open(f\"output-{fabric.global_rank}.txt\", \"w\") as file:\n    file.write(...)\n\n# Multi-node: download a dataset, the filesystem between nodes is shared\nif fabric.global_rank == 0:\n    download_dataset()\n\n# Multi-node: download a dataset, the filesystem between nodes is NOT shared\nif fabric.local_rank == 0:\n    download_dataset()\n```\n\n----------------------------------------\n\nTITLE: Executing Training and Testing Function in Python\nDESCRIPTION: This snippet calls the previously defined run() function to execute the training and testing process for the BoringModel.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/pytorch/bug_report/bug_report_model.ipynb#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nrun()\n```\n\n----------------------------------------\n\nTITLE: Implementing LightningModule with 2D Parallelism in Python\nDESCRIPTION: This code snippet shows the implementation of a LightningModule that applies 2D parallelism (Tensor Parallelism and FSDP) to the FeedForward model in the configure_model method.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/tp_fsdp.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\nfrom torch.distributed.tensor.parallel import ColwiseParallel, RowwiseParallel\nfrom torch.distributed.tensor.parallel import parallelize_module\nfrom torch.distributed._composable.fsdp.fully_shard import fully_shard\n\n\nclass LitModel(L.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = FeedForward(8192, 8192)\n\n    def configure_model(self):\n        # Lightning will set up a `self.device_mesh` for you\n        # Here, it is 2-dimensional\n        tp_mesh = self.device_mesh[\"tensor_parallel\"]\n        dp_mesh = self.device_mesh[\"data_parallel\"]\n\n        if tp_mesh.size() > 1:\n            # Use PyTorch's distributed tensor APIs to parallelize the model\n            plan = {\n                \"w1\": ColwiseParallel(),\n                \"w2\": RowwiseParallel(),\n                \"w3\": ColwiseParallel(),\n            }\n            parallelize_module(self.model, tp_mesh, plan)\n\n        if dp_mesh.size() > 1:\n            # Use PyTorch's FSDP2 APIs to parallelize the model\n            fully_shard(self.model.w1, mesh=dp_mesh)\n            fully_shard(self.model.w2, mesh=dp_mesh)\n            fully_shard(self.model.w3, mesh=dp_mesh)\n            fully_shard(self.model, mesh=dp_mesh)\n```\n\n----------------------------------------\n\nTITLE: Forward vs Training Step in LightningModule (Python)\nDESCRIPTION: Shows the recommended usage of forward for inference/predictions and training_step for training, keeping them independent.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/starter/style_guide.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ndef forward(self, x):\n    embeddings = self.encoder(x)\n    return embeddings\n\n\ndef training_step(self, batch, batch_idx):\n    x, _ = batch\n    z = self.encoder(x)\n    pred = self.decoder(z)\n    ...\n```\n\n----------------------------------------\n\nTITLE: Setting Gradient Clipping in PyTorch Lightning Trainer\nDESCRIPTION: Configures gradient clipping to prevent exploding gradients during training. The default is no clipping.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_10\n\nLANGUAGE: Python\nCODE:\n```\n# default used by the Trainer\ntrainer = Trainer(gradient_clip_val=None)\n```\n\n----------------------------------------\n\nTITLE: Selecting Built-in Strategies in PyTorch Lightning\nDESCRIPTION: Examples of how to select and configure built-in strategies in PyTorch Lightning using the Trainer class. This includes using shorthand names and instantiating Strategy objects with custom configurations.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/extensions/strategy.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Training with the DistributedDataParallel strategy on 4 GPUs\ntrainer = Trainer(strategy=\"ddp\", accelerator=\"gpu\", devices=4)\n\n# Training with the DistributedDataParallel strategy on 4 GPUs, with options configured\ntrainer = Trainer(strategy=DDPStrategy(static_graph=True), accelerator=\"gpu\", devices=4)\n\n# Training with the DDP Spawn strategy using auto accelerator selection\ntrainer = Trainer(strategy=\"ddp_spawn\", accelerator=\"auto\", devices=4)\n\n# Training with the DeepSpeed strategy on available GPUs\ntrainer = Trainer(strategy=\"deepspeed\", accelerator=\"gpu\", devices=\"auto\")\n\n# Training with the DDP strategy using 3 CPU processes\ntrainer = Trainer(strategy=\"ddp\", accelerator=\"cpu\", devices=3)\n\n# Training with the DDP Spawn strategy on 8 TPU cores\ntrainer = Trainer(strategy=\"ddp_spawn\", accelerator=\"tpu\", devices=8)\n```\n\n----------------------------------------\n\nTITLE: Removing Manual Device Placement\nDESCRIPTION: Remove all .to and .cuda calls as Fabric automatically handles device placement for models and data.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/convert.rst#2025-04-23_snippet_3\n\nLANGUAGE: diff\nCODE:\n```\n- model.to(device)\n- batch.to(device)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for MAML Meta-Learning\nDESCRIPTION: Installs the required Python packages for running the MAML meta-learning examples, including Lightning, Learn2Learn, Cherry-RL, and a specific version of Gym.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/fabric/meta_learning/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install lightning learn2learn cherry-rl 'gym<=0.22'\n```\n\n----------------------------------------\n\nTITLE: Limiting Training and Validation Batches in PyTorch Lightning\nDESCRIPTION: Demonstrates how to limit the number of training and validation batches used in PyTorch Lightning, either by percentage or absolute number.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/debug/debugging_basic.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# use only 10% of training data and 1% of val data\ntrainer = Trainer(limit_train_batches=0.1, limit_val_batches=0.01)\n\n# use 10 batches of train and 5 batches of val\ntrainer = Trainer(limit_train_batches=10, limit_val_batches=5)\n```\n\n----------------------------------------\n\nTITLE: Configuring TransformerEngine Precision in Fabric (Python)\nDESCRIPTION: Demonstrates how to configure 8-bit mixed precision using Nvidia's TransformerEngine in Fabric, including customization options.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/precision.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Select 8bit mixed precision via TransformerEngine, with model weights in bfloat16\nfabric = Fabric(precision=\"transformer-engine\")\n\n# Select 8bit mixed precision via TransformerEngine, with model weights in float16\nfabric = Fabric(precision=\"transformer-engine-float16\")\n\n# Customize the fp8 recipe or set a different base precision:\nfrom lightning.fabric.plugins import TransformerEnginePrecision\n\nrecipe = {\"fp8_format\": \"HYBRID\", \"amax_history_len\": 16, \"amax_compute_algo\": \"max\"}\nprecision = TransformerEnginePrecision(weights_dtype=torch.bfloat16, recipe=recipe)\nfabric = Fabric(plugins=precision)\n```\n\n----------------------------------------\n\nTITLE: Implementing DeepSpeed ZeRO Stage 1 in PyTorch Lightning\nDESCRIPTION: Code snippet showing how to initialize a PyTorch Lightning Trainer with DeepSpeed ZeRO Stage 1 strategy, which partitions optimizer states across GPUs to reduce memory usage.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/deepspeed.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch import Trainer\n\nmodel = MyModel()\ntrainer = Trainer(accelerator=\"gpu\", devices=4, strategy=\"deepspeed_stage_1\", precision=16)\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Removing Device Placement in PyTorch Code for Fabric\nDESCRIPTION: When using Fabric, remove manual device placement code as Fabric handles moving tensors to the correct device automatically.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning_fabric/README.md#2025-04-23_snippet_5\n\nLANGUAGE: diff\nCODE:\n```\n- model.to(device)\n- batch.to(device)\n```\n\n----------------------------------------\n\nTITLE: Logging Metrics with Fabric Loggers\nDESCRIPTION: Examples of logging metrics to all attached loggers at once using the Fabric API, allowing for simple metrics tracking during training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_args.rst#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\nfabric.log(\"loss\", loss)\nfabric.log_dict({\"loss\": loss, \"accuracy\": acc})\n```\n\n----------------------------------------\n\nTITLE: Finding Usable CUDA Devices in PyTorch Lightning\nDESCRIPTION: Demonstrates how to use a utility function to find and select available CUDA devices for running multiple experiments simultaneously. This is particularly useful in systems with GPUs configured in 'exclusive compute mode'.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/accelerators/gpu_basic.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.accelerators import find_usable_cuda_devices\n\n# Find two GPUs on the system that are not already occupied\ntrainer = Trainer(accelerator=\"cuda\", devices=find_usable_cuda_devices(2))\n\nfrom lightning.fabric.accelerators import find_usable_cuda_devices\n\n# Works with Fabric too\nfabric = Fabric(accelerator=\"cuda\", devices=find_usable_cuda_devices(2))\n```\n\n----------------------------------------\n\nTITLE: Configuring Lightning CLI for Variable Interpolation\nDESCRIPTION: This Python code snippet shows how to enable variable interpolation in the Lightning CLI by setting the parser mode to 'omegaconf'. This allows for dynamic value references in YAML configurations.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_advanced_2.rst#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ncli = LightningCLI(MyModel, parser_kwargs={\"parser_mode\": \"omegaconf\"})\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch Profiler Example with PyTorch Lightning\nDESCRIPTION: Command to run the PyTorch Profiler example. This script demonstrates how to activate and use the PyTorch Profiler within a PyTorch Lightning workflow for performance analysis.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/pytorch/basics/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython profiler_example.py\n```\n\n----------------------------------------\n\nTITLE: Configuring Kubeflow Environment in PyTorch Lightning Trainer\nDESCRIPTION: Explicit setting of Kubeflow environment in the Trainer for Kubeflow clusters, replacing automatic detection.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/2_0_advanced.rst#2025-04-23_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nTrainer(plugins=KubeflowEnvironment())\n```\n\n----------------------------------------\n\nTITLE: Using Suggested Max Workers Utility\nDESCRIPTION: Utility function that provides guidance on setting an appropriate maximum number of workers for data loading in distributed training environments.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Getting suggested maximum number of workers\nfrom lightning.pytorch.utilities import suggested_max_num_workers\n\nmax_workers = suggested_max_num_workers()\ndataloader = DataLoader(dataset, num_workers=max_workers)\n```\n\n----------------------------------------\n\nTITLE: Initializing TensorBoard Logger in PyTorch Lightning\nDESCRIPTION: Shows how to import and configure a TensorBoard logger instance and integrate it with the Lightning Trainer.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/experiment_managers.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch import loggers as pl_loggers\n\ntensorboard = pl_loggers.TensorBoardLogger()\ntrainer = Trainer(logger=tensorboard)\n```\n\n----------------------------------------\n\nTITLE: Applying Tensor Parallelism in a LightningModule\nDESCRIPTION: Implementation of a LightningModule that applies tensor parallelism to a feed-forward neural network using PyTorch's distributed tensor APIs. The configure_model hook is used to specify which layers should use column-wise or row-wise parallelism.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/tp.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\nfrom torch.distributed.tensor.parallel import ColwiseParallel, RowwiseParallel\nfrom torch.distributed.tensor.parallel import parallelize_module\n\n\nclass LitModel(L.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = FeedForward(8192, 8192)\n\n    def configure_model(self):\n        # Lightning will set up a `self.device_mesh` for you\n        tp_mesh = self.device_mesh[\"tensor_parallel\"]\n        # Use PyTorch's distributed tensor APIs to parallelize the model\n        plan = {\n            \"w1\": ColwiseParallel(),\n            \"w2\": RowwiseParallel(),\n            \"w3\": ColwiseParallel(),\n        }\n        parallelize_module(self.model, tp_mesh, plan)\n\n    def training_step(self, batch):\n        ...\n\n    def configure_optimizers(self):\n        ...\n\n    def train_dataloader(self):\n        ...\n```\n\n----------------------------------------\n\nTITLE: Detecting Graph Breaks in Compiled Models\nDESCRIPTION: Shows how to enable full graph compilation to detect potential optimization breaks in the model.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/compile.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Force an error if there is a graph break in the model\nmodel = torch.compile(model, fullgraph=True)\n```\n\n----------------------------------------\n\nTITLE: Encoder-Decoder Model Definition - Python\nDESCRIPTION: Implementation of a Lightning Module with encoder-decoder architecture using dependency injection.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_advanced_3.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass MyMainModel(LightningModule):\n    def __init__(self, encoder: nn.Module, decoder: nn.Module):\n        \"\"\"Example encoder-decoder submodules model\n\n        Args:\n            encoder: Instance of a module for encoding\n            decoder: Instance of a module for decoding\n        \"\"\"\n        super().__init__()\n        self.save_hyperparameters()\n        self.encoder = encoder\n        self.decoder = decoder\n```\n\n----------------------------------------\n\nTITLE: Defining Feed Forward Model for 2D Parallelism in Python\nDESCRIPTION: This code snippet defines a FeedForward neural network class with three linear layers, which will be used to demonstrate 2D parallelism.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/tp_fsdp.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim):\n        super().__init__()\n        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n\n    def forward(self, x):\n        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n```\n\n----------------------------------------\n\nTITLE: Updating Fabric Default Settings in Python\nDESCRIPTION: Changes the default settings for Fabric to use auto selection for accelerator, strategy, and devices.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/fabric/CHANGELOG.md#2025-04-23_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nFabric(accelerator=\"auto\", strategy=\"auto\", devices=\"auto\")\n```\n\n----------------------------------------\n\nTITLE: Running MNIST Classifier with Pure PyTorch\nDESCRIPTION: Command to train a simple CNN over MNIST using vanilla PyTorch. This implementation only supports CPU training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/fabric/image_classifier/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# CPU\npython train_torch.py\n```\n\n----------------------------------------\n\nTITLE: Defining a Simple FeedForward Network for Tensor Parallelism in PyTorch\nDESCRIPTION: Implementation of a simple MLP with three linear layers that will be used for tensor parallelism demonstration. The network consists of two parallel linear transformations that are combined element-wise, followed by a third linear layer.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/model_parallel/tp.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim):\n        super().__init__()\n        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n\n    def forward(self, x):\n        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n```\n\n----------------------------------------\n\nTITLE: Using DataModule without Lightning\nDESCRIPTION: Demonstrates how to use DataModule in plain PyTorch code for data preparation, loading and cleanup.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/data/datamodule.rst#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\ndm = MNISTDataModule()\ndm.prepare_data()\n\ndm.setup(stage=\"fit\")\n\nfor batch in dm.train_dataloader():\n    ...\n\nfor batch in dm.val_dataloader():\n    ...\n\ndm.teardown(stage=\"fit\")\n\ndm.setup(stage=\"test\")\nfor batch in dm.test_dataloader():\n    ...\n\ndm.teardown(stage=\"test\")\n```\n\n----------------------------------------\n\nTITLE: SLURM Job Submission Script Configuration\nDESCRIPTION: SLURM batch script template for submitting PyTorch Lightning training jobs. Includes resource allocation, environment setup, and execution commands.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/multi_node/slurm.rst#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash -l\n\n# SLURM SUBMIT SCRIPT\n#SBATCH --nodes=4               # This needs to match Fabric(num_nodes=...)\n#SBATCH --ntasks-per-node=8     # This needs to match Fabric(devices=...)\n#SBATCH --gres=gpu:8            # Request N GPUs per machine\n#SBATCH --mem=0\n#SBATCH --time=0-02:00:00\n\n# Activate conda environment\nsource activate $1\n\n# Debugging flags (optional)\nexport NCCL_DEBUG=INFO\nexport PYTHONFAULTHANDLER=1\n\n# On your cluster you might need this:\n# export NCCL_SOCKET_IFNAME=^docker0,lo\n\n# Run your training script\nsrun python train.py\n```\n\n----------------------------------------\n\nTITLE: Configuring Logger in PyTorch Lightning Trainer\nDESCRIPTION: Specifies the logger(s) to use for experiment tracking. By default, it uses TensorBoardLogger if available. Logging can be disabled by setting it to False.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_15\n\nLANGUAGE: Python\nCODE:\n```\nfrom lightning.pytorch.loggers import TensorBoardLogger\n\n# default logger used by trainer (if tensorboard is installed)\n```\n\n----------------------------------------\n\nTITLE: Defining Program State for Checkpointing in PyTorch\nDESCRIPTION: Shows how to create a state dictionary containing models, optimizers, and metadata for checkpointing. Includes both standard dictionary and AttributeDict approaches.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/checkpoint/checkpoint.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nstate = {\"model1\": model1, \"model2\": model2, \"optimizer\": optimizer, \"iteration\": iteration, \"hparams\": ...}\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.fabric.utilities import AttributeDict\n\nstate = AttributeDict(model1=model1, model2=model2, optimizer=optimizer, iteration=iteration, hparams=...)\n```\n\n----------------------------------------\n\nTITLE: Controlling Module State Synchronization in XLA Strategy\nDESCRIPTION: Configuration option to control whether the XLA strategy should broadcast parameters to all devices during initialization, which can be necessary in some training scenarios.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\n# Control parameter broadcasting in XLA Strategy\nstrategy = XLAStrategy(sync_module_states=False)  # Disable broadcasting\n```\n\n----------------------------------------\n\nTITLE: CLI Command for Configuring Multiple Optimizers\nDESCRIPTION: Command line example showing how to configure multiple optimizers with different parameters using the Lightning CLI.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_advanced_3.rst#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n$ python trainer.py fit \\\n    --model.optimizer1=Adam \\\n    --model.optimizer1.lr=0.01 \\\n    --model.optimizer2=AdamW \\\n    --model.optimizer2.lr=0.0001\n```\n\n----------------------------------------\n\nTITLE: Initializing PyTorch Profiler for Single Process\nDESCRIPTION: Sets up basic PyTorch profiler integration with Lightning Trainer for analyzing PyTorch operations performance. This profiler helps identify bottlenecks in operations by measuring CPU time and usage percentages.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/tuning/profiler_intermediate.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.profilers import PyTorchProfiler\n\nprofiler = PyTorchProfiler()\ntrainer = Trainer(profiler=profiler)\n```\n\n----------------------------------------\n\nTITLE: Configuring Logging Frequency in PyTorch Lightning Trainer\nDESCRIPTION: Sets how often to add logging rows during training. This does not write to disk but controls the frequency of in-memory logging.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_14\n\nLANGUAGE: Python\nCODE:\n```\n# default used by the Trainer\ntrainer = Trainer(log_every_n_steps=50)\n```\n\n----------------------------------------\n\nTITLE: Initializing Complex Tensors with 64-bit Precision in PyTorch Lightning\nDESCRIPTION: Demonstrates how to properly initialize a model with complex tensors using 64-bit precision. This should be done in the configure_model hook or under the Trainer.init_module context manager.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/precision_basic.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ntrainer = Trainer(precision=\"64-true\")\n\n# init the model directly on the device and with parameters in full-precision\nwith trainer.init_module():\n    model = MyModel()\n\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Explicit Initialization in LightningModule (Python)\nDESCRIPTION: Demonstrates how to initialize a LightningModule with explicit parameters and sensible defaults for better usability.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/starter/style_guide.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass LitModel(LightningModule):\n    def __init__(self, encoder: nn.Module, coef_x: float = 0.2, lr: float = 1e-3):\n        ...\n```\n\n----------------------------------------\n\nTITLE: Quantizing Linear Layers with BitsAndBytes\nDESCRIPTION: Using the BitsandbytesPrecision plugin to enable quantization of linear layers, which reduces memory usage and can speed up training for large models.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Linear layer quantization with bitsandbytes\ntrainer = Trainer(plugins=BitsandbytesPrecision())\n```\n\n----------------------------------------\n\nTITLE: Excluding Parameters from Hyperparameters\nDESCRIPTION: Shows how to selectively exclude parameters from being saved as hyperparameters, particularly useful for non-serializable objects that need to be provided during model loading.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/lightning_module.rst#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\nclass LitMNIST(L.LightningModule):\n    def __init__(self, loss_fx, generator_network, layer_1_dim=128):\n        super().__init__()\n        self.layer_1_dim = layer_1_dim\n        self.loss_fx = loss_fx\n\n        # call this to save only (layer_1_dim=128) to the checkpoint\n        self.save_hyperparameters(\"layer_1_dim\")\n\n        # equivalent\n        self.save_hyperparameters(ignore=[\"loss_fx\", \"generator_network\"])\n```\n\n----------------------------------------\n\nTITLE: Custom Lightning CLI with Fixed Optimizer - Python\nDESCRIPTION: Example of extending LightningCLI to use fixed optimizer and learning rate scheduler classes.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_advanced_3.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass MyLightningCLI(LightningCLI):\n    def add_arguments_to_parser(self, parser):\n        parser.add_optimizer_args(torch.optim.Adam)\n        parser.add_lr_scheduler_args(torch.optim.lr_scheduler.ExponentialLR)\n```\n\n----------------------------------------\n\nTITLE: Testing with a DataModule in PyTorch Lightning\nDESCRIPTION: Shows how to use a LightningDataModule for testing in PyTorch Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/evaluation_intermediate.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass MyDataModule(L.LightningDataModule):\n    ...\n\n    def test_dataloader(self):\n        return DataLoader(...)\n\n\n# setup your datamodule\ndm = MyDataModule(...)\n\n# test (pass in datamodule)\ntrainer.test(datamodule=dm)\n```\n\n----------------------------------------\n\nTITLE: Manual Wrapping for FSDP in PyTorch Lightning\nDESCRIPTION: Demonstrates how to use manual wrapping with FSDP in a PyTorch Lightning model to create complex sharding strategies.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/fsdp.rst#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport torch.nn as nn\nimport lightning as L\n\nfrom torch.distributed.fsdp.wrap import wrap\n\n\nclass MyModel(L.LightningModule):\n    def configure_model(self):\n        self.linear_layer = nn.Linear(32, 32)\n        self.block = nn.Sequential(nn.Linear(32, 32), nn.Linear(32, 32))\n\n        # Modules get sharded across processes as soon as they are wrapped with `wrap`.\n        linear_layer = wrap(self.linear_layer)\n\n        for i, layer in enumerate(self.block):\n            self.block[i] = wrap(layer)\n\n        self.model = nn.Sequential(linear_layer, nn.ReLU(), self.block)\n\n    def configure_optimizers(self):\n        return torch.optim.AdamW(self.model.parameters())\n\n\nmodel = MyModel()\ntrainer = L.Trainer(accelerator=\"cuda\", devices=4, strategy=\"fsdp\", precision=16)\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Loading Raw State Dictionaries in Fabric\nDESCRIPTION: Examples of loading raw state dictionary files for models and optimizers using Fabric's load_raw method.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/checkpoint/checkpoint.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nmodel = MyModel()\n\n# A model weights file saved by your friend who doesn't use Fabric\nfabric.load_raw(\"path/to/model.pt\", model)\n\n# Equivalent to this:\n# model.load_state_dict(torch.load(\"path/to/model.pt\"))\n\n# Also supports optimizers\noptimizer = torch.optim.Adam(model.parameters())\nfabric.load_raw(\"path/to/optimizer.pt\", optimizer)\n```\n\n----------------------------------------\n\nTITLE: Initializing TensorBoard Logger in PyTorch Lightning\nDESCRIPTION: This snippet shows how to initialize a TensorBoard logger and use it with a PyTorch Lightning Trainer. It imports the logger from lightning.pytorch.loggers and sets it up with a save directory.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/logging_intermediate.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch import loggers as pl_loggers\n\ntensorboard = pl_loggers.TensorBoardLogger(save_dir=\"\")\ntrainer = Trainer(logger=tensorboard)\n```\n\n----------------------------------------\n\nTITLE: Sample YAML Configurations\nDESCRIPTION: Example YAML configuration files showing different configuration patterns and structures.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_advanced.rst#2025-04-23_snippet_3\n\nLANGUAGE: yaml\nCODE:\n```\nseed_everything: null\ntrainer:\n  logger: true\n  ...\nmodel:\n  out_dim: 10\n  learning_rate: 0.02\ndata:\n  data_dir: ./\nckpt_path: null\n```\n\n----------------------------------------\n\nTITLE: Setting prepare_data_per_node in Lightning DataModule\nDESCRIPTION: Example of configuring prepare_data_per_node flag to control data preparation across nodes.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/data/datamodule.rst#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nclass LitDataModule(LightningDataModule):\n    def __init__(self):\n        super().__init__()\n        self.prepare_data_per_node = True\n```\n\n----------------------------------------\n\nTITLE: Updating PyTorch Lightning CLI Seed Configuration\nDESCRIPTION: Replacing deprecated seed_everything_default=None with seed_everything_default=False in LightningCLI configuration.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_8_regular.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nseed_everything_default=False\n```\n\n----------------------------------------\n\nTITLE: Configuring FSDP Checkpoint Format in PyTorch Lightning\nDESCRIPTION: This snippet shows how to configure the checkpoint format in FSDP strategy. It demonstrates setting the state_dict_type to either 'sharded' for efficient saving of large models or 'full' for a single consolidated file.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/model_parallel/fsdp.rst#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Default: Save individual files with state from each process\nstrategy = FSDPStrategy(state_dict_type=\"sharded\")\n\n# Save a single, consolidated checkpoint file\nstrategy = FSDPStrategy(state_dict_type=\"full\")\n```\n\n----------------------------------------\n\nTITLE: Running AutoEncoder Example with PyTorch Lightning\nDESCRIPTION: Commands to run the AutoEncoder example on CPU, GPUs, and with Distributed Data Parallel. The script demonstrates how to implement a CNN auto-encoder using PyTorch Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/pytorch/basics/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# CPU\npython autoencoder.py\n\n# GPUs (any number)\npython autoencoder.py --trainer.accelerator 'gpu' --trainer.devices 2\n\n# Distributed Data Parallel (DDP)\npython autoencoder.py --trainer.accelerator 'gpu' --trainer.devices 2 --trainer.strategy 'ddp'\n```\n\n----------------------------------------\n\nTITLE: Broadcasting Data Between Processes\nDESCRIPTION: Shows how to broadcast tensor data from one process to all other processes in the distributed setup.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/distributed_communication.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfabric = Fabric(devices=4, accelerator=\"cpu\")\nfabric.launch()\n\n# Data is different on each process\nlearning_rate = torch.rand(1)\nprint(\"Before broadcast:\", learning_rate)\n\n# Transfer the tensor from one process to all the others\nlearning_rate = fabric.broadcast(learning_rate)\nprint(\"After broadcast:\", learning_rate)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Multi-GPU Training in Jupyter Notebooks\nDESCRIPTION: Configuration for using multiple GPUs in Jupyter or Colab notebooks using DDP_NOTEBOOK strategy.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/accelerators/gpu_faq.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nTrainer(accelerator=\"gpu\", devices=4, strategy=\"ddp_notebook\")\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-Node Training in PyTorch Lightning\nDESCRIPTION: Examples showing how to set the number of GPU nodes for distributed training. This parameter is essential for multi-node cluster training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_22\n\nLANGUAGE: python\nCODE:\n```\n# default used by the Trainer\ntrainer = Trainer(num_nodes=1)\n\n# to train on 8 nodes\ntrainer = Trainer(num_nodes=8)\n```\n\n----------------------------------------\n\nTITLE: Progress Bar Metrics Access\nDESCRIPTION: Migration from LightningModule.get_progress_bar_dict to utility function for standard metrics\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_6_advanced.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Old way\nself.get_progress_bar_dict()\n\n# New way\nfrom pytorch_lightning.callbacks.progress.base import get_standard_metrics\nget_standard_metrics(module.trainer)\n```\n\n----------------------------------------\n\nTITLE: Configuring True Half-Precision Training\nDESCRIPTION: Using true half-precision training modes that perform all computations in half precision without upcasting to float32. Supports both float16 and bfloat16 formats for maximum efficiency on compatible hardware.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# True half-precision training\ntrainer = Trainer(precision=\"16-true\")  # For float16\n# or\ntrainer = Trainer(precision=\"bf16-true\")  # For bfloat16\n```\n\n----------------------------------------\n\nTITLE: Customizing Rich Progress Bar Theme\nDESCRIPTION: Shows how to create and apply custom themes to the RichProgressBar for personalized styling.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/logging_expert.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.callbacks import RichProgressBar\nfrom lightning.pytorch.callbacks.progress.rich_progress import RichProgressBarTheme\n\n# create your own theme!\ntheme = RichProgressBarTheme(description=\"green_yellow\", progress_bar=\"green1\")\n\n# init as normal\nprogress_bar = RichProgressBar(theme=theme)\ntrainer = Trainer(callbacks=progress_bar)\n```\n\n----------------------------------------\n\nTITLE: Logging a Single Metric in PyTorch Lightning\nDESCRIPTION: Demonstrates how to log a single metric within the training_step method of a LightningModule using self.log.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/logging_basic.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass LitModel(L.LightningModule):\n    def training_step(self, batch, batch_idx):\n        value = ...\n        self.log(\"some_value\", value)\n```\n\n----------------------------------------\n\nTITLE: Customizing Progress Bar in PyTorch Lightning\nDESCRIPTION: This snippet demonstrates how to override the default progress bar in PyTorch Lightning by creating a custom ProgressBar class that removes the version number from the displayed metrics.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/logging_advanced.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.callbacks.progress import Tqdm\n\nclass CustomProgressBar(Tqdm):\n    def get_metrics(self, *args, **kwargs):\n        # don't show the version number\n        items = super().get_metrics()\n        items.pop(\"v_num\", None)\n        return items\n```\n\n----------------------------------------\n\nTITLE: Limiting All-Gathers in FSDP Strategy\nDESCRIPTION: Shows how to enable the limit_all_gathers option in FSDP strategy to combat CUDA malloc retries when close to max GPU memory usage.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/fsdp.rst#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nstrategy = FSDPStrategy(\n    # Default: The CPU will schedule the transfer of weights between GPUs\n    # at will, sometimes too aggressively\n    limit_all_gathers=False,\n    # Enable this if you are close to the max. GPU memory usage\n    limit_all_gathers=True,\n)\ntrainer = L.Trainer(..., strategy=strategy)\n```\n\n----------------------------------------\n\nTITLE: Configuring 64-bit Precision in PyTorch Lightning\nDESCRIPTION: Sets up the Trainer to use 64-bit precision, which enables more accurate models for certain scientific computations. However, it doubles the memory requirements compared to 32-bit precision.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/precision_basic.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nTrainer(precision=\"64-true\")\n\n# or (legacy)\nTrainer(precision=\"64\")\n\n# or (legacy)\nTrainer(precision=64)\n```\n\n----------------------------------------\n\nTITLE: Setting FSDP Checkpoint State Dict Type\nDESCRIPTION: Configuration for controlling whether FSDP saves checkpoints with full state dictionaries or sharded state dictionaries, which affects checkpoint size and loading behavior.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Configure FSDP checkpoint saving format\nstrategy = FSDPStrategy(state_dict_type=\"full\")  # Save full state dict\n# or\nstrategy = FSDPStrategy(state_dict_type=\"sharded\")  # Save sharded state dict\n```\n\n----------------------------------------\n\nTITLE: Loading Model with Previously Excluded Parameters\nDESCRIPTION: Shows how to load a model from checkpoint while providing the parameters that were excluded during saving.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/lightning_module.rst#2025-04-23_snippet_17\n\nLANGUAGE: python\nCODE:\n```\n# the excluded parameters were `loss_fx` and `generator_network`\nmodel = LitMNIST.load_from_checkpoint(PATH, loss_fx=torch.nn.SomeOtherLoss, generator_network=MyGenerator())\n```\n\n----------------------------------------\n\nTITLE: Initializing Simple Profiler in PyTorch Lightning\nDESCRIPTION: Sets up basic profiling to measure execution time of key training loop methods including callbacks, data modules and lightning module functions.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/tuning/profiler_basic.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntrainer = Trainer(profiler=\"simple\")\n```\n\n----------------------------------------\n\nTITLE: Setting Up Debug Mode for TPU Training in Python\nDESCRIPTION: This code snippet demonstrates how to set up debug mode for training on TPUs using PyTorch Lightning. It uses the 'xla_debug' strategy to create a metrics report for diagnosing issues with TPU operations.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/accelerators/tpu_faq.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\n\nmy_model = MyLightningModule()\ntrainer = L.Trainer(accelerator=\"tpu\", devices=8, strategy=\"xla_debug\")\ntrainer.fit(my_model)\n```\n\n----------------------------------------\n\nTITLE: Configuring Lightning Fabric with ModelParallelStrategy\nDESCRIPTION: Setup code for initializing Lightning Fabric with ModelParallelStrategy for 2D parallelism implementation.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/model_parallel/tp_fsdp.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\nfrom lightning.fabric.strategies import ModelParallelStrategy\n\nstrategy = ModelParallelStrategy(\n    parallelize_fn=parallelize_feedforward,\n    data_parallel_size=2,\n    tensor_parallel_size=2,\n)\n\nfabric = L.Fabric(accelerator=\"cuda\", devices=4, strategy=strategy)\nfabric.launch()\n```\n\n----------------------------------------\n\nTITLE: Profiling StreamingDataLoader Performance\nDESCRIPTION: Demonstrates how to enable profiling in StreamingDataLoader to analyze data loading performance. The profile_batches parameter generates a Chrome trace that can be visualized for performance optimization.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/data/README.md#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.data import StreamingDataset, StreamingDataLoader\n\nStreamingDataLoader(..., profile_batches=5)\n```\n\n----------------------------------------\n\nTITLE: Updating PyTorch Lightning Import Paths\nDESCRIPTION: Migration guide for importing rank zero utilities from the new pl.utilities.rank_zero module instead of the deprecated distributed and warnings modules.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_7_devel.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Old imports\nfrom pl.utilities.distributed import rank_zero_only, rank_zero_debug, rank_zero_info\nfrom pl.utilities.warnings import rank_zero_warn, rank_zero_deprecation, LightningDeprecationWarning\n\n# New imports\nfrom pl.utilities.rank_zero import (\n    rank_zero_only,\n    rank_zero_debug,\n    rank_zero_info,\n    rank_zero_warn,\n    rank_zero_deprecation,\n    LightningDeprecationWarning\n)\n```\n\n----------------------------------------\n\nTITLE: Setting Sanity Validation Steps in PyTorch Lightning\nDESCRIPTION: Examples showing how to configure validation sanity checks before training starts. This helps catch validation-related bugs early before full training begins.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_23\n\nLANGUAGE: python\nCODE:\n```\n# default used by the Trainer\ntrainer = Trainer(num_sanity_val_steps=2)\n\n# turn it off\ntrainer = Trainer(num_sanity_val_steps=0)\n\n# check all validation data\ntrainer = Trainer(num_sanity_val_steps=-1)\n```\n\n----------------------------------------\n\nTITLE: Limiting Test Batches in PyTorch Lightning Trainer\nDESCRIPTION: Specifies how much of the test dataset to use. Can be set as a fraction or a fixed number of batches.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_12\n\nLANGUAGE: Python\nCODE:\n```\n# default used by the Trainer\ntrainer = Trainer(limit_test_batches=1.0)\n\n# run through only 25% of the test set each epoch\ntrainer = Trainer(limit_test_batches=0.25)\n\n# run for only 10 batches\ntrainer = Trainer(limit_test_batches=10)\n```\n\n----------------------------------------\n\nTITLE: Extending PyTorch Lightning Trainer with Callbacks\nDESCRIPTION: Shows how to extend the Trainer functionality using callbacks, specifically demonstrating the implementation of AWS checkpointing as a callback feature.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/model/build_model_intermediate.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntrainer = Trainer(callbacks=[AWSCheckpoints()])\n```\n\n----------------------------------------\n\nTITLE: Configuring Validation Check Intervals in PyTorch Lightning\nDESCRIPTION: Demonstrates different ways to configure how often validation is performed during training, either as a fraction of the epoch or after a fixed number of batches.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_32\n\nLANGUAGE: python\nCODE:\n```\n# default used by the Trainer\ntrainer = Trainer(val_check_interval=1.0)\n\n# check validation set 4 times during a training epoch\ntrainer = Trainer(val_check_interval=0.25)\n\n# check validation set every 1000 training batches in the current epoch\ntrainer = Trainer(val_check_interval=1000)\n\n# check validation set every 1000 training batches across complete epochs or during iteration-based training\n# use this when using iterableDataset and your dataset has no length\n# (ie: production cases with streaming data)\ntrainer = Trainer(val_check_interval=1000, check_val_every_n_epoch=None)\n```\n\nLANGUAGE: python\nCODE:\n```\n# Here is the computation to estimate the total number of batches seen within an epoch.\n\n# Find the total number of train batches\ntotal_train_batches = total_train_samples // (train_batch_size * world_size)\n\n# Compute how many times we will call validation during the training loop\nval_check_batch = max(1, int(total_train_batches * val_check_interval))\nval_checks_per_epoch = total_train_batches / val_check_batch\n\n# Find the total number of validation batches\ntotal_val_batches = total_val_samples // (val_batch_size * world_size)\n\n# Total number of batches run\ntotal_fit_batches = total_train_batches + total_val_batches\n```\n\n----------------------------------------\n\nTITLE: Accessing Device in PyTorch with Fabric\nDESCRIPTION: This snippet shows how to replace boilerplate code for device selection in PyTorch with a simpler Fabric approach, demonstrating the difference between traditional device selection and using Fabric.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/accelerators.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n- if torch.cuda.is_available():\n-     device = torch.device(\"cuda\")\n- else:\n-     device = torch.device(\"cpu\")\n\n+ device = fabric.device\n```\n\n----------------------------------------\n\nTITLE: Offloading Parameters to CPU in FSDP Strategy\nDESCRIPTION: Shows how to enable CPU offloading in the FSDP strategy to achieve significant GPU memory savings at the cost of slower training speed.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/fsdp.rst#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Set `cpu_offload=True`\nstrategy = FSDPStrategy(..., cpu_offload=True)\ntrainer = L.Trainer(..., strategy=strategy)\n```\n\n----------------------------------------\n\nTITLE: Modifying Checkpoint Content in PyTorch Lightning\nDESCRIPTION: This code shows how to modify the content of a checkpoint before saving or after loading using LightningModule hooks.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/checkpointing_intermediate.rst#2025-04-23_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nclass LitModel(L.LightningModule):\n    def on_save_checkpoint(self, checkpoint):\n        checkpoint[\"something_cool_i_want_to_save\"] = my_cool_pickable_object\n\n    def on_load_checkpoint(self, checkpoint):\n        my_cool_pickable_object = checkpoint[\"something_cool_i_want_to_save\"]\n```\n\n----------------------------------------\n\nTITLE: Updating NaN Gradients Printing\nDESCRIPTION: Change from TrainerTrainingTricksMixin.print_nan_gradients to pl.utilities.finite_checks.print_nan_gradients.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_4_advanced.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Old way\nTrainerTrainingTricksMixin.print_nan_gradients\n\n# New way\npl.utilities.finite_checks.print_nan_gradients\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch Lightning with Command Line Arguments\nDESCRIPTION: Example of how to execute a PyTorch Lightning training script with command-line arguments to override default hyperparameter values.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/hyperparameters.rst#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython trainer.py --layer_1_dim 64 --devices 1\n```\n\n----------------------------------------\n\nTITLE: Using Precision Autocasting with Fabric\nDESCRIPTION: Apply precision autocasting to operations beyond the model's forward method. While autocasting is automatically applied to model operations after setup(), this context manager allows extending it to other code blocks.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_methods.rst#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nmodel, optimizer = fabric.setup(model, optimizer)\n\n# Fabric handles precision automatically for the model\noutput = model(inputs)\n\nwith fabric.autocast():  # optional\n    loss = loss_function(output, target)\n\nfabric.backward(loss)\n...\n```\n\n----------------------------------------\n\nTITLE: Limiting Validation Batches in PyTorch Lightning Trainer\nDESCRIPTION: Controls how much of the validation dataset to use. Can be set as a fraction, a fixed number of batches, or disabled entirely.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_13\n\nLANGUAGE: Python\nCODE:\n```\n# default used by the Trainer\ntrainer = Trainer(limit_val_batches=1.0)\n\n# run through only 25% of the validation set each epoch\ntrainer = Trainer(limit_val_batches=0.25)\n\n# run for only 10 batches\ntrainer = Trainer(limit_val_batches=10)\n\n# disable validation\ntrainer = Trainer(limit_val_batches=0)\n```\n\n----------------------------------------\n\nTITLE: Configuring Validation Check Interval in PyTorch Lightning\nDESCRIPTION: Demonstrates different ways to configure validation check frequency during training epochs using the val_check_interval parameter in the Trainer class.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/speed.rst#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# default\ntrainer = Trainer(val_check_interval=1.0)\n\n# check every 1/4 th of an epoch\ntrainer = Trainer(val_check_interval=0.25)\n\n# check every 100 train batches (ie: for IterableDatasets or fixed frequency)\ntrainer = Trainer(val_check_interval=100)\n```\n\n----------------------------------------\n\nTITLE: Initializing Complex Number Models with Double Precision\nDESCRIPTION: Shows how to properly initialize models with complex numbers using the init_module context manager for complex128 dtype support.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/precision.rst#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfabric = Fabric(precision=\"64-true\")\n\n# init the model directly on the device and with parameters in full-precision\nwith fabric.init_module():\n    model = MyModel()\n\nmodel = fabric.setup(model)\n```\n\n----------------------------------------\n\nTITLE: Running a Fabric Script from Terminal\nDESCRIPTION: Execute a Fabric-enabled script through the Lightning CLI for distributed training capabilities.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning_fabric/README.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nlightning run model path/to/train.py\n```\n\n----------------------------------------\n\nTITLE: Disabling Automatic Model Summary in PyTorch Lightning\nDESCRIPTION: Demonstrates how to disable the automatic model summary printing in PyTorch Lightning Trainer.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/debug/debugging_basic.rst#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ntrainer = Trainer(enable_model_summary=False)\n```\n\n----------------------------------------\n\nTITLE: Migrating ModelCheckpoint Save Function\nDESCRIPTION: Change from using ModelCheckpoint.save_function to Trainer.save_checkpoint for saving model checkpoints.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_4_advanced.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Old way\nModelCheckpoint.save_function\n\n# New way\nTrainer.save_checkpoint\n```\n\n----------------------------------------\n\nTITLE: Migrating Gradient Norm Calculation\nDESCRIPTION: Change from using LightningModule.grad_norm to pl.utilities.grad_norm utility function.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_4_advanced.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Old way\nLightningModule.grad_norm\n\n# New way\npl.utilities.grad_norm\n```\n\n----------------------------------------\n\nTITLE: Registering Buffers in Lightning\nDESCRIPTION: Shows how to register a tensor as a buffer in a LightningModule's __init__ method, allowing it to be automatically moved to the correct device.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/starter/converting.rst#2025-04-23_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nclass LitModel(L.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.register_buffer(\"running_mean\", torch.zeros(num_features))\n```\n\n----------------------------------------\n\nTITLE: Creating Custom Cluster Environment in PyTorch Lightning\nDESCRIPTION: Example of defining a custom cluster environment plugin by subclassing ClusterEnvironment. This allows connecting to custom cluster setups and distributed training environments.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_25\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.plugins.environments import ClusterEnvironment\n\n\nclass MyCluster(ClusterEnvironment):\n    def main_address(self):\n        return your_main_address\n\n    def main_port(self):\n        return your_main_port\n\n    def world_size(self):\n        return the_world_size\n\n\ntrainer = Trainer(plugins=[MyCluster()], ...)\n```\n\n----------------------------------------\n\nTITLE: Configuring Validation Frequency in PyTorch Lightning Trainer\nDESCRIPTION: Sets the frequency of validation checks during training. The default is to check validation every epoch, but it can be customized to run less frequently.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\n# default used by the Trainer\ntrainer = Trainer(check_val_every_n_epoch=1)\n\n# run val loop every 10 training epochs\ntrainer = Trainer(check_val_every_n_epoch=10)\n```\n\n----------------------------------------\n\nTITLE: Configuring Training Strategies in Fabric\nDESCRIPTION: Examples of setting different distributed training strategies including DDP (Distributed Data Parallel), DDP with spawn, and strategies with specific parameters enabled.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_args.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Running with the DistributedDataParallel strategy on 4 GPUs\nfabric = Fabric(strategy=\"ddp\", accelerator=\"gpu\", devices=4)\n\n# Running with the DDP strategy with find unused parameters enabled on 4 GPUs\nfabric = Fabric(strategy=\"ddp_find_unused_parameters_true\", accelerator=\"gpu\", devices=4)\n\n# Running with the DDP Spawn strategy using 4 CPU processes\nfabric = Fabric(strategy=\"ddp_spawn\", accelerator=\"cpu\", devices=4)\n```\n\n----------------------------------------\n\nTITLE: Structuring HTML Content for PyTorch Lightning Documentation\nDESCRIPTION: This HTML snippet creates a structured layout for displaying different sections of the PyTorch Lightning documentation. It uses custom 'displayitem' elements to present information about modularizing datasets, CLI control, and mixing models and datasets.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/levels/intermediate_level_9.rst#2025-04-23_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"display-card-container\">\n    <div class=\"row\">\n\n.. displayitem::\n   :header: 1: Modularize your datasets\n   :description: Reuse datasets across models by using DataModules\n   :col_css: col-md-4\n   :button_link: ../data/datamodule.html\n   :height: 150\n   :tag: intermediate\n\n.. displayitem::\n   :header: 2: Control it all from the CLI\n   :description: Learn to control a LightningModule and LightningDataModule from the CLI\n   :col_css: col-md-4\n   :button_link: ../cli/lightning_cli_intermediate.html\n   :height: 150\n   :tag: intermediate\n\n.. displayitem::\n   :header: 3: Mix models and datasets\n   :description: Register models, datasets, optimizers and learning rate schedulers\n   :col_css: col-md-4\n   :button_link: ../cli/lightning_cli_intermediate_2.html\n   :height: 150\n   :tag: intermediate\n\n    </div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Using LightningLite's no_backward_sync in PyTorch Lightning\nDESCRIPTION: Shows how to use the new no_backward_sync context manager in LightningLite for efficient gradient accumulation with distributed strategies.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_25\n\nLANGUAGE: Python\nCODE:\n```\nfrom pytorch_lightning import LightningLite\n\nclass MyLite(LightningLite):\n    def run(self):\n        for epoch in range(10):\n            for batch in dataloader:\n                with self.no_backward_sync(model, enabled=(batch_idx % 2 == 0)):\n                    loss = model(batch)\n                    self.backward(loss)\n                if batch_idx % 2 == 1:\n                    optimizer.step()\n                    optimizer.zero_grad()\n```\n\n----------------------------------------\n\nTITLE: Using Lightning Trainer for Predictions\nDESCRIPTION: Demonstrates how to use the Lightning Trainer to make predictions with a custom model and dataloader.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/deploy/production_basic.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ndata_loader = DataLoader(...)\nmodel = MyModel()\ntrainer = Trainer()\npredictions = trainer.predict(model, data_loader)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom XPU Accelerator Class in PyTorch Lightning\nDESCRIPTION: Example implementation of a custom accelerator class for a hypothetical XPU hardware. Shows required methods for device parsing, parallel device handling, device counting, availability checking, and device statistics.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/extensions/accelerator.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport xpulib\n\n\nclass XPUAccelerator(Accelerator):\n    \"\"\"Support for a hypothetical XPU, optimized for large-scale machine learning.\"\"\"\n\n    @staticmethod\n    def parse_devices(devices: Any) -> Any:\n        # Put parsing logic here how devices can be passed into the Trainer\n        # via the `devices` argument\n        return devices\n\n    @staticmethod\n    def get_parallel_devices(devices: Any) -> Any:\n        # Here, convert the device indices to actual device objects\n        return [torch.device(\"xpu\", idx) for idx in devices]\n\n    @staticmethod\n    def auto_device_count() -> int:\n        # Return a value for auto-device selection when `Trainer(devices=\"auto\")`\n        return xpulib.available_devices()\n\n    @staticmethod\n    def is_available() -> bool:\n        return xpulib.is_available()\n\n    def get_device_stats(self, device: Union[str, torch.device]) -> Dict[str, Any]:\n        # Return optional device statistics for loggers\n        return {}\n```\n\n----------------------------------------\n\nTITLE: Setting Random Seeds with Fabric\nDESCRIPTION: Make code reproducible by setting random seeds for PyTorch, NumPy, and Python RNGs. Fabric also handles proper seed initialization for data loader worker processes.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_methods.rst#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# Instead of `torch.manual_seed(...)`, call:\nfabric.seed_everything(1234)\n```\n\n----------------------------------------\n\nTITLE: Configuring Fast Dev Run in PyTorch Lightning Trainer\nDESCRIPTION: Enables a fast development run mode for debugging purposes. It runs a limited number of batches through the entire training, validation, testing, and prediction loops to check for errors.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\n# default used by the Trainer\ntrainer = Trainer(fast_dev_run=False)\n\n# runs only 1 training and 1 validation batch and the program ends\ntrainer = Trainer(fast_dev_run=True)\ntrainer.fit(...)\n\n# runs 7 predict batches and program ends\ntrainer = Trainer(fast_dev_run=7)\ntrainer.predict(...)\n```\n\n----------------------------------------\n\nTITLE: Disabling Foreach in Optimizer for FSDP Performance\nDESCRIPTION: This snippet shows how to disable the foreach option in the optimizer, which can help reduce memory peaks in large models when using FSDP.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/model_parallel/fsdp.rst#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\noptimizer = torch.optim.AdamW(model.parameters(), foreach=False)\n```\n\n----------------------------------------\n\nTITLE: Main Training Script Structure\nDESCRIPTION: Template for structuring the main training script with Lightning, including model initialization and training setup.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/clouds/cluster_advanced.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# train.py\ndef main(args):\n    model = YourLightningModule(args)\n\n    trainer = Trainer(accelerator=\"gpu\", devices=8, num_nodes=4, strategy=\"ddp\")\n\n    trainer.fit(model)\n\n\nif __name__ == \"__main__\":\n    args = ...  # you can use your CLI parser of choice, or the `LightningCLI`\n    # TRAIN\n    main(args)\n```\n\n----------------------------------------\n\nTITLE: Implementing Stateful Counter Callback in PyTorch Lightning\nDESCRIPTION: Example implementation of a Counter callback that tracks either epochs or batches and demonstrates proper state management with state_dict, load_state_dict, and state_key for multiple callback instances.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/extensions/callbacks_state.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass Counter(Callback):\n    def __init__(self, what=\"epochs\", verbose=True):\n        self.what = what\n        self.verbose = verbose\n        self.state = {\"epochs\": 0, \"batches\": 0}\n\n    @property\n    def state_key(self) -> str:\n        # note: we do not include `verbose` here on purpose\n        return f\"Counter[what={self.what}]\"\n\n    def on_train_epoch_end(self, *args, **kwargs):\n        if self.what == \"epochs\":\n            self.state[\"epochs\"] += 1\n\n    def on_train_batch_end(self, *args, **kwargs):\n        if self.what == \"batches\":\n            self.state[\"batches\"] += 1\n\n    def load_state_dict(self, state_dict):\n        self.state.update(state_dict)\n\n    def state_dict(self):\n        return self.state.copy()\n\n\n# two callbacks of the same type are being used\ntrainer = Trainer(callbacks=[Counter(what=\"epochs\"), Counter(what=\"batches\")])\n```\n\n----------------------------------------\n\nTITLE: Customizing TensorBoard Logger Flushing Frequency\nDESCRIPTION: These examples demonstrate how to adjust the flushing frequency of the TensorBoard logger in PyTorch Lightning, affecting training speed and memory usage.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/logging_advanced.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Default used by TensorBoard: Write to disk after 10 logging events or every two minutes\nlogger = TensorBoardLogger(..., max_queue=10, flush_secs=120)\n\n# Faster training, more memory used\nlogger = TensorBoardLogger(..., max_queue=100)\n\n# Slower training, less memory used\nlogger = TensorBoardLogger(..., max_queue=1)\n```\n\n----------------------------------------\n\nTITLE: Launching Distributed Training with Fabric\nDESCRIPTION: Initialize and launch distributed training across multiple devices. This method can be used with scripts or functions to enable multi-GPU training without requiring complex setup.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_methods.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Launch the script on 2 devices and init distributed backend\nfabric = Fabric(devices=2)\nfabric.launch()\n```\n\n----------------------------------------\n\nTITLE: Using LightningCLI with Custom Optimizers and Learning Rate Schedulers\nDESCRIPTION: Example of using LightningCLI with automatic registration of optimizers and learning rate schedulers, supporting shorthand notation.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_34\n\nLANGUAGE: python\nCODE:\n```\nfrom pytorch_lightning.cli import LightningCLI\nfrom pytorch_lightning import LightningModule, Trainer\n\nclass MyModel(LightningModule):\n    def __init__(self, learning_rate=0.01):\n        super().__init__()\n        self.learning_rate = learning_rate\n        # ... model definition ...\n        \n    def configure_optimizers(self):\n        # The CLI will automatically handle optimizers and schedulers based on config\n        return {\"optimizer\": self.optimizer, \"lr_scheduler\": self.lr_scheduler}\n\n# Create CLI with automatic optimizer and scheduler registration\ncli = LightningCLI(MyModel)\n\n# Command line usage examples:\n# python script.py fit --optimizer=Adam --optimizer.lr=0.01\n# python script.py fit --optimizer=SGD --optimizer.lr=0.1 --optimizer.momentum=0.9\n# python script.py fit --lr_scheduler=StepLR --lr_scheduler.step_size=30\n```\n\n----------------------------------------\n\nTITLE: Defining YAML Configuration File in Python\nDESCRIPTION: Example of a YAML configuration file structure used to describe parameters for different sections of a program. It includes configuration for a car, plane, and a class with options.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_faq.rst#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# file.yaml\ncar:\n    max_speed:100\n    max_passengers:2\nplane:\n    fuel_capacity: 50\nclass_3:\n    option_1: 'x'\n    option_2: 'y'\n```\n\n----------------------------------------\n\nTITLE: PyTorch Lightning Checkpoint Structure with Multiple Callbacks\nDESCRIPTION: Example of the checkpoint dictionary structure showing how multiple callback states are stored using unique state keys.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/extensions/callbacks_state.rst#2025-04-23_snippet_1\n\nLANGUAGE: json\nCODE:\n```\n{\n    \"state_dict\": ...,\n    \"callbacks\": {\n        \"Counter{'what': 'batches'}\": {\"batches\": 32, \"epochs\": 0},\n        \"Counter{'what': 'epochs'}\": {\"batches\": 0, \"epochs\": 2},\n        ...\n    }\n}\n```\n\n----------------------------------------\n\nTITLE: Updating GPU Selection Method\nDESCRIPTION: Change in GPU selection method from direct index specification to using environment variables.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_4_advanced.rst#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Old way\nTrainer(gpus=\"i,j\")\n\n# New way\n# Set environment variable: CUDA_VISIBLE_DEVICES=i,j\nTrainer(devices=i)\n```\n\n----------------------------------------\n\nTITLE: Loading and Evaluating DeepSpeed ZeRO Stage 3 Checkpoints\nDESCRIPTION: Code example showing how to load a DeepSpeed checkpoint for testing or inference with PyTorch Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/deepspeed.rst#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch import Trainer\n\n\nclass MyModel(LightningModule):\n    ...\n\n\nmodel = MyModel()\ntrainer = Trainer(accelerator=\"gpu\", devices=4, strategy=\"deepspeed_stage_3\", precision=16)\ntrainer.test(ckpt_path=\"my_saved_deepspeed_checkpoint.ckpt\")\n```\n\n----------------------------------------\n\nTITLE: Collating Single File Checkpoint for DeepSpeed ZeRO Stage 3 in PyTorch Lightning\nDESCRIPTION: Demonstrates how to collate a single checkpoint file from sharded DeepSpeed ZeRO Stage 3 checkpoints in PyTorch Lightning. This is useful for creating a portable checkpoint after training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/deepspeed.rst#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.utilities.deepspeed import convert_zero_checkpoint_to_fp32_state_dict\n\n# lightning deepspeed has saved a directory instead of a file\nsave_path = \"lightning_logs/version_0/checkpoints/epoch=0-step=0.ckpt/\"\noutput_path = \"lightning_model.pt\"\nconvert_zero_checkpoint_to_fp32_state_dict(save_path, output_path)\n```\n\n----------------------------------------\n\nTITLE: Training Termination with Minimum Steps\nDESCRIPTION: Demonstrates training termination that respects a minimum number of steps. Despite should_stop being set at step 2, training continues until 5 steps are completed due to min_steps parameter.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_43\n\nLANGUAGE: python\nCODE:\n```\nclass LitModel(LightningModule):\n    def training_step(self, *args, **kwargs):\n        if self.global_step == 2:\n            self.trainer.should_stop = True\n\n\ntrainer = Trainer(min_steps=5, max_epochs=100)\nmodel = LitModel()\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Enabling Shape Padding in PyTorch Compilation\nDESCRIPTION: This code demonstrates how to enable shape padding during model compilation. Shape padding can improve performance by optimizing memory alignment, at the cost of increased memory usage.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/compile.rst#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Default is False\ncompiled_model = torch.compile(model, options={\"shape_padding\": True})\n```\n\n----------------------------------------\n\nTITLE: Setting custom artifact names in WandbLogger\nDESCRIPTION: Added support for custom artifact names when using WandbLogger.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_17\n\nLANGUAGE: Python\nCODE:\n```\nfrom pytorch_lightning.loggers import WandbLogger\n\nlogger = WandbLogger()\nlogger.experiment.log_artifact(artifact, name=\"my_custom_name\")\n\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Block for PyTorch Lightning Methods\nDESCRIPTION: ReStructuredText documentation blocks for Lightning module methods related to data loading and batch transfer operations. Uses automethod directives to generate method documentation.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/lightning_module.rst#2025-04-23_snippet_22\n\nLANGUAGE: rst\nCODE:\n```\npredict_dataloader\n~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.predict_dataloader\n    :noindex:\n\ntransfer_batch_to_device\n~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.transfer_batch_to_device\n    :noindex:\n\non_before_batch_transfer\n~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_before_batch_transfer\n    :noindex:\n\non_after_batch_transfer\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. automethod:: lightning.pytorch.core.module.LightningModule.on_after_batch_transfer\n    :noindex:\n```\n\n----------------------------------------\n\nTITLE: FSDP Full-Precision Parameter Configuration\nDESCRIPTION: Fix for FSDP full-precision param_dtype training to handle 16-mixed and bf16-mixed configurations correctly with PyTorch < 2.0.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_15\n\nLANGUAGE: python\nCODE:\n```\n# Configuration for FSDP with full-precision parameters\nparam_dtype=\"16-mixed\"  # or \"bf16-mixed\"\n```\n\n----------------------------------------\n\nTITLE: Defining System in LightningModule (Python)\nDESCRIPTION: Demonstrates how to structure a LightningModule that defines a system, keeping the model separate for improved modularity and testing.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/starter/style_guide.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass Encoder(nn.Module):\n    ...\n\n\nclass Decoder(nn.Module):\n    ...\n\n\nclass AutoEncoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = Encoder()\n        self.decoder = Decoder()\n\n    def forward(self, x):\n        return self.encoder(x)\n\n\nclass AutoEncoderSystem(LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.auto_encoder = AutoEncoder()\n```\n\n----------------------------------------\n\nTITLE: Using Multiple Loggers in PyTorch Lightning\nDESCRIPTION: Demonstrates how to configure and use multiple loggers simultaneously in PyTorch Lightning, including accessing them within a LightningModule.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/supported_exp_managers.rst#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.loggers import TensorBoardLogger, WandbLogger\n\nlogger1 = TensorBoardLogger()\nlogger2 = WandbLogger()\ntrainer = Trainer(logger=[logger1, logger2])\n```\n\nLANGUAGE: python\nCODE:\n```\nclass MyModule(LightningModule):\n    def any_lightning_module_function_or_hook(self):\n        tensorboard_logger = self.loggers.experiment[0]\n        wandb_logger = self.loggers.experiment[1]\n\n        fake_images = torch.Tensor(32, 3, 28, 28)\n\n        tensorboard_logger.add_image(\"generated_images\", fake_images, 0)\n        wandb_logger.add_image(\"generated_images\", fake_images, 0)\n```\n\n----------------------------------------\n\nTITLE: Configuring DataLoader with Fabric and Random Seeds\nDESCRIPTION: Example showing how to initialize a dataset with fixed random seed and configure a DataLoader using PyTorch Lightning Fabric. The code demonstrates proper setup for tensor-parallel training while maintaining data sharding across data-parallel groups.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/model_parallel/tp_fsdp.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\n\nfabric = L.Fabric(...)\n\n# Define dataset/dataloader\n# If there is randomness/augmentation in the dataset, fix the seed\ndataset = MyDataset(seed=42)\ndataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n\n# Fabric configures the sampler automatically for you such that\n# all batches in a tensor-parallel group are identical,\n# while still sharding the dataset across the data-parallel group\ndataloader = fabric.setup_dataloaders(dataloader)\n\nfor i, batch in enumerate(dataloader):\n    ...\n```\n\n----------------------------------------\n\nTITLE: Quantization using Bitsandbytes in PyTorch Lightning\nDESCRIPTION: These code snippets demonstrate how to use the BitsandbytesPrecision plugin in PyTorch Lightning for model quantization. It shows different quantization modes and how to customize the plugin settings.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/precision_intermediate.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.plugins import BitsandbytesPrecision\n\n# this will pick out the compute dtype automatically, by default `bfloat16`\nprecision = BitsandbytesPrecision(mode=\"nf4-dq\")\ntrainer = Trainer(plugins=precision)\n\n# Customize the dtype, or skip some modules\nprecision = BitsandbytesPrecision(mode=\"int8-training\", dtype=torch.float16, ignore_modules={\"lm_head\"})\ntrainer = Trainer(plugins=precision)\n\n\nclass MyModel(LightningModule):\n    def configure_model(self):\n        # instantiate your model in this hook\n        self.model = MyModel()\n```\n\n----------------------------------------\n\nTITLE: Initializing TensorBoard Logger with Fabric\nDESCRIPTION: Sets up a TensorBoard logger and integrates it with Lightning Fabric for metric tracking.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/logging.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.fabric import Fabric\nfrom lightning.fabric.loggers import TensorBoardLogger\n\n# Pick a logger and add it to Fabric\nlogger = TensorBoardLogger(root_dir=\"logs\")\nfabric = Fabric(loggers=logger)\n```\n\n----------------------------------------\n\nTITLE: Fallback Command for Unsupported MPS Operations\nDESCRIPTION: This command shows how to enable CPU fallback for unsupported MPS operations when running a Python script. It sets an environment variable to allow fallback to CPU for operations not yet supported by the MPS backend.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/accelerators/mps_basic.rst#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nPYTORCH_ENABLE_MPS_FALLBACK=1 python your_script.py\n```\n\n----------------------------------------\n\nTITLE: Initializing Trainer with Automatic Checkpointing in PyTorch Lightning\nDESCRIPTION: Demonstrates how to initialize a PyTorch Lightning Trainer with automatic checkpointing enabled. This snippet shows the basic setup and how to specify a custom checkpoint directory.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/checkpointing_basic.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# simply by using the Trainer you get automatic checkpointing\ntrainer = Trainer()\n\n# saves checkpoints to 'some/path/' at every epoch end\ntrainer = Trainer(default_root_dir=\"some/path/\")\n```\n\n----------------------------------------\n\nTITLE: Configuring CPU and GPU Accelerators in Fabric\nDESCRIPTION: Examples of configuring different accelerator types in Fabric including CPU, GPU, TPU, and auto-detection. Accelerator parameter determines the computation hardware to use for model training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_args.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# CPU accelerator\nfabric = Fabric(accelerator=\"cpu\")\n\n# Running with GPU Accelerator using 2 GPUs\nfabric = Fabric(devices=2, accelerator=\"gpu\")\n\n# Running with TPU Accelerator using 8 TPU cores\nfabric = Fabric(devices=8, accelerator=\"tpu\")\n\n# Running with GPU Accelerator using the DistributedDataParallel strategy\nfabric = Fabric(devices=4, accelerator=\"gpu\", strategy=\"ddp\")\n```\n\n----------------------------------------\n\nTITLE: Customizing CLI Arguments by Subcommand in YAML\nDESCRIPTION: This YAML configuration demonstrates how to set different Trainer arguments for the 'fit' and 'test' subcommands. It sets max_steps to 100 for training and max_epochs to 10 for testing.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_advanced_2.rst#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\nfit:\n    trainer:\n        max_steps: 100\ntest:\n    trainer:\n        max_epochs: 10\n```\n\n----------------------------------------\n\nTITLE: Using lazy_instance for Compact Class Type Defaults in Python\nDESCRIPTION: Demonstrates a more compact way to set class type defaults in LightningCLI using the lazy_instance function.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_expert.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom jsonargparse import lazy_instance\n\n\nclass MyLightningCLI(LightningCLI):\n    def add_arguments_to_parser(self, parser):\n        parser.set_defaults({\"model.backbone\": lazy_instance(MyModel, encoder_layers=24)})\n```\n\n----------------------------------------\n\nTITLE: Scaling Data Processing with Lightning Optimize\nDESCRIPTION: Shows how to use Lightning's optimize function with multi-node processing. This snippet demonstrates configuring the number of nodes and machine type for large-scale data preparation tasks on the Lightning.ai platform.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/data/README.md#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.data import optimize, Machine\n\noptimize(\n  ...\n  num_nodes=32,\n  machine=Machine.DATA_PREP, # You can select between dozens of optimized machines\n)\n```\n\n----------------------------------------\n\nTITLE: Installing Lightning in a Conda Environment\nDESCRIPTION: This snippet shows how to activate a conda environment and install Lightning within it.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/starter/installation.rst#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nconda activate my_env\nconda install lightning -c conda-forge\n```\n\n----------------------------------------\n\nTITLE: Accessing LightningModule and LightningDataModule Properties\nDESCRIPTION: Migration from prepare_data_per_node Trainer parameter to DataHooks property accessed via LightningModule and LightningDataModule\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_6_advanced.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Old way\ntrainer = Trainer(prepare_data_per_node=True)\n\n# New way\n# Access through DataHooks in LightningModule or LightningDataModule\n```\n\n----------------------------------------\n\nTITLE: Preparing Dataset with Lightning Data Optimize Operator\nDESCRIPTION: Example demonstrating how to convert raw data into Lightning Streaming format using the optimize operator. Creates random images with associated class labels.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/data/README.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport numpy as np\nfrom lightning.data import optimize\nfrom PIL import Image\n\n\n# Store random images into the chunks\ndef random_images(index):\n    data = {\n        \"index\": index,\n        \"image\": Image.fromarray(np.random.randint(0, 256, (32, 32, 3), np.uint8)),\n        \"class\": np.random.randint(10),\n    }\n    return data # The data is serialized into bytes and stored into chunks by the optimize operator.\n\nif __name__ == \"__main__\":\n    optimize(\n        fn=random_images,  # The function applied over each input.\n        inputs=list(range(1000)),  # Provide any inputs. The fn is applied on each item.\n        output_dir=\"my_dataset\",  # The directory where the optimized data are stored.\n        num_workers=4,  # The number of workers. The inputs are distributed among them.\n        chunk_bytes=\"64MB\"  # The maximum number of bytes to write into a chunk.\n    )\n```\n\n----------------------------------------\n\nTITLE: Using DeepSpeedCPUAdam Optimizer with ZeRO Stage 2 Offload\nDESCRIPTION: Example demonstrating the use of DeepSpeedCPUAdam in a Lightning module, which provides 5-7x speedup over torch.optim.adam for offloaded computation.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/deepspeed.rst#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch import LightningModule, Trainer\nfrom deepspeed.ops.adam import DeepSpeedCPUAdam\n\n\nclass MyModel(LightningModule):\n    ...\n\n    def configure_optimizers(self):\n        # DeepSpeedCPUAdam provides 5x to 7x speedup over torch.optim.adam(w)\n        return DeepSpeedCPUAdam(self.parameters())\n\n\nmodel = MyModel()\ntrainer = Trainer(accelerator=\"gpu\", devices=4, strategy=\"deepspeed_stage_2_offload\", precision=16)\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Enabling FP16 Mixed Precision in Fabric (Python)\nDESCRIPTION: Demonstrates how to enable FP16 mixed precision when initializing a Fabric object.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/precision.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Select FP16 mixed precision\nfabric = Fabric(precision=\"16-mixed\")\n```\n\n----------------------------------------\n\nTITLE: DeepSpeed Launch Configuration - Bash\nDESCRIPTION: Command for launching training with DeepSpeed Zero3 optimization and mixed precision using the Fabric CLI.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/launch.rst#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nfabric run ./path/to/train.py \\\n    --strategy=deepspeed_stage_3 \\\n    --devices=8 \\\n    --accelerator=cuda \\\n    --precision=16\n```\n\n----------------------------------------\n\nTITLE: Precision Plugin Method Rename\nDESCRIPTION: Renaming of master_params() method to main_params() in PrecisionPlugin class.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_6_devel.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nPrecisionPlugin.master_params() # deprecated\nPrecisionPlugin.main_params() # new method\n```\n\n----------------------------------------\n\nTITLE: Logging Multiple Metrics\nDESCRIPTION: Shows how to log multiple metrics simultaneously using Fabric's log_dict method.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/logging.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nvalues = {\"loss\": loss, \"acc\": acc, \"other\": other}\nfabric.log_dict(values)\n```\n\n----------------------------------------\n\nTITLE: Configuring 32-bit Precision in PyTorch Lightning\nDESCRIPTION: Sets up the Trainer to use 32-bit precision, which is the default and known to be stable. Multiple ways to configure this are shown, including legacy methods.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/precision_basic.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nTrainer(precision=\"32-true\")\n\n# or (legacy)\nTrainer(precision=\"32\")\n\n# or (legacy)\nTrainer(precision=32)\n```\n\n----------------------------------------\n\nTITLE: Basic Training Termination with should_stop\nDESCRIPTION: Demonstrates immediate training termination by setting trainer.should_stop flag in the training_step method. This will stop training as soon as the flag is set, without considering any minimum epochs or steps.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_41\n\nLANGUAGE: python\nCODE:\n```\nclass LitModel(LightningModule):\n    def training_step(self, *args, **kwargs):\n        self.trainer.should_stop = True\n\n\ntrainer = Trainer()\nmodel = LitModel()\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Applying Transforms to StreamingDataset\nDESCRIPTION: Demonstrates how to create a custom StreamingDataset with data transformations. This example shows resizing images to 224x224 using torchvision transforms in a custom ImagenetStreamingDataset class.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/data/README.md#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.data import StreamingDataset, StreamingDataLoader\nimport torchvision.transforms.v2.functional as F\n\nclass ImagenetStreamingDataset(StreamingDataset):\n\n    def __getitem__(self, index):\n        image = super().__getitem__(index)\n        return F.resize(image, (224, 224))\n\ndataset = ImagenetStreamingDataset(...)\ndataloader = StreamingDataLoader(dataset, batch_size=4)\n\nfor batch in dataloader:\n    print(batch.shape)\n    # Out: (4, 3, 224, 224)\n```\n\n----------------------------------------\n\nTITLE: Configuring TensorBoard Logger in PyTorch Lightning\nDESCRIPTION: Example showing how to create a TensorBoard logger instance and pass it to the Trainer. This enables logging metrics during training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_16\n\nLANGUAGE: python\nCODE:\n```\nlogger = TensorBoardLogger(save_dir=os.getcwd(), version=1, name=\"lightning_logs\")\nTrainer(logger=logger)\n```\n\n----------------------------------------\n\nTITLE: Validation Frequency Configuration\nDESCRIPTION: Examples of controlling how often validation is performed during training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/speed.rst#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n# default\ntrainer = Trainer(check_val_every_n_epoch=1)\n\n# runs validation after every 7th Epoch\ntrainer = Trainer(check_val_every_n_epoch=7)\n```\n\n----------------------------------------\n\nTITLE: Training on TPUs with PyTorch Lightning\nDESCRIPTION: Example of configuring PyTorch Lightning to run on TPU hardware without changing model code. This demonstrates hardware abstraction capabilities of Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/pytorch_lightning/README.md#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# no code changes needed\ntrainer = Trainer(accelerator=\"tpu\", devices=8)\n```\n\n----------------------------------------\n\nTITLE: Subclassing EarlyStopping Callback in PyTorch Lightning\nDESCRIPTION: This example demonstrates how to subclass the EarlyStopping callback to modify its behavior. It overrides methods to disable early stopping at the end of the validation loop and instead perform it at the end of the training loop.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/early_stopping.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass MyEarlyStopping(EarlyStopping):\n    def on_validation_end(self, trainer, pl_module):\n        # override this to disable early stopping at the end of val loop\n        pass\n\n    def on_train_end(self, trainer, pl_module):\n        # instead, do it at the end of training loop\n        self._run_early_stopping_check(trainer)\n```\n\n----------------------------------------\n\nTITLE: TPU Training Configuration\nDESCRIPTION: Examples of configuring TPU training with different core configurations.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/speed.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# train on 1 TPU core\ntrainer = Trainer(accelerator=\"tpu\", devices=1)\n\n# train on 7th TPU core\ntrainer = Trainer(accelerator=\"tpu\", devices=[7])\n\n# train on 8 TPU cores\ntrainer = Trainer(accelerator=\"tpu\", devices=8)\n```\n\n----------------------------------------\n\nTITLE: Using Custom Accelerator with PyTorch Lightning Trainer\nDESCRIPTION: Example showing how to instantiate and use a custom accelerator with the PyTorch Lightning Trainer class.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/extensions/accelerator.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch import Trainer\n\naccelerator = XPUAccelerator()\ntrainer = Trainer(accelerator=accelerator, devices=2)\n```\n\n----------------------------------------\n\nTITLE: Configuring TPU Training with PyTorch Lightning\nDESCRIPTION: Shows how to configure PyTorch Lightning for training on TPUs without modifying the model code.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/README.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# no code changes needed\ntrainer = Trainer(accelerator=\"tpu\", devices=8)\n```\n\n----------------------------------------\n\nTITLE: Expected Training Output with Tensor Parallelism\nDESCRIPTION: Example console output showing the initialization of distributed training across 4 GPUs, training progress, memory usage, and successful completion of the training process.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/pytorch/tensor_parallel/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n\nNumber of model parameters: 6.7 B\nStarting training ...\n\nInitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\nInitializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\nInitializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\nInitializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n----------------------------------------------------------------------------------------------------\ndistributed_backend=nccl\nAll distributed processes registered. Starting with 4 processes\n----------------------------------------------------------------------------------------------------\n\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\nLOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\nLOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\nLOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n\nEpoch 0: 100%|█████████████████████████████████████████████| 10/10 [01:49<00:00, 0.09it/s, v_num=2]\n`Trainer.fit` stopped: `max_epochs=1` reached.                                      \nSaving a (distributed) checkpoint ...\nTraining successfully completed!\nPeak memory usage: 36.73 GB\n```\n\n----------------------------------------\n\nTITLE: Including RST Documentation Sections\nDESCRIPTION: RST file inclusion directives to organize PyTorch Lightning documentation into sections for different user levels: regular users, advanced users, and developers.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/from_2_0.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n:orphan:\n\nRegular User\n************\n\n.. include:: sections/2_0_regular.rst\n\nAdvanced User\n*************\n\n.. include:: sections/2_0_advanced.rst\n\nDeveloper\n*********\n\n.. include:: sections/2_0_devel.rst\n```\n\n----------------------------------------\n\nTITLE: Updating PyTorch Lightning Import Path\nDESCRIPTION: Updating the import path from pl.core.lightning to pl.core.module to reflect new module organization.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_8_regular.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport pl.core.module\n```\n\n----------------------------------------\n\nTITLE: Updating FSDP Parameter Access in LightningModule\nDESCRIPTION: Change in how to access model parameters when using FSDP (Fully Sharded Data Parallel) in PyTorch 2.0+. The update requires using self.parameters() instead of self.trainer.model.parameters() in configure_optimizers().\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/2_0_regular.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Old way\nself.trainer.model.parameters()\n\n# New way\nself.parameters()\n```\n\n----------------------------------------\n\nTITLE: SLURM Job Submission Script\nDESCRIPTION: Bash script for submitting training jobs to SLURM scheduler with GPU resource allocation and environment setup.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/clouds/cluster_advanced.rst#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n#!/bin/bash -l\n\n# SLURM SUBMIT SCRIPT\n#SBATCH --nodes=4             # This needs to match Trainer(num_nodes=...)\n#SBATCH --gres=gpu:8\n#SBATCH --ntasks-per-node=8   # This needs to match Trainer(devices=...)\n#SBATCH --mem=0\n#SBATCH --time=0-02:00:00\n\n# activate conda env\nsource activate $1\n\n# debugging flags (optional)\nexport NCCL_DEBUG=INFO\nexport PYTHONFAULTHANDLER=1\n\n# run script from above\nsrun python3 train.py\n```\n\n----------------------------------------\n\nTITLE: Initializing Default TensorBoard Logger in Lightning Trainer\nDESCRIPTION: Shows how to create a Trainer instance that automatically uses TensorBoard logger with default settings.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/extensions/logging.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch import Trainer\n\n# Automatically logs to a directory (by default ``lightning_logs/``)\ntrainer = Trainer()\n```\n\n----------------------------------------\n\nTITLE: Configuring 8-bit Adam Optimizer in PyTorch Lightning Model\nDESCRIPTION: This code snippet demonstrates how to configure an 8-bit Adam optimizer from bitsandbytes within a PyTorch Lightning model. It also shows how to force embedding layers to use 32-bit precision for numerical stability.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/precision_intermediate.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport bitsandbytes as bnb\n\n\nclass MyModel(LightningModule):\n    def configure_optimizers(self):\n        optimizer = bnb.optim.Adam8bit(model.parameters(), lr=0.001, betas=(0.9, 0.995))\n\n        # (optional) force embedding layers to use 32 bit for numerical stability\n        # https://github.com/huggingface/transformers/issues/14819#issuecomment-1003445038\n        for module in model.modules():\n            if isinstance(module, torch.nn.Embedding):\n                bnb.optim.GlobalOptimManager.get_instance().register_module_override(\n                    module, \"weight\", {\"optim_bits\": 32}\n                )\n\n        return optimizer\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch Implementation\nDESCRIPTION: Command to execute the raw PyTorch version of DCGAN\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/fabric/dcgan/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython train_torch.py\n```\n\n----------------------------------------\n\nTITLE: CLI Instantiation Example - Python\nDESCRIPTION: Shows how to instantiate Lightning CLI in non-running mode and manually call fit.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_advanced_3.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ncli = LightningCLI(MyModel, run=False)  # True by default\n# you'll have to call fit yourself:\ncli.trainer.fit(cli.model)\n```\n\n----------------------------------------\n\nTITLE: Installing Dependencies for Custom PyTorch Lightning Trainer\nDESCRIPTION: Command to install required Python packages including Lightning, TorchMetrics, PyTorch, TorchVision, and tqdm with specific version constraints.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/fabric/build_your_own_trainer/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"lightning>=2.0\" \"torchmetrics>=0.11\" \"torchvision>=0.14\" \"torch>=1.13\" tqdm\n```\n\n----------------------------------------\n\nTITLE: Accessing Device in Lightning\nDESCRIPTION: Demonstrates how to access the current device within a LightningModule using self.device, which is automatically set by Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/starter/converting.rst#2025-04-23_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nclass LitModel(L.LightningModule):\n    def training_step(self, batch, batch_idx):\n        z = torch.randn(4, 5, device=self.device)\n        ...\n```\n\n----------------------------------------\n\nTITLE: Importing Required Libraries for PyTorch Lightning\nDESCRIPTION: Basic imports needed for a PyTorch Lightning project, including PyTorch, torchvision for datasets, and the PyTorch Lightning library.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/pytorch_lightning/README.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nimport os\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torchvision.datasets import MNIST\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import transforms\nimport pytorch_lightning as pl\n```\n\n----------------------------------------\n\nTITLE: Filtering State Dictionary in PyTorch Lightning Model\nDESCRIPTION: This snippet demonstrates how to override the state_dict() method to exclude keys containing 'encoder' from the state dictionary. This is useful for selectively saving model parameters.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/checkpointing_intermediate.rst#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\nreturn {k: v for k, v in super().state_dict().items() if \"encoder\" not in k}\n```\n\n----------------------------------------\n\nTITLE: Configuring Overfitting Options in PyTorch Lightning\nDESCRIPTION: Examples demonstrating how to use overfit_batches to limit the amount of data used during training and validation. This is useful for debugging or deliberate overfitting experiments.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_24\n\nLANGUAGE: python\nCODE:\n```\n# default used by the Trainer\ntrainer = Trainer(overfit_batches=0.0)\n\n# use only 1% of the train & val set\ntrainer = Trainer(overfit_batches=0.01)\n\n# overfit on 10 of the same batches\ntrainer = Trainer(overfit_batches=10)\n```\n\n----------------------------------------\n\nTITLE: Version Number Format Definition in RST\nDESCRIPTION: Defines the format for PyTorch Lightning version numbers using the MAJOR.MINOR.PATCH convention with explanations for each component.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/versioning.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\nMAJOR.MINOR.PATCH\n```\n\n----------------------------------------\n\nTITLE: Swapping LightningModule Implementations with Minimal Changes\nDESCRIPTION: This snippet shows how to replace one LightningModule implementation with another without changing the training loop, demonstrating the modularity achieved with this code organization approach.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/lightning_module.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Instantiate the LightningModule\n- model = LitModel()\n+ model = DopeModel()\n\n...\n```\n\n----------------------------------------\n\nTITLE: Accessing Rank and World Size Information\nDESCRIPTION: Shows how to access various rank and size information in distributed processing including world size, global rank, local rank, and node rank.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/distributed_communication.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# The total number of processes running across all devices and nodes\nfabric.world_size  # 2 * 3 = 6\n\n# The global index of the current process across all devices and nodes\nfabric.global_rank  # -> {0, 1, 2, 3, 4, 5}\n\n# The index of the current process among the processes running on the local node\nfabric.local_rank  # -> {0, 1}\n\n# The index of the current node\nfabric.node_rank  # -> {0, 1, 2}\n\n# Do something only on rank 0\nif fabric.global_rank == 0:\n    ...\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch Lightning with Package Managers\nDESCRIPTION: Commands to install Lightning using pip or conda package managers.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/starter/introduction.rst#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install lightning\n```\n\nLANGUAGE: bash\nCODE:\n```\nconda install lightning -c conda-forge\n```\n\n----------------------------------------\n\nTITLE: Setting Default Root Directory in PyTorch Lightning Trainer\nDESCRIPTION: Specifies the default path for logs and weights when no logger or ModelCheckpoint callback is passed. It supports local and remote paths, including s3 and hdfs.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\n# default used by the Trainer\ntrainer = Trainer(default_root_dir=os.getcwd())\n```\n\n----------------------------------------\n\nTITLE: Loading Raw PyTorch Checkpoints with Fabric in Python\nDESCRIPTION: Loads raw PyTorch state dict checkpoints for model or optimizer objects using Fabric's load_raw() method.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/fabric/CHANGELOG.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nFabric.load_raw()\n```\n\n----------------------------------------\n\nTITLE: Updating Precision Settings in Python\nDESCRIPTION: Changes the format of precision settings arguments to use string representations.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/fabric/CHANGELOG.md#2025-04-23_snippet_8\n\nLANGUAGE: Python\nCODE:\n```\nprecision=\"64-true\"  # formerly 64\nprecision=\"32-true\"  # formerly 32\nprecision=\"16-mixed\"  # formerly 16\nprecision=\"bf16-mixed\"  # formerly bf16\n```\n\n----------------------------------------\n\nTITLE: Upgrading Multiple Checkpoint Files in PyTorch Lightning\nDESCRIPTION: Command line instruction for permanently upgrading multiple checkpoint files in a directory to the current Lightning version.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/checkpointing_migration.rst#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m lightning.pytorch.utilities.upgrade_checkpoint /path/to/checkpoints/folder\n```\n\n----------------------------------------\n\nTITLE: Profiler Configuration Update\nDESCRIPTION: Changes to Profiler initialization parameters, replacing output_filename with separate dirpath and filename parameters.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_4_regular.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Old way\nProfiler(output_filename='path/to/file.txt')\n\n# New way\nProfiler(dirpath='path/to', filename='file.txt')\n```\n\n----------------------------------------\n\nTITLE: Checking if a model is wrapped by Fabric and unwrapping\nDESCRIPTION: Demonstrates how to check if a model is wrapped using is_wrapped() utility function, and how to access the original unwrapped model using the .module property.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/wrappers.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport lightning as L\nfrom lightning.fabric import is_wrapped\n\nfabric = L.Fabric()\nmodel = torch.nn.Linear(10, 2)\nfabric_model = fabric.setup(model)\n\nprint(is_wrapped(model))  # False\nprint(is_wrapped(fabric_model))  # True\n```\n\n----------------------------------------\n\nTITLE: Logging Custom Metric for Checkpoint Monitoring in PyTorch Lightning\nDESCRIPTION: This code shows how to log a custom metric in a LightningModule that can be used for checkpoint monitoring.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/checkpointing_intermediate.rst#2025-04-23_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\nclass LitModel(L.LightningModule):\n    def training_step(self, batch, batch_idx):\n        self.log(\"my_metric\", x)\n\n# 'my_metric' is now able to be monitored\ncheckpoint_callback = ModelCheckpoint(monitor=\"my_metric\")\n```\n\n----------------------------------------\n\nTITLE: Customizing Progress Bar Display in PyTorch Lightning\nDESCRIPTION: Shows how to customize the default progress bar by creating a subclass of TQDMProgressBar and overriding the get_metrics method. In this example, the version number is removed from the progress bar display.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/extensions/logging.rst#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.callbacks.progress import TQDMProgressBar\n\n\nclass CustomProgressBar(TQDMProgressBar):\n    def get_metrics(self, *args, **kwargs):\n        # don't show the version number\n        items = super().get_metrics(*args, **kwargs)\n        items.pop(\"v_num\", None)\n        return items\n```\n\n----------------------------------------\n\nTITLE: Printing from Main Process with Fabric\nDESCRIPTION: Print messages only from the main process to avoid duplicate output in distributed training. This helps keep logs clean when running on multiple devices or nodes.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_methods.rst#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\n# Print only on the main process\nfabric.print(f\"{epoch}/{num_epochs}| Train Epoch Loss: {loss}\")\n```\n\n----------------------------------------\n\nTITLE: Removing LightningModule Optimizer States Property\nDESCRIPTION: The LightningModule.loaded_optimizer_states_dict property has been removed. Referenced in PR8229.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_5_devel.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nLightningModule.loaded_optimizer_states_dict\n```\n\n----------------------------------------\n\nTITLE: RST Table of Contents Configuration\nDESCRIPTION: Configuration block for creating a table of contents that recursively includes all tutorial notebooks from nested subdirectories. Uses the toctree directive with maxdepth and glob options.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/notebooks.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 1\n   :name: Notebooks\n   :caption: Notebooks\n   :glob:\n\n   notebooks/**/*\n```\n\n----------------------------------------\n\nTITLE: Running MAML with Raw PyTorch in Distributed Mode\nDESCRIPTION: Command to run the raw PyTorch implementation of MAML using torchrun for distributed training across 2 processes in standalone mode.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/fabric/meta_learning/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntorchrun --nproc_per_node=2 --standalone train_torch.py\n```\n\n----------------------------------------\n\nTITLE: Implementing a Simple Feed-Forward Neural Network in PyTorch\nDESCRIPTION: A basic feed-forward neural network implementation with three linear layers where two produce outputs that are multiplied element-wise before being fed to the third layer. This architecture is suitable for demonstrating tensor parallelism concepts.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/tp.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim):\n        super().__init__()\n        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n\n    def forward(self, x):\n        return self.w2(F.silu(self.w1(x)) * self.w3(x))\n```\n\n----------------------------------------\n\nTITLE: Updating Trainer Device Configuration\nDESCRIPTION: Change in how to specify devices in Trainer configuration, particularly for auto-selecting GPUs in Jupyter notebooks. The update changes 'auto' to -1 for selecting all available GPUs.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/2_0_regular.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Old way\nTrainer(devices=\"auto\")\n\n# New way\nTrainer(devices=-1)\n```\n\n----------------------------------------\n\nTITLE: Initializing Models in Half-Precision with Fabric\nDESCRIPTION: Creates a model directly on GPU in float16 precision, avoiding redundant CPU-to-GPU transfers and precision casting, which significantly improves initialization speed and reduces peak memory usage for large models.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/model_init.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfabric = Fabric(accelerator=\"cuda\", precision=\"16-true\")\n\nwith fabric.init_module():\n    # models created here will be on GPU and in float16\n    model = MyModel()\n```\n\n----------------------------------------\n\nTITLE: OneCycleLR Scheduler Configuration with Total Steps\nDESCRIPTION: Shows how to configure OneCycleLR scheduler using the trainer's estimated_stepping_batches property to compute total training steps.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/optimization.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ndef configure_optimizers(self):\n    optimizer = ...\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n        optimizer, max_lr=1e-3, total_steps=self.trainer.estimated_stepping_batches\n    )\n    return optimizer, scheduler\n```\n\n----------------------------------------\n\nTITLE: Using CombinedLoader with Custom Mode\nDESCRIPTION: Example of explicitly using the CombinedLoader class to combine multiple DataLoaders with a specific mode setting. Shows how to initialize and use with the Trainer.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/data/iterables.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.utilities import CombinedLoader\n\niterables = {\"a\": DataLoader(), \"b\": DataLoader()}\ncombined_loader = CombinedLoader(iterables, mode=\"min_size\")\nmodel = ...\ntrainer = Trainer()\ntrainer.fit(model, combined_loader)\n```\n\n----------------------------------------\n\nTITLE: Running K-Fold Image Classification with Lightning Fabric\nDESCRIPTION: Commands to run the K-Fold cross validation script using Lightning Fabric on different hardware configurations, including CPU, single GPU, and multiple GPUs.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/fabric/kfold_cv/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# CPU\nfabric run train_fabric.py\n\n# GPU (CUDA or M1 Mac)\nfabric run train_fabric.py --accelerator=gpu\n\n# Multiple GPUs\nfabric run train_fabric.py --accelerator=gpu --devices=4\n```\n\n----------------------------------------\n\nTITLE: Setting NCCL Parameters for Multi-Node Performance\nDESCRIPTION: Sets NVIDIA Collective Communications Library (NCCL) parameters via environment variables to optimize performance for multi-node GPU training, which can provide significant speedups in distributed training workloads.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/ddp_optimizations.rst#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport NCCL_NSOCKS_PERTHREAD=4\nexport NCCL_SOCKET_NTHREADS=2\n```\n\n----------------------------------------\n\nTITLE: Using Environment Variables for Lightning CLI Configuration\nDESCRIPTION: This bash command illustrates how to pass configuration as environment variables to the Lightning CLI. It's useful for CI/CD systems where passing raw YAML config as environment variables is preferred.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_advanced_2.rst#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ python main.py fit --trainer \"$TRAINER_CONFIG\" --model \"$MODEL_CONFIG\" [...]\n```\n\n----------------------------------------\n\nTITLE: Multiple DataModules Implementation\nDESCRIPTION: Implementation showing how to support multiple data module classes with LightningCLI\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_intermediate_2.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# main.py\nimport torch\nfrom lightning.pytorch.cli import LightningCLI\nfrom lightning.pytorch.demos.boring_classes import DemoModel, BoringDataModule\n\n\nclass FakeDataset1(BoringDataModule):\n    def train_dataloader(self):\n        print(\"⚡\", \"using FakeDataset1\", \"⚡\")\n        return torch.utils.data.DataLoader(self.random_train)\n\n\nclass FakeDataset2(BoringDataModule):\n    def train_dataloader(self):\n        print(\"⚡\", \"using FakeDataset2\", \"⚡\")\n        return torch.utils.data.DataLoader(self.random_train)\n\n\ncli = LightningCLI(DemoModel)\n```\n\n----------------------------------------\n\nTITLE: Custom Model Configuration Example\nDESCRIPTION: Example of defining a custom Lightning model with configuration structure in YAML.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_advanced.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# model.py\nclass MyModel(L.LightningModule):\n    def __init__(self, criterion: torch.nn.Module):\n        self.criterion = criterion\n```\n\nLANGUAGE: yaml\nCODE:\n```\nmodel:\n  class_path: model.MyModel\n  init_args:\n    criterion:\n      class_path: torch.nn.CrossEntropyLoss\n      init_args:\n        reduction: mean\n```\n\n----------------------------------------\n\nTITLE: Importing Base Requirements for PyTorch Lightning\nDESCRIPTION: Imports the base requirements files for both Fabric and PyTorch components, which contain the core dependencies needed for PyTorch Lightning to function properly.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/requirements.txt#2025-04-23_snippet_0\n\nLANGUAGE: requirements\nCODE:\n```\n-r ./requirements/fabric/base.txt\n-r ./requirements/pytorch/base.txt\n```\n\n----------------------------------------\n\nTITLE: Renaming Strategy Options in Python\nDESCRIPTION: Renames several strategy options, including FSDP and TPU related strategies.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/fabric/CHANGELOG.md#2025-04-23_snippet_7\n\nLANGUAGE: Python\nCODE:\n```\nstrategy=\"fsdp_cpu_offload\"  # formerly \"fsdp_full_shard_offload\"\nstrategy=\"xla\"  # formerly \"tpu_spawn\"\nstrategy=\"xla_debug\"  # formerly \"tpu_spawn_debug\"\n```\n\n----------------------------------------\n\nTITLE: Accelerator Connector Protection Changes\nDESCRIPTION: Changes to access modifiers for Slurm-related attributes in AcceleratorConnector class, marking them as protected.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_6_devel.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nAcceleratorConnector.is_slurm_managing_tasks # discouraged\nAcceleratorConnector.configure_slurm_ddp # discouraged\n```\n\n----------------------------------------\n\nTITLE: Marking custom forward methods for correct Fabric handling\nDESCRIPTION: Demonstrates how to mark custom methods that should be treated as forward methods using mark_forward_method(), allowing them to work correctly with distributed strategies.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/wrappers.rst#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# You must mark special forward methods explicitly:\nmodel.mark_forward_method(model.generate)\n\n# Passing just the name is also sufficient\nmodel.mark_forward_method(\"generate\")\n\n# OK: Fabric will do some rerouting behind the scenes now\noutput = model.generate()\n```\n\n----------------------------------------\n\nTITLE: Checking Global Process Rank in PyTorch Lightning\nDESCRIPTION: Shows how to check if the current process is the global zero process in multi-node training, useful for operations that should only be performed once.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_38\n\nLANGUAGE: python\nCODE:\n```\ndef training_step(self, batch, batch_idx):\n    if self.trainer.is_global_zero:\n        print(\"in node 0, accelerator 0\")\n```\n\n----------------------------------------\n\nTITLE: Configuring WandbLogger with PyTorch Lightning Fabric\nDESCRIPTION: Code for importing and configuring the WandbLogger in a PyTorch Lightning Fabric setup. The logger is initialized with a project name and passed to the Fabric constructor.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/loggers/wandb.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.fabric import Fabric\n\n# 1. Import the WandbLogger\nfrom wandb.integration.lightning.fabric import WandbLogger\n\n# 2. Configure the logger\nlogger = WandbLogger(project=\"my-project\")\n\n# 3. Pass it to Fabric\nfabric = Fabric(loggers=logger)\n```\n\n----------------------------------------\n\nTITLE: Implementing Custom Logger\nDESCRIPTION: Shows how to create a custom logger by subclassing the Logger class to integrate with external experiment management tools.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/logging_expert.rst#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.loggers import Logger\n\n\nclass LitLogger(Logger):\n    @property\n    def name(self) -> str:\n        return \"my-experiment\"\n\n    @property\n    def version(self):\n        return \"version_0\"\n\n    def log_metrics(self, metrics, step=None):\n        print(\"my logged metrics\", metrics)\n\n    def log_hyperparams(self, params, *args, **kwargs):\n        print(\"my logged hyperparameters\", params)\n```\n\n----------------------------------------\n\nTITLE: Installing Lightning with Conda\nDESCRIPTION: This snippet demonstrates how to install Lightning using conda from the conda-forge channel.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/starter/installation.rst#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda install lightning -c conda-forge\n```\n\n----------------------------------------\n\nTITLE: Setting Precision via Command Line (Bash)\nDESCRIPTION: Shows how to set precision options through the command line interface when running a Fabric script.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/precision.rst#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nfabric run train.py --precision=bf16-mixed\n```\n\n----------------------------------------\n\nTITLE: Logging Single Metric Value\nDESCRIPTION: Demonstrates how to log a single metric value using Fabric's log method.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/logging.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nvalue = ...  # Python scalar or tensor scalar\nfabric.log(\"some_value\", value)\n```\n\n----------------------------------------\n\nTITLE: Configuring Log Frequency in PyTorch Lightning Trainer\nDESCRIPTION: Sets the frequency at which PyTorch Lightning logs during training using the log_every_n_steps parameter in the Trainer constructor. By default, Lightning logs every 50 training steps, but this can be customized to log every k steps.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/extensions/logging.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nk = 10\ntrainer = Trainer(log_every_n_steps=k)\n```\n\n----------------------------------------\n\nTITLE: Implementing Post-LocalSGD Communication in DDP Strategy\nDESCRIPTION: Configures DDP to use Post-LocalSGD communication hook with model parameter averaging. This technique allows for less frequent communication between workers while still maintaining model convergence.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/ddp_optimizations.rst#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\nfrom lightning.pytorch.strategies import DDPStrategy\nfrom torch.distributed.algorithms.ddp_comm_hooks import post_localSGD_hook as post_localSGD\n\nmodel = MyModel()\ntrainer = L.Trainer(\n    accelerator=\"gpu\",\n    devices=4,\n    strategy=DDPStrategy(\n        ddp_comm_state=post_localSGD.PostLocalSGDState(\n            process_group=None,\n            subgroup=None,\n            start_localSGD_iter=8,\n        ),\n        ddp_comm_hook=post_localSGD.post_localSGD_hook,\n        model_averaging_period=4,\n    ),\n)\ntrainer.fit(model)\n```\n\n----------------------------------------\n\nTITLE: Mixing Datasets with CombinedStreamingDataset\nDESCRIPTION: Demonstrates how to mix multiple datasets with specific weightings using CombinedStreamingDataset. This example combines SlimPajama and Starcoder data for LLM training with TokensLoader for efficient handling of tokens with specified block sizes.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/data/README.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.data import StreamingDataset, CombinedStreamingDataset\nfrom lightning.data.streaming.item_loader import TokensLoader\nfrom tqdm import tqdm\nimport os\nfrom torch.utils.data import DataLoader\n\ntrain_datasets = [\n    StreamingDataset(\n        input_dir=\"s3://tinyllama-template/slimpajama/train/\",\n        item_loader=TokensLoader(block_size=2048 + 1), # Optimized loader for tokens used by LLMs \n        shuffle=True,\n        drop_last=True,\n    ),\n    StreamingDataset(\n        input_dir=\"s3://tinyllama-template/starcoder/\",\n        item_loader=TokensLoader(block_size=2048 + 1), # Optimized loader for tokens used by LLMs \n        shuffle=True,\n        drop_last=True,\n    ),\n]\n\n# Mix SlimPajama data and Starcoder data with these proportions:\nweights = (0.693584, 0.306416)\ncombined_dataset = CombinedStreamingDataset(datasets=train_datasets, seed=42, weights=weights)\n\ntrain_dataloader = DataLoader(combined_dataset, batch_size=8, pin_memory=True, num_workers=os.cpu_count())\n\n# Iterate over the combined datasets\nfor batch in tqdm(train_dataloader):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Checking for Shared Filesystem\nDESCRIPTION: Utility function to automatically determine if the current environment uses a shared filesystem between machines, which is important for distributed training configurations.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Check if filesystem is shared between machines\nfrom lightning.pytorch.utilities import is_shared_filesystem\n\nif is_shared_filesystem():\n    # Use single checkpoint location\nelse:\n    # Configure per-node checkpoint handling\n```\n\n----------------------------------------\n\nTITLE: Configuring Loggers in Fabric\nDESCRIPTION: Examples of attaching one or multiple loggers or experiment trackers to Fabric for metrics logging, with options for using no loggers, a single logger, or multiple logger instances.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_args.rst#2025-04-23_snippet_12\n\nLANGUAGE: python\nCODE:\n```\n# Default used by Fabric; no loggers are active\nfabric = Fabric(loggers=[])\n\n# Log to a single logger\nfabric = Fabric(loggers=TensorBoardLogger(...))\n\n# Or multiple instances\nfabric = Fabric(loggers=[logger1, logger2, ...])\n```\n\n----------------------------------------\n\nTITLE: Using Custom Profiler with PyTorch Lightning Trainer\nDESCRIPTION: Demonstrates how to initialize a trainer with a custom profiler instance.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/tuning/profiler_expert.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntrainer = Trainer(profiler=ActionCountProfiler())\ntrainer.fit(...)\n```\n\n----------------------------------------\n\nTITLE: Running NVPROF Profiler Command\nDESCRIPTION: Command to run the NVPROF profiler with profile collection starting after initialization. Output is saved to a trace file.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/tuning/profiler_intermediate.rst#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nnvprof --profile-from-start off -o trace_name.prof -- <regular command here>\n```\n\n----------------------------------------\n\nTITLE: Renaming Dataloader Setup Parameter in Python\nDESCRIPTION: Renames the replace_sampler parameter to use_distributed_sampler in the setup_dataloaders method.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/fabric/CHANGELOG.md#2025-04-23_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nsetup_dataloaders(use_distributed_sampler=...)\n```\n\n----------------------------------------\n\nTITLE: Processing Large Parquet Files with Yield for Memory Efficiency\nDESCRIPTION: Demonstrates using Python's yield mechanism with Lightning's optimize function to process large parquet files without excessive memory usage. The example tokenizes text data from parquet files in batches.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/data/README.md#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nfrom pathlib import Path\nimport pyarrow.parquet as pq\nfrom lightning.data import optimize\nfrom tokenizer import Tokenizer\nfrom functools import partial\n\n# 1. Define a function to convert the text within the parquet files into tokens\ndef tokenize_fn(filepath, tokenizer=None):\n    parquet_file = pq.ParquetFile(filepath)\n    # Process per batch to reduce RAM usage\n    for batch in parquet_file.iter_batches(batch_size=8192, columns=[\"content\"]):\n        for text in batch.to_pandas()[\"content\"]:\n            yield tokenizer.encode(text, bos=False, eos=True)\n\n# 2. Generate the inputs\ninput_dir = \"/teamspace/s3_connections/tinyllama-template\"\ninputs = [str(file) for file in Path(f\"{input_dir}/starcoderdata\").rglob(\"*.parquet\")]\n\n# 3. Store the optimized data wherever you want under \"/teamspace/datasets\" or \"/teamspace/s3_connections\"\noutputs = optimize(\n    fn=partial(tokenize_fn, tokenizer=Tokenizer(f\"{input_dir}/checkpoints/Llama-2-7b-hf\")), # Note: You can use HF tokenizer or any others\n    inputs=inputs,\n    output_dir=\"/teamspace/datasets/starcoderdata\",\n    chunk_size=(2049 * 8012),\n)\n```\n\n----------------------------------------\n\nTITLE: Configuring PyTorch Lightning Console Logging\nDESCRIPTION: Demonstrates how to configure logging settings for PyTorch Lightning, including setting the root logging level and redirecting module-specific logs to files. Shows both global and module-level logging configuration.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/console_logs.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport logging\n\n# configure logging at the root level of Lightning\nlogging.getLogger(\"lightning.pytorch\").setLevel(logging.ERROR)\n\n# configure logging on module level, redirect to file\nlogger = logging.getLogger(\"lightning.pytorch.core\")\nlogger.addHandler(logging.FileHandler(\"core.log\"))\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Plugins in Fabric\nDESCRIPTION: Example of creating and using custom plugins by subclassing the appropriate class, shown here with a custom ClusterEnvironment implementation for specialized cluster configurations.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_args.rst#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.fabric.plugins.environments import ClusterEnvironment\n\n\nclass MyCluster(ClusterEnvironment):\n    @property\n    def main_address(self):\n        return your_main_address\n\n    @property\n    def main_port(self):\n        return your_main_port\n\n    def world_size(self):\n        return the_world_size\n\n\nfabric = Fabric(plugins=[MyCluster()], ...)\n```\n\n----------------------------------------\n\nTITLE: Installing Optimized Lightning Package for ML Workflows\nDESCRIPTION: This snippet demonstrates how to install the optimized Lightning package for ML workflows, which has fewer dependencies and is suitable for production deployments.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/starter/installation.rst#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npip install lightning-app\n```\n\n----------------------------------------\n\nTITLE: Running CLI with YAML Config\nDESCRIPTION: Basic commands for running the Lightning CLI with a YAML configuration file and overriding parameters.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_advanced.rst#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython main.py fit --config config.yaml\npython main.py fit --config config.yaml --trainer.max_epochs 100\n```\n\n----------------------------------------\n\nTITLE: Accessing the original unwrapped model\nDESCRIPTION: Shows how to access the original model explicitly through the .module property of the Fabric-wrapped model.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/wrappers.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Access the original model explicitly\noriginal_model = fabric_model.module\n\nprint(original_model is model)  # True\n```\n\n----------------------------------------\n\nTITLE: Configuring Experiment Logging in PyTorch Lightning\nDESCRIPTION: Examples of integrating various experiment tracking tools with PyTorch Lightning. Shows how to configure TensorBoard, Weights & Biases, Comet, MLflow, and Neptune loggers.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/pytorch_lightning/README.md#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom pytorch_lightning import loggers\n\n# tensorboard\ntrainer = Trainer(logger=TensorBoardLogger(\"logs/\"))\n\n# weights and biases\ntrainer = Trainer(logger=loggers.WandbLogger())\n\n# comet\ntrainer = Trainer(logger=loggers.CometLogger())\n\n# mlflow\ntrainer = Trainer(logger=loggers.MLFlowLogger())\n\n# neptune\ntrainer = Trainer(logger=loggers.NeptuneLogger())\n\n# ... and dozens more\n```\n\n----------------------------------------\n\nTITLE: Setting Example Input Array for Layer Dimension Printing in PyTorch Lightning\nDESCRIPTION: Shows how to set an example input array in a LightningModule to enable printing of input and output layer dimensions.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/debug/debugging_basic.rst#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nclass LitModel(LightningModule):\n    def __init__(self, *args, **kwargs):\n        self.example_input_array = torch.Tensor(32, 1, 28, 28)\n```\n\n----------------------------------------\n\nTITLE: Synchronizing Processes with Fabric Barrier\nDESCRIPTION: Force all processes to wait and synchronize at a specific point in code. This is useful for operations that need to be completed on one process before others can proceed, such as downloading data.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_methods.rst#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nif fabric.global_rank == 0:\n    print(\"Downloading dataset. This can take a while ...\")\n    download_dataset(\"http://...\")\n\n# All other processes wait here until rank 0 is done with downloading:\nfabric.barrier()\n\n# After everyone reached the barrier, they can access the downloaded files:\nload_dataset()\n```\n\n----------------------------------------\n\nTITLE: Running the Tensor Parallel Training Example\nDESCRIPTION: Commands to navigate to the example directory and execute the training script that demonstrates tensor parallelism with Llama 3 7B model.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/pytorch/tensor_parallel/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ncd examples/pytorch/tensor_parallel\npython train.py\n```\n\n----------------------------------------\n\nTITLE: Converting Distributed Checkpoints via Command Line\nDESCRIPTION: Command line utility for converting distributed checkpoints to a single-file format for use in non-FSDP contexts or deployment.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/checkpointing_expert.rst#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m lightning.pytorch.utilities.consolidate_checkpoint path/to/my/checkpoint\n```\n\n----------------------------------------\n\nTITLE: Simple Python Script Launch - Bash\nDESCRIPTION: Command to run a Fabric-enabled Python script directly from the command line.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/launch.rst#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython train.py\n```\n\n----------------------------------------\n\nTITLE: Installing Neptune.ai for PyTorch Lightning\nDESCRIPTION: Provides two methods to install the Neptune.ai package: using pip or conda.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/supported_exp_managers.rst#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install neptune\n```\n\nLANGUAGE: bash\nCODE:\n```\nconda install -c conda-forge neptune\n```\n\n----------------------------------------\n\nTITLE: Custom Progress Bar Implementation\nDESCRIPTION: Migration from process_position flag to direct ProgressBar callback configuration\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_6_advanced.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Old way\ntrainer = Trainer(process_position=2)\n\n# New way\nprogress_bar = ProgressBar(process_position=2)\ntrainer = Trainer(callbacks=[progress_bar])\n```\n\n----------------------------------------\n\nTITLE: Using LRFinder with DDP in PyTorch Lightning\nDESCRIPTION: Added support for using the Learning Rate Finder (LRFinder) with DistributedDataParallel (DDP).\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_18\n\nLANGUAGE: Python\nCODE:\n```\nfrom pytorch_lightning import Trainer\n\ntrainer = Trainer(strategy=\"ddp\")\ntrainer.tune(model)\n\n```\n\n----------------------------------------\n\nTITLE: Installing DeepSpeed via pip\nDESCRIPTION: Command to install the DeepSpeed library via pip, which is required before using any DeepSpeed strategy in PyTorch Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/deepspeed.rst#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install deepspeed\n```\n\n----------------------------------------\n\nTITLE: Running Docker Image with GPU Support\nDESCRIPTION: Command to run a PyTorch Lightning Docker image with GPU support using the '--gpus all' flag.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/dockers/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -it --gpus all pytorchlightning/pytorch_lightning:base-cuda-py3.9-torch1.13-cuda11.7.1\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch Lightning Model via Command Line\nDESCRIPTION: Commands to install the required torchvision package and run the PyTorch Lightning model script.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install torchvision\npython main.py\n```\n\n----------------------------------------\n\nTITLE: Logging Gradient Norms in PyTorch Lightning\nDESCRIPTION: Added hook to LightningModule for customizing the logging of gradient norms.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_38\n\nLANGUAGE: python\nCODE:\n```\nlog_grad_norm\n```\n\n----------------------------------------\n\nTITLE: Installing Weights and Biases for PyTorch Lightning\nDESCRIPTION: Installs the Weights and Biases (wandb) package using pip for use with PyTorch Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/supported_exp_managers.rst#2025-04-23_snippet_8\n\nLANGUAGE: bash\nCODE:\n```\npip install wandb\n```\n\n----------------------------------------\n\nTITLE: Implementing Debug Points in Training Step\nDESCRIPTION: Example of adding debugging statements and breakpoints in a Lightning Module's training step, including rank-specific debugging and process synchronization.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/debug/debugging_advanced.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass LitModel(LightningModule):\n    def training_step(self, batch, batch_idx):\n        debugging_message = ...\n        print(f\"RANK - {self.trainer.global_rank}: {debugging_message}\")\n\n        if self.trainer.global_rank == 0:\n            import pdb\n\n            pdb.set_trace()\n\n        # to prevent other processes from moving forward until all processes are in sync\n        self.trainer.strategy.barrier()\n```\n\n----------------------------------------\n\nTITLE: Example Input Array Usage in PyTorch Lightning\nDESCRIPTION: Shows how to set up and use example_input_array for generating sample outputs, typically used in model visualization or testing.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/lightning_module.rst#2025-04-23_snippet_20\n\nLANGUAGE: python\nCODE:\n```\ndef __init__(self):\n    self.example_input_array = ...\n    self.generator = ...\n\n\ndef on_train_epoch_end(self):\n    # generate some images using the example_input_array\n    gen_images = self.generator(self.example_input_array)\n```\n\n----------------------------------------\n\nTITLE: Documenting Lightning Fabric Precision Plugins with reStructuredText\nDESCRIPTION: This RST code defines the documentation structure for precision plugins in the lightning.fabric.plugins.precision module. It lists various precision classes including DoublePrecision, MixedPrecision, HalfPrecision, and specialized options like FSDPPrecision and TransformerEnginePrecision.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/precision.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: ../links.rst\n\n##################################\nlightning.fabric.plugins.precision\n##################################\n\n\nPrecision\n^^^^^^^^^\n\n.. currentmodule:: lightning.fabric.plugins.precision\n\n.. autosummary::\n    :toctree: ./generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    Precision\n    DoublePrecision\n    MixedPrecision\n    HalfPrecision\n    XLAPrecision\n    FSDPPrecision\n    DeepSpeedPrecision\n    TransformerEnginePrecision\n    BitsandbytesPrecision\n```\n\n----------------------------------------\n\nTITLE: Configuring Custom Saving Directory for PyTorch Lightning Logs\nDESCRIPTION: Demonstrates how to set a custom directory for saving PyTorch Lightning logs using the default_root_dir argument in the Trainer.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/logging_basic.rst#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nTrainer(default_root_dir=\"/your/custom/path\")\n```\n\n----------------------------------------\n\nTITLE: Linking Arguments in LightningCLI with Python\nDESCRIPTION: Shows how to link arguments between model and data module components in LightningCLI to avoid redundant configuration.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_expert.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nclass MyLightningCLI(LightningCLI):\n    def add_arguments_to_parser(self, parser):\n        parser.link_arguments(\"data.batch_size\", \"model.batch_size\")\n\n\ncli = MyLightningCLI(MyModel, MyDataModule)\n```\n\n----------------------------------------\n\nTITLE: Comparing PyTorch and Fabric Implementation Files\nDESCRIPTION: Command to visually compare the differences between the pure PyTorch and Lightning Fabric implementations using the sdiff utility.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/fabric/image_classifier/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsdiff train_torch.py train_fabric.py\n```\n\n----------------------------------------\n\nTITLE: Initializing Trainer with Default TensorBoard Logger in PyTorch Lightning\nDESCRIPTION: Shows how to initialize a Trainer with the default TensorBoard logger enabled.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/logging_basic.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# every trainer already has tensorboard enabled by default (if the dependency is available)\ntrainer = Trainer()\n```\n\n----------------------------------------\n\nTITLE: Using WandbLogger for artifact management in PyTorch Lightning\nDESCRIPTION: Demonstrates how to use the new WandbLogger methods for downloading and using artifacts with Weights and Biases integration.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_24\n\nLANGUAGE: Python\nCODE:\n```\nfrom pytorch_lightning.loggers import WandbLogger\n\nlogger = WandbLogger()\n\n# Download an artifact\nlogger.download_artifact(artifact_name=\"my_dataset\", artifact_type=\"dataset\")\n\n# Use an artifact\nlogger.use_artifact(artifact_name=\"my_model:v1\", artifact_type=\"model\")\n```\n\n----------------------------------------\n\nTITLE: RST Display Item Definition\nDESCRIPTION: RST directive for defining individual learning level cards with headers, descriptions, and navigation links.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/expertise_levels.rst#2025-04-23_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. displayitem::\n   :header: Level 1: Train a model\n   :description: Learn the basics of training a model.\n   :button_link: model/train_model_basic.html\n   :col_css: col-md-6\n   :height: 150\n   :tag: basic\n```\n\n----------------------------------------\n\nTITLE: Saving TensorBoard Logs in Subdirectory\nDESCRIPTION: Added parameter to TensorBoardLogger to specify a subdirectory for logs.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_37\n\nLANGUAGE: python\nCODE:\n```\nTensorBoardLogger(sub_dir=\"string\")\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch Lightning via conda\nDESCRIPTION: Command to install PyTorch Lightning using conda package manager from conda-forge channel\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/index.rst#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda install lightning -c conda-forge\n```\n\n----------------------------------------\n\nTITLE: Upgrading Single Checkpoint File in PyTorch Lightning\nDESCRIPTION: Command line instruction for permanently upgrading a single checkpoint file to the current Lightning version.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/checkpointing_migration.rst#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython -m lightning.pytorch.utilities.upgrade_checkpoint path/to/model.ckpt\n```\n\n----------------------------------------\n\nTITLE: Installing Lightning with conda\nDESCRIPTION: This command installs the Lightning package using conda from the conda-forge channel. This provides an alternative installation method for conda users.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/installation.rst#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda install lightning -c conda-forge\n```\n\n----------------------------------------\n\nTITLE: Enabling MPS Accelerator in PyTorch Lightning Trainer\nDESCRIPTION: This code snippet demonstrates how to configure the PyTorch Lightning Trainer to use the MPS accelerator for Apple silicon GPUs. It sets the accelerator to 'mps' and specifies the use of 1 device, as MPS only supports single-device operations currently.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/accelerators/mps_basic.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntrainer = Trainer(accelerator=\"mps\", devices=1)\n```\n\n----------------------------------------\n\nTITLE: SLURM Interactive Mode Command\nDESCRIPTION: Command to start an interactive SLURM session for development and debugging purposes.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/multi_node/slurm.rst#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\n# make sure to set `--job-name \"interactive\"`\nsrun --account <your-account> --pty bash --job-name \"interactive\" ...\n\n# now run scripts normally\npython train.py ...\n```\n\n----------------------------------------\n\nTITLE: Specifying DeepSpeed Dependency for PyTorch Lightning\nDESCRIPTION: This snippet defines the DeepSpeed library dependency for PyTorch Lightning. It specifies a version range and excludes Windows and Darwin (macOS) platforms. The upper bound is set for CI stability but may be dropped during installation.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/requirements/pytorch/strategies.txt#2025-04-23_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\ndeepspeed >=0.8.2, <=0.9.3; platform_system != \"Windows\" and platform_system != \"Darwin\"  # strict\n```\n\n----------------------------------------\n\nTITLE: Testing Pickle Compatibility in PyTorch Lightning\nDESCRIPTION: Code example for testing if a model is pickle-compatible, which is required for certain distributed strategies.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/accelerators/gpu_faq.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nimport pickle\n\nmodel = YourModel()\npickle.dumps(model)\n```\n\n----------------------------------------\n\nTITLE: Logging models with MLFlowLogger in PyTorch Lightning\nDESCRIPTION: Added log_model parameter to MLFlowLogger to control model logging behavior.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_22\n\nLANGUAGE: Python\nCODE:\n```\nfrom pytorch_lightning.loggers import MLFlowLogger\n\nlogger = MLFlowLogger(log_model=True)\n\n```\n\n----------------------------------------\n\nTITLE: Installing Rich Progress Bar Package\nDESCRIPTION: Command to install the Rich package required for using RichProgressBar in PyTorch Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/progress_bar.rst#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install rich\n```\n\n----------------------------------------\n\nTITLE: Installing Lightning with pip\nDESCRIPTION: This snippet shows how to install Lightning using pip inside a virtual environment or conda environment.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/starter/installation.rst#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython -m pip install lightning\n```\n\n----------------------------------------\n\nTITLE: Loading Checkpoints with Fabric\nDESCRIPTION: Shows different methods for loading checkpoint data, including full state restoration and partial loading for inference.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/checkpoint/checkpoint.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nfabric.load(\"path/to/checkpoint.ckpt\", state)\n```\n\nLANGUAGE: python\nCODE:\n```\nstate = {\"model1\": model1}\nremainder = fabric.load(\"path/to/checkpoint.ckpt\", state)\n```\n\nLANGUAGE: python\nCODE:\n```\nfull_checkpoint = fabric.load(\"path/to/checkpoint.ckpt\")\n\nmodel.load_state_dict(full_checkpoint[\"model\"])\noptimizer.load_state_dict(full_checkpoint[\"optimizer\"])\n...\n```\n\n----------------------------------------\n\nTITLE: Running Backbone Image Classifier Example with PyTorch Lightning\nDESCRIPTION: Commands to run the Backbone Image Classifier example on CPU, GPUs, and with Distributed Data Parallel. This script demonstrates implementing a LightningModule as a system, which simplifies exporting to production.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/pytorch/basics/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n# CPU\npython backbone_image_classifier.py\n\n# GPUs (any number)\npython backbone_image_classifier.py --trainer.accelerator 'gpu' --trainer.devices 2\n\n# Distributed Data Parallel (DDP)\npython backbone_image_classifier.py --trainer.accelerator 'gpu' --trainer.devices 2 --trainer.strategy 'ddp'\n```\n\n----------------------------------------\n\nTITLE: Running Lightning Fabric Implementation\nDESCRIPTION: Command to execute the Lightning Fabric accelerated version of DCGAN\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/fabric/dcgan/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython train_fabric.py\n```\n\n----------------------------------------\n\nTITLE: Configuring True 16-bit Precision in PyTorch Lightning\nDESCRIPTION: Sets up the Trainer to use true 16-bit precision, which can lower memory consumption by up to half, allowing for larger models. However, this may lead to unstable training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/precision_basic.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nTrainer(precision=\"16-true\")\n```\n\n----------------------------------------\n\nTITLE: Running Full PyTorch-Lightning Test Suite\nDESCRIPTION: This command executes the complete test suite for PyTorch-Lightning using a make script.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/tests/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nmake test\n```\n\n----------------------------------------\n\nTITLE: Loading Checkpoint During Training in PyTorch Lightning\nDESCRIPTION: Demonstrates how to resume training from a checkpoint using the Trainer's fit method with a checkpoint path parameter.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/checkpointing_migration.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ntrainer = Trainer(...)\ntrainer.fit(model, ckpt_path=\"path/to/checkpoint.ckpt\")\n```\n\n----------------------------------------\n\nTITLE: Defining a custom model with multiple forward methods\nDESCRIPTION: Example of a PyTorch model with both a standard forward method and a custom generate method that uses the forward method indirectly.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/wrappers.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport torch\nimport lightning as L\n\n\nclass MyModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer = torch.nn.Linear(10, 2)\n\n    def forward(self, x):\n        return self.layer(x)\n\n    def generate(self):\n        sample = torch.randn(10)\n        return self(sample)\n```\n\n----------------------------------------\n\nTITLE: RST Table of Contents Structure for PyTorch Lightning Documentation\nDESCRIPTION: This RST code defines a hidden table of contents that points to fundamental guides in the PyTorch Lightning documentation, including conversion guides, accelerator usage, code structure, launching, notebooks, and precision settings.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/levels/basic.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 1\n   :hidden:\n\n    <../fundamentals/convert>\n    <../fundamentals/accelerators>\n    <../fundamentals/code_structure>\n    <../fundamentals/launch>\n    <../fundamentals/notebooks>\n    <../fundamentals/precision>\n```\n\n----------------------------------------\n\nTITLE: Executing the Distributed, Low-Precision Transformer Training Script\nDESCRIPTION: This command runs the training script for the distributed, low-precision Transformer model. It utilizes the ModelParallelStrategy, torch ao for fp8 precision, and FSDP2 for distributed training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/fabric/fp8_distributed_transformer/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npython train.py\n```\n\n----------------------------------------\n\nTITLE: Using Trained AutoEncoder for Feature Extraction\nDESCRIPTION: Demonstrates how to use the trained AutoEncoder model to extract image representations using the forward method of the Lightning module.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/child_modules.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nsome_images = torch.Tensor(32, 1, 28, 28)\nrepresentations = lightning_module(some_images)\n```\n\n----------------------------------------\n\nTITLE: Custom Optimizers Implementation\nDESCRIPTION: Implementation showing how to create and use custom optimizer classes with LightningCLI\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_intermediate_2.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# main.py\nimport torch\nfrom lightning.pytorch.cli import LightningCLI\nfrom lightning.pytorch.demos.boring_classes import DemoModel, BoringDataModule\n\n\nclass LitAdam(torch.optim.Adam):\n    def step(self, closure):\n        print(\"⚡\", \"using LitAdam\", \"⚡\")\n        super().step(closure)\n\n\nclass FancyAdam(torch.optim.Adam):\n    def step(self, closure):\n        print(\"⚡\", \"using FancyAdam\", \"⚡\")\n        super().step(closure)\n\n\ncli = LightningCLI(DemoModel, BoringDataModule)\n```\n\n----------------------------------------\n\nTITLE: Setting Minimum Epochs in PyTorch Lightning\nDESCRIPTION: Example showing how to set the minimum number of training epochs in the Trainer. This ensures training runs for at least the specified number of epochs.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_18\n\nLANGUAGE: python\nCODE:\n```\n# default used by the Trainer\ntrainer = Trainer(min_epochs=1)\n```\n\n----------------------------------------\n\nTITLE: Installing TPU Dependencies in Google Colab\nDESCRIPTION: Command to install XLA library for TPU support in PyTorch on Google Colab environment.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/accelerators/tpu_basic.rst#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n!pip install cloud-tpu-client https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.13-cp38-cp38m-linux_x86_64.whl\n```\n\n----------------------------------------\n\nTITLE: Configuring Entry Points in setup.py\nDESCRIPTION: Setup configuration for a Python package that registers Lightning callbacks through entry points. Demonstrates how to specify the entry point group 'lightning.pytorch.callbacks_factory' and link it to the factory function.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/extensions/entry_points.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom setuptools import setup\n\nsetup(\n    name=\"my-package\",\n    version=\"0.0.1\",\n    install_requires=[\"lightning\"],\n    entry_points={\n        \"lightning.pytorch.callbacks_factory\": [\n            # The format here must be [any name]=[module path]:[function name]\n            \"monitor_callbacks=factories:my_custom_callbacks_factory\"\n        ]\n    },\n)\n```\n\n----------------------------------------\n\nTITLE: Visualizing Profile with NVVP\nDESCRIPTION: Command to visualize the profiler trace file using the NVIDIA Visual Profiler (NVVP) tool.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/tuning/profiler_intermediate.rst#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\nnvvp trace_name.prof\n```\n\n----------------------------------------\n\nTITLE: TorchRun Command for Primary Node in 2-Node Setup\nDESCRIPTION: Example command for the primary node (rank 0) in a two-node distributed setup with 8 GPUs per node, using IP 10.10.10.16 as master address and port 50000.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/clouds/cluster_intermediate_2.rst#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntorchrun \\\n    --nproc_per_node=8 --nnodes=2 --node_rank 0 \\\n    --master_addr 10.10.10.16 --master_port 50000 \\\n    train.py\n```\n\n----------------------------------------\n\nTITLE: Customizing Rich Progress Bar Theme in PyTorch Lightning\nDESCRIPTION: Demonstrates how to create and apply a custom theme for RichProgressBar with specific color and formatting settings.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/progress_bar.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.callbacks import RichProgressBar\nfrom lightning.pytorch.callbacks.progress.rich_progress import RichProgressBarTheme\n\n# create your own theme!\nprogress_bar = RichProgressBar(\n    theme=RichProgressBarTheme(\n        description=\"green_yellow\",\n        progress_bar=\"green1\",\n        progress_bar_finished=\"green1\",\n        progress_bar_pulse=\"#6206E0\",\n        batch_progress=\"green_yellow\",\n        time=\"grey82\",\n        processing_speed=\"grey82\",\n        metrics=\"grey82\",\n        metrics_text_delimiter=\"\\n\",\n        metrics_format=\".3e\",\n    )\n)\n\ntrainer = Trainer(callbacks=progress_bar)\n```\n\n----------------------------------------\n\nTITLE: Specifying litdata Package Version for PyTorch Lightning\nDESCRIPTION: Defines the version range for the 'litdata' package required for the 'data' extra installation option in PyTorch Lightning. The package version must be greater than or equal to 0.2.0rc and less than 0.3.0.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/requirements/data/data.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nlitdata >= 0.2.0rc, <0.3.0\n```\n\n----------------------------------------\n\nTITLE: Printing Model Summary Manually in PyTorch Lightning\nDESCRIPTION: Shows how to print a model summary manually without calling the fit method in PyTorch Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/debug/debugging_basic.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.utilities.model_summary import ModelSummary\n\nmodel = LitModel()\nsummary = ModelSummary(model, max_depth=-1)\nprint(summary)\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch Lightning CLI Commands\nDESCRIPTION: Examples of CLI commands for viewing help, and running different subcommands like fit, validate, test, and predict.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_intermediate.rst#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n$ python main.py --help\n```\n\nLANGUAGE: bash\nCODE:\n```\n$ python main.py fit\n$ python main.py validate\n$ python main.py test\n$ python main.py predict\n```\n\n----------------------------------------\n\nTITLE: Updating Lightning Installation Command\nDESCRIPTION: Change in pip installation command for Lightning with app dependencies. The update requires explicitly specifying [app] extra to install lightning.app dependencies.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/2_0_regular.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Old way\npip install lightning\n\n# New way\npip install lightning[app]\n```\n\n----------------------------------------\n\nTITLE: Training Lightning Model\nDESCRIPTION: Code showing how to initialize a Lightning Trainer and train the model.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/starter/introduction.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrainer = L.Trainer(limit_train_batches=100, max_epochs=1)\ntrainer.fit(model=autoencoder, train_dataloaders=train_loader)\n```\n\n----------------------------------------\n\nTITLE: Defining Model in LightningModule (Python)\nDESCRIPTION: Shows how to define a model within a LightningModule, though this approach is not recommended for reusability and scalability.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/starter/style_guide.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass LitModel(LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.layer_1 = nn.Linear()\n        self.layer_2 = nn.Linear()\n        self.layer_3 = nn.Linear()\n```\n\n----------------------------------------\n\nTITLE: Generating Class Title for PyTorch Lightning Documentation\nDESCRIPTION: Creates a title for the class documentation. The class name is dynamically inserted and underlined using a template filter.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/_templates/classtemplate.rst#2025-04-23_snippet_2\n\nLANGUAGE: reStructuredText\nCODE:\n```\n{{ name | underline }}\n```\n\n----------------------------------------\n\nTITLE: Updating PyTorch Lightning Trainer Data Loading Method\nDESCRIPTION: Replacing deprecated Trainer.reset_train_val_dataloaders() with Trainer.fit_loop.setup_data() for resetting data loaders.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_8_regular.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nTrainer.fit_loop.setup_data()\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Profiling Documentation Structure\nDESCRIPTION: ReStructuredText markup that defines the documentation structure and navigation cards for different profiling skill levels\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/tuning/profiler.rst#2025-04-23_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. _profiler:\n\n#############################\nFind bottlenecks in your code\n#############################\n\n.. displayitem::\n   :header: Basic\n   :description: Learn to find bottlenecks in the training loop.\n   :col_css: col-md-3\n   :button_link: profiler_basic.html\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Intermediate\n   :description: Learn to find bottlenecks in PyTorch operations.\n   :col_css: col-md-3\n   :button_link: profiler_intermediate.html\n   :height: 150\n   :tag: intermediate\n\n.. displayitem::\n   :header: Advanced\n   :description: Learn to profile TPU code.\n   :col_css: col-md-3\n   :button_link: profiler_advanced.html\n   :height: 150\n   :tag: advanced\n\n.. displayitem::\n   :header: Expert\n   :description: Learn to build your own profiler or profile custom pieces of code\n   :col_css: col-md-3\n   :button_link: profiler_expert.html\n   :height: 150\n   :tag: expert\n```\n\n----------------------------------------\n\nTITLE: Distributed Training Queue Operations\nDESCRIPTION: Migration from LightningModule queue operations to DDPStrategy methods\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_6_advanced.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Old way - in LightningModule\nself.add_to_queue()\nself.get_from_queue()\n\n# New way\nstrategy = DDPStrategy(start_method='spawn')\nstrategy.add_to_queue()\nstrategy.get_from_queue()\n```\n\n----------------------------------------\n\nTITLE: Setting TQDM Progress Bar Leave Option in PyTorch Lightning\nDESCRIPTION: Demonstrates how to configure the TQDMProgressBar to display a new progress bar at the end of every epoch using the leave parameter.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/progress_bar.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntrainer = Trainer(callbacks=[TQDMProgressBar(leave=True)])\n```\n\n----------------------------------------\n\nTITLE: Installing Future Stable Version of PyTorch Lightning\nDESCRIPTION: Command to install the upcoming stable release of PyTorch Lightning directly from the GitHub repository.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/pytorch_lightning/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install https://github.com/Lightning-AI/lightning/archive/refs/heads/release/stable.zip -U\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch-Lightning Development Dependencies\nDESCRIPTION: This snippet shows how to clone the PyTorch-Lightning repository and install the required dependencies for development and testing.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/tests/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# clone the repo\ngit clone https://github.com/Lightning-AI/lightning.git\ncd lightning\n\n# install required dependencies\nexport PACKAGE_NAME=pytorch\npython -m pip install \".[dev, examples]\"\n# install pre-commit (optional)\npython -m pip install pre-commit\npre-commit install\n```\n\n----------------------------------------\n\nTITLE: DDP Training Launch with GPUs - Bash\nDESCRIPTION: Command for launching distributed data parallel training using 8 GPUs and bfloat16 precision using the Fabric CLI.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/launch.rst#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nfabric run ./path/to/train.py \\\n    --strategy=ddp \\\n    --devices=8 \\\n    --accelerator=cuda \\\n    --precision=\"bf16\"\n```\n\n----------------------------------------\n\nTITLE: Installing Lightning Fabric with conda\nDESCRIPTION: Command to install Lightning Fabric using conda package manager from the conda-forge channel.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/index.rst#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nconda install lightning -c conda-forge\n```\n\n----------------------------------------\n\nTITLE: Installing NVIDIA Container Toolkit for GPU Support\nDESCRIPTION: Commands to install the NVIDIA Container Toolkit, which enables GPU access for Docker containers. This includes adding package repositories and installing necessary components.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/dockers/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# Add the package repositories\ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID)\ncurl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\ncurl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n\nsudo apt-get update && sudo apt-get install -y nvidia-container-toolkit\nsudo systemctl restart docker\n```\n\n----------------------------------------\n\nTITLE: Loading Distributed Checkpoints with FSDP Strategy\nDESCRIPTION: Shows how to load a distributed checkpoint using Fabric's FSDP strategy. The checkpoint can be loaded even with a different number of GPUs than when it was saved.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/checkpoint/distributed_checkpoint.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\nfrom lightning.fabric.strategies import FSDPStrategy\n\n# 1. Select the FSDP strategy\nfabric = L.Fabric(devices=2, strategy=FSDPStrategy(), ...)\nfabric.launch()\n...\nmodel, optimizer = fabric.setup(model, optimizer)\n\n# 2. Define model, optimizer, and other training loop state\nstate = {\"model\": model, \"optimizer\": optimizer, \"iter\": iteration}\n\n# 3. Load using Fabric's method\nfabric.load(\"path/to/checkpoint/file\", state)\n\n# DON'T do this (inefficient):\n# model.load_state_dict(torch.load(\"path/to/checkpoint/file\"))\n```\n\n----------------------------------------\n\nTITLE: Resuming from Partial Checkpoint in PyTorch Lightning\nDESCRIPTION: This code shows how to configure a LightningModule to load a partial checkpoint by disabling strict loading, useful for cases like loading pretrained feature extractors.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/checkpointing_intermediate.rst#2025-04-23_snippet_9\n\nLANGUAGE: Python\nCODE:\n```\nimport lightning as L\n\nclass LitModel(L.LightningModule):\n    def __init__(self):\n        super().__init__()\n\n        # This model only trains the decoder, we don't save the encoder\n        self.encoder = from_pretrained(...).requires_grad_(False)\n        self.decoder = Decoder()\n\n        # Set to False because we only care about the decoder\n        self.strict_loading = False\n\n    def state_dict(self):\n        # Don't save the encoder, it is not being trained\n```\n\n----------------------------------------\n\nTITLE: Initializing Lightning Trainer in Notebooks\nDESCRIPTION: Demonstrates how to initialize a Lightning Trainer in interactive notebooks with automatic device detection.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/notebooks.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\n\n# Works in Jupyter, Colab and Kaggle!\ntrainer = L.Trainer(accelerator=\"auto\", devices=\"auto\")\n```\n\n----------------------------------------\n\nTITLE: Multiple LightningModules Implementation\nDESCRIPTION: Implementation showing how to support multiple model classes with LightningCLI\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_intermediate_2.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# main.py\nfrom lightning.pytorch.cli import LightningCLI\nfrom lightning.pytorch.demos.boring_classes import DemoModel, BoringDataModule\n\n\nclass Model1(DemoModel):\n    def configure_optimizers(self):\n        print(\"⚡\", \"using Model1\", \"⚡\")\n        return super().configure_optimizers()\n\n\nclass Model2(DemoModel):\n    def configure_optimizers(self):\n        print(\"⚡\", \"using Model2\", \"⚡\")\n        return super().configure_optimizers()\n\n\ncli = LightningCLI(datamodule_class=BoringDataModule)\n```\n\n----------------------------------------\n\nTITLE: Forward Method Execution with a Compiled Model\nDESCRIPTION: Demonstrates the execution pattern with compiled models, where the first forward pass initiates compilation (slow) and subsequent passes benefit from optimization (fast).\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/compile.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# 1st execution compiles the model (slow)\noutput = model(input)\n\n# All future executions will be fast (for inputs of the same size)\noutput = model(input)\noutput = model(input)\n...\n```\n\n----------------------------------------\n\nTITLE: Building Documentation with Make in Bash\nDESCRIPTION: This snippet shows the bash commands for updating git submodules and building the documentation using make. It's essential for local documentation testing before submitting a pull request.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ngit submodule update --init --recursive\nmake docs\n```\n\n----------------------------------------\n\nTITLE: TorchRun Command for Secondary Node in 2-Node Setup\nDESCRIPTION: Example command for the secondary node (rank 1) in a two-node distributed setup with 8 GPUs per node, connecting to master node at 10.10.10.16:50000.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/clouds/cluster_intermediate_2.rst#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ntorchrun \\\n    --nproc_per_node=8 --nnodes=2 --node_rank 1 \\\n    --master_addr 10.10.10.16 --master_port 50000 \\\n    train.py\n```\n\n----------------------------------------\n\nTITLE: Updating Logging Synchronization in PyTorch Lightning\nDESCRIPTION: Replace 'sync_dist_op' with 'reduce_fx' in self.log() calls. The new parameter accepts 'mean' or a callable function.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_5_advanced.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Old\nself.log(sync_dist_op=...)\n\n# New\nself.log(reduce_fx=...)\n```\n\n----------------------------------------\n\nTITLE: Structuring Content with HTML in PyTorch Lightning Documentation\nDESCRIPTION: This HTML snippet creates a structured layout for displaying cards with links to various optimization techniques in PyTorch Lightning. It uses custom CSS classes and raw HTML within a reStructuredText document.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/levels/intermediate_level_13.rst#2025-04-23_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"display-card-container\">\n    <div class=\"row\">\n\n        </div>\n    </div>\n```\n\n----------------------------------------\n\nTITLE: Autodocumenting Class in PyTorch Lightning with Sphinx\nDESCRIPTION: Uses Sphinx's autoclass directive to automatically generate documentation for a class, including its members. The class name is provided via a template variable.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/_templates/classtemplate.rst#2025-04-23_snippet_3\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autoclass:: {{ name }}\n    :members:\n```\n\n----------------------------------------\n\nTITLE: Updating Trainer Loop Property in PyTorch Lightning\nDESCRIPTION: Replace Trainer.train_loop property assignments with Trainer.fit_loop property. Referenced in PR8025.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_5_devel.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nTrainer.train_loop\n```\n\nLANGUAGE: python\nCODE:\n```\nTrainer.fit_loop\n```\n\n----------------------------------------\n\nTITLE: Disabling Progress Bar in PyTorch Lightning\nDESCRIPTION: Demonstrates how to disable the progress bar functionality in the PyTorch Lightning Trainer.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/progress_bar.rst#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\ntrainer = Trainer(enable_progress_bar=False)\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch Lightning with Optional Dependencies\nDESCRIPTION: Installation command for PyTorch Lightning with extra optional dependencies.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/pytorch_lightning/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install pytorch-lightning['extra']\n```\n\n----------------------------------------\n\nTITLE: Configuring Network Settings for NCCL Communication\nDESCRIPTION: These bash commands demonstrate how to configure network settings to allow NCCL communication between nodes in a PyTorch Lightning cluster setup.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/multi_node/barebones.rst#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\necho \"net.ipv4.ip_local_port_range = 50000 51000\" >> /etc/sysctl.conf\nsysctl --system\nufw allow 50000:51000/tcp\n```\n\n----------------------------------------\n\nTITLE: Basic CLI Command Examples\nDESCRIPTION: Examples of using CLI commands to mix and match different models with datasets\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_intermediate_2.rst#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Mix and match anything\n$ python main.py fit --model=GAN --data=MNIST\n$ python main.py fit --model=Transformer --data=MNIST\n```\n\n----------------------------------------\n\nTITLE: Configuring Model Pruning in PyTorch Lightning\nDESCRIPTION: Added option to choose when to apply pruning during training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_36\n\nLANGUAGE: python\nCODE:\n```\nModelPruning(prune_on_train_epoch_end=True|False)\n```\n\n----------------------------------------\n\nTITLE: Migrating PyTorch Lightning Profiler Import\nDESCRIPTION: Update imports from pl.profiler.* to pl.profilers\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_8_advanced.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Old import\nfrom pl.profiler import *\n\n# New import\nfrom pl.profilers import *\n```\n\n----------------------------------------\n\nTITLE: DataModule Class Introduction\nDESCRIPTION: Introduction of new LightningDataModule class for better data handling and organization in PyTorch Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_43\n\nLANGUAGE: text\nCODE:\n```\nAdded class `LightningDataModule`\n```\n\n----------------------------------------\n\nTITLE: Configuring TQDM Progress Bar Refresh Rate in PyTorch Lightning\nDESCRIPTION: Shows how to customize the refresh rate of the TQDMProgressBar by setting the rate at which the progress bar gets updated.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/progress_bar.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.callbacks import TQDMProgressBar\n\ntrainer = Trainer(callbacks=[TQDMProgressBar(refresh_rate=10)])\n```\n\n----------------------------------------\n\nTITLE: Displaying HTML Layout in RST Documentation\nDESCRIPTION: RST markup for creating a responsive card layout that displays training documentation sections using HTML div containers and custom directives.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/levels/advanced_level_21.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. displayitem::\n   :header: Scale with distributed strategies\n   :description: Learn about different distributed strategies to reach bigger model parameter sizes.\n   :col_css: col-md-6\n   :button_link: ../accelerators/gpu_intermediate.html\n   :height: 150\n   :tag: intermediate\n\n.. displayitem::\n   :header: Train models with billions of parameters\n   :description: Scale to billions of params on GPUs with FSDP, TP or Deepspeed.\n   :col_css: col-md-6\n   :button_link: ../advanced/model_parallel/index.html\n   :height: 150\n   :tag: advanced\n\n.. raw:: html\n\n        </div>\n    </div>\n```\n\n----------------------------------------\n\nTITLE: Callback Load Checkpoint Signature Update\nDESCRIPTION: Updated method signature for on_load_checkpoint callback to include trainer and pl_module parameters.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_4_regular.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Old way\ndef on_load_checkpoint(self, checkpoint):\n    pass\n\n# New way\ndef on_load_checkpoint(self, trainer, pl_module, checkpoint):\n    pass\n```\n\n----------------------------------------\n\nTITLE: Running a Fabric-Enabled Script\nDESCRIPTION: Command to run a Python script that has been converted to use Fabric. No special launcher is needed as Fabric handles the distribution.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/convert.rst#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\npython path/to/your/script.py\n```\n\n----------------------------------------\n\nTITLE: Migrating PyTorch Lightning CLI Import\nDESCRIPTION: Update imports from pl.utilities.cli to pl.cli\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_8_advanced.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Old import\nfrom pl.utilities.cli import *\n\n# New import\nfrom pl.cli import *\n```\n\n----------------------------------------\n\nTITLE: Executing LightningCLI Subcommands in Bash\nDESCRIPTION: Demonstrates how to use subcommands with LightningCLI in a bash terminal. It shows how to run the main script with different subcommands like 'fit' and 'test'.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_faq.rst#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npython main.py [subcommand]\n```\n\nLANGUAGE: bash\nCODE:\n```\npython main.py --help\n```\n\nLANGUAGE: bash\nCODE:\n```\npython main.py fit\npython main.py test\n```\n\n----------------------------------------\n\nTITLE: Migrating PyTorch Lightning GPU Accelerator\nDESCRIPTION: Update from GPUAccelerator to CUDAAccelerator\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_8_advanced.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Old usage\nfrom pl.accelerators import GPUAccelerator\n\n# New usage\nfrom pl.accelerators import CUDAAccelerator\n```\n\n----------------------------------------\n\nTITLE: Migrating PyTorch Lightning Base Loop Import\nDESCRIPTION: Update imports from pl.loops.base to pl.loops.loop\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_8_advanced.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Old import\nfrom pl.loops.base import *\n\n# New import\nfrom pl.loops.loop import *\n```\n\n----------------------------------------\n\nTITLE: SLURM Job Submission Command\nDESCRIPTION: Command to submit the training job to the SLURM scheduler.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/multi_node/slurm.rst#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nsbatch submit.sh\n```\n\n----------------------------------------\n\nTITLE: Installing MPI Support for PyTorch Lightning\nDESCRIPTION: Command for installing the mpi4py package, which is required for Lightning to work with MPI implementations like OpenMPI and MPICH.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/multi_node/other.rst#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install mpi4py\n```\n\n----------------------------------------\n\nTITLE: Using Python Dataclass with LightningDataModule\nDESCRIPTION: Example of using Python's dataclass decorator with a LightningDataModule for more structured configuration.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_31\n\nLANGUAGE: python\nCODE:\n```\nfrom dataclasses import dataclass\nfrom pytorch_lightning import LightningDataModule\nimport torch\n\n@dataclass\nclass MyDataModule(LightningDataModule):\n    data_dir: str = \"./data\"\n    batch_size: int = 32\n    num_workers: int = 4\n    train_transforms = None\n    val_transforms = None\n    test_transforms = None\n    \n    def setup(self, stage=None):\n        # Implement setup logic here\n        pass\n    \n    def train_dataloader(self):\n        return torch.utils.data.DataLoader(\n            self.train_dataset,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers\n        )\n    \n    def val_dataloader(self):\n        return torch.utils.data.DataLoader(\n            self.val_dataset, \n            batch_size=self.batch_size,\n            num_workers=self.num_workers\n        )\n    \n    def test_dataloader(self):\n        return torch.utils.data.DataLoader(\n            self.test_dataset,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers\n        )\n```\n\n----------------------------------------\n\nTITLE: Trainer Configuration Changes in PyTorch Lightning 1.6\nDESCRIPTION: Code examples showing the migration from deprecated Trainer flags to their new alternatives in PyTorch Lightning 1.6. Includes changes to callbacks, logging, checkpointing, and monitoring features.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_6_regular.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Old way:\nTrainer(terminate_on_nan=True)\n# New way:\nTrainer(detect_anomaly=True)\n\n# Old way:\nTrainer(weights_summary='top')\n# New way:\nTrainer(callbacks=[ModelSummary(max_depth=1)])\n\n# Old way:\nTrainer(checkpoint_callback=True)\n# New way:\nTrainer(enable_checkpointing=True)\n\n# Old way:\nTrainer(stochastic_weight_avg=True)\n# New way:\nTrainer(callbacks=[StochasticWeightAveraging()])\n\n# Old way:\nTrainer(resume_from_checkpoint='path/to/checkpoint.ckpt')\n# New way:\ntrainer.fit(ckpt_path='path/to/checkpoint.ckpt')\n\n# Old way:\nTrainer(progress_bar_refresh_rate=50)\n# New way:\nTrainer(callbacks=[ProgressBar(refresh_rate=50)])\n# Or disable:\nTrainer(enable_progress_bar=False)\n```\n\n----------------------------------------\n\nTITLE: Python CLI Implementation with Args Support\nDESCRIPTION: Example of implementing a CLI that can be run both from command line and programmatically through Python code.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_advanced_3.rst#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.cli import ArgsType, LightningCLI\n\n\ndef cli_main(args: ArgsType = None):\n    cli = LightningCLI(MyModel, ..., args=args)\n    ...\n\n\nif __name__ == \"__main__\":\n    cli_main()\n```\n\n----------------------------------------\n\nTITLE: Accessing Tensorboard Profile Interface\nDESCRIPTION: URL to access the Tensorboard profiling interface in a web browser.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/tuning/profiler_advanced.rst#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nhttp://localhost:9001/#profile\n```\n\n----------------------------------------\n\nTITLE: Importing Dependencies for PyTorch Lightning\nDESCRIPTION: Imports necessary modules from PyTorch and PyTorch Lightning for building a basic model.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/pytorch/bug_report/bug_report_model.ipynb#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nimport os\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom pytorch_lightning import LightningModule, Trainer\n```\n\n----------------------------------------\n\nTITLE: Custom Config Save Callback Implementation\nDESCRIPTION: Example of extending SaveConfigCallback to add logger functionality for saving configurations.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_advanced.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nclass LoggerSaveConfigCallback(SaveConfigCallback):\n    def save_config(self, trainer: Trainer, pl_module: LightningModule, stage: str) -> None:\n        if isinstance(trainer.logger, Logger):\n            config = self.parser.dump(self.config, skip_none=False)\n            trainer.logger.log_hyperparams({\"config\": config})\n\n\ncli = LightningCLI(..., save_config_callback=LoggerSaveConfigCallback)\n```\n\n----------------------------------------\n\nTITLE: Setting NCCL Debug Environment Variable for Cluster Debugging\nDESCRIPTION: This bash command sets the NCCL_DEBUG environment variable to INFO before running the training script. This helps in debugging NCCL issues that may arise during distributed training on a cluster.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/clouds/cluster_intermediate_1.rst#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nNCCL_DEBUG=INFO python train.py ...\n```\n\n----------------------------------------\n\nTITLE: Using state_key Property in a PyTorch Lightning Callback\nDESCRIPTION: Example of implementing the new state_key property in a custom callback class, which helps with state management in callbacks.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_32\n\nLANGUAGE: python\nCODE:\n```\nfrom pytorch_lightning.callbacks import Callback\n\nclass MyCustomCallback(Callback):\n    @property\n    def state_key(self) -> str:\n        # Return a unique key for this callback instance\n        # This helps with saving and loading state for multiple callbacks of the same type\n        return f\"my_custom_callback_{id(self)}\"\n    \n    def __init__(self):\n        self.counter = 0\n    \n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n        self.counter += 1\n    \n    def state_dict(self):\n        # Return the state to be saved\n        return {\"counter\": self.counter}\n    \n    def load_state_dict(self, state_dict):\n        # Load the state\n        self.counter = state_dict[\"counter\"]\n```\n\n----------------------------------------\n\nTITLE: Plotting Learning Rate Finder results on custom matplotlib axes\nDESCRIPTION: Added an axes argument 'ax' to the .lr_find().plot() method to enable writing to a user-defined axes in a matplotlib figure.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_21\n\nLANGUAGE: Python\nCODE:\n```\nimport matplotlib.pyplot as plt\nfrom pytorch_lightning import Trainer\n\nfig, ax = plt.subplots()\ntrainer = Trainer()\ntrainer.lr_find(model).plot(ax=ax)\n\n```\n\n----------------------------------------\n\nTITLE: Adding Device Statistics Monitoring\nDESCRIPTION: Implements device statistics monitoring to track GPU/TPU/HPU usage during training using the DeviceStatsMonitor callback.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/tuning/profiler_basic.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.callbacks import DeviceStatsMonitor\n\ntrainer = Trainer(callbacks=[DeviceStatsMonitor()])\n```\n\n----------------------------------------\n\nTITLE: Creating Display Cards for Advanced Skills in reStructuredText\nDESCRIPTION: This snippet creates a series of display cards for various advanced skills in PyTorch Lightning, including gradient accumulation, distributed communication, multiple model setups, model compilation, training large models, and distributed checkpointing.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/levels/advanced.rst#2025-04-23_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. displayitem::\n    :header: Use efficient gradient accumulation\n    :description: Learn how to perform efficient gradient accumulation in distributed settings\n    :button_link: ../advanced/gradient_accumulation.html\n    :col_css: col-md-4\n    :height: 170\n    :tag: advanced\n\n.. displayitem::\n    :header: Distribute communication\n    :description: Learn all about communication primitives for distributed operation. Gather, reduce, broadcast, etc.\n    :button_link: ../advanced/distributed_communication.html\n    :col_css: col-md-4\n    :height: 170\n    :tag: advanced\n\n.. displayitem::\n    :header: Use multiple models and optimizers\n    :description: See how flexible Fabric is to work with multiple models and optimizers!\n    :button_link: ../advanced/multiple_setup.html\n    :col_css: col-md-4\n    :height: 170\n    :tag: advanced\n\n.. displayitem::\n    :header: Speed up models by compiling them\n    :description: Use torch.compile to speed up models on modern hardware\n    :button_link: ../advanced/compile.html\n    :col_css: col-md-4\n    :height: 170\n    :tag: advanced\n\n.. displayitem::\n    :header: Train models with billions of parameters\n    :description: Train the largest models with FSDP/TP across multiple GPUs and machines\n    :button_link: ../advanced/model_parallel/index.html\n    :col_css: col-md-4\n    :height: 170\n    :tag: advanced\n\n.. displayitem::\n    :header: Save and load very large models\n    :description: Save and load very large models efficiently with distributed checkpoints\n    :button_link: ../guide/checkpoint/distributed_checkpoint.html\n    :col_css: col-md-4\n    :height: 170\n    :tag: advanced\n\n.. raw:: html\n\n        </div>\n    </div>\n```\n\n----------------------------------------\n\nTITLE: Scaling Data Processing with Lightning Map\nDESCRIPTION: Shows how to use Lightning's map function with multi-node processing. This snippet demonstrates configuring the number of nodes and machine type for distributed data preparation tasks on the Lightning.ai platform.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/data/README.md#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.data import map, Machine\n\nmap(\n  ...\n  num_nodes=32,\n  machine=Machine.DATA_PREP, # You can select between dozens of optimized machines\n)\n```\n\n----------------------------------------\n\nTITLE: Enabling NCCL Debug Output in Bash\nDESCRIPTION: This bash command shows how to enable NCCL debug output when launching a PyTorch Lightning script to troubleshoot NCCL-related issues.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/multi_node/barebones.rst#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\nNCCL_DEBUG=INFO fabric run ...\n```\n\n----------------------------------------\n\nTITLE: Including Full Module Documentation with automodule Directive\nDESCRIPTION: Final directive that includes the full module documentation using the automodule Sphinx directive. This will generate complete API documentation for the specified module.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/_templates/autosummary/module.rst#2025-04-23_snippet_4\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. automodule:: {{ fullname }}\n```\n\n----------------------------------------\n\nTITLE: Loading Profile Data in Python\nDESCRIPTION: Python command to load and display the profiler trace file using PyTorch's built-in profiler utilities.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/tuning/profiler_intermediate.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\npython -c 'import torch; print(torch.autograd.profiler.load_nvprof(\"trace_name.prof\"))'\n```\n\n----------------------------------------\n\nTITLE: Troubleshooting LightningCLI in Bash\nDESCRIPTION: Shows how to enable debug mode for troubleshooting LightningCLI by setting the JSONARGPARSE_DEBUG environment variable before running the CLI.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_faq.rst#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport JSONARGPARSE_DEBUG=true\npython main.py fit\n```\n\n----------------------------------------\n\nTITLE: Specifying torchao Package Dependency for PyTorch Lightning\nDESCRIPTION: This line specifies that the PyTorch Lightning project requires torchao package version 0.7.0 or higher. The specification uses the standard pip/requirements.txt format with the >= operator to indicate minimum version requirement.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/pytorch/fp8_distributed_transformer/requirements.txt#2025-04-23_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ntorchao>=0.7.0\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch Lightning via Conda\nDESCRIPTION: Installation command for PyTorch Lightning using Conda package manager from conda-forge channel.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/pytorch_lightning/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nconda install pytorch-lightning -c conda-forge\n```\n\n----------------------------------------\n\nTITLE: Comparing Implementation Differences\nDESCRIPTION: Command to compare the differences between PyTorch and Lightning Fabric implementations using sdiff\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/fabric/dcgan/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\nsdiff train_torch.py train_fabric.py\n```\n\n----------------------------------------\n\nTITLE: Listing Available Filesystem Implementations\nDESCRIPTION: Code to display all available filesystem implementations supported by fsspec.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/remote_fs.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nfrom fsspec.registry import known_implementations\n\nprint(known_implementations)\n```\n\n----------------------------------------\n\nTITLE: Defining Version Requirements for PyTorch Lightning Dependencies\nDESCRIPTION: Specifies version requirements for core dependencies with both minimum and maximum version constraints. Maximum bounds are included for CI stability but are dropped during actual installation unless marked as 'strict'.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/requirements/fabric/examples.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntorchvision >=0.16.0, <0.21.0\ntorchmetrics >=0.10.0, <1.8.0\nlightning-utilities >=0.8.0, <0.15.0\n```\n\n----------------------------------------\n\nTITLE: Installing Lightning Data via pip\nDESCRIPTION: Command to install Lightning Data from the GitHub repository using pip package manager.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/data/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install --no-cache-dir git+https://github.com/Lightning-AI/lit-data.git@master\n```\n\n----------------------------------------\n\nTITLE: Configuring SLURM Auto-resubmission\nDESCRIPTION: Configuration for enabling automatic job resubmission when approaching wall time limits using SLURM environment plugin.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/clouds/cluster_advanced.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ntrainer = Trainer(plugins=[SLURMEnvironment(requeue_signal=signal.SIGHUP)])\n```\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.plugins.environments import SLURMEnvironment\n\ntrainer = Trainer(plugins=[SLURMEnvironment(auto_requeue=False)])\n```\n\n----------------------------------------\n\nTITLE: Including External Links in RST Documentation\nDESCRIPTION: This snippet includes an external file containing links to be used in the RST documentation.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/collectives.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: ../links.rst\n```\n\n----------------------------------------\n\nTITLE: Installing Lightning Nightly Build from Source\nDESCRIPTION: This snippet demonstrates how to install the latest nightly build of Lightning directly from the GitHub repository.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/starter/installation.rst#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npip install https://github.com/Lightning-AI/lightning/archive/refs/heads/master.zip -U\n```\n\n----------------------------------------\n\nTITLE: Upgrading all checkpoints in a folder\nDESCRIPTION: Added support to upgrade all checkpoints in a folder using the pl.utilities.upgrade_checkpoint script.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_20\n\nLANGUAGE: Python\nCODE:\n```\npython -m pytorch_lightning.utilities.upgrade_checkpoint --checkpoint-dir /path/to/checkpoints\n\n```\n\n----------------------------------------\n\nTITLE: Updating Argument Name in PyTorch Lightning Utility Function\nDESCRIPTION: Change the argument name 'model' to 'instance' when using the is_overridden function from pytorch_lightning.utilities.model_helper.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_5_advanced.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Old\nfrom pytorch_lightning.utilities.model_helper import is_overridden\nis_overridden(\"method_name\", model=...)\n\n# New\nfrom pytorch_lightning.utilities.model_helper import is_overridden\nis_overridden(\"method_name\", instance=...)\n```\n\n----------------------------------------\n\nTITLE: Disabling Foreach in Optimizer for FSDP\nDESCRIPTION: Demonstrates how to disable the foreach option in the optimizer to potentially reduce memory peaks in FSDP.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/fsdp.rst#2025-04-23_snippet_10\n\nLANGUAGE: python\nCODE:\n```\noptimizer = torch.optim.AdamW(model.parameters(), foreach=False)\n```\n\n----------------------------------------\n\nTITLE: Installing scikit-learn Dependency for K-Fold Cross Validation\nDESCRIPTION: Command to install the scikit-learn library, which is required for creating k-fold cross validation splits using the ModelSelection.KFold class.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/fabric/kfold_cv/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install scikit-learn\n```\n\n----------------------------------------\n\nTITLE: Updating Import Path for rank_zero_warn in PyTorch Lightning\nDESCRIPTION: Change the import path for rank_zero_warn from pl.utilities.distributed to pl.utilities.rank_zero.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_5_advanced.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Old\nfrom pytorch_lightning.utilities.distributed import rank_zero_warn\n\n# New\nfrom pytorch_lightning.utilities.rank_zero import rank_zero_warn\n```\n\n----------------------------------------\n\nTITLE: Specifying Documentation Dependencies for PyTorch Lightning\nDESCRIPTION: This requirements file specifies Python packages needed for PyTorch Lightning documentation. It first includes another requirements file from the parent directory and then lists tensorboard as a dependency.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/requirements/fabric/docs.txt#2025-04-23_snippet_0\n\nLANGUAGE: text\nCODE:\n```\n-r ../docs.txt\n\ntensorboard\n```\n\n----------------------------------------\n\nTITLE: Setting XLA Strategy Sync Module States in Python\nDESCRIPTION: Sets whether to broadcast parameters to all devices when using XLAStrategy. This controls if module states are synchronized across devices.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/fabric/CHANGELOG.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nXLAStrategy(sync_module_states=bool)\n```\n\n----------------------------------------\n\nTITLE: Building PyTorch Lightning Docker Image\nDESCRIPTION: Commands to clone the Lightning repository and build Docker images with default or specific arguments. This process can be time-consuming.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/dockers/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/Lightning-AI/lightning.git\n\n# build with the default arguments\ndocker image build -t pytorch-lightning:latest -f dockers/base-cuda/Dockerfile .\n\n# build with specific arguments\ndocker image build -t pytorch-lightning:base-cuda-py3.9-torch1.13-cuda11.7.1 -f dockers/base-cuda/Dockerfile --build-arg PYTHON_VERSION=3.9 --build-arg PYTORCH_VERSION=1.13 --build-arg CUDA_VERSION=11.7.1 .\n```\n\n----------------------------------------\n\nTITLE: Defining GPU CI Dependencies for PyTorch Lightning\nDESCRIPTION: This configuration specifies the GPU-specific package dependencies with version constraints for PyTorch Lightning CI testing. It includes DeepSpeed with version bounds to avoid bugs in certain releases, and bitsandbytes with specific version requirements. Both packages have platform exclusions for unsupported operating systems.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/requirements/fabric/strategies.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\n# NOTE: the upper bound for the package version is only set for CI stability, and it is dropped while installing this package\n#  in case you want to preserve/enforce restrictions on the latest compatible version, add \"strict\" as an in-line comment\n\n# note: is a bug around 0.10 with `MPS_Accelerator must implement all abstract methods`\n#  shall be resolved by https://github.com/microsoft/DeepSpeed/issues/4372\ndeepspeed >=0.8.2, <=0.9.3; platform_system != \"Windows\" and platform_system != \"Darwin\"  # strict\nbitsandbytes >=0.45.2,<0.45.3; platform_system != \"Darwin\"\n```\n\n----------------------------------------\n\nTITLE: Installing Weights and Biases Package\nDESCRIPTION: Command to install the Weights and Biases (wandb) Python package using pip.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/loggers/wandb.rst#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install wandb\n```\n\n----------------------------------------\n\nTITLE: Generating Sample Images for Cloud Storage\nDESCRIPTION: Script to generate random images with varying dimensions and save them as JPEG files for cloud storage upload.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/data/README.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nimport os\nfrom PIL import Image\nimport numpy as np\n\ndata_dir = \"my_images\"\nos.makedirs(data_dir, exist_ok=True)\n\nfor i in range(1000):\n    width = np.random.randint(224, 320) \n    height = np.random.randint(224, 320) \n    image_path = os.path.join(data_dir, f\"{i}.JPEG\")\n    Image.fromarray(\n        np.random.randint(0, 256, (width, height, 3), np.uint8)\n    ).save(image_path, format=\"JPEG\", quality=90)\n```\n\n----------------------------------------\n\nTITLE: Setting Up Jupyter Server with PyTorch Lightning Docker\nDESCRIPTION: Steps to build a PyTorch Lightning Docker image, start a Jupyter server, and connect to it from a local browser. Includes port mapping and token usage.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/dockers/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\ndocker image build -t pytorch-lightning:v1.6.5 -f dockers/nvidia/Dockerfile --build-arg LIGHTNING_VERSION=1.6.5 .\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker run --rm -it --gpus=all -p 8888:8888 pytorch-lightning:v1.6.5\n```\n\n----------------------------------------\n\nTITLE: Using Automatic MPI Detection with Lightning Fabric\nDESCRIPTION: Example of how Fabric automatically detects MPI environments and provides access to process information like world size and global rank without additional configuration.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/multi_node/other.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfabric = Fabric(...)  # automatically detects MPI\nprint(fabric.world_size)  # world size provided by MPI\nprint(fabric.global_rank)  # rank provided by MPI\n...\n```\n\n----------------------------------------\n\nTITLE: Launching TensorBoard in Jupyter Notebook for PyTorch Lightning Logs\nDESCRIPTION: Commands to launch TensorBoard dashboard in a Jupyter notebook environment for visualizing PyTorch Lightning logs.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/logging_basic.rst#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\n%reload_ext tensorboard\n%tensorboard --logdir=lightning_logs/\n```\n\n----------------------------------------\n\nTITLE: Sanity Checking Detection in Validation Step\nDESCRIPTION: Demonstrates how to detect if the trainer is performing sanity checking in the validation step, useful for conditionally executing logging or callback operations.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_45\n\nLANGUAGE: python\nCODE:\n```\ndef validation_step(self, batch, batch_idx):\n    ...\n    if not self.trainer.sanity_checking:\n        self.log(\"value\", value)\n```\n\n----------------------------------------\n\nTITLE: LightningOptimizer Refresh Implementation\nDESCRIPTION: Addition of refresh() method to update optimizer wrapper state when underlying optimizer changes.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_14\n\nLANGUAGE: python\nCODE:\n```\nLightningOptimizer.refresh()  # Updates __dict__ when wrapped optimizer state changes\n```\n\n----------------------------------------\n\nTITLE: Modifying Fabric Save and Load Methods in Python\nDESCRIPTION: Updates to the Fabric.save and Fabric.load method signatures, allowing for saving state with model and optimizer references and in-place loading.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/fabric/CHANGELOG.md#2025-04-23_snippet_6\n\nLANGUAGE: Python\nCODE:\n```\nFabric.save(state)  # state can contain model and optimizer references\nFabric.load(state)  # can load state in-place onto models and optimizers\n```\n\n----------------------------------------\n\nTITLE: Basic Rich Progress Bar Configuration in PyTorch Lightning\nDESCRIPTION: Shows how to configure and use the RichProgressBar as the progress bar in PyTorch Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/progress_bar.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.pytorch.callbacks import RichProgressBar\n\ntrainer = Trainer(callbacks=[RichProgressBar()])\n```\n\n----------------------------------------\n\nTITLE: Defining a Sample Function with Type Hints in Python\nDESCRIPTION: This snippet demonstrates how to define a function with type hints, docstrings, and examples using Google style formatting for Sphinx documentation.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/README.md#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nfrom typing import Optional\n\n\ndef my_func(param_a: int, param_b: Optional[float] = None) -> str:\n    \"\"\"Sample function.\n\n    Args:\n        param_a: first parameter\n        param_b: second parameter\n\n    Return:\n        sum of both numbers\n\n    Example::\n\n        >>> my_func(1, 2)\n        3\n\n    Note:\n        If you want to add something.\n    \"\"\"\n    p = param_b if param_b else 0\n    return str(param_a + p)\n```\n\n----------------------------------------\n\nTITLE: Demonstrating Dynamic Shape Compilation in PyTorch\nDESCRIPTION: This example shows the impact of dynamic shapes on model compilation and execution time. It compares the performance with and without automatic dynamic shape detection using a pre-trained Inception v3 model.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/compile.rst#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\nimport time\nimport torch\nimport torchvision.models as models\nimport lightning as L\n\nfabric = L.Fabric(accelerator=\"cuda\", devices=1)\n\nmodel = models.inception_v3()\n\n# dynamic=False is the default\ntorch._dynamo.config.automatic_dynamic_shapes = False\n\ncompiled_model = torch.compile(model)\ncompiled_model = fabric.setup(compiled_model)\n\ninput = torch.randn(16, 3, 512, 512, device=fabric.device)\nt0 = time.time()\ncompiled_model(input)\ntorch.cuda.synchronize()\nprint(f\"1st forward: {time.time() - t0:.2f} seconds.\")\n\ninput = torch.randn(8, 3, 512, 512, device=fabric.device)  # note the change in shape\nt0 = time.time()\ncompiled_model(input)\ntorch.cuda.synchronize()\nprint(f\"2nd forward: {time.time() - t0:.2f} seconds.\")\n```\n\n----------------------------------------\n\nTITLE: Installing Lightning Fabric with pip\nDESCRIPTION: Command to install Lightning Fabric using pip package manager.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/index.rst#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install lightning\n```\n\n----------------------------------------\n\nTITLE: Enabling Environment Variable Parsing in Lightning CLI\nDESCRIPTION: This Python code snippet shows how to enable parsing of environment variables for all CLI flags in the Lightning CLI. It uses the 'default_env' argument in parser_kwargs.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_advanced_2.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\ncli = LightningCLI(..., parser_kwargs={\"default_env\": True})\n```\n\n----------------------------------------\n\nTITLE: Launching PyTorch Lightning Script on First Node\nDESCRIPTION: This bash command launches the PyTorch Lightning script on the first node of a two-node cluster. It specifies node rank, main address, accelerator type, number of devices, and total number of nodes.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/multi_node/barebones.rst#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nfabric run \\\n    --node-rank=0  \\\n    --main-address=10.10.10.16 \\\n    --accelerator=cuda \\\n    --devices=8 \\\n    --num-nodes=2 \\\n    train.py\n```\n\n----------------------------------------\n\nTITLE: Visualizing Training Logs with TensorBoard\nDESCRIPTION: This command starts TensorBoard to visualize the training and test logs from the PPO experiments.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/fabric/reinforcement_learning/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\ntensorboard --logdir logs\n```\n\n----------------------------------------\n\nTITLE: Running PyTorch-Lightning Tests in Docker\nDESCRIPTION: This command demonstrates how to run PyTorch-Lightning tests using a Docker container with CUDA support.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/tests/README.md#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\npython -m pytest src/lightning/pytorch tests/tests_pytorch -v\n```\n\n----------------------------------------\n\nTITLE: Starting Tensorboard Server for Profiling\nDESCRIPTION: Command to launch Tensorboard server with specified log directory and port for viewing profiling data.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/tuning/profiler_advanced.rst#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntensorboard --logdir ./tensorboard --port 9001\n```\n\n----------------------------------------\n\nTITLE: RST Display Item Definition\nDESCRIPTION: RST directive blocks defining navigation items for different version upgrades with headers, descriptions, and styling parameters\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/migration_guide.rst#2025-04-23_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. displayitem::\n   :header: 2.0.x\n   :description: Upgrade from 2.0.x series to the 2.1.\n   :col_css: col-md-12\n   :button_link: from_2_0.html\n   :height: 100\n```\n\n----------------------------------------\n\nTITLE: Downloading Legacy Checkpoints for Backward Compatibility Testing\nDESCRIPTION: This command retrieves all saved version-checkpoints from public AWS storage for testing backward compatibility with older versions of PyTorch Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/tests/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nbash .actions/pull_legacy_checkpoints.sh\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch 2.3+ for Tensor Parallelism\nDESCRIPTION: Command to install PyTorch 2.3 or higher, which is required for using tensor parallelism features.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/pytorch/tensor_parallel/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install 'torch>=2.3'\n```\n\n----------------------------------------\n\nTITLE: Displaying Documentation Items in reStructuredText\nDESCRIPTION: This code snippet uses the 'displayitem' directive in reStructuredText to create a formatted list of documentation topics. Each item includes a header, description, CSS class, link, and height specification.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/glossary/index.rst#2025-04-23_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. displayitem::\n   :header: Style guide\n   :description: Best practices to improve readability and reproducibility\n   :col_css: col-md-12\n   :button_link: ../starter/style_guide.html\n   :height: 100\n\n.. displayitem::\n   :header: SWA\n   :description: Stochastic Weight Averaging (SWA) can make your models generalize better\n   :col_css: col-md-12\n   :button_link: ../advanced/training_tricks.html#stochastic-weight-averaging\n   :height: 100\n\n.. displayitem::\n   :header: SLURM\n   :description: Simple Linux Utility for Resource Management, or simply Slurm, is a free and open-source job scheduler for Linux clusters\n   :col_css: col-md-12\n   :button_link: ../clouds/cluster_advanced.html\n   :height: 100\n\n.. displayitem::\n   :header: Tensor Parallelism\n   :description: Parallelize the computation of model layers across multiple GPUs, reducing memory usage and communication overhead\n   :col_css: col-md-12\n   :button_link: ../advanced/tp.html\n   :height: 100\n\n.. displayitem::\n   :header: Transfer learning\n   :description: Using pre-trained models to improve learning\n   :col_css: col-md-12\n   :button_link: ../advanced/transfer_learning.html\n   :height: 100\n\n.. displayitem::\n   :header: Trainer\n   :description: The class that automates and customizes model training\n   :col_css: col-md-12\n   :button_link: ../common/trainer.html\n   :height: 100\n\n.. displayitem::\n   :header: Torch distributed\n   :description: Setup for running on distributed environments\n   :col_css: col-md-12\n   :button_link: ../clouds/cluster_intermediate_2.html\n   :height: 100\n\n.. displayitem::\n   :header: Warnings\n   :description: Disable false-positive warnings emitted by Lightning\n   :col_css: col-md-12\n   :button_link: ../advanced/warnings.html\n   :height: 100\n```\n\n----------------------------------------\n\nTITLE: Manual Detaching of Gradients in PyTorch Lightning\nDESCRIPTION: Explicitly call .detach() on values returned from training_step that have .grad defined, as automatic detaching is no longer performed.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_5_advanced.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\n# Old\ndef training_step(self, batch, batch_idx):\n    loss = self.calculate_loss(batch)\n    return loss  # Automatic detaching no longer happens\n\n# New\ndef training_step(self, batch, batch_idx):\n    loss = self.calculate_loss(batch)\n    return loss.detach()  # Manually detach the loss\n```\n\n----------------------------------------\n\nTITLE: Expected Terminal Output from Tensor Parallel Training\nDESCRIPTION: Sample output showing the initialization of distributed processes across 4 GPUs, model parameter count, training progress with iterations, checkpoint saving, and peak memory usage statistics.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/fabric/tensor_parallel/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nInitializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\nInitializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\nInitializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\nInitializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n----------------------------------------------------------------------------------------------------\ndistributed_backend=nccl\nAll distributed processes registered. Starting with 4 processes\n----------------------------------------------------------------------------------------------------\n\nNumber of model parameters: 6.7 B\nStarting training ...\nIteration 0 complete\nIteration 1 complete\nIteration 2 complete\nIteration 3 complete\nIteration 4 complete\nIteration 5 complete\nIteration 6 complete\nIteration 7 complete\nSaving a (distributed) checkpoint ...\nTraining successfully completed!\nPeak memory usage: 17.95 GB\n```\n\n----------------------------------------\n\nTITLE: Migrating CheckpointConnector Method Usage in PyTorch Lightning\nDESCRIPTION: Replace calls to CheckpointConnector.hpc_load() with CheckpointConnector.restore(). Referenced in PR7652.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_5_devel.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nCheckpointConnector.hpc_load()\n```\n\nLANGUAGE: python\nCODE:\n```\nCheckpointConnector.restore()\n```\n\n----------------------------------------\n\nTITLE: Configuring Quantization-Aware Training in PyTorch Lightning\nDESCRIPTION: Added argument to QuantizationAwareTraining to control when quantization is applied.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_40\n\nLANGUAGE: python\nCODE:\n```\nQuantizationAwareTraining(quantize_on_fit_end=bool)\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch Lightning CLI Dependencies\nDESCRIPTION: Commands to install the necessary dependencies for using LightningCLI, either by installing all extras or just the required jsonargparse package.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_intermediate.rst#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install \"lightning[pytorch-extra]\"\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install \"jsonargparse[signatures]\"\n```\n\n----------------------------------------\n\nTITLE: Migrating PyTorch Lightning GPU Memory Usage\nDESCRIPTION: Update from get_gpu_memory_map to get_nvidia_gpu_stats\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_8_advanced.rst#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Old usage\nfrom pl.utilities.memory import get_gpu_memory_map\n\n# New usage\nfrom pl.accelerators.cuda import get_nvidia_gpu_stats\n```\n\n----------------------------------------\n\nTITLE: Generating Functions Section in Sphinx Documentation with Jinja2\nDESCRIPTION: Template block that conditionally creates a 'Functions' section with an autosummary listing when functions are present in the module. It iterates through each function and lists them without signatures.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/_templates/autosummary/module.rst#2025-04-23_snippet_1\n\nLANGUAGE: jinja2\nCODE:\n```\n{% block functions %}\n{% if functions %}\n.. rubric:: Functions\n\n.. autosummary::\n    :nosignatures:\n{% for item in functions %}\n    {{ item }}\n{%- endfor %}\n{% endif %}\n{% endblock %}\n```\n\n----------------------------------------\n\nTITLE: Running and Cleaning Docker Images\nDESCRIPTION: Commands to list Docker images, run a PyTorch Lightning Docker container interactively, and remove the image when no longer needed.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/dockers/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ndocker image list\ndocker run --rm -it pytorch-lightning:latest bash\n```\n\nLANGUAGE: bash\nCODE:\n```\ndocker image list\ndocker image rm pytorch-lightning:latest\n```\n\n----------------------------------------\n\nTITLE: Installing Comet.ml for PyTorch Lightning\nDESCRIPTION: Installs the Comet.ml package using pip for use with PyTorch Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/supported_exp_managers.rst#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install comet-ml\n```\n\n----------------------------------------\n\nTITLE: Generating HTML Display Cards for PyTorch Lightning Documentation\nDESCRIPTION: This HTML snippet creates a container with three display cards, each linking to a different section of the PyTorch Lightning documentation. The cards cover debugging models, finding training bottlenecks, and visualizing metrics.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/levels/basic_level_5.rst#2025-04-23_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"display-card-container\">\n    <div class=\"row\">\n\n.. displayitem::\n   :header: Debug your model\n   :description: Learn the basics of model debugging\n   :col_css: col-md-4\n   :button_link: ../debug/debugging_basic.html\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Find bottlenecks in training\n   :description: Learn to find bottlenecks in the training loop.\n   :col_css: col-md-4\n   :button_link: ../tuning/profiler_basic.html\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Visualize metrics, images, and text.\n   :description: Learn how to track and visualize metrics, images and text.\n   :col_css: col-md-4\n   :button_link: ../visualize/logging_basic.html\n   :height: 150\n   :tag: basic\n\n    </div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Generating Lightning Fabric Strategies Documentation with Sphinx\nDESCRIPTION: This RST code snippet sets up the documentation structure for Lightning Fabric strategies. It includes a table of contents and uses autosummary to generate documentation for various strategy classes.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/strategies.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: ../links.rst\n\n###########################\nlightning.fabric.strategies\n###########################\n\n\nStrategies\n^^^^^^^^^^\n\n.. currentmodule:: lightning.fabric.strategies\n\n.. autosummary::\n    :toctree: ./generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    Strategy\n    DDPStrategy\n    DataParallelStrategy\n    FSDPStrategy\n    DeepSpeedStrategy\n    XLAStrategy\n    XLAFSDPStrategy\n    ParallelStrategy\n    SingleDeviceStrategy\n    SingleDeviceXLAStrategy\n    ModelParallelStrategy\n```\n\n----------------------------------------\n\nTITLE: Generating Tutorial List in reStructuredText for PyTorch Lightning\nDESCRIPTION: This snippet uses a custom reStructuredText directive 'tutoriallist' to generate a list of PyTorch Lightning tutorials. The directive is likely defined elsewhere to populate the tutorial list dynamically.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/tutorials.rst#2025-04-23_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. tutoriallist::\n```\n\n----------------------------------------\n\nTITLE: Accessing Model Outputs in Epoch End Hooks\nDESCRIPTION: Change in how outputs are accessed in train epoch end hooks. Replace direct outputs access with either hook usage or module attribute access.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_4_regular.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Old way\ndef on_train_epoch_end(self, outputs):\n    # process outputs directly\n\n# New way\ndef on_train_epoch_end(self):\n    # access attributes or use hook without outputs parameter\n```\n\n----------------------------------------\n\nTITLE: Launching PyTorch Lightning Script on Second Node\nDESCRIPTION: This bash command launches the PyTorch Lightning script on the second node of a two-node cluster. It's identical to the first node command except for the node-rank parameter.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/multi_node/barebones.rst#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nfabric run \\\n    --node-rank=1  \\\n    --main-address=10.10.10.16 \\\n    --accelerator=cuda \\\n    --devices=8 \\\n    --num-nodes=2 \\\n    train.py\n```\n\n----------------------------------------\n\nTITLE: Deprecated PyTorch Lightning API Changes\nDESCRIPTION: List of deprecated APIs and their replacement paths. These features will be removed in future versions of PyTorch Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_26\n\nLANGUAGE: python\nCODE:\n```\n# Deprecated imports and their replacements\npl.accelerators.cuda.CUDAAccelerator  # instead of pl.accelerators.gpu.GPUAccelerator\npl.loggers.logger.Logger  # instead of pl.loggers.base.LightningLoggerBase\npl.callbacks.callback.Callback  # instead of pl.callbacks.base.Callback\npl.core.module.LightningModule  # instead of pl.core.lightning.LightningModule\npl.loops.loop.Loop  # instead of pl.loops.base.Loop\n\n# Deprecated Trainer arguments\nTrainer(accelerator='gpu', devices=2)  # instead of Trainer(gpus=2)\nTrainer(accelerator='tpu', devices=8)  # instead of Trainer(tpu_cores=8)\n\n# Deprecated method calls\nTrainer.reset_train_dataloader()  # instead of Trainer.reset_train_val_dataloaders()\nTrainer.reset_val_dataloader()  # instead of Trainer.reset_train_val_dataloaders()\n```\n\n----------------------------------------\n\nTITLE: Incorrect Manual Casting Example in PyTorch Lightning\nDESCRIPTION: This snippet demonstrates an incorrect way of manually casting tensors to different precisions within a LightningModule. It highlights the potential instability that can occur when mixing precisions improperly.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/precision_intermediate.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nclass LitModel(LightningModule):\n    def training_step(self, batch, batch_idx):\n        outs = self(batch)\n\n        a_float32 = torch.rand((8, 8), device=self.device, dtype=self.dtype)\n        b_float32 = torch.rand((8, 4), device=self.device, dtype=self.dtype)\n\n        # casting to float16 manually\n        with torch.autocast(device_type=self.device.type):\n            c_float16 = torch.mm(a_float32, b_float32)\n            target = self.layer(c_float16.flatten()[None])\n\n        # here outs is of type float32 and target is of type float16\n        loss = torch.mm(target @ outs).float()\n        return loss\n\n\ntrainer = Trainer(accelerator=\"gpu\", devices=1, precision=32)\n```\n\n----------------------------------------\n\nTITLE: Implementing Prediction Logic in Lightning\nDESCRIPTION: Shows how to implement the predict_step method in a LightningModule to define the prediction logic for a batch of data.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/starter/converting.rst#2025-04-23_snippet_5\n\nLANGUAGE: Python\nCODE:\n```\nclass LitModel(L.LightningModule):\n    def predict_step(self, batch, batch_idx):\n        x, y = batch\n        pred = self.encoder(x)\n        return pred\n```\n\n----------------------------------------\n\nTITLE: Logger Base Method Deprecation\nDESCRIPTION: Deprecation of close() method in favor of finalize() for LightningLoggerBase and LoggerCollection classes.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_6_devel.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nLightningLoggerBase.close() # deprecated\nLightningLoggerBase.finalize() # new method\n\nLoggerCollection.close() # deprecated\nLoggerCollection.finalize() # new method\n```\n\n----------------------------------------\n\nTITLE: Installing Intel Neural Compressor via pip\nDESCRIPTION: Commands to install the stable basic or full version of Intel Neural Compressor using pip.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/post_training_quantization.rst#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\n# Install stable basic version from pip\npip install neural-compressor\n# Or install stable full version from pip (including GUI)\npip install neural-compressor-full\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch for Tensor Parallel Training\nDESCRIPTION: Command to install PyTorch 2.3 or newer, which is required for using the tensor parallelism features demonstrated in this example.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/fabric/tensor_parallel/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install 'torch>=2.3'\n```\n\n----------------------------------------\n\nTITLE: Running Lightning CLI with Cloud-hosted YAML Config\nDESCRIPTION: This bash command shows how to run the Lightning CLI using a configuration file hosted on an S3 bucket. It demonstrates the capability to use remote filesystems for configuration.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_advanced_2.rst#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\n$ python main.py [subcommand] --config s3://bucket/config.yaml\n```\n\n----------------------------------------\n\nTITLE: Creating Section Headers in Sphinx Documentation with Jinja2 Template\nDESCRIPTION: Template code that generates a page header using the module name and creates a currentmodule directive to set the context for documentation generation. This establishes the title and module scope for the documentation page.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/_templates/autosummary/module.rst#2025-04-23_snippet_0\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ name | escape | underline }}\n\n.. currentmodule:: {{ fullname }}\n```\n\n----------------------------------------\n\nTITLE: Updating PyTorch Lightning Training Hooks\nDESCRIPTION: Changes to the format of outputs in training hooks, affecting the dimensionality of optimizer and truncated backpropagation steps lists.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_7_devel.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Old format\ndef on_train_batch_end(self, outputs, ...):  # outputs shape: (n_optimizers, tbptt_steps)\n    pass\n\n# New format\ndef on_train_batch_end(self, outputs, ..., new_format=True):  # outputs shape: (tbptt_steps, n_optimizers)\n    pass\n\n# Old format\ndef training_epoch_end(self, outputs):  # outputs shape: (n_optimizers, n_batches, tbptt_steps)\n    pass\n\n# New format\ndef training_epoch_end(self, outputs, new_format=True):  # outputs shape: (n_batches, tbptt_steps, n_optimizers)\n    pass\n```\n\n----------------------------------------\n\nTITLE: Importing rank_zero_only from PyTorch Lightning\nDESCRIPTION: Shows the deprecated and new way to import the rank_zero_only utility function in PyTorch Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_23\n\nLANGUAGE: Python\nCODE:\n```\n# Deprecated\nfrom pl.utilities.distributed import rank_zero_only\n\n# New way\nfrom pl.utilities import rank_zero_only\n```\n\n----------------------------------------\n\nTITLE: RST Display Items Configuration\nDESCRIPTION: RestructuredText directives for configuring display cards that showcase different advanced features and capabilities of PyTorch Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/levels/advanced.rst#2025-04-23_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. displayitem::\n   :header: Level 14: Customize configs to run in production\n   :description: Enable composable YAMLs\n   :col_css: col-md-6\n   :button_link: advanced_level_15.html\n   :height: 150\n   :tag: advanced\n```\n\n----------------------------------------\n\nTITLE: Setting Number of Samples in Python\nDESCRIPTION: This snippet sets the number of samples to be used in the dataset. It's a simple variable assignment that can be modified as needed for different dataset sizes.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/pytorch/bug_report/bug_report_model.ipynb#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nnum_samples = 10000\n```\n\n----------------------------------------\n\nTITLE: Structuring HTML Layout for PyTorch Lightning Documentation\nDESCRIPTION: This HTML snippet creates a card-based layout for displaying various sections of the PyTorch Lightning documentation. It uses custom 'displayitem' elements to represent different topics and their descriptions.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/model/build_model.rst#2025-04-23_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"display-card-container\">\n    <div class=\"row\">\n\n        </div>\n    </div>\n```\n\n----------------------------------------\n\nTITLE: Structuring Complex Data Documentation with HTML in PyTorch Lightning\nDESCRIPTION: This HTML snippet creates a structured layout for displaying documentation cards about complex data handling in PyTorch Lightning. It uses divs and custom display items to organize information on LightningDataModules, iterables, data access, and faster DataLoaders.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/data/data.rst#2025-04-23_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"display-card-container\">\n    <div class=\"row\">\n\n        </div>\n    </div>\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch Lightning for TPU Usage\nDESCRIPTION: Command to install PyTorch Lightning framework for TPU training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/accelerators/tpu_basic.rst#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n!pip install lightning\n```\n\n----------------------------------------\n\nTITLE: Inserting Raw HTML in reStructuredText\nDESCRIPTION: This snippet demonstrates the use of the 'raw' directive in reStructuredText to insert HTML content directly into the document. It's used here to close div tags, likely part of the page's layout structure.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/glossary/index.rst#2025-04-23_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. raw:: html\n\n        </div>\n    </div>\n```\n\n----------------------------------------\n\nTITLE: Running and Reporting Coverage for PyTorch-Lightning Tests\nDESCRIPTION: These commands show how to generate, print, and export coverage reports for PyTorch-Lightning tests.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/tests/README.md#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\n# generate coverage (coverage is also installed as part of dev dependencies)\ncoverage run --source src/lightning/pytorch -m pytest src/lightning/pytorch tests/tests_pytorch -v\n\n# print coverage stats\ncoverage report -m\n\n# exporting results\ncoverage xml\n```\n\n----------------------------------------\n\nTITLE: Updating NaN Tensor Detection\nDESCRIPTION: Migration from TrainerTrainingTricksMixin.detect_nan_tensors to pl.utilities.grads.grad_norm.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_4_advanced.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Old way\nTrainerTrainingTricksMixin.detect_nan_tensors\n\n# New way\npl.utilities.grads.grad_norm\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure with HTML Components\nDESCRIPTION: ReStructuredText documentation layout that combines RST directives with raw HTML for creating a card-based navigation interface. Uses displayitem directives to create three different debugging guide sections.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/debug/debugging.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _debugging:\n\n################\nDebug your model\n################\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. displayitem::\n   :header: Basic\n   :description: Learn the basics of model debugging.\n   :col_css: col-md-4\n   :button_link: debugging_basic.html\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Intermediate\n   :description: Learn to debug machine learning operations\n   :col_css: col-md-4\n   :button_link: debugging_intermediate.html\n   :height: 150\n   :tag: intermediate\n\n.. displayitem::\n   :header: Advanced\n   :description: Learn to debug distributed models\n   :col_css: col-md-4\n   :button_link: debugging_advanced.html\n   :height: 150\n   :tag: advanced\n\n.. raw:: html\n\n        </div>\n   </div>\n```\n\n----------------------------------------\n\nTITLE: ModelCheckpoint Callback Period Parameter Update\nDESCRIPTION: Migration from period to every_n_epochs parameter in ModelCheckpoint callback configuration.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_4_regular.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Old way\nModelCheckpoint(period=2)\n\n# New way\nModelCheckpoint(every_n_epochs=2)\n```\n\n----------------------------------------\n\nTITLE: Defining RST Reference Links for Lightning Pull Requests\nDESCRIPTION: A series of reStructuredText reference link definitions that map shorthand labels to full GitHub pull request URLs for the Lightning-AI/lightning repository.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_9_devel.rst#2025-04-23_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. _pr16437: https://github.com/Lightning-AI/lightning/pull/16437\n.. _pr16708: https://github.com/Lightning-AI/lightning/pull/16708\n.. _pr15364: https://github.com/Lightning-AI/lightning/pull/15364\n.. _pr16204: https://github.com/Lightning-AI/lightning/pull/16204\n.. _pr16999: https://github.com/Lightning-AI/lightning/pull/16999\n.. _pr16436: https://github.com/Lightning-AI/lightning/pull/16436\n.. _pr16516: https://github.com/Lightning-AI/lightning/pull/16516\n.. _pr16533: https://github.com/Lightning-AI/lightning/pull/16533\n.. _pr16826: https://github.com/Lightning-AI/lightning/pull/16826\n.. _pr16726: https://github.com/Lightning-AI/lightning/pull/16726\n.. _pr16203: https://github.com/Lightning-AI/lightning/pull/16203\n.. _pr16462: https://github.com/Lightning-AI/lightning/pull/16462\n.. _pr16714: https://github.com/Lightning-AI/lightning/pull/16714\n.. _pr17058: https://github.com/Lightning-AI/lightning/pull/17058\n.. _pr16760: https://github.com/Lightning-AI/lightning/pull/16760\n.. _pr16759: https://github.com/Lightning-AI/lightning/pull/16759\n.. _pr16618: https://github.com/Lightning-AI/lightning/pull/16618\n```\n\n----------------------------------------\n\nTITLE: Updating Validation Check in PyTorch Lightning Trainer\nDESCRIPTION: Replace the use of Trainer.disable_validation with a check for not Trainer.enable_validation to determine if validation is disabled.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_5_advanced.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Old\nif trainer.disable_validation:\n    # Skip validation\n\n# New\nif not trainer.enable_validation:\n    # Skip validation\n```\n\n----------------------------------------\n\nTITLE: Creating Module Documentation Template with Jinja2 for Sphinx Autosummary\nDESCRIPTION: This Jinja2 template structures API documentation for PyTorch Lightning modules. It creates separate sections for functions, classes, and exceptions using the Sphinx autosummary extension, which generates tables of contents with links to detailed documentation for each item. The template maintains consistent formatting across all documentation pages.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/_templates/autosummary/module.rst#2025-04-23_snippet_0\n\nLANGUAGE: jinja2\nCODE:\n```\n{{ name | escape | underline }}\n\n.. currentmodule:: {{ fullname }}\n\n{% block functions %}\n{% if functions %}\n.. rubric:: Functions\n\n.. autosummary::\n    :nosignatures:\n{% for item in functions %}\n    {{ item }}\n{%- endfor %}\n{% endif %}\n{% endblock %}\n\n{% block classes %}\n{% if classes %}\n.. rubric:: Classes\n\n.. autosummary::\n    :nosignatures:\n{% for item in classes %}\n    {{ item }}\n{%- endfor %}\n{% endif %}\n{% endblock %}\n\n{% block exceptions %}\n{% if exceptions %}\n.. rubric:: Exceptions\n\n.. autosummary::\n    :nosignatures:\n{% for item in exceptions %}\n    {{ item }}\n{%- endfor %}\n{% endif %}\n{% endblock %}\n\n.. automodule:: {{ fullname }}\n\n```\n\n----------------------------------------\n\nTITLE: Migrating PyTorch Lightning Base Callback Import\nDESCRIPTION: Update imports from pl.callbacks.base to pl.callbacks.callback\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_8_advanced.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n# Old import\nfrom pl.callbacks.base import *\n\n# New import\nfrom pl.callbacks.callback import *\n```\n\n----------------------------------------\n\nTITLE: Replacing DDPPlugin.task_idx with DDPStrategy.local_rank in PyTorch Lightning\nDESCRIPTION: Use DDPStrategy.local_rank instead of the deprecated DDPPlugin.task_idx for accessing the local rank in distributed training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_5_advanced.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\n# Old\nfrom pytorch_lightning.plugins import DDPPlugin\nplugin = DDPPlugin()\ntask_idx = plugin.task_idx\n\n# New\nfrom pytorch_lightning.strategies import DDPStrategy\nstrategy = DDPStrategy()\nlocal_rank = strategy.local_rank\n```\n\n----------------------------------------\n\nTITLE: Configuring RST Documentation for Lightning Fabric\nDESCRIPTION: Sets up RST documentation structure with module imports, section headers, and autosummary directives for generating Fabric class documentation.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: ../links.rst\n\n#######################\nlightning.fabric.Fabric\n#######################\n\n\nFabric\n^^^^^^\n\n.. currentmodule:: lightning.fabric.fabric\n\n.. autosummary::\n    :toctree: ./generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    Fabric\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch Lightning with Extras Example\nDESCRIPTION: Shows the command for installing PyTorch Lightning with a specific extra dependency set, which corresponds to a requirements file in the project structure.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/requirements/README.md#2025-04-23_snippet_0\n\nLANGUAGE: markdown\nCODE:\n```\npip install pytorch-lightning[extra]\n```\n\n----------------------------------------\n\nTITLE: Closing HTML Structure for Display Cards in PyTorch Lightning Documentation\nDESCRIPTION: This HTML snippet closes the container and row divs opened in the previous snippet, completing the structure for display cards in the PyTorch Lightning documentation.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/model/own_your_loop.rst#2025-04-23_snippet_1\n\nLANGUAGE: html\nCODE:\n```\n    </div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: RST Table Definition for PyTorch Lightning Version History\nDESCRIPTION: A reStructuredText table definition showing the version history of PyTorch Lightning, including version numbers, titles, bug-fix versions, and upgrade guide links.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/past_versions.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. list-table:: Past versions\n   :widths: 5 50 30 15\n   :header-rows: 1\n\n   * - Version\n     - Title\n     - Bug-fix versions\n     - Upgrade guide\n```\n\n----------------------------------------\n\nTITLE: Configuring Fast Dev Run in PyTorch Lightning\nDESCRIPTION: Shows how to use the fast_dev_run argument in PyTorch Lightning Trainer to quickly test the model with a small number of batches.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/debug/debugging_basic.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntrainer = Trainer(fast_dev_run=True)\n```\n\nLANGUAGE: python\nCODE:\n```\ntrainer = Trainer(fast_dev_run=7)\n```\n\n----------------------------------------\n\nTITLE: Migrating PyTorch Lightning DeepSpeed Configuration\nDESCRIPTION: Update from LightningDeepSpeedModule to using strategy parameter\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_8_advanced.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Old usage\nfrom lightning_deepspeed import LightningDeepSpeedModule\n\n# New usage\ntrainer = Trainer(strategy=\"deepspeed\")\n# or\ntrainer = Trainer(strategy=DeepSpeedStrategy(...))\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch Lightning\nDESCRIPTION: Installs the latest version of PyTorch Lightning using pip in a Jupyter notebook cell.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/pytorch/bug_report/bug_report_model.ipynb#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n! pip install -qU pytorch-lightning\n```\n\n----------------------------------------\n\nTITLE: Configuring Inference Mode in PyTorch Lightning\nDESCRIPTION: Demonstrates how to configure whether to use torch.inference_mode or torch.no_grad during evaluation, and how to enable gradients selectively when needed.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_34\n\nLANGUAGE: python\nCODE:\n```\n# default used by the Trainer\ntrainer = Trainer(inference_mode=True)\n\n# Use `torch.no_grad` instead\ntrainer = Trainer(inference_mode=False)\n```\n\nLANGUAGE: python\nCODE:\n```\nclass LitModel(LightningModule):\n    def validation_step(self, batch, batch_idx):\n        preds = self.layer1(batch)\n        with torch.enable_grad():\n            grad_preds = preds.requires_grad_()\n            preds2 = self.layer2(grad_preds)\n\n\nmodel = LitModel()\ntrainer = Trainer(inference_mode=False)\ntrainer.validate(model)\n```\n\n----------------------------------------\n\nTITLE: Defining RST Display Grid Layout\nDESCRIPTION: RST markup defining a grid layout for displaying common workflow documentation items with HTML integration.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common_usecases.rst#2025-04-23_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. displayitem::\n   :header: Avoid overfitting\n   :description: Add a training and test loop.\n   :col_css: col-md-12\n   :button_link: common/evaluation.html\n   :height: 100\n\n[...additional display items...]\n\n.. raw:: html\n\n        </div>\n    </div>\n```\n\n----------------------------------------\n\nTITLE: Suppressing Individual Warning Messages in Python\nDESCRIPTION: Shows how to suppress a specific warning message using the warnings module, specifically for num_workers related warnings.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/warnings.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", \".*Consider increasing the value of the `num_workers` argument*\")\n```\n\n----------------------------------------\n\nTITLE: HTML Container Structure for Learning Levels\nDESCRIPTION: HTML div structure used to create card containers for displaying learning levels and their descriptions.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/expertise_levels.rst#2025-04-23_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"display-card-container\">\n    <div class=\"row\">\n    </div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Running a Single PyTorch-Lightning Test\nDESCRIPTION: This command shows how to execute a specific test case within the PyTorch-Lightning test suite.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/tests/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npython -m pytest -v tests/tests_pytorch/trainer/test_trainer_cli.py::test_default_args\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Table of Contents Definition\nDESCRIPTION: Defines the hierarchical documentation structure using toctree directive with maxdepth of 1 and hidden attribute\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/index.rst#2025-04-23_snippet_0\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. toctree::\n   :maxdepth: 1\n   :hidden:\n\n   evaluation\n   ../model/build_model\n   ../cli/lightning_cli\n   progress_bar\n   ../deploy/production\n   ../advanced/training_tricks\n   ../tuning/profiler\n   Manage experiments <../visualize/logging_intermediate>\n   Organize existing PyTorch into Lightning <../starter/converting>\n   ../clouds/cluster\n   Save and load model progress <checkpointing>\n   Save memory with half-precision <precision>\n   ../advanced/model_parallel\n   Train on single or multiple GPUs <../accelerators/gpu>\n   Train on single or multiple HPUs <../integrations/hpu/index>\n   Train on single or multiple TPUs <../accelerators/tpu>\n   Train on MPS <../accelerators/mps>\n   Use a pretrained model <../advanced/pretrained>\n   ../data/data\n   ../model/own_your_loop\n   ../advanced/model_init\n   ../common/tbptt\n```\n\n----------------------------------------\n\nTITLE: Importing PyTorch Lightning Module Changes\nDESCRIPTION: Code migration examples showing changes in import paths and module usage for PyTorch Lightning v1.9. These changes affect distributed module wrappers, utility functions, and core functionality.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_9_devel.rst#2025-04-23_snippet_0\n\nLANGUAGE: Python\nCODE:\n```\n# Old imports (deprecated)\nfrom pl.utilities.device_parser import *\nfrom pl.utilities.cloud_io import *\nfrom pl.utilities.apply_func import *\n\n# New imports\nfrom lightning_fabric.utilities.device_parser import *\nfrom lightning_fabric.utilities.cloud_io import *\nfrom lightning_utilities.core.apply_func import *\n```\n\n----------------------------------------\n\nTITLE: Registering Custom Accelerator for CLI Usage\nDESCRIPTION: Implementation of the register_accelerators class method to enable CLI-based accelerator selection.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/extensions/accelerator.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass XPUAccelerator(Accelerator):\n    ...\n\n    @classmethod\n    def register_accelerators(cls, accelerator_registry):\n        accelerator_registry.register(\n            \"xpu\",\n            cls,\n            description=f\"XPU Accelerator - optimized for large-scale machine learning.\",\n        )\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Documentation Navigation Structure\nDESCRIPTION: ReStructuredText markup defining the documentation structure with navigation cards for different cluster deployment options.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/levels/intermediate_level_14.rst#2025-04-23_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n:orphan:\n\n#####################################\nLevel 13: Run on a multi-node cluster\n#####################################\n\nIn this level you'll learn to run on cloud or on-prem clusters.\n```\n\n----------------------------------------\n\nTITLE: RST Directive Structure for Lightning Documentation\nDESCRIPTION: RST markup code defining the documentation structure for SOTA scaling techniques, including half-precision training and advanced scaling methods directives.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/levels/intermediate_level_11.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n:orphan:\n\n#########################################\nLevel 10: Explore SOTA scaling techniques\n#########################################\n\nIn this level you'll explore SOTA techniques to help convergence, stability and scalability.\n\n----\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. displayitem::\n   :header: 1: Half precision training\n   :description: Enable your models to train faster and save memory with different floating-point precision settings.\n   :col_css: col-md-6\n   :button_link: ../common/precision_basic.html\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: 2: SOTA scaling techniques\n   :description: Enable techniques to help scaling and convergence.\n   :col_css: col-md-6\n   :button_link: ../advanced/training_tricks.html\n   :height: 150\n   :tag: basic\n\n.. raw:: html\n\n        </div>\n    </div>\n```\n\n----------------------------------------\n\nTITLE: Structuring HTML Content for PyTorch Lightning Documentation\nDESCRIPTION: This HTML snippet creates a structured layout for displaying card-like items with links to different sections of the PyTorch Lightning documentation related to model prediction and deployment.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/levels/core_level_6.rst#2025-04-23_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"display-card-container\">\n    <div class=\"row\">\n\n.. displayitem::\n   :header: Load model weights\n   :description: Learn to load the weights (checkpoint) of a model.\n   :col_css: col-md-4\n   :button_link: ../common/checkpointing_basic.html#lightningmodule-from-checkpoint\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Predict with LightningModule\n   :description: Learn the basics of predicting with Lightning.\n   :col_css: col-md-4\n   :button_link: ../deploy/production_basic.html\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Predict with pure PyTorch\n   :description: Learn to use pure PyTorch without the Lightning dependencies for prediction.\n   :col_css: col-md-4\n   :button_link: ../deploy/production_intermediate.html\n   :height: 150\n   :tag: intermediate\n\n    </div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Defining RST Documentation Structure for PyTorch Lightning Strategy Integrations\nDESCRIPTION: This RST code snippet defines the structure of the documentation page for additional external strategy integrations in PyTorch Lightning. It includes a header, HTML for layout, and a displayitem directive for Hivemind integration.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/integrations/strategies/index.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _strategy-integrations:\n\nAdditional external Strategy integrations\n=========================================\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. Add callout items below this line\n\n.. displayitem::\n   :header: Hivemind\n   :description: Collaborative Training tries to solve the need for top-tier multi-GPU servers by allowing you to train across unreliable machines.\n   :col_css: col-md-4\n   :button_link: Hivemind.html\n   :height: 150\n   :tag: hivemind\n\n\n.. raw:: html\n\n        </div>\n    </div>\n```\n\n----------------------------------------\n\nTITLE: Avoiding CUDA Initialization Before Fabric Launch\nDESCRIPTION: Demonstrates the proper way to handle CUDA operations in multi-GPU notebook setups. This pattern avoids potential hangs or crashes by moving all CUDA-related code inside the training function rather than executing it before the launch call.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/notebooks.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# BAD: Don't run CUDA-related code before `.launch()`\n# x = torch.tensor(1).cuda()\n# torch.cuda.empty_cache()\n# torch.cuda.is_available()\n\n\ndef train(fabric):\n    # GOOD: Move CUDA calls into the training function\n    x = torch.tensor(1).cuda()\n    torch.cuda.empty_cache()\n    torch.cuda.is_available()\n    ...\n\n\nfabric = Fabric(accelerator=\"cuda\", devices=2)\nfabric.launch(train)\n```\n\n----------------------------------------\n\nTITLE: Defining Package Dependencies with Version Constraints\nDESCRIPTION: Specifies core dependencies for PyTorch Lightning with version constraints. Upper bounds are mainly for CI stability and are typically dropped during installation unless marked as 'strict'.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/requirements/pytorch/examples.txt#2025-04-23_snippet_0\n\nLANGUAGE: pip\nCODE:\n```\nrequests <2.32.0\ntorchvision >=0.16.0, <0.21.0\nipython[all] <8.19.0\ntorchmetrics >=0.10.0, <1.8.0\nlightning-utilities >=0.8.0, <0.15.0\n```\n\n----------------------------------------\n\nTITLE: Requirements Dependencies for PyTorch Lightning Project\nDESCRIPTION: Lists required Python packages and their version specifications for a PyTorch Lightning project. Includes Gymnasium with Box2D support, MoviePy for video processing, Lightning framework, TorchMetrics for metrics calculation, and TensorBoard for visualization.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/fabric/reinforcement_learning/requirements.txt#2025-04-23_snippet_0\n\nLANGUAGE: requirements\nCODE:\n```\ngymnasium[box2d]>=0.27.1\nmoviepy\nlightning>=1.9.0\ntorchmetrics\ntensorboard\n```\n\n----------------------------------------\n\nTITLE: Defining Package Version Constraints for PyTorch Lightning\nDESCRIPTION: A list of Python package dependencies with version constraints to ensure compatibility. It includes development dependencies like setuptools, wheel, awscli, twine, and others with specific version ranges or exact versions to maintain build stability.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/requirements/ci.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nsetuptools <70.1.1\nwheel <0.44.0\nawscli >=1.30.0, <1.31.0\ntwine ==6.0.1\nimportlib-metadata <8.0.0\nwget\npkginfo ==1.12.0\npackaging <25.1\n```\n\n----------------------------------------\n\nTITLE: Optimizing XLA Configuration Function\nDESCRIPTION: Improvement to prevent premature PJRT computation client initialization in the main process when using XLA.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\n# Original behavior was setting global rank before process launch\n# Now delays setting global rank until after process launch for XLA\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for Precision Settings\nDESCRIPTION: Sphinx/RST markup for creating a documentation layout with three display cards covering different levels of precision functionality in PyTorch Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/precision.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n:orphan:\n\n.. _precision:\n\n###############\nN-Bit Precision\n###############\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. displayitem::\n   :header: Basic\n   :description: Enable your models to train faster and save memory with different floating-point precision settings.\n   :col_css: col-md-4\n   :button_link: precision_basic.html\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Intermediate\n   :description: Enable state-of-the-art scaling with advanced mixed-precision settings.\n   :col_css: col-md-4\n   :button_link: precision_intermediate.html\n   :height: 150\n   :tag: intermediate\n\n.. displayitem::\n   :header: Expert\n   :description: Create new precision techniques and enable them through Lightning.\n   :col_css: col-md-4\n   :button_link: precision_expert.html\n   :height: 150\n   :tag: expert\n\n.. raw:: html\n\n        </div>\n    </div>\n```\n\n----------------------------------------\n\nTITLE: RST Structure for Training Visualization Documentation\nDESCRIPTION: ReStructuredText markup for creating a documentation page layout with two display cards for basic and intermediate visualization options.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/levels/core_level_3.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n:orphan:\n\n####################################\nLevel 3: Visualize training progress\n####################################\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. displayitem::\n   :header: Visualize metrics, images, and text.\n   :description: Learn how to track and visualize metrics, images and text.\n   :col_css: col-md-6\n   :button_link: ../visualize/logging_basic.html\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Use third-party experiment managers\n   :description: Enable third-party experiment managers with advanced visualizations.\n   :col_css: col-md-6\n   :button_link: ../visualize/logging_intermediate.html\n   :height: 150\n   :tag: intermediate\n\n.. raw:: html\n\n        </div>\n    </div>\n```\n\n----------------------------------------\n\nTITLE: Specifying Core Dependencies for PyTorch Lightning\nDESCRIPTION: Defines the essential package dependencies with version constraints needed for PyTorch Lightning to function. The upper bounds on package versions are primarily for CI stability and may be dropped during installation unless specifically marked as 'strict'.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/requirements/fabric/base.txt#2025-04-23_snippet_0\n\nLANGUAGE: requirements\nCODE:\n```\ntorch >=2.1.0, <2.6.0\nfsspec[http] >=2022.5.0, <2024.4.0\npackaging >=20.0, <=25.0\ntyping-extensions >=4.4.0, <4.11.0\nlightning-utilities >=0.10.0, <0.15.0\n```\n\n----------------------------------------\n\nTITLE: RST Display Card Container Structure\nDESCRIPTION: RST markup for creating a display card container layout in the documentation using raw HTML and displayitem directives.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/index.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. displayitem::\n    :header: Convert to Fabric in 5 minutes\n    :description: Learn how to add Fabric to your PyTorch code\n    :button_link: ../fundamentals/convert.html\n    :col_css: col-md-4\n    :height: 150\n    :tag: basic\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure\nDESCRIPTION: ReStructuredText document structure showing the organization of upgrade guides with includes for different user types across versions 1.7 through 1.9.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/from_1_7.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n:orphan:\n\nUpgrade from 1.7 to the 2.0\n###########################\n\nRegular User\n************\n\n.. include:: sections/1_7_regular.rst\n.. include:: sections/1_8_regular.rst\n.. include:: sections/1_9_regular.rst\n\nAdvanced User\n*************\n\n.. include:: sections/1_7_advanced.rst\n.. include:: sections/1_8_advanced.rst\n.. include:: sections/1_9_advanced.rst\n\nDeveloper\n*********\n\n.. include:: sections/1_7_devel.rst\n.. include:: sections/1_8_devel.rst\n.. include:: sections/1_9_devel.rst\n```\n\n----------------------------------------\n\nTITLE: Importing Lightning Fabric Accelerator Classes\nDESCRIPTION: This RST code snippet imports external links and defines the documentation structure for the accelerator classes in the PyTorch Lightning Fabric module.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/accelerators.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: ../links.rst\n\n#############################\nlightning.fabric.accelerators\n#############################\n\n\nAccelerators\n^^^^^^^^^^^^\n\n.. currentmodule:: lightning.fabric.accelerators\n\n.. autosummary::\n    :toctree: ./generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    Accelerator\n    CPUAccelerator\n    CUDAAccelerator\n    MPSAccelerator\n    XLAAccelerator\n```\n\n----------------------------------------\n\nTITLE: Defining Hidden Role in reStructuredText for PyTorch Lightning Docs\nDESCRIPTION: Defines a custom role named 'hidden' with a CSS class 'hidden-section'. This is used to mark sections of documentation that should be visually hidden.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/_templates/classtemplate.rst#2025-04-23_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. role:: hidden\n    :class: hidden-section\n```\n\n----------------------------------------\n\nTITLE: Structuring Expert Skills Documentation in reStructuredText\nDESCRIPTION: This RST code snippet defines the structure for documenting expert-level skills in Lightning. It uses raw HTML for layout and custom directives for displaying items with headers, descriptions, and links.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/levels/expert.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. Add callout items below this line\n\n.. displayitem::\n   :header: Level 21: Extend the Lightning CLI\n   :description: Extend the functionality of the Lightning CLI.\n   :col_css: col-md-6\n   :button_link: expert_level_22.html\n   :height: 150\n   :tag: expert\n\n.. displayitem::\n   :header: Level 22: Integrate a custom cluster\n   :description: Integrate a custom cluster into Lightning.\n   :col_css: col-md-6\n   :button_link: expert_level_23.html\n   :height: 150\n   :tag: expert\n\n.. displayitem::\n   :header: Level 23: Make your own profiler\n   :description: Make your own profiler.\n   :col_css: col-md-6\n   :button_link: ../tuning/profiler_expert.html\n   :height: 150\n   :tag: expert\n\n.. displayitem::\n   :header: Level 24: Add a new accelerator or Strategy\n   :description: Integrate a new accelerator or distributed strategy.\n   :col_css: col-md-6\n   :button_link: expert_level_24.html\n   :height: 150\n   :tag: expert\n\n.. raw:: html\n\n        </div>\n    </div>\n```\n\n----------------------------------------\n\nTITLE: RST Structured Documentation\nDESCRIPTION: ReStructuredText markup for creating a structured documentation page with navigation cards for different deployment topics.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/deploy/production.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _production_inference:\n\n#############################\nDeploy models into production\n#############################\n\n******\nBasics\n******\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. displayitem::\n   :header: Basic\n   :description: Learn the basics of predicting with Lightning\n   :col_css: col-md-6\n   :button_link: production_basic.html\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Intermediate\n   :description: Learn to remove the Lightning dependencies and use pure PyTorch for prediction.\n   :col_css: col-md-6\n   :button_link: production_intermediate.html\n   :height: 150\n   :tag: intermediate\n\n.. raw:: html\n\n        </div>\n    </div>\n\n----\n\n********\nAdvanced\n********\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. displayitem::\n   :header: Deploy with ONNX\n   :description: Optimize models for enterprise-scale production environments with ONNX.\n   :col_css: col-md-4\n   :button_link: production_advanced.html\n   :height: 180\n   :tag: advanced\n\n.. displayitem::\n   :header: Deploy with torchscript\n   :description: Optimize models for enterprise-scale production environments with torchscript.\n   :col_css: col-md-4\n   :button_link: production_advanced_2.html\n   :height: 180\n   :tag: advanced\n\n.. displayitem::\n   :header: Compress models for fast inference\n   :description: Compress models for fast inference for deployment with Quantization and Pruning.\n   :col_css: col-md-4\n   :button_link: ../advanced/pruning_quantization.html\n   :height: 180\n   :tag: advanced\n\n.. raw:: html\n\n        </div>\n    </div>\n```\n\n----------------------------------------\n\nTITLE: HTML Layout Structure for Lightning Documentation\nDESCRIPTION: Raw HTML markup defining the layout structure for displaying cards in the Lightning documentation. Uses a container with rows and custom display items for navigation.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/levels/expert_level_23.rst#2025-04-23_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"display-card-container\">\n    <div class=\"row\">\n    </div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Loading Model State from Checkpoint in PyTorch Lightning\nDESCRIPTION: Shows how to directly load a model's state from a checkpoint file using the load_from_checkpoint static method.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/checkpointing_migration.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nmodel = LitModel.load_from_checkpoint(\"path/to/checkpoint.ckpt\")\n```\n\n----------------------------------------\n\nTITLE: Downloading Environment Details Script in Jupyter\nDESCRIPTION: This Jupyter magic command downloads a Python script for collecting environment details. It uses wget to fetch the script from the Lightning-AI GitHub repository.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/pytorch/bug_report/bug_report_model.ipynb#2025-04-23_snippet_7\n\nLANGUAGE: python\nCODE:\n```\n%%capture\n! wget https://raw.githubusercontent.com/Lightning-AI/lightning/master/requirements/collect_env_details.py\n```\n\n----------------------------------------\n\nTITLE: ReStructuredText Display Cards Layout\nDESCRIPTION: HTML and ReStructuredText mixed content defining a grid layout of cards with links to different guides and tutorials\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/index.rst#2025-04-23_snippet_1\n\nLANGUAGE: restructuredtext\nCODE:\n```\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. displayitem::\n    :header: Avoid overfitting\n    :description: Learn how to add validation and test loops\n    :button_link: ../common/evaluation.html\n    :col_css: col-md-4\n    :height: 180\n\n[...additional displayitems...]\n\n.. raw:: html\n\n        </div>\n    </div>\n```\n\n----------------------------------------\n\nTITLE: Manual DataModule Setup with Lightning\nDESCRIPTION: Shows how to manually prepare and setup DataModule for accessing dataset information before model creation.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/data/datamodule.rst#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ndm = MNISTDataModule()\ndm.prepare_data()\ndm.setup(stage=\"fit\")\n\nmodel = Model(num_classes=dm.num_classes, width=dm.width, vocab=dm.vocab)\ntrainer.fit(model, dm)\n\ndm.setup(stage=\"test\")\ntrainer.test(datamodule=dm)\n```\n\n----------------------------------------\n\nTITLE: Training Configuration Changes\nDESCRIPTION: Examples of changes in training configuration and strategy implementation, particularly affecting DDP, DeepSpeed, and mixed precision training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_9_devel.rst#2025-04-23_snippet_1\n\nLANGUAGE: Python\nCODE:\n```\n# Old approach (deprecated)\ntrainer = Trainer(data_parallel=True)\n\n# New approach\nfrom pytorch_lightning.strategies import ParallelStrategy\nis_parallel = isinstance(trainer.strategy, ParallelStrategy)\n```\n\n----------------------------------------\n\nTITLE: Using Multiple Loggers in Lightning\nDESCRIPTION: Shows how to configure multiple loggers (TensorBoard and Comet) to be used simultaneously in the Trainer.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/extensions/logging.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\ntb_logger = pl_loggers.TensorBoardLogger(save_dir=\"logs/\")\ncomet_logger = pl_loggers.CometLogger(save_dir=\"logs/\")\ntrainer = Trainer(logger=[tb_logger, comet_logger])\n```\n\n----------------------------------------\n\nTITLE: Comparing PyTorch and Fabric Implementations with sdiff\nDESCRIPTION: Command to compare the differences between the PyTorch and Lightning Fabric implementations using the sdiff utility.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/fabric/meta_learning/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nsdiff train_torch.py train_fabric.py\n```\n\n----------------------------------------\n\nTITLE: Creating Display Items for PyTorch Lightning Community Links\nDESCRIPTION: This snippet uses reStructuredText directives to create a series of display items, each representing a link to a different community-related document. The display items include headers, descriptions, and links to the respective pages.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/community/index.rst#2025-04-23_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. displayitem::\n   :header: Code of conduct\n   :description: Contributor Covenant Code of Conduct\n   :col_css: col-md-12\n   :button_link: ../generated/CODE_OF_CONDUCT.html\n   :height: 100\n\n.. displayitem::\n   :header: Contribution guide\n   :description: How to contribute to PyTorch Lightning\n   :col_css: col-md-12\n   :button_link: ../generated/CONTRIBUTING.html\n   :height: 100\n\n.. displayitem::\n   :header: How to become a core contributor\n   :description: Steps to be a core contributor\n   :col_css: col-md-12\n   :button_link: ../generated/BECOMING_A_CORE_CONTRIBUTOR.html\n   :height: 100\n\n.. displayitem::\n   :header: Lightning Governance\n   :description: The governance processes we follow\n   :col_css: col-md-12\n   :button_link: governance.html\n   :height: 100\n\n.. displayitem::\n   :header: Versioning\n   :description: PyTorch Lightning's versioning policy\n   :col_css: col-md-12\n   :button_link: ../versioning.html\n   :height: 100\n\n.. displayitem::\n   :header: Past PyTorch Lightning versions\n   :description: Docs and upgrade guide for past versions\n   :col_css: col-md-12\n   :button_link: ../past_versions.html\n   :height: 100\n\n.. displayitem::\n   :header: Changelog\n   :description: All notable changes to PyTorch Lightning\n   :col_css: col-md-12\n   :button_link: ../generated/CHANGELOG.html\n   :height: 100\n\n.. displayitem::\n   :header: Ecosystem CI\n   :description: Automate issue discovery for your projects against Lightning nightly and releases\n   :col_css: col-md-12\n   :button_link: ecosystem-ci.html\n   :height: 100\n```\n\n----------------------------------------\n\nTITLE: RST Document Structure for PyTorch Lightning Checkpoints Documentation\nDESCRIPTION: ReStructuredText markup that defines a documentation page layout for PyTorch Lightning's checkpointing features. The code creates a display card container with links to checkpoint-related documentation pages.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/checkpoint/index.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n###########\nCheckpoints\n###########\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. displayitem::\n    :header: Save and load model progress\n    :description: Efficient saving and loading of model weights, training state, hyperparameters and more.\n    :button_link: checkpoint.html\n    :col_css: col-md-4\n    :height: 150\n    :tag: intermediate\n\n.. displayitem::\n    :header: Working with very large models\n    :description: Save and load very large models efficiently with distributed checkpoints\n    :button_link: distributed_checkpoint.html\n    :col_css: col-md-4\n    :height: 150\n    :tag: advanced\n\n\n.. raw:: html\n\n        </div>\n    </div>\n```\n\n----------------------------------------\n\nTITLE: HTML Layout Container Definition\nDESCRIPTION: HTML markup that creates a container for displaying profiling documentation cards in a grid layout\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/tuning/profiler.rst#2025-04-23_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"display-card-container\">\n    <div class=\"row\">\n        </div>\n    </div>\n```\n\n----------------------------------------\n\nTITLE: HTML Structure for Documentation Layout\nDESCRIPTION: HTML markup for creating a card-based layout to display different sections of the accelerator/strategy documentation with a responsive grid system.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/levels/expert_level_24.rst#2025-04-23_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"display-card-container\">\n    <div class=\"row\">\n    </div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Structuring RST Documentation with Display Items\nDESCRIPTION: RST markup for creating a structured documentation page with display cards for different learning levels in PyTorch Lightning. Uses raw HTML for layout and custom displayitem directives for content organization.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/levels/core_skills.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. displayitem::\n   :header: Level 1: Train a model\n   :description: Learn the basics of training a model.\n   :button_link: ../model/train_model_basic.html\n   :col_css: col-md-6\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Level 2: Add a validation and test set\n   :description: Add validation and test sets to avoid over/underfitting.\n   :button_link: ../levels/basic_level_2.html\n   :col_css: col-md-6\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Level 3: Use pretrained models\n   :description: Learn how to use pretrained models with Lightning\n   :button_link: ../advanced/transfer_learning.html\n   :col_css: col-md-6\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Level 4: Enable script parameters\n   :description: Add parameters to your script so you can run from the commandline.\n   :button_link: ../common/hyperparameters.html\n   :col_css: col-md-6\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Level 5: Understand and visualize your model\n   :description: Remove bottlenecks and visualize your model\n   :button_link: ../levels/basic_level_5.html\n   :col_css: col-md-6\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :description: Use your model for predictions.\n   :header: Level 6: Predict with your model\n   :button_link: core_level_6.html\n   :col_css: col-md-6\n   :height: 150\n   :tag: basic\n\n.. raw:: html\n\n        </div>\n    </div>\n```\n\n----------------------------------------\n\nTITLE: Traditional Model Selection Implementation\nDESCRIPTION: Example of traditional boilerplate code for model and dataset selection that should be avoided in favor of LightningCLI\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_intermediate_2.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# choose model\nif args.model == \"gan\":\n    model = GAN(args.feat_dim)\nelif args.model == \"transformer\":\n    model = Transformer(args.feat_dim)\n...\n\n# choose datamodule\nif args.data == \"MNIST\":\n    datamodule = MNIST()\nelif args.data == \"imagenet\":\n    datamodule = Imagenet()\n...\n\n# mix them!\ntrainer.fit(model, datamodule)\n```\n\n----------------------------------------\n\nTITLE: Structuring TPU Documentation HTML Layout\nDESCRIPTION: HTML markup for creating a responsive card layout to display TPU-related documentation sections including cloud TPU usage, scaling techniques, and profiling guides.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/levels/advanced_level_20.rst#2025-04-23_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"display-card-container\">\n    <div class=\"row\">\n    </div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Initializing Main Function Structure in Python\nDESCRIPTION: Basic boilerplate code structure for the main entry point of a Python script, ensuring proper multiprocessing functionality for data loaders.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/code_structure.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef main():\n    # Here goes all the rest of the code\n    ...\n\n\nif __name__ == \"__main__\":\n    # This is the entry point of your program\n    main()\n```\n\n----------------------------------------\n\nTITLE: Defining Logger Module Structure in RST\nDESCRIPTION: This RST code defines the documentation structure for the lightning.fabric.loggers module, including the built-in loggers and third-party logger options.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/loggers.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: ../links.rst\n\n########################\nlightning.fabric.loggers\n########################\n\n\nLoggers\n^^^^^^^\n\n.. currentmodule:: lightning.fabric.loggers\n\n.. autosummary::\n    :toctree: ./generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    Logger\n    CSVLogger\n    TensorBoardLogger\n\n\nThird-party Loggers\n^^^^^^^^^^^^^^^^^^^\n\n.. list-table::\n   :widths: 50 50\n   :header-rows: 0\n\n   * - :doc:`WandbLogger <../guide/loggers/wandb>`\n     - Log to `Weights & Biases <https://www.wandb.ai/>`_.\n```\n\n----------------------------------------\n\nTITLE: Structuring PyTorch Lightning Upgrade Guide with RST Directives\nDESCRIPTION: This RST code structures the upgrade guide into sections for different user types (regular, advanced, developer) and includes separate files for each version upgrade step.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/from_1_5.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n:orphan:\n\nUpgrade from 1.5 to the 2.0\n###########################\n\nRegular User\n************\n\n.. include:: sections/1_5_regular.rst\n.. include:: sections/1_6_regular.rst\n.. include:: sections/1_7_regular.rst\n.. include:: sections/1_8_regular.rst\n.. include:: sections/1_9_regular.rst\n\nAdvanced User\n*************\n\n.. include:: sections/1_5_advanced.rst\n.. include:: sections/1_6_advanced.rst\n.. include:: sections/1_7_advanced.rst\n.. include:: sections/1_8_advanced.rst\n.. include:: sections/1_9_advanced.rst\n\nDeveloper\n*********\n\n.. include:: sections/1_5_devel.rst\n.. include:: sections/1_6_devel.rst\n.. include:: sections/1_7_devel.rst\n.. include:: sections/1_8_devel.rst\n.. include:: sections/1_9_devel.rst\n```\n\n----------------------------------------\n\nTITLE: Updating Trainer Sanity Check Property Access\nDESCRIPTION: Migration from Trainer.running_sanity_check to Trainer.sanity_checking property.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_4_advanced.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\n# Old way\nTrainer.running_sanity_check\n\n# New way\nTrainer.sanity_checking\n```\n\n----------------------------------------\n\nTITLE: Navigation Layout in reStructuredText and HTML for TPU Documentation\nDESCRIPTION: A combination of reStructuredText and HTML markup that creates a card-based navigation interface for TPU acceleration documentation in PyTorch Lightning. It includes links to different sections organized by skill level from basic to advanced.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/accelerators/tpu.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. Add callout items below this line\n\n.. displayitem::\n   :header: Prepare your code (Optional)\n   :description: Prepare your code to run on any hardware\n   :col_css: col-md-4\n   :button_link: accelerator_prepare.html\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Basic\n   :description: Learn the basics of single and multi-TPU core training.\n   :col_css: col-md-4\n   :button_link: tpu_basic.html\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Intermediate\n   :description: Scale massive models using cloud TPUs.\n   :col_css: col-md-4\n   :button_link: tpu_intermediate.html\n   :height: 150\n   :tag: intermediate\n\n.. displayitem::\n   :header: Advanced\n   :description: Dive into XLA and advanced techniques to optimize TPU-powered models.\n   :col_css: col-md-4\n   :button_link: tpu_advanced.html\n   :height: 150\n   :tag: advanced\n\n.. displayitem::\n   :header: FAQ\n   :description: Frequently asked questions about TPU training.\n   :col_css: col-md-4\n   :button_link: tpu_faq.html\n   :height: 150\n\n.. raw:: html\n\n        </div>\n    </div>\n```\n\n----------------------------------------\n\nTITLE: Structuring HTML for Display Cards in PyTorch Lightning Documentation\nDESCRIPTION: This HTML snippet creates a container for display cards, likely used in the PyTorch Lightning documentation to present different sections or topics. It sets up a responsive grid layout for the cards.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/model/own_your_loop.rst#2025-04-23_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"display-card-container\">\n    <div class=\"row\">\n\n\n```\n\n----------------------------------------\n\nTITLE: Defining Section Header in RST Documentation\nDESCRIPTION: This snippet defines the main section header for the lightning.fabric.plugins.io module documentation. It uses RST syntax for creating a top-level header.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/io.rst#2025-04-23_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n###########################\nlightning.fabric.plugins.io\n###########################\n```\n\n----------------------------------------\n\nTITLE: Embedding Raw HTML in reStructuredText for PyTorch Lightning Community Page\nDESCRIPTION: This snippet embeds raw HTML within the reStructuredText document to create a container and row structure for the display items. It's used to enhance the layout and styling of the community documentation page.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/community/index.rst#2025-04-23_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. raw:: html\n\n        </div>\n    </div>\n```\n\n----------------------------------------\n\nTITLE: Uploading Data to AWS S3\nDESCRIPTION: AWS CLI command to recursively upload the optimized dataset to S3 storage.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/data/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\naws s3 cp --recursive my_dataset s3://my-bucket/my_dataset\n```\n\n----------------------------------------\n\nTITLE: Rendering HTML Container for Interactive Cloud Development Guide\nDESCRIPTION: This HTML snippet creates a container with clickable cards for different sections of the cloud development guide. It includes sections for code preparation, GPU training, and TPU training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/levels/intermediate_level_7.rst#2025-04-23_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"display-card-container\">\n    <div class=\"row\">\n\n.. displayitem::\n   :header: Prepare your code (Optional)\n   :description: Prepare your code to run on any hardware.\n   :col_css: col-md-3\n   :button_link: ../accelerators/accelerator_prepare.html\n   :height: 180\n   :tag: basic\n\n.. displayitem::\n   :header: GPU Training\n   :description: Learn the basics on single and multi-GPU training.\n   :col_css: col-md-3\n   :button_link: ../accelerators/gpu_basic.html\n   :height: 180\n   :tag: basic\n\n.. displayitem::\n   :header: TPU Training\n   :description: Learn the basics on single and multi-TPU core training.\n   :col_css: col-md-3\n   :button_link: ../accelerators/tpu_basic.html\n   :height: 180\n   :tag: basic\n\n    </div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Structuring Content with HTML in PyTorch Lightning Documentation\nDESCRIPTION: This HTML snippet creates a structured layout for displaying card items in the PyTorch Lightning documentation. It uses custom classes and rows to organize content into a grid-like structure.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/levels/intermediate_level_10.rst#2025-04-23_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"display-card-container\">\n    <div class=\"row\">\n\n    </div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Defining Navigation Items with reStructuredText Directives\nDESCRIPTION: This snippet uses reStructuredText 'displayitem' directives to define clickable navigation items for the Lightning documentation. Each item includes a header, description, link, and styling attributes.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/starter/introduction.rst#2025-04-23_snippet_7\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. displayitem::\n   :header: Deploy your model\n   :description: Learn how to predict or put your model into production\n   :col_css: col-md-3\n   :button_link: ../deploy/production.html\n   :height: 180\n   :tag: basic\n```\n\n----------------------------------------\n\nTITLE: Adding Experimental Feature Warning in RST Documentation\nDESCRIPTION: This snippet adds a warning message indicating that the documented feature is experimental. It includes a link to the versioning documentation for more information.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/io.rst#2025-04-23_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. warning::\n    This is an `experimental <https://lightning.ai/docs/pytorch/latest/versioning.html>`__ feature.\n```\n\n----------------------------------------\n\nTITLE: Basic HPU Training Structure in RST\nDESCRIPTION: RST markup structure for displaying training card components for HPU documentation.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/levels/advanced_level_19.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n:orphan:\n\n######################\nLevel 18: Explore HPUs\n######################\n\nExplore Intel Habana Processing Unit (HPU) for model scaling.\n\n----\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. Add callout items below this line\n\n.. displayitem::\n   :header: Train models on HPUs\n   :description: Learn the basics of single and multi-HPU core training.\n   :col_css: col-md-6\n   :button_link: ../integrations/hpu/basic.html\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Optimize models training on HPUs\n   :description: Enable state-of-the-art scaling with advanced mixed-precision settings.\n   :col_css: col-md-6\n   :button_link: ../integrations/hpu/intermediate.html\n   :height: 150\n   :tag: intermediate\n\n.. raw:: html\n\n        </div>\n    </div>\n```\n\n----------------------------------------\n\nTITLE: Deprecation Notice for Trainer Checkpoint Path\nDESCRIPTION: Notification about deprecation of Trainer's ckpt_path attribute in favor of weights_save_path parameter for setting checkpoint location.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_41\n\nLANGUAGE: text\nCODE:\n```\nDeprecated Trainer attribute `ckpt_path`, which will now be set by `weights_save_path`\n```\n\n----------------------------------------\n\nTITLE: Adding Experimental Feature Warning in RST Documentation\nDESCRIPTION: This snippet adds a warning message in RST format to indicate that the feature is experimental, with a link to the versioning documentation.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/collectives.rst#2025-04-23_snippet_2\n\nLANGUAGE: rst\nCODE:\n```\n.. warning::\n    This is an `experimental <https://lightning.ai/docs/pytorch/latest/versioning.html>`__ feature.\n```\n\n----------------------------------------\n\nTITLE: Extracting Batch Size in PyTorch Lightning\nDESCRIPTION: New utility function added to extract batch dimension from multiple batch types.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_35\n\nLANGUAGE: python\nCODE:\n```\nextract_batch_size\n```\n\n----------------------------------------\n\nTITLE: FSDP Sharding Strategy Configuration\nDESCRIPTION: Configuration options for FSDP sharding strategy to balance between memory usage and training speed.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/model_parallel/fsdp.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nstrategy = FSDPStrategy(\n    # Default: Shard weights, gradients, optimizer state (1 + 2 + 3)\n    sharding_strategy=\"FULL_SHARD\",\n    # Shard gradients, optimizer state (2 + 3)\n    sharding_strategy=\"SHARD_GRAD_OP\",\n    # Full-shard within a machine, replicate across machines\n    sharding_strategy=\"HYBRID_SHARD\",\n    # Don't shard anything (similar to DDP)\n    sharding_strategy=\"NO_SHARD\",\n)\nfabric = L.Fabric(..., strategy=strategy)\n```\n\n----------------------------------------\n\nTITLE: Updating TrainerModelHooksMixin Usage in PyTorch Lightning\nDESCRIPTION: Migrate from TrainerModelHooksMixin to utility functions in pytorch_lightning.utilities.signature_utils. Referenced in PR7422.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_5_devel.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nTrainerModelHooksMixin\n```\n\n----------------------------------------\n\nTITLE: Creating Collectives Subsection in RST Documentation\nDESCRIPTION: This snippet creates a subsection for Collectives in the RST documentation.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/collectives.rst#2025-04-23_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\nCollectives\n^^^^^^^^^^^\n\n```\n\n----------------------------------------\n\nTITLE: Defining RST Documentation Structure with HTML Components\nDESCRIPTION: RST documentation code that defines a card-based navigation layout for GPU training documentation. Uses a combination of RST directives and raw HTML for layout structure.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/accelerators/gpu.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. _gpu:\n\nAccelerator: GPU training\n=========================\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. displayitem::\n   :header: Prepare your code (Optional)\n   :description: Prepare your code to run on any hardware\n   :col_css: col-md-4\n   :button_link: accelerator_prepare.html\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Basic\n   :description: Learn the basics of single and multi-GPU training.\n   :col_css: col-md-4\n   :button_link: gpu_basic.html\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Intermediate\n   :description: Learn about different distributed strategies, torchelastic and how to optimize communication layers.\n   :col_css: col-md-4\n   :button_link: gpu_intermediate.html\n   :height: 150\n   :tag: intermediate\n\n.. displayitem::\n   :header: Advanced\n   :description: Train models with billions of parameters\n   :col_css: col-md-4\n   :button_link: gpu_advanced.html\n   :height: 150\n   :tag: advanced\n\n.. displayitem::\n   :header: Expert\n   :description: Develop new strategies for training and deploying larger and larger models.\n   :col_css: col-md-4\n   :button_link: gpu_expert.html\n   :height: 150\n   :tag: expert\n\n.. displayitem::\n   :header: FAQ\n   :description: Frequently asked questions about GPU training.\n   :col_css: col-md-4\n   :button_link: gpu_faq.html\n   :height: 150\n\n.. raw:: html\n\n        </div>\n    </div>\n```\n\n----------------------------------------\n\nTITLE: Defining RST Section Header for Lightning Fabric Plugins Collectives\nDESCRIPTION: This snippet creates a section header in RST format for the Lightning Fabric plugins collectives documentation.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/collectives.rst#2025-04-23_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n####################################\nlightning.fabric.plugins.collectives\n####################################\n```\n\n----------------------------------------\n\nTITLE: Disabling Warnings via Environment Variables\nDESCRIPTION: Shows how to disable PossibleUserWarning messages using environment variables in bash.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/warnings.rst#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\nexport POSSIBLE_USER_WARNINGS=off\n# or\nexport POSSIBLE_USER_WARNINGS=0\n```\n\n----------------------------------------\n\nTITLE: Generating Autosummary for Collective Classes in RST\nDESCRIPTION: This snippet uses the RST autosummary directive to generate documentation for Collective, TorchCollective, and SingleDeviceCollective classes.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/collectives.rst#2025-04-23_snippet_5\n\nLANGUAGE: rst\nCODE:\n```\n.. autosummary::\n    :toctree: ./generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    Collective\n    TorchCollective\n    SingleDeviceCollective\n```\n\n----------------------------------------\n\nTITLE: Build Status Table in Markdown\nDESCRIPTION: A markdown table showing build status badges for different PyTorch versions and system configurations, including Linux, OSX, and Windows with multiple Python versions.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/README.md#2025-04-23_snippet_14\n\nLANGUAGE: markdown\nCODE:\n```\n|       System / PyTorch ver.        | 1.13 | 2.0 | 2.1 |\n| :--------------------------------: | :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|\n|        Linux py3.9 \\[GPUs\\]        |  |  | [![Build Status](https://dev.azure.com/Lightning-AI/lightning/_apis/build/status%2Fpytorch-lightning%20%28GPUs%29?branchName=master)](https://dev.azure.com/Lightning-AI/lightning/_build/latest?definitionId=24&branchName=master) |\n|  Linux (multiple Python versions)  | [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml) | [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml) | [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml) |\n|   OSX (multiple Python versions)   | [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml) | [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml) | [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml) |\n| Windows (multiple Python versions) | [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml) | [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml) | [![Test PyTorch](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml/badge.svg)](https://github.com/Lightning-AI/lightning/actions/workflows/ci-tests-pytorch.yml) |\n```\n\n----------------------------------------\n\nTITLE: Setting Up Models, Optimizers and Dataloaders with Fabric\nDESCRIPTION: Configure your PyTorch models, optimizers, and dataloaders to work with Fabric's distributed training functionality.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning_fabric/README.md#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\nmodel, optimizer = fabric.setup(model, optimizer)\ndataloader = fabric.setup_dataloaders(dataloader)\n```\n\n----------------------------------------\n\nTITLE: Generating Classes Section in Sphinx Documentation with Jinja2\nDESCRIPTION: Template block that conditionally creates a 'Classes' section with an autosummary listing when classes are present in the module. It iterates through each class and lists them without signatures.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/_templates/autosummary/module.rst#2025-04-23_snippet_2\n\nLANGUAGE: jinja2\nCODE:\n```\n{% block classes %}\n{% if classes %}\n.. rubric:: Classes\n\n.. autosummary::\n    :nosignatures:\n{% for item in classes %}\n    {{ item }}\n{%- endfor %}\n{% endif %}\n{% endblock %}\n```\n\n----------------------------------------\n\nTITLE: SyncBN Feature Addition for DDP\nDESCRIPTION: Addition of Synchronized Batch Normalization (SyncBN) support for Distributed Data Parallel (DDP) training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_42\n\nLANGUAGE: text\nCODE:\n```\nAdded SyncBN for DDP\n```\n\n----------------------------------------\n\nTITLE: HTML Container Structure in RST\nDESCRIPTION: HTML wrapper elements used to create a display card container layout for the upgrade guide navigation\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/migration_guide.rst#2025-04-23_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"display-card-container\">\n    <div class=\"row\">\n    </div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Installing Lightning Patch Release from Source\nDESCRIPTION: This snippet shows how to install the latest patch release of Lightning from the GitHub repository.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/starter/installation.rst#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install https://github.com/Lightning-AI/lightning/archive/refs/heads/release/stable.zip -U\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch Lightning via pip\nDESCRIPTION: Basic installation command for PyTorch Lightning using pip package manager.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install lightning\n```\n\n----------------------------------------\n\nTITLE: Running Standalone PyTorch-Lightning Tests\nDESCRIPTION: These commands demonstrate how to download and execute standalone tests for PyTorch-Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/tests/README.md#2025-04-23_snippet_5\n\nLANGUAGE: bash\nCODE:\n```\ncd tests/\nwget https://raw.githubusercontent.com/Lightning-AI/utilities/main/scripts/run_standalone_tests.sh\n./tests/run_standalone_tests.sh tests_pytorch/\n```\n\n----------------------------------------\n\nTITLE: Auto-documenting Class Members in PyTorch Lightning\nDESCRIPTION: Uses the 'autoclass' directive to automatically document the class and its members. The class name is dynamically inserted using a template variable.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/_templates/classtemplate.rst#2025-04-23_snippet_3\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. autoclass:: {{ name }}\n    :members:\n```\n\n----------------------------------------\n\nTITLE: RST Display Cards Structure for HPU Documentation\nDESCRIPTION: ReStructuredText markup defining a card-based navigation interface for HPU training documentation. Sets up a responsive grid layout with three cards for basic, intermediate, and advanced training topics.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/integrations/hpu/index.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. displayitem::\n   :header: Basic\n   :description: Learn the basics of single and multi-HPU core training.\n   :col_css: col-md-4\n   :button_link: basic.html\n   :height: 150\n   :tag: basic\n\n.. displayitem::\n   :header: Intermediate\n   :description: Enable state-of-the-art scaling with advanced mix-precision settings.\n   :col_css: col-md-4\n   :button_link: intermediate.html\n   :height: 150\n   :tag: intermediate\n\n.. displayitem::\n   :header: Advanced\n   :description: Explore state-of-the-art scaling with additional advanced configurations.\n   :col_css: col-md-4\n   :button_link: advanced.html\n   :height: 150\n   :tag: advanced\n\n.. raw:: html\n\n        </div>\n    </div>\n```\n\n----------------------------------------\n\nTITLE: Defining Habana Validation Accelerator Version Requirements\nDESCRIPTION: Specifies the compatible version range for the lightning-habana accelerator connector. The dependency requires version 1.2.0 or higher but less than 1.3.0, ensuring compatibility with current PyTorch Lightning functionality.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/requirements/_integrations/accelerators.txt#2025-04-23_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nlightning-habana >=1.2.0, <1.3.0\n```\n\n----------------------------------------\n\nTITLE: Setting Current Module in Sphinx for PyTorch Lightning Docs\nDESCRIPTION: Sets the current module context for autodoc directives, using a template variable that will be replaced during documentation generation.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/_templates/classtemplate.rst#2025-04-23_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. currentmodule:: {{ module }}\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure\nDESCRIPTION: RST file structure defining the documentation layout for PyTorch Lightning upgrade guide with includes for different user types and versions.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/from_1_4.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n:orphan:\n\nUpgrade from 1.4 to the 2.0\n###########################\n\nRegular User\n************\n\n.. include:: sections/1_4_regular.rst\n.. include:: sections/1_5_regular.rst\n.. include:: sections/1_6_regular.rst\n.. include:: sections/1_7_regular.rst\n.. include:: sections/1_8_regular.rst\n.. include:: sections/1_9_regular.rst\n\nAdvanced User\n*************\n\n.. include:: sections/1_4_advanced.rst\n.. include:: sections/1_5_advanced.rst\n.. include:: sections/1_6_advanced.rst\n.. include:: sections/1_7_advanced.rst\n.. include:: sections/1_8_advanced.rst\n.. include:: sections/1_9_advanced.rst\n\nDeveloper\n*********\n\n.. include:: sections/1_4_devel.rst\n.. include:: sections/1_5_devel.rst\n.. include:: sections/1_6_devel.rst\n.. include:: sections/1_7_devel.rst\n.. include:: sections/1_8_devel.rst\n.. include:: sections/1_9_devel.rst\n```\n\n----------------------------------------\n\nTITLE: Generating Class Documentation Header in PyTorch Lightning\nDESCRIPTION: Creates a header for the class documentation using a template variable for the class name, which will be underlined in the rendered output.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/_templates/classtemplate.rst#2025-04-23_snippet_2\n\nLANGUAGE: reStructuredText\nCODE:\n```\n{{ name | underline }}\n```\n\n----------------------------------------\n\nTITLE: Generating Exceptions Section in Sphinx Documentation with Jinja2\nDESCRIPTION: Template block that conditionally creates an 'Exceptions' section with an autosummary listing when exceptions are present in the module. It iterates through each exception and lists them without signatures.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/_templates/autosummary/module.rst#2025-04-23_snippet_3\n\nLANGUAGE: jinja2\nCODE:\n```\n{% block exceptions %}\n{% if exceptions %}\n.. rubric:: Exceptions\n\n.. autosummary::\n    :nosignatures:\n{% for item in exceptions %}\n    {{ item }}\n{%- endfor %}\n{% endif %}\n{% endblock %}\n```\n\n----------------------------------------\n\nTITLE: Structuring PyTorch Lightning Glossary with ReStructuredText\nDESCRIPTION: This code snippet uses ReStructuredText to create a table of contents and a grid of display items for the PyTorch Lightning glossary. It includes hidden toctree entries and uses raw HTML for layout.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/glossary/index.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n########\nGlossary\n########\n\n.. toctree::\n   :maxdepth: 1\n   :hidden:\n\n   Checkpoint <../guide/checkpoint/index>\n   Weights and Biases <../guide/loggers/wandb>\n   Wrappers <../api/wrappers>\n   Model Parallel <../advanced/model_parallel/index>\n   Tensor Parallel <../advanced/model_parallel/tp>\n   2D Parallelism <../advanced/model_parallel/tp_fsdp>\n\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. displayitem::\n    :header: 2D Parallelism\n    :button_link: ../advanced/model_parallel/tp_fsdp.html\n    :col_css: col-md-4\n\n.. displayitem::\n    :header: Accelerator\n    :button_link: ../fundamentals/accelerators.html\n    :col_css: col-md-4\n\n.. displayitem::\n    :header: Apple Silicon\n    :button_link: ../fundamentals/accelerators.html\n    :col_css: col-md-4\n\n# ... (remaining displayitems omitted for brevity)\n\n.. raw:: html\n\n        </div>\n    </div>\n```\n\n----------------------------------------\n\nTITLE: RST Display Card Configuration\nDESCRIPTION: RST markup for creating a display card layout that links to profiler documentation for finding performance bottlenecks in Lightning models.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/benchmarking/benchmarks.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. displayitem::\n   :header: Find bottlenecks in your models\n   :description: Benchmark your own Lightning models\n   :button_link: ../tuning/profiler.html\n   :col_css: col-md-3\n   :height: 180\n   :tag: basic\n\n.. raw:: html\n\n        </div>\n    </div>\n```\n\n----------------------------------------\n\nTITLE: Setting Network Interface for GLOO and NCCL in Bash\nDESCRIPTION: This bash snippet shows how to set the network interface for GLOO and NCCL when troubleshooting network-related issues in a PyTorch Lightning cluster setup.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/multi_node/barebones.rst#2025-04-23_snippet_3\n\nLANGUAGE: bash\nCODE:\n```\nexport GLOO_SOCKET_IFNAME=eno1\nexport NCCL_SOCKET_IFNAME=eno1\nfabric run ...\n```\n\n----------------------------------------\n\nTITLE: Loading PyTorch Lightning Checkpoints from Cloud Storage\nDESCRIPTION: Shows how to resume training from a checkpoint stored in cloud storage by providing the cloud URL in the ckpt_path parameter when initializing training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/checkpointing_advanced.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\ntrainer = Trainer(default_root_dir=tmpdir, max_steps=3)\ntrainer.fit(model, ckpt_path=\"s3://my_bucket/ckpts/classifier.ckpt\")\n```\n\n----------------------------------------\n\nTITLE: RST Display Cards Setup for PyTorch Lightning Documentation\nDESCRIPTION: This code creates a responsive grid layout for documentation display cards using a combination of RST and HTML. It structures the basic skills documentation with visually appealing cards that link to detailed guides.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/levels/basic.rst#2025-04-23_snippet_1\n\nLANGUAGE: rst\nCODE:\n```\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. displayitem::\n    :header: Convert to Fabric in 5 minutes\n    :description: Learn how to add Fabric to your PyTorch code\n    :button_link: ../fundamentals/convert.html\n    :col_css: col-md-4\n    :height: 150\n    :tag: basic\n\n.. displayitem::\n    :header: Scale your model with Accelerators\n    :description: Take advantage of your hardware with a switch of a flag\n    :button_link: ../fundamentals/accelerators.html\n    :col_css: col-md-4\n    :height: 150\n    :tag: basic\n\n.. displayitem::\n    :header: Structure your Fabric code\n    :description: Best practices for setting up your training script with Fabric\n    :button_link: ../fundamentals/code_structure.html\n    :col_css: col-md-4\n    :height: 150\n    :tag: basic\n\n.. displayitem::\n    :header: Launch distributed training\n    :description: Launch a Python script on multiple devices and machines\n    :button_link: ../fundamentals/launch.html\n    :col_css: col-md-4\n    :height: 150\n    :tag: basic\n\n.. displayitem::\n    :header: Launch Fabric in a notebook\n    :description: Launch on multiple devices from within a Jupyter notebook\n    :button_link: ../fundamentals/notebooks.html\n    :col_css: col-md-4\n    :height: 150\n    :tag: basic\n\n.. displayitem::\n    :header: Improve performance with Mixed-Precision training\n    :description: Save memory and speed up training using mixed precision\n    :button_link: ../fundamentals/precision.html\n    :col_css: col-md-4\n    :height: 150\n    :tag: basic\n\n.. raw:: html\n\n        </div>\n    </div>\n```\n\n----------------------------------------\n\nTITLE: Cluster Environment Method to Property Change\nDESCRIPTION: Conversion of creates_children() method to creates_processes_externally property in ClusterEnvironment class.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_6_devel.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nClusterEnvironment.creates_children() # deprecated method\nClusterEnvironment.creates_processes_externally # new property\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents in reStructuredText for PyTorch Lightning Community Docs\nDESCRIPTION: This snippet defines a hidden table of contents (toctree) with links to various community-related documents in the PyTorch Lightning project. It sets the maximum depth to 1 and includes both local and generated files.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/community/index.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. toctree::\n   :maxdepth: 1\n   :hidden:\n\n   ../generated/CODE_OF_CONDUCT.md\n   ../generated/CONTRIBUTING.md\n   ../generated/BECOMING_A_CORE_CONTRIBUTOR.md\n   governance\n   ecosystem-ci\n   ../versioning\n   ../past_versions\n   ../generated/CHANGELOG.md\n```\n\n----------------------------------------\n\nTITLE: Closing HTML Tags for Navigation Section\nDESCRIPTION: This HTML snippet closes the div elements that likely wrap the navigation items created by the reStructuredText directives. It's used to properly structure the HTML output of the documentation.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/starter/introduction.rst#2025-04-23_snippet_8\n\nLANGUAGE: HTML\nCODE:\n```\n        </div>\n    </div>\n```\n\n----------------------------------------\n\nTITLE: Documenting IO Classes in RST\nDESCRIPTION: This snippet creates an autosummary section for the IO classes in the lightning.fabric.plugins.io module. It uses RST directives to generate documentation for CheckpointIO, TorchCheckpointIO, and XLACheckpointIO classes.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/io.rst#2025-04-23_snippet_3\n\nLANGUAGE: rst\nCODE:\n```\nIO\n^^\n\n.. currentmodule:: lightning.fabric.plugins.io\n\n.. autosummary::\n    :toctree: ./generated\n    :nosignatures:\n    :template: classtemplate.rst\n\n    ~checkpoint_io.CheckpointIO\n    ~torch_io.TorchCheckpointIO\n    ~xla.XLACheckpointIO\n```\n\n----------------------------------------\n\nTITLE: Installing TensorBoard for PyTorch Lightning\nDESCRIPTION: Installs the TensorBoard package using pip for use with PyTorch Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/supported_exp_managers.rst#2025-04-23_snippet_6\n\nLANGUAGE: bash\nCODE:\n```\npip install tensorboard\n```\n\n----------------------------------------\n\nTITLE: HTML Container Layout for Documentation Cards\nDESCRIPTION: HTML markup for creating a responsive grid layout to display documentation navigation cards.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/levels/intermediate_level_14.rst#2025-04-23_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"display-card-container\">\n    <div class=\"row\">\n    </div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Setting a Breakpoint in Python\nDESCRIPTION: Demonstrates how to set a breakpoint in Python code to pause execution for debugging purposes.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/debug/debugging_basic.rst#2025-04-23_snippet_0\n\nLANGUAGE: python\nCODE:\n```\ndef function_to_debug():\n    x = 2\n\n    # set breakpoint\n    breakpoint()\n    y = x**2\n```\n\n----------------------------------------\n\nTITLE: Specifying PyTorch Lightning Test Dependencies\nDESCRIPTION: A requirements list for testing PyTorch Lightning that specifies exact versions and version ranges for test dependencies. Includes testing tools like pytest and its plugins, along with supporting libraries such as numpy and tensorboardX with specific version constraints.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/requirements/fabric/test.txt#2025-04-23_snippet_0\n\nLANGUAGE: text\nCODE:\n```\ncoverage ==7.3.1\nnumpy >=1.17.2, <1.27.0\npytest ==8.3.5\npytest-cov ==4.1.0\npytest-timeout ==2.1.0\npytest-rerunfailures ==12.0\npytest-random-order ==1.1.0\nclick ==8.1.7\ntensorboardX >=2.2, <2.7.0  # min version is set by torch.onnx missing attribute\n```\n\n----------------------------------------\n\nTITLE: Setting up a Sphinx Class Documentation Template in reStructuredText\nDESCRIPTION: This template is used to generate documentation for PyTorch Lightning classes. It declares the document as orphaned (not included in TOC), defines a hidden section role, sets the current module context, and creates an auto-documented class section. The template contains placeholders that get replaced during documentation generation.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/_templates/classtemplate_no_index.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n:orphan:\n\n.. role:: hidden\n    :class: hidden-section\n.. currentmodule:: {{ module }}\n\n\n{{ name | underline }}\n\n.. autoclass:: {{ name }}\n    :members:\n    :noindex:\n```\n\n----------------------------------------\n\nTITLE: Defining Table of Contents in reStructuredText\nDESCRIPTION: This snippet defines a hidden table of contents with links to various advanced topics in PyTorch Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/levels/advanced.rst#2025-04-23_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. toctree::\n   :maxdepth: 1\n   :hidden:\n\n    <../advanced/gradient_accumulation>\n    <../advanced/distributed_communication>\n    <../advanced/multiple_setup>\n    <../advanced/compile>\n    <../advanced/model_parallel/fsdp>\n    <../guide/checkpoint/distributed_checkpoint>\n```\n\n----------------------------------------\n\nTITLE: Including Transfer Learning RST Documentation\nDESCRIPTION: RST include directive used to import the contents of transfer_learning.rst into the current documentation file. This allows modular documentation by referencing external RST files.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/finetuning.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: transfer_learning.rst\n```\n\n----------------------------------------\n\nTITLE: Installing MLflow for PyTorch Lightning\nDESCRIPTION: Installs the MLflow package using pip for use with PyTorch Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/supported_exp_managers.rst#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npip install mlflow\n```\n\n----------------------------------------\n\nTITLE: Setting Current Module for RST Autosummary\nDESCRIPTION: This snippet sets the current module for the RST autosummary directive to lightning.fabric.plugins.collectives.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/collectives.rst#2025-04-23_snippet_4\n\nLANGUAGE: rst\nCODE:\n```\n.. currentmodule:: lightning.fabric.plugins.collectives\n```\n\n----------------------------------------\n\nTITLE: Configuring Multi-Node Training in Fabric\nDESCRIPTION: Examples of setting the number of cluster nodes for distributed operations, allowing training to scale across multiple physical machines in a cluster.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_args.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Default used by Fabric\nfabric = Fabric(num_nodes=1)\n\n# Run on 8 nodes\nfabric = Fabric(num_nodes=8)\n```\n\n----------------------------------------\n\nTITLE: Requirements Specification for PyTorch Lightning Dependencies\nDESCRIPTION: Defines the exact package versions for core dependencies (mypy and torch) and lists required type stub packages for static type checking. Includes type definitions for common Python packages like PyYAML, requests, redis, and other utilities.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/requirements/typing.txt#2025-04-23_snippet_0\n\nLANGUAGE: text\nCODE:\n```\nmypy==1.11.0\ntorch==2.5.1\n\ntypes-Markdown\ntypes-PyYAML\ntypes-bleach\ntypes-cachetools\ntypes-croniter\ntypes-paramiko\ntypes-protobuf\ntypes-python-dateutil\ntypes-redis\ntypes-requests\ntypes-setuptools\ntypes-six\ntypes-tabulate\ntypes-toml\ntypes-tzlocal\ntypes-ujson\ntypes-decorator\n```\n\n----------------------------------------\n\nTITLE: Installing Documentation and Notebook Dependencies\nDESCRIPTION: Requirements file that includes documentation dependencies and notebook-specific packages. Contains version constraints to avoid compatibility issues with IPython setup and includes nbformat for empty notebook generation.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/requirements/pytorch/docs.txt#2025-04-23_snippet_0\n\nLANGUAGE: requirements\nCODE:\n```\n-r ../docs.txt\n\nnbformat  # used for generate empty notebook\nipython[notebook] <8.7.0\nsetuptools<58.0  # workaround for `error in ipython setup command: use_2to3 is invalid.`\n\n#-r ../../_notebooks/.actions/requires.txt\n```\n\n----------------------------------------\n\nTITLE: Running Environment Details Script in Python\nDESCRIPTION: This command executes the previously downloaded collect_env_details.py script to gather and display information about the current Python environment.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/pytorch/bug_report/bug_report_model.ipynb#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n! python collect_env_details.py\n```\n\n----------------------------------------\n\nTITLE: Customizing Lightning Tuner Learning Rate Finder Attribute in Python\nDESCRIPTION: This snippet shows how to use the Tuner's lr_find method with a custom attribute name for setting the learning rate. This is useful when the model uses an arbitrary attribute name instead of lr or learning_rate.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/training_tricks.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nmodel = LitModel()\ntrainer = Trainer(...)\ntuner = Tuner(trainer)\n\n# to set to your own hparams.my_value\ntuner.lr_find(model, attr_name=\"my_value\")\n```\n\n----------------------------------------\n\nTITLE: Defining Pytest Dependencies\nDESCRIPTION: Lists required pytest packages with pinned versions for consistent testing environment. Includes the main pytest framework version 8.3.5 and the doctestplus plugin version 1.4.0.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/requirements/doctests.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\npytest ==8.3.5\npytest-doctestplus ==1.4.0\n```\n\n----------------------------------------\n\nTITLE: Including Transfer Learning RST Documentation\nDESCRIPTION: Include directive to import transfer learning documentation content from an external RST file\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/pretrained.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: transfer_learning.rst\n```\n\n----------------------------------------\n\nTITLE: Specifying Sphinx and Related Package Dependencies for PyTorch Lightning Documentation\nDESCRIPTION: This snippet lists the required packages and their version constraints for building Sphinx documentation in the PyTorch Lightning project. It includes core Sphinx packages, extensions, and utilities necessary for rendering complex documentation with specific features.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/requirements/docs.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\nsphinx >5.0, <6.0\nmyst-parser >=0.18.1, <3.0.0\nnbsphinx >=0.8.5, <=0.9.2\nnbconvert <7.14  # temporary fix for https://github.com/jupyter/nbconvert/issues/2092\npandoc >=1.0, <=2.3\ndocutils >=0.16, <0.21\nsphinxcontrib-fulltoc >=1.0, <=1.2.0\nsphinxcontrib-mockautodoc\nsphinx-autobuild\nsphinx-autodoc-typehints >=1.16\nsphinx-paramlinks >=0.5.1, <=0.6.0\nsphinx-togglebutton >=0.2, <=0.3.2\nsphinx-copybutton >=0.3, <=0.5.2\nsphinx-multiproject\nsphinx-toolbox ==3.5.0\nsphinx-rtd-dark-mode\nsphinxcontrib-video ==0.2.0\njinja2 <3.2.0\n\nlightning-utilities >=0.11.1, <0.15.0\n\n# installed from S3 location and fetched in advance\nlai-sphinx-theme\n```\n\n----------------------------------------\n\nTITLE: Initializing Tensors with Fabric in Python\nDESCRIPTION: Uses Fabric's context manager to efficiently instantiate tensors directly on the target device and dtype. This optimizes memory usage and initialization.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/fabric/CHANGELOG.md#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nFabric.init_tensor()\n```\n\n----------------------------------------\n\nTITLE: Logging Accumulated Metric in PyTorch Lightning Validation Step\nDESCRIPTION: Demonstrates how to log a metric in the validation_step, which will be automatically accumulated and averaged across the epoch.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/logging_basic.rst#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\ndef validation_step(self, batch, batch_idx):\n    value = batch_idx + 1\n    self.log(\"average_value\", value)\n```\n\n----------------------------------------\n\nTITLE: Saving Partial Checkpoints with Filters\nDESCRIPTION: Shows how to save selective parameters in checkpoints using filter functions to reduce checkpoint size.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/checkpoint/checkpoint.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\nstate = {\"model\": model, \"optimizer\": optimizer, \"foo\": 123}\n\n# save only the weights that match a pattern\nfilter = {\"model\": lambda k, v: \"weight\" in k}\nfabric.save(\"path/to/checkpoint.ckpt\", state, filter=filter)\n```\n\n----------------------------------------\n\nTITLE: Running Transformer Example with PyTorch Lightning\nDESCRIPTION: Command to run the Transformer example for next-word prediction. This script uses a Transformer model on a subset of the WikiText2 dataset, demonstrating a simple training loop with PyTorch Lightning.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/pytorch/basics/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\npython transformer.py\n```\n\n----------------------------------------\n\nTITLE: Setting Up Model Checkpointing in PyTorch Lightning\nDESCRIPTION: Example of configuring model checkpointing in PyTorch Lightning. This saves model weights based on validation loss improvements.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/pytorch_lightning/README.md#2025-04-23_snippet_13\n\nLANGUAGE: python\nCODE:\n```\ncheckpointing = ModelCheckpoint(monitor=\"val_loss\")\ntrainer = Trainer(callbacks=[checkpointing])\n```\n\n----------------------------------------\n\nTITLE: Random Access in StreamingDataset\nDESCRIPTION: Shows how to access specific elements in a StreamingDataset using random access. This example demonstrates checking the dataset length and accessing a specific item by index.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/data/README.md#2025-04-23_snippet_11\n\nLANGUAGE: python\nCODE:\n```\nfrom lightning.data import StreamingDataset\n\ndataset = StreamingDataset(...)\n\nprint(len(dataset)) # display the length of your data\n\nprint(dataset[42]) # show the 42th element of the dataset\n```\n\n----------------------------------------\n\nTITLE: Using the Log Directory in PyTorch Lightning\nDESCRIPTION: Demonstrates how to access the log directory for the current experiment to save additional files like images.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/common/trainer.rst#2025-04-23_snippet_37\n\nLANGUAGE: python\nCODE:\n```\ndef training_step(self, batch, batch_idx):\n    img = ...\n    save_img(img, self.trainer.log_dir)\n```\n\n----------------------------------------\n\nTITLE: Running Decoupled PPO with GPU Player and Trainers\nDESCRIPTION: This command runs the decoupled PPO implementation with both player and trainers on GPU, using 3 devices and 4 environments.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/fabric/reinforcement_learning/README.md#2025-04-23_snippet_7\n\nLANGUAGE: bash\nCODE:\n```\nfabric run --devices=3 train_fabric_decoupled.py --num-envs 4 --cuda --player-on-gpu\n```\n\n----------------------------------------\n\nTITLE: Implementing Tensor Parallelism using PyTorch's Distributed Tensor APIs\nDESCRIPTION: Function that applies tensor parallelism to a model using PyTorch's distributed tensor APIs. It applies column-wise parallelism to the first and third linear layers, and row-wise parallelism to the second layer to optimize computation and memory usage.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/model_parallel/tp.rst#2025-04-23_snippet_1\n\nLANGUAGE: python\nCODE:\n```\nfrom torch.distributed.tensor.parallel import ColwiseParallel, RowwiseParallel\nfrom torch.distributed.tensor.parallel import parallelize_module\n\n\ndef parallelize_feedforward(model, device_mesh):\n    # Lightning will set up a device mesh for you\n    tp_mesh = device_mesh[\"tensor_parallel\"]\n    # Use PyTorch's distributed tensor APIs to parallelize the model\n    plan = {\n        \"w1\": ColwiseParallel(),\n        \"w2\": RowwiseParallel(),\n        \"w3\": ColwiseParallel(),\n    }\n    parallelize_module(model, tp_mesh, plan)\n    return model\n```\n\n----------------------------------------\n\nTITLE: Running PPO Example with Raw PyTorch\nDESCRIPTION: This command executes the PPO implementation using raw PyTorch with distributed training across 2 devices.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/fabric/reinforcement_learning/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\ntorchrun --nproc_per_node=2 --standalone train_torch.py\n```\n\n----------------------------------------\n\nTITLE: Internal Logger Implementation in PyTorch Lightning\nDESCRIPTION: Shows the internal implementation of how Fabric handles logging calls across multiple loggers.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/fabric_methods.rst#2025-04-23_snippet_19\n\nLANGUAGE: python\nCODE:\n```\n# When you call .log() or .log_dict(), we do this:\nfor logger in fabric.loggers:\n    logger.log_metrics(metrics=metrics, step=step)\n```\n\n----------------------------------------\n\nTITLE: Loading Checkpoints with Fabric in FSDP\nDESCRIPTION: This code demonstrates how to load checkpoints saved by Fabric to resume training. It shows the correct way to define the state and load it, avoiding inefficient approaches.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/model_parallel/fsdp.rst#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\n# 1. Define model, optimizer, and other training loop state\nstate = {\"model\": model, \"optimizer\": optimizer, \"iter\": iteration}\n\n# 2. Load using Fabric's method\nfabric.load(\"path/to/checkpoint/file\", state)\n\n# DON'T do this (inefficient):\n# model.load_state_dict(torch.load(\"path/to/checkpoint/file\"))\n```\n\n----------------------------------------\n\nTITLE: Structuring PyTorch Lightning Documentation with RST\nDESCRIPTION: RST file structure that includes separate documentation sections for different user types through file includes. Uses orphan directive to prevent warnings about the document not being included in any toctree.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/from_1_9.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n:orphan:\n\nRegular User\n************\n\n.. include:: sections/1_9_regular.rst\n\nAdvanced User\n*************\n\n.. include:: sections/1_9_advanced.rst\n\nDeveloper\n*********\n\n.. include:: sections/1_9_devel.rst\n```\n\n----------------------------------------\n\nTITLE: Setting FSDP Activation Checkpointing Policy in Python\nDESCRIPTION: Customizes the layer policy for automatic activation checkpointing when using FSDP strategy. Requires PyTorch 2.1 or newer.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/fabric/CHANGELOG.md#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nFSDPStrategy(activation_checkpointing_policy=...)\n```\n\n----------------------------------------\n\nTITLE: Configuring FSDP Strategy with CPU Offload in PyTorch Lightning\nDESCRIPTION: This snippet demonstrates how to set up the FSDP strategy with CPU offload enabled. It allows for significant GPU memory savings at the cost of slower training speed due to CPU-GPU communication.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/model_parallel/fsdp.rst#2025-04-23_snippet_6\n\nLANGUAGE: python\nCODE:\n```\n# Set `cpu_offload=True`\nstrategy = FSDPStrategy(..., cpu_offload=True)\nfabric = L.Fabric(..., strategy=strategy)\n```\n\n----------------------------------------\n\nTITLE: Configuring Tensor Parallelism Strategy in PyTorch Lightning Trainer\nDESCRIPTION: Configuration of the ModelParallelStrategy in the Lightning Trainer to enable tensor parallelism across multiple GPUs. This setup distributes the model computation across devices without requiring changes to the training loop.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/tp.rst#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\nfrom lightning.pytorch.strategies import ModelParallelStrategy\n\n# 1. Create the strategy\nstrategy = ModelParallelStrategy()\n\n# 2. Configure devices and set the strategy in Trainer\ntrainer = L.Trainer(accelerator=\"cuda\", devices=2, strategy=strategy)\ntrainer.fit(...)\n```\n\n----------------------------------------\n\nTITLE: Implementing Random Dataset Class in Python\nDESCRIPTION: This snippet defines a RandomDataset class that generates random tensor data for debugging purposes. It creates a dataset with specified size and number of samples using torch.randn().\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/pytorch/bug_report/bug_report_model.ipynb#2025-04-23_snippet_2\n\nLANGUAGE: python\nCODE:\n```\nclass RandomDataset(Dataset):\n    def __init__(self, size, num_samples):\n        self.len = num_samples\n        self.data = torch.randn(num_samples, size)\n\n    def __getitem__(self, index):\n        return self.data[index]\n\n    def __len__(self):\n        return self.len\n```\n\n----------------------------------------\n\nTITLE: Setting Default Config Files in Lightning CLI\nDESCRIPTION: These Python code snippets demonstrate how to set default configuration files for the Lightning CLI. The first example sets a global default, while the second sets a default specific to the 'fit' subcommand.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/cli/lightning_cli_advanced_2.rst#2025-04-23_snippet_4\n\nLANGUAGE: python\nCODE:\n```\ncli = LightningCLI(MyModel, MyDataModule, parser_kwargs={\"default_config_files\": [\"my_cli_defaults.yaml\"]})\n```\n\nLANGUAGE: python\nCODE:\n```\ncli = LightningCLI(MyModel, MyDataModule, parser_kwargs={\"fit\": {\"default_config_files\": [\"my_fit_defaults.yaml\"]}})\n```\n\n----------------------------------------\n\nTITLE: Recommended Method Order in LightningModule (Python)\nDESCRIPTION: Illustrates the recommended order of methods in a LightningModule, including training, validation, test hooks, and optimizer configuration.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/starter/style_guide.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\nclass LitModel(L.LightningModule):\n\n    def __init__(...):\n\n    def forward(...):\n\n    def training_step(...):\n\n    def on_train_epoch_end(...):\n\n    def validation_step(...):\n\n    def on_validation_epoch_end(...):\n\n    def test_step(...):\n\n    def on_test_epoch_end(...):\n\n    def configure_optimizers(...):\n\n    def any_extra_hook(...):\n```\n\n----------------------------------------\n\nTITLE: Setting Environment Variable for Fast Documentation Development in Bash\nDESCRIPTION: This snippet demonstrates how to set an environment variable to enable fast documentation development mode, which speeds up the build process by skipping notebook rendering.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\n# builds notebooks which is slow\nexport FAST_DOCS_DEV=0\n\n# fast notebook build which is fast\nexport FAST_DOCS_DEV=1\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch Lightning via pip\nDESCRIPTION: Command to install PyTorch Lightning using pip package manager\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/index.rst#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install lightning\n```\n\n----------------------------------------\n\nTITLE: Setting Up True Double Precision (64-bit)\nDESCRIPTION: Demonstrates implementation of 64-bit precision for scientific computations requiring higher accuracy, including handling of complex numbers.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/fundamentals/precision.rst#2025-04-23_snippet_8\n\nLANGUAGE: python\nCODE:\n```\n# Select FP64 precision\nfabric = Fabric(precision=\"64-true\")\nmodel = MyModel()\nmodel = fabric.setup(model)  # model gets cast to torch.float64\n```\n\n----------------------------------------\n\nTITLE: Applying torch.compile to LightningModule Submodules\nDESCRIPTION: Demonstrates a workaround for compilation errors with self.log() by applying torch.compile to submodules instead of the entire LightningModule.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/compile.rst#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\nimport lightning as L\n\nclass MyLightningModule(L.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model = MySubModule()\n        self.model = torch.compile(self.model)\n        ...\n```\n\n----------------------------------------\n\nTITLE: FSDP Model Initialization Optimization\nDESCRIPTION: Optimized model initialization approaches for FSDP, including direct GPU initialization and empty initialization for larger models.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/advanced/model_parallel/fsdp.rst#2025-04-23_snippet_3\n\nLANGUAGE: python\nCODE:\n```\n# Fast: Creates the model on the GPU directly\nwith fabric.init_module():\n    model = Transformer(vocab_size=dataset.vocab_size)\n\n# Recommended for FSDP:\nwith fabric.init_module(empty_init=True):\n    model = Transformer(vocab_size=dataset.vocab_size)\n```\n\n----------------------------------------\n\nTITLE: Defining Hidden Role in reStructuredText for PyTorch Lightning Docs\nDESCRIPTION: Defines a custom 'hidden' role for use in documentation, likely for sections that should not be visible in the rendered output.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/_templates/classtemplate.rst#2025-04-23_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. role:: hidden\n    :class: hidden-section\n```\n\n----------------------------------------\n\nTITLE: Migrating checkpoints between PyTorch Lightning versions\nDESCRIPTION: Added utilities to migrate checkpoints from one Lightning version to another.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_19\n\nLANGUAGE: Python\nCODE:\n```\nfrom pytorch_lightning.utilities.upgrade_checkpoint import upgrade_checkpoint\n\nupgraded_ckpt = upgrade_checkpoint(ckpt_path)\n\n```\n\n----------------------------------------\n\nTITLE: Installing Bleeding-Edge Version of PyTorch Lightning\nDESCRIPTION: Commands to install the latest development version of PyTorch Lightning either from GitHub or test PyPI.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/pytorch_lightning/README.md#2025-04-23_snippet_4\n\nLANGUAGE: bash\nCODE:\n```\npip install https://github.com/Lightning-AI/lightning/archive/refs/heads/master.zip -U\n```\n\nLANGUAGE: bash\nCODE:\n```\npip install -iU https://test.pypi.org/simple/ pytorch-lightning\n```\n\n----------------------------------------\n\nTITLE: Including External Links in RST Documentation\nDESCRIPTION: This snippet includes an external file containing links for use in the documentation. It's a common practice in RST to keep links separate for easier maintenance.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/api/io.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n.. include:: ../links.rst\n```\n\n----------------------------------------\n\nTITLE: Installing Requirements for PPO Implementation\nDESCRIPTION: This command installs the necessary dependencies for running the PPO implementation examples.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/fabric/reinforcement_learning/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install -r requirements.txt\n```\n\n----------------------------------------\n\nTITLE: Logging in to Weights and Biases\nDESCRIPTION: Command to authenticate with Weights and Biases using your API key which can be found in your W&B account settings.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-fabric/guide/loggers/wandb.rst#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\nwandb login <your-api-key>\n```\n\n----------------------------------------\n\nTITLE: Loading Checkpoints in PyTorch Lightning\nDESCRIPTION: Shows how to load checkpoints in PyTorch Lightning to resume training, with automatic recognition of checkpoint format.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/advanced/model_parallel/fsdp.rst#2025-04-23_snippet_9\n\nLANGUAGE: python\nCODE:\n```\ntrainer = L.Trainer(...)\n\n# Restore the training progress, weights, and optimizer state\ntrainer.fit(model, ckpt_path=\"path/to/checkpoint/file\")\n```\n\n----------------------------------------\n\nTITLE: Installing Rich Library for Progress Bar\nDESCRIPTION: Command to install the Rich library dependency for enhanced progress bar formatting.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/visualize/logging_expert.rst#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip install rich\n```\n\n----------------------------------------\n\nTITLE: Installing PyTorch Lightning via pip\nDESCRIPTION: Basic installation command for PyTorch Lightning using pip package manager.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/pytorch_lightning/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip install pytorch-lightning\n```\n\n----------------------------------------\n\nTITLE: HTML Container Structure\nDESCRIPTION: Defines the HTML container structure for displaying card items in a responsive grid layout.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/levels/advanced.rst#2025-04-23_snippet_0\n\nLANGUAGE: html\nCODE:\n```\n<div class=\"display-card-container\">\n    <div class=\"row\">\n    </div>\n</div>\n```\n\n----------------------------------------\n\nTITLE: Configuring Model Checkpoint Saving in PyTorch Lightning\nDESCRIPTION: Added option to ModelCheckpoint to choose when to run the saving logic during training.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/src/lightning/pytorch/CHANGELOG.md#2025-04-23_snippet_39\n\nLANGUAGE: python\nCODE:\n```\nModelCheckpoint(save_on_train_epoch_end=bool)\n```\n\n----------------------------------------\n\nTITLE: Setting 16-bit Precision in PyTorch Lightning\nDESCRIPTION: Demonstrates how to enable 16-bit precision training in PyTorch Lightning with a simple configuration change.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/README.md#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# no code changes needed\ntrainer = Trainer(precision=16)\n```\n\n----------------------------------------\n\nTITLE: Defining PyTorch Lightning API Structure in reStructuredText\nDESCRIPTION: This code snippet defines the structure of the PyTorch Lightning API documentation using reStructuredText directives. It organizes components into categories and uses autosummary to generate documentation for each class or function.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/api_references.rst#2025-04-23_snippet_0\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. include:: links.rst\n\naccelerators\n------------\n\n.. currentmodule:: lightning.pytorch.accelerators\n\n.. autosummary::\n    :toctree: api\n    :nosignatures:\n    :template: classtemplate.rst\n\n    Accelerator\n    CPUAccelerator\n    CUDAAccelerator\n    XLAAccelerator\n\ncallbacks\n---------\n\n.. currentmodule:: lightning.pytorch.callbacks\n\n.. autosummary::\n    :toctree: api\n    :nosignatures:\n    :template: classtemplate.rst\n\n    BackboneFinetuning\n    BaseFinetuning\n    BasePredictionWriter\n    BatchSizeFinder\n    Callback\n    DeviceStatsMonitor\n    EarlyStopping\n    GradientAccumulationScheduler\n    LambdaCallback\n    LearningRateFinder\n    LearningRateMonitor\n    ModelCheckpoint\n    ModelPruning\n    ModelSummary\n    OnExceptionCheckpoint\n    ProgressBar\n    RichModelSummary\n    RichProgressBar\n    StochasticWeightAveraging\n    SpikeDetection\n    ThroughputMonitor\n    Timer\n    TQDMProgressBar\n\ncli\n-----\n\n.. currentmodule:: lightning.pytorch.cli\n\n.. autosummary::\n    :toctree: api\n    :nosignatures:\n    :template: classtemplate.rst\n\n    LightningCLI\n    LightningArgumentParser\n    SaveConfigCallback\n\ncore\n----\n\n.. currentmodule:: lightning.pytorch.core\n\n.. autosummary::\n    :toctree: api\n    :nosignatures:\n    :template: classtemplate.rst\n\n    ~hooks.CheckpointHooks\n    ~hooks.DataHooks\n    ~hooks.ModelHooks\n    LightningDataModule\n    LightningModule\n    ~mixins.HyperparametersMixin\n    ~optimizer.LightningOptimizer\n\n\n.. _loggers-api-references:\n\nloggers\n-------\n\n.. currentmodule:: lightning.pytorch.loggers\n\n.. autosummary::\n    :toctree: api\n    :nosignatures:\n\n    logger\n    comet\n    csv_logs\n    mlflow\n    neptune\n    tensorboard\n    wandb\n\nplugins\n^^^^^^^\n\nprecision\n\"\"\"\"\"\"\"\"\n\n.. currentmodule:: lightning.pytorch.plugins.precision\n\n.. autosummary::\n    :toctree: api\n    :nosignatures:\n    :template: classtemplate.rst\n\n    DeepSpeedPrecision\n    DoublePrecision\n    HalfPrecision\n    FSDPPrecision\n    MixedPrecision\n    Precision\n    XLAPrecision\n    TransformerEnginePrecision\n    BitsandbytesPrecision\n\nenvironments\n\"\"\"\"\"\"\"\"\"\"\"\"\n\n.. currentmodule:: lightning.pytorch.plugins.environments\n\n.. autosummary::\n    :toctree: api\n    :nosignatures:\n    :template: classtemplate.rst\n\n    ClusterEnvironment\n    KubeflowEnvironment\n    LightningEnvironment\n    LSFEnvironment\n    MPIEnvironment\n    SLURMEnvironment\n    TorchElasticEnvironment\n    XLAEnvironment\n\nio\n\"\"\n\n.. currentmodule:: lightning.pytorch.plugins.io\n\n.. autosummary::\n    :toctree: api\n    :nosignatures:\n    :template: classtemplate.rst\n\n    AsyncCheckpointIO\n    CheckpointIO\n    TorchCheckpointIO\n    XLACheckpointIO\n\n\nothers\n\"\"\"\"\"\"\n\n.. currentmodule:: lightning.pytorch.plugins\n\n.. autosummary::\n    :toctree: api\n    :nosignatures:\n    :template: classtemplate.rst\n\n    LayerSync\n    TorchSyncBatchNorm\n\nprofiler\n--------\n\n.. currentmodule:: lightning.pytorch.profilers\n\n.. autosummary::\n    :toctree: api\n    :nosignatures:\n    :template: classtemplate.rst\n\n    AdvancedProfiler\n    PassThroughProfiler\n    Profiler\n    PyTorchProfiler\n    SimpleProfiler\n    XLAProfiler\n\ntrainer\n-------\n\n.. currentmodule:: lightning.pytorch.trainer.trainer\n\n.. autosummary::\n    :toctree: api\n    :nosignatures:\n    :template: classtemplate.rst\n\n    Trainer\n\nstrategies\n----------\n\n.. currentmodule:: lightning.pytorch.strategies\n\n.. autosummary::\n    :toctree: api\n    :nosignatures:\n    :template: classtemplate.rst\n\n    DDPStrategy\n    DeepSpeedStrategy\n    FSDPStrategy\n    ModelParallelStrategy\n    ParallelStrategy\n    SingleDeviceStrategy\n    SingleDeviceXLAStrategy\n    Strategy\n    XLAStrategy\n\ntuner\n-----\n\n.. currentmodule:: lightning.pytorch.tuner.tuning\n\n.. autosummary::\n    :toctree: api\n    :nosignatures:\n    :template: classtemplate.rst\n\n    Tuner\n\nutilities\n---------\n\n.. currentmodule:: lightning.pytorch.utilities\n\n.. autosummary::\n    :toctree: api\n    :nosignatures:\n\n    combined_loader\n    data\n    deepspeed\n    memory\n    model_summary\n    parsing\n    rank_zero\n    seed\n    warnings\n\n.. autofunction:: lightning.pytorch.utilities.measure_flops\n```\n\n----------------------------------------\n\nTITLE: RST Documentation Structure for PyTorch Lightning Guide\nDESCRIPTION: RST markup defining the structure and layout of the intermediate skills documentation page, including display cards for different learning levels. Uses RST directives for creating a grid-based layout of educational content.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/levels/intermediate.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n################### \nIntermediate skills\n###################\n\nLearn to scale up your models and enable collaborative model development at academic or industry research labs.\n\n.. include:: ../links.rst\n\n.. raw:: html\n\n    <div class=\"display-card-container\">\n        <div class=\"row\">\n\n.. displayitem::\n   :header: Level 7: Interactive cloud development\n   :description: Learn how to access GPUs and TPUs on the cloud.\n   :button_link: intermediate_level_7.html\n   :col_css: col-md-6\n   :height: 150\n   :tag: intermediate\n\n.. displayitem::\n   :header: Level 8: Modularize your projects\n   :description: Create DataModules to enable dataset reusability.\n   :col_css: col-md-6\n   :button_link: intermediate_level_9.html\n   :height: 150\n   :tag: intermediate\n\n.. displayitem::\n   :header: Level 9: Understand your model\n   :description: Use advanced visuals to find the best performing model.\n   :col_css: col-md-6\n   :button_link: intermediate_level_10.html\n   :height: 150\n   :tag: intermediate\n\n.. displayitem::\n   :header: Level 10: Explore SOTA scaling techniques\n   :description: Explore SOTA techniques to help convergence, stability and scalability.\n   :col_css: col-md-6\n   :button_link: intermediate_level_11.html\n   :height: 150\n   :tag: intermediate\n\n.. displayitem::\n   :header: Level 11: Deploy your models\n   :description: Learn how to deploy your models with optimizations like ONNX and torchscript.\n   :col_css: col-md-6\n   :button_link: intermediate_level_12.html\n   :height: 150\n   :tag: intermediate\n\n.. displayitem::\n   :header: Level 12: Optimize training speed\n   :description: Use compilers, advanced profilers and mixed precision to train bigger models, faster.\n   :col_css: col-md-6\n   :button_link: intermediate_level_13.html\n   :height: 150\n   :tag: intermediate\n\n.. displayitem::\n   :header: Level 13: Run on a multi-node cluster\n   :description: Learn to run on multi-node in the cloud or on your cluster\n   :col_css: col-md-6\n   :button_link: intermediate_level_14.html\n   :height: 150\n   :tag: intermediate\n\n.. raw:: html\n\n        </div>\n    </div>\n```\n\n----------------------------------------\n\nTITLE: Migrating Metrics to Scalars Conversion\nDESCRIPTION: Change from TrainerLoggingMixin.metrics_to_scalars to pl.utilities.metrics.metrics_to_scalars.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/sections/1_4_advanced.rst#2025-04-23_snippet_5\n\nLANGUAGE: python\nCODE:\n```\n# Old way\nTrainerLoggingMixin.metrics_to_scalars\n\n# New way\npl.utilities.metrics.metrics_to_scalars\n```\n\n----------------------------------------\n\nTITLE: Structuring PyTorch Lightning Upgrade Guide in RST\nDESCRIPTION: This RST code structures the upgrade guide for PyTorch Lightning, organizing content for different user types and version increments. It uses RST directives to include external files for each section.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/upgrade/from_1_8.rst#2025-04-23_snippet_0\n\nLANGUAGE: rst\nCODE:\n```\n:orphan:\n\nUpgrade from 1.8 to the 2.0\n###########################\n\nRegular User\n************\n\n.. include:: sections/1_8_regular.rst\n.. include:: sections/1_9_regular.rst\n\nAdvanced User\n*************\n\n.. include:: sections/1_8_advanced.rst\n.. include:: sections/1_9_advanced.rst\n\nDeveloper\n*********\n\n.. include:: sections/1_8_devel.rst\n.. include:: sections/1_9_devel.rst\n```\n\n----------------------------------------\n\nTITLE: Setting Current Module for PyTorch Lightning Class Documentation\nDESCRIPTION: Sets the current module context for the documentation. The actual module name is dynamically inserted using a template variable.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/_templates/classtemplate.rst#2025-04-23_snippet_1\n\nLANGUAGE: reStructuredText\nCODE:\n```\n.. currentmodule:: {{ module }}\n```\n\n----------------------------------------\n\nTITLE: Configuring Ecosystem CI YAML Structure\nDESCRIPTION: Example structure for creating a new configuration file in the configs directory. The YAML file defines runtime environments for CPU testing via GitHub Actions and references GPU configurations for Azure pipelines.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/docs/source-pytorch/community/ecosystem-ci.rst#2025-04-23_snippet_0\n\nLANGUAGE: yaml\nCODE:\n```\n# configs/<Organization-name>/<project-name>.yaml\n# Define runtime for CPU and link config for GPU testing\n```\n\n----------------------------------------\n\nTITLE: Specifying torchao Dependency Version\nDESCRIPTION: Specifies the minimum required version of the torchao package as 0.7.0 or higher.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/examples/fabric/fp8_distributed_transformer/requirements.txt#2025-04-23_snippet_0\n\nLANGUAGE: plaintext\nCODE:\n```\ntorchao>=0.7.0\n```\n\n----------------------------------------\n\nTITLE: Defining Dependencies with Version Constraints for PyTorch Lightning\nDESCRIPTION: A comprehensive list of Python package dependencies required for PyTorch Lightning, including specific version constraints. Each dependency has a minimum required version and an upper bound that is primarily for CI stability purposes.\nSOURCE: https://github.com/lightning-ai/pytorch-lightning/blob/master/requirements/pytorch/base.txt#2025-04-23_snippet_0\n\nLANGUAGE: plain\nCODE:\n```\ntorch >=2.1.0, <2.6.0\ntqdm >=4.57.0, <4.67.0\nPyYAML >=5.4, <6.1.0\nfsspec[http] >=2022.5.0, <2024.4.0\ntorchmetrics >=0.7.0, <1.8.0\npackaging >=20.0, <=25.0\ntyping-extensions >=4.4.0, <4.11.0\nlightning-utilities >=0.10.0, <0.15.0\n```"
  }
]
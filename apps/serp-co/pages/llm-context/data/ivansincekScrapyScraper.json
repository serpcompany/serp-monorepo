[
  {
    "owner": "ivan-sincek",
    "repo": "scrapy-scraper",
    "content": "TITLE: Scrapy Scraper Usage Documentation\nDESCRIPTION: Comprehensive usage documentation showing all available command-line options for the Scrapy Scraper tool, including options for URLs, whitelisting, Playwright settings, request configuration, and output control.\nSOURCE: https://github.com/ivan-sincek/scrapy-scraper/blob/main/README.md#2025-04-23_snippet_6\n\nLANGUAGE: fundamental\nCODE:\n```\nScrapy Scraper v3.6 ( github.com/ivan-sincek/scrapy-scraper )\n\nUsage:   scrapy-scraper -u urls                     -o out         [-dir directory]\nExample: scrapy-scraper -u https://example.com/home -o results.txt [-dir downloads]\n\nDESCRIPTION\n    Crawl and scrape websites\nURLs\n    File containing URLs or a single URL to start crawling and scraping from\n    -u, --urls = urls.txt | https://example.com/home | etc.\nWHITELIST\n    File containing whitelisted domain names to limit the scope\n    Specify 'off' to disable domain whitelisting\n    Default: limit the scope to domain names extracted from the URLs\n    -w, --whitelist = whitelist.txt | off | etc.\nLINKS\n    Include all 3rd party links and sources in the output file\n    -l, --links\nPLAYWRIGHT\n    Use Playwright's headless browser\n    -p, --playwright\nPLAYWRIGHT WAIT\n    Wait time in seconds before fetching the page content\n    -pw, --playwright-wait = 0.5 | 2 | 4 | etc.\nCONCURRENT REQUESTS\n    Number of concurrent requests\n    Default: 30\n    -cr, --concurrent-requests = 30 | 45 | etc.\nCONCURRENT REQUESTS PER DOMAIN\n    Number of concurrent requests per domain\n    Default: 10\n    -crd, --concurrent-requests-domain = 10 | 15 | etc.\nSLEEP\n    Sleep time in seconds between two consecutive requests to the same domain\n    -s, --sleep = 1.5 | 3 | etc.\nRANDOM SLEEP\n    Randomize the sleep time between requests to vary between '0.5 * sleep' and '1.5 * sleep'\n    -rs, --random-sleep\nAUTO THROTTLE\n    Auto throttle concurrent requests based on the load and latency\n    Sleep time is still respected\n    -at, --auto-throttle = 0.5 | 10 | 15 | 45 | etc.\nRETRIES\n    Number of retries per URL\n    Default: 2\n    -rt, --retries = 0 | 4 | etc.\nRECURSION\n    Recursion depth limit\n    Specify '0' for no limit\n    Default: 1\n    -r, --recursion = 0 | 2 | etc.\nREQUEST TIMEOUT\n    Request timeout in seconds\n    Default: 60\n    -t, --request-timeout = 30 | 90 | etc.\nHEADER\n    Specify any number of extra HTTP request headers\n    -H, --header = \"Authorization: Bearer ey...\" | etc.\nCOOKIE\n    Specify any number of extra HTTP cookies\n    -b, --cookie = PHPSESSIONID=3301 | etc.\nUSER AGENT\n    User agent to use\n    Default: Scrapy Scraper/3.6\n    -a, --user-agent = random[-all] | curl/3.30.1 | etc.\nPROXY\n    Web proxy to use\n    -x, --proxy = http://127.0.0.1:8080 | etc.\nDIRECTORY\n    Output directory\n    All extracted JavaScript files will be saved in this directory\n    -dir, --directory = downloads | etc.\nOUT\n    Output file\n    -o, --out = results.txt | etc.\nDEBUG\n    Enable debug output\n    -dbg, --debug\n```\n\n----------------------------------------\n\nTITLE: Standard Installation of Scrapy Scraper via pip\nDESCRIPTION: Command to install the Scrapy Scraper package using pip package manager.\nSOURCE: https://github.com/ivan-sincek/scrapy-scraper/blob/main/README.md#2025-04-23_snippet_1\n\nLANGUAGE: bash\nCODE:\n```\npip3 install --upgrade scrapy-scraper\n```\n\n----------------------------------------\n\nTITLE: Installing Playwright and Chromium for Scrapy Scraper\nDESCRIPTION: Commands to install Playwright and Chromium browser for headless browsing capabilities. After upgrading Playwright, Chromium should be re-installed to avoid errors.\nSOURCE: https://github.com/ivan-sincek/scrapy-scraper/blob/main/README.md#2025-04-23_snippet_0\n\nLANGUAGE: bash\nCODE:\n```\npip3 install --upgrade playwright\n\nplaywright install chromium\n```\n\n----------------------------------------\n\nTITLE: Running Scrapy Scraper with Domain Restriction\nDESCRIPTION: Example command for crawling with domain restriction to only example.com, including only links to the same domain with JavaScript extraction.\nSOURCE: https://github.com/ivan-sincek/scrapy-scraper/blob/main/README.md#2025-04-23_snippet_3\n\nLANGUAGE: fundamental\nCODE:\n```\nscrapy-scraper -u https://example.com/home -o results.txt -a random -s 2 -rs -dir js\n```\n\n----------------------------------------\n\nTITLE: Running Scrapy Scraper with Third-Party Links\nDESCRIPTION: Example command for crawling with domain restriction but including both internal and external links, with JavaScript extraction.\nSOURCE: https://github.com/ivan-sincek/scrapy-scraper/blob/main/README.md#2025-04-23_snippet_4\n\nLANGUAGE: fundamental\nCODE:\n```\nscrapy-scraper -u https://example.com/home -o results.txt -a random -s 2 -rs -dir js -l\n```\n\n----------------------------------------\n\nTITLE: Building and Installing Scrapy Scraper from Source\nDESCRIPTION: Series of commands to clone the repository, build the package, and install it from the built wheel file.\nSOURCE: https://github.com/ivan-sincek/scrapy-scraper/blob/main/README.md#2025-04-23_snippet_2\n\nLANGUAGE: bash\nCODE:\n```\ngit clone https://github.com/ivan-sincek/scrapy-scraper && cd scrapy-scraper\n\npython3 -m pip install --upgrade build\n\npython3 -m build\n\npython3 -m pip install dist/scrapy-scraper-3.6-py3-none-any.whl\n```\n\n----------------------------------------\n\nTITLE: Running Scrapy Scraper without Domain Restriction\nDESCRIPTION: Example command for unrestricted crawling that includes all links with JavaScript extraction, by disabling the whitelist feature.\nSOURCE: https://github.com/ivan-sincek/scrapy-scraper/blob/main/README.md#2025-04-23_snippet_5\n\nLANGUAGE: fundamental\nCODE:\n```\nscrapy-scraper -u https://example.com/home -o results.txt -a random -s 2 -rs -dir js -w off\n```"
  }
]